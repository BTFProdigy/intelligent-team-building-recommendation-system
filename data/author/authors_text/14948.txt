Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1051?1055,
Prague, June 2007. c?2007 Association for Computational Linguistics
Frustratingly Hard Domain Adaptation for Dependency Parsing
Mark Dredze1 and John Blitzer1 and Partha Pratim Talukdar1 and
Kuzman Ganchev1 and Joa?o V. Grac?a2 and Fernando Pereira1
1CIS Dept., University of Pennsylvania, Philadelphia, PA 19104
{mdredze|blitzer|partha|kuzman|pereira}@seas.upenn.edu
2L2F ? INESC-ID Lisboa/IST, Rua Alves Redol 9, 1000-029, Lisboa, Portugal
javg@l2f.inesc-id.pt
Abstract
We describe some challenges of adaptation
in the 2007 CoNLL Shared Task on Domain
Adaptation. Our error analysis for this task
suggests that a primary source of error is
differences in annotation guidelines between
treebanks. Our suspicions are supported by
the observation that no team was able to im-
prove target domain performance substan-
tially over a state of the art baseline.
1 Introduction
Dependency parsing, an important NLP task, can be
done with high levels of accuracy. However, adapt-
ing parsers to new domains without target domain
labeled training data remains an open problem. This
paper outlines our participation in the 2007 CoNLL
Shared Task on Domain Adaptation (Nivre et al,
2007). The goal was to adapt a parser trained on
a single source domain to a new target domain us-
ing only unlabeled data. We were given around
15K sentences of labeled text from the Wall Street
Journal (WSJ) (Marcus et al, 1993; Johansson and
Nugues, 2007) as well as 200K unlabeled sentences.
The development data was 200 sentences of labeled
biomedical oncology text (BIO, the ONCO portion
of the Penn Biomedical Treebank), as well as 200K
unlabeled sentences (Kulick et al, 2004). The two
test domains were a collection of medline chem-
istry abstracts (pchem, the CYP portion of the Penn
Biomedical Treebank) and the Child Language Data
Exchange System corpus (CHILDES) (MacWhin-
ney, 2000; Brown, 1973). We used the second or-
der two stage parser and edge labeler of McDonald
et al (2006), which achieved top results in the 2006
CoNLL-X shared task. Preliminary experiments in-
dicated that the edge labeler was fairly robust to do-
main adaptation, lowering accuracy by 3% in the de-
velopment domain as opposed to 2% in the source,
so we focused on unlabeled dependency parsing.
Our system did well, officially coming in 3rd
place out of 12 teams and within 1% of the top sys-
tem (Table 1). 1 In unlabeled parsing, we scored
1st and 2nd on CHILDES and pchem respectively.
However, our results were obtained without adap-
tation. Given our position in the ranking, this sug-
gests that no team was able to significantly improve
performance on either test domain beyond that of a
state-of-the-art parser.
After much effort in developing adaptation meth-
ods, it is critical to understand the causes of these
negative results. In what follows, we provide an er-
ror analysis that attributes domain loss for this task
to a difference in annotation guidelines between do-
mains. We then overview our attempts to improve
adaptation. While we were able to show limited
adaptation on reduced training data or with first-
order features, no modifications improved parsing
with all the training data and second-order features.
2 Parsing Challenges
We begin with an error analysis for adaptation be-
tween WSJ and BIO. We divided the available WSJ
data into a train and test set, trained a parser on
the train set and compared errors on the test set
and BIO. Accuracy dropped from 90% on WSJ to
84% on BIO. We then computed the fraction of er-
rors involving each POS tag. For the most common
1While only 8 teams participated in the closed track with us,
our score beat all of the teams in the open track.
1051
pchem l pchem ul childes ul bio ul
Ours 80.22 83.38 61.37 83.93
Best 81.06 83.42 61.37 -
Mean 73.03 76.42 57.89 -
Rank 3rd 2nd 1st -
Table 1: Official labeled (l) and other unlabeled (ul)
submitted results for the two test domains (pchem
and childes) and development data accuracy (bio).
The parser was trained on the provided WSJ data.
POS types, the loss (difference in source and tar-
get error) was: verbs (2%), conjunctions (5%), dig-
its (23%), prepositions (4%), adjectives (3%), de-
terminers (4%) and nouns (9%). 2 Two POS types
stand out: digits and nouns. Digits are less than
4% of the tokens in BIO. Errors result from the BIO
annotations for long sequences of digits which do
not appear in WSJ. Since these annotations are new
with respect to the WSJ guidelines, it is impossi-
ble to parse these without injecting knowledge of
the annotation guidelines. 3 Nouns are far more
common, comprising 33% of BIO and 30% of WSJ
tokens, the most popular POS tag by far. Addi-
tionally, other POS types listed above (adjectives,
prepositions, determiners, conjunctions) often attach
to nouns. To confirm that nouns were problem-
atic, we modified a first-order parser (no second or-
der features) by adding a feature indicating correct
noun-noun edges, forcing the parser to predict these
edges correctly. Adaptation performance rose on
BIO from 78% without the feature to 87% with the
feature. This indicates that most of the loss comes
from missing these edges.
The primary problem for nouns is the difference
between structures in each domain. The annota-
tion guidelines for the Penn Treebank flattened noun
phrases to simplify annotation (Marcus et al, 1993),
so there is no complex structure to NPs. Ku?bler
(2006) showed that it is difficult to compare the
Penn Treebank to other treebanks with more com-
plex noun structures, such as BIO. Consider theWSJ
phrase ?the New York State Insurance Department?.
The annotation indicates a flat structure, where ev-
2We measured these drops on several other dependency
parsers and found similar results.
3For example, the phrase ?(R = 28% (10/26); K=10% (3/29);
chi2 test: p=0.014).?
ery token is headed by ?Department?. In contrast,
a similar BIO phrase has a very different structure,
pursuant to the BIO guidelines. For ?the detoxi-
cation enzyme glutathione transferase P1-1?, ?en-
zyme? is the head of the NP, ?P1-1? is the head of
?transferase?, and ?transferase? is the head of ?glu-
tathione?. Since the guidelines differ, we observe no
corresponding structure in the WSJ. It is telling that
the parser labels this BIO example by attaching ev-
ery token to the final proper noun ?P1-1?, exactly as
the WSJ guidelines indicate. Unlabeled data cannot
indicate that BIO uses a different standard.
Another problem concerns appositives. For ex-
ample, the phrase ?Howard Mosher, president and
chief executive officer,? has ?Mosher? as the head
of ?Howard? and of the appositive NP delimited by
commas. While similar constructions occur in BIO,
there are no commas to indicate this. An example is
the above BIO NP, in which the phrase ?glutathione
transferase P1-1? is an appositive indicating which
?enzyme? is meant. However, since there are no
commas, the parser thinks ?P1-1? is the head. How-
ever, there are not many right to left attaching nouns.
In addition to a change in the annotation guide-
lines for NPs, we observed an important difference
in the distribution of POS tags. NN tags were almost
twice as likely in the BIO domain (14% in WSJ and
25% in BIO). NNP tags, which are close to 10% of
the tags in WSJ, are nonexistent in BIO (.24%). The
cause for this is clear when the annotation guide-
lines are considered. The proper nouns in WSJ are
names of companies, people and places, while in
BIO they are names of genes, proteins and chemi-
cals. However, for BIO these nouns are labeled NN
instead of NNP. This decision effectively removes
NNP from the BIO domain and renders all features
that depend on the NNP tag ineffective. In our above
BIO NP example, all nouns are labeled NN, whereas
the WSJ example contains NNP tags. The largest
tri-gram differences involve nouns, such as NN-NN-
NN, NNP-NNP-NNP, NN-IN-NN, and IN-NN-NN.
However, when we examine the coarse POS tags,
which do not distinguish between nouns, these dif-
ferences disappear. This indicates that while the
overall distribution of POS tags is similar between
the domains, the fine grained tags differ. These fine
grained tags provide more information than coarse
tags; experiments that removed fine grained tags
1052
hurt WSJ performance but did not affect BIO.
Finally, we examined the effect of unknown
words. Not surprisingly, the most significant dif-
ferences in error rates concerned dependencies be-
tween words of which one or both were unknown
to the parser. For two words that were seen in the
training data loss was 4%, for a single unknown
word loss was 15%, and 26% when both words were
unknown. Both words were unknown only 5% of
the time in BIO, while one of the words being un-
known was more common, reflecting 27% of deci-
sions. Upon further investigation, the majority of
unknown words were nouns, which indicates that
unknown word errors were caused by the problems
discussed above.
Recent theoretical work on domain adapta-
tion (Ben-David et al, 2006) attributes adaptation
loss to two sources: the difference in the distribu-
tion between domains and the difference in label-
ing functions. Adaptation techniques focus on the
former since it is impossible to determine the lat-
ter without knowledge of the labeling function. In
parsing adaptation, the former corresponds to a dif-
ference between the features seen in each domain,
such as new words in the target domain. The de-
cision function corresponds to differences between
annotation guidelines between two domains. Our er-
ror analysis suggests that the primary cause of loss
from adaptation is from differences in the annotation
guidelines themselves. Therefore, significant im-
provements cannot be made without specific knowl-
edge of the target domain?s annotation standards. No
amount of source training data can help if no rele-
vant structure exists in the data. Given the results
for the domain adaptation track, it appears no team
successfully adapted a state-of-the-art parser.
3 Adaptation Approaches
We survey the main approaches we explored for this
task. While some of these approaches provided a
modest performance boost to a simple parser (lim-
ited data and first-order features), no method added
any performance to our best parser (all data and
second-order features).
3.1 Features
A natural approach to improving parsing is to mod-
ify the feature set, both by removing features less
likely to transfer and by adding features that are
more likely to transfer. We began with the first ap-
proach and removed a large number of features that
we believed transfered poorly, such as most features
for noun-noun edges. We obtained a small improve-
ment in BIO performance on limited data only. We
then added several different types of features, specif-
ically designed to improve noun phrase construc-
tions, such as features based on the lexical position
of nouns (common position in NPs), frequency of
occurrence, and NP chunking information. For ex-
ample, trained on in-domain data, nouns that occur
more often tend to be heads. However, none of these
features transfered between domains.
A final type of feature we added was based on
the behavior of nouns, adjectives and verbs in each
domain. We constructed a feature representation
of words based on adjacent POS and words and
clustered words using an algorithm similar to that
of Saul and Pereira (1997). For example, our clus-
tering algorithm grouped first names in one group
and measurements in another. We then added the
cluster membership as a lexical feature to the parser.
None of the resulting features helped adaptation.
3.2 Diversity
Training diversity may be an effective source for
adaptation. We began by adding information from
multiple different parsers, which has been shown
to improve in-domain parsing. We added features
indicating when an edge was predicted by another
parser and if an edge crossed a predicted edge, as
well as conjunctions with edge types. This failed
to improve BIO accuracy since these features were
less reliable at test time. Next, we tried instance
bagging (Breiman, 1996) to generate some diversity
among parsers. We selected with replacement 2000
training examples from the training data and trained
three parsers. Each parser then tagged the remain-
ing 13K sentences, yielding 39K parsed sentences.
We then shuffled these sentences and trained a final
parser. This failed to improve performance, possibly
because of conflicting annotations or because of lack
of sufficient diversity. To address conflicting annota-
1053
tions, we added slack variables to the MIRA learn-
ing algorithm (Crammer et al, 2006) used to train
the parsers, without success. We measured diversity
by comparing the parses of each model. The dif-
ference in annotation agreement between the three
instance bagging parsers was about half the differ-
ence between these parsers and the gold annotations.
While we believe this is not enough diversity, it was
not feasible to repeat our experiment with a large
number of parsers.
3.3 Target Focused Learning
Another approach to adaptation is to favor training
examples that are similar to the target. We first mod-
ified the weight given by the parser to each training
sentence based on the similarity of the sentence to
target domain sentences. This can be done by mod-
ifying the loss to limit updates in cases where the
sentence does not reflect the target domain. We tried
a number of criteria to weigh sentences without suc-
cess, including sentence length and number of verbs.
Next, we trained a discriminative model on the pro-
vided unlabeled data to predict the domain of each
sentence based on POS n-grams in the sentence.
Training sentences with a higher probability of be-
ing in the target domain received higher weights,
also without success. Further experiments showed
that any decrease in training data hurt parser perfor-
mance. It would seem that the parser has no dif-
ficulty learning important training sentences in the
presence of unimportant training examples.
A related idea focused on words, weighing highly
tokens that appeared frequently in the target domain.
We scaled the loss associated with a token by a fac-
tor proportional to its frequency in the target do-
main. We found certain scaling techniques obtained
tiny improvements on the target domain that, while
significant compared to competition results, are not
statistically significant. We also attempted a sim-
ilar approach on the feature level. A very predic-
tive source domain feature is not useful if it does
not appear in the target domain. However, limiting
the feature space to target domain features had no
effect. Instead, we scaled each feature?s value by a
factor proportional to its frequency in the target do-
main and trained the parser on these scaled feature
values. We obtained small improvements on small
amounts of training data.
4 Future Directions
Given our pessimistic analysis and the long list of
failed methods, one may wonder if parser adapta-
tion is possible at all. We believe that it is. First,
there may be room for adaptation with our domains
if a common annotation scheme is used. Second,
we have stressed that typical adaptation, modifying
a model trained on the source domain, will fail but
there may be unsupervised parsing techniques that
improve performance after adaptation, such as a rule
based NP parser for BIO based on knowledge of the
annotations. However, this approach is unsatisfying
as it does not allow general purpose adaptation.
5 Acknowledgments
We thank Joel Wallenberg and Nikhil Dinesh for
their informative and helpful linguistic expertise,
Kevin Lerman for his edge labeler code, and Koby
Crammer for helpful conversations. Dredze is sup-
ported by a NDSEG fellowship; Ganchev and Taluk-
dar by NSF ITR EIA-0205448; and Blitzer by
DARPA under Contract No. NBCHD03001. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
author(s) and do not necessarily reflect the views of
the DARPA or the Department of Interior-National
Business Center (DOI-NBC).
References
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2006. Analysis of representations for
domain adaptation. In NIPS.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
R. Brown. 1973. A First Language: The Early Stages.
Harvard University Press.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585, Mar.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
Sandra Ku?bler. 2006. How do treebank annotation
schemes influence parsing results? or how not to com-
pare apples and oranges. In RANLP.
1054
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technol-
ogy Conference and the Annual Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL).
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency parsing with a two-
stage discriminative parser. In Conference on Natural
Language Learning (CoNLL).
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
Lawrence Saul and Fernando Pereira. 1997. Aggre-
gate and mixed-order markov models for statistical
language modeling. In EMNLP.
1055
Proceedings of ACL-08: HLT, pages 986?993,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Better Alignments = Better Translations?
Kuzman Ganchev
Computer & Information Science
University of Pennsylvania
kuzman@cis.upenn.edu
Joa?o V. Grac?a
L2F INESC-ID
Lisboa, Portugal
javg@l2f.inesc-id.pt
Ben Taskar
Computer & Information Science
University of Pennsylvania
taskar@cis.upenn.edu
Abstract
Automatic word alignment is a key step in
training statistical machine translation sys-
tems. Despite much recent work on word
alignment methods, alignment accuracy in-
creases often produce little or no improve-
ments in machine translation quality. In
this work we analyze a recently proposed
agreement-constrained EM algorithm for un-
supervised alignment models. We attempt to
tease apart the effects that this simple but ef-
fective modification has on alignment preci-
sion and recall trade-offs, and how rare and
common words are affected across several lan-
guage pairs. We propose and extensively eval-
uate a simple method for using alignment
models to produce alignments better-suited
for phrase-based MT systems, and show sig-
nificant gains (as measured by BLEU score)
in end-to-end translation systems for six lan-
guages pairs used in recent MT competitions.
1 Introduction
The typical pipeline for a machine translation (MT)
system starts with a parallel sentence-aligned cor-
pus and proceeds to align the words in every sen-
tence pair. The word alignment problem has re-
ceived much recent attention, but improvements in
standard measures of word alignment performance
often do not result in better translations. Fraser and
Marcu (2007) note that none of the tens of papers
published over the last five years has shown that
significant decreases in alignment error rate (AER)
result in significant increases in translation perfor-
mance. In this work, we show that by changing
the way the word alignment models are trained and
used, we can get not only improvements in align-
ment performance, but also in the performance of
the MT system that uses those alignments.
We present extensive experimental results evalu-
ating a new training scheme for unsupervised word
alignment models: an extension of the Expecta-
tion Maximization algorithm that allows effective
injection of additional information about the desired
alignments into the unsupervised training process.
Examples of such information include ?one word
should not translate to many words? or that direc-
tional translation models should agree. The gen-
eral framework for the extended EM algorithm with
posterior constraints of this type was proposed by
(Grac?a et al, 2008). Our contribution is a large scale
evaluation of this methodology for word alignments,
an investigation of how the produced alignments dif-
fer and how they can be used to consistently improve
machine translation performance (as measured by
BLEU score) across many languages on training cor-
pora with up to hundred thousand sentences. In 10
out of 12 cases we improve BLEU score by at least 14
point and by more than 1 point in 4 out of 12 cases.
After presenting the models and the algorithm in
Sections 2 and 3, in Section 4 we examine how
the new alignments differ from standard models, and
find that the newmethod consistently improves word
alignment performance, measured either as align-
ment error rate or weighted F-score. Section 5 ex-
plores how the new alignments lead to consistent
and significant improvement in a state of the art
phrase base machine translation by using posterior
decoding rather than Viterbi decoding. We propose
a heuristic for tuning posterior decoding in the ab-
sence of annotated alignment data and show im-
provements over baseline systems for six different
986
language pairs used in recent MT competitions.
2 Statistical word alignment
Statistical word alignment (Brown et al, 1994) is
the task identifying which words are translations of
each other in a bilingual sentence corpus. Figure
2 shows two examples of word alignment of a sen-
tence pair. Due to the ambiguity of the word align-
ment task, it is common to distinguish two kinds of
alignments (Och and Ney, 2003). Sure alignments
(S), represented in the figure as squares with bor-
ders, for single-word translations and possible align-
ments (P), represented in the figure as alignments
without boxes, for translations that are either not ex-
act or where several words in one language are trans-
lated to several words in the other language. Possi-
ble alignments can can be used either to indicated
optional alignments, such as the translation of an
idiom, or disagreement between annotators. In the
figure red/black dots indicates correct/incorrect pre-
dicted alignment points.
2.1 Baseline word alignment models
We focus on the hidden Markov model (HMM) for
alignment proposed by (Vogel et al, 1996). This is
a generalization of IBM models 1 and 2 (Brown et
al., 1994), where the transition probabilities have a
first-order Markov dependence rather than a zeroth-
order dependence. The model is an HMM, where the
hidden states take values from the source language
words and generate target language words according
to a translation table. The state transitions depend on
the distance between the source language words. For
source sentence s the probability of an alignment a
and target sentence t can be expressed as:
p(t,a | s) =
?
j
pd(aj |aj ? aj?1)pt(tj |saj ), (1)
where aj is the index of the hidden state (source lan-
guage index) generating the target language word at
index j. As usual, a ?null? word is added to the
source sentence. Figure 1 illustrates the mapping be-
tween the usual HMM notation and the HMM align-
ment model.
2.2 Baseline training
All word alignment models we consider are nor-
mally trained using the Expectation Maximization
s1 s1
s2 s3
we know
the way
sabemos       el       camino      null
usual HMM word alignment meaning
Si (hidden) source language word i
Oj (observed) target language word j
aij (transition) distortion model
bij (emission) translation model
Figure 1: Illustration of an HMM for word alignment.
(EM) algorithm (Dempster et al, 1977). The EM
algorithm attempts to maximize the marginal likeli-
hood of the observed data (s, t pairs) by repeatedly
finding a maximal lower bound on the likelihood and
finding the maximal point of the lower bound. The
lower bound is constructed by using posterior proba-
bilities of the hidden alignments (a) and can be opti-
mized in closed form from expected sufficient statis-
tics computed from the posteriors. For the HMM
alignment model, these posteriors can be efficiently
calculated by the Forward-Backward algorithm.
3 Adding agreement constraints
Grac?a et al (2008) introduce an augmentation of the
EM algorithm that uses constraints on posteriors to
guide learning. Such constraints are useful for sev-
eral reasons. As with any unsupervised induction
method, there is no guarantee that the maximum
likelihood parameters correspond to the intended
meaning for the hidden variables, that is, more accu-
rate alignments using the resulting model. Introduc-
ing additional constraints into the model often re-
sults in intractable decoding and search errors (e.g.,
IBM models 4+). The advantage of only constrain-
ing the posteriors during training is that the model
remains simple while respecting more complex re-
quirements. For example, constraints might include
?one word should not translate to many words? or
that translation is approximately symmetric.
The modification is to add a KL-projection step
after the E-step of the EM algorithm. For each sen-
tence pair instance x = (s, t), we find the posterior
987
distribution p?(z|x) (where z are the alignments). In
regular EM, p?(z|x) is used to complete the data and
compute expected counts. Instead, we find the distri-
bution q that is as close as possible to p?(z|x) in KL
subject to constraints specified in terms of expected
values of features f(x, z)
argmin
q
KL(q(z) || p?(z|x)) s.t. Eq[f(x, z)] ? b.
(2)
The resulting distribution q is then used in place
of p?(z|x) to compute sufficient statistics for the
M-step. The algorithm converges to a local maxi-
mum of the log of the marginal likelihood, p?(x) =?
z p?(z,x), penalized by the KL distance of the
posteriors p?(z|x) from the feasible set defined by
the constraints (Grac?a et al, 2008):
Ex[log p?(x)? min
q:Eq [f(x,z)]?b
KL(q(z) || p?(z|x))],
whereEx is expectation over the training data. They
suggest how this framework can be used to encour-
age two word alignment models to agree during
training. We elaborate on their description and pro-
vide details of implementation of the projection in
Equation 2.
3.1 Agreement
Most MT systems train an alignment model in each
direction and then heuristically combine their pre-
dictions. In contrast, Grac?a et al encourage the
models to agree by training them concurrently. The
intuition is that the errors that the two models make
are different and forcing them to agree rules out
errors only made by one model. This is best ex-
hibited in the rare word alignments, where one-
sided ?garbage-collection? phenomenon often oc-
curs (Moore, 2004). This idea was previously pro-
posed by (Matusov et al, 2004; Liang et al, 2006)
although the the objectives differ.
In particular, consider a feature that takes on value
1 whenever source word i aligns to target word j in
the forward model and -1 in the backward model. If
this feature has expected value 0 under the mixture
of the two models, then the forward model and back-
ward model agree on how likely source word i is to
align to target word j. More formally denote the for-
ward model??p (z) and backward model??p (z) where
??p (z) = 0 for z /?
??
Z and ??p (z) = 0 for z /?
??
Z
(
??
Z and
??
Z are possible forward and backward align-
ments). Define a mixture p(z) = 12
??p (z) + 12
??p (z)
for z ?
??
Z ?
??
Z . Restating the constraints that en-
force agreement in this setup: Eq[f(x, z)] = 0 with
fij(x, z) =
8
><
>:
1 z ?
??
Z and zij = 1
?1 z ?
??
Z and zij = 1
0 otherwise
.
3.2 Implementation
EM training of hidden Markov models for word
alignment is described elsewhere (Vogel et al,
1996), so we focus on the projection step:
argmin
q
KL(q(z) || p?(z|x)) s.t. Eq[f(x, z)] = 0.
(3)
The optimization problem in Equation 3 can be effi-
ciently solved in its dual formulation:
argmin
?
log
?
z
p?(z | x) exp {?
>f(x, z)} (4)
where we have solved for the primal variables q as:
q?(z) = p?(z | x) exp{?
>f(x, z)}/Z, (5)
with Z a normalization constant that ensures q sums
to one. We have only one dual variable per con-
straint, and we optimize them by taking a few gra-
dient steps. The partial derivative of the objective
in Equation 4 with respect to feature i is simply
Eq? [fi(x, z)]. So we have reduced the problem to
computing expectations of our features under the
model q. It turns out that for the agreement fea-
tures, this reduces to computing expectations under
the normal HMM model. To see this, we have by the
definition of q? and p?,
q?(z) =
??p (z | x) +??p (z | x)
2
exp{?>f(x, z)}/Z
=
??q (z) +??q (z)
2
.
(To make the algorithm simpler, we have assumed
that the expectation of the feature f0(x, z) =
{1 if z ?
??
Z ; ?1 if z ?
??
Z} is set to zero to
ensure that the two models ??q ,??q are each properly
normalized.) For ??q , we have: (??q is analogous)
??p (z | x)e?
>f(x,z)
=
?
j
??p d(aj |aj ? aj?1)
??p t(tj |saj )
?
ij
e?ijfij(x,zij)
=
?
j,i=aj
??p d(i|i? aj?1)
??p t(tj |si)e?ijfij(x,zij)
=
?
j,i=aj
??p d(i|i? aj?1)
??p ?t(tj |si).
988
Where we have let ??p ?t(tj |si) =
??p t(tj |si)e?ij , and
retained the same form for the model. The final pro-
jection step is detailed in Algorithm1.
Algorithm 1 AgreementProjection(??p ,??p )
1: ?ij ? 0 ?i, j
2: for T iterations do
3: ??p ?t(j|i)?
??p t(tj |si)e?ij ?i, j
4: ??p ?t(i|j)?
??p t(si|tj)e??ij ?i, j
5: ??q ? forwardBackward(??p ?t,
??p d)
6: ??q ? forwardBackward(??p ?t,
??p d)
7: ?ij ? ?ij ?E??q [ai = j] + E??q [aj = i] ?i, j
8: end for
9: return (??q ,??q )
3.3 Decoding
After training, we want to extract a single alignment
from the distribution over alignments allowable for
the model. The standard way to do this is to find
the most probable alignment, using the Viterbi al-
gorithm. Another alternative is to use posterior de-
coding. In posterior decoding, we compute for each
source word i and target word j the posterior prob-
ability under our model that i aligns to j. If that
probability is greater than some threshold, then we
include the point i? j in our final alignment. There
are two main differences between posterior decod-
ing and Viterbi decoding. First, posterior decod-
ing can take better advantage of model uncertainty:
when several likely alignment have high probabil-
ity, posteriors accumulate confidence for the edges
common to many good alignments. Viterbi, by con-
trast, must commit to one high-scoring alignment.
Second, in posterior decoding, the probability that a
0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 80 ? ? ? ? ? ? ? ? ? 0 ? ? ? ? ? ? ? ? ? it1 ? ? ? ? ? ? ? ? ? 1 ? ? ? ? ? ? ? ? ? was2 ? ? ? ? ? ? ? ? ? 2 ? ? ? ? ? ? ? ? ? an3 ? ? ? ? ? ? ? ? ? 3 ? ? ? ? ? ? ? ? ? animated4 ? ? ? ? ? ? ? ? ? 4 ? ? ? ? ? ? ? ? ? ,5 ? ? ? ? ? ? ? ? ? 5 ? ? ? ? ? ? ? ? ? very6 ? ? ? ? ? ? ? ? ? 6 ? ? ? ? ? ? ? ? ? convivial7 ? ? ? ? ? ? ? ? ? 7 ? ? ? ? ? ? ? ? ? game8 ? ? ? ? ? ? ? ? ? 8 ? ? ? ? ? ? ? ? ? .jugaban
de una manera
animada
y muycordial
. jugaban
de una manera
animada
y muycordial
.
Figure 2: An example of the output of HMM trained on
100k the EPPS data. Left: Baseline training. Right: Us-
ing agreement constraints.
target word aligns to none or more than one word is
much more flexible: it depends on the tuned thresh-
old.
4 Word alignment results
We evaluated the agreement HMM model on two
corpora for which hand-aligned data are widely
available: the Hansards corpus (Och and Ney, 2000)
of English/French parliamentary proceedings and
the Europarl corpus (Koehn, 2002) with EPPS an-
notation (Lambert et al, 2005) of English/Spanish.
Figure 2 shows two machine-generated alignments
of a sentence pair. The black dots represent the ma-
chine alignments and the shading represents the hu-
man annotation (as described in the previous sec-
tion), on the left using the regular HMM model and
on the right using our agreement constraints. The
figure illustrates a problem known as garbage collec-
tion (Brown et al, 1993), where rare source words
tend to align to many target words, since the prob-
ability mass of the rare word translations can be
hijacked to fit the sentence pair. Agreement con-
straints solve this problem, because forward and
backward models cannot agree on the garbage col-
lection solution.
Grac?a et al (2008) show that alignment error rate
(Och and Ney, 2003) can be improved with agree-
ment constraints. Since AER is the standard metric
for alignment quality, we reproduce their results us-
ing all the sentences of length at most 40. For the
Hansards corpus we improve from 15.35 to 7.01 for
the English ? French direction and from 14.45 to
6.80 for the reverse. For English? Spanish we im-
prove from 28.20 to 19.86 and from 27.54 to 19.18
for the reverse. These values are competitive with
other state of the art systems (Liang et al, 2006).
Unfortunately, as was shown by Fraser and Marcu
(2007) AER can have weak correlation with transla-
tion performance as measured by BLEU score (Pa-
pineni et al, 2002), when the alignments are used
to train a phrase-based translation system. Conse-
quently, in addition to AER, we focus on precision
and recall.
Figure 3 shows the change in precision and re-
call with the amount of provided training data for
the Hansards corpus. We see that agreement con-
straints improve both precision and recall when we
989
 65 70 75 80 85 90 95 100  1
 10
 100
 1000
Thousand
s of traini
ng senten
ces
Agreemen
t Baseline
 65 70 75 80 85 90 95 100  1
 10
 100
 1000
Thousand
s of traini
ng senten
ces
Agreemen
t Baseline
Figure 3: Effect of posterior constraints on precision
(left) and recall (right) learning curves for Hansards
En?Fr.
 10 20 30 40 50 60 70 80 90 100  1
 10
 100
 1000
Thousand
s of traini
ng senten
ces
Rare Common Agreemen
t Baseline
 10 20 30 40 50 60 70 80 90 100  1
 10
 100
 1000
Thousand
s of traini
ng senten
ces
Rare Common  Agreeme
nt Baseline
Figure 4: Left: Precision. Right: Recall. Learning curves
for Hansards En?Fr split by rare (at most 5 occurances)
and common words.
use Viterbi decoding, with larger improvements for
small amounts of training data. We see a similar im-
provement on the EPPS corpus.
Motivated by the garbage collection problem, we
also analyze common and rare words separately.
Figure 4 shows precision and recall learning curves
for rare and common words. We see that agreement
constraints improve precision but not recall of rare
words and improve recall but not precision of com-
mon words.
As described above an alternative to Viterbi de-
coding is to accept all alignments that have probabil-
ity above some threshold. By changing the thresh-
old, we can trade off precision and recall. Figure
5 compares this tradeoff for the baseline and agree-
ment model. We see that the precision/recall curve
for agreement is entirely above the baseline curve,
so for any recall value we can achieve higher preci-
sion than the baseline for either corpus. In Figure 6
we break down the same analysis into rare and non
rare words.
Figure 7 shows an example of the same sentence,
using the same model where in one case Viterbi de-
coding was used and in the other case Posterior de-
coding tuned to minimize AER on a development set
 0 0.2 0.4 0.6 0.8 1  0
 0.2 0.4
 0.6 0.8
 1
Recall
PrecisionBaseline Agreemen
t
 0 0.2 0.4 0.6 0.8 1  0
 0.2 0.4
 0.6 0.8
 1
Recall 
PrecisionBaseline Agreemen
t
Figure 5: Precision and recall trade-off for posterior de-
coding with varying threshold. Left: Hansards En?Fr.
Right: EPPS En?Es.
 0 0.2 0.4 0.6 0.8 1  0
 0.2 0.4
 0.6 0.8
 1
Recall
PrecisionBaseline Agreemen
t
 0 0.2 0.4 0.6 0.8 1  0
 0.2 0.4
 0.6 0.8
 1
Recall
PrecisionBaseline Agreemen
t
Figure 6: Precision and recall trade-off for posterior on
Hansards En?Fr. Left: rare words only. Right: common
words only.
was used. An interesting difference is that by using
posterior decoding one can have n-n alignments as
shown in the picture.
A natural question is how to tune the threshold in
order to improve machine translation quality. In the
next section we evaluate and compare the effects of
the different alignments in a phrase based machine
translation system.
5 Phrase-based machine translation
In this section we attempt to investigate whether our
improved alignments produce improved machine
0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 80 ? ? ? ? ? ? ? ? ? 0 ? ? ? ? ? ? ? ? ? firstly1 ? ? ? ? ? ? ? ? ? 1 ? ? ? ? ? ? ? ? ? ,2 ? ? ? ? ? ? ? ? ? 2 ? ? ? ? ? ? ? ? ? we3 ? ? ? ? ? ? ? ? ? 3 ? ? ? ? ? ? ? ? ? have4 ? ? ? ? ? ? ? ? ? 4 ? ? ? ? ? ? ? ? ? a5 ? ? ? ? ? ? ? ? ? 5 ? ? ? ? ? ? ? ? ? legal6 ? ? ? ? ? ? ? ? ? 6 ? ? ? ? ? ? ? ? ? framework8 ? ? ? ? ? ? ? ? ? 8 ? ? ? ? ? ? ? ? ? .en primero
lugar
, tenemos
un marco
jur??dico
. en primero
lugar
, tenemos
un marco
jur??dico
.
Figure 7: An example of the output of HMM trained on
100k the EPPS data using agreement HMM. Left: Viterbi
decoding. Right: Posterior decoding tuned to minimize
AER. The addition is en-firstly and tenemos-have.
990
translation. In particular we fix a state of the art
machine translation system1 and measure its perfor-
mance when we vary the supplied word alignments.
The baseline system uses GIZA model 4 alignments
and the open source Moses phrase-based machine
translation toolkit2, and performed close to the best
at the competition last year.
For all experiments the experimental setup is as
follows: we lowercase the corpora, and train lan-
guage models from all available data. The reason-
ing behind this is that even if bilingual texts might
be scarce in some domain, monolingual text should
be relatively abundant. We then train the com-
peting alignment models and compute competing
alignments using different decoding schemes. For
each alignment model and decoding type we train
Moses and use MERT optimization to tune its pa-
rameters on a development set. Moses is trained us-
ing the grow-diag-final-and alignment symmetriza-
tion heuristic and using the default distance base
distortion model. We report BLEU scores using a
script available with the baseline system. The com-
peting alignment models are GIZA Model 4, our im-
plementation of the baseline HMM alignment and
our agreement HMM. We would like to stress that
the fair comparison is between the performance of
the baseline HMM and the agreement HMM, since
Model 4 is more complicated and can capture more
structure. However, we will see that for moderate
sized data the agreement HMM performs better than
both its baseline and GIZA Model 4.
5.1 Corpora
In addition to the Hansards corpus and the Europarl
English-Spanish corpus, we used four other corpora
for the machine translation experiments. Table 1
summarizes some statistics of all corpora. The Ger-
man and Finnish corpora are also from Europarl,
while the Czech corpus contains news commentary.
All three were used in recent ACL workshop shared
tasks and are available online3. The Italian corpus
consists of transcribed speech in the travel domain
and was used in the 2007 workshop on spoken lan-
guage translation4. We used the development and
1www.statmt.org/wmt07/baseline.html
2www.statmt.org/moses/
3http://www.statmt.org
4http://iwslt07.itc.it/
Corpus Train Len Test Rare (%) Unk (%)
En, Fr 1018 17.4 1000 0.3, 0.4 0.1, 0.2
En, Es 126 21.0 2000 0.3, 0.5 0.2, 0.3
En, Fi 717 21.7 2000 0.4, 2.5 0.2, 1.8
En, De 883 21.5 2000 0.3, 0.5 0.2, 0.3
En, Cz 57 23.0 2007 2.3, 6.6 1.3, 3.9
En, It 20 9.4 500 3.1, 6.2 1.4, 2.9
Table 1: Statistics of the corpora used in MT evaluation.
The training size is measured in thousands of sentences
and Len refers to average (English) sentence length. Test
is the number of sentences in the test set. Rare and Unk
are the percentage of tokens in the test set that are rare
and unknown in the training data, for each language.
 26 28 30 32 34 36  1000
0
 10000
0
 1e+06
Traini
ng dat
a size 
(sente
nces)
Agree
ment P
ost-pts Model
 4
Baseli
ne Vit
erbi
Figure 8: BLEU score as the amount of training data is
increased on the Hansards corpus for the best decoding
method for each alignment model.
tests sets from the workshops when available. For
Italian corpus we used dev-set 1 as development and
dev-set 2 as test. For Hansards we randomly chose
1000 and 500 sentences from test 1 and test 2 to be
testing and development sets respectively.
Table 1 summarizes the size of the training corpus
in thousands of sentences, the average length of the
English sentences as well as the size of the testing
corpus. We also report the percentage of tokens in
the test corpus that are rare or not encountered in the
training corpus.
5.2 Decoding
Our initial experiments with Viterbi decoding and
posterior decoding showed that for our agreement
model posterior decoding could provide better align-
ment quality. When labeled data is available, we can
tune the threshold to minimize AER. When labeled
data is not available we use a different heuristic to
991
tune the threshold: we choose a threshold that gives
the same number of aligned points as Viterbi decod-
ing produces. In principle, we would like to tune
the threshold by optimizing BLEU score on a devel-
opment set, but that is impractical for experiments
with many pairs of languages. We call this heuristic
posterior-points decoding. As we shall see, it per-
forms well in practice.
5.3 Training data size
The HMM alignment models have a smaller param-
eter space than GIZA Model 4, and consequently we
would expect that they would perform better when
the amount of training data is limited. We found that
this is generally the case, with the margin by which
we beat model 4 slowly decreasing until a crossing
point somewhere in the range of 105 - 106 sentences.
We will see in section 5.3.1 that the Viterbi decoding
performs best for the baseline HMM model, while
posterior decoding performs best for our agreement
HMM model. Figure 8 shows the BLEU score for
the baseline HMM, our agreement model and GIZA
Model 4 as we vary the amount of training data from
104 - 106 sentences. For all but the largest data sizes
we outperform Model 4, with a greater margin at
lower training data sizes. This trend continues as we
lower the amount of training data further. We see a
similar trend with other corpora.
5.3.1 Small to Medium Training Sets
Our next set of experiments look at our perfor-
mance in both directions across our 6 corpora, when
we have small to moderate amounts of training data:
for the language pairs with more than 100,000 sen-
tences, we use only the first 100,000 sentences. Ta-
ble 2 shows the performance of all systems on these
datasets. In the table, post-pts and post-aer stand
for posterior-points decoding and posterior decod-
ing tuned for AER. With the notable exception of
Czech and Italian, our system performs better than
or comparable to both baselines, even though it uses
a much more limited model than GIZA?s Model 4.
The small corpora for which our models do not per-
form as well as GIZA are the ones with a lot of rare
words. We suspect that the reason for this is that we
do not implement smoothing, which has been shown
to be important, especially in situations with a lot of
rare words.
X? En En? X
Base Agree Base Agree
GIZA M4 23.92 17.89
De Viterbi 24.08 23.59 18.15 18.13
post-pts 24.24 24.65(+) 18.18 18.45(+)
GIZA M4 18.29 11.05
Fi Viterbi 18.79 18.38 11.17 11.54
post-pts 18.88 19.45(++) 11.47 12.48(++)
GIZA M4 33.12 26.90
Fr Viterbi 32.42 32.15 25.85 25.48
post-pts 33.06 33.09(?) 25.94 26.54(+)
post-aer 31.81 33.53(+) 26.14 26.68(+)
GIZA M4 30.24 30.09
Es Viterbi 29.65 30.03 29.76 29.85
post-pts 29.91 30.22(++) 29.71 30.16(+)
post-aer 29.65 30.34(++) 29.78 30.20(+)
GIZA M4 51.66 41.99
It Viterbi 52.20 52.09 41.40 41.28
post-pts 51.06 51.14(??) 41.63 41.79(?)
GIZA M4 22.78 12.75
Cz Viterbi 21.25 21.89 12.23 12.33
post-pts 21.37 22.51(++) 12.16 12.47(+)
Table 2: BLEU scores for all language pairs using up to
100k sentences. Results are after MERT optimization.
The marks (++)and (+)denote that agreement with poste-
rior decoding is better by 1 BLEU point and 0.25 BLEU
points respectively than the best baseline HMM model;
analogously for (??), (?); while (?)denotes smaller dif-
ferences.
5.3.2 Larger Training Sets
For four of the corpora we have more than 100
thousand sentences. The performance of the sys-
tems on all the data is shown in Table 3. German
is not included because MERT optimization did not
complete in time. We see that even on over a million
instances, our model sometimes performs better than
GIZA model 4, and always performs better than the
baseline HMM.
6 Conclusions
In this work we have evaluated agreement-
constrained EM training for statistical word align-
ment models. We carefully studied its effects on
word alignment recall and precision. Agreement
training has a different effect on rare and com-
mon words, probably because it fixes different types
of errors. It corrects the garbage collection prob-
lem for rare words, resulting in a higher preci-
sion. The recall improvement in common words
992
X? En En? X
Base Agree Base Agree
GIZA M4 22.78 14.72
Fi Viterbi 22.92 22.89 14.21 14.09
post-pts 23.15 23.43 (+) 14.57 14.74 (?)
GIZA M4 35.65 31.15
Fr Viterbi 35.19 35.17 30.57 29.97
post-pts 35.49 35.95 (+) 29.78 30.02 (?)
post-aer 34.85 35.48 (+) 30.15 30.07 (?)
GIZA M4 31.62 32.40
Es Viterbi 31.75 31.84 31.17 31.09
post-pts 31.88 32.19 (+) 31.16 31.56 (+)
post-aer 31.93 32.29 (+) 31.23 31.36 (?)
Table 3: BLEU scores for all language pairs using all
available data. Markings as in Table 2.
can be explained by the idea that ambiguous com-
mon words are different in the two languages, so the
un-ambiguous choices in one direction can force the
choice for the ambiguous ones in the other through
agreement constraints.
To our knowledge this is the first extensive eval-
uation where improvements in alignment accuracy
lead to improvements in machine translation per-
formance. We tested this hypothesis on six differ-
ent language pairs from three different domains, and
found that the new alignment scheme not only per-
forms better than the baseline, but also improves
over a more complicated, intractable model. In or-
der to get the best results, it appears that posterior
decoding is required for the simplistic HMM align-
ment model. The success of posterior decoding us-
ing our simple threshold tuning heuristic is fortu-
nate since no labeled alignment data are needed:
Viterbi alignments provide a reasonable estimate of
aligned words needed for phrase extraction. The na-
ture of the complicated relationship between word
alignments, the corresponding extracted phrases and
the effects on the final MT system still begs for
better explanations and metrics. We have investi-
gated the distribution of phrase-sizes used in transla-
tion across systems and languages, following recent
investigations (Ayan and Dorr, 2006), but unfortu-
nately found no consistent correlation with BLEU
improvement. Since the alignments we extracted
were better according to all metrics we used, it
should not be too surprising that they yield better
translation performance, but perhaps a better trade-
off can be achieved with a deeper understanding of
the link between alignments and translations.
Acknowledgments
J. V. Grac?a was supported by a fellowship from
Fundac?a?o para a Cie?ncia e Tecnologia (SFRH/ BD/
27528/ 2006). K. Ganchev was partially supported
by NSF ITR EIA 0205448.
References
N. F. Ayan and B. J. Dorr. 2006. Going beyond AER: An
extensive analysis of word alignments and their impact
on MT. In Proc. ACL.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, M. J.
Goldsmith, J. Hajic, R. L. Mercer, and S. Mohanty.
1993. But dictionaries are data too. In Proc. HLT.
P. F. Brown, S. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1994. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Royal Statistical Society, Ser. B, 39(1):1?
38.
A. Fraser and D. Marcu. 2007. Measuring word align-
ment quality for statistical machine translation. Com-
put. Linguist., 33(3):293?303.
J. Grac?a, K. Ganchev, and B. Taskar. 2008. Expecta-
tion maximization and posterior constraints. In Proc.
NIPS.
P. Koehn. 2002. Europarl: A multilingual corpus for
evaluation of machine translation.
P. Lambert, A.De Gispert, R. Banchs, and J. B. Marin?o.
2005. Guidelines for word alignment evaluation and
manual alignment. In Language Resources and Eval-
uation, Volume 39, Number 4.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. HLT-NAACL.
E. Matusov, Zens. R., and H. Ney. 2004. Symmetric
word alignments for statistical machine translation. In
Proc. COLING.
R. C. Moore. 2004. Improving IBM word-alignment
model 1. In Proc. ACL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In ACL.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Comput. Lin-
guist., 29(1):19?51.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: A Method for Automatic Evaluation of Ma-
chine Translation. In Proc. ACL.
S. Vogel, H. Ney, and C. Tillmann. 1996. Hmm-based
word alignment in statistical translation. In Proc.
COLING.
993
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 322?332,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Universal Morphological Analysis using Structured Nearest Neighbor
Prediction
Young-Bum Kim
University of Wisconsin-Madison
ybkim@cs.wisc.edu
Jo?o V. Gra?a
L2F INESC-ID
Lisboa, Portugal
joao.graca@l2f.inesc-id.pt
Benjamin Snyder
University of Wisconsin-Madison
bsnyder@cs.wisc.edu
Abstract
In this paper, we consider the problem of un-
supervised morphological analysis from a new
angle. Past work has endeavored to design un-
supervised learning methods which explicitly
or implicitly encode inductive biases appropri-
ate to the task at hand. We propose instead
to treat morphological analysis as a structured
prediction problem, where languages with la-
beled data serve as training examples for un-
labeled languages, without the assumption of
parallel data. We define a universal morpho-
logical feature space in which every language
and its morphological analysis reside. We de-
velop a novel structured nearest neighbor pre-
diction method which seeks to find the mor-
phological analysis for each unlabeled lan-
guage which lies as close as possible in the
feature space to a training language. We ap-
ply our model to eight inflecting languages,
and induce nominal morphology with substan-
tially higher accuracy than a traditional, MDL-
based approach. Our analysis indicates that
accuracy continues to improve substantially as
the number of training languages increases.
1 Introduction
Over the past several decades, researchers in the nat-
ural language processing community have focused
most of their efforts on developing text processing
tools and techniques for English (Bender, 2009),
a morphologically simple language. Recently, in-
creasing attention has been paid to the wide variety
of other languages of the world. Most of these lan-
guages still pose severe difficulties, due to (i) their
lack of annotated textual data, and (ii) the fact that
they exhibit linguistic structure not found in English,
and are thus not immediately susceptible to many
traditional NLP techniques.
Consider the example of nominal part-of-speech
analysis. The Penn Treebank defines only four En-
glish noun tags (Marcus et al, 1994), and as a re-
sult, it is easy to treat the words bearing these tags
as completely distinct word classes, with no inter-
nal morphological structure. In contrast, a compara-
ble tagset for Hungarian includes 154 distinct noun
tags (Erjavec, 2004), reflecting Hungarian?s rich in-
flectional morphology. When dealing with such lan-
guages, treating words as atoms leads to severe data
sparsity problems.
Because annotated resources do not exist for most
morphologically rich languages, prior research has
focused on unsupervised methods, with a focus on
developing appropriate inductive biases. However,
inductive biases and declarative knowledge are no-
toriously difficult to encode in well-founded models.
Even putting aside this practical matter, a universally
correct inductive bias, if there is one, is unlikely to
be be discovered by a priori reasoning alone.
In this paper, we argue that languages for which
we have gold-standard morphological analyses can
be used as effective guides for languages lacking
such resources. In other words, instead of treating
each language?s morphological analysis as a de novo
induction problem to be solved with a purely hand-
coded bias, we instead learn from our labeled lan-
guages what linguistically plausible morphological
analyses looks like, and guide our analysis in this
direction.
322
More formally, we recast morphological induc-
tion as a new kind of supervised structured predic-
tion problem, where each annotated language serves
as a single training example. Each language?s noun
lexicon serves as a single input x, and the analysis
of the nouns into stems and suffixes serves as a com-
plex structured label y.
Our first step is to define a universal morpholog-
ical feature space, into which each language and its
morphological analysis can be mapped. We opt for
a simple and intuitive mapping, which measures the
sizes of the stem and suffix lexicons, the entropy of
these lexicons, and the fraction of word forms which
appear without any inflection.
Because languages tend to cluster into well de-
fined morphological groups, we cast our learn-
ing and prediction problem in the nearest neighbor
framework (Cover and Hart, 1967). In contrast to
its typical use in classification problems, where one
can simply pick the label of the nearest training ex-
ample, we are here faced with a structured predic-
tion problem, where locations in feature space de-
pend jointly on the input-label pair (x, y). Finding a
nearest neighbor thus consists of searching over the
space of morphological analyses, until a point in fea-
ture space is reached which lies closest to one of the
labeled languages. See Figure 1 for an illustration.
To provide a measure of empirical validation, we
applied our approach to eight languages with inflec-
tional nominal morphology, ranging in complexity
from very simple (English) to very complex (Hun-
garian). In all but one case, our approach yields
substantial improvements over a comparable mono-
lingual baseline (Goldsmith, 2005), which uses the
minimum description length principle (MDL) as its
inductive bias. On average, our method increases
accuracy by 11.8 percentage points, corresponding
to a 42% decrease in error relative to a supervised
upper bound. Further analysis indicates that accu-
racy improves as the number of training languages
increases.
2 Related Work
In this section, we briefly review prior work on un-
supervised morphological induction, as well as mul-
tilingual analysis in NLP.
Unsupervised Morphological Induction: Unsu-
pervised morphology remains an active area of re-
search (Schone and Jurafsky, 2001; Goldsmith,
2005; Adler and Elhadad, 2006; Creutz and La-
gus, 2005; Dasgupta and Ng, 2007; Creutz and La-
gus, 2007; Poon et al, 2009). Many existing algo-
rithms derive morpheme lexicons by identifying re-
curring patterns in words. The goal is to optimize the
compactness of the data representation by finding a
small lexicon of highly frequent strings, resulting in
a minimum description length (MDL) lexicon and
corpus (Goldsmith, 2001; Goldsmith, 2005). Later
work cast this idea in a probabilistic framework in
which the the MDL solution is equivalent to a MAP
estimate in a suitable Bayesian model (Creutz and
Lagus, 2005). In all these approaches, a locally op-
timal segmentation is identified using a task-specific
greedy search.
Multilingual Analysis: An influential line of prior
multilingual work starts with the observation that
rich linguistic resources exist for some languages
but not others. The idea then is to project linguis-
tic information from one language onto others via
parallel data. Yarowsky and his collaborators first
developed this idea and applied it to the problems of
part-of-speech tagging, noun-phrase bracketing, and
morphology induction (Yarowsky and Wicentowski,
2000; Yarowsky et al, 2000; Yarowsky and Ngai,
2001), and other researchers have applied the idea
to syntactic and semantic analysis (Hwa et al, 2005;
Pad? and Lapata, 2006) In these cases, the existence
of a bilingual parallel text along with highly accurate
predictions for one of the languages was assumed.
Another line of work assumes the existence of
bilingual parallel texts without the use of any super-
vision (Dagan et al, 1991; Resnik and Yarowsky,
1997). This idea has been developed and applied to
a wide variety tasks, including morphological anal-
ysis (Snyder and Barzilay, 2008b; Snyder and Barzi-
lay, 2008a), part-of-speech induction (Snyder et al,
2008; Snyder et al, 2009b; Naseem et al, 2009),
and grammar induction (Snyder et al, 2009a; Blun-
som et al, 2009; Burkett et al, 2010). An even
more recent line of work does away with the as-
sumption of parallel texts and performs joint unsu-
pervised induction for various languages through the
use of coupled priors in the context of grammar in-
323
duction (Cohen and Smith, 2009; Berg-Kirkpatrick
and Klein, 2010).
In contrast to these previous approaches, the
method proposed in this paper does not assume the
existence of any parallel text, but does assume that
labeled data exists for a wide variety of languages, to
be used as training examples for our test language.
3 Structured Nearest Neighbor
We reformulate morphological induction as a super-
vised learning task, where each annotated language
serves as a single training example for our language-
independent model. Each such example consists
of an input-label pair (x, y), both of which contain
complex internal structure: The input x ? X con-
sists of a vocabulary list of all words observed in a
particular monolingual corpus, and the label y ? Y
consists of the correct morphological analysis of all
the vocabulary items in x.1 Because our goal is
to generalize across languages, we define a feature
function which maps each (x, y) pair to a universal
feature space: f : X ? Y ? Rd.
For each unlabeled input language x, our goal is
to predict a complete morphological analysis y ? Y
which maximizes a scoring function on the fea-
ture space, score : Rd ? R. This scoring func-
tion is trained using the n labeled-language exam-
ples: (x, y)1, . . . , (x, y)n, and the resulting predic-
tion rule for unlabeled input x is given by:
y? = argmax
y?Y
score
(
f(x, y)
)
Languages can be typologically categorized by
the type and richness of their morphology. On the
assumption that for each test language, at least one
typologically similar language will be present in the
training set, we employ a nearest neighbor scoring
function. In the standard nearest neighbor classifi-
cation setting, one simply predicts the label of the
closest training example in the input space.2 In our
structured prediction setting, the mapping to the uni-
versal feature space depends crucially on the struc-
ture of the proposed label y, not simply the input
1Technically, the label space of each input, Y , should be
thought of as a function of the input x. We suppress this depen-
dence for notational clarity.
2More generally the majority label of the k-nearest neigh-
bors.
x. We thus generalize nearest-neighbor prediction
to the structured scenario and propose the following
prediction rule:
y? = argmin
y?Y
min
`
? f(x, y)? f(x`, y`) ?, (1)
where the index ` ranges over the training languages.
In words, we predict the morphological analysis y
for our test language which places it as close as pos-
sible in the universal feature space to one of the
training languages `.
Morphological Analysis: In this paper we focus
on nominal inflectional suffix morphology. Consider
the word utiskom in Serbian, meaning impression
with the instrumental case marking. A correct analy-
sis of this word would divide it into a stem (utisak =
impression), a suffix (-om = instrumental case), and
a phonological deletion rule on the stem?s penulti-
mate vowel (..ak#? ..k#).
More generally, as we define it, a morphological
analysis of a word type w consists of (i) a stem t, (ii),
a suffix f , and (iii) a deletion rule d. Either or both
of the suffix and deletion rule can be NULL. We al-
low three types of deletion rules on stems: deletion
of final vowels (..V# ? ..#), deletion of penulti-
mate vowels (..V C# ? ..C#), and removals and
additions of final accent marks (e.g. ..a?# ? ..a#).
We require that stems be at least three characters
long and that suffixes be no more than four. And,
of course, we require that after (1) applying deletion
rule d to stem t, and (2) adding suffix f to the result,
we obtain word w.
Universal Feature Space: We employ a fairly
simple and minimal set of features, all of which
could plausibly generalize across a wide range of
languages. Consider the set of stems T , suffixes F ,
and deletion rules D, induced by the morphological
analyses y of the words x. Our first three features
simply count the sizes of these three sets.
These counting features consider only the raw
number of unique morphemes (and phonological
rules) being used, but not their individual frequency
or distribution. Our next set of features considers
the empirical entropy of these occurrences as dis-
tributed across the lexicon of words x by analysis y.
324
f(x2, y2)
f(x1, y1)
f(x3, y3)
y(t,1)
y(t+1,1)
y(t+1,2)
y(t,2)
y(t,3)
y(t+1,3)
f
?
x, y(0,?)
?
Initialization
Figure 1: Structured Nearest Neighbor Search: The inference procedure for unlabeled test language x, when trained
with three labeled languages, (x1, y1), (x2, y2), (x3, y3). Our search procedure iteratively attempts to find labels for x
which are as close as possible in feature space to each of the training languages. After convergence, the label which is
closest in distance to a training language is predicted, in this case being the label near training language (x3, y3).
For example, if the (x, y) pair consists of the ana-
lyzed words {kiss, kiss-es, hug}, then the empirical
distributions over stems, suffixes, and deletion rules
would be:
? P (t = kiss) = 2/3
? P (t = hug) = 1/3
? P (f = NULL) = 2/3
? P (f = ?es) = 1/3
? P (d = NULL) = 1
The three entropy features are defined as the shan-
non entropies of these stem, suffix, and deletion rule
probabilities: H(t), H(f), H(d).3
Finally, we consider two simple percentage fea-
tures: the percentage of words in x which according
to y are left unsegmented (i.e. have the null suf-
fix, 2/3 in the example above), and the percentage of
segmented words which employ a deletion rule (0 in
the example above). Thus, in total, our model em-
ploys 8 universal morphological features. All fea-
tures are scaled to the unit interval and are assumed
to have equal weight.
3Note that here and throughout the paper, we operate over
word types, ignoring their corpus frequencies.
3.1 Search Algorithm
The main algorithmic challenge for our model lies in
efficiently computing the best morphological analy-
sis y for each language-specific word set x, accord-
ing to Equation 1. Exhaustive search through the
set of all possible morphological analyses is impos-
sible, as the number of such analyses grows expo-
nentially in the size of the vocabulary. Instead, we
develop a greedy search algorithm in the following
fashion (the search procedure is visually depicted in
Figure 1).
At each time-step t, we maintain a set of frontier
analyses {y(t,`)}`, where ` ranges over the traininglanguages. The goal is to iteratively modify each of
these frontier analyses y(t,`) ? y(t+1,`) so that the
location of the training language in universal feature
space ? f(x, y(t+1,`)) ? is as close as possible to
the location of the training language `: f(x`, y`).
After iterating this procedure to convergence, we
are left with a set of analyses {y(`)}`, each of whichapproximates the analyses which yield minimal dis-
tances to a particular training language:
y(`) ? argmin
y?Y
? f(x, y)? f(x`, y`) ? .
We finally select from amongst these analyses and
325
make our prediction:
`? = argmin
`
? f(x, y(`))? f(x`, y`) ?
y? = y(`?)
The main outline of our search algorithm is based
on the MDL-based greedy search heuristic devel-
oped and studied by (Goldsmith, 2005). At a high
level, this search procedure alternates between indi-
vidual analyses of words (keeping the set of stems
and suffixes fixed), aggregate discoveries of new
stems (keeping the suffixes fixed), and aggregate dis-
coveries of new suffixes (keeping stems fixed). As
input, we consider the test words x in our new lan-
guage, and we run the search in parallel for each
training language (x`, y`). For each such test-train
language pair, the search consists of the following
stages:
Stage 0: Initialization
We initially analyze each word w ? x according
to peaks in successor frequency.4 If w?s n-character
prefix w:n has successor frequency > 1 and the sur-
rounding prefixes, w:n?1 and w:n+1 both have suc-
cessor frequency = 1, then we analyze w as a stem-
suffix pair: (w:n, wn+1:).5 Otherwise, we initialize
w as an unsuffixed stem. As this procedure tends to
produce an overly large set of suffixes F , we further
prune F down to the number of suffixes found in
the training language, retaining those which appear
with the largest number of stems. This initialization
stage is carried out once, and afterwards the follow-
ing three stages are repeated until convergence.
Stage 1: Reanalyze each word
In this stage, we reanalyze each word (in random
order). We use the set of stems T and suffixes F
obtained from the previous stage, and don?t permit
the addition of any new items to these lists. In-
stead, we focus on obtaining better analyses of each
word, while also building up a set of phonological
deletion rules D. For each word w ? x, we con-
sider all possible segmentations of w into a stem-
4The successor frequency of a string prefix s is defined as
the number of unique characters that occur immediately after s
in the vocabulary.
5With the restriction that at this stage we only allow suffixes
up to length 5, and stems of at least length 3.
suffix pair (t, f), for which f ? F , and where ei-
ther t ? T or some t? ? T such that t is obtained
from t? using a deletion rule d (e.g. by deleting a
final or penultimate vowel). For each such possi-
ble analysis y?, we compute the resulting location
in feature space f(x, y?), and select the analysis that
brings us closest to our target training language:
y = argminy? ? f(x, y?)? f(x`, y`) ? .
Stage 2: Find New Stems
In this stage, we keep our set of suffixes F and
deletion rules D from the previous stage fixed, and
attempt to find new stems to add to T through an ag-
gregate analysis of unsegmented words. For every
string s, we consider the set of words which are cur-
rently unsegmented, and can be analyzed as a stem-
suffix pair (s, f) for some existing suffix f ? F ,
and some deletion rule d ? D. We then consider
the joint segmentation of these words into a new
stem s, and their respective suffixes. As before, we
choose the segmentation if it brings us closer in fea-
ture space to our target training language.
Stage 3: Find New Suffixes
This stage is exactly analogous to the previous
stage, except we now fix the set of stems T and seek
to find new suffixes.
3.2 A Monolingual Supervised Model
In order to provide a plausible upper bound on per-
formance, we also formulate a supervised monolin-
gual morphological model, using the structured per-
ceptron framework (Collins, 2002). Here we as-
sume that we are given some training sequence of in-
puts and morphological analyses (all within one lan-
guage): (x1, y1), (x2, y2), . . . , (xn, yn). We define
each input xi to be a noun w, along with a morpho-
logical tag z, which specifies the gender, case, and
number of the noun. The goal is to predict the cor-
rect segmentation of w into stem, suffix, and phono-
logical deletion rule: yi = (t, f, d).6
To do so, we define a feature function over input-
label pairs, (x, y), with the following binary feature
templates: (1) According to label yi, the stem is t
6While the assumption of the correct morphological tag as
input is somewhat unrealistic, this model still gives us a strong
upper bound on how well we can expect our unsupervised
model to perform.
326
Type Counts Entropy Percentage
# words # stems # suffs # dels stem entropy suff entropy del entropy unseg deleted
BG 4833 3112 21 8 11.4 2.7 0.9 .45 .29
CS 5836 3366 28 12 11.5 3.2 1.6 .38 .53
EN 4178 3453 3 1 11.7 1.0 0.1 .73 .06
ET 6371 3742 141 5 11.5 5.0 0.2 .31 .04
HU 8051 3746 231 7 11.3 5.8 0.5 .23 .11
RO 5578 3297 23 8 11.5 2.9 1.4 .48 .51
SL 6111 3172 32 6 11.3 3.2 1.5 .33 .56
SR 5849 3178 28 5 11.4 2.9 1.4 .33 .53
Table 1: Corpus statistics for the eight languages. The first four columns give the number of unique word, stem, suffix,
and phonological deletion rule types. The next three columns give, respectively, the entropies of the distributions
of stems, suffixes (including NULL), and deletion rules (including NULL) over word types. The final two columns
give, respectively, the percentage of word types occurring with the NULL suffix, and the number of non-NULL suffix
words which use a phonological deletion rule. Note that the final eight columns define the universal feature space used
by our model. BG = Bulgarian, CS = Czech, EN = English, ET = Estonian, HU = Hungarian, RO = Romanian, SL =
Slovene, SR = Serbian
(one feature for each possible stem). (2) Accord-
ing to label yi, the suffix and deletion rule are (f, d)
(one feature for every possible pair of deletion rules
and suffixes). (3) According to label yi and morpho-
logical tag z, the suffix, deletion rule, and gender
are respectively (f, d,G). (4) According to label yi
and morphological tag z, the suffix, deletion rule,
and case are (f, d, C). (5) According to label yi and
morphological tag z, the suffix, deletion rule, and
number are (f, d,N).
We train a set of linear weights on our fea-
tures using the averaged structured perceptron algo-
rithm (Collins, 2002).
4 Experiments
In this section we turn to experimental findings to
provide empirical support for our proposed frame-
work.
Corpus: To test our cross-lingual model, we ap-
ply it to a morphologically analyzed corpus of eight
languages (Erjavec, 2004). The corpus includes a
roughly 100,000 word English text, Orwell?s novel
?Nineteen Eighty Four,? and its translation into
seven languages: Bulgarian, Czech, Estonian, Hun-
garian, Romanian, Slovene, and Serbian. All the
words in the corpus are tagged with morphologi-
cal stems and a detailed morpho-syntactic analysis.
Although the texts are parallel, we note that par-
allelism is nowhere assumed nor exploited by our
model. See Table 1 for a summary of relevant cor-
pus statistics. As indicated in the table, the raw num-
ber of nominal word types varies quite a bit across
the languages, almost doubling from 4,178 (English)
to 8,051 (Hungarian). In contrast, the number of
stems appearing within these words is relatively sta-
ble across languages, ranging from a minimum of
3,112 (Bulgarian) to a maximum of 3,746 (Hungar-
ian), an increase of just 20%.
In contrast, the number of suffixes across the lan-
guages varies quite a bit. Hungarian and Esto-
nian, both Uralic languages with very complex nom-
inal morphology, use 231 and 141 nominal suffixes,
respectively. Besides English, the remaining lan-
guages employ between 21 and 32 suffixes, and En-
glish is the outlier in the other direction, with just
three nominal inflectional suffixes.
Baselines and Results: As our unsupervised
monolingual baseline, we use the Linguistica pro-
gram (Goldsmith, 2001; Goldsmith, 2005). We ap-
ply Linguistica?s default settings, and run the ?suffix
prediction? option. Our model?s search procedure
closely mirrors the one used by Linguistica, with
the crucial difference that instead of attempting to
greedily minimize description length, our algorithm
instead tries to find the analysis as close as possi-
ble in the universal feature space to that of another
language.
To apply our model, we treat each of the eight
327
Linguistica
Our Model
SupervisedNearest Neighbor Self (oracle) Avg.
Accuracy Distance Accuracy Distance Accuracy Distance
BG 68.7 84.0 (RO) 0.13 88.7 0.03 68.6 3.90 94.7
CS 60.4 82.8 (BG) 0.40 84.5 0.03 66.3 4.05 93.5
EN 81.1 75.8 (BG) 1.29 89.3 0.10 58.3 4.30 93.4
ET 51.2 66.6 (HU) 0.35 80.9 0.03 52.8 4.57 86.5
HU 64.5 69.3 (ET) 0.81 66.5 1.10 68.0 4.94 94.9
RO 65.6 71.0 (CS) 0.11 71.2 0.15 62.3 3.95 89.1
SL 61.1 82.8 (SR) 0.07 85.5 0.04 61.7 3.69 95.4
SR 64.2 79.1 (SL) 0.06 82.2 0.04 63.0 3.71 94.8
avg. 64.6 76.4 0.40 81.1 0.19 62.6 4.14 92.8
Table 2: Prediction accuracy over word types for the Linguistica baseline, our cross-lingual model, and the monolin-
gual supervised perceptron model. For our model, we provide both prediction accuracy and resulting distance to the
training language in three different scenarios: (i) Nearest Neighbor: The training languages include all seven other
languages in our data set, and the predictions with minimal distance to a training language are chosen (the nearest
neighbor is indicated in parentheses). (ii) Self (oracle): Each language is trained to minimize the distance to its own
gold-standard analysis. (iii) Average: The feature values of all seven training languages are averaged together to
create a single objective.
languages in turn as the test language, with the other
seven serving as training examples. For each test
language, we iterate the search procedure for each
training language (performed in parallel), until con-
vergence. The number of required iterations varies
from 6 to 36 (depending on the test-training lan-
guage pair), and each iteration takes no more than 30
seconds of run-time on a 2.4GHz Intel Xeon E5620
processor. We also consider two variants of our
method. In the first (Self (oracle)), we train each
test language to minimize the distance to its own
gold standard feature values. In the second variant
(Avg.), we average the feature values of all seven
training languages into a single objective. As a plau-
sible upper bound on performance, we implemented
the structured perceptron described in Section 3.2.
For each language, we train the perceptron on a ran-
domly selected set of 80% of the nouns, and test on
the remaining 20%.
The prediction accuracy for all models is calcu-
lated as the fraction of word types with correctly
predicted suffixes. See Table 2 for the results. For
all languages other than English (which is a mor-
phological loner in our group of languages), our
model improves over the baseline by a substantial
margin, yielding an average increase of 11.8 abso-
lute percentage points, and a reduction in error rela-
tive to the supervised upper bound of 42%. Some of
the most striking improvements are seen on Serbian
and Slovene. These languages are closely related
to one another, and indeed our model discovers that
they are each others? nearest neighbors. By guiding
their morphological analyses towards one another,
our model achieves a 21 percentage point increase
in the case of Slovene and a 15 percentage point in-
crease in the case of Slovene.
Perhaps unsurprisingly, when each language?s
gold standard feature values are used as its own
target (Self (oracle) in Table 2), performance in-
creases even further, to an average of 81.1%. By the
same token, the resulting distance in universal fea-
ture space between training and test analyses is cut
in half under this variant, when compared to the non-
oracular nearest neighbor method. The remaining
errors may be due to limitations of the search proce-
dure (i.e. getting caught in local minima), or to the
coarseness of the feature space (i.e. incorrect analy-
ses might map to the same feature values as the cor-
rect analysis). Finally, we note that minimizing the
distance to the average feature values of the seven
training languages (Avg. in Table 2) yields subpar
performance and very large distances between be-
tween predicted analyses and target feature values
(4.14 compared to 0.40 for nearest neighbor). This
328
Linguistica
Gold Standard
Our Method
BG
CS
EN
ET
HU
RO
SL
SR
BG
CS
EN
ET
HU
RO
SL
SR
BG
CS
EN
ET
HU
RO
SL
SR
Figure 2: Locations in Feature Space of Linguistica predictions (green squares), gold standard analyses (red tri-
angles), and our model?s nearest neighbor predictions (blue circles). The original 8-dimensional feature space was
reduced to two dimensions using Multidimensional Scaling.
result may indicate that the average feature point be-
tween training languages is simply unattainable as
an analysis of a real lexicon of nouns.
Visualizing Locations in Feature Space: Besides
assessing our method quantitatively, we can also vi-
sualize the the eight languages in universal feature
space according to (i) their gold standard analyses,
(ii) the predictions of our model and (iii) the pre-
dictions of Linguistica. To do so, we reduce the 8-
dimensional features space down to two dimensions
while preserving the distances between the predicted
and gold standard feature vectors, using Multidi-
mensional Scaling (MDS). The results of this anal-
ysis are shown in Figure 2. With the exception of
English, our model?s analyses lie closer in feature
space to their gold standard counterparts than those
of the baseline. It is interesting to note that Serbian
and Slovene, which are very similar languages, have
essentially swapped places under our model?s anal-
ysis, as have Estonian and Hungarian (both highly
inflected Uralic languages). English has (unfortu-
nately) been pulled towards Bulgarian, the second
least inflecting language in our set.
Learning Curves: We also measured the perfor-
mance of our method as a function of the number
of languages in the training set. For each target lan-
guage, we consider all possible training sets of sizes
ranging from 1 to 7 and select the predictions which
bring our test language closest in distance to one of
the languages in the set. We then average the result-
ing accuracy over all training sets of each size. Fig-
ure 3 shows the resulting learning curves averaged
over all test languages (left), as well as broken down
by test language (right). The overall trend is clear:
as additional languages are added to the training set,
test performance improves. In fact, with only one
training language, our method performs worse (on
average) than the Linguistica baseline. However,
with two or more training languages available, our
method achieves superior results.
Accuracy vs. Distance: We can gain some in-
sight into these learning curves if we consider the
relationship between accuracy (of the test language
analysis) and distance to the training language (of
the same predicted analysis). The more training lan-
guages available, the greater the chance that we can
guide our test language into very close proximity to
329
1 2 3 4 5 6 7Number of training languages0.55
0.6
0.65
0.7
0.75
0.8
0.85
BGCSSLSRENROHUET1 2 3 4 5 6 7Number of training languages0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76 Our ModelLinguistica
Figure 3: Learning curves for our model as the number of training languages increases. The figure on the left shows
the average accuracy of all eight languages for increasingly larger training sets (results are averaged over all training
sets of size 1,2,3,...). The dotted line indicates the average performance of the baseline. The figure on the right shows
similar learning curves, broken down individually for each test language (see Figure 1 for language abbreviations).
one of them. It thus stands to reason that a strong
(negative) correlation between distance and accu-
racy would lead to increased accuracy with larger
training sets. In order to assess this correlation, we
considered all 56 test-train language pairs and col-
lected the resulting accuracy and distance for each
pair. We separately scaled accuracy and distance to
the unit interval for each test language (as some test
languages are inherently more difficult than others).
The resulting plot, shown in Figure 4, shows the ex-
pected correlation: When our test language can be
guided very closely to the training language, the re-
sulting predictions are likely to be good. If not, the
predictions are likely to be bad.
5 Conclusions and Future Work
The approach presented in this paper recasts mor-
phological induction as a structured prediction task.
We assume the presence of morphologically labeled
languages as training examples which guide the in-
duction process for unlabeled test languages. We
developed a novel structured nearest neighbor ap-
proach for this task, in which all languages and their
morphological analyses lie in a universal feature
space. The task of the learner is to search through
the space of morphological analyses for the test lan-
guage and return the result which lies closest to one
0 0.2 0.4 0.6 0.8 1Distance (normalized)
0
0.2
0.4
0.6
0.8
1
Accu
racy 
(norm
alized
)
Figure 4: Accuracy vs. Distance: For all 56 possi-
ble test-train language pairs, we computed test accuracy
along with resulting distance in universal feature space
to the training language. Distance and accuracy are sep-
arately normalized to the unit interval for each test lan-
guage, and all resulting points are plotted together. A
line is fit to the points using least-squares regression.
330
of the training languages. Our empirical findings
validate this approach: On a set of eight different
languages, our method yields substantial accuracy
gains over a traditional MDL-based approach in the
task of nominal morphological induction.
One possible shortcoming of our approach is that
it assumes a uniform weighting of the cross-lingual
feature space. In fact, some features may be far more
relevant than others in guiding our test language to
an accurate analysis. In future work, we plan to in-
tegrate distance metric learning into our approach,
allowing some features to be weighted more heavily
than others. Besides potential gains in prediction ac-
curacy, this approach may shed light on deeper rela-
tionships between languages than are otherwise ap-
parent.
References
Meni Adler and Michael Elhadad. 2006. An un-
supervised morpheme-based hmm for hebrew mor-
phological disambiguation. In Proceedings of the
ACL/CONLL, pages 665?672.
Emily M. Bender. 2009. Linguistically na?ve != lan-
guage independent: why NLP needs linguistic typol-
ogy. In Proceedings of the EACL 2009 Workshop
on the Interaction between Linguistics and Compu-
tational Linguistics, pages 26?32, Morristown, NJ,
USA. Association for Computational Linguistics.
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phyloge-
netic grammar induction. In Proceedings of the ACL,
pages 1288?1297, Uppsala, Sweden, July. Association
for Computational Linguistics.
P. Blunsom, T. Cohn, and M. Osborne. 2009. Bayesian
synchronous grammar induction. Advances in Neural
Information Processing Systems, 21:161?168.
David Burkett, Slav Petrov, John Blitzer, and Dan Klein.
2010. Learning better monolingual models with unan-
notated bilingual text. In Proceedings of CoNLL.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
the NAACL/HLT.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8.
T. Cover and P. Hart. 1967. Nearest neighbor pattern
classification. Information Theory, IEEE Transactions
on, 13(1):21?27.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using morfessor 1.0. Publications
in Computer and Information Science Report A81,
Helsinki University of Technology.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1).
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two
languages are more informative than one. In Proceed-
ings of the ACL, pages 130?137.
Sajib Dasgupta and Vincent Ng. 2007. Unsuper-
vised part-of-speech acquisition for resource-scarce
languages. In Proceedings of the EMNLP-CoNLL,
pages 218?227.
T. Erjavec. 2004. MULTEXT-East version 3: Multi-
lingual morphosyntactic specifications, lexicons and
corpora. In Fourth International Conference on Lan-
guage Resources and Evaluation, LREC, volume 4,
pages 1535?1538.
John Goldsmith. 2001. Unsupervised Learning of the
Morphology of a Natural Language. Computational
Linguistics, 27(2):153?198.
John Goldsmith. 2005. An algorithm for the unsuper-
vised learning of morphology. Technical report, Uni-
versity of Chicago.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and O. Ko-
lak. 2005. Bootstrapping parsers via syntactic projec-
tion across parallel texts. Journal of Natural Language
Engineering, 11(3):311?325.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational linguistics,
19(2):313?330.
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech
tagging: two unsupervised approaches. Journal of Ar-
tificial Intelligence Research, 36(1):341?385.
Sebastian Pad? and Mirella Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic pro-
jection. In Proceedings of ACL, pages 1161 ? 1168.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, NAACL ?09, pages 209?
217, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Philip Resnik and David Yarowsky. 1997. A perspective
on word sense disambiguation methods and their eval-
uation. In Proceedings of the ACL SIGLEX Workshop
331
on Tagging Text with Lexical Semantics: Why, What,
and How?, pages 79?86.
Patrick Schone and Daniel Jurafsky. 2001. Knowledge-
free induction of inflectional morphologies. In NAACL
?01: Second meeting of the North American Chapter of
the Association for Computational Linguistics on Lan-
guage technologies 2001, pages 1?9, Morristown, NJ,
USA. Association for Computational Linguistics.
Benjamin Snyder and Regina Barzilay. 2008a. Cross-
lingual propagation for morphological analysis. In
Proceedings of the AAAI, pages 848?854.
Benjamin Snyder and Regina Barzilay. 2008b. Unsuper-
vised multilingual learning for morphological segmen-
tation. In Proceedings of the ACL/HLT, pages 737?
745.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and
Regina Barzilay. 2008. Unsupervised multilingual
learning for POS tagging. In Proceedings of EMNLP,
pages 1041?1050.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009a. Unsupervised multilingual grammar induction.
In Proceedings of the ACL, pages 73?81.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and
Regina Barzilay. 2009b. Adding more languages im-
proves unsupervised multilingual part-of-speech tag-
ging: a Bayesian non-parametric approach. In Pro-
ceedings of the NAACL, pages 83?91.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust pro-
jection across aligned corpora. In Proceedings of the
NAACL, pages 1?8.
David Yarowsky and Richard Wicentowski. 2000. Min-
imally supervised morphological analysis by multi-
modal alignment. In ACL ?00: Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 207?216, Morristown, NJ,
USA. Association for Computational Linguistics.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2000. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of HLT, pages 161?168.
332
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 962?971, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Entropy-based Pruning for Phrase-based Machine Translation
Wang Ling, Joa?o Grac?a, Isabel Trancoso, Alan Black
L2F Spoken Systems Lab, INESC-ID, Lisboa, Portugal
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
{wang.ling,joao.graca,isabel.trancoso}@inesc-id.pt
awb@cs.cmu.edu
Abstract
Phrase-based machine translation models
have shown to yield better translations than
Word-based models, since phrase pairs en-
code the contextual information that is needed
for a more accurate translation. However,
many phrase pairs do not encode any rele-
vant context, which means that the transla-
tion event encoded in that phrase pair is led
by smaller translation events that are indepen-
dent from each other, and can be found on
smaller phrase pairs, with little or no loss in
translation accuracy. In this work, we pro-
pose a relative entropy model for translation
models, that measures how likely a phrase pair
encodes a translation event that is derivable
using smaller translation events with similar
probabilities. This model is then applied to
phrase table pruning. Tests show that con-
siderable amounts of phrase pairs can be ex-
cluded, without much impact on the transla-
tion quality. In fact, we show that better trans-
lations can be obtained using our pruned mod-
els, due to the compression of the search space
during decoding.
1 Introduction
Phrase-based Machine Translation Models (Koehn
et al 2003) model n-to-m translations of n source
words to m target words, which are encoded in
phrase pairs and stored in the translation model.
This approach has an advantage over Word-based
Translation Models (Brown et al 1993), since trans-
lating multiple source words allows the context for
each source word to be considered during trans-
lation. For instance, the translation of the En-
glish word ?in? by itself to Portuguese is not ob-
vious, since we do not have any context for the
word. This word can be translated in the con-
text of ?in (the box)? to ?dentro?, or in the con-
text of ?in (China)? as ?na?. In fact, the lexical
entry for ?in? has more than 10 good translations
in Portuguese. Consequently, the lexical translation
entry for Word-based models splits the probabilis-
tic mass between different translations, leaving the
choice based on context to the language model. On
the other hand, in Phrase-based Models, we would
have a phrase pair p(in the box, dentro da caixa)
and p(in china, na china), where the words ?in the
box? and ?in China? can be translated together to
?dentro da caixa? and ?na China?, which substan-
tially reduces the ambiguity. In this case, both the
translation and language models contribute to find
the best translation based on the local context, which
generally leads to better translations.
However, not all words add the same amount of
contextual information. Using the same example for
?in?, if we add the context ?(hid the key) in?, it is
still not possible to accurately identify the best trans-
lation for the word ?in?. The phrase extraction algo-
rithm (Ling et al 2010) does not discriminate which
phrases pairs encode contextual information, and ex-
tracts all phrase pairs with consistent alignments.
Hence, phrases that add no contextual information,
such as, p(hid the key in, escondeu a chave na)
and p(hid the key in, escondeu a chave dentro)
are extracted. This is undesirable because we are
populating translation models with redundant phrase
pairs, whose translations can be obtained using com-
962
binations of other phrases with the same probabil-
ities, namely p(hid the key, escondeu a chave),
p(in, dentro) and p(in, na). This is a problem
that is also found in language modeling, where
large amounts of redundant higher-order n-grams
can make the model needlessly large. For backoff
language models, multiple pruning strategies based
on relative entropy have been proposed (Seymore
and Rosenfeld, 1996) (Stolcke, 1998), where the ob-
jective is to prune n-grams in a way to minimize the
relative entropy between the model before and after
pruning.
While the concept of using relative entropy for
pruning is not new and frequently used in backoff
language models, there are no such models for ma-
chine translation. Thus, the main contribution of
our work is to propose a relative entropy pruning
model for translation models used in Phrase-based
Machine Translation. It is shown that our pruning
algorithm can eliminate phrase pairs with little or
no impact in the predictions made in our translation
model. In fact, by reducing the search space, less
search errors are made during decoding, which leads
to improvements in translation quality.
This paper is organized as follows. We describe
and contrast the state of the art pruning algorithms
in section 2. In section 3, we describe our relative-
entropy model for machine translation. Afterwards,
in section 4, we apply our model for pruning in
Phrase-based Machine Translation systems. We per-
form experiments with our pruning algorithm based
on phrase pair independence and analyse the results
in section 5. Finally, we conclude in section 6.
2 Phrase Table Pruning
Phrase table pruning algorithms are important in
translation, since they efficiently reduce the size of
the translation model, without having a large nega-
tive impact in the translation quality. This is espe-
cially relevant in environments where memory con-
straints are imposed, such as translation systems for
small devices like cellphones, and also when time
constraints for the translation are defined, such as
online Speech-to-Speech systems.
2.1 Significance Pruning
A relevant reference in phrase table pruning is the
work of (Johnson and Martin, 2007), where it is
shown that a significant portion of the phrase ta-
ble can be discarded without a considerable negative
impact on translation quality, or even positive one.
This work computes the probability, named p-value,
that the joint occurrence event of the source phrase
s and target phrase t occurring in same sentence pair
happens by chance, and are actually statistically in-
dependent. Phrase pairs that have a high p-value,
are more likely to be spurious and more prone to
be pruned. This work is followed in (Tomeh et al
2009), where phrase pairs are treated discriminately
based on their complexity. Significance-based prun-
ing has also been successfully applied in language
modeling in (Moore and Quirk, 2009).
Our work has a similar objective, but instead
of trying to predict the independence between the
source and target phrases in each phrase pair, we at-
tempt to predict the independence between a phrase
pair and other phrase pairs in the model.
2.2 Relevance Pruning
Another proposed approach (Matthias Eck and
Waibel, 2007) consists at collecting usage statistics
for phrase pairs. This algorithm decodes the train-
ing corpora and extracts the number of times each
phrase pair is used in the 1-best translation hypoth-
esis. Thus, phrase pairs that are rarely used during
decoding are excluded first during pruning.
This method considers the relationship between
phrase pairs in the model, since it tests whether
the decoder is more prone to use some phrase pairs
than others. However, it leads to some undesirable
pruning choices. Let us consider a source phrase
?the box in China? and 2 translation hypotheses,
where the first hypothesis uses the phrase transla-
tion p(the key in China, a chave na China) with
probability 70%, and the second hypothesis uses
two phrase translations p(the key, a chave) and
p(in China, na China) with probability 65%. This
approach will lean towards pruning the phrase pairs
in the second hypothesis, since the decoder will use
the first hypothesis. This is generally not desired,
since the 2 smaller phrase pairs can be used to trans-
late the same source sentence with a small probabil-
963
ity loss (5%), even if the longer phrase is pruned.
On the other hand, if the smaller phrases are pruned,
the longer phrase can not be used to translate smaller
chunks, such as ?the key in Portugal?. This matter is
aggravated due to the fact that the training corpora is
used to decode, so longer phrase pairs will be used
more frequently than when translating unseen sen-
tences, which will make the model more biased into
pruning shorter phrase pairs.
3 Relative Entropy Model For
Phrase-based Translation Models
In this section, we shall define our entropy model
for phrase pairs. We start by introducing some no-
tation to distinguish different types of phrase pairs
and show why some phrase pairs are more redun-
dant than others. Afterwards, we illustrate our no-
tion of relative entropy between phrase pairs. Then,
we describe our entropy model, its computation and
its application to phrase table pruning.
3.1 Atomic and Composite Phrase Pairs
We discriminate between 2 types of phrase pairs:
atomic phrase pairs and composite phrase pairs.
Atomic phrase pairs define the smallest transla-
tion units, such that given an atomic phrase pair that
translates from s to t, the same translation cannot
be obtained using any combination of other phrase
pairs. Removing these phrase pairs reduces the
range of translations that our model is capable of
translating and also the possible translations.
Composite phrase pairs define translations of a
given sequence of words that can also be obtained
using atomic or other smaller composite phrase
pairs. Each combination is called a derivation or
translation hypothesis. Removing these phrase pairs
does not change the amount of sentences that the
model can translate, since all translations encoded
in these phrases can still be translated using other
phrases, but these will lead to different translation
probabilities.
Considering table 1, we can see that atomic
phrases encode one elementary translation event,
while composite phrases encode joint events that are
encoded in atomic phrase pairs. If we look at the
source phrase ?in?, there is a multitude of possible
translations for this word in most target languages.
Taking Portuguese as the target language, the proba-
bility that ?in? is translated to ?em? is relatively low,
since it can also be translated to ?no?, ?na?, ?den-
tro?, ?dentro de? and many others.
However, if we add another word such as ?Por-
tugal? forming ?in Portugal?, it is more likely that
?in? is translated to ?em?. Thus, we define the
joint event of ?in? translating to ?em? (A1) and
?Portugal? to ?Portugal? (B1), denoted as A1 ? B1,
in the phrase pair p(in Portugal, em Portugal).
Without this phrase pair it is assumed that these
are independent events with probability given by
P (A1)P (B1)1, which would be 10%, leading to a
60% reduction. In this case, it would be more likely,
that in Portugal is translated to no Portugal or
na Portugal, which would be incorrect.
Some words, such as ?John?, forming ?John in?,
do not influence the translations for the word ?in?,
since it can still be translated to ?em?, ?no?, ?na?,
?dentro? or ?dentro de? depending on the word that
follows. By definition, if the presence of phrase
p(John, John) does not influence the translation of
p(in, em) and viceversa, we can say that probability
of the joint event P (A1?C1) is equal to the product
of the probabilities of the events P (A1)P (C1).
If we were given a choice of pruning either the
composite phrase pairs p(John in, John em) or
p(in Portugal, em Portugal), the obvious choice
would be the former, since the probability of the
event encoded in that phrase pair is composed by 2
independent events, in which case the decoder will
inherently consider the hypothesis that ?John in? is
translated to ?John em? with the same probability. In
another words, the model?s predictions even, with-
out this phrase pair will remain the same.
The example above shows an extreme case,
where the event encoded in the phrase pair
p(John in, John em) is decomposed into indepen-
dent events, and can be removed without chang-
ing the model?s prediction. However, finding and
pruning phrase pairs that are independent, based on
smaller events is impractical, since most translation
events are not strictly independent. However, many
phrase pairs can be replaced with derivations using
smaller phrases with a small loss in the model?s pre-
1For simplicity, we assume at this stage that no reordering
model is used
964
Phrase Pair Prob Event
Atomic Phrase Pairs
in? em 10% A1
in? na 20% A2
in? no 20% A3
in? dentro 5% A4
in? dentro de 5% A5
Portugal? Portugal 100% B1
John? John 100% C1
Composite Phrase Pairs
in Portugal? em Portugal 70% A1 ?B1
John in? John em 10% C1 ?A1
John in? John na 20% C1 ?A2
John in? John no 20% C1 ?A3
John in? John dentro 5% C1 ?A4
John in? John dentro de 5% C1 ?A5
Table 1: Phrase Translation Table with associated events
dictions.
Hence, we would like to define a metric for phrase
pairs that allows us evaluate how discarding each
phrase pair will affect the pruned model?s predic-
tions. By removing phrase pairs that can be derived
using smaller phrase pairs with similar probability,
it is possible to discard a significant portion of the
translation model, while minimizing the impact on
the model?s predictions.
3.2 Relative Entropy Model for Machine
Translation
For each phrase pair pa, we define the supporting
set SP (pa(s, t)) = S1, ..., Sk, where each element
Si = pi, ..., pj is a distinct derivation of pa(s, t) that
translates s to t, with probability P (Si) = P (pi) ?
...?P (pj). A phrase pair can have multiple elements
in its supporting set. For instance, the phrase pair
p(John in Portugal, John em Portugal), has 3
elements in the support set:
? S1 = {p(John, John), p(in, em), p(Portugal, Portugal)}
? S2 = {p(John, John), p(in Portugal, em Portugal)}
? S3 = {p(John in, John em), p(Portugal, Portugal)}
S1, S2 and S3 encode 3 different assumptions
about the event of translating ?John in Portugal?
to ?John em Portugal?. S1 assumes that the event
is composed by 3 independent events A1, B1 and
C1, S2 assumes that A1 and B1 are dependent, and
groups them into a single composite event A1 ?B1,
which is independent from C1, and S3 groups A1
and C1 independently from B1. As expected, the
event encoded in the phrase pair p itself isA1?B1?
C1, which assumes thatA1,B1 andC1 are all depen-
dent. We can see that if any of the events S1, S2 or
S3 has a ?similar probability? as the event coded in
the phrase pair, we can remove this phrase pair with
a minimal impact in the phrase prediction.
To formalize our notion of ?similar probabil-
ity?, we apply the relative entropy or the Kullback-
Leibler divergence, and define the divergence be-
tween a pruned translation model Pp(s, t) and the
unpruned model P (s, t) as:
D(Pp||P ) = ?
?
s,t
P (s, t)log
Pp(t|s)
P (t|s)
(1)
Where Pp(t|s)P (t|s) , measures the deviation from the
probability emission from the pruned model and the
original probability from the unpruned model, for
each source-target pair s, t. This is weighted by
the frequency that the pair s, t is observed, given by
P (s, t).
Our objective is to minimize D(Pp||P ), which
can be done locally by removing phrase pairs p(s, t)
with the lowest values for ?P (s, t)logPp(t|s)P (t|s) . Ide-
ally, we would want to minimize the relative entropy
for all possible source and target sentences, rather
than all phrases in our model. However, minimiz-
ing such an objective function would be intractable
due to reordering, since the probability assigned to a
phrase pair in a sentence pair by each model would
depend on the positioning of all other phrase pairs
used in the sentence. Because of these dependen-
cies, we would not be able to reduce this problem to
a local minimization problem. Thus, we assume that
all phrase pairs have the same probability regardless
of their context in a sentence.
Thus, our pruning algorithm takes a threshold ?
and prunes all phrase pairs that fail to meet the fol-
lowing criteria:
?P (s, t)log
Pp(t|s)
P (t|s)
> ? (2)
The main components of this function is the ratio
between the emission from the pruned model and
965
unpruned models given by Pp(t|s)P (t|s) , and the weight
given to each s, t pair given by P (s, t). In the re-
mainder of this section, we will focus on how to
model each of these components in equation 2.
3.3 Computing P (s, t)
The term P (s, t) can be seen as a weighting function
for each s, t pair. There is no obvious optimal dis-
tribution to model P (s, t). In this work, we apply 2
different distributions for P (s, t). First, an uniform
distribution, where all phrases are weighted equally.
Secondly, a multinomial function defined as:
P (s, t) =
N(s, t)
N
(3)
whereN is the number of sentence pairs in the paral-
lel data, and N(s, t) is the number of sentence pairs
where s was observed in the source sentence and t
was observed in the target sentence. Using this dis-
tribution, the model is more biased in pruning phrase
pairs with s, t pairs that do not occur frequently.
3.4 Computing Pp(t|s)P (t|s)
The computation of Pp(t|s)P (t|s) depends on how the de-
coder adapts when a phrase pair is pruned from the
model. In the case of back-off language models,
this can be solved by calculating the difference of
the logs between the n-gram estimate and the back-
off estimate. However, a translation decoder gen-
erally functions differently. In our work, we will
assume that the decoding will be performed using
a Viterbi decoder, such as MOSES (Koehn et al
2007), where the translation with the highest score
is chosen.
In the example above, where s=?John in Portu-
gal? and t=?John em Portugal?, the decoder would
choose the derivation with the highest probability
from s to t. Using the unpruned model, the possi-
ble derivations are either using phrase p(s, t) or one
element of its support set S1, S2 or S3. On the other
hand, on the pruned model where p(s, t) does not
exist, only S1, S2 and S3 can be used. Thus, given
a s, t pair one of three situations may occur. First, if
the probability of the phrase pair p(s, t) is lower than
the highest probability element in SP (p(s, t)), then
both the models will choose that element, in which
case, Pp(t|s)P (t|s) = 1. This can happen, if we define
features that penalize longer phrase pairs, such as
lexical weighting, or if we apply smoothing (Foster
et al 2006). Secondly, if the probability of p(s, t)
is equal to the most likely element in SP (p(s, t)),
regardless of whether the unpruned model choses to
use p(s, t) or that element, the probability emissions
of the pruned and unpruned model will be identi-
cal. Thus, for this case Pp(t|s)P (t|s) = 1. Finally, if the
probability of p(s, t) is higher than other possible
derivations, the unpruned model will choose to emit
the probability of p(s, t), while the pruned model
will emit the most likely element in SP (p(s, t)).
Hence, the probability loss between the 2 models,
will be the ratio between the probability of p(s, t)
and the probability of the most likely element in
SP (p(s, t)).
From the example above, we can generalize the
function for Pp(t|s)P (t|s) as:
?
p??argmax(SP (p(s,t))) P (p
?)
P (p(s, t))
(4)
Where P (p(s, t)) denotes the probability of
p(s, t) and
?
p??argmax(SP (p(s,t))) P (p
?) the most
likely sequence of phrasal translations that translates
s to t, with the probability equal to the product of all
phrase translation probabilities in that sequence.
Replacing in equation 2, our final condition that
must be satisfied for keeping a phrase pair is:
?P (s, t)log
?
p??argmax(SP (p(s,t))) P (p
?)
P (p(s, t))
> ? (5)
4 Application for Phrase-based Machine
Translation
We will now show how we apply our entropy prun-
ing model in the state-of-the-art phrase-based trans-
lation system MOSES and describe the problems
that need to be addressed during the implementation
of this model.
4.1 Translation Model
The translation model in Moses is composed by
a phrase translation model and a phrase reorder-
ing model. The first one models, for each phrase
pair p(s, t), the probability of translating the s to
t by combining multiple features ?i, weighted by
966
wTi , as PT (p) =
?n
i=1 ?i(p)
wTi . The reordering
model is similar, but models the local reordering be-
tween p, given the previous and next phrase accord-
ing to the target side, pP and pN , or more formally,
PR(p|pP , pN ) =
?m
i=1 ?i(p|pP , pP )
wRi
4.2 Building the Support Set
Essentially, implementing our model is equiva-
lent to calculating the components described in
equation 5. These are P (s, t), P (p(s|t)) and
argmax(SP (p(s, t))). Calculating the uniform dis-
tribution and multinomial distributions for P (s, t)
is simple, the uniform distribution just assumes the
same value for all s and t, and the multinomial dis-
tribution can be modeled by extracting counts from
the parallel corpora.
Calculating P (s|t) is also trivial, since it only en-
volves calculating PT (p(s, t)), which can be done
by retrieving the translation features of p and apply-
ing the weights for each feature.
The most challenging task is to calculate
argmax(SP (p(s, t))), which is similar to the de-
coding task in machine translation, where we need to
find the best translation t? for a sentence s, that is, t? =
argmaxtP (s|t)P (t). In practice, we are not search-
ing in the space of possible translations, but in the
space of possible derivations, which are sequences
of phrase translations p1(s1, t1), ..., pn(sn, tn) that
can be applied to s to generate an output t with the
score given by P (t)
?n
i=1 P (si, ti).
Our algorithm to determine SP (p(s, t)) can be
described as an adaptation to the decoding algorithm
in Moses, where we restrict the search space to the
subspace SP (p(s, t)), that is, our search space is
only composed by derivations that output t, with-
out using p itself. This can be done using the forced
decoding algorithm proposed in (Schwartz, 2008).
Secondly, the score of a given translation hypothesis
does not depend on the language model probability
P (t), since all derivations in this search space have
the same t, thus we discard this probability from
the score function. Finally, rather than using beam
search, we exhaustively search all the search space,
to reduce the hypothesis of incurring a search error
at this stage. This is possible, since phrase pairs are
generally smaller than text (less than 8 words), and
because we are constraining the search space to t,
which is an order of magnitude smaller than the reg-
ular search space with all possible translations.
4.3 Pruning Algorithm
The algorithm to generate a pruned translation
model is shown in 1. We iterate over all phrase pairs
p1(s1, t1), ..., pn(sn, tn), decode using our forced
decoding algorithm from si to ti, to obtain the best
path S. If no path is found then it means that the pi
is atomic. Then, we prune pi based on condition 5.
Algorithm 1 Independence Pruning
Require: pruning threshold ?,
unpruned model {p1(s1, t1), ..., pn(sn, tn)}
for pi(si, ti) ? {p1(s1, t1), ..., pn(sn, tn)} do
S := argmax(SP (pi)) \ pi
score :=?
if S 6= {} then
score := ?P (s, t)log
?
p?(s?,t?)?S P (s
?|t?)
P (s|t)
end if
if score ? ? then
prune(pi)
end if
end for
return pruned model
The main bottle neck in this algorithm is find-
ing argmax(SP (pi)). While this appears relatively
simple and similar to a document decoding task, the
size of our task is on a different order of magni-
tude, since we need to decode every phrase pair in
the translation model, which might not be tractable
for large models with millions of phrase pairs. We
address this problem in section 5.3.
Another problem with this algorithm is that the
decision to prune each phrase pair is made assuming
that all other phrase pairs will remain in the model.
Thus, there is a chance a phrase pair p1 is pruned
because of a derivation using p2 and p3 that leads to
the same translation. However, if p3 also happens to
be pruned, such a derivation will no longer be pos-
sible. One possible solution to address this problem
is to perform pruning iteratively, from the smallest
phrase pairs (number of words) and increase the size
at each iteration. However, we find this undesirable,
since the model will be biased into removing smaller
phrase pairs, which are generally more useful, since
they can be used in multiple derivation to replace
larger phrase pairs. In the example above, the model
967
would eliminate p3 and keep p1, yet the best deci-
sion could be to keep p3 and remove p1, if p3 is also
frequently used in derivations of other phrase pairs.
Thus, we leave the problem of finding the best set of
phrases to prune as future work.
5 Experiments
We tested the performance of our system under two
different environments. The first is the small scale
DIALOG translation task for IWSLT 2010 evalua-
tion (Paul et al 2010) using a small corpora for
the Chinese-English language pair (henceforth re-
ferred to as ?IWSLT?). The second one is a large
scale test using the complete EUROPARL (Koehn,
2005) corpora for the Portuguese-English language
pair, which we will denote by ?EUROPARL?.
5.1 Corpus
The IWSLT model was trained with 30K training
sentences. The development corpus and test corpus
were taken from the evaluation dataset in IWSLT
2006 (489 tuning and 500 test sentences with 7 ref-
erences). The EUROPARL model was trained using
the EUROPARL corpora with approximately 1.3M
sentence pairs, leaving out 1K sentences for tuning
and another 1K sentences for tests.
5.2 Setup
In the IWSLT experiment, word alignments were
generated using an HMM model (Vogel et al 1996),
with symmetric posterior constraints (V. Grac?a et
al., 2010), using the Geppetto toolkit2. This setup
was used in the official evaluation in (Ling et al
2010). For the EUROPARL experiment the word
alignments were generated using IBM model 4. In
both experiments, the translation model was built
using the phrase extraction algorithm (Paul et al
2010), with commonly used features in Moses (Ex:
probability, lexical weighting, lexicalized reordering
model). The optimization of the translation model
weights was done using MERT tuning (Och, 2003)
and the results were evaluated using BLEU-4.
5.3 Pruning Setup
Our pruning algorithm is applied after the translation
model weight optimization with MERT. We gener-
2http://code.google.com/p/geppetto/
ate multiple translation models by setting different
values for ?, so that translation models of different
sizes are generated at intervals of 5%. We also run
the significance pruning (Johnson and Martin, 2007)
algorithm in these conditions.
While the IWSLT translation model has only
88,424 phrase pairs, for the EUROPARL exper-
iment, the translation model was composed by
48,762,372 phrase pairs, which had to be decoded.
The average time to decode each phrase pair us-
ing the full translation model is 4 seconds per sen-
tence, since the table must be read from disk due to
its size. This would make translating 48M phrase
pairs unfeasible. To address this problem, we di-
vide the phrase pairs in the translation model into
blocks of K phrase pairs, that are processed sepa-
rately. For each block, we resort to the approach
used in MERT tuning, where the model is filtered to
only include the phrase pairs that are used for trans-
lating tuning sentences. We filter each block with
phrase pairs fromK to 2K with the source sentences
sK , ..., s2K . Furthermore, since we are force de-
coding using the target sentences, we also filter the
remaining translation models using the target sen-
tences tK , ..., t2K . We used blocks of 10,000 phrase
pairs and each filtered table was reduced to less than
1% of the translation table on average, reducing the
average decoding time to 0.03 seconds per sentence.
Furthermore, each block can be processed in parallel
allowing multiple processes to be used for the task,
depending on the resources that are available.
5.4 Results
Figure 1 shows the BLEU results for different sizes
of the translation model for the IWSLT experiment
using the uniform and multinomial distributions for
P (s, t). We observe that there is a range of values
from 65% to 95% where we actually observe im-
provements caused by our pruning algorithm, with
the peak at 85% for the uniform distribution, where
we improve from 15.68 to 15.82 (0.9% improve-
ment). Between 26% and 65%, the BLEU score is
lower than the baseline at 100%, with the minimum
at 26% with 15.54, where only atomic phrase pairs
remain and both the multinomial and uniform distri-
bution have the same performance, obviously. This
is a considerable reduction in phrase table size by
sacrificing 0.14 BLEU points. Regarding the com-
968
15.5	 ?
15.55	 ?
15.6	 ?
15.65	 ?
15.7	 ?
15.75	 ?
15.8	 ?
15.85	 ?
25
%	 ?
30
%	 ?
35
%	 ?
40
%	 ?
45
%	 ?
50
%	 ?
55
%	 ?
60
%	 ?
65
%	 ?
70
%	 ?
75
%	 ?
80
%	 ?
85
%	 ?
90
%	 ?
95
%	 ?
10
0%
	 ?
IWSLT	 ?Results	 ?
Uniform	 ?
Mul?nomial	 ?
Figure 1: Results for the IWSLT experiment. The x-
axis shows the percentage of the phrase table used. The
BLEU scores are shown in the y-axis. Two distributions
for P (s, t) were tested Uniform and Multinomial.
parison between the uniform and multinomial distri-
bution, we can see that both distributions yield sim-
ilar results, specially when a low number of phrase
pairs is pruned. In theory, the multinomial distri-
bution should yield better results, since the pruning
model will prefer to prune phrase pairs that are more
likely to be observed. However, longer phrase pairs,
which tend compete with other long phrase pairs on
which get pruned first. These phrase pairs gener-
ally occur only once or twice, so the multinomial
model will act similarly to the uniform model re-
garding longer phrase pairs. On the other hand, as
the model size reduces, we can see that using multi-
nomial distribution seems to start to improve over
the uniform distribution.
The comparison between our pruning model and
pruning based on significance is shown in table 2.
These models are hard to compare, since not all
phrase table sizes can be obtained using both met-
rics. For instance, the significance metric can ei-
ther keep or remove all phrase pairs that only appear
once, leaving a large gap of phrase table sizes that
cannot be attained. In the EUROPARL experiment
the sizes of the table suddenly drops from 60% to
8%. The same happens with our metric that cannot
distinguish atomic phrase pairs. In the EUROPARL
experiment, we cannot generate phrase tables with
sizes smaller than 15%. Thus, we only show re-
sults at points where both algorithms can produce
a phrase table.
Significant improvements are observed in the
Table size Significance Entropy (u) Entropy (m)
Pruning Pruning Pruning
IWSLT
57K (65%) 14.82 15.77 15.78
71K (80%) 15.14 15.76 15.77
80K (90%) 15.31 15.73 15.72
88K (100%) 15.68 15.68 15.68
EUROPARL
29M (60%) 28.64 28.82 28.91
34M (70%) 28.84 28.94 28.99
39M (80%) 28.86 28.99 28.99
44M (90%) 28.91 29.00 29.02
49M (100%) 29.18 29.18 29.18
Table 2: Comparison between Significance Pruning (Sig-
nificance Pruning) and Entropy-based pruning using the
uniform (Entropy (u) Pruning) and multinomial distribu-
tions (Entropy (m) Pruning).
IWSLT experiment, where significance pruning
does not perform as well. On the other hand, on the
EUROPARL experiment, our model only achieves
slightly higher results. We believe that this is re-
lated by the fact the EUROPARL corpora is gener-
ated from automatically aligning documents, which
means that there are misaligned sentence pairs.
Thus, many spurious phrase pairs are extracted. Sig-
nificance pruning performs well under these condi-
tions, since the measure is designed for this purpose.
In our metric, we do not have any means for detect-
ing spurious phrase pairs, in fact, spurious phrase
pairs are probably kept in the phrase table, since
each distinct spurious phrase pair is only extracted
once, and thus, they have very few derivations in
its support set. This suggests, that the significance
score can be integrated in our model to improve our
model, which we leave as future work.
John married Portugal
married 
in
in 
Portugal
married 
married 
in
John 
in 
Portugal
Portugal
a)
b)
Figure 2: Translation order in for different reordering
starting from left to right.
We believe that in language pairs such as Chinese-
969
English with large distance reorderings between
phrases are more prone to search errors and benefit
more from our pruning algorithm. To illustrate this,
let us consider the source sentence ?John married
in Portugal?, and translating either using the blocks
?John?, ?married? and ?in Portugal? or the blocks
?John?, ?married in?, ?Portugal?, the first hypoth-
esis would be much more viable, since the word
?Portugal? is more relevant as the context for the
word ?in?. Thus, the key choice for the decoder is
to decide whether to translate using ?married? with
or without ?in?, and it is only able to predict that
it is better to translate ?married? by itself until it
finds that ?in? is better translated with ?Portugal?.
Thus, a search error occurs if the hypothesis where
?married? is translated by itself is removed. In fig-
ure 2, we can see the order that blocks are consid-
ered for different reorderings, starting from left to
right. In a), we illustrate the case for a monotonous
translation. We observe that the correct decision be-
tween translating ?married in? or just ?married? is
found immediately, since the blocks ?Portugal? and
?in Portugal? are considered right afterwards. In this
case, it is unlikely that the hypothesis using ?mar-
ried? is removed. However, if we consider that due
to reordering, ?John? is translated after ?married?
and before ?Portugal?, which is shown in b). Then,
the correct decision can only be found after consid-
ering ?John?. In this case, ?John? does not have
many translations, so the likelihood of eliminating
the correct hypothesis. However, if there were many
translations for John, it is highly likely that the cor-
rect partial hypothesis is eliminated. Furthermore,
the more words exist between ?married? and ?Portu-
gal?, the more likely will the correct hypothesis not
exist when we reach ?Portugal?. By pruning the hy-
pothesis ?married in? a priori, we contribute in pre-
venting such search errors.
We observe that some categories of phrase pairs
that are systematically pruned, but these cannot
be generalized in rules, since there are many ex-
ceptions. The most obvious type of phrase pairs
are phrases with punctuations, such as ???.? to
?thanks .? and ?. ??? to ?thanks .?, since ?.?
is translated independently from most contextual
words. However, this rule should not be general-
ized, since in some cases ?.? is a relevant contextual
marker. For instance, the word ?please? is translated
to ??? in the sentence ?open the door, please.? and
translated to ????? in ?please my advisors?. An-
other example are sequences of numbers, which are
generally translated literally. For instance, ??(8)
?(3)?(8)? is translated to ?eight three eight? (Ex:
?room eight three eight?). Thus, phrase pairs for
number sequences can be removed, since those num-
bers can be translated one by one. However, for se-
quences such as ??(1)?(8)?, we need a phrase pair
to represent this specifically. This is because ??(1)?
can be translated to ?one?, but also to ?a?, ?an?, ?sin-
gle?. Other exceptions include ??(1)?(1)?, which
tends to be translated as ?eleven?, and which tends to
be translated to ?o?, rather than ?zero? in sequences
(?room eleven o five?).
6 Conclusions
We present a pruning algorithm for Machine Trans-
lation based on relative entropy, where we assess
whether the translation event encoded in a phrase
pair can be decomposed into combinations of events
encoded in other phrase pairs. We show that such
phrase pairs can be removed from the translation
model with little negative impact or even a positive
one in the overall translation quality. Tests show that
our method yields comparable or better results with
state of the art pruning algorithms.
As future work, we would like to combine our
approach with significance pruning, since both ap-
proaches are orthogonal and address different issues.
We also plan to improve the pruning step of our algo-
rithm to find the optimal set of phrase pairs to prune
given the pruning threshold.
The code used in this work will be made available.
7 Acknowledgements
This work was partially supported by FCT (INESC-
ID multiannual funding) through the PIDDAC Pro-
gram funds, and also through projects CMU-
PT/HuMach/0039/2008 and CMU-PT/0005/2007.
The PhD thesis of Wang Ling is supported by FCT
grant SFRH/BD/51157/2010. The authors also wish
to thank the anonymous reviewers for many helpful
comments.
970
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19:263?311, June.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?06, pages 53?61, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
J Howard Johnson and Joel Martin. 2007. Improv-
ing translation quality by discarding most of the
phrasetable. In In Proceedings of EMNLP-CoNLL?07,
pages 967?975.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 48?54, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-burch, Richard Zens, Rwth Aachen, Alexan-
dra Constantin, Marcello Federico, Nicola Bertoldi,
Chris Dyer, Brooke Cowan, Wade Shen, Christine
Moran, and Ondrej Bojar. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Summit,
pages 79?86, Phuket, Thailand. AAMT, AAMT.
Wang Ling, Tiago Lu??s, Joa?o Grac?a, Lu??sa Coheur, and
Isabel Trancoso. 2010. Towards a general and ex-
tensible phrase-extraction algorithm. In IWSLT ?10:
International Workshop on Spoken Language Transla-
tion, pages 313?320, Paris, France.
Stephen Vogal Matthias Eck and Alex Waibel. 2007. Es-
timating phrase pair relevance for translation model
pruning. MTSummit XI.
Robert C. Moore and Chris Quirk. 2009. Less is more:
significance-based n-gram selection for smaller, bet-
ter language models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 2 - Volume 2, EMNLP ?09,
pages 746?755, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Michael Paul, Marcello Federico, and Sebastian Stu?ker.
2010. Overview of the iwslt 2010 evaluation cam-
paign. In IWSLT ?10: International Workshop on Spo-
ken Language Translation, pages 3?27.
Lane Schwartz. 2008. Multi-source translation methods.
In Proceedings of AMTA, pages 279?288.
Kristie Seymore and Ronald Rosenfeld. 1996. Scalable
backoff language models. In In Proceedings of ICSLP,
pages 232?235.
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In In Proc. DARPA Broad-
cast News Transcription and Understanding Work-
shop, pages 270?274.
Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.
2009. Complexity-based phrase-table filtering for sta-
tistical machine translation. MTSummit XII, Aug.
Joa?o V. Grac?a, Kuzman Ganchev, and Ben Taskar. 2010.
Learning Tractable Word Alignment Models with
Complex Constraints. Comput. Linguist., 36:481?504.
S. Vogel, H. Ney, and C. Tillmann. 1996. Hmm-
based word alignment in statistical translation. In
Proceedings of the 16th conference on Computational
linguistics-Volume 2, pages 836?841. Association for
Computational Linguistics.
971
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1389?1398, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Wiki-ly Supervised Part-of-Speech Tagging
Shen Li
Computer & Information Science
University of Pennsylvania
shenli@seas.upenn.edu
Joa?o V. Grac?a
L2F INESC-ID
Lisboa, Portugal
javg@l2f.inesc-id.pt
Ben Taskar
Computer & Information Science
University of Pennsylvania
taskar@cis.upenn.edu
Abstract
Despite significant recent work, purely unsu-
pervised techniques for part-of-speech (POS)
tagging have not achieved useful accuracies
required by many language processing tasks.
Use of parallel text between resource-rich and
resource-poor languages is one source of weak
supervision that significantly improves accu-
racy. However, parallel text is not always
available and techniques for using it require
multiple complex algorithmic steps. In this
paper we show that we can build POS-taggers
exceeding state-of-the-art bilingual methods
by using simple hidden Markov models and
a freely available and naturally growing re-
source, the Wiktionary. Across eight lan-
guages for which we have labeled data to eval-
uate results, we achieve accuracy that signifi-
cantly exceeds best unsupervised and parallel
text methods. We achieve highest accuracy re-
ported for several languages and show that our
approach yields better out-of-domain taggers
than those trained using fully supervised Penn
Treebank.
1 Introduction
Part-of-speech categories are elementary building
blocks that play an important role in many natu-
ral language processing tasks, from machine trans-
lation to information extraction. Supervised learn-
ing of taggers from POS-annotated training text is
a well-studied task, with several methods achieving
near-human tagging accuracy (Ratnaparkhi, 1996;
Toutanova et al 2003; Shen et al 2007). How-
ever, while English and a handful of other languages
are fortunate enough to have comprehensive POS-
annotated corpora such as the Penn Treebank (Mar-
cus et al 1993), most of the world?s languages have
no labeled corpora. The annotated corpora that do
exist were costly to build (Abeille?, 2003), and are
often not freely available or restricted to research-
only use. Furthermore, much of the annotated text is
of limited genre, normally focusing on newswire or
literary text. Performance of treebank-trained sys-
tems degrades significantly when applied to new do-
mains (Blitzer et al 2006).
Unsupervised induction of POS taggers offers the
possibility of avoiding costly annotation, but de-
spite recent progress, the accuracy of unsupervised
POS taggers still falls far behind supervised sys-
tems, and is not suitable for most applications (Berg-
Kirkpatrick et al 2010; Grac?a et al 2011; Lee et
al., 2010). Using additional information, in the form
of tag dictionaries or parallel text, seems unavoid-
able at present. Early work on using tag dictionaries
used a labeled corpus to extract all allowed word-tag
pairs (Merialdo, 1994), which is quite an unrealis-
tic scenario. More recent work has used a subset of
the observed word-tag pairs and focused on gener-
alizing dictionary entries (Smith and Eisner, 2005;
Haghighi and Klein, 2006; Toutanova and Johnson,
2007; Goldwater and Griffiths, 2007). Using corpus-
based dictionaries greatly biases the test results, and
gives little information about the capacity to gener-
alize to different domains.
Recent work by Das and Petrov (2011) builds
a dictionary for a particular language by transfer-
ring annotated data from a resource-rich language
through the use of word alignments in parallel text.
1389
The main idea is to rely on existing dictionaries for
some languages (e.g. English) and use parallel data
to build a dictionary in the desired language and ex-
tend the dictionary coverage using label propaga-
tion. However, parallel text does not exist for many
pairs of languages and the proposed bilingual pro-
jection algorithms are fairly complex.
In this work we use the Wiktionary, a freely avail-
able, high coverage and constantly growing dic-
tionary for a large number of languages. We ex-
periment with a very simple second-order Hidden
Markov Model with feature-based emissions (Berg-
Kirkpatrick et al 2010; Grac?a et al 2011). We out-
perform best current results using parallel text su-
pervision across 8 different languages, even when
the word type coverage is as low as 20%. Further-
more, using the Brown corpus as out-of-domain data
we show that using the Wiktionary produces bet-
ter taggers than using the Penn Treebank dictionary
(88.5% vs 85.9%). Our empirical analysis and the
natural growth rate of the Wiktionary suggest that
free, high-quality and multi-domain POS-taggers for
a large number of languages can be obtained by stan-
dard and efficient models.
The source code, the dictionary mappings and
the trained models described in this work are
available at http://code.google.com/p/
wikily-supervised-pos-tagger/.
2 Related Work
The scarcity of labeled corpora for resource poor
languages and the challenges of domain adaptation
have led to several efforts to build systems for unsu-
pervised POStagging.
Several lines of research have addressed the fully
unsupervised POS-tagging task: mutual information
clustering (Brown et al 1992; Clark, 2003) has been
used to group words according to their distributional
context. Using dimensionality reduction on word
contexts followed by clustering has led to accuracy
gains (Schu?tze, 1995; Lamar et al 2010). Sequence
models, HMMs in particular, have been used to rep-
resent the probabilistic dependencies between con-
secutive tags. In these approaches, each observa-
tion corresponds to a particular word and each hid-
den state corresponds to a cluster. However, us-
ing maximum likelihood training for such models
does not achieve good results (Clark, 2003): max-
imum likelihood training tends to result in very am-
biguous distributions for common words, in contra-
diction with the rather sparse word-tag distribution.
Several approaches have been proposed to mitigate
this problem, including Bayesian approaches using
an improper Dirichlet prior to favor sparse model
parameters (Johnson, 2007; Gao and Johnson, 2008;
Goldwater and Griffiths, 2007), or using the Poste-
rior Regularization to penalize ambiguous posteri-
ors distributions of tags given tokens (Grac?a et al
2009). Berg-Kirkpatrick et al(2010) and Grac?a et
al. (2011) proposed replacing the multinomial emis-
sion distributions of standard HMMs by maximum
entropy (ME) feature-based distributions. This al-
lows the use of features to capture morphological in-
formation, and achieves very promising results. De-
spite these improvements, fully unsupervised sys-
tems require an oracle to map clusters to true tags
and the performance still fails to be of practical use.
In this paper we follow a different line of work
where we rely on a prior tag dictionary indicating for
each word type what POS tags it can take on (Meri-
aldo, 1994). The task is then, for each word token
in the corpus, to disambiguate between the possible
POS tags. Even when using a tag dictionary, disam-
biguating from all possible tags is still a hard prob-
lem and the accuracy of these methods is still fall far
behind their supervised counterparts. The scarcity
of large, manually-constructed tag dictionaries led
to the development of methods that try to generalize
from a small dictionary with only a handful of en-
tries (Smith and Eisner, 2005; Haghighi and Klein,
2006; Toutanova and Johnson, 2007; Goldwater and
Griffiths, 2007), however most previous works build
the dictionary from the labeled corpus they learn on,
which does not represent a realistic dictionary. In
this paper, we argue that the Wiktionary can serve as
an effective and much less biased tag dictionary.
We note that most of the previous dictionary
based approaches can be applied using the Wik-
tionary and would likely lead to similar accuracy in-
creases that we show in this paper. For example, the
work if Ravi and Knight (2009) minimizes the num-
ber of possible tag-tag transitions in the HMM via
a integer program, hence discarding unlikely tran-
sitions that would confuse the model. Models can
also be trained jointly using parallel corpora in sev-
1390
eral languages, exploiting the fact that different lan-
guages present different ambiguities (Snyder et al
2008).
The Wiktionary has been used extensively for
other tasks such as domain specific information
retrieval (Mu?ller and Gurevych, 2009), ontology
matching (Krizhanovsky and Lin, 2009), synonymy
detection (Navarro et al 2009), sentiment classifi-
cation (Chesley et al 2006). Recently, Ding (2011)
used the Wiktionary to initialize an HMM for Chi-
nese POS tagging combined with label propagation.
3 The Wiktionary and tagged corpora
The Wiktionary1 is a collaborative project that aims
to produce a free, large-scale multilingual dictio-
nary. Its goal is to describe all words from all lan-
guages (currently more than 400) using definitions
and descriptions in English. The coverage of the
Wiktionary varies greatly between languages: cur-
rently there are around 75 languages for which there
exists more than 1000 word types, and 27 for which
there exists more than 10,000 word types. Neverthe-
less, the Wiktionary has been growing at a consid-
erable rate (see Figure 1), and the number of avail-
able words has almost doubled in the last three years.
As more people use the Wiktionary, it is likely to
grow. Unlike tagged corpora, the Wiktionary pro-
vides natural incentives for users to contribute miss-
ing entries and expand this communal resource akin
to Wikipedia. As with Wikipedia, the questions of
accuracy, bias, consistency across languages, and se-
lective coverage are paramount. In this section, we
explore these concerns by comparing Wiktionary to
dictionaries derived from tagged corpora.
3.1 Labeled corpora and Universal tags
We collected part-of-speech tagged corpora for
9 languages, from CoNLL-X and CoNLL-2007
shared tasks on dependency parsing (Buchholz and
Marsi, 2006; Nivre et al 2007). In this work we
use the Universal POS tag set (Petrov et al 2011)
that defines 12 universal categories with a relatively
stable functional definition across languages. These
categories include NOUN, VERB, ADJ = adjective,
ADV = adverb, NUM = number, ADP = adposition,
CONJ = conjunction, DET = determiner, PRON =
1http://www.wiktionary.org/
!"#""$%
!&#""$%
!'#""$%
!(#""$%
!)#""$%
!*#""$%
!+#""$%
!,#""$%
!!#""$%
!-#""$%
-"#""$%
"%
*"""""%
&""""""%
&*"""""%
'""""""%
'*"""""%
(""""""%
(*"""""%
)""""""%
)*"""""%
*""""""%
./01&"%
2301&"%
./41&"%
5671&"%
5681&"%
2691&"%
:;31&"%
<=>1&"%
?@A1&"%
B;=1&"%
5/71&&%
C;D1&&%
./01&&%
2301&&%
./41&&%
5671&&%
5681&&%
2691&&%
:;31&&%
<=>1&&%
?@A1&&%
B;=1&&%
5/71&'%
C;D1&'%
288% -E/79F% 2==60/=4%
Figure 1: Growth of the Wiktionary over the last three
years, showing total number of entries for all languages
and for the 9 languages we consider (left axis). We
also show the corresponding increase in average accuracy
(right axis) achieved by our model across the 9 languages
(see details below).
pronoun, PUNC = punctuation, PRT = particle, and
X = residual (a category for language-specific cat-
egories which defy cross-linguistic classification).
We found several small problems with the mapping2
which we corrected as follows. In Spanish, the fine-
level tag for date (?w?) is mapped to universal tag
NUM, while it should be mapped to NOUN. In Dan-
ish there were no PRT, NUM, PUNC, or DET tags in
the mapping. After examining the corpus guidelines
and the mapping more closely, we found that the tag
AC (Cardinal numeral) and AO (Ordinal numeral)
are mapped to ADJ. Although the corpus guidelines
indicate the category SsCatgram ?adjective? that en-
compasses both ?normal? adjectives (AN) as well as
cardinal numeral (AC) and ordinal numerals (AO),
we decided to tag AC and AO as NUM, since this
assignment better fits the existing mapping. We also
reassigned all punctuation marks, which were erro-
neously mapped to X, to PUNC and the tag U which
is used for words at, de and som, to PRT.
3.2 Wiktionary to Universal tags
There are a total of 330 distinct POS-type tags
in Wiktionary across all languages which we have
mapped to the Universal tagset. Most of the map-
ping was straightforward since the tags used in the
Wiktionary are in fact close to the Universal tag
set. Some exceptions like ?Initialism?, ?Suffix?
2http://code.google.com/p/
universal-pos-tags/
1391
were discarded. We also mapped relatively rare tags
such as ?Interjection?, ?Symbol? to the ?X? tag.
A example of POS tags for several words in the
Wiktionary is shown in Table 1. All the mappings
are available at http://code.google.com/
p/wikily-supervised-pos-tagger/.
3.3 Wiktionary coverage
There are two kinds of coverage of interest: type
coverage and token coverage. We define type cov-
erage as the proportion of word types in the corpus
that simply appear in the Wiktionary (accuracy of
the tag sets are considered in the next subsection).
Token coverage is defined similarly as the portion
of all word tokens in the corpus that appear in the
Wiktionary. These statistics reflect two aspects of
the usefulness of a dictionary that affect learning in
different ways: token coverage increases the density
of supervised signal while type coverage increases
the diversity of word shape supervision. At one ex-
treme, with 100% word and token coverage, we re-
cover the POS tag disambiguation scenario and, on
the other extreme of 0% coverage, we recover the
unsupervised POS induction scenario.
The type and token coverage of Wiktionary for
each of the languages we are using for evaluation
is shown in Figure 2. We plot the coverage bar for
three different versions of Wiktionary (v20100326,
v20110321, v20120320), arranged chronologically.
We chose these three versions of the Wiktionary
simply by date, not any other factors like coverage,
quality or tagging accuracy.
As expected, the newer versions of the Wiktionary
generally have larger coverage both on type level
and token level. Nevertheless, even for languages
whose type coverage is relatively low, such as Greek
(el), the token level coverage is still quite good
(more than half of the tokens are covered). The rea-
son for this is likely the bias of the contributors to-
wards more frequent words. This trend is even more
evident when we break up the coverage by frequency
of the words. Since the number of words varies from
corpus to corpus, we normalize the word counts by
the count of the most frequent word(s) in its corpus
and group the normalized frequency into three cat-
egories labeled as ?low?, ?medium? and ?high? and
for each category, we calculate the word type cover-
age, shown in Figure 3.
Figure 2: Type-level (top) and token-level (bottom) cov-
erage for the nine languages in three versions of the Wik-
tionary.
We also compared the coverage provided by the
Wiktionary versus the Penn Treebank (PTB) ex-
tracted dictionary on the Brown corpus. Figure 4
shows that the Wiktionary provides a greater cover-
age for all sections of the Brown corpus, hence being
a better dictionary for tagging English text in gen-
eral. This is also reflected in the gain in accuracy on
Brown over the taggers learned from the PTB dic-
tionary in our experiments.
3.4 Wiktionary accuracy
A more refined notion of quality is the accuracy of
the tag sets for covered words, as measured against
dictionaries extracted from labeled tree bank cor-
pora. We consider word types that are in both the
Wiktionary (W) and the tree bank dictionaries (T).
For each word type, we compare the two tag sets
and distinguish five different possibilities:
1. Identical: W = T
2. Superset: W ? T
3. Subset: W ? T
4. Overlap: W ? T 6= ?
1392
Wiktionary Entries
Universal POS Set
Language Word POS Definition
English today Adverb # In the current [[era]]; nowadays.
{ADV, NOUN}English today Adverb # On the current [[day]] or [[date]].
English today Noun # A current day or date.
German achtzig Numeral # [[eighty]] {NUM}
Swedish SCB Acronym # [[statistiska]] ... {NOUN}
Portuguese nessa Contraction # {{contraction ... discard entry
Table 1: Examples of constructing Universal POS tag sets from the Wiktionary.
!"!!#$
%!"!!#$
&!"!!#$
'!"!!#$
(!"!!#$
)!!"!!#$
*+$ *,$ ,-$ ,.$ ,/$ 01$ .-$ 21$ /3$
-45$ 6,*076$ 8098$
Figure 3: Word type coverage by normalized frequency:
words are grouped by word count / highest word count
ratio: low [0, 0.01), medium [0.01, 0.1), high [0.1, 1].
5. Disjoint: W ? T = ?.
In Figure 5, the word types are grouped into the
categories described above. Most of the tag sets
(around 90%) in the Wiktionary are identical to or
supersets of the tree bank tag sets for our nine lan-
guages, which is surprisingly accurate. About 10%
of the Wiktionary tag sets are subsets of, partially
overlapping with, or disjoint from the tree bank tag
sets. Our learning methods, which assume the given
tag sets are correct, may be somewhat hurt by these
word types, as we discuss in Section 5.6.
4 Models
Our basic models are first and second order Hidden
Markov Models (HMM and SHMM). We also used
feature-based max-ent emission models with both
(HMM-ME and SHMM-ME). Below, we denote the
sequence of words in a sentence as boldface x and
the sequence of hidden states which correspond to
part-of-speech tags as boldface y. To simplify nota-
tion, we assume that every tag sequence is prefixed
!"!!#$
%!"!!#$
&!"!!#$
'!"!!#$
(!"!!#$
)!!"!!#$
*$ +$ ,$ -$ .$ /$ 0$ 1$ 2$ 3$ 4$ 5$ 6$ 7$ 8$9+-$ :;<=>?@AB$
Figure 4: PTB vs. Wiktionary type coverage across sec-
tions of the Brown corpus.
with two conventional start tags y0 = start, y?1 =
start, allowing us to write as p(y1|y0, y?1) the ini-
tial state probability of the SHMM.
The probability of a sentence x along with a par-
ticular hidden state sequence y in the SHMM is
given by:
p(x,y) =
length(x)?
i=1
pt(yi | yi?1, yi?2)po(xi | yi),
(1)
where po(xi | yi) is the probability of observ-
ing word xi in state yi (emission probability), and
pt(yi | yi?1, yi?2) is the probability of being in state
yi, given two previous states yi?1, yi?2 (transition
probability).
In this work, we compare multinomial and maxi-
mum entropy (log-linear) emission models. Specifi-
cally, the max-ent emission model is:
po(x|y) =
exp(? ? f(x, y))
?
x? exp(? ? f(x
?, y))
(2)
where f(x, y) is a feature function, x ranges over all
1393
!"#$!"#
%!"#&!"#
'!"#(!"#
)!"#*!"#
+!"#,!"#
$!!"#
-.# -/# /0# /1# /2# 34# 10# 54# 26#
3-/178.0# 295/:2/4# 29;2/4# <6/:0.5# -32=<314#
Figure 5: The Wiktionary vs. tree bank tag sets. Around
90% of the Wiktionary tag sets are identical or subsume
tree bank tag sets. See text for details.
word types, and ? are the model parameters. We use
the following feature templates:
? Word identity - lowercased word form if the
word appears more than 10 times in the corpus.
? Hyphen - word contains a hyphen
? Capital - word is uppercased
? Suffix - last 2 and 3 letters of a word if they
appear in more than 20 different word types.
? Number - word contains a digit
The idea of replacing the multinomial models of an
HMM by maximum entropy models has been ap-
plied before in different domains (Chen, 2003), as
well as in POS induction (Berg-Kirkpatrick et al
2010; Grac?a et al 2011).
We use the EM algorithm to learn the models,
restricting the tags of each word to those specified
by the dictionary. For each tag y, the observa-
tions probabilities po(x | y) were initialized ran-
domly for every word type that allows tag y accord-
ing to the Wiktionary and zero otherwise. For the
M-step in max-ent models, there is no closed form
solution so we need to solve an unconstrained op-
timization problem. We use L-BFGS with Wolfe?s
rule line search (Nocedal and Wright, 1999). We
found that EM achieved higher accuracy across lan-
guages compared to direct gradient approach (Berg-
Kirkpatrick et al 2010).
5 Results
We evaluate the accuracy of taggers trained using
the Wiktionary using the 4 different models: A
first order Hidden Markov Model (HMM), a sec-
ond order Hidden Markov Model (SHMM), a first
order Hidden Markov Model with Maximum En-
tropy emission models (HMM-ME) and a second or-
der Hidden Markov Model with Maximum Entropy
emission models (SHMM-ME). For each model we
ran EM for 50 iterations, which was sufficient for
convergence of the likelihood. Following previous
work (Grac?a et al 2011), we used a Gaussian prior
with variance of 10 for the max-ent model param-
eters. We obtain hard assignments using posterior
decoding, where for each position we pick the la-
bel with highest posterior probability: this produces
small but consistent improvements over Viterbi de-
coding.
5.1 Upper and lower bounds
We situate our results against several upper bounds
that use more supervision. We trained the SHMM-
ME model with a dictionary built from the train-
ing and test tree bank (ALL TBD) and also with
tree bank dictionary intersected with the Wiktionary
(Covered TBD). The Covered TBD dictionary is
more supervised than the Wiktionary in the sense
that some of the tag set mismatches of the Wik-
tionary are cleaned using the true corpus tags. We
also report results from training the SHMM-ME in
the standard supervised fashion, using 50 (50 Sent.),
100 (100 Sent.) and all sentences (All Sent.).
As a lower bound we include the results for un-
supervised systems: a regular HMM model trained
with EM (Johnson, 2007) and an HMM model using
a ME emission model trained using direct gradient
(Berg-Kirkpatrick et al 2010)3.
5.2 Bilingual baselines
Finally, we also compare our system against a strong
set of baselines that use bilingual data. These ap-
proaches build a dictionary by transferring labeled
data from a resource rich language (English) to a re-
source poor language (Das and Petrov, 2011). We
compare against two such methods. The first, pro-
jection, builds a dictionary by transferring the pos
3Values for these systems where taken from the D&P paper.
1394
tags from English to the new language using word
alignments. The second method, D&P, is the cur-
rent state-of-the-art system, and runs label propaga-
tion on the dictionary resulting from the projected
method. We note that both of these approaches are
orthogonal to ours and could be used simultaneously
with the Wiktionary.
5.3 Analysis
Table 2 shows results for the different models across
languages. We note that the results are not di-
rectly comparable since both the Unsupervised and
the Bilingual results use a different setup, using the
number of fine grained tags for each language as hid-
den states instead of 12 (as we do). This greatly in-
creases the degrees of freedom of the model allow-
ing it to capture more fine grained distinctions.
The first two observations are that using the ME
entropy emission model always improves over the
standard multinomial model, and using a second or-
der model always performs better. Comparing with
the work of D&P, we see that our model achieves
better accuracy on average and on 5 out of 8 lan-
guages.
The most common errors are due to tag set id-
iosyncrasies. For instance, for English the symbol %
is tagged as NUM by our system while in the Penn
treebank it is tagged as Noun. Other common mis-
takes for English include tagging to as an adposition
(preposition) instead of particle and tagging which
as a pronoun instead of determiner. In the next sub-
sections we analyze the errors in more detail.
Finally, for English we also trained the SHMM-
ME model using the Celex2 dictionary available
from LDC4. Celex2 coverage for the PTB cor-
pus is much smaller than the coverage provided
by the Wiktionary (43.8% type coverage versus
80.0%). Correspondingly, the accuracy of the model
trained using Celex2 is 75.5% compared 87.1%
when trained using the Wiktionary.
5.4 Performance vs. Wiktionary ambiguity
While many words overwhelmingly appear with one
tag in a given genre, in the Wiktionary a large pro-
portion of words are annotated with several tags,
even when those are extremely rare events. Around
4http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC96L14
35% of word types in English have more than one
tag according to the Wiktionary. This increases the
difficulty of predicting the correct tag as compared
to having a corpus-based dictionary, where words
have a smaller level of ambiguity. For example, in
English, for words with one tag, the accuracy is 95%
(the reason it is not 100% is due to a discrepancy be-
tween the Wiktionary and the tree bank.) For words
with two possible tags, accuracy is 81% and for three
tags, it drops to 63%.
5.5 Generalization to unknown words
Comparing the performance of the proposed model
for words in the Wiktionary against words not in
the Wiktionary, we see an average drop from 89%
to 63% for out-of-vocabulary words across nine lan-
guages. Table 2 shows that the average loss of accu-
racy between All TBD and Covered TBD of 4.5%
(which is due purely to decrease in coverage) is
larger than the loss between Covered TBD and the
best Wiktionary model, of 3.2% (which is due to tag
set inconsistency).
One advantage of the Wiktionary is that it is a gen-
eral purpose dictionary and not tailored for a partic-
ular domain. To illustrate this we compared several
models on the Brown corpus: the SHMM-ME model
using the Wiktionary (Wik), against using a model
trained using a dictionary extracted from the PTB
corpus (PTBD), or trained fully supervised using the
PTB corpus (PTB). We tested all these models on the
15 different sections of the Brown corpus. We also
compare against a state-of-the-art POS-tagger tagger
(ST)5.
Figure 6 shows the accuracy results for each
model on the different sections. The fully super-
vised SHMM-ME model did not perform as well as
the the Stanford tagger (about 3% behind on aver-
age), most likely because of generative vs. discrim-
inate training of the two models and feature differ-
ences. However, quite surprisingly, the Wiktionary-
tag-set-trained model performs much better not only
than the PTB-tag-set-trained model but also the su-
pervised model on the Brown corpus.
5Available at http://nlp.stanford.edu/
software/tagger.shtml
1395
Danish Dutch German Greek English Italian Portuguese Spanish Swedish avg.
Unsupervised
HMM 68.7 57.0 75.9 65.8 63.7 62.9 71.5 68.4 66.7
HMM-ME 69.1 65.1 81.3 71.8 68.1 78.4 80.2 70.1 73.0
Bilingual
Projection 73.6 77.0 83.2 79.3 79.7 82.6 80.1 74.7 78.8
D&P 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4
Wiktionary
HMM 71.8 80.8 77.1 73.1 85.4 84.6 79.1 83.9 76.7 78.4
HMM-ME 82.8 86.1 81.2 80.1 86.1 85.4 83.7 84.6 85.9 83.7
SHMM 74.5 81.6 81.2 73.1 85.0 85.2 79.9 84.5 78.7 79.8
SHMM-ME 83.3 86.3 85.8 79.2 87.1 86.5 84.5 86.4 86.1 84.8
Supervised
Covered TBD 90.1 91.4 89.4 79.7 92.7 86.3 91.5 85.1 91.0 88.6
All TBD 93.6 91.2 95.6 87.9 90.6 92.9 91.2 92.1 83.8 91.0
50 Sent. 65.3 48.5 74.5 74.2 70.2 76.2 79.2 76.2 54.7 68.6
100 Sent. 73.9 52.3 80.9 81.6 77.3 75.3 82.0 80.1 64.8 73.9
All Sent. 93.9 90.9 97.4 95.1 95.8 93.8 95.5 93.8 95.5 94.5
Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan-
guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.
Bilingual systems are trained using a dictionary transferred from English into the target language using word align-
ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model
extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank
information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary
and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner
using increasing numbers of training sentences.
Figure 6: Model accuracy across the Brown cor-
pus sections. ST: Stanford tagger, Wik: Wiktionary-
tag-set-trained SHMM-ME, PTBD: PTB-tag-set-trained
SHMM-ME, PTB: Supervised SHMM-ME. Wik outper-
forms PTB and PTBD overall.
5.6 Error breakdown
In Section 3.4 we discussed the accuracy of the
Wiktionary tag sets and as Table 2 shows, a dictio-
nary with better tag set quality generally (except for
Greek) improves the POS tagging accuracy. In Fig-
ure 7, we group actual errors by the word type clas-
sified into the five cases discussed above: identical,
superset, subset, overlap, disjoint. We also add oov ?
out-of-vocabulary word types. The largest source of
error across languages are out-of-vocabulary (oov)
word types at around 45% of the errors, followed
by tag set mismatch types: subset, overlap, dis-
joint, which together comprise another 50% of the
errors. As Wiktionary grows, these types of errors
will likely diminish.
Figure 7: Tag errors broken down by the word type clas-
sified into the six classes: oov, identical, superset, subset,
overlap, disjoint (see text for detail). The largest source of
error across languages are out-of-vocabulary (oov) word
types, followed by tag set mismatch types: subset, over-
lap, disjoint.
6 Conclusion
We have shown that the Wiktionary can be used
to train a very simple model to achieve state-of-
art weakly-supervised and out-of-domain POS tag-
gers. The methods outlined in the paper are stan-
dard and easy to replicate, yet highly accurate and
should serve as baselines for more complex propos-
1396
als. These encouraging results show that using free,
collaborative NLP resources can in fact produce re-
sults of the same level or better than using expensive
annotations for many languages. Furthermore, the
Wiktionary contains other possibly useful informa-
tion, such as glosses and translations. It would be
very interesting and perhaps necessary to incorpo-
rate this additional data in order to tackle challenges
that arise across a larger number of language types,
specifically non-European languages.
Acknowledgements
We would like to thank Slav Petrov, Kuzman
Ganchev and Andre? Martins for their helpful feed-
back in early versions of the manuscript. We would
also like to thank to our anonymous reviewers for
their comments and suggestions. Ben Taskar was
partially supported by a Sloan Fellowship, ONR
2010 Young Investigator Award and NSF Grant
1116676.
References
A. Abeille?. 2003. Treebanks: Building and Using Parsed
Corpora. Springer.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless unsuper-
vised learning with features. In Proc. NAACL, June.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Conference on Empirical Methods
in Natural Language Processing, Sydney, Australia.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467?479.
S. Buchholz and E. Marsi. 2006. Conll-x shared task
on multilingual dependency parsing. In Proceedings
of the Tenth Conference on Computational Natural
Language Learning, pages 149?164. Association for
Computational Linguistics.
S.F. Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Proc. ECSCT.
P. Chesley, B. Vincent, L. Xu, and R.K. Srihari. 2006.
Using verbs and adjectives to automatically classify
blog sentiment. Training, 580(263):233.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proc. EACL.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based pro-
jections. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 600?609, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Weiwei Ding. 2011. Weakly supervised part-of-speech
tagging for chinese using label propagation. Master?s
thesis, University of Texas at Austin.
Jianfeng Gao and Mark Johnson. 2008. A comparison of
Bayesian estimators for unsupervised hidden Markov
model POS taggers. In In Proc. EMNLP, pages 344?
352, Honolulu, Hawaii, October. ACL.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
In Proc. ACL, volume 45, page 744.
J.V. Grac?a, K. Ganchev, L. Coheur, F. Pereira, and
B. Taskar. 2011. Controlling complexity in part-of-
speech induction. Journal of Artificial Intelligence Re-
search, 41(2):527?551.
J. Grac?a, K. Ganchev, F. Pereira, and B. Taskar. 2009.
Parameter vs. posterior sparisty in latent variable mod-
els. In Proc. NIPS.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Proc. HTL-NAACL. ACL.
M Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In In Proc. EMNLP-CoNLL.
AA Krizhanovsky and F. Lin. 2009. Related
terms search based on wordnet/wiktionary and its
application in ontology matching. Arxiv preprint
arXiv:0907.2209.
Michael Lamar, Yariv Maron, Mark Johnson, and Elie
Bienenstock. 2010. SVD and clustering for unsuper-
vised POS tagging. In Proceedings of the ACL 2010
Conference: Short Papers, pages 215?219, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised POS tagging.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 853?
861, Cambridge, MA, October. Association for Com-
putational Linguistics.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational linguistics,
19(2):313?330.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational linguistics, 20(2):155?
171.
C. Mu?ller and I. Gurevych. 2009. Using wikipedia and
wiktionary in domain-specific information retrieval.
1397
Evaluating Systems for Multilingual and Multimodal
Information Access, pages 219?226.
E. Navarro, F. Sajous, B. Gaume, L. Pre?vot, H. ShuKai,
K. Tzu-Yi, P. Magistry, and H. Chu-Ren. 2009. Wik-
tionary and nlp: Improving synonymy networks. In
Proceedings of the 2009 Workshop on The People?s
Web Meets NLP: Collaboratively Constructed Seman-
tic Resources, pages 19?27. Association for Computa-
tional Linguistics.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The conll 2007 shared
task on dependency parsing. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007.
Association for Computational Linguistics.
J. Nocedal and Stephen J. Wright. 1999. Numerical op-
timization. Springer.
S. Petrov, D. Das, and R. McDonald. 2011. A
universal part-of-speech tagset. Arxiv preprint
ArXiv:1104.2086.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. EMNLP. ACL.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In In Proc.
ACL.
H. Schu?tze. 1995. Distributional part-of-speech tagging.
In Proc. EACL, pages 141?148.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proc. ACL, Prague, Czech Republic, June.
N. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Proc.
ACL. ACL.
B. Snyder, T. Naseem, J. Eisenstein, and R. Barzilay.
2008. Unsupervised multilingual learning for POS
tagging. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1041?1050. Association for Computational Linguis-
tics.
K. Toutanova and M. Johnson. 2007. A Bayesian LDA-
based model for semi-supervised part-of-speech tag-
ging. In Proc. NIPS, 20.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In In Proc. HLT-NAACL.
1398
Learning Tractable Word Alignment Models
with Complex Constraints
Joa?o V. Grac?a?
L2F INESC-ID
Kuzman Ganchev??
University of Pennsylvania
Ben Taskar?
University of Pennsylvania
Word-level alignment of bilingual text is a critical resource for a growing variety of tasks. Proba-
bilistic models for word alignment present a fundamental trade-off between richness of captured
constraints and correlations versus efficiency and tractability of inference. In this article, we
use the Posterior Regularization framework (Grac?a, Ganchev, and Taskar 2007) to incorporate
complex constraints into probabilistic models during learning without changing the efficiency
of the underlying model. We focus on the simple and tractable hidden Markov model, and
present an efficient learning algorithm for incorporating approximate bijectivity and symmetry
constraints. Models estimated with these constraints produce a significant boost in performance
as measured by both precision and recall of manually annotated alignments for six language
pairs. We also report experiments on two different tasks where word alignments are required:
phrase-based machine translation and syntax transfer, and show promising improvements over
standard methods.
1. Introduction
The seminal work of Brown et al (1993b) introduced a series of probabilistic models
(IBM Models 1?5) for statistical machine translation and the concept of ?word-by-
word? alignment, the correspondence between words in source and target languages.
Although no longer competitive as end-to-end translation models, the IBM Models,
as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996),
are still widely used for word alignment. Word alignments are used primarily for
extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn,
Och, and Marcu 2003] and rules [Galley et al 2004; Chiang et al 2005]) as well as for
? INESC-ID Lisboa, Spoken Language Systems Lab, R. Alves Redol 9, 1000-029 LISBOA, Portugal.
E-mail: joao.graca@l2f.inesc-id.pt.
?? University of Pennsylvania, Department of Computer and Information Science, Levine Hall, 3330 Walnut
Street, Philadelphia, PA 19104-6309. E-mail: kuzman@cis.upenn.edu.
? University of Pennsylvania, Department of Computer and Information Science, 3330 Walnut Street,
Philadelphia, PA 19104-6389. E-mail: taskar@cis.upenn.edu.
Submission received: 1 August 2009; revised submission received: 24 December 2009; accepted for
publication: 10 March 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
MT system combination (Matusov, Ueffing, and Ney 2006). But their importance has
grown far beyond machine translation: for instance, transferring annotations between
languages (Yarowsky and Ngai 2001; Hwa et al 2005; Ganchev, Gillenwater, and
Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint
unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).
IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models,
which produce the target sentence one target word at a time by choosing a source word
and generating its translation. IBM Models 3, 4, and 5 attempt to capture fertility (the
tendency of each source word to generate several target words), resulting in probabilis-
tically deficient, intractable models that require local heuristic search and are difficult to
implement and extend. Many researchers use the GIZA++ software package (Och and
Ney 2003) as a black box, selecting IBM Model 4 as a compromise between alignment
quality and efficiency. All of the models are asymmetric (switching target and source
languages produces drastically different results) and the simpler models (IBM Models 1,
2, and HMM) do not enforce bijectivity (the majority of words translating as a single
word). Although there are systematic translation phenomena where one cannot hope to
obtain 1-to-1 alignments, we observe that in over 6 different European language pairs
the majority of alignments are in fact 1-to-1 (86?98%). This leads to the common practice
of post-processing heuristics for intersecting directional alignments to produce nearly
bijective and symmetric results (Koehn, Och, and Marcu 2003).
In this article we focus on the HMM word alignment model (Vogel, Ney, and
Tillmann 1996), using a novel unsupervised learning framework that significantly
boosts its performance. The new training framework, called Posterior Regulariza-
tion (Grac?a, Ganchev, and Taskar 2007), incorporates prior knowledge in the form of
constraints on the model?s posteriors. The constraints are expressed as inequalities on
the expected value under the posterior distribution of user-defined features. Although
the base model remains unchanged, learning guides the model to satisfy these con-
straints. We propose two such constraints: (i) bijectivity: one word should not translate
to many words; and (ii) symmetry: directional alignments should agree. Both of these
constraints significantly improve the performance of the model both in precision and
recall, with the symmetry constraint generally producing more accurate alignments.
Section 3 presents the Posterior Regularization (PR) framework and describes how to
encode such constraints in an efficient manner, requiring only repeated inference in the
original model to enforce the constraints. Section 4 presents a detailed evaluation of
the alignments produced. The constraints over posteriors consistently and significantly
outperform the unconstrained HMM model, evaluated against manual annotations.
Moreover, this training procedure outperforms the more complex IBM Model 4 nine
times out of 12. We examine the influence of constraints on the resulting posterior dis-
tributions and find that they are especially effective for increasing alignment accuracy
for rare words. We also demonstrate a new methodology to avoid overfitting using a
small development corpus. Section 5 evaluates the new framework on two different
tasks that depend on word alignments. Section 5.1 focuses on MT and shows that the
better alignments also lead to better translation systems, adding to similar evidence
presented in Ganchev, Grac?a, and Taskar (2008). Section 5.2 shows that the alignments
we produce are better suited for transfer of syntactic dependency parse annotations.
An implementation of this work (Grac?a, Ganchev, and Taskar 2009) is available under a
GPL license.1
1 www.seas.upenn.edu/?strctlrn/CAT/.
482
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
2. Background
A word alignment for a parallel sentence pair represents the correspondence between
words in a source language and their translations in a target language (Brown et al
1993b). There are many reasons why a simple word-to-word (1-to-1) correspondence
is not possible for every sentence pair: for instance, auxiliary verbs used in one lan-
guage but not the other (e.g., English He walked and French Il est alle?), articles required
in one language but optional in the other (e.g., English Cars use gas and Portuguese
Os carros usam gasolina), cases where the content is expressed using multiple words
in one language and a single word in the other language (e.g., agglutination such as
English weapons of mass destruction and German Massenvernichtungswaffen), and expres-
sions translated indirectly. Due to this inherent ambiguity, manual annotations usually
distinguish between sure correspondences for unambiguous translations, and possible,
for ambiguous translations (Och and Ney 2003). The top row of Figure 1 shows two
word alignments between an English?French sentence pair. We use the following nota-
tion: the alignment on the left (right) will be referenced as source?target (target?source)
and contains source (target) words as rows and target (source) words as columns. Each
entry in the matrix corresponds to a source?target word pair, and is the candidate for an
alignment link. Sure links are represented as squares with borders, and possible links
Figure 1
Posterior marginal distributions for different models for an English to French sentence
translation. Left: EN?FR model. Right: FR? EN model. Top: Regular HMM posteriors.
Middle: After applying bijective constraint. Bottom: After applying symmetric constraint. Sure
alignments are squares with borders; possible alignments are squares without borders. Circle
size indicates probability value. Circle color in the middle and bottom rows indicates differences
in posterior from the top row: green = higher probability; red = lower probability.
483
Computational Linguistics Volume 36, Number 3
Table 1
Test corpora statistics: English?French, English?Spanish, English?Portuguese,
Portuguese?Spanish, Portuguese?French, and Spanish?French.
Corpus Sentence Pairs Ave Length Max Length % Sure % 1-1
En/Fr 447 16/17 30/30 21 98
En/Es 400 29/31 90/99 67 86
En/Pt 60 11/11 20/20 54 91
Pt/Es 60 11/11 20/20 69 92
Pt/Fr 60 11/12 20/20 77 88
Es/Fr 60 11/12 20/20 79 87
are represented as squares without borders. Circles indicate the posterior probability
associated with a given link and will be explained latter.
We use six manually annotated corpora whose characteristics are summarized in
Table 1. The corpora are: the Hansard corpus (Och and Ney 2000) of English/French
Canadian Parliamentary proceedings (En-Fr), and the English/Spanish portion of the
Europarl corpus (Koehn 2005) where the annotation is from EPPS (Lambert et al 2005)
(En-Es) using standard test and development set split. We also used the English/
Portuguese (En-Pt), Portuguese/Spanish (Pt-Es), Portuguese/French (Pt-Fr), and
Spanish/French (Es-Fr) portions of the Europarl corpus using annotations described
by Grac?a et al (2008), where we split the gold alignments into a dev/test set in a ratio of
40%/60%. Table 1 shows some of the variety of challenges presented by each corpus.
For example, En-Es has longer sentences and hence more ambiguity for alignment.
Furthermore, it has a smaller percentage of bijective (1-to-1) alignments, which makes
word fertility more important. Overall, the great majority of links are bijective across
the corpora (86?98%). This characteristic will be explored by the constraints described
in this article. For the evaluations in Section 4, the percentage of sure links (out of all
links) will correlate with difficulty because only sure links are considered for recall.
2.1 HMM Word Alignment Model
In this article we focus on the HMM for word alignment proposed by Vogel, Ney, and
Tillmann (1996). This model generalizes IBM Models 1 and 2 (Brown et al 1993b),
by introducing a first-order Markov dependence between consecutive alignment link
decisions. The model is an (input?output) HMM with I positions whose hidden state
sequence z = (z1, . . . , zI ) with zi ? {null, 1, . . . , J} corresponds to a sequence of source
word positions, where J is the source sentence length, and with null representing un-
aligned target words. Each observation corresponds to a word in the target language xi.
The probability of an alignment z and target sentence x given a source sentence y can
be expressed as:
p?(x, z | y) =
I
?
i=1
pd(zi | zi?1)pt(xi | yzi ) (1)
where pt(xi | yzi ) is the probability of a target word at position i being a translation of the
source word at position zi (translation probability), and pd(zi | zi?1) is the probability
484
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
of translating a word at position zi, given that the previous translated word was at
position zi?1 (distortion probability). Note that this model is directional: Each target
word (observation) can be aligned to at most one source word (hidden state), whereas a
source word could be used multiple times.
We refer to translation parameters pt and distortions parameters pd jointly as ?.
There are several important standard details of the parametrization: The distortion
probability pd(zi | zi?1) depends only on the distance (zi ? zi?1) between the source po-
sitions the states represent. Only distances in the range ?5 are modeled explicitly, with
larger distances assigned equal probabilities. The probability of the initial hidden state,
pd(z1 | z0) is modeled separately from the other distortion probabilities. To incorporate
null links, we add a translation probability given null: pt(xi | ynull). Following standard
practice, null links also maintain position information and do not allow distortion. To
implement this, we create position-specific null hidden states for each source position,
and set pd(nulli|yi? ) = 0 and pd(nulli|nulli? ) = 0 for all i = i?. The model is simple, with
complexity of inference O(I ? J2). There are several problems with the model that arise
from its directionality, however.
 Non-bijective: Multiple target words can be linked to a single source
word. This is rarely desirable. For instance, the model produces
non-bijective links 22% of the time for En-Fr instead of 2%.
 Asymmetric: By switching the (arbitrary) choice of which language is
source and which is target, the HMM produces very different results. For
example, intersecting the sets of alignments produced by the two possible
choices for source preserves less than half of their union for both En-Fr
and En-Pt.2
2.2 Training
Standard HMM training seeks model parameters ? that maximize the log-likelihood of
the parallel corpus:
Log-Likelihood : L(?) = ?E[log p?(x | y)] = ?E[log
?
z
p?(x, z | y)] (2)
where ?E[ f (x, y)] = 1N
?N
n=1 f (x
n, yn) denotes the empirical average of a function f (xn, yn)
over the N pairs of sentences {(x1, y1) . . . , (xN, yN )} in the training corpus. Because
of the latent alignment variables z, the log-likelihood function for the HMM model
is not concave, and the model is fit using the Expectation Maximization (EM) algo-
rithm (Dempster, Laird, and Rubin 1977). EM maximizes L(?) via block-coordinate
ascent on a lower bound F(q, ?) using an auxiliary distribution over the latent variables
q(z | x, y) (Neal and Hinton 1998):
EM Lower Bound : L(?) ? F(q, ?) = ?E
[
?
z
q(z | x, y) log p?(x, z | y)
q(z | x, y)
]
(3)
2 For both of these points, see the experimental setup in Section 4.1.
485
Computational Linguistics Volume 36, Number 3
To simplify notation, we will drop the dependence on y and will write p?(x, z | y) as
p?(x, z), p?(z | x, y) as p?(z | x) and q(z | x, y) as q(z | x). The alternating E and M steps
at iteration t + 1 are given by:
E : qt+1(z | x) = arg max
q(z|x)
F(q, ?t) = arg min
q(z|x)
KL(q(z | x) || p?t (z | x)) = p?t (z | x) (4)
M : ?t+1 = arg max
?
F(qt+1, ?) = arg max
?
?E
[
?
z
qt+1(z | x) log p?(x, z)
]
(5)
where KL(q||p) = Eq[log q(?)p(?) ] is the Kullback-Leibler divergence. The EM algorithm is
guaranteed to converge to a local maximum of L(?) under mild conditions (Neal and
Hinton 1998). The E step computes the posteriors qt+1(z | x) = p?t (z | x) over the latent
variables (alignments) given the observed variables (sentence pair) and current param-
eters ?t, which is accomplished by the forward-backward algorithm for HMMs. The M
step uses qt+1 to ?softly fill in? the values of alignments z and estimate parameters ?t+1.
This step is particularly easy for HMMs, where ?t+1 simply involves normalizing (ex-
pected) counts. This modular split into two intuitive and straightforward steps accounts
for the vast popularity of EM.
In Figure 1, each entry in the alignment matrix contains a circle indicating the align-
ment link posterior for that particular word pair after training an HMM model with the
EM algorithm (see the experimental set up in Section 4.1). Note that the link posteriors
are concentrated around particular source words (rare words occurring less than five
times in the corpus) in both directions, instead of being spread across different words.
This is a well-known problem when training using EM called the ?garbage collector ef-
fect? (Brown et al 1993a). A rare word in the source language links to many words in the
target language that we would ideally like to see unaligned, or aligned to other words
in the sentence. The reason this happens is that the generative model has to distribute
translation probability for each source word among different candidate target words.
If one translation is much more common than another, but the rare translation is used
in the sentence, the model might have a very low translation probability for the correct
alignment. On the other hand, because the rare source word occurs only in a few sen-
tences it needs to spread its probability mass over fewer competing translations. In this
case, choosing to align the rare word to all of these words leads to a higher likelihood
than correctly linking them or linking them to the special null word, because it increases
the likelihood of this sentence without lowering the likelihood of many other sentences.
2.3 Decoding
Alignments are normally predicted using the Viterbi algorithm (which selects the single
most probable path through the HMM?s lattice).
Another possibility that often works better is to use Minimum Bayes-Risk (MBR)
decoding (Kumar and Byrne 2002; Liang, Taskar, and Klein 2006; Grac?a, Ganchev, and
Taskar 2007). Using this decoding we include an alignment link i ? j if the posterior
probability that word i aligns to word j is above some threshold. This allows the
accumulation of probability from several low-scoring alignments that agree on one
alignment link. The threshold is tuned on some small amount of labeled data?in
our case the development set?to minimize some loss. Kumar and Byrne (2002) study
different loss functions that incorporate linguistic knowledge, and show significant
486
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
improvement over likelihood decoding. Note that this could potentially result in an
alignment having zero probability under the model, as many-to-many alignments can
be produced in this way. MBR decoding has several advantages over Viterbi decoding.
First, independently of the particular choice of the loss function, by picking a specific
threshold we can trade off precision and recall of the predicted word alignments. In
fact, in this work when comparing different alignment sets we do not commit to any
loss function but instead compare precision vs recall curves, by generating alignments
for different thresholds (0..1). Second, with this method we can ignore the null word
probabilities, which tend to be poorly estimated.
3. Posterior Regularization
Word alignment models in general and the HMM in particular are very gross over-
simplifications of the translation process and the optimal likelihood parameters learned
often do not correspond to sensible alignments. One solution to this problem is to
add more complexity to the model to better reflect the translation process. This is the
approach taken by IBM Models 4+ (Brown et al 1993b; Och and Ney 2003), and more
recently by the LEAF model (Fraser and Marcu 2007). Unfortunately, these changes
make the models probabilistically deficient and intractable, requiring approximations
and heuristic learning and inference prone to search errors. Instead, we propose to
use a learning framework called Posterior Regularization (Grac?a, Ganchev, and Taskar
2007) that incorporates side information into unsupervised estimation in the form of
constraints on the model?s posteriors. The constraints are expressed as inequalities on
the expected values under the posterior distribution of user-defined constraint features
(not necessarily the same features used by the model). Because in most applications
what we are interested in are the latent variables (in this case the alignments), con-
straining the posteriors allows a more direct way to achieve the desired behavior.
On the other hand, constraining the expected value of the features instead of adding
them to the model allows us to express features that would otherwise make the model
intractable. For example, enforcing that each hidden state of an HMM model should be
used at most once per sentence would break the Markov property and make the model
intractable. In contrast, we will show how to enforce the constraint that each hidden
state is used at most once in expectation. The underlying model remains unchanged,
but the learning method changes. During learning, our method is similar to the EM
algorithm with the addition of solving an optimization problem similar to a maximum
entropy problem inside the E Step. The following subsections present the Posterior
Regularization framework, followed by a description of how to encode two pieces of
prior information aimed at solving the problems described at the end of Section 2.
3.1 Posterior Regularization Framework
The goal of the Posterior Regularization (PR) framework is to guide a model during
learning towards satisfying some prior knowledge about the desired latent variables
(in this case word alignments), encoded as constraints over their expectations. The
key advantage of using regularization on posterior expectations is that the base model
remains unchanged, but during learning, it is driven to obey the constraints by setting
appropriate parameters ?. Moreover, experiments show that enforcing constraints in ex-
pectation results in predicted alignments that also satisfy the constraints. More formally,
posterior information in PR is specified with sets Qx of allowed distributions over the
487
Computational Linguistics Volume 36, Number 3
hidden variables z which satisfy inequality constraints on some user-defined feature
expectations, with violations bounded by  ? 0:
Constrained Posterior Set : Qx = {q(z | x) : ??, Eq[f(x, z)] ? bx ? ?; ||?||22 ? 2} (6)
Qx denotes the set of valid distributions where some feature expectations are bounded
by bx and  ? 0 is an allowable violation slack. Setting  = 0 enforces inequality
constraints strictly. In order to introduce equality constraints, we use two inequality
constraints with opposite signs. We assume that Qx is non-empty for each example x.
Furthermore, the set Qx needs to be convex. In this work we restrict ourselves to
linear inequalities because, as will be shown, subsequently this simplifies the learning
algorithm. Note that Qx, f(x, z), and bx also depend on y, the corresponding source
sentence, but we suppress the dependence for brevity. In PR, the log-likelihood of a
model is penalized with the KL-divergence between the desired distribution space Qx
and the model posteriors, KL(Qx ? p?(z|x)) = min
q(z|x)?Qx
KL(q(z | x) ? p?(z|x)). The regu-
larized objective is:
Posterior Regularized Likelihood : L(?) ? ?E[KL(Qx ? p?(z|x))]. (7)
The objective trades off likelihood and distance to the desired posterior subspace (mod-
ulo getting stuck in local maxima) and provides an effective method of controlling the
posteriors.
Another way of interpreting the objective is to express the marginal log-likelihood
L(?) as a KL distance: KL(?(xn) ? p?(x)) where ?(xn) is a delta function at xn. Hence the
objective is a sum of two average KL terms, one in the space of distributions over x and
one in the space of distributions over z:
?L(?) + ?E[KL(Qx ? p?(z|x))] = 1N
N
?
n=1
KL(?(xn) ? p?(x)) + KL(Qxn ? p?(z|xn)) (8)
This view of the PR objective is illustrated in Figure 2.
Figure 2
Maximizing the PR objective is equivalent to minimizing the empirical average of two
KL divergences: The negative log-likelihood ?L(?) = 1N
?N
n=1 KL(?(x
n) ? p?(x)) plus posterior
regularization 1N
?N
n=1 KL(Qxn ? p?(z|xn)), where ?(xn) is a delta function at xn. The diagram
illustrates the effect of the likelihood term and the regularization term operating over the two
spaces of distributions: the observed variables x and the latent variables z. (The effect of the prior
on ? is not shown.)
488
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
Computing the PR objective involves solving the optimization problem for each x:
Primal Projection : KL(Qx ? p?(z|x)) = min
q(z|x)?Qx
KL(q(z | x) ? p?(z|x)) (9)
Directly minimizing this objective is hard because there is an exponential number of
alignments z; however, the problem becomes easy to solve in its dual formulation (see
Appendix A for derivation):
Dual Projection : arg min
??0
bx ? + log Z(?) +  ||?||2 (10)
where Z(?) =
?
z p?(z|x) exp(?? ? f(x, z)) is the normalization constant and the primal
solution is q(z|x) = p?(z|x) exp{??f(x, z)}/Z(?). There is one dual variable per ex-
pectation constraint, and the dual gradient at ? = 0 is ?(?) = bx ? Eq[f(x, z)] +  ?i||?||2 .
Note that this primal?dual relationship is very similar to the one between maximum
likelihood and maximum entropy. If bx corresponds to empirical expectations and
p?(z|x) is uniform, then Equation (10) would be a log-likelihood and Equation (14) (fol-
lowing) would be a maximum entropy problem. As with maximum entropy, gradient
computation involves computing an expectation under q(z | x), which can be performed
efficiently if the features f(x, z) factor in the same way as the model p?(x, z), and the
constraints are linear. The conditional distribution over z represented by a graphical
model such as HMM can be written as a product of factors over cliques C:
Factored Posterior : p(z | x) = 1Z
?
c?C
?(x, zc) (11)
In an HMM, the cliques C are simply the nodes zi and the edges (zi, zi+1) and the factors
correspond to the distortion and translation probabilities. We will assume f is factorized
as a sum over the same cliques (we will show below how symmetry and bijectivity
constraints can be expressed in this way):
Factored Features : f(x, z) =
?
c?C
f(x, zc) (12)
Then q(z | x) has the same form as p?(z | x):
q(z | x) = 1Zp(z | x) exp(??
f(x, z)) = 1Z
?
c?C
?(x, zc) exp
??f(x,zc ) (13)
Hence the projection step uses the same inference algorithm (forward?backward for
HMMs) to compute the gradient, only modifying the local factors using the current
setting of ?.
?i ? 0;1
while ||?(?)||2 > ? do2
??(x, zc) ? ?(x, zc) exp??
f(x,zc );3
q(z | x) ? forwardBackward(??(x, zc));4
? ? ? + ???(?);5
end6
Algorithm 1: Computing KL(Qx ? p?(z|x)) = min
q?Qx
KL(q(z|x) ? p?(z|x))
489
Computational Linguistics Volume 36, Number 3
We optimize the dual objective using the gradient based methods shown in
Algorithm 1. Here ? is an optimization precision, ? is a step size chosen with the strong
Wolfe?s rule (Nocedal and Wright 1999). Here, ??(?) represents an ascent direction
chosen as follows: For inequality constraints, it is the projected gradient (Bertsekas
1999); for equality constraints with slack, we use conjugate gradient (Nocedal and
Wright 1999), noting that when ? = 0, the objective is not differentiable. In practice
this only happens at the start of optimization and we use a sub-gradient for the first
direction.
Computing the projection requires an algorithm for inference in the original model,
and uses that inference as a subroutine. For HMM word alignments, we need to make
several calls to forward?backward in order to choose ?. Setting the optimization pre-
cision ? more loosely allows the optimization to terminate more quickly but at a less
accurate value. We found that aggressive optimization significantly improves alignment
quality for both constraints we used and consequently choose ? so that tighter values
do not significantly improve performance. This explains why we report better results
here in this paper than in Ganchev, Grac?a, and Taskar (2008), which uses a more naive
optimization (see Section 4.1).
3.2 Posterior Regularization via Expectation Maximization
We can optimize the PR objective using a procedure very similar to the expectation
maximization (EM) algorithm. Recall from Equation (4) that in the E step, q(z | x) is
set to the posterior over hidden variables given the current ?. To converge to the PR
objective, we must modify the E step so that q(z | x) is a projection of the posteriors onto
the constraint set Qx for each example x (Grac?a, Ganchev, and Taskar 2007).
E? : arg min
q,?
KL(q(z|x) ? p?t (z|x)) s.t. Eq[f(x, z)] ? bx ? ?; ||?||22 ? 2 (14)
The new posteriors q(z|x) are used to compute sufficient statistics for this instance and
hence to update the model?s parameters in the M step (Equation (5)), which remains
unchanged. This scheme is illustrated in Figure 3 and in Algorithm 2. The only imple-
mentation difference is that we must now perform the KL projection before collecting
sufficient statistics. We found it can help to also perform this projection at test time,
using q(z | x) = arg min
q(z|x)?Qx
KL(q(z | x)|p?(z | x)) instead of p?(z | x) to decode.
for t = 1..T do1
for each training sentence x do2
E?-Step: qt+1(z | x) = arg min
q(z|x)?Qx
KL(q(z | x)||p?t (z | x))
3
end4
M-Step: ?t+1 = arg max? ?E
[
?
z q
t+1(z | x) log p?(z, x)
]
5
end6
Algorithm 2: PR optimization via modified EM. E?-Step is computed using
Algorithm 1.
490
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
Figure 3
Modified EM for optimizing PR objective L(?) ? ?E[KL(Qx ? p?(z|x))].
3.3 Bijectivity Constraints
We observed in Table 1 that most alignments are 1-to-1 and we would like to introduce
this prior information into the model. Unfortunately including such a constraint in the
model directly breaks the Markov property in a fairly fundamental way. In particular
computing the normalization would require the summation of 1-to-1 or near 1-to-1
weighted matchings, which is a classic #P-complete problem. Introducing alignment
degree constraints in expectation using the PR framework is easy and tractable. We
encode them as the constraint E[f(x, z)] ? 1 where we have one feature f for each source
word j that counts how many times it is aligned to a target word in the alignment z:
Bijective Features : fj(x, z) =
?
i
1(zi = j).
The second row of Figure 1 shows an example of the posteriors after applying bijectivity
constraints; the first row is before the projection. Green (respectively, red) circles indicate
that the probability mass for that particular link increased (respectively, decreased)
when compared with the EM-trained HMM. For example, in the top left panel, the
word schism is used more than once, causing erroneous alignments. Projecting to the
bijectivity constraint set prevents this and most of the mass is (for this example) moved
to the correct word pairs. Enforcing the constraint at training and decoding increases
the fraction of 1-to-1 alignment links from 78% to 97.3% for En-Fr (manual annotations
have 98.1%); for En-Pt the increase is from 84.7% to 95.8% (manual annotations have
90.8%) (see Section 4.1).
3.4 Symmetry Constraints
The directional nature of the generative models used to recover word alignments con-
flicts with their interpretation as translations. In practice, we see that the choice of which
language is source versus target matters and changes the mistakes made by the model
(the first row of panels in Figure 1). The standard approach is to train two models
independently and then intersect their predictions (Och and Ney 2003). However, we
show that it is much better to train two directional models concurrently, coupling
their posterior distributions over alignments to approximately agree. Let the directional
models be defined as: ??p (??z ) (source?target) and ??p (??z ) (target?source). We suppress
dependence on x and y for brevity. Define z to range over the union of all possible
491
Computational Linguistics Volume 36, Number 3
directional alignments
??
Z ???Z . We define a mixture model p(z) = 12
??p (z) + 12
??p (z)
where ??p (??z ) = 0 and vice versa (i.e., the alignment of one directional model has prob-
ability zero according to the other model). We then define the following feature for each
target?source position pair i, j:
Symmetric Features : fij(x, z) =
?
?
?
?
?
+1 z ? ??Z and ??z i = j
?1 z ? ??Z and ??z j = i
0 otherwise
.
If the feature fij has an expected value of zero, then both models predict the i, j link
with equal probability. We therefore impose the constraint Eq[ fij(x, z)] = 0 (possibly with
some small slack). Note that satisfying this implies satisfying the bijectivity constraint
presented earlier. To compute expectations of these features under the model q we only
need to be able to compute them under each directional HMM. To see this, we have by
the definition of q? and p?,
q?(z|x) =
??p (z | x) + ??p (z | x)
2
exp{??f(x, z)}
Z?
=
??q (z|x) Z??q??p (x) +
??q (z|x) Z??q??p (x)
2Z?
(15)
where we have defined:
??q (z|x) = 1Z??q
??p (z, x) exp{??f(x, z)} with Z??q =
?
z
??p (z, x) exp{??f(x, z)}
??q (z|x) = 1Z??q
??p (z, x) exp{??f(x, z)} with Z??q =
?
z
??p (z, x) exp{??f(x, z)}
All these quantities can be computed separately in each model using forward?backward
and, furthermore, Z? = 12 (
Z??q
??p (x) +
Z??q
??p (x) ). The effect of this constraint is illustrated in
the bottom panels of Figure 1. The projected link posteriors are equal for the two
models, and in most cases the probability mass was moved to the correct alignment
links. The exception is the word pair internal/le. In this case, the model chose to incor-
rectly have a high posterior for the alignment link rather than generating internal from
null in one direction and le from null in the other.
We can measure symmetry of predicted alignments as the ratio of the size of the
intersection to the size of the union. Symmetry constraints increase symmetry from 48%
to 89.9% for En-Fr and from 48% to 94.2% for En-Pt (see Section 4.1).
4. Alignment Quality Evaluation
We begin with a comparison of word alignment quality evaluated against manually
annotated alignments as measured by precision and recall. We use the six parallel
corpora with gold annotations described in the beginning of Section 2.
4.1 Experimental Setup
We discarded all training data sentence pairs where one of the sentences contained
more than 40 words. Following common practice, we added the unlabeled development
and test data sets to the pool of unlabeled sentences. We initialized the IBM Model 1
translation table with uniform probabilities over word pairs that occur together in the
same sentence and trained the IBM Model 1 for 5 iterations. All HMM alignment models
492
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
were initialized with the translation table from IBM Model 1 and uniform distortion
probabilities. We run each training procedure until the area under the precision/recall
curve measured on a development corpus stops increasing (see Figure 4 for an example
of such a curve). Using the precision/recall curve gives a broader sense of the model?s
performance than using a single point (by tuning a threshold for a particular metric). In
most cases this meant four iterations for normal EM training and two iterations using
posterior regularization. We suspect that the constraints make the space easier to search.
The convergence criterion for the projection algorithm was the normalized l2 norm
of the gradient (gradient norm divided by number of constraints) being smaller than
? (see Algorithm 1). For bijective constraints, we set ? to 0.005 and used zero slack.
For symmetric constraints, ? and slack were set to 0.001. We chose ? aggressively
and lower values did not significantly increase performance. Less aggressive settings
cause degradation of performance: For example, for En-Fr using 10k sentences, and
running four iterations of constrained EM, the area under the precision/recall curve for
the symmetric model changed from 70% with ? = 0.1 to 85% using ? = 0.001. On the
other hand, the number of iterations required to project the constraints increases for
smaller values of ?. The number of forward?backward calls for normal HMM is 40k
(one for each sentence and EM iteration), for the symmetric model using ? = 0.1 was
around 41k and using ? = 0.001 was around 26M (14 minutes to 4 hours 14 minutes
of training time, 17 times slower, for the different settings of ?). We note that better
optimization methods, such as L-BFGS, or using a warm start for the parameters at each
EM iteration (parameters from the previous iteration), or training the models online,
would potentially decrease the running time of our method.
The intent of this experimental section is to evaluate the gains from using con-
straints during learning, hence the main comparison is between HMM trained with
normal EM vs. trained with PR plus constraints. We also report results for IBM Model 4,
because it is often used as the default word alignment model, and can be used as a
reference. However, we would like to note that IBM Model 4 is a more complex model,
able to capture more structure, albeit at the cost of intractable inference. Because our
approach is orthogonal to the base model used, the constraints described here could
be applied in principle to IBM Model 4 if exact inference was efficient, hopefully
yielding similar improvements. We used a standard implementation of IBM Model
4 (Och and Ney 2003) and because changing the existing code is not trivial, we could
not use the same stopping criterion to avoid overfitting and we are not able to produce
precision/recall curves. We trained IBM Model 4 using the default configuration of the
Figure 4
Precision/Recall curves for different models using 1,000k sentences. Precision on the horizontal
axis. Left: Hansard EN-FR direction. Right: EN-PT Portuguese-English direction.
493
Computational Linguistics Volume 36, Number 3
Figure 5
Word alignment precision when the threshold is chosen to achieve IBM Model 4 recall with a
difference of ? 0.005. The average relative increase in precision (against the HMM model) is
10% for IBM Model 4, 11% for B-HMM, and 14% for S-HMM.
MOSES training script.3 This performs five iterations of IBM Model 1, five iterations of
HMM, and five iterations of IBM Model 4.
4.2 Alignment Results
In this section we present results on alignment quality. All comparisons are made using
MBR decoding because this decoding method always outperforms Viterbi decoding.4
For the models with constraints we project the posteriors at decode time (i.e., we use
q(z | x) to decode). This gives a small but consistent improvement. Figure 4 shows
precision/recall curves for the different models on the En-Fr corpus using English as
the source language (left), and on the En-Pt corpus using Portuguese as the source.
Precision/recall curves are obtained by varying the posterior threshold from 0 to 1 and
then plotting the different precision and recall values obtained.
We observe several trends from Figure 4. First, both types of constraints improve
over the HMM in terms of both precision and recall (their precision/recall curve is
always above). Second, S-HMM performs slightly better than B-HMM. IBM Model 4
is comparable with both constraints (after symmetrization). The results for all language
pairs are in Figure 5. For ease of comparison, we choose a decoding threshold for HMM
models to achieve the recall of the corresponding IBM Model 4 and report precision.
Our methods always improve over the HMM by 10% to 15%, and improve over IBM
Model 4 nine times out of 12. Comparing the constraints with each other we see that
3 www.statmt.org/moses/?n=FactoredTraining.HomePage.
4 IBM Model 4 uses Viterbi decoding as Giza++ does not support MBR decoding.
494
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
Figure 6
Word alignment precision as a function of training data size (number of sentence pairs).
Posterior decoding threshold chosen to achieve IBM Model 4 recall in the Hansard corpus. Right:
English as source. Left: French as source.
S-HMM performs better than B-HMM in 10 out of 12 cases. Because S-HMM indirectly
enforces bijectivity and models sequential correlations on both sides, this is perhaps not
surprising.
Figure 6 shows performance as a function of training data size. As before, we decode
to achieve the recall of IBM Model 4. For small training corpora adding the constraints
provides larger improvements (20?30%) but we still achieve significant gains even with
a million parallel sentences (15%). Greater improvements for small data sizes indicate
that our approach can be especially effective for resource-poor language pairs.
4.3 Rare vs. Common Words
One of the main benefits of using the posterior regularization constraints described is
an alleviation of the garbage collector effect (Brown et al 1993a). Figure 7 breaks down
performance improvements by common versus rare words. As before, we use posterior
decoding, tuning the threshold to match IBM Model 4 recall. For common words, this
tuning maintains recall very close for all models so we do not show this in the figure. In
the top left panel of Figure 7, we see that precision of common words follows the pattern
we saw for the corpus overall: Symmetric and bijective outperform both IBM Model 4
and the baseline HMM, with symmetric slightly better than bijective. The results for
common words vary more slowly as we increase the quantity of training data than they
did for the full corpus. In the top right panel of Figure 7 we show the precision for rare
words. For the baseline HMM as well as for IBM Model 4, this is very low precisely
because of the garbage collector problem: Rare words become erroneously aligned to
untranslated words, leading to low precision. In fact the constrained models achieve
absolute precision improvements of up to 50% over the baseline. By removing these
erroneous alignments the translation table becomes more accurate, allowing higher re-
call on the full corpus. In the bottom panel of Figure 7, we observe a slightly diminished
recall for rare words. This slight drop in recall is due to moving the mass corresponding
to rare words to null.
4.4 Symmetrization
As discussed earlier, the word alignment models are asymmetric, whereas most appli-
cations require a single alignment for each sentence pair. Typically this is achieved by
a symmetrization heuristic that takes two directional alignments and produces a single
495
Computational Linguistics Volume 36, Number 3
Figure 7
Precision and Recall as a function of training data size for En-Fr by common and rare words.
Top Left: Common Precision, Top Right: Rare Precision, Bottom: Rare Recall.
alignment. For MT the most commonly used heuristic is called grow diagonal final
(Och and Ney 2003). This starts with the intersection of the sets of aligned points and
adds points around the diagonal that are in the union of the two sets of aligned points.
The alignment produced has high recall relative to the intersection and only slightly
lower recall than the union. In syntax transfer the intersection heuristic is normally
used, because one wants to have high precision links to transfer knowledge between
languages. One pitfall of these symmetrization heuristics is that they can obfuscate the
link between the original alignment and the ones used for a specific task, making errors
more difficult to analyze. Because they are heuristics tuned for a particular phrase-
based translation system, it is not clear when they will help and when they will hinder
system performance. In this work we followed a more principled approach that uses
Figure 8
Precision/recall curves for the different models after soft union symmetrization. Precision is on
the horizontal axis. Left EN-FR, Right PT-ES.
496
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
the knowledge about the posterior distributions of each directional model. We include a
point in the final alignment if the average of the posteriors under the two models for that
point is above a threshold. This heuristic is called soft union (DeNero and Klein 2007).
Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.
The posterior regularization?trained models still performed better, but the differences
get smaller after doing the symmetrization. This should not be very surprising, because
the soft union symmetrization can be viewed as an approximation of our symmetry
constraint applied only at decode time. Applying the symmetrization to the model with
symmetry constraints does not affect performance.
4.5 Analysis
In this section we discuss some scenarios in which the constraints make the alignments
better, and some scenarios where they fail. We have already discussed the garbage
collector effect and how both models address it. Both of the constraints also bias the
model to have at most probability one in any row or column of the posterior matrix,
encouraging 1-to-1 alignments. Obviously whenever alignments are systematically not
1-to-1 , this can lead to errors (for instance the examples described in Section 2).
An example presented in Figure 9 shows the posterior marginal distributions for an
English/French sentence pair using the same notation as in Figure 1. In the top panel of
Figure 9
Posterior distributions for different models for an English to French sentence translation. Left:
EN?FR model. Right: FR? EN model. Top: Regular HMM posteriors. Middle: After applying
the bijective constraint. Bottom: After applying the symmetric constraint. Sure alignments are
squares with borders; possible alignments are squares without borders. Circle size indicates
probability value. Circle color in the middle and bottom rows indicates differences in posterior
from the top row; green = higher probability; red = lower probability.
497
Computational Linguistics Volume 36, Number 3
Figure 9, we see the baseline models, where the English word met is incorrectly being
aligned to se?ance est ouverte. This makes it impossible to recover the correct alignment
house/se?ance. Either constraint corrects this problem. On the other hand, by enforcing
a 1-to-1 mapping the correct alignment met / est ouverte is lost. Going back to the first
row (regular HMM) this alignment is correct in one direction and absent in the other
(due to the many-to-1 model restriction) but we can recover that information using the
symmetrization heuristics, since the point is present at least in one direction with high
probability mass. This is not the case for the constraint-based models that reduce the
mass of that alignment in both directions. Going back to the right panel of Figure 8, we
can see that for low values of precision the HMM model actually achieves better recall
than the constraint-based methods. There are two possible solutions to alleviate this
type of problem, both with their caveats. One solution is to model the fertility of each
word in a way similar to IBM Model 4, or more generally to model alignments of multi-
ple words. This can lead to significant computational burden, and is not guaranteed to
improve results. A more complicated model may require approximations that destroy
its performance gain, or require larger corpora to estimate its parameters. Another
option is to perform some linguistically motivated pre-processing of the language pair
to conjoin words. This of course has the disadvantage that it needs to be specific to a
language pair in order to include information such as ?English simple past is written
using a single word, so join together French passe? compose?.? An additional problem
with joining words to alleviate inter-language divergences is that it can increase data
sparsity.
5. Task-Specific Alignment Evaluation
In this section we evaluate the alignments resulting from using the proposed constraints
in two different tasks: Statistical machine translation where alignments are used to
restrict the number of possible minimal translation units; and syntax transfer, where
alignments are used to decide how to transfer dependency links.
5.1 Phrase-Based Machine Translation
We now investigate whether our alignments produce improvements in an end-to-end
phrase-based machine translation system. We use a state-of-the-art machine translation
system,5 and follow the experimental setup used for the 2008 shared task on machine
translation (ACL 2008 Third Workshop on Statistical Machine Translation). The full
pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and
filter long sentences); (2) build language models; (3) create word alignments in each
direction; (4) symmetrize directional word alignments; (5) build phrase table; (6) tune
weights for the phrase table. For more details consult the shared task description.6 To
evaluate the quality of the produced alignments, we keep the pipeline unchanged, and
use the models described earlier to generate the word alignments in Step 3. For Step 4,
we use the soft union symmetrization heuristic. Symmetrization has almost no effect on
alignments produced by S-HMM, but we use it for uniformity in the experiments. We
tested three values of the threshold (0.2, 0.4, 0.6) which try to capture different tradeoffs
5 The open source Moses (Hoang et al 2007) toolkit from www.statmt.org/moses/.
6 www.statmt.org/wmt08/baseline.html.
498
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
Table 2
BLEU scores for all language pairs. The best threshold was selected according to the
development set after the last MERT iteration. Bold denotes the best score.
Fr ? En En ? Fr Es ? En En ? Es Pt ? En En ? Pt
IBM M4 GDF 35.7 31.2 32.4 31.6 31.4 28.9
HMM SU 35.9 28.9 32.3 31.6 30.9 31.6
B-HMM SU 36.0 31.5 32.6 31.7 31.0 32.2
S-HMM SU 35.5 31.2 31.9 32.5 31.4 32.3
of precision vs. recall, and pick the best according to the translation performance on
development data. Table 2 summarizes the results for the different corpora. For refer-
ence we include IBM Model 4 as suggested in the task description. PR training always
outperforms EM training and outperforms IBM Model 4 in all but one experiment.
Differences in BLEU range from 0.2 to 0.9. The two constraints help to a different extent
for different corpora and translation directions, in a somewhat unpredictable manner.
In general our impression is that the connection between alignment quality and BLEU
scores is complicated, and changes are difficult to explain and justify. The number of
iterations for MERT optimization to converge varied from 2 to 28; and the best choice of
threshold on the development set did not always correspond to the best on the test set.
Contrary to conventional wisdom in the MT community, bigger phrase tables did not
always perform better. In 14 out of 18 cases, the threshold picked was 0.4 (medium size
phrase tables) and the other four times 0.2 was picked (smaller phrase tables). When
we include only high confidence alignments, more phrases are extracted but many of
these are erroneous. Potentially this leads to a poor estimate of the phrase probabilities.
See Lopez and Resnik (2006) for further discussion.
5.2 Syntax Transfer
In this section, we compare the different alignments produced with and without PR
based on how well they can be used for transfer of linguistic resources across languages.
We used the system proposed by Ganchev, Gillenwater, and Taskar (2009). This system
uses a word-aligned corpus and a parser for a resource-rich language (source language)
in order to create a parser for a resource-poor language (target language). We consider
a parse tree on the source language as a set of dependency edges to be transferred. For
each such edge, if both end points are aligned to words in the target language, then
the edge is transferred. These edges are then used as weak supervision when training
a generative or discriminative dependency parser. In order to evaluate the alignments
we computed the fraction of correctly transferred edges as a function of the average
number of edges transferred by using supervised parse trees on the target side. By
changing the threshold in MBR decoding of alignments, we can trade off accuracy of the
transferred edges vs. transferring more edges. We generated supervised parses using
the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005)
trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and
Spanish. Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links
between words with incompatible POS tags. Figure 10 shows our results for transferring
from English to Bulgarian (En?Bg) and from English to Spanish (En?Es). The En?Bg
499
Computational Linguistics Volume 36, Number 3
Figure 10
Edge conservation for cross-lingual grammar induction. Left: En?Bg subtitle corpus; Right:
En?Es parliamentary proceedings. Vertical axis: percentage of transferred edges that are correct.
Horizontal axis: average number of transferred edges per sentence.
results are based on a corpus of movie subtitles (Tiedemann 2007), and are consequently
shorter sentences, whereas the En?Es results are based on a corpus of parliamentary
proceedings (Koehn 2005). We see in Figure 10 that for both domains, the models trained
using posterior regularization perform better than the baseline model trained using EM.
6. Related Work
The idea of introducing constraints over a model to better guide the learning process
has appeared before. In the context of word alignment, Deng and Byrne (2005) use a
state-duration HMM in order to model word-to-phrase translations. The fertility of each
source word is implicitly encoded in the durations of the HMM states. Without any
restrictions, likelihood prefers to always use longer phrases and the authors try to con-
trol this behavior by multiplying every transition probability by a constant ? > 1. This
encourages more transitions and hence shorter phrases. For the task of unsupervised
dependency parsing, Smith and Eisner (2006) add a constraint of the form ?the average
length of dependencies should be X? to capture the locality of syntax (at least half
of the dependencies are between adjacent words), using a scheme they call structural
annealing. They modify the model?s distribution over trees p?(y) by a penalty term
as: p
?
?(y) ? p?(y)e
(?
?
e?y length(e)), where length(e) is the surface length of edge e. The
factor ? changes from a high value to a lower one so that the preference for short edges
(hence a smaller sum) is stronger at the start of training.
These two approaches also have the goal of controlling unsupervised learning, and
the form of the modified distributions is reminiscent of the form that the projected
posteriors take. However, the approaches differ substantially from PR. Smith and Eisner
(2006) make a statement of the form ?scale the total length of edges?, which depending
on the value of ? will prefer to have more shorter/longer edges. Such statements are
not data dependent. Depending on the value of ?, for instance if ? ? 0, even if the data
is such that the model already uses too many short edges on average, this value of
? will push for more short edges. By contrast the statements we can make in PR are
of the form ?there should be more short edges than long edges?. Such a statement is
data-dependent in the sense that if the model satisfies the constraints then we do not
need to change it; if it is far from satisfying it we might need to make very dramatic
changes.
500
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
PR is closely related to the work of Mann and McCallum (2007, 2008), who concur-
rently developed the idea of using penalties based on posterior expectations of features
to guide semi-supervised learning. They call their method generalized expectation (GE)
constraints or alternatively expectation regularization. In the original GE framework,
the posteriors of the model on unlabeled data are regularized directly. They train a
discriminative model, using conditional likelihood on labeled data and an ?expectation
regularization? penalty term on the unlabeled data:
arg max
?
Llabeled(?) ? ??E[||Ep? [f(x, z) ? b||
2
2]. (16)
Notice that there is no intermediate distribution q. For some kinds of constraints this
objective is difficult to optimize in ? and in order to improve efficiency, Bellare, Druck,
and McCallum (2009) propose interpreting the PR framework as an approximation
to the GE objective in Equation (16). They compare the two frameworks on several
data sets and find that performance is similar. Liang, Jordan, and Klein (2009) cast
the problem of incorporating partial information about latent variables into a Bayesian
framework using ?measurements,? and after several approximation steps, they arrive
at the objective we optimize.
The idea of jointly training two directional models has been explored by Liang,
Taskar, and Klein (2006), although under a very different formalization. They de-
fine a joint objective max
?1,?2
?E
[
log??p ?1 (x) + log
??p ?2 (x) + log
?
z
??p ?1 (z | x)
??p ?2 (z | x)
]
. However, the
product distribution ??p ?1 (z | x)
??p ?2 (z | x) ranges over all one-to-one alignments and
computing it is #P-complete (Liang, Taskar, and Klein 2006). They approximate this
distribution as a product of marginals: q(z) =
?
i,j
??p ?1 (zi,j | x)
??p ?2 (zi,j | x), but it is not
clear what objective the approximate procedure actually optimizes.
7. Conclusion
In this article we explored a novel learning framework, Posterior Regularization, for
incorporating rich constraints over the posterior distributions of word alignments. We
focused on the HMM word alignment model, and showed how we could incorpo-
rate complex constraints like bijectivity and symmetry while keeping the inference
in the model tractable. Using these constraints we showed consistent and significant
improvements in six different language pairs even when compared to a more complex
model such as IBM Model 4. In addition to alleviating the ?garbage collector? effect, we
show that the obtained posterior distributions better reflect the desired alignments. Both
constraints are biasing the models towards 1-to-1 alignments, which may be inappro-
priate in some situations, and we show some systematic mistakes that the constraints
introduce and suggest possible fixes.
We experimented with two different tasks that rely on word alignments, phrase-
based MT and syntax transfer. For phrase-based MT, the improved alignments lead
to a modest increase in BLEU performance. For syntax transfer, we have shown that
the number of edges of a dependency tree that can be accurately transferred from one
language to another increases as a result of improved alignments.
Our framework opens up the possibility of efficiently adding many other con-
straints that are directly applicable to word alignments, such as preferring alignments
that respect dependency tree structure, part of speech tags, or syntactic boundaries.
501
Computational Linguistics Volume 36, Number 3
Appendix A: Modified E-Step Dual Derivation
The modified E step involves a projection step that minimizes the Kullback-Leibler
divergence:
E? : arg min
q(z|x),?
KL( q(z|x) ? p?(z|x)) s.t. Eq[f(x, z)] ? bx ? ?; ||?||22 ? 2.
Assuming the set Qx = { q(z|x) : ??, Eq[f(x, z)] ? bx ? ?; ||?||22 ? 2} is non-empty, the
corresponding Lagrangian is max
?,?,?
min
q(z|x),?
L( q(z|x), ?, ?, ?, ?) with ? ? 0 and ? ? 0,
where
L( q(z|x), ?, ?, ?, ?) = KL( q(z|x) ? p?(z|x)) + ?(Eq[f(x, z)] ? bx ? ?)
+ ?(||?||22 ? 2) + ?(
?
z
q(z|x) ? 1)
?L( q(z|x), ?, ?, ?, ?)
? q(z|x) = log( q(z|x)) + 1 ? log( p?(z|x)) + ?
f(x, z) + ? = 0
=? q(z|x) = p?(z|x) exp(??
f(x, z))
e exp(?)
?L( q(z|x), ?, ?, ?, ?)
??i
= 2??i ? ?i = 0 =? ?i =
?i
2?
Plugging q(z|x) and ? in L( q(z|x), ?, ?, ?, ?) and taking the derivative with respect to ?:
?L(?, ?, ?)
?? =
?
z
p?(z|x) exp(??f(x, z))
e exp(?)
? 1 = 0 =? ? = log(
?
z p?(z|x) exp(??f(x, z))
e )
Simplifying q(z|x) = p?(z|x) exp(??
f(x,z))
Z?
where Z? =
?
z p?(z|x) exp(??f(x, z)) en-
sures that q(z|x) is properly normalized. Plugging ? into L(?, ?, ?) and taking the
derivative with respect to ?, we get:
L(?, ?) = ? log(Z?) ? bx ??
||?||22
2? +
||?||22
4? ? ?
2 (A.1)
?L(?, ?)
?? =
||?||22
2?2
? ||?||
2
2
4?2
? 2 = 0 =? ? = ||?||22 (A.2)
Replacing back into L(?, ?) we get the dual objective:
Dual E? : arg max
??0
?bx ?? log(Z?) ? ||?||2  (A.3)
Acknowledgments
J. V. Grac?a was supported by a fellowship
from Fundac?a?o para a Cie?ncia e Tecnologia
(SFRH/ BD/ 27528/ 2006) and by FCT
project CMU-PT/HuMach/0039/2008.
K. Ganchev was partially supported by
NSF ITR EIA 0205448. Ben Taskar was
partially supported by DARPA CSSG
2009 grant.
References
Bannard, Colin and Chris Callison-Burch.
2005. Paraphrasing with bilingual parallel
corpora. In ACL ?05: Proceedings of the
43rd Annual Meeting of the Association for
Computational Linguistics, pages 597?604,
Morristown, NJ.
Bellare, Kedar, Gregory Druck, and Andrew
McCallum. 2009. Alternating projections
502
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
for learning with expectation constraints.
In Proceedings of the Twenty-Fifth Conference
Annual Conference on Uncertainty in
Artificial Intelligence, pages 43?50,
Corvallis, OR.
Bertsekas, Dimitri P. 1999. Nonlinear
Programming: 2nd Edition. Athena
Scientific, Nashua, NH.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, Meredith J.
Goldsmith, Jan Hajic, Robert L. Mercer,
and Surya Mohanty. 1993a. But
dictionaries are data too. In HLT ?93:
Proceedings of the Workshop on Human
Language Technology, pages 202?205,
Morristown, NJ.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993b. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Chiang, David, Adam Lopez, Nitin
Madnani, Christof Monz, Philip Resnik,
and Michael Subotin. 2005. The Hiero
machine translation system: Extensions,
evaluation, and analysis. In Proceedings
of the Human Language Technology
Conference and Conference on Empirical
Methods in Natural Language Processing,
pages 779?786, Vancouver.
Dempster, Arthur P., Nan M. Laird, and
Donald B. Rubin. 1977. Maximum
likelihood from incomplete data via the
em algorithm. Royal Statistical Society,
Series B, 39(1):1?38.
DeNero, John and Dan Klein. 2007. Tailoring
word alignments to syntactic machine
translation. In Proceedings of the 45th
Annual Meeting of the Association of
Computational Linguistics, pages 17?24,
Prague.
Deng, Yonggang and William Byrne. 2005.
HMM word and phrase alignment for
statistical machine translation. In HLT ?05:
Proceedings of the Conference on Human
Language Technology and Empirical
Methods in Natural Language Processing,
pages 169?176, Morristown, NJ.
Association for Computational Linguistics.
Fraser, Alexander and Daniel Marcu. 2007.
Getting the structure right for word
alignment: Leaf. In Proceedings of the Joint
Conference on Empirical Methods in Natural
Language Processing and Computational
Natural Language Learning
(EMNLP-CoNLL), pages 51?60, Prague.
Galley, Michel, Mark Hopkins, Kevin Knight,
and Daniel Marcu. 2004. What?s in a
translation rule? In HLT-NAACL 2004:
Main Proceedings, pages 273?280,
Boston, MA.
Ganchev, Kuzman, Jennifer Gillenwater, and
Ben Taskar. 2009. Dependency grammar
induction via bitext projection constraints.
In ACL-IJCNLP ?09: Proceedings of the Joint
Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint
Conference on Natural Language Processing
of the AFNLP: Volume 1, pages 369?377,
Morristown, NJ.
Ganchev, Kuzman, Joa?o V. Grac?a, and Ben
Taskar. 2008. Better alignments = better
translations? In Proceedings of ACL-08: HLT,
pages 986?993, Columbus, OH.
Grac?a, Joa?o V., Kuzman Ganchev, and Ben
Taskar. 2007. Expectation maximization
and posterior constraints. In J. C. Platt,
D. Koller, Y. Singer, and S. Roweis, editors,
Advances in Neural Information Processing
Systems 20. MIT Press, Cambridge, MA,
pages 569?576.
Grac?a, Joa?o V., Kuzman Ganchev, and
Ben Taskar. 2009. Postcat - posterior
constrained alignment toolkit. The Prague
Bulletin Of Mathematical Linguistics - Special
Issue: Open Source Tools for Machine
Translation, 91:27?37.
Grac?a, Joa?o V., Joana P. Pardal, Lu??sa Coheur,
and Diamantino Caseiro. 2008. Building
a golden collection of parallel
multi-language word alignment. In
Proceedings of the Sixth International
Language Resources and Evaluation
(LREC?08), Marrakech.
Hoang, Hieu, Alexandra Birch, Chris
Callison-Burch, Richard Zens, Rwth
Aachen, Alexandra Constantin, Marcello
Federico, Nicola Bertoldi, Chris Dyer,
Brooke Cowan, Wade Shen, Christine
Moran, and Ondrej Bojar. 2007. Moses:
Open source toolkit for statistical machine
translation. In Proceedings of the 45th
Annual Meeting of the Association for
Computational Linguistics Companion
Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague.
Hwa, Rebecca, Philip Resnik, Amy
Weinberg, Clara Cabezas, and Okan Kolak.
2005. Bootstrapping parsers via syntactic
projection across parallel texts. Natural
Language Engineering, 11:11?311.
Koehn, Philipp. 2005. Europarl: A parallel
corpus for statistical machine translation.
In Machine Translation Summit,
12?15 September, Phuket.
Koehn, Philipp, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based
503
Computational Linguistics Volume 36, Number 3
translation. In Proceedings of the 2003
Conference of the North American Chapter of
the Association for Computational Linguistics
on Human Language Technology (NAACL),
pages 48?54, Morristown, NJ.
Kumar, Shankar and William Byrne. 2002.
Minimum Bayes-Risk word alignments of
bilingual texts. In Proceedings of the ACL-02
Conference on Empirical Methods in Natural
Language Processing, pages 140?147,
Philadelphia, PA.
Lambert, Patrik, Adria` De Gispert, Rafael
Banchs, and Jose? B. Marino. 2005.
Guidelines for word alignment evaluation
and manual alignment. Language Resources
and Evaluation, 39(4):267?285.
Liang, Percy, Michael I. Jordan, and Dan
Klein. 2009. Learning from measurements
in exponential families. In ICML ?09:
Proceedings of the 26th Annual International
Conference on Machine Learning,
pages 641?648, New York, NY.
Liang, Percy, Ben Taskar, and Dan Klein.
2006. Alignment by agreement. In
Proceedings of the Human Language
Technology Conference of the NAACL, Main
Conference, pages 104?111, New York, NY.
Lopez, Adam and Philip Resnik. 2006.
Word-based alignment, phrase-based
translation: Whats the link? In Proceedings
of the 7th Conference of the Association for
Machine Translation in the Americas
(AMTA): Visions for the Future of Machine
Translation, pages 90?99, Boston, MA.
Mann, G. and A. McCallum. 2007. Simple,
robust, scalable semi-supervised learning
via expectation regularization. In
Proceedings of the 24th International
Conference on Machine Learning, page 600,
Corvallis, OR.
Mann, Gideon S. and Andrew McCallum.
2008. Generalized expectation criteria
for semi-supervised learning of
conditional random fields. In Proceedings
of ACL-08: HLT, pages 870?878,
Columbus, OH.
Matusov, Evgeny, Nicola Ueffing, and
Hermann Ney. 2006. Computing
consensus translation from multiple
machine translation systems using
enhanced hypotheses alignment. In
Proceedings of the EACL, pages 33?40,
Cambridge.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online
large-margin training of dependency
parsers. In ACL ?05: Proceedings of the
43rd Annual Meeting of the Association for
Computational Linguistics, pages 91?98,
Morristown, NJ.
Neal, Radford M. and Geoffrey E. Hinton.
1998. A new view of the EM algorithm that
justifies incremental, sparse and other
variants. In M. I. Jordan, editor, Learning in
Graphical Models. Kluwer, Amsterdam,
pages 355?368.
Nocedal, Jorge and Stephen J. Wright.
1999. Numerical Optimization. Springer,
Berlin.
Och, Franz Josef and Hermann Ney. 2000.
Improved statistical alignment models.
In ACL ?00: Proceedings of the 38th
Annual Meeting on Association for
Computational Linguistics, pages 440?447,
Morristown, NJ.
Och, Franz Josef and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?51.
Smith, Noah A. and Jason Eisner. 2006.
Annealing structural bias in multilingual
weighted grammar induction. In ACL-44:
Proceedings of the 21st International
Conference on Computational Linguistics and
the 44th Annual Meeting of the Association for
Computational Linguistics, pages 569?576,
Morristown, NJ.
Snyder, Benjamin and Regina Barzilay.
2008. Unsupervised multilingual learning
for morphological segmentation. In
Proceedings of ACL-08: HLT, pages 737?745,
Columbus, OH.
Tiedemann, Jo?rg. 2007. Building a
multilingual parallel subtitle corpus. In
Proceedings of the 17th Conference on
Computational Linguistics in the Netherlands
(CLIN 17), Leuven.
Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. Hmm-based
word alignment in statistical translation.
In Proceedings of the 16th Conference on
Computational Linguistics, pages 836?841,
Morristown, NJ.
Yarowsky, David and Grace Ngai. 2001.
Inducing multilingual POS taggers and NP
bracketers via robust projection across
aligned corpora. In Proceedings of the North
American Chapter Of The Association For
Computational Linguistics, pages 1?8,
Morristown, NJ.
504
Proceedings of the ACL 2010 Conference Short Papers, pages 194?199,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Sparsity in Dependency Grammar Induction
Jennifer Gillenwater and Kuzman Ganchev
University of Pennsylvania
Philadelphia, PA, USA
{jengi,kuzman}@cis.upenn.edu
Jo?o Gra?a
L2F INESC-ID
Lisboa, Portugal
joao.graca@l2f.inesc-id.pt
Fernando Pereira
Google Inc.
Mountain View, CA, USA
pereira@google.com
Ben Taskar
University of Pennsylvania
Philadelphia, PA, USA
taskar@cis.upenn.edu
Abstract
A strong inductive bias is essential in un-
supervised grammar induction. We ex-
plore a particular sparsity bias in de-
pendency grammars that encourages a
small number of unique dependency
types. Specifically, we investigate
sparsity-inducing penalties on the poste-
rior distributions of parent-child POS tag
pairs in the posterior regularization (PR)
framework of Gra?a et al (2007). In ex-
periments with 12 languages, we achieve
substantial gains over the standard expec-
tation maximization (EM) baseline, with
average improvement in attachment ac-
curacy of 6.3%. Further, our method
outperforms models based on a standard
Bayesian sparsity-inducing prior by an av-
erage of 4.9%. On English in particular,
we show that our approach improves on
several other state-of-the-art techniques.
1 Introduction
We investigate an unsupervised learning method
for dependency parsing models that imposes spar-
sity biases on the dependency types. We assume
a corpus annotated with POS tags, where the task
is to induce a dependency model from the tags for
corpus sentences. In this setting, the type of a de-
pendency is defined as a pair: tag of the dependent
(also known as the child), and tag of the head (also
known as the parent). Given that POS tags are de-
signed to convey information about grammatical
relations, it is reasonable to assume that only some
of the possible dependency types will be realized
for a given language. For instance, in English it
is ungrammatical for nouns to dominate verbs, ad-
jectives to dominate adverbs, and determiners to
dominate almost any part of speech. Thus, the re-
alized dependency types should be a sparse subset
of all possible types.
Previous work in unsupervised grammar induc-
tion has tried to achieve sparsity through priors.
Liang et al (2007), Finkel et al (2007) and John-
son et al (2007) proposed hierarchical Dirichlet
process priors. Cohen et al (2008) experimented
with a discounting Dirichlet prior, which encour-
ages a standard dependency parsing model (see
Section 2) to limit the number of dependent types
for each head type.
Our experiments show a more effective sparsity
pattern is one that limits the total number of unique
head-dependent tag pairs. This kind of sparsity
bias avoids inducing competition between depen-
dent types for each head type. We can achieve the
desired bias with a constraint on model posteri-
ors during learning, using the posterior regulariza-
tion (PR) framework (Gra?a et al, 2007). Specifi-
cally, to implement PR we augment the maximum
marginal likelihood objective of the dependency
model with a term that penalizes head-dependent
tag distributions that are too permissive.
Although not focused on sparsity, several other
studies use soft parameter sharing to couple dif-
ferent types of dependencies. To this end, Cohen
et al (2008) and Cohen and Smith (2009) inves-
tigated logistic normal priors, and Headden III et
al. (2009) used a backoff scheme. We compare to
their results in Section 5.
The remainder of this paper is organized as fol-
194
lows. Section 2 and 3 review the models and sev-
eral previous approaches for learning them. Sec-
tion 4 describes learning with PR. Section 5 de-
scribes experiments across 12 languages and Sec-
tion 6 analyzes the results. For additional details
on this work see Gillenwater et al (2010).
2 Parsing Model
The models we use are based on the generative de-
pendency model with valence (DMV) (Klein and
Manning, 2004). For a sentence with tags x, the
root POS r(x) is generated first. Then the model
decides whether to generate a right dependent con-
ditioned on the POS of the root and whether other
right dependents have already been generated for
this head. Upon deciding to generate a right de-
pendent, the POS of the dependent is selected by
conditioning on the head POS and the direction-
ality. After stopping on the right, the root gener-
ates left dependents using the mirror reversal of
this process. Once the root has generated all its
dependents, the dependents generate their own de-
pendents in the same manner.
2.1 Model Extensions
For better comparison with previous work we
implemented three model extensions, borrowed
from Headden III et al (2009). The first exten-
sion alters the stopping probability by condition-
ing it not only on whether there are any depen-
dents in a particular direction already, but also on
how many such dependents there are. When we
talk about models with maximum stop valency Vs
= S, this means it distinguishes S different cases:
0, 1, . . . , S?2, and? S?1 dependents in a given
direction. The basic DMV has Vs = 2.
The second model extension we implement is
analogous to the first, but applies to dependent tag
probabilities instead of stop probabilities. Again,
we expand the conditioning such that the model
considers how many other dependents were al-
ready generated in the same direction. When we
talk about a model with maximum child valency
Vc = C, this means we distinguish C different
cases. The basic DMV has Vc = 1. Since this
extension to the dependent probabilities dramati-
cally increases model complexity, the third model
extension we implement is to add a backoff for the
dependent probabilities that does not condition on
the identity of the parent POS (see Equation 2).
More formally, under the extended DMV the
probability of a sentence with POS tags x and de-
pendency tree y is given by:
p?(x,y) = proot(r(x))?
Y
y?y
pstop(false | yp, yd, yvs)pchild(yc | yp, yd, yvc)?
Y
x?x
pstop(true | x, left, xvl) pstop(true | x, right, xvr )
(1)
where y is the dependency of yc on head yp in di-
rection yd, and yvc , yvs , xvr , and xvl indicate va-
lence. For the third model extension, the backoff
to a probability not dependent on parent POS can
be formally expressed as:
?pchild(yc | yp, yd, yvc) + (1? ?)pchild(yc | yd, yvc) (2)
for ? ? [0, 1]. We fix ? = 1/3, which is a crude
approximation to the value learned by Headden III
et al (2009).
3 Previous Learning Approaches
In our experiments, we compare PR learning
to standard expectation maximization (EM) and
to Bayesian learning with a sparsity-inducing
prior. The EM algorithm optimizes marginal like-
lihood L(?) = log
?
Y p?(X,Y), where X =
{x1, . . . ,xn} denotes the entire unlabeled corpus
and Y = {y1, . . . ,yn} denotes a set of corre-
sponding parses for each sentence. Neal and Hin-
ton (1998) view EM as block coordinate ascent on
a function that lower-bounds L(?). Starting from
an initial parameter estimate ?0, the algorithm it-
erates two steps:
E : qt+1 = argmin
q
KL(q(Y) ? p?t(Y | X)) (3)
M : ?t+1 = argmax
?
Eqt+1 [log p?(X,Y)] (4)
Note that the E-step just sets qt+1(Y) =
p?t(Y|X), since it is an unconstrained minimiza-
tion of a KL-divergence. The PR method we
present modifies the E-step by adding constraints.
Besides EM, we also compare to learning with
several Bayesian priors that have been applied to
the DMV. One such prior is the Dirichlet, whose
hyperparameter we will denote by ?. For ? < 0.5,
this prior encourages parameter sparsity. Cohen
et al (2008) use this method with ? = 0.25 for
training the DMV and achieve improvements over
basic EM. In this paper we will refer to our own
implementation of the Dirichlet prior as the ?dis-
counting Dirichlet? (DD) method. In addition to
195
the Dirichlet, other types of priors have been ap-
plied, in particular logistic normal priors (LN) and
shared logistic normal priors (SLN) (Cohen et al,
2008; Cohen and Smith, 2009). LN and SLN aim
to tie parameters together. Essentially, this has a
similar goal to sparsity-inducing methods in that it
posits a more concise explanation for the grammar
of a language. Headden III et al (2009) also im-
plement a sort of parameter tying for the E-DMV
through a learning a backoff distribution on child
probabilities. We compare against results from all
these methods.
4 Learning with Sparse Posteriors
We would like to penalize models that predict a
large number of distinct dependency types. To en-
force this penalty, we use the posterior regular-
ization (PR) framework (Gra?a et al, 2007). PR
is closely related to generalized expectation con-
straints (Mann and McCallum, 2007; Mann and
McCallum, 2008; Bellare et al, 2009), and is also
indirectly related to a Bayesian view of learning
with constraints on posteriors (Liang et al, 2009).
The PR framework uses constraints on posterior
expectations to guide parameter estimation. Here,
PR allows a natural and tractable representation of
sparsity constraints based on edge type counts that
cannot easily be encoded in model parameters. We
use a version of PR where the desired bias is a
penalty on the log likelihood (see Ganchev et al
(2010) for more details). For a distribution p?, we
define a penalty as the (generic) ?-norm of expec-
tations of some features ?:
||Ep? [?(X,Y)]||? (5)
For computational tractability, rather than penaliz-
ing the model?s posteriors directly, we use an aux-
iliary distribution q, and penalize the marginal log-
likelihood of a model by the KL-divergence of p?
from q, plus the penalty term with respect to q.
For a fixed set of model parameters ? the full PR
penalty term is:
min
q
KL(q(Y) ? p?(Y|X)) + ? ||Eq[?(X,Y)]||? (6)
where ? is the strength of the regularization. PR
seeks to maximize L(?) minus this penalty term.
The resulting objective can be optimized by a vari-
ant of the EM (Dempster et al, 1977) algorithm
used to optimize L(?).
4.1 `1/`? Regularization
We now define precisely how to count dependency
types. For each child tag c, let i range over an enu-
meration of all occurrences of c in the corpus, and
let p be another tag. Let the indicator ?cpi(X,Y)
have value 1 if p is the parent tag of the ith occur-
rence of c, and value 0 otherwise. The number of
unique dependency types is then:
X
cp
max
i
?cpi(X,Y) (7)
Note there is an asymmetry in this count: occur-
rences of child type c are enumerated with i, but
all occurrences of parent type p are or-ed in ?cpi.
That is, ?cpi = 1 if any occurrence of p is the par-
ent of the ith occurrence of c. We will refer to PR
training with this constraint as PR-AS. Instead of
counting pairs of a child token and a parent type,
we can alternatively count pairs of a child token
and a parent token by letting p range over all to-
kens rather than types. Then each potential depen-
dency corresponds to a different indicator ?cpij ,
and the penalty is symmetric with respect to par-
ents and children. We will refer to PR training
with this constraint as PR-S. Both approaches per-
form very well, so we report results for both.
Equation 7 can be viewed as a mixed-norm
penalty on the features ?cpi or ?cpij : the sum cor-
responds to an `1 norm and the max to an `?
norm. Thus, the quantity we want to minimize
fits precisely into the PR penalty framework. For-
mally, to optimize the PR objective, we complete
the following E-step:
argmin
q
KL(q(Y)||p?(Y|X)) + ?
X
cp
max
i
Eq[?(X,Y)],
(8)
which can equivalently be written as:
min
q(Y),?cp
KL(q(Y) ? p?(Y|X)) + ?
X
cp
?cp
s. t. ?cp ? Eq[?(X,Y)]
(9)
where ?cp corresponds to the maximum expecta-
tion of ? over all instances of c and p. Note that
the projection problem can be solved efficiently in
the dual (Ganchev et al, 2010).
5 Experiments
We evaluate on 12 languages. Following the ex-
ample of Smith and Eisner (2006), we strip punc-
tuation from the sentences and keep only sen-
tences of length ? 10. For simplicity, for all mod-
els we use the ?harmonic? initializer from Klein
196
Model EM PR Type ?
DMV 45.8 62.1 PR-S 140
2-1 45.1 62.7 PR-S 100
2-2 54.4 62.9 PR-S 80
3-3 55.3 64.3 PR-S 140
4-4 55.1 64.4 PR-AS 140
Table 1: Attachment accuracy results. Column 1: Vc-
Vs used for the E-DMV models. Column 3: Best PR re-
sult for each model, which is chosen by applying each of
the two types of constraints (PR-S and PR-AS) and trying
? ? {80, 100, 120, 140, 160, 180}. Columns 4 & 5: Con-
straint type and ? that produced the values in column 3.
and Manning (2004), which we refer to as K&M.
We always train for 100 iterations and evaluate
on the test set using Viterbi parses. Before eval-
uating, we smooth the resulting models by adding
e?10 to each learned parameter, merely to remove
the chance of zero probabilities for unseen events.
(We did not tune this as it should make very little
difference for final parses.) We score models by
their attachment accuracy ? the fraction of words
assigned the correct parent.
5.1 Results on English
We start by comparing English performance for
EM, PR, and DD. To find ? for DD we searched
over five values: {0.01, 0.1, 0.25, 1}. We found
0.25 to be the best setting for the DMV, the same
as found by Cohen et al (2008). DD achieves ac-
curacy 46.4% with this ?. For the E-DMV we
tested four model complexities with valencies Vc-
Vs of 2-1, 2-2, 3-3, and 4-4. DD?s best accuracy
was 53.6% with the 4-4 model at ? = 0.1. A
comparison between EM and PR is shown in Ta-
ble 1. PR-S generally performs better than the PR-
AS for English. Comparing PR-S to EM, we also
found PR-S is always better, independent of the
particular ?, with improvements ranging from 2%
to 17%. Note that in this work we do not perform
the PR projection at test time; we found it detri-
mental, probably due to a need to set the (corpus-
size-dependent) ? differently for the test set. We
also note that development likelihood and the best
setting for ? are not well-correlated, which un-
fortunately makes it hard to pick these parameters
without some supervision.
5.2 Comparison with Previous Work
In this section we compare to previously published
unsupervised dependency parsing results for En-
glish. It might be argued that the comparison is
unfair since we do supervised selection of model
Learning Method Accuracy
? 10 ? 20 all
PR-S (? = 140) 62.1 53.8 49.1
LN families 59.3 45.1 39.0
SLN TieV & N 61.3 47.4 41.4
PR-AS (? = 140) 64.4 55.2 50.5
DD (? = 1, ? learned) 65.0 (?5.7)
Table 2: Comparison with previous published results. Rows
2 and 3 are taken from Cohen et al (2008) and Cohen and
Smith (2009), and row 5 from Headden III et al (2009).
complexity and regularization strength. However,
we feel the comparison is not so unfair as we per-
form only a very limited search of the model-?
space. Specifically, the only values of ? we search
over are {80, 100, 120, 140, 160, 180}.
First, we consider the top three entries in Ta-
ble 2, which are for the basic DMV. The first en-
try was generated using our implementation of
PR-S. The second two entries are logistic nor-
mal and shared logistic normal parameter tying re-
sults (Cohen et al, 2008; Cohen and Smith, 2009).
The PR-S result is the clear winner, especially as
length of test sentences increases. For the bot-
tom two entries in the table, which are for the E-
DMV, the last entry is best, corresponding to us-
ing a DD prior with ? = 1 (non-sparsifying), but
with a special ?random pools? initialization and a
learned weight ? for the child backoff probabil-
ity. The result for PR-AS is well within the vari-
ance range of this last entry, and thus we conjec-
ture that combining PR-AS with random pools ini-
tialization and learned ? would likely produce the
best-performing model of all.
5.3 Results on Other Languages
Here we describe experiments on 11 additional
languages. For each we set ? and model complex-
ity (DMV versus one of the four E-DMV exper-
imented with previously) based on the best con-
figuration found for English. This likely will not
result in the ideal parameters for all languages, but
provides a realistic test setting: a user has avail-
able a labeled corpus in one language, and would
like to induce grammars for many other languages.
Table 3 shows the performance for all models and
training procedures. We see that the sparsifying
methods tend to improve over EM most of the
time. For the basic DMV, average improvements
are 1.6% for DD, 6.0% for PR-S, and 7.5% for
PR-AS. PR-AS beats PR-S in 8 out of 12 cases,
197
Bg Cz De Dk En Es Jp Nl Pt Se Si Tr
DMV Model
EM 37.8 29.6 35.7 47.2 45.8 40.3 52.8 37.1 35.7 39.4 42.3 46.8
DD 0.25 39.3 30.0 38.6 43.1 46.4 47.5 57.8 35.1 38.7 40.2 48.8 43.8
PR-S 140 53.7 31.5 39.6 44.0 62.1 61.1 58.8 31.0 47.0 42.2 39.9 51.4
PR-AS 140 54.0 32.0 39.6 42.4 61.9 62.4 60.2 37.9 47.8 38.7 50.3 53.4
Extended Model
EM (3,3) 41.7 48.9 40.1 46.4 55.3 44.3 48.5 47.5 35.9 48.6 47.5 46.2
DD 0.1 (4,4) 47.6 48.5 42.0 44.4 53.6 48.9 57.6 45.2 48.3 47.6 35.6 48.9
PR-S 140 (3,3) 59.0 54.7 47.4 45.8 64.3 57.9 60.8 33.9 54.3 45.6 49.1 56.3
PR-AS 140 (4,4) 59.8 54.6 45.7 46.6 64.4 57.9 59.4 38.8 49.5 41.4 51.2 56.9
Table 3: Attachment accuracy results. The parameters used are the best settings found for English. Values for hyperparameters
(? or ?) are given after the method name. For the extended model (Vc, Vs) are indicated in parentheses. En is the English Penn
Treebank (Marcus et al, 1993) and the other 11 languages are from the CoNLL X shared task: Bulgarian [Bg] (Simov et al,
2002), Czech [Cz] (Bohomov? et al, 2001), German [De] (Brants et al, 2002), Danish [Dk] (Kromann et al, 2003), Spanish
[Es] (Civit and Mart?, 2004), Japanese [Jp] (Kawata and Bartels, 2000), Dutch [Nl] (Van der Beek et al, 2002), Portuguese
[Pt] (Afonso et al, 2002), Swedish [Se] (Nilsson et al, 2005), Slovene [Sl] (D?eroski et al, 2006), and Turkish [Tr] (Oflazer et
al., 2003).
Unad
papeleranc esvs und
objetonc civilizadoaq
Unad
papeleranc esvs und
objetonc civilizadoaq
1.00
1.00 1.000.49
0.51
1.00
0.57
0.43
Unad
papeleranc esvs und
objetonc civilizadoaq
1.00 0.83 0.75 0.990.92
0.35
0.48
Figure 1: Posterior edge probabilities for an example sen-
tence from the Spanish test corpus. At the top are the gold
dependencies, the middle are EM posteriors, and bottom are
PR posteriors. Green indicates correct dependencies and red
indicates incorrect dependencies. The numbers on the edges
are the values of the posterior probabilities.
though the average increase is only 1.5%. PR-S
is also better than DD for 10 out of 12 languages.
If we instead consider these methods for the E-
DMV, DD performs worse, just 1.4% better than
the E-DMV EM, while both PR-S and PR-AS con-
tinue to show substantial average improvements
over EM, 6.5% and 6.3%, respectively.
6 Analysis
One common EM error that PR fixes in many lan-
guages is the directionality of the noun-determiner
relation. Figure 1 shows an example of a Span-
ish sentence where PR significantly outperforms
EM because of this. Sentences such as ?Lleva
tiempo entenderlos? which has tags ?main-verb
common-noun main-verb? (no determiner tag)
provide an explanation for PR?s improvement?
when PR sees that sometimes nouns can appear
without determiners but that the opposite situation
does not occur, it shifts the model parameters to
make nouns the parent of determiners instead of
the reverse. Then it does not have to pay the cost
of assigning a parent with a new tag to cover each
noun that doesn?t come with a determiner.
7 Conclusion
In this paper we presented a new method for unsu-
pervised learning of dependency parsers. In con-
trast to previous approaches that constrain model
parameters, we constrain model posteriors. Our
approach consistently outperforms the standard
EM algorithm and a discounting Dirichlet prior.
We have several ideas for further improving our
constraints, such as: taking into account the direc-
tionality of the edges, using different regulariza-
tion strengths for the root probabilities than for the
child probabilities, and working directly on word
types rather than on POS tags. In the future, we
would also like to try applying similar constraints
to the more complex task of joint induction of POS
tags and dependency parses.
Acknowledgments
J. Gillenwater was supported by NSF-IGERT
0504487. K. Ganchev was supported by
ARO MURI SUBTLE W911NF-07-1-0216.
J. Gra?a was supported by FCT fellowship
SFRH/BD/27528/2006 and by FCT project CMU-
PT/HuMach/0039/2008. B. Taskar was partly
supported by DARPA CSSG and ONR Young
Investigator Award N000141010746.
198
References
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002.
Floresta Sinta(c)tica: a treebank for Portuguese. In
Proc. LREC.
K. Bellare, G. Druck, and A. McCallum. 2009. Al-
ternating projections for learning with expectation
constraints. In Proc. UAI.
A. Bohomov?, J. Hajic, E. Hajicova, and B. Hladka.
2001. The prague dependency treebank: Three-level
annotation scenario. In Anne Abeill?, editor, Tree-
banks: Building and Using Syntactically Annotated
Corpora.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER treebank. In Proc.
Workshop on Treebanks and Linguistic Theories.
M. Civit and M.A. Mart?. 2004. Building cast3lb: A
Spanish Treebank. Research on Language & Com-
putation.
S.B. Cohen and N.A. Smith. 2009. The shared logistic
normal distribution for grammar induction. In Proc.
NAACL.
S.B. Cohen, K. Gimpel, and N.A. Smith. 2008. Lo-
gistic normal priors for unsupervised probabilistic
grammar induction. In Proc. NIPS.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety, 39(1):1?38.
S. D?eroski, T. Erjavec, N. Ledinek, P. Pajas,
Z. ?abokrtsky, and A. ?ele. 2006. Towards a
Slovene dependency treebank. In Proc. LREC.
J. Finkel, T. Grenager, and C. Manning. 2007. The
infinite tree. In Proc. ACL.
K. Ganchev, J. Gra?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
J. Gillenwater, K. Ganchev, J. Gra?a, F. Pereira, and
B. Taskar. 2010. Posterior sparsity in unsupervised
dependency parsing. Technical report, MS-CIS-10-
19, University of Pennsylvania.
J. Gra?a, K. Ganchev, and B. Taskar. 2007. Expec-
tation maximization and posterior constraints. In
Proc. NIPS.
W.P. Headden III, M. Johnson, and D. McClosky.
2009. Improving unsupervised dependency pars-
ing with richer contexts and smoothing. In Proc.
NAACL.
M. Johnson, T.L. Griffiths, and S. Goldwater. 2007.
Adaptor grammars: A framework for specifying
compositional nonparametric Bayesian models. In
Proc. NIPS.
Y. Kawata and J. Bartels. 2000. Stylebook for the
Japanese Treebank in VERBMOBIL. Technical re-
port, Eberhard-Karls-Universitat Tubingen.
D. Klein and C. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency
and constituency. In Proc. ACL.
M.T. Kromann, L. Mikkelsen, and S.K. Lynge. 2003.
Danish Dependency Treebank. In Proc. TLT.
P. Liang, S. Petrov, M.I. Jordan, and D. Klein. 2007.
The infinite PCFG using hierarchical Dirichlet pro-
cesses. In Proc. EMNLP.
P. Liang, M.I. Jordan, and D. Klein. 2009. Learn-
ing from measurements in exponential families. In
Proc. ICML.
G. Mann and A. McCallum. 2007. Simple, robust,
scalable semi-supervised learning via expectation
regularization. In Proc. ICML.
G. Mann and A. McCallum. 2008. Generalized expec-
tation criteria for semi-supervised learning of condi-
tional random fields. In Proc. ACL.
M. Marcus, M. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
R. Neal and G. Hinton. 1998. A new view of the EM
algorithm that justifies incremental, sparse and other
variants. In M. I. Jordan, editor, Learning in Graph-
ical Models, pages 355?368. MIT Press.
J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets
TIGER: Reconstructing a Swedish treebank from
antiquity. NODALIDA Special Session on Tree-
banks.
K. Oflazer, B. Say, D.Z. Hakkani-T?r, and G. T?r.
2003. Building a Turkish treebank. Treebanks:
Building and Using Parsed Corpora.
K. Simov, P. Osenova, M. Slavcheva, S. Kolkovska,
E. Balabanova, D. Doikoff, K. Ivanova, A. Simov,
E. Simov, and M. Kouylekov. 2002. Building a lin-
guistically interpreted corpus of bulgarian: the bul-
treebank. In Proc. LREC.
N. Smith and J. Eisner. 2006. Annealing structural
bias in multilingual weighted grammar induction. In
Proc. ACL.
L. Van der Beek, G. Bouma, R. Malouf, and G. Van No-
ord. 2002. The Alpino dependency treebank. Lan-
guage and Computers.
199
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 450?454,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Reordering Modeling using Weighted Alignment Matrices
Wang Ling, Tiago Lu??s, Joa?o Grac?a, Lu??sa Coheur and Isabel Trancoso
L2F Spoken Systems Lab
INESC-ID Lisboa
{wang.ling,tiago.luis,joao.graca}@inesc-id.pt
{luisa.coheur,isabel.trancoso}@inesc-id.pt
Abstract
In most statistical machine translation sys-
tems, the phrase/rule extraction algorithm uses
alignments in the 1-best form, which might
contain spurious alignment points. The usage
of weighted alignment matrices that encode all
possible alignments has been shown to gener-
ate better phrase tables for phrase-based sys-
tems. We propose two algorithms to generate
the well known MSD reordering model using
weighted alignment matrices. Experiments on
the IWSLT 2010 evaluation datasets for two
language pairs with different alignment algo-
rithms show that our methods produce more
accurate reordering models, as can be shown
by an increase over the regular MSD models
of 0.4 BLEU points in the BTEC French to
English test set, and of 1.5 BLEU points in the
DIALOG Chinese to English test set.
1 Introduction
The translation quality of statistical phrase-based
systems (Koehn et al, 2003) is heavily dependent
on the quality of the translation and reordering mod-
els generated during the phrase extraction algo-
rithm (Ling et al, 2010). The basic phrase extrac-
tion algorithm uses word alignment information to
constraint the possible phrases that can be extracted.
It has been shown that better alignment quality gen-
erally leads to better results (Ganchev et al, 2008).
However the relationship between the word align-
ment quality and the results is not straightforward,
and it was shown in (Vilar et al, 2006) that better
alignments in terms of F-measure do not always lead
to better translation quality.
The fact that spurious word alignments might oc-
cur leads to the use of alternative representations for
word alignments that allow multiple alignment hy-
potheses, rather than the 1-best alignment (Venu-
gopal et al, 2009; Mi et al, 2008; Christopher
Dyer et al, 2008). While using n-best alignments
yields improvements over using the 1-best align-
ment, these methods are computationally expen-
sive. More recently, the method described in (Liu
et al, 2009) produces improvements over the meth-
ods above, while reducing the computational cost
by using weighted alignment matrices to represent
the alignment distribution over each parallel sen-
tence. However, their results were limited by the
fact that they had no method for extracting a reorder-
ing model from these matrices, and used a simple
distance-based model.
In this paper, we propose two methods for gener-
ating the MSD (Mono Swap Discontinuous) reorder-
ing model from the weighted alignment matrices.
First, we test a simple approach by using the 1-best
alignment to generate the reordering model, while
using the alignment matrix to produce the translation
model. This reordering model is a simple adaptation
of the MSD model to read from alignment matrices.
Secondly, we develop two algorithms to infer the re-
ordering model from the weighted alignment matrix
probabilities. The first one uses the alignment infor-
mation within phrase pairs, while the second uses
contextual information of the phrase pairs.
This paper is organized as follows: Section 2 de-
scribes the MSD model; Section 3 presents our two
algorithms; in Section 4 we report the results from
the experiments conducted using these algorithms,
450
and comment on the results; we conclude in Sec-
tion 5.
2 MSD models
Moses (Koehn et al, 2007) allows many config-
urations for the reordering model to be used. In
this work, we will only refer to the default config-
uration (msd-bidirectional-fe), which uses the MSD
model, and calculates the reordering orientation for
the previous and the next word, for each phrase pair.
Other possible configurations are simpler than the
default one. For instance, the monotonicity model
only considers monotone and non-monotone orien-
tation types, whereas the MSD model also considers
the monotone orientation type, but distinguishes the
non-monotone orientation type between swap and
discontinuous. The approach presented in this work
can be adapted to the other configurations.
In the MSD model, during the phrase extraction,
given a source sentence S and a target sentence T ,
the alignment set A, where aji is an alignment from i
to j, the phrase pair with words in positions between
i and j in S, Sji , and n and m in T , T
m
n , can be
classified with one of three orientations with respect
to the previous word:
? The orientation is monotonous if only the pre-
vious word in the source is aligned with the pre-
vious word in the target, or, more formally, if
an?1i?1 ? A ? a
n?1
j+1 /? A.
? The orientation is swap, if only the next word
in the source is aligned with the previous word
in the target, or more formally, if an?1j+1 ? A ?
an?1i?1 /? A.
? The orientation is discontinuous if neither of
the above are true, which means, (an?1i?1 ?
A ? an?1j+1 ? A) ? (a
n?1
i?1 /? A ? a
n?1
j+1 /? A).
The orientations with respect to the next word are
given analogously. The reordering model is gener-
ated by grouping the phrase pairs that are equal, and
calculating the probabilities of the grouped phrase
pair being associated each orientation type and di-
rection, based on the orientations for each direction
that are extracted. Formally, the probability of the
phrase pair p having a monotonous orientation is
prev 
word(s)
source phrase
target phrase
prev 
word(t)
next 
word(s)
source phrase
target phrase
prev 
word(t)
a) b)
c)
source phrase
target phrase
prev 
word(t)
d)
next 
word(s)
source phrase
target phrase
prev 
word(t)
prev 
word(s)
Figure 1: Enumeration of possible reordering cases with
respect to the previous word. Case a) is classified as
monotonous, case b) is classified as swap and cases c)
and d) are classified as discontinuous.
given by:
P (p,mono) = C(mono)C(mono)+C(swap)+C(disc) (1)
Where C(o) is the number of times a phrase is ex-
tracted with the orientation o in that group of phrase
pairs. Moses also provides many options for this
stage, such as types of smoothing. We use the de-
fault smoothing configuration which adds the fixed
value of 0.5 to all C(o).
3 Weighted MSD Model
When using a weighted alignment matrix, rather
than working with alignments points, we use the
probability of each word in the source aligning with
each word in the target. Thus, the regular MSD
model cannot be directly applied here.
One obvious solution to solve this problem is to
produce a 1-best alignment set alng with the align-
ment matrix, and use the 1-best alignment to gen-
erate the reordering model, while using the align-
ment matrix to produce the translation model. How-
ever, this method would not be taking advantage of
the weighted alignment matrix. The following sub-
sections describe two algorithms that are proposed
to make use of the alignment probabilities.
3.1 Score-based
Each phrase pair that is extracted using the algorithm
described in (Liu et al, 2009) is given a score based
on its alignments. This score is higher if the align-
ment points in the phrase pair have high probabili-
ties, and if the alignment is consistent. Thus, if an
451
extracted phrase pair has better quality, its orienta-
tion should have more weight than phrase pairs with
worse quality. We implement this by changing the
C(o) function in equation 1 from being the number
of the phrase pairs with the orientation o, to the sum
of the scores of those phrases. We also need to nor-
malize the scores for each group, due to the fixed
smoothing that is applied, since if the sum of the
scores is much lower (e.g. 0.1) than the smoothing
factor (0.5), the latter will overshadow the weight
of the phrase pairs. The normalization is done by
setting the phrase pair with the highest value of the
sum of all MSD probabilities to 1, and readjusting
other phrase pairs accordingly. Thus, a group of 3
phrase pairs that have the MSD probability sums of
0.1, 0.05 and 0.1, are all set to 1, 0.5 and 1.
3.2 Context-based
We propose an alternative algorithm to calculate
the reordering orientations for each phrase pair.
Rather than classifying each phrase pair with either
monotonous (M ), swap (S) or discontinuous (D),
we calculate the probability for each orientation, and
use these as weighted counts when creating the re-
ordering model. Thus, for the previous word, given
a weighted alignment matrix W , the phrase pair be-
tween the indexes i and j in S, Sji , and n and m in
T , Tmn , the probability values for each orientation
are given by:
? Pc(M) = W
n?1
i?1 ? (1?W
n?1
j+1 )
? Pc(S) = W
n?1
j+1 ? (1?W
n?1
i?1 )
? Pc(D) = W
n?1
i?1 ?W
n?1
j+1
+ (1?Wn?1i?1 )? (1?W
n?1
j+1 )
These formulas derive from the adaptation of con-
ditions of each orientation presented in 2. In the
regular MSD model, the previous orientation for a
phrase pair is monotonous if the previous word in
the source phrase is aligned with the previous word
in the target phrase and not aligned with the next
word. Thus, the probability of a phrase pair to have a
monotonous orientation Pc(M) is given by the prob-
ability of the previous word in the source phrase
being aligned with the previous word in the target
phrase Wn?1i?1 , and the probability of the previous
word in the source to not be aligned with the next
word in the target (1 ? Wn?1j+1 ). Also, the sum of
the probabilities of all orientations (Pc(M), Pc(S),
Pc(D)) for a given phrase pair can be trivially shown
to be 1. The probabilities for the next word are
given analogously. Following equation 1, the func-
tion C(o) is changed to be the sum of all Pc(o), from
the grouped phrase pairs.
4 Experiments
4.1 Corpus
Our experiments were performed over two datasets,
the BTEC and the DIALOG parallel corpora from
the latest IWSLT evaluation 2010 (Paul et al, 2010).
BTEC is a multilingual speech corpus that contains
sentences related to tourism, such as the ones found
in phrasebooks. DIALOG is a collection of human-
mediated cross-lingual dialogs in travel situations.
The experiments performed with the BTEC cor-
pus used only the French-English subset, while the
ones perfomed with the DIALOG corpus used the
Chinese-English subset. The training corpora con-
tains about 19K sentences and 30K sentences, re-
spectively. The development corpus for the BTEC
task was the CSTAR03 test set composed by 506
sentences, and the test set was the IWSLT04 test set
composed by 500 sentences and 16 references. As
for the DIALOG task, the development set was the
IWSLT09 devset composed by 200 sentences, and
the test set was the CSTAR03 test set with 506 sen-
tences and 16 references.
4.2 Setup
We use weighted alignment matrices based on Hid-
den Markov Models (HMMs), which are produced
by the the PostCAT toolkit1, based on the poste-
rior regularization framework (V. Grac?a et al, 2010).
The extraction algorithm using weighted alignment
matrices employs the same method described in (Liu
et al, 2009), and the phrase pruning threshold was
set to 0.1. For the reordering model, we use the
distance-based reordering, and compare the results
with the MSD model using the 1-best alignment.
Then, we apply our two methods based on align-
ment matrices. Finally, we combine our two meth-
ods above by adapting the function C(o), to be the
1http://www.seas.upenn.edu/ strctlrn/CAT/CAT.html
452
sum of all Pc(o), weighted by the scores of the re-
spective phrase pairs. The optimization of the trans-
lation model weights was done using MERT, and
each experiment was run 5 times, and the final score
is calculated as the average of the 5 runs, in order to
stabilize the results. Finally, the results were eval-
uated using BLEU-4, METEOR, TER and TERp.
The BLEU-4 and METEOR scores were computed
using 16 references. The TER and TERp were com-
puted using a single reference.
4.3 Reordering model comparison
Tables 1 and 2 show the scores using the differ-
ent reordering models. Consistent improvements in
the BLEU scores may be observed when changing
from the MSD model to the models generated us-
ing alignment matrices. The results were consis-
tently better using our models in the DIALOG task,
since the English-Chinese language pair is more de-
pendent on the reordering model. This is evident
if we look at the difference in the scores between
the distance-based and the MSD models. Further-
more, in this task, we observe an improvement on all
scores from the MSD model to our weighted MSD
models, which suggests that the usage of alignment
matrices helps predict the reordering probabilities
more accurately.
We can also see that the context based reordering
model performs better than the score based model
in the BTEC task, which does not perform sig-
nificantly better than the regular MSD model in
this task. Furthermore, combining the score based
method with the context based method does not lead
to any improvements. We believe this is because the
alignment probabilities are much more accurate in
the English-French language pair, and phrase pair
scores remain consistent throughout the extraction,
making the score based approach and the regular
MSD model behave similarly. On the other hand,
in the DIALOG task, score based model has bet-
ter performance than the regular MSD model, and
the combination of both methods yields a significant
improvement over each method alone.
Table 3 shows a case where the context based
model is more accurate than the regular MSD model.
The alignment is obviously faulty, since the word
?two? is aligned with both ?deux?, although it
should only be aligned with the first occurrence.
BTEC BLEU METEOR TERp TER
Distance-based 61.84 65.38 27.60 22.40
MSD 62.02 65.93 27.40 22.80
score MSD 62.15 66.18 27.30 22.20
context MSD 62.42 66.29 27.00 22.00
combined MSD 62.42 66.14 27.10 22.20
Table 1: Results for the BTEC task.
DIALOG BLEU METEOR TERp TER
Distance-based 36.29 45.15 49.00 41.20
MSD 39.56 46.85 47.20 39.60
score MSD 40.2 47.16 46.52 38.80
context MSD 40.14 47.14 45.88 39.00
combined MSD 41.03 47.69 46.20 38.20
Table 2: Results for the DIALOG task.
Furthermore, the word ?twin? should be aligned
with ?a` deux lit?, but it is aligned with ?cham-
bres?. If we use the 1-best alignment to compute
the reordering type of the sentence pair ?Je voudrais
re?server deux? / ?I?d like to reserve two?, the re-
ordering type for the following orientation would
be monotonous, since the next word ?chambres?
is falsely aligned with ?twin?. However, it should
clearly be discontinuous, since the right alignment
for ?twin? is ?a` deux lit?. This problem is less seri-
ous when we use the weighted MSD model, since
the orientation probability mass would be divided
between monotonous and discontinuous since the
probability weighted matrix for the wrong alignment
is 0.5. On the BTEC task, some of the other scores
are lower than the MSD model, and we suspect that
this stems from the fact that our tuning process only
attempts to maximize the BLEU score.
5 Conclusions
In this paper we addressed the limitations of the
MSD reordering models extracted from the 1-best
alignments, and presented two algorithms to ex-
tract these models from weighted alignment matri-
ces. Experiments show that our models perform bet-
ter than the distance-based model and the regular
MSD model. The method based on scores showed a
good performance for the Chinese-English language
pair, but the performance for the English-French pair
was similar to the MSD model. On the other hand,
the method based on context improves the results on
453
Alignment Je vo
ud
ra
is
re?
se
rv
er
de
ux
ch
am
br
es
a` de
ux
lit
s
.
I 1
?d 0.7
like 0.7
to
reserve 1
two 1 0.5
twin 0.5 0.5
rooms 1
. 1
Table 3: Weighted alignment matrix for a training sen-
tence pair from BTEC, with spurious alignment proba-
bilities. Alignment points with 0 probabilities are left
empty.
both pairs. Finally, on the Chinese-English test, by
combining both methods we can achieve a BLEU
improvement of approximately 1.5%. The code used
in this work is currently integrated with the Geppetto
toolkit2 , and it will be made available in the next
version for public use.
6 Acknowledgements
This work was partially supported by FCT (INESC-
ID multiannual funding) through the PIDDAC Pro-
gram funds, and also through projects CMU-
PT/HuMach/0039/2008 and CMU-PT/0005/2007.
The PhD thesis of Tiago Lu??s is supported by
FCT grant SFRH/BD/62151/2009. The PhD the-
sis of Wang Ling is supported by FCT grant
SFRH/BD/51157/2010. The authors also wish to
thank the anonymous reviewers for many helpful
comments.
References
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing Word Lattice Translation. Tech-
nical Report LAMP-TR-149, University of Maryland,
College Park, February.
Kuzman Ganchev, Joa?o V. Grac?a, and Ben Taskar. 2008.
Better alignments = better translations? In Proceed-
ings of ACL-08: HLT, pages 986?993, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
2http://code.google.com/p/geppetto/
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 48?54, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-burch, Richard Zens, Rwth Aachen, Alexan-
dra Constantin, Marcello Federico, Nicola Bertoldi,
Chris Dyer, Brooke Cowan, Wade Shen, Christine
Moran, and Ondrej Bojar. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Wang Ling, Tiago Lu??s, Joao Grac?a, Lu??sa Coheur, and
Isabel Trancoso. 2010. Towards a general and ex-
tensible phrase-extraction algorithm. In IWSLT ?10:
International Workshop on Spoken Language Transla-
tion, pages 313?320, Paris, France.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 2 - Volume 2, EMNLP ?09, pages 1017?1026,
Morristown, NJ, USA. Association for Computational
Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192?199, Columbus, Ohio, June. Association
for Computational Linguistics.
Michael Paul, Marcello Federico, and Sebastian Stu?ker.
2010. Overview of the iwslt 2010 evaluation cam-
paign. In IWSLT ?10: International Workshop on Spo-
ken Language Translation, pages 3?27.
Joa?o V. Grac?a, Kuzman Ganchev, and Ben Taskar. 2010.
Learning Tractable Word Alignment Models with
Complex Constraints. Comput. Linguist., 36:481?504.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Wider pipelines: N-best
alignments and parses in MT training.
David Vilar, Maja Popovic, and Hermann Ney. 2006.
Aer: Do we need to ?improve? our alignments? In
International Workshop on Spoken Language Transla-
tion (IWSLT), pages 205?212.
454
Rich Prior Knowledge in 
Learning for NLP
Gregory Druck, Kuzman Ganchev, Jo?o Gra?a
Why Incorporate Prior Knowledge?
have: unlabeled data
option: hire
linguist
annotators
Why Incorporate Prior Knowledge?
have: unlabeled data
option: hire
linguist
annotators
This approach does not 
scale to every task and 
domain of interest.
However, we already 
know a lot about most 
problems of interest.
Example: Document Classification 
?
Prior Knowledge: 
?
labeled features: information about the label 
distribution when word w is present
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
Documents
Labels
newsgroups classification
baseball Mac politics
...
hit Apple senate
...
Braves Macintosh taxes
...
runs Powerbook liberal
...
sentiment polarity
positive negative
memorable terrible
perfect boring
exciting mess
Example: Information Extraction
?
Prior Knowledge: 
?
labeled features: 
?
the word ACM should be labeled either journal or 
booktitle most of the time
?
non-Markov (long-range) dependencies:
?
each reference has at most one segment of each type
W. H. Enright. Improving the efficiency of matrix operations 
in the numerical solution of stiff ordinary differential 
equations. ACM Trans. Math. Softw., 4(2), 127-136, June 1978.
extraction from 
research papers:
Example: Part-of-speech Induction
?
Prior Knowledge: 
?
linguistic knowledge: each sentence should have a verb
?
posterior sparsity: the total number of different POS tags 
assigned to each word type should be small
Tags
A career with the European 
institutions must become more 
attractive. Too many young, new...
Text
Example: Dependency Grammar Induction
?
Prior Knowledge: 
?
linguistic rules: nouns are usually dependents of verbs
?
noisy labeled data: target language parses should be 
similar to aligned parses in a resource-rich source language 
Example: Word Alignment
?
Prior Knowledge: 
?
Bijectivity: alignment should be mostly one-to-one
?
Symmetry: source?target and target?source 
alignments should agree
A career with the European institutions must become more attractive. 
Uma carreira nas institui??es europeias t?m de se tornar mais atractiva. 
This Tutorial
In general, how can we leverage such knowledge 
and an unannotated corpus during learning?
Notation & Models
input variables (documents, sentences):
structured output variables (parses, sequences):
unstructured output variables (labels):
input / output variables for entire corpus: 
probabilistic model parameters:
generative models:
discriminative models:
model feature function:  
p ?(y|x)
p?(x,y)
x
y
?
f(x , y)
X Y
y
Learning Scenarios
?
Unsupervised: 
?
unlabeled data + prior knowledge
?
Lightly Supervised: 
?
unlabeled data + ?informative? prior knowledge
?
i.e. provides specific information about labels 
?
Semi-Supervised: 
?
labeled data + unlabeled data + prior knowledge
Running Example #1:
Document Classification
?
model: Maximum Entropy Classifier (Logistic Regression) 
?
setting: lightly supervised; no labeled data
?
prior knowledge: 
?
labeled features: information about the label 
distribution when word w is present
?
label is often hockey or baseball when game is present
p?(y|x) =
1
Z(x)
exp(? ? f(x, y))
Running Example #2:
Word Alignment
?
model: first-order Hidden Markov Model (HMM)
?
setting: unsupervised
?
prior knowledge: 
?
Bijectivity: alignment should be mostly one-to-one
1 1
2 3
we know
the way
sabemos       el       camino      null
1 2 3 0
p?(y,x) = p?(y0)
N?
i=1
p?(yi|yi?1)p?( x i|yi)
Problem
?
This output does not agree with prior knowledge!
?
 six target words align to source word animada
?
 five source words do not align with any target word 
gameconvivialvery,animatedanwasit
cordialmuyyanimadamaneraunadejugaban
model
data output
x
1
x
2
x
3
y
1
y
2
y
3
+
Limited Approach: Labeling Data
limitation: Often unclear how to do conversion
?
Example #1: often (not always) game ? {hockey,baseball} 
?
Example #2: alignment should be mostly one-to-one
prior 
knowledge
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
approach: Convert prior knowledge to labeled data.
Prototypes (+ cluster features):   
?
 [Haghighi & Klein 06]
Others: 
?
[Raghavan & Allan 07]       
?
[Schapire et al 02]
Limited Approach: Bayesian Approach
approach: Encode prior knowledge with a prior on parameters.
limitation: Our prior knowledge is not about parameters! 
Parameters are difficult to interpret; hard to get desired effect.
?
Example #1: often (not always) game ? {hockey,baseball}
?
Example #2: alignment should be mostly one-to-one
natural: ?   should be small (or sparse)??
( informative prior )
possible: ?    should be close to   ??i ??i
p (?)
specifying 
x
1
x
2
x
3
y
1
y
2
y
3
??
[Dayanik et al 06]
[Johnson 07], among many others
Limited Approach: Augmenting Model
limitation: can be difficult to get desired effect
?
Example #1: often (not always) game ? {hockey,baseball}
limitation: may make exact inference intractable
?
Example #2: Bijectivity makes inference #P-complete
x
1
x
2
x
3
y
1
y
2
y
3
z
1
approach: Encode prior knowledge with 
additional variables and dependencies.
This Tutorial
develop:
?
a language for directly encoding prior knowledge
?
methods for learning with knowledge in this language
?
( approximations to modeling this language directly )
?
(loosely) these methods perform mappings for us: 
?
encoded prior knowledge            parameters
?
encoded prior knowledge            labeling 
?
--- --- 
----- -- -- 
--- ---- 
--- --- --- 
--- --- 
----- -- -- 
--- ---- 
--- --- --- 
--- --- 
----- -- -- 
--- ---- 
--- --- --- 
?
?
A Language for Encoding Prior Knowledge
Our prior knowledge is about distributions over latent 
output variables. (output variables are interpretable)
Specifically, we know some properties of this distribution:
?
Example #1: often (not always) game?{hockey,baseball}
Formulation: know about the expectations of some 
functions under distribution over latent output variables
Constraint Features
?
constraint feature function: 
?
Example #1: 
?
for document x, returns a vector with a 1 in the lth 
position if y is the lth label and the word w is in x
?
Example #2: 
?
returns a vector with mth value = number of target 
words in sentence x that align with source word m
?(x , y )
?w(x, y) = 1 (y = l )1( w ? x)
?(x,y) =
N?
i=1
1(yi = m)
Expectations of Constraint Features
?
Example #1:  Corpus expectation: 
?
vector with expected distribution over labels for 
documents that contain w (     is the count of w)
?
Example #2:  Per-example expectation: 
?
vector with mth value = expected number of target 
words that align with source word m 
Ep? [?(X,Y)] =
1
c w
?
x
?
y
p?(y|x)?w(x, y)
E p ? [?(x,y)] =
?
y
p?(y|x)?(x,y)
c w
Expressing Preferences
?
express preferences using target values: 
?
Example #1:                           
?
label distribution for game is close to [40% 40% 20%]
?
Example #2:                           
?
expected number of target words that align with each 
source word is at most one
??
E p ? [? w ( X , Y )] ? ??
E p ? [?(x , y)] ? ??
Preview: Labeled Features
User Experiments [Druck et al 08]
0 100 200 300 400 500 600 700 8000.4
0.5
0.6
0.7
0.8
0.9
1
labeling time in seconds
tes
ting
 ac
cur
acy
 
 
GEER
~2 minutes, 100 
features labeled 
(or skipped): 
82% accuracy
~15 minutes, 100 
documents labeled 
(or skipped):
78% accuracy
PC vs. Mac
complete set of 
labeled features
PC Mac
dos mac
ibm apple
hp quadra
dx
targets set with 
simple heuristic: 
majority label gets 
90% of mass
Preview:  Word Alignment
[Gra?a et al 10]
60
68.75
77.5
86.25
95
En-Pt Pt-En En-Es Es-En
HMM HMM + Bijectivity Constraint
Overview of the Frameworks
Running Example
Model Family: conditional exponential models
                   are model features
p?(Y|X) =
exp( ? ? f(X,Y))
Z(X)
Z(X) =
?
Y
e x p( ? ? f(X , Y))
f( X , Y )
                Choosing parameters
Model Family: conditional exponential models
Objective: maximize observed data likelihood
Note: Frameworks also suitable for 
generative models (no labeled data necessary)   
?
p?(Y|X) =
exp( ? ? f(X,Y))
Z(X)
max
?
log p?( Y L | X L ) + log p(?)
d e f
= L (?; D L )
Visual Example: Maximum Likelihood
Model:                                      
Objective:
-
+
oo
o
o o
o o o
max
?
log p?( Y L | X L )? 0. 1???
2
2
p(Y|X) =
?
i
exp( y i x i ? ?)
Z ( x i)
A language for prior information
The expectations of user-defined constraint 
features             are close to some value 
?( X , Y ) ??
E [?( X , Y )] ? ??
Running Example:
Want to ensure that 25% of unlabeled 
documents are about politics
?
constraint features
 
?
preferred expected value
?
Expectation w.r.t. unlabeled data
?(x , y) =
?
1 if y is ?politics?
0 otherwise
?? = 0 . 25
Constraint-Driven Learning
Motivation: Hard EM algorithm with preferences
Hard EM: 
Constraint Driven Learning:
M - Step: set ? = arg max
?
log p ?(
?Y | X )
E-Step: set ?Y = argmax
Y
log p ?( Y | X )?penalty( Y )
M - Step: set ? = arg max
?
log p ?(
?Y | X )
E-Step: set ?Y = argmax
Y
log p ?( Y | X )
M. Chang, L. Ratinov, D. Roth (2007).
Constraint-Driven Learning
Motivation: Hard EM algorithm with preferences
Constraint Driven Learning:
?
penalties encode similar information as 
* more on this later *
?
E-Step can be hard; use beam search
E-Step: set ?Y = argmax
Y
log p ?( Y | X )?penalty( Y )
M - Step: set ? = arg max
?
log p ?(
?Y | X )
E [ ? ] ? ??
Visual Example: Constraint Driven Learning
    where     are ?imagined? labels and
?Y
-
+
oo
o
o o
o o o
?[ ?Y ] = count(+ , ?Y )
max
?,Y?
log p?( Y L| X L)? 0. 1???
2
2 s.t. ?(
?Y ) = 2
Posterior Regularization
Motivation: EM algorithm with sane posteriors
EM:
Constrained EM:
E-Step: set q ( Y ) = argmin
q
D KL ( q ( Y )||p?( Y | X ))
M-Step: set ? = argmax
?
E q ( Y ) [ p ?( Y | X )]
E-Step: set q ( Y ) = argmin
q ?Q
D KL ( q ( Y )||p?(y|x))
M-Step: set ? = argmax
?
E q ( Y )[ p ?( Y | X )]
J. Gra?a, K. Ganchev, B. Taskar (2007).
Posterior Regularization
Motivation: EM algorithm with sane posteriors
Idea:                  provide constraints
Objective:
E [ ? ] ? ??
define Q : set of q such that E q [?] ? ??
m ax
?
L(? ; D L )?D KL (Q || p?( Y | X ))
run EM-like procedure but use proposal q ? Q
where
D KL is Kullback-Leibler divergence
X = D U are the input variables for unlabeled corpus
Y is label for entire unlabeled corpus
Posterior Regularization
Hard constraints:
Soft constraints:
max
?
L(?; D L) ? min
q ?Q
D KL ( q ( Y )|| p ?( Y | X ))
Q =
?
q ( Y ) :
?
?
? E q [?( Y )] = ??
?
?
?
2
2
? ?
?
max
?
L(?;D L ) ? min
q
?
D KL (q( Y )|| p?( Y | X )) +
?
?
?
? E q [?( Y )] = ??
?
?
?
2
2
?
Visual Example: Posterior Regularization 
 where: 
-
+
oo
o
o o
o o o
max
?
log p?( Y L | X L )? 0. 1???
2
2 ? D KL ( Q|| p?)
D KL ( Q|| p?) = min
q
D KL ( q ||p?) s.t. E q [?] = 2
Generalized Expectation Constraints
Motivation: augment log-likelihood with cost for ?bad? 
posteriors.
Objective:
where
                                                                    is short-hand
Optimization: gradient descent on    
max
?
L (?; D L)?
?
?
? E p ? ( Y | X ) [?] ? ??
?
?
?
?
E p ? ( Y | X )
[?] = E p ? ( Y | X ) [?( X , Y )]
=
?
Y
p?( Y | X )?( X , Y )
?
G. Mann, A. McCallum (2007). 
A visual comparison of the frameworks
Objective: Generalized Expectation Constraints
-
+
oo
o
o o
o o o
max
?
log p?( Y L | X L ) ? 0. 1???
2
2 ? 500?E p ? [?] ? 2?
2
2
Types of constraints
Constraint Driven Learning: Penalized Viterbi
?
Easy if                           decompose as the model.
                 and
?
Otherwise:
?
Beam search
?
Integer linear program 
p ( Y | X ) =
?
c
p c (y c | X )
argmax
Y
log p?( Y | X ) ? ??( X , Y )? ????
??( X , Y )? ????
??( X , Y )? ???? =
?
c
? c ( X , y c )
Types of constraints
Posterior Regularization: KL projection
?
Usually easy if               decompose as the model:
     and
?
Otherwise: Sample (e.g. K. Bellare, G. Druck, and A. McCallum, 2009)
?( Y , X )
p ( Y | X ) =
?
c
p c (y c | X )
q ( Y | X ) =
?
c
q c (y c | X )
?( X , Y ) =
?
c
? c ( X , y c )
?
min
q
D KL ( q ||p?) s.t. ?E q [?] ? ???? ? ?
Types of constraints
Generalized Expectation Constraints: Direct gradient
?
Usually easy if:
?
decomposes as the model
?
Can compute                * more on this later *
?
Unstructured
?
Sequence, Grammar (semiring trick)
?
Otherwise: sample or approximate the gradient.
?( Y , X )
max
?
L (?; D L)?
?
?
? E p ? ( Y | X ) [?] ? ??
?
?
?
?
?( X , Y ) =
?
c
? c ( X , y c )
E [ ?? f ]
A Bayesian View: Measurements
Objective: mode of    given observations
X L ? X
Y L Y
?( X , Y )
b
Figure 4.1: The model used by Liang et al [2009], using our notation. We have separated
treatment of the labeled data (XL,YL) from treatment of the unlabeled data X.
and produce some value ?(X,Y), which is never observed directly. Instead, we observe
some noisy version b ? ?(X,Y). The measured values b are distributed according to
some noise model pN(b|?(X,Y)). Liang et al [2009] note that the optimization is convex
for log-concave noise and use box noise in their experiments, giving b uniform probability
in some range near ?(X,Y).
In the Bayesian setting, the model parameters ? as well as the observed measurement
values b are random variables. Liang et al [2009] use the mode of p(?|XL,YL,X,b) as a
point estimate for ?:
argmax
?
p(?|XL,YL,X,b) = argmax
?
?
Y
p(?,Y,b|X,XL,YL), (4.6)
with equality because p(?|XL,YL,X,b) ? p(?,b|XL,YL,X) =
?
Y p(?,Y,b|X,XL,YL). Liang et al [2009] focus on computing p(?,Y,b|X,XL,YL).
They define their model for this quantity as follows:
p(?,Y,b|X,XL,YL) = p(?|XL,YL) p?(Y|X) pN(b|?(X,Y)) (4.7)
where the Y and X are particular instantiations of the random variables in the entire unla-
beled corpusX. Equation 4.7 is a product of three terms: a prior on ?, the model probability
p?(Y|X), and a noise model pN(b|?). The noise model is the probability that we observe
a value, b, of the measurement features ?, given that its actual value was ?(X,Y). The
idea is that we model errors in the estimation of the posterior probabilities as noise in the
measurement process. Liang et al [2009] use a uniform distribution over ?(X,Y) ? ?,
which they call ?box noise?. Under this model, observing b farther than ? from ?(X,Y)
has zero probability. In log space, the exact MAP objective, becomes:
max
?
L(?) + logEp?(Y|X)
?
pN(b|?(X,Y))
?
. (4.8)
31
max
?
l og p(?) +
?
( x , y ) ? D L
l og p?( y |x) = L (? ; D L )
?
P. Liang, M. Jordan, D. Klein (2009)
Objective: mode of    given observations
A Bayesian View: Measurements
X L ? X
Y L Y
?( X , Y )
b
Figure 4.1: The model used by Liang et l. [2009], using our notation. We have separated
treatment of the labeled data (XL,YL) from treatment of the unlabeled data X.
and produce some value ?(X,Y), which is never observed directly. Instead, we observe
some noisy version b ? ?(X,Y). The measured values b are distributed according to
some noise model pN(b|?(X,Y)). Liang et al [2009] note that the optimization is convex
for log-concave noise and use box noise in their experiments, giving b uniform probability
in some range near ?(X,Y).
In the Bayesian setting, the model parameters ? as well as the observed measurement
values b are random variables. Liang et al [2009] use the mode of p(?|XL,YL,X,b) as a
point estimate for ?:
argmax
?
p(?|XL,YL,X,b) = argmax
?
?
Y
p(?,Y,b|X,XL,YL), (4.6)
with equality because p(?|XL,YL,X,b) ? p(?,b|XL,YL,X) =
?
Y p(?,Y,b|X,XL,YL). Liang et al [2009] focus on computing p(?,Y,b|X,XL,YL).
They define their model for this quantity as follows:
p(?,Y,b|X,XL,YL) = p(?|XL,YL) p?(Y|X) pN(b|?(X,Y)) (4.7)
where the Y and X are particular instantiations of the random variables in the entire unla-
beled corpusX. Equation 4.7 is a product of three terms: a prior on ?, the model probability
p?(Y|X), and a noise model pN(b|?). The noise model is the probability that we observe
a value, b, of the measurement features ?, given that its actual value was ?(X,Y). The
idea is that we model errors in the estimation of the posterior probabilities as noise in the
measurement process. Liang et al [2009] use a uniform distribution over ?(X,Y) ? ?,
which they call ?box noise?. Under this model, observing b farther than ? from ?(X,Y)
has zero probability. In log space, the exact MAP objective, becomes:
max
?
L(?) + logEp?(Y|X)
?
pN(b|?(X,Y))
?
. (4.8)
31
max
?
L (?;D L ) + log E p ? ( Y | X )
?
p(??|?( X , Y ))
?
?
What's wrong with this picture?
Objective: mode of    given observations
Example: Exactly 25% of articles are ?politics?
What is the probability exactly 25% of the articles are 
labeled ``politics''?
How do we optimize this with respect to  ?
max
?
L (? ; D L ) + log E p ?(Y | X)
?
p(??|?( X , Y ))
?
?
?
p(??|?( X , Y )) = 1
?
?? = ?( X , Y )
?
E p ?(Y | X)
?
1 (?? = ?(X , Y))
?
What's wrong with this picture?
Example: Compute prob:  25% of docs are ?politics?.
    Naively:
      in this case we can use a DP, but if 
there are many constraints, that doesn?t 
work.
Easier: What is the expected number of ?politics? articles?
Article p(?politics?)
1 0.2
2 0.4
3 0.1
4 0.6
0 . 2 + 0 . 4 + 0 . 1 + 0 . 6
0 . 2 ? (1 ? 0 . 4) ? (1 ? 0 . 1) ? (1 ? 0 . 6)
+ . . . +
+(1 ? 0 . 2) ? (1 ? 0 . 4) ? (1 ? 0 . 1) ? 0 . 6
Probabilities and Expectations
difficult to compute expectations of arbitrary functions but...
Usually:             decomposes as a sum
e.g. 25% of articles are ?politics?
Idea: approximate 
?( X , Y )
?(X , Y) =
?
instances
?( x , y )
E p ?(Y | X)
?
p
?
?? | ?( X , Y )
??
? p
?
?? | E p ?(Y | X) [?( X , Y )]
?
Probabilities and Expectations
Approximation:
Objective:
Example:                      is Gaussian     
                                              is 
so for appropriate                           this is identical to GE!
E p ? ( Y | X )
?
p
?
?? | ?
??
? p
?
?? | E p ? ( Y | X ) [ ? ]
?
max
?
L (?;D L ) + log p
?
?? | E p ?(Y | X) [?]
?
l og p
?
?? | E [?]
?
?
p
?
?? | E [ ? ]
?
l og p
?
?? | E [ ? ]
?
?
?
?
? E [ ? ] ? ??
?
?
?
2
2
Optimizing GE objective
GE Objective:
?
Gradient involves covariance
this can be hard because
and the usual dynamic programs (inside outside, forward 
backward) can?t compute this.
C ov( ? , f) = E [ ?? f ] ? E [ ? ] ? E [ f ]
E [?? f ] =
?
Y
p ( Y )?( Y )? f ( Y )
O GE = max
?
L (? ; D L )?
?
?
? E p ?(Y | X) [ ?( X , Y )] ? ??
?
?
?
?
Optimizing GE Objective
Maintaining both     and      in the DP is expensive
* Semiring trick can help for some problems *
x1 x2 x3 x3
y1 y2 y3 y4
E [?? f ] =
?
Y
p ( Y )?( Y )? f ( Y )
?( Y )? f ( Y ) =
?
?
i
?(yi)
?
?
?
?
?
j
f (y j )
?
?
yi y j
   E.g. if inference is a hypergraph problem.
A Variational Approximation
GE Objective:
?
Can be hard to compute                   in gradient. 
Idea: use variational approximation
* Note: this is the PR objective *
q ( Y ) ? p?( Y | X )
max? , q (Y) L(?; D L )?D KL
?
q ( Y ) || p?( Y | X )
?
?
?
?
? E q [?( X , Y )] ? ??
?
?
?
?
C ov( ? , f )
O GE = max
?
L (? ; D L )?
?
?
???? E p ?(Y|X) [ ?( X , Y )]
?
?
?
?
Approximating with the mode
PR Objective: 
sometimes minimizing the KL is hard.  
Idea: use hard assignment                               :
?
                                    becomes 
?
                                 becomes 
?
use EM-like procedure to optimize
Constraint Driven Learning Objective:
max? , q ( Y ) L(?; D L )?D KL
?
q ( Y ) || p?( Y | X )
?
?
?
?
? E q [?( X , Y )] ? ??
?
?
?
?
q ( Y ) ? 1 ( Y = ?Y )
?
?
? E q [?( X , Y )] ? ??
?
?
?
?
l og p ( ?Y )D KL
?
q ( Y ) || p?( Y | X )
?
l og p(?? | ?( X , ?Y ))
m ax
?, ?Y
L (? ; D L) + log p ?(
?Y ) + log p (??|?( X , ?Y ))
Visual Summary
Measurements
Generalized
Expectation
Distribution
Matching
Posterior
Regularization
Coupled Semi-
Supervised
Learning
Constraint
Driven
Learning
variational approximation;
Jensen?s inequality
variational
approximation
MAP
approximation
MAP
approximation
logE[ p N (??|?)] ? log p N (??|E[?])
Applications
?
Unstructured problems:
?
Document Classification
?
Sequence problems:
?
Information Extraction
?
Pos-Induction 
?
Word Alignment
?
Tree problems:
?
Grammar Induction
Document Classification
?
Model: Max. Entropy Classifier (Logistic Regression)
?
Challenge: What if we have no labeled data?
?
cannot use standard unsupervised learning:
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
--- --- ----- 
-- -- --- ---- 
--- --- --- --- 
-- ---------  
------- ---- -- 
Documents
Labels
p ?(y|x) =
exp(? ? f(x, y))
?
y
exp(? ? f(x, y))
?
y
p?( y | x )= 1
Labeled Features
?
often we can still provide some light supervision
?
prior knowledge: labeled features
?
formally: have an estimate of the distribution over labels 
for documents that contain word w:
?? w
newsgroups classification
baseball Mac politics
...
hit Apple senate
...
Braves Macintosh taxes
...
runs Powerbook liberal
...
sentiment polarity
positive negative
memorable terrible
perfect boring
exciting mess
Leveraging Labeled Features with GE
[Mann & McCallum 07], [Druck et al 08]
?
constraint feature: 
?
for a document x, returns a vector with a 1 in the lth 
position if y is the lth label and the word w is in x
?
expectation: label distribution for docs that contain w
?
GE penalty: KL divergence from target distribution
? w (x, y) = 1 (y = l )1( w ? x)
1
c w
?
x
E p? ( y | x ) [ ?w(x, y)]
D KL
?
??w||
1
c w
?
x
Ep?(y | x)[?w(x , y )]
?
User Experiments with Labeled Features
[Druck et al 08]
0 100 200 300 400 500 600 700 8000.4
0.5
0.6
0.7
0.8
0.9
1
labeling time in seconds
tes
ting
 ac
cur
acy
 
 
GEER
~2 minutes, 100 
features labeled 
(or skipped): 
82% accuracy
~15 minutes, 100 
documents labeled 
(or skipped):
78% accuracy
PC vs. Mac
complete set of 
labeled features
PC Mac
dos mac
ibm apple
hp quadra
dx
targets set with 
simple heuristic: 
majority label gets 
90% of mass
Experiments with Labeled Features
[Druck et al 08]
60
65
70
75
80
sentiment (50) webkb (100) newsgroups (500)
GE (model contains only labeled features)
GE (model also contains unlabeled features)
15x
3.5x
6.5x
learning about ?unlabeled features? through 
covariance improves generalization
estimated speed-up over 
labeling documents
Information Extraction: Example Tasks
?
citation extraction: 
?
apartment listing extraction: 
Detached single family house. 3 bedrooms 1 1/2 baths.  Almost 
1000 square feet in living area. 1 car garage. New pergo floor 
and tile kitchen floor. New interior/exterior paint. Close to 
shopping mall and bus stop. Near 101/280. Available July 1, 
2004. If you are interested, email for more details.
Cousot, P. and Cousot, R. 1978. Static determination of 
dynamic properties of recursive procedures. In Proceedings of 
the IFIP Conference on Programming Concepts, E. Neuhold, 
Ed. North-Holland Pub. Co., 237-277.
Information Extraction: Markov Models
?
models for sequence labeling based IE
?
Hidden Markov Model (HMM):  
?
Conditional Random Field (CRF):  
p?(y,x) = p?(y0)
N?
i=1
p?(yi|yi?1)p?( x i|yi)
p?(y|x) =
1
Z(x)
exp(
N?
i =1
? ? f (x, yi? 1 , yi))
expectation:
label distribution when q is true
model: Linear Chain CRF
note: Semiring trick makes GE 
O(L
2
) instead of O(L
3
) as in 
[Mann & McCallum 08]
Information Extraction: Labeled Features
[Mann & McCallum 08], [Liang et al 09]
ROOMMATES respectful
CONTACT *phone*
FEATURES laundry
apartments example 
labeled features:
1
c q
?
x
?
i
E p?(yi | x )[?q(x, yi, i )]
constraint features:
vector with a 1 in the lth 
position if y is the lth label 
and predicate q is true (i.e. w 
is present at i)
? q (x, yi , i) = 1 (y i = l)q(x, i)
Information Extraction: Labeled Features
[Haghighi & Klein 06], [Mann & McCallum 08], [Liang et al 09]
apartment listing extraction
Prototype
GE (KL)
Measurements/PR
650
700
750
800
850
0 labeled 10 labeled 100 labeled
supervised CRF (100) [MM08]
?
accurate with constraints alone 
?
outperform fully supervised with 
constraints and labeled data
Limitations of Markov Models
?
predicted: 
?
prediction has two author and two title segments:
?
error #1: Neuhold, Ed. should be editor
?
error #2: North-Holland Pub. Co., should be 
publisher
?
A Markov model cannot represent that at most one segment 
of each type appears in each reference.
Cousot, P. and Cousot, R. 1978. Static determination of 
dynamic properties of recursive procedures. In Proceedings of 
the IFIP Conference on Programming Concepts, E. Neuhold, 
Ed. North-Holland Pub. Co., 237-277.
Long-Range Constraints
[Chang et al 07] [Bellare et al 09]
?
?Each field is a contiguous sequence of tokens and appears 
at most once in a citation.?
?
constraint feature: counts the number of segments of 
each type
?
constrained to be ? 1 using PR or CODL
?
additional constraints: 10 labeled features such as:
?
pages?pages 
?
proc.?booktitle
Long-Range Constraints
[Chang et al 07] [Bellare et al 09]
constraints improve both 
CRF (PR) and HMM (CODL)
50
60
70
80
90
5 labeled 20 labeled
CRF CRF + PR
HMM HMM + CODL
citation model method description
[Mann et al 07] MaxEnt GE
constraints on 
label marginals
[Druck et al 09] CRF GE
actively labeled 
features
[Bellare & 
McCallum 09] 
alignment 
CRF
GE labeled features
[Singh et al 10] 
semi-Markov 
CRF
PR labeled gazetteers 
[Druck et al 10] HMM PR
constraints derived 
from labeled data
Other Applications in 
Information Extraction
Pos Induction
Low Tag Ambiguity
[Gra?a et al 09] 
JJ
VB
NN
car
object
romantic
offensive
being
E[degree] = 1.5E[degree] = 10000  0 2 4 6 8 10  0  200  400  600  800  1000 1200 1400 1600 1800L 1L ! rank of word by L1L!SupervisedHMMDistribution of word ambiguity
N V ADJ Prep ADV
0.9 0.1 0 0 0
0.7 0.1 0.1 0 0.1
0.1 0.3 0 0.6 0
0.3 0.6 0 0 0.1
0.3 0.7 0 0 0
?
Pick a particular word type: run
?
Stack all occurrences
?
Calculate posterior probability 
?
Take the maximum for each tag
?
Sum the maxes
a run into town.
of the mile run.
run gold.
run errands.
run for mayor.
Sum
1
Sum
1
1
1
1
0.9 0.7 0.1 0.6 0.2
Max
Sum
2.5
Measuring Tag Ambiguity
[Gra?a et al 09] 
?wti :Word type w  has hidden state t at occurrence i
m i n
cwt
E q ( y ) [?wti] ? c wt
?1 / ?? =
?
w t
c w t
Tag Sparsity
[Gra?a et al 09] 
0
1.25
2.5
3.75
5
En Pt Es
A
m
b
i
g
u
i
t
y
 
d
i
f
f
e
r
e
n
c
e
HMM L1LMax
Average ambiguity 
difference 0 2 4 6 8 10  0  200  400  600  800  1000 1200 1400 1600 1800L 1L ! rank of word by L1L!SupervisedHMMHMM+SpDistribution of word ambiguity
Results
[Gra?a et al 09] 
50
57.5
65
72.5
80
En Pt Bg Es Dk Tr
HMM HMM+Sp
3.8
6.7
7.4
7.6 9.6
3.8
6.5 % Average Improvement
Word Alignments
[Gra?a et al 10] 
?
Bijectivity constraints:
?
Each word should align to at most one other word
?
Symmetry constraints:
?
Directional models should agree
Bijectivity Constraints
[Gra?a et al 10]
Bijective Constraints
0 1 2 3 4 5 6 7 8
0 ? ? ? ? ? ? ? ? ? jugaban ? ? 1
1 ? ? ? ? ? ? ? ? ? de ? ? 1
2 ? ? ? ? ? ? ? ? ? una ? ? 1
3 ? ? ? ? ? ? ? ? ? manera ? ? 1
4 ? ? ? ? ? ? ? ? ? animada ? ? 1
5 ? ? ? ? ? ? ? ? ? y ? ? 1
6 ? ? ? ? ? ? ? ? ? muy ? ? 1
7 ? ? ? ? ? ? ? ? ? cordial ? ? 1
8 ? ? ? ? ? ? ? ? ?. ? ? 1
it was an animated
, very convivial
game
.
50 / 74
Bijective Constraints - After projection
0 1 2 3 4 5 6 7 8
0 ? ? ? ? ? ? ? ? ? jugaban ? ? 1
1 ? ? ? ? ? ? ? ? ? de ? ? 1
2 ? ? ? ? ? ? ? ? ? una ? ? 1
3 ? ? ? ? ? ? ? ? ? manera ? ? 1
4 ? ? ? ? ? ? ? ? ? animada ? ? 1
5 ? ? ? ? ? ? ? ? ? y ? ? 1
6 ? ? ? ? ? ? ? ? ? muy ? ? 1
7 ? ? ? ? ? ? ? ? ? cordial ? ? 1
8 ? ? ? ? ? ? ? ? ? . ? ? 1
it was an animated
, very convivial
game
.
51 / 74Feature: 
Constraint: 
?(x,y) =
N?
i=1
1(yi = m)
E q [?(x , y)] ? 1
Symmetry Constraints
[Gra?a et al 10]
Feature:
Constraint: 
Sym metric - Original posteriors
0 1 2 3 4
??p ?t (z | x)
0 ? ? ? ? ? no
1 ? ? ? ? ? hay
2 ? ? ? ? ? estad??sticas
3 ? ? ? ? ?.
0 1 2 3 4
??p ?t (z | x)
0 ? ? ? ? ? no
1 ? ? ? ? ? hay
2 ? ? ? ? ? estad??sticas
3 ? ? ? ? ?.
no statistical
data
exists
.
p ? t
q
??
p ? t
??
p ? t
55 / 74
Eq [ ?(x , y)] = 0
?(x, y ) =
?
??
??
+ 1 y ?
??
y and
??
y i = j
? 1 y ?
??
y and
??
y j = i
0 otherwise
??
p ?( y | )
??
p ?( y )
Symmetry Constraints
[Gra?a et al 10]
Before projection: After projection:
Symmetric - After projection
E-Step qs(z) = arg min
q(z)? Q s
KL [qs(z) || p?t (z | xs)]
0 1 2 3 4
??p ?t (z | x)
0 ? ? ? ? ? no
1 ? ? ? ? ? hay
2 ? ? ? ? ? estad??sticas
3 ? ? ? ? ?.
0 1 2 3 4
??p ?t (z | x)
0 ? ? ? ? ? no
1 ? ? ? ? ? hay
2 ? ? ? ? ? estad??sticas
3 ? ? ? ? ?.
no statistical
data
exists
.
0 1 2 3 4
0 ? ? ? ? ? no
??q (z)1
? ? ? ? ? hay
2 ? ? ? ? ? estad??sticas
3 ? ? ? ? ?.
0 1 2 3 4
0 ? ? ? ? ? no
??q (z)1
? ? ? ? ? hay
2 ? ? ? ? ? estad??sticas
3 ? ? ? ? ?.
no statistical
data
exists
.
M-Step Does not change
56 / 74
??
p ?( y |x)
??
p ?( y |x
?
q ( y )
?
q ( y )
Results
[Gra?a et al 10]
 50 60 70 80 90 100 1000  10000  100000  1e+06precision size S-HMMB-HMMHMM 50 60 70 80 90 100 1000  10000  100000  1e+06precision size S-HMMB-HMMHMM
Evolution with data size 50 60 70 80 90 100 1000  10000  100000  1e+06precision size S-HMMB-HMMM4HMM  50 60 70 80 90 100 1000  10000  100000  1e+06precision size S-HMMB-HMMM4HMM
? Specially useful for low data situations
6 1 / 74
Evolution with data size 50 60 70 80 90 100 1000  10000  100000  1e+06precision size S-HMMB-HMMM4HMM  50 60 70 80 90 100 1000  10000  100000  1e+06precision size S-HMMB-HMMM4HMM
? Specially useful for low data situations
6 1 / 74
Results
[Gra?a et al 10]
 60 65 70
 75 80 85
 90 95
En-Pt Pt-En Pt-Fr Fr-Pt En-Es Es-En Es-Fr Fr-Es Pt-Es Es-Pt En-Fr Fr-EnLanguagesHMM70.5 67.5
73.0 77.6 75.7 74.9 80.9 84.0 82.4 79.8 76.3 78.3
B-HMM
85.0 74.4 71.3
86.3 88.4 87.2 87.2 86.5 82.5 90.1 90.8 91.6
S-HMM
86.2 85.0 82.4 87.9 82.7 84.6 89.1 88.9 84.6 91.8 93.4 9
4.6
Dependency Parsing
DMV Model
[Gra?a et al 04]
Dependency model with valence
(Klein and Manning, ACL 2004)
x
y
Regularization
N
creates
V
sparse
ADJ
grammars
N
p?(x, y) = ?root(V )
? ?stop(nostop|V ,right,false) ? ?child(N|V ,right)
? ?stop(stop|V ,right,true) ? ?stop(nostop|V ,left,false) ? ?child(N|V ,left)
. . .
3/9
Dependency Parsing
?
Transfer annotations from another language
?
[Ganchev et al 09]
?
Constrain the number of child/parent 
relations
?
[Gillenwater et al 11]
?
Use linguistic rules
?
[Druck et al 09] [Naseem et al 10]
Dependency Parsing
Transfer annotations
[Ganchev et al 09]
?
Use information from a resource rich 
language
?
Make the annotation transfer robust
?
Preserve n % of the edges
Dependency Parsing
Transfer annotations
[Ganchev et al 09]
E q [?(x,y)] =
1
| C
x
|
?
y? C x
q (y|x)
E q [ ?(x,y)] ? b
Dependency Parsing
Transfer annotations
[Ganchev et al 09]
66
67
68
69
70
ES BG
DMV PR-Transfer
Dependency Parsing
Posterior Sparsity
[Gra?a et al 10]
?
ML learns very ambiguous grammars
?
all productions have some probability
?
constrain the number of possible 
productions
Dependency Parsing
Posterior Sparsity
[Gillenwater et al 11]
Measuring ambiguity on distributions over trees
N
?
N
V
?
N
AD
J
?
N
N
?
V
V
?
V
AD
J
?
V
N
?
AD
J
V
?
AD
J
AD
J
?
AD
J
SparsityN isV workingV
0.40.6 0 1 0
SparsityN isV workingV
0.4 0.6 .4 .6 0
UseV goodADJ grammarsN
0.70.3 0 .7 .3
UseV goodADJ grammarsN
0.40.6 .4 .6 0
max ?
sum = 3.3 ? 0 1 .3 .4 .6 0 .4 .6 0
7/9
Dependency Parsing
Posterior Sparsity
[Gillenwater et al 11]
GILLENWATER, GANCHEV, GRA?A, PEREIRA, TASKAR
Una
d
papelera
nc
es
vs
un
d
objeto
nc
civilizadoaq
Una
d
papelera
nc
es
vs
un
d
objeto
nc
civilizadoaq
1.00
1.00 1.000.49
0.51
1.00
0.57
0.43
Una
d
papelera
nc
es
vs
un
d
objeto
nc
civilizadoaq
1.00 0.83 0.75 0.990.92
0.35
0.48
Figure 14: Posterior edge probabilities for an example sentence from the Spanish test corpus. Top
is Gold, middle is EM, and bottom is PR.
since then it does not have to pay the cost of assigning a parent with a new tag to cover each noun
that does not come with a determiner.
Table 4 contrasts the most frequent types of errors EM, SDP, and PR make on several test sets
where PR does well. The ?acc? column is accuracy and the ?errs? column is the absolute number
of errors of the key type. Accuracy for the key ?parent POS truth/guess? child POS? is computed
as a function of the true relation. So, if the key is pt /p g ? c , then accuracy is:
acc =
# of pt ? c in Viterbi parses
# of pt ? c in gold parses
. (25)
In the following subsections we provide some analysis of the results from Table 4.
7.1 English Corrections
Considering English first, there are several notable differences between EM and PR errors. Similar
to the example for Spanish, the direction of the noun-determiner relation is corrected by PR. This is
reflected by the VB/DT? NN key, the NN/VBZ? DT key, the NN/IN? DT key, the IN/DT?
NN key, the NN/VBD? DT key, the NN/VBP? DT key, and the NN/VB? DT key, which for
EM and SDP have accuracy 0. PR corrects these errors.
A second correction PR makes is reflected in the VB/TO? VB key. One explanation for the
reason PR is able to correctly identify VBs as the parents of other VBs instead of mistakenly making
TO the parent of VBs is that ?VB CC VB? is a frequently occurring sequence. For example, ?build
and hold? and ?panic and bail? are two instances of the ?VB CC VB? pattern from the test corpus.
Presented with such scenarios, where there is no TO present to be the parent of VB, PR chooses the
first VB as the parent of the second. It maintains this preference for making the first VB a parent of
the second when encountered with ?VB TO VB? sequences, such as ?used to eliminate?, because it
would have to pay an additional penalty to make TO the parent of the second VB. In this manner,
PR corrects the VB/TO? VB key error of EM and SDP.
26
Gold:
DVM:
DMV+Sparsity:
Dependency Parsing
Posterior Sparsity
[Gillenwater t al. 11]
0
17.5
35
52.5
70
English Bulgarian Portuguese Checz Spanish German
DMV DMV+Sparsity
Dependency Parsing
Linguistic Rules
[Naseem et al 10]
Using Universal Linguistic Knowledge to Guide Grammar Induction
Tahira Naseem, Harr Chen, Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{ tahira, harr, regina} @csail.mit.edu
Mark Johnson
Department of Computing
Macquarie University
mark.johnson@mq.edu.au
Abstract
We present an approach to grammar induc-
tion that utilizes syntactic universals to im-
prove dependency parsing across a range of
languages. Our method uses a single set
of manually-specified language-independent
rules that identify syntactic dependencies be-
tween pairs of syntactic categories that com-
monly occur across languages. During infer-
ence of the probabilistic model, we use pos-
terior expectation constraints to require that a
minimum proportion of the dependencies we
infer be instances of these rules. We also auto-
matically refine the syntactic categories given
in our coarsely tagged input. Across six lan-
guages our approach outperforms state-of-the-
art unsupervised methods by a significant mar-
gin.1
1 Introduction
Despite surface differences, human languages ex-
hibit striking similarities in many fundamental as-
pects of syntactic structure. These structural corre-
spondences, referred to as syntactic universals, have
been extensively studied in linguistics (Baker, 2001;
Carnie, 2002; White, 2003; Newmeyer, 2005) and
underlie many approaches in multilingual parsing.
In fact, much recent work has demonstrated that
learning cross-lingual correspondences from cor-
pus data greatly reduces the ambiguity inherent in
syntactic analysis (Kuhn, 2004; Burkett and Klein,
2008; Cohen and Smith, 2009a; Snyder et al, 2009;
Berg-Kirkpatrick and Klein, 2010).
1The source code for the work presented in this paper is
available at http://groups.csail.mit.edu/rbg/code/dependency/
Root? Auxiliary Noun? Adjective
Root? Verb Noun? Article
Verb? Noun Noun? Noun
Verb? Pronoun Noun? Numeral
Verb? Adverb Preposition? Noun
Verb? Verb Adjective? Adverb
Auxiliary? Verb
Table 1: The manually-specified universal dependency
rules used in our experiments. These rules specify head-
dependent relationships between coarse (i.e., unsplit)
syntactic categories. An explanation of the ruleset is pro-
vided in Section 5.
In this paper, we present an alternative gram-
mar induction approach that exploits these struc-
tural correspondences by declaratively encoding a
small set of universal dependency rules. As input
to the model, we assume a corpus annotated with
coarse syntactic categories (i.e., high-level part-of-
speech tags) and a set of universal rules defined over
these categories, such as those in Table 1. These
rules incorporate the definitional properties of syn-
tactic categories in terms of their interdependencies
and thus are universal across languages. They can
potentially help disambiguate structural ambiguities
that are difficult to learn from data alone ? for
example, our rules prefer analyses in which verbs
are dependents of auxiliaries, even though analyz-
ing auxiliaries as dependents of verbs is also consis-
tent with the data. Leveraging these universal rules
has the potential to improve parsing performance
for a large number of human languages; this is par-
ticularly relevant to the processing of low-resource
Small set of 
universal rules
= 1 if edge in rule set
E q [ ?(x,y)] ? b
?(x , y)
Dependency Parsing
Linguistic Rules
[Nas em et al 10]
0
20
40
60
80
English Danish Portuguese Slovene Spanish Swedish
DMV DMV+Rules
Dependency Parsing:
Applications using Other Models
?
Tree CRF
?
[Druck et al 09]
?
MST Parser
?
[Ganchev et al 09]
Other Applications
?
Multi view learning:
?
[Ganchev et al 08]
?
Relation extraction:
?
[Chen et al 11]
Implementation Tips and Tricks
Off-the-Shelf Tools: MALLET
http://mallet.cs.umass.edu
?
off-the-shelf support for labeled features
?
models: MaxEnt Classifier, Linear Chain CRF (one and two 
label constraints)
?
methods: GE and PR
?
constraints on label distributions for input features
?
GE penalties:  KL divergence,     (+ soft inequalities)
?
PR penalties:     (+ soft inequalities)
?
in development: Tree CRF,      and other penalties
?
2
2
?
2
2
?1
Off-the-Shelf Tools: MALLET
http://mallet.cs.umass.edu
?
import data in SVMLight-like or CoNLL03-like formats
?
import constraints in a simple text format:
?
easily specify method options (i.e. SimpleTagger):
positive interesting:2 film:1 ...
negative tired:1 sequel:1 ...
positive best:1 recommend:2 ...
U.N.       NNP  B-NP  B-ORG 
official   NN   I-NP  O 
heads      VBZ  B-VP  O 
tired negative:0.8 positive:0.2
best positive:0.9 negative:0.1
U.N. B-ORG:0.7,0.9
B-VP O:0.95,
java cc.mallet.fst.semi_supervised.tui.SemiSupSimpleTagger \
--train true --test lab --loss l2 --learning ge \
unlabeled.txt test.txt constraints.txt
New GE Constraints: MALLET
http://mallet.cs.umass.edu
?
Java Interfaces for implementing new GE constraints
?
covariance computation implemented (MaxEnt, CRF)
?
primarily need to write methods to:
?
restriction: constraints must factor with model
?
restriction: GE objective must be differentiable
compute constraint features and expectations
compute GE objective value
compute GE objective gradient (but not covariance)
New PR Constraints: MALLET
http://mallet.cs.umass.edu
?
Java Interfaces for implementing new PR constraints
?
inference algorithms implemented (MaxEnt, CRF)
?
primarily need to write methods for E-step (projection):
?
restriction: constraints must factor with model
compute constraint features and expectations
compute scores under q for E-step
compute objective function for E-step
compute gradient for E-step
GE Implementation Advice
?
computing covariance (required for gradient): 
?
trick: compute cov. of composite constraint feature
?
example:     penalty: 
?
result: only need to store vectors of size            in 
computation, rather than covariance matrix
?
trick: efficient gradient computation in hypergraphs
?
use semiring algorithms of [Li & Eisner 09] 
?
result: same time complexity as supervised (w. both)
? c (x , y) =
?
?
2( ??? E [?])?(x , y)?22
d i m( f )
GE Implementation Advice
?
parameter regularization: 
?
    regularization encourages bootstrapping by penalizing 
very large parameter values:
?
optimization: non-convex
?
usually L-BFGS still preferable (use ?restart trick?)
?
zero initialization usually works well
?
other init: supervised, MaxEnt, GE in simpler model
?
2
2
>
Off-the-Shelf Tools: PR Toolkit
http://code.google.com/p/pr-toolkit/
?
off-the-shelf support for PR
?
models:  
?
MaxEnt Classifier, HMM,DMV
?
applications:  
?
Word Alignment, Pos Induction, Grammar Induction
?
constraints: posterior sparsity, bijectivity, agreement
?
No command line mode
?
Smaller support base
PR Implementation example:
Word Alignment - Bijectivity
?
Learning: EM, PR
?
void eStep(counts, lattices);
?
void mStep(counts);
?
lattice constraint.project(lattice);
?
Model: HMM
?
lattice computePosteriors(lattice);
?
void addCount(lattice, counts);
?
void updateParameters(counts);
?
Constraints: Bijectivity
?
lattice project(lattice);
PR Implementation example:
EM
class EM {
 model;
 	
void em(n){
 lattices= model.getLattices();
 counts = model.counts();	 	 	
 for(i=0; i< n; i++) {	 	 	
	 eStep(counts, lattices);
	 mStep(counts);
 }
}
	
void eStep(counts, lattices) {	
	 counts.clear();
	 for(l : lattices)  {		 	
	  model.computePosterior(l);
	  model.addCount(l,counts);	
	 }
}	
void mStep(counts) {
	 model.updateParameters(counts);
}
......
}
PR Implementation example:
PR
class PR {
 model;
 constraint;
	
void em(n){
 lattices= model.getLattices();
 counts = model.counts();	 	 	
 for(i=0; i< n; i++) {	 	 	
	 eStep(counts, lattices);
	 mStep(counts);
 }
}
	
void eStep(counts, lattices) {	
	 counts.clear();
	 for(l : lattices){	 	 	
	  model.computePosterior(l);
    constraint.project(l);
	  model.addCount(l,counts);	
	 }
}	
void mStep(counts) {
	 model.updateParameters(counts);
}
......
}
PR Implementation example:
HMM
class HMM {
 obsProb, transProbs,initProbs;
	
lattice computerPosteriors(lattice){
 ?Run forward backward?
}
	
void addCount(lattice,counts){
 ?Add posteriors to count table?
}
void updateParams(counts){
 ?Normalize counts?
 ?Copy counts to params table?
}
void getCounts(){
 ?return copy of params structures?
}
void getLattices(){
 ?return structure of all lattices 
in the corpus?
}
......
}
PR Implementation example:
Bijective constraints
?
Constraint: returns a vector with mth value = number of 
target words in sentence x that align with source word m
?(x,y) =
N?
i=1
1(yi = m) Q = { q : E q [?(x,y)] ? 1}
?
Primal: Hard
D KL ( Q| p?) = arg min
q
D KL ( q |p?)
?
Dual: Easy
arg max
?? 0
? b T ? ?? l og Z (?)? ||?|| 2
Z (?) =
?
y
p?( y |x) exp(?? ? ?(x , y ))
PR Implementation example:
Bijective Constraints
class BijectiveConstraints {
model;
lattice project(lattice){
 obj = BijectiveObj(model,lattice);
 Optimizer.optimize(obj);
}
	
}
class BijectiveObj {
  lattice;
  
void updateModel(newLambda){
 lattice_ = lattice*exp(newLambda);
 computerPosteriors(lattice)
}
double getObj(){
  obj = -dot(lambda,b);
  obj -= lattice.likelihood;
  obj -= l2Norm(lambda);
}
double[] getGrad(){
 grad = lattice.posteriors - b;
 grad -= norm(lambda);
 return grad;
}
Other Software Packages
?
Learning Based Java:  
?
http://cogcomp.cs.illinois.edu/page/software_view/11
?
support for Constraint-Driven Learning
?
Factorie:   
?
http://code.google.com/p/factorie/
?
support for GE and PR in development
Rich Prior Knowledge in Learning for Natural
Language Processing
Bibliography
For a more up-to-date bibliography as well as additional information about
these methods, point your browser to: http://sideinfo.wikkii.com/
1 Constraint-Driven Learning
Constraint driven learning (CoDL) was first introduced in Chang et al [2007],
and has been used also in Chang et al [2008]. A further paper on the topic is
in submission [Chang et al, 2010].
2 Generalized Expectation
Generalized Expectation (GE) constraints were first introduced by Mann and
McCallum [2007] 1 and were used to incorporate prior knowledge about the label
distribution into semi-supervised classification. GE constraints have also been
used to leverage ?labeled features? in document classification [Druck et al, 2008]
and information extraction [Mann and McCallum, 2008, Druck et al, 2009b,
Bellare and McCallum, 2009], and to incorporate linguistic prior knowledge
into dependency grammar induction [Druck et al, 2009a].
3 Posterior Regularization
The most clearly written overview of Posterior Regularization (PR) is Ganchev
et al [2010]. PR was first introduced in Graca et al [2008], and has been
applied to dependency grammar induction [Ganchev et al, 2009, Gillenwater
et al, 2009, 2011, Naseem et al, 2010], part of speech induction [Grac?a et al,
2009a], multi-view learning [Ganchev et al, 2008], word alignment [Graca et al,
2008, Ganchev et al, 2009, Grac?a et al, 2009b], and cross-lingual semantic
alignment [Platt et al, 2010]. The framework was independently discovered
by Bellare et al [2009] as an approximation to GE constraints, under the name
Alternating Projections, and used under that name also by Singh et al [2010]
and Druck and McCallum [2010] for information extraction. The framework
was also independently discovered by Liang et al [2009] as an approximation to
1In Mann and McCallum [2007] the method was called Expectation Regularization.
a Bayesian model motivated by modeling prior information as measurements,
and applied to information extraction.
4 Closely related frameworks
Quadrianto et al [2009] introduce a distribution matching framework very
closely related to GE constraints, with the idea that the model should pre-
dict the same feature expectations on labeled and undlabeled data for a set of
features, formalized as a kernel.
Carlson et al [2010] introduce a framework for semi-supervised learning
based on constraints, and trained with an iterative update algorithm very similar
to CoDL, but introducing only confident constraints as the algorithm progresses.
Gupta and Sarawagi [2011] introduce a framework for agreement that is
closely related to the PR-based work in Ganchev et al [2008], with a slightly
different objective and a different training algorithm.
References
K. Bellare, G. Druck, and A. McCallum. Alternating projections for learning
with expectation constraints. In Proc. UAI, 2009.
Kedar Bellare and Andrew McCallum. Generalized expectation criteria for boot-
strapping extractors using record-text alignment. In EMNLP, pages 131?140,
2009.
Andrew Carlson, Justin Betteridge, Richard C. Wang, Estevam R. Hruschka Jr.,
and Tom M. Mitchell. Coupled Semi-Supervised Learning for Information
Extraction. In Proceedings of the Third ACM International Conference on
Web Search and Data Mining (WSDM), 2010.
M. Chang, L. Ratinov, and D. Roth. Guiding semi-supervision with constraint-
driven learning. In Proc. ACL, 2007.
Ming-Wei Chang, Lev Ratinov, and Dan Roth. Structured learning with con-
strained conditional models. 2010. In submission.
M.W. Chang, L. Ratinov, N. Rizzolo, and D. Roth. Learning and inference
with constraints. In Proceedings of the National Conference on Artificial
Intelligence (AAAI). AAAI, 2008.
G. Druck, G. Mann, and A. McCallum. Learning from labeled features using
generalized expectation criteria. In Proc. SIGIR, 2008.
G. Druck, G. Mann, and A. McCallum. Semi-supervised learning of dependency
parsers using generalized expectation criteria. In Proc. ACL-IJCNLP, 2009a.
Gregory Druck and Andrew McCallum. High-performance semi-supervised
learning using discriminatively constrained generative models. In Proceedings
of the International Conference on Machine Learning (ICML 2010), pages
319?326, 2010.
Gregory Druck, Burr Settles, and Andrew McCallum. Active learning by label-
ing features. In EMNLP, pages 81?90, 2009b.
K. Ganchev, J. Grac?a, J. Blitzer, and B. Taskar. Multi-view learning over
structured and non-identical outputs. In Proc. UAI, 2008.
K. Ganchev, J. Gillenwater, and B. Taskar. Dependency grammar induction via
bitext projection constraints. In Proc. ACL-IJCNLP, 2009.
Kuzman Ganchev, Joo Graa, Jennifer Gillenwater, and Ben Taskar. Posterior
sparsity in unsupervised dependency parsing. Journal of Machine Learn-
ing Research, 11:2001?2049, July 2010. URL http://jmlr.csail.mit.edu/
papers/v11/ganchev10a.html.
Jennifer Gillenwater, Kuzman Ganchev, Joo Graa, Ben Taskar, and Fernando
Pereira. Sparsity in grammar induction. In NIPS Workshop on Grammar
Induction, Representation of Language and Language Learning, 2009.
Jennifer Gillenwater, Kuzman Ganchev, Joo Graa, Fernando Pereira, and Ben
Taskar. Posterior sparsity in unsupervised dependency parsing. Journal of
Machine Learning Research, 12:455?490, February 2011. URL http://jmlr.
csail.mit.edu/papers/v12/gillenwater11a.html.
Joao Graca, Kuzman Ganchev, and Ben Taskar. Expectation maximization
and posterior constraints. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis,
editors, Advances in Neural Information Processing Systems 20, pages 569?
576. MIT Press, Cambridge, MA, 2008.
J. Grac?a, K. Ganchev, F. Pereira, and B. Taskar. Parameter vs. posterior
sparisty in latent variable models. In Proc. NIPS, 2009a.
J. Grac?a, K. Ganchev, and B. Taskar. Postcat - posterior constrained alignment
toolkit. In The Third Machine Translation Marathon, 2009b.
Rahul Gupta and Sunita Sarawagi. Joint training for open-domain extraction
on the web: exploiting overlap when supervision is limited. In Proceedings of
the Fourth ACM International Conference on Web Search and Data Mining
(WSDM), 2011.
P. Liang, M. I. Jordan, and D. Klein. Learning from measurements in exponen-
tial families. In Proc. ICML, 2009.
G. S. Mann and A. McCallum. Simple, robust, scalable semi-supervised learning
via expectation regularization. In Proc. ICML, 2007.
G. S. Mann and A. McCallum. Generalized expectation criteria for semi-
supervised learning of conditional random fields. In Proc. ACL, 2008.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark Johnson. Using uni-
versal linguistic knowledge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Natural Language Processing,
pages 1234?1244, Cambridge, MA, October 2010. Association for Computa-
tional Linguistics. URL http://www.aclweb.org/anthology/D10-1120.
John Platt, Kristina Toutanova, and Wen-tau Yih. Translingual document rep-
resentations from discriminative projections. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language Processing, pages 251?261,
Cambridge, MA, October 2010. Association for Computational Linguistics.
URL http://www.aclweb.org/anthology/D10-1025.
Novi Quadrianto, James Petterson, and Alex Smola. Distribution matching for
transduction. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams,
and A. Culotta, editors, Advances in Neural Information Processing Systems
22, pages 1500?1508. MIT Press, 2009.
Sameer Singh, Dustin Hillard, and Chris Leggetter. Minimally-supervised ex-
traction of entities from text advertisements. In Human Language Tech-
nologies: The 2010 Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages 73?81, Los Angeles,
California, June 2010. Association for Computational Linguistics. URL
http://www.aclweb.org/anthology/N10-1009.
