Proceedings of the Linguistic Annotation Workshop, pages 49?52,
Prague, June 2007. c?2007 Association for Computational Linguistics
 
Criteria for the Manual Grouping of Verb Senses  Cecily Jill Duffield, Jena D. Hwang, Susan Windisch Brown,  Dmitriy Dligach, Sarah E.Vieweg, Jenny Davis, Martha Palmer Departments of Linguistics and Computer Science University of Colorado Boulder, C0 80039-0295, USA {cecily.duffield, hwangd, susan.brown, dmitry.dligach,  sarah.vieweg, jennifer.davis, martha.palmer}@colorado.edu  
Abstract  In this paper, we argue that clustering WordNet senses into more coarse-grained groupings results in higher inter-annotator agreement and increased system performance. Clustering of verb senses involves examining syntactic and semantic features of verbs and arguments on a case-by-case basis rather than applying a strict methodology. Determining appropriate criteria for clustering is based primarily on the needs of annotators.  1  Credits  We gratefully acknowledge the support of the National Science Foundation Grant NSF-0415923, Word Sense Disambiguation, and the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-C-0022, a subcontract from the BBN-AGILE Team.  2  Introduction  Word sense ambiguity poses significant obstacles to accurate and efficient information extraction and automatic translation. Successful disambiguation of polysemous words in NLP applications depends on determining an appropriate level of granularity of sense distinctions, perhaps more so for distinguishing between multiple senses of verbs than for any other grammatical category. WordNet, an important and widely used lexical resource, uses fine-grained distinctions that provide subtle information about the particular usages of various 
lexical items (Felbaum, 1998). When used as a resource for annotation of various genres of text, this fine level of granularity has not been conducive to high rates of inter-annotator agreement (ITA) or high automatic tagging performance. Annotation of verb senses as described by coarse-grained Proposition Bank framesets may result in higher ITA scores, but the blurring of distinctions between verb senses with similar argument structures may fail to alleviate the problems posed by ambiguity. Our goal in this project is to create verb sense distinctions at a middle level of granularity that allow us to capture as much information as possible from a lexical item while still attaining high ITA scores and high system performance in automatic sense disambiguation. We have demonstrated that clear sense distinctions improve annotator productivity and accuracy. System performance typically lags around 10% behind ITA rates. ITA scores of at least 90% for a majority of our sense-groupings result in the expected corresponding improvement in system performance. Training on this new data, Chen et al, (2006) report 86.7% accuracy for verbs using a smoothed maximum entropy model and rich linguistic features. (Also Semeval071) They also report state-of-the-art performance on fine-grained senses, but the results are more than 16% lower. We begin by describing the overall process.  3  The Grouping and Annotation Process  The process for building our database with the appropriate level of verb sense distinctions                                                 1 Task 17,  http://nlp.cs.swarthmore.edu/semeval/.  
49
  
involves two steps: sense grouping and annotation (Figure 1). During our sense grouping process, linguists (henceforth, ?groupers?) cluster fine-grained sense distinctions listed in WordNet 2.1 into more coarse-grained groupings. These rough clusters of WordNet entries are based on speaker intuition. Other resources, including PropBank, VerbNet (based on Levin?s verb classes (Levin, 1993)), and online dictionaries are consulted in further refining the distinctions between senses (Palmer, et. al., 2005, Kipper et al, 2006). To aid annotators in understanding the distinctions, sense groupings are ordered according to saliency and frequency. Detailed information, including syntactic frames and semantic features, is provided as commentary for the groupings. We also provide the annotators with simple example sentences from WordNet as well as syntactically complex and ambiguous attested usages from Google search results. These examples are intended to guide annotators faced with similar challenges in the data to be tagged.  Completed verb sense groupings are sent through sample-annotation and tagged by two annotators. Groupings that receive an ITA score of 90% or above are then used to annotate all instances of that verb in our corpora in actual-annotation. Groupings that receive less than 90% ITA scores are regrouped (Hovy et al, 2006). Revisions are made based on a second grouper?s evaluation of the original grouping, as well as patterns of annotator disagreement. Verb groupings receiving ITA scores of 85% or above are sent through actual-annotation. Verbs scoring below 85% are regrouped by a third grouper, and in some cases, by the entire grouping team. It is sometimes impossible to get ITA scores over 85% for high   
frequency verbs that also have high entropy. These have to be carefully adjudicated to produce a gold standard. Revised verbs are then evaluated and either deemed ready for actual-annotation or are sent for a third and final round of sample-annotation. Verbs subject to the re-annotation process are tagged by different annotators. Data from actual-annotation is examined by an adjudicator who resolves remaining disagreements between annotators. The adjudicated data is then used as the gold standard for automatic annotation. The final versions of the sense groupings are mapped to VerbNet and FrameNet and linked to the Omega Ontology (Philpot et al, 2005).  Verbs are selected based on frequency of appearance in the WSJ corpus. As the most frequent verbs are also the most polysemous, the number of sense distinctions per verb as well as the number of instances to be tagged decreases as the project continues. The 740 most frequent verbs in the WSJ corpus were grouped in order of frequency. They have an average polysemy of 7 senses in WordNet; our sense groups have reduced the polysemy to 3.75 senses. Of these, 307 verb groupings have undergone regrouping to some extent. A total of 670 verbs have completed actual-annotation and adjudication. The next 660 verbs have been divided into rough semantic domains based on VerbNet classes, and grouping will proceed according to these semantic domains rather than by verb frequency. As groupers create sense groupings for new verbs, old verb sense groupings in the same semantic domain are consulted. This organization allows for more consistent grouping methodologies, as well as more efficiency in integrating our sense groupings into the Ontology.  
 Figure 1:  The grouping and annotation process.  
50
  
4  Grouping Methodology  Various criteria are considered when disambiguating senses and creating sense groupings for the verbs, including frequent lexical usages and collocations, syntactic features and alternations, and semantic features, similarly to Senseval2 (Palmer, et. al. 2006). Because these criteria do not apply uniformly to every verb, groupers take various approaches when creating sense groupings. Groupers recognize that there are many alternate ways to cluster senses at this level of granularity; each grouping represents only one possible clustering as a middle ground between PropBank and WordNet senses for each verb. Our highest priority is to then create clear distinctions among sense groupings that will be easily understood by the annotators and consequently result in high ITA scores. Initial clustering is based on groupers? intuitions of the most salient categories. Many verb groupings, such as that for the verb kill, provide little detailed syntactic or semantic analysis and yet have received high ITA scores. The success of these intuitive sense groupings is not due to lack of polysemy; kill has 15 WordNet senses and 2 multi-word expressions clustered into 9 sense groupings, yet it received 94% ITA in first round sample-annotation.  While annotators have little trouble tagging text with verb senses that fall neatly into intuitive categories, many verbs have fine-grained WordNet senses that fall on a continuum between two distinct lexical usages. In such cases, syntactic and semantic aspects of the verb and its arguments help groupers cluster senses in such a way that annotators can make consistent decisions in tagging the text. 
Syntactic criteria: Annotators have found syntactic frames, such as those defining VerbNet classes, to be useful in understanding boundaries between sense groupings. For example, split was originally grouped with consideration for the units resulting from a splitting event (i.e. whether a whole unit had been split into incomplete portions of the whole, or into smaller, but complete, individual units.)  This grouping proved difficult for annotators to distinguish, with an ITA of 42%. Using the causative/inchoative alternation for verbs in the ?break-45.1? class to regroup resulted in higher consistency among annotators, increasing the ITA score to 95%. Semantic criteria: When senses of a verb have similar syntactic frames, and usages fall along a continuum between these senses, semantic features of the arguments, or less often, of the verb itself, can clarify these senses and help groupers draw clear distinctions between them. Argument features that are considered when creating sense groupings include [+/-attribute], [+/-patient], and [+/-locative]. It is most common for groupers to mark these features on nominal arguments, but a prepositional phrase may also be described in semantic terms. Semantic features of the verb that are considered include aspectual features, as illustrated by the use of [+/-punctual] in sense groupings for make (Figure 2). However, it may be argued that this feature is unnecessary for annotators to be able to distinguish between the sense groupings, as the prepositional phrase in sense 9 is a more salient feature for annotators. Other features of the verb that were used earlier in the project include concrete/abstract, continuative, stative, and others. However, these features proved less useful than those Sense group Description and Commentary WordNet 2.1 senses Examples 8 Attain or reach something desired NP1[+agent] MAKE[+punctual] NP2[desired goal, destination, state] This sense implies the goal has been met. Includes: MAKE IT 
make 13, 22, 38 - He made the basketball team. - We barely made the plane. - I made the opening act in plenty of time. - Can you believe it? We made it!  
9 Move toward or away from a location NP1[+agent] MAKE[-punctual] (pronoun+way) PP/INFP  
make 30, 37 make off 1 make way 1  
- As the enemy approached our town, we made for the hills. - He made his way carefully across the icy parking lot. - They made off with the jewels. Figure 2: Sense groupings 8 and 9 for ?make.? Senses are distinguished in part by aspectual features marked on the verb.  
51
 described above, and annotators not familiar with linguistic theory found them to be confusing. Therefore, they are now rarely used to label sense groupings. Such concepts, when used, are more likely to be described in prose commentary for the sake of the annotators. Certain compositional features of verbs have also proven to be confusing for annotators. In several cases, attempts to distinguish sense groupings based on manner and path have resulted in increased annotator disagreement. In the first attempt at grouping roll, syntactic and semantic information, as well as prose commentary, was presented to help annotators distinguish the manner and path sense groupings. Despite this, the admissibility of certain prepositions in both senses (?The baby rolled over,? vs ?She rolled over to the wall,?) may have blurred the distinction. In two rounds of sample-annotation, the greatest number of disagreements occurred with respect to these two senses for roll, which were then merged in the final version of the sense groupings.  5  Conclusion  Building on results in grouping fine-grained WordNet senses into more coarse-grained senses that led to improved inter-annotator agreement (ITA) and system performance (Palmer et al, 2004; Palmer et al, 2007), we have developed a process for rapid sense inventory creation and annotation of verbs that also provides critical links between the grouped word senses and the ontology (Philpot et al, 2005). This process is based on recognizing that sense distinctions can be represented by linguists in a hierarchical structure, that is rooted in very coarse-grained distinctions which become increasingly fine-grained until reaching WordNet (or similar) senses at the leaves. Sets of senses under specific nodes of the tree are grouped together into single entries, along with the syntactic and semantic criteria for their groupings, to be presented to the annotators. Criteria are applied on a case-by-case basis, considering syntactic and semantic features as consistently as possible when grouping verbs in similar semantic domains as defined by VerbNet. By using this approach when creating sense groupings, we are 
able to provide annotators with clear and reliable descriptions of senses, resulting in improved accuracy and performance.  References Chen, J., A. Schein, L. Ungar and M. Palmer. 2006. An Empirical Study of the Behavior of Word Sense Disambiguation. Proceedings of HLT-NAACL 2006. New York, NY. Fellbaum, C. (ed.) 1998. WordNet: An On-line Lexical Database and Some of its Applications. MIT Press, Cambridge, MA. Kipper, K., A. Korhonen, N. Ryant, and M. Palmer. 2006. Extensive Classifications of English Verbs. Proceedings of the 12th EURALEX International Congress. Turin, Italy. Levin, B. 1993. English Verb Classes and Alternations. The University of Chicago Press, Chicago, IL. OntoNotes, 2006. Hovy, E.H., M. Marcus, M. Palmer, S. Pradhan, L. Ramshaw, and R. Weischedel. 2006. OntoNotes: The 90% Solution. Short paper. Proceedings of HLT-NAACL 2006. New York, NY. Palmer, M., O. Babko-Malaya, and H.T. Dang. 2004. Different Sense Granularities for Different Applications. Proceedings of the 2nd Workshop on Scalable Natural Language Understanding Systems (HLT-NAACL 2004). Boston, MA.  Palmer, M., Dang, H.T., and Fellbaum, C., Making Fine-grained and Coarse-grained sense distinctions, both manually and automatically, Journal of Natural Language Engineering (to appear, 2007). Palmer, M., Gildea, D., Kingsbury, P., The Proposition Bank: A Corpus Annotated with Semantic Roles, Computational Linguistics Journal, 31:1, 2005. Philpot, A., E.H. Hovy, and P. Pantel. 2005. The Omega Ontology. Proceedings of the ONTOLEX Workshop at the International Conference on Natural Language Processing (IJCNLP). Jeju Island, Korea.  
52
Proceedings of the NAACL HLT Workshop on Extracting and Using Constructions in Computational Linguistics, pages 1?8,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
 
Towards a Domain Independent Semantics: Enhancing Semantic Representation with Construction Grammar   Jena D. Hwang1,2 Rodney D. Nielsen1 Martha Palmer1,2   1Ctr. for Computational Language and Education Research University of Colorado at Boulder Boulder, CO 80302 2Department of Linguistics University of Colorado at Boulder Boulder, CO 80302 {hwangd,rodney.nielsen,martha.palmer}@colorado.edu     Abstract 
In Construction Grammar, structurally patterned units called constructions are assigned meaning in the same way that words are ?  via convention rather than composition. That is, rather than piecing semantics together from individual lexical items, Construction Grammar proposes that semantics can be assigned at the construction level. In this paper, we investigate whether a classifier can be taught to identify these constructions and consider the hypothesis that identifying construction types can improve the semantic interpretation of previously unseen predicate uses. Our results show that not only can the constructions be automatically identified with high accuracy, but the classifier also performs just as well with out-of-vocabulary predicates.  1 Introduction The root of many challenges in natural language processing applications is the fact that humans can convey a single piece of information in numerous and creative ways. Syntactic variations (e.g. I gave him my book. vs. I gave my book to him.), the use of synonyms (e.g. She bought a used car. vs. She purchased a pre-owned automobile.) and numerous other variations can complicate the semantic analysis and the automatic understanding of a text.  Consider the following sentence.  (1) They hissed him out of the university  While (1) is clearly understandable for humans, to automatically discern the meaning of hissed in this 
instance would take more than learning that the verb hiss is defined as ?make a sharp hissing sound? (WordNet 3.0). Knowing that hiss can also mean ?a show of contempt? is helpful. However, it would also require the understanding that the sentence describes a causative event if we are to interpret this sentence as meaning something like ?They caused him to leave the university by means of hissing or contempt?. The problem of novel words, expressions and usages are especially significant because discriminative learning methods used for automatic text classification do not perform as well when tested on text with a feature distribution that is different from what was seen in the training data. This is recognized to be a critical issue in domain adaptation (Ben-David et. al, 2006). Whether we seek to account for words or usages that are infrequent in the training data or to adapt a trained classifier to a new domain of text that includes new vocabulary or new forms of expressions, success in overcoming these challenges partly lies in the successful identification and the use of features that generalize over linguistic variation.  In this paper we borrow from the theories presented by Construction Grammar (CxG) to explore the development of general features that may help account for the linguistic variability and creativity we see in the data.  Specifically, we investigate whether a classifier can be taught to identify constructions as described by CxG and gauge their value in interpreting novel words. The development of approaches to effectively capture such novel semantics will enhance applications requiring richer representations of language understanding such as machine 
1
 translation, information retrieval, and text summarization. Consider, for instance, the following machine translation into Spanish by the Google translate (http://translate.google.com/):   They hissed him out of the university. ? Silbaban fuera de la universidad. Tr. They were whistling outside the university.1  The translation has absolutely no implication that a group of people did something to cause another person to leave the university. However, when the verb is changed to a verb that is seen to frequently appear in a caused motion interpretation (e.g. throw), the results are correct:  They threw him out of the university. ? Lo sacaron de la universidad. Tr. They took him out of the university.  Thus, if we could facilitate a caused motion interpretation by bootstrapping semantics from constructions (e.g. ?X ___ Y out of Z? implies caused motion), we could enable accurate translations that otherwise would not be possible. 2 Current Approaches In natural language processing (NLP), the issue of semantic analysis in the presence of lexical and syntactic variability is often perceived as the purview of either word sense disambiguation (WSD) or semantic role labeling (SRL) or both. In the case of WSD, the above issue is often tackled through the use of large corpora tagged with sense information to train a classifier to recognize the different shades of meaning of a semantically ambiguous word (Ng and Lee, 2006; Agirre and Edmonds, 2006).  In the case of SRL, the goal is to identify each of the arguments of the predicate and label them according to their semantic relationship to the predicate (Gildea and Jurafsky, 2002).   There are several corpora available for training WSD classifiers such as WordNet?s SemCor (Miller 1995; Fellbaum 1998) and the GALE OntoNotes data (Hovy et. al., 2006). However, most, if not all, of these corpora include only a small fraction of all English predicates. Since WSD systems train separate classifiers for each                                                 1 We have hand translated the Google translation back to English for comparison. 
predicate, if a particular predicate does not exist in the sparse training data, a system cannot create an accurate semantic interpretation. Even if the predicate is present, the appropriate sense might not be. In such a case, the WSD will again be unable to contribute to a correct overall semantic interpretation. This is the case in example (1), where even the extremely fine-grained sense distinctions provided by WordNet do not include a sense of hiss that is consistent with the caused motion interpretation rendered in the example. Available for SRL tasks are efforts such as PropBank (Palmer et al, 2005) and FrameNet (Fillmore et al, 2003) that have developed semantic role labels (based on differing approaches) and have labeled large corpora for training and testing of SRL systems. PropBank (PB) identifies and labels the semantic arguments of the verb on a verb-by-verb basis, creating a separate frameset that includes verb specific semantic roles to account for each subcategorization frame of the verb. Much like PB, FrameNet (FN) identifies and labels semantic roles, known as Frame Elements, around a relational target, usually a verb.2 But unlike PB, Frame Elements less verb specific, but rather are defined in terms of semantic structures called frames evoked by the verb. That is, one or more verbs can be associated with a single semantic frame. Currently FN has over 2000 distinct Frame Elements.  The lexical resource VerbNet (Kipper-Schuler, 2005) details semantic classes of verbs, where a class is composed of verbs that have similar syntactic realizations, following work by Levin (1993). Verbs are grouped by their syntactic realization or frames, and each frame is associated with a meaning. For example, the verbs loan and rent are grouped together in class 13.1 with roughly a ?give? meaning, and the verbs deposit and situate are grouped into 9.1 with roughly a ?put? meaning.  Although differing in the nature of their tasks, WSD and SRL systems both treat lexical items as the source of meaning in a clause. In WSD, for every sense we need a new entry in our dictionary to be able to interpret the sentence. With SRL, we                                                 2 PropBank labels Arg0 and Arg1, for the most part, correspond to Dowty?s Prototypical Agent and Prototypical Patient, respectively, providing important generalizations. 
2
 need the semantic role labels that describe the predicate argument relationships in order to extract the meaning.  In either case, we are still left with the same issue ? if the meaning lies in the lexical items, how do we interpret unseen words and novel lexical usages? As shown in the CoNLL-2005 shared task (Carreras and Marquez, 2005), system performance numbers drop significantly when a classifier, trained on the Wall Street Journal (WSJ) corpus, is tested on the Brown corpus. This is largely due to the ?highly ambiguous and unseen predicates (i.e. predicates that do not have training examples)? (Giuglea and Moschitti, 2006). 3 Construction Grammar This issue of scalability and generalizability across genres could possibly be improved by linking semantics more directly with syntax, as theorized by Construction Grammar (CxG) (Fillmore et. al., 1988; Golderg, 1995; Kay, 2002; Michaelis, 2004; Goldberg, 2006). This theory suggests that the meaning of a sentence arises not only from the lexical items but also from the patterned structures or constructions they sit in. The meaning of a given phrase, a sentence, or an utterance, then, arises from the combination of lexical items and the syntactic structure in which they are found, including any patterned structural configurations (e.g. patterns of idiomatic expressions such as ?The Xer, the Yer? ? The bigger, the better) or recurring structural elements (e.g. function words such as determiners, particles, conjunctions, and prepositions). That is, instead of focusing solely on the semantic label of words, as is done in SRL and in many traditional theories in Linguistics, CxG brings more into focus the interplay of lexical items and syntactic forms or structural patterns as the source of meaning.  3.1 Application of Construction Grammar Thus, rather than just assigning labels at the level of lexical items and predicate arguments as a way of piecing together the meaning of a sentence, we follow the central premise of CxG. Specifically, that semantics can be and should be interpreted at the level of the larger structural configuration.  Consider the following three sentences, each having the same syntactic structure, each taken 
from different genres of writing available on the web.  Blogger arrested - blog him out of jail! [Blog] Someone mind controlled me off the cliff. [Gaming] He clocked the first pitch into center field. [Baseball]  Each of these sentences makes use of words, especially the verb, in ways particular to their genre. Even if we are unfamiliar with the specific jargon used, as a human we can infer the general meaning intended by each of the three sentences: a person X causes an entity Y to move in the path specified by the prepositional phrase (e.g. third sentence: ?A player causes something to land in the center field.?).   In a similar way, if we can assign a meaning of caused motion at the sentence level and an automatic learner can be trained to accurately identify the construction, then even when presented with an unseen word, a useful semantic analysis is still possible. 3.2 Caused-Motion Construction For this effort, we focused on the caused-motion construction, which can be defined as having the coarse-grained syntactic structure of Subject Noun Phrase followed by a verb that takes both a Noun Phrase Object and a Prepositional Phrase: (NP-SBJ (V NP PP)); and the semantic meaning ?the agent, NP-SBJ, directly causes the patient, NP, to move along the path specified by the PP? (Goldberg 1995). This construction is exemplified by the following sentences from (Goldberg 1995):  (2) Frank sneezed the tissue off the table. (3) Mary urged Bill into the house. (4) Fred stuffed the papers in the envelope. (5) Sally threw a ball to him.  However, not all syntactic structures of the form (NP-SBJ (V NP PP)) belong to the caused-motion construction. Consider the following sentences.  (6) I considered Ben as one of my brothers. (7) Jen took the highway into Pennsylvania. (8) We saw the bird in the shopping mall. (9) Mary kicked the ball to my relief.  In (6) and (9), the PPs do not specify a location, a direction or a path. In (8), the PP is a location; 
3
 however, the PP indicates the location in which the ?seeing? event happened, not a path along which ?we? caused ?the bird? to move.  Though the PP in (7) expresses a path, it is not a path in which Jen causes ?the highway? to move. 3.3 Goals As an initial step in determining the usefulness of construction grammar for interpreting semantics in computational linguistics, we present the results of our study aimed at ascertaining if a classifier can be taught to identify caused-motion constructions. We also report on our investigations into which features were most useful in the classification of caused-motion constructions.  4 Data & Experiments The data for this study was pulled from the WSJ part of Penn Treebank II (Marcus et al, 1994). From this corpus, all sentences with the syntactic form (NP-SBJ (V NP PP)) were selected. The selection allowed for intervening adverbial phrases (e.g. ?Sally threw a ball quickly to him?) and additional prepositional phrases (e.g. ?Sally threw a ball to him on Tuesday? or ?Sally threw a ball in anger into the scorer?s table?). A total of 14.7k instances3 were identified in this manner. To reduce the size of the corpus to be labeled to a target of 1800 instances, we removed, firstly, instances containing traces as parsed by the TreeBank. These included passive usages (e.g. ?Coffee was shipped from Colombia by Gracie?) and instances with traces in the object NP or PP including questions and relative clauses (e.g. ?What did Gracie ship from Colombia??). In construction grammar, however, traces do not exist, since grammar is a set of patterns of varying degrees of complexity. Thus CxG would characterize passives, questions structures, and relative clauses as having their own respective phrasal constructions, which combine with the caused-motion construction. In order to ensure sufficient training data with the standard form of the caused-motion construction as defined in Goldberg 1995 and 2006 (see Section 3.2), we                                                 3 We use the term instances over sentences since a sentence can have more than one instance. For example, the sentence ?I gave the ball to Bill, and he kicked it to the wall.? is composed of 2 instances. 
chose to remove these usages.  Secondly, we removed the instances of sentences that can be deterministically categorized as non-caused motion constructions: instances containing ADV, EXT, PRD, VOC, or TMP type object NPs (e.g.?Cindy drove five hours from Dallas?, ?You listen, boy, to what I say!?). Because we can automatically identify this category, keeping these examples in our data would have resulted in even higher performance. We also considered the possibility of reducing the size by removing certain classes of verbs such as verbs of communication (e.g. reply, bark), psychological state (e.g. amuse, admire), or existence (e.g. be, exist). While it is reasonable to say that these verb types are highly unlikely to appear in a caused-motion construction, if we were to remove sets of verbs based on their likely behavior, we would also be excluding interesting usages such as ?The stand-up comedian amused me into a state of total enjoyment.? or ?The leader barked a command into a radio.? After filtering these sentences, 8700 remained. From the remaining instances, we selected 1800 instances at random for the experiments presented. 4.1 Labels and Classifier The 1800 instances were hand-labeled with one of the following two labels:   - Caused-Motion (CM)  - Non Caused-Motion (NON-CM)  The CM label included both literal usages (e.g. ?Well-wishers stuck little ANC flags in their hair.?) and non-literal usages (e.g. ?Producers shepherded ?Flashdance? through several scripts.?) of caused-motion. After the annotation, the corpus was randomly divided into two sets: 75% for training data and 25% for testing data. The distribution of the labels in the test data is 33.3% CM and 66.7% NON-CM. The distribution in the training set is 31.8% CM and 68.2% NON-CM. For our experiments, we used a Support Vector Machine (SVM) classifier with a linear kernel. In particular we made use of the LIBSVM (Chang and Lin, 2001) as training and testing software. 
4
 4.2 Baseline Features The baseline consisted of a single conceptual feature - the lemmatized, case-normalized verb. We chose the verb as a baseline feature because it is generally accepted to be the core lexical item in a sentence, which governs the syntactic structure and semantic constituents around it. This is especially evidenced in the Penn Treebank where NP nodes are assigned with syntactic labels according to the position in the tree relative to the verb (e.g. Subject). In VerbNet and PropBank, the semantic labels are assigned to the constituents around the verb, each according to its semantic relationship with the verb.   This verb feature was encoded as 478 binary features (one for each unique verb in the dataset), where the feature value corresponding to the instance?s verb was 1 and all others were 0. 4.3 Additional Features In the present experiments, we utilize gold-standard values for two of the PP features for a proof of feasibility. Future work will evaluate the effect of automatically extracting these features. In addition to the baseline verb feature (feature 1), our full feature set consisted of 8 additional types for a total of 334 features. Examples used in the feature descriptions are pulled from our data.  PP features:  2. Preposition (76 features) The preposition heading the prepositional phrase (e.g. ?Producers shepherded ?Flashdance? [[through]P several scripts]PP.?) was encoded as 76 binary features, one per preposition type in the training data. For instances with multiple PPs, preposition features were extracted from each of the PPs. 3. Function Tag on PP (11 features) Penn Treebank encodes grammatical, adverbial, and other related information on the PP?s POS tag (e.g. ?PP-LOC?). The function tag on the prepositional phrase was encoded as 10 binary features plus an extra feature for PPs without function tags. Again, for instances with multiple PPs, each corresponding function tag feature was set to 1. 4. Complement Category to P (19 features) Normally a PP node consists of a P and a NP. 
However, there are some cases where the complement of the P can be of a different syntactic category (e.g. ?So, view permanent insurance [[for]P [what it is]SBAR]PP.?). Thus, the phrasal category tags (e.g. NP, SBAR) of the preposition?s sister nodes were encoded as 19 binary features. For instances with multiple PPs, all sister nodes of the prepositions were collected.  VerbNet features: The following features were automatically extracted from VerbNet classes with frames matching the target syntactic structure, namely ?NP V NP PP?.  5. VerbNet Classes (123 features) The verbs in the data were associated with one or more of the above VerbNet classes according to their membership. The VerbNet classes were then encoded as 122 binary features with one additional feature for verbs that were not found to be members of any of these classes. If a verb belongs to multiple matching classes, each corresponding feature was set. 6. VerbNet PP Type (27 features) VerbNet frames associate the PP with a description (e.g. ?NP V NP PP.location?). The types were encoded as 26 binary features, plus an extra feature for PPs without a description. The features represented the union of all PP types (i.e. if a VerbNet class included multiple PPs, each of the corresponding features was assigned a value of 1). If a verb was associated with multiple VerbNet classes, the features were set according to the union over both the corresponding classes and their set of PP types.  Named Entity features: These features were automatically annotated using BBN?s IdentiFinder (Bikel, 1999). The feature counts for the subject NP and object NP differ strictly due to what entities were represented in the data. For example, the entity type ?DISEASE? was found in an object NP position but not in a subject NP. 7. NEs for Subject NP (23 features) The union of all named entities under the NP-SBJ node was encoded as 23 binary features.  8. NEs for Object NP (27 features) The union of all named entities under the object NP node was encoded as 27 binary features.  9. NEs for PP?s Object (28 features) The union 
5
 of all named entities under the NP under the PP node was encoded as 28 binary features. 5 Results For the baseline system, the model was built from the training data using a linear kernel and a cost parameter of C=1 (LIBSVM default value). When using the full feature set, the model was also built from the training data using a linear kernel, but the cost parameter was C=0.5, the best value from 10-fold cross validation on the training data.  In Table 1, we report the precision (P), recall (R), F1 score, and accuracy (A) for identifying caused-motion constructions4.  Features P% R% F A% Baseline* Set 78.0 52.0 0.624 79.1 Full Set 87.2 86.0 0.866 91.1 Table 1: System Performance (*verb feature baseline) The results show that the addition of the features presented in section 4.3 resulted in a significant increase in both precision and recall, which in turn boosted the F score from 0.624 to 0.857, an increase of 0.233.  6 Feature Performance In order to determine the usefulness of the individual features in the classification of caused-motion, we evaluated the features in two ways. In one (Table 2), we compared the performance of each of the features to a majority class baseline (i.e. 66.7% accuracy). A useful feature was expected to show an increase over this baseline with statistical significance. Significance of each feature?s performance was evaluated via a chi-squared test (p<0.05).  Our results show that the features 3, 1, 2 and 5 performed significantly better over the majority class baseline. The features 4, 7 and 8 were unable to distinguish between the caused-motion constructions and the non caused-motion usages.                                                 4 As we can see in Table 1, the accuracy is higher than precision or recall. This is because precision and recall are calculated with regard to identifying caused-motion constructions, whereas accuracy is based on identifying both caused-motion and non-caused motion constructions. Since it?s easier to get better performance on the majority class (NON-CM), the overall accuracy is higher.  
Their precision values could not be calculated due to the fact that these features resulted in zero positive (CM) classification.  In a second study, we evaluated the performance of the system when each feature was removed individually from the full set of features (Table 3). The removal of a useful feature was expected to show a statistically significant drop in performance compared to that of the full feature set.  Significance in this performance degradation when compared against the full set of features was evaluated via chi-squared test (p<0.05). Here, features 3, 8 and 1, when removed, showed a statistically significant performance drop. The rest of the features were not shown to have a statistically significant effect on the performance. Our results show that the preposition feature is the single most predictive feature and the feature that has the most significant effect in the full feature set. These results are encouraging: unlike the purely lexical features like the named entity features (6, 7, and 8) that are dependent on the particular expression used in the sentence, 
Table 2:  Effect of each feature on the performance in classification of the caused-motion construction, in the order of decreasing F-score. Features that performed statistically higher than the majority class baseline are marked with an * in the last column.  
# Removed Feature P% R% F A%  3 Preposition 76.9 73.3 0.751 83.8 * 8 NEs for Object NP 84.6 80.7 0.826 88.7 * 1 Verb 85.9 81.3 0.836 89.3 * 2 Function Tag on PP 85.2 84.7 0.849 90.0  9 NEs for PP?s Object 87.5 84.0 0.857 90.7  7 NEs for Subject NP 87.0 84.7 0.858 90.7  5 VerbNet Classes 86.0 86.0 0.860 90.7  4 Comp. Cat. of P 86.7 86.7 0.867 91.1  6 VerbNet PP Type 87.8 86.0 0.869 91.3  Table 3: System performance when the specified feature is removed from the full set of features, in the order of increasing F-score. Significant performance degradation, when compared against the full feature set performance (Table 1) was labeled with an * in the last column. 
# Included Feature P% R% F A%  3 Preposition 82.4 65.3 0.729 83.8 * 1 Verb  78.0 52.0 0.624 79.1 * 2 Function Tag on PP 82.6 38.0 0.521 76.7 * 5 VerbNet Classes 73.5 33.3 0.459 73.8 * 6 VerbNet PP Type 59.6 33.3 0.427 70.2  9 NEs for PP?s Object 71.4 6.7 0.122 68.0  4 Comp. Cat. of P   0.0  66.7  7 NEs for Subject NP  0.0  66.7  8 NEs for Object NP  0.0  66.7  
6
 prepositions are function words. Like syntactic elements, these function words also contribute to the patterned structures of a construction as discussed in Section 3. Furthermore, unlike the semantics of features that are dependent on content words that are subject to lexical variability, prepositions are limited in their lexical variability, which make them good general features that scale well across different semantic domains. In addition to the preposition feature, the verb feature was found to affect performance at a statistically significant level in both cases. Based on the numerous studies in the past that have shown the usefulness of the verb as a feature, this is not an unexpected result. Interestingly, our results seem to indicate interactions between features. This can be seen in two different instances. First, while feature 8 (NEs for Object NP) alone was not found to be a predictive feature, when removed, it resulted in a statistically significant drop in performance compared to that of the full feature set. The opposite effect can be seen with the VerbNet Classes feature. While it showed a statistically significant boost in performance when introduced into the system by itself, when dropped from the full feature set, the drop in the system performance was not found to be significant. This seems to indicate that NEs for Object NP and the VerbNet Classes features have strong interactions with one or more of the other features. We will continue investigating these interactions in future work. 7 Out-of-Vocabulary Verbs Additionally, we separately examined the performance on the test set verbs that were not seen in the training data (i.e. out-of-vocabulary/OOV items).  Just over a fifth of the instances (92 out of 450 constructions) in the test data had unseen verbs, with a total of 83 unique verb types. The results show that there was no decrease in the accuracy or F-score. In fact, there was a chance increase, not statistically significant, in a two-sample t-test (t=1.13; p>0.2).  We carried out the same feature studies for the OOV verbs, as detailed in section 6 (Tables 4 and 5). The performance in both of the studies reflected the results seen in Tables 2 and 3, with one expected exception. The verb feature was, of course, found to be of no value to the predictor. 
What is interesting here is that the verb feature did perform at a significant level for the full test data. By this observation, it would be expected that the overall performance on the OOV verbs would be negatively affected since there is no available verb information. However, this was not the case. 8 Discussion and Conclusion  The results presented show that a classifier can be trained to automatically identify the semantics of constructions; at least for the caused-motion construction, and that it can do this with high accuracy. Furthermore, we have determined that the preposition feature is the most useful feature when identifying caused-motion constructions. Moreover, in considering our results in light of the performance of the SRL systems (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005), where unseen predicates result in significant performance degradation, we found in contrast that using CxG to inform semantics resulted in equally high performance on the out-of-vocabulary predicates. This serves as evidence that semantic 
Table 4: Effect of each feature on the performance in classification of the caused-motion construction with OOV verbs, in the order of decreasing F-score. The precision values could not be calculated for the performance of the features 1,4,7, and 8 due to the fact that these features resulted in zero positive classifications. 
# Removed Feature P% R% F A% 3 Preposition 63 76 0.69 90 2 Function Tag on PP 83 80 0.82 82 6 VerbNet PP Type 84 84 0.84 67 5 VerbNet Classes 84 84 0.84 73 9 NEs for PP?s Object 84 84 0.84 74 1 Verb  0  73 4 Comp. Cat. of P  0  73 7 NEs for Subject NP  0  73 8 NEs for Object NP  0  73 
# Removed Feature P% R% F A% 3 Preposition 63 76 0.69 82 8 NEs for Object NP 83 80 0.82 90 2 Function Tag on PP 84 84 0.84 91 5 VerbNet Classes 84 84 0.84 91 7 NEs for Subject NP 84 84 0.84 91 1 Verb 88 88 0.88 93 4 Comp. Cat. of P 88 88 0.88 93 6 VerbNet PP Type 92 88 0.90 95 9 NEs for PP?s Object 92 88 0.90 95 Table 5: System performance when the specified feature is removed from the full set of features in the classification of constructions with OOV items, in the order of increasing F-score. 
7
 analysis of novel lexical combinations and unseen verbs can be improved by enriching semantics with a construction-level analysis. 9 Future Work There are several directions to go from here. First, in this paper we have kept our study within the scope of caused-motion constructions. We intend to introduce more types of constructions and include more syntactic variation in our data.  We will also add more annotated instances. Secondly, we examine the impact of the introduction of additional features, such as a bag-of-words feature. In particular, we will include semantic features based on FrameNet to the VerbNet semantic features we are already using.  This will be more feasible once the SemLink semantic role labeler for FrameNet becomes available (Palmer, 2009). Finally, we plan to include a more detailed analysis of the feature interactions, and examine the benefit that a construction grammar perspective might add to our semantic analysis. Acknowledgements We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA/IPTO) under the GALE program, DARPA/CMO Contract No. HR0011-06-C-0022, subcontract from BBN, Inc. We are also grateful to Laura Michaelis for helpful discussions and comments. References  Agirre, Eneko and Philip Edmonds. 2006. Introduction. In Word Sense Disambiguation: Algorithms and Applications, Agirre and Edmonds (eds.), Springer. Ben-David, Shai, Blitzer, John, Crammer, Koby  Pereira, Fernando. 2006. 'Analysis of representations for domain adaptation', in NIPS. Bikel, D., Schwartz, R., Weischedel, R.  1999.  An algorithm that learns what?s in a name.  Machine Learning: Special Issue on NL Learning, 34, 1-3. Carreras, Xavier and Lluis Marquez. 2005. Introduction to the CoNLL- 2005 shared task: Semantic role labeling. Procs of CoNLL- 2005.  Chih-Chung Chang and Chih-Jen Lin, LIBSVM : a library for support vector machines, 2001. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm Gildea, Daniel and Daniel Jurafsky. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics 28:3, 245-288. 
Fillmore, Charles J., Christopher R. Johnson and Miriam R.L. Petruck (2003) Background to Framenet, International Journal of Lexicography, Vol 16.3: 235-250. Fillmore, Charles, Paul Kay and Catherine O'Connor (1988). Regularity and Idiomaticity in Grammatical Constructions: The Case of let alne. Language 64: 501-38. Giuglea, Ana-Maria and Alessandro Moschitti. 2006. Shallow semantic parsing based on FrameNet, Verb-Net and PropBank. In Proceedings of the 17th European Conference on Artificial Intelligence, Riva del Garda, Italy. Goldberg, Adele E. 2006. Constructions at work. The nature of generalization in language. Oxford: Oxford University Press Goldberg, Adele. E. 1995. Constructions: A construction grammar approach to argument structure. Chicago: University of Chicago Press. Hovy, Edward H., Mitch Marcus, Martha Palmer, Sameer Pradhan, Lance Ramshaw, and Ralph M. Weischedel. 2006. OntoNotes: The 90% Solution. Short paper. Proceedings of the Human Language Technology / North American Association of Computational Linguistics conference (HLT-NAACL 2006). pp. 57-60, New York, NY. Kay, Paul. 2002. English Subjectless Tag Sentences. Language 78: 453-81. Kipper-Schuler, Karin. 2005. VerbNet: A broad coverage, comprehensive verb lexicon. Ph.D. thesis, University of Pennsylvania. Levin, Beth. 1993. English Verb Classes and Alternations: A Preliminary Investigation, University of Chicago Press, Chicago, IL. Michaelis, Laura A. (2004). Type Shifting in Construction Grammar: An Integrated Approach to Aspectual Coercion. Cognitive Linguistics 15: 1-67. Ng, Hwee Tou and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, Santa Cruz, California, 40?47.  Marcus, Mitchell P, Santorini, Beatrice, Marcinkiewicz, Mary A. (1994) "Building a large annotated corpus of English: the Penn Treebank" Computational Linguistics 19: 313-330. Palmer, Martha. "Semlink: Linking PropBank, VerbNet and FrameNet." Proceedings of the Generative Lexicon Conference. Sept. 2009, Pisa, Italy: GenLex-09, 2009. Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71?106. 
8
Proceedings of the NAACL HLT Workshop on Extracting and Using Constructions in Computational Linguistics, pages 17?24,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
 
Identifying Assertions in Text and Discourse: The Presentational Relative Clause Construction  Cecily Jill Duffield, Jena D. Hwang,  and Laura A. Michaelis    Department of Linguistics, Institute of Cognitive Science  University of Colorado at Boulder Boulder, CO 80309  USA {cecily.duffield, hwangd, laura.michaelis}@colorado.edu        Abstract 
In this paper we investigate the Presentational Relative Clause (PRC) construction. In both the linguistic and NLP literature, relative clauses have been considered to contain background information that is not directly relevant or highly useful in semantic analysis. In text summarization in particular, the information contained in the relative clauses is often removed, being viewed as non-central content to the topic or discourse. We discuss the importance of distinguishing the PRC construction from other relative clause types. We show that in the PRC, the relative clause, rather than the main clause, contains the assertion of the utterance. Based on linguistic analysis, we suggest informative features that may be used in automatic extraction of PRC constructions. We believe that identifying this construction will be useful in discriminating central information from peripheral. 1 Introduction Identifying and extracting relevant information in a given text is an important task for human readers and natural language processing applications.  To do this, proper identification and treatment of complex sentences containing relative clauses and other embedded structures such as appositive clauses (e.g., My mother, a patient at the center, met him last year,) and participial clauses (e.g., Once he ate Werthers, including the wrapper,) is necessary.  Thus, the tasks of text simplification and text summarization in NLP have focused their efforts 
on finding effective ways of simplifying long and complex sentences into shorter and simpler ones. This has in turn proven useful in machine translation (Chandrasekar et. al., 1996), parsing and information extraction (Chandrasekar and Srinivas, 1997), as well as document simplification designed to make texts accessible to wider audiences. Such audiences include readers at low literacy levels (Siddharthan, 2003), second language learners (Petersen and Ostendorf, 2007) and aphasic readers (Devilin and Unthank, 2006). The goal of text simplification and summarization is to reduce syntactic or structural complexities while preserving the central meaning or relevant information in the given text. Unfortunately, syntactic simplification algorithms often assume a uniform treatment of syntactic structures. This is especially true in the domain of relative clauses.  Relative clauses are often considered to contain parenthetical information. That is, their putative role in the sentence is to provide background information about the mentioned entity or entities. Consider (1): 1  (1) You [get] a guy down the street who comes up, uh, carrying a knife.2                                                 1 Relative clauses are shown in boldface and the referent noun phrase is underlined. The matrix verb, the verb of the sentence in which the relative clause is embedded, is in brackets. 2 Unless otherwise noted, all examples in this paper are taken from the Switchboard Treebank corpus (Godfrey et al 1992, Marcus et al 1993), a syntactically parsed version of the Switchboard corpus of American English telephone conversations. 
17
  According to the above definition of relative clauses, they key information in (1) is in the main clause You get a guy down the street. That is, (1) means something like: the entity you comes to possess a guy down the street. If we accordingly consider the relative clause as the background or incidental information and remove it from the semantic analysis, the assertion ? the key piece of the information ? would be lost.  In this paper, we discuss the Presentational Relative Clause (PRC) construction, as seen in (1). In the PRC, the relative clause, rather than the main clause, contains the assertion of the utterance (Duffield & Michaelis, 2009).  Moreover, we analyze the construction in detail to assess its potential usefulness in NLP applications. Based on linguistic analysis, we suggest features that may be useful for implementation of automatic identification of PRC constructions. We believe that the identification of this construction will be useful in discriminating central information units from the peripheral ones.  2 Relative Clauses Relative clauses are constructions in which a verbal clause modifies a nominal element, the ?head,? as shown in (2) and (3):   (2) I [like] cars that _____ are designed with human beings in mind.  (3) I [like] those movies that you watch ____ time and time again.   In (2), the relative clause that are designed with human beings in mind describes the head nominal cars, while in (3), the head nominal those movies is described by the relative clause that you watch time and time again. The gap in the relative clause indicating the position of the co-referential noun phrase is shown. Relative clauses are typically embedded in main clauses, with the result that the nominal element satisfies a semantic-role requirement of two different verbs. For example, consider sentence (2), where the head nominal cars serves as an argument of the verb like while a gap that shares its referent with the head nominal marks the argument of the passive verb designed. Likewise, in (3) the referent shared by the head nominal those movies 
in the main clause and the gap in the relative clause satisfies the requirements of two separate verbs, like and watch.  2.1 Types of Relative Clauses  In the linguistic tradition, relative clauses are typically classified into restrictive and non-restrictive types, as seen in (4-5), respectively.   (4) And, you know, I [want] a car that I can work on ____, because I think it just costs too much even to get the oil changed anymore.  (5) And once you?ve [reached] the river walk area, which ____ is the tourist area, it?s usually pretty safe during the day.   In (4), the speaker has asserted that he would like a car; the restrictive relative clause specifies the type of car as one belonging to the set of cars that he could work on, as opposed to a type of car that he would be unable to repair. In (5), the relative clause does not identify the river walk area as one out of a set of areas, but simply provides additional information about it.  In neither case does the relative clause assert information in the discourse; rather, it expresses a presupposed proposition (e.g., ?I can work on x,??x is the tourist area?) that is assumed to be known by both the speaker and the addressee. Accordingly, relative clauses are assumed to provide background information concerning the entities they modify. This background material serves either to distinguish the referent from others of its kind, as in a restrictive relative clause, or provide additional material, as in a non-restrictive relative clause, rather than asserting something new about the referent.  2.2 Relative Clauses in NLP In line with the linguistic consensus, work in NLP has also viewed relative clauses as expressing background information about a referent. In syntactic simplification, the structural complexity is resolved by splitting a sentence into multiple ones (Siddharthan, 2003; Chandrasekar, 1996). In effect, the relative clause is pulled out of the main clause into an independent sentence. For example, (5) would be simplified into (6).  
18
 (6) And once you?ve reached the river walk area, it?s usually pretty safe during the day. The river walk area is the tourist area.  In text summarization, where background information is considered parenthetical and identified with non-key content, relative clauses are simply disregarded (Siddharthan et al, 2004). That is, if we consider the relative clause to contain parenthetical information, it is reasonable to simply remove the non-key content from the text prior to any semantic analysis.  3 Presentational Relative Clauses As the analysis of (1) above suggests, however, not all relative clauses contain parenthetical information. In fact, many linguistic studies have argued that subordinate clauses can make assertions (Goldberg, 2006; Menn, 1974; McCawley, 1981; Fox & Thompson, 1990), as a counterpoint to the studies that view them as expressing exclusively backgrounded information (Shibatani, 2009). Here, in line with Michaelis and Lambrecht (1996), Kay and Fillmore (1999) and other work in the Construction Grammar tradition, we analyze a particular construction, the Presentational Relative Clause construction (PRC), as a productive idiomatic pattern.  The PRC is a construction in which the material presented in the relative clause is not backgrounded, either in the sense of being unnecessary parenthetical material, or in the sense of being already known to both speaker and addressee (Duffield & Michaelis, 2009). In the PRC, information is asserted in the relative clause that modifies the nominal element, which is introduced by a semantically bleached main clause. Examples of the PRC include (1) and the following:  (7) They [had] some guy that ___ was defending himself.  (8) And I [know] people who ___have been drug tested and who have not, you know, been hired by a corporation. (9) And they've [got] a fifteen year old that ___'s their boss that ___ is carrying a gun  Each of the examples above were examined in their original contexts to determining that an uninformative main clause introduces the head 
nominal, while an assertion is contained within the relative clause. In (7), the main clause that introduces the referent some guy is semantically uninformative. By this we mean that it does NOT assert that the entity they possessed some guy. Rather, the asserted proposition in the utterance is in the relative clause, and (7) can be paraphrased as A guy was defending himself.  In (8) the important information is not that the speaker knows a certain set of people. Instead, the key assertion here is: ?Some people have been drug tested and have not been hired by a corporation.?  Likewise, (9), illustrating what might be described as a double-PRC, could be rephrased as, ?A fifteen-year-old is their boss and is carrying a gun.? 3.1 Anatomy of the PRC construction The PRC construction is typically characterized by three main properties:  a semantically empty main clause, a head nominal in the object position of the main clause that is newly introduced into the discourse, and a subject-gap relative clause that modifies the head nominal.   First property: The semantically bleached main clause serves to convey the restriction on the range of the existential quantifier rather than an assertion. In (7), for example, the main clause conveys the restriction ?x is a guy?. Consequently, a PRC, unlike a restrictive relative, is not optional. It is a required part of the clause in which it appears, exemplified by the fact that (7) cannot reasonably be construed as asserting ?They had some guy?.  Being uninformative, the main verbs of PRC tend to have low semantic weight, as in (10-12).  (10) I've [seen] some statistics that ___ say it's more expensive to kill somebody than to keep them in prison for life. (11) You [get] a guy down the street who ___ comes up, uh, carrying a knife. (12) When our kids were small we [had] a couple of, uh, good women who ___ would often come to the house.  The bracketed main verbs in (10-12), which otherwise denote relations of perception, obtaining, and possession, respectively, here appear simply to ?set the stage? for their object referents. In other words, (10) does not assert that the speaker sees something, (11) does not assert that the addressee 
19
 obtains something and (12) does not assert that some people possessed someone. Rather than predicating a property or action of the main clause subject, the main clause predications in (10-12) provide an explicit or inferred center of perspective from which to view the entity denoted by the head nominal (Koenig and Lambrecht, 1999).  Second property: The discourse-new head nominal is in the object position of the main clause.  Thus, the PRC enables the speaker to avoid violating a hearer-based information-packaging constraint that Lambrecht (1994) refers to as the Principle of Separation of Reference and Role (PSRR): ?Do not introduce a referent and talk about it in the same clause? (ibid). In other words, to aid the hearer in anchoring the new referent in discourse, the speaker introduces it in the object position of the main clause, and then predicates upon it in the relative clause, as in example (13):  (13) Speaker A: We have more options now then (sic) we did when my kids were born, with being able to take off full-time longer, you can phase your schedule in so that it 's not full-time for up to six months. Speaker B: Oh boy, that's great. Speaker A: It 's really neat. I've [had] a couple of assistants that ___ came back just three days a week or they've, you know, whatever schedule they want from a pay standpoint.  Consider Speaker A?s second turn, restructured as a declarative clause rather than as a PRC, and thus violating the PSRR. This time the assertion is conveyed in the main clause, but with the new entity in the in the subject position the result is pragmatically awkward:  (14) Speaker A: It 's really neat. A couple of assistants came back just three days a week or they've, you know, whatever schedule they want from a pay standpoint.  An additional example is provided in (15):  (15) Speaker B: I?ve never liked D.C. a whole lot and a really hate the Redskins. And a lot of it's because, you know, I 
[got] a lot of people, you know, at work with and everything that __ are big Redskin fans. (??A lot of people I work with and everything are big Redskin fans.)  Thus, a crucial identifying characteristic of the PRC is that it always modifies head nominals that are main-clause objects.  Third property: Third identifying property of the PRC is the presence of a subject-gap relative clause.  That is, the relative clause modifying the head nominal contains a gap in the subject position of the relative clause that is co-referential with the head nominal, as in (7) repeated here as (16):  (16) They [had] some guy that ___ was defending himself.  There are, however, cases in which the head nominal is modified by an object-gap relative clause, which conveys an assertion, as in (17):  (17) Everybody [gets] five pounds of garbage that they can throw away____ you know uh but more than that every week uh you?ve got to pay by the pound. In the example above, as with the more prototypical subject-gap PRC, the main clause does not make an assertion (in this case, the main clause does not assert that everybody receives five pounds of garbage).  Rather, the assertion in the relative clause is demonstrated by the appropriate paraphrase, ?Everybody can throw away five pounds of garbage.? While speakers do produce object-gap sentences to convey assertions, subject-gap PRC tokens account for the majority of assertoric relative clauses in spoken discourse (Duffield and Michaelis, 2009). This results in the subject-gap structure being a useful property for identifying prototypical instances of the PRC. 4 Why identify the PRC construction? As argued above, identifying the PRC is important because, unlike restrictive and non-restrictive relative clauses, the PRC does not present backgrounded or parenthetical information. Rather, the loss of information asserted in a PRC results in the loss of inferences crucial for the discourse.   
20
 4.1 ?My son is an animal lover.? So far we have seen sentences or utterances that would inarguably be interpreted as sentences containing a PRC. However, there are PRCs, which, while equipped with every relevant PRC characteristic, initially appear to contain relative clauses expressing parenthetical information. Consider the following sentence:  (18) I [had] a son, he?s now gone from the home,  that ___ was an animal lover. In isolation, (18) could be interpreted as asserting that the speaker has a son, who now happens to have left home. The relative clause that was an animal lover would be treated as background information about the son.  Yet an examination of the context of the conversation reveals that the relative clause contains crucial information with regard to the discourse as a whole:  (19) Speaker A: Do you want to hear about my other animals I've had? Speaker B: Sure, sure. Speaker A: I've had a skunk  Speaker B: Yeah. Speaker A: I've had a Burmese python, I've had rats, I?ve had mice.  Speaker B: Wow. Speaker A: Uh, let's see, I've had gerbils, I have, I [had] a son, he?s now gone from the home, that ___ was an animal lover.  Speaker B: Uh-huh. Speaker A: So at one point I had a snake, skunk, dog and a cat running loose in the house.  In this case, we see even a more compelling reason to identify this sentence as a PRC. Disregarding the relative clause in (18) and treating the main clause as containing an asserted proposition, results in a radically different reading: the speaker?s son is among the animals that the speaker claims to have owned (i.e. ?I?ve had a skunk, I?ve had a Burmese python, I?ve had rats, I?ve had mice, I?ve had gerbils, I had a son??). By classifying this sentence as a PRC, we reach the intended assertion, ?My son was an animal lover,? which in turn explains why the speaker has been the proud owner of a menagerie of animals. 
4.2 Other examples Much like the example in (18), the discourse context of the other PRCs presented in this paper substantiates the claim that they present information central to the discourse.  The PRC in (7) ?They [had] some guy that was defending himself,? used in a conversation describing a trial, signals that the situation departs from the prototypical courtroom schema in a crucial respect (the defendant is without a lawyer).  Other instances of the PRC, such as (8) ?And I [know] people who ___have been drug tested and who have not, you know, been hired by a corporation,? like (18), provide explanatory information: the reason for the speaker?s negative view of drug-testing. Finally, in (15) ?I [got] a lot of people, you know, at work with and everything that ___ are big Redskin fans,? the PRC utterance explains why the speaker dislikes a particular football team. Treating these clauses as background information, restricting categories of entities, or removing them from semantic analysis results in the loss of information about causal connections in the text.    5 Identifying the PRC construction Thus far we have presented the identifying linguistic properties of the PRC constructions. We will now demonstrate how these properties (see Section 3.1) lend themselves to features that could be useful for automatic identification and classification of PRCs. For the purposes of this section we make the assumption that we will only retain instances that can be parsed by an automatic parser (Collins 1999, Charniak, 1997).  The features we suggest are based on the results of a corpus study carried out by Duffield and Michaelis (in prep) examining the role of the PRC in the distribution of relative clause types in spoken discourse. 5.1 The distribution of the PRC in discourse In the study by Duffield and Michaelis (2009, in prep), 1000 sentences (500 each of subject-gap and object-gap relative clauses) from the Switchboard Corpus (Godfrey, 1996) were manually examined for the first two of the identifying properties of PRC tokens as described in Section 3.1. In addition, each of the 1000 sentences was examined 
21
 within a context of 50 lines of previous discourse to determine whether or not the relative clause conveyed an assertion.   Their results showed that three properties, namely, a semantically bleached verb, discourse-new head nominals, and an assertion in the relative clause, were found to significantly predict relative clauses of the subject-gap type, suggesting that PRCs account for the prevalence of subject-gap relative clauses in discourse. In fact, 22.4% of subject-gap relative clauses were PRCs, while only 6.8% of object-gap relative clauses displayed features of the PRC.  The manner in which Duffield and Michaelis manually annotated their data, although based on linguistic analysis as discussed above, easily lends itself to a list of properties that could be automatically used to identify PRCs in larger corpora. 5.2 Verb in the main clause Corresponding to the first property (Section 3.1) of a semantically empty main clause, Duffield & Michaelis have observed that PRC tokens have the tendency to co-occur with verbs of existence, perception and discovery. Table 1 lists these verbs.  Be Get See Hear Tell Have Find Know Look Wonder Table 1: Main-clause verbs likely to appear in PRCs. This suggests that encoding the lemmatized verbs as features may help in automatic classification of PRCs.  5.3 Position of head nominal The head nominal of the relative clause was found to occur in two positions relative to the main-clause verb. It was either the second argument of the main-clause verb (20) or the complement in the prepositional phrase (21), which in turn was the second argument of the main-clause verb.  (20) They [had] {some guy that ___ was defending himself.}-NP (21) I have a friend who was [telling] me {about her brother who ___ gets high all the time.}-PP Duffield and Michaelis also observed that there was a correlation between the main-clause verb and the position in which the head nominal was 
found. That is, the verbs such as look, tell, and wonder were regularly found when the head nominal was the complement in the PP, while other verbs in Table 1 more frequently occurred with the head nominal in the direct argument position of the main-clause verb.  Furthermore, Duffield and Michaelis found that in cases where the head nominal was the complement of the PP, the head of the PP was found to be either of or about.  This suggests that in conjunction with the features derived from the lemmatized verb, the position of the head nominal in relation to the main-clause verb could be encoded as a feature. That is, for each of the sentences examined, a feature can be coded for either a NP complement or PP complement, given which type of complement the relative clause sits in. In addition to the position of the head nominal, for those relative clauses that are found in the PP complement a feature can also be coded for the preposition heading the phrase.  5.4 Head nominal: noun and modifiers Corresponding to the second property (Section 3.1) of PRCs, discourse-new status of modified head nominals, is indefinite form. Although Duffield & Michaelis recognize that the distinction between the ?given? and ?new? discourse statuses is not the same thing as definite versus indefinite form, discourse-active entities tend to be formally marked as definite, while discourse-new entities tend to be marked as indefinite (Prince 1992). Head nominals considered as indefinite include bare plural nouns (e.g., engineers), determinerless nominals modified by adjectives or cardinal numbers (e.g., about forty kindergarteners), bare mass nouns (e.g., material), nominals with weak quantifiers (e.g., some companies), indefinite pronouns (e.g., somebody, anybody) and nominals containing the indefinite article a (e.g., a fish).   Definite head nominals include those containing the definite article the (e.g., the thing, the resources), demonstrative determiners (e.g., this recording, that attitude), possessive determiners (e.g., my bass), strong quantifiers (e.g., every story, all these people), demonstrative pronouns (e.g., that, those) and proper nouns (e.g., Rockport, Albany). Partitive nominal expressions with 
22
 indefinite heads (e.g., one of those things, some of my friends) are also considered as indefinite. To turn the above into linguistic features that are characteristic of definite and indefinite head nominals, we suggest a number of possible features for classification. Several of these relating to the head noun phrase may contribute to the identification of PRCs:  Head nominal features: - the phrasal categories of the sisters to the noun in the head noun phrase. These features will encode the presence of any adjectival or prepositional phrases within the head noun phrase. The inclusion of these features will account for the existence of any adjectival modification on the head noun phrase and/or partitive nominals. - the existence of named entities in the head noun phrase to ascertain the existence of any proper nouns in the head nominal.  Head nominal features encoding:  - whether or not the nominal is a pronoun. This will serve to introduce the indefinite and demonstrative pronouns into the classification of PRCs. - singularity/plurarity of the head nominal.  Modifier features encoding: - articles and determiners, - quantifiers, and - possessive pronouns that modify the head noun or noun phrase.   5.5 Gap in the relative clause The third and final property shared by PRCs concerns the gap in the relative clause. The gap occurs in subject position and is co-referential with the head nominal. This can be identified in the syntactic parses by the presence of a trace in the syntactic position co-indexed3 with the relative pronouns that, who, or which.  
                                                3 Note that coindexation is distinct from coreference. In Treebank, coindexation involves the creation of a syntactic link between the trace and the constituent that was moved out of the position trace now occupies. Coreference is the relationship between the gap and the referent.  Most parsers, however, do not supply co-indexation. 
 The syntactic position of the gap can be coded as a feature. These would also include a feature for cases where the gap is entirely missing from the relative clause. This is to account for cases of relative clauses containing a pronoun in the position where the gap should be (e.g. The gap in this example is filled with the pronoun it: ?Here[?s] a journal that I?m in the board of it.?). 5.6 Subject position of matrix clause In addition to the above features, based on Duffield & Michaelis? characterization of the PRC, there is one other syntactic characteristic worth investigating?the subject of the matrix clause. Consider the following PRCs:  (22) They [had] some guy that ___ was defending himself. (23) There[?s] a lot of people that fall into that category  (24) It [was] a moving man ____ pulled right up to her house, broke in and stole everything she owned4. General observation of PRCs is that they seem to display a tendency to have either a pronoun (22), or an expletive there (23) or it (24) in the subject position of the matrix clause. This suggests that the lexical content of the subject position may be a useful predictor for PRC classification. 6 Discussion and Conclusion In this paper, we have presented identifying properties of the PRC construction.  We recognize that individual properties as presented here contribute to but do not determine the final meaning of the PRC construction as a whole, but in combination, they are likely predictors. Not all syntactic forms can be treated in the same way.  By not privileging the syntactic level, but rather treating lexical, morphological, and syntactic features equally, we are able to identify key indicators that could be used to identify the function of a relative clause in discourse as conveying an assertion as opposed to backgrounded information.                                                   4 Certain types of PRCs, such as in this example, are produced without the relative pronoun.  Such PRCs are referred to as amalgams (see Lambrecht, 1988 for discussion) 
23
 For the purposes of NLP, we must work within the framework of phrasal structures, constrained by the resources currently available. Yet as we have suggested here, those resources, although not constructionally based, can be used to identify constructions for the purpose of extracting relevant information from naturally occurring data. We have further investigated the applicability of a construction-based approach to identifying relative clause types when the individual components, such as lexical items themselves are not themselves effective predictors.  This clause-level information allows for richer representations of textual meaning. Our future plans include experiments with implementing automatic classifiers of relative clause type based on these features. Such empirical study will give us a better understanding of the degree of usefulness of these features in identifying PRCs in text data. We anticipate that additional features will be discovered during the implementation process.  Acknowledgements We would like to thank Martha Palmer, Jim Martin, Jinho Choi, Susan Brown, Les Sikos, and Steve Duman for valuable feedback.  References   Chandrasekar, Raman, and Bangalore Srinivas. 1997. Automatic Induction of Rules for Text Simplification. Knowledge-Based Systems, 10(3): 183-190. Chandrasekar, Raman, Christine Doran, and Bangalore Srinivas. 1996. Motivations and Methods for Text Simplification. In Proceedings of the 16th International Conference on Computational Linguistics (COLING 1996), pages 1041-1044, Copenhagen, Denmark. Charniak, Eugene. 2000. A Maximum-Entropy-Inspired Parser. In Proceedings of NAACL00, pages 132-139, Seattle, WA, USA. Collins, Michael John. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA. Devlin, Siobhan, and Gary Unthank. 2006. Helping Aphasic People Process Online Information. In Proceedings of the Eighth International ACM SIGACCESS Conference on Computers and Accessibility, pages 225-226, Portland, OR, USA. Duffield, Cecily Jill and Laura A. Michaelis. 2009. Why Subject Relatives Prevail: Constraints versus 
Constructional Licensing. Presented at the 2009 Annual Meeting of the Linguistics Society of America. San Francisco. CA, USA Duffield, Cecily Jill and Laura A. Michaelis. in prep. Why Subject Relatives Prevail: Constraints versus Constructional Licensing. Kay, Paul and Charles J. Fillmore. 1999. Grammatical Constructions and Linguistic Generalizations: the What's X doing Y? Construction. Language, 75(1):1-33. Fox, Barbara, and Sandra Thompson. 1990. A Discourse Explanation of the Grammar of Relative Clauses in English Conversation. Language 66:51-64 Goldberg, Adele E. 2006. Constructions at work. The nature of generalization in language. Oxford University Press, Oxford. Menn, Lise. 1974. Assertions not made by the main clause of a sentence. Studies in the Linguistic Sciences (University of Illinois) 4(1):132-143. Koenig, Jean-Pierre and Knud Lambrecht. 1999. French Relative Clauses as Secondary Predicates. In Francis Corbin, Carmen Dobrovie-Sorin, and Jean-Marie Marandin, editors, Empirical issues in Formal Syntax and Semantics 2. Thesus, pages 191-214, The Hague, The Netherlands. Lambrecht, Knud. 1994. Information structure and sentence form: Topic, focus, and the mental representation of discourse referents. In Cambridge Studies in Linguistics 71. Cambridge, Cambridge University Press. Lambrecht, Knud. 1988. There was a Farmer had a Dog: Syntactic Amalgams Revisited. In S. Axmaker, A. Jaisser and H. Singmaster, editors, The Proceedings of the Fourteenth Annual Meeting of the Berkeley Linguistics Society. BLS, Inc., pages 319-339 Berkeley, CA. McCawley, James D. 1981. The Syntax and Semantics of English Relative Clauses. Lingua, 53: 99-149. Michaelis, Laura A. and Knud Lambrecht. 1996. The Exclamative Sentence Type in English. In Adele Goldberg, editor, Conceptual Structure, Discourse and Language. Center for the Study of Language and Information, pages 375-389, Stanford, CA. Petersen, Sarah E. and Mari Ostendorf. 2007. Text simplification for language learners: a corpus analysis. In SLaTE-2007, pages 69-72, Farmington, PA. Siddharthan, Advaith. 2003. Syntactic simplification and Text Cohesion. Ph.D. thesis, University of Cambridge, UK. Siddharthan, Advaith, Ani Nenkova and Kathleen McKeown. 2004. Syntactic Simplification for Improving Content Selection in Multi-Document Summarization. In Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004), Geneva, Switzerland. 
24
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 82?90,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
PropBank Annotation of Multilingual Light Verb Constructions 
 
 
Jena D. Hwang1, Archna Bhatia3, Clare Bonial1, Aous Mansouri1,  
Ashwini Vaidya1, Nianwen Xue2, and Martha Palmer1 
1Department of Linguistics, University of Colorado at Boulder, Boulder CO 80309 
2Department of Computer Science, Brandeis University, Waltham MA 02453 
3Department of Linguistics, University of Illinois at Urbana-Champaign, Urbana IL 61801 
{hwangd,claire.bonial,aous.mansouri,ashwini.vaidya,martha.palmer} 
@colorado.edu, bhatia@illinois.edu, xuen@brandeis.edu 
 
  
 
Abstract 
In this paper, we have addressed the task 
of PropBank annotation of light verb 
constructions, which like multi-word 
expressions pose special problems. To 
arrive at a solution, we have evaluated 3 
different possible methods of annotation. 
The final method involves three passes: 
(1) manual identification of a light verb 
construction, (2) annotation based on the 
light verb construction?s Frame File, and 
(3) a deterministic merging of the first 
two passes. We also discuss how in 
various languages the light verb 
constructions are identified and can be 
distinguished from the non-light verb 
word groupings.  
1 Introduction  
One of the aims in natural language processing, 
specifically the task of semantic role labeling 
(SRL), is to correctly identify and extract the 
different semantic relationships between words 
in a given text. In such tasks, verbs are 
considered important, as they are responsible for 
assigning and controlling the semantic roles of 
the arguments and adjuncts around it. Thus, the 
goal of the SRL task is to identify the arguments 
of the predicate and label them according to their 
semantic relationship to the predicate (Gildea 
and Jurafsky, 2002; Pradhan et al, 2003).  
To this end, PropBank (Palmer et. al., 2005) 
has developed semantic role labels and labeled 
large corpora for training and testing of 
supervised systems. PropBank identifies and 
labels the semantic arguments of the verb on a 
verb-by-verb basis, creating a separate Frame 
File that includes verb specific semantic roles to 
account for each subcategorization frame of the 
verb. It has been shown that training supervised 
systems with PropBank?s semantic roles for 
shallow semantic analysis yield good results (see 
CoNLL 2005 and 2008).  
However, semantic role labeling tasks are 
often complicated by multiword expressions 
(MWEs) such as idiomatic expressions (e.g., 
?Stop pulling my leg!?), verb particle 
constructions (e.g., ?You must get over your 
shyness.?), light verb constructions (e.g., ?take a 
walk?, ?give a lecture?), and other complex 
predicates (e.g., V+V predicates such as Hindi?s 
???? ??? nikal gayaa, lit. ?exit went?, means 
?left? or ?departed?). MWEs that involve verbs 
are especially challenging because the 
subcategorization frame of the predicate is no 
longer solely dependent on the verb alone. 
Rather, in many of these cases the argument 
structure is assigned by the union of two 
predicating elements. Thus, it is important that 
the manual annotation of semantic roles, which 
will be used by automatic SRL systems, define 
and label these MWEs in a consistent and 
effective manner. 
In this paper we focus on the PropBank 
annotation of light verb constructions (LVCs). 
We have developed a multilingual schema for 
annotating LVCs that takes into consideration the 
similarities and differences shared by the 
construction as it appears in English, Arabic, 
Chinese, and Hindi. We also discuss in some 
detail the practical challenges involved in the 
crosslinguistic analysis of LVCs, which we hope 
will bring us a step closer to a unified 
crosslinguistic analysis.    
Since NomBank, as a companion to 
PropBank, provides corresponding semantic role 
82
labels for noun predicates (Meyers et al, 2004), 
we would like to take advantage of NomBank?s 
existing nominalization Frame Files and 
annotations as much as possible.  A question that 
we must therefore address is, ?Are 
nominalization argument structures exactly the 
same whether or not they occur within an LVC?? 
as will be discussed in section 6.1. 
2 Identifying Light Verb Constructions 
Linguistically LVCs are considered a type of a 
complex predicate. Many studies from differing 
angles and frameworks have characterized 
complex predicates as a fusion of two or more 
predicative elements. For example, Rosen (1997) 
treats complex structures as complementation 
structures, where the argument structure of 
elements in a complex predicate are fused 
together.  Goldberg (1993) takes a constructional 
approach to complex predicates and arrives at an 
analysis that is comparable to viewing complex 
predicates as a single lexical item. Similarly, 
Mohanan (1997) assumes different levels of 
linguistic representation for complex predicates 
in which the elements, such as the noun and the 
light verb, functionally combine to give a single 
clausal nucleus. Alsina (1997) and Butt (1997) 
suggest that complex predicates may be formed 
by syntactically independent elements whose 
argument structures are brought together by a 
predicate composition mechanism.  
While there is no clear-cut definition of LVCs, 
let alne the whole range of complex predicates, 
for the purposes of this study, we have adapted 
our approach largely from Butt?s (2004) criteria 
for defining LVCs. LVCs are characterized by a 
light verb and a predicating complement 
(henceforth, true predicate) that ?combine to 
predicate as a single element.? (Ibid.) In LVC, 
the verb is considered semantically bleached in 
such a way that the verb does not hold its full 
predicating power. Thus, the light verb plus its 
true predicate can often be paraphrased by a 
verbal form of the true predicate without loss of 
the core meaning of the expression. For example, 
the light verb ?gave? and the predicate ?lecture? 
in ?gave a lecture?, together form a single 
predicating unit such that it can be paraphrased 
by ?lectured?. 
True predicates in LVCs can be a noun (the 
object of the verb or the object of the preposition 
in a prepositional phrase), an adjective, or a verb. 
One light verb plus true predicate combination 
found commonly across all our PropBank 
languages (i.e., English, Arabic, Chinese, and 
Hindi) is the noun as the object of the verb as in 
?Sara took [a stroll] along the beach?. In Hindi, 
true predicates can be adjectives or verbs, in 
addition to the nouns. 
??? ? ??? [?????]  ???         (Adjective) 
to-me  you [nice]  seem 
lit. ?You seem nice to me? 
'You (are) liked to me (=I like you).' 
?????  ?? ???  [??] ????   (Verb) 
I-ERG everything  [do] took 
lit. ?I took do everything? 
'I have done everything.' 
As for Arabic, the LVCs come in verb+noun 
pairings. However, they surface in two syntactic 
forms. It can either be the object of the verb just 
like in English: 
 
 ???? ????]?????? ] ????? ??  
gave.he Georges [lecture] PREP Lebanon 
lit.'Georges gave a lecture about Lebanon' 
?Georges lectured about Lebanon? 
or the complement can be the object of a 
preposition: 
 
 ??????]????? ]????? ????? 
conduct.I [PREP-visit] our.saint Ilias 
lit. ?I will conduct with visit Saint Ilias?s? 
?I will visit Saint Ilias?s? 
3 Standard PropBank  
Annotation Procedure 
The PropBank annotation process can be broken 
down into two major steps: creation of the Frame 
Files for verbs occurring in the data and 
annotation of the data using the Frame Files. 
During the creation of the Frame Files, the 
usages of the verbs in the data are examined by 
linguists (henceforth, ?framers?). Based on these 
observations, the framers create a Frame File for 
each verb containing one or more framesets, 
which correspond to coarse-grained senses of the 
predicate lemma. Each frameset specifies the 
PropBank labels (i.e., ARG0, ARG1,?ARG5) 
corresponding to the argument structure of the 
verb. Additionally, illustrative examples are 
included for each frameset, which will later be 
referenced by the annotators. These examples 
also include the use of the ARGM labels. 
Thus, the framesets are based on the 
examination of the data, the framers? linguistic 
knowledge and native-speaker intuition. At 
83
times, we also make use of the syntactic and 
semantic behavior of the verb as described by 
certain lexical resources. These resources include 
VerbNet (Kipper et. al., 2006) and FrameNet 
(Baker et. al., 1998) for English, a number of 
monolingual and bilingual dictionaries for 
Arabic, and Hindi WordNet and DS Parses 
(Palmer et. al., 2009) for Hindi. Additionally, if 
available, we consult existing framesets of words 
with similar meanings across different languages. 
The data awaiting annotation are passed onto 
the annotators for a double-blind annotation 
process using the previously created framesets. 
The double annotated data is then adjudicated by 
a third annotator, during which time the 
differences of the two annotations are resolved to 
produce the Gold Standard. 
Two major guiding considerations during the 
framing and annotating process are data 
consistency and annotator productivity. During 
the frameset creation process, verbs that share 
similar semantic and syntactic characteristics are 
framed similarly. During the annotation process, 
the data is organized by verbs so that each verb is 
tackled all at once. In doing so, we firstly ensure 
that the framesets of similar verbs, and in turn, 
the annotation of the verbs, will both be 
consistent across the data. Secondly, by tackling 
annotation on verb-by-verb basis, the annotators 
are able to concentrate on a single verb at a time, 
making the process easier and faster for the 
annotators. 
4 Annotating LVC 
A similar process must be followed when 
annotating light verb constructions The first step 
is to create consistent Frame Files for light verbs. 
Then in order to make the annotation process 
produce consistent data at a reasonable speed, we 
have decided to carry out the light verb 
annotation in three passes (Table 1):  (1) annotate 
the light verb, (2) annotate the true predicate, and 
(3) merge the two annotations into one. 
The first pass involves the identification of the 
light verb. The most important parts of this step 
are to identify a verb as having bleached 
meaning, thereafter assign a generic light verb 
frameset and identify the true predicating 
expression of the sentence, which would be 
marked with ARG-PRX (i.e., ARGument-
PRedicating eXpression). For English, for 
example, annotators were instructed to use Butt?s 
(2004) criteria as described in Section 2. These 
criteria required that annotators be able to 
recognize whether or not the complement of a 
potential light verb was itself a predicating 
element. To make this occasionally difficult 
judgment, annotators used a simple heuristic test 
of whether or not the complement was headed by 
an element that has a verbal counterpart.  If so, 
the light verb frameset was selected. 
The second pass involves the annotation of the 
sentence with the true predicate as the relation. 
During this pass, the true predicate is annotated 
with an appropriate frameset. In the third pass, 
the arguments and the modifiers of the two 
previous passes are reconciled and merged into a 
single annotation. In order to reduce the number 
of hand annotation, it is preferable for this last 
pass, the Pass 3, to be done automatically. 
Since the nature of the light verb is different 
from that of other verbs as described in Section 
2, the advantage of doing the annotation of the 
light verb and the true predicate on separate 
passes is that in the light verb pass the annotators 
will be able to quickly dispose of the verb as a 
light verb and in the second pass, they will be 
allowed to solely focus on the annotation of the 
light verb?s true predicate. 
The descriptions of how the arguments and 
modifiers of the light verbs and their true 
predicates are annotated are mentioned in Table 
1, but notably, none of the examples in it 
currently include the annotation of arguments 
 Pass 1: Pass 2: Pass 3: 
 Light Verb Annotation True Predicate Annotation Merge of Pass1&2 Annotation 
Relation Light verb True predicate Light verb + true predicate 
Arguments 
and 
Modifiers 
- Predicating expression is 
annotated with ARG-PRX 
- Arguments and modifiers of 
the light verb are annotated 
- Arguments and modifiers of 
the true predicate are annotated 
- Arguments and modifiers 
found in the two passes are 
merged, preferably 
automatically. 
Frameset Light verb frameset True predicate?s frameset LVC?s frameset 
 
Example 
?John took a brisk walk through the park.? 
REL: took 
ARG-PRX: a brisk walk 
ARG-MNR: brisk  
REL: walk 
REL: took walk 
ARG-MNR: brisk 
Table 1. Preliminary Annotation Scheme 
84
and modifiers.  This is intentional, as coming to 
an agreement concerning the details of what 
exactly each of the three passes looks like while 
meeting the needs of the four PropBank 
languages is quite challenging. Thus, for the rest 
of the paper we will discuss the strengths and 
weaknesses of the two trial methods of 
annotation we have considered and discarded in 
Section 5, as well as the final annotation scheme 
we chose in Section 6. 
5 Trials 
5.1 Method 1 
As our first attempt, the annotation of argument 
and adjuncts was articulated in the following 
manner (Table 2). 
Pass 1: Pass 2: 
Light verb True predicate 
- Predicating expression 
is labeled ARG-PRX 
- Annotate the Subject 
argument of the light 
verb as the Arg0. 
- Annotate the rest of the 
arguments and modifiers 
of the light verb with 
ARGM labels. 
- Annotate arguments 
and modifiers of the 
true predicate within 
its domain of locality. 
Generic light verb Frame 
File 
True predicate?s 
Frame File 
?-RKQ WRRN D EULVN ZDON WKURXJK WKH SDUN? 
ARG0: John 
REL: took 
ARG-PRX: a brisk walk 
ARG-DIR: through the park 
ARG-MNR: brisk  
REL: walk 
Table 2. Method 1 for annotation for Passes 1 and 2. 
Revised information is in italics. 
In Pass 1, in addition to annotating the 
predicating expression of the light verb with 
ARG-PRX, the subject argument was marked 
with an ARG0. The choice of ARG0, which 
corresponds to a proto-typical agent, was guided 
by the observation that English LVCs tend to 
lend a component of agentivity to the subject 
even in cases where the true predicate would not 
necessarily assign an agent as its subject. The 
rest of the arguments and modifiers were labeled 
with corresponding ARGM (i.e., modifier) 
labels. The assumption here is that the arguments 
of the light verb will also be the arguments of the 
true predicate.   
In Pass 2, then, the annotation of the 
arguments of the true predicate was restricted to 
its domain of locality (i.e., the span of the ARG-
PRX as marked in Pass1). That is, in the example 
?John took a brisk walk through the park?, the 
labeled spans for the true predicate would be 
limited to the NP ?a brisk walk? and neither 
?John? nor through the park? would be annotated 
as the arguments of the true predicate ?walk?. 
Frame Files: This method would require three 
Frame Files: a generic light verb Frame File, a 
true predicate Frame File, and an LVC Frame 
File. The Frame File for the light verb would not 
be specific to the form of the light verb (e.g., 
same frame for take and make). Rather, it would 
indicate a skeletal argument structure in order to 
reduce the amount of Frame Files made, 
including only Arg0 as its argument1.  
5.2 Weakness of Method 1 
This method has one glaring problem: the 
assumption that the semantic roles of the 
arguments as assigned by the light verb 
uniformly coincide with those assigned by the 
true predicate does not always hold. Consider the 
following English sentence2. 
whether Wu Shu-Chen would make another 
[appearance] in court was subject to observation 
In this example, ?Wu Shu-Chen? is the agent 
argument (Arg0) of the light verb ?make? and is 
the theme or patient argument (Arg1) of a typical  
?appearance? event. Also consider the following 
example from Hindi.  
It is possible that in a light verb construction, 
the light verb actually modifies the standard 
underlying semantics of a nominalization like 
appearance.  In any event, we cannot assume that 
the expected argument labels for the light verb 
and for the standard interpretation of the 
nominalization will always coincide. Thus, we 
could say that Pass 2?s true predicate annotation 
is only partial and is not representative of the 
complete argument structure. In particular, we 
are left with a very difficult merging problem, 
because the argument labels of the two separate 
passes conflict as seen in the above examples. 
5.3 Method 2 
In order to remedy the problem of conflicting 
argument labels, we revised Method 1?s Pass 2 
annotation scheme. This is shown in Table 3. 
Pass 1 remains unchanged from Method 1. 
In this method, both the light verb and the true 
predicate of the sentence receive complete sets of 
                                                          
1 This is why the rest of the argument/modifiers would be 
annotated using ARGM modifier labels. 
2  The light verb is in boldface, the true predicate is in bold 
and square brackets, and the argument/adjunct under 
consideration is underlined. 
85
argument and modifier labels. In Pass 2, the 
limitation of annotating within the domain of 
locality is removed. That is, the arguments and 
modifiers inside and outside the true predicate?s 
domain of control are annotated with respect to 
their semantic relationship to the true predicate 
(e.g., in the English example of Section 5.2, ?Wu 
Shu-Chen? would be considered ARG1 of 
?appearance?).  
Frame Files: This method would also require 
three Frame Files. The major difference is that 
with this method the Frame File for the true 
predicate includes arguments that are sisters to 
the light verb.  
5.4 Weaknesses of Method 2 
If in Method 1 we have committed the error of 
semantic unfaithfulness due to omission, in 
Method 2 we are faced with the problem of 
including too much. In the following sentence, 
consider the role of the underlined adjunct: 
A New York audience ? gave it a big round 
of applause when the music started to play. 
By the annotation in Method 2, the underlined 
temporal adjunct ?when the music started to 
play? is labeled as both the argument of ?give? 
and of ?applause?. The question here is does the 
argument apply to both the giving and the 
applauding event? In other words, does the 
adjunct play an equal role in both passes?  
 Since it could be easily said that the temporal 
phrase applies to both the applauding and the 
giving of the applause events, this example may 
not be particularly compelling. However, what if 
a syntactic complement of the light verb is a 
semantic argument of the true predicate and the 
true predicate only? This is seen more frequently 
in the cases where the light verb is less bleached 
than in the case of ?give? above. Consider the 
following Arabic example. 
 
 ????? ??]???????? ] ????? ????????? ??????? ??????  
took.we PREP DEF-consideration PREP 
prepertations.our possibility sustain.their losses 
?We took into [consideration] during our prepa-
rations the possibility of them sustaining losses? 
 
Here, even though the constituent ?of them 
sustaining losses? is the syntactic complement of 
the verb ?to take;? semantically, it modifies only 
the nominal object of the PP ?consideration.?  
There are similar phenomena in Chinese light 
verb constructions. Syntactic modifiers of the 
light verb are semantic arguments of the true 
predicate, which is usually a nominalization that 
serves as its complement.  
 
?? ?  ?    ? ? ??    [??]    ?? ? 
we now regarding this CL issue [conduct] discussion. 
lit.?We are conducting a discussion on this issue.? 
 ?We are discussing this issue.? 
 
The prepositional phrase ????? ?regarding 
this issue? is a sister to the light verb but 
semantically it is an argument of the nominalized 
predicate ?? ?discussion?. 
The logical next question would be: does the 
annotation of the arguments, adjuncts and 
modifiers have to be all or nothing? It could 
conceivably be possible to assign a selected set 
of arguments at the light verb or true predicate 
level. For example, in the Chinese sentence, the 
modifier ?regarding this CL issue?, though a 
syntactic adjunct to the light verb, could be left 
out from the semantic annotation in Pass 1 and 
included only in the Pass 2. 
However, the objection to this treatment 
comes from a more practical need. As mentioned 
above, in order to keep the manual annotation to 
a minimum, it would be necessary to keep Pass 3 
completely deterministic. As is, with the 
unmodified Method 2, there would be the need to 
choose between Pass 1 or Pass 2 annotation to 
when doing the automatic Pass 3. If we modify 
Method 2 by annotating only a selected set of 
syntactic arguments for the light verb or the true 
predicate, then this issue is exacerbated. In such 
a case there we would have to develop with strict 
rules for which arguments of which pass should 
be included in Pass 3. Pass 3 would no longer be 
automatic, and should be done manually.  
Pass 2: 
True predicate 
- Annotate the Subject argument of the light verb 
with the appropriate role of the true predicate 
- Annotate arguments and modifiers of the true 
predicate without limitation as to the domain of 
locality. 
True predicate?s Frame File 
?+H PDGH DQRWKHU DSSHDUDQFH DW WKH SDUW\? 
ARG1: He 
ARG-ADV: another 
REL: appearance 
ARG-DIR: at court 
Table 3. Method 2 for annotation for Pass 2. Pass 
1 as presented in Table 2 remains unchanged. 
Revised information for Pass 2 is in italics 
 
86
6 Final Annotation Scheme 
6.1 Semantic Fidelity 
Many of the objections so far to Methods 1 and 2 
have centered on the issue of semantic fidelity 
during the annotation of each of the two passes. 
The debate of whether both passes should be 
annotated and to what extent has practical 
implications for the third Pass, as described 
above. However, more importantly it comes 
down to whether or not the semantics of the final 
light verb plus true predicate combination is 
indeed distinct from the semantics of its parts 
(i.e. light verb and true predicate, separately). 
This may be a fascinating linguistic question, but 
it is not something our annotators can be 
debating for each and every instance.   
Instead, we argue that the semantic argument 
structure of the light verb plus true predicate 
combination can in practice be different from 
that of the expressions taken independently as 
has been proposed by various studies (Butt, 
2004; Rosen, 1997; Grimshaw & Mester, 1988). 
Thus, we resolve the cases in which the 
differences in argument roles as assigned by the 
light verb and the nominalization (Section 5.2) 
by handling the argument structure of the 
standard nominalization separately from that of 
the nominalization participating in the LVC. In 
the example ?Chen made another appearance in 
court?, we annotate ?Chen? as the Agent (ARG0) 
of the full predicate ?[make] [appearance]?, 
which is different from the argument structure of 
the standard nominalization which would label 
?Chen? to be the Patient argument (ARG1). 
6.2 Method 3: Final Method 
Our final method of light verb annotation reflects 
the notion that the noun, verb, or adjective as a 
true predicate within an LVC can have a 
different argument structure from that of the 
word alone. Table 4 shows the final annotation 
scheme for light verb construction.  
During Pass 1, the LVCs and their predicating 
expressions are identified in the data. Instances 
identified as LVCs in Pass 1 are then manually 
annotated during Pass 2, annotating the 
arguments and adjuncts of the light verb and the 
true predicate with roles that reflect their 
semantic relationships to the light verb plus true 
predicate. In practice, Pass 1 becomes a way of 
simply manually identifying the light verb 
usages. It is in Pass 2 that we make the final 
choice of argument labels for all of the 
arguments. Thus in Pass 3, the light verb and the 
true predicate lemmas from Pass 1 and 2 are 
joined into a single unit (e.g., in the example 
found in Table 4, the light verb ?took? would be 
joined with the true predicate ?walk? into 
?took+walk?) 3. In this final method, Pass 3 can 
be achieved completely deterministically. 
The major difference in this annotation 
scheme from that of Methods 1 and 2 is that 
instead of annotating in terms of the semantics of 
the bare noun, adjective or verb, the argument 
structure is determined for the entire predicate or 
the full event: semantics of the light verb plus the 
true predicate. This means that for the sentences 
where the argument roles of the verb and the 
nominalization disagree like ?Chen? in ?Chen 
                                                          
3 The order of Pass 2 and Pass 3 as presented in Table 4 is 
arguably a product of how the annotation tools for 
PropBank are set up for Arabic, Chinese, and English. That 
is, the order of the Pass 2 and Pass 3 could potentially be 
flipped provided that the tools and procedures of annotation 
support it, as is the case for Hindi PropBank. After the LVC 
and ARG-PRX are identified in Pass 1, the light verb and 
the true predicate can be deterministically joined into a 
single relation in Pass 2, leaving the manual annotation of 
LVC for Pass 3.  The advantage of this alternative ordering 
is that because the annotation of LVC is done around light 
verb plus the true predicate as a single relation, rather than 
the true predicate alone as in Table 4, the argument 
annotation may in actuality be more intuitive for annotators 
even with less training. 
 Pass 1: Pass 2:  Pass 3: 
 Light Verb Identification LVC Annotation Deterministic relation merge 
Relation Light verb True predicate Light verb + true predicate 
Arguments 
& Modifiers 
- Predicating expression is 
annotated with ARG-PRX 
- Arguments and modifiers of 
the LVCs are annotated 
- Arguments and modifiers 
are taken from Pass 2 
Frame File <no Frame File needed> LVC?s Frame File LVC?s Frame File 
 
Example 
?John took a brisk walk through the park.? 
REL: took 
ARG-PRX: a brisk walk 
ARG0: John 
ARG-MNR: brisk  
REL: walk 
ARGM-DIR: through the park 
ARG0: John 
ARG-MNR: brisk  
REL: [took][walk] 
ARGM-DIR: through the park 
Table 4. Final Annotation Scheme 
87
made another4 appearance in court?, we label the 
argument with the role that is consistent with the 
entire predicate (i.e. Agent, ARG0).  
Frame Files: The final advantage to this 
method is that only one Frame File is needed. 
Since Pass 1 is an identification round, no Frame 
File is required. A single Frame File for LVC 
that includes the argument structure with respect 
to the light verb plus true predicate combination 
will suffice for Pass 2 and Pass 3. 
7 Distinguishing LVCs from MWEs 
As we have discussed in Section 2, we adapted 
our approach from Butt?s (2004) definition of 
LVCs. That is, an LVC is characterized by a 
semantically bleached light verb and a true 
predicate. These elements combine as a single 
predicating unit, in such a way that the light verb 
plus its true predicate can be paraphrased by a 
verbal form of the true predicate without loss of 
the core meaning of the expression (e.g. 
?lectured? for ?gave a lecture?). Also, as 
discussed in Section 6.1, our approach advocates 
the notion that the semantic argument structure 
of the light verb plus true predicate is different 
from that of the expressions taken independently 
(as also proposed by Butt, 2004; Rosen, 1997; 
Grimshaw & Mester, 1988 among others). 
While these definitions are appropriate for the 
PropBank annotation task as we have presented 
it, there are still cases that merit closer attention. 
Even English with a rather limited set of verbs 
that are commonly cited as LVCs, includes a 
problematic mixture of what could arguably be 
termed either LVCs or idiomatic expressions: 
?make exception?, ?take charge?. This difficulty 
in part is the effect of frequency and 
entrenchment of particular constructions.  The 
light verbs themselves do not diminish in form 
over time in a manner similar to auxiliaries (Butt, 
2004), although the complements of common 
LVCs can change over time such that it is no 
longer clear that the complement is a predicating 
element.   
In the case of English, the expressions ?take 
charge? may be more commonly found today as a 
LVC than independently in its verbal form.  As 
we discovered with our annotators, native 
English speakers are uncomfortable using the 
verb ?charge? (i.e. to burden with a 
                                                          
4 The adjective ?another? is annotated as the modifier of the 
full predicate ?[make][appearance]? as it can be interpreted 
to mean that the make appearance event happened a 
previous appearance has been made. 
responsibility) as an independent matrix verb. A 
similar phenomenon can be seen in Arabic, 
where the predicate ??? ???? lit. ?release name? 
exemplifies a prototypical LVC that means ?to 
name?. However, in our data we see cases in 
which the complement is missing, while the 
semantics of the LVC remains intact: 
 ???? ???? ?? ??????? ??????  
CONJ REL be released.he PREP-him/it  
DEF-sector DEF-public 
lit ?Or what is released to it ?the public sector?? 
?Or what is called/named ?the public sector.?? 
This raises the question of: when does a 
construction that may have once been an LVC 
become more properly defined as an idiomatic 
expression due to such entrenchment?  Idiomatic 
expressions can potentially be distinguished from 
LVCs through judgments of how fixed or 
syntactically variable a construction is, and on 
the basis of how semantically transparent or 
decomposable the construction is (Nunberg et. 
al., 1994). However, sometimes the dividing line 
is hard to draw.  
A similar problem arises in determining 
whether a construction is a case of an LVC or 
simply a usage with a distinct sense of the verb. 
Take, for example, the following Arabic 
sentence. 
 ?????? ????? 
   take.he DEF-food 
lit. ?(he) took food? 
?he ate? 
Here, the Arabic word ???? ?food? is the noun 
derivation of the root shared by the verb ???? ?to 
eat?, in such a way that the sentence could be 
rephrased as ???? ?(he) ate?. This example falls 
neatly into the LVC category. However, further 
examples suggest that the example is a case of a 
distinct sense of ?to take orally? where the 
restrictions on the object are that the theme must 
be something that can be taken by mouth: 
?????? ????? 
take.he DEF-medicine 
?he took medicine? 
?????? ????? 
take.he DEF-soup 
?he took soup? 
Finally, determining the appropriate criteria to 
distinguish between a truly semantically 
bleached verb and verbs that seem to be 
participating in complex predication but 
contribute more to the semantics of the 
construction is a challenge for all languages. For 
example, in English data, there are potential 
LVCs with verbs that are not often thought of as 
light verbs, such as ?produce an alteration? and 
88
?issue a complaint?.  Although most English 
speakers would agree that the verbs in these 
constructions do not contribute to the semantics 
of the construction (e.g. ?issue a complaint? can 
be paraphrased to ?to complain?), there are 
similar constructions such as ?register a 
complaint,? wherein the verb cannot be 
considered light. For the purposes of annotation, 
where it is necessary for annotators to understand 
clear criteria for distinguishing light verbs, such 
cases are highly problematic because there is no 
deterministic way to measure the extent to which 
the verbal element contributes to the semantics 
of the construction.  In turn, there is not a good 
way to distinguish some of these borderline 
verbs from their normal, heavy usages.  
Such problems can be resolved by establishing 
language-specific semantic or syntactic tests that 
can be used for taking care of the borderline 
cases of LVCs. However, there is one other 
plausible manner we have identified that could 
help in detecting such atypical LVCs. This can 
be done by focusing on the argument structures 
of predicating complements rather than focusing 
on the verbs themselves.  Grimshaw & Mester 
(1988) suggest that the formation of LVCs 
involves argument transfer from the predicating 
complement to the verb, which is semantically 
bleached and thematically incomplete and 
assigns no thematic roles itself.  Similarly, 
Stevenson et al (2004) suggest that the 
acceptability of a potential LVC depends on the 
semantic properties of the complement.  Thus, 
atypical LVCs, such as the English construction 
?issue a complaint,? can potentially be detected 
during the annotation of eventive nouns, planned 
for all PropBank languages.  
This process will make our treatment of LVCs 
more comprehensive. Used with our language-
specific semantic and syntactic criteria relating to 
both the verb and the predicating complement, it 
will help us to more effectively capture as many 
types of LVCs as possible, including those of the 
V+ADJ and V+V varieties. 
8 Usefulness of our Approach 
Two basic approaches have previously been 
taken to handle all types of MWEs, including 
LVCs in natural language processing 
applications. The first is to treat MWEs quite 
simply as fixed expressions or long strings of 
words with spaces in between; the second is to 
treat MWEs as purely compositional (Sag et al, 
2002). The words-with-spaces approach is 
adequate for handling fixed idiomatic 
expressions, but issues of lexical proliferation 
and flexibility quickly arise when this approach 
is applied to light verbs, which are syntactically 
flexible and can number in the tens of thousands 
for a given language (Stevenson et al, 2004; Sag 
et al, 2002).  Nonetheless, large-scale lexical 
resources such as FrameNet (Baker et al, 1998) 
and WordNet (Fellbaum, 1999) continue to 
expand with entries that are MWEs.   
The purely compositional approach is also 
problematic for light verbs because it is 
notoriously difficult to predict which light verbs 
can grammatically combine with other 
predicating elements; thus, this approach leads to 
problems of overgeneration (Sag et al, 2002).  In 
order to overcome this problem, Stevenson et al 
(2004) attempted to determine which 
nominalizations could form a valid complement 
to the English light verbs take, give and make, 
using Levin?s (1993) verb classes to group 
similar nominalizations.  This approach was 
rather successful for take and give, but 
inconclusive for the verb make.  
Our approach can help to develop a resource 
that is useful whether one takes a words-with-
spaces approach or a compositional approach. 
Specifically, for those implementing a words-
with-spaces approach, the resulting PropBank 
annotation can serve as a lexical resource listing 
for LVCs. For those interested in implementing a 
compositional approach the PropBank annotation 
can serve to assist in predicting likely 
combinations. Moreover, information in the 
PropBank Frame Files can be used to generalize 
across classes of nouns that can occur with a 
given light verb with the help of lexical resources 
such as WordNet (Fellbaum, 1998), FrameNet 
(Baker et. al., 1998), and VerbNet (Kipper-
Schuler, 2005) (in a manner similar to the 
approach of Stevenson et al (2004)). 
Acknowledgements 
We also gratefully acknowledge the support of the 
National Science Foundation Grant CISE-CRI 
0709167, Collaborative: A Multi-Representational 
and Multi-Layered Treebank for Hindi/Urdu, and a 
grant from the Defense Advanced Research Projects 
Agency (DARPA/IPTO) under the GALE program, 
DARPA/CMO Contract No HR0011-06-C-0022, 
subcontract from BBN, Inc.  
Any opinions, findings, and conclusions or 
recommendations expressed in this material are those 
of the authors and do not necessarily reflect the views 
of the National Science Foundation. 
89
Reference 
Alsina, A. 1997. Causatives in Bantu and Romance. 
In A. Alsina, J. Bresnan, and P. Sells eds. Complex 
Predicates. Stanford, California: CSLI 
Publications, p. 203-246. 
Baker, Collin F., Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet Project. In 
Proceedings of the 17th International Conference 
on Computational Linguistics (COLING/ACL-98), 
pages 86?90, Montreal. ACL. 
Butt, M. 2004.  The Light Verb Jungle. In G. Aygen, 
C. Bowern & C. Quinn eds.  Papers from the 
GSAS/Dudley House Workshop on Light Verbs. 
Cambridge, Harvard Working Papers in 
Linguistics, p. 1-50.   
Butt, M. 1997. Complex Predicates in Urdu. In A. 
Alsina, J. Bresnan, and P. Sells eds. Complex 
Predicates. Stanford, California: CSLI 
Publications, p. 107-149. 
Fellbaum, Christine, ed.: 1998, WordNet: An 
Electronic Lexical Database, Cambridge, MA: 
MIT Press.  
Grimshaw, J., and A. Mester. 1988. Light verbs and 
?-marking. Linguistic Inquiry 19(2):205?232. 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational 
Linguistics 28:3, 245-288. 
Goldberg, Adele E. 2003.  ?Words by Default: 
Inheritance and the Persian Complex Predicate 
Construction.? In E. Francis and L. Michaelis 
(eds). Mismatch: Form-Function Incongruity and 
the Architecture of Grammar. CSLI Publications.  
84-112. 
Kipper-Schuler, Karin. 2005. VerbNet: A broad 
coverage, comprehensive verb lexicon. Ph.D. 
thesis, University of Pennsylvania. 
Levin, B. 1993. English Verb Classes and 
Alternations: A Preliminary Investigation. 
Chicago: Chicago Univ. Press.  
Meyers, A., R. Reeves, C. Macleod, R. Szekely, V. 
Zielinska, B. Young, and R. Grishman. 2004. The 
NomBank Project: An interim report. In 
Proceedings of the HLT-NAACL 2004 Workshop: 
Frontiers in Corpus Annotation, pages 24- 31, 
Boston, MA. pages 430?437, Barcelona, Spain. 
Mohanan, T. 1997. Multidimensionality of 
Representation: NV Complex Predicates in Hindi. 
In A. Alsina, J. Bresnan, and P. Sells eds. Complex 
Predicates. Stanford, California: CSLI 
Publications, p. 431-471. 
Martha Palmer, Rajesh Bhatt, Bhuvana Narasimhan, 
Owen Rambow, Dipti Misra Sharma, Fei Xia, 
Hindi Syntax: Annotating Dependency, Lexical 
Predicate-Argument Structure, and Phrase 
Structure, In the Proceedings of the 7th 
International Conference on Natural Language 
Processing, ICON-2009, Hyderabad, India, Dec 
14-17, 2009 
Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Corpus 
of Semantic Roles. Computational Linguistics, 
31(1):71?106. 
Pradhan, Sameer, Kadri Hacioglu, Valerie Krugler, 
Wayne Ward, James H. Martin, Daniel Jurafsky. 
2004. Shallow Semantic Parsing using Support 
Vector Machines. University of Colorado 
Technical Report: TR-CSLR 2003-03. 
Rosen, C. 1997. Auxiliation and Serialization: On 
Discerning the Difference. In A. Alsina, J. 
Bresnan, and P. Sells eds. Complex Predicates. 
Stanford, California: CSLI Publications, p. 175-
202. 
Sag, I., Baldwin, T. Bond, F., Copestake, A., 
Flickinger, D. 2002.  Multiword expressions: A 
pain in the neck for NLP.  In Proceedings of teh 
Third International Conference on Intelligent Text 
processing and Computatinal Linguistics 
(CICLING 2002), p. 1-15, Mexico City, Mexico. 
ACL. 
Stevenson, S., Fazly, A., and North, R. (2004). 
Statistical measures of the semi-productivity of 
light verb constructions. In Proceedings of the 
ACL-04 Workshop on Multiword Expressions: 
Integrating Processing, p. 1?8. 
 
90
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 72?80,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
Incorporating Coercive Constructions into a Verb Lexicon 
Claire Bonial*, Susan Windisch Brown*, Jena D. Hwang*, Christopher Parisien**, 
Martha Palmer* and Suzanne Stevenson** 
*Department of Linguistics, University of Colorado at Boulder 
**Department of Computer Science, University of Toronto 
{Claire.Bonial, Susan.Brown, hwangd, Martha.Palmer}@colorado.edu 
{chris, suzanne}@cs.toronto.edu 
 
 
Abstract 
We take the first steps towards augmenting a lexical 
resource, VerbNet, with probabilistic information 
about coercive constructions. We focus on CAUSED-
MOTION as an example construction occurring with 
verbs for which it is a typical usage or for which it 
must be interpreted as extending the event semantics 
through coercion, which occurs productively and adds 
substantially to the relational semantics of a verb. 
However, through annotation we find that VerbNet 
fails to accurately capture all usages of the 
construction. We use unsupervised methods to 
estimate  probabilistic measures from corpus data for 
predicting usage of the construction across verb 
classes in the lexicon and evaluate against VerbNet. 
We discuss how these methods will form the basis for 
enhancements for VerbNet supporting more accurate 
analysis of the relational semantics of a verb across 
productive usages. 
1 Introduction  
Automatic semantic analysis has been very successful 
when taking a supervised learning approach on data 
labeled with sense tags and semantic roles (e.g., see 
M?rquez et al, 2008). Underlying these recent successes 
are lexical resources, such as PropBank (Palmer et al, 
2005), VerbNet (Kipper et al, 2008), and FrameNet 
(Baker et al, 1998; Fillmore et al, 2002), which encode 
the relational semantics of numerous lexical items, 
especially verbs. However, because authors and speakers 
use verbs productively in previously unseen ways, 
semantic analysis systems must not be limited to direct 
extrapolation from previously seen usages licensed by 
static lexical resources (cf. Pustejovsky & Jezek, 2008). 
To achieve more accurate semantic analyses, we must 
augment such resources with knowledge of the 
extensibility of verbs. 
Central to verb extensibility is the process of semantic 
and syntactic coercion. Coercion allows a verb to be used 
in ?atypical? contexts that extend its relational semantics, 
thereby enabling expression of a novel concept, or simply 
more fluid expression of a complex concept. For 
example, consider a strictly intransitive action verb such 
as blink. This verb may instead be used in a construction 
with an object, as in She blinked the snow off her lashes, 
leading to an interpretation of the verb in which the object 
is causally affected and changes location (the CAUSED-
MOTION construction; Goldberg, 1995). This type of 
constructional coercion is common in language and 
underlies much extensibility of verb usages. 
Understanding such coercive processes thus has 
significant impact on how we should represent 
knowledge about verbs in a lexical resource. 
Importantly, constructional coercion is not an all-or-
nothing process ? a word must be semantically and 
syntactically compatible in some respects with a context 
in order for its use to be extended to that context, but the 
restrictions on compatibility are not hard-and-fast rules 
(Langacker, 1987; Kay & Fillmore, 1999; Goldberg, 
2006; Goldberg, to appear). Gradience of compatibility 
plays an important role in coercion, suggesting that a 
probabilistic approach may be necessary for encoding 
knowledge of constructional coercion in a verb lexicon 
(cf. Lapata & Lascarides, 2003). 
Our hypothesis here is that, due to this gradient process 
of productivity, existing verb lexicons do not adequately 
capture the actual patterns of use of extensible 
constructions. In this paper, we focus on the CAUSED-
MOTION (CM) construction as an initial test case. We first 
annotate the classes of an extensive verb lexicon, 
VerbNet, as to whether the CM construction is allowed 
for all, some, or none of the verbs in the class, noting 
additionally whether it is a typical or coerced usage. We 
find that many of the classes that allow the construction 
for at least some verbs do not include the CM frame in 
their definition, indicating a significant shortcoming in the 
relational knowledge encoded in the lexicon. Next, we 
72
develop probabilistic measures for determining to what 
degree a class is likely to admit the CM construction. We 
then test our measures over corpus data, manually 
annotated for use of the CM construction. Finally, we 
present preliminary work on automatic techniques for 
calculating the proposed measures in an unsupervised 
way, to avoid the need for expensive manual annotation. 
This work forms the preliminary steps toward empirically 
augmenting VerbNet?s predictive capabilities concerning 
the event semantics of verbs in coercible constructions. 
2 Extensible Constructions and VerbNet 
Construction grammar has much insight to offer on the 
topic of productivity and on the resulting statistical 
patterns and gradience of usages (e.g., Langacker, 1987; 
Kay & Fillmore, 1999; Goldberg, 2006). A construction 
is formally defined to be any pairing of linguistic form 
(e.g., a syntactic frame) and meaning. Words can be used 
in constructions to the extent that their lexical semantics is 
compatible with ? or can be coerced to be compatible 
with ? the semantic constraints on the construction. 
It is this notion of constructional coercion, and degree 
of coercibility, that accounts for the richness of usages 
that go beyond those thought of as typical or definitional 
for a verb: by coercing a verb not normally associated 
with a particular frame to occur in it, the meaning of the 
event can take on additional properties not considered a 
core part of the verb?s semantics. For example, in the case 
of the sentence discussed above, She blinked the snow off 
her lashes, it is not the verb but rather the CM 
construction itself that licenses the direct object and adds 
the notion of ?motion causally affecting the object? to the 
event semantics. Amongst other examples of well-known 
constructional coercions are: (1) The CAUSE-RECEIVE 
construction has the syntactic form of NP-V-NP-NP. For 
example, in Bob painted Sally a picture, the simple 
transitive verb paint gains the CAUSE TO RECEIVE sense, 
in which Sally is the recipient and the picture is the 
transferred item. (2) The WAY construction has the form 
of NP-V-[POSS way]-PP. For example, in Frank found 
his way to New York, the construction allows the verb 
find to gain a motion reading (i.e., ?Frank traveled to New 
York?) that would not otherwise be allowed (e.g., *Frank 
found to New York).  
Recognizing such extensions to the relational 
semantics of verbs is very important for accurate 
semantic interpretation in NLP. However, precise 
specifications for capturing the notion of coercible 
constructions, such as are needed for a computational 
resource, have heretofore been lacking. 
2.1 VerbNet & Knowledge of Constructions 
Computational verb lexicons are key to supporting NLP 
systems aimed at semantic interpretation. Verbs express 
the semantics of an event being described as well as the 
relational information among participants in that event, 
and project the syntactic structures that encode that 
information. Verbs are also highly variable, displaying a 
rich range of semantic and syntactic behavior. 
Verb classifications help NLP systems to deal with 
this complexity by organizing verbs into groups that 
share core semantic and syntactic properties. For 
example, VerbNet (derived from Levin?s [1993] work, 
Kipper et al, 2008) is widely used for a number of 
semantic processing tasks, including semantic role 
labeling (Swier and Stevenson, 2004), the creation of 
semantic parse trees (Shi and Mihalcea, 2005), and 
implicit argument resolution (Gerber and Chai, 2010). 
The detailed semantic predicates listed with each 
VerbNet class also have the potential to contribute to text-
specific semantic representations and, thereby, to tasks 
requiring inferencing (Zaenen et al, 2008; Palmer et al, 
2009). 
VerbNet identifies semantic roles and syntactic 
patterns characteristic of the verbs in each class makes 
explicit the connections between the syntactic patterns 
and the underlying semantic relations that can be inferred 
for all members of the class. Each syntactic frame in a 
class has a corresponding semantic representation that 
details the semantic relations between event participants 
across the course of the event. For example, one of the 
characteristic patterns listed for the Pour class is a 
CAUSED-MOTION pattern, which accounts for sentences 
like She poured water from the pitcher into the bowl. This 
is represented in VerbNet as follows: 
Syntactic representation: 
NP V NP PP PP 
Agent V Theme Source Location 
Semantic representation: 
MOTION (DURING(E), THEME)  
NOT (PREP (START(E), THEME, LOCATION)) 
PREP (START(E), THEME, SOURCE) 
PREP (END(E), THEME, LOCATION) 
CAUSE (AGENT, E) 
This representation details connections between the 
syntax and semantics using the semantic roles as links, 
indicating that the Agent is the Subject NP and has 
CAUSED the Event, and that the Theme is the Object NP 
and has a new LOCATION at the end of the event. These 
types of inferences provide the foundation for deep 
semantic analysis of text.  
73
However, the specifications in VerbNet (as in other 
predicate lexicons, such as FrameNet, Baker et al, 1998; 
Fillmore et al, 2002) are seen as definitional ? they are 
restricted to the core usages of the verbs that are valid for 
all verbs in the class. However, as noted above, people 
often use verbs productively, in ways that go beyond the 
boundaries of the verb class structure. It is important to 
correctly identify these productive usages when they 
occur, since they may be explicitly adding crucial 
inferences. If a construction is not recognized in the form 
of a syntactic frame in VerbNet, such inferences are not 
possible, greatly reducing VerbNet?s utility and coverage. 
For example, creative uses of a verb, such as She blinked 
the snow off her lashes, would have no corresponding 
frame in blink?s class, the Hiccup class.  It contains one 
intransitive frame: 
 NP V 
Agent V 
  
 
BODY_PROCESS (E, AGENT) 
INVOLUNTARY (E, AGENT) 
 
Sentences that coerce the meaning of blink to fit with a 
CM event would currently be misanalysed. One option 
might be to augment the Hiccup class with the CM frame 
from the Pour class, which would ensure that such 
sentences would be analyzed more accurately. However, 
given the productive nature of constructional coercion 
and its widespread applicability, the approach of adding 
any possible pattern to each class is not appropriate: this 
would undermine the definitional distinctions between 
classes and greatly lessen their usefulness.  
Complicating the issue is the phenomenon of regular 
sense extensions (Dang et al, 1998), where what once 
may have been coercion has become entrenched and is 
now seen as a different sense of the verb. For example, 
the verbs in the Push class express the general meaning of 
exerting force on an object, such as She pushed on the 
wall. Often, the exertion of force moves the object, which 
can be expressed in a CM construction such as She 
pushed the box across the room. VerbNet accounts for 
this regular sense extension by including most of the Push 
verbs in the Carry class as well, which has the CM 
construction as one of its frames. Deciding when to 
include a verb in another class based on regular sense 
extensions, when to add a frame for a construction to a 
class, or when to reject the frame as a defining part of a 
class, is made difficult by the graded nature of matches 
between verbs and a construction. Our goal is to maintain 
the advantages of the class structure of VerbNet while 
enhancing it with a graded view of the applicability of a 
construction for each class. Noting the applicability of a 
construction will enable the inclusion of its appropriate 
semantic predicates, and the inferencing over them, 
which are currently not supported. 
3 Our Proposal: Constructional Profiles 
We aim to augment VerbNet with knowledge of 
constructions that are likely to be used extensibly with a 
range of verbs. Such extensible constructions will be core 
usages for some classes (such as the CM for the Pour 
class, as noted above) but will be less characteristic of the 
fundamental semantics of other verb classes (such as CM 
for the Hiccup class). We propose to identify such a 
construction and its varying roles in the different classes 
by using relevant statistics over usages of verbs in a 
corpus ? what we call a constructional profile. 
A constructional profile is a probabilistic assessment 
of the usage of a particular construction by the verbs in a 
class. We developed the following three measures to 
capture the relevant behavior, with the goal of providing 
both type- and token-based views of the behavior of a 
verb class with respect to a target construction: 
P1 Ptype(X|C): probability that a verb type in class C is 
attested in construction X 
P1 gives a type-based assessment, indicating how 
widespread the use of the construction is across the 
verb types in the class. For example, if 8 out of 10 
members of a class appear with the construction, we 
might estimate P1 as 0.8. 
P2 Ptoken(X|C): probability that the instances of a typical 
verb in class C occur in construction X 
P2 gives a token-based assessment, indicating, for a 
typical verb in the class, the relative amount of usage of 
the construction among all usages of the verb. For 
example, to estimate this, we might average across all 
verbs in the class, the percentage of tokens in this 
construction. 
P3: Ptoken(X|X-verbs-in-C): same as P2 but considering 
only verbs that have been attested in construction X 
P3 is the same as P2, but looking only at those verbs in 
the class that have an attested usage of the construction, 
removing verbs without attested usages. 
We hypothesize that these measures will have high 
values for those classes for which the construction should 
be definitional; very low values for those classes that are 
not compatible with the construction; and varying values 
for those classes that allow coerced usages to a greater or 
lesser extent. 
Although these probabilities are intuitively very 
simple, estimating them from corpus data poses a 
significant challenge. Since a construction is a pairing of 
form with meaning, recognizing the use of a particular 
74
construction is not simply a matter of determining the 
syntactic pattern of the usage; rather, certain semantic 
properties and relations must co-occur with the syntactic 
pattern. Earlier work has shown that a supervised learning 
method was able to discriminate potential usages of the 
CM construction given training sentences manually 
labeled as either CM or not (Hwang et al, 2010). Here, 
we aim instead to identify usages of the CM construction, 
but without requiring an expensive manual annotation 
effort. That is, we seek an unsupervised method for 
estimating the probabilities in P1?P3 above. 
We approach this goal in steps as follows. First, we 
examine all the classes in VerbNet to see which allow the 
CM construction (Section 4). This anno-tation reveals 
shortcomings in VerbNet?s representa-tion (classes that 
allow the CM construction but do not list it) and also 
provides a gold standard with which to evaluate our 
method of identifying an exten-sible construction using 
our constructional profiles. Second, we use the manually 
annotated CM construction data from Hwang et al 
(2010) to estimate probabilities P1?P3 using maximum 
likelihood formulations (Section 5). An analysis of the 
predictive power of these constructional profile measures 
shows a good match with the distinctions made in the 
human annotation of the classes. Thus, our annotation 
based constructional profile measures show promise for 
identifying relevant behaviors of the construction across 
the classes. Third, we explore automatic methods for 
estimating the constructional profile measures without the 
need for manual annotations (Section 6). We use a 
hierarchical Bayesian model that learns verb classes from 
corpus data to provide unsupervised estimates of the 
constructional profiles, which also exhibit the relevant 
distinctions across the classes. 
4 Annotating the VerbNet Resource  
We begin with a manual examination of the resource and 
a thorough annotation of the status of each class with 
respect to the CM construction. This effort reveals a 
number of shortcomings in VerbNet, and the need for 
developing methods that can support the extension of 
VerbNet to better reflect the coercive uses of 
constructions across the classes. The annotation described 
here also forms the basis for the evaluation in the 
following sections of our new probabilistic measures, by 
motivating hypotheses about the expected patterns of use 
of the CM construction across the classes. 
4.1 Annotation Guidelines and Results 
The first goal of our manual annotation of VerbNet 
classes was to determine which classes currently 
represent CM in one of their frames. To this end, we 
identified which classes contain the following frame:  
NP [Agent/Cause]-V-NP [Patient/Theme]- 
PP [Source/Destination/Recipient/Location]  
These frames correspond to classes such as Slide, with its 
frame NP-V-NP-PP.Destination: Carla slid the books to 
the floor. We also examined classes with the patterns NP-
V-NP-PP.Oblique, NP-V-NP-PP. Theme2, and NP-V-
NP-PP.Patient2. In these classes, annotators had to judge 
whether the final PP was compatible with CM. For 
example, the Breathe class contains the frame NP-V-
NP.Theme-PP.Oblique, The dragon breathed fire on 
Mary, which is compatible with CM; whereas the same 
basic frame in the Other_cos class is not: NP V NP 
PP.Oblique, The summer sun tanned her skin to a golden 
bronze. 
In addition, we annotated which classes were 
potentially compatible with CM for either all verbs in the 
class or only some verbs. The "some" classification has 
the drawback that it may be applied to classes with very 
different proportions of compatible verbs; while suitable 
for our exploratory work here, we plan to make finer 
distinctions in the future. A secondary determination was 
whether or not the class was compatible with CM as part 
of its core semantics, or if it was compatible with CM 
because it was coercible into the construction. A verb was 
considered ?compatible with CM? and ?not coerced? if 
the verb could be used in the CM construction and its 
semantics, as reflected in VerbNet?s semantic predicates, 
involved a CAUSE predicate in combination with another 
predicate such as CONTACT, TRANSFER, (EN)FORCE, 
EMIT, TAKE_IN (predicates potentially involving 
movement along some path). For example, although CM 
is not already included as a frame for the Bend class 
containing the verb fold, the semantics of this class 
include CAUSE and CONTACT, and the verb can be used 
in a CM construction: She folded the note into her 
journal. Therefore, this class would have been considered 
?compatible with CM? but ?not coerced?. Conversely, a 
verb was considered ?compatible with CM? and 
?coerced? if the verb could be used in the CM 
construction, yet its semantics, again as reflected in 
VerbNet, did not involve CAUSE and MOVEMENT 
ALONG A PATH (e.g., the verb wiggle of the 
Body_internal_motion class: She wiggled her foot out of 
the boot). 
In summary, as presented in the table below, we 
annotated each class according to whether (1) the CM 
construction was already represented in VerbNet for this 
class, (2) the construction was possible for all, some, or 
75
none of the verbs in that class, and (3) the verbs of any 
class compatible with CM were coerced into the 
construction or not. The classification for (3) was made 
regardless of whether ?all? verbs or only ?some? were 
compatible with CM. This determination was made 
uniformly for a class: there were no classes in which only 
certain CM-compatible verbs were considered ?coerced?.  
VN class example  
[# of classes like this] 
CM in 
VN 
CM is 
possible 
CM is 
coerced 
Banish [50] Yes All No 
Nonverbal_Expression [2] Yes All Yes 
Cheat [6] Yes Some No 
Exhale [18] No All No 
Hiccup [30] No All Yes 
Fill [46] No Some No 
Wish [54] No Some Yes 
Matter [64] No None N/A 
Notably, we identified 206 classes where at least some of 
the verbs in that class are compatible with the CM 
construction; however, VerbNet currently only 
recognizes the CM construction in 58 classes. There were 
several classes of interest: First, although it may seem 
unusual that CM is represented in 6 classes where we 
found that only ?some? verbs were compatible with CM 
(e.g., Cheat class), these were cases where only more 
restricted subclasses are compatible with CM, and this 
syntactic frame is listed for that subclass. This suggests 
subclasses may provide a more precise characterization 
of which verbs are compatible with a construction.  
Secondly, we identified 18 classes in which all verbs 
were compatible with CM without coercion; thus, these 
classes could likely be improved by the addition of the 
CM syntactic frame. Additionally, we found 30 classes in 
which all verbs are coercible into the CM construction; 
however, the actual likelihood of a verb in those classes 
occurring in a CM construction remains to be 
investigated in the following sections. Like those classes 
where it was determined that only ?some? verbs are 
compatible with CM, usefully incorporating the CM 
construction into classes that require coercion relies on 
accurately determining the probability that verbs in those 
classes will actually appear in the CM construction.  
For those classes in which ?all? verbs are compatible 
with CM, our intuition was that some aspect of the verb?s 
semantics either inherently includes or allows the verb to 
be coerced into the CM construction. Conversely, for 
those classes in which no verbs are compatible with CM, 
presumably some aspect of the verb?s semantics is 
logically incompatible with CM. Although pinpointing 
precisely what aspect of a verb?s semantics makes it 
compatible with CM may not be possible, we can 
investigate whether or not our intuitions are supported by 
examining the actual frequencies of CM constructions for 
given verbs or a given class.  
4.2 Hypotheses  
Using these annotations, we were able to develop two 
simple hypotheses. 
Hypothesis 1: We expect the constructional profile 
measures for the CM construction in a given corpus to be 
highest for those classes in which all verbs were found to 
be compatible with CM; lower for classes in which only 
some verbs were found to be compatible; and lowest for 
classes in which no verbs were found to be compatible. 
Hypothesis 2: We expect the constructional profile 
measures for the CM construction in a given corpus to be 
highest for verbs that fall into classes where CM is not 
considered coerced (for either some or all of the verbs in 
the class); lower for verbs that fall into classes in which 
the CM construction only works through coercion (for 
either some or all of the verbs in the class); and lowest for 
verbs that fall into classes in which no verbs are 
compatible with CM.  
To investigate Hypothesis 1, we grouped the annotated 
classes according to whether all, some, or no verbs in the 
class are compatible with CM: 
 Class example # of classes 
Allowed by All Bring, Carry 106 
Allowed by Some Appoint, Lodge 100 
Allowed by None Try, Own 64 
To investigate Hypothesis 2, we did a second grouping 
of the classes according to whether CM is not coerced, 
CM is coerced, or CM is simply not compatible with the 
class. This second grouping did not distinguish whether 
CM was compatible with ?all? or ?some? of the verbs in 
a given class. 
 Class example # of classes 
Not Coerced Put, Throw 120 
Coerced Floss, Wink 86 
Not Compatible Differ 64 
5 Evaluation using Constructional Profiles 
5.1 Annotated data description 
Our research uses the data annotated for Hwang et al 
(2010), in which 1800 instances in the form NP-V-NP-
PP were identified in the Wall Street Journal portion of 
the Penn Treebank II (Marcus et al, 1994). Each instance 
76
of the data was single annotated with one of the two 
labels: CM or non-CM. The annotation guidelines were 
based on the CM analysis of Goldberg (1995). 
Our analysis began with the same data but adopted a 
slightly narrower definition of CM. We diverged from 
the Hwang et al (2010) study in the following two ways: 
(1) sentences where the object NP is an item that is 
created by the event denoted by the verb were not 
considered CM (e.g., Mr. Pilson scribbled a frighteningly 
large figure on a slip of paper, where the figure is created 
through the scribbling event); and (2) sentences in which 
movement is prevented were not considered CM (e.g., 
He kept her at arm?s length). In agreement with Hwang 
et al, our annotation included both metaphorical senses 
(e.g., [It] cast a shadow over world oil markets) and 
literal senses (e.g., The company moved the employees to 
New York) of CM. Our annotation using the narrower 
guidelines resulted in 85.8% agreement with the original 
annotation.1  The distribution of labels in our data is 
21.8% for CM and 78.2% for NON-CM. 
5.2 Annotated data description 
Using statistics over the manually annotated data, we 
calculate maximum likelihood estimates of the three 
constructional profile measures introduced in Section 3, 
as follows. First, let the probability that a verb v is used in 
the CM construction be estimated as: 
P(CM|v,C) = 
#(CM usages of     ) 
#(CM+non-CM usages of    ) 
That is, P(CM|v,C) is estimated as the relative frequency 
of the CM construction for v out of all annotated usages 
of v that are labeled as class C. Now let CCM be all verbs v 
in C with at least one usage annotated as CM; i.e.: 
    *      |  (  |   )    + 
Then we calculate estimates of P1?P3 as: 
P1: Ptype(CM|C) = |CCM |/|C| 
This measure indicates how widespread the use of CM is 
across the verb types in the class. 
P2: Ptoken(CM|C) =,?  (  |   )   - | |?  
The average over all verbs v in C of P(CM|v,C) 
This indicates the relative amount of usage of CM among 
all usages of the verbs in the class.  
P3: Ptoken(CM|v,C) = [?  (  |   ))- |   |       
The average over all verbs v in CCM of P(CM|v,C) 
P3 narrows the P2 measure to only those verbs in the 
                                                          
1We found that 34.0% of the disagreements were directly due to 
the changes in annotation resulting from our two new criteria. 
class for which there is an attested usage of CM. 
5.3 Analysis of the Constructional Profiles 
The tables below provide a summary of the profile 
measures P1-P3 for the groups of VerbNet classes as 
defined in section 4.2. For each group listed, we report 
the averages of P1-P3 over all classes in the group where 
at least one verb in the class occurred in the data 
manually annotated for CM usage. 
 P1 P2 P3 
CM Allowed by All 0.413 0.323 0.437 
CM Allowed by Some 0.087 0.078 0.224 
CM Not Allowed 0.055 0.055 0.083 
As seen here, the constructional profile measures over 
CM in the data corroborate our Hypothesis 1 (Section 
4.2). All three measures on average are highest for the 
classes that fall into the ?all allowed? group, next highest 
for those in the ?some allowed? group, and lowest for the 
?not allowed? classes.  
 P1 P2 P3 
CM Non-Coerced 0.354 0.274 0.418 
CM Coerced 0.091 0.091 0.185 
CM Not Allowed2 0.056 0.056 0.083 
Furthermore, the second table here confirms our 
expectations for Hypothesis 2 (Section 4.2). Again, all 
three measures on average are highest for classes that fall 
into the ?non-coerced? group, next highest for classes in 
the ?coerced? group (in which the construction is 
achievable only through coercion), and lowest for the 
?not allowed? group.  
Thus, our two hypotheses are borne out, showing that 
our constructional profile measures, when estimated over 
manually annotated data, can be useful in capturing 
important distinctions among classes of verbs with regard 
to their usage in an extensible construction such as CM. 
6 Automatic Creation of Constructional 
Profiles Using a Bayesian Model  
Manually annotating a corpus for usages of a con-
struction can be prohibitively expensive, so we also 
investigate the use of automatic methods to estimate 
constructional profile measures. By using a hierarchi-cal 
Bayesian model (HBM) that acquires latent prob-abilistic 
verb classes from corpus data, we provide unsupervised 
                                                          
2 Note the non-zero values result from actual CM verb usages in 
the data belonging to classes believed to be not compatible with 
CM by VerbNet expert annotators. 
77
estimates of the constructional profiles. 
6.1 Overview of Model and Data 
We use the HBM of Parisien & Stevenson (2011), a 
model that automatically acquires probabilistic 
knowledge about verb argument structure and verb 
classes from large-scale corpora. The model is based on a 
large body of research in nonparametric Bayesian topic 
modeling (e.g., Teh et al, 2004), a robust method of 
discovering syntactic and semantic structure in very large 
datasets. For each verb encountered in a corpus, the 
model provides an estimate of the verb?s expected overall 
pattern of usage. By using latent probabilistic verb classes 
to influence these expected usage patterns, the model can, 
for example, estimate the probability that a verb like blink 
might occur in a CM construction, even if no such 
attested usages appear in the corpus. 
In this preliminary study, we use the corpus data from 
Parisien & Stevenson (2011), since the model has been 
trained and evaluated on this data. As that study was 
aimed at modeling facts of child language acquisition, it 
uses child-directed speech from the Thomas corpus 
(Lieven et al, 2009), part of the CHILDES database 
(MacWhinney, 2000). In this preliminary study, we use 
their development dataset containing approx. 170,000 
verb usages, covering approx. 1,400 verb types. (We 
reserve the test set for future experiments.) For each verb 
usage in the input, a number of features are automatically 
extracted that indicate the number and type of syntactic 
arguments occurring with the verb and general semantic 
properties of the verb. The semantic features are drawn 
from the set of VerbNet semantic predicates, such as 
CAUSE, MOTION, and CONTACT. These are automatically 
extracted from all classes compatible with the verb (with 
no sense disambiguation). 
6.2 Measures for Constructional Profiles 
Using the argument structure constructions, verb usage 
patterns and classes learned by the model, we estimate 
the three constructional profile measures in Section 3, as 
follows. First, we note that since the constructions 
acquired by the model are probabilistic in nature, a 
particular CM instance may be a partial match to more 
than one of the model?s constructions.  
For each verb in the input, we consider the likelihood 
of use of the CM construction to be the likelihood of a 
contrived frame intended to capture the important 
properties of a CM usage. FCM is a usage taking a direct 
object and a prepositional phrase, and including the 
semantic features CAUSE and MOTION, with all other 
semantic features left unspecified. For a given verb v, we 
estimate the likelihood of this CM usage, over all 
constructions in the model, as follows: 
 (   | )  ? (   | ) (
 
 | ) 
Here, P(FCM |k) is the likelihood of the CM usage FCM 
being an instance of the probabilistic construction k, and 
P(k|v) is the likelihood that verb v occurs with 
construction k. These component probabilities are 
estimated using the probability distributions acquired by 
the model and averaged over 100 samples from the 
Markov Chain Monte Carlo simulation, as described in 
Parisien & Stevenson (2011). 
Now, we let CCM be the set of verbs in VerbNet class 
C where the expected likelihood of a CM usage is non-
negligible (akin to the set of verbs with attested usage in 
Section 5.2): 
CCM = {v C | P(FCM|v)>? } 
where ? is a small threshold, here 0.0001. Note that since 
v is not disambiguated for class in our data, all usages of v 
contribute to this estimate. 
The estimates of P1-P3 are comparable to those in 
Section 5.2. The difference is that since we are un-able to 
disambiguate individual usages of the verbs, each usage 
of v is considered to belong to all possible classes C of 
which v is a member. P1 is estimated as before; P2 and 
P3 are averages of P(FCM|v). 
6.3 Analysis of the Constructional Profiles 
The tables below provide a summary of the profile 
estimates P1-P3 for the groups of VerbNet classes as 
given in Section 4.2. For each group listed, we report the 
averages of P1-P3 over all classes in the group where at 
least one of the verbs in the class occurred in the training 
input to the model. 
 P1 P2 P3 
All allowed 0.569 0.0180 0.0250 
Some allowed 0.449 0.0106 0.0192 
Not allowed 0.363 0.0044 0.0079 
These profile measures align with the hypotheses in 
Section 4.2 and with the measures based on manually 
annotated data in Section 5.2. The estimates are high-est 
for classes where all verbs permit the CM con-struction, 
second highest for classes where only some permit it, and 
lowest for classes that do not permit it. 
 P1 P2 P3 
CM non-coerced 0.546 0.0178 0.0260 
CM coerced 0.458 0.0095 0.0167 
CM not allowed 0.363 0.0044 0.0079 
78
Again, the overall patterns of the profile measures align 
with Sections 4.2 and 5.2. The profile estimates are 
highest for classes annotated to be non-coerced usages of 
CM, second highest for coerced classes, and lowest for 
?not allowed?.  
The measures show the overall differences among 
classes in the different groups (for both groupings) ? i.e., 
the average behavior among classes in the different 
groups varies as we predicted.  This indicates that the 
measures are tapping into aspects of construction usage 
that are relevant to making the desired distinctions in 
VerbNet, and validates the use of automatic 
techniques.  However, there is a substantial amount of 
variability in these measures across the classes, so we also 
consider how well the estimates can predict the 
appropriate group for individual classes. That is, can we 
automatically predict whether the CM construction can 
be used by all, some, or none of the verbs in a given verb 
class, and can we predict whether such usages are 
coerced? 
We consider the P3 measure as it provides the best 
separation among the class groupings. The tables below 
report precision (P), recall (R) and F-measures (F) for 
each group, where ?all? and ?some? have been collapsed. 
For exploratory purposes, we pick P3 = 0.006 as the 
value that optimizes F-measures of this classification. 
Future work will explore more principled means for 
setting these thresholds. 
 P R F 
CM allowed 0.880 0.742 0.806 
CM not allowed 0.407 0.636 0.497 
Only a 2-way distinction can be made reliably for the 
allowed grouping. The F-score of over 80% for the 
?allowed? label is very promising. The low precision for 
the ?not allowed? case suggests that the model can?t 
generalize sufficiently due to sparse data. 
 P R F 
CM non-coerced 0.691 0.491 0.574 
CM coerced 0.461 0.417 0.438 
CM not allowed 0.406 0.709 0.517 
We use thresholds of P3 = 0.021 to separate non-coerced 
from coerced classes, and P3 = 0.007 to separate coerced 
from not allowed classes. The model estimates show 
moderate success in distinguishing classes with coerced 
vs. non-coerced usage of the CM construction. However, 
our measures simply cannot distinguish non-occurrence 
due to semantic incompatibility from non-occurrence due 
to chance, given the expected low frequency of a novel 
coerced use of a construction.  To separate the allowed 
cases into whether they are coerced or not requires a 
more detailed assessment of the semantic compatibility of 
the class, which means looking at finer-grained features 
of verb usages that are indicative of the semantic 
predicates compatible with the particular construction.  
Moreover, this kind of assessment likely needs to be 
applied on a verb-specific (and not just class-specific) 
level, in order to identify those verbs out of a potentially 
coercible class that are indeed coercible (i.e., identifying 
the coercible verbs in a class labeled as "some allowed"). 
7 Conclusion 
Our investigation demonstrates that VerbNet does not 
currently represent the CM construction for all verbs or 
verb classes that are compatible with this construction, 
and the existing static representation of verbs is 
inadequate for analyzing extensions of verb meaning 
brought about by coercion. The utility of VerbNet would 
be greatly enhanced by an improved representation of 
constructions: specifically, the incorporation of 
probabilities that verbs in a given (sub)class would occur 
in a particular construction, and whether this constitutes a 
regular sense extension. This addition to VerbNet would 
increase the resource?s coverage of syntactic frames that 
are compatible with a given verb, and therefore enable 
appropriate inferences when coercion occurs. We have 
made preliminary steps towards developing this 
probabilistic distribution over both verb instances and 
classes, based on a large corpus. Unsupervised methods 
for estimating the probabilities achieve an F-score of over 
80% in distinguishing the classes that allow the target 
construction. However, making distinctions among 
coerced and non-coerced cases will require us to go 
beyond these class-based probabilities to finer-grained, 
corpus-based assessments of a verb?s semantic 
compatibility with a coercible construction.  
To move beyond these preliminary findings, we must 
therefore shift our focus to the behavior of individual 
verbs. Additionally, to reduce the impact of errors 
resulting from low-frequency verbs and classes, we plan 
to expand our research to more data, specifically the 
OntoNotes TreeBank data (Weischedel et al, 2011). 
Finally, to achieve our ultimate goal of creating a lexicon 
that can flexibly account for a variety of constructions, we 
will examine other constructions as well. While 
determining the set of coercible constructions in a 
language is itself a topic of current research, we propose 
initially to include the widely recognized CAUSE-
RECEIVE and WAY constructions in addition to CM. 
79
References  
Baker, Collin F., Charles J. Fillmore, and John B. Lowe. 1998. 
The Berkeley FrameNet Project. Proceedings of the 17th 
International Conference on Computational Linguistics 
(COLING/ACL-98), pp. 86?90, Montreal. 
Dang, HoaTrang, Karin Kipper, Martha Palmer, and Joseph 
Rosenzweig. 1998. Investigating regular sense extensions 
based on intersective Levin classes. Proceedings of 
COLING-ACL98, pp. 293?299. 
Fillmore, Charles J., Christopher R. Johnson, and Miriam R.L. 
Petruck. 2002. Background to FrameNet. International 
Journal of Lexicography, 16(3):235-250.  
Gerber, Matthew, and Joyce Y. Chai. 2010. Beyond 
NomBank: A study of implicit arguments for nominal 
predicates. Proceedings of the 48th Annual Meeting of the 
Association of Computational Linguistics, pp. 1583?1592, 
Uppsala, Sweden, July. 
Goldberg, A. E. 1995. Constructions: A construction 
grammar approach to argument structure. Chicago: 
University of Chicago Press. 
Goldberg, A. E. 2006. Constructions at work: The nature of 
generalization in language. Oxford: Oxford University 
Press. 
Goldberg, A. E. To appear. Corpus evidence of the viability of 
statistical preemption. Cognitive Linguistics. 
Hwang Jena D., Rodney D. Nielsen and Martha Palmer. 2010. 
Towards a domain-independent semantics: Enhancing 
semantic representation with construction grammar. 
Proceedings of Extracting and Using Constructions in 
Computational Linguistic Workshop, held with NAACL 
HLT 2010, Los Angeles, June. 
Kay, P., and C. J. Fillmore. 1999. Grammatical constructions 
and linguistic generalizations: The What's X Doing Y? 
construction. Language, 75:1?33. 
Kipper, Karin, Anna Korhonen, Neville Ryant, and Martha 
Palmer. 2008. A large-scale classification of English verbs. 
Language Resources and Evaluation Journal, 42:21?40. 
Langacker, R. W. 1987. Foundations of cognitive grammar: 
Theoretical prerequisites. Stanford, CA: Stanford 
University Press. 
Lapata, M., and A. Lascarides. 2003. Detecting novel 
compounds: The role of distributional evidence. 
Proceedings of the 11th Conference of the European 
Chapter of the Association for Computational 
Linguistics(EACL03), pp.235?242. Budapest, Hungary. 
Levin, B. 1993.English Verb Classes and Alternations: A 
Preliminary Investigation. Chicago: Chicago University 
Press.  
 
 
Lieven, E., D. Salomo, and M. Tomasello. 2009. Two-year-
old children?s production of multiword utterances: A 
usage-based analysis. Cognitive Linguistics 20(3):481?507. 
MacWhinney, B. 2000.The CHILDES Project: Tools for 
analyzing talk (3rd ed., Vol. 2: The Database). Erlbaum. 
M?rquez, L., X. Carreras, K. Litkowski, and S. Stevenson. 
2008. Semantic role labeling: An introduction to the special 
issue. Computational Linguistics, 34(2): 145?159. 
Martha Palmer, Jena D. Hwang, Susan Windisch Brown, 
Karin Kipper Schuler and Arrick Lanfranchi. 2009. 
Leveraging lexical resources for the detection of event 
relations. Proceedings of the AAAI 2009 Spring 
Symposium on Learning by Reading, Stanford, CA, March. 
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.2005. 
The Proposition Bank: An annotated corpus of semantic 
roles. Computational Linguistics, 31(1):71?106. 
Parisien, Christopher, and Suzanne Stevenson. 2011. To 
appear in Proceedings of the 33rd Annual Meeting of the 
Cognitive Science Society, Boston, MA, July. 
Pustejovsky, J., and E. Jezek. 2008. Semantic coercion in 
language: Beyond distributional analysis. Italian Journal of 
Linguistics/RivistaItaliana di Linguistica 20(1): 181?214. 
Shi, Lei, and Rada Mihalcea. 2005. Putting pieces together: 
Combining FrameNet, VerbNet and WordNet for robust 
semantic parsing. Proceedings of the 6th International 
Conference on Intelligent Text Processing and 
Computational Linguistics, Mexico City, Mexico. 
Swier, R., and S. Stevenson. 2004. Unsupervised semantic 
role labeling. Proceedings of the 2004 Conf. on Empirical 
Methods in Natural Language Processing, pp. 95?102, 
Barcelona, Spain. 
Teh, Y. W., M. I. Jordan, M. J.Beal, and D. M.Blei.2006. 
Hierarchical Dirichlet processes. Jrnl of the American 
Statistical Asscn, 101(476): 1566?1581. 
Weischedel, R., E. Hovy, M. Marcus, M. Palmer, .R. Belvin, 
S. Pradan, L. Ramshaw and N. Xue. 2011.OntoNotes: A 
Large Training Corpus for Enhanced Processing. In Part 1: 
Data Acquisition and Linguistic Resources of The 
Handbook of Natural Language Processing and Machine 
Translation: Global Automatic Language Exploitation, 
Eds.: Joseph Olive, Caitlin Christianson, John McCary. 
Springer Verlag, pp. 54-63. 
Zaenen, A., C. Condoravdi, and D. G. Bobrow. 2008. The 
encoding of lexical implications in VerbNet. Proceedings 
of LREC 2008, Morocco, May. 
80
