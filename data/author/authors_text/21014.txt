Building a Web-based parallel corpus and filtering out machine-
translated text 
 
 
Alexandra Antonova, Alexey Misyurev 
Yandex 
 16, Leo Tolstoy St., Moscow, Russia 
{antonova, misyurev}@yandex-team.ru 
 
 
 
 
 
 
Abstract 
We describe a set of techniques that have 
been developed while collecting parallel 
texts for Russian-English language pair and 
building a corpus of parallel sentences for 
training a statistical machine translation 
system. We discuss issues of verifying 
potential parallel texts and filtering out 
automatically translated documents. Finally 
we evaluate the quality of the 1-million-
sentence corpus which we believe may be a 
useful resource for machine translation 
research. 
1 Introduction 
The Russian-English language pair is rarely used 
in statistical machine translation research, because 
the number of freely available bilingual corpora for 
Russian-English language pair is very small 
compared to European languages. Available 
bilingual corpora1 often belong to a specific genre 
(software documentation, subtitles) and require 
additional processing for conversion to a common 
format. At the same time many Russian websites 
contain pages translated to or from English. 
Originals or translations of these documents can 
also be found in the Internet. By our preliminary 
estimates these bilingual documents may yield 
more than 100 million unique parallel sentences 
                                                          
1
 e.g. http://opus.lingfil.uu.se/ 
while it is still a difficult task to find and extract 
them. 
The task of unrestricted search of parallel 
documents all over the Web including content-
based search is seldom addressed by researchers. 
At the same time the properties of the set of 
potential parallel texts found in that way are not 
well investigated. Building a parallel corpus of 
high quality from that kind of raw data is not 
straightforward because of low initial precision, 
frequent embedding of nonparallel fragments in 
parallel texts, and low-quality parallel texts. In this 
paper we address the tasks of verification of 
parallel documents, extraction of the best parallel 
fragments and filtering out automatically translated 
texts. 
Mining parallel texts from a big document 
collection usually involves three phases: 
? Detecting a set of potential parallel 
document pairs with fast but low-precision 
algorithms 
? Pairwise verification procedure 
? Further filtering of unwanted texts, e.g. 
automatically translated texts 
 
Finding potential parallel texts in a collection of 
web documents is a challenging task that does not 
yet have a universal solution. There exist methods 
based on the analysis of meta-information (Ma and 
Liberman, 1999; Resnik, 2003; Mohler and 
Mihalcea, 2008, Nadeau and Foster 2004), such as 
URL similarity, HTML markup, publication date 
and time. More complicated methods are aimed at 
136
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 136?144,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
detecting potential parallel texts by their content. 
In this case mining of parallel documents in the 
Internet can be regarded as the task of near-
duplicate detection (Uszkoreit et al, 2010). All of 
the above mentioned approaches are useful as each 
of them is able to provide some document pairs 
that are not found by other methods. 
In our experiments, fast algorithms of the first 
phase classify every pair of documents as parallel 
with very low precision, from 20% to 0.001%. 
That results in a huge set of candidate pairs of 
documents, for which we must decide if they are 
actually parallel or not. For example, if we need to 
get 100 000 really parallel documents we should 
check from 500 thousand to 100 million pairs. The 
large number of pairwise comparisons to be made 
implies that the verification procedure must be fast 
and scalable. Our approach is based on a sentence-
alignment algorithm similar to (Brown et al, 1991; 
Gale and Church, 1993; Chen, 1993; Moore 2002; 
Ma, 2006) but it is mainly aimed at achieving high 
precision rather than high recall. The algorithm is 
able to extract parallel fragments from comparable 
documents, as web documents often are not exactly 
parallel. The similarity estimate relies on 
probabilistic dictionary trained on initial parallel 
corpus and may improve when the corpus grows.  
Due to growing popularity of machine 
translation systems, Russian websites are being 
increasingly filled with texts that are translated 
automatically. According to selective manual 
annotation the share of machine translation among 
the texts that have passed the verification 
procedure is 25-35%. Machine-translated 
sentences  often demonstrate better word 
correspondence than human-translated sentences 
and are easier to align, but the longer phrases 
extracted from them are likely to be unnatural and 
may confuse the statistical translation system at the 
training stage. The large share of automatically 
translated data decreases the value of the corpus, 
especially if it is intended for research. Also it will 
make it difficult to outperform the translation 
quality of the system which generated those 
sentences. 
To the best of our knowledge, there is no 
existing research concerning the task of filtering 
out machine translation. Our filtering method is 
based on a special decoding algorithm that 
translates sentence-aligned document and then 
scores the output against the reference document 
with BLEU metric. This method allows reducing 
the number of automatically translated texts to 5% 
in the final corpus. 
Our final goal is to build a quality corpus of 
parallel sentences appropriate for training a 
statistical machine translation system. We evaluate 
the 1-million-sentence part of our corpus by 
training a phrase-based translation system (Koehn 
et al, 2007) on these sentences and compare the 
results with the results of training on noisy data, 
containing automatically translated texts as its part. 
The rest of the paper is organized as follows: 
Section 2 provides an overview of the system 
architecture and addresses specific problems at the 
preparatory stage. Section 3 describes the 
sentence-alignment algorithm and the pairwise 
verification procedure. The algorithm makes use of 
statistical dictionaries trained beforehand. In 
Section 4 we discuss the problem of filtering out 
automatically translated texts. In Section 5 we 
evaluate the quality of the final parallel corpus and 
provide some statistical information about 
Russian-English language pair. We conclude in 
Section 6 with short summary remarks. 
2 System description  
The corpus building procedure includes several 
stages represented in Figure 1. Initial training 
provides bilingual probabilistic dictionaries which 
are used in sentence alignment and verification of 
potential parallel texts. We used Russian/English 
correspondent pages from a number of bilingual 
web-sites of good quality. We performed robust 
alignment based on sentence lengths as in (Gale 
and Church, 1993). The obtained probabilistic 
dictionaries were gradually improved in a sort of a 
bootstrapping procedure when the corpus size 
increased. 
Our main source of Web documents are web 
pages from search engine database with their 
textual contents already extracted and sentence 
boundaries detected. Nevertheless documents often 
include sentences that are site-specific and carry 
some meta-information, advertising, or just some 
noise. When often repeated such sentences may 
confuse statistical training, so we choose to delete 
subsequent sentences that have been encountered 
recently. 
 
137
  
Figure 1. Corpus building procedure. 
 
In morphologically rich languages nouns, verbs 
and adjectives have many different forms in text, 
which complicates statistical training, especially 
when the initial collection is comparatively small. 
At the same time, the task of sentence alignment 
relies on robust algorithms which allow for some 
data simplification. Word stemming, truncation of 
word endings and lemmatization may be used to 
reduce the data sparseness problem when dealing 
with morphologically rich languages. The accurate 
lemmatization algorithms for Russian language are 
complicated and comparatively slow because they 
should resolve morphological ambiguity as many 
word forms have more than one possible lemma. 
We chose a simple and fast algorithm of 
probabilistic lemmatization where a word is always 
assigned the most frequent of its possible lemmas. 
There are several reasons why it is appropriate for 
the task of sentence and word alignment:  
? The algorithm runs almost as fast as the 
word truncation method, and in most cases 
it yields correct lemmas. 
? Most of the information is contained in 
low-frequency words and those are usually 
less ambiguous than the frequent words. 
? Individual mistakes in lemmatization do 
not necessarily result in wrong similarity 
estimation for the whole sentence. 
3 Verification of potential parallel 
documents 
Potential parallel documents are a pair of texts; 
each of them represents the textual content of some 
HTML page. The size of texts may vary from 
several sentences to several thousand sentences. 
Our approach to the task of verification of 
potential parallel documents is motivated by the 
properties of the set of potential parallel texts, 
which is the output of different search algorithms 
including unrestricted content-based search over 
the Web. 
The first problem is that most of the potential 
parallel texts on the Web, even if they prove to 
have parallel fragments, often contain non-parallel 
fragments as well, especially at the beginning or at 
the end. Since the parallel fragment can be located 
anywhere in the document pair, the verification 
algorithm performs exhaustive dynamic 
programming search within the entire document 
and not only within a fixed width band around the 
main diagonal. Our similarity measure relies 
heavily on features derived from the sentence 
alignment of the best parallel fragment and does 
not utilize any information from the rest of the text. 
We allow that the parallel fragment begins and 
ends anywhere in the text and also it is possible to 
skip one or several sentences without breaking the 
fragment. 
 We have also considered the possibility that 
documents can contain more than one parallel 
fragment separated by greater non-parallel 
fragments. Though such documents do exist, the 
contribution of lesser parallel fragments to parallel 
corpus is insignificant compared to much simpler 
case where each pair of documents can contain 
only one parallel fragment. 
The second problem of the input data is low 
initial precision of potential parallel texts and the 
fact that there are many comparable but not 
parallel texts. It is worth noting that the marginal 
and joint probabilities of words and phrases in the 
138
set of documents with similar content may differ 
substantially from the probabilities obtained from 
the parallel corpus of random documents. For this 
reason we cannot completely rely on statistical 
models trained on the initial parallel corpus. It is 
important to have a similarity measure that allows 
for additional adjustment in order to take into 
account the probability distributions in the 
potential parallel texts found by different search 
algorithms. 
The third problem is the large number of 
pairwise comparisons to be made. It requires that 
the verification procedure must be fast and 
scalable. Due to the fact that the system uses 
precomputed probabilistic dictionaries, each pair of 
documents can be processed independently and 
this stage fits well into the MapReduce framework 
(Dean and Ghemawat, 2004). For example, 
verification of 40 million pairs of potential parallel 
texts took only 35 minutes on our 250-node 
cluster. 
The algorithm of verifying potential parallel 
documents takes two texts as input and tries to find 
the best parallel fragment, if there is any, by 
applying a dynamic programming search of 
sentence alignment. We use sentence-alignment 
algorithm for handling four tasks: 
? Search of parallel fragments in pairs 
? Verification of parallel document pairs 
? Search of per-sentence alignment 
? Filtering out sentences that are not 
completely parallel 
Each sentence pair is scored using a similarity 
measure that makes use of two sources of prior 
statistical information: 
? Probabilistic phrase dictionary, consisting 
of phrases up to two words 
? Empirical distribution of lengths of 
Russian/English parallel sentences 
 
Both have been obtained using initial parallel 
corpus. In a sort of bootstrapping procedure one 
can recalculate that prior statistical information as 
soon as a bigger parallel corpus is collected and 
then realign the input texts. 
The algorithm neither attempts to find a word 
alignment between two sentences, nor it tries to 
translate the sentence as in (Uszkoreit et al, 
2010). Instead, it takes account of all phrases from 
probabilistic dictionary that are applicable to a 
given pair of sentences disregarding position in the 
sentence or phrase intersection. Our probabilistic 
dictionar? consists of 70?000 phrase translations of 
1 or 2 words.  
Let S and T be the set of source/target parts of 
phrases from a probabilistic dictionary, and 
TSE ?? - the set of ordered pairs, representing 
the source-target dictionary entries ? ?ts, . Let the 
source sentence contain phrases SS ?0 and the 
target sentence contain phrases TT ?0 . Then the 
similarity between the two sentences is estimated 
by taking the following factors into account: 
? ? ?tsp | , ? ?stp | , translation probabilities; 
? TS lenlen , , length of source and target 
sentences; 
? ? ?TS lenlenp ,log ? , the empirical 
distribution of length correspondence 
between source and target sentences. 
 
The factors are log-linearly combined and the 
factor weights are tuned on the small development 
set containing 700 documents. We choose the 
weights so that the result of comparison of 
nonparallel sentences is usually negative. As a 
result of the search procedure we choose a parallel 
fragment with the biggest score.  If that score is 
above a certain threshold the parallel fragment is 
extracted, otherwise the whole document is 
considered to be nonparallel. 
Relative sentence order is usually preserved in 
parallel texts, though some local transformations 
may have been introduced by the translator, such 
as sentence splitting, merge or swap. Though 
sentence-alignment programs usually try to detect 
some of those transformations, we decided to 
ignore them for several reasons:  
? Split sentences are not well suited to train 
a phrase-based translation system. 
? One part of a split sentence can still be 
aligned with its whole translation as one-
to-one correspondence. 
? Cases of sentence swap are too rare to 
justify efforts needed to detect them. 
139
4 Filtering out machine translation 
After the verification procedure and sentence-
alignment procedure our collection consists of 
sentence-aligned parallel fragments extracted from 
initial documents. A closer look at the parallel 
fragments reveals that some texts contain mistakes 
typically made by machine translation systems. It 
is undesirable to include such documents into the 
corpus, because a phrase-based translation system 
trained on this corpus may learn a great deal of 
badly constructed phrases. 
The output of a rule-based system can be 
recognized without even considering its source 
text, as having no statistical information to rely on, 
the rule-based systems tend to choose the safest 
way of saying something, which leads to 
uncommonly frequent use of specific words and 
phrases. The differences in n-gram distributions 
can be captured by comparing the probabilities 
given by two language models: one trained on a 
collection of the outputs of a rule-based system and 
the other ? on normal texts. 
Our method of filtering out statistical machine 
translation is based on the similarity of algorithms 
of building phrase tables in the existing SMT 
systems. Those systems also have restrictions on 
reordering of words. Therefore their output is 
different from human translation, and this 
difference can be measured and serve as an 
indicator of a machine translated text. We designed 
a special version of phrase-based decoding 
algorithm whose goal was not just translate, but to 
provide a translation as close to the reference as 
possible while following the principles of phrase-
based translation. The program takes two sentence-
aligned documents as an input. Prior to translating 
each sentence, a special language model is built 
consisting of n-grams from the reference sentence. 
That model serves as a sort of soft constraint on the 
result of translation. The decoder output is scored 
against reference translation with the BLEU metric 
(Papineni et al, 2002) - we shall call it r-bleu for 
the rest of this section. The idea is that the higher is 
r-bleu, the more likely the reference is statistical 
translation itself. 
The program was implemented based on the 
decoder of the statistical phrase-based translation 
system. The phrase table and the factor weights 
were not modified. Phrase reordering was not 
allowed. The phrase table contained 13 million 
phrases. The language model was modified in the 
following way. We considered only n-grams no 
longer than 4 words and only those that could be 
found in the reference sentence. The language 
model score for each n-gram depended only on its 
length. 
We evaluated the method efficiency as follows. 
A collection of 245 random parallel fragments has 
been manually annotated as human or machine 
translation. 
There are some kinds of typical mistakes 
indicating that the text is generated by a machine 
translation system. The most indicative mistake is 
wrong lexical choice, which can be easily 
recognized by a human annotator. Additional 
evidence are cases of incorrect agreement or 
unnatural word order. We considered only 
fragments containing more than 4 parallel 
sentences, because it was hard to identify the 
origin of shorter fragments. The annotation 
provided following results: 
? 150 documents - human translation (64% 
of sentences) 
? 55 documents - English-Russian machine 
translation (22% of sentences) 
? 32 documents - Russian-English machine 
translation (12% of sentences) 
? 8 documents - not classified (2% of 
sentences) 
 
Sometimes it was possible for a human 
annotator to tell if a translation has been made by a 
rule-based or phrase-based translation system, but 
generally it was difficult to identify reliably the 
origin of a machine translated text.  Also there 
were a number of automatically translated texts 
which had been post-edited by humans. Such texts 
often preserved unnatural word order and in that 
case they were annotated as automatically 
translated. 
The annotation quality was verified by cross-
validation. We took 27 random documents out of 
245 and compared the results of the annotation 
with those performed by another annotator. There 
was no disagreement in identifying the translation 
direction. There were 4 cases of disagreement in 
identifying automatic translation: 3 cases of post-
edited machine translation and 1 case of verbatim 
human translation. We realized that in case of post-
140
edited machine translation the annotation was 
subjective. Nevertheless, after the question was 
discussed we decided that the initial annotation 
was correct. Table 1 represents the results of the 
annotation along with the range of r-bleu score. 
 
r-bleu Human Automatic 
0 - 5 0 0 
5-10 252 0 
10-15 899 0 
15-20 1653 0 
20-25 1762 0 
25-30 1942 154 
30-35 1387 538 
35-40 494 963 
40-45 65 1311 
45-50 76 871 
50-55 23 658 
55-60 0 73 
Total 8553 4568 
 
Table 1. Number of parallel sentences in 
human/machine translated documents depending 
on the range of r-bleu score. 
 
Let maxhC denote the total number of sentences 
in all documents which were annotated as human 
translation. In our case 8553max ?hC . Let 
hC denote the number of sentences in human 
translated documents with a r-bleu beyond certain 
threshold, and mtC  ? the number of sentences in 
automatically translated documents with a r-bleu 
beyond the same threshold. Then recall(R) and 
precision(P) are defined as 
 
maxhh CCR ? , ? ?mthh CCCP ?? . 
 
For example, if we discard documents with r-
bleu > 33.0, we get R = 90.1, P = 94.1. Figure 2 
illustrates the dependency between these 
parameters.  
The evaluation showed that parallel documents 
that have been translated automatically tend to get 
higher r-bleu scores and may be filtered out with 
reasonable precision and recall. As it is shown in 
Table 1, the total rate of machine translated 
sentence pairs is about 35% before the filtration. 
According to manual evaluation (see section 5, 
Table 4), this rate is reduced down to 5% in the 
final corpus. 
 
 
 
Figure 2. Dependency between r-bleu score and 
recall(R)/precision(P) rates of filtering procedure. 
 
We chose the BLEU criterion partly due to its 
robustness. For the English-Russian language pair 
it yielded satisfactory results. We believe that our 
approach is applicable to many other language 
pairs as well, probably except the pairs of 
languages with similar word order. For those 
languages some other metric is possibly needed 
taking into account properties of particular 
language pair. We expect that the r-bleu threshold 
also depends on the language pair and has to be re-
estimated. 
5 Corpus of parallel sentences 
After we choose a threshold value of the r-bleu 
criterion, we remove texts with the r-bleu score 
higher than the threshold from our collection of 
parallel fragments. Then we extract parallel 
sentences from the remaining texts in order to get a 
corpus of parallel sentences. 
Sentences inside parallel fragments undergo 
some additional filtering before they can be 
included into the final corpus. We discard sentence 
pairs for which a similarity score is below a given 
threshold, or word-length ratio is less than ?. It is 
also useful to drop sentences whose English part 
contains Cyrillic symbols as those are extremely 
unlikely to be seen in original English texts and 
their presence usually means that the text is a result 
of machine translation or some sort of spam. All 
141
sentence pairs are lowercase and distinct.  
Sentences of more than 100 words have been 
excluded from the corpus.  
In the rest of this section we estimate the quality 
of a 1-million-sentence part of the final parallel 
corpus that we are going to share with the research 
community. The corpus characteristics are 
represented in Table 2 and examples of parallel 
sentences are given in Table 3. 
 
 English Russian 
Sentences 1`022`201 
Distinct sentences 1`016`580 1`013`426 
Words 27`158`657 25`135`237 
Distinct words 323`310 651`212 
Av. Sent. Len 26.5 24.6 
 
Table 2. Corpus characteristics: number of parallel 
sentences, distinct sentences, words2, distinct 
words and average sentence length in words. 
 
We evaluate corpus quality in two ways:  
? Selecting each 5000-th sentence pair from 
the corpus and manually annotating the 
sentences as parallel or not. The results of 
the manual annotation are represented in 
Table 4. 
? Training a statistical machine translation 
system on the corpus and testing its output 
with BLEU metric 
 
We trained two phrase-based translation 
systems3. The first system was trained on 1 million 
random sentences originated in the documents 
which were human translations according to our r-
bleu criterion. The other system was trained on the 
same corpus except that 35% of sentences were 
replaced to random sentences taken from 
documents which had been previously excluded as 
automatically translated. We reserved each 1000-th 
sentence from the first ?clean? corpus as test data.  
We get word-alignment by running Giza++ (Och et 
al., 2000) on lemmatized texts. The phrase-table 
training procedure and decoder are the parts of 
Moses statistical machine translation system 
(Koehn et al, 2007). The language model has been 
                                                          
2
 Punctuation symbols are considered as separate words. 
3
 http://www.statmt.org/moses/ 
trained on target side of the first corpus using SRI 
Language Modeling Toolkit (Stolcke, 2002). 
 
? 2004 ?????? ??????????? ?? ???? ??? 
????????? ????????? ?????????, ??????? 
??????????? ?? ???? ???????. 
in 2004 maidan became-famous over all 
world due-to orange revolution , which took-
place at this place . 
in 2004, maidan became famous all over the 
world because the orange revolution was 
centered here. 
???????? ? ???????, ??? ???? ????????? 
????????????, ??? ?? ?????? ???????????? 
???????, - ?????? ????. 
stories about peoples , whose language so-
much imperfect , that it should be-supplied 
gestures-with , - pure myths . 
tales about peoples whose language is so 
defective that it has to be eked out by gesture, 
are pure myths. 
????????? ????? ????? ??? ????? 
???????, ????? ??? ????????? ????????? 
????? ??????? ????! 
the-rest-of time let they be open , so-that all 
inhabitants universe-of could see you ! 
the rest of the time, let the doors be open so 
that all the residents of the universe may have 
access to see you. 
"? ??????????? ???? ??????. 
"i control my destiny. 
"i control my own destiny. 
 
Table 3. Sample parallel sentences. 
 
Parallel 169 
Parallel including non-parallel 
fragments 
19 
Non-parallel 6 
English-Russian automatic 4 
translation  
7 
Russian-English automatic 
translation 
3 
Total sentences 204 
  
Table 4. Results of manual annotation of 204 
sample sentences from the corpus. 
 
                                                          
4
 Sentences containing mistakes typical for MT systems 
were annotated as automatic translations. 
142
We tested both Russian-to-English and English-
to-Russian translation systems on 1022 test 
sentences varying the language model order from 
trigram to 5-gram. We have not tuned the weights 
on the development set of sentences, because we 
believe that in this case the quality of translation 
would depend on the degree of similarity between 
the test and development sets of sentences and it 
would make our evaluation less reliable. In all 
experiments we used default Moses parameters, 
except that the maximum reordering parameter was 
reduced to 3 instead of 6. The results are 
represented in Table 5. 
 
 Ru-En / +mt En-Ru / +mt 
3-gram 20.97 / +0.06 16.35 / -0.10 
4-gram 21.04 / -0.13 16.33 / -0.13 
5-gram 21.17 / -0.06 16.42 / -0.16 
OnlineA5 25.38 21.01 
OnlineB6 23.86 16.56 
 
Table 5. BLEU scores measured on 1022 test 
sentences depending on the order of language 
model. The column +mt shows relative change in 
BLEU score of the s?stem trained on ?mt-nois?? 
data.  
 
The overall system performance can be  
improved by tuning and/or training a bigger 
language model, but our goal is only to show to 
what extent the corpus itself is suitable for training 
statistical machine translation system. Online 
translation systems have been tested on the same 
test set, except that the input was detokenized and 
the output was lowercased. The online translation 
could have been better if the input text was in its 
original format - not lowercased. 
6 Conclusion 
We have described our approaches to main 
problems faced when building a parallel Russian-
English corpus from the Internet.  
We have proposed a method of filtering out 
automatically translated texts. It allowed us to 
reduce the rate of sentence pairs that originate from 
machine translated documents from 35% to 5%. 
The approach relies on general properties of the 
                                                          
5
 http://translate.google.ru/ 
6
 http://www.microsofttranslator.com/ 
state-of-the-art statistical translation systems and 
therefore is applicable to many other language 
pairs. 
We presented results of evaluation of the 
resulting Russian-English parallel corpus. We 
believe that the 1-million-sentence Russian-
English corpus of parallel sentences used in this 
paper is a useful resource for machine translation 
research and machine translation contests. 
References 
Brown, P.F., Lai, J.C., Mercer, R.L. 1991. Aligning 
Sentences in Parallel Corpora. Proceedings of the 
29th Annual Meeting of the Association for 
Computational Linguistics, Berkeley, California 
169?176. 
Chen, S.F. 1993. Aligning sentences in bilingual 
corpora using lexical information. Conference of the 
Association for Computational Linguistics, 
Columbus, Ohio, 9-16. 
Dean, J. and Ghemawat, S. 2004. MapReduce: 
Simplified data processing on large clusters. In 
Proceedings of the Sixth Symposium on Operating 
System Design and Implementation (San Francisco, 
CA, Dec. 6?8). Usenix Association. 
Gale, W. A., & Church, K. W. 1993. A program for 
aligning sentences in bilingual corpora. 
Computational Linguistics, 19(3), 75-102. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, Evan Herbst. 2007. Moses: Open Source 
Toolkit for Statistical Machine Translation, Annual 
Meeting of the Association for Computational 
Linguistics (ACL), demonstration session, Prague, 
Czech Republic, June.  
Xiaoyi Ma and Mark Liberman. 1999. BITS: A method 
for bilingual text search over the web. Proceedings of 
the Machine Translation Summit VII. 
Xiaoyi Ma. 2006. Champollion: A Robust Parallel Text 
Sentence Aligner. LREC 2006: Fifth International 
Conference on Language Resources and Evaluation. 
Michael Mohler and Rada Mihalcea. 2008. BABYLON 
Parallel Text Builder: Gathering Parallel Texts for 
Low-Density Languages. Proceedings of the 
Language Resources and Evaluation Conference. 
Moore, Robert C., 2002. Fast and Accurate Sentence 
Alignment of Bilingual Corpora. Machine 
Translation: From Research to Real Users 
143
(Proceedings, 5th Conference of the Association for 
Machine Translation in the Americas, Tiburon, 
California), Springer-Verlag, Heidelberg, Germany, 
135-244. 
David Nadeau and George Foster, 2004. Real-time 
identification of parallel texts from bilingual 
newsfeed. Computational Linguistic in the North-
East (CLiNE 2004): 21-28. 
Franz Josef Och, Hermann Ney. 2000. Improved 
Statistical Alignment Models. Proceedings of the 
38th Annual Meeting of the Association for 
Computational Linguistics, pp. 440-447, Hongkong, 
China, October. 
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
the 40th Annual Meeting of the Association for 
Computational Linguistics (ACL), pages 311?318, 
Philadelphia, PA, USA. 
Resnik, Philip and Noah A. Smith. 2003. The web as a 
parallel corpus. Computational Linguistics, 29:349?
380. 
Andreas Stolcke. 2002. SRILM?an extensible 
language modeling toolkit. Proceedings ICSLP, vol. 
2, pp. 901?904, Denver, Sep. 
Jakob Uszkoreit, Jay Ponte, Ashok Popat and Moshe 
Dubiner. 2010. Large Scale Parallel Document 
Mining for Machine Translation. Coling 
 
144
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 58?66,
Gothenburg, Sweden, April 27, 2014.
c?2014 Association for Computational Linguistics
Improving the precision of automatically constructed human-oriented
translation dictionaries
Alexandra Antonova
Yandex
16, Leo Tolstoy St., Moscow, Russia
antonova@yandex-team.ru
Alexey Misyurev
Yandex
16, Leo Tolstoy St., Moscow, Russia
misyurev@yandex-team.ru
Abstract
In this paper we address the problem of
automatic acquisition of a human-oriented
translation dictionary from a large-scale
parallel corpus. The initial translation
equivalents can be extracted with the help
of the techniques and tools developed for
the phrase-table construction in statistical
machine translation. The acquired transla-
tion equivalents usually provide good lexi-
con coverage, but they also contain a large
amount of noise. We propose a super-
vised learning algorithm for the detection
of noisy translations, which takes into ac-
count the context and syntax features, av-
eraged over the sentences in which a given
phrase pair occurred. Across nine Euro-
pean language pairs the number of seri-
ous translation errors is reduced by 43.2%,
compared to a baseline which uses only
phrase-level statistics.
1 Introduction
The automatic acquisition of translation equiva-
lents from parallel texts has been extensively stud-
ied since the 1990s. At the beginning, the acquired
bilingual lexicons had much poorer quality as
compared to the human-built translation dictionar-
ies. The limited size of available parallel corpora
often resulted in small coverage and the imper-
fections of alignment methods introduced a con-
siderable amount of noisy translations. However,
the automatimacally acquired lexicons served as
internal resources for statistical machine trans-
lation (SMT) (Brown et al., 1993), information
retrieval (IR) (McEvan et al., 2002; Velupillai,
2008), or computer-assisted lexicography (Atkins,
1994; Hartmann, 1994).
The current progress in search of web-based
parallel documents (Resnik, 2003; Smith, 2013)
makes it possible to automatically construct large-
scale bilingual lexicons. These lexicons can al-
ready compare in coverage to the traditional trans-
lation dictionaries. Hence a new interesting pos-
sibility arises - to produce automatically acquired
human-oriented translation dictionaries, that have
a practical application. A machine translation sys-
tem can output an automatically generated dictio-
nary entry in response to the short queries. The
percentage of short queries can be quite large, and
the system benefits from showing several possible
translations instead of a single result of machine
translation (Figure 1).
Figure 1: Examples of dictionary entries in two
online statistical machine translation systems.
The initial translation equivalents for a bilin-
gual lexicon can be extracted with the help of
the techniques and tools developed for the phrase-
table construction in SMT. The widely used word
alignment and phrase extraction algorithms are de-
scribed in Brown et.al (1993) and Och (2004).
Though an SMT phrase-table actually consists of
translation equivalents, it may differ substantially
from a traditional dictionary (Table 1).
58
Human-oriented dic-
tionary
SMT phrase-table
Lemmatized entries
are preferred.
Words and phrases in
all forms are accept-
able.
Only linguistically
motivated phrases are
acceptable.
Any multiword phrase
is acceptable.
Precision is important.
Any noise is undesir-
able.
Having lots of low-
probability noise is
acceptable, since it is
generally overridden
by better translations.
Table 1: Differences between a human-oriented
dictionary and an SMT phrase-table.
While the problems of lemmatization and se-
lection of linguistically motivated phrases can be
addressed by applying appropriate morphological
and syntactic tools, the problem of noise reduc-
tion is essential for the dictionary quality. The cur-
rent progress in the automatic acquisition of simi-
lar Web documents in different languages (Resnik,
2003; Smith, 2013) allows to collect large-scale
corpora. But the automatically found documents
can be non-parallel, or contain spam, machine
translation, language recognition mistakes, badly
parsed HTML-markup. The noisy parallel sen-
tences can be the source of lots of noisy transla-
tions ? unrelated, misspelled, or belonging to a
different language. For example, non-parallel sen-
tences
The apartment is at a height of 36
floors! (English)
La plage est `a 1 minute en
voiture. (French: The beach is 1
minute by car.)
may produce a wrong translation ?apartment -
plage?. Or, automatically translated sentences
The figures in the foreground and back-
ground play off each other well. (En-
glish)
Les chiffres du premier plan et jouer
hors de l?autre bien. (French: The dig-
its of the foreground and play out of the
other well.)
may produce a wrong phrase translation ?figures
in the foreground - chiffres du premier plan?.
An intuitive approach would be to apply noise
filtering to the corpus, not to the lexicon. One
could discard those sentences that deviate too
much from the expected behavior. For example,
sentences that have many unknown words and few
symmetrically aligned words are unlikely to be re-
ally parallel. However, natural language demon-
strates a great variability. A single sentence pair
can deviate strongly from the expected behavior,
and still contain some good translations. On the
other hand, many noisy translations can still pen-
etrate the lexicon, and further noise detection is
necessary.
In a bilingual lexicon we want not just to lower
the probabilities of noisy translations, but to re-
move them completely. This can be regarded as a
binary classification task ? the phrase pairs are to
be classified into good and noisy ones.
Different types of information can be com-
bined in a feature vector. We take advantage of
the phrase-level features, such as co-occurrence
counts or translation probabilities, and also pro-
pose a number of sentence-level context features.
To calculate the sentence-level features for a given
phrase-pair, we average the characteristics of all
the sentences where it occurs.
We test the proposed algorithm experimentally,
by constructing the bilingual lexicons for nine lan-
guage pairs. The manually annotated samples
of phrase pairs serve as the data for training su-
pervised classifiers. The experiment shows that
the use of the sentence-level features increases
the classification accuracy, compared to a baseline
which uses only phrase frequencies and translation
probabilities. We compare the accuracy of differ-
ent classifiers and evaluate the importance of dif-
ferent features.
The rest of the paper is organized as follows. In
Section 2 we outline the related work. Section 3
describes our approach to the noise reduction in a
bilingual lexicon and discusses the proposed fea-
tures. We describe our experiments on training
classifiers in Section 4. Section 5 concludes the
paper.
2 Previous work
The methods of extracting a bilingual lexicon from
parallel texts as a part of the alignment process
are discussed in Brown (1993), Melamed (1996),
Tufis? and Barbu (2001). Melamed (1996) pro-
poses a method of noise reduction that allows
59
to re-estimate and filter out indirect word asso-
ciations. However, he works with a carefully
prepared Hansards parallel corpus and the noise
comes only from the imperfections of statistical
modeling.
Sahlgren (2004) proposes a co-occurrence-
based approach, representing words as high-
dimensional random index vectors. The vectors
of translation equivalents are expected to have
high correlation. Yet, he notes that low-frequency
words do not produce reliable statistics for this
method.
The methods of bilingual lexicon extraction
from comparable texts (Rapp, 1995; Fung, 1998;
Otero, 2007) also deal with the problem of noise
reduction. However, the precision/recall ratio of a
lexicon extracted from comparable corpus is gen-
erally lower. For the purpose of building a human-
oriented dictionary, the parallel texts may provide
the larger coverage and better quality of the trans-
lation equivalents.
The noise reduction task is addressed by some
of the SMT phrase-table pruning techniques. The
most straightforward approach is thresholding on
the translation probability (Koehn et al., 2003).
Moore (2004) proposes the log-likelihood ratio
and Fisher?s exact test to re-estimate word asso-
ciation strength. Johnson et al. (2007) applies
Fisher?s exact test to dramatically reduce the num-
ber of phrase pairs in the phrase-table. They get
rid of phrases that appear as alignment artifacts or
are unlikely to occur again. The implementation
of their algorithm requires a special index of all
parallel corpus in order to enable a quick look-up
for a given phrase pair. Eck et al. (2007) assesses
the phrase pairs based on the actual usage statistics
when translating a large amount of text. Entropy-
based criteria are proposed in Ling et al. (2012),
Zens et al. (2012).
Automatically acquired bilingual lexicons are
capable to reflect many word meanings and trans-
lation patterns, which are often not obvious even
to the professional lexicographers (Sharoff, 2004).
Their content can also be updated regularly to in-
corporate more parallel texts and capture the trans-
lations of new words and expressions. Thus, the
methods allowing to improve the quality of au-
tomatic bilingual lexicons are of practical impor-
tance.
3 Noise detection features
We treat the noise recognition task as a binary
classification problem. A set of nonlexical con-
text features is designed to be sensitive to differ-
ent types of noise in the parallel corpus. We ex-
pect that the combination of these features with
the phrase-level features based on co-occurrence
statistics can improve the accuracy of the classifi-
cation and the overall quality of a bilingual lexi-
con.
3.1 Context feature extraction algorithm
The procedure of getting the context features
is outlined in Algorithm 1. Unlike Johnson et
al. (2007) we do not rely on any pre-constructed
index of the parallel sentences, because it requires
a lot of RAM on large corpora. Instead we re-
run the phrase extraction algorithm of the Moses
toolkit (Koehn et al., 2007) and update the con-
text features at the moment when a phrase pair t is
found.
Algorithm 1 Calculate context features for all lex-
icon entries
Require: Parallel corpus ? C; {word-aligned
sentences}
Require: Bilingual lexicon ? D; {this is a
phrase-table, derived from C and modified as
descibed in 4.1}
Ensure: V = {v?(d): d ? D}; {resulting fea-
tures}
for all d ? D do
v?(d)? 0;
n(d)? 0;
for all s ? C do
T ? PhraseExtraction(s);{Moses func-
tion}
for all t ? T do
if t ? D then
v?(t)? v?(t) + SentFeats(s); {Alg. 2}
n(t)? n(t) + 1;
for all d ? D do
v?(d) ? v?(d)/(1 + n(d)); {average,
+1 smoothing}
return V
3.2 Sentence-level features
The phrase extraction algorithms do not preserve
the information about the sentences in which a
given phrase pair occurred, assuming that all the
sentences are equally good. As a result, the
60
phrase-level statistics is insufficient in case of a
noisy corpus.
The sentence-level features are designed to
partly restore the information which is lost dur-
ing the phrase extraction process. We try to es-
timate the general characteristics of the whole set
of parallel sentences where a given phrase pair oc-
curred. The proposed sentence-level features rely
on the different sources of information, which are
discussed in 3.2.1, 3.2.2 and 3.2.3. Table 2 pro-
vides illustrating examples of noisy phrase pairs
and sample sentences.
3.2.1 Word-alignment annotation
We use the intersection of direct and reverse
Giza++ (Och and Ney, 2004) alignments as a
heuristic rule to find words reliably aligned to each
other. The alignment information gives rise to sev-
eral sentence-level features:
? UnsafeAlign - percentage of words that are
not symmetrically aligned to each other.
? UnsafeJump - average distance between
the translations of subsequent input words.
? UnsafeDigAlign percentage of unequal
digits among the symmetrically aligned
words.
The UnsafeAlign and UnsafeJump values can
vary in different sentences. However, their being
too large on the whole set of sentences where a
given phrase pair occurred possibly indicates some
systematic noise.
The translations of digits are not included to the
dictionary by themselves. But if a pair of digits is
wrongly aligned, then its nearest context may also
be aligned wrongly.
3.2.2 One-side morphological and syntactic
annotation
The target side of our parallel sentences has been
processed by a rule-based parser. The syntax gives
rise to:
? UnsafeStruct - percentage of words having
no dependence on any other word in the parse
tree.
The morphological annotation participates in:
? OOV - percentage of out-of-vocabulary
words in the sentence.
The low parse tree connectivity may indicate that
the sentence is ungrammatical or produced by a
poor-quality machine translation system. Sen-
tences containing many out-of-vocabulary words
probably do not belong to the given language. We
compute out-of-vocabulary words according to an
external vocabulary, which is embedded in tagging
and parsing tools. However, instead one can use a
collection of unigrams filtered by some frequency
threshold..
gratuit ? internet access, S
lem
= 215
Sample sentence:
Petit d?ejeuner continental de luxe gratuit
Business center with free wireless Internet ac-
cess
UnsafeAlign = 0.387
`
a ? you, S
lem
= 586
La plainte `a transmettre
You should submit your complaint
UnsafeJump = 1.75
juin ? May, S
lem
= 35
Membre depuis: 17 juin 2011
Member since: 01 May 2012
UnsafeAlignDig = 0.08
le ? Fr, S
lem
= 24
Edvaldo et le p`ere Antenore
Edvaldo and Fr Antenore
OOV = 0.117
Paris ? England, S
lem
= 54
TERTIALIS (Paris, Paris)
(England)
Punct = 0.117
Table 2: Examples of noisy French-English trans-
lations to which different sentence-level features
may be sensitive. S
lem
? is the number of
sentences where a lemmatized phrase pair co-
occurred. Sample sentences are provided.
3.2.3 Surface text
The surface word tokens can be used for:
? Punct - percentage of non-word/punctuation
tokens in the sentence.
? Uniqueness - the percentage of unique uni-
grams in both source and target language sen-
tences.
Sentences with lots of punctuation can be un-
natural or contain enumeration. Large enumera-
tion lists are often not exactly parallel and can be
61
aligned incorrectly, because punctuation tokens,
like many commas, are easily mapped to each
other. The low Uniqueness possibly indicates
that the sentences containing a given phrase pair
are similar to each other. This can lead to overes-
timated translation probabilities.
Algorithm 2 Get features of one sentence pair
(SentFeats)
Require: sent
src
= (w
1
, ..., w
m
);
Require: sent
dst
= (w
1
, ..., w
n
);
Require: Alignment matrix ? M
m,n
: x ?
{0, 1}; {intersection of two Giza++ align-
ments}
Require: oov = (x
1
, ..., x
n
), x ? {0, 1};
{x
i
= 1 ?? sent
dst
[i] is out-of-
vocabulary}
Require: pnt = (x
1
, ..., x
n
), x ? {0, 1};
{x
i
= 1 ?? sent
dst
[i] is punctuation}
Require: nohead = (x
1
, ..., x
n
), x ? {0, 1};
{x
i
= 1 ?? sent
dst
[i] is not dependent
on any other word in the parse}
Ensure: v? = (v
1
, ..., v
7
); {features}
v? ? 0;
v
2
?
1
n
?
x?nohead
x; {UnsafeStruct}
Let A be the set of pairs of indices of symmet-
rically aligned words, ordered by the source in-
dices:
A? {(i, j) |M(i, j) = 1};
v
3
? 1?
|A|
m+n
; {UnsafeAlign}
for all (i, j) ? A do
if words with indices i, j are unequal digits
then
v
4
? v
4
+ 1;
v
4
?
v
4
|A|
; {UnsafeAlignDig}
v
5
?
1
|A|
?
(i,j)?A
j
i
? j
i?1
; {UnsafeJump}
v
6
?
1
n
?
x?oov
x; {OOV }
v
7
?
1
n
?
x?pnt
x; {Punct}
return v?
3.3 Phrase-level statistics
Multiple phrase-level features can be derived from
the occurrence and co-occurrence counts, that are
calculated during the phrase extraction procedure
as described in Koehn et. al (2003).
? C(f), C(e), C(e, f) ? surface phrase occur-
rence counts.
? C
lem
(f), C
lem
(e), C
lem
(e, f) ? same for
lemmatized phrases.
? S(e, f), S
lem
(e, f) ? the number of sen-
tences, in which the surface (or lemmatized)
phrases co-occurred.
? P (e|f), P (f |e) ? translation probabilities
of surface phrases.
? P
lem
(e|f), P
lem
(f |e) ? translation proba-
bilities of lemmatized phrases.
Some of these features are highly correlated, and
it is hard to tell in advance which subset leads to
better performance.
4 Experiment
We conducted experiments on nine language
pairs: German-English, German-Russian, French-
English, French-Russian, Italian-English, Italian-
Russian, Spanish-English, Spanish-Russian and
English-Russian. The parallel corpora consisted
of the sentence-aligned documents automatically
collected from multilingual web-sites.
We implemented the procedure of bilingual lex-
icon construction and the algorithm calculating the
sentence-level features (Section 3).
The annotated phrase pair samples, one for
each language pair, provided positive and nega-
tive examples for training a supervised classifier.
We compared the accuracy of several classifiers
trained on different feature sets. The importance
of different features was evaluated .
4.1 Bilingual lexicon creation
We used Giza++ for word alignment and Moses
toolkit for phrase extraction procedure. The fol-
lowing automatic annotation had been provided.
The source side of the parallel corpora had been
processed by a part-of-speech tagger, and each
word had been assigned a lemma based on its tag.
The target side of the parallel corpora, which was
always either English or Russian, was processed
by a rule-based dependency parser, which also
supplied morphological annotations and lemmas.
In the case of English-Russian corpus, the source
side had also been processed by the parser.
62
The extracted English phrases were restricted
to at most 3 words, provided that they were con-
nected in the dependency tree. The same restric-
tions were imposed on the Russian phrases. The
extracted phrases for all other languages were re-
stricted to single words to avoid the ungrammati-
cal multiword expressions.
Each extracted phrase pair was assigned a lem-
matized key consisting of lemmas of all words
in it. The co-occurrence counts were summed
over all phrase pairs sharing the same key, giv-
ing the aggregate count C
lem
(e, f). Then a sin-
gle pair was chosen to serve as a best substitute
for a lemmatized lexicon entry. The choice was
made heuristically, based on the morphological at-
tributes and co-occurrence counts.
As a preliminary lexicon cleanup we removed
the phrase pairs which contained punctuation sym-
bols or digits on either side. We also removed the
pairs that co-occurred only once in the corpus. An
example of differences between the size of original
phrase table and the size of bilingual lexicon af-
ter lemmatization and preliminary cleanup is rep-
resented in Table 3.
Millions of phrase pairs
fr-en fr-ru
Initial 1-3 phrase-table 16.4 30.8
After lemmatization 7.9 6.4
After preliminary cleanup 1.6 0.8
Table 3: The number of phrase pairs on different
stages of French-English and French-Russian dic-
tionary creation. Phrase pairs in the initial phrase
table are restricted to at most 1 source word and at
most 3 target words.
4.2 Experimental data
For the experiment we selected random
1
transla-
tion equivalents from the nine translation lexicons,
to which no further noise reduction had been ap-
plied. The resulting translation equivalents were
assessed by human experts. The annotation task
was to determine how well a phrase pair fits for a
human-oriented translation dictionary. The anno-
tators classified each translation according to the
following gradation:
Class 0 ? difficult to assess.
1
Random was used proportionally to the square root of
joint frequency, in order to balance rare and frequent phrase
pairs in the sample.
Class 1 ? totally wrong or noisy (e.g.
misspelled);
Class 2 ? incorrect or incomplete trans-
lation;
Class 3 ? not a mistake, but unneces-
sary translation;
Class 4 ? good, but not vital;
Class 5 ? vital translation (must be
present in human-built dictionary);
The pairs annotated as 0 usually represented
the translations of unfamiliar words, abbreviations
and the like. Such phrases were excluded from
training and testing. We didn?t use ?acceptable,
but unnecessary? translation pairs either, because
they do not influence the quality of the lexicon.
We treated as negative the phrase pairs that were
annotated as 1 or 2. Analogously, the positive ex-
amples had to belong to 4 or 5 class. The annota-
tion statistics is given in Table 4.
Language Size %Negative %Positive
it-ru 2340 56.6 28.7
it-en 2366 59.9 21.4
es-ru 2388 55.5 27.2
es-en 2384 69.0 24.0
de-ru 2397 50.3 37.6
de-en 2438 72.1 24.5
fr-ru 2461 44.5 31.2
fr-en 2325 57.0 24.4
en-ru 2346 27.8 33.2
Table 4: Statistics of the annotated data: the num-
ber of annotated phrase pairs, the percentage of
negative and positive examples.
4.3 Training setting
The experiments were run with two different fea-
ture sets:
? Baseline ? features based on co-occurrence
counts.
? Full ? baseline and sentence-level features.
We had to choose a subset of co-occurrence-based
features experimentally (see, Section 3.3). The
best subset for our data consisted of three features:
log(S
lem
), log(P (e|f)), log(P (f |e)). In the full
feature set we combined the baseline features and
the sentence-level features calculated as described
in Algorithm 2.
63
We considered three metrics related to the im-
provement of the lexicon quality:
? Err ? the percentage of prediction errors;
? Err-1 ? the percentage of class 1 examples
which were classified as positive.
? F1 ? the harmonic mean of precision and re-
call w.r.t. the positive and negative examples;
We used the standard packages of the R pro-
gramming language, to train and tune differ-
ent classifiers: random forest (RF), support vec-
tor machines (SVM), logistic regression (GLM),
Naive Bayes classifier, neural networks, k-Nearest
Neighbors and some of the combinations of these
methods with SVD. To assess the predictive accu-
racy we used repeated random sub-sampling val-
idation. In each of 40 iterations, a 10% test set
was randomly chosen from the dataset, the model
was trained on the rest of the data, and then tested.
The resulting accuracy was averaged over the iter-
ations.
Classifier Full feature set Base feature set
%Err %Err-1 %Err %Err-1
RF 19.80 8.31 24.00 14.62
SVM 19.63 9.36 23.49 12.91
GLM 22.74 6.35 25.23 7.30
Table 5: Percentage of prediction errors of dif-
ferent classifiers, averaged over the nine language
pairs.
The results of RF, SVM and GLM are reported
in Table 5. Though the composition of different
classifiers could perform slightly better, it would
require an individual tuning for each language
pair. For clearness, we use a single classifier (RF)
for the rest of the experiments.
The experiment showed that training on the full
feature set reduced the total amount of prediction
errors by 17.5%, compared to the baseline setting.
The number of false positives among the class 1
examples reduced by 43%. It is also important that
better results were obtained on each of the nine
language pairs, not only on average. In Table 6
the baseline results are shown in brackets and one
can see that F1 diminishes in the baseline setting,
while the percentage of errors goes up. The classi-
fication accuracy depends on the size of the train-
ing set (Table 7).
Lang %Err %Err-1 F1
de-en 18.0 (+3.6) 4.0 (+5.2) .562 (-.050)
de-ru 25.7 (+4.0) 13.5 (+6.7) .672 (-.040)
es-en 16.4 (+3.8) 3.2 (+4.0) .610 (-.059)
es-ru 20.6 (+4.7) 8.3 (+6.0) .643 (-.064)
fr-en 20.5 (+1.5) 6.0 (+5.8) .603 (-.031)
fr-ru 21.4 (+6.1) 15.5 (+10.8) .704 (-.070)
it-en 15.2 (+3.3) 3.5 (+2.9) .663 (-.059)
it-ru 19.6 (+5.5) 9.4 (+6.7) .670 (-.071)
en-ru 20.8 (+5.6) 11.5 (+8.8) .797 (-.048)
Table 6: Classification quality of the classifier
trained on all features, compared to the baseline
trained only on phrase-level features. The relative
change of the baseline values is given in brackets.
Examples 1700 680 272 108 43
Accuracy .803 .794 .780 .757 .709
Table 7: Classification accuracy w.r.t different size
of training set averaged over eight language pairs.
We measured the impact of different features,
as described in Breiman (2001), with the help of
the standard function of the R library ?random-
Forest? (Table 8). The three baseline features
were ranked as most important, followed by Un-
safeAlign, OOV, UnsafeJump and others.
Feature Importance
log(S
lem
) 35.679
log(P (e|f)) 33.9729
log(P (f |e)) 28.8637
UnsafeAlign 24.3705
OOV 22.8306
UnsafeJump 20.1108
Punct 15.4501
UnsafeStruct 15.1157
Uniqueness 13.5049
UnsafeDigAlign 12.915
Table 8: Feature importance measured by
the mean decrease of classification accu-
racy (Breiman, 2001). The value is averaged over
the nine language pairs.
We explored the dependence of the prediction
accuracy on the co-occurrence frequency of a
phrase pair for the classifiers trained on the full
feature set and on the baseline feature set. The re-
sults for German-English and French-English lan-
64
guage pairs are shown in Figure 2. The accu-
racy function was smoothed with cubic smooth-
ing spline. The differences in the distribution of
classification errors between language pairs sug-
gest that the nature of the noise can vary for dif-
ferent corpora. The general U shape of the curves
in Figure 2 is partly due to the fact that there are
many true negatives in the low-frequency area, and
many true positives in the high-frequency area.
0 500 1000 1500 2000
0.
6
0.
7
0.
8
0.
9
rf
svm
glm
rf?b
svm?b
glm?b
0 500 1000 1500
0.
70
0.
75
0.
80
0.
85
0.
90
0.
95
rf
svm
glm
rf?b
svm?b
glm?b
Figure 2: Prediction accuracy of different classi-
fiers w.r.t. the phrase pairs sorted by the ascend-
ing co-occurrence count. The upper plot relates
to the German-English pair, the bottom relates to
French-English pair. The labels rf, svm, glm re-
fer to the classifiers trained on the full feature set;
rf-b, svm-b, glm-b refer to the baseline setting.
Table 9 reports the top English translations of
the French word ?connexion? before the noise re-
duction and shows which variants were recognized
as positive and negative by the RF classifier.
English C(e, f) p(f |e) p(e|f) RF
connection 58018 0.689 0.374 +
wireless 32630 0.450 0.211 -
free 31775 0.113 0.205 -
wifi 16272 0.382 0.105 -
login 4910 0.443 0.032 +
connectivity 394 0.055 0.003 +
logon 290 0.185 0.002 +
access 276 0.001 0.002 -
link 148 0.001 0.001 -
Table 9: English translations of the French word
?connexion?. C(e, f) is the co-occurrence count,
p(f |e), p(e|f) are the translation probabilities of
lemmatized pairs. The last column shows the clas-
sification result.
5 Conclusion
The main contributions of this paper are the fol-
lowing. We address the problem of noise reduc-
tion in automatic construction of human-oriented
translation dictionary. We introduce an approach
to increase the precision of automatically acquired
bilingual lexicon, which allows to mitigate the
negative impact of a noisy corpus. Our noise
reduction method relies on the supervised learn-
ing on a small set of annotated translation pairs.
In addition to the phrase-level statistics, such as
co-occurrence counts and translation probabilities,
we propose a set of non-lexical context features
based on the analysis of sentences in which a
phrase pair occurred. The experiment demon-
strates a substantial improvement in the accuracy
of the detection of noisy translations, compared to
a baseline which uses only phrase-level statistics.
We have shown that the proposed noise de-
tection method is applicable to various language
pairs. The alignment-based features can be easily
obtained for any parallel corpus, even if other tools
do not exist. We hope that our noise detection ap-
proach can also be adapted for SMT phrase-tables,
if the initial parallel sentences are still available.
References
B. T. Sue Atkins. 1994. A corpus-based dictionary.
In Oxford-Hachette French Dictionary, Introduction
xix-xxxii. Oxford: Oxford University Press.
Leo Breiman. 2001. Random Forests. Machine Learn-
ing 45 5-32.
65
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation:
Parameter estimation. Computational Linguistics,
19(2):263?312, June.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2007.
Translation model pruning via usage statistics for
statistical machine translation. In Human Language
Technologies 2007: The Conference of the NAACL;
Companion Volume, Short Papers, pages 21?24,
Rochester, New York, April. Association for Com-
putational Linguistics
Pascale Fung. 1998. A Statistical View on Bilingual
Lexicon Extraction from Parallel Corpora to Non-
parallel Corpora. Parallel Text Processing: Align-
ment and Use of Translation Corpora. Kluwer Aca-
demic Publishers
Hartmann, R.R.K. 1994. The use of parallel text cor-
pora in the generation of translation equivalents for
bilingual lexicography. In W. Martin, et al. (Eds.),
Euralex 1994 Proceedings (pp. 291-297). Amster-
dam: Vrije Universiteit.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn 2007. Improving translation quality
by discarding most of the phrasetable. In Proceed-
ings of EMNLP-CoNLL, ACL, Prague, Czech Re-
public, pages 967-975.
Philipp Koehn, Franz Josef Och, and Daniel Marcu
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180, Prague, Czech Republic
Akira Kumano and Hideki Hirakawa. 1994. Build-
ing An MT Dictionary From Parallel Texts Based
On Linguistic And Statistical Information. COLING
1994: 76-81
Wang Ling, Jo?ao Grac?a, Isabel Trancoso and Alan
Black 2012. Entropy-based Pruning for Phrase-
based Machine Translation. In Proceedings of
EMNLP-CoNLL, Association for Computational
Linguistics, Jeju Island, Korea, pp. 972-983
C. J. A. McEwan, I. Ounis, and I. Ruthven. 2002.
Building bilingual dictionaries from parallel web
documents. In Proceedings of the 24th BCS-IRSG
European Colloquium on IR Research, pp. 303-323.
Springer-Verlag.
I. Dan Melamed. 1996. Automatic construction
of clean broad-coverage translation lexicons. In
Proceedings of the 2nd Conference of the Associa-
tion for Machine Translation in the Americas, pages
125?134, Montreal, Canada
I. Dan Melamed. 2000. Models of Translational
Equivalence among Words. Computational Linguis-
tics 26(2), 221-249, June.
Robert C. Moore. 2004. On Log-Likelihood-Ratios
and the Significance of Rare Events. In Proceed-
ings of the 2004 Conference on Empirical Methods
in Natural Language Processing, Barcelona, Spain.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. Proceedings of the
38th Annual Meeting of the ACL, pp. 440-447,
Hongkong, China.
Franz Josef Och and Hermann Ney. 2004. The
Alignment Template Approach to Statistical Ma-
chine Translation. Computational Linguistics, vol.
30 (2004), pp. 417-449.
Pablo Gamallo Otero. 2007. Learning bilingual lexi-
cons from comparable English and Spanish corpora.
Proceedings of MT Summit XI, pages 191?198.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the ACL 33,
320-322.
Resnik, Philip and Noah A. Smith. 2003. The web as
a parallel corpus.. Computational Linguistics, 29,
pp.349?380
Magnus Sahlgren. 2004. Automatic Bilingual Lexi-
con Acquisition Using Random Indexing. Journal
of Natural Language Engineering, Special Issue on
Parallel Texts, 11.
Serge Sharoff. 2004. Harnessing the lawless: using
comparable corpora to find translation equivalents.
Journal of Applied Linguistics 1(3), 333-350.
Jason Smith, Herve Saint-Amand, Magdalena Pla-
mada, Philipp Koehn, Chris Callison-Burch and
Adam Lopez. 2013. Dirt Cheap Web-Scale Par-
allel Text from the Common Crawl. To appear in
Proceedings of ACL 2013.
Dan Tufis? and Ana-Maria Barbu. 2001. Computa-
tional Bilingual Lexicography: Automatic Extrac-
tion of Translation Dictionaries. In International
Journal on Science and Technology of Informa-
tion, Romanian Academy, ISSN 1453-8245, 4/3-4,
pp.325-352
Velupillai, Sumithra, Martin Hassel, and Hercules
Dalianis. 2008. ?Automatic Dictionary Construc-
tion and Identification of Parallel Text Pairs. In Pro-
ceedings of the International Symposium on Using
Corpora in Contrastive and Translation Studies (UC-
CTS).
Richard Zens, Daisy Stanton and Peng Xu. 2012.
A Systematic Comparison of Phrase Table Pruning
Techniques. In Proceedings of EMNLP-CoNLL,
ACL, Jeju Island, Korea, pp. 972-983.
66
