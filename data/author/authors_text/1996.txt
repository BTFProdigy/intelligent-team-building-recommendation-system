Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 151?160, Prague, June 2007. c?2007 Association for Computational Linguistics
Using Foreign Inclusion Detection to Improve Parsing Performance
Beatrice Alex, Amit Dubey and Frank Keller
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW, UK
{balex,adubey,keller}@inf.ed.ac.uk
Abstract
Inclusions from other languages can be a
significant source of errors for monolin-
gual parsers. We show this for English in-
clusions, which are sufficiently frequent to
present a problem when parsing German.
We describe an annotation-free approach for
accurately detecting such inclusions, and de-
velop two methods for interfacing this ap-
proach with a state-of-the-art parser for Ger-
man. An evaluation on the TIGER cor-
pus shows that our inclusion entity model
achieves a performance gain of 4.3 points in
F-score over a baseline of no inclusion de-
tection, and even outperforms a parser with
access to gold standard part-of-speech tags.
1 Introduction
The status of English as a global language means
that English words and phrases are frequently bor-
rowed by other languages, especially in domains
such as science and technology, commerce, adver-
tising, and current affairs. This is an instance of lan-
guage mixing, whereby inclusions from other lan-
guages appear in an otherwise monolingual text.
While the processing of foreign inclusions has re-
ceived some attention in the text-to-speech (TTS) lit-
erature (see Section 2), the natural language process-
ing (NLP) community has paid little attention both
to the problem of inclusion detection, and to poten-
tial applications thereof. Also the extent to which
inclusions pose a problem to existing NLP methods
has not been investigated.
In this paper, we address this challenge. We focus
on English inclusions in German text. Anglicisms
and other borrowings from English form by far the
most frequent foreign inclusions in German. In spe-
cific domains, up to 6.4% of the tokens of a Ger-
man text can be English inclusions. Even in regular
newspaper text as used for many NLP applications,
English inclusions can be found in up to 7.4% of all
sentences (see Section 3 for both figures).
Virtually all existing NLP algorithms assume that
the input is monolingual, and does not contain for-
eign inclusions. It is possible that this is a safe
assumption, and inclusions can be dealt with ac-
curately by existing methods, without resorting to
specialized mechanisms. The alternative hypothe-
sis, however, seems more plausible: foreign inclu-
sions pose a problem for existing approaches, and
sentences containing them are processed less ac-
curately. A parser, for example, is likely to have
problems with inclusions ? most of the time, they
are unknown words, and as they originate from
another language, standard methods for unknown
words guessing (suffix stripping, etc.) are unlikely to
be successful. Furthermore, the fact that inclusions
are often multiword expressions (e.g., named enti-
ties) means that simply part-of-speech (POS) tag-
ging them accurately is not sufficient: if the parser
posits a phrase boundary within an inclusion this is
likely to severely decrease parsing accuracy.
In this paper, we focus on the impact of En-
glish inclusions on the parsing of German text. We
describe an annotation-free method that accurately
recognizes English inclusions, and demonstrate that
inclusion detection improves the performance of a
state-of-the-art parser for German. We show that the
way of interfacing the inclusion detection and the
parser is crucial, and propose a method for modify-
ing the underlying probabilistic grammar in order to
151
enable the parser to process inclusions accurately.
This paper is organized as follows. We review re-
lated work in Section 2, and present the English in-
clusion classifier in Section 3. Section 4 describes
our results on interfacing inclusion detection with
parsing, and Section 5 presents an error analysis.
Discussion and conclusion follow in Section 6.
2 Related Work
Previous work on inclusion detection exists in the
TTS literature. Here, the aim is to design a sys-
tem that recognizes foreign inclusions on the word
and sentence level and functions at the front-end to
a polyglot TTS synthesizer. Pfister and Romsdor-
fer (2003) propose morpho-syntactic analysis com-
bined with lexicon lookup to identify foreign words
in mixed-lingual text. While they state that their sys-
tem is precise at detecting the language of tokens
and determining the sentence structure, it is not eval-
uated on real mixed-lingual text. A further approach
to inclusion detection is that of Marcadet et. al
(2005). They present experiments with a dictionary-
driven transformation-based learning method and a
corpus-based n-gram approach and show that a com-
bination of both methods yields the best results.
Evaluated on three mixed-lingual test sets in differ-
ent languages, the combined approach yields word-
based language identification error rates (i.e. the per-
centage of tokens for which the language is identi-
fied incorrectly) of 0.78% on the French data, 1.33%
on the German data and 0.84% on the Spanish data.
Consisting of 50 sentences or less for each language,
their test sets are very small and appear to be se-
lected specifically for evaluation purposes. It would
therefore be interesting to determine the system?s
performance on random and unseen data and exam-
ine how it scales up to larger data sets.
Andersen (2005), noting the importance of rec-
ognizing anglicisms to lexicographers, tests algo-
rithms based on lexicon lookup, character n-grams
and regular expressions and a combination thereof to
automatically extract anglicisms in Norwegian text.
On a 10,000 word subset of the neologism archive
(Wangensteen, 2002), the best method of combin-
ing character n-grams and regular expression match-
ing yields an accuracy of 96.32% and an F-score of
59.4 (P = 75.8%, R = 48.8%). This result is unsur-
prisingly low as no differentiation is made between
full-word anglicisms and tokens with mixed-lingual
morphemes in the gold standard.
In the context of parsing, Forst and Kaplan (2006)
have observed that the failure to properly deal with
foreign inclusions is detrimental to a parser?s accu-
racy. However, they do not substantiate this claim
using numeric results.
3 English Inclusion Detection
Previous work reported by Alex (2006; 2005) has
focused on devising a classifier that detects angli-
cisms and other English inclusions in text written in
other languages, namely German and French. This
inclusion classifier is based on a lexicon and search
engine lookup as well as a post-processing step.
The lexicon lookup is performed for tokens
tagged as noun (NN ), named entity (NE ), foreign
material (FM ) or adjective (ADJA/ADJD ) using the
German and English CELEX lexicons. Tokens only
found in the English lexicon are classified as En-
glish. Tokens found in neither lexicon are passed
to the search engine module. Tokens found in
both databases are classified by the post-processing
module. The search engine module performs lan-
guage classification based on the maximum nor-
malised score of the number of hits returned for two
searches per token, one for each language (Alex,
2005). This score is determined by weighting the
number of hits, i.e. the ?absolute frequency? by the
estimated size of the accessible Web corpus for that
language (Alex, 2006). Finally, the rule-based post-
processing module classifies single-character tokens
and resolves language classification ambiguities for
interlingual homographs, English function words,
names of currencies and units of measurement. A
further post-processing step relates language infor-
mation between abbreviations or acronyms and their
definitions in combination with an abbreviation ex-
traction algorithm (Schwartz and Hearst, 2003). Fi-
nally, a set of rules disambiguates English inclusions
from person names (Alex, 2006).
For German, the classifier has been evaluated
on test sets in three different domains: newspaper
articles, selected from the Frankfurter Allgemeine
Zeitung, on internet and telecoms, space travel and
European Union related topics. Table 1 presents an
152
Domain EI tokens EI types EI TTR Accuracy Precision Recall F
Internet 6.4% 5.9% 0.25 98.13% 91.58% 78.92% 84.78
Space 2.8% 3.5% 0.33 98.97% 84.02% 85.31% 84.66
EU 1.1% 2.1% 0.50 99.65% 82.16% 87.36% 84.68
Table 1: English inclusion (EI) token and type statistics, EI type-token-ratios (TTR) as well as accuracy,
precision, recall and F-scores for the unseen German test sets.
overview of the percentages of English inclusion to-
kens and types within the gold standard annotation
of each test set, and illustrates how well the English
inclusion classifier is able to detect them in terms
of F-score. The figures show that the frequency of
English inclusions varies considerably depending on
the domain but that the classifier is able to detect
them equally well with an F-score approaching 85
for each domain.
The recognition of English inclusions bears sim-
ilarity to classification tasks such as named en-
tity recognition, for which various machine learning
(ML) techniques have proved successful. In order to
compare the performance of the English inclusion
classifier against a trained ML classifier, we pooled
the annotated English inclusion evaluation data for
all three domains. As the English inclusion classifier
does not rely on annotated data, it can be tested and
evaluated once for the entire corpus. The ML classi-
fier used for this experiment is a conditional Markov
model tagger which is designed for, and proved suc-
cessful in, named entity recognition in newspaper
and biomedical text (Klein et al, 2003; Finkel et al,
2005). It can be trained to perform similar informa-
tion extraction tasks such as English inclusion detec-
tion. To determine the tagger?s performance over the
entire set and to investigate the effect of the amount
of annotated training data available, a 10-fold cross-
validation test was conducted whereby increasing
sub-parts of the training data are provided when test-
ing on each fold. The resulting learning curves in
Figure 1 show that the English inclusion classifier
has an advantage over the supervised ML approach,
despite the fact the latter requires expensive hand-
annotated data. A large training set of 80,000 tokens
is required to yield a performance that approximates
that of our annotation-free inclusion classifier. This
system has been shown to perform similarly well on
unseen texts in different domains, plus it is easily
 20
 30
 40
 50
 60
 70
 80
 90
 10000  20000  30000  40000  50000  60000  70000  80000
F-
sc
or
e
Amount of training data (in tokens)
Statistical Tagger
English Inclusion Classifier
Figure 1: Learning curve of a ML classifier versus
the English inclusion classifier?s performance.
extendable to a new language (Alex, 2006).
4 Experiments
The primary focus of this paper is to apply the En-
glish inclusion classifier to the German TIGER tree-
bank (Brants et al, 2002) and to evaluate the clas-
sifier on a standard NLP task, namely parsing. The
aim is to investigate the occurrence of English in-
clusions in more general newspaper text, and to ex-
amine if the detection of English inclusions can im-
prove parsing performance.
The TIGER treebank is a bracketed corpus con-
sisting of 40,020 sentences of newspaper text. The
English inclusion classifier was run once over the
entire TIGER corpus. In total, the system detected
English inclusions in 2,948 of 40,020 sentences
(7.4%), 596 of which contained at least one multi-
word inclusion. This subset of 596 sentences is the
focus of the work reported in the remainder of this
paper, and will be referred to as the inclusion set.
A gold standard parse tree for a sentence contain-
ing a typical multi-word English inclusion is illus-
trated in Figure 2. The tree is relatively flat, which
153
is a trait trait of TIGER treebank annotation (Brants
et al, 2002). The non-terminal nodes of the tree rep-
resent the phrase categories, and the edge labels the
grammatical functions. In the example sentence, the
English inclusion is contained in a proper noun (PN )
phrase with a grammatical function of type noun
kernel element (NK ). Each terminal node is POS-
tagged as a named entity (NE ) with the grammatical
function ot type proper noun component (PNC ).
4.1 Data
Two different data sets are used in the experiments:
(1) the inclusion set, i.e., the sentences containing
multi-word English inclusions recognized by the in-
clusion classifier, and (2) a stratified sample of sen-
tences randomly extracted from the TIGER corpus,
with strata for different sentence lengths. The strata
were chosen so that the sentence length distribution
of the random set matches that of the inclusion set.
The average sentence length of this random set and
the inclusion set is therefore the same at 28.4 tokens.
This type of sampling is necessary as the inclusion
set has a higher average sentence length than a ran-
dom sample of sentences from TIGER, and because
parsing accuracy is correlated with sentence length.
Both the inclusion set and the random set consist of
596 sentences and do not overlap.
4.2 Parser
The parsing experiments were performed with a
state-of-the-art parser trained on the TIGER corpus
which returns both phrase categories and grammati-
cal functions (Dubey, 2005b). Following Klein and
Manning (2003), the parser uses an unlexicalized
probabilistic context-free grammar (PCFG) and re-
lies on treebank transformations to increase parsing
accuracy. Crucially, these transformations make use
of TIGER?s grammatical functions to relay pertinent
lexical information from lexical elements up into the
tree.
The parser also makes use of suffix analysis.
However, beam search or smoothing are not em-
ployed. Based upon an evaluation on the NEGRA
treebank (Skut et al, 1998), using a 90%-5%-5%
training-development-test split, the parser performs
with an accuracy of 73.1 F-score on labelled brack-
ets with a coverage of 99.1% (Dubey, 2005b). These
figures were derived on a test set limited to sentences
containing 40 tokens or less. In the data set used
in this paper, however, sentence length is not lim-
ited. Moreover, the average sentence length of our
test sets is considerably higher than that of the NE-
GRA test set. Consequently, a slightly lower perfor-
mance and/or coverage is anticipated, albeit the type
and domain as well as the annotation of both the NE-
GRA and the TIGER treebanks are very similar. The
minor annotation differences that do exist between
NEGRA and TIGER are explained in Brants et. al
(2002).
4.3 Parser Modifications
We test several variations of the parser. The baseline
parser does not treat foreign inclusions in any spe-
cial way: the parser attempts to guess the POS tag
and grammatical function labels of the word using
the same suffix analysis as for rare or unseen Ger-
man words. The additional versions of the parser
are inspired by the hypothesis that inclusions make
parsing difficult, and this difficulty arises primarily
because the parser cannot detect inclusions prop-
erly. Therefore, a suitable upper bound is to give
the parser perfect tagging information. Two further
versions interface with our inclusion classifier and
treat words marked as inclusions differently from
native words. The first version does so on a word-
by-word basis. In contrast, the inclusion entity ap-
proach attempts to group inclusions, even if a group-
ing is not posited by phrase structure rules. We now
describe each version in more detail.
In the TIGER annotation, preterminals include
both POS tags and grammatical function labels.
For example, rather than a preterminal node hav-
ing the category PRELS (personal pronoun), it is
given the category PRELS-OA (accusative personal
pronoun). Due to these grammatical function tags,
the perfect tagging parser may disambiguate more
syntactic information than provided with POS tags
alone. Therefore, to make this model more realistic,
the parser is required to guess grammatical functions
(allowing it to, for example, mistakenly tag an ac-
cusative pronoun as nominative, dative or genitive).
This gives the parser information about the POS tags
of English inclusions (along with other words), but
does not give any additional hints about the syntax
of the sentence.
The two remaining models both take advantage
154
SNP-SB
ART-NK
Das
ADJA-NK
scho?nste
PN-NK
NE-PNC
Road
NE-PNC
Movie
VVFIN-HD
kam
PP-MO
APPR-AC
aus
ART-NK
der
NE-NK
Schweiz
Figure 2: Example parse tree of a German TIGER sentence containing an English inclusion. Translation:
The nicest road movie came from Switzerland.
NE FM NN KON CARD ADJD APPR
1185 512 44 8 8 1 1
Table 2: POS tags of foreign inclusions.
PN
FOM
. . .
FOM
. . .
(a) Whenever a FOM is encoun-
tered...
PN
FP
FOM
. . .
FOM
. . .
(b) ...a new FP category is cre-
ated
Figure 3: Tree transformation employed in the in-
clusion entity parser.
of information from the inclusion detector. To inter-
face the detector with the parser, we simply mark
any inclusion with a special FOM (foreign mate-
rial) tag. The word-by-word parser attempts to guess
POS tags itself, much like the baseline. However,
whenever it encounters a FOM tag, it restricts itself
to the set of POS tags observed in inclusions during
training (the tags listed in Table 2). When a FOM is
detected, these and only these POS tags are guessed;
all other aspects of the parser remain the same.
The word-by-word parser fails to take advantage
of one important trend in the data: that foreign in-
clusion tokens tend to be adjacent, and these adja-
cent words usually refer to the same entity. There
is nothing stopping the word-by-word parser from
positing a constituent boundary between two adja-
cent foreign inclusions. The inclusion entity model
was developed to restrict such spurious bracketing.
It does so by way of another tree transformation.
The new category FP (foreign phrase) is added be-
low any node dominating at least one token marked
FOM during training. For example, when encoun-
tering a FOM sequence dominated by PN as in Fig-
ure 3(a), the tree is modified so that it is the FP rule
which generates the FOM tokens. Figure 3(b) shows
the modified tree. In all cases, a unary rule PN?FP
is introduced. As this extra rule decreases the proba-
bility of the entire tree, the parser has a bias to intro-
duce as few of these rules as possible ? thus limiting
the number of categories which expand to FOMs.
Once a candidate parse is created during testing, the
inverse operation is applied, removing the FP node.
4.4 Method
For all experiments reported in this paper, the parser
is trained on the TIGER treebank. As the inclusion
and random sets are drawn from the whole TIGER
treebank, it is necessary to ensure that the data used
to train the parser does not overlap with these test
sentences. The experiments are therefore designed
as multifold cross-validation tests. Using 5 folds,
each model is trained on 80% of the data while the
remaining 20% are held out. The held out set is then
155
Data P R F Dep. Cov. AvgCB 0CB ?2CB
Baseline model
Inclusion set 56.1 62.6 59.2 74.9 99.2 2.1 34.0 69.0
Random set 63.3 67.3 65.2 81.1 99.2 1.6 40.4 75.1
Perfect tagging model
Inclusion set 61.3 63.0 62.2 75.1 92.7 1.7 41.5 72.6
Random set 65.8 68.9 67.3 82.4 97.7 1.4 45.9 77.1
Word-by-word model
Inclusion set 55.6 62.8 59.0 73.1 99.2 2.1 34.2 70.2
Random set 63.3 67.3 65.2 81.1 99.2 1.6 40.4 75.1
Inclusion entity model
Inclusion set 61.3 65.9 63.5 78.3 99.0 1.7 42.4 77.1
Random set 63.4 67.5 65.4 80.8 99.2 1.6 40.1 75.7
Table 3: Baseline and perfect tagging for inclusion and random sets and results for the word-by-word and
the inclusion entity models.
intersected with the inclusion set (or, respectively,
the random set). The evaluation metrics are calcu-
lated on this subset of the inclusion set (or random
set), using the parser trained on the corresponding
training data. This process ensures that the test sen-
tences are not contained in the training data.
The overall performance metrics of the parser are
calculated on the aggregated totals of the five held
out test sets. For each experiment, we report pars-
ing performance in terms of the standard PARSE-
VAL scores (Abney et al, 1991), including cov-
erage (Cov), labeled precision (P) and recall (R),
F-score, the average number of crossing brackets
(AvgCB), and the percentage of sentences parsed
with zero and with two or fewer crossing brack-
ets (0CB and ?2CB). In addition, we also report
dependency accuracy (Dep), calculated using the
approach described in Lin (1995), using the head-
picking method used by Dubey (2005a). The la-
beled bracketing figures (P, R and F), and the de-
pendency score are calculated on all sentences, with
those which are out-of-coverage getting zero nodes.
The crossing bracket scores are calculated only on
those sentences which are successfully parsed.
4.5 Baseline and Perfect Tagging
The baseline, for which the unmodified parser is
used, achieves a high coverage at over 99% for both
the inclusion and the random sets (see Table 3).
However, scores differ for the bracketing measures.
Using stratified shuffling1, we performed a t-test on
precision and recall, and found both to be signif-
icantly worse in the inclusion condition. Overall,
the harmonic mean (F) of precision and recall was
65.2 on the random set, 6 points better than 59.2
F observed on the inclusion set. Similarly, depen-
dency and cross-bracketing scores are higher on the
random test set. This result strongly indicates that
sentences containing English inclusions present dif-
ficulty for the parser, compared to length-matched
sentences without inclusions.
When providing the parser with perfect tagging
information, scores improve both for the inclusion
and the random TIGER samples, resulting in F-
scores of 62.2 and 67.3, respectively. However, the
coverage for the inclusion set decreases to 92.7%
whereas the coverage for the random set is 97.7%.
In both cases, the lower coverage is caused by the
parser being forced to use infrequent tag sequences,
with the much lower coverage of the inclusion set
likely due to infrequent tags (notable FM ), solely
associated with inclusions. While perfect tagging
increases overall accuracy, a difference of 5.1 in F-
score remains between the random and inclusion test
sets. Although smaller than that of the baseline runs,
this difference shows that even with perfect tagging,
1This approach to statistical testing is described in: http:
//www.cis.upenn.edu/?dbikel/software.html
156
parsing English inclusions is harder than parsing
monolingual data.
So far, we have shown that the English inclusion
classifier is able to detect sentences that are difficult
to parse. We have also shown that perfect tagging
helps to improve parsing performance but is insuffi-
cient when it comes to parsing sentences containing
English inclusions. In the next section, we will ex-
amine how the knowledge provided by the English
inclusion classifier can be exploited to improve pars-
ing performance for such sentences.
4.6 Word-by-word Model
The word-by-word model achieves the same cover-
age on the inclusion set as the baseline but with a
slightly lower F of 59.0. All other scores, includ-
ing dependency accuracy and cross bracketing re-
sults are similar to those of the baseline (see Ta-
ble 3). This shows that limiting the parser?s choice
of POS tags to those encountered for English inclu-
sions is not sufficient to deal with such constructions
correctly. In the error analysis presented in Sec-
tion 5, we report that the difficulty in parsing multi-
word English inclusions is recognizing them as con-
stituents, rather than recognizing their POS tags. We
attempt to overcome this problem with the inclusion
entity model.
4.7 Inclusion Entity Model
The inclusion entity parser attains a coverage of
99.0% on the inclusion set, similiar to the cover-
age of 99.2% obtained by the baseline model on
the same data. On all other measures, the inclu-
sion entity model exceeds the performance of the
baseline, with a precision of 61.3% (5.2% higher
than the baseline), a recall of 65.9% (3.3% higher),
an F of 63.5 (4.3 higher) and a dependency accu-
racy of 78.3% (3.4% higher). The average number
of crossing brackets is 1.7 (0.4 lower), with 42.4%
of the parsed sentences having no crossing brack-
ets (8.2% higher), and 77.1% having two or fewer
crossing brackets (8.1% higher). When testing the
inclusion entity model on the random set, the per-
formance is very similar to the baseline model on
this data. While coverage is the same, F and cross-
brackting scores are marginally improved, and the
dependency score is marginally deteriorated. This
shows that the inclusion entity model does not harm
 0
 0.002
 0.004
 0.006
 0.008
 0.01
 0.012
 0.014
 10  20  30  40  50  60  70  80
A
ve
ra
ge
 T
ok
en
 F
re
qu
en
cy
Sentence Length in Tokens
Inclusion sample
Stratified random sample
Figure 4: Average relative token frequencies for sen-
tences of equal length.
the parsing accuracy of sentences that do not actu-
ally contain foreign inclusions.
Not only did the inclusion entity parser perform
above the baseline on every metric for the inclusion
set, its performance also exceeds that of the perfect
tagging model on all measures except precision and
average crossing brackets, where both models are
tied. These results clearly indicate that the inclusion
entity model is able to leverage the additional infor-
mation about English inclusions provided by our in-
clusion classifier. However, it is also important to
note that the performance of this model on the in-
clusion set is still consistently lower than that of all
models on the random set. This demonstrates that
sentences with inclusions are more difficult to parse
than monolingual sentences, even in the presence of
information about the inclusions that the parser can
exploit.
Comparing the inclusion set to the length-
matched random set is arguably not entirely fair as
the latter may not contain as many infrequent tokens
as the inclusion set. Figure 4 shows the average rel-
ative token frequencies for sentences of equal length
for both sets. The frequency profiles of the two data
sets are broadly similar (the difference in means of
both groups is only 0.000676), albeit significantly
different according to a paired t-test (p? 0.05). This
is one reason why the inclusion entity model?s per-
formance on the inclusion set does not reach the up-
per limit set by the random sample.
157
Phrase cat. Frequency Example
PN 91 The Independent
CH 10 Made in Germany
NP 4 Peace Enforcement
CNP 2 Botts and Company
? 2 Chief Executives
Table 4: Gold phrase categories of inclusions.
5 Error Analysis
The error analysis is limited to 100 sentences se-
lected from the inclusion set parsed with both the
baseline and the inclusion entity model. This sam-
ple contains 109 English inclusions, five of which
are false positives, i.e., the output of the English in-
clusion classifier is incorrect. The precision of the
classifier in recognizing multi-word English inclu-
sions is therefore 95.4% for this TIGER sample.
Table 4 illustrates that the majority of multi-word
English inclusions are contained in a proper noun
(PN ) phrase, including names of companies, politi-
cal parties, organizations, films, newspapers, etc. A
less frequent phrasal category is chunk (CH ) which
tends to be used for slogans, quotes or expressions
like Made in Germany. Even in this small sam-
ple, annotations of inclusions as either PN or CH ,
and not the other, can be misleading. For example,
the organization Friends of the Earth is annotated
as a PN , whereas another organization International
Union for the Conservation of Nature is marked as
a CH in the gold standard. This suggests that the
annotation guidelines on foreign inclusions could be
improved when differentiating between phrase cate-
gories containing foreign material.
For the majority of sentences (62%), the baseline
model predicts more brackets than are present in the
gold standard parse tree (see Table 5). This number
decreases by 11% to 51% when parsing with the in-
clusion entity model. This suggests that the baseline
parser does not recognize English inclusions as con-
stituents, and instead parses their individual tokens
as separate phrases. Provided with additional infor-
mation of multi-word English inclusions in the train-
ing data, the parser is able to overcome this problem.
We now turn our attention to how accurately the
various parsers are at predicting both phrase brack-
eting and phrase categories (see Table 6). For 46
Phrase bracket (PB) frequency BL IE
PBPRED > PBGOLD 62% 51%
PBPRED < PBGOLD 11% 13%
PBPRED = PBGOLD 27% 36%
Table 5: Bracket frequency of the predicted baseline
(BL) and inclusion entity (IE) model output com-
pared to the gold standard.
(42.2%) of inclusions, the baseline model makes an
error with a negative effect on performance. In 39
cases (35.8%), the phrase bracketing and phrase cat-
egory are incorrect, and constituent boundaries oc-
cur within the inclusion, as illustrated in Figure 5(a).
Such errors also have a detrimental effect on the
parsing of the remainder of the sentence. Overall,
the baseline model predicts the correct phrase brack-
eting and phrase category for 63 inclusions (57.8%).
Conversely, the inclusion entity model, which is
given information on tag consistency within inclu-
sions via the FOM tags, is able to determine the
correct phrase bracketing and phrase category for
67.9% inclusions (10.1% more), e.g. see Figure 5(b).
Both the phrase bracketing and phrase category are
predicted incorrectly in only 6 cases (5.5%). The
inclusion entity model?s improved phrase boundary
prediction for 31 inclusions (28.4% more correct) is
likely to have an overall positive effect on the pars-
ing decisions made for the context which they ap-
pear in. Nevertheless, the inclusion entity parser still
has difficulty determining the correct phrase cate-
gory in 25 cases (22.9%). The main confusion lies
between assigning the categories PN , CH and NP ,
the most frequent phrase categories of multi-word
English inclusions. This is also partially due to the
ambiguity between these phrases in the gold stan-
dard. Finally, few parsing errors (4) are caused by
the inclusion entity parser due to the markup of false
positive inclusions (mainly boundary errors).
6 Discussion and Conclusion
This paper has argued that English inclusions in
German text is an increasingly pervasive instance
of language mixing. Starting with the hypothesis
that such inclusions can be a significant source of
errors for monolingual parsers, we found evidence
that an unmodified state-of-the-art parser for Ger-
158
...
PN-NK
NP-PNC
NE-NK
Made
PP-MNR
APPR-AD
In
NE-NK
Heaven
(a) Partial parsing output of the baseline model with a con-
stiuent boundary in the English inclusion.
...
PN-NK
FOM
Made
FOM
In
FOM
Heaven
(b) Partial parsing output of the inclusion en-
tity model with the English inclusion parsed cor-
rectly.
Figure 5: Comparing baseline model output to inclusion entity model output.
Errors No. of inclusions (in %)
Parser: baseline model, data: inclusion set
Incorrect PB and PC 39 (35.8%)
Incorrect PC 5 (4.6%)
Incorrect PB 2 (1.8%)
Correct PB and PC 63 (57.8%)
Parser: inclusion entity model, data: inclusion set
Incorrect PB and PC 6 (5.5%)
Incorrect PC 25 (22.9%)
Incorrect PB 4 (3.7%)
Correct PB and PC 74 (67.9%)
Table 6: Baseline and inclusion entity model errors
for inclusions with respect to their phrase bracketing
(PB) and phrase category (PC).
man performs substantially worse on a set of sen-
tences with English inclusions compared to a set of
length-matched sentences randomly sampled from
the same corpus. The lower performance on the
inclusion set persisted even when the parser when
given gold standard POS tags in the input.
To overcome the poor accuracy of parsing inclu-
sions, we developed two methods for interfacing the
parser with an existing annotation-free inclusion de-
tection system. The first method restricts the POS
tags for inclusions that the parser can assign to those
found in the data. The second method applies tree
transformations to ensure that inclusions are treated
as phrases. An evaluation on the TIGER corpus
shows that the second method yields a performance
gain of 4.3 in F-score over a baseline of no inclusion
detection, and even outperforms a model involving
perfect POS tagging of inclusions.
To summarize, we have shown that foreign inclu-
sions present a problem for a monolingual parser.
We also demonstrated that it is insufficient to know
where inclusions are or even what their parts of
speech are. Parsing performance only improves if
the parser also has knowledge about the structure of
the inclusions. It is particularly important to know
when adjacent foreign words are likely to be part of
the same phrase. As our error analysis showed, this
prevents cascading errors further up in the parse tree.
Finally, our results indicate that future work could
improve parsing performance for inclusions further:
we found that parsing the inclusion set is still harder
than parsing a randomly sampled test set, even for
our best-performing model. This provides an up-
per bound on the performance we can expect from
a parser that uses inclusion detection. Future work
will also involve determining the English inclusion
classifier?s merit when applied to rule-based parsing.
Acknowledgements
This research is supported by grants from the Scot-
tish Enterprise Edinburgh-Stanford Link (R36759),
ESRC, and the University of Edinburgh. We would
also like to thank Claire Grover for her comments
and feedback.
159
References
Steven Abney, Dan Flickenger, Claudia Gdaniec, Ralph
Grishman, Philip Harrison, Donald Hindle, Robert In-
gria, Frederick Jelinek, Judith Klavans, Mark Liber-
man, Mitchell P. Marcus, Salim Roukos, Beatrice San-
torini, and Tomek Strzalkowski. 1991. Procedure for
quantitatively comparing the syntactic coverage of En-
glish grammars. In Ezra Black, editor, HLT?91: Pro-
ceedings of the workshop on Speech and Natural Lan-
guage, pages 306?311, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Beatrice Alex. 2005. An unsupervised system for identi-
fying English inclusions in German text. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL 2005), Student Re-
search Workshop, pages 133?138, Ann Arbor, Michi-
gan, USA.
Beatrice Alex. 2006. Integrating language knowledge
resources to extend the English inclusion classifier to
a new language. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC 2006), Genoa, Italy.
Gisle Andersen. 2005. Assessing algorithms for auto-
matic extraction of Anglicisms in Norwegian texts. In
Corpus Linguistics 2005, Birmingham, UK.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER Tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories (TLT02), pages 24?41, So-
zopol, Bulgaria.
Amit Dubey. 2005a. Statistical Parsing for German:
Modeling syntactic properties and annotation differ-
ences. Ph.D. thesis, Saarland University, Germany.
Amit Dubey. 2005b. What to do when lexicalization
fails: parsing German with suffix analysis and smooth-
ing. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL
2005), pages 314?321, Ann Arbor, Michigan, USA.
Jenny Finkel, Shipra Dingare, Christopher D. Manning,
Malvina Nissim, Beatrice Alex, and Claire Grover.
2005. Exploring the boundaries: Gene and protein
identification in biomedical text. BMC Bioinformat-
ics, 6(Suppl 1):S5.
Martin Forst and Ronald M. Kaplan. 2006. The impor-
tance of precise tokenizing for deep grammars. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation (LREC 2006), pages
369?372, Genoa, Italy.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2003), pages 423?430,
Saporo, Japan.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Proceedings of the
Seventh Conference on Natural Language Learning
(CoNLL-03), pages 180?183, Edmonton, Canada.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings
of the International Joint Conference on Artificial In-
telligence (IJCAI-95), pages 1420?1425, Montreal,
Canada.
Jean-Christophe Marcadet, Volker Fischer, and Claire
Waast-Richard. 2005. A transformation-based learn-
ing approach to language identification for mixed-
lingual text-to-speech synthesis. In Proceedings of
Interspeech 2005 - ICSLP, pages 2249?2252, Lisbon,
Portugal.
Beat Pfister and Harald Romsdorfer. 2003. Mixed-
lingual analysis for polyglot TTS synthesis. In
Proceedings of Eurospeech 2003, pages 2037?2040,
Geneva, Switzerland.
Ariel Schwartz and Marti Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In Proceedings of the Pacific Sym-
posium on Biocomputing (PSB 2003), pages 451?462,
Kauai, Hawaii.
Wojciech Skut, Thorsten Brants, Brigitte Krenn, and
Hans Uszkoreit. 1998. A linguistically interpreted
corpus of German newspaper text. In Proceedings of
the Conference on Language Resources and Evalua-
tion (LREC 1998), pages 705?712, Granada, Spain.
Boye Wangensteen. 2002. Nettbasert nyordsinnsamling.
Spra?knytt, 2:17?19.
160
Proceedings of the 43rd Annual Meeting of the ACL, pages 314?321,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
What to do when lexicalization fails: parsing German with suffix analysis
and smoothing
Amit Dubey
University of Edinburgh
Amit.Dubey@ed.ac.uk
Abstract
In this paper, we present an unlexical-
ized parser for German which employs
smoothing and suffix analysis to achieve
a labelled bracket F-score of 76.2, higher
than previously reported results on the
NEGRA corpus. In addition to the high
accuracy of the model, the use of smooth-
ing in an unlexicalized parser allows us
to better examine the interplay between
smoothing and parsing results.
1 Introduction
Recent research on German statistical parsing has
shown that lexicalization adds little to parsing per-
formance in German (Dubey and Keller, 2003; Beil
et al, 1999). A likely cause is the relative produc-
tivity of German morphology compared to that of
English: German has a higher type/token ratio for
words, making sparse data problems more severe.
There are at least two solutions to this problem: first,
to use better models of morphology or, second, to
make unlexicalized parsing more accurate.
We investigate both approaches in this paper. In
particular, we develop a parser for German which at-
tains the highest performance known to us by mak-
ing use of smoothing and a highly-tuned suffix ana-
lyzer for guessing part-of-speech (POS) tags from
the input text. Rather than relying on smoothing
and suffix analysis alone, we also utilize treebank
transformations (Johnson, 1998; Klein and Man-
ning, 2003) instead of a grammar induced directly
from a treebank.
The organization of the paper is as follows: Sec-
tion 2 summarizes some important aspects of our
treebank corpus. In Section 3 we outline several
techniques for improving the performance of unlex-
icalized parsing without using smoothing, including
treebank transformations, and the use of suffix anal-
ysis. We show that suffix analysis is not helpful
on the treebank grammar, but it does increase per-
formance if used in combination with the treebank
transformations we present. Section 4 describes how
smoothing can be incorporated into an unlexicalized
grammar to achieve state-of-the-art results in Ger-
man. Rather using one smoothing algorithm, we use
three different approaches, allowing us to compare
the relative performance of each. An error analy-
sis is presented in Section 5, which points to several
possible areas of future research. We follow the er-
ror analysis with a comparison with related work in
Section 6. Finally we offer concluding remarks in
Section 7.
2 Data
The parsing models we present are trained and tested
on the NEGRA corpus (Skut et al, 1997), a hand-
parsed corpus of German newspaper text containing
approximately 20,000 sentences. It is available in
several formats, and in this paper, we use the Penn
Treebank (Marcus et al, 1993) format of NEGRA.
The annotation used in NEGRA is similar to that
used in the English Penn Treebank, with some dif-
ferences which make it easier to annotate German
syntax. German?s flexible word order would have
required an explosion in long-distance dependencies
(LDDs) had annotation of NEGRA more closely
resembled that of the Penn Treebank. The NE-
GRA designers therefore chose to use relatively flat
trees, encoding elements of flexible word order us-
314
ing grammatical functions (GFs) rather than LDDs
wherever possible.
To illustrate flexible word order, consider the sen-
tences Der Mann sieht den Jungen (?The man sees
the boy?) and Den Jungen sieht der Mann. Despite
the fact the subject and object are swapped in the
second sentence, the meaning of both are essentially
the same.1 The two possible word orders are dis-
ambiguated by the use of the nominative case for
the subject (marked by the article der) and the ac-
cusative case for the object (marked by den) rather
than their position in the sentence.
Whenever the subject appears after the verb, the
non-standard position may be annotated using a
long-distance dependency (LDD). However, as men-
tioned above, this information can also be retrieved
from the grammatical function of the respective
noun phrases: the GFs of the two NPs above would
be ?subject? and ?accusative object? regardless of
their position in the sentence. These labels may
therefore be used to recover the underlying depen-
dencies without having to resort to LDDs. This is
the approach used in NEGRA. It does have limita-
tions: it is only possible to use GF labels instead of
LDDs when all the nodes of interest are dominated
by the same parent. To maximize cases where all
necessary nodes are dominated by the same parent,
NEGRA uses flat ?dependency-style? rules. For ex-
ample, there is no VP node when there is no overt
auxiliary verb. category. Under the NEGRA anno-
tation scheme, the first sentence above would have
a rule S   NP-SB VVFIN NP-OA and the second,
S   NP-OA VVFIN NP-SB, where SB denotes sub-
ject and OA denotes accusative object.
3 Parsing with Grammatical Functions
3.1 Model
As explained above, this paper focuses on unlexi-
calized grammars. In particular, we make use of
probabilistic context-free grammars (PCFGs; Booth
(1969)) for our experiments. A PCFG assigns each
context-free rule LHS   RHS a conditional prob-
ability Pr

RHS LHS  . If a parser were to be given
POS tags as input, this would be the only distribution
1Pragmatically speaking, the second sentence has a slightly
different meaning. A better translation might be: ?It is the boy
the man sees.?
required. However, in this paper we are concerned
with the more realistic problem of accepting text as
input. Therefore, the parser also needs a probabil-
ity distribution Pw

w LHS  to generate words. The
probability of a tree is calculated by multiplying the
probabilities all the rules and words generated in the
derivation of the tree.
The rules are simply read out from the treebank,
and the probabilities are estimated from the fre-
quency of rules in the treebank. More formally:
Pr

RHS LHS  c

LHS   RHS 
c

LHS  (1)
The probabilities of words given tags are simi-
larly estimated from the frequency of word-tag co-
occurrences:
Pw

w LHS  c

LHS  w 
c

LHS  (2)
To handle unseen or infrequent words, all words
whose frequency falls below a threshold ? are
grouped together in an ?unknown word? token,
which is then treated like an additional word. For
our experiments, we use ?  10.
We consider several variations of this simple
model by changing both Pr and Pw. In addition to
the standard formulation in Equation (1), we con-
sider two alternative variants of Pr. The first is a
Markov context-free rule (Magerman, 1995; Char-
niak, 2000). A rule may be turned into a Markov
rule by first binarizing it, then making independence
assumptions on the new binarized rules. Binarizing
the rule A   B1 		 Bn results in a number of smaller
rules A   B1AB1 , AB1
  B2AB1B2 , 		 , AB1 
 
 
 Bn  1
 
Bn. Binarization does not change the probability of
the rule:
P

B1 		 Bn A 

i  1
?
n
P

  Bi A  B1  		  Bi  1 
Making the 2nd order Markov assumption ?forgets?
everything earlier then 2 previous sisters. A rule
would now be in the form ABi  2Bi  1
  BiABi  1Bi , and
the probability would be:
P

B1 		 Bn A 

i  1
?
n
P

Bi A  Bi  2  Bi  1 
315
The other rule type we consider are linear prece-
dence/immediate dominance (LP/ID) rules (Gazdar
et al, 1985). If a context-free rule can be thought
of as a LHS token with an ordered list of tokens on
the RHS, then an LP/ID rule can be thought of as
a LHS token with a multiset of tokens on the RHS
together with some constraints on the possible or-
ders of tokens on the RHS. Uszkoreit (1987) argues
that LP/ID rules with violatable ?soft? constraints
are suitable for modelling some aspects of German
word order. This makes a probabilistic formulation
of LP/ID rules ideal: probabilities act as soft con-
straints.
Our treatment of probabilistic LP/ID rules gener-
ate children one constituent at a time, conditioning
upon the parent and a multiset of previously gener-
ated children. Formally, the the probability of the
rule is approximated as:
P

B1 		 Bn A 

i  1
?
n
P

Bi A   B j  j  i  
In addition to the two additional formulations of
the Pr distribution, we also consider one variant of
the Pw distribution, which includes the suffix anal-
ysis. It is important to clarify that we only change
the handling of uncommon and unknown words;
those which occur often are handled as normal. sug-
gested different choices for Pw in the face of un-
known words: Schiehlen (2004) suggests using a
different unknown word token for capitalized ver-
sus uncapitalized unknown words (German orthog-
raphy dictates that all common nouns are capital-
ized) and Levy and Manning (2004) consider in-
specting the last letter the unknown word to guess
the part-of-speech (POS) tags. Both of these models
are relatively impoverished when compared to the
approaches of handling unknown words which have
been proposed in the POS tagging literature. Brants
(2000) describes a POS tagger with a highly tuned
suffix analyzer which considers both capitalization
and suffixes as long as 10 letters long. This tagger
was developed with German in mind, but neither it
nor any other advanced POS tagger morphology an-
alyzer has ever been tested with a full parser. There-
fore, we take the novel step of integrating this suffix
analyzer into the parser for the second Pw distribu-
tion.
3.2 Treebank Re-annotation
Automatic treebank transformations are an impor-
tant step in developing an accurate unlexicalized
parser (Johnson, 1998; Klein and Manning, 2003).
Most of our transformations focus upon one part of
the NEGRA treebank in particular: the GF labels.
Below is a list of GF re-annotations we utilise:
Coord GF In NEGRA, a co-ordinated accusative
NP rule might look like NP-OA   NP-CJ KON NP-
CJ. KON is the POS tag for a conjunct, and CJ
denotes the function of the NP is a coordinate sis-
ter. Such a rule hides an important fact: the two
co-ordinate sisters are also accusative objects. The
Coord GF re-annotation would therefore replace the
above rule with NP-OA   NP-OA KON NP-OA.
NP case German articles and pronouns are
strongly marked for case. However, the grammati-
cal function of all articles is usually NK, meaning
noun kernel. To allow case markings in articles and
pronouns to ?communicate? with the case labels on
the GFs of NPs, we copy these GFs down into the
POS tags of articles and pronouns. For example,
a rule like NP-OA   ART-NK NN-NK would be
replaced by NP-OA   ART-OA NN-NK. A simi-
lar improvement has been independently noted by
Schiehlen (2004).
PP case Prepositions determine the case of the NP
they govern. While the case is often unambiguous
(i.e. fu?r ?for? always takes an accusative NP), at
times the case may be ambiguous. For instance,
in ?in? may take either an accusative or dative NP.
We use the labels -OA, -OD, etc. for unambiguous
prepositions, and introduce new categories AD (ac-
cusative/dative ambiguous) and DG (dative/genitive
ambiguous) for the ambiguous categories. For ex-
ample, a rule such as PP   P ART-NK NN-NK is
replaced with PP   P-AD ART-AD NN-NK if it is
headed by the preposition in.
SBAR marking German subordinate clauses have
a different word order than main clauses. While sub-
ordinate clauses can usually be distinguished from
main clauses by their GF, there are some GFs which
are used in both cases. This transformation adds
an SBAR category to explicitly disambiguate these
316
No suffix With suffix
F-score F-score
Normal rules 66.3 66.2
LP/ID rules 66.5 66.6
Markov rules 69.4 69.1
Table 1: Effect of rule type and suffix analysis.
cases. The transformation does not add any extra
nonterminals, rather it replaces rules such as S  
KOUS NP V NP (where KOUS is a complementizer
POS tag) with SBAR   KOUS NP V NP.
S GF One may argue that, as far as syntactic dis-
ambiguation is concerned, GFs on S categories pri-
marily serve to distinguish main clauses from sub-
ordinate clauses. As we have explicitly done this
in the previous transformation, it stands to reason
that the GF tags on S nodes may therefore be re-
moved without penalty. If the tags are necessary for
semantic interpretation, presumably they could be
re-inserted using a strategy such as that of Blaheta
and Charniak (2000) The last transformation there-
fore removes the GF of S nodes.
3.3 Method
To allow comparisons with earlier work on NEGRA
parsing, we use the same split of training, develop-
ment and testing data as used in Dubey and Keller
(2003). The first 18,602 sentences are used as train-
ing data, the following 1,000 form the development
set, and the last 1,000 are used as the test set. We re-
move long-distance dependencies from all sets, and
only consider sentences of length 40 or less for ef-
ficiency and memory concerns. The parser is given
untagged words as input to simulate a realistic pars-
ing task. A probabilistic CYK parsing algorithm is
used to compute the Viterbi parse.
We perform two sets of experiments. In the
first set, we vary the rule type, and in the second,
we report the additive results of the treebank re-
annotations described in Section 3.2. The three rule
types used in the first set of experiments are stan-
dard CFG rules, our version of LP/ID rules, and 2nd
order Markov CFG rules. The second battery of ex-
periments was performed on the model with Markov
rules.
In both cases, we report PARSEVAL labeled
No suffix With suffix
F-score F-score
GF Baseline 69.4 69.1
+Coord GF 70.2 71.5
+NP case 71.1 72.4
+PP case 71.0 72.7
+SBAR 70.9 72.6
+S GF 71.3 73.1
Table 2: Effect of re-annotation and suffix analysis
with Markov rules.
bracket scores (Magerman, 1995), with the brackets
labeled by syntactic categories but not grammatical
functions. Rather than reporting precision and recall
of labelled brackets, we report only the F-score, i.e.
the harmonic mean of precision and recall.
3.4 Results
Table 1 shows the effect of rule type choice, and Ta-
ble 2 lists the effect of the GF re-annotations. From
Table 1, we see that Markov rules achieve the best
performance, ahead of both standard rules as well as
our formulation of probabilistic LP/ID rules.
In the first group of experiments, suffix analysis
marginally lowers performance. However, a differ-
ent pattern emerges in the second set of experiments.
Suffix analysis consistently does better than the sim-
pler word generation probability model.
Looking at the treebank transformations with suf-
fix analysis enabled, we find the coordination re-
annotation provides the greatest benefit, boosting
performance by 2.4 to 71.5. The NP and PP case
re-annotations together raise performance by 1.2 to
72.7. While the SBAR annotation slightly lowers
performance, removing the GF labels from S nodes
increased performance to 73.1.
3.5 Discussion
There are two primary results: first, although LP/ID
rules have been suggested as suitable for German?s
flexible word order, it appears that Markov rules ac-
tually perform better. Second, adding suffix analysis
provides a clear benefit, but only after the inclusion
of the Coord GF transformation.
While the SBAR transformation slightly reduces
performance, recall that we argued the S GF trans-
formation only made sense if the SBAR transforma-
317
tion is already in place. To test if this was indeed the
case, we re-ran the final experiment, but excluded
the SBAR transformation. We did indeed find that
applying S GF without the SBAR transformation re-
duced performance.
4 Smoothing & Search
With the exception of DOP models (Bod, 1995), it is
uncommon to smooth unlexicalized grammars. This
is in part for the sake of simplicity: unlexicalized
grammars are interesting because they are simple
to estimate and parse, and adding smoothing makes
both estimation and parsing nearly as complex as
with fully lexicalized models. However, because
lexicalization adds little to the performance of Ger-
man parsing models, it is therefore interesting to in-
vestigate the impact of smoothing on unlexicalized
parsing models for German.
Parsing an unsmoothed unlexicalized grammar is
relatively efficient because the grammar constraints
the search space. As a smoothed grammar does not
have a constrained search space, it is necessary to
find other means to make parsing faster. Although
it is possible to efficiently compute the Viterbi parse
(Klein and Manning, 2002) using a smoothed gram-
mar, the most common approach to increase parsing
speed is to use some form of beam search (cf. Good-
man (1998)), a strategy we follow here.
4.1 Models
We experiment with three different smoothing mod-
els: the modified Witten-Bell algorithm employed
by Collins (1999), the modified Kneser-Ney algo-
rithm of Chen and Goodman (1998) the smooth-
ing algorithm used in the POS tagger of Brants
(2000). All are variants of linear interpolation, and
are used with 2nd order Markovization. Under this
regime, the probability of adding the ith child to
A   B1 		 Bn is estimated as
P

Bi A  Bi  1  Bi  2 
 ?1P

Bi A  Bi  1  Bi  2  
?2P

Bi A  Bi  1   ?3P

Bi A   ?4P

Bi 
The models differ in how the ??s are estimated. For
both the Witten-Bell and Kneser-Ney algorithms,
the ??s are a function of the context A  Bi  2  Bi  1. By
contrast, in Brants? algorithm the ??s are constant
?1  ?2  ?3  0
for each trigram x1  x2  x3 with c  x1  x2  x3  0
d3 
	
c 
 xi  xi  1  xi  2   1
c 
 xi  1  xi  2   1 if c

xi  1  xi  2  1
0 if c

xi  1  xi  2   1
d2 
	
c 
 xi  xi  1   1
c 
 xi  1   1 if c

xi  1  1
0 if c

xi  1   1
d1  c 
 xi   1N  1
if d3  max d1  d2  d3 then
?3  ?3   c

xi  xi  1  xi  2 
elseif d2  max d1  d2  d3 then
?2  ?2   c

xi  xi  1  xi  2 
else
?1  ?1   c

xi  xi  1  xi  2 
end
?1  ?1?1  ?2  ?  3
?2  ?2?1  ?2  ?  3
?3  ?3?1  ?2  ?  3
Figure 1: Smoothing estimation based on the Brants
(2000) approach for POS tagging.
for all possible contexts. As both the Witten-Bell
and Kneser-Ney variants are fairly well known, we
do not describe them further. However, as Brants?
approach (to our knowledge) has not been used else-
where, and because it needs to be modified for our
purposes, we show the version of the algorithm we
use in Figure 1.
4.2 Method
The purpose of this is experiment is not only to im-
prove parsing results, but also to investigate the over-
all effect of smoothing on parse accuracy. Therefore,
we do not simply report results with the best model
from Section 3. Rather, we re-do each modification
in Section 3 with both search strategies (Viterbi and
beam) in the unsmoothed case, and with all three
smoothing algorithms with beam search. The beam
has a variable width, which means an arbitrary num-
ber of edges may be considered, as long as their
probability is within 4  10  3 of the best edge in a
given span.
4.3 Results
Table 3 summarizes the results. The best result in
each column is italicized, and the overall best result
318
No Smoothing No Smoothing Brants Kneser-Ney Witten-Bell
Viterbi Beam Beam Beam Beam
GF Baseline 69.1 70.3 72.3 72.6 72.3
+Coord GF 71.5 72.7 75.2 75.4 74.5
+NP case 72.4 73.3 76.0 76.1 75.6
+PP case 72.7 73.2 76.1 76.2 75.7
+SBAR 72.6 73.1 76.3 76.0 75.3
+S GF Removal 73.1 72.6 75.7 75.3 75.1
Table 3: Effect of various smoothing algorithms.
in shown in bold. The column titled Viterbi repro-
duces the second column of Table 2 whereas the col-
umn titled Beam shows the result of re-annotation
using beam search, but no smoothing. The best re-
sult with beam search is 73.3, slightly higher than
without beam search.
Among smoothing algorithms, the Brants ap-
proach yields the highest results, of 76.3, with the
modified Kneser-Ney algorithm close behind, at
76.2. The modified Witten-Bell algorithm achieved
an F-score of 75.7.
4.4 Discussion
Overall, the best-performing model, using Brants
smoothing, achieves a labelled bracketing F-score
of 76.2, higher than earlier results reported by Dubey
and Keller (2003) and Schiehlen (2004).
It is surprisingly that the Brants algorithm per-
forms favourably compared to the better-known
modified Kneser-Ney algorithm. This might be due
to the heritage of the two algorithms. Kneser-Ney
smoothing was designed for language modelling,
where there are tens of thousands or hundreds of
thousands of tokens having a Zipfian distribution.
With all transformations included, the nonterminals
of our grammar did have a Zipfian marginal distri-
bution, but there were only several hundred tokens.
The Brants algorithm was specifically designed for
distributions with fewer tokens.
Also surprising is the fact that each smoothing al-
gorithm reacted differently to the various treebank
transformations. It is obvious that the choice of
search and smoothing algorithm add bias to the final
result. However, our results indicate that the choice
of search and smoothing algorithm also add a degree
of variance as improvements are added to the parser.
This is worrying: at times in the literature, details
of search or smoothing are left out (e.g. Charniak
(2000)). Given the degree of variance due to search
and smoothing, it raises the question if it is in fact
possible to reproduce such results without the nec-
essary details.2
5 Error Analysis
While it is uncommon to offer an error analysis for
probabilistic parsing, Levy and Manning (2003) ar-
gue that a careful error classification can reveal pos-
sible improvements. Although we leave the imple-
mentation of any improvements to future research,
we do discuss several common errors. Because the
parser with Brants smoothing performed best, we
use that as the basis of our error analysis.
First, we found that POS tagging errors had a
strong effect on parsing results. This is surpris-
ing, given that the parser is able to assign POS tags
with a high degree of accuracy. POS tagging results
are comparable to the best stand-alone POS taggers,
achieving results of 97.1% on the test set, match-
ing the performance of the POS tagger described
by Brants (2000) When GF labels are included (e.g.
considering ART-SB instead of just ART), tagging
accuracy falls to 90.1%. To quantify the effect of
POS tagging errors, we re-parsed with correct POS
tags (rather than letting the parser guess the tags),
and found that labelled bracket F-scores increase
from 76.3 to 85.2. A manual inspection of 100 sen-
tences found that GF mislabelling can accounts for
at most two-thirds of the mistakes due to POS tags.
Over one third was due to genuine POS tagging er-
rors. The most common problem was verb mistag-
ging: they are either confused with adjectives (both
2As an anonymous reviewer pointed out, it is not always
straightforward to reproduce statistical parsing results even
when the implementation details are given (Bikel, 2004).
319
Model LB F-score
This paper 76.3
Dubey and Keller (2003) 74.1
Schiehlen (2004) 71.1
Table 4: Comparison with previous work.
take the common -en suffix), or the tense was incor-
rect. Mistagged verb are a serious problem: it entails
an entire clause is parsed incorrectly. Verb mistag-
ging is also a problem for other languages: Levy and
Manning (2003) describe a similar problem in Chi-
nese for noun/verb ambiguity. This problem might
be alleviated by using a more detailed model of mor-
phology than our suffix analyzer provides.
To investigate pure parsing errors, we manu-
ally examined 100 sentences which were incorrectly
parsed, but which nevertheless were assigned the
correct POS tags. Incorrect modifier attachment ac-
counted for for 39% of all parsing errors (of which
77% are due to PP attachment alone). Misparsed co-
ordination was the second most common problem,
accounting for 15% of all mistakes. Another class
of error appears to be due to Markovization. The
boundaries of VPs are sometimes incorrect, with the
parser attaching dependents directly to the S node
rather than the VP. In the most extreme cases, the
VP had no verb, with the main verb heading a sub-
ordinate clause.
6 Comparison with Previous Work
Table 4 lists the result of the best model presented
here against the earlier work on NEGRA parsing de-
scribed in Dubey and Keller (2003) and Schiehlen
(2004). Dubey and Keller use a variant of the lex-
icalized Collins (1999) model to achieve a labelled
bracketing F-score of 74.1%. Schiehlen presents a
number of unlexicalized models. The best model on
labelled bracketing achieves an F-score of 71.8%.
The work of Schiehlen is particularly interest-
ing as he also considers a number of transforma-
tions to improve the performance of an unlexicalized
parser. Unlike the work presented here, Schiehlen
does not attempt to perform any suffix or morpho-
logical analysis of the input text. However, he does
suggest a number of treebank transformations. One
such transformation is similar to one we prosed here,
the NP case transformation. His implementation is
different from ours: he annotates the case of pro-
nouns and common nouns, whereas we focus on ar-
ticles and pronouns (articles are pronouns are more
strongly marked for case than common nouns). The
remaining transformations we present are different
from those Schiehlen describes; it is possible that an
even better parser may result if all the transforma-
tions were combined.
Schiehlen also makes use of a morphological ana-
lyzer tool. While this includes more complete infor-
mation about German morphology, our suffix analy-
sis model allows us to integrate morphological am-
biguities into the parsing system by means of lexical
generation probabilities.
Levy and Manning (2004) also present work on
the NEGRA treebank, but are primarily interested
in long-distance dependencies, and therefore do not
report results on local dependencies, as we do here.
7 Conclusions
In this paper, we presented the best-performing
parser for German, as measured by labelled bracket
scores. The high performance was due to three fac-
tors: (i) treebank transformations (ii) an integrated
model of morphology in the form of a suffix ana-
lyzer and (iii) the use of smoothing in an unlexical-
ized grammar. Moreover, there are possible paths
for improvement: lexicalization could be added to
the model, as could some of the treebank transfor-
mations suggested by Schiehlen (2004). Indeed, the
suffix analyzer could well be of value in a lexicalized
model.
While we only presented results on the German
NEGRA corpus, there is reason to believe that the
techniques we presented here are also important to
other languages where lexicalization provides lit-
tle benefit: smoothing is a broadly-applicable tech-
nique, and if difficulties with lexicalization are due
to sparse lexical data, then suffix analysis provides
a useful way to get more information from lexical
elements which were unseen while training.
In addition to our primary results, we also pro-
vided a detailed error analysis which shows that
PP attachment and co-ordination are problematic
for our parser. Furthermore, while POS tagging is
highly accurate, the error analysis also shows it does
320
have surprisingly large effect on parsing errors. Be-
cause of the strong impact of POS tagging on pars-
ing results, we conjecture that increasing POS tag-
ging accuracy may be another fruitful area for future
parsing research.
References
Franz Beil, Glenn Carroll, Detlef Prescher, Stefan Rie-
zler, and Mats Rooth. 1999. Inside-Outside Estima-
tion of a Lexicalized PCFG for German. In Proceed-
ings of the 37th Annual Meeting of the Association for
Computational Linguistics, University of Maryland,
College Park.
Daniel M. Bikel. 2004. Intricacies of Collins? Parsing
Model. Computational Linguistics, 30(4).
Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In Proceedings of the 1st
Conference of the North American Chapter of the ACL
(NAACL), Seattle, Washington., pages 234?240.
Rens Bod. 1995. Enriching Linguistics with Statistics:
Performance Models of Natural Language. Ph.D. the-
sis, University of Amsterdam.
Taylor L. Booth. 1969. Probabilistic Representation of
Formal Languages. In Tenth Annual IEEE Symposium
on Switching and Automata Theory, pages 74?81.
Thorsten Brants. 2000. TnT: A statistical part-of-speech
tagger. In Proceedings of the 6th Conference on Ap-
plied Natural Language Processing, Seattle.
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of the 1st Conference of North
American Chapter of the Association for Computa-
tional Linguistics, pages 132?139, Seattle, WA.
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language model-
ing. Technical Report TR-10-98, Center for Research
in Computing Technology, Harvard University.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Amit Dubey and Frank Keller. 2003. Parsing German
with Sister-head Dependencies. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 96?103, Sapporo, Japan.
Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and Ivan
Sag. 1985. Generalized Phase Structure Grammar.
Basil Blackwell, Oxford, England.
Joshua Goodman. 1998. Parsing inside-out. Ph.D. the-
sis, Harvard University.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Dan Klein and Christopher D. Manning. 2002. A* Pars-
ing: Fast Exact Viterbi Parse Selection. Technical Re-
port dbpubs/2002-16, Stanford University.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan.
Roger Levy and Christopher D. Manning. 2003. Is it
Harder to Parse Chinese, or the Chinese Treebank? In
Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics.
Roger Levy and Christopher D. Manning. 2004. Deep
Dependencies from Context-Free Statistical Parsers:
Correcting the Surface Dependency Approximation.
In Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics.
David M. Magerman. 1995. Statistical Decision-Tree
Models for Parsing. In Proceedings of the 33rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 276?283, Cambridge, MA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Micheal Schiehlen. 2004. Annotation Strategies for
Probabilistic Parsing in German. In Proceedings of
the 20th International Conference on Computational
Linguistics.
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and
Hans Uszkoreit. 1997. An annotation scheme for
free word order languages. In Proceedings of the 5th
Conference on Applied Natural Language Processing,
Washington, DC.
Hans Uszkoreit. 1987. Word Order and Constituent
Structure in German. CSLI Publications, Stanford,
CA.
321
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 417?424,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Integrating Syntactic Priming into an Incremental Probabilistic Parser,
with an Application to Psycholinguistic Modeling
Amit Dubey and Frank Keller and Patrick Sturt
Human Communication Research Centre, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, UK
{amit.dubey,patrick.sturt,frank.keller}@ed.ac.uk
Abstract
The psycholinguistic literature provides
evidence for syntactic priming, i.e., the
tendency to repeat structures. This pa-
per describes a method for incorporating
priming into an incremental probabilis-
tic parser. Three models are compared,
which involve priming of rules between
sentences, within sentences, and within
coordinate structures. These models sim-
ulate the reading time advantage for par-
allel structures found in human data, and
also yield a small increase in overall pars-
ing accuracy.
1 Introduction
Over the last two decades, the psycholinguistic
literature has provided a wealth of experimental
evidence for syntactic priming, i.e., the tendency
to repeat syntactic structures (e.g., Bock, 1986).
Most work on syntactic priming has been con-
cerned with sentence production; however, recent
studies also demonstrate a preference for struc-
tural repetition in human parsing. This includes
the so-called parallelism effect demonstrated by
Frazier et al (2000): speakers processes coordi-
nated structures more quickly when the second
conjunct repeats the syntactic structure of the first
conjunct.
Two alternative accounts of the parallelism ef-
fect have been proposed. Dubey et al (2005) ar-
gue that the effect is simply an instance of a perva-
sive syntactic priming mechanism in human pars-
ing. They provide evidence from a series of cor-
pus studies which show that parallelism is not lim-
ited to co-ordination, but occurs in a wide range
of syntactic structures, both within and between
sentences, as predicted if a general priming mech-
anism is assumed. (They also show this effect is
stronger in coordinate structures, which could ex-
plain Frazier et al?s (2000) results.)
Frazier and Clifton (2001) propose an alterna-
tive account of the parallelism effect in terms of a
copying mechanism. Unlike priming, this mecha-
nism is highly specialized and only applies to co-
ordinate structures: if the second conjunct is en-
countered, then instead of building new structure,
the language processor simply copies the structure
of the first conjunct; this explains why a speed-
up is observed if the two conjuncts are parallel. If
the copying account is correct, then we would ex-
pect parallelism effects to be restricted to coordi-
nate structures and not to apply in other contexts.
This paper presents a parsing model which im-
plements both the priming mechanism and the
copying mechanism, making it possible to com-
pare their predictions on human reading time data.
Our model also simulates other important aspects
of human parsing: (i) it is broad-coverage, i.e.,
it yields accurate parses for unrestricted input,
and (ii) it processes sentences incrementally, i.e.,
on a word-by-word basis. This general modeling
framework builds on probabilistic accounts of hu-
man parsing as proposed by Jurafsky (1996) and
Crocker and Brants (2000).
A priming-based parser is also interesting from
an engineering point of view. To avoid sparse
data problems, probabilistic parsing models make
strong independence assumptions; in particular,
they generally assume that sentences are indepen-
dent of each other, in spite of corpus evidence for
structural repetition between sentences. We there-
fore expect a parsing model that includes struc-
tural repetition to provide a better fit with real cor-
pus data, resulting in better parsing performance.
A simple and principled approach to handling
structure re-use would be to use adaptation prob-
abilities for probabilistic grammar rules (Church,
2000), analogous to cache probabilities used in
caching language models (Kuhn and de Mori,
1990). This is the approach we will pursue in this
paper.
Dubey et al (2005) present a corpus study that
demonstrates the existence of parallelism in cor-
pus data. This is an important precondition for un-
derstanding the parallelism effect; however, they
417
do not develop a parsing model that accounts for
the effect, which means they are unable to evaluate
their claims against experimental data. The present
paper overcomes this limitation. In Section 2, we
present a formalization of the priming and copy-
ing models of parallelism and integrate them into
an incremental probabilistic parser. In Section 3,
we evaluate this parser against reading time data
taken from Frazier et al?s (2000) parallelism ex-
periments. In Section 4, we test the engineering
aspects of our model by demonstrating that a small
increase in parsing accuracy can be obtained with
a parallelism-based model. Section 5 provides an
analysis of the performance of our model, focus-
ing on the role of the distance between prime and
target.
2 Priming Models
We propose three models designed to capture the
different theories of structural repetition discussed
above. To keep our model as simple as possi-
ble, each formulation is based on an unlexicalized
probabilistic context free grammar (PCFG). In this
section, we introduce the models and discuss the
novel techniques used to model structural similar-
ity. We also discuss the design of the probabilistic
parser used to evaluate the models.
2.1 Baseline Model
The unmodified PCFG model serves as the Base-
line. A PCFG assigns trees probabilities by treat-
ing each rule expansion as conditionally indepen-
dent given the parent node. The probability of a
rule LHS ? RHS is estimated as:
P(RHS|LHS) = c(LHS ? RHS)
c(LHS)
2.2 Copy Model
The first model we introduce is a probabilistic
variant of Frazier and Clifton?s (2001) copying
mechanism: it models parallelism in coordination
and nothing else. This is achieved by assuming
that the default operation upon observing a coordi-
nator (assumed to be anything with a CC tag, e.g.,
?and?) is to copy the full subtree of the preced-
ing coordinate sister. Copying impacts on how the
parser works (see Section 2.5), and in a probabilis-
tic setting, it also changes the probability of trees
with parallel coordinated structures. If coordina-
tion is present, the structure of the second item is
either identical to the first, or it is not.1 Let us call
1The model only considers two-item coordination or thelast two sisters of multiple-item coordination.
the probability of having a copied tree as pident.This value may be estimated directly from a cor-
pus using the formula
p?ident =
cident
ctotal
Here, cident is the number of coordinate structuresin which the two conjuncts have the same internal
structure and ctotal is the total number of coordi-nate structures. Note we assume there is only one
parameter pident applicable everywhere (i.e., it hasthe same value for all rules).
How is this used in a PCFG parser? Let t1 and t2represent, respectively, the first and second coor-
dinate sisters and let PPCFG(t) be the PCFG prob-ability of an arbitrary subtree t.
Because of the independence assumptions of
the PCFG, we know that pident ? PPCFG(t). Oneway to proceed would be to assign a probability
of pident when structures match, and (1? pident) ?
PPCFG(t2) when structures do not match. However,some probability mass is lost this way: there is
a nonzero PCFG probability (namely, PPCFG(t1))that the structures match.
In other words, we may have identical subtrees
in two different ways: either due to a copy oper-
ation, or due to a PCFG derivation. If pcopy is theprobability of a copy operation, we can write this
fact more formally as: pident = PPCFG(t1)+ pcopy.Thus, if the structures do match, we assign the
second sister a probability of:
pcopy +PPCFG(t1)
If they do not match, we assign the second con-
junct the following probability:
1?PPCFG(t1)? pcopy
1?PPCFG(t1) ?PPCFG(t2)
This accounts for both a copy mismatch and a
PCFG derivation mismatch, and assures the prob-
abilities still sum to one. These probabilities for
parallel and non-parallel coordinate sisters, there-
fore, gives us the basis of the Copy model.
This leaves us with the problem of finding an
estimate for pcopy. This value is approximated as:
p?copy = p?ident ?
1
|T2| ?t?T2 PPCFG(t)
In this equation, T2 is the set of all second con-juncts.
2.3 Between Model
While the Copy model limits itself to parallelism
in coordination, the next two models simulate
structural priming in general. Both are similar in
design, and are based on a simple insight: we may
418
condition a PCFG rule expansion on whether the
rule occurred in some previous context. If Prime is
a binary-valued random variable denoting if a rule
occurred in the context, then we define:
P(RHS|LHS,Prime) = c(LHS ? RHS,Prime)
c(LHS,Prime)
This is essentially an instantiation of Church?s
(2000) adaptation probability, albeit with PCFG
rules instead of words. For our first model, this
context is the previous sentence. Thus, the model
can be said to capture the degree to which rule use
is primed between sentences. We henceforth refer
to this as the Between model. Following the con-
vention in the psycholinguistic literature, we refer
to a rule use in the previous sentence as a ?prime?,
and a rule use in the current sentence as the ?tar-
get?. Each rule acts once as a target (i.e., the event
of interest) and once as a prime. We may classify
such adapted probabilities into ?positive adapta-
tion?, i.e., the probability of a rule given the rule
occurred in the preceding sentence, and ?negative
adaptation?, i.e., the probability of a rule given that
the rule did not occur in the preceding sentence.
2.4 Within Model
Just as the Between model conditions on rules
from the previous sentence, the Within sentence
model conditions on rules from earlier in the cur-
rent sentence. Each rule acts once as a target, and
possibly several times as a prime (for each subse-
quent rule in the sentence). A rule is considered
?used? once the parser passes the word on the left-
most corner of the rule. Because the Within model
is finer grained than the Between model, it can be
used to capture the parallelism effect in coordina-
tion. In other words, this model could explain par-
allelism in coordination as an instance of a more
general priming effect.
2.5 Parser
As our main purpose is to build a psycholinguistic
model of structure repetition, the most important
feature of the parsing model is to build structures
incrementally.2
Reading time experiments, including the paral-
lelism studies of Frazier et al (2000), make word-
by-word measurements of the time taken to read
2In addition to incremental parsing, a characteristic someof psycholinguistic models of sentence comprehension is toparse deterministically. While we can compute the best in-cremental analysis at any point, ours models do not parse de-terministically. However, following the principles of rationalanalysis (Anderson, 1991), our goal is not to mimic the hu-man parsing mechanism, but rather to create a model of hu-man parsing behavior.
a novel and a bookwrote
0 3
Terry
4 5 61 2 7
NP NP
NP
a novel and a bookTerry wrote
0 31 4 5 62 7
NP
NP NP
Figure 1: Upon encountering a coordinator, the
copy model copies the most likely first conjunct.
sentences. Slower reading times are known to be
correlated with processing difficulty, and faster
reading times (as is the case with parallel struc-
tures) are correlated with processing ease. A prob-
abilistic parser may be considered to be a sen-
tence processing model via a ?linking hypothesis?,
which links the parser?s word-by-word behavior to
human reading behavior. We discuss this topic in
more detail in Section 3. At this point, it suffices
to say that we require a parser which has the pre-
fix property, i.e., which parses incrementally, from
left to right.
Therefore, we use an Earley-style probabilis-
tic parser, which outputs Viterbi parses (Stolcke,
1995). We have two versions of the parser: one
which parses exhaustively, and a second which
uses a variable width beam, pruning any edges
whose merit is 12000 of the best edge. The meritof an edge is its inside probability times a prior
P(LHS) times a lookahead probability (Roark and
Johnson, 1999). To speed up parsing time, we right
binarize the grammar,3 remove empty nodes, coin-
dexation and grammatical functions. As our goal
is to create the simplest possible model which can
nonetheless model experimental data, we do not
make any tree modification designed to improve
accuracy (as, e.g., Klein and Manning 2003).
The approach used to implement the Copy
model is to have the parser copy the subtree of the
first conjunct whenever it comes across a CC tag.
Before copying, though, the parser looks ahead to
check if the part-of-speech tags after the CC are
equivalent to those inside the first conjunct. The
copying model is visualized in Figure 1: the top
panel depicts a partially completed edge upon see-
ing a CC tag, and the second panel shows the com-
pleted copying operation. It should be clear that
3We found that using an unbinarized grammar did not al-ter the results, at least in the exhaustive parsing case.
419
the copy operation gives the most probable sub-
tree in a given span. To illustrate this, consider Fig-
ure 1. If the most likely NP between spans 2 and 7
does not involve copying (i.e. only standard PCFG
rule derivations), the parser will find it using nor-
mal rule derivations. If it does involve copying, for
this particular rule, it must involve the most likely
NP subtree from spans 2 to 3. As we parse in-
crementally, we are guaranteed to have found this
edge, and can use it to construct the copied con-
junct over spans 5 to 7 and therefore the whole
co-ordinated NP from spans 2 to 7.
To simplify the implementation of the copying
operation, we turn off right binarization so that the
constituent before and after a coordinator are part
of the same rule, and therefore accessible from the
same edge. This makes it simple to calculate the
new probability: construct the copied subtree, and
decide where to place the resulting edge on the
chart.
The Between and Within models require a cache
of recently used rules. This raises two dilem-
mas. First, in the Within model, keeping track of
full contextual history is incompatible with chart
parsing. Second, whenever a parsing error occurs,
the accuracy of the contextual history is compro-
mised. As we are using a simple unlexicalized
parser, such parsing errors are probably quite fre-
quent.
We handle the first problem by using one sin-
gle parse as an approximation of the history. The
more realistic choice for this single parse is the
best parse so far according to the parser. Indeed,
this is the approach we use for our main results in
Section 3. However, because of the second prob-
lem noted above, in Section 4, we simulated the
context by filling the cache with rules from the
correct tree. In the Between model, these are the
rules of the correct parse of the previous tree; in
the Within model, these are the rules used in the
correct parse at points up to (but not including) the
current word.
3 Human Reading Time Experiment
In this section, we test our models by applying
them to experimental reading time data. Frazier
et al (2000) reported a series of experiments that
examined the parallelism preference in reading. In
one of their experiments, they monitored subjects?
eye-movements while they read sentences like (1):
(1) a. Hilda noticed a strange man and a tall
woman when she entered the house.
b. Hilda noticed a man and a tall woman
when she entered the house.
They found that total reading times were faster on
the phrase tall woman in (1a), where the coordi-
nated noun phrases are parallel in structure, com-
pared with in (1b), where they are not.
There are various approaches to modeling pro-
cessing difficulty using a probabilistic approach.
One possibility is to use an incremental parser
with a beam search or an n-best approach. Pro-
cessing difficulty is predicted at points in the input
string where the current best parse is replaced by
an alternative derivation (Jurafsky, 1996; Crocker
and Brants, 2000). An alternative is to keep track
of all derivations, and predict difficulty at points
where there is a large change in the shape of
the probability distribution across adjacent pars-
ing states (Hale, 2001). A third approach is to
calculate the forward probability (Stolcke, 1995)
of the sentence using a PCFG. Low probabilities
are then predicted to correspond to high process-
ing difficulty. A variant of this third approach is
to assume that processing difficulty is correlated
with the (log) probability of the best parse (Keller,
2003). This final formulation is the one used for
the experiments presented in this paper.
3.1 Method
The item set was adapted from that of Frazier et al
(2000). The original two relevant conditions of
their experiment (1a,b) differ in terms of length.
This results in a confound in the PCFG frame-
work, because longer sentences tend to result in
lower probabilities (as the parses tend to involve
more rules). To control for such length differences,
we adapted the materials by adding two extra con-
ditions in which the relation between syntactic
parallelism and length was reversed. This resulted
in the following four conditions:
(2) a. DT JJ NN and DT JJ NN (parallel)
Hilda noticed a tall man and a strange
woman when she entered the house.
b. DT NN and DT JJ NN (non-parallel)
Hilda noticed a man and a strange
woman when she entered the house.
c. DT JJ NN and DT NN (non-parallel)
Hilda noticed a tall man and a woman
when she entered the house.
d. DT NN and DT NN (parallel)
Hilda noticed a man and a woman when
she entered the house.
420
In order to account for Frazier et al?s paral-
lelism effect a probabilistic model should pre-
dict a greater difference in probability be-
tween (2a) and (2b) than between (2c) and (2d)
(i.e., (2a)?(2b) > (2c)?(2d)). This effect will not
be confounded with length, because the relation
between length and parallelism is reversed be-
tween (2a,b) and (2c,d). We added 8 items to the
original Frazier et al materials, resulting in a new
set of 24 items similar to (2).
We tested three of our PCFG-based models on
all 24 sets of 4 conditions. The models were the
Baseline, the Within and the Copy models, trained
exactly as described above. The Between model
was not tested as the experimental stimuli were
presented without context. Each experimental sen-
tence was input as a sequence of correct POS tags,
and the log probability estimate of the best parse
was recorded.
3.2 Results and Discussion
Table 1 shows the mean log probabilities estimated
by the models for the four conditions, along with
the relevant differences between parallel and non-
parallel conditions.
Both the Within and the Copy models show a
parallelism advantage, with this effect being much
more pronounced for the Copy model than the
Within model. To evaluate statistical significance,
the two differences for each item were compared
using a Wilcoxon signed ranks test. Significant
results were obtained both for the Within model
(N = 24, Z = 1.67, p < .05, one-tailed) and for
the Copy model (N = 24, Z = 4.27, p < .001, one-
tailed). However, the effect was much larger for
the Copy model, a conclusion which is confirmed
by comparing the differences of differences be-
tween the two models (N = 24, Z = 4.27, p < .001,
one-tailed). The Baseline model was not evalu-
ated statistically, because by definition it predicts a
constant value for (2a)?(2b) and (2c)?(2d) across
all items. This is simply a consequence of the
PCFG independence assumption, coupled with the
fact that the four conditions of each experimen-
tal item differ only in the occurrences of two NP
rules.
The results show that the approach taken here
can be successfully applied to the modeling of
experimental data. In particular, both the Within
and the Copy models show statistically reliable
parallelism effects. It is not surprising that the
copy model shows a large parallelism effect for
the Frazier et al (2000) items, as it was explicitly
designed to prefer structurally parallel conjuncts.
The more interesting result is the parallelism ef-
fect found for the Within model, which shows that
such an effect can arise from a more general prob-
abilistic priming mechanism.
4 Parsing Experiment
In the previous section, we were able to show that
the Copy and Within models are able to account
for human reading-time performance for parallel
coordinate structures. While this result alone is
sufficient to claim success as a psycholinguistic
model, it has been argued that more realistic psy-
cholinguistic models ought to also exhibit high ac-
curacy and broad-coverage, both crucial properties
of the human parsing mechanism (e.g., Crocker
and Brants, 2000).
This should not be difficult: our starting point
was a PCFG, which already has broad coverage
behavior (albeit with only moderate accuracy).
However, in this section we explore what effects
our modifications have to overall coverage, and,
perhaps more interestingly, to parsing accuracy.
4.1 Method
The models used here were the ones introduced
in Section 2 (which also contains a detailed de-
scription of the parser that we used to apply the
models). The corpus used for both training and
evaluation is the Wall Street Journal part of the
Penn Treebank. We use sections 1?22 for train-
ing, section 0 for development and section 23 for
testing. Because the Copy model posits coordi-
nated structures whenever POS tags match, pars-
ing efficiency decreases if POS tags are not pre-
determined. Therefore, we assume POS tags as in-
put, using the gold-standard tags from the treebank
(following, e.g., Roark and Johnson 1999).
4.2 Results and Discussion
Table 2 lists the results in terms of F-score on
the test set.4 Using exhaustive search, the base-
line model achieves an F-score of 73.3, which is
comparable to results reported for unlexicalized
incremental parsers in the literature (e.g. the RB1
model of Roark and Johnson, 1999). All models
exhibit a small decline in performance when beam
search is used. For the Within model we observe a
slight improvement in performance over the base-
line, both for the exhaustive search and the beam
4Based on a ?2 test on precision and recall, all results arestatistically different from each other. The Copy model actu-ally performs slightly better than the Baseline in the exhaus-tive case.
421
Model para: (2a) non-para: (2b) non-para: (2c) para: (2d) (2a)?(2b) (2c)?(2d)
Baseline ?33.47 ?32.37 ?32.37 ?31.27 ?1.10 ?1.10
Within ?33.28 ?31.67 ?31.70 ?29.92 ?1.61 ?1.78
Copy ?16.18 ?27.22 ?26.91 ?15.87 11.04 ?11.04
Table 1: Mean log probability estimates for Frazier et al(2000) items
Exhaustive Search Beam Search Beam + Coord Fixed Coverage
Model F-score Coverage F-score Coverage F-score Coverage F-score Coverage
Baseline 73.3 100 73.0 98.0 73.1 98.1 73.0 97.5
Within 73.6 100 73.4 98.4 73.0 98.5 73.4 97.5
Between 71.6 100 71.7 98.7 71.5 99.0 71.8 97.5
Copy 73.3 100 ? ? 73.0 98.1 73.1 97.5
Table 2: Parsing results for the Within, Between, and Copy model compared to a PCFG baseline.
search conditions. The Between model, however,
resulted in a decrease in performance.
We also find that the Copy model performs at
the baseline level. Recall that in order to simplify
the implementation of the copying, we had to dis-
able binarization for coordinate constituents. This
means that quaternary rules were used for coordi-
nation (X ? X1 CC X2 X ?), while normal binaryrules (X ? Y X ?) were used everywhere else. It
is conceivable that this difference in binarization
explains the difference in performance between
the Between and Within models and the Copy
model when beam search was used. We there-
fore also state the performance for Between and
Within models with binarization limited to non-
coordinate structures in the column labeled ?Beam
+ Coord? in Table 2. The pattern of results, how-
ever, remains the same.
The fact that coverage differs between models
poses a problem in that it makes it difficult to
compare the F-scores directly. We therefore com-
pute separate F-scores for just those sentences that
were covered by all four models. The results are
reported in the ?Fixed Coverage? column of Ta-
ble 2. Again, we observe that the copy model per-
forms at baseline level, while the Within model
slightly outperforms the baseline, and the Between
model performs worse than the baseline. In Sec-
tion 5 below we will present an error analysis that
tries to investigate why the adaptation models do
not perform as well as expected.
Overall, we find that the modifications we intro-
duced to model the parallelism effect in humans
have a positive, but small, effect on parsing ac-
curacy. Nonetheless, the results also indicate the
success of both the Copy and Within approaches
to parallelism as psycholinguistic models: a mod-
ification primarily useful for modeling human be-
havior has no negative effects on computational
measures of coverage or accuracy.
5 Distance Between Rule Uses
Although both the Within and Copy models suc-
ceed at the main task of modeling the paral-
lelism effect, the parsing experiments in Section 4
showed mixed results with respect to F-scores:
a slight increase in F-score was observed for the
Within model, but the Between model performed
below the baseline. We therefore turn to an error
analysis, focusing on these two models.
Recall that the Within and Between models es-
timate two probabilities for a rule, which we have
been calling the positive adaptation (the probabil-
ity of a rule when the rule is also in the history),
and the negative adaptation (the probability of a
rule when the rule is not in the history). While
the effect is not always strong, we expect positive
adaptation to be higher than negative adaptation
(Dubey et al, 2005). However, this is not always
the case.
In the Within model, for example, the rule
NP ? DT JJ NN has a higher negative than posi-
tive adaptation (we will refer to such rules as ?neg-
atively adapted?). The more common rule NP ?
DT NN has a higher positive adaptation (?pos-
itively adapted?). Since the latter is three times
more common, this raises a concern: what if adap-
tation is an artifact of frequency? This ?frequency?
hypothesis posits that a rule recurring in a sentence
is simply an artifact of the its higher frequency.
The frequency hypothesis could explain an inter-
esting fact: while the majority of rules tokens have
positive adaptation, the majority of rule types have
negative adaptation. An important corollary of the
frequency hypothesis is that we would not expect
to find a bias towards local rule re-uses.
422
Iterate through the treebank
Remember how many words each constituent spans
Iterate through the treebank
Iterate through each tree
Upon finding a constituent spanning 1-4 words
Swap it with a randomly chosen constituent
of 1-4 words
Update the remembered size of the swapped
constituents and their subtrees
Iterate through the treebank 4 more times
Swap constituents of size 5-9, 10-19, 20-35
and 35+ words, respectively
Figure 2: The treebank randomization algorithm
Nevertheless, the NP ? DT JJ NN rule is
an exception: most negatively adapted rules have
very low frequencies. This raises the possibility
that sparse data is the cause of the negatively
adapted rules. This makes intuitive sense: we need
many rule occurrences to accurately estimate pos-
itive or negative adaptation.
We measure the distribution of rule use to ex-
plore if negatively adapted rules owe more to fre-
quency effects or to sparse data. This distributional
analysis also serves to measure ?decay? effects in
structural repetition. The decay effect in priming
has been observed elsewhere (Szmrecsanyi, 2005),
and suggests that positive adaptation is higher the
closer together two rules are.
5.1 Method
We investigate the dispersion of rules by plot-
ting histograms of the distance between subse-
quent rule uses. The basic premise is to look for
evidence of an early peak or skew, which sug-
gests rule re-use. To ensure that the histogram it-
self is not sensitive to sparse data problems, we
group all rules into two categories: those which are
positively adapted, and those which are negatively
adapted.
If adaptation is not due to frequency alone, we
would expect the histograms for both positively
and negatively adapted rules to be skewed towards
local rule repetition. Detecting a skew requires a
baseline without repetition. We propose the con-
cept of ?randomizing? the treebank to create such
a baseline. The randomization algorithm is de-
scribed in Figure 2. The algorithm entails swap-
ping subtrees, taking care that small subtrees are
swapped first (otherwise large chunks would be
swapped at once, preserving a great deal of con-
text). This removes local effects, giving a distribu-
tion due frequency alone.
After applying the randomization algorithm to
the treebank, we may construct the distance his-
0 5 10Logarithm of Word Distance
0
0.005
0.01
0.015
0.02
No
rm
aliz
ed 
Fre
que
ncy
 of 
Ru
le O
ccu
ran
ce
+ Adapt, Untouched Corpus
+ Adapt, Randomized Corpus
- Adapt, Untouched Corpus
- Adapt, Randomized Corpus
Figure 3: Log of number of words between rule
invocations
togram for both the non-randomized and random-
ized treebanks. The distance between two occur-
rences of a rule is calculated as the number of
words between the first word on the left corner of
each rule. A special case occurs if a rule expansion
invokes another use of the same rule. When this
happens, we do not count the distance between the
first and second expansion. However, the second
expansion is still remembered as the most recent.
We group rules into those that have a higher
positive adaptation and those that have a higher
negative adaptation. We then plot a histogram of
rule re-occurrence distance for both groups, in
both the non-randomized and randomized corpora.
5.2 Results and Discussion
The resulting plot for the Within model is shown
in Figure 3. For both the positive and negatively
adapted rules, we find that randomization results
in a lower, less skewed peak, and a longer tail.
We conclude that rules tend to be repeated close
to one another more than we expect by chance,
even for negatively adapted rules. This is evidence
against the frequency hypothesis, and in favor of
the sparse data hypothesis. This means that the
small size of the increase in F-score we found in
Section 4 is not due to the fact that the adaption
is just an artifact of rule frequency. Rather, it can
probably be attributed to data sparseness.
Note also that the shape of the histogram pro-
vides a decay curve. Speculatively, we suggest that
this shape could be used to parameterize the decay
effect and therefore provide an estimate for adap-
tation which is more robust to sparse data. How-
ever, we leave the development of such a smooth-
ing function to future research.
423
6 Conclusions and Future Work
The main contribution of this paper has been to
show that an incremental parser can simulate syn-
tactic priming effects in human parsing by incor-
porating probability models that take account of
previous rule use. Frazier et al (2000) argued that
the best account of their observed parallelism ad-
vantage was a model in which structure is copied
from one coordinate sister to another. Here, we ex-
plored a probabilistic variant of the copy mecha-
nism, along with two more general models based
on within- and between-sentence priming. Al-
though the copy mechanism provided the strongest
parallelism effect in simulating the human reading
time data, the effect was also successfully simu-
lated by a general within-sentence priming model.
On the basis of simplicity, we therefore argue that
it is preferable to assume a simpler and more gen-
eral mechanism, and that the copy mechanism is
not needed. This conclusion is strengthened when
we turn to consider the performance of the parser
on the standard Penn Treebank test set: the Within
model showed a small increase in F-score over the
PCFG baseline, while the copy model showed no
such advantage.5
All the models we proposed offer a broad-
coverage account of human parsing, not just a lim-
ited model on a hand-selected set of examples,
such as the models proposed by Jurafsky (1996)
and Hale (2001) (but see Crocker and Brants
2000).
A further contribution of the present paper has
been to develop a methodology for analyzing the
(re-)use of syntactic rules over time in a corpus. In
particular, we have defined an algorithm for ran-
domizing the constituents of a treebank, yielding
a baseline estimate of chance repetition.
In the research reported in this paper, we have
adopted a very simple model based on an unlex-
icalized PCFG. In the future, we intend to ex-
plore the consequences of introducing lexicaliza-
tion into the parser. This is particularly interest-
ing from the point of view of psycholinguistic
modeling, because there are well known inter-
actions between lexical repetition and syntactic
priming, which require lexicalization for a proper
treatment. Future work will also involve the use
of smoothing to increase the benefit of priming
for parsing accuracy. The investigations reported
5The broad-coverage parsing experiment speaks againsta ?facilitation? hypothesis, i.e., that the copying and prim-ing mechanisms work together. However, a full test of this(e.g., by combining the two models) is left to future research.
in Section 5 provide a basis for estimating the
smoothing parameters.
References
Anderson, John. 1991. Cognitive architectures in a ratio-nal analysis. In K. VanLehn, editor, Architectures for In-
telligence, Lawrence Erlbaum Associates, Hillsdale, N.J.,pages 1?24.
Bock, J. Kathryn. 1986. Syntactic persistence in languageproduction. Cognitive Psychology 18:355?387.
Church, Kenneth W. 2000. Empirical estimates of adapta-
tion: the chance of two Noriegas is closer to p/2 than p2.In Proceedings of the 17th Conference on Computational
Linguistics. Saarbru?cken, Germany, pages 180?186.
Crocker, Matthew W. and Thorsten Brants. 2000. Wide-coverage probabilistic sentence processing. Journal of
Psycholinguistic Research 29(6):647?669.
Dubey, Amit, Patrick Sturt, and Frank Keller. 2005. Paral-lelism in coordination as an instance of syntactic priming:Evidence from corpus-based modeling. In Proceedings
of the Human Language Technology Conference and the
Conference on Empirical Methods in Natural Language
Processing. Vancouver, pages 827?834.
Frazier, Lyn, Alan Munn, and Chuck Clifton. 2000. Process-ing coordinate structures. Journal of Psycholinguistic Re-
search 29(4):343?370.
Frazier, Lynn and Charles Clifton. 2001. Parsing coordinatesand ellipsis: Copy ?. Syntax 4(1):1?22.
Hale, John. 2001. A probabilistic Earley parser as a psy-cholinguistic model. In Proceedings of the 2nd Confer-
ence of the North American Chapter of the Association
for Computational Linguistics. Pittsburgh, PA.
Jurafsky, Daniel. 1996. A probabilistic model of lexical andsyntactic access and disambiguation. Cognitive Science20(2):137?194.
Keller, Frank. 2003. A probabilistic parser as a model ofglobal processing difficulty. In R. Alterman and D. Kirsh,editors, Proceedings of the 25th Annual Conference of the
Cognitive Science Society. Boston, pages 646?651.
Klein, Dan and Christopher D. Manning. 2003. Accurate Un-lexicalized Parsing. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics.Sapporo, Japan, pages 423?430.
Kuhn, Roland and Renate de Mori. 1990. A cache-based nat-ural language model for speech recognition. IEEE Tran-
sanctions on Pattern Analysis and Machine Intelligence12(6):570?583.
Roark, Brian and Mark Johnson. 1999. Efficient probabilistictop-down and left-corner parsing. In Proceedings of the
37th Annual Meeting of the Association for Computational
Linguistics. pages 421?428.
Stolcke, Andreas. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.
Computational Linguistics 21(2):165?201.
Szmrecsanyi, Benedikt. 2005. Creatures of habit: A corpus-linguistic analysis of persistence in spoken English. Cor-
pus Linguistics and Linguistic Theory 1(1):113?149.
424
Probabilistic Parsing for German using Sister-Head Dependencies
Amit Dubey
Department of Computational Linguistics
Saarland University
PO Box 15 11 50
66041 Saarbru?cken, Germany
adubey@coli.uni-sb.de
Frank Keller
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
keller@inf.ed.ac.uk
Abstract
We present a probabilistic parsing model
for German trained on the Negra tree-
bank. We observe that existing lexicalized
parsing models using head-head depen-
dencies, while successful for English, fail
to outperform an unlexicalized baseline
model for German. Learning curves show
that this effect is not due to lack of training
data. We propose an alternative model that
uses sister-head dependencies instead of
head-head dependencies. This model out-
performs the baseline, achieving a labeled
precision and recall of up to 74%. This in-
dicates that sister-head dependencies are
more appropriate for treebanks with very
flat structures such as Negra.
1 Introduction
Treebank-based probabilistic parsing has been the
subject of intensive research over the past few years,
resulting in parsing models that achieve both broad
coverage and high parsing accuracy (e.g., Collins
1997; Charniak 2000). However, most of the ex-
isting models have been developed for English and
trained on the Penn Treebank (Marcus et al, 1993),
which raises the question whether these models
generalize to other languages, and to annotation
schemes that differ from the Penn Treebank markup.
The present paper addresses this question by
proposing a probabilistic parsing model trained on
Negra (Skut et al, 1997), a syntactically annotated
corpus for German. German has a number of syn-
tactic properties that set it apart from English, and
the Negra annotation scheme differs in important re-
spects from the Penn Treebank markup. While Ne-
gra has been used to build probabilistic chunkers
(Becker and Frank, 2002; Skut and Brants, 1998),
the research reported in this paper is the first attempt
to develop a probabilistic full parsing model for Ger-
man trained on a treebank (to our knowledge).
Lexicalization can increase parsing performance
dramatically for English (Carroll and Rooth, 1998;
Charniak, 1997, 2000; Collins, 1997), and the lexi-
calized model proposed by Collins (1997) has been
successfully applied to Czech (Collins et al, 1999)
and Chinese (Bikel and Chiang, 2000). However, the
resulting performance is significantly lower than the
performance of the same model for English (see Ta-
ble 1). Neither Collins et al (1999) nor Bikel and
Chiang (2000) compare the lexicalized model to an
unlexicalized baseline model, leaving open the pos-
sibility that lexicalization is useful for English, but
not for other languages.
This paper is structured as follows. Section 2 re-
views the syntactic properties of German, focusing
on its semi-flexible wordorder. Section 3 describes
two standard lexicalized models (Carroll and Rooth,
1998; Collins, 1997), as well as an unlexicalized
baseline model. Section 4 presents a series of experi-
ments that compare the parsing performance of these
three models (and several variants) on Negra. The
results show that both lexicalized models fail to out-
perform the unlexicalized baseline. This is at odds
with what has been reported for English. Learning
curves show that the poor performance of the lexi-
calized models is not due to lack of training data.
Section 5 presents an error analysis for Collins?s
(1997) lexicalized model, which shows that the
head-head dependencies used in this model fail to
cope well with the flat structures in Negra. We pro-
pose an alternative model that uses sister-head de-
pendencies instead. This model outperforms the two
original lexicalized models, as well as the unlexical-
ized baseline. Based on this result and on the review
of the previous literature (Section 6), we argue (Sec-
tion 7) that sister-head models are more appropriate
for treebanks with very flat structures (such as Ne-
gra), typically used to annotate languages with semi-
free wordorder (such as German).
2 Parsing German
2.1 Syntactic Properties
German exhibits a number of syntactic properties
that distinguish it from English, the language that
has been the focus of most research in parsing.
Prominent among these properties is the semi-free
Language Size LR LP Source
English 40,000 87.4% 88.1% (Collins, 1997)
Chinese 3,484 69.0% 74.8% (Bikel and Chiang, 2000)
Czech 19,000 ?- 80.0% ?- (Collins et al, 1999)
Table 1: Results for the Collins (1997) model for
various languages (dependency precision for Czech)
wordorder, i.e., German wordorder is fixed in some
respects, but variable in others. Verb order is largely
fixed: in subordinate clauses such as (1a), both the
finite verb hat ?has? and the non-finite verb kom-
poniert ?composed? are in sentence final position.
(1) a. Weil
because
er
er
gestern
yesterday
Musik
music
komponiert
composed
hat.
has
?Because he has composed music yesterday.?
b. Hat er gestern Musik komponiert?
c. Er hat gestern Musik komponiert.
In yes/no questions such as (1b), the finite verb is
sentence initial, while the non-finite verb is sen-
tence final. In declarative main clauses (see (1c)), on
the other hand, the finite verb is in second position
(i.e., preceded by exactly one constituent), while the
non-finite verb is final.
While verb order is fixed in German, the order
of complements and adjuncts is variable, and influ-
enced by a variety of syntactic and non-syntactic
factors, including pronominalization, information
structure, definiteness, and animacy (e.g., Uszkor-
eit 1987). The first position in a declarative sen-
tence, for example, can be occupied by various con-
stituents, including the subject (er ?he? in (1c)), the
object (Musik ?music? in (2a)), an adjunct (gestern
?yesterday? in (2b)), or the non-finite verb (kom-
poniert ?composed? in (2c)).
(2) a. Musik hat er gestern komponiert.
b. Gestern hat er Musik komponiert .
c. Komponiert hat er gestern Musik.
The semi-free wordorder in German means that a
context-free grammar model has to contain more
rules than for a fixed wordorder language. For tran-
sitive verbs, for instance, we need the rules S !
V NP NP, S ! NP V NP, and S ! NP NP V to
account for verb initial, verb second, and verb final
order (assuming a flat S, see Section 2.2).
2.2 Negra Annotation Scheme
The Negra corpus consists of around 350,000 words
of German newspaper text (20,602 sentences). The
annotation scheme (Skut et al, 1997) is modeled to a
certain extent on that of the Penn Treebank (Marcus
et al, 1993), with crucial differences. Most impor-
tantly, Negra follows the dependency grammar tra-
dition in assuming flat syntactic representations:
(a) There is no S ! NP VP rule. Rather, the sub-
ject, the verb, and its objects are all sisters of each
other, dominated by an S node. This is a way of
accounting for the semi-free wordorder of German
(see Section 2.1): the first NP within an S need not
be the subject.
(b) There is no SBAR ! Comp S rule. Main
clauses, subordinate clauses, and relative clauses all
share the category S in Negra; complementizers and
relative pronouns are simply sisters of the verb.
(c) There is no PP ! P NP rule, i.e., the prepo-
sition and the noun it selects (and determiners and
adjectives, if present) are sisters, dominated by a
PP node. An argument for this representation is that
prepositions behave like case markers in German; a
preposition and a determiner can merge into a single
word (e.g., in dem ?in the? becomes im).
Another idiosyncrasy of Negra is that it assumes
special coordinate categories. A coordinated sen-
tence has the category CS, a coordinate NP has the
category CNP, etc. While this does not make the
annotation more flat, it substantially increases the
number of non-terminal labels. Negra also contains
grammatical function labels that augment phrasal
and lexical categories. Example are MO (modifier),
HD (head), SB (subject), and OC (clausal object).
3 Probabilistic Parsing Models
3.1 Probabilistic Context-Free Grammars
Lexicalization has been shown to improve pars-
ing performance for the Penn Treebank (e.g., Car-
roll and Rooth 1998; Charniak 1997, 2000; Collins
1997). The aim of the present paper is to test if this
finding carries over to German and to the Negra cor-
pus. We therefore use an unlexicalized model as our
baseline against which to test the lexicalized models.
More specifically, we used a standard proba-
bilistic context-free grammar (PCFG; see Charniak
1993). Each context-free rule RHS ! LHS is anno-
tated with an expansion probability P(RHSjLHS).
The probabilities for all rules with the same lefthand
side have to sum to one, and the probability of a
parse tree T is defined as the product of the prob-
abilities of all rules applied in generating T .
3.2 Carroll and Rooth?s Head-Lexicalized
Model
The head-lexicalized PCFG model of Carroll and
Rooth (1998) is a minimal departure from the stan-
dard unlexicalized PCFG model, which makes it
ideal for a direct comparison.1
A grammar rule LHS ! RHS can be written as
P ! C1 . . .Cn, where P is the mother category, and
C1 . . .Cn are daughters. Let l(C) be the lexical head
1Charniak (1997) proposes essentially the same model; we
will nevertheless use the label ?Carroll and Rooth model? as we
are using their implementation (see Section 4.1).
of the constituent C. The rule probability is then de-
fined as (see also Beil et al 2002):
P(RHSjLHS) = Prule(C1 . . .CnjP, l(P))(3)

n
?
i=1
Pchoice(l(Ci)jCi,P, l(P))
Here Prule(C1 . . .CnjP, l(P)) is the probability that
category P with lexical head l(P) is expanded by the
rule P ! C1 . . .Cn, and Pchoice(l(C)jC,P, l(P)) is the
probability that the (non-head) category C has the
lexical head l(C) given that its mother is P with lex-
ical head l(P).
3.3 Collins?s Head-Lexicalized Model
In contrast to Carroll and Rooth?s (1998) approach,
the model proposed by Collins (1997) does not com-
pute rule probabilities directly. Rather, they are gen-
erated using a Markov process that makes certain in-
dependence assumptions. A grammar rule LHS !
RHS can be written as P ! Lm . . .L1 H R1 . . .Rn
where P is the mother and H is the head daughter.
Let l(C) be the head word of C and t(C) the tag of
the head word of C. Then the probability of a rule is
defined as:
P(RHSjLHS) = P(Lm . . .L1 H R1 . . .RnjP)(4)
= Ph(HjP)Pl(Lm . . .L1jP,H)Pr(R1 . . .RnjP,H)
= Ph(HjP)
m
?
i=0
Pl(LijP,H,d(i))
n
?
i=0
Pr(RijP,H,d(i))
Here, Ph is the probability of generating the head,
and Pl and Pr are the probabilities of generating the
nonterminals to the left and right of the head, re-
spectively; d(i) is a distance measure. (L0 and R0 are
stop categories.) At this point, the model is still un-
lexicalized. To add lexical sensitivity, the Ph, Pr and
Pl probability functions also take into account head
words and their POS tags:
P(RHSjLHS) = Ph(HjP,t(P), l(P))(5)

m
?
i=0
Pl(Li,t(Li), l(Li)jP,H,t(H), l(H),d(i))

n
?
i=0
Pr(Ri,t(Ri), l(Ri)jP,H,t(H), l(H),d(i))
4 Experiment 1
This experiment was designed to compare the per-
formance of the three models introduced in the
last section. Our main hypothesis was that the lex-
icalized models will outperform the unlexicalized
baseline model. Another prediction was that adding
Negra-specific information to the models will in-
crease parsing performance. We therefore tested a
model variant that included grammatical function la-
bels, i.e., the set of categories was augmented by the
function tags specified in Negra (see Section 2.2).
Adding grammatical functions is a way of deal-
ing with the wordorder facts of German (see Sec-
tion 2.1) in the face of Negra?s very flat annota-
tion scheme. For instance, subject and object NPs
have different wordorder preferences (subjects tend
to be preverbal, while objects tend to be postver-
bal), a fact that is captured if subjects have the la-
bel NP-SB, while objects are labeled NP-OA (ac-
cusative object), NP-DA (dative object), etc. Also
the fact that verb order differs between subordinate
and main clauses is captured by the function labels:
the former are labeled S, while the latter are labeled
S-OC (object clause), S-RC (relative clause), etc.
Another idiosyncrasy of the Negra annotation is
that conjoined categories have separate labels (S and
CS, NP and CNP, etc.), and that PPs do not contain
an NP node. We tested a variant of the Carroll and
Rooth (1998) model that takes this into account.
4.1 Method
Data Sets All experiments reported in this paper
used the treebank format of Negra. This format,
which is included in the Negra distribution, was de-
rived from the native format by replacing crossing
branches with traces. We split the corpus into three
subsets. The first 18,602 sentences constituted the
training set. Of the remaining 2,000 sentences, the
first 1,000 served as the test set, and the last 1000 as
the development set. To increase parsing efficiency,
we removed all sentences with more than 40 words.
This resulted in a test set of 968 sentences and a
development set of 975 sentences. Early versions
of the models were tested on the development set,
and the test set remained unseen until all parameters
were fixed. The final results reported this paper were
obtained on the test set, unless stated otherwise.
Grammar Induction For the unlexicalized PCFG
model (henceforth baseline model), we used the
probabilistic left-corner parser Lopar (Schmid,
2000). When run in unlexicalized mode, Lopar im-
plements the model described in Section 3.1. A
grammar and a lexicon for Lopar were read off the
Negra training set, after removing all grammatical
function labels. As Lopar cannot handle traces, these
were also removed from the training data.
The head-lexicalized model of Carroll and Rooth
(1998) (henceforth C&R model) was again realized
using Lopar, which in lexicalized mode implements
the model in Section 3.2. Lexicalization requires that
each rule in a grammar has one of the categories on
its righthand side annotated as the head. For the cate-
gories S, VP, AP, and AVP, the head is marked in Ne-
gra. For the other categories, we used rules to heuris-
tically determine the head, as is standard practice for
the Penn Treebank.
The lexicalized model proposed by Collins (1997)
(henceforth Collins model) was re-implemented by
one of the authors. For training, empty categories
were removed from the training data, as the model
cannot handle them. The same head finding strategy
was applied as for the C&R model.
In this experiment, only head-head statistics were
used (see (5)). The original Collins model uses
sister-head statistics for non-recursive NPs. This will
be discussed in detail in Section 5.
Training and Testing For all three models, the
model parameters were estimated using maximum
likelihood estimation. Both Lopar and the Collins
model use various backoff distributions to smooth
the estimates. The reader is referred to Schmid
(2000) and Collins (1997) for details. For the C&R
model, we used a cutoff of one for rule frequencies
Prule and lexical choice frequencies Pchoice (the cutoff
value was optimized on the development set).
We also tested variants of the baseline model and
the C&R model that include grammatical function
information, as we hypothesized that this informa-
tion might help the model to handle wordorder vari-
ation more adequately, as explained above.
Finally, we tested variant of the C&R model that
uses Lopar?s parameter pooling feature. This fea-
ture makes it possible to collapse the lexical choice
distribution Pchoice for either the daughter or the
mother categories of a rule (see Section 3.2). We
pooled the estimates for pairs of conjoined and non-
conjoined daughter categories (S and CS, NP and
CNP, etc.): these categories should be treated as the
same daughters; e.g., there should be no difference
between S !NP V and S!CNP V. We also pooled
the estimates for the mother categories NPs and PPs.
This is a way of dealing with the fact that there is no
separate NP node within PPs in Negra.
Lopar and the Collins model differ in their han-
dling of unknown words. In Lopar, a POS tag distri-
bution for unknown words has to be specified, which
is then used to tag unknown words in the test data.
The Collins model treats any word seen fewer than
five times in the training data as unseen and uses an
external POS tagger to tag unknown words. In order
to make the models comparable, we used a uniform
approach to unknown words. All models were run
on POS-tagged input; this input was created by tag-
ging the test set with a separate POS tagger, for both
known and unknown words. We used TnT (Brants,
2000), trained on the Negra training set. The tagging
accuracy was 97.12% on the development set.
In order to obtain an upper bound for the perfor-
mance of the parsing models, we also ran the parsers
on the test set with the correct tags (as specified in
Negra), again for both known and unknown words.
We will refer to this mode as ?perfect tagging?.
All models were evaluated using standard PAR-
SEVAL measures. We report labeled recall (LR)
labeled precision (LP), average crossing brackets
(CBs), zero crossing brackets (0CB), and two or less
crossing brackets (2CB). We also give the cover-
age (Cov), i.e., the percentage of sentences that the
parser was able to parse.
4.2 Results
The results for all three models and their variants
are given in Table 2, for both TnT tags and per-
fect tags. The baseline model achieves 70.56% LR
and 66.69% LP with TnT tags. Adding grammatical
functions reduces both figures slightly, and cover-
age drops by about 15%. The C&R model performs
worse than the baseline, at 68.04% LR and 60.07%
LP (for TnT tags). Adding grammatical function
again reduces performance slightly. Parameter pool-
ing increases both LR and LP by about 1%. The
Collins models also performs worse than the base-
line, at 67.91% LR and 66.07% LP.
Performance using perfect tags (an upper bound
of model performance) is 2?3% higher for the base-
line and for the C&R model. The Collins model
gains only about 1%. Perfect tagging results in a per-
formance increase of over 10% for the models with
grammatical functions. This is not surprising, as the
perfect tags (but not the TnT tags) include grammat-
ical function labels. However, we also observe a dra-
matic reduction in coverage (to about 65%).
4.3 Discussion
We added grammatical functions to both the base-
line model and the C&R model, as we predicted
that this would allow the model to better capture the
wordorder facts of German. However, this predic-
tion was not borne out: performance with grammat-
ical functions (on TnT tags) was slightly worse than
without, and coverage dropped substantially. A pos-
sible reason for this is sparse data: a grammar aug-
mented with grammatical functions contains many
additional categories, which means that many more
parameters have to be estimated using the same
training set. On the other hand, a performance in-
crease occurs if the tagger also provides grammati-
cal function labels (simulated in the perfect tags con-
dition). However, this comes at the price of an unac-
ceptable reduction in coverage.
When training the C&R model, we included a
variant that makes use of Lopar?s parameter pool-
ing feature. We pooled the estimates for conjoined
daughter categories, and for NP and PP mother cat-
egories. This is a way of taking the idiosyncrasies of
the Negra annotation into account, and resulted in a
small improvement in performance.
The most surprising finding is that the best per-
formance was achieved by the unlexicalized PCFG
TnT tagging Perfect tagging
LR LP CBs 0CB 2CB Cov LR LP CBs 0CB 2CB Cov
Baseline 70.56 66.69 1.03 58.21 84.46 94.42 72.99 70.00 0.88 60.30 87.42 95.25
Baseline + GF 70.45 65.49 1.07 58.02 85.01 79.24 81.14 78.37 0.46 74.25 95.26 65.39
C&R 68.04 60.07 1.31 52.08 79.54 94.42 70.79 63.38 1.17 54.99 82.21 95.25
C&R + pool 69.07 61.41 1.28 53.06 80.09 94.42 71.74 64.73 1.11 56.40 83.08 95.25
C&R + GF 67.66 60.33 1.31 55.67 80.18 79.24 81.17 76.83 0.48 73.46 94.15 65.39
Collins 67.91 66.07 0.73 65.67 89.52 95.21 68.63 66.94 0.71 64.97 89.73 96.23
Table 2: Results for Experiment 1: comparison of lexicalized and unlexicalized models (GF: grammatical
functions; pool: parameter pooling for NPs/PPs and conjoined categories)
0 20 40 60 80 100
percent of training corpus
45
50
55
60
65
70
75
f-s
co
re
unlexicalized PCFG
lexicalized PCFG (Collins)
lexicalized PCFG (C&R)
Figure 1: Learning curves for all three models
baseline model. Both lexicalized models (C&R and
Collins) performed worse than the baseline. This re-
sults is at odds with what has been found for En-
glish, where lexicalization is standardly reported to
increase performance by about 10%. The poor per-
formance of the lexicalized models could be due to
a lack of sufficient training data: our Negra training
set contains approximately 18,000 sentences, and is
therefore significantly smaller than the Penn Tree-
bank training set (about 40,000 sentences). Negra
sentences are also shorter: they contain, on average,
15 words compared to 22 in the Penn Treebank.
We computed learning curves for the unmodified
variants (without grammatical functions or parame-
ter pooling) of all three models (on the development
set). The result (see Figure 1) shows that there is no
evidence for an effect of sparse data. For both the
baseline and the C&R model, a fairly high f-score
is achieved with only 10% of the training data. A
slow increase occurs as more training data is added.
The performance of the Collins model is even less
affected by training set size. This is probably due to
the fact that it does not use rule probabilities directly,
but generates rules using a Markov chain.
5 Experiment 2
As we saw in the last section, lack of training data is
not a plausible explanation for the sub-baseline per-
formance of the lexicalized models. In this experi-
ment, we therefore investigate an alternative hypoth-
esis, viz., that the lexicalized models do not cope
Penn Negra
NP 2.20 3.08
PP 2.03 2.66
Penn Negra
VP 2.32 2.59
S 2.22 4.22
Table 3: Average number of daughters for the gram-
matical categories in the Penn Treebank and Negra
well with the fact that Negra rules are so flat (see
Section 2.2). We will focus on the Collins model, as
it outperformed the C&R model in Experiment 1.
An error analysis revealed that many of the errors
of the Collins model in Experiment 1 are chunking
errors. For example, the PP neben den Mitteln des
Theaters should be analyzed as (6a). But instead the
parser produces two constituents as in (6b)):
(6) a. [PP neben
apart
den
the
Mitteln
means
[NP des
the
Theaters]]
theater?s
?apart from the means of the theater?.
b. [PP neben den Mitteln] [NP des Theaters]
The reason for this problem is that neben is the head
of the constituent in (6), and the Collins model uses
a crude distance measure together with head-head
dependencies to decide if additional constituents
should be added to the PP. The distance measure is
inadequate for finding PPs with high precision.
The chunking problem is more widespread than
PPs. The error analysis shows that other con-
stituents, including Ss and VPs, also have the wrong
boundary. This problem is compounded by the fact
that the rules in Negra are substantially flatter than
the rules in the Penn Treebank, for which the Collins
model was developed. Table 3 compares the average
number of daughters in both corpora.
The flatness of PPs is easy to reduce. As detailed
in Section 2.2, PPs lack an intermediate NP projec-
tion, which can be inserted straightforwardly using
the following rule:
(7) [PP P . . . ] ! [PP P [NP . . . ]]
In the present experiment, we investigated if parsing
performance improves if we test and train on a ver-
sion of Negra on which the transformation in (7) has
been applied.
In a second series of experiments, we investigated
a more general way of dealing with the flatness of
C&R Collins Charniak Current
Head sister category X X X
Head sister head word X X X
Head sister head tag X X
Prev. sister category X X X
Prev. sister head word X
Prev. sister head tag X
Table 4: Linguistic features in the current model
compared to the models of Carroll and Rooth
(1998), Collins (1997), and Charniak (2000)
Negra, based on Collins?s (1997) model for non-
recursive NPs in the Penn Treebank (which are also
flat). For non-recursive NPs, Collins (1997) does not
use the probability function in (5), but instead sub-
stitutes Pr (and, by analogy, Pl) by:
Pr(Ri,t(Ri), l(Ri)jP,Ri?1,t(Ri?1), l(Ri?1),d(i))(8)
Here the head H is substituted by the sister Ri?1
(and Li?1). In the literature, the version of Pr in (5)
is said to capture head-head relationships. We will
refer to the alternative model in (8) as capturing
sister-head relationships.
Using sister-head relationships is a way of coun-
teracting the flatness of the grammar productions;
it implicitly adds binary branching to the grammar.
Our proposal is to extend the use of sister-head re-
lationship from non-recursive NPs (as proposed by
Collins) to all categories.
Table 4 shows the linguistic features of the result-
ing model compared to the models of Carroll and
Rooth (1998), Collins (1997), and Charniak (2000).
The C&R model effectively includes category infor-
mation about all previous sisters, as it uses context-
free rules. The Collins (1997) model does not use
context-free rules, but generates the next category
using zeroth order Markov chains (see Section 3.3),
hence no information about the previous sisters is
included. Charniak?s (2000) model extends this to
higher order Markov chains (first to third order), and
therefore includes category information about previ-
ous sisters.The current model differs from all these
proposals: it does not use any information about the
head sister, but instead includes the category, head
word, and head tag of the previous sister, effectively
treating it as the head.
5.1 Method
We first trained the original Collins model on a mod-
ified versions of the training test from Experiment 1
in which the PPs were split by applying rule (7).
In a second series of experiments, we tested a
range of models that use sister-head dependencies
instead of head-head dependencies for different cat-
egories. We first added sister-head dependencies for
NPs (following Collins?s (1997) original proposal)
and then for PPs, which are flat in Negra, and thus
similar in structure to NPs (see Section 2.2). Then
we tested a model in which sister-head relationships
are applied to all categories.
In a third series of experiments, we trained mod-
els that use sister-head relationships everywhere ex-
cept for one category. This makes it possible to de-
termine which sister-head dependencies are crucial
for improving performance of the model.
5.2 Results
The results of the PP experiment are listed in Ta-
ble 5. Again, we give results obtained using TnT tags
and using perfect tags. The row ?Split PP? contains
the performance figures obtained by including split
PPs in both the training and in the testing set. This
leads to a substantial increase in LR (6?7%) and LP
(around 8%) for both tagging schemes. Note, how-
ever, that these figures are not directly comparable to
the performance of the unmodified Collins model: it
is possible that the additional brackets artificially in-
flate LR and LP. Presumably, the brackets for split
PPs are easy to detect, as they are always adjacent to
a preposition. An honest evaluation should therefore
train on the modified training set (with split PPs),
but collapse the split categories for testing, i.e., test
on the unmodified test set. The results for this evalu-
ation are listed in rows ?Collapsed PP?. Now there is
no increase in performance compared to the unmod-
ified Collins model; rather, a slight drop in LR and
LP is observed.
Table 5 also displays the results of our exper-
iments with the sister-head model. For TnT tags,
we observe that using sister-head dependencies for
NPs leads to a small decrease in performance com-
pared to the unmodified Collins model, resulting in
67.84% LR and 65.96% LP. Sister-head dependen-
cies for PPs, however, increase performance sub-
stantially to 70.27% LR and 68.45% LP. The high-
est improvement is observed if head-sister depen-
dencies are used for all categories; this results in
71.32% LR and 70.93% LP, which corresponds to an
improvement of 3% in LP and 5% in LR compared
to the unmodified Collins model. Performance with
perfect tags is around 2?4% higher than with TnT
tags. For perfect tags, sister-head dependencies lead
to an improvement for NPs, PPs, and all categories.
The third series of experiments was designed to
determine which categories are crucial for achiev-
ing this performance gain. This was done by train-
ing models that use sister-head dependencies for all
categories but one. Table 6 shows the change in LR
and LP that was found for each individual category
(again for TnT tags and perfect tags). The highest
drop in performance (around 3%) is observed when
the PP category is reverted to head-head dependen-
cies. For S and for the coordinated categories (CS,
TnT tagging Perfect tagging
LR LP CBs 0CB 2CB Cov LR LP CBs 0CB 2CB Cov
Unmod. Collins 67.91 66.07 0.73 65.67 89.52 95.21 68.63 66.94 0.71 64.97 89.73 96.23
Split PP 73.84 73.77 0.82 62.89 88.98 95.11 75.93 75.27 0.77 65.36 89.03 93.79
Collapsed PP 66.45 66.07 0.89 66.60 87.04 95.11 68.22 67.32 0.94 66.67 85.88 93.79
Sister-head NP 67.84 65.96 0.75 65.85 88.97 95.11 71.54 70.31 0.60 68.03 93.33 94.60
Sister-head PP 70.27 68.45 0.69 66.27 90.33 94.81 73.20 72.44 0.60 68.53 93.21 94.50
Sister-head all 71.32 70.93 0.61 69.53 91.72 95.92 73.93 74.24 0.54 72.30 93.47 95.21
Table 5: Results for Experiment 2: performance for models using split phrases and sister-head dependencies
CNP, etc.), a drop in performance of around 1% each
is observed. A slight drop is observed also for VP
(around 0.5%). Only minimal fluctuations in perfor-
mance are observed when the other categories are
removed (AP, AVP, and NP): there is a small effect
(around 0.5%) if TnT tags are used, and almost no
effect for perfect tags.
5.3 Discussion
We showed that splitting PPs to make Negra less
flat does not improve parsing performance if test-
ing is carried out on the collapsed categories. How-
ever, we observed that LR and LP are artificially in-
flated if split PPs are used for testing. This finding
goes some way towards explaining why the parsing
performance reported for the Penn Treebank is sub-
stantially higher than the results for Negra: the Penn
Treebank contains split PPs, which means that there
are lot of brackets that are easy to get right. The re-
sulting performance figures are not directly compa-
rable to figures obtained on Negra, or other corpora
with flat PPs.2
We also obtained a positive result: we demon-
strated that a sister-head model outperforms the un-
lexicalized baseline model (unlike the C&R model
and the Collins model in Experiment 1). LR was
about 1% higher and LP about 4% higher than the
baseline if lexical sister-head dependencies are used
for all categories. This holds both for TnT tags and
for perfect tags (compare Tables 2 and 5). We also
found that using lexical sister-head dependencies for
all categories leads to a larger improvement than us-
ing them only for NPs or PPs (see Table 5). This
result was confirmed by a second series of experi-
ments, where we reverted individual categories back
to head-head dependencies, which triggered a de-
crease in performance for all categories, with the ex-
ception of NP, AP, and AVP (see Table 6).
On the whole, the results of Experiment 2 are at
odds with what is known about parsing for English.
The progression in the probabilistic parsing litera-
ture has been to start with lexical head-head depen-
dencies (Collins, 1997) and then add non-lexical sis-
2This result generalizes to Ss, which are also flat in Negra
(see Section 2.2). We conducted an experiment in which we
added an SBAR above the S. No increase in performance was
obtained if the evaluation was carried using collapsed Ss.
TnT tagging Perfect tagging
?LR ?LP ?LR ?LP
PP ?3.45 ?1.60 ?4.21 ?3.35
S ?1.28 0.11 ?2.23 ?1.22
Coord ?1.87 ?0.39 ?1.54 ?0.80
VP ?0.72 0.18 ?0.58 ?0.30
AP ?0.57 0.10 0.08 ?0.07
AVP ?0.32 0.44 0.10 0.11
NP 0.06 0.78 ?0.15 0.02
Table 6: Change in performance when reverting to
head-head statistics for individual categories
ter information (Charniak, 2000), as illustrated in
Table 4. Lexical sister-head dependencies have only
been found useful in a limited way: in the original
Collins model, they are used for non-recursive NPs.
Our results show, however, that for parsing Ger-
man, lexical sister-head information is more im-
portant than lexical head-head information. Only a
model that replaced lexical head-head with lexical
sister-head dependencies was able to outperform a
baseline model that uses no lexicalization.3 Based
on the error analysis for Experiment 1, we claim that
the reason for the success of the sister-head model is
the fact that the rules in Negra are so flat; using a
sister-head model is a way of binarizing the rules.
6 Comparison with Previous Work
There are currently no probabilistic, treebank-
trained parsers available for German (to our knowl-
edge). A number of chunking models have been pro-
posed, however. Skut and Brants (1998) used Ne-
gra to train a maximum entropy-based chunker, and
report LR and LP of 84.4% for NP and PP chunk-
ing. Using cascaded Markov models, Brants (2000)
reports an improved performance on the same task
(LR 84.4%, LP 88.3%). Becker and Frank (2002)
train an unlexicalized PCFG on Negra to perform
a different chunking task, viz., the identification of
topological fields (sentence-based chunks). They re-
port an LR and LP of 93%.
The head-lexicalized model of Carroll and Rooth
(1998) has been applied to German by Beil et al
3It is unclear what effect bi-lexical statistics have on the
sister-head model; while Gildea (2001) shows bi-lexical statis-
tics are sparse for some grammars, Hockenmaier and Steedman
(2002) found they play a greater role in binarized grammars.
(1999, 2002). However, this approach differs in the
number of ways from the results reported here: (a) a
hand-written grammar (instead of a treebank gram-
mar) is used; (b) training is carried out on unan-
notated data; (c) the grammar and the training set
cover only subordinate and relative clauses, not un-
restricted text. Beil et al (2002) report an evaluation
using an NP chunking task, achieving 92% LR and
LP. They also report the results of a task-based eval-
uation (extraction of sucategorization frames).
There is some research on treebank-based pars-
ing of languages other than English. The work by
Collins et al (1999) and Bikel and Chiang (2000)
has demonstrated the applicability of the Collins
(1997) model for Czech and Chinese. The perfor-
mance reported by these authors is substantially
lower than the one reported for English, which might
be due to the fact that less training data is avail-
able for Czech and Chinese (see Table 1). This hy-
pothesis cannot be tested, as the authors do not
present learning curves for their models. However,
the learning curve for Negra (see Figure 1) indicates
that the performance of the Collins (1997) model
is stable, even for small training sets. Collins et al
(1999) and Bikel and Chiang (2000) do not compare
their models with an unlexicalized baseline; hence
it is unclear if lexicalization really improves parsing
performance for these languages. As Experiment 1
showed, this cannot be taken for granted.
7 Conclusions
We presented the first probabilistic full parsing
model for German trained on Negra, a syntactically
annotated corpus. This model uses lexical sister-
head dependencies, which makes it particularly suit-
able for parsing Negra?s flat structures. The flatness
of the Negra annotation reflects the syntactic proper-
ties of German, in particular its semi-free wordorder.
In Experiment 1, we applied three standard pars-
ing models from the literature to Negra: an un-
lexicalized PCFG model (the baseline), Carroll
and Rooth?s (1998) head-lexicalized model, and
Collins?s (1997) model based on head-head depen-
dencies. The results show that the baseline model
achieves a performance of up to 73% recall and 70%
precision. Both lexicalized models perform substan-
tially worse. This finding is at odds with what has
been reported for parsing models trained on the Penn
Treebank. As a possible explanation we considered
lack of training data: Negra is about half the size of
the Penn Treebank. However, the learning curves for
the three models failed to produce any evidence that
they suffer from sparse data.
In Experiment 2, we therefore investigated an al-
ternative hypothesis: the poor performance of the
lexicalized models is due to the fact that the rules in
Negra are flatter than in the Penn Treebank, which
makes lexical head-head dependencies less useful
for correctly determining constituent boundaries.
Based on this assumption, we proposed an alterna-
tive model hat replaces lexical head-head dependen-
cies with lexical sister-head dependencies. This can
the thought of as a way of binarizing the flat rules in
Negra. The results show that sister-head dependen-
cies improve parsing performance not only for NPs
(which is well-known for English), but also for PPs,
VPs, Ss, and coordinate categories. The best perfor-
mance was obtained for a model that uses sister-head
dependencies for all categories. This model achieves
up to 74% recall and precision, thus outperforming
the unlexicalized baseline model.
It can be hypothesized that this finding carries
over to other treebanks that are annotated with flat
structures. Such annotation schemes are often used
for languages that (unlike English) have a free or
semi-free wordorder. Testing our sister-head model
on these languages is a topic for future research.
References
Becker, Markus and Anette Frank. 2002. A stochastic topological parser of Ger-
man. In Proceedings of the 19th International Conference on Computational
Linguistics. Taipei.
Beil, Franz, Glenn Carroll, Detlef Prescher, Stefan Riezler, and Mats Rooth. 1999.
Inside-outside estimation of a lexicalized PCFG for German. In Proceedings
of the 37th Annual Meeting of the Association for Computational Linguistics.
College Park, MA.
Beil, Franz, Detlef Prescher, Helmut Schmid, and Sabine Schulte im Walde. 2002.
Evaluation of the Gramotron parser for German. In Proceedings of the LREC
Workshop Beyond Parseval: Towards Improved Evaluation Measures for Pars-
ing Systems. Las Palmas, Gran Canaria.
Bikel, Daniel M. and David Chiang. 2000. Two statistical parsing models applied
to the Chinese treebank. In Proceedings of the 2nd ACL Workshop on Chinese
Language Processing. Hong Kong.
Brants, Thorsten. 2000. TnT: A statistical part-of-speech tagger. In Proceedings
of the 6th Conference on Applied Natural Language Processing. Seattle.
Carroll, Glenn and Mats Rooth. 1998. Valence induction with a head-lexicalized
PCFG. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing. Granada.
Charniak, Eugene. 1993. Statistical Language Learning. MIT Press, Cambridge,
MA.
Charniak, Eugene. 1997. Statistical parsing with a context-free grammar and word
statistics. In Proceedings of the 14th National Conference on Artificial Intel-
ligence. AAAI Press, Cambridge, MA.
Charniak, Eugene. 2000. A maximum-entropy-inspired parser. In Proceedings
of the 1st Conference of the North American Chapter of the Association for
Computational Linguistics. Seattle.
Collins, Michael. 1997. Three generative, lexicalised models for statistical pars-
ing. In Proceedings of the 35th Annual Meeting of the Association for Com-
putational Linguistics and the 8th Conference of the European Chapter of the
Association for Computational Linguistics. Madrid.
Collins, Michael, Jan Hajic?, Lance Ramshaw, and Christoph Tillmann. 1999. A
statistical parser for Czech. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics. College Park, MA.
Gildea, Daniel. 2001. Corpus variation and parser performance. In Proceedings
of the Conference on Empirical Methods in Natural Language Processing.
Pittsburgh.
Hockenmaier, Julia and Mark Steedman. 2002. Generative models for statistical
parsing with combinatory categorial grammar. In Proceedings of 40th Annual
Meeting of the Association for Computational Linguistics. Philadelphia.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn Treebank. Compu-
tational Linguistics 19(2).
Schmid, Helmut. 2000. LoPar: Design and implementation. Ms., Institute for
Computational Linguistics, University of Stuttgart.
Skut, Wojciech and Thorsten Brants. 1998. A maximum-entropy partial parser for
unrestricted text. In Proceedings of the 6th Workshop on Very Large Corpora.
Montre?al.
Skut, Wojciech, Brigitte Krenn, Thorsten Brants, and Hans Uszkoreit. 1997. An
annotation scheme for free word order languages. In Proceedings of the 5th
Conference on Applied Natural Language Processing. Washington, DC.
Uszkoreit, Hans. 1987. Word Order and Constituent Structure in German. CSLI
Publications, Stanford, CA.
Deep Syntactic Processing by Combining Shallow Methods
Pe?ter Dienes and Amit Dubey
Department of Computational Linguistics
Saarland University
PO Box 15 11 50
66041 Saarbru?cken, Germany
{dienes,adubey}@coli.uni-sb.de
Abstract
We present a novel approach for find-
ing discontinuities that outperforms pre-
viously published results on this task.
Rather than using a deeper grammar for-
malism, our system combines a simple un-
lexicalized PCFG parser with a shallow
pre-processor. This pre-processor, which
we call a trace tagger, does surprisingly
well on detecting where discontinuities
can occur without using phase structure
information.
1 Introduction
In this paper, we explore a novel approach for find-
ing long-distance dependencies. In particular, we
detect such dependencies, or discontinuities, in a
two-step process: (i) a conceptually simple shal-
low tagger looks for sites of discontinuties as a pre-
processing step, before parsing; (ii) the parser then
finds the dependent constituent (antecedent).
Clearly, information about long-distance relation-
ships is vital for semantic interpretation. However,
such constructions prove to be difficult for stochas-
tic parsers (Collins et al, 1999) and they either avoid
tackling the problem (Charniak, 2000; Bod, 2003)
or only deal with a subset of the problematic cases
(Collins, 1997).
Johnson (2002) proposes an algorithm that is
able to find long-distance dependencies, as a post-
processing step, after parsing. Although this algo-
rithm fares well, it faces the problem that stochastic
parsers not designed to capture non-local dependen-
cies may get confused when parsing a sentence with
discontinuities. However, the approach presented
here is not susceptible to this shortcoming as it finds
discontinuties before parsing.
Overall, we present three primary contributions.
First, we extend the mechanism of adding gap vari-
ables for nodes dominating a site of discontinu-
ity (Collins, 1997). This approach allows even a
context-free parser to reliably recover antecedents,
given prior information about where discontinuities
occur. Second, we introduce a simple yet novel
finite-state tagger that gives exactly this information
to the parser. Finally, we show that the combina-
tion of the finite-state mechanism, the parser, and
our new method for antecedent recovery can com-
petently analyze discontinuities.
The overall organization of the paper is as fol-
lows. First, Section 2 sketches the material we use
for the experiments in the paper. In Section 3, we
propose a modification to a simple PCFG parser that
allows it to reliably find antecedents if it knows the
sites of long-distance dependencies. Then, in Sec-
tion 4, we develop a finite-state system that gives the
parser exactly that information with fairly high accu-
racy. We combine the models in Section 5 to recover
antecedents. Section 6 discusses related work.
2 Annotation of empty elements
Different linguistic theories offer various treatments
of non-local head?dependent relations (referred to
by several other terms such as extraction, discon-
tinuity, movement or long-distance dependencies).
The underlying idea, however, is the same: extrac-
tion sites are marked in the syntactic structure and
this mark is connected (co-indexed) to the control-
Type Freq. Example
NP?NP 987 Sam was seen *
WH?NP 438 the woman who you saw *T*
PRO?NP 426 * to sleep is nice
COMP?SBAR 338 Sam said 0 Sasha snores
UNIT 332 $ 25 *U*
WH?S 228 Sam had to go, Sasha said *T*
WH?ADVP 120 Sam told us how he did it *T*
CLAUSE 118 Sam had to go, Sasha said 0
COMP?WHNP 98 the woman 0 we saw *T*
ALL 3310
Table 1: Most frequent types of EEs in Section 0.
ling constituent.
The experiments reported here rely on a train-
ing corpus annotated with non-local dependencies
as well as phrase-structure information. We used
the Wall Street Journal (WSJ) part of the Penn Tree-
bank (Marcus et al, 1993), where extraction is rep-
resented by co-indexing an empty terminal element
(henceforth EE) to its antecedent. Without commit-
ting ourselves to any syntactic theory, we adopt this
representation.
Following the annotation guidelines (Bies et
al., 1995), we distinguish seven basic types of
EEs: controlled NP-traces (NP), PROs (PRO),
traces of A
 
-movement (mostly wh-movement:
WH), empty complementizers (COMP), empty units
(UNIT), and traces representing pseudo-attachments
(shared constituents, discontinuous dependencies,
etc.: PSEUDO) and ellipsis (ELLIPSIS ). These la-
bels, however, do not identify the EEs uniquely: for
instance, the label WH may represent an extracted
NP object as well as an adverb moved out of the
verb phrase. In order to facilitate antecedent re-
covery and to disambiguate the EEs, we also anno-
tate them with their parent nodes. Furthermore, to
ease straightforward comparison with previous work
(Johnson, 2002), a new label CLAUSE is introduced
for COMP-SBAR whenever it is followed by a moved
clause WH?S. Table 1 summarizes the most frequent
types occurring in the development data, Section 0
of the WSJ corpus, and gives an example for each,
following Johnson (2002).
For the parsing and antecedent recovery exper-
iments, in the case of WH-traces (WH?  ) and
SBAR
NP
who
S 	

NP
you
VP 	

V
saw
NP 	

*WH-NP*
Figure 1: Threading gap+WH-NP.
controlled NP-traces (NP?NP), we follow the stan-
dard technique of marking nodes dominating the
empty element up to but not including the par-
ent of the antecedent as defective (missing an ar-
gument) with a gap feature (Gazdar et al, 1985;
Collins, 1997).1 Furthermore, to make antecedent
co-indexation possible with many types of EEs, we
generalize Collins? approach by enriching the anno-
tation of non-terminals with the type of the EE in
question (eg. WH?NP) by using different gap+ fea-
tures (gap+WH-NP; cf. Figure 1). The original non-
terminals augmented with gap+ features serve as
new non-terminal labels.
In the experiments, Sections 2?21 were used to
train the models, Section 0 served as a develop-
ment set for testing and improving models, whereas
we present the results on the standard test set, Sec-
tion 23.
3 Parsing with empty elements
The present section explores whether an unlexical-
ized PCFG parser can handle non-local dependen-
cies: first, is it able to detect EEs and, second, can
it find their antecedents? The answer to the first
question turns out to be negative: due to efficiency
reasons and the inappropriateness of the model, de-
tecting all types of EEs is not feasible within the
parser. Antecedents, however, can be reliably recov-
ered provided a parser has perfect knowledge about
EEs occurring in the input. This shows that the main
bottleneck is detecting the EEs and not finding their
antecedents. In the following section, therefore, we
explore how we can provide the parser with infor-
mation about EE sites in the current sentence without
1This technique fails for 82 sentences of the treebank where
the antecedent does not c-command the corresponding EE.
relying on phrase structure information.
3.1 Method
There are three modifications required to allow a
parser to detect EEs and resolve antecedents. First,
it should be able to insert empty nodes. Second, it
must thread the gap+ variables to the parent node of
the antecedent. Knowing this node is not enough,
though. Since the Penn Treebank grammar is not
binary-branching, the final task is to decide which
child of this node is the actual antecedent.
The first two modifications are not diffi-
cult conceptually. A bottom-up parser can be
easily modified to insert empty elements (c.f.
Dienes and Dubey (2003)). Likewise, the changes
required to include gap+ categories are not compli-
cated: we simply add the gap+ features to the non-
terminal category labels.
The final and perhaps most important concern
with developing a gap-threading parser is to ensure
it is possible to choose the correct child as the an-
tecedent of an EE. To achieve this task, we em-
ploy the algorithm presented in Figure 2. At any
node in the tree where the children, all together,
have more gap+ features activated than the par-
ent, the algorithm deduces that a gap+ must have
an antecedent. It then picks a child as the an-
tecedent and recursively removes the gap+ feature
corresponding to its EE from the non-terminal la-
bels. The algorithm has a shortcoming, though: it
cannot reliably handle cases when the antecedent
does not c-command its EE. This mostly happens
with PSEUDOs (pseudo-attachments), where the al-
gorithm gives up and (wrongly) assumes they have
no antecedent.
Given the perfect trees of the development set,
the antecedent recovery algorithm finds the correct
antecedent with 95% accuracy, rising to 98% if
PSEUDOs are excluded. Most of the remaining mis-
takes are caused either by annotation errors, or by
binding NP-traces (NP?NP) to adjunct NPs, as op-
posed to subject NPs.
The parsing experiments are carried out with an
unlexicalized PCFG augmented with the antecedent
recovery algorithm. We use an unlexicalized model
to emphasize the point that even a simple model de-
tects long distance dependencies successfully. The
parser uses beam thresholding (Goodman, 1998) to
for a tree T, iterate over nodes bottom-up
for a node with rule P   C0  Cn
N  multiset of EEs in P
M  multiset of EEs in C0  Cn
foreach EE of type e in M  N
pick a j such that e allows C j
as an antecedent
pick a k such that k

 j and
Ck dominates an EE of type e
if no such j or k exist,
return no antecedent
else
bind the EE dominated by Ck to
the antecedent C j
Figure 2: The antecedent recovery algorithm.
ensure efficient parsing. PCFG probabilities are cal-
culated in the standard way (Charniak, 1993). In
order to keep the number of independently tunable
parameters low, no smoothing is used.
The parser is tested under two different condi-
tions. First, to assess the upper bound an EE-
detecting unlexicalized PCFG can achieve, the input
of the parser contains the empty elements as sepa-
rate words (PERFECT). Second, we let the parser
introduce the EEs itself (INSERT).
3.2 Evaluation
We evaluate on all sentences in the test section of the
treebank. As our interest lies in trace detection and
antecedent recovery, we adopt the evaluation mea-
sures introduced by Johnson (2002). An EE is cor-
rectly detected if our model gives it the correct la-
bel as well as the correct position (the words before
and after it). When evaluating antecedent recovery,
the EEs are regarded as four-tuples, consisting of the
type of the EE, its location, the type of its antecedent
and the location(s) (beginning and end) of the an-
tecedent. An antecedent is correctly recovered if
all four values match the gold standard. The preci-
sion, recall, and the combined F-score is presented
for each experiment. Missed parses are ignored for
evaluation purposes.
3.3 Results
The main results for the two conditions are summa-
rized in Table 2. In the INSERT case, the parser de-
tects empty elements with precision 64.7%, recall
40.3% and F-Score 49.7%. It recovers antecedents
Condition PERFECT INSERT
Empty element
detection (F-score) ? 49   7%
Antecedent recovery
(F-score) 91   4% 43   0%
Parsing time (sec/sent) 2   5 21
Missed parses 1   6% 44   3%
Table 2: EE detection, antecedent recovery, parsing
times, and missed parses for the parser
with overall precision 55.7%, recall 35.0% and F-
score 43.0%. With a beam width of 1000, about
half of the parses were missed, and successful parses
take, on average, 21 seconds per sentence and enu-
merate 1.7 million edges. Increasing the beam size
to 40000 decreases the number of missed parses
marginally, while parsing time increases to nearly
two minutes per sentence, with 2.9 million edges
enumerated.
In the PERFECT case, when the sites of the empty
elements are known before parsing, only about 1.6%
of the parses are missed and average parsing time
goes down to 2   5 seconds per sentence. More impor-
tantly, the overall precision and recall of antecedent
recovery is 91.4%.
3.4 Discussion
The result of the experiment where the parser is to
detect long-distance dependencies is negative. The
parser misses too many parses, regardless of the
beam size. This cannot be due to the lack of smooth-
ing: the model with perfect information about the
EE-sites does not run into the same problem. Hence,
the edges necessary to construct the required parse
are available but, in the INSERT case, the beam
search loses them due to unwanted local edges hav-
ing a higher probability. Doing an exhaustive search
might help in principle, but it is infeasible in prac-
tice. Clearly, the problem is with the parsing model:
an unlexicalized PCFG parser is not able to detect
where EEs can occur, hence necessary edges get low
probability and are, thus, filtered out.
The most interesting result, though, is the dif-
ference in speed and in antecedent recovery accu-
racy between the parser that inserts traces, and the
parser which uses perfect information from the tree-
bank about the sites of EEs. Thus, the question
wi
 X ; wi  1  X ; wi  1  X
X is a prefix of wi,

X

4
X is a suffix of wi,

X

4
wi contains a number
wi contains uppercase character
wi contains hyphen
li  1  X
posi  X ; posi  1  X ; posi  1  X
posi  1 posi  XY
posi  2 posi  1 posi  XYZ
posi posi  1  XY
posi posi  1 posi  2  XYZ
Table 3: Local features at position i  1.
naturally arises: could EEs be detected before pars-
ing? The benefit would be two-fold: EEs might be
found more reliably with a different module, and the
parser would be fast and accurate in recovering an-
tecedents. In the next section we show that it is in-
deed possible to detect EEs without explicit knowl-
edge of phrase structure, using a simple finite-state
tagger.
4 Detecting empty elements
This section shows that EEs can be detected fairly
reliably before parsing, i.e. without using phrase
structure information. Specifically, we develop a
finite-state tagger which inserts EEs at the appro-
priate sites. It is, however, unable to find the an-
tecedents for the EEs; therefore, in the next section,
we combine the tagger with the PCFG parser to re-
cover the antecedents.
4.1 Method
Detecting empty elements can be regarded as a sim-
ple tagging task: we tag words according to the ex-
istence and type of empty elements preceding them.
For example, the word Sasha in the sentence
Sam said COMP?SBAR Sasha snores.
will get the tag EE=COMP?SBAR , whereas the word
Sam is tagged with EE=* expressing the lack of an
EE immediately preceding it. If a word is preceded
by more than one EE, such as to in the following
example, it is tagged with the concatenation of the
two EEs, i.e., EE=COMP?WHNP PRO?NP.
It would have been too late COMP?WHNP
PRO?NP to think about on Friday.
Target Matching regexp Explanation
NP?NP BE RB* VBN passive
 
NP?NP
PRO-NP 
RB* to RB* VB to-infinitive
N [,:] RB* VBG gerund
COMP?SBAR (V  ,) !that* (MD  V) lookahead for that
WH?NP !IN 
  WP
WDT
COMP?WHNP
 

!WH?NP* V lookback for pending WHNPs
WH?ADVP WRB !WH?ADVP* V !WH?ADVP* [.,:] lookback for pending WHADVP before a verb
UNIT $ CD* $ sign before numbers
Table 4: Non-local binary feature templates; the EE-site is indicated by
Although this approach is closely related to POS-
tagging, there are certain differences which make
this task more difficult. Despite the smaller tagset,
the data exhibits extreme sparseness: even though
more than 50% of the sentences in the Penn Tree-
bank contain some EEs, the actual number of EEs is
very small. In Section 0 of the WSJ corpus, out of
the 46451 tokens only 3056 are preceded by one or
more EEs, that is, approximately 93.5% of the words
are tagged with the EE=* tag.
The other main difference is the apparently non-
local nature of the problem, which motivates our
choice of a Maximum Entropy (ME) model for the
tagging task (Berger et al, 1996). ME allows the
flexible combination of different sources of informa-
tion, i.e., local and long-distance cues characterizing
possible sites for EEs. In the ME framework, linguis-
tic cues are represented by (binary-valued) features
( fi), the relative importance (weight, ?i) of which is
determined by an iterative training algorithm. The
weighted linear combination of the features amount
to the log-probability of the label (l) given the con-
text (c):
p  l 	 c 
 1
Z  c 

exp  ?i ?i fi  l  c 

 (1)
where Z  c 
 is a context-dependent normalizing fac-
tor to ensure that p  l 	 c 
 be a proper probability dis-
tribution. We determine weights for the features
with a modified version of the Generative Iterative
Scaling algorithm (Curran and Clark, 2003).
Templates for local features are similar to the ones
employed by Ratnaparkhi (1996) for POS-tagging
(Table 3), though as our input already includes POS-
tags, we can make use of part-of-speech information
as well. Long-distance features are simple hand-
written regular expressions matching possible sites
for EEs (Table 4). Features and labels occurring less
than 10 times in the training corpus are ignored.
Since our main aim is to show that finding empty
elements can be done fairly accurately without us-
ing a parser, the input to the tagger is a POS-tagged
corpus, containing no syntactic information. The
best label-sequence is approximated by a bigram
Viterbi-search algorithm, augmented with variable
width beam-search.
4.2 Results
The results of the EE-detection experiment are sum-
marized in Table 5. The overall unlabeled F-score is
85   3%, whereas the labeled F-score is 79   1%, which
amounts to 97   9% word-level tagging accuracy.
For straightforward comparison with Johnson?s
results, we must conflate the categories PRO?NP and
NP?NP. If the trace detector does not need to differ-
entiate between these two categories, a distinction
that is indeed important for semantic analysis, the
overall labeled F-score increases to 83   0%, which
outperforms Johnson?s approach by 4%.
4.3 Discussion
The success of the trace detector is surprising, es-
pecially if compared to Johnson?s algorithm which
uses the output of a parser. The tagger can reliably
detect extraction sites without explicit knowledge of
the phrase structure. This shows that, in English, ex-
traction can only occur at well-defined sites, where
local cues are generally strong.
Indeed, the strength of the model lies in detecting
such sites (empty units, UNIT; NP traces, NP?NP)
or where clear-cut long-distance cues exist (WH?S,
COMP?SBAR). The accuracy of detecting uncon-
EE Prec. Rec. F-score
Here Here Here Johnson
LABELED 86.5% 72.9% 79.1% ?
UNLABELED 93.3% 78.6% 85.3% ?
NP?NP 87.8% 79.6% 83.5% ?
WH?NP 92.5% 75.6% 83.2% 81.0%
PRO?NP 68.7% 70.4% 69.5% ?
COMP?SBAR 93.8% 78.6% 85.5% 88.0%
UNIT 99.1% 92.5% 95.7% 92.0%
WH?S 94.4% 91.3% 92.8% 87.0%
WH?ADVP 81.6% 46.8% 59.5% 56.0%
CLAUSE 80.4% 68.3% 73.8% 70.0%
COMP?WHNP 67.2% 38.3% 48.8% 47.0%
Table 5: EE-detection results on Section 23 and com-
parison with Johnson (2002) (where applicable).
trolled PROs (PRO?NP) is rather low, since it is a dif-
ficult task to tell them apart from NP traces: they are
confused in 10   15% of the cases. Furthermore, the
model is unable to capture for. . . to+INF construc-
tions if the noun-phrase is long.
The precision of detecting long-distance NP ex-
traction (WH?NP) is also high, but recall is lower:
in general, the model finds extracted NPs with
overt complementizers. Detection of null WH-
complementizers (COMP?WHNP), however, is fairly
inaccurate (48   8% F-score), since finding it and the
corresponding WH?NP requires information about
the transitivity of the verb. The performance of the
model is also low (59   5%) in detecting movement
sites for extracted WH-adverbs (WH?ADVP) despite
the presence of unambiguous cues (where, how, etc.
starting the subordinate clause). The difficulty of the
task lies in finding the correct verb-phrase as well
as the end of the verb-phrase the constituent is ex-
tracted from without knowing phrase boundaries.
One important limitation of the shallow approach
described here is its inability to find the antecedents
of the EEs, which clearly requires knowledge of
phrase structure. In the next section, we show
that the shallow trace detector and the unlexicalized
PCFG parser can be coupled to efficiently and suc-
cessfully tackle antecedent recovery.
Condition NOINSERT INSERT
Antecedent recovery
(F-score) 72   6% 69   3%
Parsing time (sec/sent) 2   7 25
Missed parses 2   4% 5   3%
Table 6: Antecedent recovery, parsing times, and
missed parses for the combined model
5 Combining the models
In Section 3, we found that parsing with EEs is only
feasible if the parser knows the location of EEs be-
fore parsing. In Section 4, we presented a finite-state
tagger which detects these sites before parsing takes
place. In this section, we validate the two-step ap-
proach, by applying the parser to the output of the
trace tagger, and comparing the antecedent recovery
accuracy to Johnson (2002).
5.1 Method
Theoretically, the ?best? way to combine the trace
tagger and the parsing algorithm would be to build a
unified probabilistic model. However, the nature of
the models are quite different: the finite-state model
is conditional, taking the words as given. The pars-
ing model, on the other hand, is generative, treat-
ing the words as an unlikely event. There is a rea-
sonable basis for building the probability models in
different ways. Most of the tags emitted by the EE
tagger are just EE=*, which would defeat genera-
tive models by making the ?hidden? state uninfor-
mative. Conditional parsing algorithms do exist, but
they are difficult to train using large corpora (John-
son, 2001). However, we show that it is quite ef-
fective if the parser simply treats the output of the
tagger as a certainty.
Given this combination method, there still are two
interesting variations: we may use only the EEs
proposed by the tagger (henceforth the NOINSERT
model), or we may allow the parser to insert even
more EEs (henceforth the INSERT model). In both
cases, EEs outputted by the tagger are treated as sep-
arate words, as in the PERFECT model of Section 3.
5.2 Results
The NOINSERT model did better at antecedent de-
tection (see Table 6) than the INSERT model. The
Type Prec. Rec. F-score
Here Here Here Johnson
OVERALL 80.5% 66.0% 72.6% 68.0%
NP?NP 71.2% 62.8% 66.8% 60.0%
WH?NP 91.6% 71.9% 80.6% 80.0%
PRO?NP 68.7% 70.4% 69.5% 50.0%
COMP?SBAR 93.8% 78.6% 85.5% 88.0%
UNIT 99.1% 92.5% 95.7% 92.0%
WH?S 86.7% 83.9% 84.8% 87.0%
WH?ADVP 67.1% 31.3% 42.7% 56.0%
CLAUSE 80.4% 68.3% 73.8% 70.0%
COMP?WHNP 67.2% 38.8% 48.8% 47.0%
Table 7: Antecedent recovery results for the
combined NOINSERT model and comparison with
Johnson (2002).
NOINSERT model was also faster, taking on aver-
age 2.7 seconds per sentence and enumerating about
160,000 edges whereas the INSERT model took 25
seconds on average and enumerated 2 million edges.
The coverage of the NOINSERT model was higher
than that of the INSERT model, missing 2.4% of all
parses versus 5.3% for the INSERT model.
Comparing our results to Johnson (2002), we find
that the NOINSERT model outperforms that of John-
son by 4.6% (see Table 7). The strength of this sys-
tem lies in its ability to tell unbound PROs and bound
NP?NP traces apart.
5.3 Discussion
Combining the finite-state tagger with the parser
seems to be invaluable for EE detection and an-
tecedent recovery. Paradoxically, taking the com-
bination to the extreme by allowing both the parser
and the tagger to insert EEs performed worse.
While the INSERT model here did have wider
coverage than the parser in Section 3, it seems the
real benefit of using the combined approach is to
let the simple model reduce the search space of
the more complicated parsing model. This search
space reduction works because the shallow finite-
state method takes information about adjacent words
into account, whereas the context-free parser does
not, since a phrase boundary might separate them.
6 Related Work
Excluding Johnson (2002)?s pattern-matching al-
gorithm, most recent work on finding head?
dependencies with statistical parser has used statis-
tical versions of deep grammar formalisms, such as
CCG (Clark et al, 2002) or LFG (Riezler et al,
2002). While these systems should, in theory, be
able to handle discontinuities accurately, there has
not yet been a study on how these systems handle
such phenomena overall.
The tagger presented here is not the first one
proposed to recover syntactic information deeper
than part-of-speech tags. For example, supertag-
ging (Joshi and Bangalore, 1994) also aims to do
more meaningful syntactic pre-processing. Unlike
supertagging, our approach only focuses on detect-
ing EEs.
The idea of threading EEs to their antecedents in
a stochastic parser was proposed by Collins (1997),
following the GPSG tradition (Gazdar et al, 1985).
However, we extend it to capture all types of EEs.
7 Conclusions
This paper has three main contributions. First, we
show that gap+ features, encoding necessary infor-
mation for antecedent recovery, do not incur any
substantial computational overhead.
Second, the paper demonstrates that a shallow
finite-state model can be successful in detecting sites
for discontinuity, a task which is generally under-
stood to require deep syntactic and lexical-semantic
knowledge. The results show that, at least in En-
glish, local clues for discontinuity are abundant.
This opens up the possibility of employing shal-
low finite-state methods in novel situations to exploit
non-apparent local information.
Our final contribution, but the one we wish to em-
phasize the most, is that the combination of two or-
thogonal shallow models can be successful at solv-
ing tasks which are well beyond their individual
power. The accent here is on orthogonality ? the two
models take different sources of information into ac-
count. The tagger makes good use of adjacency at
the word level, but is unable to handle deeper re-
cursive structures. A context-free grammar is better
at finding vertical phrase structure, but cannot ex-
ploit linear information when words are separated
by phrase boundaries. As a consequence, the finite-
state method helps the parser by efficiently and re-
liably pruning the search-space of the more compli-
cated PCFG model. The benefits are immediate: the
parser is not only faster but more accurate in recov-
ering antecedents. The real power of the finite-state
model is that it uses information the parser cannot.
Acknowledgements
The authors would like to thank Jason Baldridge,
Matthew Crocker, Geert-Jan Kruijff, Miles Osborne
and the anonymous reviewers for many helpful com-
ments.
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):39?71.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre, 1995. Bracketting Guidelines for Treebank II
style Penn Treebank Project. Linguistic Data Consor-
tium.
Rens Bod. 2003. An efficient implementation of a new
dop model. In Proceedings of the 11th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, Budapest.
Eugene Charniak. 1993. Statistical Language Learning.
MIT Press, Cambridge, MA.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Conference of North
American Chapter of the Association for Computa-
tional Linguistics, Seattle, WA.
Stephen Clark, Julia Hockenmaier, and Mark Steedman.
2002. Building deep dependency structures with a
wide-coverage CCG parser. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, Philadelphia.
Michael Collins, Jan Hajic?, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
Czech. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics, Uni-
versity of Maryland, College Park.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics and the 8th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, Madrid.
James R. Curran and Stephen Clark. 2003. Investigat-
ing GIS and smoothing for maximum entropy taggers.
In Proceedings of the 11th Annual Meeting of the Eu-
ropean Chapter of the Association for Computational
Linguistics, Budapest, Hungary.
Pe?ter Dienes and Amit Dubey. 2003. Antecedent recov-
ery: Experiments with a trace tagger. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, Sapporo, Japan.
Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and Ivan
Sag. 1985. Generalized Phase Structure Grammar.
Basil Blackwell, Oxford, England.
Joshua Goodman. 1998. Parsing inside-out. Ph.D. the-
sis, Harvard University.
Mark Johnson. 2001. Joint and conditional estimation
of tagging and parsing models. In Proceedings of the
39th Annual Meeting of the Association for Computa-
tional Linguistics and the 10th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, Toulouse.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
Philadelphia.
Aravind K. Joshi and Srinivas Bangalore. 1994. Com-
plexity of descriptives?supertag disambiguation or al-
most parsing. In Proceedings of the 1994 Inter-
national Conference on Computational Linguistics
(COLING-94), Kyoto, Japan.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-
of-Speech tagger. In Proceedings of the Empirical
Methods in Natural Language Processing Conference.
University of Pennsylvania.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, Philadelphia.
Antecedent Recovery: Experiments with a Trace Tagger
Pe?ter Dienes and Amit Dubey
Department of Computational Linguistics
Saarland University
PO Box 15 11 50
66041 Saarbru?cken, Germany
{dienes,adubey}@coli.uni-sb.de
Abstract
This paper explores the problem of find-
ing non-local dependencies. First, we
isolate a set of features useful for this
task. Second, we develop both a two-step
approach which combines a trace tagger
with a state-of-the-art lexicalized parser
and a one-step approach which finds non-
local dependencies while parsing. We find
that the former outperforms the latter be-
cause it makes better use of the features
we isolate.
1 Introduction
Many broad-coverage statistical parsers (Charniak,
2000; Collins, 1999; Bod, 2001) are not able to give
a full interpretation for sentences such as:
(1) It is difficult to guess what she wants to buy.
Building the semantic interpretation of this sentence
requires recovering three non-local relations: (i) the
object of buy is what ;1 (ii) the subject of buy is she;
and (iii) guess does not have a subject in the sen-
tence.
Three approaches have been proposed to de-
tect such relations: (i) post-processing the output
of a parser not designed to detect extraction sites
(Johnson, 2002); (ii) integrating antecedent recov-
ery into the parser (henceforth in-processing) by ei-
ther enriching a syntactically simple model (Collins,
1999) or using a more powerful syntactic framework
1Collins (1999) can handle this case (Model 3).
(Clark et al, 2002; Riezler et al, 2002); and (iii) de-
tecting non-local dependencies as a pre-processing
step before parsing (Dienes and Dubey, 2003).
While the pre-processing approach is reported
to give state-of-the-art performance using unlexi-
calized parsers, it has not been tested using lexi-
calized models. Our main claim is that that the
pre-processing approach, coupled with a lexical-
ized parser outperforms both state-of-the-art post-
processing and in-processing. However, we show
that Model 3 of Collins (1999) can be generalized
to handle all types of long-distance dependencies
with performance close to the pre-processing archi-
tecture.
A general contribution of this paper is that it gives
important insights about the nature of the problem.
Recovering non-local semantic relations is regarded
to be a difficult problem. The successes (and fail-
ures) of the simple architecture outlined here help
determine what features are to be incorporated into
a parser in order to improve recovery of non-local
dependencies.
The overall organization of the paper is as fol-
lows. First, Section 2 sketches the material we use
for the experiments in the paper. In Section 3, we
discuss a finite-state system, a trace tagger, that de-
tects extraction sites without knowledge of phrase-
structure and we isolate important cues for the task.
Section 4 combines the trace tagger with a parser in
order to recover antecedents. Finally, in Section 5,
we investigate whether and how detection of extrac-
tion sites and antecedent recovery can be integrated
into a lexicalized stochastic parser.
Type Freq. Explanation Example
NP?NP 987 controlled NP-traces Sam was seen *
WH?NP 438 NP-traces of A
 
-movement the woman who you saw *T*
PRO?NP 426 uncontrolled PROs * to sleep is nice
COMP?SBAR 338 empty complementizer (that) Sam said 0 Sasha snores
UNIT 332 empty units $ 25 *U*
WH?S 228 trace of topicalized sentence Sam had to go, Sasha said *T*
WH?ADVP 120 traces of WH adverbs Sam told us how he did it *T*
CLAUSE 118 trace of a moved SBAR Sam had to go, Sasha said 0
COMP?WHNP 98 empty WH-complementizer the woman 0 we saw *T*
ALL 3310
Table 1: Most frequent types of EEs in Section 0.
2 Data
In the experiments we use the same train-
ing, test, and development data as in
Dienes and Dubey (2003), where non-local de-
pendencies are annotated with the help of empty
elements (EEs) co-indexed with their controlling
constituents (if any). The most frequent types of
EEs are summarized in Table 1. Thus, the example
sentence (1) will get the annotation:
(2) It is difficult PRO-NP to guess what she wants
NP-NP to buy WH-NP.
For the parsing and antecedent recovery exper-
iments, in the case of WH-traces (WH?  ) and
controlled NP-traces (NP?NP), we follow the stan-
dard technique of marking nodes dominating the
empty element up to but not including the par-
ent of the antecedent as defective (missing an ar-
gument) with a gap feature (Gazdar et al, 1985;
Collins, 1999). Furthermore, to make antecedent
co-indexation possible with many types of EEs, we
generalize Collins? approach by enriching the an-
notation of non-terminals with the type of the EE
in question (eg. WH?NP), using different gap+ fea-
tures (gap+WH-NP; c.f. Figure 1). The original non-
terminals augmented with gap+ features serve as
new non-terminal labels. Note, however, that not
all EEs have antecedents. In these cases, the gap+
feature does not show up in the dominating non-
terminal (Figure 2).
3 Detecting empty elements
Previous work (Dienes and Dubey, 2003) shows that
detecting empty elements can be performed fairly
reliably before parsing using a trace tagger, which
tags words with information on EEs immediately
preceding them. For example, the first occurrence
of the word to in our example sentence (2) gets the
tag EE=TT-NP , whereas the word wants is tagged as
having no EE. The trace tagger uses three main types
of features: (i) combination of POS tags in a win-
dow of five words around the EEs; (ii) lexical fea-
tures of the words in a window of three lexical items;
and (iii) long-distance cues (Table 2). An EE is cor-
rectly detected if and only if (i) the label matches
that of the gold standard and (ii) it occurs between
the same words. Dienes and Dubey (2003) report
79  1% labeled F-score on this evaluation metric, the
SBAR
WHNPi
what
S 
	
NP j
she
VP 
		
	
V
wants
S 
		
	
NP 	
	
NP?NP j
VP 
	
TO
to
VP 
	
V
buy
NP 
	
WH-NPi
Figure 1: Threading gap+WH-NP and gap+NP-NP.
SNP
It
VP
V
is
ADJP
ADJ
difficult
S
NP
PRO-NP
VP
TO
to
VP
V
guess
SBAR
Figure 2: Representing EEs without antecedents.
best published result on the EE detection task.
While Dienes and Dubey (2003) report overall
scores, they do not evaluate the relative importance
of the features used by the tagger. This can be
achieved by testing how the model fares if only a
subset of the features are switched on (performance
analysis). Another way to investigate the problem
is to analyze the average weight and the activation
frequency of each feature type.
According to the performance analysis, the most
important features are the ones encoding POS-
information. Indeed, by turning only these features
on, the accuracy of the system is already fairly high:
the labeled F-score is 71  2%. A closer look at
the feature weights shows that the right context is
slightly more informative than the left one. Lex-
icalization of the model contributes further 6% to
the overall score (the following word being slightly
more important than the preceding one), whereas
the features capturing long-distance cues only im-
prove the overall score by around 2%. Interestingly,
long-distance features get higher weights in general,
but their contribution to the overall performance is
small since they are rarely activated. Finally, the
model with only lexical features performs surpris-
ingly well: the labeled F-score is 68  9%, showing
that a very small window already contains valuable
information for the task.
In summary, the most important result here is that
a relatively small window of up to five words con-
tains important cues for detecting EEs.
4 Antecedent recovery
Antecedent recovery requires knowledge of phrase
structure, and hence calls for a parsing component.
In this section, we show how to recover the an-
tecedents given a parse tree, and how to incorporate
information about EE-sites into the parser.
4.1 Antecedent recovery algorithm
The main motivation for the introduction of gap+
variables is that they indicate a path from the EE to
the antecedent. In case of a non-binary-branching
grammar, however, this path only determines the
node immediately dominating the antecedent, but
does not indicate the child the EE should be co-
indexed with. Moreover, a node might contain sev-
eral gap+ variables, which further complicates an-
tecedent recovery, even in the case of perfect trees.
This calls for a sophisticated algorithm to recover
antecedents.
1 foreach gap
2 do find antecedent
 
?gap?  gap  ;
3
4 proc find antecedent
 
?gap?  node 
5 var par  node.parent ;
6 p  # of gap+ features of type ?gap? on par;
7 ch  sum of the gap+ features of type ?gap? on
8 par.children ;
9 node.remove one gap
 
?gap?  ;
10 if p  ch
11 then Drop the gap here
12 ante  leftmost non-adjunct of par.children
13 allowed by ?gap?
 
 node  ;
14 return ante if ante;
15 ante  leftmost child of par.children
16 allowed by ?gap?
 
 node  ;
17 return ante if ante;
18 return nil;
19 else Pass up the tree recursively
20 find antecedent
 
?gap?  par  .
Figure 3: The antecedent recovery algorithm.
The algorithm, presented in Figure 3, runs af-
ter the best parse has been selected. It works in
a bottom-up fashion, and for each empty node the
main recursive function find antecedent is called
separately (lines 1 and 2). At every call, the number
of gap+ variables of type ?gap? are calculated for
the parent par of the current node node (p; line 6)
and for all the children (ch; line 7). If the parent
has at least as many unresolved gap+ variables as
its children, we conclude that the current EE is re-
Target Matching regexp Explanation
NP?NP BE RB* VBN passive
 
NP?NP
PRO-NP 
RB* to RB* VB to-infinitive
N [,:] RB* VBG gerund
COMP?SBAR (V|,) !that* (MD|V) lookahead for that
WH?NP !IN 
  WP
WDT
COMP?WHNP
 

!WH?NP* V lookback for pending WHNPs
WH?ADVP WRB !WH?ADVP* V !WH?ADVP* [.,:] lookback for pending WHADVP before a verb
UNIT $ CD* $ sign before numbers
Table 2: Non-local binary feature templates; the EE-site is indicated by
solved further up in the tree and call the same al-
gorithm for the parent (line 20). If, however, the
parent has fewer unresolved gaps (p  ch), the an-
tecedent of the EE is among the children. Thus the
algorithm attempts to find this antecedent (lines 11?
18). For an antecedent to be selected, the syntactic
category must match, i.e. an NP?NP must resolve to
a NP. The algorithm searches from left to right for a
possible candidate, preferring non-adjuncts over ad-
juncts. The node found (if any) is returned as the
antecedent for the EE. Finally, note that in line 9, we
have to remove the threaded gap+ feature in order to
avoid confusion if the same parent is visited again
while resolving another EE.
Although the algorithm is simple and works in a
greedy manner, it does perform well. Tested on the
gold standard trees containing the empty nodes with-
out antecedent co-reference information, it is able to
recover the antecedents with an F-score of 95% (c.f.
Section 4.3).
4.2 Method
Antecedent recovery is tested using two parsers: an
unlexicalized PCFG (Dienes and Dubey, 2003) and
a lexicalized parser with near state-of-the-art perfor-
mance (Collins, 1999). Both parsers treat EEs as
words. In order to recover antecedents, both were
modified to thread gap+ variables in the nontermi-
nals as described in Section 2.
Each parser is evaluated in two cases: (i) an upper
bound case which uses the perfect EEs of the tree-
bank (henceforth PERFECT) and (ii) a case that uses
EEs suggested by the finite-state mechanism (hence-
forth TAGGER). In the TAGGER case, the parser sim-
ply takes the hypotheses of the finite-state mecha-
nism as true.
Condition Bracketing Antecedent
recovery
PERFECT UNLEX 78.5% 91.4%
LEX 88.6% 93.3%
TAGGER UNLEX 76.3% 72.6%
LEX 86.4% 74.6%
Johnson 89.1% 68.0%
Table 3: F-Scores for parsing and antecedent recov-
ery on Section 23.
4.3 Evaluation
We evaluate on all sentences in the test section of
the treebank. As with trace detection, we use the
measure introduced by Johnson (2002). This metric
works by treating EEs and their antecedents as four-
tuples, consisting of the type of the EE, its location,
the type of its antecedent and the location(s) (begin-
ning and end) of the antecedent. An antecedent is
correctly recovered if all four values match the gold
standard. We calculate the precision, recall, and F-
score; however for brevity?s sake we only report the
F-score for most experiments in this section.
In addition to antecedent recovery, we also re-
port parsing accuracy, using the bracketing F-Score,
the combined measure of PARSEVAL-style labeled
bracketing precision and recall (Magerman, 1995).
4.4 Results
The results of the experiments are summarized in
Table 3. UNLEX and LEX refer to the unlexicalized
and lexicalized models, respectively. In the upper-
bound case, PERFECT, the F-score for antecedent
recovery is quite high in both the unlexicalized and
lexicalized cases: 91.4% and 93.3%.
Type Prec. Rec. F-score
Here Here Here Johnson
OVERALL 81.5% 68.7% 74.6% 68.0%
NP?NP 74.3% 67.4% 70.7% 60.0%
WH?NP 91.0% 74.5% 82.0% 80.0%
PRO?NP 68.7% 70.4% 69.5% 50.0%
COMP?SBAR 93.8% 78.6% 85.5% 88.0%
UNIT 99.1% 92.5% 95.7% 92.0%
WH?S 86.3% 82.8% 84.5% 87.0%
WH?ADVP 74.5% 42.0% 53.6% 56.0%
CLAUSE 80.4% 68.3% 73.8% 70.0%
COMP?WHNP 67.2% 38.3% 48.8% 47.0%
Table 4: Comparison of our antecedent recov-
ery results with the lexicalized parser and John-
son?s (2002).
Johnson (2002)?s metric includes EE without an-
tecedents. To test how well the antecedent-detection
algorithm works, it is useful, however, to count the
results of only those EEs which have antecedents
in the tree (NP?NP, PSEUDO attachments, and all
WH traces). In these cases, the unlexicalized parser
has an F-score of 70.4%, and the lexicalized parser
83.9%, both in the PERFECT case.
In the TAGGER case, which is our main con-
cern, the unlexicalized parser achieves an F-score
of 72.6%, better than the 68.0% reported by
Johnson (2002). The lexicalized parser outperforms
both, yielding results of F-score of 74.6%.
Table 4 gives a closer look at the antecedent
recovery score for some common EE types using
the lexicalized parser, also showing the results of
Johnson (2002) for comparison.
4.5 Discussion
The pre-processing system does quite well, manag-
ing an F-score 6.6% higher than the post-processing
system of Johnson (2002). However, while the lexi-
calized parser performs better than the unlexicalized
one, the difference is quite small: only 2%. This
suggests that many of the remaining errors are actu-
ally in the pre-processor rather than in the parser.
Two particular cases of interest are NP?NPs and
PRO?NPs. In both cases, a NP is missing, often in a
to-infinitival clause. The two are only distinguished
by their antecedent: NP?NP has an antecedent in the
tree, while PRO?NP has none. The lexicalized parser
has, for most types of EEs, quite high antecedent de-
tection results, but the difficulty in telling the differ-
ence between these two cases results in low F-scores
for antecedent recovery of NP?NP and PRO?NP, de-
spite the fact that they are among the most common
EE types. Even though this is a problem, our system
still does quite well: 70.4% for NP?NP, and 69.5%
for PRO?NP compared to the 60.0% and 50.0% re-
ported by Johnson (2002).
Since it appears the pre-processor is the cause
of most of the errors, in-processing with a state-of-
the-art lexicalized parser might outperform the pre-
processing approach. In the next section, we explore
this possibility.
5 Detecting empty elements in the parser
Having compared pre-processing to post-processing
in the previous section, in this section, we consider
the relative advantages of pre-processing as com-
pared to detecting EEs while parsing, with both an
unlexicalized and a lexicalized model.
In making the comparison between detecting EEs
during pre-processing versus parsing, we are not
only concerned with the accuracy of parsing, EE
detection and antecedent recovery, but also with
the running time of the parsers. In particular,
Dienes and Dubey (2003) found that detecting EEs
is infeasible with an unlexicalized parser: the parser
was slow and inaccurate at EE detection.
Recall that the runtime of many parsing algo-
rithms depends on the size of the grammar or the
number of nonterminals. The unlexicalized CYK
parser we use has a worst-case asymptotic runtime
of O   n3N3  where n is the number of words and N
is the number of nonterminals. Collins (1999) re-
ports a worst-case asymptotic runtime of O   n5N3 
for a lexicalized parser.
The O   N3  bound becomes important when the
parser is to insert traces because there are more non-
terminals. Three factors contribute to this larger
nonterminal set: (i) nonterminals are augmented
with EE types that contain the parent node of the
EE (i.e. S may become S 	 , S 
		 , etc.) (ii) we
must include combinations of EEs as nonterminals
may dominate more than one unbound EE (i.e.
S 			  and (iii) a single nonterminal may
be repeated in the presence of co-ordination (i.e.
S 		  		 ). These three factors greatly increase
the number of nonterminals, potentially reducing the
efficiency of a parser that detects EEs. On the other
hand, when EE-sites are pre-determined, the effect
of the number of nonterminals on parsing speed is
moot: the parser can ignore large parts of the gram-
mar.
In this section, we empirically explore the relative
advantages of pre-processing over in-processing,
with respect to runtime efficiency and the accuracy
of parsing and antecedent recovery.
5.1 Method
As in Section 4, we use the unlexicalized parser
from Dienes and Dubey (2003), and as a lexicalized
parser, an extension of Model 3 of Collins (1999).
While Model 3 inserts WH?NP traces, it makes some
assumptions that preclude it from being used here
directly:
(i) it cannot handle multiple types of EEs;
(ii) it does not allow multiple instances of EEs at a
node;
(iii) it expects all EEs to be complements, though
some are not (e.g. WH?ADVP);
(iv) it expects all EEs to have antecedents, though
some do not (e.g. PRO?NP);
(v) it cannot model EEs with dependents, for ex-
ample COMP?. . . .
Hence, Model 3 must be generalized to other
types of discontinuities. In order to handle the
first four problems, we propose generating ?gap-
categorization? frames in the same way as subcat-
egorization frames are used in the original model.
We do not offer a solution to the final problem, as
the syntactic structure (usually the unary production
SBAR  S) will identify these cases.
After calculating the probability of the head
(with its gaps), the left and right gapcat frame are
generated independently of each other (and of the
subcat frames). For example, the probability for the
rule:
VP (to) (+gap=  WH-NP  ) 
TO (to) (+gap=   ) VP (buy) (+gap=  WH-NP  )
Relative # of Relative Missed
Condition Nonterminals Parsing Time Parses
NOTRACE 1.00 1.00 0.2%
WH?NP 1.63 2.17 10.3%
PRO&WH 7.15 3.58 35.1%
TAGGER 7.15 1.49 1.3%
Table 5: INSERT model unlexicalized parsing results
on Section 23.
is generated as:
Ph(TO|VP,to) 
PRGC(

WH-NP  |VP ,TO,to)  PLGC(   |VP,TO,to) 
PRC(

VP-C  |VP,TO,to)  PLC(   |VP,TO,to) 
Pr(VP-CWH-NP(buy)|VP,TO,to,  VP-C  ,  WH-NP  ) 
Pr(STOP|VP,TO,to,   ,   ) 
Pl(STOP|VP,TO,to,   ,   )
Generating the actual EE is done in a similar fash-
ion: the EE cancels the corresponding ?gapcat? re-
quirement. If it is a complement (e.g. WH?NP), it
also removes the corresponding element from the
subcat frame. The original parsing algorithm was
modified to accommodate ?gapcat? requirements and
generate multiple types of EEs.
We compare the parsing performance of the two
parsers in four cases: the NOTRACE model which re-
moves all traces from the test and training data, the
TAGGER model of Section 4, and two cases where
the parser inserts EEs (we will collectively refer to
these cases as the INSERT models). In order to
show the effects of increasing the size of nontermi-
nal vocabulary, the first INSERT model only consid-
ers one EE type, WH?NP while the second (hence-
forth PRO&WH) considers all WH traces as well as
NP?NP and PRO?NP discontinuities.
5.2 Results
The results of the unlexicalized and lexicalized ex-
periments are summarized in Tables 5 and Table 6,
respectively. The tables compare relative pars-
ing time (slowdown with respect to the NOTRACE
model), and in the lexicalized case, PARSEVAL-
style bracketing scores. However, in the case of
the unlexicalized model, the increasing number of
Relative # of Relative Bracketing
Condition Nonterminals Parsing Time
NOTRACE 1.00 1.00 88.0%
WH?NP 1.63 1.07 87.4%
PRO&WH 7.15 1.33 86.6%
TAGGER 7.15 0.95 86.4%
Table 6: INSERT model lexicalized parsing results
on Section 23.
Type EE detection Antecedent rec.
parser tagger parser tagger
NP?NP 80.4% 83.5% 70.3% 70.7%
WH?NP 81.5% 83.2% 80.2% 82.0%
PRO?NP 64.5% 69.5% 64.5% 69.5%
WH?S 92.0% 92.8% 82.2% 84.5%
WH?ADVP 57.9% 59.5% 53.0% 53.6%
Table 7: Comparison of pre-processing with lexical-
ized in-processing (F-scores).
missed parses precludes straightforward comparison
of bracketing scores, therefore we report the per-
centage of sentences where the parser fails. In the
case of the lexicalized parser, less than 1% of the
parses are missed, hence the comparisons are re-
liable. Finally, we compare EE detection and an-
tecedent recovery F-scores of the TAGGER and the
PRO&WH models for the overlapping EE types (Ta-
ble 7).
5.3 Discussion
As noted by Dienes and Dubey (2003), unlexical-
ized parsing with EEs does not seem to be viable
without pre-processing. However, the lexicalized
parser is competitive with the pre-processing ap-
proach.
As for the bracketing scores, there are two inter-
esting results. First, lexicalized models which han-
dle EEs have lower bracketing scores than the NO-
TRACE model. Indeed, as the number of EEs in-
creases, so does the number of nonterminals, which
results in increasingly severe sparse data problem.
Consequently, there is a trade-off between finding
local phrase structure and long-distance dependen-
cies.
Second, comparing the TAGGER and the
PRO&WH models, we find that the bracketing
results are nearly identical. Nonetheless, the
PRO&WH model inserting EEs can match neither
the accuracy for antecedent recovery nor the time
efficiency of the pre-processing approach. Thus,
the results show that treating EE-detection as a pre-
processing step is beneficial to both to antecedent
recovery accuracy and to parsing efficiency.
Nevertheless, pre-processing is not necessarily
the only useful strategy for trace detection. Indeed,
by taking advantage of the insights that make the
finite-state and lexicalized parsing models success-
ful, it may be possible to generalize the results to
other strategies as well. There are two key observa-
tions of importance here.
The first observation is that lexicalization is very
important for detecting traces, not just for the lex-
icalized parser, but, as discussed in Section 3, for
the trace-tagger as well. The two models may con-
tain overlapping information: in many cases, the lex-
ical cue corresponds to the immediate head-word
the EE depends on. However, other surrounding
words (which frequently correspond to the head-
word of grandparent of the empty node) often carry
important information, especially for distinguishing
NP?NP and PRO?NP nodes.
Second, local information (i.e. a window of five
words) proves to be informative for the task. This
explains why the finite-state tagger is more accurate
than the parser: this window always crosses a phrase
boundary, and the parser cannot consider the whole
window.
These two observations give a set of features that
seem to be useful for EE detection. We conjecture
that a parser that takes advantage of these features
might be more accurate in detecting EEs while pars-
ing than the parsers presented here. Apart from the
pre-processing approach presented here, there are a
number of ways these features could be used:
1. in a pre-processing system that only detects
EEs, as we have done here;
2. as part of a larger syntactic pre-processing sys-
tem, such as supertagging (Joshi and Banga-
lore, 1994);
3. with a more informative beam search (Charniak
et al, 1998);
4. or directly integrated into the parsing mecha-
nism, for example by combining the finite-state
and the parsing probability models.
6 Conclusions
One of the main contributions of this paper is that
a two-step pre-processing approach to finding EEs
outperforms both post-processing and in-processing.
We found the pre-processing technique was success-
ful because it used features not explicitly incorpo-
rated into the other models.
Furthermore, we found that the result presented
in Dienes and Dubey (2003), i.e. pre-processing is
better for antecedent recovery than unlexicalized
in-processing, also holds when comparing lexical-
ized models. However, comparing the lexicalized
pre-processing system to the unlexicalized one, we
find that although lexicalization results in much bet-
ter trees, there is only a slight improvement in an-
tecedent recovery.
Third, we present a generalization of Model 3
of Collins (1999) to handle a broader range of EEs.
While this particular model was not able to outper-
form the pre-processing method, it can be further de-
veloped into a parsing model which can handle non-
local dependencies by incorporating the local cues
we found relevant.
In particular, a local window of five words, ac-
companied by the gap+ threads proved to be crucial.
Thus we claim that, in order to detect long-distance
dependencies, a robust stochastic parser should in-
tegrate lexical information as well as local cues cut-
ting across phrase boundaries by either incorporat-
ing them into the probability model or using them in
the beam-search.
Acknowledgements
The authors would like to thank Jason Baldridge,
Matthew Crocker, Geert-Jan Kruijff, Shravan Va-
sishth and the anonymous reviewers for their invalu-
able suggestions and comments.
References
Rens Bod. 2001. What is the minimal set of fragments
that achieves maximal parse accuracy? In Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics and the 10th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, Toulouse, France.
Eugene Charniak, Sharon Goldwater, and Mark Johnson.
1998. Edge-based best-first chart parsing. In Proceed-
ings of the 14th National Conference on Artificial In-
telligence, Madison, WI.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Conference of North
American Chapter of the Association for Computa-
tional Linguistics, Seattle, WA.
Stephen Clark, Julia Hockenmaier, and Mark Steedman.
2002. Building deep dependency structures with a
wide-coverage CCG parser. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, Philadelphia, PA.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Pe?ter Dienes and Amit Dubey. 2003. Deep syntactic pro-
cessing by combining shallow methods. In Proceed-
ings of the 41st Annual Meeting of the Association for
Computational Linguistics, Sapporo, Japan.
Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and Ivan
Sag. 1985. Generalized Phase Structure Grammar.
Basil Blackwell, Oxford, England.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
Philadelphia, PA.
Aravind K. Joshi and Srinivas Bangalore. 1994. Com-
plexity of descriptives?supertag disambiguation or al-
most parsing. In Proceedings of the 1994 Interna-
tional Conference on Computational Linguistics, Ky-
oto, Japan.
David Magerman. 1995. Statistical decision-tree models
for parsing. In Proceedings of the 33rd Annual Meet-
ing of the Association for Computational Linguistics,
Cambridge, MA.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, Philadelphia, PA.
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 827?834, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Parallelism in Coordination as an Instance of Syntactic Priming:
Evidence from Corpus-based Modeling
Amit Dubey and Patrick Sturt and Frank Keller
Human Communication Research Centre, Universities of Edinburgh and Glasgow
2 Buccleuch Place, Edinburgh EH8 9LW, UK
{adubey,sturt,keller}@inf.ed.ac.uk
Abstract
Experimental research in psycholinguis-
tics has demonstrated a parallelism effect
in coordination: speakers are faster at pro-
cessing the second conjunct of a coordi-
nate structure if it has the same internal
structure as the first conjunct. We show
that this phenomenon can be explained by
the prevalence of parallel structures in cor-
pus data. We demonstrate that parallelism
is not limited to coordination, but also ap-
plies to arbitrary syntactic configurations,
and even to documents. This indicates that
the parallelism effect is an instance of a
general syntactic priming mechanism in
human language processing.
1 Introduction
Experimental work in psycholinguistics has pro-
vided evidence for the so-called parallelism prefer-
ence effect: speakers processes coordinated struc-
tures more quickly when the two conjuncts have
the same internal syntactic structure. The processing
advantage for parallel structures has been demon-
strated for a range coordinate constructions, includ-
ing NP coordination (Frazier et al, 2000), sentence
coordination (Frazier et al, 1984), and gapping and
ellipsis (Carlson, 2002; Mauner et al, 1995).
The parallelism preference in NP coordination
can be illustrated using Frazier et al?s (2000) Exper-
iment 3, which recorded subjects? eye-movements
while they read sentences like (1):
(1) a. Terry wrote a long novel and a short poem
during her sabbatical.
b. Terry wrote a novel and a short poem dur-
ing her sabbatical
Total reading times for the underlined region were
faster in (1-a), where short poem is coordinated with
a syntactically parallel noun phrase (a long novel),
compared to (1-b), where it is coordinated with a
syntactically non-parallel phrase.
These results raise an important question that the
present paper tries to answer through corpus-based
modeling studies: what is the mechanism underlying
the parallelism preference? One hypothesis is that
the effect is caused by low-level processes such as
syntactic priming, i.e., the tendency to repeat syntac-
tic structures (e.g., Bock, 1986). Priming is a very
general mechanism that can affect a wide range of
linguistic units, including words, constituents, and
semantic concepts. If the parallelism effect is an in-
stance of syntactic priming, then we expect it to ap-
ply to a wide range of syntactic construction, and
both within and between sentences. Previous work
has demonstrated priming effects in corpora (Gries,
2005; Szmrecsanyi, 2005); however, these results
are limited to instances of priming that involve a
choice between two structural alternatives (e.g., da-
tive alternation). In order to study the parallelism ef-
fect, we need to model priming as general syntac-
tic repetition (independent of the structural choices
available). This is what the present paper attempts.
Frazier and Clifton (2001) propose an alternative
account of the parallelism effect in terms of a copy-
ing mechanism. Unlike priming, this mechanism is
highly specialized and only applies to coordinate
structures: if the second conjunct is encountered,
then instead of building new structure, the language
processor simply copies the structure of the first con-
junct; this explains why a speed-up is observed if
the second conjunct is parallel to the first one. If
the copying account is correct, then we would ex-
pect parallelism effects to be restricted to coordinate
structures and would not apply in other contexts.
In the present paper, we present corpus evidence
that allows us to distinguish between these two com-
peting explanations. Our investigation will proceed
as follows: we first establish that there is evidence
827
for a parallelism effect in corpus data (Section 3).
This is a crucial prerequisite for our wider inves-
tigation: previous work has only dealt with paral-
lelism in comprehension, hence we need to establish
that parallelism is also present in production data,
such as corpus data. We then investigate whether
the parallelism effect is restricted to coordination, or
whether it also applies also arbitrary syntactic con-
figurations. We also test if parallelism can be found
for larger segments of text, including, in the limit,
the whole document (Section 4). Then we investi-
gate parallelism in dialog, testing the psycholinguis-
tic prediction that parallelism in dialog occurs be-
tween speakers (Section 5). In the next section, we
discuss a number of methodological issues and ex-
plain the way we measure parallelism in corpus data.
2 Adaptation
Psycholinguistic studies have shown that priming
affects both speech production (Bock, 1986) and
comprehension (Branigan et al, 2005). The impor-
tance of comprehension priming has also been noted
by the speech recognition community (Kuhn and
de Mori, 1990), who use so-called caching language
models to improve the performance of speech com-
prehension software. The concept of caching lan-
guage models is quite simple: a cache of recently
seen words is maintained, and the probability of
words in the cache is higher than those outside the
cache.
While the performance of caching language mod-
els is judged by their success in improving speech
recognition accuracy, it is also possible to use an
abstract measure to diagnose their efficacy more
closely. Church (2000) introduces such a diagnostic
for lexical priming: adaptation probabilities. Adap-
tation probabilities provide a method to separate the
general problem of priming from a particular imple-
mentation (i.e., caching models). They measure the
amount of priming that occurs for a given construc-
tion, and therefore provide an upper limit for the per-
formance of models such as caching models.
Adaptation is based upon three concepts. First is
the prior, which serves as a baseline. The prior mea-
sures the probability of a word appearing, ignoring
the presence or absence of a prime. Second is the
positive adaptation, which is the probability of a
word appearing given that it has been primed. Third
is the negative adaptation, the probability of a word
appearing given it has not been primed.
In Church?s case, the prior and adaptation prob-
abilities are estimated as follows. If a corpus is di-
vided into individual documents, then each docu-
ment is then split in half. We refer to the halves as the
prime set (or prime half) and the target set (or target
half).1 We measure how frequently a document half
contains a particular word. For each word w, there
are four combinations of the prime and target halves
containing the word. This gives us four frequencies
to measure, which are summarized in the following
table:
fwp,t fwp?,t
fwp,?t fwp?,?t
These frequencies represent:
fwp,t = # of times w occurs in prime set
and target set
fwp?,t = # of times w occurs in target set
but not prime set
fwp,?t = # of times w occurs in prime set
but not target set
fwp?,?t = # of times w does not occur in either
target set or prime set
In addition, let N represent the sum of these four
frequencies. From the frequencies, we may formally
define the prior, positive adaptation and negative
adaptation:
Prior Pprior(w) =
fwp,t + fw p?,t
N
(1)
Positive Adaptation P+(w) =
fwp,t
fwp,t + fwp,?t
(2)
Negative Adaptation P?(w) =
fw p?,t
fw p?,t+ fw p?,?t
(3)
In the case of lexical priming, Church observes that
P+  Pprior > P?. In fact, even in cases when Pprior
quite small, P+ may be higher than 0.8. Intuitively,
a positive adaptation which is higher than the prior
entails that a word is likely to reappear in the target
set given that it has already appeared in the prime
set. We intend to show that adaptation probabilities
provide evidence that syntactic constructions behave
1Our terminology differs from that of Church, who uses ?his-
tory? to describe the first half, and ?test? to describe the second.
Our terms avoid the ambiguity of the phrase ?test set? and coin-
cide with the common usage in the psycholinguistic literature.
828
similarity to lexical priming, showing positive adap-
tation P+ greater than the prior. As P? must become
smaller than Pprior whenever P+ is larger than Pprior,
we only report the positive adaptation P+ and the
prior Pprior.
While Church?s technique was developed with
speech recognition in mind, we will show that
it is useful for investigating psycholinguistic phe-
nomenon. However, the connection between cogni-
tive phenomenon and engineering approaches go in
both directions: it is possible that syntactic parsers
could be improved using a model of syntactic prim-
ing, just as speech recognition has been improved
using models of lexical priming.
3 Experiment 1: Parallelism in
Coordination
In this section, we investigate the use of Church?s
adaptation metrics to measure the effect of syntac-
tic parallelism in coordinated constructions. For the
sake of comparison, we restrict our study to several
constructions used in Frazier et al (2000). All of
these constructions occur in NPs with two coordi-
nate sisters, i.e., constructions such as NP1 CC NP2,
where CC represents a coordinator such as and.
3.1 Method
The application of the adaptation metric is straight-
forward: we pick NP1 as the prime set and NP2 as
the target set. Instead of measuring the frequency of
lexical elements, we measure the frequency of the
following syntactic constructions:
SBAR An NP with a relative clause, i.e.,
NP ? NP SBAR.
PP An NP with a PP modifier, i.e., NP ? NP PP.
NN An NP with a single noun, i.e., NP ? NN.
DT NN An NP with a determiner and a noun, i.e.,
NP ? DT NN.
DT JJ NN An NP with a determiner, an adjective
and a noun, i.e., NP ? DT JJ NN.
Parameter estimation is accomplished by iterating
through the corpus for applications of the rule NP
? NP CC NP. From each rule application, we create
a list of prime-target pairs. We then estimate adap-
tation probabilities for each construction, by count-
ing the number of prime-target pairs in which the
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 1: Adaptation within coordinate structures in
the Brown corpus
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 2: Adaptation within coordinate structures in
the WSJ corpus
construction does or does not occur. This is done
similarly to the document half case described above.
There are four frequencies of interest, but now they
refer to the frequency that a particular construction
(rather than a word) either occurs or does not occur
in the prime and target set.
To ensure results were general across genres, we
used all three parts of the English Penn Treebank:
the Wall Street Journal (WSJ), the balanced Brown
corpus of written text (Brown) and the Switchboard
corpus of spontaneous dialog. In each case, we use
the entire corpus.
Therefore, in total, we report 30 probabilities: the
prior and positive adaptation for each of the five con-
structions in each of the three corpora. The primary
objective is to observe the difference between the
prior and positive adaptation for a given construction
in a particular corpus. Therefore, we also perform a
?2 test to determine if the difference between these
two probabilities are statistically significant.
829
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 3: Adaptation within coordinate structures in
the Switchboard corpus
3.2 Results
The results are shown in Figure 1 for the Brown cor-
pus, Figure 2 for the WSJ and Figure 3 for Switch-
board. Each figure shows the prior and positive
adaptation for all five constructions: relative clauses
(SBAR) a PP modifier (PP), a single common noun
(N), a determiner and noun (DT N), and a determiner
adjective and noun (DT ADJ N). Only in the case of
a single common noun in the WSJ and Switchboard
corpora is the prior probability higher than the posi-
tive adaptation. In all other cases, the probability of
the given construction is more likely to occur in NP2
given that it has occurred in NP1. According to the
?2 tests, all differences between priors and positive
adaptations were significant at the 0.01 level. The
size of the data sets means that even small differ-
ences in probability are statistically significant. All
differences reported in the remainder of this paper
are statistically significant; we omit the details of in-
dividual ?2 tests.
3.3 Discussion
The main conclusion we draw is that the parallelism
effect in corpora mirrors the ones found experimen-
tally by Frazier et al (2000), if we assume higher
probabilities are correlated with easier human pro-
cessing. This conclusion is important, as the experi-
ments of Frazier et al (2000) only provided evidence
for parallelism in comprehension data. Corpus data,
however, are production data, which means that the
our findings are first ones to demonstrate parallelism
effects in production.
The question of the relationship between compre-
hension and production data is an interesting one.
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 4: Adaptation within sentences in the Brown
corpus
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 5: Adaptation within sentences in the WSJ
corpus
We can expect that production data, such as corpus
data, are generated by speakers through a process
that involves self-monitoring. Written texts (such as
the WSJ and Brown) involve proofreading and edit-
ing, i.e., explicit comprehension processes. Even the
data in a spontaneous speech corpus such as Swtich-
board can be expected to involve a certain amount
of self-monitoring (speakers listen to themselves and
correct themselves if necessary). It follows that it is
not entirely unexpected that similar effects can be
found in both comprehension and production data.
4 Experiment 2: Parallelism in Documents
The results in the previous section showed that
the parallelism effect, which so far had only been
demonstrated in comprehension studies, is also at-
tested in corpora, i.e., in production data. In the
present experiment, we will investigate the mech-
anisms underlying the parallelism effect. As dis-
cussed in Section 1, there are two possible explana-
830
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 6: Adaptation between sentences in the
Brown corpus
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 7: Adaptation between sentences in the WSJ
corpus
tion for the effect: one in terms of a construction-
specific copying mechanism, and one in terms of
a generalized syntactic priming mechanism. In the
first case, we predict that the parallelism effect is re-
stricted to coordinate structures, while in the second
case, we expect that parallelism (a) is independent of
coordination, and (b) occurs in the wider discourse,
i.e., not only within sentences but also between sen-
tences.
4.1 Method
The method used was the same as in Experiment 1
(see Section 3.1), with the exception that the prime
set and the target set are no longer restricted to
being the first and second conjunct in a coordi-
nate structure. We investigated three levels of gran-
ularity: within sentences, between sentences, and
within documents. Within-sentence parallelism oc-
curs when the prime NP and the target NP oc-
cur within the same sentence, but stand in an ar-
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 8: Adaptation within documents in the Brown
corpus (all items exhibit weak yet statistically signif-
icant positive adaptation)
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 9: Adaptation within documents in the WSJ
corpus
bitrary structural relationship. Coordinate NPs were
excluded from this analysis, so as to make sure that
any within-sentence parallelism is not confounded
coordination parallelism as established in Experi-
ment 1. Between-sentence parallelism was measured
by regarding as the target the sentence immediately
following the prime sentence. In order to investi-
gate within-document parallelism, we split the doc-
uments into equal-sized halves; then the adaptation
probability was computed by regarding the first half
as the prime and the second half as the target (this
method is the same as Church?s method for measur-
ing lexical adaptation).
The analyses were conducted using the Wall
Street Journal and the Brown portion of the Penn
Treebank. The document boundary was taken to be
the file boundary in these corpora. The Switchboard
corpus is a dialog corpus, and therefore needs to
be treated differently: turns between speakers rather
831
than sentences should be level of analysis. We will
investigate this separately in Experiment 3 below.
4.2 Results
The results for the within-sentence analysis are
graphed in Figures 4 and 5 for the Brown and WSJ
corpus, respectively. We find that there is a paral-
lelism effect in both corpora, for all the NP types
investigated. Figures 6?9 show that the same is true
also for the between-sentence and within-document
analysis: parallelism effects are obtained for all NP
types and for both corpora, even it the parallel struc-
tures occur in different sentences or in different doc-
ument halves. (The within-document probabilities
for the Brown corpus (in Figure 8) are close to one
in most cases; the differences between the prior and
adaptation are nevertheless significant.)
In general, note that the parallelism effects un-
covered in this experiment are smaller than the
effect demonstrated in Experiment 1: The differ-
ences between the prior probabilities and the adap-
tation probabilities (while significant) are markedly
smaller than those uncovered for parallelism in co-
ordinate structure.2
4.3 Discussion
This experiment demonstrated that the parallelism
effect is not restricted to coordinate structures.
Rather, we found that it holds across the board: for
NPs that occur in the same sentence (and are not part
of a coordinate structure), for NPs that occur in ad-
jacent sentences, and for NPs that occur in differ-
ent document halves. The between-sentence effect
has been demonstrated in a more restricted from by
Gries (2005) and Szmrecsanyi (2005), who investi-
gate priming in corpora for cases of structural choice
(e.g., between a dative object and a PP object for
verbs like give). The present results extend this find-
ing to arbitrary NPs, both within and between sen-
tences.
The fact that parallelism is a pervasive phe-
nomenon, rather than being limited to coordinate
structures, strongly suggests that it is an instance of
a general syntactic priming mechanism, which has
been an established feature of accounts of the human
sentence production system for a while (e.g., Bock,
2The differences between the priors and adaptation proba-
bilities are also much smaller than noted by Church (2000). The
probabilities of the rules we investigate have a higher marginal
probability than the lexical items of interest to Church.
1986). This runs counter to the claims made by Fra-
zier et al (2000) and Frazier and Clifton (2001), who
have argued that parallelism only occurs in coordi-
nate structures, and should be accounted for using a
specialized copying mechanism. (It is important to
bear in mind, however, that Frazier et al only make
explicit claims about comprehension, not about pro-
duction.)
However, we also found that parallelism effects
are clearly strongest in coordinate structures (com-
pare the differences between prior and adaptation
in Figures 1?3 with those in Figures 4?9). This
could explain why Frazier et al?s (2000) experi-
ments failed to find a significant parallelism effect
in non-coordinated structures: the effect is simply
too week to detect (especially using the self-paced
reading paradigm they employed).
5 Experiment 3: Parallelism in
Spontaneous Dialog
Experiment 1 showed that parallelism effects can be
found not only in written corpora, but also in the
Switchboard corpus of spontaneous dialog. We did
not include Switchboard in our analysis in Experi-
ment 2, as this corpus has a different structure from
the two text corpora we investigated: it is organized
in terms of turns between two speakers. Here, we
exploit this property and conduct a further experi-
ment in which we compare parallelism effects be-
tween speakers and within speakers.
The phenomenon of structural repetition between
speakers has been discussed in the experimental
psycholinguistic literature (see Pickering and Gar-
rod 2004 for a review). According to Pickering
and Garrod (2004), the act of engaging in a dia-
log facilitates the use of similar representations at
all linguistic levels, and these representations are
shared between speech production and comprehen-
sion processes. Thus structural adaptation should be
observed in a dialog setting, both within and be-
tween speakers. An alternative view is that produc-
tion and comprehension processes are distinct. Bock
and Loebell (1990) suggest that syntactic priming
in speech production is due to facilitation of the
retrieval and assembly procedures that occur dur-
ing the formulation of utterances. Bock and Loebell
point out that this production-based procedural view
predicts a lack of priming between comprehension
and production or vice versa, on the assumption that
832
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 10: Adaptation between speakers in the
Switchboard corpus
production and parsing use distinct mechanisms. In
our terms, it predicts that between-speaker positive
adaptation should not be found, because it can only
result from priming from comprehension to produc-
tion, or vice versa. Conversely, the prodedural view
outlined by Bock and Loebell predicts that positive
adaptation should be found within a given speaker?s
dialog turns, because such adaptation can indeed be
the result of the facilitation of production routines
within a given speaker.
5.1 Method
We created two sets of prime and target data to
test within-speaker and between-speaker adaptation.
The prime and target sets were defined in terms of
pairs of utterances. To test between-speaker adapta-
tion, we took each adjacent pair of utterances spo-
ken by speaker A and speaker B, in each dialog, and
these were treated as prime and target sets respec-
tively. In the within-speaker analysis, the prime and
target sets were taken from the dialog turns of only
one speaker?we took each adjacent pair of dialog
turns uttered by a given speaker, excluding the in-
tervening utterance of the other speaker. The earlier
utterance of the pair was treated as the prime, and
the later utterance as the target. The remainder of
the method was the same as in Experiments 1 and 2
(see Section 3.1).
5.2 Results
The results for the between-speaker and within-
speaker adaptation are shown in Figure 10 and Fig-
ure 11 for same five phrase types as in the previous
experiments.
PP SBAR N DT N DT ADJ N0
0.5
1
Pr
ob
ab
ili
ty
Prior
Adaptation
Figure 11: Adaptation within speakers in the Switch-
board corpus
A positive adaptation effect can be seen in the
between-speaker data. For each phrase type, the
adaptation probability is greater than the prior. In the
within-speaker data, by comparison, the magnitude
of the adaptation advantage is greatly decreased, in
comparison with Figure 10. Indeed, for most phrase
types, the adaptation probability is lower than the
prior, i.e., we have a case of negative adaptation.
5.3 Discussion
The results of the two analyses confirm that adap-
tation can indeed be found between speakers in di-
alog, supporting the results of experimental work
reviewed by Pickering and Garrod (2004). The re-
sults do not support the notion that priming is due
to the facilitation of production processes within a
given speaker, an account which would have pre-
dicted adaptation within speakers, but not between
speakers.
The lack of clear positive adaptation effects in
the within-speaker data is harder to explain?all
current theories of priming would predict some ef-
fect here. One possibility is that such effects may
have been obscured by decay processes: doing a
within-speaker analysis entails skipping an interven-
ing turn, in which priming effects were lost. We in-
tend to address these concerns using more elaborate
experimental designs in future work.
6 Conclusions
In this paper, we have demonstrated a robust, perva-
sive effect of parallelism for noun phrases. We found
the tendency for structural repetition in two different
corpora of written English, and also in a dialog cor-
833
pus. The effect occurs in a wide range of contexts:
within coordinate structures (Experiment 1), within
sentences for NPs in an arbitrary structural config-
uration, between sentences, and within documents
(Experiment 2). This strongly indicates that the par-
allelism effect is an instance of a general processing
mechanism, such as syntactic priming (Bock, 1986),
rather than specific to coordination, as suggested
by (Frazier and Clifton, 2001). However, we also
found that the parallelism effect is strongest in co-
ordinate structures, which could explain why com-
prehension experiments so far failed to demonstrate
the effect for other structural configurations (Frazier
et al, 2000). We leave it to future work to explain
why adaptation is much stronger in co-ordination:
is co-ordination special because of extra constrains
(i.e., some kind of expected contrast/comparison be-
tween co-ordinate sisters) or because of fewer con-
straints (i.e., both co-ordinate sisters have a similar
grammatical role in the sentence)?
Another result (Experiment 3) is that the paral-
lelism effect occurs between speakers in dialog. This
finding is compatible with Pickering and Garrod?s
(2004) interactive alignment model, and strengthens
the argument for parallelism as an instance of a gen-
eral priming mechanism.
Previous experimental work has found parallelism
effects, but only in comprehension data. The present
work demonstrates that parallelism effects also oc-
cur in production data, which raises an interesting
question of the relationship between the two data
types. It has been hypothesized that the human lan-
guage processing system is tuned to mirror the prob-
ability distributions in its environment, including the
probabilities of syntactic structures (Mitchell et al,
1996). If this tuning hypothesis is correct, then the
parallelism effect in comprehension data can be ex-
plained as an adaptation of the human parser to the
prevalence of parallel structures in its environment
(as approximated by corpus data) that we demon-
strated in this paper.
Note that the results in this paper not only have an
impact on theoretical issues regarding human sen-
tence processing, but also on engineering problems
in natural language processing, e.g., in probabilistic
parsing. To avoid sparse data problems, probabilistic
parsing models make strong independence assump-
tions; in particular, they generally assume that sen-
tences are independent of each other. This is partly
due to the fact it is difficult to parameterize the many
possible dependencies which may occur between
adjacent sentences. However, in this paper, we show
that structure re-use is one possible way in which
the independence assumption is broken. A simple
and principled approach to handling structure re-use
would be to use adaptation probabilities for prob-
abilistic grammar rules, analogous to cache proba-
bilities used in caching language models (Kuhn and
de Mori, 1990). We are currently conducting further
experiments to investigate of the effect of syntactic
priming on probabilistic parsing.
References
Bock, J. Kathryn. 1986. Syntactic persistence in language pro-
duction. Cognitive Psychology 18:355?387.
Bock, Kathryn and Helga Loebell. 1990. Framing sentences.
Cognition 35(1):1?39.
Branigan, Holly P., Marin J. Pickering, and Janet F. McLean.
2005. Priming prepositional-phrase attachment during com-
prehension. Journal of Experimental Psychology: Learning,
Memory and Cognition 31(3):468?481.
Carlson, Katy. 2002. The effects of parallelism and prosody on
the processing of gapping structures. Language and Speech
44(1):1?26.
Church, Kenneth W. 2000. Empirical estimates of adaptation:
the chance of two Noriegas is closer to p/2 than p2. In Pro-
ceedings of the 17th Conference on Computational Linguis-
tics. Saarbru?cken, Germany, pages 180?186.
Frazier, Lyn, Alan Munn, and Chuck Clifton. 2000. Processing
coordinate structures. Journal of Psycholinguistic Research
29(4):343?370.
Frazier, Lyn, Lori Taft, Tom Roeper, Charles Clifton, and Kate
Ehrlich. 1984. Parallel structure: A source of facilitation in
sentence comprehension. Memory and Cognition 12(5):421?
430.
Frazier, Lynn and Charles Clifton. 2001. Parsing coordinates
and ellipsis: Copy ?. Syntax 4(1):1?22.
Gries, Stefan T. 2005. Syntactic priming: A corpus-based ap-
proach. Journal of Psycholinguistic Research 35.
Kuhn, Roland and Renate de Mori. 1990. A cache-based natural
language model for speech recognition. IEEE Transanctions
on Pattern Analysis and Machine Intelligence 12(6):570?
583.
Mauner, Gail, Michael K. Tanenhaus, and Greg Carlson. 1995.
A note on parallelism effects in processing deep and surface
verb-phrase anaphors. Language and Cognitive Processes
10:1?12.
Mitchell, Don C., Fernando Cuetos, Martin M. B. Corley, and
Marc Brysbaert. 1996. Exposure-based models of human
parsing: Evidence for the use of coarse-grained (non-lexical)
statistical records. Journal of Psycholinguistic Research
24(6):469?488.
Pickering, Martin J. and Simon Garrod. 2004. Toward a mech-
anistic psychology of dialogue. Behavioral and Brain Sci-
ences 27(2):169?225.
Szmrecsanyi, Benedikt. 2005. Creatures of habit: A corpus-
linguistic analysis of persistence in spoken English. Corpus
Linguistics and Linguistic Theory 1(1):113?149.
834
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 304?312,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Model of Discourse Predictions in Human Sentence Processing
Amit Dubey and Frank Keller and Patrick Sturt
Human Communication Research Centre, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
{amit.dubey,frank.keller,patrick.sturt}@ed.ac.uk
Abstract
This paper introduces a psycholinguistic
model of sentence processing which combines
a Hidden Markov Model noun phrase chun-
ker with a co-reference classifier. Both mod-
els are fully incremental and generative, giv-
ing probabilities of lexical elements condi-
tional upon linguistic structure. This allows
us to compute the information theoretic mea-
sure of surprisal, which is known to correlate
with human processing effort. We evaluate
our surprisal predictions on the Dundee corpus
of eye-movement data show that our model
achieve a better fit with human reading times
than a syntax-only model which does not have
access to co-reference information.
1 Introduction
Recent research in psycholinguistics has seen a
growing interest in the role of prediction in sentence
processing. Prediction refers to the fact that the hu-
man sentence processor is able to anticipate upcom-
ing material, and that processing is facilitated when
predictions turn out to be correct (evidenced, e.g.,
by shorter reading times on the predicted word or
phrase). Prediction is presumably one of the factors
that contribute to the efficiency of human language
understanding. Sentence processing is incremental
(i.e., it proceeds on a word-by-word basis); there-
fore, it is beneficial if unseen input can be antici-
pated and relevant syntactic and semantic structure
constructed in advance. This allows the processor to
save time and makes it easier to cope with the con-
stant stream of new input.
Evidence for prediction has been found in a range
of psycholinguistic processing domains. Semantic
prediction has been demonstrated by studies that
show anticipation based on selectional restrictions:
listeners are able to launch eye-movements to the
predicted argument of a verb before having encoun-
tered it, e.g., they will fixate an edible object as soon
as they hear the word eat (Altmann and Kamide,
1999). Semantic prediction has also been shown in
the context of semantic priming: a word that is pre-
ceded by a semantically related prime or by a seman-
tically congruous sentence fragment is processed
faster (Stanovich and West, 1981; Clifton et al,
2007). An example for syntactic prediction can be
found in coordinate structures: readers predict that
the second conjunct in a coordination will have the
same syntactic structure as the first conjunct (Fra-
zier et al, 2000). In a similar vein, having encoun-
tered the word either, readers predict that or and a
conjunct will follow it (Staub and Clifton, 2006).
Again, priming studies corroborate this: Compre-
henders are faster at naming words that are syntacti-
cally compatible with prior context, even when they
bear no semantic relationship to it (Wright and Gar-
rett, 1984).
Predictive processing is not confined to the sen-
tence level. Recent experimental results also provide
evidence for discourse prediction. An example is the
study by van Berkum et al (2005), who used a con-
text that made a target noun highly predictable, and
found a mismatch effect in the ERP (event-related
brain potential) when an adjective appeared that was
inconsistent with the target noun. An example is (we
give translations of their Dutch materials):
(1) The burglar had no trouble locating the secret
family safe.
a. Of course, it was situated behind a
304
bigneu but unobtrusive paintingneu.
b. Of course, it was situated behind a
bigcom but unobtrusive bookcasecom.
Here, the adjective big, which can have neutral or
common gender in Dutch, is consistent with the pre-
dicted noun painting in (1-a), but inconsistent with it
in (1-b), leading to a mismatch ERP on big in (1-b)
but not in (1-a).
Previous results on discourse effects in sentence
processing can also be interpreted in terms of pre-
diction. In a classical paper, Altmann and Steed-
man (1988) demonstrated that PP-attachment pref-
erences can change through discourse context: if the
context contains two potential referents for the tar-
get NP, then NP-attachment of a subsequent PP is
preferred (to disambiguate between the two refer-
ents), while if the context only contains one target
NP, VP-attachment is preferred (as there is no need
to disambiguate). This result (and a large body of
related findings) is compatible with an interpretation
in which the processor predicts upcoming syntactic
attachment based on the presence of referents in the
preceding discourse.
Most attempts to model prediction in human lan-
guage processing have focused on syntactic pre-
diction. Examples include Hale?s (2001) surprisal
model, which relates processing effort to the con-
ditional probability of the current word given the
previous words in the sentence. This approach has
been elaborated by Demberg and Keller (2009) in a
model that explicitly constructs predicted structure,
and includes a verification process that incurs ad-
ditional processing cost if predictions are not met.
Recent work has attempted to integrate semantic
and discourse prediction with models of syntactic
processing. This includes Mitchell et al?s (2010)
approach, which combines an incremental parser
with a vector-space model of semantics. However,
this approach only provides a loose integration of
the two components (through simple addition of
their probabilities), and the notion of semantics used
is restricted to lexical meaning approximated by
word co-occurrences. At the discourse level, Dubey
(2010) has proposed a model that combines an incre-
mental parser with a probabilistic logic-based model
of co-reference resolution. However, this model
does not explicitly model discourse effects in terms
of prediction, and again only proposes a loose in-
tegration of co-reference and syntax. Furthermore,
Dubey?s (2010) model has only been tested on two
experimental data sets (pertaining to the interaction
of ambiguity resolution with context), no broad cov-
erage evaluation is available.
The aim of the present paper is to overcome these
limitations. We propose a computational model that
captures discourse effects on syntax in terms of pre-
diction. The model comprises a co-reference com-
ponent which explicitly stores discourse mentions
of NPs, and a syntactic component which adjust
the probabilities of NPs in the syntactic structure
based on the mentions tracked by the discourse com-
ponent. Our model is HMM-based, which makes
it possible to efficiently process large amounts of
data, allowing an evaluation on eye-tracking cor-
pora, which has recently become the gold-standard
in computational psycholinguistics (e.g., Demberg
and Keller 2008; Frank 2009; Boston et al 2008;
Mitchell et al 2010).
The paper is structured as follows: In Section 2,
we describe the co-reference and the syntactic mod-
els and evaluate their performance on standard data
sets. Section 3 presents an evaluation of the overall
model on the Dundee eye-tracking corpus. The pa-
per closes with a comparison with related work and
a general discussion in Sections 4 and 5.
2 Model
This model utilises an NP chunker based upon a hid-
den Markov model (HMM) as an approximation to
syntax. Using a simple model such as an HMM fa-
cilitates the integration of a co-reference component,
and the fact that the model is generative is a prereq-
uisite to using surprisal as our metric of interest (as
surprisal require the computation of prefix probabil-
ities). The key insight in our model is that human
sentence processing is, on average, facilitated when
a previously-mentioned discourse entity is repeated.
This facilitation depends upon keeping track of a list
of previously-mentioned entities, which requires (at
the least) shallow syntactic information, yet the fa-
cilitation itself is modeled primarily as a lexical phe-
nomenon. This allows a straightforward separation
of concerns: shallow syntax is captured using the
HMM?s hidden states, whereas the co-reference fa-
305
cilitation is modeled using the HMM?s emissions.
The vocabulary of hidden states is described in Sec-
tion 2.1 and the emission distribution in Section 2.2
2.1 Syntactic Model
A key feature of the co-reference component of our
model (described below) is that syntactic analysis
and co-reference resolution happen simultaneously.
This could potentially slow down the syntactic anal-
ysis, which tends to already be quite slow for ex-
haustive surprisal-based incremental parsers. There-
fore, rather than using full parsing, we use an HMM-
based NP chunker which allows for a fast analysis.
NP chunking is sufficient to extract NP discourse
mentions and, as we show below, surprisal values
computed using HMM chunks provide a useful fit
on the Dundee eye-movement data.
To allow the HMM to handle possessive construc-
tions as well as NP with simple modifiers and com-
plements, the HMM decodes NP subtrees with depth
of 2, by encoding the start, middle and end of a
syntactic category X as ?(X?, ?X? and ?X)?, respec-
tively. To reduce an explosion in the number of
states, the category begin state ?(X? only appears at
the rightmost lexical token of the constituent?s left-
most daughter. Likewise, ?X)? only appears at the
leftmost lexical token of the constituent?s rightmost
daughter. An example use of this state vocabulary
can be seen in Figure 1. Here, a small degree of re-
cursion allows for the NP ((new york city?s) general
obligation fund) to be encoded, with the outer NP?s
left bracket being ?announced? at the token ?s, which
is the rightmost lexical token of the inner NP. Hid-
den states also include part-of-speech (POS) tags,
allowing simultaneous POS tagging. In the exam-
ple given in Figure 1, the full state can be read by
listing the labels written above a word, from top to
bottom. For example, the full state associated with
?s is (NP-NP)-POS. As ?s can also be a contraction
of is, another possible state for ?s is VBZ (without
recursive categories as we are only interested in NP
chunks).
The model uses unsmoothed bi-gram transition
probabilities, along with a maximum entropy dis-
tribution to guess unknown word features. The re-
sulting distribution has the form P(tag|word) and is
therefore unsuitable for computing surprisal values.
However, using Bayes? theorem we can compute:
P(word|tag) = P(tag|word)P(word)P(tag) (1)
which is what we need for surprisal. The pri-
mary information from this probability comes from
P(tag|word), however, reasonable estimates of
P(tag) and P(word) are required to ensure the prob-
ability distribution is proper. P(tag) may be esti-
mated on a parsed treebank. P(word), the probabil-
ity of a particular unseen word, is difficult to esti-
mate directly. Given that our training data contains
approximately 106 words, we assume that this prob-
ability must be bounded above by 10?6. As an ap-
proximation, we use this upper bound as the proba-
bility of P(word).
Training The chunker is trained on sections 2?
22 of the Wall Street Journal section of the Penn
Treebank. CoNLL 2000 included chunking as a
shared task, and the results are summarized by Tjong
Kim Sang and Buchholz (2000). Our chunker is not
comparable to the systems in the shared task for sev-
eral reasons: we use more training data, we tag si-
multaneously (the CoNLL systems used gold stan-
dard tags) and our notion of a chunk is somewhat
more complex than that used in CoNLL. The best
performing chunker from CoNLL 2000 achieved an
F-score of 93.5%, and the worst performing system
an F-score of 85.8%. Our chunker achieves a com-
parable F-score of 85.5%, despite the fact that it si-
multaneously tags and chunks, and only uses a bi-
gram model.
2.2 Co-Reference Model
In a standard HMM, the emission probabilities are
computed as P(wi|si) where wi is the ith word and si
is the ith state. In our model, we replace this with a
choice between two alternatives:
P(wi|si) =
{ ?Pseen before(wi|si)
(1??)Pdiscourse new(wi|si) (2)
The ?discourse new? probability distribution is the
standard HMM emission distribution. The ?seen be-
fore? distribution is more complicated. It is in part
based upon caching language models. However, the
contents of the cache are not individual words but
306
(NP NP NP NP)
(NP NP) (NP NP NP NP) NP (NP NP NP)
JJ NN IN NNP NNP NNP POS JJ NN NNS VBN RP DT NN NN
strong demand for new york city ?s general obligation bonds propped up the municipal market
Figure 1: The chunk notation of a tree from the training data.
Variable Type
l, l? List of trie nodes
w,wi Words
t Tag
n,n? Trie nodes
l? List(root of mention trie)
for w? w0 to wn do
l?? l
l? /0
Clear tag freq array f t
Clear word freq array f wt
for t ? tag set do
for n ? l? do
f t(t)? f t(t)+FreqO f (n, t)
n?? Getchild(w, t)
if n? 6= /0 then
f wt(t)? f wt(t)+FreqO f (n?,w, t)
l? n? :: l
end if
end for
end for
Pseen before(w|t) = f t(t)/ f wt(t)
end for
Figure 2: Looking up entries from the NP Cache
rather a collection of all NPs mentioned so far in the
document.
Using a collection of NPs rather than individual
words complicates the decoding process. If m is the
size of a document, and n is the size of the current
sentence, decoding occurs in O(mn) time as opposed
to O(n), as the collection of NPs needs to be ac-
cessed at each word. However, we do not store the
NPs in a list, but rather a trie. This allows decoding
to occur in O(n logm) time, which we have found
to be quite fast in practise. The algorithm used to
keep track of currently active NPs is presented in
Figure 2. This shows how the distribution Pseen before
is updated on a word-by-word basis. At the end of
each sentence, the NPs of the Viterbi parse are added
to the mention trie after having their leading arti-
cles stripped. A weakness of the algorithm is that
mentions are only added on a sentence-by-sentence
basis (disallowing within-sentence references). Al-
though the algorithm is intended to find whole-string
matches, in practise, it will count any NP whose pre-
fix matches as being co-referent.
A consequence of Equation 2 is that co-reference
resolution is handled at the same time as HMM de-
coding. Whenever the ?seen before? distribution is
applied, an NP is co-referent with one occurring ear-
lier. Likewise, whenever the ?discourse new? dis-
tribution is applied, the NP is not co-referent with
any NP appearing previously. As one choice or the
other is made during decoding, the decoder there-
fore also selects a chain of co-referent entities. Gen-
erally, for words which have been used in this dis-
course, the magnitude of probabilities in the ?seen
before? distribution are much higher than in the ?dis-
course new? distribution. Thus, there is a strong
bias to classify NPs which match word-for-word as
being co-referent. There remains a possibility that
the model primarily captures lexical priming, rather
than co-reference. However, we note that string
match is a strong indicator of two NPs being corefer-
307
ent (cf. Soon et al 2001), and, moreover, the match-
ing is done on an NP-by-NP basis, which is more
suitable for finding entity coreference, rather than a
word-by-word basis, which would be more suitable
for lexical priming.
An appealing side-effect of using a simple co-
reference decision rule which is applied incremen-
tally is that it is relatively simple to incremen-
tally compute the transitive closure of co-reference
chains, resulting in the entity sets which are then
used in evaluation.
The co-reference model only has one free param-
eter, ?, which is estimated from the ACE-2 corpus.
The estimate is computed by counting how often a
repeated NP actually is discourse new. In the current
implementation of the model, ? is constant through-
out the test runs. However, ? could possibly be
a function of the previous discourse, allowing for
more complicated classification probabilities.
3 Evaluation
3.1 Data
Our evaluation experiments were conducted upon
the Dundee corpus (Kennedy et al, 2003), which
contains the eye-movement record of 10 participants
each reading 2,368 sentences of newspaper text.
This data set has previously been used by Demberg
and Keller (2008) and Frank (2009) among others.
3.2 Evaluation
Eye tracking data is noisy for a number of rea-
sons, including the fact that experimental partici-
pants can look at any word which is currently dis-
played. While English is normally read in a left-
to-right manner, readers often skip words or make
regressions (i.e., look at a word to the left of the
one they are currently fixating). Deviations from
a strict left-to-right progression of fixations moti-
vate the need for several different measures of eye
movement. The model presented here predicts the
Total Time that participants spent looking at a re-
gion, which includes any re-fixations after looking
away. In addition to total time, other possible mea-
sures include (a) First Pass, which measures the ini-
tial fixation and any re-fixations before looking at
any other word (this occurs, for instance, if the eye
initially lands at the start of a long word ? the eye
will tend to re-fixate on a more central viewing lo-
cation), (b) Right Bounded reading time, which in-
cludes all fixations on a word before moving to the
right of the word (i.e., re-fixations after moving left
are included), and (c) Second Pass, which includes
any re-fixation on a word after looking at any other
word (be it to the left or the right of the word of inter-
est). We found that the model performed similarly
across all these reading time metrics, we therefore
only report results for Total Time.
As mentioned above, reading measures are hy-
pothesised to correlate with Surprisal, which is de-
fined as:
S(wt) =? log(P(wt |w1...wt1) (3)
We compute the surprisal scores for the syntax-only
HMM, which does not have access to co-reference
information (henceforth referred to as ?HMM?)
and the full model, which combines the syntax-
only HMM with the co-reference model (henceforth
?HMM+Ref?). To determine if our Dundee corpus
simulations provide a reasonable model of human
sentence processing, we perform a regression anal-
ysis with the Dundee corpus reading time measure
as the dependent variable and the surprisal scores as
the independent variable.
To account for noise in the corpus, we also use
a number of additional explanatory variables which
are known to strongly influence reading times.
These include the logarithm of the frequency of a
word (measured in occurrences per million) and the
length of a word in letters. Two additional explana-
tory variables were available in the Dundee corpus,
which we also included in the regression model.
These were the position of a word on a line, and
which line in a document a word appeared in. As
participants could only view one line at a time (i.e.,
one line per screen), these covariates are known as
line position and screen position, respectively.
All the covariates, including the surprisal es-
timates, were centered before including them in
the regression model. Because the HMM and
HMM+Ref surprisal values are highly collinear, the
HMM+Ref surprisal values were added as residuals
of the HMM surprisal values.
In a normal regression analysis, one must either
assume that participants or the particular choice of
308
items add some randomness to the experiment, and
either each participant?s responses for all items must
be averaged (treating participants as a random fac-
tor), or all participant?s responses for each item is
averaged (treating items as a random factor). How-
ever, in the present analysis we utilise a mixed ef-
fects model, which allows both items and partici-
pants to be treated as random factors.1
The are a number of criteria which can be used
to test the efficacy of one regression model over an-
other. These include the Aikake Information Cri-
terion (AIC), the Bayesian Information Criterion
(BIC), which trade off model fit and number of
model parameters (lower scores are better). It is also
common to compare the log-likelihood of the mod-
els (higher log-likelihood is better), in which case a
?2 can be used to evaluate if a model offers a sig-
nificantly better fit, given the number of parameters
is uses. We test three models: (i) a baseline, with
only low-level factors as independent variables; (ii)
the HMM model, with the baseline factors plus sur-
prisal computed by the syntax-only HMM; and (iii)
the HMM+Ref model which includes the raw sur-
prisal values of the syntax-only HMM and the sur-
prisal of the HMM+Ref models as computed as a
residual of the HMM surprisal score. We compare
the HMM and HMM+Ref to the baseline, and the
HMM+Ref model against the HMM model.
Some of the data needed to be trimmed. If, due to
data sparsity, the surprisal of a word goes to infinity
for one of the models, we entirely remove that word
from the analysis. This occurred seven times form
the HMM+Ref model, but did not occur at all with
the HMM model. Some of the eye-movement data
was trimmed, as well. Fixations on the first and last
words of a line were excluded, as were tracklosses.
However, we did not trim any items due to abnor-
1We assume that each participant and item bias the reading
time of the experiment. Such an analysis is known as having
random intercepts of participant and item. It is also possible
to assume a more involved analysis, known as random slopes,
where the participants and items bias the slope of the predictor.
The model did not converge when using random intercept and
slopes on both participant and item. If random slopes on items
were left out, the HMM regression model did converge, but not
the HMM+Ref model. As the HMM+Ref is the model of inter-
est random slopes were left out entirely to allow a like-with-like
comparison between the HMM and HMM+Ref regression mod-
els.
mally short or abnormally long fixation durations.
3.3 Results
The result of the model comparison on Total Time
reading data is summarised in Table 1. To allow this
work to be compared with other models, the lower
part of the table gives the abosolute AIC, BIC and
log likelihood of the baseline model, while the upper
part gives delta AIC, BIC and log likelihood scores
of pairs of models.
We found that both the HMM and HMM+Ref
provide a significantly better fit with the reading
time data than the Baseline model; all three crite-
ria agree: AIC and BIC lower than for the base-
line, and log-likelihood is higher. Moreover, the
HMM+Ref model provides a significantly better fit
than the HMM model, which demonstrates the bene-
fit of co-reference information for modeling reading
times. Again, all three measures provide the same
result.
Table 2 corroborates this result. It list the
mixed-model coefficients for the HMM+Ref model
and shows that all factors are significant predic-
tors, including both HMM surprisal and residualized
HMM+Ref surprisal.
4 Related Work
There have been few computational models of hu-
man sentence processing that have incorporated
a referential or discourse-level component. Niv
(1994) proposed a parsing model based on Com-
binatory Categorial Grammar (Steedman, 2001), in
which referential information was used to resolve
syntactic ambiguities. The model was able to cap-
ture effects of referential information on syntactic
garden paths (Altmann and Steedman, 1988). This
model differs from that proposed in the present pa-
per, as it is intended to capture psycholinguistic pref-
erences in a qualitative manner, whereas the aim
of the present model is to provide a quantitative
fit to measures of processing difficulty. Moreover,
the model was not based on a large-scale grammar,
and was not tested on unrestricted text. Spivey and
Tanenhaus (1998) proposed a sentence processing
model that examined the effects of referential infor-
mation, as well as other constraints, on the resolu-
tion of ambiguous sentences. Unlike Niv (1994),
309
From To ? AIC ? BIC ? logLik ?2 Significance
Baseline HMM -80 -69 41 82.112 p < .001
Baseline HMM+Ref -99 -89 51 101.54 p < .001
HMM HMM+Ref -19 -8 11 21.424 p < .001
Model AIC BIC logLik
Baseline 10567789 10567880 -5283886
Table 1: Model comparison (upper part) and absolute scores for the Baseline model (lower part)
Coefficient Estimate Std Error t-value
(Intercept) 991.4346 23.7968 41.66
log(Word Frequency) -55.3045 1.4830 -37.29
Word Length 128.6216 1.4677 87.63
Screen Position -1.7769 0.1326 -13.40
Line Position 10.1592 0.7387 13.75
HMM 12.1287 1.3366 9.07
HMM+Ref 19.2772 4.1627 4.63
Table 2: Coefficients of the HMM+Ref model on Total Reading Times. Note that t > 2 indicates that the factor in
question is a significant predictor.
Spivey and Tanenhaus?s (1998) model was specifi-
cally designed to provide a quantitative fit to reading
times. However, the model lacked generality, being
designed to deal with only one type of sentence. In
contrast to both of these earlier models, the model
proposed here aims to be general enough to provide
estimated reading times for unrestricted text. In fact,
as far as we are aware, the present paper represents
the first wide-coverage model of human parsing that
has incorporated discourse-level information.
5 Discussion
The primary finding of this work is that incorporat-
ing discourse information such as co-reference into
an incremental probabilistic model of sentence pro-
cessing has a beneficial effect on the ability of the
model to predict broad-coverage human parsing be-
haviour.
Although not thoroughly explored in this paper,
our finding is related to an ongoing debate about the
structure of the human sentence processor. In par-
ticular, the model of Dubey (2010), which also sim-
ulates the effect of discourse on syntax, is aimed at
examining interactivity in the human sentence pro-
cessor. Interactivity describes the degree to which
human parsing is influenced by non-syntactic fac-
tors. Under the weakly interactive hypothesis, dis-
course factors may prune or re-weight parses, but
only when assuming the strongly interactive hypoth-
esis would we argue that the sentence processor pre-
dicts upcoming material due to discourse factors.
Dubey found that a weakly interactive model sim-
ulated a pattern of results in an experiment (Grodner
et al, 2005) which was previously believed to pro-
vide evidence for the strongly interactive hypothesis.
However, as Dubey does not provide broad-coverage
parsing results, this leaves open the possibility that
the model cannot generalise beyond the experiments
expressly modeled in Dubey (2010).
The model presented here, on the other hand,
is not only broad-coverage but could also be de-
scribed as a strongly interactive model. The strong
interactivity arises because co-reference resolution
is strongly tied to lexical generation probabilities,
which are part of the syntactic portion of our model.
This cannot be achieve in a weakly interactive
model, which is limited to pruning or re-weighting
of parses based on discourse information. As our
analysis on the Dundee corpus showed, the lexical
probabilities (in the form of HMM+Ref surprisal)
are key to improving the fit on eye-tracking data.
We therefore argue that our results provide evidence
310
against a weakly interactive approach, which may be
sufficient to model individual phenomena (as shown
by Dubey 2010), but is unlikely to be able to match
the broad-coverage result we have presented here.
We also note that psycholinguistic evidence for dis-
course prediction (such as the context based lexi-
cal prediction shown by van Berkum et al 2005,
see Section 1) is also evidence for strong interac-
tivity; prediction goes beyond mere pruning or re-
weighting and requires strong interactivity.
References
Gerry Altmann and Mark Steedman. Interaction
with context during human sentence processing.
Cognition, 30:191?238, 1988.
Gerry T. M. Altmann and Yuki Kamide. Incremen-
tal interpretation at verbs: Restricting the domain
of subsequent reference. Cognition, 73:247?264,
1999.
Marisa Ferrara Boston, John T. Hale, Reinhold
Kliegl, and Shravan Vasisht. Surprising parser
actions and reading difficulty. In Proceedings of
ACL-08:HLT, Short Papers, pages 5?8, 2008.
Charles Clifton, Adrian Staub, and Keith Rayner.
Eye movement in reading words and sentences.
In R V Gompel, M Fisher, W Murray, and R L
Hill, editors, Eye Movements: A Window in Mind
and Brain, pages 341?372. Elsevier, 2007.
Vera Demberg and Frank Keller. Data from eye-
tracking corpora as evidence for theories of syn-
tactic processing complexity. Cognition, 109:
192?210, 2008.
Vera Demberg and Frank Keller. A computational
model of prediction in human parsing: Unifying
locality and surprisal effects. In Proceedings of
the 29th meeting of the Cognitive Science Society
(CogSci-09), 2009.
Amit Dubey. The influence of discourse on syntax:
A psycholinguistic model of sentence processing.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL
2010), Uppsala, Sweden, 2010.
Stefan Frank. Surprisal-based comparison between
a symbolic and a connectionist model of sentence
processing. In 31st Annual Conference of the
Cognitive Science Society (COGSCI 2009), Ams-
terdam, The Netherlands, 2009.
Lyn Frazier, Alan Munn, and Charles Clifton. Pro-
cessing coordinate structure. Journal of Psy-
cholinguistic Research, 29:343?368, 2000.
Daniel J. Grodner, Edward A. F. Gibson, and Du-
ane Watson. The influence of contextual constrast
on syntactic processing: Evidence for strong-
interaction in sentence comprehension. Cogni-
tion, 95(3):275?296, 2005.
John T. Hale. A probabilistic earley parser as a psy-
cholinguistic model. In In Proceedings of the Sec-
ond Meeting of the North American Chapter of
the Asssociation for Computational Linguistics,
2001.
A. Kennedy, R. Hill, and J. Pynte. The dundee cor-
pus. In Proceedings of the 12th European confer-
ence on eye movement, 2003.
Jeff Mitchell, Mirella Lapata, Vera Demberg, and
Frank Keller. Syntactic and semantic factors in
processing difficulty: An integrated measure. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, Uppsala,
Sweden, 2010.
M. Niv. A psycholinguistically motivated parser for
CCG. In Proceedings of the 32nd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL-94), pages 125?132, Las Cruces, NM,
1994.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. A ma-
chine learning approach to coreference resolution
of noun phrases. Computational Linguistics, 27
(4):521?544, 2001.
M. J. Spivey and M. K. Tanenhaus. Syntactic am-
biguity resolution in discourse: Modeling the ef-
fects of referential context and lexical frequency.
Journal of Experimental Psychology: Learning,
Memory and Cognition, 24(6):1521?1543, 1998.
Kieth E. Stanovich and Richard F. West. The effect
of sentence context on ongoing word recognition:
Tests of a two-pricess theory. Journal of Exper-
imental Psychology: Human Perception and Per-
formance, 7:658?672, 1981.
Adrian Staub and Charles Clifton. Syntactic predic-
tion in language comprehension: Evidence from
311
either . . .or. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 32:425?436,
2006.
Mark Steedman. The Syntactic Process. Bradford
Books, 2001.
Erik F. Tjong Kim Sang and Sabine Buchholz. In-
troduction to the conll-2000 shared task: Chunk-
ing. In Proceedings of CoNLL-2000 and LLL-
2000, pages 127?132. Lisbon, Portugal, 2000.
Jos J. A. van Berkum, Colin M. Brown, Pienie Zwit-
serlood, Valesca Kooijman, and Peter Hagoort.
Anticipating upcoming words in discourse: Evi-
dence from erps and reading times. Journal of Ex-
perimental Psychology: Learning, Memory and
Cognition, 31(3):443?467, 2005.
Barton Wright and Merrill F. Garrett. Lexical deci-
sion in sentences: Effects of syntactic structure.
Memory and Cognition, 12:31?45, 1984.
312
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1179?1188,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
The Influence of Discourse on Syntax
A Psycholinguistic Model of Sentence Processing
Amit Dubey
Abstract
Probabilistic models of sentence com-
prehension are increasingly relevant to
questions concerning human language
processing. However, such models are of-
ten limited to syntactic factors. This paper
introduces a novel sentence processing
model that consists of a parser augmented
with a probabilistic logic-based model
of coreference resolution, which allows
us to simulate how context interacts with
syntax in a reading task. Our simulations
show that a Weakly Interactive cognitive
architecture can explain data which had
been provided as evidence for the Strongly
Interactive hypothesis.
1 Introduction
Probabilistic grammars have been found to be
useful for investigating the architecture of the
human sentence processing mechanism (Jurafsky,
1996; Crocker and Brants, 2000; Hale, 2003;
Boston et al, 2008; Levy, 2008; Demberg and
Keller, 2009). For example, probabilistic models
shed light on so-called locality effects: contrast
the non-probabilistic hypothesis that dependants
which are far away from their head always cause
processing difficulty for readers due to the cost
of storing the intervening material in memory
(Gibson, 1998), compared to the probabilistic
prediction that there are cases when faraway
dependants facilitate processing, because readers
have more time to predict the head (Levy, 2008).
Using a computational model to address funda-
mental questions about sentence comprehension
motivates the work in this paper.
So far, probabilistic models of sentence pro-
cessing have been largely limited to syntactic
factors. This is unfortunate because many out-
standing questions in psycholinguistics concern
interactions between different levels of process-
ing. This paper addresses this gap by building
a computational model which simulates the
influence of discourse on syntax.
Going beyond the confines of syntax alone is a
sufficiently important problem that it has attracted
attention from other authors. In the literature on
probabilistic modeling, though, the bulk of this
work is focused on lexical semantics (e.g. Pado?
et al, 2006; Narayanan and Jurafsky, 1998) or
only considers syntactic decisions in the preceed-
ing text (e.g. Dubey et al, 2009; Levy and Jaeger,
2007). This is the first model we know of which
introduces a broad-coverage sentence processing
model which takes the effect of coreference and
discourse into account.
A major question concerning discourse-syntax
interactions involves the strength of communica-
tion between discourse and syntactic information.
The Weakly Interactive (Altmann and Steed-
man, 1988) hypothesis states that a discourse
context can reactively prune syntactic choices
that have been proposed by the parser, whereas
the Strongly Interactive hypothesis posits that
context can proactively suggest choices to the
syntactic processor.
Support for Weak Interaction comes from
experiments in which there are temporary ambi-
guities, or garden paths, which cause processing
difficulty. The general finding is that supportive
contexts can reduce the effect of the garden path.
However, Grodner et al (2005) found that sup-
portive contexts even facilitate the processing of
unambiguous sentences. As there are no incorrect
analyses to prune in unambiguous structures, the
authors claimed their results were not consistent
with the Weakly Interactive hypothesis, and
suggested that their results were best explained by
a Strongly Interactive processor.
The model we present here implements the
Weakly Interactive hypothesis, but we will show
that it can nonetheless successfully simulate the
results of Grodner et al (2005). There are three
main parts of the model: a syntactic processor,
a coreference resolution system, and a simple
pragmatics processor which computes certain
limited forms of discourse coherence. Following
Hale (2001) and Levy (2008), among others, the
syntactic processor uses an incremental proba-
bilistic Earley parser to compute a metric which
correlates with increased reading difficulty. The
coreference resolution system is implemented
1179
in a probabilistic logic known as Markov Logic
(Richardson and Domingos, 2006). Finally, the
pragmatics processing system contains a small set
of probabilistic constraints which convey some
intuitive facts about discourse processing. The
three components form a pipeline, where each part
is probabilistically dependent on the previous one.
This allows us to combine all three into a single
probability for each reading of an input sentence.
The rest of the paper is structured as follows. In
Section 2, we discuss the details two experiments
showing support of the Weakly and Strongly In-
teractive hypotheses: we discuss Grodner et al?s
result on unambiguous syntactic structures and we
present a new experiment on involving a garden
path which was designed to be similar to the Grod-
ner et al experiment. Section 3 introduces techni-
cal details of model, and Section 4 shows the pre-
dictions of the model on the experiments discussed
in Section 2. Finally, we discuss the theoretical
consequences of these predictions in Section 5.
2 Cognitive Experiments
2.1 Discourse and Ambiguity Resolution
There is a fairly large literature on garden path
experiments involving context (Crain and Steed-
man, 1985; Mitchell et al, 1992, ibid). The
experiments by Altmann and Steedman (1988)
involved PP attachment ambiguity. Other authors
(e.g. Spivey and Tanenhaus, 1998) have used
reduced relative clause attachment ambiguity. In
order to be more consistent with the design of the
experiment in Section 2.2, however, we performed
our own reading-time experiment which partially
replicated previous results.1
The experimental items all had a target sen-
tence containing a relative clause, and one of two
possible context sentences, one of which supports
the relative clause reading and the other which
does not.
The context sentence was one of:
(1) a. There were two postmen, one of
whom was injured and carried by
paramedics, and another who was
unhurt.
b. Although there was a medical emer-
gency at the post office earlier today,
regular mail delivery was unaffected.
1This experiment was previously reported by Dubey et al
(2010).
The target sentences, which were drawn from the
experiment of McRae et al (1998), were either
the reduced or unreduced sentences similar to:
(2) The postman who was carried by the
paramedics was having trouble breathing.
The reduced version of the sentence is produced
by removing the words who was. We measured
reading times in the underlined region, which is
the first point at which there is evidence for the
relative clause interpretation. The key evidence is
given by the word ?by?, but the previous word is in-
cluded as readers often do not fixate on short func-
tion words, but rather process them while overtly
fixating on the previous word (Rayner, 1998).
The relative clauses in the target sentence act as
restrictive relative clauses, selecting one referent
from a larger set. The target sentences are there-
fore more coherent in a context where a restricted
set and a contrast set are easily available, than one
in which these sets are absent. This makes the
context in Example (1-a) supportive of a reduced
relative reading, and the context in Example (1-b)
unsupportive of a reduced relative clause. Other
experiments, for instance Spivey and Tanenhaus
(1998), used an unsupportive context where only
one postman was mentioned. Our experiments
used a neutral context, where no postmen are
mentioned, to be more similar to the Grodner et
al. experiment, as described below.
Overall, there were 28 items, and 28 partici-
pants read these sentences using an EyeLink II
eyetracker. Each participant read items one at a
time, with fillers between subsequent items so as
to obfuscate the nature of the experiment.
Results An ANOVA revealed that all conditions
with a supportive context were read faster than one
with a neutral context (i.e. a main effect of con-
text), and all conditions with unambiguous syntax
were read faster than those with a garden path
(i.e. a main effect of ambiguity). Finally, there
was a statisically significant interaction between
syntax and discourse whereby context decreases
reading times much more when a garden path is
present compared to an unambiguous structure. In
other words, a supportive context helped reduce
the effect of a garden path. This is the prediction
made by both the Weakly Interactive and Strongly
Interactive hypothesis. The pattern of results are
shown in Figure 2a in Section 4, where they are
directly compared to the model results.
1180
2.2 Discourse and Unambiguous Syntax
As mentioned in the Introduction, Grodner et al
(2005) proposed an experiment with a supportive
or unsupportive discourse followed by an unam-
biguous target sentence. In their experiment, the
target sentence was one of the following:
(3) a. The director that the critics praised
at a banquet announced that he was
retiring to make room for young
talent in the industry.
b. The director, who the critics praised
at a banquet, announced that he was
retiring to make room for young
talent in the industry.
They also manipulated the context, which was
either supportive of the target, or a null context.
The two supportive contexts are:
(4) a. A group of film critics praised a
director at a banquet and another
director at a film premiere.
b. A group of film critics praised a
director and a producer for lifetime
achievement.
The target sentence in (3-a) is a restrictive
relative clause, as in the garden path exper-
iments. However, the sentence in (3-b) is a
non-restrictive relative clause, which does not
assume the presence of a constrast set. Therefore,
the context (4-a) is only used with the restrictive
relative clause, and the context (4-b), where only
one director is mentioned, is used as the context
for the non-restrictive relative clause. In the
conditions with a null context, the target sentence
was not preceded by any contextual sentence.
Results Grodner et al measured residual
reading times, i.e. reading times compared to
a baseline in the embedded subject NP (?the
critics?). They found that the supportive contexts
decreased reading time, and that this effect was
stronger for restrictive relatives compared to non-
restricted relatives. As there was no garden path,
and hence no incorrect structure for the discourse
processor to prune, the authors conclude that this
must be evidence for the Strongly Interactive
hypothesis. Unlike the garden path experiment
above, these results do not appear to be consistent
with a Weakly Interactive model. We plot their
results in Figure 3a in Section 4, where they are
S
NP
NP
The postman
VP
VBD
carried
PP
IN
by
NP-LGS
The paramedics
VP
. . .
(a) Standard WSJ Tree
S
NP
NPbase
The postman
VP-LGS
VBD1
carried
PP:by
IN:by
by
NPbase-LGS
The paramedics
VP
. . .
(b) Minimally Modified Tree
Figure 1: A schematic representation of the smallest set of
grammar transformations which we found were required to
accurately parse the experimental items.
directly compared to the model results. Because
these results are computed as regressions against a
baseline, a reading time of 0ms indicates average
difficulty, with negative numbers showing some
facilitation has occured, and positive number
indicating reading difficulty.
3 Model
The model comprises three parts: a parser, a
coreference resolution system, and a pragmatics
subsystem. Let us look at each individually.
3.1 Parser
The parser is an incremental unlexicalized proba-
bilistic Earley parser, which is capable of comput-
ing prefix probabilities. A PCFG parser outputs
the generative probability Pparser(w, t), where w is
the text and t is a parse tree. A probabilistic Earley
parser can retrieve all possible derivations at word
i (Stolcke, 1995), allowing us to compute the prob-
ability P (wi . . . w0) =
?
t Pparser(wi . . . w0, t).
Using the prefix probability, we can compute
the word-by-word Surprisal (Hale, 2001), by
taking the log ratio of the previous word?s prefix
probability against this word?s prefix probability:
log
(
P (wi?1 . . . w0)
P (wi . . . w0)
)
(1)
Higher Surprisal scores are interpreted as
1181
being correlated with more reading difficulty, and
likewise lower scores with greater reading ease.
For most of the remainder of the paper we will
simply refer to the prefix probability at word i as
P (w). While the prefix probability as presented
here is suitable for syntax-based computations, a
main technical contribution of our model, detailed
in Sections 3.2 and 3.3 below, is that we include
non-syntactic probabilities in the computation of
Surprisal.
As per Hale?s original suggestion, our parser
can compute Surprisal using an exhaustive search,
which entails summing over each licensed deriva-
tion. This can be done efficiently using the packed
representation of an Earley chart. However, as
the coreference processor takes trees as input, we
must therefore unpack parses before resolving
referential ambiguity. Given the ambiguity of our
grammar, this is not tractable. Therefore, we only
consider an n-best list when computing Surprisal.
As other authors have found that a relatively small
set of analyses can give meaningful predictions
(Brants and Crocker, 2000; Boston et al, 2008),
we set n = 10.
The parser is trained on the Wall Street Journal
(WSJ) section of the Penn treebank. Unfortu-
nately, the standard WSJ grammar is not able to
give correct incremental parses to our experimen-
tal items. We found we could resolve this problem
by using four simple transformations, which are
shown in Figure 1: (i) adding valency information
to verb POS tags (e.g. VBD1 represents a tran-
sitive verb); (ii) we lexicalize ?by? prepositions;
(iii) VPs containing a logical subject (i.e. the
agent), get the -LGS label; (iv) non-recursive
NPs are renamed NPbase (the coreference system
treats each NPbase as a markable).
3.2 Discourse Processor
The primary function of the discourse processing
module is to perform coreference resolution for
each mention in an incrementally processed text.
Because each mention in a coreference chains is
transitive, we cannot use a simple classifier, as
they cannot enforce global transitivity constraints.
Therefore, this system is implemented in Markov
Logic (Richardson and Domingos, 2006), a
probabilistic logic, which does allow us to include
such constraints.
Markov Logic attempts to combine logic
with probabilities by using a Markov random
field where logical formulas are features. The
Expression Meaning
Coref (x , y) x is coreferent with y.
First(x ) x is a first mention.
Order(x , y) x occurs before y.
SameHead(x , y)
Do x and y share the
same syntactic head?
ExactMatch(x , y) x and y are same string.
SameNumber(x , y) x and y match in number.
SameGender(x , y) x and y match in gender.
SamePerson(x , y) x and y match in person.
Distance(x , y , d)
The distance between
x and y, in sentences.
Pronoun(x ) x is a pronoun.
EntityType(x , e)
x has entity type e
(person, organization, etc.)
Table 1: Predicates used in the Markov Logic Network
Markov Logic Network (MLN) we used for our
system uses similar predicates as the MLN-based
corference resolution system of Huang et al
(2009).2 Our MLN uses the predicates listed
in Table 1. Two of these predicates, Coref and
First , are the output of the MLN ? they provide
a labelling of coreference mentions into entity
classes. Note that, unlike Huang et al, we assume
an ordering on x and y if Coref (x , y) is true: y
must occur earlier in the document than x. The
remaining predicates in Table 1 are a subset of
features used by other coreference resolution
systems (cf. Soon et al, 2001). The predicates
we use involve matching strings (checking if two
mentions share a head word or if they are exactly
the same string), matching argreement features (if
the gender, number or person of pairs of NPs are
the same; especially important for pronouns), the
distance between mentions, and if mentions have
the same entity type (i.e. do they refer to a person,
organization, etc.) As our main focus is not to
produce a state-of-the-art coreference system, we
do not include predicates which are irrevelant for
our simulations even if they have been shown to be
effective for coreference resolution. For example,
we do not have predicates if two mentions are in
an apposition relationship, or if two mentions are
synonyms for each other.
Table 2 lists the actual logical formulae which
are used as features in the MLN. It should be
2As we are not interested in unsupervised inference, the
system of Poon and Domingos (2008) was unsuitable for our
needs.
1182
Description Rule
Transitivity
Coref (x , z ) ? Coref (y , z ) ?Order(x , y)? Coref (x , y)
Coref (x , y) ? Coref (y , z )? Coref (x , z )
Coref (x , y) ? Coref (x , z ) ?Order(y , z )? Coref (y , z )
First Mentions
Coref (x , y)? ?First(x )
First(x )? ?Coref (x , y)
String Match
ExactMatch(x , y)? Coref (x , y)
SameHead(x , y)? Coref (x , y)
Pronoun
Pronoun(x ) ? Pronoun(y) ? SameGender(x , y)? Coref (x , y)
Pronoun(x ) ? Pronoun(y) ? SameNumber(x , y)? Coref (x , y)
Pronoun(x ) ? Pronoun(y) ? SamePerson(x , y)? Coref (x , y)
Other
EntityType(x , e) ? EntityType(y , e)? Coref (x , y)
Distance(x , y ,+d)? Coref (x , y)
Table 2: Rules used in the Markov Logic Network
noted that, because we are assuming an order
on the arguments of Coref (x , y), we need three
formulae to capture transivity relationships. To
test that the coreference resolution system was
producing meaningful results, we evaluated our
system on the test section of the ACE-2 dataset.
Using b3 scoring (Bagga and Baldwin, 1998),
which computes the overlap of a proposed set with
the gold set, the system achieves an F -score of
65.4%. While our results are not state-of-the-art,
they are reasonable considering the brevity of our
feature list.
The discourse model is run iteratively at each
word. This allows us to find a globally best
assignment at each word, which can be reanalyzed
at a later point in time. It assumes there is a
mention for each base NP outputted by the parser,
and for all ordered pairs of mentions x, y, it
outputs all the ?observed? predicates (i.e. ev-
erything but First and Coref ), and feeds them
to the Markov Logic system. At each step, we
compute both the maximum a posteriori (MAP)
assignment of coreference relationships as well
as the probability that each individual coreference
assignment is true. Taken together, they allow
us to calculate, for a coreference assignment c,
Pcoref(c|w, t) where w is the text input (of the
entire document until this point), and t is the parse
of each tree in the document up to and including
the current incremental parse. As we have previ-
ously calculated Pparser(w, t), it is then possible
to compute the joint probability P (c, w, t) at each
word, and therefore the prefix probability P (w)
due to syntax and coreference. Overall, we have:
P (w) =
?
c
?
t
P (c, w, t)
=
?
c
?
t
Pcoref(c|w, t)Pparser(w, t)
Note that we only consider one possible as-
signment of NPs to coreference entities per parse,
as we only retrieve the probabilities of the MAP
solution.
3.3 Pragmatics Processor
The effect of context in the experiments described
in Section 2 cannot be fully explained using a
coreference resolution system alone. In the case
of restrictive relative clauses, the referential ?mis-
match? in the unsupported conditions is caused
by an expectation elicited by a restrictive relative
clause which is inconsistent with the previous
discourse when there is no salient restricted subset
of a larger set. When the larger set is not found
in the discourse, the relative clause becomes
incoherent given the context, causing reading
difficulty. Modeling this coherence constraint is
essentially a pragmatics problem, and is under the
purview of the pragmatics processor in our sys-
tem. The pragmatics processor is quite specialised
and, although the information it encapsulates is
quite intuitive, it nonetheless relies on hand-coded
expert knowledge.
The pragmatics processor takes as input an
incremental pragmatics configuration p and
computes the probability Pprag(p|w, t, c). The
pragmatics configuration we consider is quite
simple. It is a 3-tuple where one element is true
if the current noun phrase being processed is a
discourse new definite noun phrase, the second
1183
element is true if the current NP is a discourse
new indefinite noun phrase, and the final element
is true if we encounter an unsupported restrictive
relative clause. We simply conjecture that there
is little processing cost (and hence a high proba-
bility) if the entire vector is false; there is a small
processing cost for discourse new indefinites,
a slightly larger processing cost for discourse
new definites and a large processing cost for an
incoherent reduced relative clause.
The first two elements of the 3-tuple depend
on the identity of the determiner as recovered by
the parser, and on whether the coreference system
adduces the predicate First for the current NP.
As the coreference system wasn?t designed to
find anaphoric contrast sets, these sets were found
using a simple post-processing check. This post-
processing approach worked well for our experi-
mental items, but finding such sets is, in general,
quite a difficult problem (Modjeska et al, 2003).
The distribution Pprag(p|w, t, c) applies a
processing penalty for an unsupported restrictive
relative clause whenever a restrictive relative
clause is in the n best list. Because Surprisal
computes a ratio of probabilities, this in ef-
fect means we only pay this penality when
an unsupported restrictive relative clause first
appears in the n best list (otherwise the effect is
cancelled out). The penalty for discourse new
entities is applied on the first word (ignoring
punctuation) following the end of the NP. This
spillover processing effect is simply a matter of
modeling convenience: without it, we would have
to compute Surprisal probabilities over regions
rather than individual words. Thus, the overall
prefix probability can be computed as: P (w) =
?
p,c,t Pprag(p|w, t, c)Pcoref(c|w, t)Pparser(w, t),
which is then substituted in Equation (1) to get a
Surprisal prediction for the current word.
4 Evaluation
4.1 Method
When modeling the garden path experiment we
presented in Section 2.1, we compute Surprisal
values on the word ?by?, which is the earliest point
at which there is evidence for a relative clause
interpretation. For the Grodner et al experiment,
we compute Surprisal values on the relativiser
?who? or ?that?. Again, this is the earliest point at
which there is evidence for a relative clause, and
depending upon the presence or absence of a pre-
ceding comma, it will be known to be restrictive
or nonrestrictive clause. In addition to the overall
Surprisal values, we also compute syntactic
Surprisal scores, to test if there is any benefit from
the discourse and pragmatics subsystems. As we
are outputting n best lists for each parse, it is
also straightforward to compute other measures
which predict reading difficulty, including pruning
(Jurafsky, 1996), whereby processing difficulty
is predicted when a parse is removed from the n
best list, and attention shift (Crocker and Brants,
2000), which predicts parsing difficulty at words
where the most highly ranked parse flips from one
interpretation to another.
For the garden path experiment, the simulation
was run on each of the 28 experimental items in
each of the 4 conditions, resulting in a total of 112
runs. For the Grodner et al experiment, the sim-
ulation was run on each of the 20 items in each
of the 4 conditions, resulting in a total of 80 runs.
For each run, the model was reset, purging all dis-
course information gained while reading earlier
items. As the system is not stochastic, two runs us-
ing the exact same items in the same condition will
produce the same result. Therefore, we made no
attempt to model by-subject variability, but we did
perform by-item ANOVAs on the system output.
4.2 Results
Garden Path Experiment The simulated
results of our experiment are shown in Figure 2.
Comparing the full simulated results in Figure 2b
to the experimental results in Figure 2a, we find
that the simulation, like the actual experiment,
finds both main effects and an interaction: there
is a main effect of context whereby a supportive
context facilitates reading, a main effect of syntax
whereby the garden path slows down reading,
and an interaction in that the effect of context is
strongest in the garden path condition. All these
effects were highly significant at p < 0.01. The
pattern of results between the full simulation and
the experiment differed in two ways. First, the
simulated results suggested a much larger reading
difficulty due to ambiguity than the experimental
results. Also, in the unambiguous case, the model
predicted a null cost of an unsupportive context on
the word ?by?, because the model bears the cost
of an unsupportive context earlier in the sentence,
and assumes no spillover to the word ?by?. Finally,
we note that the syntax-only simulation, shown in
Figure 2c, only produced a main effect of ambigu-
1184
(a) Results from our garden path
experiment
(b) Simulation of our garden path
experiment
(c) Syntax-only simulation
Figure 2: The simulated results predict the same interaction as the garden path experiment, but show a stronger main effect of
ambiguity, and no influence of discourse in the unambiguous condition on the word ?by?.
(a) Results from the Grodner et al
experiment
(b) Simulation of the Grodner et al
experiment
(c) Syntax-only simulation
Figure 3: The simulated results predict the outcome of the Grodner et al experiment.
ity, and was not able to model the effect of context.
Grodner et al Experiment The simulated
results of the Grodner et al experiment are shown
in Figure 3. In this experiment, the pattern of
simulated results in Figure 3b showed a much
closer resemblance to the experimental results in
Figure 3a than the garden path experiment. There
is a main effect of context, which is much stronger
in the restrictive relative case compared to non-
restrictive relatives. As with the garden path
experiment, the ANOVA reported that all effects
were significant at the p < 0.01 level. Again, as
we can see from Figure 3c, there was no effect
of context in the syntax-only simulation. The nu-
merical trend did show a slight facilitation in the
unrestricted supported condition, with a Surprisal
of 4.39 compared to 4.41 in the supported case,
but this difference was not significant.
4.3 Discussion
We have shown that our incremental sentence pro-
cessor augmented with discourse processing can
successfully simulate syntax-discourse interaction
effects which have been shown in the literature.
The difference between a Weakly Interactive
and Strongly Interactive model can be thought of
computationally in terms of a pipeline architecture
versus joint inference. In a weaker sense, even
a pipeline architecture where the discourse can
influence syntactic probabilities could be claimed
to be a Strongly Interactive model. However,
as our model uses a pipeline where syntactic
probabilities are independent of the discourse,
we claim that our model is Weakly Interactive.
Unlike Altmann and Steedman, who posited that
the discourse processor actually removes parsing
hypotheses, we were able to simulate this pruning
behaviour by simply re-weighting parses in our
coreference and pragmatics modules.
The fact that a Weakly Interactive system can
simulate the result of an experiment proposed in
support of the Strongly Interactive hypothesis is
initially counter-intuitive. However, this naturally
falls out from our decision to use a probabilistic
1185
SNP
NPbase
The postman
VP-LGS
VBD1
carried
PP:by
IN:by
by
. . .
. . .
(a) Best parse: p = 9.99 ? 10?10 main
clause, expecting more dependents
S
NP
NPbase
The postman
VP-LGS
VBD1
carried
PP:by
IN:by
by
. . .
(b) 2nd parse: p = 9.93 ? 10?10
main clause, no more dependents
S
NP
NPbase
The postman
VP-LGS
VBD1
carried
PP:by
IN:by
by
. . .
. . .
(c) 3rd parse: p = 7.69??10 relative
clause
Figure 4: The top three parses on the word ?by? in the our
first experimental item.
model: a lower probability, even in an unambigu-
ous structure, is associated with increased reading
difficulty. As an aside, we note that when using
realistic computational grammars, even the struc-
tures used in the Grodner et al experiment are
not unambiguous. In the restrictive relative clause
condition, even though there was not any compe-
tition between a relative and main clause reading,
our n best list was at all times filled with analyses.
For example, on the word ?who? in the restricted
relative clause condition, the parser is already
predicting both the subject-relative (?the postman
who was bit by the dog?) and object-relative (?the
postman who the dog bit?) readings.
Overall, these results are supportive of the
growing importance of probabilistic reasoning as
a model of human cognitive behaviour. Therefore,
especially with respect to sentence processing,
it is necessary to have a proper understanding
of how probabilities are linked to real-world
behaviours. We note that Surprisal does indeed
show processing difficulty on the word ?by? in
the garden path experiment. However, Figure 4
(which shows the top three parses on the word
?by?) indicates that not only are there still main
clause interpretations present, but in fact, the
top two parses are main clause interpretations.
This is also true if we limit ourselves to syntactic
probabilities (which are the probabilities listed
in Figure 4). This suggests that neither Jurafsky
(1996)?s notion of pruning as processing difficulty
nor Crocker and Brants (2000) notion of attention
shifts would correctly predict higher reading times
on a region containing the word ?by?. In fact, the
main clause interpretation remains the highest-
ranked interpretation until it is finally pruned at an
auxiliary of the main verb of the sentence (?The
postman carried by the paramedics was having?).
This result is curious as our experimental items
closely match some of those simulated by Crocker
and Brants (2000). We conjecture that the differ-
ence between our attention shift prediction and
theirs is due to differences in the grammar. It is
possible that using a more highly tuned grammar
would result in attention shift making the correct
prediction, but this possibly shows one benefit of
using Surprisal as a linking hypothesis. Because
Surprisal sums over several derivations, it is not
as reliant upon the grammar as the attention shift
or pruning linking hypotheses.
5 Conclusions
The main result of this paper is that it is possible
to produce a Surprisal-based sentence process-
ing model which can simulate the influence of
discourse on syntax in both garden path and
unambiguous sentences. Computationally, the
inclusion of Markov Logic allowed the discourse
module to compute well-formed coreference
chains, and opens two avenues of future re-
search. First, it ought to be possible to make the
probabilistic logic more naturally incremental,
rather than re-running from scratch at each word.
Second, we would like to make greater use of the
logical elements by applying it to problems where
inference is necessary, such as resolving bridging
anaphora (Haviland and Clark, 1974).
Our primary cognitive finding that our model,
which assumes the Weakly Interactive hypothesis
(whereby discourse is influenced by syntax in a
reactive manner), is nonetheless able to simulate
the experimental results of Grodner et al (2005),
which were claimed by the authors to be in
1186
support of the Strongly Interactive hypothesis.
This suggests that the evidence is in favour of the
Strongly Interactive hypothesis may be weaker
than thought.
Finally, we found that the attention shift
(Crocker and Brants, 2000) and pruning (Jurafsky,
1996) linking theories are unable to correctly
simulate the results of the garden path experiment.
Although our main results above underscore the
usefulness of probabilistic modeling, this obser-
vation emphasizes the importance of finding a
tenable link between probabilities and behaviours.
Acknowledgements
We would like to thank Frank Keller, Patrick
Sturt, Alex Lascarides, Mark Steedman, Mirella
Lapata and the anonymous reviewers for their
insightful comments. We would also like to thank
ESRC for their financial supporting on grant
RES-062-23-1450.
References
Gerry Altmann and Mark Steedman. Inter-
action with context during human sentence
processing. Cognition, 30:191?238, 1988.
Amit Bagga and Breck Baldwin. Algorithms for
scoring coreference chains. In The First Inter-
national Conference on Language Resources
and Evaluation Workshop on Linguistics
Coreference (LREC 98), 1998.
Marisa Ferrara Boston, John T. Hale, Reinhold
Kliegl, and Shravan Vasisht. Surprising parser
actions and reading difficulty. In Proceedings
of ACL-08:HLT, Short Papers, pages 5?8, 2008.
Thorsten Brants and Matthew Crocker. Probabilis-
tic parsing and psychological plausibility. In
Proceedings of 18th International Conference
on Computational Linguistics (COLING-2000),
pages 111?117, 2000.
Stephen Crain and Mark Steedman. On not being
led down the garden path: the use of context
by the psychological syntax processor. In
D. Dowty, L. Karttunen, and A. Zwicky, edi-
tors, Natural language parsing: Psychological,
computational, and theoretical perspectives.
Cambridge University Press, 1985.
Matthew Crocker and Thorsten Brants. Wide
coverage probabilistic sentence processing.
Journal of Psycholinguistic Research, 29(6):
647?669, 2000.
Vera Demberg and Frank Keller. A computational
model of prediction in human parsing: Unifying
locality and surprisal effects. In Proceedings
of the 29th meeting of the Cognitive Science
Society (CogSci-09), 2009.
Amit Dubey, Frank Keller, and Patrick Sturt.
A probabilistic corpus-based model of paral-
lelism. Cognition, 109(2):193?210, 2009.
Amit Dubey, Patrick Sturt, and Frank Keller. The
effect of discourse inferences on syntactic am-
biguity resolution. In Proceedings of the 23rd
Annual CUNY Conference on Human Sentence
Processing (CUNY 2010), page 151, 2010.
Ted Gibson. Linguistic complexity: Locality of
syntactic dependencies. Cognition, 68:1?76,
1998.
Daniel J. Grodner, Edward A. F. Gibson, and
Duane Watson. The influence of contextual
constrast on syntactic processing: Evidence for
strong-interaction in sentence comprehension.
Cognition, 95(3):275?296, 2005.
John T. Hale. A probabilistic earley parser as
a psycholinguistic model. In In Proceedings
of the Second Meeting of the North American
Chapter of the Asssociation for Computational
Linguistics, 2001.
John T. Hale. The information conveyed by
words in sentences. Journal of Psycholinguistic
Research, 32(2):101?123, 2003.
Susan E. Haviland and Herbert H. Clark. What?s
new? acquiring new information as a process
in comprehension. Journal of Verbal Learning
and Verbal Behavior, 13:512?521, 1974.
Shujian Huang, Yabing Zhang, Junsheng Zhou,
and Jiajun Chen. Coreference resolution using
markov logic. In Proceedings of the 2009
Conference on Intelligent Text Processing and
Computational Linguistics (CICLing 09), 2009.
D. Jurafsky. A probabilistic model of lexical and
syntactic access and disambiguation. Cognitive
Science, 20:137?194, 1996.
Roger Levy. Expectation-based syntactic compre-
hension. Cognition, 106(3):1126?1177, March
2008.
Roger Levy and T. Florian Jaeger. Speakers
optimize information density through syntactic
reduction. In Proceedings of the Twentieth
Annual Conference on Neural Information
Processing Systems, 2007.
1187
Ken McRae, Michael J. Spivey-Knowlton, and
Michael K. Tanenhaus. Modeling the influence
of thematic fit (and other constraints) in on-line
sentence comprehension. Journal of Memory
and Language, 38:283?312, 1998.
Don C. Mitchell, Martin M. B. Corley, and Alan
Garnham. Effects of context in human sentence
parsing: Evidence against a discourse-baed
proposal mechanism. Journal of Experimental
Psychology: Learning, Memory and Cognition,
18(1):69?88, 1992.
Natalia N. Modjeska, Katja Markert, and Malvina
Nissim. Using the web in machine learning for
other-anaphora resolution. In Proceedings of
the 2003 Conference on Empirical Methods in
Natural Language Processing (EMNLP-2003),
pages 176?183, Sapporo, Japan, 2003.
Shrini Narayanan and Daniel Jurafsky. Bayesian
models of human sentence processing. In Pro-
ceedings of the 20th Annual Conference of the
Cognitive Science Society (CogSci 98), 1998.
Ulrike Pado?, Matthew Crocker, and Frank Keller.
Modelling semantic role plausability in human
sentence processing. In Proceedings of the 28th
Annual Conference of the Cognitive Science
Society (CogSci 2006), pages 657?662, 2006.
Hoifung Poon and Pedro Domingos. Joint unsu-
pervised coreference resolution with markov
logic. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language
Processing (EMNLP-08), 2008.
Keith Rayner. Eye movements in reading and
information processing: 20 years of research.
Psychological Bulletin, 124(3):372?422, 1998.
Matthew Richardson and Pedro Domingos.
Markov logic networks. Machine Learning, 62
(1-2):107?136, 2006.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. A
machine learning approach to coreference
resolution of noun phrases. Computational
Linguistics, 27(4):521?544, 2001.
M. J. Spivey and M. K. Tanenhaus. Syntactic
ambiguity resolution in discourse: Modeling
the effects of referential context and lexical
frequency. Journal of Experimental Psychol-
ogy: Learning, Memory and Cognition, 24(6):
1521?1543, 1998.
Andreas Stolcke. An efficient probabilistic
context-free parsing algorithm that computes
prefix probabilities. Computational Linguistics,
21(2):165?201, 1995.
1188
