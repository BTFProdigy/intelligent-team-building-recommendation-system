Sentential Structure and Discourse Parsing 
Livia Polanyi, Chris Culy, Martin van den Berg, Gian Lorenzo Thione, David Ahn1 
FX Palo Alto Laboratory 
3400 Hillview Ave, Bldg. 4 
Palo Alto, CA 94304 
{polanyi|culy|vdberg|thione}@fxpal.com, ahn@science.uva.nl 
 
 
Abstract1 
In this paper, we describe how the LIDAS 
System (Linguistic Discourse Analysis Sys-
tem), a discourse parser built as an implemen-
tation of the Unified Linguistic Discourse 
Model (U-LDM) uses information from sen-
tential syntax and semantics along with lexical 
semantic information to build the Open Right 
Discourse Parse Tree (DPT) that serves as a 
representation of the structure of the discourse 
(Polanyi et al, 2004; Thione 2004a,b). More 
specifically, we discuss how discourse seg-
mentation, sentence-level discourse parsing, 
and text-level discourse parsing depend on the 
relationship between sentential syntax and dis-
course. Specific discourse rules that use syn-
tactic information are used to identify possible 
attachment points and attachment relations for 
each Basic Discourse Unit to the DPT.  
1 Introduction 
In this paper, we describe discourse parsing under 
the Unified Linguistic Discourse Model (U-LDM) 
(Polanyi et al 2004). In particular, we describe the 
relationship between the output of sentential pars-
ing and discourse processing.  
The goal of discourse parsing under the U-LDM 
is to assign a proper semantic interpretation to 
every utterance in a text. In order to do so, the 
model constructs a structural representation of rela-
tions among the discourse segments that constitute 
a text. This representation is realized as a Dis-
course Parse Tree (DPT). Incoming discourse ut-
terances are matched with the informational con-
text needed for interpretation, and attached to 
nodes on the right edge of the tree.  
                                                     
1 Current address:  
Language and Inference Technology Group,  
Informatics Institute 
Kruislaan 403 
1098 SJ Amsterdam, The Netherlands  
1.1 Discourse Parse Tree 
The U-LDM builds upon the insights and mecha-
nisms of the Linguistic Discourse Model (LDM) 
(Polanyi 1988). The DPT specifies which segments 
are coordinated to one another (bear a similar rela-
tionship to a common higher order construct), 
which are subordinated to other constituents (give 
more information about entities or situations de-
scribed in that constituent, or, alternatively, inter-
rupt the flow of the discourse to interject unrelated 
information), and which are related as constituents 
of logical, language specific, rhetorical, genre spe-
cific or interactional structures (n-ary relations). 
Importantly, the representation identifies which 
constituents are available for continuation at any 
moment in the development of a text and which are 
no longer structurally accessible.  
1.2 Discourse Parsing 
 While full understanding of the meaning of con-
stituent utterances, world knowledge, inference 
and complex reasoning are needed to create the 
correct Discourse Parse Tree to represent the struc-
ture of discourse under the LDM, in developing the 
U-LDM, it became apparent that most of the in-
formation needed to assign structural relations to a 
text can be recovered from relating lexical, syntac-
tic and semantic information in constituent sen-
tences to information available at nodes on the 
DPT. Largely formal methods involving manipu-
lating information in lexical ontologies and the 
output of sentential syntactic and semantic analysis 
are sufficient to account for most cases of dis-
course continuity, and give rise to only limited 
ambiguity in most other cases.2 The assignment of 
correct temporal, personal and spatial interpreta-
tion to utterances, which relies in large part on the 
relative location of referential expression and their 
                                                     
2 Complex default logic based reasoning as in Struc-
tured Discourse Representation Theory (Asher 1993; 
Asher and Lascarides 2003), speculations about the in-
tentions or beliefs of speakers (as in Grosz and Sidner 
(1986)), or the intricate node labeling exercises familiar 
from Rhetorical Structure Theory (Mann and Thompson 
1988; Marcu 1999, 2000) are not necessary. 
referents in the DPT representation of the structure 
of the discourse, can then often be recovered. From 
a computational point of view, this is good news. 
Parsing consists of the following steps for every 
sentence of the discourse. 
1. The sentence is parsed by a sentence level 
parser, in our case the LFG-based Xerox Lin-
guistic Environment (XLE).  
2. The sentence is broken up in discourse relevant 
segment based on the syntactic information 
from the sentence parser.  
3. The segments are recombined into one or more 
small discourse trees, called Basic Discourse 
Unit (BDU) trees, representing the discourse 
structure of the sentence.  
4. The BDU trees corresponding to the sentence 
are each attached to the DPT tree.3  
 
In the remainder of this paper, we will describe 
how the LIDAS System (Linguistic Discourse 
Analysis System), a discourse parser built as an 
implementation of the U-LDM, uses information 
from sentential syntax and semantics along with 
lexical semantic information to build up the struc-
tural representation of source texts4. Specifically, 
we will discuss discourse segmentation, sentence-
level discourse parsing, and text-level discourse 
parsing?phases of the discourse parsing process 
that depend on the relationship between sentential 
syntax and discourse.  
2 Discourse Segments and Basic Discourse 
Units 
U-LDM discourse segmentation is based on the 
syntactic reflexes of the semantic content of the 
linguistic phenomena making up discourse. Since 
elementary discourse units must be identified to 
build up discourse structure recursively, discourse 
segments under the U-LDM are identified as the 
syntactic units that encode a minimum unit of dis-
course function and/or meaning that must be inter-
preted relative to a set of contexts to be under-
stood. Minimal Functional units include greetings, 
connectives, discourse PUSH/POP markers and 
other ?cue phrases? that connect or modify content 
segments. Minimal meaning units are units that 
express information about not more than one event, 
                                                     
3 If a sentence is sufficiently complex, it may consist 
of two or more completely different independent dis-
course units. Such cases are treated as if the two parts of 
the sentence were really two different sentences. One 
consequence of this is that a syntactic coordination of 
two sentences may correspond to a subordination in the 
DPT, because they are treated independently. 
4 The LiveTree environment for discourse parsing is 
described in detail in Thione 2004b. 
event-type or state of affairs in a possible world. 
Roughly speaking they are ?elementary proposi-
tions? or ?event-type predicates? corresponding in 
(neo-) Davidsonian semantics to an elementary 
statement that contains at most one event-
quantifier. Structurally, a minimum meaning unit 
does not contain a proper subpart that itself com-
municates content and has a syntactic correlate that 
can stand on its own.  
Under the U-LDM, segments can be discontinu-
ous (if there is overt material on both sides of an 
intervening segment) or fragmentary. A single 
word answer to a question is a complete segment, 
whereas the same word uttered in an incomplete 
and unrecoverable phrase is a fragment. 
The U-LDM defines segmentation in purely sen-
tence syntactic terms. However, the choices of 
definitions are motivated by semantic considera-
tions. For example, since auxiliaries and modals do 
not refer to events distinctly from the main verb, 
they do not form segments separate from the corre-
sponding main verbs. By the same reasoning, other 
modal constructions that involve infinitives (e.g. 
have to, ought to, etc.) also constitute a single 
segment with their complements as do Cleft con-
structions, despite the presence of two verbs.7 On 
the other hand, Equi verbs (e.g., try, persuade) and 
Raising verbs (e.g., seem, believe, etc) form sepa-
rate BDUs from their verbal complements, since 
both eventualities can be continued. In contrast, 
even though event nominals, including gerunds, 
refer to eventualities (possibly) distinct from the 
verbs of which they are arguments or adjuncts, 
                                                     
5 For example, ?Chris served the chapter | as Social 
Chairman? 
6 For example: ?Finally, | the audio is summarized.? 
7 For example: ?It is segmentation that we are dis-
cussing.? 
Content segments  
(BDUs) 
Simple clauses, Subordinate 
clauses and participial phrases, 
Secondary predications,5 Inter-
polations (e.g. parentheticals, 
appositives, interruptions, etc.), 
Fragments (e.g. section head-
ings, bullet items, etc.) 
Operator segments Conjunctions that conjoin seg-
ments, Discourse operators 
(e.g. ?scene-setting? preposed 
modifiers)6Discourse connec-
tives 
Table 1: Examples BDU and Operator Seg-
ments. 
those eventualities can not (easily) be continued 
and therefore are not segments.8 
The U-LDM, in contrast to other discourse theo-
ries, has a fine-grained taxonomy of minimal units. 
The most prominent distinction of discourse seg-
ments is between Basic Discourse Units (BDUs) 
and Operator segments (see Table 1). BDUs are 
segments realized in a linguistic utterance that can 
independently provide the context for segments 
later in the discourse. Operator segments can not 
do so, they can only provide context for segments 
in their scope. 
In the following section, we explain more fully 
how sentence syntax is used to segment sentences 
into discourse segments.  
2.1 The role of the XLE in discourse segmen-
tation 
Dividing a text into discourse segments begins 
by applying a sentence breaking algorithm to de-
termine the tokens to be segmented. These tokens 
are normally complete sentences, but may also be 
fragments in some cases such as titles or elliptical 
phrases such as ?Yes?. The tokens are processed 
by a discourse segmenter. After the segmenter has 
completed its work, segments are passed to a U-
LDM BDU-Tree parser, which constructs one or 
more BDU-Trees from the BDUs identified in each 
sentence.  
The LIDAS segmenter first sends the input 
chunk to be parsed by the Xerox Linguistic Envi-
ronment parser (XLE) (Maxwell and Kaplan 
1989). The XLE is a robust Lexical Functional 
Grammar (LFG) parser. The XLE tries to parse the 
input, and returns either a packed representation of 
all possible parses or the most probable parse as 
selected by its stochastic disambiguation compo-
nent (Riezler et al 2002). If the XLE can not find a 
parse of the input as a complete sentence it tries to 
construct a sequence of partially parsed fragments.9 
The LIDAS segmenter segments the most probable 
parse and, as a backup, the first parse from the 
packed representation, if the first and the most 
probable parse differ. 
                                                     
8 Note that the contrast between modal verbs on the 
one hand and Raising and Equi verbs on the other 
clearly shows that the surface form of a phrase (e.g., the 
infinitival complements) is not always sufficient to de-
termine segment status. Similarly, a finite verb is not a 
sufficient indicator of a segment, as seen in the cleft 
constructions. Crucially, and appropriately, we need to 
refer to the discourse property of potential continuation. 
9 The XLE has a failure rate (i.e., no parse whatso-
ever) of approximately 0.5% on our corpus of technical 
reports. 
The parse information consists of a c(onstituent) 
structure (essentially a standard parse tree) and a 
f(unctional) structure containing predicate-
argument information, modification information, 
and other grammatical information such as tense 
and agreement features. F-structures make up the 
primary source of linguistic information used in 
discourse parsing.  
To identify the discourse segments within the 
sentence, seven syntactic configurations in c-
structure and f-structure are examined. The rela-
tively small set of configurations in Table 2 ac-
counts for the full range of discourse segments. 
According to these segmentation rules, it is pos-
sible for a segment to be embedded in another 
segment. Because on the tree-projection of U-
LDM structures terminal nodes represent a con-
tiguous textual span, we recombine the two parts 
of a non-contiguous segment in the BDU-tree (sec-
tion 4) and DPT (section 5) using the concatena-
tion relation (+). Concatenated nodes contain the 
complete f-structure information of the completed 
segment and are full-fledged BDUs, available for 
further ?anaphoric anchoring?. 
All segments are returned to the discourse parser 
in an XML format, which includes the f-structure 
information as well as the textual spans for each 
sentential segment. 
3 Discourse Parsing 
The next step after segmentation is combining the 
segments into a BDU-tree according to the rules of 
discourse parsing, resulting in a discourse tree rep-
resenting the sentence. After that, the BDU-tree is 
                                                     
10 F-structures with SUBJ (subject) attributes are con-
sidered possible discourse segments since they typically 
encode an eventuality. However, because they do not 
introduce independent anchor points for future attach-
ment, the f-structures corresponding to modal and auxil-
iary verbs are excluded. 
Content Segments 
Certain F-structures with subjects10 
Fragments 
Parentheticals 
Headers 
Syntactic coordination (except conjunct itself)  
Operator segments 
Conjuncts in coordination 
Initial comma separated modifiers 
Subordinating conjunctions 
Table 2. Segment classification configura-
tions. 
combined with the Discourse Parse Tree, again 
according to the discourse parsing rules.  
In discourse parsing, units of discourse are at-
tached to an emergent discourse tree. Attachment 
always takes place on the right edge of the tree. 
The parser has to make two decisions: what unit to 
attach to and what relationship obtains between the 
incoming unit and the selected attachment site. The 
types of evidence are used to determine this in-
clude: 
 
? syntactic information 
o subordinate/complement relations, parallel 
syntactic structure, topic/focus and centering 
information, syntactic status of re-used lex-
emes, preposed adverbial constituents, etc.  
? lexical information 
o Information from lexical ontology: re-use of 
same lexeme, synonym/antonym, hypernym, 
participation in the same lexical frame as 
well as specific discourse connectives tem-
poral and adverbial connectives indicating 
set or sub-set membership of any type for 
example, specifically, alternatively11. 
o Modal information: realis status, modality, 
genericity, tense, aspect, point of view12),  
? Structural information of both the local at-
tachment point and of the BDU-tree 
? Presence of constituents of incomplete n-ary 
constructions on the right edge 
o questions, initial greetings, genre-internal 
units such as sections and sub-sections, etc. 
 
The combined weight of the evidence determines 
the attachment point and the attachment relation. 
Interestingly, the weight given to each type of in-
formation is different for attachment site selection 
and relationship determination. Lexical ontological 
information, for example, is generally more impor-
tant in determining site, while semantic, syntactic 
and lexical ?cue? information is more relevant in 
determining relationship. 
In Polanyi et al 2004, a small set of robust rules 
is given for determining the attachment site and 
relationship of an incoming BDU-tree to the exist-
ing parse tree of the discourse. In the present pa-
per, which has as its focus understanding the rela-
tionship of sentential syntax to discourse structure, 
we concentrate on describing some fundamental 
                                                     
11 These expressions are used as input to specific 
lexically driven rules that indicate language or genre 
specific binary relations between BDUs as suggested by 
Webber, Joshi and colleagues in their work on D-
LTAGs. (Webber and Joshi, 1998; Webber, Knott and 
Joshi, 1999; Webber, Knott, Stone and Joshi 1999.) 
12 See discussion Wiebe, 1994. 
aspects of the relationship between the sentential 
syntactic structure of an incoming sentence and its 
corresponding sentential discourse structure. 
3.1 The Sentential Syntax - Discourse Struc-
ture Interface 
In discourse parsing with LIDAS, the output of 
the XLE parser supplies functional constraints and 
roles for syntactic structures. The syntactic struc-
ture of a sentence accounts for the discourse-
relevant relationships between segments within a 
sentence. LIDAS grammars exploit this informa-
tion by mapping syntactic relations onto discourse 
relations. The LIDAS grammar formalism permits 
the parser to leverage the XLE?s output by (1) 
checking positive or negative constraints (equality 
or inequality operators) (2) recursively searching f-
structures for specified attributes (* and ? wild-
cards), (3) enforcing dependent constraints,13 and 
(4) using Boolean connectives to combine con-
straints. (5) applying constraints universally or ex-
istentially to the set of matching f-structures. While 
LIDAS grammar rules can incorporate constraints 
that operate on all supported types of linguistic 
evidence, Table 3 gives three examples of rules 
that specify attachments based on the syntactic re-
lationship between constituents. 
 
1. 
BDU-1,BDU-2:  
BDU-1/phi = BDU-2/ADJUNCT/link;  
 ? Right-Subordination. 
2. 
BDU-1,BDU-2:  
BDU-1/*/ADJUNCT/link = BDU-2/phi;  
 ? Subordination. 
3. 
BDU-1,BDU-2:  
BDU-1/*/{XCOMP|COMP|COMP-EX}/link 
  = BDU-2/phi;  
 ? Context. 
Table 3: Rules based on syntactic relationship. 
 
Rule 1 captures one general case for preposed 
modifiers. Prepositional and adverbial phrases, 
often temporal modifiers, that precede the main 
clause they modify can either elaborate on the 
main clause or modify the context in which the 
main clause is interpreted. Lexical information is 
used to distinguish among different types of modi-
fiers. Rule 2 expresses the general case of subordi-
nate adjunct clauses and shows that the syntax for 
                                                     
13 For example, the following constraints show a de-
pendency between (1) and (3),  
(1) BDU-1/(*)/ADJUNCT/link = BDU-2/phi; 
(2) BDU-2/ADJUNCT-TYPE = ?relative?; 
(3) $1/SPEC/DET/DET-TYPE = "indef"; 
One or more sub-f-structures that match the wildcard 
in the first constraint are tested in the third constraint. 
This constraint is part of a rule of sentential discourse 
syntax used to identify non-restrictive relative clauses. 
LIDAS? discourse rules allows for recursive search 
over f-structures by seeking adjunct phrases match-
ing the incoming BDU anywhere within the f-
structure of the attachment point. Rule 3, which 
shows a compact syntax for disjunctive constraint, 
builds a sentence level discourse relation, the Con-
text Binary, that forms a complex context unit 
from its child constituents. Context Binaries are the 
general case for clausal complementizers. 
In the next section, we discuss one of the basic 
discourse parsing rules. 
3.2 Discourse Subordination  
We will assume the following extended hierarchy 
of grammatical functions:14 PRED > SUBJ > OBJ 
> COMP > ADJUNCT > SPEC.15 Given this hier-
archy we propose as a general principle of dis-
course construction that promotion in the hierar-
chy means demotion in the discourse. For exam-
ple, if the SUBJ of the incoming unit of discourse 
refers to the same entity16 as OBJ at the attachment 
node, then the relationship between them will be a 
subordination. In general, if an expression with 
grammatical function GF in a BDU B refers to the 
same (or a subcase of the same) entity as an ex-
pression with grammatical function GF? in an ac-
cessible antecedent BDU A, where GF > GF?, then 
B will be attached as a subordinate structure to A 
on the DPT. This principle is expressed as a rule in 
the grammar that fires if it is not superseded by 
other rules. For example, the Narrative rule, which 
coordinates event clauses, takes precedence over 
the Discourse Demotion Rule. 
If the grammatical function hierarchy rule does 
not apply, but the BDU refers to a subclass17 of the 
antecedent BDU, there is evidence for a subordina-
tion relation. For example, if the subject of the 
BDU stands in a part-of relationship with the sub-
ject of the antecedent BDU, we can conclude that 
the relationship is a subordination. 
                                                     
14 PRED denotes the tensed verb. It plays a role in the 
following discussion because verbs can be nominalized. 
15 For the purposes of this hierarchy, grammatical 
function COMP includes the features COMP-EX and 
XCOMP. An element inside an expression with 
a grammatical function GF is itself in that position with 
respect to the elements that are not in that expression, 
although a separate ordering might exist between ele-
ments within the same expression. C.f. Obliqueness 
command in HPSG (Pollard and Sag, 1994). 
16 In LIDAS, no reference resolution is done. Identity 
of reference is approximated using lexical semantics. 
17 The notion of subcase as used here covers a num-
ber of different notions. An expression e is a subcase of 
f if (1) e is a set that is a subset of f, (2) e is a subtype of 
f, or (3) e is a part of f, among other relations. 
Table 4 gives the interaction of this rule with the 
hierarchy rule and the resulting relationships: G 
denotes whether a shift in the grammatical function 
hierarchy occurred, and L whether the shifted ele-
ment refers to the same entity, part of that entity or 
to an entity that is larger. If more than one such 
expression can be found, we consider the expres-
sion in the incoming unit with the highest gram-
matical function. 
 
 G+ G0 G- 
L+  S/C18 C S 
L0 S C N/A 
L- S S N/A 
Table 4. Interactions of the hierarchy 
and subcase rules. 
The table is read as follows: take the expressions in 
the incoming unit that have a relationship with an 
expression in the attachment point. Let e be the 
expression among these that has the highest gram-
matical function, and f be the corresponding ex-
pression in the attachment point. If the grammati-
cal function of e is higher than that of f, we write 
G+, if it is the same G0, if it is lower G-. Similarly, 
if e is a supercase of f, we write L+, if e and f refer 
to the same entity L0, and if e is a subcase of f, L-. 
For example 
 
1. U1: The man was wearing a coat. 
U2: The Burberry looked brand new. 
 
In this case we notice that the words coat and 
Burberry are such that L<coat, Burberry> = L- 
and we also notice that Burberry gets promoted to 
the subject of the incoming unit, while coat was 
the direct object of U1. Therefore G<coat, Bur-
berry> = G+. From Table 4 we correctly identify 
that U2 does indeed subordinate on U1. 
Both L- and G+ give evidence for discourse sub-
ordination. Grammatical function demotion G- is a 
less clear case.19 Some mixed combinations sug-
gest discourse coordination (as for the preservation 
case <L0 , G0>) while others contribute too little 
                                                     
18 Semantics would help to disambiguate this case. If 
the antecedent f is more specific than the anaphoric 
element e, two cases are possible. Either e is used as a 
definite description referring to the same entity as f, in 
which case the relation is a coordination, or e is used to 
denote a larger class of entities than f, in which therela-
tion is likely to be a subordination. 
19 As is understanding precisely what is involved 
from a discourse relation point of view with complex, 
mixed promotion/demotion phenomena (from the sub-
ject of an XCOMP to the adjunct of an OBJ, for exam-
ple) 
significant information to independently determine 
the discourse attachment (N/A cases). In those 
cases, evidence from other rules in the grammar 
determines the result.  
4  BDU-Trees 
Identifying the relationship of a BDU to the dis-
course in which it occurs is a complex parsing 
process involving lexical, semantic, structural and 
syntactic information. The first step in understand-
ing the relationship of a given BDU to the extra-
sentential discourse context is to understand the 
role a BDU plays within the sentence in which it 
occurs. As a first step of constructing a discourse 
parse-tree of the sentence, the XLE parse and sen-
tential discourse rules are used to identify relation-
ships between the BDUs within the sentence re-
sulting in one or more BDU-trees. These small 
sentence-level discourse trees consist of one main 
clause and its subordinated clauses or preposed 
modifiers.20 
The root node of a BDU-tree represents the non-
subordinated material (often referred to as active 
material) within that BDU-tree, including at least 
the information of the main clause. The projection 
of the root node on the active leaves is referred to 
as the M-BDU (Main BDU). Only syntactic infor-
mation from the M-BDU is used to attached the 
BDU-tree to the DPT. In general, a sentence yields 
as many BDU-trees as top-level coordinated 
clauses.  
Consider, Example 2, a sentence taken from our 
corpus of Technical Reports: 
 
2. As a consequence, any hypertext link followed 
opened a new browser window, which we think of 
as a "Rabbit Hole" because the new window indi-
cates to users that they are no longer navigating in-
side the slideshow, but are instead navigating the 
Web. 
 
The U-LDM segmentation of this sentence:  
 
As a consequence | any hypertext link | followed | 
opened a new browser window | which we think of as a 
"Rabbit Hole" | because | the new window indicates to 
users | that they | are no longer navigating inside the 
slideshow | but | are instead navigating the Web. 
                                                     
20 BDU-trees are very similar to the D-LTAG dis-
course segments (D-LDSs). However, BDU-trees in-
clude subordinated material, so they are typically larger 
than D-LDSs. Furthermore, because U-LDM sentence 
and discourse grammars are different, two BDU-trees 
may stand in a coordinated relationship in sentence 
grammar and be subordinated on the discourse level (cf. 
example 1). 
 
For compound sentences, members of the top-
level coordinate structures are attached independ-
ently, reflecting the fact that top-level coordinated 
clauses can escape the boundaries of the sentence 
when attaching to the main discourse structure. For 
example, the discourse parse of the following pas-
sage, illustrated in Example 3, consists of two sen-
tences, five segments, four BDUs and three BDU-
Trees that eventually form one DPT. 
 
3. S1: B1: The man soaked himself in the water.  
S2: B2: It was warm and soothing B3 and he 
decided to linger a little longer than usual.  
 
Despite the apparent syntactic coordination be-
tween the two main clauses, the two BDU-Trees in 
S2 show the independence of BDU-Tree/DPT at-
tachments. B1 describes a punctual event on a 
main story line. B3 describes the next event. The 
semantic and aspectual information of the verb to 
soak in BDU-1 and of the copula in BDU-2 com-
municates a switch from an event-line to an em-
bedded elaboration. The syntactic promotion of 
water from object of the preposition in the adjunct 
phrase in the water to the subject (through ana-
phoric reference) in the next segment indicates dis-
course subordination. Given L0<water, it> and 
G+<ADJUNCT/OBJ/PRED, SUBJ/PRED>, we 
find from table X, <L0 ,G+> ? S. As a conse-
quence, B2 is subordinated to B1. In the DPT, B3 
is coordinated with B1 at a point above B2, despite 
being syntactically coordinated to it at the senten-
tial level.  
5 Global Discourse Construction 
BDU-trees are attached to the DPT of the entire 
discourse as single units by computing the rela-
tionship between their M-BDUs and the accessible 
nodes aligned along the right edge of the DPT. 
Rules of discourse attachment that include those 
discussed for BDU construction as well as other 
genre and structural level rules are used in global 
DPT construction. 
The parsing process at the Discourse Parse Tree 
(DPT) level works as follows. Once BDU-Trees 
have been constructed and are ready to be attached 
to the DPT, each node along the right edge is ex-
amined, and, through a set of discourse rules, an 
ordered set of active Discourse Constituent Units 
(DCUs) is produced, representing possible attach-
ment points.21 This set can then be pruned of its n 
                                                     
21 Lexical information is the main source of evidence 
in attachment site determination while other sources 
contribute to a lesser degree. The opposite is true for 
determining attachment relations 
lowest scoring constituents, according to a preset 
threshold or other criteria. 
 
Example 
Our group is developing new techniques for helping 
manage information for enhanced collaboration. We 
explore solutions for seamlessly connecting people to 
their personal and shared resources. Our solutions in-
clude services for contextual and proactive information 
access, personalized and collaborative office appli-
ances, collaborative annotation, and symbolic, statisti-
cal, and hybrid processing of natural language. 
Our team includes researchers with diverse back-
grounds including: ubiquitous computing, computer-
supported collaboration, HCI, IR , and NLP. 
Segmented Text 
1. Our group is developing new techniques for helping 
2. manage information for enhanced collaboration.  
3. We explore solutions for seamlessly connecting 
people to their personal and shared resources.  
4. Our solutions include services for contextual and 
proactive information access, personalized and col-
laborative office appliances, collaborative annota-
tion, and symbolic, statistical, and hybrid process-
ing of natural language. 
5. Our team includes researchers with diverse back-
grounds  
6. including:  
7. ubiquitous computing, computer-supported col-
laboration, HCI, IR , and NLP. 
Rules 
1-2 Intrasentential XCOMPS --> CX 
CX(1,2)-3 Demotion 2  
 [ Pres. Progressive to Simple Pre-
sent] 
 [ Our group --> We ] 
 [ Same Verb Class ] --> S 
3-4 Promotion  
 [Solutions from OBJ to SUBJ] --> S 
5-6 Relative Adjunct with Null Deter-
miner --> S 
6-7 Colon + OBJ linked to NP segment 
 --> CX 
CX(1,2)-5 Synonym Subjects, Same 
Tense --> C22 
Table 5. Analyzed Webpage Example 
In the second stage, attachment rules are checked 
against possible attachment sites. Rules that fire 
successfully attach the BDU-Tree to the DPT at the 
chosen site with the relationship specified by the 
rule. Local semantic, lexical and syntactic informa-
tion is then percolated into the parent DCU. If mul-
tiple attachments at different sites are possible, 
                                                     
22 Discourse parsing is not unambiguous and different 
analyses may apply. So, here the same subject, change 
in progressive feature, and different verb class, which 
also apply would create an S. Future research is needed 
to understand precisely which rules take precedence. 
ambiguous parses are generated; less preferred at-
tachments are discarded and the remaining attach-
ment choices generate valid parse trees. 
Once a BDU-tree has been attached, its leaves 
become terminal nodes of the DPT and nodes on 
its right edge are accessible for continuation and 
may serve as contextual anchors for subsequent 
sentences. Table 5 shows an example text taken 
from the FXPAL Webpage describing our research 
group, a segmentation of the text and the DPT con-
structed following the rules. Figure 1 gives a s-
creenshot of the resulting tree. 
 
 
Figure 1: Tree of Analyzed Webpage Example 
6 Comparison with D-LTAG 
LIDAS is a purely symbolic discourse parser most 
similar to the D-LTAG parser described in (Forbes 
et. al. 2003). The overall structures of the LIDAS 
and D-LTAG parsers are almost identical. There 
are a number of apparently significant differences 
between the underlying theories although some of 
these may turn out to be notational variants after 
more extensive analysis. 
An important difference between D-LTAG and 
U-LDM derives from the fundamental question of 
segmentation; D-LTAG segments are larger than 
our segments, corresponding more or less to BDU-
trees (but cf. footnote 20). In D-LTAG, because 
only one grammar formalism governs both dis-
course and sentence level parsing, continuation can 
also take place on parts of segments as defined by 
sentence-level syntactic relations. Under the U-
LDM, which employs independent sentential and 
discourse grammars, only segments are potential 
anchors for continuation. Not only because we use 
an external syntactic parser as an oracle that in-
forms segment attachment on the BDU-tree, but 
more importantly because sentential syntax can be 
overridden by discourse syntax in some cases. 
Another basic difference between the two ap-
proaches is that D-LTAG builds its initial and aux-
iliary trees around connectives. Every discourse 
relation is expressed by a, possibly empty, connec-
tive. In U-LDM, connectives give evidence about 
the possible discourse relations, as do other parts 
of the sentence, but they do not solely determine 
discourse relation. We can thus account correctly 
for cases in which the wrong connective is selected 
to express a semantic relationship among seg-
ments.  
Lastly, our parser is meant to be incremental at 
the discourse level, whereas the D-LTAG parser 
appears to operate on the discourse as a whole. 23 
7 Conclusion 
In this paper we described a novel approach to dis-
course segmentation and discourse structure analy-
sis that unifies sentential syntax with discourse 
structure and argued that most of the information 
needed to assign a structural description to a text is 
available from sentential syntactic parsing, senten-
tial semantic analysis and relationships among 
words captured in a lexical ontology such as 
WordNet. The U-LDM discourse rules and parsing 
strategies presented here are a first step. We have 
tested out these rules in analyzing a corpus of over 
300 Technical Reports that have been summarized 
under the PALSUMM System that operates on top 
of LIDAS. (Polanyi et al2004; Thione et al2004) 
Much work remains to be done. Understanding the 
complex inter-relationships between rules is a for-
midable task. Critically important, too, is to unify 
the semantically motivated structural analyses pre-
sented here with an explicit S-DRT type formal 
semantic account of discourse semantics. How-
ever, we believe that the results presented here rep-
resent an important advance in understanding the 
nature of natural language texts. 
8 References 
Nicholas Asher. 1993. Reference to Abstract Objects in 
English: A Philosophical Semantics for Natural Lan-
guage Metaphysics. Kluwer Academic Publishers. 
Nicholas Asher and Alex Lascarides. 2003. Logics of 
Conversation. Cambridge University Press. 
Katherine Forbes, Eleni Miltsakaki, Rashmi Prasad, 
Anoop Sarkar, Aravind Joshi and Bonnie Webber. 
2003. D-LTAG System - Discourse Parsing with a 
Lexicalized Tree-Adjoining Grammar, Journal of 
Language, Logic and Information, 12(3). 
Barbara Grosz and Candace Sidner. 1986. Attention, 
Intention and the Structure of Discourse. Computa-
tional Linguistics 12:175-204. 
William C. Mann and Sandra A. Thompson. 1988. Rhe-
torical Structure Theory: Towards a Functional The-
ory of Text Organization. Text 8(3)243-281. 
                                                     
23 In the current LIDAS implementation, we do not 
represent ambiguity directly, but implement a greedy 
parsing algorithm with backtracking. The non-locality 
of the D-LTAG parser as described in Forbes et. al. 
2003 may likewise be a consequence of their current 
implementation. 
Daniel Marcu. 1999. Discourse trees are good indicators 
of importance in text. In Advances in Automatic Text 
Summarization. I. Mani and Mark Maybury (eds), 
123-136, The MIT Press. 
Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. The MIT Press. 
Cambridge, MA. 
John Maxwell and Ronald M. Kaplan. 1989. An over-
view of disjunctive constraint satisfaction. In Pro-
ceedings of the International Workshop on Parsing 
Technologies, Pittsburgh, PA. 
Livia Polanyi. 1988. A Formal Model of Discourse 
Structure. Journal of Pragmatics 12: 601-639. 
Livia Polanyi. 2004. A Rule-based Approach to Dis-
course Parsing. In Proceedings of SIGDIAL ?04. 
Boston MA. 
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, 
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a 
Lexical-Functional Grammar and discriminative es-
timation techniques. In Proceedings of the 40th An-
nual Meeting of the Association for Computational 
Linguistics (ACL?02), Philadelphia, PA. 
Stefan Riezler, Tracy H. King, Richard Crouch, and 
Annie Zaenen. 2003. Statistical Sentence Condensa-
tion using Ambiguity Packing and Stochastic Disam-
biguation Methods for Lexical-Functional Grammar. 
In Proceedings of HLT-NAACL'03, Edmonton, Can-
ada.  
Radu Soricut and Daniel Marcu. 2003. Sentence Level 
Discourse Parsing using Syntactic and Lexical In-
formation. In Proceedings of HLT/NAACL?03, May 
27-June 1, Edmonton, Canada 
Thione, Gian Lorenzo, Martin van den Berg, Chris Culy 
and Livia Polanyi. 2004a. Hybrid Text Summariza-
tion: Combining external relevance measures with 
Structural Analysis. Proceedings ACL Workshop 
Text Summarization Branches Out. Barcelona. 
Thione, Gian Lorenzo, Martin van den Berg, Chris Culy 
and Livia Polanyi. 2004b. LiveTree: An Integrated 
Workbench for Discourse Processing. Proceedings 
ACL Workshop on Discourse Annotation. Barcelona. 
Bonnie Webber and Aravind Joshi, 1998. Anchoring a 
lexicalized tree-adjoining grammar for discourse. 
COLING/ACL Workshop in Discourse Realtions and 
Discourse Markers. Montreal, Canada. 86-92. 
Bonnie Webber, Alistair Knott and Aravind Joshi. 
1999a. Multiple discourse connectives in a lexical-
ized grammar for discourse. In 3rd Int?l Workshop on 
Computational Semantics. Tilburg. 309-325. 
Bonnie Webber, Alistair Knott, Matthew Stone and 
Aravind Joshi. 1999b. Discourse Relations: A Struc-
tural and Presuppositional Account Using Lexical-
ized TAGS. 37th ACL. College Park, MD. 41-48. 
Wiebe, Janyce M. 1994. Tracking point of view in 
narrative. Computational Linguistics 20 (2): 233-287. 
LiveTree: An Integrated Workbench for Discourse Processing 
Gian Lorenzo Thione, Martin van den Berg, Chris Culy, Livia Polanyi 
FX Palo Alto Laboratory 
3400 Hillview Ave, Bldg. 4 
Palo Alto, CA 94304 
{thione|vdberg|culy|polanyi}@fxpal.com 
 
 
 
Abstract 
In this paper, we introduce LiveTree, 
a core component of LIDAS, the Lin-
guistic Discourse Analysis System for 
automatic discourse parsing with the 
Unified Linguistic Discourse Model 
(U-LDM) (X et al 2004). LiveTree is 
an integrated workbench for super-
vised and unsupervised creation, stor-
age and manipulation of the discourse 
structure of text documents under the 
U-LDM. The LiveTree environment 
provides tools for manual and auto-
matic U-LDM segmentation and dis-
course parsing. Document manage-
ment, grammar testing, manipulation 
of discourse structures and creation 
and editing of discourse relations are 
also supported. 
1 Introduction 
In this paper, we introduce LiveTree, a 
core component of LIDAS (the Linguistic 
Discourse Analysis System) for automatic 
discourse parsing with the Unified Lin-
guistic Discourse Model (U-LDM) (Po-
lanyi et al 2004). The U-LDM is a theory 
of discourse structure and semantics that 
has as its goal assigning the correct inter-
pretation to natural language utterances.  
1.1 Overview of LiveTree 
LiveTree is an integrated workbench for 
supervised and unsupervised creation, 
storage and manipulation of the discourse 
structure of text documents under the U-
LDM. LiveTree does not support speech, 
dialog or interaction annotation (Bernsen 
et al 2002, 2003 and over view of systems 
in Bernsen et al 2002). The LiveTree en-
vironment provides tools for manual and 
automatic U-LDM segmentation and dis-
course parsing. Like RSTTool, LiveTree 
provides support for segmentation, mark-
ing structural relations among segments, 
and creating and editing discourse rela-
tions (O?Donnell 2003). Similar to the D-
LTAG system described in Forbes et al
(2003) LiveTree is an experimental dis-
course parser implementing a theory of 
sentential and discourse relations. How-
ever, LiveTree is also a complete docu-
ment handling and manual and automatic 
discourse parsing system. Various applica-
tions are supported as web services. Ac-
cordingly, LiveTree serves as both the user 
interface and theory development envi-
ronment for PALSUMM, a text summari-
zation system built on top of LIDAS (See 
Section 5 below) In this paper, we de-
scribe the resources LiveTree workbench 
provides for discourse level theoretical 
development as well as document han-
dling, manual and automatic text annota-
tion and parsing. 
1.2 LiveTree Functionalities 
LiveTree?s Java architecture shown in 
Figures 1 is modular and highly extensi-
ble. LiveTree is made up by: (1) a Model 
Manager which provides interfaces for 
manipulation, storage and retrieval of ac-
tual documents and discourse representa-
tions; (2) a Module Manager, which han-
dles and provides access to the main GUI 
and to all installed modules; and (3) a Ser-
vice Manager providing a polling interface 
for all active LiveTree Services. Manager 
components rely on stubs which can be 
implemented and extended from outside 
the framework?s core. 
The LiveTree Module Manager and all 
installed LiveTree Modules lie on top of a 
general GUI Layer handling the main Li-
veTree window, which includes a menu 
bar, a tool bar, a status bar and four doc-
king areas. The status bar is used for mes-
sages to the user and notification of status 
for asynchronous services. The menu bar 
and the toolbar allow rapid access to gene-
ral functions and to module-specific ac-
tions, including hiding and showing modu-
le windows. Every module is assigned a 
window which can be resized, docked or 
hidden/shown. 
When multiple modules are docked in 
the same docking area they are arranged in 
a tabbed interface which allows easy ac-
cess and maximizes display real-estate. 
Finally, the GUI Layer administers con-
textual pop-up menus in a general, mod-
ule-independent fashion: any module can 
register a number of actions bound to a 
specific context (e.g. a sentence, a node, a 
sub-tree, etc.) and at the user's request, the 
GUI Layer polls the Module Manager for 
appropriate actions from every installed 
module. LiveTree?s clean and intuitive 
interface is independent of the specific 
modules installed and allows for seamless 
integration of custom modules not part of 
the current implementation. 
Table 1 gives a comprehensive over-
view of LiveTree features as well as iden-
tifying the modules or services that pro-
vide them. 
2 Document Handling 
The Model Manager (MM) is the main 
access point to models, defined as the syn-
chronized unions of a document and its 
(annotated) discourse structure. The MM 
requires that appropriate LiveTree Ser-
vices provide functionalities needed for 
persistent storage and retrieval of anno-
tated documents. As long as documents 
are not modified externally, their discourse 
representations can also be retrieved from 
a persistent XML format encoding U-
LDM tree structure, visualization parame-
ters, surface and deep node content along 
with other user-defined annotations.  
The Document Module (DM) enables 
full document creation, modification and 
annotation at the document, re-
gion/selection, and sentence level. The 
DM provides the visual representation of 
an HTML document 1  and preserves the 
text organization, formatting, and non-
textual information (figures, tables, etc.) of 
HTML source documents. The DM also 
provides visual feedback capabilities in-
cluding highlighting and hiding/showing 
sections of documents. The Document 
Stub Interface provides the mapping be-
tween a document?s content and notions of 
paragraphs, sentences, units and spans. In 
the current implementation, a document is 
divided in paragraphs according to stan-
dard notions; paragraphs are then token-
ized in sentences using simple heuristics 
and sentences are segmented into Basic 
                                                     
1 Currently, only HTML document formats 
are supported. Other data formats can also be 
supported by implementing the Document Stub 
Interface (DSI) appropriately,  
Module Manager Model Manager Service Manager
Model Stub
Module Stub
Service Synchronization Layer
Document
Stub
Discourse
Theory
Stub
Service Stub
HTML U-LDM
LDM ParserPALSUMM
OntoPAL
Document 
Module Tree Module
BDU-Tree
Module
Context
Module
D-Grammar
Module
GUI Interface Layer
Parsing
Ontology
Xle2Xml
Parsing
Segmentation
RST
- Framework/Architectural 
o Module Manager Architecture with 
independent dockable floating win-
dows for each independent module 
o Service Manager with multi-threaded 
support for asynchronous services, no-
tification management and SOAP/web-
service support 
o Generalized Model Manager for sup-
port of multiple discourse theo-
ries/models  
o Scripting Engine for automatic batch 
execution of actions and commands 
- Default Modules 
o Document Module; BDU-Tree Module; DPT Module; 
Search Module; Content Module; Grammar Module; 
Segments Module. 
- Default Services 
o Discourse Parsing Service; U-LDM 
Model Persistence Service; Xle2Xml 
Syntax Parsing Service; Discourse 
Segmentation Service; OntoPAL 
Figure 1: LiveTree Architecture. 
Core system and several implemented modules and services 
 
Discourse Units following the U-LDM 
discourse segmentation conventions dis-
cussed below.  
3 LiveTree Support for Discourse 
Annotation 
The LiveTree Workbench supports manual 
and automatic, supervised and unsuper-
vised annotation practices for each step in 
the analysis process. In addition, our de-
fault implementation includes a com-
pletely integrated interface for writing, 
testing and debugging U-LDM Discourse 
grammar rules which are used for auto-
matically constructing the discourse repre-
sentation for individual sentences and en-
tire texts. 
3.1 U-LDM Parsing Steps 
The U-LDM specifies rules both for seg-
menting sentences into Basic Discourse 
Units (BDUs) and then for combining the 
BDUs into an Open-Right Discourse Parse 
Tree (DPT) that captures structural rela-
tions among constituent structures. The U-
LDM discourse parsing process can be 
summarized as follows: 
? Identify potential Basic Discourse 
Units (BDUs) within sentence from 
output of analysis of sentence docu-
ments from the Xerox Linguistic Envi-
ronment (XLE) Lexical Functional 
Grammar (LFG) parser using sen-
tence-segmentation rules. 
? Construct a set of Open-Right BDU-
trees representations which map onto 
top-level coordinated structures within 
the sentence, using syntactic informa-
tion from the XLE parse and sentential 
discourse rules to identify the relation-
ships among BDUs.  
Feature Description Modules & Services  
Document Handling Support for HTML Documents (Import, Export, 
Create, Edit, Print, Tokenize in sentences) 
Document Module 
Discourse 
Segmentation 
(Automatic & Manual) 
Support for LDM Discourse Segments (Automatic 
Sentence Segmentation; Manual Editing of Segments; 
Manual Sentence Segmentation; Inspect Segments? 
Syntax Content) 
BDU-Tree Module, 
Content Module, 
Xle2Xml Syntax parsing Ser-
vice, 
Discourse Segmenter 
Discourse Structure 
Creation 
Document and 
Sentence Level  
(Automatic & Manual) 
Support for LDM DPT and BDU Trees (Automatic 
Discourse Parsing; Sub-tree Attachment via Drag ?n 
Drop; Editing including Node Type Editing and Con-
tent Editing; Node/Sub-tree Removal; Node-Specific 
Notes Editing; Expand/Collapse Sub-Trees; Export to 
JPG; Printing; Extensible Semantic Composition) 
BDU-Tree Module, 
DPT Module, 
Content Module, 
Notes Module, 
Discourse Parsing Service 
Semantic Content 
Inspection 
Support for Feature Structure-like Semantic Con-
tent of LDM Nodes (Node Specific via mouse selec-
tion; F-Structure graphical view; In Place Editing; 
Grammar Condition Querying) 
BDU-Tree Module, 
DPT Module, 
Content Module 
Search Full Text and RE search on: Document content, 
Node Surface Content, Nodes Semantic Content, 
Node-Specific Notes ; Online retrieval of matching 
sources 
Search Module, 
Document Module,  
DPT Module 
U-LDM Rule Editing  Grammar Editor: reusable conditions; easy-to-use 
GUI 
Grammar Module 
Discourse Grammar 
Testing 
(Manual & Scripted) 
Support for Manual Grammar Testing (Check for 
rule enablement between attachment point and M-
BDU selected from actual subtrees; Support Scripted 
Testing with XML Based Language) 
Grammar Module, 
DPT Module, 
Discourse Parsing Service 
Persistence Support Implements and supports serialization and deseri-
alization in LiveTree XML format of LDM Annota-
tion for documents.  
LDM Persistence Service 
Other Functionalities Tree Structure Zooming and Panning, Print Pre-
view Functionalities, Copy/Cut/Paste for text and 
trees) 
Tree Module, 
Document Module, 
BDU-Tree Module 
Table 1: Overview of LiveTree features and the modules or services that provide them. 
 
? Attach the BDU-trees, each one as a 
single unit, to the DPT by computing 
the relationship between the node cor-
responding to the root of a BDU-Tree 
to accessible DCUs aligned along the 
right edge of the DPT using rules of 
discourse relations. There are 3 possi-
ble macro-types of relation: 
Coordination: new unit continues de-
velopment of previous unit 
Subordination: new unit provides 
additional information about previ-
ous unit 
N-ary: new unit bears a special logi-
cal, rhetorical or genre based rela-
tionship to previous unit 
? Once a BDU-tree is attached, its 
leaves become terminal nodes of the 
DPT and nodes on its right edge be-
come therefore accessible for attach-
ment in the next iteration of the proc-
ess. 
3.2 Live Tree Modules for U-LDM 
discourse annotation 
Live Tree Modules (LTM) provide exten-
sive manual and automatic capabilities for 
annotating documents with U-LDM dis-
course tags. They are local to the frame-
work and provide user-directed functional-
ities, relying on mutual interaction through 
the LiveTree GUI. Two modules in Li-
veTree?s current implementation contrib-
ute primarily to discourse annotation (be-
sides the DM): the BDU-Tree Module and 
the DPT Module. 
3.2.1 Discourse Segmentation 
A critical task for U-LDM analysis is to 
account for the availability for update of 
appropriate discourse contexts or sub-
contexts introduced in earlier text. Thus, 
discourse segmentation under the U-LDM 
requires the identification of discourse 
units within the sentence that can function 
as possible attachment points as well as 
segmenting sentential units and non-
sentential structures such as titles from 
other units. The U-LDM may match in-
coming discourse utterances with target 
contexts which are in syntactically subor-
dinated positions within a previous sen-
tence. In order to construct the appropriate 
representation of the rhetorical or semantic 
structure of discourse we must therefore 
keep sub-sentential units available for at-
tachment at independent nodes along the 
right edge of the DPT. 
For discourse segmentation, the U-LDM 
depends on the syntactic analysis of con-
stituent sentences. Initially, sentences are 
divided up into discourse segments reflect-
ing syntactic encodings of minimal units 
of meaning or function. Subsequently, 
some segments are identified as Basic 
Discourse Units (BDUs). Only those dis-
course segments that are of a type that can 
be independently continued are BDUs. 
Operator segments are one example of 
non-BDU segments. Gerunds, nominaliza-
tions, auxiliary and modal verbs or clefts 
are verb based constituents but not seg-
ments because they do not independently 
establish an interpretation context for up-
date by subsequent units (Polanyi, 2004). 
 
Figure 2: A segmented sentence and 
the BDU-Trees corresponding to its two 
coordination-chunks. 
In LiveTree, the BDU-Tree Module 
shown in Figure 2 provides the visual in-
terface and annotation tools for sentence 
segmentation. The top section of the 
BDU-Tree window is composed of two 
areas: a small toolbar, and the sen-
tence/segment viewer. A simple toggle-
button interface allows the user to select 
between automatic or manual segmenta-
tion. In automatic mode, an external Seg-
mentation Service (part of LIDAS) is 
polled and a set of segments retrieved. 
Segments are automatically colored, and 
segments embedding other segments are 
represented by non-contiguous spans of 
text associated by the same highlighting 
color. In manual mode segmentation is 
performed by dragging the divider (the 
rightmost button in the toolbar) to the de-
sired span boundaries and, if necessary, 
assigning non-contiguous spans to the 
same segment using drag-n-drop. 
3.2.2 BDU Tree Construction 
In LIDAS operating in automatic mode, 
BDU-Trees are constructed from seg-
mented sentences by mapping the LFG f-
structure representations of sentential syn-
tax produced by the XLE onto appropri-
ated sentence-level discourse attachments. 
The resulting structure is a BDU-Tree, a 
DPT of an individual sentence. Although 
automatic BDU-Tree parsing can only be 
performed on automatically generated seg-
ments, LiveTree supports manual con-
struction of BDU-Trees regardless of how 
segmentation occurred.  
In manual mode, segments can be 
dragged from the Sentence Viewer area to 
the bottom section of the window. When 
dropped, these become BDU nodes and 
the content of the node can be manually 
annotated. To create the relationship be-
tween two nodes the user drags one node 
over the other as attachment point and se-
lects a preferred relation from a pop-up 
menu. If syntactic/semantic annotations 
are present they are correctly percolated 
and composed throughout the BDU-Tree.  
BDU Trees can be easily edited and 
manipulated for correcting or changing 
annotations, and for improving results 
generated by automatic BDU-Tree pars-
ing. Nodes can be removed, their associ-
ated annotations inspected and modified, 
and the type of relation node changed. 
When the type of a relation node is 
changed, the annotations of all nodes 
dominating the changed relation are up-
dated and the correct syntactic/semantic 
information percolated through the tree in 
accord with the new relation type. Nodes 
and whole sub-trees can be detached and 
reattached at a different point using simple 
mouse gestures. 
3.2.3 Discourse Parse Tree Construc-
tion 
U-LDM discourse parsing is a three step 
process: (1) segmentation, (2) BDU-Tree 
Parsing, and (3) DPT parsing. LiveTree 
supports automatic and manual modes at 
all three stages enabling multiple annota-
tion scenarios.  
For example, users can segment and an-
notate a document entirely by hand, or, 
alternatively, rely on automatic segmenta-
tion and BDU-Tree parsing while manu-
ally completing the more error-prone stage 
of DPT parsing. Another option is to boot-
strap the annotation at every stage using 
LiveTree automatic resources and then 
manually correct mistakes and undesired 
choices (supervised mode). A Discourse 
Segmentation Service and a Discourse 
Parsing Service using two separate dis-
course grammars provide automation. The 
user interfaces of the BDU-Tree Module 
and of the DPT module allow for manual 
and supervised annotation. 
3.3 Discourse Relations under the U-
LDM 
Automatic DPT parsing is rule based. 
Lexical information (synonym, antonym, 
hypernym, discourse connectives), seman-
tics (involving genericity, modality, cardi-
nality, temporal interpretation etc.), and 
syntactic information (including topicali-
zation, grammatical function promo-
tion/demotion, etc.) are used by weighted 
ordered discourse grammar rules to deter-
mine both the site of attachment and the 
relationship obtaining among the nodes. 
Rules may combine different sources of 
evidential information. LiveTree provides 
a complete rule development and testing 
environment used for both theoretical in-
vestigation and automatic parsing.  
When a BDU-Tree is available for at-
tachment, linguistic information available 
at DCUs along the right edge of the DPT 
is compared with evidence retrieved from 
the incoming BDU-Tree to identify se-
mantic information that acts as an ?ana-
phoric anchor? for information in the in-
coming BDU-Tree by examining the 
content of the root node (M-BDU). Each 
attachment rule is checked against infor-
mation available at the M-BDU and at the 
available DCUs and an ordered set of at-
tachment sites and associated relations, as 
specified by the winning rules, is gener-
ated. Local semantic, lexical and syntactic 
information is percolated up through the 
tree according to the constraints of the dis-
course relations at each dominating node. 
If multiple attachments are possible, am-
biguous parses ordered by likelihood are 
generated. In LiveTree operating in auto-
matic mode, the system proposes a pre-
ferred structure. Dispreferred structures 
can be obtained by operating in supervised 
mode. 
3.4 The DPT Module 
The DPT Module shown in Figure 3 
provides the visual representation and ma-
nipulation interface for U-LDM Discourse 
Trees. Advanced viewing capabilities help 
the user analyze large complex discourse 
structures: sub-trees can be collapsed and 
expanded; zooming and panning capabili-
ties and fit-to-page printing are fully sup-
ported. Trees and sub-trees can be moved, 
rearranged, and removed with the same 
editing functions available as in the BDU-
Tree Module. In addition, automatic layout 
capabilities enable even the most graphi-
cally complex discourse structures to be 
displayed clearly.  
4 Discourse parsing with LiveTree 
In order to create an DPT, a user can 
work in different modes. In Fully Auto-
matic (Unsupervised) mode, the user sim-
ply selects a document for full processing. 
The document is tokenized, each sentence 
is automatically segmented, and passed to 
the parser. The discourse parsing service 
automatically creates BDU-Trees from 
each sentence and as trees are created they 
are attached to the emerging DPT. The 
user can then revise the structure and make 
changes2. In Incremental Automatic (Su-
pervised) mode, the user is prompted for 
corrections at selected stages of the proc-
ess. For example, after a sentence is se-
lected by the system for processing, auto-
matically segmented 3 , and parsed into 
BDU-Trees, the user can rearrange nodes, 
change relationships between nodes, and if 
necessary, even merge multiple BDU-
Trees into one. The BDU-Tree(s) might 
then be automatically attached to the DPT 
and the user prompted again to correct any 
mistakes. When the parsing process is su-
pervised in this way, the number of overall 
mistakes is often reduced because attach-
ments occur on incrementally checked 
structures thus maintaining the correct 
open right edge at all times. 
Finally, in Manual DPT Parsing mode, 
BDU-Trees can be dragged from the 
BDU-Tree module to the DPT module and 
manually attached to the DPT however the 
BDU-Trees were computed. The decision 
of how to combine manual and automatic 
processing is made by the user.  
                                                     
2 For large documents problems often arise 
as parsing mistakes build on themselves as the 
right edge changes and large structures are 
harder to examine and manipulate. 
3 Optionally the user can correct any seg-
mentation mistake at this stage, though this 
interrupts the automatic mode and the attach-
ment of the sentence must be completed ma-
nually, since the necessary syntactic informa-
tion is no longer attached to the segments. Of 
course, this information which was likely to 
have been incorrect anyway, thereby necessita-
ting correcting the segmentation. 
        
Figure 3: Two views of a document?s discourse structure. Trees and subtrees can be modi-
fied, rearranged and moved through simple drag ?n drop operations. 
 
4.1 Discourse Grammar Writing and 
Testing 
LiveTree incorporates facilities for writ-
ing, accessing and testing discourse 
grammars both at the sentence and at the 
document level. Rules are edited via a dia-
log window which allows the user to cre-
ate new rules by reusing macros and con-
ditions previously used in other rules. 
Access to all defined types of discourse 
relations is permitted and it is easy to set 
priorities and preempting relations among 
existing rules. 
Rules are tested in two ways. In scripted 
mode, testcase files are written specifying 
exemplary sentences and the discourse 
rule(s) to be tested, along with the ex-
pected outcome. This way, several rules 
and several testcases can be tested auto-
matically at once. A report is created at the 
end of the process with information about 
the outcome of the tests. In manual mode, 
a rule can be selected for testing from the 
Grammar Module and a node or subtree 
from the DPT can be dragged on a candi-
date attachment site. The parser attempts 
to make an attachment using the selected 
rule and reports the result to the user. This 
mechanism has proven very useful during 
grammar creation providing important in-
formation to understand why expected 
structures are not created by the parser. 
5 The PALSUMM Text Summarizer 
The global discourse trees resulting 
from U-LDM parsing in the LiveTree En-
vironment are used for text summarization 
in the PALSUMM System. PALSUMM is 
a hybrid sentence extraction system that 
uses conventional statistical methods to 
identify important information in a text 
and then marks for extraction those dis-
course segments in the DPT that are nec-
essary in order to provide context for ref-
erence and proper resolution of anaphors. 
The goal of PALSUMM Summarization is 
to produce high quality readable summa-
ries. We have tested our summarization 
methods using both manually annotated 
and automatically created U-LDM struc-
tures of Technical Reports taken from the 
FX Palo Alto archive of over 300 reports 
in more than 10 domains of computer sci-
ence. These reports vary in size from a few 
to thirty or more pages. All 300 reports 
have been automatically summarized. Ini-
tial results, though hardly perfect, are en-
couraging.  
6 Conclusion 
LiveTree is a powerful and extremely 
flexible workbench for discourse level 
NLP annotation and parsing tasks. 
Throughout the design and implementa-
tion of LiveTree, our goal has been to sup-
port a full range of work-practices and to 
make sure that annotation steps were inte-
grated in an intuitive and seamless fash-
ion. Services and modules make use of 
available resources efficiently and interop-
erate unobtrusively. New functionalities 
can be easily added on top of existing ones 
and the service-oriented LiveTree archi-
tecture enables concurrent and asynchro-
nous services to be executed locally or 
remotely as automatically generated web 
services. Working in LiveTree has proven 
very efficient without waste of user?s time. 
For example, a document can be parsed 
automatically in the background while 
other tasks such as manual annotation, 
grammar writing or testing are performed. 
While LiveTree has been designed an im-
plemented as a workbench for U-LDM 
analysis, many of the features and aspects 
of the architecture could be adopted for 
use with other analytic frameworks. 
References 
Bernsen, N. O., Dybkj?r, L., and Kolodnytsky, 
M.: An Interface for Annotating Natural In-
teractivity. In J. v. Kuppevelt and R. W. 
Smith (Eds.): Current and New Directions 
in Discourse and Dialogue, Dordrecht: 
Kluwer 2003. Ch. 3. pp. 35?62.  
Bernsen, N. O., Dybkj?r, L. and Kolodnytsky, 
M.: The NITE Workbench - A Tool for An-
notation of Natural Interactivity and Multi-
modal Data. Proceedings of the Third Inter-
national Conference on Language 
Resources and Evaluation (LREC-2002), 
Las Palmas, 2002, 43-49.  
Katherine Forbes, Eleni Miltsakaki, Rashmi 
Prasad, Anoop Sarkar, Aravind Joshi and 
Bonnie Webber. 2003. D-LTAG System - 
Discourse Parsing with a Lexicalized Tree-
Adjoining Grammar  , Journal of Language, 
Logic and Information, 12(3). 
Barbara Grosz and Candace Sidner. 1986. At-
tention, Intention and the Structure of Dis-
course. Computational Linguistics 12:175-
204. 
Bonnie Webber and Aravind Joshi. 1998. An-
choring a Lexicalized Tree-Adjoining 
Grammar for Discourse. ACL/COLING 
Workshop on Discourse Relations and Dis-
course Markers, Montreal, Canada.  
William C. Mann and Sandra A. Thompson. 
1988. Rhetorical Structure Theory: Towards 
a Functional Theory of Text Organization. 
Text 8(3)243-281. 
Marcu, Daniel. 2000. The Theory and Practice 
of Discourse Parsing and Summarization. 
The MIT Press. Cambridge, MA. 
O?Donnell, Michael. 2003. RSTTool. 
(http:www.waysoft.com/RSTTool.) 
Livia Polanyi and Remko Scha. 1984. A syn-
tactic approach to discourse semantics. In 
Proceedings of COLING 6. Stanford, CA. 
413-419. 
Livia Polanyi, Martin van den Berg, Chris 
Culy, Gian Lorenzo Thione, David Ahn. 
2004. A Rule Based Approach to Discourse 
Parsing. Proceedings of SIGDIAL ?04. Bos-
ton MA. 
Hybrid Text Summarization: Combining External Relevance Meas-
ures with Structural Analysis 
Gian Lorenzo Thione,  Martin van den Berg, Livia Polanyi and Chris Culy 
FX Palo Alto Laboratory 
3400 Hillview Ave, Bldg. 4 
Palo Alto, CA 94304 
{thione|vdberg|polanyi|culy}@fxpal.com 
 
Abstract 
In this paper, a novel linguistically advanced text 
summarization system is described for reducing 
the minimum size of highly readable variable -
sized summaries of digitized text documents pro-
duced by text summarization methods that use 
discourse analysis to rank sentences for inclusion 
in the final summary. The basic algorithm used in 
FXPAL?s PALSUMM text summarization sys-
tem combines text structure methods that pre-
serve readability and correct reference resolution 
with statistical methods to reduce overall sum-
mary length while promoting the inclusion of 
important material. 
1 Introduction 
In this paper, we present algorithms to address the 
shortcomings of both purely structural and purely 
statistical methods of sentence extraction summa-
rization. We present the PALSUMM hybrid sum-
marization algorithms that use structural methods 
based on discourse parsing to construct a repre-
sentation of the text, apply conventional statistical 
methods to identify salient information (See dis-
cussion and references in Marcu 2003) and then 
construct a partial discourse tree that includes the 
information identified as most salient along with 
the text at all nodes dominating that salient infor-
mation. Optionally, sentence compression tech-
niques are applied to the resulting summary to 
further compress text length (Grefenstette, 1998; 
Knight and Marcu, 2002).  
The novelty of our approach lies in combining 
text structural methods with sentence extraction 
methods which evaluate relevance on the basis of 
external factors such as lexical frequency or lexi-
cal field information in the specific document, in 
related or documents in general or, alternatively 
by matching lexical items in a query against lexi-
cal items in a document. The sentences selected 
by the external oracle are then providing context 
for anaphora resolution and reference interpreta-
tion through inclusion of hierarchically superordi-
nate information from the structural tree. 
2 The PALSUMM System 
PALSUMM summarization algorithms operate on 
data structures generated by FX Palo Alto?s Lin-
guistic Discourse Analysis System (LIDAS). 
LIDAS is a computational discourse parser im-
plementing the Unified Linguistic Discourse 
Model (U-LDM). A description of the LIDAS 
system and the U-LDM as well as a summary of 
an article from the New Yorker are described in 
earlier work (Polanyi et al 2004a, b, Thione 
2004). Due to space limitations we can only 
sketch the main points of the system here. 
The LIDAS parser itself is purely symbolic. It 
parses a text discourse segment by discourse seg-
ment to construct a tree that captures discourse 
continuity and accessibility relations between the 
segments. The tree identifies what discourse con-
stituents are available for further development and 
what information given by discourse constituents 
is available to be referred to. We use the fact that 
the resulting tree encodes (semantic) accessibility 
relations between the segments, and not rhetorical 
relations, to guarantee that the pruning algorithm 
used to summarize preserve antecedents for ana-
phors thus fostering readability. 
The basic units of this theory (Basic Discourse 
Unit or BDUs) are the syntactic reflexes of lin-
guistically realized minimal semantic unit of 
meaning1 or functions, interpreted relative to the 
context given by the preceding discourse. To iden-
tify the BDUs in a text, LIDAS relies on the 
Xerox Linguistic Environment to parse sentences 
from a text (Maxwell and Kaplan, 1989). After 
sentential parsing is complete, the XLE sentence 
parse trees are segmented into BDUs using a set 
of robust sentence and discourse level rules de-
scribed in detail in Polanyi et al2004a, b. After 
parsing, BDUs (which need not be contiguous) are 
recombined into one or more discourse trees cor-
responding to (parts of) the sentence, called BDU-
trees. 
For each BDU-tree, one BDU, normally the 
main clause of a sentence or a compound unit of 
discourse directly derived from it, is designated as 
the Main-BDU (M-BDU) and is represented by 
the root node of the BDU-tree. The entire BDU-
tree is attached as a unit to the emerging Open 
Right Tree representation of the structure of the 
discourse by relating syntactic, semantic and lexi-
cal information in the M-BDU (and preposed ad-
verbial modifiers, clauses and ?cue? words) to 
information available in nodes along the right 
edge of the tree using formal linguistic discourse 
attachment rules involving relationships among 
semantic, syntactic and lexical information to 
compute both the site of attachment and the at-
tachment relation.  
Although a full discussion of these rules lies 
beyond the scope of this paper, Table 1 sketches 
some simple principles which are both language 
and domain independent.2 
These rules are weighted and ordered in appli-
cation, and multiple rules may ?vote? for the same 
or different attachment points and discourse rela-
tions. The precise relationships among the rules 
remains a subject for future research. 
The U-LDM is similar in form to RST, but its 
primitives are rather different. Whereas RST takes 
rhetorical relations as primitives, the LDM takes 
its primitives from syntactic structure. The ontol-
ogy of LDM relations has three top relations: co-
ordination, subordination and n-ary.  
                                                                 
1 We understand a minimum unit of Meaning to communicate information 
about not more than one ?event? or state of affairs in a ?possible world? of 
some type (roughly event -type predicates); while a minimal Functional unit 
encodes information about how previously occurring (or possibly subsequent) 
utterances relate structurally, semantically, interactionally or rhetorically to 
other units in the discourse or context in which the discourse takes place 
(Greetings, discourse PUSH/POP markers, connectives etc. are all Functional 
segments). 
2 One reviewer remarked, quite correctly: ?how a sentence is attached to the 
emerging representation of the structure of the discourse ? is the heart of  the 
algorithm?. This issue is discussed in detail in Polanyi et al, 2004a,b  ; Thione 
et al 2004. 
Evidence attachment is a subordination 
Syntactic promotion: If the subject of an M-BDU 
co-refers with the object of the AP. 
Sub-cases: If the subject of the M-BDU refers to a 
sub-case of the subject of the AP. Sub-cases include 
subsets (all children /some children), sub-types  (peo-
ple/children), etc. 
Verbal properties: If the tense, aspect, modality or 
genericity of the verbs are different.  
 
Evidence attachment is a coordination 
Narrative: If the verbs express events. 
Lists: If the subjects are synonyms/antonyms and/or 
the syntactic structures of M-BDU and AP are suffi-
ciently similar. 
Table 1: Some simple examples of dis-
course principles 
 
 
Coordinations express a symmetric relation-
ship between the children, including: lists, narra-
tives, etc. Subordinations express an asymmetric 
relationship between children, including: elabora-
tions, interruptions, etc. Finally, n-aries include a 
number of cases where the structure is defined by 
specific language constructions. Note that these 
constructions are not arbitrary, and often follow 
from (sentence) syntactic constructions. Examples 
include scope setting operators and units (when 
john comes, he will be happy), and more or less 
fixed forms like greetings and question-answer 
pairs, etc. It is the practice to also consider genre-
specific structures (e.g. ?a paper consists of a title, 
an abstract, an introduction, some sections, a con-
clusion, and references?) to be n-aries. 
Because to characterize the large structure of 
the discourse we only need to refer to coordina-
tions, subordinations and n-aries, it is often 
claimed that the number of relations in the LDM 
is much smaller than in RST, even though strictly 
speaking this depends on which versions of LDM 
and RST one compares. The real difference be-
tween the two theories lies in the rather different 
origins of the rules. 
All non-terminal nodes in U-LDM trees are 
first class citizens and contain, in addition to a 
node label, content and context information inher-
ited from child nodes. Under RST only terminal 
nodes have content; non-terminal nodes that rep-
resent the relationships obtaining among spans of 
the text longer than one sentence are labeled for 
the relationship between daughter nodes only.  
As in Summarist (Hovy and Lin, 1999), once 
the source text has been parsed and a discourse 
tree incrementally constructed, text summarization 
algorithms are applied to the resulting tree. How-
ever, the difference between constructing a 
semantic rather than a rhetorical representation of 
the text  accounts for how PALSUMM summaries 
preserve readability and reference resolution: be-
cause the entire analysis involves matching se-
mantically defined contextual units to the 
appropriate contexts available on the tree, nodes 
that structurally dominate other nodes necessarily 
contain the information needed to contextually 
interpret the dominated units. 
3 Pruning PALSUMM Trees 
Summarization methods based on discourse struc-
ture all rely on assigning a numeric value to all 
intermediate and leaf nodes encoding their impor-
tance , based on the labels at the nodes. The dif-
ference between different methods orig inates in 
the different ways this importance measure is cal-
culated. Because RST (Marcu, 2000) and U-LDM 
trees differ, there are key differences between the 
simple pruning methods applied to U-LDM trees 
as opposed to RST trees. 
Under the U-LDM theory of discourse, the 
asymmetric relationship expressed by subordina-
tions implicitly encodes a notion of importance. 
The subordinated child elaborates or further quali-
fies the head, or temporarily interrupts the flow of 
discourse. Subordinated material is almost always 
less important to the main line of the text than 
subordinating material: the level of embedding 
thus gives a first rough measure of importance of 
a unit of discourse.  
Our original summarization algorithm, Sym-
Trim, used the level of embedding directly. It 
pruned the tree at a given level of embedding, and 
generated a summary based on the span of the 
remaining tree. The number of possible summary 
lengths, however, was restricted to the number of 
embedding levels, resulting in a discreet number 
of summaries of a fixed length, often ones longer 
than desired. This led to a need for more subtle 
pruning algorithms. 
3.1 Solving the SymTrim Restriction 
There are two theoretic problems that underlie the 
practical problems of SymTrim. First, across the 
board pruning at a fixed level is of limited utility. 
If two sections of a document differ significantly 
in size, the larger section will have more space for 
deeper sub-trees. Consequently, units of equal 
importance may occur at deeper levels of larger 
sub-trees. 
Secondly, no method that relies solely on 
purely structural information can determine what 
parts of the document contain important informa-
tion. For this an approximation the meaning of the 
units is needed. A description of the relationships 
among them does not suffice. 
We address the first issue by not trimming the 
tree at an absolute level, but at a level relative to 
the depth of the sub-branch in which a node is 
found. We address the second issue by skewing 
the pruning level using statistical methods3 as an 
oracle to indicate relative importance. 
3.2 Score Adjustment and Percolation 
We assign every node a relative depth T(l), based 
on the local and global structure of the tree branch 
to which it belongs, calculated as follows: (1) es-
tablish the absolute depth D(l) of each node, (2) 
calculate an embedding branch weight W(l) by 
percolating the value of D(l) up from the leaves 
according to the percolation algorithm outlined in 
Figure 1, (3) assign each node a relative depth T(l) 
= 1 ? (D(l) ? 1) / W(l).4 
We also compute a statistical score that ap-
proximates the ?semantic importance? of every 
node. To do so, we begin by seeding every leaf 
node l with a statistical seed S(l) using the MEAD 
statistical summarizer. Each segment is scored by 
MEAD in the context of the full document, with a 
score that mirrors its judgment of the relevance of 
that segment for a summary. MEAD?s metrics 
include: TF/IDF cosine similarity between a seg-
ment and the document ? optionally skewed to-
wards a query entered by the user, the relative 
position of a segment within the document, an 
adverse score against segments deemed as too 
similar to the current summary, and our own im-
plementation of a feature concerning the presence 
of certain cue words (Hirschberg and Litman, 
1993). After scoring, the values are percolated up 
through the tree, as before. During percolation of 
both structurally and statistically obtained scores, 
the new value of a node that receives a higher 
score from a child node is percolated downwards 
through all non-subordinated children. Children of 
                                                                 
3 We use the publicly available MEAD (Radev et al 2003). Adopting a sen-
tence extraction approach, it is capable of assigning scores to each and every 
sentence. PALSUMM does its own discourse segmentation and sends the 
segments to MEAD as if they were sentences. This allows us to assign inde-
pendent scores to discourse segments, thus enabling sub -sentential summariza-
tion (segment-extraction vs. sentence extraction) and yielding more 
compressed yet still highly readable summ aries. 
4 The expression for T(l) was chosen to assign the top node relative depth 1.  
coordinations and n-aries are considered equally 
relevant and scored equally, whereas subordinated 
children are less relevant then subordinating 
ones.5 After percolation we normalize the statisti-
cal scores, dividing by the maximum occurring 
value. 
Different summarization algorithms result from 
the choice of seeding algorithms and methods of 
combining scores. Note that the percolation algo-
rithm in Figure 1 respects structural embedding by 
always assigning lower or equal scores to subor-
dinated nodes.  
3.3 Pruning Algorithms 
In order for summaries to maintain textual coher-
ence and readability, constituents that contain con-
textual or referential information necessary to 
interpreting other constituents selected for the 
summary must be marked for inclusion. For any 
node, this information is available in nodes that 
are siblings of the same coordination or n-ary, and 
in nodes that dominate it through subordination-
type relations. As long as the score assigned to 
nodes respects subordinations as in Figure 1, any 
pruning of the tree that excludes constituents 
whose final relevance score is smaller than a cho-
sen value is guaranteed to preserve the antece-
dents for the anaphora in the text, preserving well-
formedness of the resulting tree and the readabil-
ity of the summary it yields.  
In Table 2 we list four different final score as-
signments, based on the embedding level of the 
nodes (L), their percolated statistical score (S) and 
the percolated relative depth score (T). 
 
SymTrim F = 1/L 
SymTrim-R F =1/T 
HybReduce F = 1/L * S 
HybReduce-R F = 1/T * S 
Table 2: Different scoring algorithms. 
                                                                 
5 In a modified percolation scheme, downward percolation is restricted to 
preceding siblings in discourse-level coordination nodes. This is a result of the 
fact that contextual information necessary to preserve readability and referen-
tial integrity must appear before access. 
 
After scores are calculated and combined, a 
relative threshold is computed by sorting the set of 
constituent by final score and identifying the cut-
off value that more closely approximates the re-
quest of the user in terms of desired summary 
length. Note that the root node will always have 
normalized score 1 and will therefore always be 
included in a full summary. 6 
4 Evaluating PALSUMM  
The PALSUMM corpus contains over 300 
FXPAL Technical Reports in a wide range of do-
mains. The Reports vary in size from 10 to 30 
pages. To evaluate the readability of summaries 
and create a baseline for evaluating the SymTrim-
R and HybRduce-R algorithms, we conducted a 
small pilot study on five documents selected from 
the corpus. The documents were hand-annotated 
with their U-LDM discourse structures. The Sym-
Trim-R and HybReduce-R variants were then 
automatically applied to these discourse struc-
tures, and the summaries submitted to a panel of 
12 non-experts.  The panelists were asked to judge 
the summaries on a 6-point scale for readability 
by answering a set of questions including "How 
readable is this summary?" and "Did you get con-
fused at any point in the summary?? The initial 
results suggest that the discourse algorithms pro-
duced readable summaries and that the relative 
effectiveness of the discourse algorithms varies 
according to some still to be determined property 
of the documents. 
5 Conclusion 
Structural sentence extraction systems including 
Summarist and PALSUMM that create summaries 
by choosing sentences or parts of sentences corre-
sponding to nodes at a given level of depth of a 
                                                                 
6 In other applications of the algorithms described here, where the purpose is 
not that of retrieving a full summary of a document but rather that of building 
the necessary minimal context for interpreting a certain selected discourse 
constituent, percolation is only limited to the immediate surrounding context, 
where certain relations (usually ad-hoc binaries) constitute a barrier to further 
percolation towards upwards constituent. 
1. For seeding V, each leaf node l is assigned an a priori score V(l). 
2. Repeat for each node c0 with children c1 ?cn, and relation type R until  no values change: 
2.1 Percolate or maintain highest score: V(c0) := maxi (V(ci)) , 0=i=n 
2.2 Percolate highest score downwards into non-subordinated nodes: 
if R is subordination and ci is  the head of n: V(ci) := V(c0) if V(ci) < V(c0) 
if R is coordination or n-aries: for all i=n, V(ci) := V(c0) if V(ci) < V(c0), 
Figure 1: General percolation algorithm. Both statistical seeds (V=S) and structural seeds 
(V=T) are percolated according to this algorithm, resulting in values S(n) and T(n) for nodes n. 
 
tree structured representation of the structure of 
the text produce excellent summaries that preserve 
the style and ?flavor? of the original text. How-
ever, the summaries constructed may be longer 
than needed, including information that could be 
omitted without serious loss of informativity7. The 
excessive length results from the top-down nature 
of standard structural extraction algorithms which 
start by choosing the top context and then includes 
every possible sub-context down to a certain level. 
In this paper, we have proposed hybrid algo-
rithms which capitalize on the strengths of these 
methods while compensating for their limitations 
by proposing additional manipulations on the base 
trees. In our view, the value of the summarization 
methods described here, is the ability to compress 
a summary further without substantia l loss of in-
formativity. For summaries, especially those de-
signed for display on various sized devices, the 
work presented here constitutes an advance in the 
state of the art. 
6 Acknowledgements 
The authors would like to thank Dr. Sara Bly 
(Sara Bly Consulting) who designed and carried 
out the evaluation and Dr. Candace Kamm of 
FXPAL who provided help and guidance in the 
design and organization of the study. 
7 References 
Gregory Grefenstette. 1998. Producing intelligent 
text reduction to provide an audio screening 
service for the blind. Working Notes of AAAI 
Spring Symposium on Intelligent Text Summa-
rization, Pages 111?118,  Stanford. 
Julia Hirschberg and Diane Litman. 1993. Empir i-
cal studies on the disambiguation of cue 
phrases. Computational Linguistics, 19-3:501-
530.  
Ed Hovy and C-Y. Lin. 1999. Automated Text 
Summarization in SUMMARIST. In I. Mani 
and M. Maybury (eds), Advances in Automated 
Text Summarization, pages 81-94, MIT Press, 
Cambridge.  
Kevin Knight and Daniel Marcu. 2000. Statistics 
Based Summarization ? Step One: Sentence 
Compression. In AAAI-2000 Proceedings, 
pages 703-710, Austin TX. 
                                                                 
7 Some of this excessive length can be addressed through compressing less 
relevant aspects of constituent sentences as in Grefenstette, 1998; Knight and 
Marcu, 2002. 
Daniel Marcu. 2000. The Theory and Practice of 
Discourse Parsing and Summarization. The 
MIT Press,  Cambridge, MA. 
Daniel Marcu. 2003. Automatic abstracting. In 
Encyclopedia of Library and Information Sci-
ence, pages 245-256.  
John Maxwell and Ronald M. Kaplan. 1989. An 
overview of disjunctive constraint satisfaction. 
In Proceedings of the International Workshop 
on Parsing Technologies. Pittsburgh, PA. 
Livia Polanyi, Martin van den Berg, Chris Culy, 
Gian Lorenzo Thione, and David Ahn. 2004a. 
A rule based approach to discourse parsing. 5th 
SigDial Workshop. 
Livia Polanyi, Martin van den Berg, Chris Culy, 
and Gian Lorenzo Thione 2004b. Sentential 
structure and discourse parsing. Discourse An-
notation Workshop, ACL04. 
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, and John Blitzer, Arda ?elebi, 
Elliott Drabek, Wai Lam, Danyu Liu, Hong Qi, 
Horacio Saggion, Simone Teufel, Michael 
Topper, and Adam Winkel. 2003. The MEAD 
Multidocument Summarizer. 
http://www.summarization.com/mea
d/ 
Radu Soricut and Daniel Marcu. 2003. Sentence 
level discourse parsing using syntactic and lexi-
cal information. In Proceedings of 
HLT/NAACL, May 27-June 1, Edmonton, Can-
ada. 
Gian Lorenzo Thione, Martin van den Berg, Livia 
Polanyi, and Chris Culy. 2004. LiveTree: An 
integrated workbench for discourse processing. 
Discourse Annotation Workshop, ACL04.  
 A Rule Based Approach to Discourse Parsing 
Livia Polanyi, Chris Culy, Martin van den Berg, Gian Lorenzo Thione, David Ahn1 
FX Palo Alto Laboratory 
3400 Hillview Avenue, Bldg 4 
Palo Alto CA 94304 
{polanyi|culy|vdberg|thione}@fxpal.com ; ahn@science.uva.nl 
 
 
Abstract 
In this paper we present an overview of 
recent developments in discourse the-
ory and parsing under the Linguistic 
Discourse Model (LDM) framework, a 
semantic theory of discourse structure. 
We give a novel approach to the prob-
lem of discourse segmentation based 
on discourse semantics and sketch a 
limited but robust approach to sym-
bolic discourse parsing based on syn-
tactic, semantic and lexical rules. To 
demonstrate the utility of the system in 
a real application, we briefly describe 
the architecture of the PALSUMM sys-
tem, a symbolic summarization system 
being developed at FX Palo Alto Labo-
ratory that uses discourse structures 
constructed using the theory outlined 
to summarize written English prose 
texts. 1 
1 Introduction 
In this paper we present an overview of recent 
theoretical and computational developments in 
discourse theory and parsing under the Linguistic 
Discourse Model (LDM) framework, a semantic 
account of discourse structure. In Section 2, we 
                                                          
1 Current address:  
Language and Inference Technology Group,  
ILLC, University of Amsterdam,  
Nieuwe Achtergracht 166,  
1018 WV Amsterdam, The Netherlands 
present an overview of what we will term the 
Classical LDM (C-LDM) and identify critical 
problems encountered in implementing the 
model: the difficulty in segmenting complex 
sentences within a text and calculating the at-
tachment site and relationship of an incoming 
unit to an appropriate node in a developing Dis-
course Tree. In Sections 3 and 4 we introduce the 
Unified Linguistic Discourse Model (U-LDM) 
that incorporates solutions to these problems. 
Specifically, in Section 3 we describe a novel 
approach to discourse segmentation based on the 
relationship of sentential syntax to discourse 
semantics. In Section 4, a limited but robust ap-
proach to symbolic discourse processing based 
on syntactic, semantic and lexical rules is given. 
In Section 5, we sketch the architecture of the 
PALSUMM system, a summarization system 
being developed at FX Palo Alto Laboratory that 
uses algorithms operating on discourse represen-
tations generated by a U-LDM parser to summa-
rize written English prose texts. In Section 6 we 
present our conclusions and suggest directions 
for future research. 
2 The Classical Linguistic Dis-
course Model (C-LDM) 
Unlike the Discourse Structures Model (DSM) of 
Grosz and Sidner (1986), a pragmatic and psy-
chological theory that aims to clarify the rela-
tionship between speakers? intentions and their 
focus of attention in discourse, or the rhetorical 
model of Rhetorical Structures Theory (Mann 
and Thompson, 1988) that is designed to identify 
the coherence relations between segments of 
text, the Linguistic Discourse Model (LDM) 
(Polanyi and Scha, 1984; Polanyi, 1988; Polanyi 
 and van den Berg, 1996) is a syntactically in-
formed, semantically driven model developed to 
provide proper semantic interpretation for every 
utterance in a discourse despite the apparent dis-
continuities that are present even in well struc-
tured written texts. In its focus on understanding 
discourse meaning, the LDM is close in spirit to 
Structured Discourse Representation Theory (S-
DRT) (Asher, 1993). While S-DRT attempts to 
account for discourse structure purely semanti-
cally, the LDM framework is concerned to main-
tain a separation between discourse ?syntactic? 
structure, on the one hand, and discourse inter-
pretation on the other. Therefore, like DSM and 
RST, the LDM incorporates an explicit tree 
structured model of relationships between dis-
course segments as its model of discourse ?syn-
tax?. In discourse parsing under the LDM, any 
attachment to the developing discourse tree of a 
textual unit is treated as an instruction to update 
an appropriate semantic representation. We con-
struct dynamic semantic representations (DSRs), 
similar to the Discourse Representation Struc-
tures (Kamp, 1981; Kamp and Reyle, 1993) used 
in S-DRT as its model of discourse semantics. 
The DSRs correspond to the contexts relative to 
which subsequent segments can be interpreted.  
The analysis of intra-sentential structure is 
done by sentential syntax which identifies the 
syntactic and semantic structures within the sen-
tence and makes the resulting analysis available 
for discourse processing. 
2.1 Overview of the Classic LDM 
In the Linguistic Discourse Model (LDM) dis-
course is formed through the recursive combina-
tion of discourse constituent units (DCUs). The 
structure of a discourse is represented by an open 
right tree of DCUs. Basic discourse units 
(BDUs), resulting from a segmentation of the 
discourse according to rules of discourse seg-
mentation, form the content of the leaves of the 
tree. Once a text has been segmented into BDUs, 
an open right tree representing the structure of 
the discourse is built up. The completed tree 
shows, for any given point in the discourse, 
which discourse units (DCUs) remain available 
for continuation and which DCUs are no longer 
available. Because discourse anaphora resolution 
is critically constrained by discourse structure, 
the tree representation makes clear the domain in 
which the antecedent for a given anaphoric refer-
ential expression is to be found. Antecedents 
must be available at a node along the right edge 
of the discourse tree. (Polanyi, 1985; Grosz and 
Sidner 1986; Webber, 1991) 
The LDM posits three structural relations be-
tween discourse units: 
 
1. discourse coordination  
a. Units related by bearing a similar rela-
tionship to an existing or newly formed 
common parent in the tree (lists, narra-
tives).  
b. Available at the C-node is information 
common to all child nodes. 
2. discourse subordination  
a. Units related by an elaboration relation-
ship in which the subordinated unit pro-
vides more information about an entity 
or situation described in the subordinat-
ing unit. 
b. Units unrelated to existing units avail-
able on the right edge of the tree, 
viewed as intrusions or interruptions.  
c. Available at the S-node is information 
specific only to the subordinating or 
dominant constituent (usually the left 
child). 
3. n-ary constructions 
a. Units related by logical or rhetorical, 
genre or interactional conventions spe-
cific to a given language. 
b. Preposed modifier, sentence initial ad-
verbial, ?cue word?, (reported speech) 
attribution phrase. 
c. Available at N-nodes is information 
about each constituent and the relation-
ship connecting them. 
 
Although we believe that the general approach to 
discourse structure captured by the Classical 
LDM is essentially sound, there are three critical 
problems with the existing framework:  
 
1. Segmenting the incoming text into BDUs 
2. Determining the existing or new node at 
which to attach an incoming BDU  
3. Determining the relationship between the 
incoming BDU and the attachment node 
 
Although very difficult challenges associated 
with each of these discourse parsing tasks re-
main, in developing the Unified Linguistic Dis-
course Model (U-LDM) we have made 
significant progress recently on solving them. 
These are discussed in Sections 3 and 4 below. 
3 Discourse Segmentation  
The problem of segmenting discourse into the 
elementary units appropriate for building up the 
 structure of the discourse is an extremely diffi-
cult one. Each discourse theory must specify 
how ?segments? should be identified in light of 
the questions the theory is set up to answer. 
Models based on Grosz and Sidner?s 1986 work, 
especially those which form the basis of spoken 
language systems, define segments in terms of 
the intentions of the speaker: when the speaker?s 
intention shifts, the segment associated with that 
intention ends and immediately following talk is 
included in new (or resumed) segments. While 
very useful in dealing with task oriented talk 
where speakers move between asking questions, 
informing others and giving commands, this 
model is less applicable to determining discourse 
segments within a sentence. The problem is an 
acute one for the analysis of written texts be-
cause often a subsequent, not necessarily adja-
cent, segment will continue the development of 
material introduced in a sub-sentential, often 
subordinate, constituent. Construction of the 
appropriate representation of the rhetorical or 
semantic structure of discourse must therefore 
keep sub-sentential units available for attach-
ment at independent nodes on the tree along. The 
entire sentence or sentential main constituent 
must also be available to be continued after any 
continuation on sub-sentential units has been 
completed. As reported by Carlson et al (2003), 
under RST2, lexical and syntactic information 
used to segment discourse into Elementary Dis-
course Units (EDUs) is based on verbal constitu-
ents including clauses and infinitives.3  
As we show below, the approach taken to 
segmentation under the U-LDM, while it in-
cludes as segments (and non-segments) many of 
the constructions currently used in RST, pro-
vides a rationalization for the choice of units. 
Rather than posit which syntactic objects func-
tion as discourse segments, we started by estab-
lishing the semantic basis for functioning as a 
segment and then identified which syntactic con-
structions carry the semantic information needed 
for discourse segment status. We then identified 
as Basic Discourse Units (BDUs) segments that 
have the potential to independently establish an 
anchor point for future continuation. We then 
drew a further distinction between BDUs as a 
class of syntactic structures with the potential to 
                                                          
2  Under S-DRT, no explicit structural tree is 
constructed and no explicit segmentation criteria 
have been proposed in the literature. 
3 Although some clauses are not treated as ele-
mentary units and ?a small number of phrasal EDUs 
are allowed, provided that the phrase begins with a 
strong discourse marker.? 
establish anchor points and the actual BDUs in a 
given sentence which can function as indexical 
anchor points in a specific discourse. We believe 
these distinctions, while cumbersome, are neces-
sary for both theoretical and practical text analy-
sis. 
3.1 Discourse Segments under the U-LDM 
As a semantic theory, the U-LDM must account 
for the interpretation of utterances. Specifically, 
we must account for the availability for update of 
appropriate discourse contexts or sub-contexts 
introduced in earlier text. In order to do so, we 
must be able to match incoming discourse utter-
ances with their target contexts, some of which 
may have been introduced in syntactically sub-
ordinated positions within a sentence. Therefore, 
in designing U-LDM discourse segmentation, we 
have identified the syntactic reflexes of the se-
mantic content of the linguistic or paralinguistic 
phenomena making up discourse. 
 Since elementary discourse units are needed 
to build up discourse structure recursively, we 
have identified as discourse segments the syntac-
tic constructions that encode a minimum unit of 
meaning and/or discourse function interpretable 
relative to a set of contexts. We understand a 
minimum unit of meaning to communicate in-
formation about not more than one ?event?, 
?event-type? or state of affairs in a ?possible 
world? of some type4. Clauses, and many other 
verb based structures, carry indexical informa-
tion that ties the content to the context in which 
it is to be interpreted. Minimal functional units, 
on the other hand encode information about how 
previously occurring (or possibly subsequent) 
linguistic gestures relate structurally, semanti-
cally, interactionally or rhetorically to other units 
in the discourse or to information in the context 
in which the discourse takes place5. 
 Examples of discourse segments are given in 
Table 1. Note that while discourse segments un-
der the U-LDM are the syntactic reflex of a lin-
guistically realized semantic ?gesture? 
interpreted relative to context, they need not be 
contiguous, but may completely surround an-
other segment (e.g. an appositive, or non-
restrictive relative clause.) Discontinuous seg-
                                                          
4  Roughly speaking an ?elementary proposi-
tion?, ?event-type predicate? etc. In a Davidsonian 
style semantics, quantification over an event vari-
able signals a separate unit of meaning. 
5 Greetings, discourse PUSH/POP markers and 
other ?cue phrases?, connectives etc. are all func-
tional segments. 
 ments occur when there is overt material on both 
sides of the intervening segment. With fragmen-
tary segments, the full interpretation remains 
unrecoverable from surrounding context. For 
example: a single word answer to a question is a 
complete segment, whereas the same word ut-
tered but ?left hanging? would be an un-
interpretable fragment. (See Appendix for exten-
sive example of a segmented text.) 
3.2 Basic Discourse Units 
An important contrast between the U-LDM and 
other approaches to segmentation concerns the 
distinction made in the U-LDM between dis-
course segments such as those we have identified 
above and Basic Discourse Units (BDUs). While 
all BDUs under the U-LDM are segments, not 
all segments are BDUs. BDUs, under this model, 
are discourse segments of a type that can be in-
dependently continued: operator segments are 
one example of non-BDU segments. Other verb 
bases constituents that might be expected to be 
segments are not because they do not establish 
an interpretation context independent of other 
segments that can be updated by subsequent 
units. In general, these ?notable non-segments?, 
summarized in Table 2, are heavily integrated 
into other nominal or verbal constructions and 
cannot be accessed for independent continuation. 
 
Non-segments Examples 
Gerunds [Singing is fun.] 
Nominalizations [Rationalization is useless.] 
Auxiliary and 
modal verbs 
[I might have succeeded.] 
Clefts [It was the tiger that we liked best.] 
Table 2. Notable non-segments (underlined). 6 
                                                          
6 In answer to a reviewer who asked if in "Sing-
ing is fun", singing should not be an independent 
In order to account for continuation in specific 
sentences, we further identify one class of in-
stances of BDU: Active BDUs (A-BDUs) are 
BDUs on the right edge of a discourse tree. The 
main clause of any sentence will be an A-BDU 
and, depending on the deployment of BDU seg-
ments within a given sentence, other BDUs may 
also be accessible for continuation. (See Section 
4 below.)  
4 Discourse Parsing with the 
U-LDM 
Ascertaining the relationship of a BDU to the 
discourse is a complex parsing process involving 
lexical, semantic, structural and syntactic infor-
mation7. For the case of written prose we are 
concentrating on here, the unit of analysis is the 
sentence (or sentence fragment). Sentences are 
attached to the DPT of the text as a unit8. Dis-
course attachment of the sentence involves two 
decisions: where along the right edge to attach, 
and what is the relationship to the attachment 
point. The process, which includes constructing a 
BDU tree of the sentence, can be summarized as 
follows: 
                                                                                
segment, we would answer that this sentences con-
cerns one eventuality (something being fun), not two. 
Since any noun can be referred to by a pronoun in 
the next sentence simply referring to the noun is not 
equivalent to referring to the eventuality in which 
the referent of the noun is a participant. 
7 Although the linguistic (and lexical) informa-
tion we discuss could be augmented with processes 
relying upon high level world knowledge and infer-
ence, we believe that it is extremely significant to 
see how far one can get with discourse parsing with-
out invoking non-linguistic information. 
8 See discussion of MBDU below. 
 
Segments Common realizations Examples 
Content segments 
Clauses: main, subordinate [I heard the dog] [that was barking.] 
Predication [California elected Schwarzenegger] [governor] 
Participial modifiers [The donkey [braying next door] was annoying.] 
Eventualities (activities or 
states) and their participants. 
Infinitival modifiers [We persuaded them] [to leave.] 
[They left] [to get the tickets.] 
Parentheticals [The show [(and what a show it was)] lasted 4 hours.] 
Appositives [The building, [an example of the Mozarabic style,] was 
recently restored.] 
Interpolations 
Interruptions [They were [? Stop that! ?] leaving at 8:00.] 
Fragments Section headings [4. Discussion] 
List items [e.g., [hydrogen,] [helium]]  
?Restarted? material [My dog,] [no,] [my cat ran away.] 
Operator Segments 
Conjunction Conjunctions [We arrived] [and] [got seats.] 
Discourse operators ?scene-setting? preposed modifi-
ers  
[On Tuesday,] [we will see the sites.] 
 ?cue? words  [Anyway,] [we did get there on time.] 
Table 1. Examples of Discourse Segments, Unlabelled bracketing is used to indicate segments. 
  
? Identify potential BDUs within sentence 
using sentential syntax  
? Construct a BDU-tree from the segments of 
the sentence, using sentential syntactic in-
formation and discourse rules to map seg-
ments and relationships among them. This 
BDU-tree is itself an Open Right Tree domi-
nated by the node corresponding to the Main 
clause of sentence9. (This is the Main BDU 
or MBDU). 
? Attach the BDU-tree as a unit to the Dis-
course Parse Tree by computing the rela-
tionship of MBDU and preposed modifiers, 
if any, to accessible DCUs aligned along the 
right edge of the tree using rules of dis-
course relations (See Section 4.1 below). 
Lexical information used for attachment de-
cisions can come from anywhere in the 
BDU tree. 
? Once the BDU-tree is attached, its terminal 
leaves are terminal nodes of the Discourse 
Parse Tree (DPT) and any terminal or inter-
mediary nodes on the right edge of the BDU 
tree are DCUs on the DPT accessible for at-
tachment in the next iteration of the process. 
 
In order to determine which accessible DCUs are 
candidates for M-BDU attachment and what re-
lationship obtains between the incoming unit and 
the selected DCU, a number of distinct types of 
evidence are used, including: 
 
1) lexical information 
reuse somewhere in the BDU tree of the 
same lexeme, synonym/antonym, hypernym, 
or participation in the same lexical frame or 
?semantic field? as item in target node. 
2) syntactic information 
parallel syntactic structure; topic/focus and 
centering information, syntactic status of re-
used lexemes, pre-posed adverbial constitu-
ents, etc. 
3) semantic information 
realis status, genericity, tense, aspect, point 
of view etc. in the MBDU 
                                                          
9 This process is too complex to describe in de-
tail here but it involves looking at both the F-
structure of the sentential parsing information re-
turned by the XLE and applying discourse rules to 
the BDUs identified. Soricut and Marcu (2003) also 
build up RST sentential trees to use in discourse 
parsing. Both the information and methods used to 
construct RST trees as well as the trees themselves 
differ from ours.  
4) constituents of incomplete n-ary construc-
tions on the right edge 
Questions, initial greetings, genre-internal 
units like sections and sub-sections, etc. 
5) structure of both the local attachment 
point and the BDU-tree 
 
While we are still experimenting with under-
standing the complexities involved in attach-
ment, we believe that different types of evidence 
have different weights10 and that the combined 
weight of evidence determines the attachment 
point. We have noted, however even at this stage 
of our investigations, that the weight given to 
each type of information differs for attachment 
site selection and relationship determination. 
Lexical information, for example, is often very 
important in determining site, while semantic 
and syntactic information is most relevant in 
determining relationship. In the remainder of this 
section we will give a small set of robust rules 
for determining the attachment site and relation-
ship of an incoming BDU-tree to the existing 
parse tree of the discourse.  
4.1 Rules for Determining Discourse 
Attachment Site Candidates and 
Attachment Relations 
Both the attachment site choice and the actual 
attachment process rely on partially ordered sets 
of hybrid rules, each of which are conditioned on 
a set of constraints. Constraints for rules used in 
attachment site selection are primarily lexical 
constraints, although other information is also 
relevant.  
All types of evidence play a role in choosing 
the attachment relation. A rule is a pair: Rule 
<C, O> where C is the set of constraints that 
enable the rule and O is the associated operation. 
The operation associated with a rule can there-
fore be either the markup of a DCU as a possible 
attachment site, or an actual discourse relation, 
such as Subordination, Coordination or N-ary. A 
rule is enabled when all sub-conditions in C are 
satisfied and no other rules having priority are 
enabled. Rules may combine different sources of 
evidential information (semantic, syntactic, 
structural and lexical). If more than one rule is 
enabled at the same time, ambiguous parses are 
produced11. Some rules are listed in Table 3. 
                                                          
10 We assign weights heuristically at this point. 
11 At this stage in our research, we rely only on a 
partial order among the rules. In future work, we 
will investigate (1) how evidence is weighed and 
combined in order to make better attachment deci-
 The parsing process at the Discourse Parse 
Tree (DPT) level works as follows. When a 
BDU-Tree has been constructed and is ready to 
be attached to the right edge of the DPT, each 
DCU along the right edge is examined and the 
lexical information in the right-edge DPT nodes 
are compared with the lexical evidence retrieved 
                                                                                
sions and (2) the extent to which discourse ambigu-
ity generated in this fashion is legitimate and how to 
reduce grammar overgeneration by more efficient 
handling of interactions among rules and the weigh-
ing of the linguistic evidence. 
from the incoming BDU-Tree. This process, 
guided by the set of discourse rules, produces an 
ordered set of active DCUs, representing the 
possible attachment points in order of likelihood. 
The set can then be pruned of its n lowest scor-
ing constituents, according to an appropriate 
policy such as a threshold. 
In a second stage, each attachment rule is 
checked against possible attachment sites. Rules 
that fire successfully attach the BDU-Tree to the 
DPT at the chosen site with the relationship 
specified by the rule. Local semantic, lexical and 
syntactic information is then percolated up to the 
Attachment Relation Sub Conditions 
Nary-Attachment 
Frame(AP,MBDU) matches genre-specific construction 
Greetings, Argument, Question/Answer, Speech Event,  
Genre Meta Structure(Story, Technical Paper, Lecture, etc..) 
Reported speech/reporting clause 
Subordination M-BDU Realis status differs from Status of AP (MBDU is Irrealis; AP is Realis OR MBDU is Realis; AP is Irrealis) 
Nary-Attachment 
(intrasentential) 
Tense(AP) = past  
Tense(MBDU) = pluperfect  
AP is time-reference for MBDU 
Nary-Attachment 
(intrasentential) 
VerbClass(AP)=?SpeechAct? 
Type(MBDU) = ADJUNCT  
Nary-Attachment 
(intrasentential) 
Tense(AP) = present  
Tense(MBDU) = past  
AP is time-reference for MBDU 
Coordinate 
Parent(AP) is Coordination 
Parent(AP) would coordinate with MBDU  
AP would coordinate with MBDU 
Subordination 
Tense(AP) = past 
Genericty(AP) = specific 
Tense(MBDU) = present 
Genericty(MBDU) = generic 
Subordination M-BDU genericity status differs from Status of AP (MBDU is specific; AP is generic OR MBDU is generic; AP is specific) 
Subordination SUBJ(MBDU) = OBJ(AP) 
Subordination SUBJ(MBDU) = XCOMP(AP) 
Subordination MBDU/Lexeme is a subcase of AP/Lexeme Role(AP/Lexeme) = Role(MBDU/Lexeme) 
Right Headed Subordination 
(intrasentential) 
Type(AP) = ADJUNCT 
Type(MBDU) = S 
Nary-Attach 
(intrasentential) 
PRED(ADJUNCT(AP)) = ?if? 
AP is Irrealis 
MBDU is Realis 
Nary-Attachment 
(intrasentential) AP and MBDU related by logical connective (cf Webber& Joshi, 1998; Forbes (2003) 
Subordination  Tense(AP) = past Tense(MBDU) = pluperfect 
Subordination Tense(AP) = present Tense(MBDU) = past 
Subordinate AP is Bottom of DPT M-BDU is Footnote or Parenthetical 
Coordinate AP is Narrative( = Specific, punctual ,event) MBDU is Narrative 
Coordinate Tense(AP) = Tense(MBDU) Aspect(AP) = Aspect(MBDU) 
Coordinate MBDU/Lexeme is synonym or antonym of AP/Lexeme Role(AP/Lexeme) = Role(MBDU/Lexeme) 
Subordinate AP is Bottom of DPT 
Table 3. Discourse Attachment Rules ordered to express priority of the rules. AP denotes (potential) 
attachment point.  
 
 DCU consisting of the parent of both attachment 
point and incoming MBDU according to con-
straints of the discourse relation selected. If mul-
tiple attachments at different sites are possible, 
ambiguous parses are generated; less preferred 
attachments are discarded and the remaining 
attachment choices generate valid parse trees. 
5 PALSUMM Text Summarization  
So far, we have described the U-LDM only as a 
theoretical approach to discourse parsing. We 
now turn briefly to describe a computational 
implementation of these methods. The 
PALSUMM Text Summarization System is a 
domain independent symbolic sentence extrac-
tion system that produces high level readable 
summaries that preserve the language and style 
of the original text and eliminate problems with 
unresolved or incorrect reference. Our system is 
currently used to summarize a corpus of 300 
technical reports produced by our laboratory. 12 
The PALSUMM System relies on the Xerox 
Linguistic Environment (XLE) to parse the sen-
tences of our source texts. The f-structure output 
of the XLE parser is segmented into units ac-
cording to the criteria identified above. The seg-
ments are then combined into a BDU-tree. Using 
syntactic information about syntactic coordina-
tion and subordination relations, lexical onto-
logical information taken from WordNet and a 
customized lexical domain ontology as well as 
discourse rules, the M-BDU of the sentence 
along with any other BDUs that must be accessi-
ble along the right edge of the discourse tree to 
accommodate possible continuations are identi-
fied, Both the site of attachment and the attach-
ment relation are then computed using discourse 
attachment rules of the type presented above. 
Text summarization algorithms are then applied 
to the resulting tree.  
Running in purely symbolic mode, the tree is 
pruned at a given level of embeddedness to pro-
duce a summary of a desired length or degree of 
summarization.13 Because the resulting summa-
                                                          
12 For illustration purposes, we present in Ap-
pendix A a summary of a document that was hand 
coded using the rules given and then summarized 
automatically using the PALSUMM tree pruning al-
gorithm. The PALSUMM Summaries were judged 
to be significantly more readable than summaries 
produced by MEAD in a small comparative study. 
In Appendix B, we present a diagram of the 
PALSUMM system. 
13 Although closely related to methods reported 
by Marcu (1999, 2000) for summarization using 
ries may be longer than desired, alternatively we 
also use statistical methods to identify salient 
information (see discussion and references in 
Marcu 2003) and then construct a partial dis-
course tree that includes only information identi-
fied as most salient and the text at all nodes 
dominating that salient information.  
 
6 Conclusions and Directions for 
future work 
The U-LDM discussed in this paper represents a 
significant advance in the theoretical understand-
ing of the nature of discourse structure. The ex-
plicit rules for discourse segmentation based on 
the syntactic reflexes of semantic structures al-
low analysts for the first time to relate the se-
mantics underlying the syntactic structure of 
sentences to the discourse segments needed to 
account for continuity. In order to adapt the rules 
to other languages which may have different 
syntactic reflexes of semantic information, un-
derstanding the semantic justification for the 
choice of segments is important. In addition, the 
rules for discourse attachment for the first time 
make clear the principles of discourse continuity 
for ?coherent? discourse. In the future, we plan 
to deepen our understanding of the rules for dis-
course attachment and, in particular, begin to 
apply machine learning techniques to increase 
our understanding of the complex interrelation-
ship that obtain among them.  
While full implementation of the principles 
of discourse organization outlined here are be-
yond the state of the art in some respects (i.e. 
determining that a sentence is generic in English 
is non-trivial in many instances although ma-
chine learning techniques might be useful in this 
regard), we believe that the PALSUMM System 
demonstrates the practicality of symbolic dis-
course parsing using the U-LDM Model. The 
infrastructure for this system has been success-
fully applied to the task of summarizing docu-
ments without a complex semantic component, 
extensive world knowledge and inference or a 
subjectively annotated corpus. We believe that 
the U-LDM parsing methods discussed here can 
be used for all other complex NLP tasks in 
which symbolic parsing is appropriate, especially 
                                                                                
RST trees, our basic algorithm is essentially simpler 
because RST trees are dependency trees over a large 
set of different link types, whereas LDM trees are 
constituent trees over effectively two basic node 
types: subordinations and non-subordinations. 
 those involving high value document collections 
where precision is critical. In addition, the struc-
tures generated through symbolic parsing by the 
system will be invaluable for training statistical 
and probabilistic systems.  
References 
 
Nicholas Asher. 1993. Reference to Abstract 
Objects in English: A Philosophical Seman-
tics for Natural Language Metaphysics. Klu-
wer Academic Publishers. 
Lynn Carlson, Daniel Marcu, and Mary Ellen 
Okurowski. To appear. Building a Discourse-
Tagged Corpus in the Framework of Rhetori-
cal Structure Theory. In Current Directions in 
Discourse and Dialogue, Jan van Kuppevelt 
and Ronnie Smith eds. Kluwer Academic 
Publishers.  
Katherine M. Forbes, Eleni Miltsakaki, Rashmi 
Prasad, Anoop Sarkar, Aravind Joshi and 
Bonnie Webber. 2003. D-LTAG System - 
Discourse Parsing with a Lexicalized Tree-
Adjoining Grammar, Journal of Language, 
Logic and Information, 12(3). 
Barbara Grosz and Candace Sidner. 1986. Atten-
tion, Intention and the Structure of Discourse. 
Computational Linguistics 12:175-204.. 
Hans Kamp. 1981. A theory of truth and seman-
tic representation.? In Formal Methods in the 
Study of Language. Jeroen A. G. Groenendijk, 
Theo Janssen, and Martin Stokhof (eds.). Am-
sterdam: Mathematisch Centrum, 277-322.  
Hans Kamp and Uwe Reyle. 1993. From Dis-
course to Logic: Introduction to Model-
theoretic Semantics of Natural Language, 
Formal Logic and Discourse Representation 
Theory. Kluwer Academic Publishers, 
Dordrecht, The Netherlands. 
William C. Mann and Sandra A. Thompson. 
1988. Rhetorical Structure Theory: Towards a 
Functional Theory of Text Organization. Text 
8(3)243-281. 
Daniel Marcu. 1999. Discourse trees are good 
indicators of importance in text. In Advances 
in Automatic Text Summarization. I. Mani and 
Mark Maybury (Eds.), 123-136, The MIT 
Press. 
Daniel Marcu. 2000. The Theory and Practice of 
Discourse Parsing and Summarization. The 
MIT Press. Cambridge, MA. 
Daniel Marcu. 2003. Automatic Abstracting, 
Encyclopedia of Library and Information Sci-
ence, 245-256, 2003. 
Livia Polanyi. 1988. A Formal Model of Dis-
course Structure. Journal of Pragmatics 12: 
601-639. 
Livia Polanyi and Martin van den Berg. 1996. 
Discourse Structure and Discourse Interpreta-
tion. In Proceedings of the 10th Amsterdam 
Colloquium on Formal Semantics. University 
of Amsterdam. 
Livia Polanyi and Remko Scha. 1984. A syntac-
tic approach to discourse semantics. In Pro-
ceedings of COLING 6. Stanford, CA. 413-
419. 
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, and John Blitzer, Arda ?elebi, 
Elliott Drabek, Wai Lam, Danyu Liu, Hong 
Qi, Horacio Saggion, Simone Teufel, Michael 
Topper, Adam Winkel. 2003. ?The MEAD 
Multidocument Summarizer?. 
http://www.summarization.com/mead/ 
Radu Soricut and Daniel Marcu. 2003. Sentence 
Level Discourse Parsing using Syntactic and 
Lexical Information. In Proceedings of 
HLT/NAACL?03, May 27-June 1, Edmonton, 
Canada 
Bonnie Webber.1991. Structure and Ostension in 
the Interpretation of discourse Deixis. In Lan-
guage and Cognitive Processes, 6(2):107-135. 
Bonnie Webber and Aravind Joshi. 1998. An-
choring a Lexicalized Tree-Adjoining Gram-
mar for Discourse. ACL/COLING Workshop 
on Discourse Relations and Discourse Mark-
ers, Montreal, Canada.  
Alec Wilkinson. 2003. Talk of the Town, Sep-
tember 26, 2003, New Yorker 
 APPENDIX A. PALSUMM Example 
The text below, taken from a recent issue of The 
New Yorker magazine (Alec Wilkinson, 2003)14, 
has been analyzed by hand using the segmenta-
tion and discourse structure construction rules 
given in Sections 3 and 4 above, resulting in the 
Discourse Parse Tree given in Figure 1. The 
summary of the text was automatically generated 
using the automatic summarization algorithm 
mentioned in Section 5 and a Genre Specific rule 
for stories in which stories are treated as consist-
ing of an Orientation, Narrative and Coda. The 
first specific, non-habitual eventive clause closes 
the Orientation and begins the Narrative Section. 
The function of a Coda is to make the point of a 
story explicit. This is often done, as in the pre-
sent case, by using an anaphor that refers to an 
entire section of text (Webber, 1991)15. 
 
(1) In the spring of 1947, (2)William Katavolos 
is the solitary occupant of the Ram?s Head Inn, 
(3) on Ram Island,(4) off eastern Long Island. 
(5) Katavolos is twenty-three. (6) His father has 
leased the inn. (7) Katavolos has returned from 
the war (8) and (9) wants a place (10) where he 
can paint (11) and (12) be left alone. (13) The 
hotel is reached by a causeway from Shelter Is-
land, (14) and the causeway sometimes floods, 
(15) leaving Katavolos as isolated as a light-
house keeper. (16) To amuse himself one evening, 
(17) he puts some water in a glass, (18) covers 
the rim of the glass with waxed paper, (19) then 
presses the paper into the water (20) to create a 
vacuum. (21) He secures the paper to the glass 
with a rubber band, (22) then turns the glass 
upside down. (23) The water fills the vacuum, 
(24) preserving the dome (25) ? it looks like the 
bottom of a wine bottle. (26) Then he begins to 
wonder (27) what would happen (28) if he re-
peated the experiment on a larger scale.(29) A 
few days later, (30) he throws a tarpaulin over a 
section of Gardiners Bay (31) He weights down 
the edges (32) so that no air can get beneath the 
tarpaulin, (33) then he swims underneath it. (34) 
Using two oars, (35) he raises the center of the 
tarpaulin. (36) The water fills the cavity (37) and 
                                                          
14 Alec Wilkinson. 2003. Talk of the Town, Sep-
tember 26, 2003, New Yorker 
15 Bonnie Webber.1991. Structure and Ostension 
in the Interpretation of discourse Deixis. In Lan-
guage and Cognitive Processes, 6(2):107-135. 
 
he swims into it, (38) floating above sea level, 
(39) which, (40) he says later, (41) ?fascinated 
the hell out of me.? (42) This is the beginning of 
(43) what Katavolos will call hydronics, (44) the 
practice of making buildings from soft plastic 
forms (45) filled with water. (46) In 1949, (47) 
Katavolos gives up painting (48) to design furni-
ture (49) ? his chairs are in the collections of 
the Museum of Modern Art, the Metropolitan 
Museum, and the Louvre? (50) and, (51) in 
1960, (52) he begins teaching architecture at the 
Pratt Institute, (54) in Brooklyn, (55) where he 
will become the co-director of the Center for 
Experimental Structures. (56) In 1970, (57) in a 
courtyard at Pratt, (58) he builds the first hy-
dronic structure (59) ? a plastic dome filled 
with water (60) and supported by a plastic cylin-
der, (61) also filled with water. (62) The plastic 
is like Saran Wrap, (63) only thicker. (64) Each 
year after that, (65) he builds a new structure 
(66) He calls the structures (67) liquid villas. 
(68) They consist of columns, arches, and vaults. 
(69) The elements, (70) that is (71) of classical 
architecture. 
Summary 152/363 = 42% 
In the spring of 1947, William Katavolos is the 
solitary occupant of the Ram?s Head Inn, 
Katavolos is twenty-three. To amuse himself one 
evening, he puts some water in a glass, covers 
the rim of the glass with waxed paper, then 
presses the paper into the water. He secures the 
paper to the glass with a rubber band, then turns 
the glass upside down. A few days later, he 
throws a tarpaulin over a section of Gardiners 
Bay. 
He weights down the edges then he swims 
underneath it. Using two oars, he raises the cen-
ter of the tarpaulin. The water fills the cavity, 
This is the beginning of what Katavolos will call 
hydronics. In 1949, Katavolos gives up painting 
and in 1960 he begins teaching architecture at 
the Pratt Institute. In 1970, in a courtyard at 
Pratt, he builds the first hydronic structure. Each 
year after that, he builds a new structure. 
 
   
 
Figure 1. Discourse Parse Tree of the New Yorker text. 
 
 
APPENDIX B. System Diagram 
 
WordNet
Lookup
Flat
Ontology
Files
DAML
Encoded
Ontology
HTML
document
XLE Server
XLE
Parser
Segmentation
Webservice
Lexical Ontology Server
Ontology
Webservice
Lexical
Ontology
Lookup
English
Grammar
LiveTree
Sentence
Breaker
Sentence
Level
Discourse
Parser
Text
Level
Discourse
Parser
Discourse
tree
XML
XML
document
Hybrid
Summarizer
MEAD Statistical Summarizer
WebserviceSentence
Level
Discourse
Segmenter
Sentence
Level
Discourse
Grammar
Text
Level
Discourse
Grammar
LDM
Engine
TEXT XMLXML
SOAP
 
Figure 2. Diagram of the PALSUMM system, a symbolic summarization system currently being devel-
oped at FX Palo Alto Laboratory. 
 
