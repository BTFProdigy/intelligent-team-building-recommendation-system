TALK'N'TRAVEL: A CONVERSATIONAL SYSTEM FOR AIR 
TRAVEL PLANNING 
David Stallard 
BBN Technologies, GTE 
70 Fawcett St. 
Cambridge, MA, USA, 02238 
Stallard@bbn.com 
Abstract 
We describe Talk'n'Travel, a spoken 
dialogue language system for making air 
travel plans over the telephone. 
Talk'n'Travel is a fully conversational, 
mixed-initiative system that allows the 
user to specify the constraints on his travel 
plan in arbitrary order, ask questions, etc., 
in general spoken English. The system 
operates according to a plan-based agenda 
mechanism, rather than a finite state 
network, and attempts to negotiate with 
the user when not all of his constraints can 
be met. 
Introduction 
This paper describes Talk'n'Travel, a spoken 
language dialogue system for making complex 
air travel plans over the telephone. 
Talk'n'Travel is a research prototype system 
sponsored under the DARPA Communicator 
program (MITRE, 1999). Some other systems 
in the program are Ward and Pellom (1999), 
Seneff and Polifroni (2000) and Rudnicky et al
(1999). The common task of this program is a 
mixed-initiative dialogue over the telephone, in 
which the user plans a multi-city trip by air, 
including all flights, hotels, and rental cars, all in 
conversational English over the telephone. 
The Communicator common task presents 
special challenges. It is a complex task with 
many subtasks, including the booking of each 
flight, hotel, and car reservation. Because the 
number of legs of the trip may be arbitrary, the 
number of such subtasks is not known in 
advance. Furthermore, the user has complete 
freedom to say anything at any time. His 
utterances can affect just the current subtask, or 
multiple subtasks at once ("I want to go from 
Denver to Chicago and then to San Diego"). He 
can go back and change the specifications for 
completed subtasks. And there are important 
constraints, such as temporal relationships 
between flights, that must be maintained for the 
solution to the whole task to be coherent. 
In order to meet this challenge, we have sought 
to develop dialogue techniques for 
Talk'n'Travel that go beyond the rigid system- 
directed style of familiar IVR systems. 
Talk'n'Travel is instead a mixed initiative 
system that allows the user to specify constraints 
on his travel plan in arbitrary order. At any 
point in the dialogue, the user can supply 
information other than what the system is 
currently prompting for, change his mind about 
information he has previously given and even 
ask questions himself. The system also tries to 
be helpful, eliciting constraints from the user 
when necessary. Furthermore, if at any point the 
constraints the user has specified cannot all be 
met, the system steps in and offers a relaxation 
of them in an attempt o negotiate a partial 
solution with the user. 
The next section gives a brief overview of the 
system. Relevant components are discussed in 
subsequent sections. 
I System Overview 
The system consists of the following modules: 
speech recognizer, language understander, 
dialogue manager, state manager, language 
generator, and speech synthesizer. The modules 
68 
interact with each other via the central hub 
module of the Communicator Common 
Architecture. 
The speech recognizer is the Byblos system 
(Nguyen, 1995). It uses an acoustic model 
trained from the Macrophone telephone corpus, 
and a bigram/trigram language model trained 
from -40K utterances derived from various 
sources, including data collected under the 
previous ATIS program (Dahl et al 1994). 
The speech synthesizer is Lucent's commercial 
system. Synthesizer and recognizer both 
interface to the telephone via Dialogics 
telephony board. The database is currently a 
frozen snapshot of actual flights between 40 
different US cities (we are currently engaged in 
interfacing to a commercial air travel website). 
The various language components are written in 
Java. The complete system runs on Windows 
NT, and is compliant with the DARPA 
Communicator Common architecture. 
The present paper is concerned with the dialogue 
and discourse management, language generation 
and language understanding components. In the 
remainder of the paper, we present more detailed 
discussion of these components, beginning with 
the language understander in Section 2. Section 
3 discusses the discourse and dialogue 
components, and Section 4, the language 
generator. 
2 Language Understanding 
2.1 Meaning Representation 
Semantic frames have proven useful as a 
meaning representation for many applications. 
Their simplicity and useful computational 
properties have often been seen as more 
important han their limitations in expressive 
power, especially in simpler domains. 
Even in such domains, however, flames still 
have some shortcomings. While most naturally 
representing equalities between slot and filler, 
flames have a harder time with inequalities, uch 
as 'the departure time is before 10 AM', or 'the 
airline is not Delta'. These require the slot-filler 
to be some sort of predicate, interval, or set 
object, at a cost to simplicity uniformity. Other 
problematic ases include n-ary relations ('3 
miles from Denver'), and disjunctions of 
properties on different slots. 
In our Talk'n'Travel work, we have developed 
a meaning representation formalism called path 
constraints, which overcomes these problems, 
while retaining the computational advantages 
that made frames attractive in the first place. A 
path constraint is an expression of the form : 
(<path> <relation> <arguments>*) 
The path is a compositional chain of one or more 
attributes, and relations are 1-place or higher 
predicates, whose first argument is implicitly the 
path. The relation is followed by zero or more 
other arguments. In the simplest case, path 
constraints can be thought of as flattenings of a 
tree of frames. The following represents the 
constraint that the departure time of the first leg 
of the itinerary is the city Boston : 
LEGS.0.ORIG_CITY EQ BoSToN 
Because this syntax generalizes to any relation, 
however, the constraint "departing before 10 
AM" can be represented in a syntactically 
equivalent way: 
LEGS.0.DEPART_TIME LT 1000 
Because the number of arguments i arbitrary, it 
is equally straightforward to represent a one- 
place property like "x is nonstop" and a three 
place predicate like "x is 10 miles from Denver". 
Like flames, path constraints have a fixed 
format that is indexed in a computationally 
useful way, and are simpler than logical forms. 
Unlike flames, however, path constraints can be 
combined in arbitrary conjunctions, disjunctions, 
and negations, even across different paths. Path 
constraint meaning representations are also flat 
lists of constraints rather than trees, making 
matching rules, etc, easier to write for them. 
69 
2.2 The GEM Understanding System 
Language understanding in Talk'n'Travel is 
carried out using a system called GEM (for 
Generative Extraction Model). GEM (Miller, 
1998) is a probabilistic semantic grammar that is 
an outgrowth of the work on the HUM system 
(Miller, 1996), but uses hand-specified 
knowledge in addition to probability. The hand- 
specified knowledge is quite simple, and is 
expressed by a two-level semantic dictionary. In 
the first level, the entries map alternative word 
strings to a single word class. For example, the 
following entry maps several alternative forms 
to the word class DEPART: 
Leave, depart, get out of => DEPART 
In the second level, entries map sequences of 
word classes to constraints: 
Name: DepartCity 1 
Head: DEPART 
Classes: \[DEPART FROM CITY\] 
Meaning: (DEST_CITY EQ <CITY>) 
The "head" feature allows the entry to pass one 
of its constituent word classes up to a higher 
level pattern, allowing the given pattern to be a 
constituent of others. 
The dictionary entries generate a probabilistic 
recursive transition network (PRTN), whose 
specific structure is determined by dictionary 
entries. Paths through this network correspond 
one-to-one with parse trees, so that given a path, 
there is exactly one corresponding tree. The 
probabilities for the arcs in this network can be 
estimated from training data using the EM 
(Expectation-Maximization) procedure. 
GEM also includes a noise state to which 
arbitrary input between patterns can be mapped, 
making the system quite robust to ill-formed 
input. There is no separate phase for handling 
ungrammatical input, nor any distinction 
between grammatical nd ungrammatical input. 
3 Discourse and Dialogue Processing 
A key feature of the Communicator task is that 
the user can say anything at any time, adding or 
changing information at will. He may add new 
subtasks (e.g. trip legs) or modifying existing 
ones. A conventional dialogue state network 
approach would be therefore infeasible, as the 
network would be almost unboundedly arge and 
complex. 
A signifigant additional problem is that changes 
need not be monotonic. In particular, when 
changing his mind, or correcting the system's 
misinterpretations, the user may delete subtask 
structures altogether, as in the subdialog: 
S: What day are you returning to Chicago? 
U: No, I don't want a return flight. 
Because they take information away rather than 
add it, scenarios like this one make it 
problematic to view discourse processing as 
producing a contextualized, or "thick frame", 
version of the user's utterance. In our system, 
therefore, we have chosen a somewhat different 
approach. 
The discourse processor, called the state 
manager, computes the most likely new task 
state, based on the user's input and the current 
task state. It also computes a discourse vent, 
representing its interpretation f what happened 
in the conversation as a result of the user's 
utterance. 
The dialogue manager is a separate module, as 
has no state managing responsibilities at all. 
Rather, it simply computes the next action to 
take, based on its current goal agenda, the 
discourse vent returned by the state manager, 
and the new state. This design has the advantage 
of making the dialogue manager considerably 
simpler. The discourse event also becomes 
available to convey to the user as confirmation. 
We discuss these two modules in more detail 
below. 
70 
3.1 State Manager 
The state manager is responsible for computing 
and maintaining the current ask state. The task 
state is simply the set of path constraints which 
currently constrain the user's itinerary. Also 
included in the task state are the history of user 
and system utterances, and the current subtask 
and object in focus, if any. 
The state manager takes the N-best list of 
recognition hypotheses as input. It invokes the 
understanding module on a hypothesis to obtain 
a semantic interpretation. The semantic 
interpretation so obtained is subjected to the 
following steps: 
1. Resolve ellipses if any 
2. Match input meaning to subtask(s) 
3. Expand local ambiguities 
4. Apply inference and coherency rules 
5. Compute database satisfiers 
6. Relax constraints if neccesary 
7. Determine the most likely alternative and 
compute the discourse vent 
At any of these steps, zero or more alternative 
new states can result, and are fed to the next 
step. If zero states result at any step, the new 
meaning representation is rejected, and another 
one requested from the understander. If no more 
hypotheses are available, the entire utterance is
rejected, and a DONT_UNDERSTAND event is 
returned to the dialogue manager. 
Step 1 resolves ellipses. Ellipses include both 
short responses like "Boston" and yes/no 
responses. In this step, a complete meaning 
representation such as ' (ORIQCITY EQ 
BOSTON)' is generated based on the system's 
prompt and the input meaning. The hypothesis 
rejected if this cannot be done. 
Step 2 matches the input meaning to one or more 
of the subtasks of the problem. For the 
Communicator p oblem, the subtasks are legs of 
the user's itinerary, and matching is done based 
on cities mentioned in the input meaning. The 
default is the subtask currently in focus in the 
dialogue. 
A match to a subtask is represented by adding 
the prefix for the subtask to the path of the 
constraint. For example, "I want to arrive in 
Denver by 4 PM" and then continue on to 
Chicago would be : 
LEGS.0.DEST_CITY EQ DENVER 
LEGS.0.ARRIVE_TIME LE 1600 
LEGS. 1.ORIG_CITY EQ DENVER 
LEGS. 1.DEST CITY EQ CHICAGO 
In Step 3, local ambiguities are expanded into 
their different possibilities. These include 
partially specified times such as "2 o'clock" 
Step 4 applies inference and coherency rules. 
These rules will vary from application to 
application. They are written in the path 
constraint formalism, augmented with variables 
that can range over attributes and other values. 
The following is an example, representing the 
constraint a flight leg cannot be scheduled to 
depart until after the preceding flight arrives: 
LEGS.$N.ARRIVE 
LT 
LEGS. $N+ 1 .DEPART 
States that violate coherency constraints are 
discarded. 
Step 5 computes the set of objects in the 
database that satisfy the constraints on the 
current subtask. This set will be empty when the 
constraints are not all satisfiable, in which case 
the relaxation of Step 6 is invoked. This 
relaxation is a best-first search for the satisfiable 
subset of the constraints that are deemed closest 
to what the user originally wanted. Alternative 
relaxations are scored according to a sum of 
penalty scores for each relaxed constraint, 
derived from earlier work by Stallard (1995). 
The penalty score is the sum of two terms: one 
for the relative importance of the attribute 
concerned (e.g. relaxations of DEPART_DATE 
are penalised more than relaxations of 
AIRLINE) and the other for the nearness of the 
satisfiers to the original constraint (relevant for 
number-like attributes like departure time). 
71 
The latter allows the system to give credit to 
solutions that are near fits to the user's goals, 
even if they relax strongly desired constraints. 
For example, suppose the user has expressed a 
desire to fly on Delta and arrive by 3 PM, while 
the system is only able to find a flight on Delta 
that arrives at 3:15 PM. In this case, this flight, 
which meets one constraint and almost meets the 
other, may well satisfy the user more than a 
flight on a different airline that happens to meet 
the time constraint exactly. 
In the final step, the alternative new states are 
rank-ordered according to a pragmatic score, and 
the highest-scoring alternative is chosen. The 
pragmatic score is computed based on a number 
of factors, including the plausibility of 
disambiguated times and whether or not the state 
interpreted the user as responding to the system 
prompt. 
The appropriate discourse event is then 
deterministicaUy computed and returned. There 
are several types of discourse vent. The most 
common is UPDATE, which specifies the 
constraints that have been added, removed, or 
relaxed. Another type is REPEAT, which is 
generated when the user has simply repeated 
constraints the system already knows. Other 
types include QUESTION, TIMEOUT, and 
DONT UNDERSTAND. 
3.1 Dialogue Manager 
Upon receiving the new discourse event from 
the state manager, the dialogue manager 
determines what next action to take. Actions 
can be external, such as speaking to the user or 
asking him a question, or internal, such as 
querying the database or other elements of the 
system state. The current action is determined by 
consulting a stack-based agenda of goals and 
actions. 
The agenda stack is in turn determined by an 
application-dependent library of plans. Plans are 
tree structures whose root is the name of the goal 
the plan is designed to solve, and whose leaves 
are either other goal names or actions. An 
example of a plan is the following: 
Completeltinerary => 
(Prompt "How can I help you?") 
(forall legs $n 
GetRoutelnfo 
GetSpecificFlight 
GetHotelAndCar 
GetNextLeg)) 
This is a plan for achieving the goal 
Completeltinerary. It begins with a open-ended 
prompt and then iterates over values of the 
variable $N for which constraints on the prefix 
LEGS.$N exist, working on high-level subgoals, 
such as getting the route and booking a flight, 
for each leg. The last goal determines whether 
there is another leg to the itinerary, in which 
case the itera 
The system begins the interaction with the high- 
level goal START on its stack. At each step, the 
system examines the top of its goal stack and 
either executes it if it is an action suitable for 
execution, or replaces it on the stack with its 
plan steps if it is a goal. 
Actions are objects with success and relevancy 
predicates and an execute method, somewhat 
similar to the "handlers" of Rudnicky and Xu 
(1999). An action has an underlying oal, such 
as finding out the user's constraints on some 
attribute. The action's success predicate will 
return true if this underlying goal has been 
achieved, and its relevancy predicate will return 
true if it is still relevant to the current situation. 
Before carrying out an action, the dialogue 
manager first checks to see if its success 
predicate returns false and its relevancy 
predicate returns true. If either condition is not 
met, the action is popped off the stack and 
disposed of without being executed. Otherwise, 
the action's execute method is invoked. 
The system includes a set of actions that are 
built in, and may be parameterized for each each 
domain. For example, the action type ELICIT is 
parameterized by an attribute A, a path prefix P, 
and verbalization string S. Its success predicate 
returns true if the path 'P.A' is constrained in the 
current state. Its execute method generates a
meaning frame that is passed to the language 
72 
generator, ultimately prompting the user with a 
question such as "What city are you flying to?" 
Once an action's execute method is invoked, it 
remains on the stack for the next cycle, where it 
is tested again for success and relevancy. In this 
case, if the success condition is met - that is, if 
the user did indeed reply with a specification of 
his destination city - the action is popped off the 
stack. If the system did not receive this 
information, either because the user made a 
stipulation about some different attribute, asked 
a question, or simply was not understood, the 
action remains on the stack to be executed again. 
Of course, the user may have already specified 
the destination city in a previous utterance. In 
this case, the action is already satisfied, and is 
not executed. In this way, the user has 
flexibility in how he actually carries out the 
dialogue. 
In certain situations, other goals and actions may 
be pushed onto the stack, temporarily 
interrupting the execution of the current plan. 
For example, the user himself may ask a 
question. In this case, an action to answer the 
question is created, and pushed onto the stack. 
The dialogue manager then executes this action 
to answer the user's question before continuing 
on with the plan. Or the state manager may 
generate a clarification question, which the 
dialogue manager seeks to have the user answer. 
Actions can also have a set of conditional 
branchings that are tested after the action is 
executed. If present, these determine the next 
action to execute or goal to work on. For 
example, the action that asks the user "Do you 
want a return flight to X?" specifies the branch 
to be taken when the user replies in the negative. 
This branch includes an action that asks the user 
"Is Y your final destination?", an action that is 
executed if the user did not specify an additional 
destination along with his negative reply. 
Unlike the approach taken by Ward and Pellom 
(1999), which seeks to avoid scripting entirely 
by driving the dialogue off the current status of 
the itinerary, the Talk'n'Travel dialogue 
manager thus seeks to allow partially scripted 
dialogue where appropriate to the situation. 
4 Language Generation 
The language generator takes a meaning frame 
from the dialogue manager, and generates a text 
string in English for it. It uses a set of pattern- 
based rules that map constraints into alternative 
syntactic realisations. For example, the 
following rule allows a constraint on departure 
time to be realized as "leave at 3 PM" 
or "3 PM flight": 
LEG.$N.DEPART_TIME EQ $X 
=~ \[leave at $X\], 
\[nom-comp $X\] 
Different realization rules can be selected for 
depending upon whether the constraint is to be 
realized as an assertion or as a description. The 
generation algorithm assembles the selected 
realizations for each constraint into a simplified 
syntax tree, selecting appropriate inflections of 
verb and noun heads as it does so. Terminal 
values in constraints are realized as type-specific 
nominals, such as "3 PM" or "Delta". 
A crucial feature of the generation process is 
that it adds to each prompt a paraphrase of the 
most recent discourse event, corresponding to 
what the system thinks the user just said. This 
helps keep the conversation grounded in terms 
of mutual understanding between the 
participants. 
5 Example Scenario 
The following is an example dialog with the 
system: 
S: Welcome to Talk'n'Travel. How can I help 
you? 
U: I need a flight from Boston to Denver 
tomorrow. 
S: On Thursday, November 18, flying from 
Boston to Denver. What time do you want to 
leave? 
U: I want to leave before 10 AM. 
S: Leaving before 10 AM. Do you have an 
airline preference? 
U: Continental. 
73 
S: Continental. I 'm sorry, there are no flights on 
Continental leaving before 10 AM. I can get 
you a flight on Continental leaving at 10:10 AM 
instead. Would that be OK? 
U: Yes. 
S: OK, I've got you on Continental flight 429, 
departing Boston at 10:10 AM, arriving at 
Denver at 3:44 PM. Do you want to book a 
return flight from Denver to Boston? 
U: Yes, on Monday. 
S: Alright, we'll add a flight on Monday, 
November 22, from Boston to Denver to your 
itinerary. I have a Continental flight at 7 AM. 
Is that OK? 
U. Anything later? 
S: Leaving after 7 AM. There are no flights on 
Continental after 7 AM. I can get you a flight 
after 7 AM on Frontier, Northwest, and United 
instead. Would that be OK? 
U: How about United. 
S: United. I have a United flight at 8:01 AM. Is 
that flight OK? 
U: Yes. 
S: OK, I've got you on United flight 1762, 
departing Denver at 8:01 AM, arriving at Boston 
at 1:42 PM. 
6 Current Status and Conclusions 
The Talk'n'Travel system described here was 
successfully demonstrated at the DARPA 
Communicator Compare and Contrast 
Workshop in June 1999. We are currently 
collecting data with test subjects and are using 
the results to improve the system's performance 
in all areas, in preparation for the forthcoming 
common evaluation of Communicator systems 
in June 2000. 
8 of the subjects were successful. Of successful 
sessions, the average duration was 387 seconds, 
with a minimum of 272 and a maximum of 578. 
The average number of user utterances was 25, 
with a minimum of 18 and a maximum of 37. 
The word error rate of the recognizer was 
11.8%. 
The primary cause of failure to complete the 
scenario, as well as excessive time spent on 
completing it, was corruption of the discourse 
state due to recognition or interpretation errors. 
While the system informs the user of the change 
in state after every utterance, the user was not 
always successful in correcting it when it made 
errors, and sometimes the user did not even 
notice when the system had made an error. If the 
user is not attentive at the time, or happens not 
to understand what the synthesizer said, there is 
no implicit way for him to find out afterwards 
what the system thinks his constraints are. 
While preliminary, these results point to two 
directions for future work. One is that the system 
needs to be better able to recognize and deal 
with problem situations in which the dialogue is 
not advancing. The other is that the system 
needs to be more communicative about its 
current understanding of the user's goals, even 
at points in the dialogue at which it might be 
assumed that user and system were in 
agreement. 
Acknowledgements 
This work was sponsored by DARPA and 
monitored by SPAWAR Systems Center under 
Contract No. N66001-99-D-8615. 
To determine the performance of the system, we 
ran an informal experiment in which 11 different 
subjects called into the system and attempted to 
use it to solve a travel problem. None of the 
subjects were system developers. Each subject 
had a single session in which he was given a 
three-city trip to plan, including dates of travel, 
constraints on departure and arrival times, airline 
preferences. 
The author wishes to thank Scott Miller for the 
use of his GEM system. 
References 
MITRE (1999) DARPA Communicator homepage 
http://fofoca.mitre.org\]. 
Ward W., and Pellom, B. (1999) The CU 
Communicator System. In 1999 IEEE Workshop 
on Automatic Speech Recognition and 
Understanding, Keystone, Colorado. 
-/4. 
Miller S. (1998) The Generative Extraction Model. 
Unpublished manuscript. 
Dahl D., Bates M., Brown M., Fisher, W. Hunicke- 
Smith K., Pallet D., Pao C., Rudnicky A., and 
Shriberg E. (1994) Expanding the scope of the 
ATIS task. In Proceedings of the ARPA Spoken 
Language Technology Workshop, Plainsboro, NJ., 
pp 3-8. 
Constantinides P., Hansma S., Tchou C. and 
Rudnicky, A. (1999) A schema-based approach to 
dialog control. Proceedings oflCSLP, Paper 637. 
Rudnicky A., Thayer, E., Constantinides P., Tchou 
C., Shern, R., Lenzo K., Xu W., Oh A. (1999) 
Creating natural dialogs in the Carnegie Mellon 
Communicator system. Proceedings of 
Eurospeech, 1999, Vol 4, pp. 1531-1534 
Rudnicky A., and Xu W. (1999) An agenda-based 
dialog management architecture for soken 
language systems. In 1999 IEEE Workshop on 
Automatic Speech Recognition and Understanding, 
Keystone, Colorado. 
Seneff S., and Polifroni, J. (2000) Dialogue 
Management in the Mercury Flight Reservation 
System. ANLP Conversational Systems Workshop. 
Nguyen L., Anastasakos T., Kubala F., LaPre C., 
Makhoul J., Schwartz R., Yuan N., Zavaliagkos 
G., and Zhao Y. (1995) The 1994 BBN/BYBLOS 
Speech Recognition System, In Proc of ARPA 
Spoken Language Systems Technology Workshop, 
Austin, Texas, pp. 77-81. 
Stallard D. (1995) The Initial Implementation of the 
BBN ATIS4 Dialog System, In Proc of ARPA 
Spoken Language Systems Technology Workshop, 
Austin, Texas, pp. 208-211. 
Miller S. and Stallard D. (i996) A Fully Statistical 
Approach to Natural Language Interfaces, In Proc 
of the 34 th Annual Meeting of the Association for 
Computational Linguistics, Santa Cruz, California. 
76 
Evaluation Results for the Talk?n?Travel System
David Stallard
BBN Technologies, Verizon
70 Fawcett. St.
Cambridge, MA, 02140
Stallard@bbn.com
ABSTRACT
We describe and present evaluation results for Talk?n?Travel, a
spoken dialogue language system for making air travel plans over
the telephone.  Talk?n?Travel is a fully conversational, mixed-
initiative system that allows the user to specify the constraints on
his travel plan in arbitrary order, ask questions, etc., in general
spoken English.  The system was independently evaluated as part
of the DARPA Communicator program and achieved a high
success rate.
.
1. INTRODUCTION
This paper describes and presents evaluation results for
Talk?n?Travel, a spoken language dialogue system for making
complex air travel plans over the telephone.  Talk?n?Travel is a
research prototype system sponsored under the DARPA
Communicator program (MITRE, 1999). Some other systems in
the program are Ward and Pellom (1999), Seneff and Polifroni
(2000) and Rudnicky et al(1999).  The common task of this
program is a mixed-initiative dialogue over the telephone, in
which the user plans a multi-city trip by air, including all flights,
hotels, and rental cars, all in conversational English over the
telephone. A similar research program is the European ARISE
project (Den Os et al 1999).
An earlier version of Talk?n?Travel was presented in (Stallard,
2000). The present paper presents and discusses results of an
independent evaluation of Talk?n?Travel, recently conducted as
part of the DARPA Communicator program.
The next section gives a brief overview of the system.
2. SYSTEM OVERVIEW
The figure shows a block diagram of Talk?n?Travel. Spoken
language understanding is provided by statistical N-gram speech
recognition and a robust language understanding component.  A
plan-based dialogue manager coordinates interaction with the
user, handling unexpected user input more flexibly than
conventional finite-state dialogue control networks. It works in
tandem with a state management component that adjusts the
current model of user intention based on the user?s last utterance
in context.
Meaning and task state are represented by the path constraint
representation (Stallard, 2000). An inference component is
included which allows the system to deduce implicit requirements
from explicit statements by the user, and to retract them if the
premises change.
The system is interfaced to the Yahoo/Travelocity flight schedule
website, for access to live flight schedule information. Queries to
the website are spawned off in a separate thread, which the
dialogue manager monitors ands reports on to the user.
3. DIALOGUE STRATEGY
Talk?n?Travel employs both open-ended and directed prompts.
Sessions begin with open prompts like "What trip would you to
take?". The system then goes to directed prompts to get any
information the user did not provide ("What day are you
leaving?", etc). The user may give arbitrary information at any
prompt, however. The system provides implicit confirmation of
the change in task state caused by the user?s last utterance
("Flying from Boston to Denver tomorrow ") to ensure mutual
understanding.
The system seeks explicit confirmation in certain cases, for
example where the user appears to be making a change in date of
travel. Once sufficient information is obtained, the system offers a
set of candidate flights, one at a time, for the user to accept or
reject.
4. EVALUATION
4.1 Evaluation Design
The 9 groups funded by the Communicator program (ATT, BBN,
CMU, Lucent, MIT, MITRE, SRI, and University of Colorado)
BYBLOS
Recognizer
Speech
Synthesizer
Discourse
State
Manager
Dialog
Manager
Flight
Database
Language
Generator
GEM NL
Understander
Phone
Figure 1 : System Architecture
took part in an experimental common evaluation conducted by the
National Institute of Standards and Technology (NIST) in June
and July of 2000. A pool of approximately 80 subjects was
recruited from around the United States. The only requirements
were that the subjects be native speakers of American English and
have Internet access. Only wireline or home cordless phones were
allowed.
The subjects were given a set of travel planning scenarios to
attempt. There were 7 such prescribed scenarios and 2 open ones,
in which the subject was allowed to propose his own task.
Prescribed scenarios were given in a tabular format. An example
scenario would be a round-trip flight between two cities,
departing and returning on given dates, with specific arrival or
departure time preferences.
Each subject called each system once and attempted to work
through a single scenario; the design of the experiment attempted
to balance the distributions of scenarios and users across the
systems.
Following each scenario attempt, subjects filled out a Web-based
questionnaire to determine whether subjects thought they had
completed their task, how satisfied they were with using the
system, and so forth. The overall form of this evaluation was thus
similar to that conducted under the ARISE program (Den Os, et al
1999).
4.2 Results
Table 1 shows the result of these user surveys for Talk?n?Travel.
The columns represent specific questions on the user survey. The
first column represents the user?s judgement as to whether or not
he completed his task. The remaining columns, labeled Q1-Q5,
are Likert scale items, for which a value of 1 signifies complete
agreement, and 5 signifies complete disagreement. Lower
numbers for these columns are thus better scores.  The legend
below the table identifies the questions.
The first row gives the mean value for the measurements over all
78 sessions with Talk?n?Travel. The second row gives the mean
value of the same measurements for all 9 systems participating.
Talk?n?Travel?s task completion score of 80.5% was the highest
for all 9 participating systems. Its score on question Q5,
representing user satisfaction, was the second highest.
An independent analysis of task completion was also performed
by comparing the logs of the session with the scenario given.
Table 2 shows Talk?n?Travel?s results for this metric, which are
close to that seen for the user questionnaire.
Table 2: Objective Analysis
Completion of required scenario 70.5%
Completion of different scenario 11.5%
Total completed scenarios 82.0%
Besides task completion, other measurements were made of
system operation. These included time to completion, word error
rate, and interpretation accuracy. The values of these
measurements are given in Table 3.
Table 3: Other Metrics
Average time to completion 246 secs
Average word error rate 21%
Semantic error rate/utterance 10%
4.3 Analysis and Discussion
We analyzed the log files of the 29.5% of the sessions that did not
result in the completion of the required scenario. Table 4 gives a
breakdown of the causes.
Table 4: Causes of Failure
City not in lexicon 39% (9)
Unrepaired recognition error 22% (5)
User error 17% (4)
System diversion 13% (3)
Other  9% (2)
The largest cause (39%) was the inability of the system to
recognize a city referred to by the user, simply because that city
was absent from the recognizer language model or language
understander?s lexicon.  These cases were generally trivial to fix.
The second, and most serious, cause (22%) was recognition errors
that the user either did not attempt to repair or did not succeed in
repairing. Dates proved troublesome in this regard, in which one
date would be misrecognized for another, e.g. ?October twenty
third? for ?October twenty first
Another class of errors were caused by the user, in that he either
gave the system different information than was prescribed by the
scenario, or failed to supply the information he was supposed to.
A handful of sessions failed because of additional causes,
including system crashes and backend failure.
Both time to completion and semantic error rate were affected by
scenarios that failed because because of a missing city. In such
scenarios, users would frequently repeat themselves many times in
a vain attempt to be understood, thus increasing total utterance
count and utterance error.
Q1  It was easy to get the information I wanted
Q2  I found it easy to understand what the system said
Q3  I knew what I could do or say at each point in the dialog
Q4  The system worked the way I expected it to
Q5  I would use this system regularly to get travel information
Comp%     Q1 Q2  Q3  Q4 Q5
BBN 80.5% 2.23 2.09 2.10 2.36 2.84
Mean 62.0% 2.88 2.23 2.54 2.95 3.36
Task
Scale: 1 = strongly agree, 5 = strongly disagree
Table 1 : Survey Results
An interesting result is that task success did not depend too
strongly on word error rate. Even successful scenarios had an
average WER of 18%, while failed scenarios had average WER of
only 22%.
A key issue in this experiment was whether users would actually
interact with the system conversationally, or would respond only
to directive prompts. For the first three sessions, we experimented
with a highly general open prompt ("How can I help you??), but
quickly found that it tended to elicit overly general and
uninformative responses (e.g. "I want to plan a trip"). We
therefore switched to the more purposeful "What trip would you
like to take?" for the remainder of the evaluation.  Fully 70% of
the time, users replied informatively to this prompt, supplying
utterances "I would like an American flight from Miami to
Sydney" that moved the dialogue forward.
In spite of the generally high rate of success with open prompts,
there was a pronounced reluctance by some users to take the
initiative, leading them to not state all the constraints they had in
mind. Examples included requirements on airline or arrival time.
In fully 20% of all sessions, users refused multiple flights in a
row, holding out for one that met a particular unstated
requirement. The user could have stated this requirement
explicitly, but chose not to, perhaps underestimating what the
system could do. This had the effect of lengthening total
interaction time with the system.
4.4 Possible Improvements
Several possible reasons for this behavior on the part of users
come to mind, and point the way to future improvements. The
synthesized speech was fairly robotic in quality, which naturally
tended to make the system sound less capable. The prompts
themselves were not sufficiently variable, and were often repeated
verbatim when a reprompt was necessary. Finally, the system?s
dialogue strategy needs be modified to detect when more initiative
is needed from the user, and cajole him with open prompts
accordingly.
5. ACKNOWLEDGMENTS
This work was sponsored by DARPA and monitored by
SPAWAR Systems Center under Contract No. N66001-99-D-
8615.
6. REFERENCES
[1] MITRE (1999)  DARPA Communicator  homepage
http://fofoca.mitre.org/
[2] Ward W., and Pellom, B. (1999) The CU
Communicator System.  In 1999 IEEE Workshop on
Automatic Speech Recognition and Understanding,
Keystone, Colorado.
[3] Den Os,  E, Boves, L., Lamel,  L, and Baggia, P.
(1999) Overview of the ARISE Project. Proceedings of
Eurospeech, 1999, Vol 4,   pp. 1527-1530.
[4] Miller S. (1998) The Generative Extraction Model.
Unpublished manuscript.
[5] Constantinides P., Hansma S., Tchou C. and Rudnicky,
A. (1999) A schema-based approach to dialog control.
Proceedings of ICSLP, Paper 637.
[6] Rudnicky A., Thayer, E., Constantinides P., Tchou C.,
Shern, R., Lenzo K., Xu W., Oh A. (1999) Creating
natural dialogs in the Carnegie Mellon Communicator
system. Proceedings of Eurospeech, 1999, Vol 4,   pp.
1531-1534
[7] Rudnicky A., and Xu W. (1999)  An agenda-based
dialog management  architecture for soken language
systems. In 1999 IEEE Workshop on Automatic Speech
Recognition and Understanding, Keystone, Colorado.
[8] Seneff S., and Polifroni, J. (2000) Dialogue
Management in the Mercury Flight Reservation
System. ANLP Conversational Systems Workshop.
Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 10?12,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Wearable Headset Speech-to-Speech Translation System 
 
 
Kriste Krstovski, Michael Decerbo, Rohit Prasad, David Stallard, Shirin Saleem, 
Premkumar Natarajan 
Speech and Language Processing Department 
BBN Technologies 
10 Moulton Street, Cambridge, MA, 02138 
{krstovski, mdecerbo, rprasad, stallard, ssaleem, prem}@bbn.com 
 
 
 
 
Abstract 
In this paper we present a wearable, headset 
integrated eyes- and hands-free speech-to-
speech (S2S) translation system. The S2S sys-
tem described here is configured for translin-
gual communication between English and 
colloquial Iraqi Arabic. It employs an n-gram 
speech recognition engine, a rudimentary 
phrase-based translator for translating recog-
nized Iraqi text, and a rudimentary text-to-
speech (TTS) synthesis engine for playing 
back the English translation. This paper de-
scribes the system architecture, the functional-
ity of its components, and the configurations 
of the speech recognition and machine transla-
tion engines.  
1 Background 
Humanitarian personnel, military personnel, and 
visitors in foreign countries often need to commu-
nicate with residents of a host country. Human in-
terpreters are inevitably in short supply, and 
training personnel to speak a new language is diffi-
cult. Under the DARPA TRANSTAC and Babylon 
programs, various teams have developed systems 
that enable two-way communication over a lan-
guage barrier (Waibel et al, 2003; Zhou et al, 
2004; Stallard et al, 2006). The two-way speech-
to-speech (S2S) translation systems seek, in prin-
ciple, to translate any utterance, by using general 
statistical models trained on large amounts of 
speech and text data.  
The performance and usability of such two-way 
speech-to-speech (S2S) translation systems is 
heavily dependent on the computational resources, 
such as processing power and memory, of the plat-
form they are running on. To enable open-ended 
conversation these S2S systems employ powerful 
but highly memory- and computation-intensive 
statistical speech recognition and machine transla-
tion models. Thus, at the very minimum they re-
quire the processing and memory configuration of 
common-of-the-shelf (COTS) laptops.  
Unfortunately, most laptops do not have a form 
factor that is suitable for mobile users. The size, 
weight, and shape of laptops render them unsuit-
able for handheld use. Moreover, simply carrying 
the laptop can be infeasible for users, such as mili-
tary personnel, who are already overburdened with 
other equipment. Embedded platforms, on the 
other hand, offer a more suitable form factor in 
terms of size and weight, but lack the computa-
tional resources required to run more open-ended 
2-way S2S systems. 
In previous work, Prasad et al (2007) reported 
on the development of a S2S system for Windows 
Mobile based handheld computers. To overcome 
the challenges posed by the limited resources of 
that platform, the PDA version of the S2S system 
was designed to be more constrained in terms of 
the ASR and MT vocabulary. As described in de-
tail in (Prasad et al, 2007), the PDA based S2S 
system configured for English/Iraqi S2S translation 
delivers fairly accurate translation at faster than 
real-time.  
In this paper, we present ongoing development 
work on an S2S system that runs on an even more 
constrained hardware platform; namely, a proces-
sor embedded in a wearable headset with just 32 
MB of memory. Compared to the PDA based sys-
10
tem described in (Prasad et al, 2007), the wearable 
system is designed for both eyes- and hands-free 
operation. The headset-integrated translation de-
vice described in this paper is configured for two-
way conversation in English/Iraqi. The target do-
main is the force protection, which includes sce-
narios of checkpoints, house searches, civil affairs, 
medical, etc.   
In what follows, we discuss the hardware and 
software details of the headset-integrated transla-
tion device. 
2 Hardware Platform  
The wearable S2S system described in this paper 
runs on a headset-integrated computational plat-
form developed by Integrated Wave Technologies, 
Inc. (IWT). The headset-integrated platform em-
ploys a 200 MHz StrongARM integer processor 
with a total of just 32MB RAM available for both 
the operating system and the translation software. 
The operating system currently running on the 
platform is Embedded Linux. 
There are two audio cards on the headset plat-
form for two-way communication through separate 
audio input and output channels. The default sound 
card uses the headset integrated close-talking mi-
crophone as an audio input and the second audio 
card can be used with an ambient microphone 
mounted on the device or an external microphone. 
In addition, each headset earpiece contains inner 
and outer set of speakers. The inner earpiece 
speakers are for the English speaking user who 
wears the headset, whereas the outer speakers are 
for the foreign language speaker who is not re-
quired to wear the headset. 
3 Software Architecture 
Depicted in Figure 1 is the software system archi-
tecture for the headset-integrated wearable S2S 
system. We are currently using a fixed-phrase Eng-
lish-to-Iraqi speech translation module from IWT 
for translating from English to Iraqi. In the Iraqi-
to-English (I2E) direction, we use an n-gram ASR 
engine to recognize Iraqi speech, a custom, phrase-
based ?micro translator? for translating Iraqi text to 
English text, and finally a TTS module for convert-
ing the English text into speech.  The rest of this 
paper focuses on the components of the Iraqi-to-
English translation module. 
 
Fixed point ASR Engine: The ASR engine uses 
phonetic hidden Markov models (HMM) with one 
or more forms of the following parameter tying: 
Phonetic-Tied Mixture (PTM), State-Tied Mixture 
(STM), and State-Clustered-Tied Mixture (SCTM) 
models. 
For the headset-integrated platform, we use a 
fixed-point ASR engine described in (Prasad et al, 
2007). As in (Prasad et al, 2007) for real-time per-
formance we use the compact PTM models in both 
recognition passes of our two-pass ASR decoder. 
Phrase-based Micro Translator: Phrase-based 
statistical machine translation (SMT) has been 
widely adopted as the translation engine in S2S 
systems. Such SMT engines require only a large 
corpus of bilingual sentence pairs to deliver robust 
performance on the domain of that corpus. How-
ever, phrase-based SMT engines require significant 
amount of memory, even when configured for me-
dium vocabulary tasks. Given the limited memory 
on the headset platform, we chose to develop in-
stead a phrase-based ?micro translator? module, 
which acts like a bottom-up parser. The micro-
translator uses translation rules derived from our 
phrase-based SMT engine. Rules are created auto-
matically by running the SMT engine on a small 
training corpus and recording the phrase pairs it 
used in decoding it. These phrase pairs then be-
come rules which are treated just as though they 
had been written by hand. The micro translator 
currently makes no use of probabilities.  Instead, as 
shown in Figure 2, for any given Arabic utterance, 
the translator greedily chooses the longest match-
ing source phrase that does not overlap a source 
phrase already chosen. The target phrases for these 
source phrases are then output as the translation. 
These target phrases come out in source-language 
 
Figure 1. Software architecture of the S2S system. 
 
 
11
order, as no language model is currently used for 
reordering.  
The micro translator currently consists of 1300 
rules and 2000 words. Its memory footprint is just 
32KB. This small memory footprint is achieved by 
representing the rules in binary format rather than 
text format.  
 
 
English Playback using TTS: To play the Eng-
lish translation to the headset user we developed a 
rudimentary TTS module. The TTS module parses 
the output of the I2E translator to extract each 
translated word. It then uses the list of extracted 
words to read the appropriate pre-recorded (or syn-
thesized) audio. Once the word pronunciations au-
dio files are read we splice the beginning and the 
end of the audio files to reduce the amount of si-
lence and concatenate them into a single file which 
is then played to the user on the inner earphone 
speakers. 
The total memory footprint of our current Iraqi 
to English translation module running on the head-
set-integrated platform is just 9MB. The current 
configuration of the translation module?s Iraqi 
ASR engine yields word error rate (WER) of 20% 
on test-set utterances without out-of-vocabulary 
(OOV) words.  
4 Conclusions and Future Work 
In this paper we have presented the initial setup of 
a speech-to-speech translation system configured 
for the headset platform. Our current work is fo-
cused on expanding the vocabulary of the Iraqi-to-
English translation module by exploiting the rich 
morphology of Iraqi Arabic. In particular, we are 
investigating the use of morphemes (prefix, stems, 
and suffixes) for expanding the effective vocabu-
lary of the headset translator. We are also develop-
ing use cases for performing a formal evaluation of 
both the usability and performance of the headset 
translator. 
References  
Alex Waibel, Ahmed Badran, Alan W Black, Robert 
Frederking, Donna Gates ,Alon Lavie, Lori Levin, 
Kevin Lenzo, Laura Mayfield Tomokiyo, J?urgen 
Reichert, Tanja Schultz, Dorcas Wallace, Monika 
Woszczyna and Jing Zhang. 2003. ?Speechalator: 
Two-way Speech-to-Speech Translation on a Con-
sumer PDA,? Proc. 8th European Conference on 
Speech Communication and Technology 
(EUROSPEECH 2003), Geneva, Switzerland. 
Bowen Zhou, Daniel D?echelotte and Yuqing Gao. 
2004. ?Two-way Speech-to-Speech Translation on 
Handheld Devices,? Proc. 8th International Confer-
ence on Spoken Language Processing, Jeju Island, 
Korea. 
David Stallard, Frederick Choi, Kriste Krstovski, Prem 
Natarajan and Shirin Saleem. 2006. ?A Hybrid 
Phrase-based/Statistical Speech Translation System,? 
Proc. The 9th International Conference on Spoken 
Language Processing (Interspeech 2006 - ICSLP), 
Pittsburg, PA. 
David Stallard, John Makhoul, Frederick Choi, Ehry 
Macrostie, Premkumar Natarajan, Richard Schwartz 
and Bushra Zawaydeh. 2003. ?Design and Evaluation 
of a Limited two-way  Speech Translator,? Proc. 8th 
European Conference on Speech Communication and 
Technology (EUROSPEECH 2003), Geneva, Swit-
zerland. 
Rohit Prasad, Kriste Krstovski, Frederick Choi, Shirin 
Saleem, Prem Natarajan, Michael Decerbo and David 
Stallard. 2007. ?Real-Time Speech-to-Speech Trans-
lation for PDAs,? Proc. IEEE International Confer-
ence on Portable Information Devices (IEEE Portable 
2007), Orlando, FL. 
 
 
 
 
Figure 2.  Decoding in micro translator. 
 
12
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 626?635,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Discriminative Sample Selection for Statistical Machine Translation?
Sankaranarayanan Ananthakrishnan, Rohit Prasad, David Stallard, and Prem Natarajan
Raytheon BBN Technologies
10 Moulton Street
Cambridge, MA, U.S.A.
{sanantha,rprasad,stallard,prem}@bbn.com
Abstract
Production of parallel training corpora for the
development of statistical machine translation
(SMT) systems for resource-poor languages
usually requires extensive manual effort. Ac-
tive sample selection aims to reduce the la-
bor, time, and expense incurred in produc-
ing such resources, attaining a given perfor-
mance benchmark with the smallest possible
training corpus by choosing informative, non-
redundant source sentences from an available
candidate pool for manual translation. We
present a novel, discriminative sample selec-
tion strategy that preferentially selects batches
of candidate sentences with constructs that
lead to erroneous translations on a held-out de-
velopment set. The proposed strategy supports
a built-in diversity mechanism that reduces
redundancy in the selected batches. Simu-
lation experiments on English-to-Pashto and
Spanish-to-English translation tasks demon-
strate the superiority of the proposed approach
to a number of competing techniques, such
as random selection, dissimilarity-based se-
lection, as well as a recently proposed semi-
supervised active learning strategy.
1 Introduction
Resource-poor language pairs present a significant
challenge to the development of statistical machine
translation (SMT) systems due to the latter?s depen-
dence on large parallel texts for training. Bilingual
human experts capable of producing the requisite
?Distribution Statement ?A? (Approved for Public Release,
Distribution Unlimited)
data resources are often in short supply, and the task
of preparing high-quality parallel corpora is labori-
ous and expensive. In light of these constraints, an
attractive strategy is to construct the smallest pos-
sible parallel training corpus with which a desired
performance benchmark may be achieved.
Such a corpus may be constructed by selecting the
most informative instances from a large collection
of source sentences for translation by a human ex-
pert, a technique often referred to as active learn-
ing. A SMT system trained with sentence pairs thus
generated is expected to perform significantly better
than if the source sentences were chosen using, say,
a na??ve random sampling strategy.
Previously, Eck et al (2005) described a selec-
tion strategy that attempts to maximize coverage by
choosing sentences with the highest proportion of
previously unseen n-grams. Depending on the com-
position of the candidate pool with respect to the
domain, this strategy may select irrelevant outliers.
They also described a technique based on TF-IDF to
de-emphasize sentences similar to those that have al-
ready been selected, thereby encouraging diversity.
However, this strategy is bootstrapped by random
initial choices that do not necessarily favor sentences
that are difficult to translate. Finally, they worked
exclusively with the source language and did not use
any SMT-derived features to guide selection.
Haffari et al (2009) proposed a number of fea-
tures, such as similarity to the seed corpus, transla-
tion probability, n-gram and phrase coverage, etc.,
that drive data selection. They also proposed a
model in which these features combine linearly to
predict a rank for each candidate sentence. The
626
top-ranked sentences are chosen for manual transla-
tion. However, this approach requires that the pool
have the same distributional characteristics as the
development sets used to train the ranking model.
Additionally, batches are chosen atomically. Since
similar or identical sentences in the pool will typi-
cally meet the selection criteria simultaneously, this
can have the undesired effect of choosing redundant
batches with low diversity.
The semi-supervised active learning strategy pro-
posed by Ananthakrishnan et al (2010) uses multi-
layer perceptrons (MLPs) to rank candidate sen-
tences based on various features, including domain
representativeness, translation difficulty, and batch
diversity. A greedy, incremental batch construction
technique encourages diversity. While this strat-
egy was shown to be superior to random as well
as n-gram based dissimilarity selection, its coarse
granularity (reducing a candidate sentence to a low-
dimensional feature vector for ranking) makes it un-
suitable for many situations. In particular, it is seen
to have little or no benefit over random selection
when there is no logical separation of the candidate
pool into ?in-domain? and ?out-of-domain? subsets.
This paper introduces a novel, active sample se-
lection technique that identifies translation errors on
a held-out development set, and preferentially se-
lects candidate sentences with constructs that are
incorrectly translated in the former. A discrimina-
tive pairwise comparator function, trained on the
ranked development set, is used to order candidate
sentences and pick sentences that provide maximum
potential reduction in translation error. The feature
functions that power the comparator are updated af-
ter each selection to encourage batch diversity. In
the following sections, we provide details of the pro-
posed sample selection approach, and describe sim-
ulation experiments that demonstrate its superiority
over a number of competing strategies.
2 Error-Driven Active Learning
Traditionally, unsupervised selection strategies have
dominated the active learning literature for natural
language processing (Hwa, 2004; Tang et al, 2002;
Shen et al, 2004). Sample selection for SMT has
followed a similar trend. The work of Eck et al
(2005) and most of the techniques proposed by Haf-
fari et al (2009) fall in this category. Notable ex-
ceptions include the linear ranking model of Haf-
fari et al (2009) and the semi-supervised selection
technique of Ananthakrishnan et al (2010), both of
which use one or more held-out development sets to
train and tune the sample selector. However, while
the former uses the posterior translation probability
and the latter, a sentence-level confidence score as
part of the overall selection strategy, current active
learning techniques for SMT do not explicitly target
the sources of error.
Error-driven active learning attempts to choose
candidate instances that potentially maximize error
reduction on a reference set (Cohn et al, 1996;
Meng and Lee, 2008). In the context of SMT, this
involves decoding a held-out development set with
an existing baseline (seed) SMT system. The selec-
tion algorithm is then trained to choose, from the
candidate pool, sentences containing constructs that
give rise to translation errors on this set. Assum-
ing perfect reference translations and word align-
ment in subsequent SMT training, these sentences
provide maximum potential reduction in translation
error with respect to the seed SMT system. It is a su-
pervised approach to sample selection. We assume
the following are available.
? A seed parallel corpus S for training the initial
SMT system.
? A candidate pool of monolingual source sen-
tences P from which samples must be selected.
? A held-out development set D for training the
selection algorithm and for tuning the SMT.
? A test set T for evaluating SMT performance.
We further make the following reasonable as-
sumptions: (a) the development set D and the test
set T are drawn from the same distribution and (b)
the candidate pool P consists of both in- and out-
of-domain source sentences, as well as an allowable
level of redundancy (similar or identical sentences).
Using translation errors on the development set to
drive sample selection has the following advantages
over previously proposed active learning strategies
for SMT.
? The seed training corpus S need not be derived
from the same distribution as D and T. The seed
SMT system can be trained with any available
627
parallel corpus for the specified language pair.
This is very useful if, as is often the case, lit-
tle or no in-domain training data is available to
bootstrap the SMT system. This removes a criti-
cal restriction present in the semi-supervised ap-
proach of Ananthakrishnan et al (2010).
? Sentences chosen are guaranteed to be relevant
to the domain, because selection is based on n-
grams derived from the development set. This
alleviates potential problems with approaches
suggested by Eck et al (2005) and several tech-
niques used by Haffari et al (2009), where ir-
relevant outliers may be chosen simply because
they contain previously unseen n-grams, or are
deemed difficult to translate.
? The proposed technique seeks to minimize
held-out translation error rather than maximize
training-set coverage. This is the more intuitive,
direct approach to sample selection for SMT.
? Diversity can be encouraged by preventing n-
grams that appear in previously selected sen-
tences from playing a role in choosing subse-
quent sentences. This provides an efficient alter-
native to the cumbersome ?batch diversity? fea-
ture proposed by Ananthakrishnan et al (2010).
The proposed implementation of error-driven ac-
tive learning for SMT, discriminative sample selec-
tion, is described in the following section.
3 Discriminative Sample Selection
The goal of active sample selection is to induce an
ordering of the candidate instances that satisfies an
objective criterion. Eck et al (2005) ordered can-
didate sentences based on the frequency of unseen
n-grams. Haffari et al (2009) induced a ranking
based on unseen n-grams, translation difficulty, etc.,
as well as one that attempted to incrementally max-
imize BLEU using two held-out development sets.
Ananthakrishnan et al (2010) attempted to order the
candidate pool to incrementally maximize source n-
gram coverage on a held-out development set, sub-
ject to difficulty and diversity constraints.
In the case of error-driven active learning, we at-
tempt to learn an ordering model based on errors
observed on the held-out development set D. We
achieve this in an innovative fashion by casting the
ranking problem as a pairwise sentence compari-
son problem. This approach, inspired by Ailon and
Mohri (2008), involves the construction of a binary
classifier functioning as a relational operator that can
be used to order the candidate sentences. The pair-
wise comparator is trained on an ordering of D that
ranks constituent sentences in decreasing order of
the number of translation errors. The comparator is
then used to rank the candidate pool in decreasing
order of potential translation error reduction.
3.1 Maximum-Entropy Pairwise Comparator
Given a pair of source sentences (u, v), we define,
adopting the notation of Ailon and Mohri (2008), the
pairwise comparator h(u, v) as follows:
h(u, v) =
{
1, u < v
0, u >= v
(1)
In Equation 1, the binary comparator h(u, v)
plays the role of the ?less than? (?<?) relational op-
erator, returning 1 if u is preferred to v in an or-
dered list, and 0 otherwise. As detailed in Ailon and
Mohri (2008), the comparator must satisfy the con-
straint that h(u, v) and h(v, u) be complementary,
i.e. h(u, v) + h(v, u) = 1 to avoid ambiguity. How-
ever, it need not satisfy the triangle inequality.
We implement h(u, v) as a combination of dis-
criminative maximum entropy classifiers triggered
by feature functions drawn from n-grams of u and v.
We define p(u, v) as the conditional posterior prob-
ability of the Bernoulli event u < v given (u, v) as
shown in Equation 2.
p(u, v) = Pr(u < v | u, v) (2)
In our implementation, p(u, v) is the output of
a binary maximum-entropy classifier trained on the
development set. However, this implementation
poses two problems.
First, if we use constituent n-grams of u and v
as feature functions to trigger the classifier, there is
no way to distinguish between (u, v) and (v, u) as
they will trigger the same feature functions. This
will result in identical values for p(u, v) and p(v, u),
a contradiction. We resolve this issue by intro-
ducing a set of ?complementary? feature functions,
which are formed by simply appending a recogniz-
able identifier to the existing n-gram feature func-
628
u: how are you
v: i am going
f(u) = {how:1, are:1, you:1, how*are:2, are*you:2, how*are*you:3}
f(v) = {i:1, am:1, going:1, i*am:2, am*going:2, i*am*going:3}
f ?(u) = {!how:1, !are:1, !you:1, !how*are:2, !are*you:2, !how*are*you:3}
f ?(v) = {!i:1, !am:1, !going:1, !i*am:2, !am*going:2, !i*am*going:3}
Table 1: Standard and complementary trigram feature functions for a source pair (u, v).
tions. Then, to evaluate p(u, v), for instance, we
invoke the classifier with standard feature functions
for u and complementary feature functions for v.
Similarly, p(v, u) is evaluated by triggering comple-
mentary feature functions for u and standard feature
functions for v. Table 1 illustrates this with a simple
example.
Note that each feature function is associated with
a real value, whose magnitude is an indicator of its
importance. In our implementation, an n-gram fea-
ture function (standard or complementary) receives
a value equal to its length. This is based on our intu-
ition that longer n-grams play a more important role
in dictating SMT performance.
Second, the introduction of complementary trig-
gers implies that evaluation of p(u, v) and p(v, u)
now involves disjoint sets of feature functions. Thus,
p(u, v) is not guaranteed to satisfy the complemen-
tarity condition imposed on h(u, v), and therefore
cannot directly be used as the binary pairwise com-
parator. We resolve this by normalizing across the
two possible permutations, as follows:
h?(u, v) = p(u, v)p(u, v) + p(v, u) (3)
h?(v, u) = p(v, u)p(u, v) + p(v, u) (4)
Since h?(u, v) + h?(v, u) = 1, the complemen-
tarity constraint is now satisfied, and h(u, v) is just
a binarized (thresholded) version of h?(u, v). Thus,
the binary pairwise comparator can be constructed
from the permuted classifier outputs.
3.2 Training the Pairwise Comparator
Training the maximum-entropy classifier for the
pairwise comparator requires a set of target labels
and input feature functions, both of which are de-
rived from the held-out development set D. We be-
gin by decoding the source sentences in D with the
seed SMT system, followed by error analysis using
the Translation Edit Rate (TER) measure (Snover
et al, 2006). TER measures translation quality by
computing the number of edits (insertions, substitu-
tions, and deletions) and shifts required to transform
a translation hypothesis to its corresponding refer-
ence. We then rank D in decreasing order of the
number of post-shift edits, i.e. the number of in-
sertions, substitutions, and deletions after the shift
operation is completed. Since shifts are often due to
word re-ordering issues within the SMT decoder (es-
pecially for phrase-based systems), we do not con-
sider them as errors for the purpose of ranking D.
Sentences at the top of the ordered list D? contain
the maximum number of translation errors.
For each pair of sentences (u, v) : u < v in D?,
we generate two training entries. The first, signify-
ing that u appears before v in D?, assigns the label
true to a trigger list consisting of standard feature
functions derived from u, and complementary fea-
ture functions derived from v. The second, reinforc-
ing this observation, assigns the label false to a trig-
ger list consisting of complementary feature func-
tions from u, and standard feature functions from v.
The labeled training set (feature:label pairs) for the
comparator can be expressed as follows:
?(u, v) ? D? : u < v,
{f(u) f ?(v)} : true
{f ?(u) f(v)} : false
Thus, if there are d sentences in D?, we obtain a
total of d(d? 1) labeled examples to train the com-
parator. We use the standard L-BFGS optimization
629
algorithm (Liu and Nocedal, 1989) to estimate the
parameters of the maximum entropy model.
3.3 Greedy Discriminative Selection
The discriminatively-trained pairwise comparator
can be used as a relational operator to sort the candi-
date pool P in decreasing order of potential transla-
tion error reduction. A batch of pre-determined size
K can then be selected from the top of this list to
augment the existing SMT training corpus. Assum-
ing the pool contains N candidate sentences, and
given a fast sorting algorithm such as Quicksort, the
complexity of this strategy is O(N logN). Batches
can be selected iteratively until a specified perfor-
mance threshold is achieved.
A potential downside of this approach reveals it-
self when there is redundancy in the candidate pool.
Since the batch is selected in a single atomic opera-
tion from the sorted candidates, and because similar
or identical sentences will typically occupy the same
range in the ordered list, it is likely that this approach
will result in batches with low diversity. Whereas
we desire diverse batches for better coverage and ef-
ficient use of manual translation resources. This is-
sue was previously addressed in Shen et al (2004) in
the context of named-entity recognition, where they
used a two-step procedure to first select the most in-
formative and representative samples, followed by a
diversity filter. Ananthakrishnan et al (2010) used a
greedy, incremental batch construction strategy with
an integrated, explicit batch diversity feature as part
of the ranking model. Based on these ideas, we de-
sign a greedy selection strategy using the discrimi-
native relational operator.
Rather than perform a full sort on P, we sim-
ply invoke the minh(u,v)(? ? ? ) function to find the
sentence that potentially minimizes translation er-
ror. The subscript indicates that our implementation
of this function utilizes the discriminative relational
operator trained on the development set D. The best
choice sentence s is then added to our batch at the
current position (we begin with an empty batch). We
then remove the standard and complementary fea-
ture functions f(s) and f ?(s) triggered by s from the
global pool of feature functions obtained from D,
so that they do not play a role in the selection of
subsequent sentences for the batch. Subsequently,
a candidate sentence that is similar or identical to
Algorithm 1 Greedy Discriminative Selection
B? ()
for k = 1 to K do
s? minh(u,v)(P)
B(k)? s
P? P? {s}
f(D)? f(D)? f(s)
f ?(D)? f ?(D)? f ?(s)
end for
return B
s will not be preferred, because the feature func-
tions that previously caused it to rank highly will
no longer trigger. Algorithm 1 summarizes our se-
lection strategy in pseudocode. Since each call to
minh(u,v)(? ? ? ) is O(N), the overall complexity of
greedy discriminative selection is O(K ?N).
4 Experiments and Results
We conduct a variety of simulation experiments
with multiple language pairs (English-Pashto and
Spanish-English) and different data configurations
in order to demonstrate the utility of discrimina-
tive sample selection in the context of resource-poor
SMT. We also compare the performance of the pro-
posed strategy to numerous competing active and
passive selection methods as follows:
? Random: Source sentences are uniformly sam-
pled from the candidate pool P.
? Similarity: Choose sentences from P with the
highest fraction of n-gram overlap with the seed
corpus S.
? Dissimilarity: Select sentences from P with the
highest proportion of n-grams not seen in the
seed corpus S (Eck et al, 2005; Haffari et al,
2009).
? Longest: Pick the longest sentences from the
candidate pool P.
? Semi-supervised: Semi-supervised active learn-
ing with greedy incremental selection (Anan-
thakrishnan et al, 2010).
? Discriminative: Choose sentences that po-
tentially minimize translation error using a
maximum-entropy pairwise comparator (pro-
posed method).
630
Identical low-resource initial conditions are ap-
plied to each selection strategy so that they may be
objectively compared. A very small seed corpus S is
sampled from the available parallel training data; the
remainder serves as the candidate pool. Following
the literature on active learning for SMT, our simula-
tion experiments are iterative. A fixed-size batch of
source sentences is constructed from the candidate
pool using one of the above selection strategies. We
then look up the corresponding translations from the
candidate targets (simulating an expert human trans-
lator), augment the seed corpus with the selected
data, and update the SMT system with the expanded
training corpus. The selected data are removed from
the candidate pool. This select-update cycle is then
repeated for either a fixed number of iterations or
until a specified performance benchmark is attained.
At each iteration, we decode the unseen test set T
with the most current SMT configuration and eval-
uate translation performance in terms of BLEU as
well as coverage (defined as the fraction of untrans-
latable source words in the target hypotheses).
We use a phrase-based SMT framework similar to
Koehn et al (2003) for all experiments.
4.1 English-Pashto Simulation
Our English-Pashto (E2P) data originates from a
two-way collection of spoken dialogues, and con-
sists of two parallel sub-corpora: a directional E2P
corpus and a directional Pashto-English (P2E) cor-
pus. Each sub-corpus has its own independent train-
ing, development, and test partitions. The direc-
tional E2P training, development, and test sets con-
sist of 33.9k, 2.4k, and 1.1k sentence pairs, respec-
tively. The directional P2E training set consists of
76.5k sentence pairs. The corpus was used as-is, i.e.
no length-based filtering or redundancy-reduction
(i.e. removal of duplicates, if any) was performed.
The test-set BLEU score with the baseline E2P SMT
system trained from all of the above data was 9.5%.
We obtained a seed training corpus by randomly
sampling 1,000 sentence pairs from the directional
E2P training partition. The remainder of this set, and
the entire reversed P2E training partition were com-
bined to create the pool (109.4k sentence pairs). In
the past, we have observed that the reversed direc-
tional P2E data gives very little performance gain
in the E2P direction even though its vocabulary is
similar, and can be considered ?out-of-domain? as
far as the E2P translation task is concerned. Thus,
our pool consists of 30% in-domain and 70% out-
of-domain sentence pairs, making for a challeng-
ing active learning problem. A pool training set of
10k source sentences is sampled from this collection
for the semi-supervised selection strategy, leaving us
with 99.4k candidate sentences, which we use for all
competing techniques. The data configuration used
in this simulation is identical to Ananthakrishnan et
al. (2010), allowing us to compare various strategies
under the same conditions. We simulated a total of
20 iterations with batches of 200 sentences each; the
original 1,000 sample seed corpus grows to 5,000
sentence pairs and the end of our simulation.
Figure 1(a) illustrates the variation in BLEU
scores across iterations for each selection strategy.
The proposed discriminative sample selection tech-
nique performs significantly better at every iteration
than random, similarity, dissimilarity, longest, and
semi-supervised active selection. At the end of 20
iterations, the BLEU score gained 3.21 points, a rel-
ative improvement of 59.3%. This was followed by
semi-supervised active learning, which improved by
2.66 BLEU points, a 49.2% relative improvement.
Table 2 summarizes the total number of words se-
lected by each strategy, as well as the total area
under the BLEU curve with respect to the base-
line. The latter, labeled BLEUarea and expressed in
percent-iterations, is a better measure of the over-
all performance of each strategy across all iterations
than comparing BLEU scores at the final iteration.
Figure 1(b) shows the variation in coverage (per-
centage of untranslatable source words in target
hypotheses) for each selection technique. Here,
discriminative sample selection was better than all
other approaches except longest-sentence selection.
4.2 Spanish-English Simulation
The Spanish-English (S2E) training corpus was
drawn from the Europarl collection (Koehn, 2005).
To prevent length bias in selection, the corpus was
filtered to only retain sentence pairs whose source
ranged between 7 and 15 words (excluding punc-
tuation). Additionally, redundancy was reduced by
removing all duplicate sentence pairs. After these
steps, we obtained approximately 253k sentence
pairs for training. The WMT10 held-out develop-
631
(a) Variation in BLEU (E2P)
(b) Variation in coverage (E2P)
Figure 1: Simulation results for E2P data selection.
632
(a) Variation in BLEU (S2E)
(b) Variation in coverage (S2E)
Figure 2: Simulation results for S2E data selection.
633
Method E2P size E2P BLEUarea S2E size S2E BLEUarea
Random 58.1k 26.4 26.5k 45.0
Similarity 30.7k 21.9 24.7k 13.2
Dissimilarity 39.2k 12.4 24.2k 54.9
Longest 173.0k 27.5 39.6k 48.3
Semi-supervised 80.0k 34.1 27.6k 45.6
Discriminative 109.1k 49.6 31.0k 64.5
Table 2: Source corpus size (in words) and BLEUarea after 20 sample selection iterations.
ment and test sets (2k and 2.5k sentence pairs, re-
spectively) were used to tune our system and eval-
uate performance. Note that this data configuration
is different from that of the E2P simulation in that
there is no logical separation of the training data into
?in-domain? and ?out-of-domain? sets. The baseline
S2E SMT system trained with all available data gave
a test-set BLEU score of 17.2%.
We randomly sampled 500 sentence pairs from
the S2E training partition to obtain a seed train-
ing corpus. The remainder, after setting aside an-
other 10k source sentences for training the semi-
supervised strategy, serves as the candidate pool. We
again simulated a total of 20 iterations, except in
this case, we used batches of 100 sentences in an at-
tempt to obtain smoother performance trajectories.
The training corpus grows from 500 sentence pairs
to 2,500 as the simulation progresses.
Variation in BLEU scores and coverage for the
S2E simulation are illustrated in Figures 2(a) and
2(b), respectively. Discriminative sample selection
outperformed all other selection techniques across
all iterations of the simulation. After 20 iterations,
we obtained a 4.51 point gain in BLEU, a rela-
tive improvement of 142.3%. The closest com-
petitor was dissimilarity-based selection, which im-
proved by 4.38 BLEU points, a 138.1% relative
improvement. The proposed method also outper-
formed other selection strategies in improving cov-
erage, with significantly better results especially in
the early iterations. Table 2 summarizes the number
of words chosen, and BLEUarea, for each strategy.
5 Conclusion and Future Directions
Building SMT systems for resource-poor language
pairs requires significant investment of labor, time,
and money for the development of parallel training
corpora. We proposed a novel, discriminative sam-
ple selection strategy that can help lower these costs
by choosing batches of source sentences from a large
candidate pool. The chosen sentences, in conjunc-
tion with their manual translations, provide signifi-
cantly better SMT performance than numerous com-
peting active and passive selection techniques.
Our approach hinges on a maximum-entropy pair-
wise comparator that serves as a relational operator
for comparing two source sentences. This allows us
to rank the candidate pool in decreasing order of po-
tential reduction in translation error with respect to
an existing seed SMT system. The discriminative
comparator is coupled with a greedy, incremental se-
lection technique that discourages redundancy in the
chosen batches. The proposed technique diverges
from existing work on active sample selection for
SMT in that it uses machine learning techniques in
an attempt to explicitly reduce translation error by
choosing sentences whose constituents were incor-
rectly translated in a held-out development set.
While the performance of competing strategies
varied across language pairs and data configurations,
discriminative sample selection proved consistently
superior under all test conditions. It provides a pow-
erful, flexible, data selection front-end for rapid de-
velopment of SMT systems. Unlike some selection
techniques, it is also platform-independent, and can
be used as-is with a phrase-based, hierarchical, syn-
tactic, or other SMT framework.
We have so far restricted our experiments to simu-
lations, obtaining expert human translations directly
from the sequestered parallel corpus. We are now
actively exploring the possibility of linking the sam-
ple selection front-end to a crowd-sourcing back-
end, in order to obtain ?non-expert? translations us-
ing a platform such as the Amazon Mechanical Turk.
634
References
Nir Ailon and Mehryar Mohri. 2008. An efficient reduc-
tion of ranking to classification. In COLT ?08: Pro-
ceedings of the 21st Annual Conference on Learning
Theory, pages 87?98.
Sankaranarayanan Ananthakrishnan, Rohit Prasad, David
Stallard, and Prem Natarajan. 2010. A semi-
supervised batch-mode active learning strategy for
improved statistical machine translation. In CoNLL
?10: Proceedings of the 14th International Conference
on Computational Natural Language Learning, pages
126?134, July.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-
dan. 1996. Active learning with statistical models.
Journal of Artificial Intelligence Research, 4(1):129?
145.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2005.
Low cost portability for statistical machine translation
based in N-gram frequency and TF-IDF. In Proceed-
ings of IWSLT, Pittsburgh, PA, October.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based ma-
chine translation. In NAACL ?09: Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 415?423,
Morristown, NJ, USA. Association for Computational
Linguistics.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30:253?276.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL
?03: Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 48?54, Morristown, NJ, USA. Association for
Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In MT Summit X:
Proceedings of the 10th Machine Translation Summit,
pages 79?86.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Math.
Program., 45(3):503?528.
Qinggang Meng and Mark Lee. 2008. Error-driven
active learning in growing radial basis function net-
works for early robot learning. Neurocomputing, 71(7-
9):1449?1461.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-
Lim Tan. 2004. Multi-criteria-based active learning
for named entity recognition. In ACL ?04: Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, pages 589?596, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings AMTA, pages 223?231, August.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. Ac-
tive learning for statistical natural language parsing.
In ACL ?02: Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
120?127, Morristown, NJ, USA. Association for Com-
putational Linguistics.
635
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 49?59,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Machine Translation of Arabic Dialects
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas,
Richard Schwartz, John Makhoul, Omar F. Zaidan?, Chris Callison-Burch?
Raytheon BBN Technologies, Cambridge MA
?Microsoft Research, Redmond WA
?Johns Hopkins University, Baltimore MD
Abstract
Arabic Dialects present many challenges for
machine translation, not least of which is the
lack of data resources. We use crowdsourc-
ing to cheaply and quickly build Levantine-
English and Egyptian-English parallel cor-
pora, consisting of 1.1M words and 380k
words, respectively. The dialectal sentences
are selected from a large corpus of Arabic web
text, and translated using Amazon?s Mechan-
ical Turk. We use this data to build Dialec-
tal Arabic MT systems, and find that small
amounts of dialectal data have a dramatic im-
pact on translation quality. When translating
Egyptian and Levantine test sets, our Dialec-
tal Arabic MT system performs 6.3 and 7.0
BLEU points higher than a Modern Standard
Arabic MT system trained on a 150M-word
Arabic-English parallel corpus.
1 Introduction
The Arabic language is a well-known example of
diglossia (Ferguson, 1959), where the formal vari-
ety of the language, which is taught in schools and
used in written communication and formal speech
(religion, politics, etc.) differs significantly in its
grammatical properties from the informal varieties
that are acquired natively, which are used mostly for
verbal communication. The spoken varieties of the
Arabic language (which we refer to collectively as
Dialectal Arabic) differ widely among themselves,
depending on the geographic distribution and the
socio-economic conditions of the speakers, and they
diverge from the formal variety known as Mod-
ern Standard Arabic (MSA) (Embarki and Ennaji,
2011). Significant differences in the phonology,
morphology, lexicon and even syntax render some
of these varieties mutually incomprehensible.
The use of Dialectal Arabic has traditionally been
confined to informal personal speech, while writ-
ing has been done almost exclusively using MSA
(or its ancestor Classical Arabic). This situation is
quickly changing, however, with the rapid prolifer-
ation of social media in the Arabic-speaking part
of the world, where much of the communication
is composed in dialect. The focus of the Arabic
NLP research community, which has been mostly on
MSA, is turning towards dealing with informal com-
munication, with the introduction of the DARPA
BOLT program. This new focus presents new chal-
lenges, the most obvious of which is the lack of di-
alectal linguistic resources. Dialectal text, which is
usually user-generated, is also noisy, and the lack
of standardized orthography means that users often
improvise spelling. Dialectal data also includes a
wider range of topics than formal data genres, such
as newswire, due to its informal nature. These chal-
lenges require innovative solutions if NLP applica-
tions are to deal with Dialectal Arabic effectively.
In this paper:
? We describe a process for cheaply and quickly
developing parallel corpora for Levantine-
English and Egyptian-English using Amazon?s
Mechanical Turk crowdsourcing service (?3).
? We use the data to perform a variety of machine
translation experiments showing the impact of
morphological analysis, the limited value of
adding MSA parallel data, the usefulness of
cross-dialect training, and the effects of trans-
lating from dialect to MSA to English (?4).
We find that collecting dialect translations has a low
cost ($0.03/word) and that relatively small amounts
of data has a dramatic impact on translation quality.
When trained on 1.5M words of dialectal data, our
system performs 6.3 to 7.0 BLEU points higher than
when it is trained on 100 times more MSA data from
a mismatching domain.
49
2 Previous Work
Existing work on natural language processing of Di-
alectal Arabic text, including machine translation, is
somewhat limited. Previous research on Dialectal
Arabic MT has focused on normalizing dialectal in-
put words into MSA equivalents before translating
to English, and they deal with inputs that contain
a limited fraction of dialectal words. Sawaf (2010)
normalized the dialectal words in a hybrid (rule-
based and statistical) MT system, by performing a
combination of character- and morpheme-level map-
pings. They then translated the normalized source
to English using a hybrid MT or alternatively a
Statistical MT system. They tested their method
on proprietary test sets, observing about 1 BLEU
point (Papineni et al, 2002) increase on broadcast
news/conversation and about 2 points on web text.
Salloum and Habash (2011) reduced the proportion
of dialectal out-of-vocabulary (OOV) words also by
mapping their affixed morphemes to MSA equiva-
lents (but did not perform lexical mapping on the
word stems). They allowed for multiple morpho-
logical analyses, passing them on to the MT system
in the form of a lattice. They tested on a subset of
broadcast news and broadcast conversation data sets
consisting of sentences that contain at least one re-
gion marked as non-MSA, with an initial OOV rate
against an MSA training corpus of 1.51%. They
obtained a 0.62 BLEU point gain. Abo Bakr et
al. (2008) suggested another hybrid system to map
Egyptian Arabic to MSA, using morphological anal-
ysis on the input and an Egyptian-MSA lexicon.
Other work that has focused on tasks besides MT
includes that of Chiang et al (2006), who built a
parser for spoken Levantine Arabic (LA) transcripts
using an MSA treebank. They used an LA-MSA
lexicon in addition to morphological and syntac-
tic rules to map the LA sentences to MSA. Riesa
and Yarowsky (2006) built a statistical morphologi-
cal segmenter for Iraqi and Levantine speech tran-
scripts, and showed that they outperformed rule-
based segmentation with small amounts of training.
Some tools exist for preprocessing and tokenizing
Arabic text with a focus on Dialectal Arabic. For ex-
ample, MAGEAD (Habash and Rambow, 2006) is a
morphological analyzer and generator that can ana-
lyze the surface form of MSA and dialect words into
their root/pattern and affixed morphemes, or gener-
ate the surface form in the opposite direction.
Amazon?s Mechanical Turk (MTurk) is becom-
ing an essential tool for creating annotated resources
for computational linguistics. Callison-Burch and
Dredze (2010) provide an overview of various tasks
for which MTurk has been used, and offer a set of
best practices for ensuring high-quality data.
Zaidan and Callison-Burch (2011a) studied the
quality of crowdsourced translations, by quantifying
the quality of non-professional English translations
of 2,000 Urdu sentences that were originally trans-
lated by the LDC. They demonstrated a variety of
mechanisms that increase the translation quality of
crowdsourced translations to near professional lev-
els, with a total cost that is less than one tenth the
cost of professional translation.
Zaidan and Callison-Burch (2011b) created the
Arabic Online Commentary (AOC) dataset, a 52M-
word monolingual dataset rich in dialectal content.
Over 100k sentences from the AOC were annotated
by native Arabic speakers on MTurk to identify the
dialect level (and dialect itself) in each, and the col-
lected labels were used to train automatic dialect
identification systems. Although a large number
of dialectal sentences were identified (41% of sen-
tences), none were passed on to a translation phase.
3 Data Collection and Annotation
Following Zaidan and Callison-Burch (2011a,b), we
use MTurk to identify Dialectal Arabic data and to
create a parallel corpus by hiring non-professional
translators to translate the sentences that were la-
beled as being dialectal. We had Turkers perform
three steps for us: dialect classification, sentence
segmentation, and translation.
Since Dialectal Arabic is much less common in
written form than in spoken form, the first challenge
is to simply find instances of written Dialectal Ara-
bic. We draw from a large corpus of monolingual
Arabic text (approximately 350M words) that was
harvested from the web by the LDC, largely from
weblog and online user groups.1 Before present-
ing our data to annotators, we filter it to identify
1Corpora: LDC2006E32, LDC2006E77, LDC2006E90,
LDC2007E04, LDC2007E44, LDC2007E102, LDC2008E41,
LDC2008E54, LDC2009E14, LDC2009E93.
50
M
ag
hr
eb
i
E
gy
Ir
aq
i
G
ul
f
Ot
he
r
L
ev
Figure 1: One possible breakdown of spoken Arabic into
dialect groups: Maghrebi, Egyptian, Levantine, Gulf and
Iraqi. Habash (2010) gives a breakdown along mostly
the same lines. We used this map as an illustration for
annotators in our dialect classification task (Section 3.1),
with Arabic names for the dialects instead of English.
segments most likely to be dialectal (unlike Zaidan
and Callison-Burch (2011b), who did no such pre-
filtering). We eliminate documents with a large per-
centage of non-Arabic or MSA words. We then
retain documents that contain some number of di-
alectal words, using a set of manually selected di-
alectal words that was assembled by culling through
the transcripts of the Levantine Fisher and Egyp-
tian CallHome speech corpora. After filtering, the
dataset contained around 4M words, which we used
as a starting point for creating our Dialectal Arabic-
English parallel corpus.
3.1 Dialect Classification
To refine the document set beyond our keyword fil-
tering heuristic and to label which dialect each doc-
ument is written in, we hire Arabic annotators on
MTurk to perform classification similar to Zaidan
and Callison-Burch (2011b). Annotators were asked
to classify the filtered documents for being in MSA
or in one of four regional dialects: Egyptian, Lev-
antine, Gulf/Iraqi or Maghrebi, and were shown the
map in Figure 1 to explain what regions each of the
dialect labels corresponded to. We allowed an addi-
tional ?General? dialect option for ambiguous docu-
ments. Unlike Zaidan and Callison-Burch, our clas-
sification was applied to whole documents (corre-
sponding to a user online posting) instead of individ-
ual sentences. To perform quality control, we used
a set of documents for which correct labels were
known. We presented these 20% of the time, and
Dialect Classification HIT $10,064
Sentence Segmentation HIT $1,940
Translation HIT $32,061
Total cost $44,065
Num words translated 1,516,856
Cost per word 2.9 cents/word
Table 1: The total costs for the three MTurk subtasks in-
volved with the creation of our Dialectal Arabic-English
parallel corpus.
eliminated workers who did not correctly classify
them (2% of labels).
Identifying the dialect of a text snippet can be
challenging in the absence of phonetic cues. We
therefore required 3 classifications from different
workers for every document, and accepted a dialect
label if at least two of them agreed. The dialect dis-
tribution of the final output was: 43% Gulf/Iraqi,
28% Levantine, 11% Egyptian, and 16% could not
be classified. MSA and the other labels accounted
for 2%. We decided to translate only the Levantine
and Egyptian documents, since the pool of MTurk
workers contained virtually no workers from Iraq or
the Gulf region.
3.2 Sentence Segmentation
Since the data we annotated was mostly user-
generated informal web content, the existing punc-
tuation was often insufficient to determine sentence
boundaries. Since sentence boundaries are impor-
tant for correct translation, we segmented passages
into individual sentences using MTurk. We only re-
quired sentences longer than 15 words to be seg-
mented, and allowed Turkers to split and rejoin at
any point between the tokens. The instructions were
simply to ?divide the Arabic text into individual sen-
tences, where you believe it would be appropriate
to insert a period.? We also used a set of correctly
segmented passages for quality control, and scored
Turkers using a metric based on the precision and
recall of correct segmentation points. The rejection
rate was 1.2%.
3.3 Translation to English
Following Zaidan and Callison-Burch (2011a), we
hired non-professional translators on MTurk to
translate the Levantine and Egyptian sentences into
51
Sentence Arabic English
Data Set Pairs Tokens Tokens
MSA-150MW 8.0M 151.4M 204.4M
Dialect-1500KW 180k 1,545,053 2,257,041
MSA-1300KW 71k 1,292,384 1,752,724
MSA-Web-Tune 6,163 145,260 184,185
MSA-Web-Test 5,454 136,396 172,357
Lev-Web-Tune 2,600 20,940 27,399
Lev-Web-Test 2,600 21,092 27,793
Egy-Web-Test 2,600 23,671 33,565
E-Facebook-Tune 3,351 25,130 34,753
E-Facebook-Test 3,188 25,011 34,244
Table 2: Statistics about the training/tuning/test datasets
used in our experiments. The token counts are calculated
before MADA segmentation.
English. Among several quality control measures,
we rendered the Arabic sentences as images to pre-
vent Turkers from simply copying the Arabic text
into translation software. We still spot checked the
translations against the output of Google Translate
and Bing Translator. We also rejected gobbledygook
garbage translations that have a high percentage of
words not found in an English lexicon.
We quantified the quality of an individual Turker?s
translations in two ways: first by asking native Ara-
bic speaker judges to score a sample of the Turker?s
translations, and second by inserting control sen-
tences for which we have good reference translations
and measuring the Turker?s METEOR (Banerjee and
Lavie, 2005) and BLEU-1 scores (Papineni et al,
2002).2 The rejection rate of translation assignments
was 5%. We promoted good translators to a re-
stricted access ?preferred worker queue?. They were
paid at a higher rate, and were required to translate
control passages only 10% of the time as opposed
to 20% for general Turkers, thus providing us with a
higher translation yield for unseen data.
Worker turnout was initially slow, but increased
quickly as our reputation for being reliable payers
was established; workers started translating larger
volumes and referring their acquaintances. We had
121 workers who each completed 20 or more trans-
lation assignments. We eventually reached and sus-
tained a rate of 200k words of acceptable quality
2BLEU-1 provided a more reliable correlation with human
judgment in this case that the regular BLEU score (which uses
n-gram orders 1, . . . , 4), given the limited size of the sample
measured.
translated per week. Unlike Zaidan and Callison-
Burch (2011a), who only translated 2,000 Urdu sen-
tences, we translated sufficient volumes of Dialectal
Arabic to train machine translation systems. In total,
we had 1.1M words of Levantine and 380k words of
Egyptian translated into English, corresponding to
about 2.3M words on the English side.
Table 1 outlines the costs involved with creating
our parallel corpus. The total cost was $44k, or
$0.03/word ? an order of magnitude cheaper than
professional translation.
4 Experiments in Dialectal Arabic-English
Machine Translation
We performed a set of experiments to contrast sys-
tems trained using our dialectal parallel corpus with
systems trained on a (much larger) MSA-English
parallel corpus. All experiments use the same meth-
ods for training, decoding and parameter tuning, and
we only varied the corpora used for training, tun-
ing and testing. The MT system we used is based
on a phrase-based hierarchical model similar to that
of Shen et al (2008). We used GIZA++ (Och and
Ney, 2003) to align sentences and extract hierar-
chical rules. The decoder used a log-linear model
that combines the scores of multiple feature scores,
including translation probabilities, smoothed lexi-
cal probabilities, a dependency tree language model,
in addition to a trigram English language model.
Additionally, we used 50,000 sparse, binary-valued
source and target features based on Chiang et al
(2009). The English language model was trained on
7 billion words from the Gigaword and from a web
crawl. The feature weights were tuned to maximize
the BLEU score on a tuning set using the Expected-
BLEU optimization procedure (Devlin, 2009).
The Dialectal Arabic side of our corpus consisted
of 1.5M words (1.1M Levantine and 380k Egyp-
tian). Table 2 gives statistics about the various
train/tune/test splits we used in our experiments.
Since the Egyptian set was so small, we split it only
to training/test sets, opting not to have a tuning set.
The MSA training data we used consisted of Arabic-
English corpora totaling 150M tokens (Arabic side).
The MSA train/tune/test sets were constructed for
the DARPA GALE program.
We report translation quality in terms of BLEU
52
Simple Segment MADA Segment
Training Tuning BLEU OOV BLEU OOV ?BLEU ?OOV
MSA-Web-Test
MSA-150MW MSA-Web 26.21 1.69% 27.85 0.48% +1.64 -1.21%
MSA-1300KW 21.24 7.20% 25.23 1.95% +3.99 -5.25%
Egyptian-Web-Test
Dialect-1500KW Levantine-Web 18.55 6.31% 20.66 2.85% +2.11 -3.46%
Levantine-Web-Test
Dialect-1500KW Levantine-Web 17.00 6.22% 19.29 2.96% +2.29 -3.26%
Table 3: Comparison of the effect of morphological segmentation when translating MSA web text and Dialectal
Arabic web text. The morphological segmentation uniformly improves translation quality, but the improvements are
more dramatic for MSA than for Dialectal Arabic when comparing similarly-sized training corpora.
Training Tuning BLEU OOV BLEU OOV BLEU OOV
Egyptian-Web-Test Levantine-Web-Test MSA-Web-Test
MSA-150MW MSA-Web 14.76 4.42% 11.83 5.53% 27.85 0.48%
MSA-150MW Lev-Web 14.34 4.42% 12.29 5.53% 24.63 0.48%
MSA-150MW+Dial-1500KW 20.09 2.04% 19.11 2.27% 24.30 0.45%
Dialect-1500KW 20.66 2.85% 19.29 2.96% 15.53 3.70%
Egyptian-360KW 19.04 4.62% 11.21 9.00% - -
Levantine-360KW 14.05 7.11% 16.36 5.24% - -
Levantine-1100KW 17.79 4.83% 19.29 3.31% - -
Table 4: A comparison of translation quality of Egyptian, Levantine, andMSAweb text, using various training corpora.
The highest BLEU scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian),
since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the
dialectal data results in marginally worse translations.
score.3 In addition, we also report the OOV rate of
the test set relative to the training corpus in each ex-
perimental setups.
4.1 Morphological Decomposition
Arabic has a complex morphology compared to En-
glish. Preprocessing the Arabic source by morpho-
logical segmentation has been shown to improve the
performance of Arabic MT (Lee, 2004; Habash and
Sadat, 2006) by decreasing the size of the source vo-
cabulary, and improving the quality of word align-
ments. The morphological analyzers that underlie
most segmenters were developed for MSA, but the
different dialects of Arabic share many of the mor-
phological affixes of MSA, and it is therefore not
unreasonable to expect MSA segmentation to also
improve Dialect Arabic to English MT. To test this,
3We also computed TER (Snover et al, 2006) andMETEOR
scores, but omit them because they demonstrated similar trends.
we ran experiments using the MADA morpholog-
ical analyzer (Habash and Rambow, 2005). Table
3 shows the effect of applying segmentation to the
text, for both MSA and Dialectal Arabic. The BLEU
score improves uniformly, although the improve-
ments are most dramatic for smaller datasets, which
is consistent with previous work (Habash and Sadat,
2006). Morphological segmentation gives a smaller
gain on dialectal input, which could be due to two
factors: the segmentation accuracy likely decreases
since we are using an unmodified MSA segmenter,
and there is higher variability in the written form of
dialect compared to MSA. Given the significant, al-
beit smaller gain on dialectal input, we use MADA
segmentation in all our experiments.
4.2 Effect of Dialectal Training Data Size
We next examine how the size of the dialectal train-
ing data affects MT performance, and whether it is
useful to combine it with MSA training data. We
53
oh
 ti
me
 (s
pa
ce
 om
itt
ed
). 
Ap
pe
are
d w
ith
in
 a 
po
em
.
11
yA
zm
n

?
lik
e y
ou
 (c
or
ru
pti
on
 of
 M
SA
 m
vl
k)
.
10
m
tlk
"#
$
by
 m
uc
h (
co
rru
pti
on
 of
 M
SA
 bk
vy
r).
11
bk
ty
r
&'$
()
I m
iss
 yo
u (
sp
ok
en
 to
 a 
fe
ma
le)
 ?
Eg
yp
tia
n.
14
w
H
$t
yn
y
/0
'$1
2?
Th
e l
as
t n
am
e (
Al
-N
a'o
om
) o
f a
 fo
ru
m 
ad
mi
n.
16
A
ln
E
w
m
?:;
0<?
a l
oo
ot 
(c
or
ru
pti
on
 of
 M
SA
 kv
yr
A
).
17
kt
yy
yr
&''
'$?
rea
lly
/fo
r r
ea
l ?
Le
va
nti
ne
.
31
E
nj
d
DE
0F
En
gli
sh
 E
qu
iva
len
t
Co
un
t
TL
Ar
ab
ic
Table 5: The most frequent OOV?s (with counts ? 10) of the dialectal test sets against the MSA training data.
Source (EGY):  ? ? ??	
?   ? ! !
Transliteration: Ant btEml lh AElAn wlA Ayh?!!
MSA-Sys. Output: You are working for a declarationand not?
Dial-Sys. Output: You are making the advertisementfor him or what?
Reference: Are you promoting it or what?!!
Source (EGY):  01?. ??78 6 35 34? ?
 9:;? <=>
Transliteration: nfsY Atm}n Elyh bEd mA $Af
AlSwrh dy
MSA-Sys. Output: Myself feel to see this image.
Dial-Sys. Output: I wish to check on him afterhe saw this picture.
Reference: I wish to be sure that he is fineafter he saw this images
Source (LEV):  ?0??? E7770 ?F? G7H
Transliteration: lhyk Aljw ktyyyr kwwwl
MSA-Sys. Output: God you the atmosphere.
Dial-Sys. Output: this is why the weather is so cool
Reference: This is why the weather is so cool
Source (LEV):  ?L M
 G3 0?;
Transliteration: Twl bAlk Em nmzH
MSA-Sys. Output: Do you think about a joke long.
Dial-Sys. Output: Calm down we are kidding
Reference: calm down, we are kidding
Figure 2: Examples of improvement in MT output when
training on our Dialectal Arabic-English parallel corpus
instead of an MSA-English parallel corpus.
Source (EGY):   	
 	  ? 
Transliteration: qAltlp Tb tEAlY nEd ,
MSA-Sys. Output: Medicine almighty promise.
Dial-Sys. Output: She said, OK, come and then
Reference: She told him, OK, lets count them ,
Source (LEV):  "#$%& 
#'01 ?-%. ! -,%+? ?? ?2 
Transliteration: fbqrA w>HyAnA bqDyhA Em
>tslY mE rfqAty
MSA-Sys. Output: I read and sometimes with gowith my uncle.
Dial-Sys. Output: So I read, and sometimes I spendtrying to make my self comfortwith my friends
Reference: So i study and sometimes I spendthe time having fun with my friends
Source (LEV):  ?@ ?< ??' => +? &#:9? B:C12D E?
?? %$?+G 
Transliteration: Allh ysAmHkn hlq kl wAHd TAlb
qrb bykwn bdw Erws
MSA-Sys. Output: God now each student near the
Bedouin bride.
Dial-Sys. Output: God forgive you, each one is aclose student would want the bride
Reference: God forgive you. Is every oneasking to be close, want a bride!
Figure 3: Examples of ambiguous words that are trans-
lated incorrectly by the MSA-English system, but cor-
rectly by the Dialectal Arabic-English system.
54
!"
!#
!$
!%
"&
""
&' "&&' #&&' %&&' !(&&'
!
"#
$
%
!"#$%&'()*#"+"+,(-./0(/1(2/*345
)*+,-./0123
-./0123
Egyptian web test
!"
!#
!$
!%
"&
""
&' "&&' #&&' %&&' !(&&'
!
"#
$
!"#$%&'()*#"+"+,(-./0(/1(2/*345
)*+,-./0123
-./0123
Levantine web test
Figure 4: Learning curves showing the effects of increas-
ing the size of dialectal training data, when combined
with the 150M-word MSA parallel corpus, and when
used alone. Adding the MSA training data is only use-
ful when the dialectal data is scarce (200k words).
started with a baseline system trained on the 150M-
word MSA parallel corpus, and added various sized
portions of the dialect parallel corpus to it. Figure 4
shows the resulting learning curve, and compares it
to the learning curve for a system trained solely on
the dialectal parallel corpus. When only 200k words
of dialectal data are available, combining it with the
150M-word MSA corpus results in improved BLEU
scores, adding 0.8?1.5 BLEU points. When 400k
words or more of dialectal data are available, the
MSA training data ceases to provide any gain, and
in fact starts to hurt the performance.
The performance of a system trained on the 1.5M-
word dialectal data is dramatically superior to a sys-
tem that uses only the 150M-word MSA data: +6.32
BLEU points on the Egyptian test set, or 44% rela-
tive gain, and +7.00 BLEU points on the Levantine
test set, or 57% relative gain (fourth line vs. second
line of Table 4). In Section 4.4, we show that those
gains are not an artifact of the similarity between test
and training datasets, or of using the same translator
pool to translate both sets.
Inspecting the difference in the outputs of the Di-
alectal vs. MSA systems, we see that the improve-
ment in score is a reflection of a significant improve-
ment in the quality of translations. Figure 2 shows
a few examples of sentences whose translations im-
prove significantly using the Dialectal system. Fig-
ure 3 shows a particularly interesting category of ex-
amples. Many words are homographs, with different
meanings (and usually different pronunciations) in
MSA vs. one or more dialects. The bolded tokens
in the sentences in Figure 3 are examples of such
words. They are translated incorrectly by the MSA
system, while the dialect system translates them cor-
rectly.4 If we examine the most frequent OOVwords
against the MSA training data (Table 5), we find a
number of corrupted MSA words and names, but
that a majority of OOVs are dialect words.
4.3 Cross-Dialect Training
Since MSA training data appeared to have little ef-
fect when translating dialectal input, we next inves-
tigated the effect of training data from one dialect on
translating the input of another dialect. We trained a
system with the 360k-word Egyptian training subset
of our dialectal parallel corpus, and another system
with a similar amount of Levantine training data. We
used each system to translate the test set of the other
dialect. As expected, a system performs better when
it translates a test set in the same dialect that it was
trained on (Table 4).
That said, since the Egyptian training set is so
small, adding the (full) Levantine training data im-
proves performance (on the Egyptian test set) by
1.62 BLEU points, compared to using only Egyp-
tian training data. In fact, using the Levantine
training data by itself outperforms the MSA-trained
system on the Egyptian test set by more than 3
BLEU points. (For the Levantine test set, adding
the Egyptian training data has no affect, possibly
due to the small amount of Egyptian data.) This
may suggest that the mismatch between dialects is
less severe than the mismatch between MSA and
dialects. Alternatively, the differences may be due
to the changes in genre from the MSA parallel cor-
pus (which is mainly formal newswire) to the news-
groups and weblogs that mainly comprise the dialec-
tal corpus.
4The word nfsY of Figure 2 (first word of second example)
is also a homograph, as it means myself in MSA and I wish in
Dialectal Arabic.
55
Training Tuning BLEU OOV
MSA-150MW Levantine-Web 13.80 4.16%
MSA-150MW+Dialect-1500KW 16.71 2.43%
Dialect-1500KW 15.75 3.79%
MSA-150MW Egyptian-Facebook 15.80 4.16%
MSA-150MW+Dialect-1500KW 18.50 2.43%
Dialect-1500KW 17.90 3.79%
Dialect-1000KW (random selection) Egyptian-Facebook 17.09 4.64%
Dialect-1000KW (no Turker overlap) 17.10 4.60%
Table 6: Results on a truly independent test set, consisting of data harvested from Egyptian Facebook pages that are
entirely distinct from the our dialectal training set. The improvements over the MSA baseline are still considerable:
+2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set.
4.4 Validation on Independent Test Data
To eliminate the possibility that the gains are solely
due to similarity between the test/training sets in the
dialectal data, we ran experiments using the same
dialectal training data, but using truly independent
test/tuning data sets selected at random from a larger
set of monolingual data that we collected from pub-
lic Egyptian Facebook pages. This data consists of
a set of original user postings and the subsequent
comments on each, giving the data a more conversa-
tional style than our other test sets. The postings
deal with current Egyptian political affairs, sports
and other topics. The test set we selected consisted
of 25,011 words (3,188 comments and 427 postings
from 86 pages), and the tuning set contained 25,130
words (3,351 comments and 415 conversations from
58 pages). We obtained reference translations for
those using MTurk as well.
Table 6 shows that using the 1.5M-word dialect
parallel corpus for training yields a 2 point BLEU
improvement over using the 150M-word MSA cor-
pus. Adding the MSA training data does yield an
improvement, though of less than a single BLEU
point. It remains true that training on 1.5M words
of dialectal data is better than training on 100 times
more MSA parallel data. The system performance
is sensitive to the tuning set choice, and improves
when it matches the test set in genre and origin.
To eliminate another potential source of artificial
bias, we also performed an experiment where we
removed any training translation contributed by a
Turker who translated any sentence in the Egyptian
Facebook set, to eliminate translator bias. For this,
we were left with 1M words of dialect training data.
This gave the same BLEU score as when training
with a randomly selected subset of the same size
(bottom part of Table 6).
4.5 Mapping from Dialectal Arabic to MSA
Before Translating to English
Given the large amount of linguistic resources that
have been developed for MSA over the past years,
and the extensive research that was conducted on
machine translation from MSA to English and other
languages, an obvious research question is whether
Dialectal Arabic is best translated to English by first
pivoting through MSA, rather than directly. The
proximity of Dialectal Arabic to MSA makes the
mapping in principle easier than general machine
translation, and a number of researchers have ex-
plored this direction (Salloum and Habash, 2011).
In this scenario, the dialectal source would first be
automatically transformed to MSA, using either a
rule-based or statistical mapping module.
The Dialectal Arabic-English parallel corpus we
created presents a unique opportunity to compare
the MSA-pivoting approach against direct transla-
tion. First, we collected equivalent MSA data for
the Levantine Web test and tuning sets, by asking
Turkers to transform dialectal passages to valid and
fluent MSA. Turkers were shown example transfor-
mations, and we encouraged fewer changes where
applicable (e.g. morphological rather than lexical
mapping), but allowed any editing operation in gen-
eral (deletion, substitution, reordering). Sample sub-
missions were independently shown to native Ara-
bic speaking judges, who confirmed they were valid
MSA. A lowOOV rate also indicated the correctness
of the mappings. By manually transforming the test
56
Training BLEU OOV BLEU OOV ?BLEU ?OOV
Direct dialect trans Map to MSA then trans
MSA-150MW 12.29 5.53% 14.59 1.53% +2.30 -4.00%
MSA-150MW+Dialect-200KW 15.37 3.59% 15.53 1.22% +0.16 -2.37%
MSA-150MW+Dialect-400KW 16.62 3.06% 16.25 1.13% -0.37 -1.93%
MSA-150MW+Dialect-800KW 17.83 2.63% 16.69 1.04% -1.14 -1.59%
MSA-150MW+Dialect-1500KW 19.11 2.27% 17.20 0.98% -1.91 -1.29%
Table 7: A comparison of the effectiveness of performing Levantine-to-MSA mapping before translating into English,
versus translating directly from Levantine into English. The mapping from Levantine to MSA was done manually, so it
is an optimistic estimate of what might be done automatically. Although initially helpful to the MSA baseline system,
the usefulness of pivoting through MSA drops as more dialectal data is added, eventually hurting performance.
dialectal sentence into MSA, we establish an opti-
mistic estimate of what could be done automatically.
Table 7 compares direct translation versus piv-
oting to MSA before translating, using the base-
line MSA-English MT system.5 The performance
of the system improves by 2.3 BLEU points with
dialect-to-MSA pivoting, compared to attempting to
translate the untransformed dialectal input directly.
As we add more dialectal training data, the BLEU
score when translating the untransformed dialect
test set improves rapidly (as seen previously in the
MSA+Dialect learning curve in Figure 4), while the
improvement is less rapid when the text is first trans-
formed to MSA. Direct translation becomes a better
option than mapping to MSA once 400k words of di-
alectal data are added, despite the significantly lower
OOV rate with MSA-mapping. This indicates that
simple vocabulary coverage is not sufficient, and
data domain mismatch, quantified by more complex
matching patterns, is more important.
5 Conclusion
We have described a process for building a Dialec-
tal Arabic-English parallel corpus, by selecting pas-
sages with a relatively high percentage of non-MSA
words from a monolingual Arabic web text corpus,
then using crowdsourcing to classify them by di-
alect, segment them into individual sentences and
translate them to English. The process was success-
fully scaled to the point of reaching and sustaining a
rate of 200k translated words per week, at 1/10 the
cost of professional translation. Our parallel corpus,
consisting of 1.5M words, was produced at a total
5The systems in each column of the table are tuned consis-
tently, using their corresponding tuning sets.
cost of $40k, or roughly $0.03/word.
We used the parallel corpus we constructed to
analyze the behavior of a Dialectal Arabic-English
MT system as a function of the size of the dialec-
tal training corpus. We showed that relatively small
amounts of training data render larger MSA corpora
from different data genres largely ineffective for this
test data. In practice, a system trained on the com-
bined Dialectal-MSA data is likely to give the best
performance, since informal Arabic data is usually
a mixture of Dialectal Arabic and MSA. An area of
future research is using the output of a dialect clas-
sifier, or other features to bias the translation model
towards the Dialectal or the MSA parts of the data.
We also validated the models built from the di-
alectal corpus by using them to translate an inde-
pendent data set collected from Egyptian Facebook
public pages. We finally investigated using MSA
as a ?pivot language? for Dialectal Arabic-English
translation, by simulating automatic dialect-to-MSA
mapping using MTurk. We obtained limited gains
from mapping the input to MSA, even when the
mapping is of good quality, and only at lower train-
ing set sizes. This suggests that the mismatch be-
tween training and test data is an important aspect of
the problem, beyond simple vocabulary coverage.
The aim of this paper is to contribute to setting
the direction of future research on Dialectal Arabic
MT. The gains we observed from using MSA mor-
phological segmentation can be further increased
with dialect-specific segmenters. Input preprocess-
ing can also be used to decrease the noise of the
user-generated data. Topic adaptation is another im-
portant problem to tackle if the large MSA linguistic
resources already developed are to be leveraged for
Dialectal Arabic-English MT.
57
Acknowledgments
This work was supported in part by DARPA/IPTO
Contract No. HR0011-12-C-0014 under the BOLT
Program, and in part by the EuroMatrixPlus project
funded by the European Commission (7th Frame-
work Programme). The views expressed are those
of the authors and do not reflect the official policy
or position of the Department of Defense or the U.S.
Government. Distribution Statement A (Approved
for Public Release, Distribution Unlimited).
References
Hitham M. Abo Bakr, Khaled Shaalan, and Ibrahim
Ziedan. 2008. A hybrid approach for converting writ-
ten Egyptian colloquial dialect into diacritized Arabic.
In The 6th International Conference on Informatics
and Systems, INFOS2008, Cairo, Egypt.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with improved
correlation with human judgments. In In Proc. of ACL
2005 Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization, Ann Arbor,
Michigan.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic di-
alects. In Proceedings of the Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, Trento, Italy.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL ?09: Proceedings of the 2009 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Boulder, Colorado.
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master?s thesis, University of Mary-
land, December.
Mohamed Embarki and Moha Ennaji, editors. 2011.
Modern Trends in Arabic Dialectology. The Red Sea
Press.
Charles A. Ferguson. 1959. Diglossia. Word, 15:325?
340.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
the 43th Annual Meeting of the Association for Com-
putational Linguistics (ACL), Ann Arbor, Michigan.
Nizar Habash and Owen Rambow. 2006. MAGEAD: A
morphological analyzer and generator for the Arabic
dialects. In Proceedings of the 44th Annual Meeting of
the Association for Computational Linguistics (ACL),
Sydney, Australia.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
Proceedings of the 2006 Human Language Technol-
ogy Conference of the North American Chapter of the
Association for Computational Linguistics, New York,
New York.
Nizar Y. Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool.
Young-Suk Lee. 2004. Morphological analysis for
statistical machine translation. In HLT-NAACL ?04:
Proceedings of HLT-NAACL 2004, Boston, Mas-
sachusetts.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), Philadelphia, PA.
Jason Riesa and David Yarowsky. 2006. Minimally
supervised morphological segmentation with applica-
tions to machine translation. In Proceedings of the 7th
Conf. of the Association for Machine Translation in the
Americas (AMTA 2006), Cambridge, MA.
Wael Salloum and Nizar Habash. 2011. Dialectal to stan-
dard Arabic paraphrasing to improve Arabic-English
statistical machine translation. In Proceedings of the
2011 Conference of Empirical Methods in Natural
Language Processing, Edinburgh, Scotland, UK.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the 9th Conf. of
the Association for Machine Translation in the Ameri-
cas (AMTA 2010), Denver, Colorado.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
577?585, Columbus, Ohio.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and Ralph Weischedel. 2006. A study of
translation error rate with targeted human annotation.
In Proceedings of the 7th Conf. of the Association for
Machine Translation in the Americas (AMTA 2006),
pages 223?231, Cambridge, MA.
58
Omar F. Zaidan and Chris Callison-Burch. 2011a.
The Arabic online commentary dataset: an annotated
dataset of informal Arabic with high dialectal content.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 37?41, Portland, Oregon,
June.
Omar F. Zaidan and Chris Callison-Burch. 2011b.
Crowdsourcing translation: Professional quality from
non-professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220?
1229, Portland, Oregon, June.
59
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 322?327,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Unsupervised Morphology Rivals Supervised Morphology for Arabic MT
David Stallard Jacob Devlin
Michael Kayser
BBN Technologies
{stallard,jdevlin,rzbib}@bbn.com
Yoong Keok Lee Regina Barzilay
CSAIL
Massachusetts Institute of Technology
{yklee,regina}@csail.mit.edu
Abstract
If unsupervised morphological analyzers
could approach the effectiveness of super-
vised ones, they would be a very attractive
choice for improving MT performance on
low-resource inflected languages. In this
paper, we compare performance gains for
state-of-the-art supervised vs. unsupervised
morphological analyzers, using a state-of-the-
art Arabic-to-English MT system. We apply
maximum marginal decoding to the unsu-
pervised analyzer, and show that this yields
the best published segmentation accuracy
for Arabic, while also making segmentation
output more stable. Our approach gives
an 18% relative BLEU gain for Levantine
dialectal Arabic. Furthermore, it gives higher
gains for Modern Standard Arabic (MSA), as
measured on NIST MT-08, than does MADA
(Habash and Rambow, 2005), a leading
supervised MSA segmenter.
1 Introduction
If unsupervised morphological segmenters could ap-
proach the effectiveness of supervised ones, they
would be a very attractive choice for improving ma-
chine translation (MT) performance in low-resource
inflected languages. An example of particular cur-
rent interest is Arabic, whose various colloquial di-
alects are sufficiently different from Modern Stan-
dard Arabic (MSA) in lexicon, orthography, and
morphology, as to be low-resource languages them-
selves. An additional advantage of Arabic for study
is the availability of high-quality supervised seg-
menters for MSA, such as MADA (Habash and
Rambow, 2005), for performance comparison. The
MT gain for supervised MSA segmenters on dialect
establishes a lower bound, which the unsupervised
segmenter must exceed if it is to be useful for dialect.
And comparing the gain for supervised and unsuper-
vised segmenters on MSA tells us how useful the
unsupervised segmenter is, relative to the ideal case
in which a supervised segmenter is available.
In this paper, we show that an unsupervised seg-
menter can in fact rival or surpass supervised MSA
segmenters on MSA itself, while at the same time
providing superior performance on dialect. Specifi-
cally, we compare the state-of-the-art morphological
analyzer of Lee et al (2011) with two leading super-
vised analyzers for MSA, MADA and Sakhr1, each
serving as an alternative preprocessor for a state-of-
the-art statistical MT system (Shen et al, 2008). We
measure MSA performance on NIST MT-08 (NIST,
2010), and dialect performance on a Levantine di-
alect web corpus (Zbib et al, 2012b).
To improve performance, we apply maximum
marginal decoding (Johnson and Goldwater, 2009)
(MM) to combine multiple runs of the Lee seg-
menter, and show that this dramatically reduces the
variance and noise in the segmenter output, while
yielding an improved segmentation accuracy that
exceeds the best published scores for unsupervised
segmentation on Arabic Treebank (Naradowsky and
Toutanova, 2011). We also show that it yields MT-
08 BLEU scores that are higher than those obtained
with MADA, a leading supervised MSA segmenter.
For Levantine, the segmenter increases BLEU score
by 18% over the unsegmented baseline.
1http://www.sakhr.com/Default.aspx
322
2 Related Work
Machine translation systems that process highly in-
flected languages often incorporate morphological
analysis. Some of these approaches rely on mor-
phological analysis for pre- and post-processing,
while others modify the core of a translation system
to incorporate morphological information (Habash,
2008; Luong et al, 2010; Nakov and Ng, 2011). For
instance, factored translation Models (Koehn and
Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis
and Koehn, 2008) parametrize translation probabili-
ties as factors encoding morphological features.
The approach we have taken in this paper is
an instance of a segmented MT model, which di-
vides the input into morphemes and uses the de-
rived morphemes as a unit of translation (Sadat and
Habash, 2006; Badr et al, 2008; Clifton and Sarkar,
2011). This is a mainstream architecture that has
been shown to be effective when translating from a
morphologically rich language.
A number of recent approaches have explored
the use of unsupervised morphological analyzers
for MT (Virpioja et al, 2007; Creutz and Lagus,
2007; Clifton and Sarkar, 2011; Mermer and Ak?n,
2010; Mermer and Saraclar, 2011). Virpioja et al
(2007) apply the unsupervised morphological seg-
menter Morfessor (Creutz and Lagus, 2007), and
apply an existing MT system at the level of mor-
phemes. The system does not outperform the word
baseline partially due to the insufficient accuracy of
the automatic morphological analyzer.
The work of Mermer and Ak?n (2010) and Mer-
mer and Saraclar (2011) attempts to integrate mor-
phology and MT more closely than we do, by in-
corporating bilingual alignment probabilities into a
Gibbs-sampled version of Morfessor for Turkish-to-
English MT. However, the bilingual strategy shows
no gain over the monolingual version, and nei-
ther version is competitive for MT with a super-
vised Turkish morphological segmenter (Oflazer,
1993). By contrast, the unsupervised analyzer we
report on here yields MSA-to-English MT perfor-
mance that equals or exceed the performance ob-
tained with a leading supervised MSA segmenter,
MADA (Habash and Rambow, 2005).
3 Review of Lee Unsupervised Segmenter
The segmenter of Lee et al (2011) is a probabilis-
tic model operating at word-type level. It is di-
vided into four sub-model levels. Model 1 prefers
small affix lexicons, and assumes that morphemes
are drawn independently. Model 2 generates a la-
tent POS tag for each word type, conditioning the
word?s affixes on the tag, thereby encouraging com-
patible affixes to be generated together. Model 3
incorporates token-level contextual information, by
generating word tokens with a type-level Hidden
Markov Model (HMM). Finally, Model 4 models
morphosyntactic agreement with a transition proba-
bility distribution, encouraging adjacent tokens with
the same endings to also have the same final suffix.
4 Applying Maximum Marginal Decoding
to Reduce Variance and Noise
Maximum marginal decoding (Johnson and Gold-
water, 2009) (MM) is a technique which assigns
to each latent variable the value with the high-
est marginal probability, thereby maximizing the
expected number of correct assignments (Rabiner,
1989). Johnson and Goldwater (2009) extend MM
to Gibbs sampling by drawing a set of N indepen-
dent Gibbs samples, and selecting for each word the
most frequent segmentation found in them. They
found that MM improved segmentation accuracy
over the mean, consistent with its maximization cri-
terion. However, for our setting, we find that MM
provides several other crucial advantages as well.
First, MM dramatically reduces the output vari-
ance of Gibbs sampling (GS). Table 1 documents the
severity of this variance for the MT-08 lexicon, as
measured by the average exact-match accuracy and
segmentation F-measure between different runs. It
shows that on average, 13% of the word tokens, and
25% of the word types, are segmented differently
from run to run, which obviously makes the input to
MT highly unstable. By contrast the ?MM? column
of Table 1 shows that two different runs of MM, each
derived by combining separate sets of 25 GS runs,
agree on the segmentations of over 95% of the word
token ? a dramatic improvement in stability.
Second, MM reduces noise from the spurious af-
fixes that the unsupervised segmenter induces for
large lexicons. As Table 2 shows, the segmenter
323
Decoding Level Rec Prec F1 Acc
Gibbs Type 82.9 83.2 83.1 74.5
Token 87.5 89.1 88.3 86.7
MM Type 95.9 95.8 95.9 93.9
Token 97.3 94.0 95.6 95.1
Table 1: Comparison of agreement in outputs between
25 runs of Gibbs sampling vs. 2 runs of MM on the
full MT-08 data set. We give the average segmentation
recall, precision, F1-measure, and exact-match accuracy
between outputs, at word-type and word-token levels.
ATB MT-08
GS GS MM Morf
Unique prefixes 17 130 93 287
Unique suffixes 41 261 216 241
Top-95 prefixes 7 7 6 6
Top-95 suffixes 14 26 19 19
Table 2: Affix statistics of unsupervised segmenters. For
the ATB lexicon, we show statistics for the Lee seg-
menter with regular Gibbs sampling (GS). For the MT-
08 lexicon, we also show the output of the Lee segmenter
with maximum marginal decoding (MM). In addition, we
show statistics for Morfessor.
induces 130 prefixes and 261 suffixes for MT-08
(statistics for Morfessor are similar). This phe-
nomenon is fundamental to Bayesian nonparamet-
ric models, which expand indefinitely to fit the data
they are given (Wasserman, 2006). But MM helps
to alleviate it, reducing unique prefixes and suffixes
for MT-08 by 28% and 21%, respectively. It also re-
duces the number of unique prefixes/suffixes which
account for 95% of the prefix/suffix tokens (Top-95).
Finally, we find that in our setting, MM increases
accuracy not just over the mean, but over even the
best-scoring of the runs. As shown in Table 3, MM
increases segmentation F-measure from 86.2% to
88.2%. This exceeds the best published results on
ATB (Naradowsky and Toutanova, 2011).
These results suggest that MM may be worth con-
sidering for other GS applications, not only for the
accuracy improvements pointed out by Johnson and
Goldwater (2009), but also for its potential to pro-
vide more stable and less noisy results.
Model Mean Min Max MM
M1 80.1 79.0 81.5 81.8
M2 81.4 80.2 83.0 82.0
M3 81.4 80.1 82.8 83.2
M4 86.2 85.4 87.2 88.2
Table 3: Segmentation F-scores on ATB dataset for Lee
segmenter, shown for each Model level M1?M4 on the
Arabic segmentation dataset used by (Poon et al, 2009):
We give the mean, minimum, and maximum F-scores for
25 independent runs of Gibbs sampling, together with the
F-score from running MM over that same set of runs.
5 MT Evaluation
5.1 Experimental Design
MT System. Our experiments were performed
using a state-of-the-art, hierarchical string-to-
dependency-tree MT system, described in Shen et
al. (2008).
Morphological Analyzers. We compare the Lee
segmenter with the supervised MSA segmenter
MADA, using its ?D3? scheme. We also compare
with Sakhr, an intensively-engineered, supervised
MSA segmenter which applies multiple NLP tech-
nologies to the segmentation problem, and which
has given the best results for our MT system in pre-
vious work (Zbib et al, 2012a). We also compare
with Morfessor.
MT experiments. We apply the appropriate seg-
menter to split words into morphemes, which we
then treat as words for alignment and decoding. Fol-
lowing Lee et al (2011), we segment the test and
training sets jointly, estimating separate translation
models for each segmenter/dataset combination.
Training and Test Corpora. Our ?Full MSA? cor-
pus is the NIST MT-08 Constrained Data Track Ara-
bic training corpus (35M total, 336K unique words);
our ?Small MSA? corpus is a 1.3M-word subset.
Both are tested on the MT-08 evaluation set. For
dialect, we use a Levantine dialectal Arabic cor-
pus collected from the web with 1.5M total, 160K
unique words and 18K words held-out for test (Zbib
et al, 2012b)
PerformanceMetrics. We evaluate MTwith BLEU
score. To calculate statistical significance, we use
the boot-strap resampling method of Koehn (2004).
324
5.2 Results and Discussion
Table 4 summarizes the BLEU scores obtained from
using various segmenters, for three training/test sets:
Full MSA, Small MSA, and Levantine dialect.
As expected, Sakhr gives the best results for
MSA. Morfessor underperforms the other seg-
menters, perhaps because of its lower accuracy on
Arabic, as reported by Poon et al (2009). The
Lee segmenter gives the best results for Levantine,
inducing valid Levantine affixes (e.g ?hAl+? for
MSA?s ?h*A-Al+?, English ?this-the?) and yielding
an 18% relative gain over the unsegmented baseline.
What is more surprising is that the Lee segmenter
compares favorably with the supervised MSA seg-
menters on MSA itself. In particular, the Lee seg-
menter with MM yields higher BLEU scores than
does MADA, a leading supervised segmenter, while
preserving almost the same performance as GS on
dialect. On Small MSA, it recoups 93% of even
Sakhr?s gain.
By contrast, the Lee segmenter recoups only 79%
of Sakhr?s gain on Full MSA. This might result from
the phenomenon alluded to in Section 4, where addi-
tional data sometimes degrades performance for un-
supervised analyzers. However, the Lee segmenter?s
gain on Levantine (18%) is higher than its gain on
Small MSA (13%), even though Levantine has more
data (1.5M vs. 1.3M words). This might be be-
cause dialect, being less standardized, has more or-
thographic and morphological variability, which un-
supervised segmentation helps to resolve.
These experiments also show that while Model 4
gives the best F-score, Model 3 gives the best MT
scores. Comparison of Model 3 and 4 segmentations
shows that Model 4 induces a much larger num-
ber of inflectional suffixes, especially the feminine
singular suffix ?-p?, which accounts for a plurality
(16%) of the differences by token. While such suf-
fixes improve F-measure on the segmentation refer-
ences, they do not correspond to any English lexical
unit, and thus do not help alignment.
An interesting question is how much performance
might be gained from a supervised segmenter that
was as intensively engineered for dialect as Sakhr
was for MSA. Assuming a gain ratio of 0.93, similar
to Small MSA, the estimated BLEU score would be
20.38, for a relative gain of just 5% over the unsuper-
System Small Full Lev
MSA MSA Dial
Unsegmented 38.69 43.45 17.10
Sakhr 43.99 46.51 19.60
MADA 43.23 45.64 19.29
Morfessor 42.07 44.71 18.38
Lee GS
M1 43.12 44.80 19.70
M2 43.16 45.45 20.15+
M3 43.07 44.82 19.97
M4 42.93 45.06 19.55
Lee MM
M1 43.53 45.14 19.75
M2 43.45 45.29 19.75
M3 43.64+ 45.84 20.09
M4 43.56 45.16 19.93
Table 4: BLEU scores for all experiments. Full MSA is
the the full MT-08 corpus, Small MSA is a 1.3M-word
subset, Lev Dial our Levantine dataset. For each of these,
the highest Lee segmenter score is in bold, with ?+? if
statistically significant vs. MADA at the 95% confidence
level or higher. The highest overall score is in bold italic.
vised segmenter. Given the large engineering effort
that would be required to achieve this gain, the un-
supervised segmenter may be a more cost-effective
choice for dialectal Arabic.
6 Conclusion
We compare unsupervised vs. supervised morpho-
logical segmentation for Arabic-to-English machine
translation. We add maximum marginal decoding
to the unsupervised segmenter, and show that it
surpasses the state-of-the-art segmentation perfor-
mance, purges the segmenter of noise and variabil-
ity, yields BLEU scores on MSA competitive with
those from supervised segmenters, and gives an 18%
relative BLEU gain on Levantine dialectal Arabic.
Acknowledgements
This material is based upon work supported by
DARPA under Contract Nos. HR0011-12-C00014
and HR0011-12-C00015, and by ONR MURI Con-
tract No. W911NF-10-1-0533. Any opinions, find-
ings and conclusions or recommendations expressed
in this material are those of the author(s) and do not
necessarily reflect the views of the US government.
We thank Rabih Zbib for his help with interpreting
Levantine Arabic segmentation output.
325
References
Eleftherios Avramidis and Philipp Koehn. 2008. Enrich-
ing morphologically poor languages for statistical ma-
chine translation. In Proceedings of ACL-08: HLT.
Ibrahim Badr, Rabih Zbib, and James Glass. 2008. Seg-
mentation for English-to-Arabic statistical machine
translation. In Proceedings of ACL-08: HLT, Short
Papers.
Ann Clifton and Anoop Sarkar. 2011. Combin-
ing morpheme-based machine translation with post-
processing morpheme prediction. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Trans. Speech Lang. Process., 4:3:1?
3:34, February.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
ACL.
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in Arabic-English statisti-
cal machine translation. In Proceedings of ACL-08:
HLT, Short Papers.
Mark Johnson and Sharon Goldwater. 2009. Improv-
ing nonparametric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of EMNLP-CoNLL, pages
868?876.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004.
Yoong Keok Lee, Aria Haghighi, and Regina Barzi-
lay. 2011. Modeling syntactic context improves
morphological segmentation. In Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning.
Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan.
2010. A hybrid morpheme-word representation
for machine translation of morphologically rich lan-
guages. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing.
Cos?kun Mermer and Ahmet Afs??n Ak?n. 2010. Unsuper-
vised search for the optimal segmentation for statisti-
cal machine translation. In Proceedings of the ACL
2010 Student Research Workshop, pages 31?36, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Cos?kun Mermer and Murat Saraclar. 2011. Unsuper-
vised Turkish morphological segmentation for statis-
tical machine translation. In Workshop on Machine
Translation and Morphologically-rich languages, Jan-
uary.
Preslav Nakov and Hwee Tou Ng. 2011. Trans-
lating from morphologically complex languages: A
paraphrase-based approach. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies.
Jason Naradowsky and Kristina Toutanova. 2011. Unsu-
pervised bilingual morpheme segmentation and align-
ment with context-rich hidden semi-Markov models.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies.
NIST. 2010. NIST 2008 Open Machine Translation
(Open MT) Evaluation. http://www.ldc.
upenn.edu/Catalog/catalogEntry.jsp?
catalogId=LDC2010T21/.
Kemal Oflazer. 1993. Two-level description of Turkish
morphology. In Proceedings of the Sixth Conference
of the European Chapter of the Association for Com-
putational Linguistics.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. In Proceedings of the IEEE, pages 257?
286.
Fatiha Sadat and Nizar Habash. 2006. Combination
of Arabic preprocessing schemes for statistical ma-
chine translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT.
Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz, and
Markus Sadeniemi. 2007. Morphology-aware statisti-
cal machine translation based on morphs induced in an
unsupervised manner. In Proceedings of the Machine
Translation Summit XI.
LarryWasserman. 2006. All of Nonparametric Statistics.
Springer.
326
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly in-
flected languages. In Proceedings of EACL.
Rabih Zbib, Michael Kayser, Spyros Matsoukas, John
Makhoul, Hazem Nader, Hamdy Soliman, and Rami
Safadi. 2012a. Methods for integrating rule-based and
statistical systems for Arabic to English machine trans-
lation. Machine Translation, 26(1-2):67?83.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012b. Machine translation of Arabic dialects. In
NAACL 2012: Proceedings of the 2012 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Montreal, Quebec, Canada, June. Association for
Computational Linguistics.
327
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 126?134,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Semi-Supervised Batch-Mode Active Learning Strategy for Improved
Statistical Machine Translation
Sankaranarayanan Ananthakrishnan, Rohit Prasad, David Stallard and Prem Natarajan
BBN Technologies
10 Moulton Street
Cambridge, MA, U.S.A.
{sanantha,rprasad,stallard,prem}@bbn.com
Abstract
The availability of substantial, in-domain
parallel corpora is critical for the develop-
ment of high-performance statistical ma-
chine translation (SMT) systems. Such
corpora, however, are expensive to pro-
duce due to the labor intensive nature of
manual translation. We propose to al-
leviate this problem with a novel, semi-
supervised, batch-mode active learning
strategy that attempts to maximize in-
domain coverage by selecting sentences,
which represent a balance between domain
match, translation difficulty, and batch di-
versity. Simulation experiments on an
English-to-Pashto translation task show
that the proposed strategy not only outper-
forms the random selection baseline, but
also traditional active learning techniques
based on dissimilarity to existing training
data. Our approach achieves a relative im-
provement of 45.9% in BLEU over the
seed baseline, while the closest competitor
gained only 24.8% with the same number
of selected sentences.
1 Introduction
Rapid development of statistical machine transla-
tion (SMT) systems for resource-poor language
pairs is a problem of significant interest to the
research community in academia, industry, and
government. Tight turn-around schedules, bud-
get restrictions, and scarcity of human translators
preclude the production of large parallel corpora,
which form the backbone of SMT systems.
Given these constraints, the focus is on making
the best possible use of available resources. This
usually involves some form of prioritized data col-
lection. In other words, one would like to con-
struct the smallest possible parallel training corpus
that achieves a desired level of performance on un-
seen test data.
Within an active learning framework, this can
be cast as a data selection problem. The goal is
to choose, for manual translation, the most infor-
mative instances from a large pool of source lan-
guage sentences. The resulting sentence pairs, in
combination with any existing in-domain seed par-
allel corpus, are expected to provide a significantly
higher performance gain than a na??ve random se-
lection strategy. This process is repeated until a
certain level of performance is attained.
Previous work on active learning for SMT has
focused on unsupervised dissimilarity measures
for sentence selection. Eck et al (2005) describe a
selection strategy that attempts to maximize cov-
erage by choosing sentences with the highest pro-
portion of previously unseen n-grams. However,
if the pool is not completely in-domain, this strat-
egy may select irrelevant sentences, whose trans-
lations are unlikely to improve performance on an
in-domain test set. They also propose a technique,
based on TF-IDF, to de-emphasize sentences sim-
ilar to those that have already been selected. How-
ever, this strategy is bootstrapped by random ini-
tial choices that do not necessarily favor sentences
that are difficult to translate. Finally, they work
exclusively with the source language and do not
use any SMT-derived features to guide selection.
Haffari et al (2009) propose a number of fea-
tures, such as similarity to the seed corpus, trans-
lation probability, relative frequencies of n-grams
and ?phrases? in the seed vs. pool data, etc., for
active learning. While many of their experiments
use the above features independently to compare
their relative efficacy, one of their experiments
attempts to predict a rank, as a linear combina-
tion of these features, for each candidate sentence.
The top-ranked sentences are chosen for manual
translation. The latter strategy is particularly rel-
evant to this paper, because the goal of our active
126
learning strategy is not to compare features, but to
learn the trade-off between various characteristics
of the candidate sentences that potentially maxi-
mizes translation improvement.
The parameters of the linear ranking model pro-
posed by Haffari et al (2009) are trained using
two held-out development sets D1 and D2 - the
model attempts to learn the ordering of D1 that
incrementally maximizes translation performance
on D2. Besides the need for multiple parallel
corpora and the computationally intensive nature
of incrementally retraining an SMT system, their
approach suffers from another major deficiency.
It requires that the pool have the same distribu-
tional characteristics as the development sets used
to train the ranking model. Additionally, they se-
lect all sentences that constitute a batch in a single
operation following the ranking procedure. Since
similar or identical sentences in the pool will typ-
ically meet the selection criteria simultaneously,
this can have the undesired effect of choosing re-
dundant batches with low diversity. This results in
under-utilization of human translation resources.
In this paper, we propose a novel batch-mode
active learning strategy that ameliorates the above
issues. Our semi-supervised learning approach
combines a parallel ranking strategy with sev-
eral features, including domain representativeness,
translation confidence, and batch diversity. The
proposed approach includes a greedy, incremental
batch selection strategy, which encourages diver-
sity and reduces redundancy. The following sec-
tions detail our active learning approach, includ-
ing the experimental setup and simulation results
that clearly demonstrate its effectiveness.
2 Active Learning Paradigm
Active learning has been studied extensively in the
context of multi-class labeling problems, and the-
oretically optimal selection strategies have been
identified for simple classification tasks with met-
ric features (Freund et al, 1997). However, nat-
ural language applications such as SMT present a
significantly higher level of complexity. For in-
stance, SMT model parameters (translation rules,
language model n-grams, etc.) are not fixed in
number or type, and vary depending on the train-
ing instances. This gives rise to the concept of
domain. Even large quantities of out-of-domain
training data usually do not improve translation
performance. As we will see, this causes simple
active selection techniques based on dissimilarity
or translation difficulty to be ineffective, because
they tend to favor out-of-domain sentences.
Our proposed active learning strategy is moti-
vated by the idea that the chosen sentences should
maximize coverage, and by extension, translation
performance on an unseen test set. It should
pick sentences that represent the target domain,
while simultaneously enriching the training data
with hitherto unseen, difficult-to-translate con-
structs that are likely to improve performance on a
test set. We refer to the former as representative-
ness and to the latter as difficulty.
Since it is computationally prohibitive to re-
train an SMT system for individual translation
pairs, a batch of sentences is usually selected at
each iteration. We desire that each batch be suffi-
ciently diverse; this increases the number of con-
cepts (phrase pairs, translation rules, etc.) that can
be learned from manual translations of a selected
batch. Thus, our active learning strategy attempts,
at each iteration, to select a batch of mutually di-
verse source sentences, which, while introducing
new concepts, shares at least some commonality
with the target domain. This is done in a com-
pletely statistical, data-driven fashion.
In designing this active learning paradigm, we
make the following assumptions.
? A small seed parallel corpus S is available
for training an initial SMT system. This may
range from a few hundred to a few thousand
sentence pairs.
? Sentences must be selected from a large pool
P. This may be an arbitrary collection of in-
and out-of-domain source language sentences.
Some measure of redundancy is permitted and
expected, i.e. some sentences may be identical
or very similar to others.
? A development set D is available to tune the
SMT system and train the selection algorithm.
An unseen test set T is used to evaluate it.
? The seed, development, and test sets are de-
rived from the target domain distribution.
To re-iterate, we do not assume or require the
pool to have the same domain distribution as the
seed, development, and test sets. This reflects a
real-world scenario, where the pool may be drawn
from multiple sources (e.g. targeted collections,
newswire text, web, etc.). This is a key departure
from existing work on active learning for SMT.
127
S e e d  c o r p u s
S M T  s y s t e m
M o n o l i n g u a l  p o o l
P o o l  t r a i n i n g D e v e l .  s e t
D o m a i n  m a t c h
T r a n s .  d i f f i c u l t y
D i v e r s i t y
P r e f e r r e d  o r d e r
C 1
C 2
Input features MLP Classifiers Classifier targets
Figure 1: Flow-diagram of the active learner.
3 Active Learning Architecture
Figure 1 illustrates the proposed active learning
architecture in the form of a high-level flow-
diagram. We begin by randomly sampling a small
fraction of the large monolingual pool P to cre-
ate a pool training set PT, which is used to train
the learner. The remainder, which we call the pool
evaluation set PE, is set aside for active selection.
We also train an initial phrase-based SMT system
(Koehn et al, 2003) with the available seed cor-
pus. The pool training set PT, in conjunction with
the seed corpus S, initial SMT system, and held-
out development set D, is used to derive a number
of input features as well as target labels for train-
ing two parallel classifiers.
3.1 Preferred Ordering
The learner must be able to map input features
to an ordering of the pool sentences that attempts
to maximize coverage on an unseen test set. We
teach it to do this by providing it with an ordering
of PT that incrementally maximizes source cov-
erage on D. This preferred ordering algorithm in-
crementally maps sentences in PT to a ordered set
OT by picking, at each iteration, the sentence with
the highest coverage criterion with respect to D,
and inserting it at the current position within OT.
The coverage criterion is based on content-word
n-gram overlap with D, discounted by constructs
already observed in S and higher-ranked sentences
in OT, as illustrated in Algorithm 1. Our hypoth-
esis is that sentences, which maximally improve
coverage, likely lead to bigger gains in translation
performance as well.
The O(|PT|2) complexity of this algorithm is
one reason we restrict PT to a few thousand sen-
tences. Another reason not to order the entire pool
and simply select the top-ranked sentences, is that
batches thus constructed would overfit the devel-
opment set on which the ordering is based, and
not generalize well to an unseen test set.
3.2 Ranker Features
Each candidate sentence in the pool is represented
by a vector of features, which fall under one of
the three categories, viz. representativeness, dif-
ficulty, and diversity. We refer to the first two
as context-independent, because they can be com-
puted independently for each sentence. Diversity
is a context-dependent feature and must be evalu-
ated in the context of an ordering of sentences.
128
Algorithm 1 Preferred ordering
OT ? ()
Sg ? count(g) ?g ? ngr(S)
Dg ? count(g) ?g ? ngr(D)
for k = 1 to |PT| do
PU ? PT ?OT
y? ? argmax
y?PU
?
g?ngr(y)
yg ?Dg ? n
Sg + 1
OT (k)? y?
Sg ? Sg + y?g ?g ? ngr(y?)
end for
return OT
3.2.1 Domain Representativeness
Domain representativeness features gauge the de-
gree of similarity between a candidate pool sen-
tence and the seed training data. We quantify this
using an n-gram overlap measure between candi-
date sentence x and the seed corpus S defined by
Equation 1.
sim(x,S) =
?
g?ngr(x)
xg ?
min(Sng , Cn)
Cn
?
g?ngr(x)
xg
(1)
xg is the number of times n-gram g occurs in x,
Sg the number of times it occurs in the seed cor-
pus, n its length in words, and Cn the count of
n-grams of length n in S. Longer n-grams that
occur frequently in the seed receive high similar-
ity scores, and vice-versa. In evaluating this fea-
ture, we only consider n-grams up to length five
that contain least one content word.
Another simple domain similarity feature we
use is sentence length. Sentences in conversational
domains are typically short, while those in web
and newswire domains run longer.
3.2.2 Translation Difficulty
All else being equal, the selection strategy should
favor sentences that the existing SMT system finds
difficult to translate. To this end, we estimate a
confidence score for each SMT hypothesis, using
a discriminative classification framework reminis-
cent of Blatz et al (2004). Confidence estima-
tion is treated as a binary classification problem,
where each hypothesized word is labeled ?cor-
rect? or ?incorrect?. Word-level reference labels
for training the classifier are obtained from Trans-
lation Edit Rate (TER) analysis, which produces
the lowest-cost alignment between the hypothe-
ses and the gold-standard references (Snover et
al., 2006). A hypothesized word is ?correct? if
it aligns to itself in this alignment, and ?incorrect?
otherwise.
We derive features for confidence estimation
from the phrase derivations used by the decoder in
generating the hypotheses. For each target word,
we look up the corresponding source phrase that
produced it, and use this information to compute
a number of features from the translation phrase
table and target language model (LM). These in-
clude the in-context LM probability of the target
word, the forward and reverse phrase translation
probabilities, the maximum forward and reverse
word-level lexical translation probabilities, num-
ber of competing target phrases in which the tar-
get word occurs, etc. In all, we use 11 word-level
features (independent of the active learning fea-
tures) to train the classifier in conjunction with the
abovementioned binary reference labels.
A logistic regression model is used to directly
estimate the posterior probability of the binary
word label. Thus, our confidence score is es-
sentially the probability of the word being ?in-
correct?. Sentence-level confidence is computed
as the geometric average of word-level posteriors.
Confidence estimation models are trained on the
held-out development set.
We employ two additional measures of transla-
tion difficulty for active learning: (a) the number
of ?unknown? words in target hypotheses caused
by untranslatable source words, and (b) the aver-
age length of source phrases in the 1-best SMT
decoder derivations.
3.2.3 Batch Diversity
Batch diversity is evaluated in the context of an
explicit ordering of the candidate sentences. In
general, sentences that are substantially similar to
those above them in a ranked list have low diver-
sity, and vice-versa. We use content-word n-gram
overlap to measure similarity with previous sen-
tences, per Equation 2.
d(b | B) = 1.0?
?
g?ngr(b)
n?Bg
?
g?ngr(b)
n?max(Bg, 1.0)
(2)
B represents the set of sentences ranked higher
than the candidate b, for which we wish to evalu-
ate diversity. Bg is the number of times n-gram g
129
occurs in B. Longer, previously unseen n-grams
serve to boost diversity. The first sentence in a
given ordering is always assigned unit diversity.
The coverage criterion used by the preferred or-
dering algorithm in Section 3.1 ensures good cor-
respondence between the rank of a sentence and its
diversity, i.e. higher-ranked in-domain sentences
have higher diversity, and vice-versa.
3.3 Training the Learner
The active learner is trained on the pool training
set PT. The seed training corpus S serves as the
basis for extracting domain similarity features for
each sentence in this set. Translation difficulty fea-
tures are evaluated by decoding sentences in PT
with the seed SMT system. Finally, we compute
diversity for each sentence in PT based on its pre-
ferred order OT according to Equation 2. Learn-
ing is semi-supervised as it does not require trans-
lation references for either PT or D.
Traditional ranking algorithms such as PRank
(Crammer and Singer, 2001) work best when the
number of ranks is much smaller than the sample
size; more than one sample can be assigned the
same rank. In the active learning problem, how-
ever, each sample is associated with a unique rank.
Moreover, the dynamic range of ranks in OT is
significantly smaller than that in PE, to which the
ranking model is applied, resulting in a mismatch
between training and evaluation conditions.
We overcome these issues by re-casting the
ranking problem as a binary classification task.
The top 10% sentences in OT are assigned a ?se-
lect? label, while the remaining are assigned a
contrary ?do-not-select? label. The input features
are mapped to class posterior probabilities using
multi-layer perceptron (MLP) classifiers. The use
of posteriors allows us to assign a unique rank to
each candidate sentence. The best candidate sen-
tence is the one to which the classifier assigns the
highest posterior probability for the ?select? la-
bel. We use one hidden layer with eight sigmoid-
activated nodes in this implementation.
Note that we actually train two MLP classi-
fiers with different sets of input features as shown
in Figure 1. Classifier C1 is trained using only
the context-independent features, whereas C2 is
trained with the full set of features including batch
diversity. These classifiers are used to implement
an incremental, greedy selection algorithm with
parallel ranking, as explained below.
Algorithm 2 Incremental greedy selection
B? ()
for k = 1 to N do
Pci ? {x ? PE | d(x | B) = 1.0}
Pcd ? {x ? PE | d(x | B) < 1.0}
C? C1(fci(Pci)) ? C2(fcd(Pcd,B))
bk ? argmax
x?PE
C(x)
PE ? PE ? {bk}
end for
return B
4 Incremental Greedy Selection
Traditional rank-and-select batch construction ap-
proaches choose constituent sentences indepen-
dently, and therefore cannot ensure that the cho-
sen sentences are sufficiently diverse. Our strat-
egy implements a greedy selection algorithm that
constructs each batch iteratively; the decision bk
(the sentence to fill the kth position in a batch)
depends on all previous decisions b1, ? ? ? , bk?1.
This allows de-emphasizing sentences similar to
those that have already been placed in the batch,
while favoring samples containing previously un-
seen constructs.
4.1 Parallel Ranking
We begin with an empty batch B, to which sen-
tences from the pool evaluation set PE must be
added. We then partition the sentences in PE in
two mutually-exclusive groups Pcd and Pci. The
former contains candidates that share at least one
content-word n-gram with any existing sentences
in B, while the latter consists of sentences that
do not share any overlap with them. Note that
B is empty to start with; thus, Pcd is empty and
Pci = PE at the beginning of the first iteration
of selection. The diversity feature is computed for
each sentence in Pcd based on existing selections
in B, while the context-independent features are
evaluated for sentences in both partitions.
Next, we apply C1 to Pci and C2 to Pcd and in-
dependently obtain posterior probabilities for the
?select? label for both partitions. We take the
union of class posteriors from both partitions and
select the sentence with the highest probability of
the ?select? label to fill the next slot bk, corre-
sponding to iteration k, in the batch. The selected
sentence is subsequently removed from PE.
The above parallel ranking technique (Algo-
rithm 2) is applied iteratively until the batch
130
reaches a pre-determined size N . At itera-
tion k, the remaining sentences in PE are par-
titioned based on overlap with previous selec-
tions b1, ? ? ? , bk?1 and ranked based on the union
of posterior probabilities generated by the corre-
sponding classifiers. This ensures that sentences
substantially similar to those that have already
been selected receive a low diversity score, and are
suitably de-emphasized. Depending on the char-
acteristics of the pool, batches constructed by this
algorithm are likely more diverse than a simple
rank-and-select approach.
5 Experimental Setup and Results
We demonstrate the effectiveness of the proposed
sentence selection algorithm by performing a set
of simulation experiments in the context of an
English-to-Pashto (E2P) translation task. We sim-
ulate a low-resource condition by using a very
small number of training sentence pairs, sampled
from the collection, to bootstrap a phrase-based
SMT system. The remainder of this parallel cor-
pus is set aside as the pool.
At each iteration, the selection algorithm picks a
fixed-size batch of source sentences from the pool.
The seed training data are augmented with the
chosen source sentences and their translations. A
new set of translation models is then estimated and
used to decode the test set. We track SMT perfor-
mance across several iterations and compare the
proposed algorithm to a random selection baseline
as well as other common selection strategies.
5.1 Data Configuration
Our English-Pashto data originates from a two-
way collection of spoken dialogues, and thus con-
sists of two parallel sub-corpora: a directional E2P
corpus and a directional Pashto-to-English (P2E)
corpus. Each sub-corpus has its own independent
training, development, and test partitions. The di-
rectional E2P training, development, and test sets
consist of 33.9k, 2.4k, and 1.1k sentence pairs, re-
spectively. The directional P2E training set con-
sists of 76.5k sentence pairs.
We obtain a seed training corpus for the simula-
tion experiments by randomly sampling 1,000 sen-
tence pairs from the directional E2P training par-
tition. The remainder of this set, and the entire re-
versed directional P2E training partition are com-
bined to create the pool (109.4k sentence pairs). In
the past, we have observed that the reversed direc-
tional P2E data gives very little performance gain
in the E2P direction even though its vocabulary is
similar, and can be considered ?out-of-domain? as
far as the E2P translation task is concerned. Thus,
our pool consists of 30% in-domain and 70% out-
of-domain sentence pairs, making for a challeng-
ing active learning problem. A pool training set of
10k source sentences is sampled from this collec-
tion, leaving us with 99.4k candidate sentences.
5.2 Selection Strategies
We implement the following strategies for sen-
tence selection. In all cases, we use a fixed-size
batch of 200 sentences per iteration.
? Random selection, in which source sentences
are uniformly sampled from PE.
? Similarity selection, where we choose sen-
tences that exhibit the highest content-word n-
gram overlap with S.
? Dissimilarity selection, which selects sen-
tences having the lowest degree of content-
word n-gram overlap with S.
? Active learning with greedy incremental selec-
tion, using a learner to maximize coverage by
combining various input features.
We simulate a total of 30 iterations, with the
original 1,000 sample seed corpus growing to
7,000 sentence pairs.
5.3 Simulation Results
We track SMT performance at each iteration in
two ways. The first and most effective method is
to simply use an objective measure of translation
quality, such as BLEU (Papineni et al, 2001). Fig-
ure 2(a) illustrates the variation in BLEU scores
across iterations for each selection strategy. We
note that the proposed active learning strategy per-
forms significantly better at every iteration than
random, similarity, and dissimilarity-based selec-
tion. At the end of 30 iterations, the BLEU
score gained 2.46 points, a relative improvement
of 45.9%. By contrast, the nearest competitor was
the random selection baseline, whose performance
gained only 1.33 points in BLEU, a 24.8% im-
provement. Note that we tune the phrase-based
SMT feature weights using MERT (Och, 2003)
once in the beginning, and use the same weights
across all iterations. This allowed us to compare
selection methods without variations introduced
by fluctuation of the weights.
131
(a) Trajectory of BLEU
(b) Trajectory of untranslated word ratio
(c) Directionality match (d) Diversity/Uniqueness
Figure 2: Simulation results for data selection. Batch size at each iteration is 200 sentences.
132
The second method measures test set coverage
in terms of the proportion of untranslated words
in the SMT hypotheses, which arise due to the
absence of appropriate in-context phrase pairs in
the training data. Figure 2(b) shows the varia-
tion in this measure for the four selection tech-
niques. Again, the proposed active learning algo-
rithm outperforms its competitors across nearly all
iterations, with very large improvements in the ini-
tial stages. Overall, the proportion of untranslated
words dropped from 8.74% to 2.28% after 30 iter-
ations, while the closest competitor (dissimilarity
selection) dropped to 2.59%.
It is also instructive to compare the distribu-
tion of the 6,000 sentences selected by each strat-
egy at the end of the simulation to determine
whether they came from the ?in-domain? E2P
set or the ?out-of-domain? P2E collection. Fig-
ure 2(c) demonstrates that only 1.3% of sentences
were selected from the reversed P2E set by the
proposed active learning strategy. On the other
hand, 70.9% of the sentences selected by the
dissimilarity-based technique came from the P2E
collection, explaining its low BLEU scores on the
E2P test set. Surprisingly, similarity selection also
chose a large fraction of sentences from the P2E
collection; this was traced to a uniform distribu-
tion of very common sentences (e.g. ?thank you?,
?okay?, etc.) across the E2P and P2E sets.
Figure 2(d) compares the uniqueness and over-
all n-gram diversity of the 6,000 sentences chosen
by each strategy. The similarity selector received
the lowest score on this scale, explaining the lack
of improvement in coverage as measured by the
proportion of untranslated words in the SMT hy-
potheses. Again, the proposed approach exhibits
the highest degree of uniqueness, underscoring its
value in lowering batch redundancy.
It is interesting to note that dissimilarity selec-
tion is closest to the proposed active learning strat-
egy in terms of coverage, and yet exhibits the
worst BLEU scores. This confirms that, while
there is overlap in their vocabularies, the E2P and
P2E sets differ significantly in terms of longer-
span constructs that influence SMT performance.
These results clearly demonstrate the power
of the proposed strategy in choosing diverse, in-
domain sentences that not only provide superior
performance in terms of BLEU, but also improve
coverage, leading to fewer untranslated concepts
in the SMT hypotheses.
6 Conclusion and Future Directions
Rapid development of SMT systems for resource-
poor language pairs requires judicious use of hu-
man translation capital. We described a novel ac-
tive learning strategy that automatically learns to
pick, from a large monolingual pool, sentences
that maximize in-domain coverage. In conjunc-
tion with their translations, they are expected to
improve SMT performance at a significantly faster
rate than existing selection techniques.
We introduced two key ideas that distinguish
our approach from previous work. First, we uti-
lize a sample of the candidate pool, rather than an
additional in-domain development set, to learn the
mapping between the features and the sentences
that maximize coverage. This removes the restric-
tion that the pool be derived from the target do-
main distribution; it can be an arbitrary collection
of in- and out-of-domain sentences.
Second, we construct batches using an incre-
mental, greedy selection strategy with parallel
ranking, instead of a traditional batch rank-and-
select approach. This reduces redundancy, allow-
ing more concepts to be covered in a given batch,
and making better use of available resources.
We showed through simulation experiments that
the proposed strategy selects diverse batches of
high-impact, in-domain sentences that result in a
much more rapid improvement in translation per-
formance than random and dissimilarity-based se-
lection. This is reflected in objective indicators of
translation quality (BLEU), and in terms of cover-
age as measured by the proportion of untranslated
words in SMT hypotheses. We plan to evaluate
the scalability of our approach by running simu-
lations on a number of additional language pairs,
domains, and corpus sizes.
An issue with iterative active learning in gen-
eral is the cost of re-training the SMT system for
each batch. Small batches provide for smooth per-
formance trajectories and better error recovery at
an increased computational cost. We are currently
investigating incremental approaches that allow
SMT models to be updated online with minimal
performance loss compared to full re-training.
Finally, there is no inherent limitation in the
proposed framework that ties it to a phrase-based
SMT system. With suitable modifications to the
input feature set, it can be adapted to work with
various SMT architectures, including hierarchical
and syntax-based systems.
133
References
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. In COLING ?04:
Proceedings of the 20th international conference on
Computational Linguistics, page 315, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Koby Crammer and Yoram Singer. 2001. Pranking
with ranking. In Advances in Neural Information
Processing Systems 14, pages 641?647. MIT Press.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2005.
Low cost portability for statistical machine transla-
tion based in N-gram frequency and TF-IDF. In
Proceedings of IWSLT, Pittsburgh, PA, October.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naf-
tali Tishby. 1997. Selective sampling using the
query by committee algorithm. Machine Learning,
28(2-3):133?168.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. In NAACL ?09: Proceedings
of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 415?423, Morristown, NJ, USA. Association
for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Morristown, NJ, USA.
Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160?
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation. In ACL ?02: Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 311?318,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings AMTA, pages 223?231, August.
134
