Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 731?740, Dublin, Ireland, August 23-29 2014.
Low-Dimensional Manifold Distributional Semantic Models
Georgia Athanasopoulou
School of Electronic &
Computer Engineering
T.U.C. Chania, Greece
gathanasopoulou@isc.tuc.gr
Elias Iosif
Athena Research and
Innovation Center,
15125 Maroussi, Greece
iosif.elias@gmail.com
Alexandros Potamianos
School of Electrical &
Computer Engineering
N.T.U.A, Athens, Greece
apotam@gmail.com
Abstract
Motivated by evidence in psycholinguistics and cognition, we propose a hierarchical distributed
semantic model (DSM) that consists of low-dimensional manifolds built on semantic neighbor-
hoods. Each semantic neighborhood is sparsely encoded and mapped into a low-dimensional
space. Global operations are decomposed into local operations in multiple sub-spaces; results
from these local operations are fused to come up with semantic relatedness estimates. Manifold
DSM are constructed starting from a pairwise word-level semantic similarity matrix. The pro-
posed model is evaluated on semantic similarity estimation task significantly improving on the
state-of-the-art.
1 Introduction
The estimation of semantic similarity between words, sentences and documents is a fundamental problem
for many research disciplines including computational linguistics (Malandrakis et al., 2011), semantic
web (Corby et al., 2006), cognitive science and artificial intelligence (Resnik, 2011; Budanitsky and
Hirst, 2001). In this paper, we study the geometrical structure of the lexical space in order to extract se-
mantic relations among words. In (Karlgren et al., 2008), the high-dimensional lexical space is assumed
to consist of manifolds of very low dimensionality that are embedded in this high dimensional space.
The manifold hypothesis is compatible with evidence from psycholinguistics and cognitive science. In
(Tenenbaum et al., 2011), the question ?How does the mind work?? is answered as follows: cognitive
organization is based on domains with similar items connected to each other and lexical information
is represented hierarchically, i.e., a domain that consists of similar lexical entries may be represented
by a more abstract concept. An example of such a domain is {blue, red, yellow, pink, ...} that corre-
sponds by the concept of color. An inspiring analysis about the geometry of thought, as well as cognitive
evidence for the low-dimensional manifold assumption can be found in (Gardenfors, 2000), e.g., the
domain of color is argued to be cognitively represented as an one-dimensional manifold. Following the
low-dimensional manifold hypothesis we propose to extend distributional semantic models (DSMs) into
a hierarchical model of domains (or concepts) that contain semantically similar words. Global operations
on the lexical space are decomposed into local operations on the low-dimensional domain sub-manifolds.
Our goal is to exploit this hierarchical low-rank model to estimate relations between words, such as se-
mantic similarity.
There has been much research interest on devising data-driven approaches for estimating semantic
similarity between words. DSMs (Baroni and Lenci, 2010) are based on the distributional hypothesis
of meaning (Harris, 1954) assuming that semantic similarity between words is a function of the overlap
of their linguistic contexts. DSMs are typically constructed from co-occurrence statistics of word tuples
that are extracted on existing corpora or on corpora specifically harvested from the web. In (Iosif and
Potamianos, 2013), general-purpose, language-agnostic algorithms were proposed for estimating seman-
tic similarity using no linguistic resources other than a corpus created via web queries. The key idea of
this work was the construction of semantic networks and semantic neighborhoods that capture smooth
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are
added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
731
co-occurrence and context similarity statistics. The majority of DSMs adopt high-dimensional represen-
tations, while the underlying space geometry is not explicitly taken into consideration during the design
of algorithms aimed for performing several semantic tasks.
We propose the construction of a low-dimensional manifold DSM consting of four steps: 1) identify
the domains that correspond to the low-dimensional manifolds, 2) run the dimensionality reduction al-
gorithm for each domain, 3) construct a DSM for each domain, and 4) combine the manifold DSMs to
come up with global measures of lexical relations. A variety of algorithms can be found in the literature
for projecting a set of tokens into low dimensional sub-spaces, given a token similarity or dissimilarity
matrix. Depending on the nature of the dataset, these projection algorithms may or may not preserve
the local geometries of the original dataset. Most dimensionality reduction algorithms make the implicit
assumption that the underlying space is metric, e.g., Multidimensional Scaling (MDS) (Torgerson, 1952)
or Principal Component Analysis (PCA) (Jolliffe, 2005) or the ones using non-negative matrix factor-
ization (Tsuge et al., 2001) and typically fail to capture the geometry of manifolds embedded in high
dimensional spaces. A variety of dimensionality reduction algorithms have been developed that respect
the local geometry. Some examples are the Isomap algorithm (Tenenbaum et al., 2000) that performs
the projection based on a weighted neighborhood graph, Local Linear Embedings (LLE) (Roweis and
Saul, 2000) that assigns neighbors to each data point, Random Projections (Baraniuk and Wakin, 2009),
(Li et al., 2006) that preserves the manifold geometry by executing random linear projections and oth-
ers (Hessian Eigenmaps (HLLE) (Donoho and Grimes, 2003); Maximum Variance Unfolding (MVU)
(Wang, 2011)). The manifold hypothesis has also been studied by the representation learning commu-
nity where the local geometry is disentangled from the global geometry mainly by using neighborhood
graphs (Weston et al., 2012) or coding schemes (Yu et al., 2009). For a review see (Bengio et al., 2013).
A fundamental problem with all aforementioned methods when applied to lexical semantic spaces is
that they do not account for ambiguous tokens, i.e., word senses. The main assumption of dimensionality
reduction and manifold unfolding algorithms is that each token (word) belongs to a single sub-manifold.
This in fact is not true for polysemous words, for example the word ?green? could belong both to the
domain colors, as well as to the domain plants. In essence, lexical semantic spaces are manifolds that
have singularities: the manifold collapses in the neighborhood of polysemous words that can be thought
of semantic black holes that can instantaneously transfer you from one domain to another. Our proposed
solution to this problem is to allow words to live in multiple sub-manifolds.
The algorithms proposed in this paper build on recent research work on distributional semantic models
and manifold representational learning. Manifold DSMs can be trained directly from a corpus and do
not require a-priori knowledge or any human-annotated resources (just like DSMs). We show that the
proposed low-dimensional, sparse and hierarchical manifold representation significantly improves on the
state-of-the-art for the problem of semantic similarity estimation.
2 Metrics of Semantic Similarity
Semantic similarity metrics can be broadly divided into the following types: (i) metrics that rely on
knowledge resources (e.g., WordNet), and (ii) corpus-based that do not require any external knowledge
source. Corpus-based metrics are formalized as Distributional Semantic Models (DSMs) (Baroni and
Lenci, 2010) based on the distributional hypothesis of meaning (Harris, 1954). DSMs can be distin-
guished into (i) unstructured: use bag-of-words model (Iosif and Potamianos, 2010) and (ii) structured:
exploitation of syntactic relationships between words (Grefenstette, 1994; Baroni and Lenci, 2010). The
vector space model (VSM) constitutes the main implementation for both unstructured and structured
DSMs. Cosine similarity constitutes a measurement of word similarity that is widely used on top of
the VSM. The similarity between two words is estimated as the cosine of their respective vectors whose
elements correspond to corpus-based co-occurrence statistics. In essence, the similarity between words
is computed via second-order co-occurrences.
Direct (i.e., first-order) co-occurrences can be also used for the estimation of semantic similarity (Bol-
legala et al., 2007; Gracia et al., 2006). The exploitation of first-order co-occurrence statistics constitutes
the simplest form of unstructured DSMs. A key parameter for such models is the definition of the context
in which the words of interest co-occur: from entire documents (Bollegala et al., 2007) to paragraphs
732
(V?eronis, 2004) and sentences (Iosif and Potamianos, 2013). The effect of co-occurrence context for
the task of similarity computation between nouns is discussed in (Iosif and Potamianos, 2013). The
underlying assumption is that two words that co-occur in a specified context are semantically related.
3 Collapsed Manifold Hypothesis, Low-Dimensionality and Sparsity
The intuition behind this work is that although the lexical semantic space proper is high-dimensional, it
is organized in such a way that interesting semantic relations can be exported from manifolds of much
lower dimensionality embedded in this high dimensional space (Karlgren et al., 2008). We assume that
(at least some of) these sub-manifolds contain semantically similar words (or word senses). For example,
a potential sub-manifold in the lexical space could be the one that contains the colors (e.g., red, blue,
green). But in fact many words, such as book, green, fruit, are expected to belong simultaneously in
semantically different manifolds because they have multiple meanings.
A simple way to bootstrap the manifold recreation process is to build a domain around each word,
i.e., the semantic neighborhood of each word defines a domain. For example, in Figure 1 we show
the semantic neighborhood of fruit. The connections between words indicate high semantic similarity,
i.e., this is a pruned semantic similarity graph of all words in the semantic neighborhood of the word
?fruit?. It is clear from this example that in a typical neighborhood there exist word pairs that should be
native
genus
b
shurb
b
plant
flowering
b
tree
b
species
b
b
garden
b
soil
b
animal
b
water
b
seed
flower
b
b
fruit
b
vegetable
b
apple
juice
daiquiri
b
orange
b
drink
b
zest
b
lemon
sugar
salt
b
flour
b
cream
b
butter
b
b
b
milk
b
corn
b
honey
b
tomato
b
b b
b
Figure 1: Visualization of the semantic neighborhood of the word ?fruit?.
?connected? to each other because they have close semantic relation, like {flower, plant} and others that
should not be ?connected? because they are semantically apart, like {garden, salt}. A sparse encoding of
the semantic similarity relations in a neighborhood is needed in order to achieve (via multi-dimensional
scaling) a parsimonious representation with good geometric properties
1
.
The graph connectivity or sparseness matrix identifies the word pairs that should be encoded in a
neighborhood is defined as
?
S ? {0, 1}
n?n
, where value
?
S(i, j) = 1 indicates that the i
th
, j
th
word
pair is encoded, while
?
S(i, j) = 0 indicates that the pair is ignored (n is the number of words and
i, j = 1, .., n in the neighborhood). We define the degree of sparseness of matrix
?
S as the percentage of
0?s in the matrix.
4 Dimensionality Reduction
In this section, the Sparse Projection (SP) algorithm is described (see also Algorithm 1). SP is the core
algorithm for constructing manifold DSMs presented in Section 5. SP is a dimensionality reduction
algorithm that projects a set of n words into a vector space of d dimensions. The input to the algorithm
is a dissimilarity or semantic distance matrix P ? R
n?n
, where element P(i, j) encodes the degree
of dissimilarity between words w
i
and w
j
. The output of SP are the d-dimensional coordinate vectors
of the n projected words that form a matrix X ? R
n?d
. Each row x
i
? R
1?d
of matrix X ? R
n?d
corresponds to the coordinates of the i
th
word w
i
. Once X is estimated the dissimilarity matrix is
recomputed and updated to new values, as discussed next. Each paragraph that follows corresponds to a
module in Algorithm 1.
1
Compare for example with Isomap (Tenenbaum et al., 2000) were a short- and long-distance metric is used. When using
sparse encoding the long-distance metric is set to a very large fixed number (similarity set to 0). In both cases, the underlying
manifold is unfolded and low-dimensional representation with (close to) metric properties are discovered.
733
Semantic Distance Re-estimation: Given the matrix X ? R
n?d
containing the vector projections of
words in the d-dimensional space, the dissimilarity matrix is re-estimated using the Euclidean distance
2
.
Let
?
P ? R
n?n
be the matrix with the new dissimilarity scores then the new dissimilarity score between
words w
i
and w
j
is simply:
?
P(i, j) = ?x
i
? x
j
?
2
, where x
i
, x
j
are the vectors corresponding to words
w
i
, w
j
respectively, i, j = 1, .., n and ?.?
2
is the Euclidean norm.
Connectivity Graph and Sparsity: As discussed in Section 3, given a set of words only a small
subset of lexical relations should be explicitly encoded between pairs of these words. Therefore,
the SP algorithm should only take into account strongly related word pairs and ignore the rest. This
is the main difference between our approach compared to the generic MDS algorithm proposed in
(Torgerson, 1952). In order to apply the sparseness constraint, we first construct the connectivity
matrix
?
S ? {0, 1}
n?n
. Word pairs (w
i
, w
j
) with small similarity values (or equivalently large semantic
distance) are penalized: zero values are assigned to their corresponding position (i, j) in
?
S matrix. In
essence, the matrix
?
S is obtained by hard {0, 1} thresholding on the dissimilarity matrix P: all values
that are under a threshold are set to 0, while all values equal or greater to the threshold are set to 1.
Let n be the number of words under investigation, then the number of word pairs is p =
n?(n?1)
2
. The
degree of sparseness is defined as the number of unordered word pairs (w
i
, w
j
), i 6= j where
?
S(i, j) = 0
normalized over the total number of pairs p
3
.
Error Criterion: The algorithm employs a local and a global error criterion defined as follows:
1. The local error corresponds to the projection error for each individual word w
i
e ? R
n?1
, where
i = 1...n and is defined as the sum of the dissimilarity matrix errors before and after projection
computed only for the words that are ?connected? to w
i
, as follows:
e
i
=
n
?
j=1
?
S(i, j) ?
(
?
P(i, j)?P(i, j)
)
2
(1)
2. The global error of the projection is simply the sum over local errors for all words: e
tot
=
?
n
i=1
e
i
Algorithm 1 Sparse projection (SP)
Require: v // Vocabulary: vector of n words
Require: P // n?n dissimilarity matrix
1:
?
S? ComputeConnectivityMatrix(S)
2: for each word w
i
? v do
3: X
i
? RandomInitialization(X
i
)
4: end for
5: k = 0 // Iteration counter: initialization
6: e
k
tot
= inf // Global error: initialization
7: repeat
8: k = k + 1
9: for each word w
i
? v do
10: for each direction z do
11: X?MoveWordToDirection(w
i
, z)
12: e
z
i
? ComputeLocalError(
?
S,P,X,i)
13: end for
14: z?
i
? FindDirectionOfMinLocalError(e
z
i
)
15: X = MoveWordToDirection(w
i
, z?
i
)
16: end for
17: e
k
tot
? UpdateGlobalError(
?
S,P,X)
18: until e
k?1
tot
< e
k
tot
// Stopping condition
19:
?
P? SemanticDistanceReestimation(X)
20:
?
P? SparseDistanceNormalizedRanges(
?
P,
?
S)
21: return X // n?d matrix with coordinates;
22: return
?
S // n?n matrix with connections;
23: return
?
P // n?n updated dissimilarity matrix;
24: return
?
P // n?n sparse-normalized distances;
Random Walk SP: In function MoveWordToDirection(?) of Algorithm 1, the pseudo-variable direction
z refers to a standard set of perturbations of each word in the d-dimensional space. For example, if the
dimension of the projection is d = 2 then the coordinates of each word are modeled as (k
1
, k
2
), where
k
1
, k
2
? R. A potential set of perturbations are the following: (k
1
, k
2
+ s), (k
1
, k
2
? s), (k
1
+ s, k
2
)
and (k
1
? s, k
2
), where s is the perturbation step parameter of the algorithm. For coordinates systems
normalized in [0, 1]
d
we chose a value of s equal to 0.1. Good convergence properties to global maxima
have been experimentally shown for this algorithm for multiple runs on (noisy) randomly generated data.
2
Other metrics, e.g., cosine similarity, have also been tested out but results are not shown here due to lack of space. Euclidean
distance performed somewhat better that cosine similarity for the semantic similarity estimation task.
3
The SP algorithm with 0% degree of sparseness is equivalent to the MDS algorithm.
734
Sparse Semantic Distance Normalized Ranges: This function normalizes all the distance scores of
?
P
in a range of values, [0 r
1
], where r
1
? R
+
is an arbitrary positive constant and also it imposes the
sparsity constraint as follows: if
?
S(i, j) = 0 then
?
P(i, j) = r
1
. If
?
S(i, j) = 1 then
?
P(i, j) = r
2
?
?
P(i,j)
r
3
,
where r
3
is the maximum distance over all ?connected? pairs, i.e. r
3
, max{
?
P 
?
S}, with  denoting
the Hadamard product, and r
2
? R
+
can be either equal to r
1
or slightly smaller than r
1
. The assignment
of r
2
< r
1
aims to differentiate the ?unconnected? pairs from the ?connected? but dissimilar ones
4
.
5 Low-Dimensional Manifold DSMs
The end-to-end low-dimensional manifold DSM (LDMS) system is depicted in Figure 2. Note that
v
1
, v
2
, ..., v
|V|
? V are the domains or sub-manifolds of the LDMS, for each domain v
i
a separate DSM
is built. V is the set of domains (concept vocabulary) and |V| denotes to the cardinality of V. The input
Figure 2: LDMS system.
to LDMS is a (global) similarity matrix S ? R
n?n
, where n is the total number of tokens (words) in
the LDMS model. Note that S can be estimated using any of the baseline semantic similarity metrics
5
presented in Section 2. Since the SP algorithm uses as input a dissimilarity or semantic distance matrix,
the pairwise word similarity matrix S ? R
n?n
is transformed to a semantic distance (or dissimilarity)
matrix P ? R
n?n
as: P(i, j) = c
1
? e
?c
2
?S(i,j)
where c
1
, c
2
? R are constants and the i, j indexes run
from 1 to n. In this work, we used c
1
=c
2
=20. The transformation defined by (5) was selected in order
to non-linearly scale and increase the relative distance of dissimilar words compared to similar ones
6
.
The steps followed by the LDMS system are the following:
1. Domain Selection: The domains v
1
, v
2
, ..., v
|V|
are created as follows: for each word w
i
in our
model we create a corresponding domain v
i
that consists of all the words that are semantically
similar to w
i
, i.e., the ith domain is the semantic neighborhood of word w
i
. Thus in our model
the vocabulary size is equal to the domain set cardinality, i.e., n = |V|. Domain v
i
is created by
selecting the top N most semantically similar words to w
i
based on the (global) similarity matrix
S ? R
n?n
. We have experimented with various domain sizes N ranging between 20 and 200
neighbors; note that each word in the LDMS may belong to multiple domains.
2. Sparse Projections on Domains: Following the selection of domain v
i
? V the (local) dissimilarity
matrix for each domain P
v
i
? R
N?N
is defined as a submatrix of P ? R
n?n
. Then, the SP
algorithm is applied to each domain separately, resulting in i = 1, .., |V| re-estimated bounded
semantic distance matrices
?
P
v
i
.
3. Fusion: To reach a decision on the strength of the semantic relation between words w
i
and w
j
the
semantic distance matrices from each domain
?
P
v
i
must be combined. Only domains were both
words w
i
and w
j
appear are relevant in this fusion process. This procedure is described next.
4
We experimented with various values for r
1
and r
2
achieving comparable performance; we selected r
2
? 0.9r
1
that had
slightly better performance. The value of r
1
can be chosen arbitrary, the results reported here were obtained for r
1
= 20 and
r
2
= 18.
5
Here, the Google-based Semantic Relatedness was employed using a corpus of web-harvested document snippets.
6
Similar nonlinear scaling function from similarity to distance can be found in the literature, e.g., (Borg, 2005)
735
5.1 Fusion
Motivation: Given a set of words L = {w
1
, w
2
, ...w
n
} we assume that their corresponding set of word
senses
7
is M = {s
11
, s
12
, .., s
1n
1
, .., .., s
n1
, s
n2
, .., s
nn
n
}. The set of senses is defined as M = ?
n
i=1
M
i
,
where M
i
= {s
i1
, s
i2
, ..., s
in
i
} is the set of senses for word w
i
. Let S(.) be a metric of semantic similar-
ity, e.g., the metric defined in Section 2, which is symmetric, i.e., S(x, y) ? S(y, x). The notations S
w
(.)
and S
s
(.) are used in order to distinguish the similarity at word and sense level, respectively. According to
the maximum sense similarity assumption (Resnik, 1995), the similarity between w
i
and w
j
, S
w
(w
i
, w
j
),
is defined as the pairwise maximum similarity between their corresponding senses S
s
(s
ik
, s
jl
):
S
w
(w
i
, w
j
) ? S
s
(s
ik
, s
jl
), where (k, l) = argmax
(p?M
i
,r?M
j
)
S
s
(s
ip
, s
jr
).
Note that the maximum pairwise similarity metric (or equivalently the minimum pairwise distance
metric) is also known as the ?common sense? set similarity (or distance) employed by human cognition
when evaluating the similarity (or distance) between two sets.
Fusion of local dissimilarity scores: Next we describe a domain fusion model that follows the min-
imum pairwise distance (dissimilarity) principle motivated by human cognition. The steps for the re-
computation of the (global) dissimilarity between words w
i
and w
j
are:
1. Search for all the domains where w
i
and w
j
co-exist.
2. Let U ? V be the subset of domains from the previous step. The distances between words w
i
and
w
j
are retrieved from domain dissimilarity matrices
?
P
u
for all u ? U . The distances are stored into
vector d ? R
|U |?1
.
3. Motivated by the maximum sense similarity assumption (see above) the dissimilarity between w
i
and w
j
is defined as
8
:
?
P(i, j) = min
k=1..|U |
{d
k
} (2)
4. If words w
i
and w
j
do not co-exist in any domain then r
1
is assigned as their dissimilarity score,
where r
1
is the upper bound of
?
P
u
matrices as defined in the previous section.
For example, let one pair of words (w
1
, w
2
) co-exists in |U | = 3 different domains with corresponding
local distances d = [9 20 11] then the global distance of (w
1
, w
2
) is 9.
6 Evaluation
In this section, we evaluate the performance of the proposed approach with respect to the task of simi-
larity judgment between nouns. Results are reported with respect to several domain/neighborhood sizes,
sparse percentages and domain dimensions.
The performance of similarity metrics were evaluated against human ratings from three standard
datasets of noun pairs, namely WS353 (Finkelstein et al., 2001), RG (Rubenstein and Goodenough,
1965) MC (Miller and Charles, 1991). The first and the second datasets consist of the subset of 272 and
57 pairs, respectively, that are also included in SemCor3
9
corpus, while the third dataset consists of 28
noun pairs. The Pearson?s correlation coefficient was selected as evaluation metric to compare estimated
similarities against the ground truth.
The similarity matrix computed using the Google-based Semantic Relatedness (Gracia et al., 2006)
was used as baseline, as well as to bootstrap the LDMS global similarity matrix S, for a list of 8752 nouns
extracted from the SemCor3 corpus
10
. The performance of the proposed LDMS approach is presented
in Table 1. In addition, the performance of other unsupervised similarity estimation algorithms are
reported for comparison purposes: 1) SEMNET is an alternative implementation of unstructured DSMs
based on the idea of semantic neighborhoods and networks (Iosif and Potamianos, 2013) 2) WikiRelate!
includes various taxonomy-based metrics that are typically applied to the WordNet hierarchy; the basic
7
This is a simplification. In reality, some of the word senses will be the same, so strictly speaking this is not a set definition.
8
Other fusion methods have also been evaluated, e.g., (weighted) average. Results are omitted here due to lack of space.
Minimum pairwise distance fusion outperformed other fusion schemes.
9
http://www.cse.unt.edu/
?
rada/downloads.html
10
The baseline similarity matrix and the 8752 nouns are public available in:
http://www.telecom.tuc.gr/
?
iosife/downloads.html
736
idea behind WikiRelate! is to adapt these metrics to a hierarchy extracted from the links between the
pages of the English Wikipedia (Strube and Ponzetto, 2006) . 3) TypeDM is a structured DSM (Baroni
and Lenci, 2010), 4) AAHKPS1 constitutes an unstructured paradigm of DSM development using four
billion web documents that were acquired via crawling (Agirre et al., 2009), 5) Moreover, two well-
established dimensionality reduction algorithms (Isomap and LLE) that support the manifold hypothesis,
were applied to the task of semantic similarity computation
11
. LDMS, Isomap and LLE were given as
input the matrix P ? R
n?n
, where n = 8752 is the number of words in our models. Isomap and LLE
used dimensionality reduction down to d = 5 and neighborhood size equal to N = 120. SEMNET was
run for neighborhood size equal to N = 100. While LDMS run for dimensionality down to d = 5,
domain/neighborhood size equal to N = 140 and degree of sparseness 90%. The proposed LDMS
system surpassed the performance of the baseline system for all three datasets, as well as the performance
of the other corpus-based approaches for the WS353 and MC datasets. The dimensionality reduction
algorithms (Isomap - LLE) are shown to perform poorly for this particular task.
Datasets Algorithm
Baseline SEMNET WikiRelate! TypeDM AAHKPS1 Isomap LLE LDMS
WS353 0.61 0.64 0.48 - - 0.14 0.04 0.69
RG 0.81 0.87 0.53 0.82 - 0.04 0 0.86
MC 0.85 0.91 0.45 - 0.89 -0.04 -0.04 0.94
Table 1: Performance of various algorithms for the task of similarity judgment.
The performance (Pearson correlation) of the LDMS approach is shown in Figures 3a, 3b and 4a as
a function of neighborhood size and degree of sparseness. Results are presented for all three datasets:
WS353, MC, and RG. The baseline performance is also plotted (dotted line). For all three datasets,
we see a clear relationship between neighborhood size, degree of sparseness and performance. Sparse
representations achieve peak performance for larger neighborhood sizes. High degree of sparseness
between 80 and 90% achieves the best results for domain/neighborhood sizes between 100 and 140. The
figures show that there is potential for even better performance by fine-tuning the LDMS parameters.
The performance of LDMS is shown in Figure 4b as a function of the projection dimension d. The de-
gree of sparseness is fixed at 80% and the domain/neighborhood size is equal to 100 for all experiments.
It is observed that the performance for all three datasets remains relatively constant when at least d = 3
is used. In fact results are slightly better for d = 3 than for higher dimensions but the differences in
performance are not significant. The results suggest that even a 3D sub-space is adequate for accurately
representing the semantics of each underlying domain.
20 40 60 80 100 120 140 160 180 2000.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.705
Neighborhood size
Corr
elati
on
 
 Baseline95% Sparse90% Sparse80% Sparse40% Sparse0% Sparse
20 40 60 80 100 120 140 160 180 2000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 0.947
Neighborhood size
Corr
elati
on
 
 
Baseline95% Sparse90% Sparse80% Sparse40% Sparse0% Sparse
Figure 3: Performance as a function of domain size N and sparseness percentage for the (a) WS353
dataset and (b) MC dataset.
11
LDMS is not directly comparable with Isomap-LLE algorithms because it represents only the domains in low-dimensional
spaces and not the whole dataset.
737
20 40 60 80 100 120 140 160 180 2000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.865
Neighborhood size
Corr
elati
on
 
 
Baseline95% Sparse90% Sparse80% Sparse40% Sparse0% Sparse
2 3 4 5 6 7 80.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Dimensions
Corr
elati
on
 
 MCRGWS353
Figure 4: Performance for the (a) RG dataset as a function of domain size N and sparseness percentage
and (b) WS353, MC, RG datasets as a function of projection dimension d.
7 Conclusions
In this work, we proposed a novel, hierarchical DSM that was applied to semantic relation estimation
task obtaining very good results. The proposed representation consists of low-dimensional manifolds
that are derived from sparse projections of semantic neighborhoods. The core idea of low dimensional
subspaces was motivated by cognitive models of conceptual spaces. The validity of this motivation was
experimentally verified via the estimation of semantic similarity between nouns. The proposed approach
was found to be (at least) competitive with other state-of-the-art DSM approaches that adopt flat feature
representations and do not explicitly include the sparsity and dimensionality as a key design parameter.
The poor performance of Isomap and LLE can be attributed to the nature of the specific application,
i.e., word semantics. A key characteristic of this application is the ambiguity of word senses. These
algorithms assume only one sense for each word (i.e., a word is represented as a single point in a high-
dimensional space). Although the disambiguation task is not explicitly addressed, LDMS approach
handles the ambiguity of words by isolating each word?s senses in different domains.
Our initial intuition regarding the semantic fragmentation of lexical neighborhoods due to singularities
introduced by word senses was supported by the high performance when large (i.e., 80% - 90%) degree of
sparseness was imposed. The hypothesis of low-dimensional representation was validated by the finding
that as little as three dimensions are adequate for representing domain/neighborhood semantics. It was
also observed that the parameters of the LDMS model, i.e., number of dimensions, neighborhoodsize
and degree of sparseness, are interrelated: very sparse projections achieve best results with very low
dimensionality when large neighborhood sizes are used.
This is only a first step toward using ensembles of low-dimensional DSMs for semantic relation esti-
mation. As future work we plan to further investigate the creation of domains based on more complex
geometric properties of the underlying space (Kreyszig, 2007). A more formal investigation of the re-
lation between sparseness, dimensionality and performance is also needed. Finally, creating multi-level
hierarchical representations that are consistent with cognitive organization is an important challenge that
can further improve manifold DSM performance.
Acknowledgments
This work has been partially funded by two projects supported by the EU Seventh Framework Pro-
gramme (FP7): 1) PortDial, grant number 296170 and 2) SpeDial, grant number 611396.
738
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca, and A. Soroa. 2009. A study on similarity and relatedness
using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies, pages
19?27. Association for Computational Linguistics.
R. G Baraniuk and M. B Wakin. 2009. Random projections of smooth manifolds. Foundations of computational
mathematics, 9(1):51?77.
M. Baroni and A. Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Com-
putational Linguistics, 36(4):673?721.
Y. Bengio, A. Courville, and P. Vincent. 2013. Representation learning: A review and new perspectives.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007. Measuring semantic similarity between words using web search
engines. In Proc. of International Conference on World Wide Web, pages 757?766, Banff, Alberta, Canada.
Ingwer Borg. 2005. Modern multidimensional scaling: Theory and applications. Springer.
A. Budanitsky and G. Hirst. 2001. Semantic distance in wordnet: An experimental, application-oriented evalua-
tion of five measures. In Workshop on WordNet and Other Lexical Resources.
O. Corby, R. Dieng-Kuntz, F. Gandon, and C. Faron-Zucker. 2006. Searching the semantic web: Approximate
query processing based on ontologies. Intelligent Systems, IEEE, 21(1):20?27.
D. L Donoho and C. Grimes. 2003. Hessian eigenmaps: Locally linear embedding techniques for high-
dimensional data. Proceedings of the National Academy of Sciences, 100(10):5591?5596.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2001. Placing search in
context: The concept revisited. In Proceedings of the 10th international conference on World Wide Web, pages
406?414. ACM.
P. Gardenfors. 2000. Conceptual spaces: The geometry of thought. Cambridge, Massachusetts: USA. ISBN,
262071991.
J. Gracia, R. Trillo, M. Espinoza, and E. Mena. 2006. Querying the web: A multiontology disambiguation method.
In Proc. of International Conference on Web Engineering, pages 241?248, Palo Alto, California, USA.
G. Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Norwell,
MA, USA.
Z. Harris. 1954. Distributional structure. Word, 10(23):146?162.
E. Iosif and A. Potamianos. 2010. Unsupervised semantic similarity computation between terms using web
documents. Knowledge and Data Engineering, IEEE Transactions on, 22(11):1637?1647.
E. Iosif and A. Potamianos. 2013. Similarity computation using semantic networks created from web-harvested
data. Natural Language Engineering (DOI: 10.1017/S1351324913000144).
I. Jolliffe. 2005. Principal component analysis. Wiley Online Library.
J. Karlgren, A. Holst, and M. Sahlgren. 2008. Filaments of meaning in word space. In Advances in Information
Retrieval, pages 531?538. Springer.
E. Kreyszig. 2007. Introductory functional analysis with applications. Wiley. com.
P. Li, T. J Hastie, and K. W Church. 2006. Very sparse random projections. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge discovery and data mining, pages 287?296. ACM.
N. Malandrakis, A. Potamianos, E. Iosif, and S. S Narayanan. 2011. Kernel models for affective lexicon creation.
In INTERSPEECH, pages 2977?2980.
G. A Miller and W. G Charles. 1991. Contextual correlates of semantic similarity. Language and cognitive
processes, 6(1):1?28.
P. Resnik. 1995. Using information content to evaluate semantic similarity in a taxanomy. In Proc. of International
Joint Conference for Artificial Intelligence, pages 448?453.
739
P. Resnik. 2011. Semantic similarity in a taxonomy: An information-based measure and its application to prob-
lems of ambiguity in natural language. arXiv preprint arXiv:1105.5444.
S. T Roweis and L. K Saul. 2000. Nonlinear dimensionality reduction by locally linear embedding. Science,
290(5500):2323?2326.
H. Rubenstein and J. B Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM,
8(10):627?633.
Michael Strube and Simone Paolo Ponzetto. 2006. Wikirelate! computing semantic relatedness using wikipedia.
In AAAI, pages 1419?1424.
J. B Tenenbaum, V. De Silva, and J. C Langford. 2000. A global geometric framework for nonlinear dimensional-
ity reduction. Science, 290(5500):2319?2323.
J. B Tenenbaum, C. Kemp, T. L Griffiths, and N. D Goodman. 2011. How to grow a mind: Statistics, structure,
and abstraction. science, 331(6022):1279?1285.
Warren S Torgerson. 1952. Multidimensional scaling: I. theory and method. Psychometrika, 17(4):401?419.
S. Tsuge, M. Shishibori, S. Kuroiwa, and K. Kita. 2001. Dimensionality reduction using non-negative matrix
factorization for information retrieval. In Systems, Man, and Cybernetics, 2001 IEEE International Conference
on, volume 2, pages 960?965 vol.2.
J. V?eronis. 2004. Hyperlex: Lexical cartography for information retrieval. Computer Speech and Language,
18(3):223?252.
Jianzhong Wang. 2011. Maximum variance unfolding. In Geometric Structure of High-Dimensional Data and
Dimensionality Reduction, pages 181?202. Springer.
J. Weston, F. Ratle, H. Mobahi, and R. Collobert. 2012. Deep learning via semi-supervised embedding. In Neural
Networks: Tricks of the Trade, pages 639?655. Springer.
K. Yu, T. Zhang, and Y. Gong. 2009. Nonlinear learning using local coordinate coding. In Advances in Neural
Information Processing Systems, pages 2223?2231.
740
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565?570,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
DeepPurple: Estimating Sentence Semantic Similarity using
N-gram Regression Models and Web Snippets
Nikos Malandrakis, Elias Iosif, Alexandros Potamianos
Department of ECE, Technical University of Crete, 73100 Chania, Greece
[nmalandrakis,iosife,potam]@telecom.tuc.gr
Abstract
We estimate the semantic similarity between
two sentences using regression models with
features: 1) n-gram hit rates (lexical matches)
between sentences, 2) lexical semantic sim-
ilarity between non-matching words, and 3)
sentence length. Lexical semantic similarity is
computed via co-occurrence counts on a cor-
pus harvested from the web using a modified
mutual information metric. State-of-the-art re-
sults are obtained for semantic similarity com-
putation at the word level, however, the fusion
of this information at the sentence level pro-
vides only moderate improvement on Task 6
of SemEval?12. Despite the simple features
used, regression models provide good perfor-
mance, especially for shorter sentences, reach-
ing correlation of 0.62 on the SemEval test set.
1 Introduction
Recently, there has been significant research activ-
ity on the area of semantic similarity estimation
motivated both by abundance of relevant web data
and linguistic resources for this task. Algorithms
for computing semantic textual similarity (STS) are
relevant for a variety of applications, including in-
formation extraction (Szpektor and Dagan, 2008),
question answering (Harabagiu and Hickl, 2006)
and machine translation (Mirkin et al, 2009). Word-
or term-level STS (a special case of sentence level
STS) has also been successfully applied to the prob-
lem of grammar induction (Meng and Siu, 2002)
and affective text categorization (Malandrakis et al,
2011). In this work, we built on previous research
on word-level semantic similarity estimation to de-
sign and implement a system for sentence-level STS
for Task6 of the SemEval?12 campaign.
Semantic similarity between words can be re-
garded as the graded semantic equivalence at the
lexeme level and is tightly related with the tasks of
word sense discovery and disambiguation (Agirre
and Edmonds, 2007). Metrics of word semantic sim-
ilarity can be divided into: (i) knowledge-based met-
rics (Miller, 1990; Budanitsky and Hirst, 2006) and
(ii) corpus-based metrics (Baroni and Lenci, 2010;
Iosif and Potamianos, 2010).
When more complex structures, such as phrases
and sentences, are considered, it is much harder
to estimate semantic equivalence due to the non-
compositional nature of sentence-level semantics
and the exponential explosion of possible interpre-
tations. STS is closely related to the problems of
paraphrasing, which is bidirectional and based on
semantic equivalence (Madnani and Dorr, 2010) and
textual entailment, which is directional and based
on relations between semantics (Dagan et al, 2006).
Related methods incorporate measurements of sim-
ilarity at various levels: lexical (Malakasiotis and
Androutsopoulos, 2007), syntactic (Malakasiotis,
2009; Zanzotto et al, 2009), and semantic (Rinaldi
et al, 2003; Bos and Markert, 2005). Measures
from machine translation evaluation are often used
to evaluate lexical level approaches (Finch et al,
2005; Perez and Alfonseca, 2005), including BLEU
(Papineni et al, 2002), a metric based on word n-
gram hit rates.
Motivated by BLEU, we use n-gram hit rates and
word-level semantic similarity scores as features in
565
a linear regression model to estimate sentence level
semantic similarity. We also propose sigmoid scal-
ing of similarity scores and sentence-length depen-
dent modeling. The models are evaluated on the Se-
mEval?12 sentence similarity task.
2 Semantic similarity between words
In this section, two different metrics of word simi-
larity are presented. The first is a language-agnostic,
corpus-based metric requiring no knowledge re-
sources, while the second metric relies on WordNet.
Corpus-based metric: Given a corpus, the se-
mantic similarity between two words, wi and wj ,
is estimated as their pointwise mutual information
(Church and Hanks, 1990): I(i, j) = log p?(i,j)p?(i)p?(j) ,
where p?(i) and p?(j) are the occurrence probabili-
ties of wi and wj , respectively, while the probability
of their co-occurrence is denoted by p?(i, j). These
probabilities are computed according to maximum
likelihood estimation. The assumption of this met-
ric is that co-occurrence implies semantic similarity.
During the past decade the web has been used for
estimating the required probabilities (Turney, 2001;
Bollegala et al, 2007), by querying web search en-
gines and retrieving the number of hits required
to estimate the frequency of individual words and
their co-occurrence. However, these approaches
have failed to obtain state-of-the-art results (Bolle-
gala et al, 2007), unless ?expensive? conjunctive
AND queries are used for harvesting a corpus and
then using this corpus to estimate similarity scores
(Iosif and Potamianos, 2010).
Recently, a scalable approach1 for harvesting a
corpus has been proposed where web snippets are
downloaded using individual queries for each word
(Iosif and Potamianos, 2012b). Semantic similar-
ity can then be estimated using the I(i, j) metric
and within-snippet word co-occurrence frequencies.
Under the maximum sense similarity assumption
(Resnik, 1995), it is relatively easy to show that a
(more) lexically-balanced corpus2 (as the one cre-
1The scalability of this approach has been demonstrated in
(Iosif and Potamianos, 2012b) for a 10K vocabulary, here we
extend it to the full 60K WordNet vocabulary.
2According to this assumption the semantic similarity of two
words can be estimated as the minimum pairwise similarity of
their senses. The gist of the argument is that although words
often co-occur with their closest senses, word occurrences cor-
ated above) can significantly reduce the semantic
similarity estimation error of the mutual information
metric I(i, j). This is also experimentally verified in
(Iosif and Potamianos, 2012c).
In addition, one can modify the mutual informa-
tion metric to further reduce estimation error (for
the theoretical foundation behind this see (Iosif and
Potamianos, 2012a)). Specifically, one may intro-
duce exponential weights ? in order to reduce the
contribution of p(i) and p(j) in the similarity met-
ric. The modified metric Ia(i, j), is defined as:
Ia(i, j)=
1
2
[
log
p?(i, j)
p??(i)p?(j) + log
p?(i, j)
p?(i)p??(j)
]
. (1)
The weight ? was estimated on the corpus of (Iosif
and Potamianos, 2012b) in order to maximize word
sense coverage in the semantic neighborhood of
each word. The Ia(i, j) metric using the estimated
value of ? = 0.8 was shown to significantly out-
perform I(i, j) and to achieve state-of-the-art results
on standard semantic similarity datasets (Rubenstein
and Goodenough, 1965; Miller and Charles, 1998;
Finkelstein et al, 2002). For more details see (Iosif
and Potamianos, 2012a).
WordNet-based metrics: For comparison pur-
poses, we evaluated various similarity metrics on
the task of word similarity computation on three
standard datasets (same as above). The best re-
sults were obtained by the Vector metric (Patward-
han and Pedersen, 2006), which exploits the lexical
information that is included in the WordNet glosses.
This metric was incorporated to our proposed ap-
proach. All metrics were computed using the Word-
Net::Similarity module (Pedersen, 2005).
3 N-gram Regression Models
Inspired by BLEU (Papineni et al, 2002), we pro-
pose a simple regression model that combines evi-
dence from two sources: number of n-gram matches
and degree of similarity between non-matching
words between two sentences. In order to incorpo-
rate a word semantic similarity metric into BLEU,
we apply the following two-pass process: first lexi-
cal hits are identified and counted, and then the se-
mantic similarity between n-grams not matched dur-
respond to all senses, i.e., the denominator of I(i, j) is overes-
timated causing large underestimation error for similarities be-
tween polysemous words.
566
ing the first pass is estimated. All word similar-
ity metrics used are peak-to-peak normalized in the
[0,1] range, so they serve as a ?degree-of-match?.
The semantic similarity scores from word pairs are
summed together (just like n-gram hits) to obtain
a BLEU-like semantic similarity score. The main
problem here is one of alignment, since we need
to compare each non-matched n-gram from the hy-
pothesis with an n-gram from the reference. We
use a simple approach: we iterate on the hypoth-
esis n-grams, left-to-right, and compare each with
the most similar non-matched n-gram in the refer-
ence. This modification to BLEU is only applied
to 1-grams, since semantic similarity scores for bi-
grams (or higher) were not available.
Thus, our list of features are the hit rates obtained
by BLEU (for 1-, 2-, 3-, 4-grams) and the total se-
mantic similarity (SS) score for 1-grams3. These
features are then combined using a multiple linear
regression model:
D?L = a0 +
4
?
n=1
an Bn + a5 M1, (2)
where D?L is the estimated similarity, Bn is the
BLEU hit rate for n-grams, M1 is the total semantic
similarity score (SS) for non-matching 1-grams and
an are the trainable parameters of the model.
Motivated by evidence of cognitive scaling of
semantic similarity scores (Iosif and Potamianos,
2010), we propose the use of a sigmoid function to
scale DL sentence similarities. We have also ob-
served in the SemEval data that the way humans rate
sentence similarity is very much dependent on sen-
tence length4. To capture the effect of length and
cognitive scaling we propose next two modifications
to the linear regression model. The sigmoid fusion
scheme is described by the following equation:
D?S = a6D?L + a7D?L
[
1 + exp
(
a8 ? l
a9
)]
?1
, (3)
where we assume that sentence length l (average
3Note that the features are computed twice on each sentence
in a forward and backward fashion (where the word order is
reversed), and then averaged between the two runs.
4We speculate that shorter sentences are mostly compared at
the lexical level using the short-term memory language buffers,
while longer sentences tend to be compared at a higher cogni-
tive level, where the non-compositional nature of sentence se-
mantics dominate.
length for each sentence pair, in words) acts as a
scaling factor for the linearly estimated similarity.
The hierarchical fusion scheme is actually a col-
lection of (overlapping) linear regression models,
each matching a range of sentence lengths. For ex-
ample, the first model DL1 is trained with sentences
with length up to l1, i.e., l ? l1, the second model
DL2 up to length l2 etc. During testing, sentences
with length l ? [1, l1] are decoded with DL1, sen-
tences with length l ? (l1, l2] with model DL2 etc.
Each of these partial models is a linear fusion model
as shown in (2). In this work, we use four models
with l1 = 10, l2 = 20, l3 = 30, l4 =?.
4 Experimental Procedure and Results
Initially all sentences are pre-processed by the
CoreNLP (Finkel et al, 2005; Toutanova et al,
2003) suite of tools, a process that includes named
entity recognition, normalization, part of speech tag-
ging, lemmatization and stemming. The exact type
of pre-processing used depends on the metric used.
For the plain lexical BLEU, we use lemmatization,
stemming (of lemmas) and remove all non-content
words, keeping only nouns, adjectives, verbs and ad-
verbs. For computing semantic similarity scores, we
don?t use stemming and keep only noun words, since
we only have similarities between non-noun words.
For the computation of semantic similarity we have
created a dictionary containing all the single-word
nouns included in WordNet (approx. 60K) and then
downloaded snippets of the 500 top-ranked docu-
ments for each word by formulating single-word
queries and submitting them to the Yahoo! search
engine.
Next, results are reported in terms of correlation
between the automatically computed scores and the
ground truth, for each of the corpora in Task 6 of
SemEval?12 (paraphrase, video, europarl, WordNet,
news). Overall correlation (?Ovrl?) computed on the
join of the dataset, as well as, average (?Mean?) cor-
relation across all task is also reported. Training is
performed on a subset of the first three corpora and
testing on all five corpora.
Baseline BLEU: The first set of results in Ta-
ble 1, shows the correlation performance of the
plain BLEU hit rates (per training data set and over-
all/average). The best performing hit rate is the one
567
calculated using unigrams.
Table 1: Correlation performance of BLEU hit rates.
par vid euro Mean Ovrl
BLEU 1-grams 0.62 0.67 0.49 0.59 0.57
BLEU 2-grams 0.40 0.39 0.37 0.39 0.34
BLEU 3-grams 0.32 0.36 0.30 0.33 0.33
BLEU 4-grams 0.26 0.25 0.24 0.25 0.28
Semantic Similarity BLEU (Purple): The perfor-
mance of the modified version of BLEU that in-
corporates various word-level similarity metrics is
shown in Table 2. Here the BLEU hits (exact
matches) are summed together with the normalized
similarity scores (approximate matches) to obtain a
single B1+M1 (Purple) score5. As we can see, there
are definite benefits to using the modified version,
particularly with regards to mean correlation. Over-
all the best performers, when taking into account
both mean and overall correlation, are the WordNet-
based and Ia metrics, with the Ia metric winning by
a slight margin, earning a place in the final models.
Table 2: Correlation performance of 1-gram BLEU
scores with semantic similarity metrics (nouns-only).
par vid euro Mean Ovrl
BLEU 0.54 0.60 0.39 0.51 0.58
SS-BLEU WordNet 0.56 0.64 0.41 0.54 0.58
SS-BLEU I(i, j) 0.56 0.63 0.39 0.53 0.59
SS-BLEU Ia(i, j) 0.57 0.64 0.40 0.54 0.58
Regression models (DeepPurple): Next, the per-
formance of the various regression models (fusion
schemes) is investigated. Each regression model is
evaluated by performing 10-fold cross-validation on
the SemEval training set. Correlation performance
is shown in Table 3 both with and without seman-
tic similarity. The baseline in this case is the Pur-
ple metric (corresponding to no fusion). Clearly
the use of regression models significantly improves
performance compared to the 1-gram BLEU and
Purple baselines for almost all datasets, and espe-
cially for the combined dataset (overall). Among
the fusion schemes, the hierarchical models perform
the best. Following fusion, the performance gain
from incorporating semantic similarity (SS) is much
smaller. Finally, in Table 4, correlation performance
of our submissions on the official SemEval test set is
5It should be stressed that the plain BLEU unigram scores
shown in this table are not comparable to those in Table 1, since
here scores are calculated over only the nouns of each sentence.
Table 3: Correlation performance of regression model
with (SS) and without semantic similarities on the train-
ing set (using 10-fold cross-validation).
par vid euro Mean Ovrl
None (SS-BLEU Ia) 0.57 0.64 0.40 0.54 0.58
Linear (D?L, a5=0) 0.62 0.72 0.47 0.60 0.66
Sigmoid (D?S, a5=0) 0.64 0.73 0.42 0.60 0.73
Hierarchical 0.64 0.74 0.48 0.62 0.73
SS-Linear (D?L) 0.64 0.73 0.47 0.61 0.66
SS-Sigmoid (D?S) 0.65 0.74 0.42 0.60 0.74
SS-Hierarchical 0.65 0.74 0.48 0.62 0.73
shown. The overall correlation performance of the
Hierarchical model ranks somewhere in the middle
(43rd out of 89 systems), while the mean correla-
tion (weighted by number of samples per set) is no-
tably better: 23rd out of 89. Comparing the individ-
ual dataset results, our systems underperform for the
two datasets that originate from the machine transla-
tion (MT) literature (and contain longer sentences),
while we achieve good results for the rest (19th for
paraphrase, 37th for video and 29th for WN).
Table 4: Correlation performance on test set.
par vid euro WN news Mean Ovrl
None 0.50 0.71 0.44 0.49 0.24 0.51 0.49
Sigm. 0.60 0.76 0.26 0.60 0.34 0.56 0.55
Hier. 0.60 0.77 0.43 0.65 0.37 0.60 0.62
5 Conclusions
We have shown that: 1) a regression model that
combines counts of exact and approximate n-gram
matches provides good performance for sentence
similarity computation (especially for short and
medium length sentences), 2) the non-linear scal-
ing of hit-rates with respect to sentence length im-
proves performance, 3) incorporating word semantic
similarity scores (soft-match) into the model can im-
prove performance, and 4) web snippet corpus cre-
ation and the modified mutual information metric
is a language agnostic approach that can (at least)
match semantic similarity performance of the best
resource-based metrics for this task. Future work,
should involve the extension of this approach to
model larger lexical chunks, the incorporation of
compositional models of meaning, and in general
the phrase-level modeling of semantic similarity, in
order to compete with MT-based systems trained on
massive external parallel corpora.
568
References
E. Agirre and P. Edmonds, editors. 2007. Word
Sense Disambiguation: Algorithms and Applications.
Springer.
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673?721.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007. Mea-
suring semantic similarity between words using web
search engines. In Proc. of International Conference
on World Wide Web, pages 757?766.
J. Bos and K. Markert. 2005. Recognising textual en-
tailment with logical inference. In Proceedings of the
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, page 628635.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics, 32:13?47.
K. W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22?29.
I. Dagan, O. Glickman, and B. Magnini. 2006.
The pascal recognising textual entailment challenge.
In Joaquin Quionero-Candela, Ido Dagan, Bernardo
Magnini, and Florence dAlch Buc, editors, Machine
Learning Challenges. Evaluating Predictive Uncer-
tainty, Visual Object Classification, and Recognising
Tectual Entailment, volume 3944 of Lecture Notes in
Computer Science, pages 177?190. Springer Berlin /
Heidelberg.
A. Finch, S. Y. Hwang, and E. Sumita. 2005. Using ma-
chine translation evaluation techniques to determine
sentence-level semantic equivalence. In Proceedings
of the 3rd International Workshop on Paraphrasing,
page 1724.
J. R. Finkel, T. Grenager, and C. D. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 363?370.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
Transactions on Information Systems, 20(1):116?131.
S. Harabagiu and A. Hickl. 2006. Methods for Us-
ing Textual Entailment in Open-Domain Question An-
swering. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 905?912.
E. Iosif and A. Potamianos. 2010. Unsupervised seman-
tic similarity computation between terms using web
documents. IEEE Transactions on Knowledge and
Data Engineering, 22(11):1637?1647.
E. Iosif and A. Potamianos. 2012a. Minimum error se-
mantic similarity using text corpora constructed from
web queries. IEEE Transactions on Knowledge and
Data Engineering (submitted to).
E. Iosif and A. Potamianos. 2012b. Semsim: Resources
for normalized semantic similarity computation using
lexical networks. Proc. of Eighth International Con-
ference on Language Resources and Evaluation (to ap-
pear).
E. Iosif and A. Potamianos. 2012c. Similarity com-
putation using semantic networks created from web-
harvested data. Natural Language Engineering (sub-
mitted to).
N. Madnani and B. J. Dorr. 2010. Generating phrasal and
sentential paraphrases: A survey of data-driven meth-
ods. Computational Linguistics, 36(3):341387.
P. Malakasiotis and I. Androutsopoulos. 2007. Learn-
ing textual entailment using svms and string similar-
ity measures. In Proceedings of of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pages 42?47.
P. Malakasiotis. 2009. Paraphrase recognition using ma-
chine learning to combine similarity measures. In Pro-
ceedings of the 47th Annual Meeting of ACL and the
4th Int. Joint Conference on Natural Language Pro-
cessing of AFNLP, pages 42?47.
N. Malandrakis, A. Potamianos, E. Iosif, and
S. Narayanan. 2011. Kernel models for affec-
tive lexicon creation. In Proc. Interspeech, pages
2977?2980.
H. Meng and K.-C. Siu. 2002. Semi-automatic acquisi-
tion of semantic structures for understanding domain-
specific natural language queries. IEEE Transactions
on Knowledge and Data Engineering, 14(1):172?181.
G. Miller and W. Charles. 1998. Contextual correlates
of semantic similarity. Language and Cognitive Pro-
cesses, 6(1):1?28.
G. Miller. 1990. Wordnet: An on-line lexical database.
International Journal of Lexicography, 3(4):235?312.
S. Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymet-
man, and S. Idan. 2009. Source-language entailment
modeling for translating unknown terms. In Proceed-
ings of the 47th Annual Meeting of ACL and the 4th Int.
Joint Conference on Natural Language Processing of
AFNLP, pages 791?799.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 311?318.
569
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based context vectors to estimate the semantic related-
ness of concepts. In Proc. of the EACL Workshop on
Making Sense of Sense: Bringing Computational Lin-
guistics and Psycholinguistics Together, pages 1?8.
T. Pedersen. 2005. WordNet::Similarity.
http://search.cpan.org/dist/
WordNet-Similarity/.
D. Perez and E. Alfonseca. 2005. Application of the
bleu algorithm for recognizing textual entailments. In
Proceedings of the PASCAL Challenges Worshop on
Recognising Textual Entailment.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxanomy. In Proc. of In-
ternational Joint Conference for Artificial Intelligence,
pages 448?453.
F. Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, and
D. Molla. 2003. Exploiting paraphrases in a question
answering system. In Proceedings of the 2nd Interna-
tional Workshop on Paraphrasing, pages 25?32.
H. Rubenstein and J. B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627?633.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics, pages 849?856.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology, pages 173?180.
P. D. Turney. 2001. Mining the web for synonyms: PMI-
IR versus LSA on TOEFL. In Proc. of the European
Conference on Machine Learning, pages 491?502.
F. Zanzotto, M. Pennacchiotti, and A. Moschitti.
2009. A machine-learning approach to textual en-
tailment recognition. Natural Language Engineering,
15(4):551582.
570
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 103?108, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
DeepPurple: Lexical, String and Affective Feature Fusion for Sentence-Level
Semantic Similarity Estimation
Nikolaos Malandrakis1, Elias Iosif2, Vassiliki Prokopi2, Alexandros Potamianos2,
Shrikanth Narayanan1
1Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA
2Department of ECE, Technical University of Crete, 73100 Chania, Greece
malandra@usc.edu, iosife@telecom.tuc.gr, vprokopi@isc.tuc.gr, potam@telecom.tuc.gr,
shri@sipi.usc.edu
Abstract
This paper describes our submission for the
*SEM shared task of Semantic Textual Sim-
ilarity. We estimate the semantic similarity
between two sentences using regression mod-
els with features: 1) n-gram hit rates (lexical
matches) between sentences, 2) lexical seman-
tic similarity between non-matching words, 3)
string similarity metrics, 4) affective content
similarity and 5) sentence length. Domain
adaptation is applied in the form of indepen-
dent models and a model selection strategy
achieving a mean correlation of 0.47.
1 Introduction
Text semantic similarity estimation has been an ac-
tive research area, thanks to a variety of potential ap-
plications and the wide availability of data afforded
by the world wide web. Semantic textual similar-
ity (STS) estimates can be used for information ex-
traction (Szpektor and Dagan, 2008), question an-
swering (Harabagiu and Hickl, 2006) and machine
translation (Mirkin et al, 2009). Term-level simi-
larity has been successfully applied to problems like
grammar induction (Meng and Siu, 2002) and affec-
tive text categorization (Malandrakis et al, 2011). In
this work, we built on previous research and our sub-
mission to SemEval?2012 (Malandrakis et al, 2012)
to create a sentence-level STS model for the shared
task of *SEM 2013 (Agirre et al, 2013).
Semantic similarity between words has been
well researched, with a variety of knowledge-based
(Miller, 1990; Budanitsky and Hirst, 2006) and
corpus-based (Baroni and Lenci, 2010; Iosif and
Potamianos, 2010) metrics proposed. Moving to
sentences increases the complexity exponentially
and as a result has led to measurements of simi-
larity at various levels: lexical (Malakasiotis and
Androutsopoulos, 2007), syntactic (Malakasiotis,
2009; Zanzotto et al, 2009), and semantic (Rinaldi
et al, 2003; Bos and Markert, 2005). Machine trans-
lation evaluation metrics can be used to estimate lex-
ical level similarity (Finch et al, 2005; Perez and
Alfonseca, 2005), including BLEU (Papineni et al,
2002), a metric using word n-gram hit rates. The pi-
lot task of sentence STS in SemEval 2012 (Agirre et
al., 2012) showed a similar trend towards multi-level
similarity, with the top performing systems utilizing
large amounts of partial similarity metrics and do-
main adaptation (the use of separate models for each
input domain) (Ba?r et al, 2012; S?aric? et al, 2012).
Our approach is originally motivated by BLEU
and primarily utilizes ?hard? and ?soft? n-gram hit
rates to estimate similarity. Compared to last year,
we utilize different alignment strategies (to decide
which n-grams should be compared with which).
We also include string similarities (at the token and
character level) and similarity of affective content,
expressed through the difference in sentence arousal
and valence ratings. Finally we added domain adap-
tation: the creation of separate models per domain
and a strategy to select the most appropriate model.
2 Model
Our model is based upon that submitted for the same
task in 2012 (Malandrakis et al, 2012). To esti-
mate semantic similarity metrics we use a super-
vised model with features extracted using corpus-
103
based word-level similarity metrics. To combine
these metrics into a sentence-level similarity score
we use a modification of BLEU (Papineni et al,
2002) that utilizes word-level semantic similarities,
string level comparisons and comparisons of affec-
tive content, detailed below.
2.1 Word level semantic similarity
Co-occurrence-based. The semantic similarity be-
tween two words, wi and wj , is estimated as their
pointwise mutual information (Church and Hanks,
1990): I(i, j) = log p?(i,j)p?(i)p?(j) , where p?(i) and p?(j) are
the occurrence probabilities of wi and wj , respec-
tively, while the probability of their co-occurrence
is denoted by p?(i, j). In our previous participation
in SemEval12-STS task (Malandrakis et al, 2012)
we employed a modification of the pointwise mutual
information based on the maximum sense similar-
ity assumption (Resnik, 1995) and the minimization
of the respective error in similarity estimation. In
particular, exponential weights ? were introduced in
order to reduce the overestimation of denominator
probabilities. The modified metric Ia(i, j), is de-
fined as:
Ia(i, j)=
1
2
[
log
p?(i, j)
p??(i)p?(j) + log
p?(i, j)
p?(i)p??(j)
]
. (1)
The weight ? was estimated on the corpus of (Iosif
and Potamianos, 2012) in order to maximize word
sense coverage in the semantic neighborhood of
each word. The Ia(i, j) metric using the estimated
value of ? = 0.8 was shown to significantly
outperform I(i, j) and to achieve state-of-the-art
results on standard semantic similarity datasets
(Rubenstein and Goodenough, 1965; Miller and
Charles, 1998; Finkelstein et al, 2002).
Context-based: The fundamental assumption
behind context-based metrics is that similarity
of context implies similarity of meaning (Harris,
1954). A contextual window of size 2H + 1 words
is centered on the word of interest wi and lexical
features are extracted. For every instance of wi
in the corpus the H words left and right of wi
formulate a feature vector vi. For a given value of
H the context-based semantic similarity between
two words, wi and wj , is computed as the cosine
of their feature vectors: QH(i, j) = vi.vj||vi|| ||vj || .
The elements of feature vectors can be weighted
according various schemes [(Iosif and Potamianos,
2010)], while, here we use a binary scheme.
Network-based: The aforementioned similarity
metrics were used for the definition of a semantic
network (Iosif and Potamianos, 2013; Iosif et al,
2013). A number of similarity metrics were pro-
posed under either the attributional similarity (Tur-
ney, 2006) or the maximum sense similarity (Resnik,
1995) assumptions of lexical semantics1.
2.2 Sentence level similarities
To utilize word-level semantic similarities in the
sentence-level task we use a modified version of
BLEU (Papineni et al, 2002). The model works in
two passes: the first pass identifies exact matches
(similar to baseline BLEU), the second pass com-
pares non-matched terms using semantic similarity.
Non-matched terms from the hypothesis sentence
are compared with all terms of the reference sen-
tence (regardless of whether they were matched dur-
ing the first pass). In the case of bigram and higher
order terms, the process is applied recursively: the
bigrams are decomposed into two words and the
similarity between them is estimated by applying the
same method to the words. All word similarity met-
rics used are peak-to-peak normalized in the [0,1]
range, so they serve as a ?degree-of-match?. The se-
mantic similarity scores from term pairs are summed
(just like n-gram hits) to obtain a BLEU-like hit-rate.
Alignment is performed via maximum similarity:
we iterate on the hypothesis n-grams, left-to-right,
and compare each with the most similar n-gram in
the reference. The features produced by this process
are ?soft? hit-rates (for 1-, 2-, 3-, 4-grams)2. We also
use the ?hard? hit rates produced by baseline BLEU
as features of the final model.
2.3 String similarities
We use the following string-based similarity fea-
tures: 1) Longest Common Subsequence Similarity
(LCSS) (Lin and Och, 2004) based on the Longest
Common Subsequence (LCS) character-based dy-
1The network-based metrics were applied only during the
training phase of the shared task, due to time limitations. They
exhibited almost identical performance as the metric defined by
(1), which was used in the test runs.
2Note that the features are computed twice on each sentence
pair and then averaged.
104
namic programming algorithm. LCSS represents the
length of the longest string (or strings) that is a sub-
string (or are substrings) of two or more strings. 2)
Skip bigram co-occurrence measures the overlap of
skip-bigrams between two sentences or phrases. A
skip-bigram is defined as any pair of words in the
sentence order, allowing for arbitrary gaps between
words (Lin and Och, 2004). 3) Containment is de-
fined as the percentage of a sentence that is con-
tained in another sentence. It is a number between
0 and 1, where 1 means the hypothesis sentence is
fully contained in the reference sentence (Broder,
1997). We express containment as the amount of n-
grams of a sentence contained in another. The con-
tainment metric is not symmetric and is calculated
as: c(X,Y ) = |S(X) ? S(Y )|/S(X), where S(X)
and S(Y ) are all the n-grams of sentences X and Y
respectively.
2.4 Affective similarity
We used the method proposed in (Malandrakis et al,
2011) to estimate affective features. Continuous (va-
lence and arousal) ratings in [?1, 1] of any term are
represented as a linear combination of a function of
its semantic similarities to a set of seed words and
the affective ratings of these words, as follows:
v?(wj) = a0 +
N
?
i=1
ai v(wi) dij , (2)
where wj is the term we mean to characterize,
w1...wN are the seed words, v(wi) is the valence rat-
ing for seed word wi, ai is the weight corresponding
to seed word wi (that is estimated as described next),
dij is a measure of semantic similarity between wi
andwj (for the purposes of this work, cosine similar-
ity between context vectors is used). The weights ai
are estimated over the Affective norms for English
Words (ANEW) (Bradley and Lang, 1999) corpus.
Using this model we generate affective ratings for
every content word (noun, verb, adjective or adverb)
of every sentence. We assume that these can ad-
equately describe the affective content of the sen-
tences. To create an ?affective similarity metric? we
use the difference of means of the word affective rat-
ings between two sentences.
d?affect = 2? |?(v?(s1))? ?(v?(s2))| (3)
where ?(v?(si)) the mean of content word ratings in-
cluded in sentence i.
2.5 Fusion
The aforementioned features are combined using
one of two possible models. The first model is a
Multiple Linear Regression (MLR) model
D?L = a0 +
k
?
n=1
an fk, (4)
where D?L is the estimated similarity, fk are the un-
supervised semantic similarity metrics and an are
the trainable parameters of the model.
The second model is motivated by an assumption
of cognitive scaling of similarity scores: we expect
that the perception of hit rates is non-linearly af-
fected by the length of the sentences. We call this the
hierarchical fusion scheme. It is a combination of
(overlapping) MLR models, each matching a range
of sentence lengths. The first model DL1 is trained
with sentences with length up to l1, i.e., l ? l1, the
second model DL2 up to length l2 etc. During test-
ing, sentences with length l ? [1, l1] are decoded
with DL1, sentences with length l ? (l1, l2] with
model DL2 etc. Each of these partial models is a
linear fusion model as shown in (4). In this work,
we use four models with l1 = 10, l2 = 20, l3 = 30,
l4 = ?.
Domain adaptation is employed, by creating sep-
arate models per domain (training data source). Be-
yond that, we also create a unified model, trained
on all data to be used as a fallback if an appropriate
model can not be decided upon during evaluation.
3 Experimental Procedure and Results
Initially all sentences are pre-processed by the
CoreNLP (Finkel et al, 2005; Toutanova et al,
2003) suite of tools, a process that includes named
entity recognition, normalization, part of speech tag-
ging, lemmatization and stemming. We evaluated
multiple types of preprocessing per unsupervised
metric and chose different ones depending on the
metric. Word-level semantic similarities, used for
soft comparisons and affective feature extraction,
were computed over a corpus of 116 million web
snippets collected by posing one query for every
word in the Aspell spellchecker (asp, ) vocabulary to
the Yahoo! search engine. Word-level emotional rat-
ings in continuous valence and arousal scales were
produced by a model trained on the ANEW dataset
105
and using contextual similarities. Finally, string sim-
ilarities were calculated over the original unmodified
sentences.
Next, results are reported in terms of correla-
tion between the generated scores and the ground
truth, for each corpus in the shared task, as well as
their weighted mean. Feature selection is applied
to the large candidate feature set using a wrapper-
based backward selection approach on the train-
ing data.The final feature set contains 15 features:
soft hit rates calculated over content word 1- to 4-
grams (4 features), soft hit rates calculated over un-
igrams per part-of-speech, for adjectives, nouns, ad-
verbs, verbs (4 features), BLEU unigram hit rates
for all words and content words (2 features), skip
and containment similarities, containment normal-
ized by sum of sentence lengths or product of sen-
tence lengths (3 features) and affective similarities
for arousal and valence (2 features).
Domain adaptation methods are the only dif-
ference between the three submitted runs. For all
three runs we train one linear model per training set
and a fallback model. For the first run, dubbed lin-
ear, the fallback model is linear and model selection
during evaluation is performed by file name, there-
fore results for the OnWN set are produced by a
model trained with OnWN data, while the rest are
produced by the fallback model. The second run,
dubbed length, uses a hierarchical fallback model
and model selection is performed by file name. The
third run, dubbed adapt, uses the same models as
the first run and each test set is assigned to a model
(i.e., the fallback model is never used). The test set -
model (training) mapping for this run is: OnWN ?
OnWN, headlines ? SMTnews, SMT ? Europarl
and FNWN? OnWN.
Table 1: Correlation performance for the linear model us-
ing lexical (L), string (S) and affect (A) features
Feature headl. OnWN FNWN SMT mean
L 0.68 0.51 0.23 0.25 0.46
L+S 0.69 0.49 0.23 0.26 0.46
L+S+A 0.69 0.51 0.27 0.28 0.47
Results are shown in Tables 1 and 2. Results for
the linear run using subsets of the final feature set
are shown in Table 1. Lexical features (hit rates) are
obviously the most valuable features. String similar-
ities provided us with an improvement in the train-
Table 2: Correlation performance on the evaluation set.
Run headl. OnWN FNWN SMT mean
linear 0.69 0.51 0.27 0.28 0.47
length 0.65 0.51 0.25 0.28 0.46
adapt 0.62 0.51 0.33 0.30 0.46
ing set which is not reflected in the test set. Af-
fect proved valuable, particularly in the most diffi-
cult sets of FNWN and SMT.
Results for the three submission runs are shown
in Table 2. Our best run was the simplest one, using
a purely linear model and effectively no adaptation.
Adding a more aggressive adaptation strategy im-
proved results in the FNWN and SMT sets, so there
is definitely some potential, however the improve-
ment observed is nowhere near that observed in the
training data or the same task of SemEval 2012. We
have to question whether this improvement is an ar-
tifact of the rating distributions of these two sets
(SMT contains virtually only high ratings, FNWN
contains virtually only low ratings): such wild mis-
matches in priors among training and test sets can
be mitigated using more elaborate machine learning
algorithms (rather than employing better semantic
similarity features or algorithms). Overall the sys-
tem performs well in the two sets containing large
similarity rating ranges.
4 Conclusions
We have improved over our previous model of sen-
tence semantic similarity. The inclusion of string-
based similarities and more so of affective content
measures proved significant, but domain adaptation
provided mixed results. While expanding the model
to include more layers of similarity estimates is
clearly a step in the right direction, further work is
required to include even more layers. Using syntac-
tic information and more levels of abstraction (e.g.
concepts) are obvious next steps.
5 Acknowledgements
The first four authors have been partially funded
by the PortDial project (Language Resources for
Portable Multilingual Spoken Dialog Systems) sup-
ported by the EU Seventh Framework Programme
(FP7), grant number 296170.
106
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre.
2012. Semeval-2012 task 6: A pilot on semantic tex-
tual similarity. In Proc. SemEval, pages 385?393.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In Proc. *SEM.
Gnu aspell. http://www.aspell.net.
D. Ba?r, C. Biemann, I. Gurevych, and T. Zesch. 2012.
Ukp: Computing semantic textual similarity by com-
bining multiple content similarity measures. In Proc.
SemEval, pages 435?440.
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673?721.
J. Bos and K. Markert. 2005. Recognising textual en-
tailment with logical inference. In Proceedings of the
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, page 628635.
M. Bradley and P. Lang. 1999. Affective norms for En-
glish words (ANEW): Stimuli, instruction manual and
affective ratings. Technical report C-1. The Center for
Research in Psychophysiology, University of Florida.
Andrei Z. Broder. 1997. On the resemblance and con-
tainment of documents. In In Compression and Com-
plexity of Sequences (SEQUENCES97, pages 21?29.
IEEE Computer Society.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics, 32:13?47.
K. W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22?29.
A. Finch, S. Y. Hwang, and E. Sumita. 2005. Using ma-
chine translation evaluation techniques to determine
sentence-level semantic equivalence. In Proceedings
of the 3rd International Workshop on Paraphrasing,
page 1724.
J. R. Finkel, T. Grenager, and C. D. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 363?370.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
Transactions on Information Systems, 20(1):116?131.
S. Harabagiu and A. Hickl. 2006. Methods for Us-
ing Textual Entailment in Open-Domain Question An-
swering. In Proceedings of the 21st InternationalCon-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 905?912.
Z. Harris. 1954. Distributional structure. Word,
10(23):146?162.
E. Iosif and A. Potamianos. 2010. Unsupervised seman-
tic similarity computation between terms using web
documents. IEEE Transactions on Knowledge and
Data Engineering, 22(11):1637?1647.
E. Iosif and A. Potamianos. 2012. Semsim: Resources
for normalized semantic similarity computation using
lexical networks. In Proc. Eighth International Con-
ference on Language Resources and Evaluation, pages
3499?3504.
Elias Iosif and Alexandros Potamianos. 2013. Similarity
Computation Using Semantic Networks Created From
Web-Harvested Data. Natural Language Engineering,
(submitted).
E. Iosif, A. Potamianos, M. Giannoudaki, and K. Zer-
vanou. 2013. Semantic similarity computation for ab-
stract and concrete nouns using network-based distri-
butional semantic models. In 10th International Con-
ference on Computational Semantics (IWCS), pages
328?334.
Chin-Yew Lin and Franz Josef Och. 2004. Automatic
evaluation of machine translation quality using longest
common subsequence and skip-bigram statistics. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, ACL ?04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
P. Malakasiotis and I. Androutsopoulos. 2007. Learn-
ing textual entailment using svms and string similar-
ity measures. In Proceedings of of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pages 42?47.
P. Malakasiotis. 2009. Paraphrase recognition using ma-
chine learning to combine similarity measures. In Pro-
ceedings of the 47th Annual Meeting of ACL and the
4th Int. Joint Conference on Natural Language Pro-
cessing of AFNLP, pages 42?47.
N. Malandrakis, A. Potamianos, E. Iosif, and
S. Narayanan. 2011. Kernel models for affec-
tive lexicon creation. In Proc. Interspeech, pages
2977?2980.
N. Malandrakis, E. Iosif, and A. Potamianos. 2012.
DeepPurple: Estimating sentence semantic similarity
using n-gram regression models and web snippets. In
Proc. Sixth International Workshop on Semantic Eval-
uation (SemEval) ? The First Joint Conference on
Lexical and Computational Semantics (*SEM), pages
565?570.
H. Meng and K.-C. Siu. 2002. Semi-automatic acquisi-
tion of semantic structures for understanding domain-
107
specific natural language queries. IEEE Transactions
on Knowledge and Data Engineering, 14(1):172?181.
G. Miller and W. Charles. 1998. Contextual correlates
of semantic similarity. Language and Cognitive Pro-
cesses, 6(1):1?28.
G. Miller. 1990. Wordnet: An on-line lexical database.
International Journal of Lexicography, 3(4):235?312.
S. Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymet-
man, and S. Idan. 2009. Source-language entailment
modeling for translating unknown terms. In Proceed-
ings of the 47th AnnualMeeting of ACL and the 4th Int.
Joint Conference on Natural Language Processing of
AFNLP, pages 791?799.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 311?318.
D. Perez and E. Alfonseca. 2005. Application of the
bleu algorithm for recognizing textual entailments. In
Proceedings of the PASCAL Challenges Worshop on
Recognising Textual Entailment.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxanomy. In Proc. of In-
ternational Joint Conference for Artificial Intelligence,
pages 448?453.
F. Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, and
D. Molla. 2003. Exploiting paraphrases in a question
answering system. In Proceedings of the 2nd Interna-
tional Workshop on Paraphrasing, pages 25?32.
H. Rubenstein and J. B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627?633.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics, pages 849?856.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology, pages 173?180.
P. Turney. 2006. Similarity of semantic relations. Com-
putational Linguistics, 32(3):379?416.
F. S?aric?, G. Glavas?, M. Karan, J. S?najder, and B. Dal-
belo Bas?ic?. 2012. Takelab: Systems for measuring
semantic text similarity. In Proc. SemEval, pages 441?
448.
F. Zanzotto, M. Pennacchiotti, and A. Moschitti.
2009. A machine-learning approach to textual en-
tailment recognition. Natural Language Engineering,
15(4):551582.
108
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 9?16,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 2: Grammar Induction for Spoken Dialogue Systems
Ioannis Klasinas
1
, Elias Iosif
2,4
, Katerina Louka
3
, Alexandros Potamianos
2,4
1
School of ECE, Technical University of Crete, Chania 73100, Greece
2
School of ECE, National Technical University of Athens, Zografou 15780, Greece
3
Voiceweb S.A., Athens 15124, Greece
4
Athena Research Center, Marousi 15125, Greece
iklasinas@isc.tuc.gr,{iosife,potam}@telecom.tuc.gr,klouka@voiceweb.eu
Abstract
In this paper we present the SemEval-
2014 Task 2 on spoken dialogue gram-
mar induction. The task is to classify
a lexical fragment to the appropriate se-
mantic category (grammar rule) in order
to construct a grammar for spoken dia-
logue systems. We describe four sub-
tasks covering two languages, English and
Greek, and three speech application do-
mains, travel reservation, tourism and fi-
nance. The classification results are com-
pared against the groundtruth. Weighted
and unweighted precision, recall and f-
measure are reported. Three sites partic-
ipated in the task with five systems, em-
ploying a variety of features and in some
cases using external resources for training.
The submissions manage to significantly
beat the baseline, achieving a f-measure of
0.69 in comparison to 0.56 for the base-
line, averaged across all subtasks.
1 Introduction
This task aims to foster the application of com-
putational models of lexical semantics to the field
of spoken dialogue systems (SDS) for the problem
of grammar induction. Grammars constitute a vi-
tal component of SDS representing the semantics
of the domain of interest and allowing the system
to correctly respond to a user?s utterance.
The task has been developed in tight collabo-
ration between the research community and com-
mercial SDS grammar developers, under the aus-
pices of the EU-IST PortDial project
1
. Among the
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
1
http://www.portdial.eu/
project aims is to help automate the grammar de-
velopment and localization process. Unlike previ-
ous approaches (Wang and Acero, 2006; Cramer,
2007) that have focused on full automation, Port-
Dial adopts a human-in-the-loop approach were
a developer bootstraps each grammar rule or re-
quest type with a few examples (use cases) and
then machine learning algorithms are used to pro-
pose grammar rule enhancements to the developer.
The enhancements are post-edited by the devel-
oper and new grammar rule suggestions are pro-
posed by the system, in an iterative fashion un-
til a grammar of sufficient quality is achieved. In
this task, we focus on a snapshot of this process,
where a portion of the grammar is already induced
and post-edited by the developer and new candi-
date fragments are rolling in order to be classified
to an existing rule (or rejected). The goal is to de-
velop machine learning algorithms for classifying
candidate lexical fragments to the correct grammar
rule (semantic category). The task is equally rel-
evant for both finite-state machine and statistical
grammar induction.
In this task the semantic hierarchy of SDS
grammars has two layers, namely, low- and high-
level. Low-level rules are similar to gazetteers
referring to terminal concepts that can be as rep-
resented as sets of lexical entries. For example,
the concept of city name can be represented as
<CITY> = (?London?, ?Paris?, ...). High-level
rules are defined on top of low-level rules, while
they can be lexicalized as textual fragments (or
chunks), e.g., <TOCITY> = (?fly to <CITY>?,
...). Using the above examples the sentence ?I
want to fly to Paris? will be first parsed as ?I
want to fly to <CITY>? and finally as ?I want to
<TOCITY>?.
In this task, we focus exclusively on high-level
rule induction, assuming that the low-level rules
are known. The problem of fragment extraction
and selection is simplified by investigating the
9
binary classification of (already extracted) frag-
ments into valid and non-valid. The task boils
down mainly to a semantic similarity estimation
problem for the assignment of valid fragments into
high-level rules.
2 Prior Work
The manual development of grammars is a time-
consuming and tedious process that requires hu-
man expertise, posing an obstacle to the rapid port-
ing of SDS to new domains and languages. A
semantically coherent workflow for SDS gram-
mar development starts from the definition of low-
level rules and proceeds to high-level ones. This
process is also valid for the case of induction
algorithms. Automatic or machine-aided gram-
mar creation for spoken dialogue systems can
be broadly divided in two categories (Wang and
Acero, 2006): knowledge-based (or top-down)
and data-driven (or bottom-up) approaches.
Knowledge-based approaches rely on the man-
ual or semi-automatic development of domain-
specific grammars. They start from the domain on-
tology (or taxonomy), often in the form of seman-
tic frames. First, terminal concepts in the ontology
(that correspond to low-level grammar rules) get
populated with values, e.g., <CITY>, and then
high-level concepts (that correspond to high-level
grammar rules) get lexicalized creating grammar
fragments. Finally, phrase headers and trailers are
added to create full sentences. The resulting gram-
mars often suffer from limited coverage (poor re-
call). In order to improve coverage, regular ex-
pressions and word/phrase order permutations are
used, however at the cost of over-generalization
(poor precision). Moreover, knowledge-based
grammars are costly to create and maintain, as
they require domain and engineering expertise,
and they are not easily portable to new domains.
This led to the development of grammar authoring
tools that aim at facilitating the creation and adap-
tation of grammars. SGStudio (Semantic Gram-
mar Studio), (Wang and Acero, 2006), for exam-
ple, enables 1) example-based grammar learning,
2) grammar controls, i.e., building blocks and op-
erators for building more complex grammar frag-
ments (regular expressions, lists of concepts), and
3) configurable grammar structures, allowing for
domain-adaptation and word-spotting grammars.
The Grammatical Framework Resource Grammar
Library (GFRGL) (Ranta, 2004) enables the cre-
ation of multilingual grammars adopting an ab-
straction formalism, which aims to hide the lin-
guistic details (e.g., morphology) from the gram-
mar developer.
Data-driven approaches rely solely on corpora
(bottom-up) of transcribed utterances (Meng and
Siu, 2002; Pargellis et al., 2004). The induction
of low-level rules consists of two steps dealing
with the 1) identification of terms, and 2) assign-
ment of terms into rules. Standard tokenization
techniques can be used for the first step, however,
different approaches are required for the case of
multiword terms, e.g., ?New York?. In such cases,
gazetteer lookup and named entity recognition can
be employed (if the respective resources and tools
are available), as well as corpus-based colloca-
tion metrics (Frantzi and Ananiadou, 1997). Typ-
ically, the identified terms are assigned into low-
level rules via clustering algorithms operating over
a feature space that is built according to the term
semantic similarity. The distributional hypothe-
sis of meaning (Harris, 1954) is a widely-used ap-
proach for estimating term similarity. A compar-
ative study of similarity metrics for the induction
of SDS low-level rules is presented in (Pargellis
et al., 2004), while the combination of metrics
was investigated in (Iosif et al., 2006). Different
clustering algorithms have been applied includ-
ing hard- (Meng and Siu, 2002) and soft-decision
(Iosif and Potamianos, 2007) agglomerative clus-
tering.
High-level rule induction is a less researched
area that consists of two main sub-problems: 1)
the extraction and selection of candidate frag-
ments from a corpus, and 2) assignment of terms
into rules. Regarding the first sub-problem,
consider the fragments ?I want to depart from
<CITY> on? and ?depart from <CITY>? for the
air travel domain. Both express the meaning of de-
parture city, however, the (semantics of the) latter
fragment are more concise and generalize better.
The application of syntactic parsers for segment
extraction is not straightforward since the output
is a full parse tree. Moreover, such parsers are
typically trained over annotated corpora of formal
language usage, while the SDS corpora often are
ungrammatical due to spontaneous speech. There
are few statistical parsing algorithms that rely only
on plain lexical features (Ponvert et al., 2011; Bisk
and Hockenmaier, 2012) however, as other algo-
rithms, one needs to decide where to prune the
10
parse tree. In (Georgiladakis et al., 2014), the ex-
plicit extraction and selection of fragments is in-
vestigated following an example-driven approach
where few rule seeds are provided by the gram-
mar developer. The second sub-problem of high-
level rule induction deals with the formulation
of rules using the selected fragments. Each rule
is meant to consist of semantically similar frag-
ments. For this purpose, clustering algorithms can
be employed exploiting the semantic similarity be-
tween fragments as features. This is a challenging
problem since the fragments are multi-word struc-
tures whose overall meaning is composed accord-
ing to semantics of the individual constituents. Re-
cently, several models have been proposed regard-
ing phrase (Mitchell and Lapata, 2010) and sen-
tence similarity (Agirre et al., 2012), while an
approach towards addressing the issue of seman-
tic compositionality is presented in (Milajevs and
Purver, 2014).
The main drawback of data-driven approaches
is the problem of data sparseness, which may af-
fect the coverage of the grammar. A popular so-
lution to the data sparseness bottleneck is to har-
vest in-domain data from the web. Recently, this
has been an active research area both for SDS
systems and language modeling in general. Data
harvesting is performed in two steps: (i) query
formulation, and (ii) selection of relevant docu-
ments or sentences (Klasinas et al., 2013). Posing
the appropriate queries is important both for ob-
taining in-domain and linguistically diverse sen-
tences. In (Sethy et al., 2007), an in-domain lan-
guage model was used to identify the most ap-
propriate n-grams to use as web queries. An in-
domain language model was used in (Klasinas et
al., 2013) for the selection of relevant sentences.
A more sophisticated query formulation was pro-
posed in (Sarikaya, 2008), where from each in-
domain utterance a set of queries of varying length
and complexity was generated. These approaches
assume the availability of in-domain data (even if
limited) for the successful formulation of queries;
this dependency is also not eliminated when us-
ing a mildly lexicalized domain ontology to for-
mulate the queries, as in (Misu and Kawahara,
2006). Selecting the most relevant sentences that
get returned from web queries is typically done
using statistical similarity metrics between in do-
main data and retrieved documents, for example
the BLEU metric (Papineni et al., 2002) of n-
gram similarity in (Sarikaya, 2008) and a metric
of relative entropy (Kullback-Leibler) in (Sethy et
al., 2007). In cases where in-domain data is not
available, cf. (Misu and Kawahara, 2006), heuris-
tics (pronouns, sentence length, wh-questions) and
matches with out-of-domain language models can
be used to identify sentences for training SDS
grammars. In (Sarikaya, 2008), the produced
grammar fragments are also parsed and attached
to the domain ontology. Harvesting web data can
produce high-quality grammars while requiring up
to 10 times less in-domain data (Sarikaya, 2008).
Further, data-driven approaches induce syntac-
tic grammars but do not learn their corresponding
meanings, for this purpose an additional step is re-
quired of parsing the grammar fragments and at-
taching them to the domain ontology (Sarikaya,
2008). Also, in many cases it was observed
that the fully automated bottom-up paradigm re-
sults to grammars of moderate quality (Wang
and Acero, 2006), especially on corpora con-
taining longer sentences and more lexical vari-
ety (Cramer, 2007). Finally, algorithms focusing
on crosslingual grammar induction, like CLIoS
(Kuhn, 2004), are often even more resource-
intensive, as they require training corpora of par-
allel text and sometimes also a grammar for one of
the languages. Grammar quality can be improved
by introducing a human in the loop of grammar in-
duction (Portdial, 2014a); an expert that validates
the automatically created results (Meng and Siu,
2002).
3 Task Description
Next we describe in detail the candidate grammar
fragment classification SemEval task. This task
is part of a grammar rule induction scenario for
high-level rules. The evaluation focuses in spoken
dialogue system grammars for multiple domains
and languages.
3.1 Task Design
The goal of the task is to classify a number frag-
ment to the rules available in the grammar. For
each grammar we provide a training and develop-
ment set, i.e., a set of rules with the associated
fragments and the test set which is composed of
plain fragments. An excerpt of the train set for the
rule ?<TOCITY>? is ?ARRIVE AT <CITY>,
ARRIVES AT <CITY>, GOING TO <CITY>?
and of the test set ?GOING INTO <CITY>, AR-
11
RIVES INTO <CITY>?.
In preliminary experiments during the task de-
sign we noticed that if the test set consists of valid
fragments only, good classification performance is
achieved, even when using the naive baseline sys-
tem described later in this paper. To make the task
more realistic we have included a set of ?junk?
fragments not corresponding to any specific rule.
Junk fragments were added both in the train set
where they are annotated as such and in the test
set. For this task we have artificially created the
junk fragments by removing or adding words from
legitimate fragments. Example junk fragments
used are ?HOLD AT AT <TIME> TRY? and
?ANY CHOICE EXCEPT <AIRLINE> OR?, the
first one having a repetition of the word ?AT?
while the second one should include one more
time the concept ?<AIRLINE>? in the end to be
meaningful.
Junk fragments help better model a real-world
scenario, where the candidate fragments will in-
clude irrelevant examples too. For example, if
web corpora are used to extract the candidate frag-
ments grammatical mistakes and out-of-domain
sentences might appear. Similarly, if the transcrip-
tions from a deployed SDS system are used for
grammar induction, transcription errors might in-
troduce noise (Bechet et al., 2014).
Junk fragments account for roughly 5% of the
train test and 15% of the test set. The discrep-
ancy between train and test set ratios is due to a
conscious effort to model realistic train/test condi-
tions, where train data is manually processed and
does not include errors, while candidate fragments
are typically more noisy.
3.2 Datasets
We have provided four datasets, travel English,
travel Greek, tourism English and finance English.
The travel domain grammar covers flight, car and
hotel reservation utterances. The tourism domain
covers touristic information including accommo-
dation, restaurants and movies. The finance do-
main covers utterances of a bank client asking
questions about his bank account as well as re-
porting problems. In Table 1 are presented typical
examples of fragments for every subtask.
All grammars have been manually constructed
by a grammar developer. For the three English
grammars, a small corpus (between 500 and 2000
sentences) was initially available. The grammar
developer first identified terminal concepts, which
correspond to low-level rules. Typical examples
include city names for the travel domain, restau-
rant names for the tourism domain and credit card
names in the finance domain. After covering all
low-level rules the grammar developer proceeded
to identify high-level rules present in the corpus,
like the departure city in the travel domain, or the
user request type for a credit card. The gram-
mar developer was instructed to identify all rules
present in the corpus, but also spend some effort
to include rules not appearing in the corpus so that
the resulting grammar better covers the domain at
hand. For the case of Greek travel grammar no
corpus was initially available. The Greek gram-
mar was instead produced by manually translat-
ing the English one, accounting for the differences
in syntax between the two languages. The gram-
mars have been developed as part of the PortDial
FP7 project and are explained in detail in (Portdial,
2014b).
For the first three datasets that have been avail-
able from the beginning of the campaign we have
split the release into train, development and test
set. For the finance domain which was announced
when the test sets were released we only provided
the train and test set, to simulate a resource poor
scenario. The statistics of the datasets for all lan-
guage/domain pairs are given in Table 2.
In addition to the high-level rules we made
available the low-level rules for each grammar,
which although not used in the evaluation, can be
useful for expanding the high-level rules to cover
all lexicalizations expressed by the grammar.
3.3 Evaluation
For the evaluation of the task we have used preci-
sion, recall and f-measure, both weighted and un-
weighted.
If R
j
denotes the set of fragments for one rule
and C
j
the set of fragments classified to this rule
by a system then per-rule precision is computed by
the equation:
Pr
j
=
|R
j
? C
j
|
|C
j
|
and per-rule recall by:
Rc
j
=
|R
j
? C
j
|
|R
j
|
F-measure is then computed by:
12
Grammar Rule Fragment
Travel English <FLIGHTFROM> FLIGHT FROM <CITY>
Travel Greek <FLIGHTFROM> ?TH?H A?O <CITY>
Tourism English <TRANSFERQ> TRANSFERS FROM <airportname> TO <cityname>
Finance English <CARDNAME> <BANKNAME> CARD
Table 1: Example grammar fragments for each application domain.
Grammar Rules Fragments
Train set Dev set Test set
Travel English 32 623 331 284
Travel Greek 35 616 340 324
Tourism English 24 694 334 285
Finance English 9 136 - 37
Table 2: Number of rules in the training, development and test sets for each application domain.
F
j
=
2Pr
j
Rc
j
Pr
j
+ Rc
j
.
Precision for all the J rules R
j
, 1 ? j ? J is
computed by the following equation:
Pr =
?
j
Pr
j
w
j
In the unweighted case the weight w
j
has a fixed
value for all rules, so w
j
=
1
J
. Taking into account
the fact that the rules are not balanced in terms of
fragments, a better way to compute for the weight
is w
j
=
|R
j
|
?
j
|R
j
|
. In the latter, weighted, case the
total precision will better describe the results.
Recall is similarly computed using the same
weighting scheme as:
Rc =
?
j
Rc
j
w
j
3.4 Baseline
For comparison purposes we have developed a
naive baseline system. To classify a test fragment,
first its similarity with all the train fragments is
computed, and it is classified to the rule where
the most similar train fragment belongs. Fragment
similarity is computed as the ratio of their Longest
Common Substring (LCS) divided by the sum of
their lengths:
Sim(s, t) =
|LCS(s, t)|
|s|+ |t|
where s and t are two strings, |s| and |t| their
length in characters and |LCS(s, t)| the length of
their LCS. This is a very simple baseline, comput-
ing similarity without taking into account context
or semantics.
4 Participating Systems
Three teams have participated in the task with five
systems. All teams participated in all subtasks
with the exception of travel Greek, where only
two teams participated. An overview of core
system features is presented in Table 3. The
remainder of this section briefly describes each
of the submissions and then compares them. A
brief description for each system is provided in
the following paragraphs.
tucSage. The core of the tucSage system is
a combination of two components. The first
component is used for the selection of candidate
rule fragments from a corpus. Specifically, the
posterior probability of a candidate fragment
belonging to a rule is computed using a variety of
features. The feature set includes various lexical
features (e.g., the number of tokens), the fragment
perplexity computed using n-gram language
modeling, and features based on lexical similarity.
The second component is used for computing
the similarity between a candidate fragment and
a grammar rule. In total, two different types of
similarity metrics are used relying on the overlap
of character bigrams and contextual features.
These similarities are fused with the posterior
probabilities produced by the fragment selection
model. The contribution of the two components is
adjusted using an exponential weight.
SAIL-GRS. The SAIL-GRS system is based
on the well-established term frequency?inverse
document frequency (TF ?IDF ) measurement.
This metric is adapted to the present task by
considering each grammar rule as a ?document?.
For each rule, all its fragments are aggregated
13
System Use of Features Similarity External Language-
acronym machine learn. used metrics corpora specific
Baseline no lexical Longest Common no no
Substring
tucSage yes: lexical, perplexity, character overlap, web no
random forests similarity-based , heuristic cosine similarity documents
SAIL-GRS no lexical cosine similarity no no
Biel no lexical, expansion of cosine Wikipedia yes
low-level rules similarity articles
Table 3: Overview of the characteristics of the participating systems.
and the frequency of the respective n-grams
(constituents) is computed. The inverse document
frequency is casted as inverse rule frequency
and it is computed for the extracted n-grams.
The process is performed for both unigrams and
bigrams.
Biel. The fundamental idea behind the Biel
system is the encoding of domain semantics via
topic modeling. For this purpose a background
document space is constructed using thousands
of Wikipedia articles. Particular focus is given
to the transformation of the initial document
space according to the paradigm of explicit
semantic analysis. For each domain, a topic
space is defined and a language-specific function
is employed for the mapping of documents. In
essence, the mapping function is an association
measurement that is based on TF?IDF scores.
An approximation regarding the construction of
the topic space is investigated in order to reduce
data sparsity, while a number of normalization
schemes are also presented.
Overall, only the tucSage system employs a ma-
chine learning-based approach (random forests),
while an unsupervised approach is followed by the
SAIL-GRS and Biel systems. All systems exploit
lexical information extracted from rule fragments.
This information is realized as the lexical surface
form of the constituents of fragments. For ex-
ample, consider the ?depart for <CITY>? frag-
ment that corresponds to the high-level rule refer-
ring to the notion of departure city. The follow-
ing set of lexical features can be extracted from
the aforementioned fragment: (?depart?, ?from?,
?<CITY>?). Unlike the other systems, the Biel
system utilizes low-level rules to expand high-
level rules with terminal concept instances. For
example, the ?<CITY>? rule is not processed as
is, but it is represented as a list of city names
(?New York?, ?Boston?, . . . ). The most rich fea-
ture set is used by the tucSage system which com-
bines lexical, perplexity and similarity features
with a set of heuristic rules. All three systems
employ the widely-used cosine similarity metric.
Both SAIL-GRS and Biel systems rely solely on
this metric during the assignment of an unknown
fragment to a high-level rule. A more sophis-
ticated approach is presented by tucSage, where
first a classifier is built for every grammar rule,
computing the probability of a fragment belong-
ing to this rule and then the similarity between the
fragment and the rule is computed. Classification
is then performed by combining the two scores.
Also, another difference regarding the employ-
ment of the cosine similarity deals with the com-
putation of the vectorial feature values. A simple
binary scheme is used in the tucSage system, while
variations of the term frequency-inverse document
frequency scheme are used in SAIL-GRS and Biel.
Besides cosine similarity, a similarity metric based
on the overlap of character bigrams is used by the
tucSage system. External corpora (i.e., corpora
that were not provided as part of the official task
data) were used by the tucSage and Biel systems.
Such corpora were meant as an additional source
of information with respect to the domains under
investigation. Regarding tucSage, the training data
were exploited in order to construct web search
queries for harvesting a collection of web docu-
ments from which a number of sentences were se-
lected for corpus creation. In the case of the Biel
system, a set of Wikipedia articles was exploited.
Language specific resources where used for the
Biel system, while the other two teams used lan-
guage agnostic methods.
5 Results
The results for all participating teams and the
baseline system are given in Table 4. The tucSage
team submitted three runs, the first one being the
primary, indicated with an asterisk in the results.
14
Focusing on the weighted F-measure we see
that in all domains but the tourism English, at
least one submission manages to outperform the
baseline provided by the organizers. In travel En-
glish the baseline system achieves 0.51 weighted
f-measure, with two out of the three systems
achieving 0.68 and 0.58. The improvement over
the baseline is greater for the travel Greek sub-
task, where the baseline score of 0.26 is much
lower than the achieved 0.52 from tucSage. In the
tourism English subtask the best submitted sys-
tems managed to match the performance of the
baseline system, but not to exceed it. This can
be attributed to the good performance of the base-
line system, due to the fact that the tourism gram-
mar is composed of longer fragments than the rest,
helping the naive baseline system achieve top per-
formance exploiting lexical similarity only. We
can however assume that more complex systems
would beat the baseline if the test set fragments
were built using different lexicalizations, as would
be the case in unannotated data coming from de-
ployed SDS.
In the finance domain, even though the amount
of training data is quite smaller than in all other
subtasks the submitted systems still manage to
outperform the baseline system. This means that
the submitted systems display robust performance
both in resource-rich and resource-poor condi-
tions.
6 Conclusion
The tucSage and SAIL-GRS systems are shown to
be portable across domains and languages, achiev-
ing performance that exceeds the baseline for three
out of four datasets. The highest performance of
the tucSage system compared to the SAIL-GRS
system may be attributed to the use of a model for
fragment selection. Interestingly, the simple vari-
ation of the TF?IDF scheme used by the SAIL
system achieved very good results being a close
second performer. The UNIBI system proposed
a very interesting new application of the frame-
work of topic modeling to the task of grammar in-
duction, however, the respective performance does
not exceed the state-of-the-art. The combination
of the tucSage and SAIL-GRS systems could give
better results.
team Weighted Unweighted
Pr. Rec. F-m. Pr. Rec. F-m.
Travel English
Baseline 0.40 0.69 0.51 0.38 0.67 0.48
tucSage1
?
0.60 0.73 0.66 0.59 0.74 0.66
tucSage2 0.59 0.72 0.65 0.59 0.74 0.65
tucSage3 0.69 0.67 0.68 0.66 0.69 0.67
SAIL-GRS 0.54 0.62 0.58 0.57 0.66 0.61
Biel 0.13 0.39 0.20 0.09 0.34 0.14
Travel Greek
Baseline 0.17 0.65 0.26 0.16 0.73 0.26
tucSage1
?
0.47 0.58 0.52 0.55 0.72 0.62
tucSage2 0.46 0.53 0.49 0.50 0.59 0.54
tucSage3 0.51 0.48 0.49 0.52 0.56 0.54
SAIL-GRS 0.46 0.51 0.49 0.49 0.62 0.55
Biel - - - - - -
Tourism English
Baseline 0.80 0.94 0.87 0.82 0.94 0.87
tucSage1
?
0.79 0.94 0.86 0.76 0.91 0.83
tucSage2 0.78 0.93 0.85 0.73 0.90 0.80
tucSage3 0.80 0.93 0.86 0.77 0.90 0.83
SAIL-GRS 0.75 0.90 0.82 0.75 0.90 0.82
Biel 0.04 0.14 0.06 0.02 0.08 0.04
Finance English
Baseline 0.48 0.78 0.60 0.40 0.63 0.49
tucSage1
?
0.61 0.81 0.70 0.43 0.54 0.48
tucSage2 0.55 0.74 0.63 0.40 0.51 0.45
tucSage3 0.52 0.67 0.58 0.39 0.43 0.41
SAIL-GRS 0.78 0.78 0.78 0.67 0.62 0.65
Biel 0.22 0.30 0.25 0.06 0.18 0.09
Average over all four tasks
Baseline 0.46 0.73 0.56 0.44 0.74 0.53
tucSage1
?
0.62 0.77 0.69 0.58 0.73 0.65
tucSage2 0.60 0.73 0.66 0.56 0.69 0.61
tucSage3 0.63 0.69 0.65 0.59 0.65 0.61
SAIL-GRS 0.63 0.70 0.67 0.62 0.70 0.66
Biel 0.13 0.28 0.17 0.06 0.20 0.09
Table 4: Weighted and unweighted precision, re-
call and f-measure for all systems. Best perfor-
mance per metric and dataset shown in bold.
Acknowledgements
The task organizers wish to thank Maria Gian-
noudaki and Maria Vomva for the editing of the
hand-crafted grammars used in this evaluation
task. The authors would like to thank the anony-
mous reviewer for the valuable comments and sug-
gestions to improve the quality of the paper. This
work has been partially funded by the SpeDial and
PortDial projects, supported by the EU Seventh
Framework Programme (FP7), with grant number
611396 and 296170 respectively.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In Proceedings
15
of the First Joint Conference on Lexical and Compu-
tational Semantics, pages 385?393.
Frederic Bechet, Benoit Favre, Alexis Nasr, and Math-
ieu Morey. 2014. Retrieving the syntactic structure
of erroneous ASR transcriptions for open-domain
spoken language understanding. In Proceedings of
the International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pages 4125?4129.
Yonatan Bisk and Julia Hockenmaier. 2012. Simple
robust grammar induction with combinatory catego-
rial grammars. In Proceedings of the 26th Confer-
ence on Artificial Intelligence, pages 1643?1649.
Bart Cramer. 2007. Limitations of current grammar
induction algorithms. In Proceedings of the 45th
annual meeting of the ACL: Student Research Work-
shop, pages 43?48.
Katerina T. Frantzi and Sophia Ananiadou. 1997. Au-
tomatic term recognition using contextual cues. In
Proceedings of the International Joint Conference
on Artificial Intelligence, pages 41?46.
Spiros Georgiladakis, Christina Unger, Elias Iosif,
Sebastian Walter, Philipp Cimiano, Euripides Pe-
trakis, and Alexandros Potamianos. 2014. Fusion
of knowledge-based and data-driven approaches to
grammar induction. In Proceedings of Interspeech
(accepted).
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Elias Iosif and Alexandros Potamianos. 2007. A soft-
clustering algorithm for automatic induction of se-
mantic classes. In Proceedings of Interspeech, pages
1609?1612.
Elias Iosif, Athanasios Tegos, Apostolos Pangos, Eric
Fosler-Lussier, and Alexandros Potamianos. 2006.
Unsupervised combination of metrics for semantic
class induction. In Proceedings of the International
Workshop on Spoken Language Technology (SLT),
pages 86?89.
Ioannis Klasinas, Alexandros Potamianos, Elias Iosif,
Spiros Georgiladakis, and Gianluca Mameli. 2013.
Web data harvesting for speech understanding gram-
mar induction. In Proceedings of Interspeech, pages
2733?2737.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proceedings of the 42nd an-
nual meeting of the ACL, pages 470?477.
Helen M. Meng and Kai-chung Siu. 2002. Semi-
automatic acquisition of semantic structures for
understanding domain-specific natural language
queries. IEEE Transactions on Knowledge and Data
Engineering, 14(1):172?181.
Dmitrijs Milajevs and Matthew Purver. 2014. Inves-
tigating the contribution of distributional semantic
information for dialogue act classification. In Pro-
ceedings of the 2nd Workshop on Continuous Vec-
tor Space Models and their Compositionality, pages
40?47.
Teruhisa Misu and Tatsuya Kawahara. 2006. A boot-
strapping approach for developing language model
of new spoken dialogue systems by selecting web
texts. In Proceedings of Interspeech, pages 9?12.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting of the ACL, pages 311?318.
Andrew N. Pargellis, Eric Fosler-Lussier, Chin-Hui
Lee, Alexandros Potamianos, and Augustine Tsai.
2004. Auto-induced semantic classes. Speech Com-
munication, 43(3):183?203.
Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011.
Simple unsupervised grammar induction from raw
text with cascaded finite state models. In Proceed-
ings of the 49th annual meeting of the ACL, pages
1077?1086.
Portdial. 2014a. PortDial project, final
report on automatic grammar induction
and evaluation D3.3. Technical report,
https://sites.google.com/site/portdial2/deliverables-
publications.
Portdial. 2014b. PortDial project, free
data deliverable D3.2. Technical report,
https://sites.google.com/site/portdial2/deliverables-
publications.
Aarne Ranta. 2004. Grammatical framework: A type-
theoretical grammar formalism. Journal of Func-
tional Programming, 14(2):145?189.
Ruhi Sarikaya. 2008. Rapid bootstrapping of statisti-
cal spoken dialogue systems. Speech Communica-
tion, 50(7):580?593.
Abhinav Sethy, Shrikanth S. Narayanan, and Bhuvana
Ramabhadran. 2007. Data driven approach for lan-
guage model adaptation using stepwise relative en-
tropy minimization. In Proceedings of the Interna-
tional Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pages 177?180.
Ye-Yi Wang and Alex Acero. 2006. Rapid develop-
ment of spoken language understanding grammars.
Speech Communication, 48(3-4):390?416.
16
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 668?672,
Dublin, Ireland, August 23-24, 2014.
tucSage: Grammar Rule Induction for Spoken Dialogue Systems via
Probabilistic Candidate Selection
Arodami Chorianopoulou
?
, Georgia Athanasopoulou
?
, Elias Iosif
? ?
,
Ioannis Klasinas
?
, Alexandros Potamianos
?
?
School of ECE, Technical University of Crete, Chania 73100, Greece
?
School of ECE, National Technical University of Athens, Zografou 15780, Greece
?
?Athena? Research Center, Marousi 15125, Greece
{achorianopoulou,gathanasopoulou,iklasinas}@isc.tuc.gr
iosife@telecom.tuc.gr, apotam@gmail.com
Abstract
We describe the grammar induction sys-
tem for Spoken Dialogue Systems (SDS)
submitted to SemEval?14: Task 2. A sta-
tistical model is trained with a rich fea-
ture set and used for the selection of can-
didate rule fragments. Posterior probabil-
ities produced by the fragment selection
model are fused with estimates of phrase-
level similarity based on lexical and con-
textual information. Domain and language
portability are among the advantages of
the proposed system that was experimen-
tally validated for three thematically dif-
ferent domains in two languages.
1 Introduction
A critical task for Spoken Dialogue Systems
(SDS) is the understanding of the transcribed user
input, that utilizes an underlying domain grammar.
An obstacle to the rapid deployment of SDS to
new domains and languages is the time-consuming
development of grammars that require human ex-
pertise. Machine-assisted grammar induction has
been an open research area for decades (K. Lari
and S. Young, 1990; S. F. Chen, 1995) aiming
to lower this barrier. Induction algorithms can
be broadly distinguished into resource-based, e.g.,
(A. Ranta, 2004), and data-driven, e.g., (H. Meng
and K.-C. Siu, 2002). The main drawback of
the resource-based paradigm is the requirement of
pre-existing knowledge bases. This is addressed
by the data-driven paradigm that relies (mostly)
on plain corpora. SDS grammars are built by uti-
lizing low- and high-level rules. Low-level rules
This work is licenced under a Creative Commons Attri-
bution 4.0 International License. Page numbers and pro-
ceedings footer are added by the organizers. License de-
tails: http://creativecommons.org/licenses/
by/4.0/
are similar to gazetteers consisting of terminal en-
tries, e.g., list of city names. High-level rules can
be lexicalized as textual fragments (or chunks),
which are semantically defined on top of low-
level rules, e.g., ?depart from <City>?.
The data-driven induction of low-level rules is a
well-researched area enabled by various technolo-
gies including web harvesting for corpora creation
(Klasinas et al., 2013), term extraction (K. Frantzi
and S. Ananiadou, 1997), word-level similarity
computation (Pargellis et al., 2004) and cluster-
ing (E. Iosif and A. Potamianos, 2007). High-level
rule induction is a less researched area that poses
two main challenges: 1) the extraction and selec-
tion of salient candidate fragments from a corpus
that convey semantics relevant to the domain of in-
terests and 2) the organization of such fragments
(e.g., via clustering) according to their semantic
similarity. Despite the recent interest on phrase (J.
Mitchell and M. Lapata, 2010) and sentence simi-
larity, each respective problem remains open.
Next, our submission
1
for the Se-
mEval?14: Task2 is briefly described, which
constitutes a data-driven approach for inducing
high-level SDS grammar rules. At the system?s
core lies a statistical model for the selection of
textual fragments based on a rich set of features.
This set includes various lexical features, aug-
mented with statistics from n-gram language
models, as well as with heuristic features. The
candidate selection model posterior is fused
with a phrase-level semantic similarity metric.
Two different approaches are used for similarity
computation relying on the overlap of character
bigrams or context-based similarity according
to the distributional hypothesis of meaning.
The domain and language portability of the
proposed system is demonstrated by its successful
application across three different domains and
1
Please note that the last three authors of this submission
are among the organizers of this task.
668
two languages. All the four subtasks defined by
the organizers were completed with very good
performance that exceeds the baseline.
2 System Description
The basic functionality of the proposed system
is the mapping (assignment) of unknown textual
fragments into known high-level grammar rules.
Let E be the set of unknown fragments, while the
set of known rules is denoted byR. Each unknown
fragment f ?E is allowed to be mapped to a sin-
gle high-level rule r
s
?R, where 1? s? m and
m is the total number of rules in the grammar.
Figure 1: Overview of system architecture.
The system consists of three major components as
shown at the system architecture diagram in Fig.
1, specifically: 1) candidate selection: a set of
classifiers is built, one for each r
s
to select whether
f ? E is a candidate member of the specific rule
2
,
2) similarity computation between f and r
s
, and
3) mapping f to a high-level rule r
s
(denoted as
f 7? r
s
) according to the following model:
argmax
s
{p(r
s
|f)
w
S(f, r
s
)} : f 7? r
s
(1)
where p(r
s
|f) stands for the probability of f
belonging to rule r
s
and it is estimated via the
respective classifier. The similarity between
f and r
s
is denoted by S(f |r
s
), while w is
a fixed weight taking values in the interval
[0 ?). The fusion weight w controls the rela-
tive importance of the candidate selection and
semantic similarity modules, e.g., for w = 0
only the similarity metric S(f, r
s
) is used in the
decision. For example, consider the fragment f
?leaving <City>?. Also, assume two high-
level rules, namely, <ArrCity>={?arrive
2
The requirement for building a classifier for each gram-
mar rule is realistic for the case of SDS, especially for the typ-
ical iterative human-in-the-loop grammar development sce-
nario.
at <City>?,...} and <DepCity>=
{?depart <City>?,...}. According to (1)
f is mapped to the <DepCity> rule.
2.1 Candidate Selection
In this section, the features used for building the
candidate selection module for each r
s
? R are
briefly described. Given a pair (f ,r
s
) a two-class
statistical classification model that corresponds to
r
s
is used for estimating p(r
s
|f) in (1).
Definitions. A high-level rule r
s
can be con-
sidered as a set of fragments, e.g.,?depart
<City>?, ?leaving <City>?. For each
fragment there are two types of constituents,
namely, lexical (e.g., ?depart?,?leaving?)
and low-level rules (e.g., ?<City>?). The fol-
lowing features are extracted for r
s
considering its
respective fragments, as well as for f .
Shallow features. 1) the number of constituents
(i.e., tokens), 2) the count of lexical constituents
to the number of tokens, 3) the count of low-level
rules to the number of tokens, 4) the count of lex-
ical constituents that follow the right-most low-
level rule of the fragment, and 5) the count of low-
level rules that appear twice in a fragment.
Perplexity-based features. A fragment
?
f can
be represented as a sequence of tokens as
w
1
w
2
... w
z
. The perplexity of
?
f is defined as
PP (
?
f)=2
H(
?
f)
, where H(
?
f)=
1
z
log(p(
?
f)). p(
?
f)
stands for the probability of
?
f estimated using an
n-gram language model. Two PP values were
used as features computed for n=2, 3.
Features of lexical similarity. Four scores of lex-
ical similarity computed between f and r
s
were
used as features. Let N
s
denote the set of frag-
ments that are included in the training set of each
rule r
s
. The following metrics were employed
for computing the similarity between the unknown
fragment f and a fragment f
s
? N
s
: 1) the nor-
malized longest common subsequence (Stoilos et
al., 2005) denoted as S
C
, 2) the normalized over-
lap in character bigrams that is denoted as S
B
and
it is defined in (2), 3) a proposed variation of the
Levenshtein distance, S
L
, defined as S
L
(f, f
s
) =
l
1
?L(f,f
s
)
l
1
+d
, where l
1
and l
2
are the lengths (in char-
acters) of the lengthiest and the shortest fragment
between f and f
s
, respectively, while d= l
1
? l
2
.
L(.) stands for the Levenshtein distance (V. I. Lev-
enshtein, 1966; R. A. Wagner and M. J. Fischer,
1974). 4) if f and f
s
differ by one token exactly
S
L
is applied, otherwise their similarity is set to
0. Regarding S
C
and S
B
, the similarity between
669
f and r
s
was estimated as the maximum similarity
yielded when computing the similarities between
f and each f
s
?N
s
. For the rest metrics, the sim-
ilarity between f and r
s
was estimated by averag-
ing the |N
s
| similarities computed between f and
each f
s
?N
s
.
Heuristic features. Considering an unknown
fragment f and the set of training fragments N
s
corresponding to rule r
s
, in total nine features
were used: 1) the difference between the aver-
age length (in tokens) of fragments in N
s
and the
length of f , 2) the difference between the average
number of low-level rules in N
s
and the number
of low-level rules in f , 3) as 2) but considering
the lexical constituents instead of low-level rules,
4) the number of low-level rules shared between
N
s
and f , 5) as 4) but considering the lexical con-
stituents instead of low-level rules, 6) a boolean
function that equals 1 if f is a substring of at least
one f
s
? N
s
, 7) a boolean function that equals 1 if
f shares the same lexical constituents at least one
f
s
? N
s
, 8) a boolean function that equals 1 if f
is shorter by one token compared to any f
s
? N
s
,
9) a boolean function that equals 1 if f is lengthier
by one token compared to any f
s
? N
s
.
Selection. The aforementioned features are used
for building a binary classifier for each r
s
? R,
where 1 ? s ? m, for deciding whether f can
be regarded as a candidate member of r
s
or not.
Given an unknown fragment f these classifiers are
employed for estimating in total m probabilities
p(r
s
|f).
2.2 Similarity Metrics
Here, two types of similarity metrics are defined,
which are used for estimating S(f, r
s
) in (1).
String-based similarity. Consider two fragments
f
i
and f
j
whose sets of character bigrams are de-
noted as M
i
and M
j
, respectively. Also, M
min
=
min(|M
i
|, |M
j
|) and M
max
= max(|M
i
|, |M
j
|
). The similarity between f
i
and f
j
is based on
the overlap of their respective character bigrams
defined as (Jimenez et al., 2012):
S
B
(f
i
, f
j
) =
|M
i
?M
j
|
?M
max
+ (1? ?)M
min
, (2)
where 0??? 1, while, here we use ?=0.5. The
similarity between a fragment f and a rule r
s
is
computed by averaging the similarities computed
between f and each f
s
?N
s
.
Context-based similarity. This is a corpus-based
metric relying on the distributional hypothesis of
meaning suggesting that similarity of context im-
plies similarity of meaning (Z. Harris, 1954). A
contextual window of size 2K+1 words is cen-
tered on the fragment of interest f
i
and lexical
features are extracted. For every instance of f
i
in
the corpus the K words left and right of f
i
for-
mulate a feature vector v
i
. For a given value of K
the context-based semantic similarity between two
fragments, f
i
and f
j
, is computed as the cosine of
their feature vectors: S
K
(f
i
, f
j
) =
v
i
.v
j
||v
i
|| ||v
j
||
. The
elements of feature vectors can be weighted ac-
cording various schemes (E. Iosif and A. Potami-
anos, 2010), while, here we use a binary scheme.
The similarity between a fragment f and a rule
r
s
is computed by averaging the similarities com-
puted between f and each f
s
?N
s
.
2.3 Mapping of Unknown Fragments
The output of the described system is the mapping
of a fragment f to a single (i.e., one-to-one assign-
ment) high-level rule r
s
? R, where 1 ? s ? m.
This is achieved by applying (1). The p(r
s
|f)
probabilities were estimated as described in Sec-
tion 2.1. The S(f, r
s
) similarities were estimated
using either S
K
or S
B
defined in Section 2.2.
3 Datasets and Experiments
Datasets. The data was organized with respect to
three different domains: 1) air travel (flight book-
ing, car rental etc.), 2) tourism (information for
city guide), and 3) finance (currency exchange). In
total, there are four separate datasets: two datasets
for the air travel domain in English (EN) and
Greek (GR), one dataset for the tourism domain
in English, and one dataset for the finance domain
in English.
The number of high-level rules for each dataset
Domain #rules #train frag. #test frag.
Travel:EN 32 982 284
Travel:GR 35 956 324
Tourism:EN 24 1004 285
Finance:EN 9 136 37
Table 1: Number of rules and train/test fragments.
are shown in Table 1, along with the number
of fragments included in training and test data.
Experiments. Regarding the computation of
perplexity-based features (defined in Section 2.1)
the SRILM toolkit (A. Stolcke, 2002) was used.
The n-gram probabilities were estimated over a
corpus that was created by aggregating all the
670
valid fragments included in the training data.
For the computation of the context-based similar-
ity metric S
K
(defined in Section 2.2) a corpus
of web-harvested data was created for each do-
main/language. The context window size K was
Domain # sentences
Travel:EN 5721
Travel:GR 6359
Tourism:EN 829516
Finance:EN 168380
Table 2: Size of corpora used in S
K
metric.
set to 1. The size of the used corpora are presented
Table 2, while the process of corpus creation is
detailed in (Klasinas et al., 2013). The classifiers
used for the candidate selection module, described
in Section 2.1 were random forests with 50 trees
(L. Breiman, 2001).
4 Evaluation Metrics and Results
The proposed model defined by (1) was evaluated
in terms of weighted F-measure, (FM ). Initially,
we run our system using the training and develop-
ment set provided by the task organizers, in order
to tune the w and K parameters. The tuning was
conducted on the Travel English domain, while the
respective evaluation results are shown in Table 3
in terms of FM . We observe that the best re-
Weight w 0 1 50 500
FM 0.68 0.72 0.70 0.72
Table 3: Results for the tuning of w.
sults are achieved for w = 1 and w = 500. In
the case where w = 0 the rule mapping relies only
on the similarity metric. In addition, we exper-
imented with various values the context window
size K of the context-based similarity metric S
K
:
K = 1, 3, 7. For all values of K similar perfor-
mance was obtained (0.70). Given the aforemen-
Domains Baseline Run 1 Run 2 Run 3
Travel:EN 0.51 0.66 0.65 0.68
Travel:GR 0.26 0.52 0.49 0.49
Tourism:EN 0.87 0.86 0.85 0.86
Finance:EN 0.60 0.70 0.63 0.58
UA 0.56 0.69 0.66 0.65
WA 0.52 0.66 0.64 0.65
Table 4: Official results.
tioned tuning the following values were selected
for the official runs: w = 1, w = 500 and K = 1.
In total, three system runs were submitted:
Run 1. The character bigram similarity metric was
used, while w was set to 1.
Run 2. The context-based similarity metrics was
used with K = 1, while w was set to 1.
Run 3. The character bigram similarity metric was
used, while w was set to 500.
The results for the aforementioned runs, along
with the baseline performance are shown in Ta-
ble 4. An overview of the participating systems
suggests that our submission achieved the high-
est performance for almost all domains and lan-
guages. The weighted (WA) and unweighted (UA)
average across the 4 datasets are also presented,
where the weight depends on the number of rules
in the dataset. Using these measures, our main
run (Run 1) obtained the best results. We ob-
serve that the performance is consistently worse
for Runs 2 and 3, with the exception of the Travel
English dataset. Comparing the performance of
Runs 1 and 2, we observe that the character bigram
metric consistently outperforms the context-based
one. For individual datasets, our system underper-
forms for the Finance (in Run 3) and the Tourism
domain (in all Runs). For the case of the Finance
domain this may be attributed to the relatively lim-
ited training data.
5 Conclusions
We proposed a supervised grammar induction sys-
tem using the fusion of a grammar fragment se-
lection and similarity estimation modules. The
best configuration of our system was Run 1 which
achieved the highest performance compared to
other submissions, in almost all domains. To sum-
marize, 1) the selection module boost the sys-
tem?s performance significanlty, 2) the high per-
formance in different domains is a promising indi-
cator for domain and language portability. Future
work should involve the implementation of more
complex features for the candidate selection algo-
rithm and further investigation of phrase level sim-
ilarity metrics.
Acknowledgements
This work has been partially funded by the
projects: 1) SpeDial, and 2) PortDial, supported
by the EU Seventh Framework Programme (FP7),
with grant number 611396 and 296170, respec-
tively.
671
References
Elias Iosif and Alexandros Potamianos. 2010. Un-
supervised semantic similarity computation between
terms using web documents. IEEE Transactions on
Knowledge and Data Engineering, 22(11), pp. 1637-
1647.
Sergio Jimenez, Claudia Becerra and Alexander Gel-
bukh. 2012. Soft Cardinality: A parameterized sim-
ilarity function for text comparison. In Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics (*SEM), pp. 449-453
Ioannis Klasinas, Alexandros Potamianos, Elias Iosif,
Spyros Georgiladakis and Gianluka Mameli. 2013.
Web data harvesting for speech understanding
grammar induction. in Proceedings of the Inter-
speech.
Helen M. Meng and Kai-Chung Siu 2002. Semi-
automatic acquisition of semantic structures for
understanding domain-specific natural language
queries. IEEE Transactions on Knowledge and Data
Engineering, 14(1), pp. 172-181.
PortDial Project free data deliverable D3.1.
https://sites.google.com/site/portdial2/deliverables-
publication
Andreas Stolcke 2002 Srilm-an extensible language
modeling toolkit in Proceedings of the Interspeech
2002
Karim Lari and Steve J. Young 2002. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
4(1), pp. 35-56.
Stanley F. Chen 1995. Bayesian grammar induction
for language modeling. in Proceedings of the 33rd
annual meeting of ACL
Zellig Harris 1954. Distributional structure. Word,
10(23), pp. 146-162.
Rebecca Hwa 1999. Supervised grammar induction
using training data with limited constituent informa-
tion. in Proceedings of the 37th annual meeting of
ACL
Matthew Lease, Eugene Charniak, and Mark Johnson
2005. Parsing and its applications for conversa-
tional speech. in Proceedings of Acoustics, Speech,
and Signal Processing (ICASSP)
Vladimir I. Levenshtein 1966. Binary codes capable
of correcting deletions, insertions and reversals. in
Soviet physics doklady, 10(8), pp. 707-710.
Leo Breiman 2001. Random forests. in Machine
Learning, 45(1), pp. 5-32.
Dan Jurafsky and James H. Martin 2009. Speech
and language processing an introduction to natural
language processing, computational linguistics, and
speech. Pearson Education Inc
Giorgos Stoilos, Giorgos Stamou, and Stefanos Kollias
2005. A string metric for ontology alignment. in
The Semantic WebISWC, pp. 624637
Robert A. Wagner and Michael J. Fisher 1974. The
string-to-string correction problem. Journal of the
ACM (JACM), 21(1), pp. 168-173
Katerina Frantzi and Sophia Ananiadou 1997. Au-
tomatic term recognition using contextual cues. in
Proceedings of International Joint Conferences on
Artificial Intelligence
Elias Iosif and Alexandros Potamianos 2007. A soft-
clustering algorithm for automatic induction of se-
mantic classes. in Proceedings of Interspeech
Jeffrey Mitchell and Mirela Lapata 2010. Composi-
tion in distributional models of semantics. Cognitive
Science, 34(8):1388-1429.
Ye-Yi Wang and Alex Acero 2006. Rapid develop-
ment of spoken language understanding grammars.
Speech Communication, 48(3), pp. 360-416.
Eric Brill 1992. A simple rule-based part of speech
tagger. in Proceedings of the workshop on Speech
and Natural Language
Alexander Clark 2001. Unsupervised induction
of stochastic context-free grammars using distribu-
tional clustering. in Proceedings of the 2001 work-
shop on Computational Natural Language Learning
Benjamin Snyder, Tahira Naseem, and Regina Barzilay
2009. Unsupervised multilingual grammar induc-
tion. in Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL
Aarne Ranta 2009. Grammatical framework: A type-
theoretical grammar formalism. Journal of Func-
tional Programming: 14(2), pp. 145-189
Andrew Pargellis, Eric Fosler-Lussier, Chin Hui Lee,
Alexandros Potamianos and Augustine Tsai 2009.
Auto-induced Semantic Classes. Speech Communi-
cation: 43(3), pp. 183-203
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre 2012. SemEval-2012 Task 6: A
Pilot on Semantic Textual Similarity. in Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics (*Sem), pp. 385-393
672
Proceedings of the 3rd Workshop on Computational Linguistics for Literature (CLfL) @ EACL 2014, pages 40?49,
Gothenburg, Sweden, April 27, 2014. c?2014 Association for Computational Linguistics
From Speaker Identification to Affective Analysis:
A Multi-Step System for Analyzing Children?s Stories
Elias Iosif? and Taniya Mishra?
? School of ECE, Technical University of Crete, Chania 73100, Greece
? AT&T Labs, 33 Thomas Street, New York, NY 10007, USA
iosife@telecom.tuc.gr, taniya@research.att.com
Abstract
We propose a multi-step system for the
analysis of children?s stories that is in-
tended to be part of a larger text-to-speech-
based storytelling system. A hybrid ap-
proach is adopted, where pattern-based
and statistical methods are used along with
utilization of external knowledge sources.
This system performs the following story
analysis tasks: identification of charac-
ters in each story; attribution of quotes
to specific story characters; identification
of character age, gender and other salient
personality attributes; and finally, affective
analysis of the quoted material. The differ-
ent types of analyses were evaluated using
several datasets. For the quote attribution,
as well as for the gender and age estima-
tion, substantial improvement over base-
line was realized, whereas results for per-
sonality attribute estimation and valence
estimation are more modest.
1 Introduction
Children love listening to stories. Listening to
stories ? read or narrated ? has been shown
to be positively correlated with children?s linguis-
tic and intellectual development (Natsiopoulou et
al., 2006). Shared story reading with parents or
teachers helps children to learn about vocabulary,
syntax and phonology, and to develop narrative
comprehension and awareness of the concepts of
print, all of which are linked to developing reading
and writing skills (National Early Literacy Panel
2008). While acknowledging that the parental
role in storytelling is irreplaceable, we consider
text-to-speech (TTS) enabled storytelling systems
(Rusko et al., 2013; Zhang et al., 2003; Theune
et al., 2006) to be aligned with the class of child-
oriented applications that aim to aid learning.
For a TTS-based digital storytelling system to
successfully create an experience as engaging as
human storytelling, the underlying speech synthe-
sis system has to narrate the story in a ?story-
telling speech style? (Theune et al., 2006), gen-
erate dialogs uttered by different characters using
synthetic voices appropriate for each character?s
gender, age and personality (Greene et al., 2012),
and express quotes demonstrating emotions such
as sadness, fear, happiness, anger and surprise
(Alm, 2008) with realistic expression (Murray and
Arnott, 2008). However, before any of the afore-
mentioned requirements ? all related to speech
generation ? can be met, the text of the story has
to be analyzed to identify which portions of the
text should be rendered by the narrator and which
by each of the characters in the story, who are the
different characters in the story, what is each char-
acter?s gender, age, or other salient personality at-
tributes that may influence the voice assigned to
that character, and what is the expressed affect in
each of the character quotes.
Each of these text analysis tasks has been ap-
proached in past work (as described in our Re-
lated Works section). However, there appears to
be no single story analysis system that performs
all four of these tasks, which can be pipelined with
one of the many currently available text-to-speech
systems to build a TTS-based storyteller system.
Without such a story analysis system, it will not be
possible to develop an engaging and lively digital
storyteller system, despite the prevalence of sev-
eral mature TTS systems.
40
In this paper, we present a multi-step text analy-
sis system for analyzing children?s stories that per-
forms all four analysis tasks: (i) Character Identi-
fication, i.e., identifying the different characters in
the story, (ii) Quote Attribution, i.e., identifying
which portions of the text should be rendered by
the narrator versus by particular characters in the
story, (iii) Character Attribute Identification, i.e.,
identifying each character?s gender, age, or salient
personality attributes that may influence the voice
that the speech synthesis system assigns to each
character, and (iv) Affective Analysis, i.e., esti-
mating the affect of the character quotes.
This story analysis system was developed to
be part of a larger TTS-based storyteller system
aimed at children. As a result, the data used for
developing the computational models or rules in
each step of our system were obtained from chil-
dren?s stories. A majority of children?s stories
are short. They often contain multiple characters,
each with different personalities, genders, age,
ethnicities, etc., with some characters even be-
ing anthropomorphic, e.g., the singing candlestick
or the talking teapot. In addition, there are sev-
eral prototypical templates characterizing the main
characters in the story (Rusko et al., 2013). How-
ever, character development is limited in these sto-
ries due to the shorter length of text. Overall,
children?s stories can be regarded as a parsimo-
nious yet fertile framework for developing compu-
tational models for literature analysis in general.
2 Related Work
Elson and McKeown (2010) used rule-based and
statistical learning approaches to identify candi-
date characters and attribute each quote to the most
likely speaker. Two broad approaches for the iden-
tification of story characters were followed: (i)
named entity recognition, and (ii) identification
of character nominals, e.g., ?her grandma?, using
syntactic patterns. A long list of heuristics for
character identification is proposed in (Mamede
and Chaleira, 2004). He et al. (2013) use a su-
pervised machine learning approach to address the
same problem, though many of their preliminary
steps and input features are similar to those used in
(Elson and McKeown, 2010). Our character iden-
tification and quote attribution is based on syntac-
tic and heuristic rules that is motivated by each of
these works.
There are two interesting sub-problems related
to quote attribution. First is the problem of iden-
tifying anaphoric speakers, i.e., in the utterance
?Hello?, he said, which character is referred to by
the pronoun he? This problem is addressed in (El-
son and McKeown, 2010) and (He et al., 2013) but
not in (Mamede and Chaleira, 2004). The second
problem is resolving utterance chains with implicit
speakers. Elson and McKeown (2010) describe
and address two basic types of utterance chains: (i)
one-character chains, and (ii) intertwined chains.
In these chains of utterances, the speaker is not
explicitly mentioned because the author relies on
the shared understanding with the reader that adja-
cent pieces of quoted speech are not independent
(Zhang et al., 2003; Elson and McKeown, 2010).
They are either a continuation of the same charac-
ter?s speech (one-character chains) or a dialogue
between the two characters (intertwined chains).
In (Zhang et al., 2003), the quote-identification
module detects whether a piece of quoted speech
is a new quote (NEW), spoken by a speaker dif-
ferent from the previous speaker, or a continuation
quote (CONT) spoken by the same speaker as that
of the previous quote. He et al. (2013) also iden-
tified similar chains of utterances and addressed
their attribution to characters using a model-based
approach. In this work, we address both sub-
problems, namely, anaphoric speaker and implicit
speaker identification.
Cabral et al. (2006) have shown that assign-
ing an appropriate voice for a character in a digi-
tal storyteller system is significant for understand-
ing a story, perceiving affective content, perceiv-
ing the voice as credible, and overall listener sat-
isfaction. Greene et al. (2012) have shown that
the appropriateness of the voice assigned to a syn-
thetic character is strongly related to knowing the
gender, age and other salient personality attributes
of the character. Given this, we have developed
rule-based, machine-learning-based and resource-
based approaches for estimation of character gen-
der, age and salient personality attributes. In con-
trast, the majority of past works on the analysis of
children stories for TTS-based storytelling is lim-
ited to the attribution of quotes to speakers, though
studies that focused on anaphoric speaker iden-
tification have also approached character gender
estimation such as (Elson and McKeown, 2010)
and (He et al., 2013). The utilization of available
resources containing associations between person
names and gender was followed in (Elson and
41
McKeown, 2010). In (He et al., 2013), associ-
ations between characters and their gender were
performed using anaphora rules (Mitkov, 2002).
There is of course a significant body of work
from other research areas that are related to the
estimation of character attributes, similar to what
we have attempted in our work. Several shal-
low linguistic features were proposed in (Schler
et al., 2006) for gender identification, applied to
the identification of users in social media. Several
socio-linguistic features were proposed in (Rao et
al., 2010) for estimating the age and gender of
Twitter users. The identification of personality at-
tributes from text is often motivated by psycho-
logical models. In (Celli, 2012), a list of linguis-
tic features were used for the creation of character
models in terms of the the Big Five personality di-
mensions (Norman, 1963).
Analysis of text to estimate affect or sentiment
is a relatively recent research topic that has at-
tracted great interest, as reflected by a series of
shared evaluation tasks, e.g., analysis of news
headlines (Strapparava and Mihalcea, 2007) and
tweets (Nakov et al., 2013). Relevant applications
deal with numerous domains such as blogs (Balog
et al., 2006), news stories (Lloyd et al., 2005), and
product reviews (Hu and Liu, 2004). In (Turney
and Littman, 2002), the affective ratings of un-
known words were predicted using the affective
ratings for a small set of words (seeds) and the se-
mantic relatedness between the unknown and the
seed words. An example of sentence-level analy-
sis was proposed in (Malandrakis et al., 2013). In
(Alm et al., 2005) and (Alm, 2008), linguistic fea-
tures were used for affect analysis in fairy tales. In
our work, we employ a feature set similar to that
in (Alm et al., 2005). We deal with the prediction
of three basic affective labels which are adequate
for the intended application (i.e., storytelling sys-
tem), while in (Alm, 2008) more fine-grained pre-
dictions are considered.
The integration of various types of analysis con-
stitutes the distinguishing character of our work.
3 Overview of System Architecture
The system consists of several sub-systems that
are linked in a pipeline. The input to the system
is simply the text of a story with no additional
annotation. The story analysis is performed se-
quentially, with each sub-system extracting spe-
cific information needed to perform the four anal-
ysis tasks laid out in this paper.
3.1 Linguistic Preprocessing
The first step is linguistic pre-processing of the
stories. This includes (i) tokenization, (ii) sen-
tence splitting and identification of paragraph
boundaries, (iii) part-of-speech (POS) tagging,
(iv) lemmatization, (v) named entity recognition,
(vi) dependency parsing, and (vii) co-reference
analysis. These sub-tasks ? except task (ii) ?
were performed using the Stanford CoreNLP suite
of tools (CoreNLP, 2014). Sentence splitting and
identification of paragraph boundaries was per-
formed using a splitter developed by Piao (2014).
Linguistic information extracted by this analysis is
exploited by the subsequent parts of the pipeline.
3.2 Identification of Story Characters
The second step is identifying candidate charac-
ters (i.e., entities) that appear in the stories under
analysis. A story character is not necessarily a
story speaker. A character may appear in the story
but may not have any quote associated with him
and hence, is not a speaker. Characters in chil-
dren?s stories can either be human or non-human
entities, i.e., animals and non-living objects, ex-
hibiting anthropomorphic traits. The interactions
among characters can either be human-to-human
or human-to-non-human interactions.
We used two approaches for identifying story
characters motivated by (Elson and McKeown,
2010): 1) named entity recognition was used for
identifying proper names, e.g., ?Hansel?, 2) a
set of part-of-speech patterns was used for the
extraction of human and non-human characters
that were not represented by proper names, e.g.,
?wolf?. The used patterns are: 1) (DT|CD)
(NN|NNS), 2) DT JJ (NN|NNS), 3) NN POS
(NN|NNS), and 4) PRP$ JJ (NN|NNS).
These POS-based patterns are quite generic, al-
lowing for the creation of large sets of characters.
In order to restrict the characters, world knowl-
edge was incorporated through the use of Word-
Net (Fellbaum, 2005). A similar approach was
also followed in (Elson and McKeown, 2010). For
each candidate character the hierarchy of its hy-
pernyms was traversed up to the root. Regarding
polysemous characters the first two senses were
considered. A character was retained if any of its
hypernyms was found to fall into certain types of
WordNet concepts: person, animal, plant, artifact,
spiritual being, physical entity.
42
3.3 Quote Attribution & Speaker
Identification
Here the goal is to attribute (or assign) each quote
to a specific story character from the set identified
in the previous step. The identification of quotes
in the story is based on a simple pattern-based ap-
proach: the quote boundaries are signified by the
respective symbols, e.g., ? and ?. The pattern is
applied at the sentence level.
The quotes are not modeled as NEW/CONT as
in (Zhang et al., 2003), however, we adopt a more
sophisticated approach for the quote attribution.
Three types of attribution are possible in our sys-
tem: 1) explicit mention of speakers, e.g., ?Done!?
said Hans, merrily, 2) anaphoric mention of speak-
ers, e.g., ?How happy am I!? cried he, 3) sequence
of quotes, e.g., ?And where did you get the pig??
. . . ?I gave a horse for it.?. In the first type of attri-
bution, the speaker is explicitly mentioned in the
vicinity of the quote. This is also true for the sec-
ond type, however, a pronominal anaphora is used
to refer to the the speaker. The first two attribution
types are characterized by the presence of ?within-
quote? (e.g., ?Done!?) and ?out-of-quote? (e.g.,
?said Hans, merrily.?) content. This is not the
case for the third attribution type for which only
?in-quote? content is available. We refer to such
quotes as ?pure? quotes. Each attribution type is
detailed below.
Preliminary filtering of characters. Before
quote-attribution is performed, the list of story
characters is pruned by identifying the characters
that are ?passively? associated with speech verbs
(SV). This is applied at the sentence level. Some
examples of speech verbs are: said, responds, sing,
etc. For instance, in ?. . . Hans was told . . . ?,
?Hans? is a passive character. The passive char-
acters were identified via the following relations
extracted by dependency parsing: nsubjpass
(passive nominal subject) and pobj (object of a
preposition). Given a sentence that includes one
or more quotes, the respective passive characters
were not considered as candidate speakers. Some
other criteria for pruning of list of characters to
identify candidate speakers are presented in Sec-
tion 4.2 (see the three schemes for Tasks 1-2).
Explicit mention of speakers. Several syntac-
tic patterns were applied to associate quotes with
explicit mention of speakers in their vicinity to
characters from the pruned list of story charac-
ters. These patterns were developed around SV.
In the example above, ?Hans? is associated with
the quote ?Done!? via the SV ?said?. Variations
of the following basic patterns (Elson and McKe-
own, 2010) were used: 1) QT SV CH, 2) QT CH
SV, and 3) CH SV QT, where QT denotes a quote
boundary and CH stands for a story character. For
example, a variation of the first pattern is QT SV
the? CH, where ? stands for zero or one oc-
currence of ?the?.
A limitation of the aforementioned patterns
is that they capture associations when the CH
and SV occur in close textual distance. As
a result, distant associations are missed, e.g.,
?Hans stood looking on for a while, and at last
said, ? You must . . . ??. In order to address
this distant association issue, we examined the
collapsed-ccprocessed-dependencies
output besides the basic-dependenciesout-
put of the Stanford CoreNLP dependency engine
(de Marneffe and Manning, 2012). The former
captures more distant relations compared to
the latter. We specifically extract the character
reference CH either from the dependency relation
nsubj, which links a speech verb SV with a CH
that is the syntactic subject of a clause, or from the
dependency relation dobj, which links a SV with
a CH that is the direct object of the speech verb,
across a conjunct (e.g., and). A similar approach
was used in (He et al., 2013).
Anaphoric mention of speakers. The same
procedure was followed as in the case of the ex-
plicit mentions of speakers described above. The
difference is that CH included the following pro-
nouns: ?he?, ?she?, ?they?, ?himself?, ?herself?,
and ?themselves?. After associating a pronoun
with a quote, the quote was attributed to a story
character via co-reference resolution. This was
done using the co-reference analysis performed
by CoreNLP. If a pronominal anaphora was not
resolved by the CoreNLP analysis, the follow-
ing heuristic was adopted. The previous n para-
graphs1 were searched and the pronoun under in-
vestigation was mapped to the closest (in terms
of textual proximity) story character that had the
same gender as the pronoun (see Section 3.4.1 re-
garding gender estimation). During the paragraph
search, anaphoric mentions were also taken into
consideration followed by co-reference resolution.
Despite the above approaches, it is possible to
have non-attributed quotes. In such cases, the fol-
1For the reported results n was set to 5.
43
lowing procedure is followed for those story sen-
tences that: (i) do not constitute ?pure? quotes
(i.e., consist of ?in-quote? and ?out-of-quote? con-
tent), and (ii) include at least one ?out-of-quote?
SV: 1) all the characters (as well as pronouns) that
occur within the ?out-of-quote? content are aggre-
gated and serve as valid candidates for attribution,
2) if multiple characters and pronouns exist, then
they are mapped (if possible) via co-reference res-
olution in order to narrow down the list of attri-
bution candidates, and 3) the quote is attributed
to the nearest quote character (or pronoun). For
the computation of the textual distance both quote
boundaries (i.e., start and end) are considered. If
the quote is attributed to a pronoun that was not
mapped to any character, then co-reference reso-
lution is applied.
Sequence of ?pure? quotes. Sentences that
are ?pure? quotes (i.e., include ?in-quote? con-
tent only) are not attributed to any story charac-
ter via the last two attribution methods. ?Pure?
quotes are attributed as follows: The sentences
are parsed sequentially starting from the begin-
ning of the story. Each time a character is encoun-
tered within a sentence, it is pushed into a ?bag-
of-characters?. This is done until a non-attributed
?pure? quote is found. At this point we assume
that the candidate speakers for the current (and
next) ?pure? quote are included within the ?bag-
of-characters?. This is based on the hypothesis
that the author ?introduces? the speakers before
their utterances. The subsequent ?pure? quotes are
examined in order to spot any included characters.
Such characters are regarded as ?good? candidates
enabling the pruning of the list of candidate speak-
ers. The goal is to end up with exactly two candi-
date speakers for a back and forth dialogue. Then,
the initiating speaker is identified by taking into
account the order of names mentioned within the
quote. Then, the quote attribution is performed in
an alternating fashion. For example, consider a
sequence of four non-attributed ?pure? quotes and
a bag of two2 candidate speakers, s
i
and s
j
. If s
i
was identified as the initiating speaker, then the 1st
and the 3th quote are attributed to it, while the 2nd
and the 4th quote are attributed to s
j
. Finally, the
?bag-of-characters? is reset, and the same process
is repeated for the rest of the story.
Identification of speakers. The speakers for a
2If more than two candidates exist, then the system gives
ambiguous attributions, i.e., multiple speakers for one quote.
given story are identified by selecting those char-
acters that were attributed at least one quote.
3.4 Gender, Age and Personality Attributes
The next three steps in our system involve estima-
tion of the (i) gender, (ii) age, and (iii) personality
attributes for the identified speakers.
3.4.1 Gender Estimation
We used a hybrid approach for estimating the gen-
der of the story characters. This is applied to char-
acters (rather than only speakers) because the gen-
der information is exploited during the attribution
of quotes (see Section 3.3). The characterization
?hybrid? refers to the fusion of two different types
of information: (i) linguistic information extracted
from the story under analysis, and (ii) information
taken from external resources that do not depend
on the analyzed story. Regarding the story-specific
information, the associations between characters
and third person pronouns (identified via anaphora
resolution) were counted. The counts were used in
order to estimate the gender probability.
The story-independent resources that we used
are: (a) the U.S. Social Security Administration
baby name database (Security, 2014), in which
person names are linked with gender and (b) a
large name-gender association list developed us-
ing a corpus-based bootstrapping approach, which
even included the estimated gender for non-person
entities (Bergsma and Lin, 2006). For each entity
included in (b) a numerical estimate is provided
for each gender. As in the case of story-specific in-
formation, those estimates were utilized for com-
puting the gender probability. Using the above in-
formation the following procedure was followed
for each character: The external resource (a) was
used when the character name occurred in it. Oth-
erwise, the information from the external resource
(b) and the story-specific information was taken
into account. If the speaker was covered by both
types of information, the respective gender prob-
abilities were compared and the gender was esti-
mated to be the one corresponding to the high-
est probability. If the character was not covered
by the story-specific information, the external re-
source (b) was used.
3.4.2 Age Estimation
We used a machine-learning based approach for
age estimation. The used features are presented in
Table 1, while they were extracted from speaker
44
quotes, based on the assumption that speakers of
different ages use language differently. The
No. Description
1 count of . , ;
2 count of ,
3 count of !
4 count of 1st person singular pronouns
5 count of negative particles
6 count of numbers
7 count of prepositions
8 count of pronouns
9 count of ?
10 count of tokens longer than 6 letters
11 count of 1st pers. (sing. & plur.) pronouns
12 count of quote tokens
13 count of 1st person plural pronouns
14 count of 2nd person singular pronouns
15 count of quote positive words
16 count of quote negative words
17 count of nouns
18 count of verbs
19 count of adjectives
20 count of adverbs
21 up to 3-grams extracted from quote
Table 1: Common feature set.
development of this feature set was inspired by
(Celli, 2012) and (Alm et al., 2005). All fea-
tures were extracted from the lemmatized form of
quotes. Also, all feature counts (except Feature
21) were normalized by Feature 12. For com-
puting the counts of positive and negative words
(Feature 15 and 16) we used the General Inquirer
database (Stone et al., 1966). Feature 21 stands
for n-grams (up to 3-grams) extracted from the
speaker quotes. Two different schemes were fol-
lowed for extracting this feature: (i) using the
quote as-is, i.e., its lexical form, and (ii) using the
part-of-speech tags of quote. So, two slightly dif-
ferent feature sets were defined: 1) ?lex?: No.1-20
+ lexical form for No.21, 2) ?pos?: No.1-20 + POS
tags for No.21
3.4.3 Estimation of Personality Attributes
A machine-learning based approach was also used
for personality attribute estimation. For estimat-
ing the personality attributes of story speakers, the
linguistic feature set (see Table 1) used in the task
for age estimation was used again . Again our ap-
proach was based on the assumption that words
people use reflect their personality, and the latter
can be estimated by these linguistic features.
3.5 Affective Analysis
The last step of our system is the estimation of
the affective content of stories. The analysis is
performed for each identified quote. The features
presented in Table 1 are extracted for each quote
and affect is estimated using a machine-learning
model, based on the assumption that such features
serve as cues for revealing the underlying affective
content (Alm et al., 2005; Alm, 2008).
4 Experiments and Evaluation
Here we present the experimental evaluation of
our system in performing the following tasks: 1)
speaker-to-quote attribution, 2) gender estimation,
3) age estimation, 4) identification of personality
attributes, and 5) affective analysis of stories.
4.1 Datasets Used
The datasets used for our experiments along with
the related tasks are presented in Table 2.
No. Task Type of dataset
1 Quote attribution STORIES
2 Gender estimation STORIES
3 Age estimation QUOTES(1,2)
4 Personality attrib. QUOTES(3,4)
5 Affective analysis STORY-AFFECT
Table 2: Experiment datasets and related tasks.
Tasks 1-2. For the first two tasks (quote-to-
speaker attribution, and gender estimation) we
used a dataset (STORIES) consisting of 17 chil-
dren stories selected from Project Gutenberg3 .
This set of stories includes 98 unique speakers
with 554 quotes assigned to them. The average
number of sentences and quotes per story is 61.8
and 32.5, respectively. The average sentence and
quote length is 30.4 and 29.0 tokens, respectively.
Each speaker was attributed 5.7 quotes on aver-
age. Ground truth annotation, which involved as-
signing quotes to speakers and labeling gender,
was performed by one4 annotator. The follow-
ing ground truth labels were used to mark gender:
?male?, ?female?, and ?plural?.
3www.telecom.tuc.gr/
?
iosife/chst.html
4Due to the limited ambiguity of the task, the availability
of a single annotator was considered acceptable.
45
Task 3. Evaluation of the age estimation task was
performed with respect to two different (propri-
etary) datasets QUOTES1 and QUOTES2. These
datasets consisted of individual quotes assigned to
popular children?s story characters. The dataset
QUOTES1 consisted of 6361 quotes assigned to
69 unique speakers. The average quote length
equals 7.6 tokens, while each speaker was at-
tributed 141.4 quotes on average. The dataset
QUOTES2 consisted of 23605 quotes assigned to
262 unique speakers. The average quote length
equals 8.3 tokens, while each speaker was at-
tributed 142.6 quotes on average. For ground truth
annotation, four annotators were employed. The
annotators were asked to use the following age
labels: ?child? (0?15 years old), ?young adult?
(16?35 y.o.), ?middle-aged? (36?55 y.o.), and ?el-
derly? (56? y.o.). The age of each character was
inferred by the annotators either based on personal
knowledge of these stories or by consulting pub-
licly available sources online. The inter-annotator
agreement equals to 70%.
Task 4. To evaluate system performance on Task
4, two datasets QUOTES3 and QUOTES4, con-
sisting of individual quotes assigned to popular
children?s story characters, were used. The set
QUOTES3 consisted of 68 individual characters
and QUOTES4 consisted of 328 individual charac-
ters. The ground truth assignment, assigning each
character with personality attributes, was extracted
from a free, public collaborative wiki (Wiki,
2014). Since the wiki format allows people to add
or edit information, we considered the personality
attributes extracted from this wiki to be the aver-
age ?crowd?s opinion? of these characters. Of the
open-ended list of attributes that were used to de-
scribe the characters, in this task we attempted to
extract the following salient personality attributes:
?beautiful?, ?brave?, ?cowardly?, ?evil?, ?feisty?,
?greedy?, ?handsome?, ?kind?, ?loving?, ?loyal?,
?motherly?, ?optimistic?, ?spunky?, ?sweet?, and
?wise?. The pseudo-attribute ?none? was used
when a character was not described with any of
those aforementioned attributes.
Task 5. An annotated dataset, referred to as
STORY-AFFECT in this paper, consisting of 176
stories was used. Each story sentence (regard-
less if quotes were included or not) was anno-
tated regarding primary emotions and mood us-
ing the following labels: ?angry? (AN), ?dis-
gusted? (DI), ?fearful? (FE), ?happy? (HA), ?neu-
tral? (NE), ?sad? (SA), ?positive surprise? (SU+),
and ?negative surprise? (SU?). Overall, two anno-
tators were employed, while each annotator pro-
vided two annotations: one for emotion and one
for mood. More details about this dataset are pro-
vided in (Alm, 2008).
Instead of using the aforementioned emo-
tions/moods as annotated, we adopted a 3-class
scheme for sentence affect (valence): ?negative?,
?neutral?, and ?positive?. In order to align the
existing annotations to our three-class scheme the
following mapping5 was adopted: (i) AN, DI, FE,
SA were mapped to negative affect, (ii) NE was
mapped to neutral affect, and (iii) HA was mapped
to positive affect. Given the proposed mapping,
we retained those sentences (in total 11018) that
exhibited at least 75% annotation agreement.
4.2 Evaluation Results
The evaluation results for the aforementioned
tasks are presented below.
Tasks 1-2. The quote-to-speaker attribution was
evaluated in terms of precision (AT
p
), while the
estimation of speakers? gender was evaluated in
terms of precision (G
p
) and recall (G
r
). Note that
G
p
includes both types of errors: (i) erroneous age
estimation, and (ii) estimations for story charac-
ters that are not true speakers. In order to exclude
the second type of error, the precision of gender
estimation was also computed for only the true
story speaker identified by the system (G?
p
). For
Speaker filter. AT
p
G
p
G
r
G
?
p
Baseline 0.010 0.333
10 stories (subset of dataset)
Scheme 1 0.833 0.780 0.672 0.929
Scheme 2 0.868 0.710 0.759 0.917
Scheme 3 0.835 0.710 0.759 0.917
17 stories (full dataset)
Scheme 2 0.845 0.688 0.733 0.892
Table 3: Quote attribution and gender estimation.
a subset of the STORIES dataset that included 10
stories, the following schemes were used for filter-
ing of candidate speakers: (i) Scheme 1: all speak-
ers linked with speech verbs, (ii) Scheme 2: speak-
ers, who are persons or animals or spiritual entities
according to their first WordNet sense, linked with
speech verbs , and (iii) Scheme 3: as Scheme 2,
5SU+/? were excluded for simplicity.
46
but the first two WordNet senses were considered.
For the full STORIES dataset (17 stories) Scheme
2 was used. The results are presented in Table 3 in-
cluding the weighted averages of precision and re-
call. Using random guesses, the baseline precision
is 0.010 and 0.333 for quote-to-speaker attribution
and gender estimation, respectively. For the subset
of 10 stories, the highest speaker-to-quote attribu-
tion attribution is obtained by Scheme 2. When
this scheme is applied over the entire dataset, sub-
stantially high6 precision (0.892) is achieved in the
estimation of gender of true story speakers.
Task 3. For the estimation of age using quote-
based features, a boosting approach was fol-
lowed using BoosTexter (Schapire and Singer,
2000). For evaluation, 10-fold cross valida-
Dataset Relaxed Exact
lex pos lex pos
Baseline 0.625 0.250
QUOTES1 0.869 0.883 0.445 0.373
QUOTES2 0.877 0.831 0.450 0.435
BOTH 0.886 0.858 0.464 0.383
Table 4: Age estimation: average accuracy.
tion (10FCV) was used for the QUOTES1 and
QUOTES2 datasets for the ?lex? and ?pos? fea-
ture sets. The results are reported in Table 4 in
terms of average classification accuracy. In this
table, BOTH refers to the datasets QUOTES1 and
QUOTES2 combined together. The evaluation
was performed according to two schemes: (i) ?re-
laxed match?: the prediction is considered as cor-
rect even if it deviates one class from the true one,
e.g., ?child? and ?middle-aged? considered as cor-
rect for ?young adult?, and (ii) ?exact match?: the
prediction should exactly match the true label. The
relaxed scheme was motivated by the nature of in-
tended application (storytelling system) for which
such errors are tolerable. For the exact match
scheme, the obtained performance is higher7 than
the baseline (random guess) that equals to 0.250.
The accuracy for the relaxed scheme is quite high,
i.e., greater than 0.85 for almost all cases. On aver-
age, the ?lex? feature set appears to yield slightly
higher performance than the ?pos? set.
Task 4. The personality attributes were estimated
using BoosTexter fed with the ?lex? feature set.
10FCV was used for evaluation, while the aver-
6Statistically significant at 95% lev. (t-test wrt baseline).
7Statistically significant at 95% lev. (t-test wrt baseline).
age accuracy was computed by taking into account
the top five attributes predicted for each charac-
ter. The baseline accuracy equals 0.31 given that
random guesses are used. Moderate performance
was achieved for the QUOTES3 and QUOTES4
datasets, 0.426 and 0.411, respectively.
Task 5. The affect of story sentences was esti-
mated via BoosTexter using the ?lex? and ?pos?
feature sets. As in the previous two tasks 10FCV
was applied for evaluation purposes. Using ran-
dom guesses, the baseline accuracy is 0.33. The
average accuracy for the ?lex? and ?pos? feature
sets is 0.838 and 0.658, respectively8 . It is clear
that the use of the ?lex? set outperforms the results
yielded by the ?pos? set.
5 Conclusions and Future Directions
In this paper, we described the development of a
multi-step system aimed for story analysis with
particular emphasis on analyzing children?s sto-
ries. The core idea was the integration of sev-
eral systems into a single pipelined system. The
proposed methodology has a strong hybrid char-
acter in that it employs different approaches that
range from pattern-based to machine learning-
based to the incorporation of external knowledge
resources. Going beyond the usual task of works
in this genre, i.e., speaker-to-quote attribution, the
proposed system also supports the estimation of
speaker-oriented attributes and affect estimation.
Very promising results were obtained for quote at-
tribution and estimation of speaker gender, as well
as for age assuming an application-depended error
tolerance. The estimation of personality attributes
and the affective analysis of story sentences re-
main open research problems, while the results are
more modest especially for the former task.
In the next phase of our work, we hope to im-
prove and generalize each individual component
of the proposed system. The most challenging as-
pects of the system, dealing with personality at-
tributes and affective analysis, will be further in-
vestigated. Towards this task, psychological mod-
els, e.g., the Big Five model, can provide useful
theoretical and empirical findings. Last but not
least, the proposed system will be evaluated within
the framework of a digital storytelling application
including metrics related with user experience.
8Statistically significant at 90% lev. (t-test wrt baseline).
47
References
C. O. Alm, D. Roth, and R. Sproat. 2005. Emotions
from text: Machine learning for text-based emotion
prediction. In Proc. of Conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, pages 579?586.
C. O. Alm. 2008. Affect in Text and Speech. Ph.D.
thesis, University of Illinois at Urbana-Champaign.
K. Balog, G. Mishne, and M. de Rijke. 2006. Why
are they excited? identifying and explaining spikes
in blog mood levels. In Proc. 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 207?210.
S. Bergsma and D. Lin. 2006. Bootstrapping path-
based pronoun resolution. In Proc. of Conference
on Computational Lingustics / Association for Com-
putational Linguistics, pages 33?40.
J. Cabral, L. Oliveira, G. Raimundo, and A. Paiva.
2006. What voice do we expect from a synthetic
character? In Proceedings of SPECOM, pages 536?
539.
F. Celli. 2012. Unsupervised personality recognition
for social network sites. In Proc. of Sixth Interna-
tional Conference on Digital Society.
CoreNLP. 2014. Stanford CoreNLP tool.
http://nlp.stanford.edu/software/
corenlp.shtml.
M.-C. de Marneffe and C. D. Manning. 2012. Stanford
typed dependencies manual.
D. K. Elson and K. R. McKeown. 2010. Automatic
attribution of quoted speech in literary narrative. In
Proc. of Twenty-Fourth AAAI Conference on Artifi-
cial Intelligence.
C. Fellbaum. 2005. Wordnet and wordnets. In
K. Brown et al., editor, Encyclopedia of Language
and Linguistics, pages 665?670. Oxford: Elsevier.
E. Greene, T. Mishra, P. Haffner, and A. Conkie. 2012.
Predicting character-appropriate voices for a TTS-
based storyteller system. In Proc. of Interspeech.
H. He, D. Barbosa, and G. Kondrak. 2013. Identifica-
tion of speakers in novels. In Proc. of 51st Annual
Meeting of the Association for Computational Lin-
guistics, pages 1312?1320.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proc. of Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177.
L. Lloyd, D. Kechagias, and S. Skiena. 2005. Lydia:
A system for large-scale news analysis. In Proc.
SPIRE, number 3772 in Lecture Notes in Computer
Science, pages 161?166.
N. Malandrakis, A. Potamianos, E. Iosif, and
S. Narayanan. 2013. Distributional semantic
models for affective text analysis. IEEE Transac-
tions on Audio, Speech, and Language Processing,
21(11):2379?2392.
N. Mamede and P. Chaleira. 2004. Character identifi-
cation in children stories. In J. Vicedo, P. Martnez-
Barco, R. Muoz, and M. Saiz Noeda, editors, Ad-
vances in Natural Language Processing, volume
3230 of Lecture Notes in Computer Science, pages
82?90. Springer Berlin Heidelberg.
R. Mitkov. 2002. Anaphora Resolution. Longman.
I. R. Murray and J. L. Arnott. 2008. Applying an anal-
ysis of acted vocal emotions to improve the simu-
lation of synthetic speech. Computer Speech and
Language, 22(2):107?129.
P. Nakov, S. Rosenthal, Z. Kozareva, V. Stoyanov,
A. Ritter, and T. Wilson. 2013. Semeval 2013 task
2: Sentiment analysis in twitter. In Proc. of Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Seventh International Workshop on
Semantic Evaluation, pages 312?320.
T. Natsiopoulou, M. Souliotis, and A. G. Kyridis.
2006. Narrating and reading folktales and pic-
ture books: storytelling techniques and approaches
with preschool children. Early Childhood Re-
search and Practice, 8(1). Retrieved on Jan 13th,
2014 from http://ecrp.uiuc.edu/v8n1/
natsiopoulou.html.
T. W. Norman. 1963. Toward an adequate taxonomy of
personality attributes: Replicated factor structure in
peer nomination personality rating. Journal of Ab-
normal and Social Psychology, 66:574?583.
S. Piao. 2014. Sentence splitting pro-
gram. http://text0.mib.man.ac.uk:
8080/scottpiao/sent_detector.
D. Rao, D. Yarowsky, A. Shreevats, and M. Gupta.
2010. Classifying latent user attributes in twitter. In
Proc. of the 2nd International Workshop on Search
and Mining User-generated Contents, pages 37?44.
M. Rusko, M. Trnka, S. Darjaa, and J. Hamar. 2013.
The dramatic piece reader for the blind and visu-
ally impaired. In Proc. of 4th Workshop on Speech
and Language Processing for Assistive Technolo-
gies, pages 83?91.
R. E. Schapire and Y. Singer. 2000. Boostexter: A
boosting-based system for text categorization. Ma-
chine. Learning, 39(2-3):135?168.
J. Schler, M. Koppel, S. Argamon, and J. W. Pen-
nebaker. 2006. Effects of age and gender on blog-
ging. In Proc. of AAAI Spring Symposium: Compu-
tational Approaches to Analyzing Weblogs.
Social Security. 2014. U.S. social security adminis-
tration baby name database. http://www.ssa.
gov/OACT/babynames/limits.html.
48
P. J. Stone, D. C. Dunphy, M. S. Smith, and D. M.
Ogilvie. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press.
C. Strapparava and R. Mihalcea. 2007. Semeval 2007
task 14: Affective text. In Proc. SemEval, pages 70?
74.
M. Theune, K. Meijs, and D. Heylen. 2006. Gener-
ating expressive speech for storytelling applications.
In IEEE Transactions on Audio, Speech and Lan-
guage Processing, pages 1137?1144.
P. Turney and M. L. Littman. 2002. Unsupervised
learning of semantic orientation from a hundred-
billion-word corpus (technical report erc-1094).
Disney Wiki. 2014. Description of Disney char-
acters. http://disney.wikia.com/wiki/
Category:Disney_characters#.
J. Y. Zhang, A. W. Black, and R. Sproat. 2003. Iden-
tifying speakers in children?s stories for speech syn-
thesis. In Proc. of Interspeech.
49
