Proceedings of the 43rd Annual Meeting of the ACL, pages 197?204,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Automatic Measurement of Syntactic Development in Child Language
Kenji Sagae and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15232
{sagae,alavie}@cs.cmu.edu
Brian MacWhinney
Department of Psychology
Carnegie Mellon University
Pittsburgh, PA 15232
macw@cmu.edu
Abstract
To facilitate the use of syntactic infor-
mation in the study of child language
acquisition, a coding scheme for Gram-
matical Relations (GRs) in transcripts of
parent-child dialogs has been proposed by
Sagae, MacWhinney and Lavie (2004).
We discuss the use of current NLP tech-
niques to produce the GRs in this an-
notation scheme. By using a statisti-
cal parser (Charniak, 2000) and memory-
based learning tools for classification
(Daelemans et al, 2004), we obtain high
precision and recall of several GRs. We
demonstrate the usefulness of this ap-
proach by performing automatic measure-
ments of syntactic development with the
Index of Productive Syntax (Scarborough,
1990) at similar levels to what child lan-
guage researchers compute manually.
1 Introduction
Automatic syntactic analysis of natural language has
benefited greatly from statistical and corpus-based
approaches in the past decade. The availability of
syntactically annotated data has fueled the develop-
ment of high quality statistical parsers, which have
had a large impact in several areas of human lan-
guage technologies. Similarly, in the study of child
language, the availability of large amounts of elec-
tronically accessible empirical data in the form of
child language transcripts has been shifting much of
the research effort towards a corpus-based mental-
ity. However, child language researchers have only
recently begun to utilize modern NLP techniques
for syntactic analysis. Although it is now common
for researchers to rely on automatic morphosyntactic
analyses of transcripts to obtain part-of-speech and
morphological analyses, their use of syntactic pars-
ing is rare.
Sagae, MacWhinney and Lavie (2004) have
proposed a syntactic annotation scheme for the
CHILDES database (MacWhinney, 2000), which
contains hundreds of megabytes of transcript data
and has been used in over 1,500 studies in child lan-
guage acquisition and developmental language dis-
orders. This annotation scheme focuses on syntactic
structures of particular importance in the study of
child language. In this paper, we describe the use
of existing NLP tools to parse child language tran-
scripts and produce automatically annotated data in
the format of the scheme of Sagae et al We also
validate the usefulness of the annotation scheme and
our analysis system by applying them towards the
practical task of measuring syntactic development in
children according to the Index of Productive Syn-
tax, or IPSyn (Scarborough, 1990), which requires
syntactic analysis of text and has traditionally been
computed manually. Results obtained with current
NLP technology are close to what is expected of hu-
man performance in IPSyn computations, but there
is still room for improvement.
2 The Index of Productive Syntax (IPSyn)
The Index of Productive Syntax (Scarborough,
1990) is a measure of development of child lan-
guage that provides a numerical score for grammat-
ical complexity. IPSyn was designed for investigat-
ing individual differences in child language acqui-
197
sition, and has been used in numerous studies. It
addresses weaknesses in the widely popular Mean
Length of Utterance measure, or MLU, with respect
to the assessment of development of syntax in chil-
dren. Because it addresses syntactic structures di-
rectly, it has gained popularity in the study of gram-
matical aspects of child language learning in both
research and clinical settings.
After about age 3 (Klee and Fitzgerald, 1985),
MLU starts to reach ceiling and fails to properly dis-
tinguish between children at different levels of syn-
tactic ability. For these purposes, and because of its
higher content validity, IPSyn scores often tells us
more than MLU scores. However, the MLU holds
the advantage of being far easier to compute. Rel-
atively accurate automated methods for computing
the MLU for child language transcripts have been
available for several years (MacWhinney, 2000).
Calculation of IPSyn scores requires a corpus of
100 transcribed child utterances, and the identifica-
tion of 56 specific language structures in each ut-
terance. These structures are counted and used to
compute numeric scores for the corpus in four cat-
egories (noun phrases, verb phrases, questions and
negations, and sentence structures), according to a
fixed score sheet. Each structure in the four cate-
gories receives a score of zero (if the structure was
not found in the corpus), one (if it was found once
in the corpus), or two (if it was found two or more
times). The scores in each category are added, and
the four category scores are added into a final IPSyn
score, ranging from zero to 112.1
Some of the language structures required in the
computation of IPSyn scores (such as the presence
of auxiliaries or modals) can be recognized with the
use of existing child language analysis tools, such
as the morphological analyzer MOR (MacWhinney,
2000) and the part-of-speech tagger POST (Parisse
and Le Normand, 2000). However, more complex
structures in IPSyn require syntactic analysis that
goes beyond what POS taggers can provide. Exam-
ples of such structures include the presence of an
inverted copula or auxiliary in a wh-question, con-
joined clauses, bitransitive predicates, and fronted
or center-embedded subordinate clauses.
1See (Scarborough, 1990) for a complete listing of targeted
structures and the IPSyn score sheet used for calculation of
scores.
Sentence (input):
We eat the cheese sandwich
Grammatical Relations (output):
[Leftwall]     We     eat     the     cheese     sandwich
SUBJ
ROOT OBJ
DET
MOD
Figure 1: Input sentence and output produced by our
system.
3 Automatic Syntactic Analysis of Child
Language Transcripts
A necessary step in the automatic computation of
IPSyn scores is to produce an automatic syntac-
tic analysis of the transcripts being scored. We
have developed a system that parses transcribed
child utterances and identifies grammatical relations
(GRs) according to the CHILDES syntactic annota-
tion scheme (Sagae et al, 2004). This annotation
scheme was designed specifically for child-parent
dialogs, and we have found it suitable for the iden-
tification of the syntactic structures necessary in the
computation of IPSyn.
Our syntactic analysis system takes a sentence
and produces a labeled dependency structure repre-
senting its grammatical relations. An example of the
input and output associated with our system can be
seen in figure 1. The specific GRs identified by the
system are listed in figure 2.
The three main steps in our GR analysis are: text
preprocessing, unlabeled dependency identification,
and dependency labeling. In the following subsec-
tions, we examine each of them in more detail.
3.1 Text Preprocessing
The CHAT transcription system2 is the format
followed by all transcript data in the CHILDES
database, and it is the input format we use for syn-
tactic analysis. CHAT specifies ways of transcrib-
ing extra-grammatical material such as disfluency,
retracing, and repetition, common in spontaneous
spoken language. Transcripts of child language may
contain a large amount of extra-grammatical mate-
2http://childes.psy.cmu.edu/manuals/CHAT.pdf
198
SUBJ, ESUBJ, CSUBJ, XSUBJ
COMP, XCOMP
JCT, CJCT, XJCT
OBJ, OBJ2, IOBJ
PRED, CPRED, XPRED
MOD, CMOD, XMOD
AUX NEG DET QUANT POBJ PTL
CPZR COM INF VOC COORD ROOT
Subject, expletive subject, clausal subject (finite and non?finite) Object, second object, indirect object
Clausal complement (finite and non?finite) Predicative, clausal predicative (finite and non?finite)
Adjunct, clausal adjunct (finite and non?finite) Nominal modifier, clausal nominal modifier (finite and non?finite)
Auxiliary Negation Determiner Quantifier Prepositional object Verb particle
CommunicatorComplementizer Infinitival "to" Vocative Coordinated item Top node
Figure 2: Grammatical relations in the CHILDES syntactic annotation scheme.
rial that falls outside of the scope of the syntactic an-
notation system and our GR identifier, since it is al-
ready clearly marked in CHAT transcripts. By using
the CLAN tools (MacWhinney, 2000), designed to
process transcripts in CHAT format, we remove dis-
fluencies, retracings and repetitions from each sen-
tence. Furthermore, we run each sentence through
the MOR morphological analyzer (MacWhinney,
2000) and the POST part-of-speech tagger (Parisse
and Le Normand, 2000). This results in fairly clean
sentences, accompanied by full morphological and
part-of-speech analyses.
3.2 Unlabeled Dependency Identification
Once we have isolated the text that should be ana-
lyzed in each sentence, we parse it to obtain unla-
beled dependencies. Although we ultimately need
labeled dependencies, our choice to produce unla-
beled structures first (and label them in a later step)
is motivated by available resources. Unlabeled de-
pendencies can be readily obtained by processing
constituent trees, such as those in the Penn Tree-
bank (Marcus et al, 1993), with a set of rules to
determine the lexical heads of constituents. This
lexicalization procedure is commonly used in sta-
tistical parsing (Collins, 1996) and produces a de-
pendency tree. This dependency extraction proce-
dure from constituent trees gives us a straightfor-
ward way to obtain unlabeled dependencies: use an
existing statistical parser (Charniak, 2000) trained
on the Penn Treebank to produce constituent trees,
and extract unlabeled dependencies using the afore-
mentioned head-finding rules.
Our target data (transcribed child language) is
from a very different domain than the one of the data
used to train the statistical parser (the Wall Street
Journal section of the Penn Treebank), but the degra-
dation in the parser?s accuracy is acceptable. An
evaluation using 2,018 words of in-domain manu-
ally annotated dependencies shows that the depen-
dency accuracy of the parser is 90.1% on child lan-
guage transcripts (compared to over 92% on section
23 of the Wall Street Journal portion of the Penn
Treebank). Despite the many differences with re-
spect to the domain of the training data, our domain
features sentences that are much shorter (and there-
fore easier to parse) than those found in Wall Street
Journal articles. The average sentence length varies
from transcript to transcript, because of factors such
as the age and verbal ability of the child, but it is
usually less than 15 words.
3.3 Dependency Labeling
After obtaining unlabeled dependencies as described
above, we proceed to label those dependencies with
the GR labels listed in Figure 2.
Determining the labels of dependencies is in gen-
eral an easier task than finding unlabeled dependen-
cies in text.3 Using a classifier, we can choose one
of the 30 possible GR labels for each dependency,
given a set of features derived from the dependen-
cies. Although we need manually labeled data to
train the classifier for labeling dependencies, the size
of this training set is far smaller than what would be
necessary to train a parser to find labeled dependen-
3Klein and Manning (2002) offer an informal argument that
constituent labels are much more easily separable in multidi-
mensional space than constituents/distituents. The same argu-
ment applies to dependencies and their labels.
199
cies in one pass.
We use a corpus of about 5,000 words with man-
ually labeled dependencies to train TiMBL (Daele-
mans et al, 2003), a memory-based learner (set to
use the k-nn algorithm with k=1, and gain ratio
weighing), to classify each dependency with a GR
label. We extract the following features for each de-
pendency:
? The head and dependent words;
? The head and dependent parts-of-speech;
? Whether the dependent comes before or after
the head in the sentence;
? How many words apart the dependent is from
the head;
? The label of the lowest node in the constituent
tree that includes both the head and dependent.
The accuracy of the classifier in labeling depen-
dencies is 91.4% on the same 2,018 words used to
evaluate unlabeled accuracy. There is no intersec-
tion between the 5,000 words used for training and
the 2,018-word test set. Features were tuned on a
separate development set of 582 words.
When we combine the unlabeled dependencies
obtained with the Charniak parser (and head-finding
rules) and the labels obtained with the classifier,
overall labeled dependency accuracy is 86.9%, sig-
nificantly above the results reported (80%) by Sagae
et al (2004) on very similar data.
Certain frequent and easily identifiable GRs, such
as DET, POBJ, INF, and NEG were identified with
precision and recall above 98%. Among the most
difficult GRs to identify were clausal complements
COMP and XCOMP, which together amount to less
than 4% of the GRs seen the training and test sets.
Table 1 shows the precision and recall of GRs of par-
ticular interest.
Although not directly comparable, our results
are in agreement with state-of-the-art results for
other labeled dependency and GR parsers. Nivre
(2004) reports a labeled (GR) dependency accuracy
of 84.4% on modified Penn Treebank data. Briscoe
and Carroll (2002) achieve a 76.5% F-score on a
very rich set of GRs in the more heterogeneous and
challenging Susanne corpus. Lin (1998) evaluates
his MINIPAR system at 83% F-score on identifica-
tion of GRs, also in data from the Susanne corpus
(but using simpler GR set than Briscoe and Carroll).
GR Precision Recall F-score
SUBJ 0.94 0.93 0.93
OBJ 0.83 0.91 0.87
COORD 0.68 0.85 0.75
JCT 0.91 0.82 0.86
MOD 0.79 0.92 0.85
PRED 0.80 0.83 0.81
ROOT 0.91 0.92 0.91
COMP 0.60 0.50 0.54
XCOMP 0.58 0.64 0.61
Table 1: Precision, recall and F-score (harmonic
mean) of selected Grammatical Relations.
4 Automating IPSyn
Calculating IPSyn scores manually is a laborious
process that involves identifying 56 syntactic struc-
tures (or their absence) in a transcript of 100 child
utterances. Currently, researchers work with a par-
tially automated process by using transcripts in elec-
tronic format and spreadsheets. However, the ac-
tual identification of syntactic structures, which ac-
counts for most of the time spent on calculating IP-
Syn scores, still has to be done manually.
By using part-of-speech and morphological anal-
ysis tools, it is possible to narrow down the num-
ber of sentences where certain structures may be
found. The search for such sentences involves pat-
terns of words and parts-of-speech (POS). Some
structures, such as the presence of determiner-noun
or determiner-adjective-noun sequences, can be eas-
ily identified through the use of simple patterns.
Other structures, such as front or center-embedded
clauses, pose a greater challenge. Not only are pat-
terns for such structures difficult to craft, they are
also usually inaccurate. Patterns that are too gen-
eral result in too many sentences to be manually ex-
amined, but more restrictive patterns may miss sen-
tences where the structures are present, making their
identification highly unlikely. Without more syntac-
tic analysis, automatic searching for structures in IP-
Syn is limited, and computation of IPSyn scores still
requires a great deal of manual inspection.
Long, Fey and Channell (2004) have developed
a software package, Computerized Profiling (CP),
for child language study, which includes a (mostly)
200
automated computation of IPSyn.4 CP is an exten-
sively developed example of what can be achieved
using only POS and morphological analysis. It does
well on identifying items in IPSyn categories that
do not require deeper syntactic analysis. However,
the accuracy of overall scores is not high enough to
be considered reliable in practical usage, in particu-
lar for older children, whose utterances are longer
and more sophisticated syntactically. In practice,
researchers usually employ CP as a first pass, and
manually correct the automatic output. Section 5
presents an evaluation of the CP version of IPSyn.
Syntactic analysis of transcripts as described in
section 3 allows us to go a step further, fully au-
tomating IPSyn computations and obtaining a level
of reliability comparable to that of human scoring.
The ability to search for both grammatical relations
and parts-of-speech makes searching both easier and
more reliable. As an example, consider the follow-
ing sentences (keeping in mind that there are no ex-
plicit commas in spoken language):
(a) Then [,] he said he ate.
(b) Before [,] he said he ate.
(c) Before he ate [,] he ran.
Sentences (a) and (b) are similar, but (c) is dif-
ferent. If we were looking for a fronted subordinate
clause, only (c) would be a match. However, each
one of the sentences has an identical part-speech-
sequence. If this were an isolated situation, we
might attempt to fix it by having tags that explic-
itly mark verbs that take clausal complements, or by
adding lexical constraints to a search over part-of-
speech patterns. However, even by modifying this
simple example slightly, we find more problems:
(d) Before [,] he told the man he was cold.
(e) Before he told the story [,] he was cold.
Once again, sentences (d) and (e) have identical
part-of-speech sequences, but only sentence (e) fea-
tures a fronted subordinate clause. These limited toy
examples only scratch the surface of the difficulties
in identifying syntactic structures without syntactic
4Although CP requires that a few decisions be made man-
ually, such as the disambiguation of the lexical item ??s? as
copula vs. genitive case marker, and the definition of sentence
breaks for long utterances, the computation of IPSyn scores is
automated to a large extent.
analysis beyond part-of-speech and morphological
tagging. In these sentences, searching with GRs
is easy: we simply find a GR of clausal type (e.g.
CJCT, COMP, CMOD, etc) where the dependent is
to the left of its head.
For illustration purposes of how searching for
structures in IPSyn is done with GRs, let us look
at how to find other IPSyn structures5:
? Wh-embedded clauses: search for wh-words
whose head, or transitive head (its head?s head,
or head?s head?s head...) is a dependent in
GR of types [XC]SUBJ, [XC]PRED, [XC]JCT,
[XC]MOD, COMP or XCOMP;
? Relative clauses: search for a CMOD where the
dependent is to the right of the head;
? Bitransitive predicate: search for a word that is
a head of both OBJ and OBJ2 relations.
Although there is still room for under- and over-
generalization with search patterns involving GRs,
finding appropriate ways to search is often made
trivial, or at least much more simple and reliable
than searching without GRs. An evaluation of our
automated version of IPSyn, which searches for IP-
Syn structures using POS, morphology and GR in-
formation, and a comparison to the CP implemen-
tation, which uses only POS and morphology infor-
mation, is presented in section 5.
5 Evaluation
We evaluate our implementation of IPSyn in two
ways. The first is Point Difference, which is cal-
culated by taking the (unsigned) difference between
scores obtained manually and automatically. The
point difference is of great practical value, since
it shows exactly how close automatically produced
scores are to manually produced scores. The second
is Point-to-Point Accuracy, which reflects the overall
reliability over each individual scoring decision in
the computation of IPSyn scores. It is calculated by
counting how many decisions (identification of pres-
ence/absence of language structures in the transcript
being scored) were made correctly, and dividing that
5More detailed descriptions and examples of each structure
are found in (Scarborough, 1990), and are omitted here for
space considerations, since the short descriptions are fairly self-
explanatory.
201
number by the total number of decisions. The point-
to-point measure is commonly used for assessing the
inter-rater reliability of metrics such as the IPSyn. In
our case, it allows us to establish the reliability of au-
tomatically computed scores against human scoring.
5.1 Test Data
We obtained two sets of transcripts with correspond-
ing IPSyn scoring (total scores, and each individual
decision) from two different child language research
groups. The first set (A) contains 20 transcripts of
children of ages ranging between two and three. The
second set (B) contains 25 transcripts of children of
ages ranging between eight and nine.
Each transcript in set A was scored fully manu-
ally. Researchers looked for each language structure
in the IPSyn scoring guide, and recorded its pres-
ence in a spreadsheet. In set B, scoring was done
in a two-stage process. In the first stage, each tran-
script was scored automatically by CP. In the second
stage, researchers checked each automatic decision
made by CP, and corrected any errors manually.
Two transcripts in each set were held out for de-
velopment and debugging. The final test sets con-
tained: (A) 18 transcripts with a total of 11,704
words and a mean length of utterance of 2.9, and
(B) 23 transcripts with a total of 40,819 words and a
mean length of utterance of 7.0.
5.2 Results
Scores computed automatically from transcripts
parsed as described in section 3 were very close
to the scores computed manually. Table 2 shows a
summary of the results, according to our two eval-
uation metrics. Our system is labeled as GR, and
manually computed scores are labeled as HUMAN.
For comparison purposes, we also show the results
of running Long et al?s automated version of IPSyn,
labeled as CP, on the same transcripts.
Point Difference
The average (absolute) point difference between au-
tomatically computed scores (GR) and manually
computed scores (HUMAN) was 3.3 (the range of
HUMAN scores on the data was 21-91). There was
no clear trend on whether the difference was posi-
tive or negative. In some cases, the automated scores
were higher, in other cases lower. The minimum dif-
System Avg. Pt. Difference Point-to-Point
to HUMAN Reliability
GR (Total) 3.3 92.8%
CP (Total) 8.3 85.4%
GR (Set A) 3.7 92.5%
CP (Set A) 6.2 86.2%
GR (Set B) 2.9 93.0%
CP (Set B) 10.2 84.8%
Table 2: Summary of evaluation results. GR is our
implementation of IPSyn based on grammatical re-
lations, CP is Long et al?s (2004) implementation of
IPSyn, and HUMAN is manual scoring.
Histogram of Point Differences (3 point bins)
0
10
20
30
40
50
60
3 6 9 12 15 18 21Point Difference
Freque
ncy (%) GRCP
Figure 3: Histogram of point differences between
HUMAN scores and GR (black), and CP (white).
ference was zero, and the maximum difference was
12. Only two scores differed by 10 or more, and 17
scores differed by two or less. The average point dif-
ference between HUMAN and the scores obtained
with Long et al?s CP was 8.3. The minimum was
zero and the maximum was 21. Sixteen scores dif-
fered by 10 or more, and six scores differed by 2 or
less. Figure 3 shows the point differences between
GR and HUMAN, and CP and HUMAN.
It is interesting to note that the average point dif-
ferences between GR and HUMAN were similar on
sets A and B (3.7 and 2.9, respectively). Despite the
difference in age ranges, the two averages were less
than one point apart. On the other hand, the average
difference between CP and HUMAN was 6.2 on set
A, and 10.2 on set B. The larger difference reflects
CP?s difficulty in scoring transcripts of older chil-
dren, whose sentences are more syntactically com-
plex, using only POS analysis.
202
Point-to-Point Accuracy
In the original IPSyn reliability study (Scarborough,
1990), point-to-point measurements using 75 tran-
scripts showed the mean inter-rater agreement for
IPSyn among human scorers at 94%, with a min-
imum agreement of 90% of all decisions within a
transcript. The lowest agreement between HUMAN
and GR scoring for decisions within a transcript was
88.5%, with a mean of 92.8% over the 41 transcripts
used in our evaluation. Although comparisons of
agreement figures obtained with different sets of
transcripts are somewhat coarse-grained, given the
variations within children, human scorers and tran-
script quality, our results are very satisfactory. For
direct comparison purposes using the same data, the
mean point-to-point accuracy of CP was 85.4% (a
relative increase of about 100% in error).
In their separate evaluation of CP, using 30 sam-
ples of typically developing children, Long and
Channell (2001) found a 90.7% point-to-point ac-
curacy between fully automatic and manually cor-
rected IPSyn scores.6 However, Long and Channell
compared only CP output with manually corrected
CP output, while our set A was manually scored
from scratch. Furthermore, our set B contained
only transcripts from significantly older children (as
in our evaluation, Long and Channell observed de-
creased accuracy of CP?s IPSyn with more com-
plex language usage). These differences, and the
expected variation from using different transcripts
from different sources, account for the difference in
our results and Long and Channell?s.
5.3 Error Analysis
Although the overall accuracy of our automatically
computed scores is in large part comparable to man-
ual IPSyn scoring (and significantly better than the
only option currently available for automatic scor-
ing), our system suffers from visible deficiencies in
the identification of certain structures within IPSyn.
Four of the 56 structures in IPSyn account for al-
most half of the number of errors made by our sys-
tem. Table 3 lists these IPSyn items, with their re-
spective percentages of the total number of errors.
6Long and Channell?s evaluation also included samples
from children with language disorders. Their 30 samples of
typically developing children (with a mean age of 5) are more
directly comparable to the data used in our evaluation.
IPSyn item Error
S11 (propositional complement) 16.9%
V15 (copula, modal or aux for 12.3%
emphasis or ellipsis)
S16 (relative clause) 10.6%
S14 (bitransitive predicate) 5.8%
Table 3: IPSyn structures where errors occur most
frequently, and their percentages of the total number
of errors over 41 transcripts.
Errors in items S11 (propositional complements),
S16 (relative clauses), and S14 (bitransitive predi-
cates) are caused by erroneous syntactic analyses.
For an example of how GR assignments affect IP-
Syn scoring, let us consider item S11. Searching for
the relation COMP is a crucial part in finding propo-
sitional complements. However, COMP is one of
the GRs that can be identified the least reliably in
our set (precision of 0.6 and recall of 0.5, see table
1). As described in section 2, IPSyn requires that
we credit zero points to item S11 for no occurrences
of propositional complements, one point for a single
occurrence, and two points for two or more occur-
rences. If there are several COMPs in the transcript,
we should find about half of them (plus others, in
error), and correctly arrive at a credit of two points.
However, if there are very few or none, our count is
likely to be incorrect.
Most errors in item V15 (emphasis or ellipsis)
were caused not by incorrect GR assignments, but
by imperfect search patterns. The searching failed to
account for a number of configurations of GRs, POS
tags and words that indicate that emphasis or ellip-
sis exists. This reveals another general source of er-
ror in our IPSyn implementation: the search patterns
that use GR analyzed text to make the actual IP-
Syn scoring decisions. Although our patterns are far
more reliable than what we could expect from POS
tags and words alone, these are still hand-crafted
rules that need to be debugged and perfected over
time. This was the first evaluation of our system,
and only a handful of transcripts were used during
development. We expect that once child language
researchers have had the opportunity to use the sys-
tem in practical settings, their feedback will allow us
to refine the search patterns at a more rapid pace.
203
6 Conclusion and Future Work
We have presented an automatic way to annotate
transcripts of child language with the CHILDES
syntactic annotation scheme. By using existing re-
sources and a small amount of annotated data, we
achieved state-of-the-art accuracy levels.
GR identification was then used to automate the
computation of IPSyn scores to measure grammati-
cal development in children. The reliability of our
automatic IPSyn was very close to the inter-rater re-
liability among human scorers, and far higher than
that of the only other computational implementation
of IPSyn. This demonstrates the value of automatic
GR assignment to child language research.
From the analysis in section 5.3, it is clear that the
identification of certain GRs needs to be made more
accurately. We intend to annotate more in-domain
training data for GR labeling, and we are currently
investigating the use of other applicable GR parsing
techniques.
Finally, IPSyn score calculation could be made
more accurate with the knowledge of the expected
levels of precision and recall of automatic assign-
ment of specific GRs. It is our intuition that in a
number of cases it would be preferable to trade re-
call for precision. We are currently working on a
framework for soft-labeling of GRs, which will al-
low us to manipulate the precision/recall trade-off
as discussed in (Carroll and Briscoe, 2002).
Acknowledgments
This work was supported in part by the National Sci-
ence Foundation under grant IIS-0414630.
References
Edward J. Briscoe and John A. Carroll. 2002. Robust ac-
curate statistical annotation of general text. Proceed-
ings of the 3rd International Conference on Language
Resources and Evaluation, (pp. 1499?1504). Las Pal-
mas, Gran Canaria.
John A. Carroll and Edward J. Briscoe. 2002. High pre-
cision extraction of grammatical relations. Proceed-
ings of the 19th International Conference on Compu-
tational Linguistics, (pp. 134-140). Taipei, Taiwan.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the First Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics. Seattle, WA.
Michael Collins. 1996. A new statistical parser based on
bigram lexical dependencies. Proceedings of the 34th
Meeting of the Association for Computational Linguis-
tics (pp. 184-191). Santa Cruz, CA.
Walter Daelemans, Jacub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2004. TiMBL: Tilburg Memory
Based Learner, version 5.1, Reference Guide. ILK Re-
search Group Technical Report Series no. 04-02, 2004.
T. Klee and M. D. Fitzgerald. 1985. The relation be-
tween grammatical development and mean length of
utterance in morphemes. Journal of Child Language,
12, 251-269.
Dan Klein and Christopher D. Manning. 2002. A genera-
tive constituent-context model for improved grammar
induction. Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics (pp.
128-135).
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In Proceedings of the Workshop on the
Evaluation of Parsing Systems. Granada, Spain.
Steve H. Long and Ron W. Channell. 2001. Accuracy of
four language analysis procedures performed automat-
ically. American Journal of Speech-Language Pathol-
ogy, 10(2).
Steven H. Long, Marc E. Fey, and Ron W. Channell.
2004. Computerized Profiling (Version 9.6.0). Cleve-
land, OH: Case Western Reserve University.
Brian MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Mahwah, NJ: Lawrence Erlbaum
Associates.
Mitchel P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewics. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19.
Joakim Nivre and Mario Scholz. 2004. Deterministic de-
pendency parsing of English text. Proceedings of In-
ternational Conference on Computational Linguistics
(pp. 64-70). Geneva, Switzerland.
Christophe Parisse and Marie-Thrse Le Normand. 2000.
Automatic disambiguation of the morphosyntax in
spoken language corpora. Behavior Research Meth-
ods, Instruments, and Computers, 32, 468-481.
Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2004.
Adding Syntactic annotations to transcripts of parent-
child dialogs. Proceedings of the Fourth International
Conference on Language Resources and Evaluation
(LREC 2004). Lisbon, Portugal.
Hollis S. Scarborough. 1990. Index of Productive Syn-
tax. In Applied Psycholinguistics, 11, 1-22.
204
Proceedings of the Second Workshop on Psychocomputational Models of Human Language Acquisition, pages 53?68,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Item-based Constructions and the Logical Problem  Brian MacWhinney  Department of Psychology Carnegie Mellon University Pittsburgh, PA 15213 macw@cmu.edu      Abstract The logical problem of language is grounded on arguments from poverty of positive evidence and arguments from poverty of negative evidence. Careful analysis of child language cor-pora shows that, if one assumes that children learn through item-based con-structions, there is an abundance of positive evidence. Arguments regarding the poverty of negative evidence can also be addressed by the mechanism of conservative item-based learning. When conservativism is abandoned, children can rely on competition, cue construc-tion, monitoring and probabilistic iden-tification to derive information from positive data to recover from overgener-alization. 
1. The Logical Problem Chomsky (1957, 1980) has argued that the child?s acquisition of grammar is ?hopelessly underdeter-mined by the fragmentary evidence available.? He attributed this indeterminacy to two major sources. The first is the degenerate nature of the input. Ac-cording to Chomsky, the sentences heard by the child are so full of retracing, error, and incomple-tion that they provide no clear indication of the possible sentences of the language. Coupled with this problem of input degeneracy is the problem of unavailability of negative evidence. According to this view, children have a hard time knowing which forms of their language are acceptable and which are unacceptable, because parents fail to provide consistent evidence regarding the un-grammaticality of unacceptable sentences. Worse 
still, when such evidence is provided, children ap-pear to ignore it. Chomsky?s (1957) views about the degeneracy of the input did not stand up well to the test of time. As Newport, Gleitman & Gleitman (1977) reported, ?the speech of mothers to children is unswervingly well-formed.? More recently, Sagae, Lavie & MacWhinney (2004) examined several of the corpora in the CHILDES database and found that adult input to children can be parsed with an accuracy level parallel to that for corpora such as the Wall Street Journal database.  This evidence for well formedness of the input did not lead to the collapse of the ?argument from poverty of stimulus? (APS).  However, it did place increased weight on the remaining claims regard-ing the absence of relevant evidence. The overall claim is that, given the absence of appropriate positive and negative evidence, no child can ac-quire language without guidance from a rich set of species-specific innate hypotheses. Some refer to the argument from poverty of stimulus as the ?logi-cal problem of language acquisition (Baker, 1979), while others have called it ?Plato?s Problem,? ?Chomsky?s Problem,? ?Gold?s Problem,? or ?Baker?s Paradox.?  2. Absence of Negative Evidence In the 1970s, generativist analyses of learnability (Wexler & Hamburger, 1973) relied primarily on an analysis presented by Gold (1967). Gold?s analysis contrasted two different language-learning situations: text presentation and informant presen-tation. With informant presentation, the language learner can receive feedback from an infallible in-formant regarding the grammaticality of every candidate sentence. This corrective feedback is called ?negative evidence? and it only requires that 
53
ungrammatical strings be clearly identified as un-acceptable. Whenever the learner formulates an overly general guess about some particular linguis-tic structure, the informant will label the resulting structure as ungrammatical and the learner will use this information to restrict the developing gram-mar. Based on initial empirical results reported by Brown & Hanlon (1970), Gold argued that nega-tive evidence is not available to the child and that language learning cannot be based on informant presentation. Marcus (1993) has argued that the feedback that parents provide does not discriminate consistently between grammatical and ungrammatical construc-tions.  As a result, children cannot rely on simple, overt negative evidence for recovery from over-generalization. Although I will argue that parents provide positive evidence in a form that solves the logical problem (Bohannon et al, 1990), I agree with the observation that this evidence does not constitute overt grammatical correction of the type envisioned by Gold.  3. Absence of Positive Evidence Beginning about 1980, generative analyses of learnability began to shift away from an emphasis on the unavailability of negative evidence to argu-ments based on the unavailability of positive evi-dence.  This conceptual shift led to a relative decline in attention to recovery from overgenerali-zation and an increase in attention to reported cases of error-free learning. For example, Chomsky?s (1980) statement of the logical problem relies on the notion of error-free learning without positive evidence.  The argumentation here is that, if a structure is never encountered in the input, correct use of this structure would have to indicate innate knowledge. Researchers have claimed that the child pro-duces error-free learning without receiving positive evidence for structures such as: structural depend-ency, c-command, the binding conditions, subja-cency, negative polarity items, that-trace deletion, nominal compound formation, control, auxiliary phrase ordering, and the empty category principle. In each of these cases, it is necessary to assume that the underlying universal is a part of the non-parameterized core of universal grammar.  If the dimension involved were parameterized, there would be a need for some form of very early pa-
rameter setting (Wexler, 1998), which could itself introduce some error. Thus, we would expect er-ror-free learning to occur primarily for those as-pects of the grammar that are completely universal and not parameterized. Parameterized features, such as subject pro-drop, could still be guided by universal grammar. However, their learning would not necessarily be error-free. 3.1.  Structural dependency The paradigm case of error-free learning is the child?s obedience to the Structural Dependency condition, as outlined by Chomsky in his formal discussion with Jean Piaget (Piattelli-Palmarini, 1980). Chomsky notes that children learn early on to move the auxiliary to initial position in ques-tions, such as ?Is the man coming?? One formula-tion of this rule is that it stipulates the movement of the first auxiliary to initial position. This formu-lation would be based on surface order, rather than structural relations. However, if children want to question the proposition given in (1), they will never produce a movement such as (2). Instead, they will always produce (3). 1. The man who is running is coming. 2. Is the man who __ running is coming? 3. Is the man who is running __ coming?? In order to produce (3), children must be basing the movement on structure, rather than surface order.  Thus, according to Chomsky, they must be in-nately guided to formulate rules in terms of struc-ture. In the theory of barriers (Chomsky, 1986),  the repositioning of the auxiliary in the tree and then in surface structure involves a movement of INFL to COMP that is subject to the head movement con-straint. In (2) the auxiliary would need to move around the N? of ?man? and the CP and COMP of the relative clause, but this movement would be blocked by the head movement constraint (HMC). No such barriers exist in the main clause. In addi-tion, if the auxiliary moves as in (2), it leaves a gap that will violate the empty category principle (ECP). Chomsky?s discussion with Piaget does not rely on these details. Chomsky simply argues that the child has to realize that phrasal structure is somehow involved in this process and that one cannot formulate the rule of auxiliary movement as ?move the first auxiliary to the front.?  
54
Chomsky claims that, ?A person might go through much or all of his life without ever having been exposed to relevant evidence, but he will nevertheless unerringly employ the structure-dependent generalization, on the first relevant oc-casion.? A more general statement of this type pro-vided by Hornstein & Lightfoot (1981) who claim that, ?People attain knowledge of the structure of their language for which no evidence is available in the data to which they are exposed as children.?  In order to evaluate these claims empirically, we need to know when children first produce such sentences and whether they have been exposed to relevant examples in the input prior to this time.  In searching for instances of relevant input as well as first uses, we should include two types of sen-tences. First, we want to include sentences such as (3) in which the moved verb was a copula in the relative clause, as well as sentences with auxilia-ries in both positions, such as ?Will the boy who is wearing a Yankee?s cap step forward??  The auxil-iaries do not have to be lexically identical, since Chomsky?s argument from poverty of stimulus would also apply to a child who was learning the movement rule on the basis of lexical class, as op-posed to surface lexical form.  Examining the TreeBank structures for the Wall Street Journal in the Penn TreeBank, Pullum & Scholz (Pullum & Scholz, 2002) estimate that adult corpora contain up to 1% of such sentences. How-ever, the presence of such structures in formal written English says little about their presence in the input to the language-learning child.  A search by Lewis & Elman (2001) of the input to English-speaking children in the CHILDES database (MacWhinney, 2000) turned up only one case of this structure out of approximately 3 million utter-ances. Since CHILDES includes good sampling of target children up to age 5;0, we can safely say that positive evidence for this particular structure is seldom encountered in the language addressed to children younger than 5;0. Because children do not produce sentences of this type themselves, it is difficult to use produc-tion data to demonstrate the presence of the con-straint. Crain & Nakayama (1987) attempted to get around this problem by eliciting these forms from children directly.  They asked children (3;2 to 5;11) to, ?Ask Jabba if the boy who is watching Mickey is happy.? Children responded with a vari-ety of structures, none of which involved the 
movement of the auxiliary from the relative clause. Unfortunately, this elicitation procedure encour-ages children to treat the relative clause (?the boy who is watching Mickey?) as an imitated chunk. Despite the serious methodological limitation in this particular study, it seems reasonable to believe that four-year-old children are beginning to behave in accordance with the Structural Dependency condition for sentences like (2) and (3).  But does this mean that they reach this point without learn-ing? There is another type of sentence that provides equally useful positive evidence regarding auxil-iary movement.  These are wh-questions with em-bedded relative clauses. It turns out that there are hundreds of input sentences of this type in the CHILDES corpus.  Most of these have the form of (4), but some take the form of (5). 4. Where is the dog that you like? 5. Which is the dog that is clawing at the door? In (5) the child receives clear information demon-strating that moved auxiliaries derive from the main clause and not the relative clause.  Using evi-dence of the type provided in (4), the child simply learns that moved auxiliaries and the wh-words that accompany them are arguments of the verb of the main clause.  Sentences like (4) and (5) are highly frequent in the input to children and both types instruct the child in the same correct gener-alization.  Based on evidence from the main clause, the child could formulate the rule as a placement after the wh-word of the auxiliary that is conceptually related to the verb being questioned. In other words, it is an attachment to the wh-word of an argument of the main verb. This is a complex ap-plication of the process of item-based construction generation proposed in MacWhinney (1975, 1982). This formulation does not rely on barriers, ECP, HCP, INFL, COMP, or movement. It does rely on the notion of argument structure, but only as it emerges from the application of item-based con-structions. Given this formulation, a few simple yes?no questions would be enough to demonstrate the pattern.  When children hear ?is the baby happy? they can learn that the initial copula auxil-iary ?is? takes a subject argument in the next slot and a predicate argument in the following slot.  They will learn similar frames for each of the other fronted auxiliaries.  When they then encounter sen-
55
tences such as (11) and (12), they will further elaborate the item-based auxiliary frames to allow for positioning of the initial wh-words and for at-tachment of the auxiliaries to these wh-words. One might argue that this learning scenario amounts to a restatement of Chomsky?s claim, since it requires the child to pay attention to rela-tional patterns, rather than serial order as calcu-lated from the beginning of the sentence. However, if the substance of Chomsky?s claim is that chil-dren learn to fill argument slots with compound constituents, then his analysis seems indistinguish-able from that of MacWhinney (1975; 1987a). 3.2  Auxiliary phrases Kimball (1973) presented perhaps the first example of a learnability problem based on poverty of posi-tive evidence. He noted that children are exposed to scores of sentences with zero, one, or two auxil-iaries as in (6)?(13). However, his searches of a million sentences in early machine-readable cor-pora located not a single instance of a structure such as (13). 6. It rains. 7. It may rain. 8. It may have rained. 9. It may be raining. 10. It has rained. 11. It has been raining. 12. It is raining. 13. It may have been raining. Kimball argued that, despite the absence of posi-tive data for (13), children are still able to infer its grammaticality from the data in (6) to (12). He took this as evidence that children have innate knowledge of structural compositionality. The em-pirical problem with Kimball?s analysis is that sen-tences like (13) are not nearly as rare as his corpus analysis suggests.  My search of the CHILDES database for the string ?might have been? located 27 instances in roughly 3 million sentences. In ad-dition there were 24 cases of ?could have been?, 15 cases of ?should have been?, and 70 cases of ?would have been.? Thus, there seems to be little shortage of positive evidence for the direct learn-ing of this pattern. Perhaps Kimball?s findings to the contrary arose from focusing exclusively on ?may?, since a search for ?may have been? turned up only 5 cases. 
3.3 The complex-NP constraint The complex-NP constraint blocks movement of a noun from a relative clause, as in (14) and (15). 14. *Who did John believe the man that kissed __ arrived  15. Who did John believe __ kissed his buddy? This same constraint also blocks movement from prepositional phrases and other complex NPs, as in (16) ? (18): 16. *Who did pictures of ___ surprise you?  17. *What did you see a happy ___ ? 18. *What did you stand between the wall and ___ ? The constraint in (18) has also been treated as the coordinated-NP constraint in some accounts. Although it appears that most children obey these constraints, there are some exceptions. Wilson & Peters (1988) list these violations of the complex NP constraint from Wilson?s son Seth between the ages of 3;0 and 5;0. 19. What am I cooking on a hot __ ? (stove) 20. What are we gonna look for some __ ? (houses) 21. What is this a funny __ , Dad? 22. What are we gonna push number __ ? (9) 23. Where did you pin this on my __ ? (robe) 24. What are you shaking all the __ ? (batter and milk) 25. What is this medicine for my __ ? (cold) These seven violations all involve separation of a noun from its modifiers. Two other examples, il-lustrate violation of the complex-NP constraint in other environments: 26. What did I get lost at the __ , Dad? 27. What are we gonna go at Auntie and __ ?  Here, the prohibited raising involves prepositional phrases and a conjoined noun phrase. Violations of the latter type are particularly rare, but still do oc-cur occasionally. One might object that a theory of universal grammar should not be rejected on the basis of a few violations from a single child. However, other observers have reported similar errors. In the recordings from my sons Ross and Mark, I observed a few such violations. One occurred when my son Mark (at 5;4.4) said, ?Dad, next time when it's Indian Guides and my birthday, what do you think a picture of ___ should be on my cake?? Catherine Snow reports that at age 10;10, her son Nathaniel said, ?I have a fever, but I don't want to 
56
said, ?I have a fever, but I don't want to be taken a temperature of.?  Most researchers would agree that violations of the complex-NP constraint are rare, but certainly not nonexistent. At the same time, the structures or meanings that might trigger these violations are also very rare, as is the input that would tell the child how to handle these structures. Given this, it seems to me that these patterns cannot reasonably be described as cases of error-free learning.  In-stead, we should treat them as instances of ?low-error constructions.? In this regard, they resemble errors such as stative progressives (?I am know-ing?) and double-object violations (?He recom-mended the library the book?). As soon as we shift from error-free learning to low-error learning, we need to apply a very different form of analysis, since we now have to explain how children recover from making these overgeneralization errors, once they have produced them. This then induces us to again focus on the availability of negative evi-dence.  Of course, we could assume that the violation of the complex-NP constraint was a transient per-formance error and that, once the relevant per-formance factors are eliminated, the constraints of UG operate to block further wh-raising from com-plex noun phrases. But the important point here is that we now need to consider specific mechanisms for allowing for recovery from overgeneralization, even for what have been offered as the clearest cases of the application of universal constraints. 3.4  Binding conditions Binding theory (Chomsky, 1981) offers three pro-posed universal conditions on the binding of pro-nouns and reflexives to referents. Sentence (28) illustrates two of the constraints. In (28), ?he? can-not be coreferential with ?Bill? because ?Bill? does not c-command the pronoun. At the same time, ?himself? must be coreferential with ?Bill? because it is a clausemate and does c-command ?Bill.?  28. He said that Bill hurt himself. When attempting to relate the logical problem to the study of the binding constraints, it is important to remember that the sentences produced or inter-preted are fully grammatical. However, the inter-pretation in which the pronoun is coreferential with the full NP is disallowed by the binding principles. This means that, to study the imposition of the 
constraints, researchers must rely on comprehen-sion studies, often with very young children. It is well known that children often fail to apply these principles, even in carefully controlled ex-periments (O'Grady, 1997). Various accounts have been offered to reconcile these facts with the sup-posed universality of the constraint. However, one possibility that has seldom been explored is the idea that the binding conditions are learned on the basis of positive data. To illustrate the role that learning can play in this area, consider a study of long-distance movement of adjuncts by De Villiers, Roeper & Vainikka (De Villiers et al, 1990). Children were divided into two age groups: 3;7 to 5;0 and 5;1 to 6;11. They were given sen-tences such as: 29. When did the boy say he hurt himself? 30. When did the boy say how he hurt him-self? 31. Who did the boy ask what to throw? For (29), 44% of the children gave long distance interpretations, associating ?when? with ?hurt him-self?, rather than ?say.? For (30), with a medial wh-phrase blocking a long-distance interpretation, only 6% gave long-distance responses. This shows that children were sensitive to the conditions on traces, in accord with P&P (Chomsky & Lasnik, 1993) theory. However, the fact that sensitivity to this contrast increases markedly across the two age groups indicates that children are learning this pat-tern. In the youngest group, children had trouble even understanding sentences with medial argu-ments like (31). The fact that this ability improves over time again points to learning of the possible interpretations of these structures. Children can learn to interpret these sentences correctly by applying conservative learning princi-ples that rely on positive data.  First, they learn short-distance interpretations that attach the wh-word to the main clause.  Then, when they hear sentences with medial ?how? they add the addi-tional possibility of the long-distance interpreta-tion.  However, they do this in a conservative item-based manner, limiting the new interpretation to sentences like (30) with medial ?how.? P&P theory can also provide an account of this development in terms of the setting of parameters. First, children must realize that their language al-lows movement, unlike Chinese. Next they must decide whether the movement can be local, as in German, or both local and distant as in English. 
57
Finally, they must decide whether the movement is indexed by pronouns, traces, or both. However, once a parameter-setting account is detailed in a way that requires careful attention to complex cue patterns over time (Buttery, 2004; Sakas & Fodor, 2001), it can be difficult to distinguish it from a learning account. Using positive evidence, children can first learn that some movement can occur. Next, they can learn to move locally and finally they can acquire the cues to linking the moved ar-gument to its original argument position, one by one. 3.5  Learnability or learning? What have we learned from our examination of these four examples? First, we have seen that the application of universal constraints is not error-free. This is particularly true in the case of the binding conditions. Because the binding conditions involve parameter setting, it is perhaps not surpris-ing that we see errors in this domain. However, we also find errors in the application of the non-parameterized constraint against raising from com-plex noun phrases. Only in the case of the struc-tural dependency condition do we find no errors. However, for that structure there is also no usage at all by either parents or children, unless we consider attachment of auxiliaries to wh-words, which is quite frequent. It is possible that error-free learning exists in various other corners of syntactic, seman-tic, or lexical learning. But there is no evidence that error-free learning occurs in association with an absence of positive evidence.  This is the crucial association that has been claimed in the literature and it is the one that we have shown to be false. Second, for each of the four learnability prob-lems we examined, we have seen that there are effective learning methods based on available posi-tive evidence. This learning involves mechanisms of conservative, item-based learning followed by later generalization.   4. Multiple Solutions Having now briefly surveyed the role of the logical problem in generative theory, we turn next to a consideration of seven factors that, operating to-gether, allow the child to solve the logical problem. Of these seven factors, the first two are simply formal considerations that help us understand the 
scope of the problem.  The last five are processes that can actually guide the child during acquisition. 4.1  Limiting the class of grammars The first solution to the logical problem addresses the Gold analysis directly by showing how lan-guage can be generated from finite-state grammars (Reich, 1969). For example, Hausser (1999) has developed an efficient parser for left-associative grammars. He has shown that left-associative grammar can be expressed as a finite automaton that orders words in terms of part-of-speech cate-gories. Because we know that finite automata can be identified from positive evidence (Hopcroft & Ullman, 1979), this means that children should be able to learn left-associative grammars directly without triggering a logical problem. Given the fact that these grammars can parse sentences in a time-linear and psycholinguistically plausible fash-ion, they would seem to be excellent candidates for further exploration by child language researchers.  A formal solution to the logical problem also arises in the context of the theory of categorical grammar. Kanazawa (1998) shows that a particular class of categorial grammars known as the k-valued grammars can be learned on positive data. Moreover, he shows that most of the customary versions of categorial grammar discussed in the linguistic literature can be included in this k-valued class. Shinohara (1994) and Jain, Osherson, Royer & Sharma (1999) examine still further classes of complex non-finite languages that can be learned on the basis of positive data alone. These attempts to recharacterize the nature of human language by revised formal analysis all stand as useful ap-proaches to the logical problem. By characterizing the target language in a way that makes it learnable by children, linguists help bridge the gap between linguistic theory and child language studies. 4.2  Revised end-state criterion The second solution to the logical problem in-volves resetting our notion of what it means to ac-quire an end-state grammar. Horning (1969) showed that, if the language identification is al-lowed to involve a stochastic probability of identi-fication, rather than an absolute guarantee of no further error ever, then language can be identified on positive evidence alone. It is surprising that this 
58
solution has not received more attention, since this analysis undercuts the core logic of the logical problem, as it applies to the learning of all rule sys-tems up to the level of context-sensitive grammars. If learning were deterministic, children would go through a series of attempts to hypothesize the ?correct? grammar for the language. Once they hit on the correct identification, they would then never abandon this end-state grammar. The fact that adults make speech errors and differ in their judg-ments regarding at least some syntactic structures suggests that this criterion is too strong and that the view of grammar as stochastic is more realistic. 4.3  Conservative Item-based Learning The third solution to the logical problem empha-sizes the conservative nature of children?s lan-guage learning.  The most direct way for a language learner to solve Gold?s problem is to avoid formulating overly general grammars in the first place. If the child never overgeneralizes, there is no problem of recovery from overgeneralization and no need for negative evidence or corrective feedback. Taking this basic idea one step further, let us imagine that grammars are ordered strictly in terms of their relative generative power. If this is true, then the forms generated by a grammar are a subset of the next slightly larger grammar.  This is known as the Subset Principle. If the child always chooses the least powerful grammar that is consis-tent with the input data, then the problem of the unavailability of negative evidence disappears and learning can be based simply on positive evidence.  The Subset Principle has often been used to ar-gue for abstract relations between grammars. For example, Fodor & Crain (1987) argue that the child learns the periphrastic dative (?give the book to John?) for each new verb and only assumes that the double object construction (?give John the book?) can be applied if it is attested in the input. In this particular case, the grammar with only the periphrastic is ordered as a subset of the grammar with both constructions. This follows from the principles for expansion of curly braces in GPSG.   Conservatism can control acquisition of these structures without invoking the Subset Principle. The theory of item-based acquisition (MacWhinney, 1975, 1982, 1987a; Tomasello, 2000) holds that syntactic learning is driven by the induction and combination of item-based construc-
tions. Each item-based construction specifies a set of slots for arguments.  Initially, these slots encode features that are specific to the first words encoun-tered in this slot during comprehension.  For ex-ample, the item ?more? has a slot for a following argument.  If the first combinations the child picks up from comprehension are ?more cookies? and ?more milk?, then this slot will initially be limited to foods.  However, as the child hears ?more? used in additional combinations, the semantics of the slot filler will extend to any mass noun or plural.  This learning is based entirely on generalization from positive evidence. When learning the item-based construction for ?give?, children encounter sentences such as ?Bill gives John the book.? From this, they learn the double-object construction: giver + ?give? + recipi-ent + gift.  They also learn the competing item-based construction of giver + ?give? + gift + ?to? recipient.  There is no need to invoke the Subset Principle to explain this learning, since item-based constructions are inherently conservative and pro-vide their own constraints on the form of gram-mars. Having acquired these two basic constructions, children can them join them into a single item-based finite automaton that operates on narrowly defined lexical categories.         Children can learn this item-based grammar frag-ment on the basis of simple positive data.  This example uses the formalism of a finite-state automaton to annotate the use of positive data.  However, in the Competition Model and other connectionist accounts, the two verb frames com-pete probabilistically with the outcome of the competition being determined by further cues such as focusing or topicalization. Item-based learning involves an ongoing proc-ess of generalization for the semantic features of the arguments. During these processes of generali-zation, to minimize the possibility of error, the child has to be conservative in three ways: ? The child needs to formulate each syntactic combination as an item-based construction. 
giver gives gift recip 
to recip 
gift 
59
? Each item-based construction needs to record the exact semantic status of each positive in-stance of an argument in a particular gram-matical configuration (MacWhinney, 1987a). ? Attempts to use the item-based construction with new arguments must be closely guided by the semantics of previously encountered posi-tive instances. If the child has a good memory and applies this method cautiously, overgeneralization will be minimized and there will be no need to recover from overgeneralization. Each item-based construction is linked to a spe-cific lexical item.  This item must be a predicate. There are no item-based constructions for nouns.  Predicates can have up to three arguments. Item-based constructions for verbs can also include the verbs of embedded clauses as possible arguments.  Item-based constructions for prepositions and aux-iliaries include both a phrase internal head (endo-head) and a head for the phrase attachment (exohead). For details on the implementation of this grammatical relations model through a parser see Sagae, MacWhinney, and Lavie (2004).  In section 4.6, we will see how item-based construc-tions are generalized to feature-based constructions in accord with the account of MacWhinney (1987a) Conservatism also applies to non-local move-ment patterns.  For example, Wolfe Quintero (1992) has shown that conservatism can be used to account for L2 acquisition of the wh-movement patterns. She notes that L2 learners acquire these positive contexts for wh-movement in this order: 32. What did the little girl hit __ with the block today? 33. What did the boy play with __ behind his mother? 34. What did the boy read a story about __ this morning? Because they are proceeding conservatively, learn-ers never produce forms such as (35): 35. *What did the boy with ___ read a story this morning? They never hear this structure in the input and never hypothesize a grammar that includes it. As a result, they never make overgeneralizations and never attempt wh-movement in this particular con-text. Data from Maratsos, Kuczaj, Fox & Chalkley (1979) show that this same analysis applies to first language learners. 
4.4  Competition Conservatism is a powerful mechanism for ad-dressing the logical problem. However, children will eventually go ?beyond the information given? and produce errors (Jespersen, 1922). When the child produces errors, some mechanism must force recovery. The four processes that have been pro-posed by emergentist theory are: competition, cue construction, monitoring, and indirect negative evidence.  Each of these processes can work to correct overgeneralization.  These processes are important for addressing the version of the logical problem that emphasizes the poverty of negative evidence. The fourth solution to the problem of poverty of negative evidence relies on the mechanism of competition. Of the four mechanisms for promot-ing recovery from overgeneralization, competition is the most basic, general, and powerful. Psycho-logical theories have often made reference to the notion of competition. In the area of language ac-quisition, MacWhinney (1978) used competition to account for the interplay between ?rote? and ?anal-ogy? in learning morphophonology. Competition was later generalized to all levels of linguistic processing in the Competition Model. In the 1990s, specific aspects of learning in the Competition Model were formulated through both neural net-work theory and the ACT-R production system. The Competition Model views overgeneraliza-tions as arising from two types of pressures. The first pressure is the underlying analogic force that produces the overgeneralization. The second pres-sure is the growth in the rote episodic auditory representation of a correct form. This representation slowly grows in strength over time, as it is repeatedly strengthened through encounters with the input data. These two forces compete for the control of production. Consider the case of ?*goed? and ?went?. The overgeneralization ?goed? is supported by analogy. It competes against the weak rote form ?went,? which is supported by auditory memory. As the strength of the rote auditory form for ?went? grows, it begins to win out in the competition against the analogic form ?*goed?. Finally, the error is eliminated. This is the Competition Model account for recovery from overgeneralization. Th  competition between two candidate forms is governed by the strength of their episodic auditory representations. In the case of the competition be-
60
tween ?*goed? and ?went?, the overgeneralized form has little episodic auditory strength, since it is heard seldom if at all in the input. Although ?*goed? lacks auditory support, it has strong analogic support from the general pattern for past tense formation. In the Competition Model, analogic pressure stimulates overgeneralization and episodic auditory encoding reins it in. The analogic pressure hypothesized in this account has been described in detail in several connectionist models of morphophonological learning. The mod-els that most closely implement the type of compe-tition being described here are the models of MacWhinney and Leinbach (1991) for English and MacWhinney, Leinbach, Taraban & McDonald (1989) for German. In these models, there is a pressure for regularization according to the general pattern that produces forms such as ?*goed? and ?*ranned?. In addition, there are weaker gang ef-fects that lead to overgeneralizations such as ?*stang? for the past tense of ?sting?. Competition implements the notion of blocking developed first by Baker (1979) and later by Pinker (1994). Blocking is more limited than competition because it requires either strict rule-ordering or all-or-none competition. The assumption that forms are competing for the same meaning is identical to the Principle of Uniqueness postulated by Pinker (1994).  Competition is also the general case of the Direct Contrast noted by Saxton (1997). Competition goes beyond the analyses offered by Baker, Pinker, and Saxton by emphasizing the fact that the child is continually internalizing adult forms in episodic memory.  Recent evidence for the power of episodic memory in infant audition (Aslin et al, 1999) has underscored the power of neural mechanisms for storing linguistic input and extracting patterns from this input without con-scious processing. The Competition Model as-sumes that children are continually storing traces of the words and phrases they hear along with tags that indicate that these phrases derive directly from adult input. When the child then comes to produce a spontaneous form, these stored forms function as an ?oracle? or ?informant?, providing delayed nega-tive evidence that corresponds (because of compe-tition or Uniqueness) to the currently generated productive form. The ultimate source of this nega-tive evidence is the input. Children do not use this evidence when it is initially presented. It is only later when the information is retrieved in the con-
text of productive combinations that it provides negative evidence. This can only happen if it is clear that stored adult forms compete directly (Saxton, 1997) with productive child forms. The crucial claim of the Competition Model is that the same retrieval cues that trigger the formation of the overgeneralized productive form also trigger the retrieval of the internalized negative evidence. When these assumptions hold, there is a direct so-lution to the logical problem through the availabil-ity of internalized negative evidence. To gain a better understanding of the range of phenomena that can be understood in terms of competition, let us look at examples from mor-phology. lexical semantics, and syntactic construc-tions. 4.4.1  Morphological competition Bowerman (1987) argued that recovery from overgeneralizations such as ?*unsqueeze? is par-ticularly problematic for a Competition Model ac-count. She holds that recovery depends on processes of semantic reorganization that lie out-side the scope of competition. To make her exam-ple fully concrete, let us imagine that ?*unsqueeze? is being used to refer to the voluntary opening of a clenched fist. Bowerman holds that there is no ob-vious competitor to ?*unsqueeze.? However, when presented with this concrete example, most native speakers will say that both ?release? and ?let go? are reasonable alternatives. The Competition Model claim is that, because there is no rote audi-tory support for ?*unsqueeze,? forms like ?release? or ?let go? will eventually compete against and eliminate this particular error. Several semantic cues support this process of recovery. In particular, inanimate objects such as rubber balls and sponges cannot be ?*unsqueezed? in the same way that they can be ?squeezed.? Squeezing is only reversible if we focus on the ac-tion of the body part doing the squeezing, not the object being squeezed. It is possible that, at first, children do not fully appreciate these constraints on the reversibility of this particular action. How-ever, it is equally likely that they resort to using ?*unsqueeze? largely because of the unavailability of more suitable competitors such as ?release.? An error of this type is equivalent to production of ?falled? when the child is having trouble remem-bering the correct form ?fell.? Or consider the 
61
competition between ?*unapproved? and its ac-ceptable competitor  ?disapproved?. We might imagine that a mortgage loan application that was initially approved could then be subsequently ?*unapproved.? We might have some uncertainty about the reversibility of the approval process, but the real problem is that we have not sufficiently solidified our notion of ?disapproved? in order to have it apply in this case. The flip side of this coin is that many of the child?s extensional productions of reversives will end up being acceptable. For ex-ample, the child may produce ?unstick? without ever having encountered the form in the input.  In this case, the form will survive.  Although it will compete with ?remove?, it will also receive occa-sional support from the input and will survive long enough for it to begin to carve out further details in the semantic scope of verbs that can be reversed with the prefix ?un-? (Li & MacWhinney, 1996). 4.4.2 Lexical competition The same logic that can be used to account for re-covery from morphological overgeneralizations can be used to account for recovery from lexical overgeneralizations. For example, a child may overgeneralize the word ?kitty? to refer to tigers and lions. The child will eventually learn the cor-rect names for these animals and restrict the over-generalized form. The same three forces are at work here: analogic pressure, competition, and episodic encoding. Although the child has never actually seen a ?kitty? that looks like a tiger, there are enough shared features to license the generali-zation. If the parent supplies the name ?tiger.? there is a new episodic encoding that then begins to compete with the analogic pressure. If no new name is supplied, the child may still begin to ac-cumulate some negative evidence, noting that this particular use of ?kitty? is not yet confirmed in the input. Merriman (1999) has shown how the linking of competition to a theory of attentional focusing can account for the major empirical findings in the lit-erature on Mutual Exclusivity (the tendency to treat each object as having only one name). By treating this constraint as an emergent bias, we avoid a variety of empirical problems. Since com-petition is probabilistic, it only imposes a bias on learning, rather than a fixed innate constraint. The probabilistic basis for competition allows the child 
to deal with hierarchical category structure without having to enforce major conceptual reorganization. Competition may initially lead a child to avoid re-ferring to a ?robin? as a ?bird,? since the form ?robin? would be a better direct match. However, sometimes ?bird? does not compete directly with ?robin.? This occurs when referring to a collection of different types of birds that may include robins, when referring to an object that cannot be clearly identified as a robin, or when making anaphoric reference to an item that was earlier mentioned as a ?robin.? 4.4.3  Syntactic frame competition  Overgeneralizations in syntax arise when a feature-based construction common to a group or ?gang? of verbs is incorrectly overextended to a new verb. This type of overextension has been analyzed in both distributed networks (Miikkulainen & May-berry, 1999) and interactive activation networks (Elman et al, 2005; MacDonald et al, 1994; MacWhinney, 1987b). These networks demon-strate the same gang effects and generalizations found in networks for morphological forms (Plunkett & Marchman, 1993) and spelling correspondences (Taraban & McClelland, 1987). If a word shares a variety of semantic features with a group of other words, it will be treated syntacti-cally as a member of the group. Consider the example of overgeneralizations of dative movement. Verbs like ?give?, ?send?, and ?ship? all share a set of semantic features involving the transfer of an object through some physical medium. In this regard, they are quite close to a verb like ?deliver? and the three-argument verb group exerts strong analogic pressure on the verb ?deliver?. However, dative movement only applies to certain frequent, monosyllabic transfer verbs and not to multisyllabic, Latinate forms with a less transitive semantics such as ?deliver? or ?recom-mend.? When children overgeneralize and say, ?Tom delivered the library the book,? they are obeying analogic pressure from the group of trans-fer verbs that permit dative movement. In effect, the child has created a new argument frame for the verb ?deliver.? The first argument frame only specifies two arguments ? a subject or ?giver? and an object or ?thing transferred.? The new lexical entry specifies three arguments. These two homo-phonous entries for ?deliver? are now in competi-
62
tion, just as ?*goed? and ?went? were in competi-tion. Like the entry for ?*goed?, the three-place entry for ?deliver? has good analogic support, but no support from episodic encoding derived from the input. Over time, it loses in its competition with the two-argument form of ?deliver? and its progressive weakening along with strengthening of the competing form leads to recovery from over-generalization. Thus, the analysis of recovery from ?Tom delivered the library the book? is identical to the analysis of recovery from ?*goed?. 4.4.4  Modeling construction strength It may be useful to characterize the temporal course of competitive item-based learning in slightly more formal terms.  To do this, we can say that a human language is generated by the applica-tion of a set of constructions that map arguments to predicates. For each item-based construction (IC), there is a correct mapping (CM) from argument to its predicates and any number of incorrect map-pings (IM).  The IMs receive support from analogical relations to groups of CM with similar structure. From these emerge feature-based con-structions (FC). The CMs receive support from positive input, as well as analogical relations to other CMs and FCs. Each positive input increases the strength S of a matching CM by amount A. Learning of an IC occurs when the S of CM ex-ceeds the S of each of the strongest competing IM by some additional amount. This is the dominance strength or DS.  To model language learning within this frame-work, we need to understand the distribution of the positive data and the sources of analogical support. From database searches and calculation of ages of learning of CM, we can estimate the number of positive input examples (P) needed to bring a CM to strength DS. For each C, if the input has in-cluded P cases by time T, we can say that a par-ticular CM reaches DS monotonically in time T.  At this point, IC is learned. Languages are learn-able if their component ICs can be learned in time T. To measure learning to various levels, we can specify learning states in which there remain cer-tain specified slow constructions (SC) that have not yet reached DS. Constructions learned by this time can be called NC or normal constructions. Thus, at time T, the degree of completion of the learning of L can be expressed as NC/NC + SC.  
This is a number that approaches 1.0 as T in-creases.  The residual presence of a few SC, as well as occasional spontaneous declines in DS of CM will lead to deviations from 1.0. The study of the SCs requires a model of analogic support from FCs. In essence, the logical problem of language acquisition is then restated as the process of under-standing how analogical pressures lead to learning courses that deviate from what is predicted by sim-ple learning on positive exemplars for individual item-based constructions. 4.5  Cue construction The fifth solution to the logical problem and the second of the solutions that promotes recovery from overgeneralization is cue construction. Most recovery from overgeneralization relies on compe-tition. However, competition will eventually en-counter limits in its ability to deal with the fine details of grammatical patterns. To illustrate these limits, consider the case of recovery from resulta-tive overgeneralizations such as ?*I untied my shoes loose?. This particular extension receives analogic support from verbs like ?shake? or ?kick? which permit ?I shook my shoes loose? or ?I kicked my shoes loose.? It appears that the child is not initially tuned in to the fine details of these seman-tic classifications. Bowerman (1988) has suggested that the process of recovery from overgeneraliza-tion may lead the child to construct new features to block overgeneralization.  We can refer to this process as ?cue construction.? Recovering from other resultative overgenerali-zations may also require cue construction. For ex-ample, an error such as ?*The gardener watered the tulips flat? can be attributed to the operation of a feature-based construction which yields three-argument verbs from ?hammer? or ?rake?, as in ?The gardener raked the grass flat.? Source-goal overgeneralization can also fit into this framework. Consider, ?*The maid poured the tub with water? instead of ?The maid poured water into the tub? and ?*The maid filled water into the tub? instead of ?The maid filled the tub with water.? In each case, the analogic pressure from one group of words leads to the establishment of a case frame that is incorrect for a particular verb. Although this com-petition could be handled just by the strengthening of the correct patterns, it seems likely that the child 
63
also needs to clarify the shape of the semantic fea-tures that unify the ?pour? verbs and the ?fill? verbs. Bowerman (personal communication) provides an even more challenging example. One can say ?The customers drove the taxi driver crazy,? but not ?*The customers drove the taxi driver sad.? The error involves an overgeneralization of the exact shape of the resultative adjective. A connectionist model of the three-argument case frame for ?drive? would determine not only that certain verbs license a third possible argument, but also what the exact semantic shape of that argument can be. In the case of the standard pattern for verbs like ?drive,? the resultant state must be terminative, rather than transient. To express this within the Competition Model context, we would need to have a competi-tion between a confirmed three-argument form for ?drive? and a looser overgeneral form based only on analogic pressure. A similar competition ac-count can be used to account for recovery from an error such as, ?*The workers unloaded the truck empty? which contrasts with ?The workers loaded the truck full?. In both of these cases, analogic pressure seems weak, since examples of such er-rors are extremely rare in the language learning literature. The actual modelling of these competitions in a neural network will require detailed lexical work and extensive corpus analysis. A sketch of the types of models that will be required is given in MacWhinney (1999). 4.6  Monitoring The sixth solution to the logical problem involves children?s abilities to monitor and detect their own errors. The Competition Model holds that, over time, correct forms gain strength from encounters with positive exemplars and that this increasing strength leads them to drive out incorrect forms. If we make further assumptions about uniqueness, this strengthening of correct forms can guarantee the learnability of language. However, by itself, competition does not fully account for the dynam-ics of language processing in real social interac-tions. Consider a standard self-correction such as ?I gived, uh, gave my friend a peach.? Here the cor-rect form ?gave? is activated in real time just after the production of the overgeneralization. MacWhinney (1978) and Elbers & Wijnen (1993) have treated this type of self-correction as involv-
ing ?expressive monitoring? in which the child lis-tens to her own output, compares the correct weak rote form with the incorrect overgeneralization, and attempts to block the output of the incorrect form. One possible outcome of expressive moni-toring is the strengthening of the weak rote form and weakening of the analogic forms. Exactly how this is implemented will vary from model to model In general, retraced false starts move from incor-rect forms to correct forms, indicating that the in-correct forms are produced quickly, whereas the correct rote forms take time to activate. Kawamoto (1994) has shown how a recurrent connectionist network can simulate exactly these timing asym-metries between analogic and rote retrieval. For example, Kawamoto?s model captures the experi-mental finding that incorrect regularized pronun-ciations of ?pint? to rhyme with ?hint? are produced faster than correct irregular pronunciations.  An even more powerful learning mechanism is what MacWhinney (1978) called ?receptive moni-toring.? If the child shadows input structures closely, he will be able to pick up many discrepan-cies between his own productive system and the forms he hears. Berwick (1987) found that syntac-tic learning could arise from the attempt to extract meaning during comprehension. Whenever the child cannot parse an input sentence, the failure to parse can be used as a means of expanding the grammar. The kind of analysis through synthesis that occurs in some parsing systems can make powerful use of positive instances to establish new syntactic frames. Receptive monitoring can also be used to recover from overgeneralization. The child may monitor the form ?went? in the input and at-tempt to use his own grammar to match that input. If the result of the receptive monitoring is ?*goed?, the child can use the mismatch to reset the weights in the analogic system to avoid future overgener-alizations. Neural network models that rely on back-propagation assume that negative evidence is con-tinually available for every learning trial. For this type of model to make sense, the child would have to depend heavily on both expressive and receptive monitoring. It is unlikely that these two mecha-nisms operate as continuously as would be re-quired for a mechanism such as back-propagation.  However, not all connectionist models rely on the availability of negative evidence. For example, Kohonen?s self-organizing feature map model 
64
(Miikkulainen, 1993) learns linguistic patterns simply using cooccurences in the data with no reli-ance on negative evidence.  4.7  Indirect negative evidence The seventh solution to the logical problem of language acquisition relies on the computation of indirect negative evidence. This computation can be illustrated with the error ?*goed.? To construct indirect negative evidence in this case, children need to track the frequency of all verbs and the frequency of the past tense as marked by the regu-lar ?-ed.? Then they need to compute regular ?-ed? as a percentage of all verbs.   Next they need to track the frequency of the verb ?go? in all of its uses and the frequency of ?*goed?. To gain a bit more certainty, they should also calculate the fre-quency of a verb like ?jump? and the frequency of ?jumped.?  With these ratios in hand, the child can then compare the ratio for ?go? with those for ?jump? or verbs in general and conclude that the attested cases of ?*goed? are fewer than would be expected on the basis of evidence from verbs like ?jump.?  They can then conclude that ?*goed? is ungrammatical. Interestingly, they can do this without receiving overt correction. The structures for which indirect negative evi-dence could provide the most useful accounts are ones that are learned rather late. These typically involve low-error constructions of the type that motivate the strong form of the logical problem. For example, children could compute indirect negative evidence that would block wh-raising from object-modifying relatives in sentences such as (37). 36. The police arrested the thieves who were carrying the loot. 37. *What did the police arrest the thieves who were carrying? 38. To do this, they would need to track the frequency of sentences such as: 39. Bill thought the thieves were carrying the loot. 40. What did Bill think the thieves were carry-ing? Noting that raising from predicate complements occurs fairly frequently, children could reasonably conclude that the absence of raising from object modification position means that it is ungrammati-cal. Coupled with conservatism, indirect negative 
evidence can be a useful mechanism for avoiding overgeneralization of complex syntactic structures.  The item-based acquisition component of the Competition Model provides a framework for computing indirect negative evidence. The indirect negative evidence tracker could note that, although ?squeeze? occurs frequently in the input, ?*unsqueeze? does not. This mechanism works through the juxtaposition of a form receiving epi-sodic support (?squeeze?) with a predicted inflected form (?unsqueeze?). This mechanism uses analogic pressure to pre-dict the form ?*unsqueeze.? This is the same mechanism as used in the generation of ?*goed.? However, the child does not need to actually pro-duce ?*unsqueeze,? only to hypothesize its exis-tence. This form is then tracked in the input. If it is not found, the comparison of the near-zero strength of the unconfirmed form ?unsqueeze? with the con-firmed form ?squeeze? leads to the strengthening of competitors such as ?release? and blocking of any attempts to use ?unsqueeze.? Although this mecha-nism is plausible, it is more complicated than the basic competition mechanism and places a greater requirement on memory for tracking of non-occurrences. Since the end result of this tracking of indirect negative evidence is the same as that of the basic competition mechanism, it is reasonable to imagine that learners use this mechanism only as a fall back strategy, relying on simple competition to solve most problems requiring recovery from overgeneralization.    5.  Consequences and Conclusions This analysis suggests that we should not longer speak of language learning as being confined by the poverty of positive evidence or negative evi-dence. Both types of evidence are far more abun-dant than has been imagined. Nor should we assume that recovery from overgeneralization in-volves a fundamental logical problem. Recovery is supported by a set of four powerful processes (competition, cue construction, monitoring, and indirect negative evidence) that provide redundant and complementary solutions to the logical prob-lem. In addition, we know that alternative charac-terizations of the nature of the target grammar can 
65
take much of the logical bit out of the logical prob-lem.  Finally, we have seen that the language ad-dressed to children is not at all unparsable or degenerate, once a few superficial retracing struc-tures are repaired.  We have reviewed seven solutions to the logical problem that work together to buffer the process of language acquisition. When we consider the inter-action of the seven solutions in this way, we soon come to realize the pivotal role played by the item-based construction. First, the item-based construc-tion directly enforces conservatism by requiring that each generalization of each argument frame be based on directly observable positive evidence. Second, the probabilistic competition between item-based constructions provides a meaningful way of understanding the probabilistic nature of grammar. Third, the competition between item-based constructions directly promotes recovery from overgeneralization.  Fourth, the additional mechanisms of cue construction, indirect negative evidence, and monitoring serve to fine-tune the operations of competition. These processes operate particularly in those cases where uniqueness is not fully transparent or where the restriction of a gen-eral process requires additional fine-tuning of cues.  The current analysis assigns great importance to good positive data. Marcus (1993) has suggested that parents are inconsistent in their provision of negative evidence to the child. But the Competition Model assumes that it is positive data that is cru-cial for learning. One way in which a parent can provide crucial positive evidence is through recast-ing, but other methods are possible too.  In various cultures and subgroups, positive evidence can be presented and focused through elicited repetition, choral recitation of stories, interaction with sib-lings, or games. Methods that emphasize shared attention and shared understanding can guide chil-dren toward the control of literate expression. This shared attention can arise in groups of co-wives in Central Africa just as easily as it can from isolated mother?child dyads in New England. Recently, Hauser, Chomsky, & Fitch (2002) have argued that the core evolutionary adaptation that was required to support human language in-volved the introduction of a facility for recursion. The analysis in the current paper modifies and ex-tends this claim by emphasizing the evolutionary (MacWhinney, 2005) and developmental (Tomasello, 2000) centrality of the item-based 
construction as the controller of recursive composi-tion of phrases and sentences. However MacWhin-ney  (2005) views linguistic recursion as emerging gradually from preexisting structures in spatial cognition, rather than as appearing suddenly during the Late Pleistocene. Studies of the functional neu-ral underpinnings of recursion can go a long ways toward clarifying the details of these issues.  Acknowledgements  This work was supported by NSF Grant SBE-0354420 to the Pittsburgh Science of Learning Center. References Aslin, R. N., Saffran, J. R., & Newport, E. L. (1999). Statistical learning in linguistic and nonlinguistic domains. In B. MacWhinney (Ed.), The emergence of language (pp. 359-380). Mahwah, NJ: Lawrence Erl-baum Associates. Baker, C. L. (1979). Syntactic theory and the projection problem. Linguistic Inquiry, 10, 533-581. Berwick, R. (1987). Parsability and learnability. In B. MacWhinney (Ed.), Mechanisms of Language Acqui-sition. Hillsdale, NJ: Lawrence Erlbaum Associates. Bohannon, N., MacWhinney, B., & Snow, C. (1990). No negative evidence revisited: Beyond learnability or who has to prove what to whom. Developmental Psychology, 26, 221-226. Bowerman, M. (1987). Commentary. In B. MacWhin-ney (Ed.), Mechanisms of language acquisition. Hillsdale, N.J.: Lawrence Erlbaum Associates. Bowerman, M. (1988). The "no negative evidence" problem. In J. Hawkins (Ed.), Explaining language universals (pp. 73-104). London: Blackwell. Brown, R., & Hanlon, C. (1970). Derivational complex-ity and order of acquisition in child speech. In J. R. Hayes (Ed.), Cognition and the development of lan-guage (pp. 11-54). New York: Wiley. Buttery, P. (2004). A quantitative evaluation of natural-istic models of language acquisition; the efficiency of the Triggering Learning Algorithm compared to a Categorial Grammar Learner. Coling 2004, 1-8. Chomsky, N. (1957). Syntactic Structures. The Hague: Mouton. Chomsky, N. (1980). Rules and Representations. New York: Columbia University Press. 
66
Chomsky, N. (1981). Lectures on government and bind-ing. Cinnaminson, NJ: Foris. Chomsky, N. (1986). Barriers. Cambridge, MA: MIT Press. Chomsky, N., & Lasnik, H. (1993). The theory of prin-ciples and parameters. In J. Jacobs (Ed.), Syntax: An international handbook of contemporary research (pp. 1-32). Berlin: Walter de Gruyter. Crain, S., & Nakayama, M. (1987). Structure depend-ence in grammar formation. Language, 63 No. 3, 522-543. De Villiers, J., Roeper, T., & Vainikka, A. (1990). The acquisition of long distance rules. In L. Frazier & J. De Villiers (Eds.), Language processing and lan-guage acquisition. Amsterdam: Kluwer. Elbers, L., & Wijnen, F. (1993). Effort, production skill, and language learning. In C. Ferguson, L. Menn & C. Stoel-Gammon (Eds.), Phonological development (pp. 337-368). Timonium, MD: York. Elman, J. L., Hare, M., & McRae, K. (2005). Cues, con-straints, and competition in sentence processing. In M. Tomasello & D. Slobin (Eds.), Beyond nature-nurture: Essays in honor of Elizabeth Bates. Mah-wah, NJ: Lawrence Erlbaum Associates. Fodor, J., & Crain, S. (1987). Simplicity and generality of rules in language acquisition. In B. MacWhinney (Ed.), Mechanisms of Language Acquisition. Hillsdale, N.J.: Lawrence Erlbaum. Gold, E. (1967). Language identification in the limit. Information and Control, 10, 447-474. Hauser, M., Chomsky, N., & Fitch, T. (2002). The fac-ulty of language: What is it, who has it, and how did it evolve? Science, 298, 1569-1579. Hausser, R. (1999). Foundations of computational lin-guistics: Man-machine communication in natural language. Berlin: Springer. Hopcroft, J., & Ullman, J. (1979). Introduction to auto-mata theory, languages, and computation. Reading, Mass.: Addison-Wesley. Horning, J. J. (1969). A study of grammatical inference: Stanford University, Computer Science Department. Hornstein, N., & Lightfoot, D. (1981). Explanation in linguistics: the logical problem of language acquisi-tion. London: Longmans. Jain, S., Osherson, D., Royer, J., & Sharma, A. (1999). Systems that learn. Cambridge, MA: MIT Press. Jespersen, O. (1922). Language: Its nature, develop-ment, and origin. London: George Allen and Unwin. 
Kanazawa, M. (1998). Learnable classes of categorial grammars. Stanford, CA: CSLI Publications. Kawamoto, A. (1994). One system or two to handle regulars and exceptions: How time-course of proc-essing can inform this debate. In S. D. Lima, R. L. Corrigan & G. K. Iverson (Eds.), The reality of lin-guistic rules (pp. 389-416). Amsterdam: John Ben-jamins. Kimball, J. (1973). The formal theory of grammar. Englewood Cliffs, NJ: Prentice-Hall. Lewis, J. D., & Elman, J. (2001). Learnability and the statistical structure of language: Poverty of stimulus arguments revisited. Proceedings of the 26th Annual Boston University Conference on Language Devel-opment. Li, P., & MacWhinney, B. (1996). Cryptotype, overgen-eralization, and competition: A connectionist model of the learning of English reversive prefixes. Connec-tion Science, 8, 3-30. MacDonald, M. C., Pearlmutter, N. J., & Seidenberg, M. S. (1994). Lexical nature of syntactic ambiguity resolution. Psychological Review, 101(4), 676-703. MacWhinney, B. (1975). Pragmatic patterns in child syntax. Stanford Papers And Reports on Child Lan-guage Development, 10, 153-165. MacWhinney, B. (1978). The acquisition of morpho-phonology. Monographs of the Society for Research in Child Development, 43, Whole no. 1, pp. 1-123. MacWhinney, B. (1982). Basic syntactic processes. In S. Kuczaj (Ed.), Language acquisition: Vol. 1. Syntax and semantics (pp. 73-136). Hillsdale, NJ: Lawrence Erlbaum. MacWhinney, B. (1987a). The Competition Model. In B. MacWhinney (Ed.), Mechanisms of language ac-quisition (pp. 249-308). Hillsdale, NJ: Lawrence Erl-baum. MacWhinney, B. (1987b). Toward a psycholinguisti-cally plausible parser. In S. Thomason (Ed.), Pro-ceedings of the Eastern States Conference on Linguistics. Columbus, Ohio: Ohio State University. MacWhinney, B. (2000). The CHILDES Project: Tools for Analyzing Talk. Mahwah, NJ: Lawrence Erlbaum Associates. MacWhinney, B. (2005). Language evolution and hu-man development. In D. Bjorklund & A. Pellegrini (Eds.), Child development and evolutionary psychol-ogy. New York: Academic. 
67
MacWhinney, B., & Leinbach, J. (1991). Implementa-tions are not conceptualizations: Revising the verb learning model. Cognition, 29, 121-157. MacWhinney, B., Leinbach, J., Taraban, R., & McDon-ald, J. (1989). Language learning: Cues or rules? Journal of Memory and Language, 28, 255-277. Maratsos, M., Kuczaj, S. A., Fox, D. E., & Chalkley, M. A. (1979). Some empirical studies in the acquisition of transformational relations: Passives, negatives, and the past tense. In W. A. Collins (Ed.), Children's lan-guage and communication. Hillsdale, N.J.: Lawrence Erlbaum. Marcus, G. (1993). Negative evidence in language ac-quisition. Cognition, 46, 53-85. Merriman, W. (1999). Competition, attention, and young children's lexical processing. In B. MacWhin-ney (Ed.), The emergence of language (pp. 331-358). Mahwah, NJ: Lawrence Erlbaum. Miikkulainen, R. (1993). Subsymbolic natural language processing. Cambridge, MA: MIT Press. Miikkulainen, R., & Mayberry, M. R. (1999). Disam-biguation and grammar as emergent soft constraints. In B. MacWhinney (Ed.), The emergence of lan-guage (pp. 153-176). Mahwah, NJ: Lawrence Erl-baum Associates. Newport, E., Gleitman, H., & Gleitman, L. (1977). Mother, I'd rather do it myself: Some effects and noneffects of maternal speech style. In C. Snow (Ed.), Talking to children: Language input and ac-quisition. Cambridge: Cambridge University Press. O'Grady, W. (1997). Syntactic development. Chicago: Chicago University Press. Piattelli-Palmarini, M. (1980). Language and learning: the debate between Jean Piaget and Noam Chomsky. Cambridge MA: Harvard University Press. Pinker, S. (1994). The language instinct. New York: William Morrow. Plunkett, K., & Marchman, V. (1993). From rote learn-ing to system building. Cognition, 49, 21-69. Pullum, G., & Scholz, B. (2002). Empirical assessment of stimulus poverty arguments. Linguistic Review, 19, 9-50. Reich, P. (1969). The finiteness of natural language. Language, 45, 831-843. Sagae, K., MacWhinney, B., & Lavie, A. (2004). Add-ing syntactic annotations to transcripts of parent-child dialogs. In LREC 2004 (pp. 1815-1818). Lis-bon: LREC. 
Sakas, W., & Fodor, J. (2001). The structural triggers learner. In S. Bertolo (Ed.), Language acquisition and learnability. New York: Cambridge University Press. Saxton, M. (1997). The Contrast Theory of negative input. Journal of Child Language, 24, 139-161. Shinohara, T. (1994). Rich classes inferable from posi-tive data: length-bounded elementary formal systems. Information and Computation, 108, 175-186. Taraban, R., & McClelland, J. (1987). Conspiracy ef-fects in word pronunciation. Journal of Memory and Language, 26, 608-631. Tomasello, M. (2000). The item-based nature of chil-dren's early syntactic development. Trends in Cogni-tive Sciences, 4, 156-163. Wexler, K. (1998). Very early parameter setting and the unique checking constraint: A new explanation of the optional infinitive stage. Lingua, 106, 23-79. Wexler, K., & Hamburger, H. (1973). On the insuffi-ciency of surface data for the learning of transforma-tional languages. In K. Hintikka (Ed.), Approaches to natural language. Dordrecht-Holland: D. Reidel. Wilson, B., & Peters, A. M. (1988). What are you cookin' on a hot? Movement Constraints in the Speech of a Three-Year-Old Blind Child. Language, 64, No.2, 249-273. Wolfe Quintero, K. (1992). Learnability and the acquisi-tion of extraction in relative clauses and wh-questions. Studies in Second Language Acquisition, 14, 39-70.   
68
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 17?24,
Prague, Czech Republic, June 2007 c?2007 Association for Computational Linguistics
Phon 1.2: A Computational Basis for Phonological  Database Elaboration and Model Testing 
 Yvan Rose1, Gregory J. Hedlund1, Rod Byrne2, Todd Wareham2, Brian MacWhinney3 
1Department of Linguistics  Memorial University of  Newfoundland 
2Department of Computer Science  Memorial University of  Newfoundland 
3Department of Psychology  Carnegie Mellon  University  yrose@mun.ca, ghedlund@cs.mun.ca, rod@cs.mun.ca,  harold@cs.mun.ca, macw@cmu.edu   Abstract This paper discusses a new, open-source software program, called Phon, that is de-signed for the transcription, coding, and analysis of phonological corpora. Phon provides support for multimedia data link-age, segmentation, multiple-blind transcrip-tion, transcription validation, syllabifica-tion, alignment of target and actual forms, and data analysis. All of these functions are available through a user-friendly graphical interface. Phon, available on most com-puter platforms, supports data exchange among researchers with the TalkBank XML document format and the Unicode character set.. This program provides the basis for the elaboration of PhonBank, a database project that seeks to broaden the scope of CHILDES into phonological de-velopment and disorders. 1 Introduction Empirical studies of natural language and language acquisition will always be required in most types of linguistic research. These studies provide the basis for describing languages and linguistic pat-terns. In addition to providing us with baseline data, empirical data allow us to test theoretical, neuro-logical, psychological and computational models. However, the construction of natural language cor-pora is an extremely tedious and resource-consuming process, despite tremendous advances 
in data recording, storage, and coding methods in recent decades.  Thanks to corpora and tools such as those de-veloped in the context of the CHILDES project (http://childes.psy.cmu.edu/), researchers in areas such as morphology and syntax have enjoyed a convenient and powerful method to analyze the morphosyntactic properties of adult languages and their acquisition by first and second language learners. In the area of phonetics, the Praat system  (http://www.fon.hum.uva.nl/praat/) has expanded our abilities to conduct phonological modeling, computational simulations based on a variety of theoretical approaches, and articulatory synthesis.  In this rapidly-expanding software universe, phonologists interested in the organization of sound systems (e.g. phones, syllables, stress and intonational patterns) and their acquisition have not yet enjoyed the same level of computational sup-port. There is no developed platform for phonological analysis and no system for data-sharing parallel to that found in CHILDES. Unfor-tunately, this situation negatively affects the study of natural language phonology and phonological development. It also undermines potential studies pertaining to interfaces between various compo-nents of the grammar or the elaboration of compu-tational models of language or language develop-ment.  It is largely accepted that the grammar is hierar-chically organized such that larger domains (e.g. a sentence or a phrase) provide the conditioning en-vironments for patterns occurring in the domains 
17
located lower in the hierarchy (e.g. the word or the syllable), as indicated in Figure 1.   
  Figure 1: General grammatical hierarchy  This hierarchical view of grammatical organization allows us to make reference to factors that link phonology to syntax. For example, in English, the phonological phrase, a domain that constrains phonological phenomena such as intonation, is best described using syntactic criteria (e.g. Selkirk 1986). Data on the acquisition of these grammati-cal structures and their phonological consequences can help us understand how they are learned and assimilated by the learner.  In this paper we discuss Phon 1.2, the current version of an open-source software program that offers significant methodological advances in re-search in phonology and phonological develop-ment. On the one hand, Phon provides a powerful and flexible solution for phonological corpus elaboration and analysis. On the other hand, its ability to integrate with other open-source software will facilitate the construction of complete analyses across all levels of grammatical organization repre-sented in Figure 1.  The paper is organized as follows. In section 2, we discuss the general motivation behind the Phon project. In section 3, we discuss the current func-tionality supported in Phon 1.2. In section 4, we offer a glance at future plans for this project. Sec-tion 5 provides a final summary.  2 The PhonBank Project PhonBank, the latest initiative within the CHILDES project, focuses on the construction of corpora suitable for phonological and phonetic an-alysis. In this section we first describe the goals and orientations of PhonBank. We then describe Phon, the software project designed to facilitate this endeavor. 
2.1 PhonBank The PhonBank project seeks to broaden the scope of the current CHILDES system to include the analysis of phonological development in first and second languages for language learners with and without language disorders. To achieve this goal, we will create a new phonological database called PhonBank and a program called Phon to facilitate analysis of PhonBank data. Using these tools, re-searchers will be in position to conduct a series of developmental, crosslinguistic, and methodological analyses based on large-scale corpora. 2.2 Phon Phon consists of inter-connected modules that offer functionality to assist the researcher in important tasks related to corpus transcription, coding and analysis. (The main functions supported are dis-cussed in the next section.) The application is developed in Java and is packaged to run on Macintosh (Mac OS X 10.4+) and Windows (Vista not tested yet) platforms.1 Phon is Unicode-compliant, a required feature for the sharing of data transcribed with phonetic sym-bols across computer platforms. Phon can share data with programs which utilize the TalkBank XML schema for their documents such as those provided by the TalkBank and CHILDES projects. Phon is available as free download directly from CHILDES (http://childes.psy.cmu.edu/phon/). At the time of writing these lines, Phon is avail-able in its version 1.1, an iteration of the program that offered a proof of concept for the application envisioned (see Rose et al, 2006). Over the past year, however, we have thoroughly revised signifi-cant portions of the code to refine the functionality, ensure further compatibility with other TalkBank-compliant applications, and streamline the inter-face for better user experience and improved workflow. Despite what the minor version incre-ment (1.1 to 1.2) may imply, the new version, which is currently being tested internally and due for public release in June 2007, offers significant improvements as well as novel and innovative functionality.  
                                                1 Support for the Unix/Linux platform is currently compro-mised, primarily because of licensing issues related to the multimedia functions of the application. 
18
3 Phon 1.2 As illustrated in Figure 2, the general interface of Phon 1.2 consists of a media centre (top left of the interface), a section for metadata (e.g. recorded participants and their linguistic profiles; bottom left) and a Transcript Editor, the interface that pro-vides access to most of the functionality (right).   
 Figure 2: Phon 1.2 General Interface  One of the most significant improvements brought to version 1.2 comes from the integration of common tasks within the same user interface. In the previous version, completely separate inter-faces had to be accessed to achieve the following tasks, all of which are required in the elaboration of any corpus: ? Media linkage and segmentation. ? Data transcription and validation (including support for multiple-blind transcriptions). ? Segmentation of transcribed utterances (into e.g. phrases, words). ? Labeling of transcribed forms for syllabifi-cation. ? Phone and syllable alignment between target (expected) and actual (produced) forms. As a result the user often had to navigate between various modules in order to accomplish relatively simple operations. For example, a simple modifica-tion to a transcription required, in addition to the modification itself, revalidation of the data, and then a verification of the syllabification and align-ment data generated from this revised transcrip-
tion, each of these steps requiring access to and subsequent exit from a separate module.  In Phon 1.2, most of this hurdle has been allevi-ated through an integration of most of the functions into the Transcript Editor, while the others (e.g. media linkage and segmentation; transcript valida-tion) are accessed directly from the general inter-face, without a need to exit the Transcript Editor. In the next subsections, we describe the main func-tions supported by the application.2 3.1 Media linkage and segmentation As mentioned above, linkage of multimedia data and subsequent identification of the portions of the recorded media that are relevant for analysis are now available directly from the application?s main interface. These tasks follow the same logic as similar systems in programs like CLAN (http://childes.psy.cmu.edu/clan/). In addition to its integrated interface, Phon 1.2 offers support for linking different portions to a single transcript to different media files. 3.2 Data transcription The Transcript Editor now incorporates in a single interface access to data transcription and annota-tion, transcription segmentation, syllabification and alignment. This module is illustrated in more detail with the screen shot of a data record (correspond-ing to an utterance) in Figure 3.   
 Figure 3: Data record in Transcript Editor                                                  2 Additional functions, such as user management, are also supported by Phon; we will however restrict ourselves to the most central functions of the program. 
19
As can be seen, the interface incorporates tiers for orthographic and phonetic transcriptions as well as other textual annotations. Phon also provides sup-port for an unlimited number of user-defined fields that can be used for all kinds of textual annotations that may be relevant to the coding of a particular dataset. All fields can be ordered to accommodate specific data visualization needs. Phonetic tran-scriptions are based on the phonetic symbols and conventions of the International Phonetic Associa-tion (IPA). A useful IPA character map is easily accessible from within the application, in the shape of a floating window within which IPA symbols and diacritics are organized into intuitive catego-ries. This map facilitates access to the IPA symbols for which there is no keyboard equivalent.  Target and actual IPA transcriptions are stored internally as strings of phonetic symbols. Each symbol is automatically associated with a set of descriptive features generally accepted in the fields of phonetics and phonology (e.g. bilabial, alveolar, voiced, voiceless, aspirated) (Ladefoged and Mad-dieson, 1996). These features are extremely useful in the sense that they provide series of descriptive labels to each transcribed symbol. The availability of these labels is essential for research involving the grouping of various sounds into natural classes (e.g. voiced consonants; non-high front vowels). The built-in set of features can also be reconfig-ured as needed to fit special research needs. Phon 1.2 is also equipped with functionality to automatically insert IPA Target transcriptions based on the orthographic transcriptions. Citation form IPA transcriptions of these words are cur-rently available for English and French. The Eng-lish forms were obtained from the CMU Pronounc-ing Dictionary (www.speech.cs.cmu.edu/cgi-bin/cmudict); the French forms were obtained from the Lexique Project database (www.lexique.org).  In cases when more than one pronunciation are available from the built-in dictionaries for a given written form (e.g. the present and past tense ver-sions of the English word ?read?), the application provides a quick way to select the wanted form.  Of course, idealized citation forms do not pro-vide accurate fine-grained characterizations of variations in the target language (e.g. dialect-specific pronunciation variants; phonetic details such as degree of aspiration in obstruent stops). They however typically provide a useful general baseline against which patterns can be identified. 
1.1 Media playback and exporting Actual forms (e.g. the forms produced by a lan-guage learner) must be transcribed manually. Tran-script validation, the task described in the next sec-tion, also requires access to the recorded data. To facilitate these tasks, Phon provides direct access to the segmented portions of the media for play-back in each record (see the ?Segment? tier in Fig-ure 3). The beginning and end times of these seg-ments can be edited directly from the record, which facilitates an accurate circumscription of the relevant portions of the recorded media. Finally, Phon can export the segmented portions of the me-dia into a sound file, which enables quick acoustic verifications using sound visualizing software such as Praat (http://www.fon.hum.uva.nl/praat/), SFS (http://www.phon.ucl.ac.uk/resource/sfs/), Signa-lyze (http://www.signalyze.com/) or CSL (http://www.kayelemetrics.com/).  1.2 Transcript validation In projects where only a single transcription of the recorded data is utilized, this transcription can be entered directly in the Transcript Editor. In projects that rely on a multiple-blind transcription method, each transcription for a given form is stored sepa-rately. To appear in the Transcript Editor, a blind transcription must be selected through the Tran-script Validation mode. This interface allows the transcription supervisor (or, in a better setting, a team of supervisors working together) to compare competing transcriptions and resolve divergences. Alternative, non-validated transcriptions are pre-served for data recoverability and verification pur-poses. They are however unavailable for further processing, coding or analysis.  1.3 Transcription segmentation Researchers often wish to divide transcribed utter-ances into specific domains such as the phrase or the word. Phon fulfills this need by incorporating a text segmentation module that enables the identifi-cation of strings of symbols corresponding to such morphosyntactic and phonological domains. For example, using the syllabification module de-scribed immediately below, the researcher can test hypotheses about what domains are relevant for resyllabification processes across words. Word-level segmentation is exemplified in Figure 3, as can be seen from the gray bracketing circumscrib-
20
ing each word. Not readily visible from this inter-face however is the important fact that the bracket-ing enforces a logical organization between Ortho-graphic, IPA Target and IPA Actual forms, the lat-ter two being treated as daughter nodes directly related to their corresponding parent bracketed form in the Orthography tier. This system of tier dependency offers several analytical advantages, for example for the identification of patterns that can relate to a particular grammatical category or position within the utterance. In addition to the textual entry fields just de-scribed, the Transcript Editor contains color-coded graphical representations of syllabification infor-mation for both IPA Target and IPA Actual forms as well as for the segmental and syllabic alignment of these forms.  1.4 Syllabification algorithm Once the researcher has identified the domains that are relevant for analysis, segmentation at the level of the syllable is performed automatically: seg-ments are assigned descriptive syllable labels (visually represented with colors) such as ?onset? or ?coda? for consonants and ?nucleus? for vowels. The program also identifies segmental sequences within syllable constituents (e.g. complex onsets or nuclei). Since controversy exists in both phonetic and phonological theory regarding guidelines for syllabification, the algorithm is parameterized to allow for analytical flexibility. The availability of different parameter settings also enables the re-searcher to test hypotheses on which analysis makes the best prediction for a given dataset. Phon 1.2 contains built-in syllabification algorithms for both English and French. The algorithm for Eng-lish incorporates fine distinctions such as those proposed by Davis and Hammond (1995) for the syllabification of on-glides. Both algorithms are based on earlier work by, e.g. Selkirk (1982) and Kaye and Lowenstamm (1984), the latter also documenting the most central properties of French syllabification. While these algorithms use specific syllable positions such as the left appendix (util-ized to identify strident fricatives at the left-edge of triconsonantal onset clusters; e.g. ?strap?), a simple syllabification algorithm is also supplied, which restricts syllable position to onset, nucleus and coda only. Additional algorithms (for other lan-guages or assuming different syllable constructs) can easily be added to the program. 
Our currently-implemented syllabification algo-rithms use a scheme based on a composition-cascade of seven deterministic FSTs  (Finite State Tools). This cascade takes as input a sequence of phones and produces a sequence of phones and associated syllable-constituent symbols, which is subsequently parsed to create the full multi-level metrical structure. The initial FST in the cascade places syllable nuclei and the subsequent FSTs establish and adjust the boundaries of associated onset- and coda-domains. Changes in the definition of syllable nuclei in the initial FST and/or the or-dering and makeup of the subsequent FSTs give language-specific syllabification algorithms. To ease the development of this cascade, initial FST prototypes were written and tested using the Xerox Finite-State Tool (xFST) (Beesley and Karttunen 2003). However, following the requirements of easy algorithm execution within and integration into Phon, these FSTs were subsequently coded in Java. To date, the implemented algorithm has been tested on corpora from English and French, and has obtained accuracies of almost 100%. Occasionally, the algorithm may produce spuri-ous results or flag symbols as unsyllabified. This is particularly true in the case of IPA Actual forms produced by young language learners, which sometimes contain strings of sounds that are not attested in natural languages. Syllabification is generated on the fly upon transcription of IPA forms; the researcher can thus quickly verify all results and modify them through a contextual menu (represented in Figure 3) whenever needed. Segments that are left unsyllabified are available for all queries on segmental features and strings of segments, but are not available for queries refer-ring to aspects of syllabification (see also Figure 4 for a closer look at the display of syllabification). The syllabification labels can then be used in da-tabase query (for example, to access specific in-formation about syllable onsets or codas). In addi-tion, because the algorithm is sensitive to main and secondary stress marks and domain edges (i.e. first and final syllables), each syllable identified is given a prosodic status and position index. Using the search functions, the researcher can thus use search criteria as precisely defined as, for example, complex onsets realized in word-medial, secon-dary-stressed syllables. This level of functionality is central to the study of several phenomena in phonological acquisition that are determined by the 
21
status of the syllable as stressed or unstressed, or by the position of the syllable within the word (e.g. Inkelas and Rose 2003). 1.5 Alignment algorithm After syllabification, a second algorithm per-forms automatic, segment-by-segment and sylla-ble-by-syllable alignment of IPA-transcribed target and actual forms. Building on featural similarities and differences between the segments in each syl-lable and on syllable properties such as stress, this algorithm automatically aligns corresponding seg-ments and syllables in target and actual forms. It provides alignments for both corresponding sounds and syllables. For example, in the target-actual word pair ?apricot? > ?a_cot?, the algorithm aligns the first and final syllables of each form, and iden-tifies the middle syllable (?pri?) as truncated. This is illustrated in Figure 4. Similarly, in cases of ren-ditions such as ?blow? > ?bolow? the alignment algorithm relates both syllables of the actual form to the only syllable of the target form and diagno-ses a case of vowel epenthesis.   
 Figure 4: Syllabification and Alignment  In this alignment algorithm, forms are viewed as sequences of phones and syllable-boundary mark-ers and the alignment is done on the phones in a way that preserves syllable integrity. This algo-rithm is a variant of the standard dynamic pro-gramming algorithm for pairwise global sequence alignment (see Sankoff and Kruskal 1983 and ref-erences therein); as such, it is similar to but ex-tends the phone-alignment algorithm described in Kondrak (2003). At the core of the Phon alignment algorithm is a function sim(x, y) that assesses the degree of similarity of a symbol x from the first given sequence and a symbol y from the second given sequence. In our sim() function, the similar-ity value of phones x and y is a function of a basic 
score (which is the number of phonetic features shared by x and y) and the associated values of various applicable reward and penalty conditions, each of which encodes a linguistically-motivated constraint on the form of the alignment. There are nine such reward and penalty conditions, and the interaction of these rewards and penalties on phone matchings effectively simulates syllable integrity and matching constraints. Subsequent to this en-hanced phone alignment, a series of rules is in-voked to reintroduce the actual and target form syllable boundaries. A full description of the alignment algorithm is given in Maddocks (2005) and Hedlund et al (2005). Preliminary tests on attested data from the published literature on Dutch- and English-learning children (Fikkert, 1994; Pater, 1997) indi-cate an accuracy rate above 95% (96% for a Dutch corpus and 98% for an English corpus). As it is the case with the other algorithms included in the pro-gram, the user is able to perform manual adjust-ments of the computer-generated syllable align-ments whenever necessary. This process was made as easy as possible: it consists of clicking on the segment that needs to be realigned and moving it leftward or rightward using keyboard arrows.  The alignment algorithm, as well as the data processing steps that precede it (especially, syllabi-fication), are essential to any acquisition study that requires pair-wise comparisons between target and actual forms, from both segmental and syllabic perspectives.  Implicit to the description of the implementation of the syllabification and alignment functions is a careful approach whereby the algorithms imple-mented at this stage are used to assist data compi-lation; because every result generated by the algo-rithms can be modified by the user, no data analy-sis directly depends on them. The user thus has complete control on the processing of the data be-ing readied for analysis. After extensive testing on additional types of data sets, we will be able to op-timize their degree of reliability and then deter-mined how they can be used in truly automated analyses. 1.6 Database query Phon sports a simple search function built directly in the main interface (see Figure 2 above). More complex queries are now supported through a se-ries of built-in analysis and reporting functions. 
22
Using these functions, the research can identify records that contain: ? Phones and phone sequences (defined with IPA symbols or descriptive feature sets). ? Syllable types (e.g. CV, CVC, CGV, ?).3 ? Word types (e.g. number of syllables and the stress patterns that they compose). ? Segmental processes (obtained through fea-tural comparisons between Target-Actual aligned phones; e.g. devoicing, gliding). ? Syllabic processes (obtained through com-parisons between target-actual aligned sylla-bles e.g. complex onset reduction).  Using these functions, the researcher can quickly identify the records that match the search criteria within the transcript. The reported data are visual-ized in tables which can be saved as comma-separated value text files (.csv) that can subse-quently be open in statistical or spreadsheet appli-cations. Using an expression builder, i.e. a system to combine simple searches using functions such as intersection and union, the researcher can also take advantage of more elaborate search criteria. The expression builder thus enables the study of inter-action between factors such as feature combina-tions, stress, position within the syllable, word or any other larger domain circumscribed through the utterance segmentation function described above. 2 Future projects Phon 1.2 now provides all the functionality re-quired for corpus elaboration, as well as a versatile system for data extraction. In future versions, we will incorporate an interface for the management of acoustic data and fuller support for data query-ing and searching.  At a later stage, we will con-struct a system for model testing. We discuss these plans briefly in the next subsections. 2.1 Interface for acoustic data In order to facilitate research that requires acoustic measurements, Phon will also incorporate full in-terfacing with Praat and Speech Filing System, two software programs designed for acoustic analysis of speech sounds. As a result, researchers that util-                                                3 C=consonant; V=vowel; G=glide. 
ize these programs will be able to take advantage of some of Phon?s unique functions and, similarly, researchers using Phon will be able to take advan-tage of the functionality of these two applications. 2.2 Extension of database query functionality The search and report functions described in section 3.8 provide simple and flexible tools to generate general assessments of the corpus or de-tect and extract particular phonological patterns. However, to take full advantage of all of the re-search potential that Phon offers, a more powerful query system will be designed. This system will take the form of a query language supplemented with statistical functions. Such a system will enable precise assessments of developmental data within and across corpora of language learners or learning situations. The query language will also offer the relevant functionality to take full advantage of the module for manage-ment of acoustic data described in the preceding subsection.  2.3 Platform for model testing As presently implemented, Phon will allow us to continue with the construction of PhonBank and will provide tools for analyzing the new database. Once this system is in place, we will begin to de-velop additional tools for model testing. These new systems will formalize learning algorithms in ways that will allow users to run these algorithms on stored data, much as in the ?Learn? feature in Praat. This new model-testing application will in-clude functions such as: ? Run an arbitrary language learning algo-rithm. ? Compare the results of the grammar pro-duced by such a language learning algorithm against actual language data. ? In the event that the learning algorithm pro-vides a sequence of grammars correspond-ing to the stages of human language learn-ing, compare the results of this sequence of grammars against actual longitudinal lan-guage data. By virtue of its software architecture, form-comparison routines, and stored data, Phon pro-vides an excellent platform for implementing such an application. Running arbitrary language learn-
23
ing algorithms could be facilitated using a Java API/interface-class combination specifying sub-routines provided by Phon. The outputs of a given computational model could be compared against adult productions stored in Phon using the align-ment algorithm described in Section 3.7 (which internally produces but does not output a score giv-ing the similarity of the two forms being aligned). Finally, the outputs of a sequence of algorithm-produced grammars relative to a given target word could be compared against the sequence of produc-tions of that word made over the course of acquisi-tion by a particular learner by aligning these pro-duction sequences. Such an alignment could be done using the alignment algorithm described in Section 3.7 as a sim() function for matching up production-pairs in these sequences. In this case, more exotic forms of alignment such as local alignment or time-warping may be more appropri-ate than the global alignment used in Section 3.7. For a full description of such alignment options, see Gusfield (1997) and Sankoff and Kruskal (1983). 3 Discussion In its current form, Phon 1.2 provides a powerful system for corpus transcription, coding and analy-sis. It also offers a sound computational foundation for the elaboration of the PhonBank database and its incorporation to the CHILDES system. Finally, it sets the basis for further improvements of its functionality, some of which was discussed briefly in the preceding section.  The model-testing tool design sketched above is ambitious and perhaps premature in some aspects ?for example, should we expect the current (or even next) generation of language learning algo-rithms to mimic the longitudinal behavior of actual language learners? This question is especially rele-vant given that some language behaviors observed in learners can be driven by articulatory or percep-tual factors, the consideration of which implies relatively more complex models. That being said, the above suggests how Phon, by virtue of its lon-gitudinal data, output-form comparison routines, and software architecture, may provide an excel-lent platform for implementing the next generation of computational language analysis tools. 
References Beesley, K.R. and L. Karttunen (2003) Finite-State Morphology. Stanford CA: CSLI Publications. Davis, S. and M. Hammond (1995). On the Status of Onglides in American English. Phonology 12:159-182. Fikkert, P. (1994). On the Acquisition of Prosodic Structure. Dordrecht: ICG Printing. Gusfield, D. (1997) Algorithms on Strings, Trees, and Sequences: Computer Science and Computational Biology. Cambridge: Cambridge University Press. Hedlund, G.J., K. Maddocks, Y. Rose, and T. Wareham (2005) Natural Language Syllable Alignment: From Conception to Implementation. Proceedings of the Fifteenth Annual Newfoundland Electrical and Com-puter Engineering Conference (NECEC 2005).  Inkelas, S. and Y. Rose (2003). Velar Fronting Revis-ited. Proceedings of the 27th Boston University Con-ference on Language Development. Somerville, MA: Cascadilla Press. 334-345. Kaye, J. and J. Lowenstamm (1984). De la syllabicit?. Forme sonore du langage. Paris: Hermann, 123-161. Kondrak, G. (2003) Phonetic alignment and similarity. Computers and the Humanities 37: 273-291. Ladefoged, P. and I. Maddieson (1996). The Sounds of the World?s Languages. Cambridge, MA: Blackwell. Maddocks, K. (2005) An Effective Algorithm for the Alignment of Target and Actual Syllables for the Study of Language Acquisition. B.Sc.h. Thesis. De-partment of Computer Science, Memorial University of Newfoundland. Pater, J. (1997). Minimal Violation and Phonological Development. Language Acquisition 6, 201-253. Rose, Y., B. MacWhinney, R. Byrne, G. Hedlund, K. Maddocks, P. O?Brien and T. Wareham (2006). In-troducing Phon: A Software Solution for the Study of Phonological Acquisition. Proceedings of the 30th Boston University Conference on Language Devel-opment. Somerville, MA: Cascadilla Press. 489-500. Sankoff, D. and J.B. Kruskal (eds., 1983) Time Warps, String Edits, and Macromolecules: The Theory and Practice of String Comparison. Reading, MA: Addison-Wesley. Selkirk, E. (1982) The Syllable. The Structure of Phonological Representation. Dordrecht: Foris, 337-385. ___ (1986) On Derived domains in Sentence Phonol-ogy. Phonology 3: 371-405. 
24
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 25?32,
Prague, Czech Republic, June 2007 c?2007 Association for Computational Linguistics
High-accuracy Annotation and Parsing of CHILDES Transcripts
Kenji Sagae
Department of Computer Science
University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
sagae@is.s.u-tokyo.ac.jp
Eric Davis
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
dhdavis@cs.cmu.edu
Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
alavie@cs.cmu.edu
Brian MacWhinney
Department of Psychology
Carnegie Mellon University
Pittsburgh, PA 15213
macw@cmu.edu
Shuly Wintner
Department of Computer Science
University of Haifa
31905 Haifa, Israel
shuly@cs.haifa.ac.il
Abstract
Corpora of child language are essential for
psycholinguistic research. Linguistic anno-
tation of the corpora provides researchers
with better means for exploring the develop-
ment of grammatical constructions and their
usage. We describe an ongoing project that
aims to annotate the English section of the
CHILDES database with grammatical re-
lations in the form of labeled dependency
structures. To date, we have produced a cor-
pus of over 65,000 words with manually cu-
rated gold-standard grammatical relation an-
notations. Using this corpus, we have devel-
oped a highly accurate data-driven parser for
English CHILDES data. The parser and the
manually annotated data are freely available
for research purposes.
1 Introduction
In order to investigate the development of child lan-
guage, corpora which document linguistic interac-
tions involving children are needed. The CHILDES
database (MacWhinney, 2000), containing tran-
scripts of spoken interactions between children at
various stages of language development with their
parents, provides vast amounts of useful data for lin-
guistic, psychological, and sociological studies of
child language development. The raw information in
CHILDES corpora was gradually enriched by pro-
viding a layer of morphological information. In par-
ticular, the English section of the database is aug-
mented by part of speech (POS) tags for each word.
However, this information is usually insufficient for
investigations dealing with the syntactic, semantic
or pragmatic aspects of the data.
In this paper we describe an ongoing effort aim-
ing to annotate the English portion of the CHILDES
database with syntactic information based on gram-
matical relations represented as labeled dependency
structures. Although an annotation scheme for syn-
tactic information in CHILDES data has been pro-
posed (Sagae et al, 2004), until now no significant
amount of annotated data had been made publicly
available. In the process of manually annotating sev-
eral thousands of words, we updated the annotation
scheme, mostly by extending it to cover syntactic
phenomena that occur in real data but were unac-
counted for in the original annotation scheme.
The contributions of this work fall into three main
categories: revision and extension of the annota-
tion scheme for representing syntactic information
in CHILDES data; creation of a manually annotated
65,000 word corpus with gold-standard syntactic
analyses; and implementation of a complete parser
that can automatically annotate additional data with
high accuracy. Both the gold-standard annotated
data and the parser are freely available. In addi-
tion to introducing the parser and the data, we re-
port on many of the specific annotation issues that
we encountered during the manual annotation pro-
25
cess, which should be helpful for those who may
use the annotated data or the parser. The anno-
tated corpora and the parser are freely available from
http://childes.psy.cmu.edu/.
We describe the annotation scheme in the next
section, along with issues we faced during the pro-
cess of manual annotation. Section 3 describes the
parser, and an evaluation of the parser is presented in
section 4. We analyze the remaining parsing errors
in section 5 and conclude with some applications of
the parser and directions for future research in sec-
tion 6.
2 Syntactic annotation
The English section of the CHILDES database is
augmented with automatically produced ambiguous
part-of-speech and morphological tags (MacWhin-
ney, 2000). Some of these data have been manually
disambiguated, but we found that some annotation
decisions had to be revised to facilitate syntactic an-
notation. We discuss below some of the revisions we
introduced, as well as some details of the syntactic
constructions that we account for.
2.1 The morphological annotation scheme
The English morphological analyzer incorporated
in CHILDES produces various part-of-speech tags
(there are 31 distinct POS tags in the CHILDES
tagset), including ADJective, ADVerb, COmmuni-
cator, CONJunction, DETerminer, FILler, Noun,
NUMeral, ONomatopoeia, PREPosition, PROnoun,
ParTicLe, QuaNtifier, RELativizer and Verb1. In
most cases, the correct annotation of a word is obvi-
ous from the context in which the word occurs, but
sometimes a more subtle distinction must be made.
We discuss some common problematic issues below.
Adverb vs. preposition vs. particle The words
about, across, after, away, back, down, in, off, on,
out, over, up belong to three categories: ADVerb,
PREPosition and ParTicLe. To correctly annotate
them in context, we apply the following criteria.
First, a preposition must have a prepositional ob-
ject, which is typically realized as a noun phrase
(which may be topicalized, or even elided). Sec-
ond, a preposition forms a constituent with its noun
1We use capital letters to denote the actual tag names in the
CHILDES tagset.
phrase object. Third, a prepositional object can be
fronted (for example, he sat on the chair becomes
the chair on which he sat), whereas a particle-NP
sequence cannot (*the phone number up which he
looked cannot be obtained from he looked up the
phone number). Finally, a manner adverb can be
placed between the verb and a preposition, but not
between a verb and a particle.
To distinguish between an adverb and a particle,
the meaning of the head verb is considered. If the
meaning of the verb and the target word, taken to-
gether, cannot be predicted from the meanings of the
verb and the target word separately, then the target
word is a particle. In all other cases it is an adverb.
Verbs vs. auxiliaries Distinguishing between
Verb and AUXiliary is often straightforward, but
special attention is given when tagging the verbs be,
do and have. If the target word is accompanied by an
non-finite verb in the same clause, as in I have had
enough or I do not like eggs, it is an auxiliary. Ad-
ditionally, in interrogative sentences, the auxiliary is
moved to the beginning of the clause, as in have I
had enough? and do I like eggs?, whereas the main
verb is not. However, this test does not always work
for the verb be, which may head a non-verbal pred-
icate, as in John is a teacher, vs. John is smiling. In
verb-participle constructions headed by the verb be,
if the participle is in the progressive tense, then the
head verb is labeled as auxiliary.
Communicators vs. locative adverbs COmmu-
nicators can be hard to distinguish from locative ad-
verbs, especially at the beginning of a sentence. Our
convention is that CO must modify an entire sen-
tence, so if a word appears by itself, it cannot be a
CO. For example, utterances like here or there are
labeled as ADVerb. However, if these words appear
at the beginning of a sentence, are followed by a
break or pause, and do not clearly express a location,
then they are labeled CO. Additionally, in here/there
you are/go, here and there are labeled CO.
2.2 The syntactic annotation scheme
Our annotation scheme for representing grammati-
cal relations, or GRs (such as subjects, objects and
adjuncts), in CHILDES transcripts is a slightly ex-
tended version of the scheme proposed by Sagae et
al. (2004), which was inspired by a general annota-
26
tion scheme for grammatical relations (Carroll et al,
1998), but adapted specifically for CHILDES data.
Our scheme contains 37 distinct GR types. Sagae
et al reported 96.5% interannotator agreement, and
we do not believe our minor updates to the annota-
tion scheme should affect interannotator agreement
significantly.
The scheme distinguishes among SUBJects, (fi-
nite) Clausal SUBJects2 (e.g., that he cried moved
her) and XSUBJects (eating vegetables is impor-
tant). Similarly, we distinguish among OBJects,
OBJect2, which is the second object of a ditran-
sitive verb, and IOBjects, which are required verb
complements introduced by prepositions. Verb com-
plements that are realized as clauses are labeled
COMP if they are finite (I think that was Fraser) and
XCOMP otherwise (you stop throwing the blocks).
Additionally, we mark required locative adjectival
or prepositional phrase arguments of verbs as LOCa-
tives, as in put the toys in the box/back.
PREDicates are nominal, adjectival or prepo-
sitional complements of verbs such as get, be
and become, as in I?m not sure. Again, we
specifically mark Clausal PREDicates (This is
how I drink my coffee) and XPREDicates (My goal
is to win the competition).
Adjuncts (denoted by JCT) are optional modi-
fiers of verbs, adjectives or adverbs, and we dis-
tinguish among non-clausal ones (That?s much bet-
ter; sit on the stool), finite clausal ones (CJCT, Mary
left after she saw John) and non-finite clausal ones
(XJCT, Mary left after seeing John).
MODifiers, which modify or complement nouns,
again come in three flavors: MOD (That?s a nice
box); CMOD (the movie that I saw was good ); and
XMOD (the student reading a book is tall ).
We then identify AUXiliary verbs, as in did you
do it? ; NEGation (Fraser is not drinking his coffee);
DETerminers (a fly); QUANTifiers (some juice); the
objects of prepositions (POBJ, on the stool); verb
ParTicLes (can you get the blocks out? ); ComPle-
mentiZeRs (wait until the noodles are cool ); COM-
municators (oh, I took it); the INfinitival to; VOCa-
tives (Thank you, Eve); and TAG questions (you
know how to count, don?t you? ).
2As with the POS tags, we use capital letters to represent the
actual GR tags used in the annotation scheme.
Finally, we added some specific relations for han-
dling problematic issues. For example, we use
ENUMeration for constructions such as one, two,
three, go or a, b, c. In COORDination construc-
tions, each conjunct is marked as a dependent of the
conjunction (e.g., go and get your telephone). We
use TOPicalization to indicate an argument that is
topicalized, as in tapioca, there is no tapioca. We
use SeRiaL to indicate serial verbs as in come see
if we can find it or go play with your toys. Finally,
we mark sequences of proper names which form the
same entity (e.g., New York ) as NAME.
The format of the grammatical relation (GR) an-
notation, which we use in the examples that follow,
associates with each word in a sentence a triple i|j|g,
where i is the index of the word in the sentence, j the
index of the word?s syntactic head, and g is the name
of the grammatical relation represented by the syn-
tactic dependency between the i-th and j-th words.
If the topmost head of the utterance is the i-th word,
it is labeled i|0|ROOT. For example, in:
a cookie .
1|2|DET 2|0|ROOT 3|2|PUNCT
the first word a is a DETerminer of word 2 (cookie),
which is itself the ROOT of the utterance.
2.3 Manual annotation of the corpus
We focused our manual annotation on a set of
CHILDES transcripts for a particular child, Eve
(Brown, 1973), and we refer to these transcripts,
distributed in a set of 20 files, as the Eve corpus.
We hand-annotated (including correcting POS tags)
the first 15 files of the Eve corpus following the
GR scheme outlined above. The annotation pro-
cess started with purely manual annotation of 5,000
words. This initial annotated corpus was used to
train a data-driven parser, as described later. This
parser was then used to label an additional 20,000
words automatically, followed by a thorough manual
checking stage, where each syntactic annotation was
manually verified and corrected if necessary. We re-
trained the parser with the newly annotated data, and
proceeded in this fashion until 15 files had been an-
notated and thoroughly manually checked.
Annotating child language proved to be challeng-
ing, and as we progressed through the data, we no-
ticed grammatical constructions that the GRs could
27
not adequately handle. For example, the original GR
scheme did not differentiate between locative argu-
ments and locative adjuncts, so we created a new GR
label, LOC, to handle required verbal locative argu-
ments such as on in put it on the table. Put licenses
a prepositional argument, and the existing JCT rela-
tion could not capture this requirement.
In addition to adding new GRs, we also faced
challenges with telegraphic child utterances lack-
ing verbs or other content words. For instance,
Mommy telephone could have one of several mean-
ings: Mommy this is a telephone, Mommy I want
the telephone, that is Mommy?s telephone, etc. We
tried to be as consistent as possible in annotating
such utterances and determined their GRs from con-
text. It was often possible to determine the VOC
reading vs.the MOD (Mommy?s telephone) reading
by looking at context. If it was not possible to deter-
mine the correct annotation from context, we anno-
tated such utterances as VOC relations.
After annotating the 15 Eve files, we had 18,863
fully hand-annotated utterances, 10,280 adult
and 8,563 child. The utterances consist of 84,226
GRs (including punctuation) and 65,363 words.
The average utterance length is 5.3 words (in-
cluding punctuation) for adult utterances, 3.6 for
child, 4.5 overall. The annotated Eve corpus
is available at http://childes.psy.cmu.
edu/data/Eng-USA/brown.zip. It was used
for the Domain adaptation task at the CoNLL-2007
dependency parsing shared task (Nivre, 2007).
3 Parsing
Although the CHILDES annotation scheme pro-
posed by Sagae et al (2004) has been used in prac-
tice for automatic parsing of child language tran-
scripts (Sagae et al, 2004; Sagae et al, 2005), such
work relied mainly on a statistical parser (Char-
niak, 2000) trained on the Wall Street Journal por-
tion of the Penn Treebank, since a large enough cor-
pus of annotated CHILDES data was not available
to train a domain-specific parser. Having a corpus
of 65,000 words of CHILDES data annotated with
grammatical relations represented as labeled depen-
dencies allows us to develop a parser tailored for the
CHILDES domain.
Our overall parsing approach uses a best-first
probabilistic shift-reduce algorithm, working left-to-
right to find labeled dependencies one at a time. The
algorithm is essentially a dependency version of the
data-driven constituent parsing algorithm for prob-
abilistic GLR-like parsing described by Sagae and
Lavie (2006). Because CHILDES syntactic annota-
tions are represented as labeled dependencies, using
a dependency parsing approach allows us to work
with that representation directly.
This dependency parser has been shown to have
state-of-the-art accuracy in the CoNLL shared tasks
on dependency parsing (Buchholz and Marsi, 2006;
Nivre, 2007)3. Sagae and Tsujii (2007) present a
detailed description of the parsing approach used in
our work, including the parsing algorithm. In sum-
mary, the parser uses an algorithm similar to the LR
parsing algorithm (Knuth, 1965), keeping a stack of
partially built syntactic structures, and a queue of
remaining input tokens. At each step in the pars-
ing process, the parser can apply a shift action (re-
move a token from the front of the queue and place
it on top of the stack), or a reduce action (pop the
two topmost stack items, and push a new item com-
posed of the two popped items combined in a sin-
gle structure). This parsing approach is very similar
to the one used successfully by Nivre et al (2006),
but we use a maximum entropy classifier (Berger et
al., 1996) to determine parser actions, which makes
parsing extremely fast. In addition, our parsing ap-
proach performs a search over the space of possible
parser actions, while Nivre et al?s approach is de-
terministic. See Sagae and Tsujii (2007) for more
information on the parser.
Features used in classification to determine
whether the parser takes a shift or a reduce action
at any point during parsing are derived from the
parser?s current configuration (contents of the stack
and queue) at that point. The specific features used
are:4
? Word and its POS tag: s(1), q(2), and q(1).
? POS: s(3) and q(2).
3The parser used in this work is the same as the probabilistic
shift-reduce parser referred to as ?Sagae? in the cited shared
task descriptions. In the 2007 shared task, an ensemble of shift-
reduce parsers was used, but only a single parser is used here.
4s(n) denotes the n-th item from the top of the stack (where
s(1) is the item on the top of the stack), and q(n) denotes the
n-th item from the front of the queue.
28
? The dependency label of the most recently at-
tached dependent of: s(1) and s(2).
? The previous parser action.
4 Evaluation
4.1 Methodology
We first evaluate the parser by 15-fold cross-
validation on the 15 manually curated gold-standard
Eve files (to evaluate the parser on each file, the re-
maining 14 files are used to train the parser). Single-
word utterances (excluding punctuation) were ig-
nored, since their analysis is trivial and their inclu-
sion would artificially inflate parser accuracy mea-
surements. The size of the Eve evaluation corpus
(with single-word utterances removed) was 64,558
words (or 59,873 words excluding punctuation). Of
these, 41,369 words come from utterances spoken
by adults, and 18,504 come from utterances spo-
ken by the child. To evaluate the parser?s portabil-
ity to other CHILDES corpora, we also tested the
parser (trained only on the entire Eve set) on two ad-
ditional sets, one taken from the MacWhinney cor-
pus (MacWhinney, 2000) (5,658 total words, 3,896
words in adult utterances and 1,762 words in child
utterances), and one taken from the Seth corpus (Pe-
ters, 1987; Wilson and Peters, 1988) (1,749 words,
1,059 adult and 690 child).
The parser is highly efficient: training on the en-
tire Eve corpus takes less that 20 minutes on stan-
dard hardware, and once trained, parsing the Eve
corpus takes 18 seconds, or over 3,500 words per
second.
Following recent work on dependency parsing
(Nivre, 2007), we report two evaluation measures:
labeled accuracy score (LAS) and unlabeled accu-
racy score (UAS). LAS is the percentage of tokens
for which the parser predicts the correct head-word
and dependency label. UAS ignores the dependency
labels, and therefore corresponds to the percentage
of words for which the correct head was found. In
addition to LAS and UAS, we also report precision
and recall of certain grammatical relations.
For example, compare the parser output of go buy
an apple to the gold standard (Figure 1). This se-
quence of GRs has two labeled dependency errors
and one unlabeled dependency error. 1|2|COORD
for the parser versus 1|2|SRL is a labeled error be-
cause the dependency label produced by the parser
(COORD) does not match the gold-standard anno-
tation (SRL), although the unlabeled dependency is
correct, since the headword assignment, 1|2, is the
same for both. On the other hand, 5|1|PUNCT ver-
sus 5|2|PUNCT is both a labeled dependency error
and an unlabeled dependency error, since the head-
word assignment produced by the parser does not
match the gold-standard.
4.2 Results
Trained on domain-specific data, the parser per-
formed well on held-out data, even though the train-
ing corpus is relatively small (about 60,000 words).
The results are listed in Table 1.
LAS UAS
Eve cross-validation 92.0 93.8
Table 1: Average cross-validation results, Eve
The labeled dependency error rate is about 8%
and the unlabeled error rate is slightly over 6%. Per-
formance in individual files ranged between the best
labeled error rate of 6.2% and labeled error rate of
4.4% for the fifth file, and the worst error rates of
8.9% and 7.8% for labeled and unlabeled respec-
tively in the fifteenth file. For comparison, Sagae et
al. (2005) report 86.9% LAS on about 2,000 words
of Eve data, using the Charniak (2000) parser with
a separate dependency-labeling step. Part of the rea-
son we obtain levels of accuracy higher than usu-
ally reported for dependency parsers is that the aver-
age sentence length in CHILDES transcripts is much
lower than in, for example, newspaper text. The av-
erage sentence length for adult utterances in the Eve
corpus is 6.1 tokens, and 4.3 tokens for child utter-
ances5.
Certain GRs are easily identifiable, such as DET,
AUX, and INF. The parser has precision and recall
of nearly 1.00 for those. For all GRs that occur more
than 1,000 times in the Eve corpus (which contrains
more than 60,000 tokens), precision and recall are
above 0.90, with the exception of COORD, which
5This differs from the figures in section 2.3 because for the
purpose of parser evaluation we ignore sentences composed
only of a single word plus punctuation.
29
go buy an apple .
parser: 1|2|COORD 2|0|ROOT 3|4|DET 4|2|OBJ 5|1|PUNCT
gold: 1|2|SRL 2|0|ROOT 3|4|DET 4|2|OBJ 5|2|PUNCT
Figure 1: Example output: parser vs. gold annotation
occurs 1,163 times in the gold-standard data. The
parser?s precision for COORD is 0.73, and recall
is 0.84. Other interesting GRs include SUBJ, OBJ,
JCT (adjunct), COM, LOC, COMP, XCOMP, CJCT
(subordinate clause acting as an adjunct), and PTL
(verb particle, easily confusable with prepositions
and adverbs). Their precision and recall is shown
in table 2.
GR Precision Recall F-score
SUBJ 0.96 0.96 0.96
OBJ 0.93 0.94 0.93
JCT 0.91 0.90 0.90
COM 0.96 0.95 0.95
LOC 0.95 0.90 0.92
COMP 0.83 0.86 0.84
XCOMP 0.86 0.87 0.87
CJCT 0.61 0.59 0.60
PTL 0.97 0.96 0.96
COORD 0.73 0.84 0.78
Table 2: Precision, recall and f-score of selected
GRs in the Eve corpus
We also tested the accuracy of the parser on child
utterances and adult utterances separately. To do
this, we split the gold standard files into child and
adult utterances, producing gold standard files for
both child and adult utterances. We then trained
the parser on 14 of the 15 Eve files with both child
and adult utterances, and parsed the individual child
and adult files. Not surprisingly, the parser per-
formed slightly better on the adult utterances due to
their grammaticality and the fact that there was more
adult training data than child training data. The re-
sults are listed in Table 3.
LAS UAS
Eve - Child 90.0 91.7
Eve - Adult 93.1 94.8
Table 3: Average child vs. adult results, Eve
Our final evaluation of the parser involved test-
ing the parser on data taken from a different parts of
the CHILDES database. First, the parser was trained
on all gold-standard Eve files, and tested on man-
ually annotated data taken from the MacWhinney
transcripts. Although accuracy was lower for adult
utterances (85.8% LAS) than on Eve data, the accu-
racy for child utterances was slightly higher (92.3%
LAS), even though child utterances were longer on
average (4.7 tokens) than in the Eve corpus.
Finally, because a few aspects of the many tran-
script sets in the CHILDES database may vary in
ways not accounted for in the design of the parser
or the annotation of the training data, we also re-
port results on evaluation of the Eve-trained parser
on a particularly challenging test set, the Seth cor-
pus. Because the Seth corpus contains transcriptions
of language phenomena not seen in the Eve corpus
(see section 5), parser performance is expected to
suffer. Although accuracy on adult utterances is high
(92.2% LAS), accuracy on child utterances is very
low (72.7% LAS). This is due to heavy use of a GR
label that does not appear at all in the Eve corpus
that was used to train the parser. This GR is used to
represent relations involving filler syllables, which
appear in nearly 45% of the child utterances in the
Seth corpus. Accuracy on the sentences that do not
contain filler syllables is at the same level as in the
other corpora (91.1% LAS). Although we do not ex-
pect to encounter many sets of transcripts that are as
problematic as this one in the CHILDES database, it
is interesting to see what can be expected from the
parser under unfavorable conditions.
The results of the parser on the MacWhinney and
Seth test sets are summarized in table 4, where Seth
(clean) refers to the Seth corpus without utterances
that contain filler sylables.
5 Error Analysis
A major source for parser errors on the Eve cor-
pus (112 out of 5181 errors) was telegraphic speech,
30
LAS UAS
MacWhinney - Child 92.3 94.8
MacWhinney - Adult 85.8 89.4
MacWhinney - Total 88.0 91.2
Seth - Child 72.7 82.0
Seth - Adult 92.2 94.4
Seth - Total 84.6 89.5
Seth (clean) - Child 91.1 92.7
Seth (clean) - Total 92.0 93.9
Table 4: Training on Eve, testing on MacWhinney
and Seth
as in Mommy telephone or Fraser tape+recorder
floor. Telegraphic speech may be the most chal-
lenging, since even for a human annotator, deter-
mining a GR is difficult. The parser usually labeled
such utterances with the noun as the ROOT and the
proper noun as the MOD, while the gold annotation
is context-dependent as described above.
Another category of errors, with about 150 in-
stances, is XCOMP errors. The majority of the er-
rors in this category revolve around dropped words
in the main clause, for example want eat cookie. Of-
ten, the parser labels such utterances with COMP
GRs, because of the lack of to. Exclusive training on
utterances of this type may resolve the issue. Many
of the errors of this type occur with want : the parser
could be conditioned to assign an XCOMP GR with
want as the ROOT of an utterance.
COORD and PRED errors would both benefit
from more data as well. The parser performs ad-
mirably on simple coordination and predicate con-
structions, but has troubles with less common con-
structions such as PRED GRs with get, e.g., don?t
let your hands get dirty (69 errors), and coordina-
tion of prepositional objects, as in a birthday cake
with Cathy and Becky (154 errors).
The performance drop on the Seth corpus can be
explained by a number of factors. First and fore-
most, Seth is widely considered in the literature to
be the child who is most likely to invalidate any the-
ory (Wilson and Peters, 1988). He exhibits false
starts and filler syllables extensively, and his syn-
tax violates many ?universal? principles. This is
reflected in the annotation scheme: the Seth cor-
pus, following the annotation of Peters (1983), is
abundant with filler syllables. Because there was
no appropriate GR label for representing the syn-
tactic relationships involving the filler syllables, we
annotated those with a special GR (not used during
parser training), which the parser is understandably
not able to produce. Filler syllables usually occur
near the start of the sentence, and once the parser
failed to label them, it could not accurately label the
remaining GRs. Other difficulties in the Seth cor-
pus include the usage of dates, of which there were
no instances in the Eve corpus. The parser had not
been trained on the new DATE GR and subsequently
failed to parse it.
6 Conclusion
We described an annotation scheme for represent-
ing syntactic information as grammatical relations
in CHILDES data, a manually curated gold-standard
corpus of 65,000 words annotated according to this
GR scheme, and a parser that was trained on the an-
notated corpus and produces highly accurate gram-
matical relations for both child and adult utterances.
These resources are now freely available to the re-
search community, and we expect them to be in-
strumental in psycholinguistic investigations of lan-
guage acquisition and child language.
Syntactic analysis of child language transcripts
using a GR scheme of this kind has already been
shown to be effective in a practical setting, namely
in automatic measurement of syntactic development
in children (Sagae et al, 2005). That work relied on
a phrase-structure statistical parser (Charniak, 2000)
trained on the Penn Treebank, and the output of that
parser had to be converted into CHILDES grammat-
ical relations. Despite the obvious disadvantage of
using a parser trained on a completely different lan-
guage genre, Sagae et al (2005) demonstrated how
current natural language processing techniques can
be used effectively in child language work, achiev-
ing results that are close to those obtained by man-
ual computation of syntactic development scores for
child transcripts. Still, the use of tools not tailored
for child language and extra effort necessary to make
them work with community standards for child lan-
guage transcription present a disincentive for child
language researchers to incorporate automatic syn-
tactic analysis into their work. We hope that the GR
31
representation scheme and the parser presented here
will make it possible and convenient for the child
language community to take advantage of some of
the recent developments in natural language parsing,
as was the case with part-of-speech tagging when
CHILDES specific tools were first made available.
Our immediate plans include continued improve-
ment of the parser, which can be achieved at least in
part by the creation of additional training data from
other English CHILDES corpora. We also plan to
release automatic syntactic analyses for the entire
English portion of CHILDES.
Although we have so far focused exclusively on
English CHILDES data, dependency schemes based
on functional relationships exist for a number of lan-
guages (Buchholz and Marsi, 2006), and the general
parsing techniques used in the present work have
been shown to be effective in several of them (Nivre
et al, 2006). As future work, we plan to adapt
existing dependency-based annotation schemes and
apply our current syntactic annotation and pars-
ing framework to other languages in the CHILDES
database.
Acknowledgments
We thank Marina Fedner for her help with annota-
tion of the Eve corpus. This work was supported in
part by the National Science Foundation under grant
IIS-0414630.
References
A. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996.
Amaximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
Roger Brown. 1973. A first language: the early stages.
George Allen & Unwin Ltd., London.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning (CoNLL-X), pages 149?164,
New York City, June. Association for Computational
Linguistics.
John Carroll, Edward Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: a survey and a new proposal.
In Proceedings of the 1st International Conference on
Language Resources and Evaluation, pages 447?454,
Granada, Spain.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the first conference on North
American chapter of the Association for Computa-
tional Linguistics, pages 132?139, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
D. Knuth. 1965. On the translation of languages from
left to right. Information and Control, 8(6):607?639.
Brian MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum Associates,
Mahwah, NJ, third edition.
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector ma-
chines. In Proceedings of the Tenth Conference on
Computational Natural Language Learning.
Joakim Nivre, editor. 2007. CoNLL-XI Shared Task on
Multilingual Dependency Parsing, Prague, June. As-
sociation for Computational Linguistics.
Ann M. Peters. 1983. The Units of Language Acquisi-
tion. Monographs in Applied Psycholinguistics. Cam-
bridge University Press, New York.
Ann M. Peters. 1987. The role of immitation in the de-
veloping syntax of a blind child. Text, 7:289?311.
Kenji Sagae and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In Proceedings of the
COLING/ACL 2006 Main Conference Poster Sessions,
pages 691?698, Sydney, Australia, July. Association
for Computational Linguistics.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with lr models and parser
ensembles. In Proceedings of the Eleventh Conference
on Computational Natural Language Learning.
Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2004.
Adding syntactic annotations to transcripts of parent-
child dialogs. In Proceedings of the Fourth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC 2004), Lisbon, Portugal.
Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2005.
Automatic measurement of syntactic development in
child language. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?05), pages 197?204, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
B. Wilson and Ann M. Peters. 1988. What are you
cookin? on a hot?: A three-year-old blind child?s ?vi-
olation? of universal constraints on constituent move-
ment. Language, 64:249?273.
32
Proceedings of the EACL 2012 Workshop on Computational Models of Language Acquisition and Loss, pages 20?22,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
A Morphologically Annotated Hebrew CHILDES Corpus
Aviad Albert
Linguistics
Tel Aviv Uni.
Israel
Brian MacWhinney
Psychology
Carnegie Mellon Uni.
USA
Bracha Nir
Communication Sciences
Uni. of Haifa
Israel
Shuly Wintner
Computer Science
Uni. of Haifa
Israel
Abstract We present a corpus of transcribed
spoken Hebrew that reflects spoken interactions
between children and adults. The corpus is an
integral part of the CHILDES database, which
distributes similar corpora for over 25 languages.
We introduce a dedicated transcription scheme for
the spoken Hebrew data that is sensitive to both
the phonology and the standard orthography of
the language. We also introduce a morphologi-
cal analyzer that was specifically developed for
this corpus. The analyzer adequately covers the
entire corpus, producing detailed correct analyses
for all tokens. Evaluation on a new corpus reveals
high coverage as well. Finally, we describe a mor-
phological disambiguation module that selects the
correct analysis of each token in context. The re-
sult is a high-quality morphologically-annotated
CHILDES corpus of Hebrew, along with a set of
tools that can be applied to new corpora.
CHILDES We present a corpus of transcribed
spoken Hebrew that forms an integral part of
a comprehensive data system that has been de-
veloped to suit the specific needs and inter-
ests of child language researchers: CHILDES
(MacWhinney, 2000). CHILDES is a system of
programs and codes designed to facilitate the pro-
cess of free speech analysis. It involves three
integrated components: 1. CHAT, a system for
discourse notation and coding, designed to ac-
commodate a large variety of analyses, while
still permitting a barebones form of transcription;
2. CLAN, a set of computer programs; and 3. A
large, internationally recognized database of lan-
guage transcripts formatted in CHAT. These in-
clude child-caretaker interactions from normally-
developing children, children with language dis-
orders, adults with aphasia, learners of second
languages, and bilinguals who have been exposed
to language in early childhood. Researchers can
directly test a vast range of empirical hypotheses
against data from nearly a hundred major research
projects. While about half of the CHILDES cor-
pus consists of English data, there is also a signif-
icant body of transcripts in 25 other languages.
Corpus We focus on the Hebrew section of
CHILDES, consisting of two corpora: the Ber-
man longitudinal corpus, with data from four chil-
dren between the ages of 1;06 and 3;05 (Berman
and Weissenborn, 1991), and the Ravid longitudi-
nal corpus, with data from two siblings between
the ages of 0;09 to around 6 years of age. The
corpora consist of 110,819 utterances comprising
of 417,938 word-tokens (13,828 word-types).
Transcription The Hebrew data are transcribed
with a Latin-based phonemic transcription (Nir
et al, 2010). We use a set of monoglyph Unicode
characters (mostly in line with standard IPA con-
ventions) that has already been applied for other
complex scripts. In contrast to previous tran-
scription methods, the current transcription re-
flects phonemic, orthographic and prosodic fea-
tures. The advantages of our approach in reducing
ambiguity are:
? Unlike the standard script, our phonemic
transcriptions includes the five vowels of Mod-
ern Hebrew, and prosodic information on primary
stress location, thereby yielding fewer ambigui-
ties that stem from homographs.
? At the same time, we retain valuable phone-
mic and phonetic distinctions that are standard in
the orthography but are no longer distinct in Mod-
ern Hebrew speech (e.g., t /t., k /q, P/Q).
? We separate and mark prefix particles, mak-
ing it easier to recognize them as separate mor-
phemes, which never participate in homographs.
20
Our transcription thus conforms to the three
major goals which the CHAT format is designed
to achieve (MacWhinney, 1996): systematicity
and clarity, human and computerized readability,
and ease of data entry.
Morphological Analysis CLAN includes a lan-
guage for expressing morphological grammars,
implemented as a system, MOR, for the construc-
tion of morphological analyzers. A MOR gram-
mar consists of three components: a set of lexi-
cons specifying lexical entries (base lexemes) and
lists of affixes; a set of rules that govern allomor-
phic changes in the stems of lexical entries (A-
rules); and a set of rules that govern linear affixa-
tion processes by concatenation (C-rules).
Different languages vary in their requirements
and their need to utilize these MOR devices.
The Hebrew MOR extensively uses all of them
in order to account for vocalic and consonantal
changes of the stem allomorphs (handled within
the A-Rules), and the proper affixation possibili-
ties (via the C-rules and affix lists).
The lexicon includes over 5,800 entries, in
16 part-of-speech (POS) categories. Lexically-
specified information includes root and pattern
(for verbs mainly), gender (for nouns), plural suf-
fix (for nouns), and other information that cannot
be deduced from the form of the word. Over 1,000
A-rules describe various allomorphs of morpho-
logical paradigms, listing their morphological and
morphosyntactic features, including number, gen-
der, person, nominal status, tense, etc. Lexical en-
tries then instantiate the paradigms described by
the rules, thereby generating specific allomorphs.
These, in turn, can combine with affixes via over
100 C-rules that govern the morphological alter-
nations involved in affixation.
Results and Evaluation The corpora include
over 400,000 word tokens (about 14,000 types).
More than 27,000 different morphological analy-
ses are produced for the tokens observed in the
corpus; however, we estimate that the application
of the morphological rules to our lexicon would
result in hundreds of thousands of forms, so that
the coverage of the MOR grammar is substan-
tially wider. The grammar fully covers our cur-
rent corpus. Figure 1 depicts a small fragment of
a morphologically-annotated corpus.
To evaluate the coverage of the grammar, we
applied it to a new corpus that is currently being
transcribed. Of the 10,070 tokens in this corpus,
176 (1.75%) do not obtain an analysis (77 of the
1431 types, 5.3%). While some analyses may be
wrong, we believe that most of them are valid, and
that the gaps can be attributed mostly to missing
lexical entries and inconsistent transcription.
As another evaluation method, we developed a
program that converts the transcription we use to
the standard Hebrew script. We then submit the
Hebrew forms to the MILA morphological ana-
lyzer (Itai and Wintner, 2008), and compare the
results. The mismatch rate is 11%. While few
mismatches indeed indicate errors in the MOR
grammar, many are attributable to problems with
the MILA analyzer or the conversion and compar-
ison script.
Morphological Disambiguation The MOR
grammar associates each surface form with all its
possible analyses, independently of the context.
This results in morphological ambiguity. The
level of ambiguity is much lower than that of the
standard Hebrew script, especially due to the vo-
calic information encoded in the transcription, but
several forms are still ambiguous. These include
frequent words that can function both as nouns,
adjectives or adverbs and as communicators (e.g.,
yo?fi ?beauty/great!?, t.ov ?good/OK?); verbs
whose tense is ambiguous (e.g., baP ?come? can
be either present or past); etc.
We manually disambiguated 18 of the 304 files
in the corpus, and used them to train a POS tag-
ger with tools that are embedded in CLAN (POS-
TRAIN and POST). We then automatically disam-
biguated the remaining files. Preliminary evalua-
tion shows 80% accuracy on ambiguous tokens.
Future Plans Our ultimate plan is to add syn-
tactic annotation to the transcripts. We have de-
vised a syntactic annotation scheme, akin to the
existing scheme used for the English section of
CHILDES (Sagae et al, 2010), but with special
consideration for Hebrew constructions that are
common in the corpora. We have recently begun
to annotate the corpora according to this scheme.
Acknowledgments This research was sup-
ported by Grant No. 2007241 from the United
States-Israel Binational Science Foundation
(BSF). We are grateful to Arnon Lazerson for
developing the conversion script, and to Shai
Gretz for helping with the manual annotation.
21
Figure 1: A fragment of the annotated corpus
References
Ruth A. Berman and Ju?rgen Weissenborn. Acquisi-
tion of word order: A crosslinguistic study. Final
Report. German-Israel Foundation for Research and
Development (GIF), 1991.
Alon Itai and Shuly Wintner. Language resources for
Hebrew. Language Resources and Evaluation, 42
(1):75?98, March 2008.
Brian MacWhinney. The CHILDES system. Ameri-
can Journal of Speech Language Pathology, 5:5?14,
1996.
Brian MacWhinney. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum Associates,
Mahwah, NJ, third edition, 2000.
Bracha Nir, Brian MacWhinney, and Shuly Wintner.
A morphologically-analyzed CHILDES corpus of
Hebrew. In Proceedings of the Seventh conference
on International Language Resources and Evalua-
tion (LREC?10), pages 1487?1490, Valletta, Malta,
May 2010. European Language Resources Associ-
ation (ELRA). ISBN 2-9517408-6-7.
Kenji Sagae, Eric Davis, Alon Lavie, Brian MacWhin-
ney, and Shuly Wintner. Morphosyntactic
annotation of CHILDES transcripts. Jour-
nal of Child Language, 37(3):705?729, 2010.
doi: 10.1017/S0305000909990407. URL
http://journals.cambridge.org/
article_S0305000909990407.
22
