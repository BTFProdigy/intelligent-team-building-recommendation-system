Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 442?449,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improved Pronunciation Features for Construct-driven Assessment of
Non-native Spontaneous Speech
Lei Chen, Klaus Zechner, Xiaoming Xi
Educational Testing Service
Princeton, NJ, USA
{LChen,KZechner,XXi}@ets.org
Abstract
This paper describes research on automatic as-
sessment of the pronunciation quality of spon-
taneous non-native adult speech. Since the
speaking content is not known prior to the
assessment, a two-stage method is developed
to first recognize the speaking content based
on non-native speech acoustic properties and
then forced-align the recognition results with
a reference acoustic model reflecting native
and near-native speech properties. Features
related to Hidden Markov Model likelihoods
and vowel durations are extracted. Words with
low recognition confidence can be excluded
in the extraction of likelihood-related fea-
tures to minimize erroneous alignments due
to speech recognition errors. Our experiments
on the TOEFL R?Practice Online test, an En-
glish language assessment, suggest that the
recognition/forced-alignment method can pro-
vide useful pronunciation features. Our new
pronunciation features are more meaningful
than an utterance-based normalized acoustic
model score used in previous research from a
construct point of view.
1 Introduction
Automated systems for evaluating highly pre-
dictable speech (e.g. read speech or speech that
is quite constrained in the use of vocabulary and
syntactic structures) have emerged in the past
decade (Bernstein, 1999; Witt, 1999; Franco et al,
2000; Hacker et al, 2005) due to the growing matu-
rity of speech recognition and processing technolo-
gies. However, endeavors into automated scoring
for spontaneous speech have been sparse given the
challenge of both recognizing and assessing spon-
taneous speech. This paper addresses the develop-
ment and evaluation of pronunciation features for an
automated system for scoring spontaneous speech.
This system was deployed for the TOEFL R?Practice
Online (TPO) assessment used by prospective test
takers to prepare for the official TOEFL R?test.
A construct is a set of knowledge, skills, and abil-
ities measured by a test. The construct of the speak-
ing test is embodied in the rubrics that human raters
use to score the test. It consists of three key cat-
egories: delivery, language use, and topic devel-
opment. Delivery refers to the pace and the clar-
ity of the speech, including performance on into-
nation, rhythm, rate of speech, and degree of hesi-
tancy. Language use refers to the range, complex-
ity, and precision of vocabulary and grammar use.
Topic development refers to the coherence and full-
ness of the response. Most of the research on spon-
taneous speech assessment focuses on the delivery
aspect given the low recognition accuracy on non-
native spontaneous speech.
The delivery aspect can be measured on four di-
mensions: fluency, intonation, rhythm, and pronun-
ciation. For the TPO assessment, we have defined
pronunciation as the quality of vowels, consonants
and word-level stress (segmentals). Intonation and
sentence-level stress patterns (supra-segmentals) are
not defined as part of pronunciation. Pronuncia-
tion is one of the key factors that impact the intelli-
gibility and perceived comprehensibility of speech.
Because pronunciation plays an important role in
speech perception, features measuring pronuncia-
442
tion using speech technologies have been explored
in many previous studies. However, the bulk of the
research on automatic pronunciation evaluation con-
cerns read speech or highly predictable speech (Witt,
1999; Franco et al, 2000; Hacker et al, 2005),
where there is a high possibility of success in speech
recognition. Automatic pronunciation evaluation is
challenging for spontaneous speech and has been
under-explored.
In this paper, we will describe a method for
extracting pronunciation features based on sponta-
neous speech that is well motivated by theories and
supported by empirical evaluations of feature per-
formance. In conceptualizing and computing these
features, we draw on the literature on automatic pro-
nunciation evaluation for constrained speech. As de-
scribed in the related work in Section 2, the widely
used features for measuring pronunciation are (1)
likelihood (posterior probability) of a phoneme be-
ing spoken given the observed audio sample that
is computed in a Viterbi decoding process, and (2)
phoneme length measurements that are compared to
standard references based on native speech.
However, we have also come up with unique solu-
tions to address the issue of relatively low accuracy
in recognizing spontaneous speech. Our methods of
feature extraction are designed with considerations
of how to best capture the quality of pronunciation
given technological constraints.
The remainder of the paper is organized as fol-
lows: Section 2 reviews the related research; Sec-
tion 3 describes our method to extract a set of fea-
tures for measuring pronunciation; Section 4 de-
scribes the design of the experiments, including the
questions investigated, the data, the speech process-
ing technologies, and the measurement metrics; Sec-
tion 5 reports on the experimental results; Section 6
discusses the experimental results; and Section 7
summaries the findings and future research planned.
2 Related work
There is previous research on utilizing speech recog-
nition technology to automatically assess non-native
speakers? communicative competence (e.g., fluency,
intonation, and pronunciation). Witt (Witt, 1999)
developed the Goodness of Pronunciation (GOP)
measurement for measuring pronunciation based on
Hidden Markov Model (HMM) log likelihood. Us-
ing a similar method, Neumeyer et al (Neumeyer et
al., 2000) designed a series of likelihood related pro-
nunciation features, e.g., the local average likelihood
and global average likelihood. Hacker et al (Hacker
et al, 2005) utilized a relatively large feature vector
for scoring pronunciation.
Pronunciation has been the focus of assessment in
several automatic speech scoring systems. Franco et
al. (Franco et al, 2000) presented a system for au-
tomatic evaluation of pronunciation quality on the
phoneme level and the sentence level of speech by
native and non-native speakers of English and other
languages (e.g., French). A forced alignment be-
tween the speech read by subjects and the ideal path
through the HMM was computed. Then, the log
posterior probabilities for a certain position in the
signal were computed to achieve a local pronunci-
ation score. Cucchiarini et al (Cucchiarini et al,
1997a; Cucchiarini et al, 1997b) designed a system
for scoring Dutch pronunciation along a similar line.
Their pronunciation feature set was more extensive,
including various log likelihood HMM scores and
phoneme duration scores. In these two systems, the
speaking skill scores computed on features by ma-
chine are found to have good agreement with scores
provided by humans.
A limited number of studies have been conducted
on assessing speaking proficiency based on sponta-
neous speech. Moustroufas and Digalakis (Mous-
troufas and Digalakis, 2007) designed a system to
automatically evaluate the pronunciation of foreign
speakers using unknown text. The difference in the
recognition results between a recognizer trained on
speakers? native languages (L1) and another recog-
nizer trained on their learned languages (L2) was
used for pronunciation scoring. Zechner and Be-
jar (Zechner and Bejar, 2006) presented a system
to score non-native spontaneous speech using fea-
tures derived from the recognition results. Follow-
ing their work, an operational assessment system,
SpeechRaterTM , was implemented with further im-
provements (Zechner et al, 2007).
There are some issues with the method to extract
pronunciation features in the previous research on
automated assessment of spontaneous speech (Zech-
ner and Bejar, 2006; Zechner et al, 2007). For ex-
443
ample, the acoustic model (AM) that was used to es-
timate a likelihood of a phoneme being spoken was
well-fitted to non-native speech acoustic properties.
Further, other important aspects of pronunciation,
e.g., vowel duration, have not been utilized as a fea-
ture in the current SpeechRaterTMsystem. Likeli-
hoods estimated on non-words (such as silences and
fillers) that were not central to the measurement of
pronunciation were used in the feature extraction. In
addition, mis-recognized words lead to wrong like-
lihood estimation. Our paper attempts to address all
of these limitations described above.
3 Extraction of Pronunciation Features
Figure 1 depicts our new method for extracting an
expanded set of pronunciation features in a more
meaning way.
Figure 1: Two-stage pronunciation feature extraction
We used two different AMs for pronunciation fea-
ture extraction. First, we used an AM optimized
for speech recognition (typically an AM adapted
on non-native speech to better fit non-native speak-
ers? acoustics patterns) to generate word hypotheses;
then we used the other AM optimized for pronun-
ciation scoring (typically trained on native or near-
native speech to be a good reference model reflect-
ing expected speech characteristics) to force align
the speech signals to the word hypotheses and to
compute the likelihoods of individual words being
spoken and durations of phonemes; finally new pro-
nunciation features were extracted based on these
measurements.
Some notations used for computing the pronunci-
ation features are listed in Table 1. Based on these
notations, the proposed new pronunciation features
are described in Table 2. To address the limita-
tions of previous research on automated assessment
of pronunciation, which was described in Section 2,
our proposed method has achieved improvements on
(1) using the two-stage method to compute HMM
likelihoods using a reference acoustic model trained
on native and near-native speech, (2) expanding the
coverage of pronunciation features by using vowel
duration shifts that are compared to standard norms
of native speech, (3) and using likelihoods on the
audio portions that are recognized as words and ap-
plying various normalizations.
Table 1: Notations used for pronunciation feature extrac-
tion
Variable Meaning
L(xi) the likelihood of word xi being spo-
ken given the observed audio signal
ti the duration of word i in a response
Ts the duration of the entire response
T
n?
i=1
ti, the summation of the duration
of all words, where T ? Ts
n the number of words in a response
m the number of letters in a response
R mTs , the frequency of letters (as the rateof speech)
vi vowel i
Nv the total number of vowels
Pvi the duration of vowel vi
P? the average vowel duration (across all
vowels in the response being scored)
Dvi the standard average duration of
vowel vi (estimated on a native
speech corpus)
D? the averaged vowel duration (on all
vowels in a native speech corpus)
Svi |Pvi ? Dvi |, duration shift of vowel
vi (measured as the absolute value of
the difference between the duration of
vowel vi and its standard value)
Snvi |
Pvi
P? ?
Dvi
D? |, normalized duration shiftof vowel vi (measured as the absolute
value of the normalized difference be-
tween the duration of vowel vi and its
standard value)
4 Experiment design
We first raise three questions that we try to answer
with our experiments. Then, we describe the data
sets and the speech recognizers, especially the two
444
Table 2: A list of proposed pronunciation features
Feature Formula Meaning
L1
n?
i=1
L(xi) summation of likeli-
hoods of all the indi-
vidual words
L2 L1/n average likelihood
across all words
L3 L1/m average likelihood
across all letters
L4 L1/T average likelihood
per second
L5
n?
i=1
L(xi)
ti
n average likelihooddensity across all
words
L6 L4/R L4 normalized by the
rate of speech
L7 L5/R L5 normalized by the
rate of speech
S?
Nv?
i=1
Svi
Nv average vowel dura-tion shifts
S?n
Nv?
i=1
Snvi
Nv average normalizedvowel duration shifts
different acoustic models fitted to non-native and ex-
pected speech respectively. Finally, we describe the
evaluation criterion used in the experiment.
4.1 Research questions
In order to justify that the two-stage method for ex-
tracting pronunciation features is a valid method that
provides useful features for assessing pronunciation,
the following questions need to be answered:
Q1: Can the words hypothesized be used to approx-
imate the human transcripts in the forced align-
ment step?
Q2: Are the new pronunciation features effective
for assessment?
Q3: Can the likelihood-related features be im-
proved when using only words correctly recog-
nized?
4.2 Data
Table 3 lists the data sets used in the experiment.
Non-native speech collected in the TPO was used in
training a non-native AM. For feature evaluations,
we selected 1, 257 responses from the TPO data col-
lected in 2006. Within this set, 645 responses were
transcribed. Holistic scores were assigned by human
raters based on a score scale of 1 (the lowest profi-
ciency) to 4 (the highest proficiency).
In the TOEFL R?Native Speaker Study, native
speakers of primarily North American English
(NaE) took the TOEFL R?test and their speech files
were collected. This TOEFL R?native speech data
and some high-scored TPO responses were used
in the adaptation of an AM representing expected
speech properties. In addition, 1, 602 responses of
native speech, which had the highest speech profi-
ciency scores in NaE, were used to estimate standard
average vowel durations.
Type Function Source Size
non-
native
speech
AM training TPO ? 30 hrs
feature evalua-
tion
TPO col-
lected in
2006
1, 257
responses
(645 with
tran-
scripts)
native
or
near-
native
speech
AM adaptation TPO and
TOEFL
Native
? 2, 000
responses
estimation of
standard vowel
durations
TOEFL
Native
1, 602 re-
sponses
Table 3: Data sets used in the experiment
4.3 Speech technologies
For speech recognition and forced alignment, we
used a gender-independent fully continuous HMM
speech recognizer. Two different AMs were used in
the recognition and forced alignment steps respec-
tively.
The AM used in the recognition was trained
on about 30 hours of non-native speech from the
TPO. For language model training, a large corpus
of non-native speech (about 100 hours) was used
445
and mixed with a large general-domain language
model (trained from the Broadcast News (BN) cor-
pus (Graff et al, 1997) of the Linguistic Data Con-
sortium (LDC)). In the pronunciation feature extrac-
tion process depicted in Figure 1, this AM was used
to recognize non-native speech to generate the word
hypotheses.
The AM used in the forced alignment was trained
on native speech and high-scored non-native speech.
It was trained as follows: starting from a generic
recognizer, which was trained on a large and var-
ied native speech corpus, we adapted the AM using
batch-mode MAP adaptation. The adaptation corpus
contained about 2, 000 responses with high scores in
previous TPO tests and the TOEFL R?Native Speaker
Study. In addition, this AM was used to estimate
standard norms of vowels as described in Table 1.
4.4 Measurement metric
To measure the quality of the developed features,
a widely used metric is the Pearson correlation (r)
computed between the features and human scores.
In previous studies, human holistic scores of per-
ceived proficiency have been widely used in esti-
mating the correlations. In our experiment, we will
use the absolute value of Pearson correlation with
human holistic scores (|r|) to evaluate the features.
Given the close relationship between pronunciation
quality and overall speech proficiency, |r| is ex-
pected to approximate the strength of its relationship
with the human pronunciation scores.
5 Experimental Results
5.1 Results for Q1
When assessing read speech, the transcription of
the spoken content is known prior to the assess-
ment and used to forced-align the speech for fea-
ture extraction. However, when assessing sponta-
neous speech, we do not know the spoken content
and cannot provide a correct word transcription for
the forced alignment with imperfect speech recogni-
tion. A practical solution is to use the recognition
hypothesis to approximate the human transcript in
the forced alignment. Since the recognition word ac-
curacy on non-native spontaneous speech is not very
high (for example, a word accuracy of about 50% on
the TPO data was reported in (Zechner et al, 2007)),
it is critical to verify that the approximation can pro-
vide good enough pronunciation features compared
to the ones computed in an ideal scenario (by using
the human transcript in the forced alignment step).
We ran forced alignment on 645 TPO responses
with human transcriptions, using both the manual
transcription and the word hypotheses from the rec-
ognizer described in Section 4.3. Then, based on
these two forced alignment outputs, we extracted the
pronunciation features as described in Section 3.
Table 4 reports the |r|s between the proposed
pronunciation features and human holistic scores
when using the forced alignment results from ei-
ther transcriptions or recognition hypotheses. The
relative |r| reduction (defined as (|r|transcriptions ?
|r|hypotheses)/|r|transcriptions ? 100) is reported to
measure the magnitude reduction.
Based on the results shown in Table 4, we find that
the pronunciation features computed based on the
forced alignment results using transcriptions have
higher |r|s with the human holistic scores than the
corresponding features computed based on the FA
results using the recognition hypotheses. This is not
surprising given that only 50% ? 60% word accu-
racy can be achieved when recognizing non-native
spontaneous speech. However, the pronunciation
features computed using the recognition hypothe-
ses that is feasible in practice show some promising
correlations to human holistic scores. For example,
L3, L6, and L7 have |r|s larger than 0.45 and S?n
has an |r| larger than 0.35. Compared to the cor-
responding features computed using the FA results
based on transcriptions, these promising pronuncia-
tion features that can be obtained practically, show
some reduction in quality (from 13.4% to 21.1%)
but are still usable. Therefore, our proposed two-
stage method for pronunciation feature extraction is
proven to be a practical way for the computation of
features that have acceptable performance.
5.2 Result for Q2
Although our proposed modifications described in
Section 3 have improved the meaningfulness of the
features, an empirical study is needed to examine the
actual utility of these features for the assessment of
pronunciation.
In the experiment described in Section 5.1, four
pronunciation features (including L3, L6, L7, and
446
Feature |r| using
transcrip-
tion
|r| using
recog-
nition
hypothesis
relative |r|
reduction
(%)
L1 0.216 0.107 50.5
L2 0.443 0.416 6.1
L3 0.506 0.473 6.5
L4 0.363 0.294 19
L5 0.333 0.287 13.8
L6 0.549 0.475 13.5
L7 0.546 0.473 13.4
S? 0.396 0.296 25.3
S?n 0.451 0.356 21.1
Table 4: |r| between the pronunciation features and hu-
man holistic scores under two forced alignment input
conditions (using transcriptions vs. using recognition hy-
potheses) and relative |r| reduction
S?n) show promising correlations to human holistic
scores. To check the quality of the newly developed
pronunciation features, we compared these four fea-
tures with the amscore feature used in (Zechner et
al., 2007) on the TPO data set collected in 2006
(with 1, 257 responses). We first ran speech recog-
nition using the recognizer designed for non-native
speech. The recognition results were used to com-
pute the amscore, which is calculated by dividing
the likelihood over an entire response by the number
of letters. Then, we used the recognition hypothe-
ses to do the forced alignment using the other AM
trained on the native and near-native speech to ex-
tract those four pronunciation features. Finally, we
calculated the correlation coefficients between fea-
tures and the human holistic scores. The results are
reported in Table 5.
feature |r| to human holistic scores
amscore 0.434
L3 0.369
L6 0.444
L7 0.443
S?n 0.363
Table 5: A comparison of new pronunciation features to
amscore, the one used in SpeechRaterTM
Compared to the feature amscore, L6 and L7
have slightly higher |r|s with the human holistic
scores. This suggests that our construct-driven ap-
proach yields pronunciation features that are empiri-
cally comparable or even better than the amscore. In
addition, S?n, a new feature representing the vowel
production aspect of pronunciation, shows a rela-
tively high correlation with human holistic scores.
This suggests that our new pronunciation feature set
has an expanded coverage of pronunciation.
It is interesting to note that L3 has a lower |r|with
human holistic scores than the amscore does. Al-
though the computation of L3 is quite similar to that
of amscore, the major difference is that likelihoods
of non-word portions (such as silences and fillers)
are used to compute amscore but not L3. This sug-
gests that likelihood-related pronunciation features
that involve information related to non-words may
perform better in predicting human holistic scores.
For example, for amscore, the likelihoods measured
on those non-word units were involved in the feature
calculation; for L6 and L7, the temporal information
of those non-word units (e.g., duration of units) was
involved in the feature calculation 1.
5.3 Result for Q3
In the feature extraction, we used the words hy-
pothesized by the speech recognizer as the input for
the forced alignment. Since a considerable num-
ber of words are recognized incorrectly (especially
for non-native spontaneous speech), a natural way
to further improve the likelihood related features is
to only consider words which are correctly recog-
nized. A useful metric associated with the recog-
nition performance is the confidence score (CS) out-
put by the recognizer, which reflects the recognizer?s
estimation about the probability that a hypothesized
word is correctly recognized. The recognized words
with high confidence scores tend to be correctly rec-
ognized. Therefore, focusing on words recognized
with high confidence scores may reduce the negative
impact caused by recognition errors on the quality of
the likelihood related features.
On the TPO data with human transcripts, we used
the NIST?s sclite scoring tool (Fiscus, 2009) to mea-
sure the percentage of correct words (correct%),
which is defined as the ratio of the number of words
1L6 and L7 use R, which is computed as mTs , where Ts con-tains durations of non-words.
447
correctly recognized given the number of words in
the reference transcript. On all words (correspond-
ing to confidence scores ranging from 0.0 to 1.0), the
correct% is 53.3%. Figure 2 depicts the correct%
corresponding to ten confidence score bins ranging
from 0.0 to 1.0. Clearly, with the increase of the con-
fidence score, more words tend to be accurately rec-
ognized. Therefore, it is reasonable to only use like-
lihoods estimated on the hypothesized words with
high confidence scores for extracting likelihood re-
lated features.
 
0
 
10
 
20
 
30
 
40
 
50
 
60  0
 
0.2
 
0.4
 
0.6
 
0.8
 
1
Correct% of words hypothesized
Confid
ence s
core (C
S) bin
Figure 2: Correct% of words recognized across 10 confi-
dence score bins
On the TPO data set collected in 2006, we com-
puted three likelihood related features (including L3,
L6, and L7) only on words whose SC is equal to
or higher than a threshold (i.e., 0.5, 0.6, 0.7, 0.8,
and 0.9) and measured the |r| of a feature with the
human holistic scores. Table 6 lists the confidence
score cutting thresholds, the percentage of words
whose confidence scores are not lower than the cut-
ting threshold selected, and |r| between each like-
lihood feature to human holistic scores. In the Ta-
ble 6, we observe that only using words recognized
with high confidence improves the correlations be-
tween the features and the human holistic scores.
One issue about only using words recognized with
high confidence scores is that the number of words
used in the feature extraction has been reduced and
may reduce the robustness of the feature calculation.
Tc percentage
of words
whose CS
? Tc (%)
L3
|r|
L6
|r|
L7
|r|
0.0 100 0.369 0.444 0.443
0.5 84.21 0.38 0.462 0.461
0.6 77.07 0.377 0.465 0.464
0.7 69.31 0.363 0.461 0.461
0.8 60.86 0.371 0.466 0.466
0.9 50.76 0.426 0.477 0.475
Table 6: |r| between L3, L6, and L7 and human holistic
scores using only words recognized whose CSs are not
lower than a threshold (Tc)
6 Discussion
To assess the pronunciation of spontaneous speech,
we proposed a method for extracting a set of pro-
nunciation features. The method consists of two
stages: (1) recognizing speech using an AM well fit-
ted to non-native speech properties and (2) forced-
aligning the hypothesized words using the other
AM, which was trained on native and near-native
speech, and extracting features related to spectral
properties (HMM likelihood) and vowel production.
This method of using one AM optimized for speech
recognition and another AM optimized for pronun-
ciation evaluation is well motivated theoretically.
The derived pronunciation features have also been
found to have reasonably high correlations with hu-
man holistic scores. The results support the link-
age of the features to the construct of pronunciation
and their utility of being used in a scoring model to
predict human holistic judgments. Several contribu-
tions of this paper are described as below.
First, the two-stage method allows us to utilize
an AM trained on native and near-native speech as
a reference model when computing pronunciation
features. The decision to include high-scored non-
native speech was driven by the scoring rubrics de-
rived from the construct, where the pronunciation
quality of the highest level performance does not
necessarily require native-like accent, but highly in-
telligible speech. The way the reference model was
trained is consistent with the scoring rubrics, and
makes it an appropriate standard based on which the
pronunciation quality of non-native speech can be
448
evaluated. By using the recognition hypotheses from
the recognition step as input in the forced alignment
step, our experiments show a relatively small reduc-
tion in correlations with human holistic scores in
comparison to the features based on the human tran-
scriptions. This suggests that our method has po-
tential to be implemented in a real-time operational
setting.
Second, a few decisions we have made in com-
puting the pronunciation features are driven by
considerations of how these features are meaning-
fully linked to the construct of pronunciation as-
sessment. For example, we have excluded the
HMM likelihoods on non-words (such as pauses
and fillers) in the computations of likelihood-related
features. In addition, only using words recognized
with high confidence scores yields more informative
likelihood-related features for assessing the quality
of speech. The inclusion of vowel duration measures
in the feature set expanded the coverage of the qual-
ity of pronunciation.
7 Summary and future work
This paper presents a method for computing features
for assessing the pronunciation quality of non-native
spontaneous speech, guided by construct considera-
tions. We were able to show that using a two-stage
method of first recognizing speech with a non-native
AM and then forced aligning of the hypothesis using
a native or near-native speech AM we can generate
pronunciation features with promising correlations
with holistic scores assigned by human raters.
We plan to continue our research in the follow-
ing directions: (1) we will improve the native speech
norms for vowel durations, such as using the distri-
bution of vowel durations rather than just the mean
of durations in our feature computations; (2) we
will investigate other aspects of pronunciation, e.g.,
consonant quality and word stress; (3) we will add
other standard varieties of English (such as British,
Canadian, Australian, etc) to the training corpus for
the reference pronunciation model as the current
model is trained on primarily North American En-
glish (NaE).
References
J. Bernstein. 1999. PhonePass testing: Structure and
construct. Technical report, Ordinate Corporation.
C. Cucchiarini, H. Strik, and L. Boves. 1997a. Au-
tomatic evaluation of Dutch Pronunciation by us-
ing Speech Recognition Technology. In IEEE Auto-
matic Speech Recognition and Understanding Work-
shop (ASRU), Santa Barbara, CA.
C. Cucchiarini, H. Strik, and L. Boves. 1997b. Us-
ing Speech Recognition Technology to Assess Foreign
Speakers? Pronunciation of Dutch. In 3rd interna-
tional symosium on the acquision of second language
speech, Klagenfurt, Austria.
J. Fiscus. 2009. Speech Recognition Scoring Toolkit
(SCTK) Version 2.3.10.
H. Franco, V. Abrash, K. Precoda, H. Bratt, R. Rao, and
J. Butzberger. 2000. The SRI EduSpeak system:
Recognition and pronunciation scoring for language
learning. In InSTiLL (Intelligent Speech Technology
in Language Learning), Dundee, Stotland.
D. Graff, J. Garofolo, J. Fiscus, W. Fisher, and D. Pallett.
1997. 1996 English Broadcast News Speech (HUB4).
C. Hacker, T. Cincarek, R. Grubn, S. Steidl, E. Noth, and
H. Niemann. 2005. Pronunciation Feature Extraction.
In Proceedings of DAGM 2005.
N. Moustroufas and V. Digalakis. 2007. Automatic
pronunciation evaluation of foreign speakers using
unknown text. Computer Speech and Language,
21(6):219?230.
L. Neumeyer, H. Franco, V. Digalakis, and M. Weintraub.
2000. Automatic Scoring of Pronunciation Quality.
Speech Communication, 6.
S. M. Witt. 1999. Use of Speech Recognition in
Computer-assisted Language Learning. Ph.D. thesis,
University of Cambridge.
K. Zechner and I. Bejar. 2006. Towards Automatic Scor-
ing of Non-Native Spontaneous Speech. In NAACL-
HLT, NewYork NY.
K. Zechner, D. Higgins, and Xiaoming Xi. 2007.
SpeechRater: A Construct-Driven Approach to Scor-
ing Spontaneous Non-Native Speech. In Proc. SLaTE.
449
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
 
        Automatic Scoring of Children's Read-Aloud Text Passages and 
Word Lists 
 
Klaus Zechner and John Sabatini and Lei Chen 
Educational Testing Service 
Rosedale Road 
Princeton, NJ 08541, USA 
{kzechner,jsabatini,lchen}@ets.org 
 
 
 
 
Abstract 
Assessment of reading proficiency is typically 
done by asking subjects to read a text passage 
silently and then answer questions related to 
the text. An alternate approach, measuring 
reading-aloud proficiency, has been shown to 
correlate well with the aforementioned com-
mon method and is used as a paradigm in this 
paper.  
We describe a system that is able to automati-
cally score two types of children?s read speech 
samples (text passages and word lists), using 
automatic speech recognition and the target 
criterion ?correctly read words per minute?. 
Its performance is dependent on the data type 
(passages vs. word lists) as well as on the rela-
tive difficulty of passages or words for indi-
vidual readers. Pearson correlations with 
human assigned scores are around 0.86 for 
passages and around 0.80 for word lists. 
1 Introduction 
It has long been noted that a substantial number of 
U.S. students in the 10-14 years age group have 
deficiencies in their reading competence (National 
Center of Educational Statistics, 2006). With the 
enactment of the No Child Left Behind Act (2002), 
interest and focus on objectively assessing and im-
proving this unsatisfactory situation has come to 
the forefront. 
While assessment of reading is usually done post-
hoc with measures of reading comprehension, di-
rect reading assessment is also often performed 
using a different method, oral (read-aloud) reading. 
In this paradigm, students read texts aloud and 
their proficiency in terms of speed, fluency, pro-
nunciation, intonation etc. can be monitored di-
rectly while reading is in progress. In the reading 
research literature, oral reading has been one of the 
best diagnostic and predictive measures of founda-
tional reading weaknesses and of overall reading 
ability (e.g., Deno et al, 2001; Wayman et al, 
2007).  An association between low reading com-
prehension and slow, inaccurate reading rate has 
been confirmed repeatedly in middle school popu-
lations (e.g., Deno & Marsten, 2006).  Correlations 
consistently fall in the 0.65-0.7 range for predict-
ing untimed passage reading comprehension test 
outcomes (Wayman et al, 2007). 
 
In this paper, we investigate the feasibility of 
large-scale, automatic assessment of read-aloud 
speech of middle school students with a reasonable 
degree of accuracy (these students typically attend 
grades 6-8 and their age is in the 10-14 years 
range).  If possible, this would improve the utility 
of oral reading as a large-scale, school-based as-
sessment technique, making it more efficient by 
saving costs and time of human annotations and 
grading of reading errors. 
The most widely used measure of oral reading pro-
ficiency is ?correctly read words per minute? 
(cwpm) (Wayman et al, 2007). To obtain this 
measure, students? read speech samples are first 
10
recorded, then the reading time is determined, and 
finally a human rater has to listen to the recording 
and note all reading errors and sum them up. Read-
ing errors are categorized into word substitutions, 
deletions etc.  
We have several sets of digitally recorded read-
aloud samples from middle school students avail-
able which were not collected for use with auto-
matic speech recognition (ASR) but which were 
scored by hand. 
Our approach here is to pass the children?s speech 
samples through an automatic speech recognizer 
and then to align its output word hypotheses with 
the original text that was read by the student. From 
this alignment and from the reading time, an esti-
mate for the above mentioned measure of cwpm 
can then be computed. If the automatically com-
puted cwpm measures are close enough to those 
obtained by human hand-scoring, this process may 
be employed in real world settings eventually to 
save much time and money. 
 
Recognizing children?s speech, however, has been 
shown to be substantially harder than adult speech 
(Lee et al, 1999; Li and Russell, 2002), which is 
partly due to children?s higher degree of variability 
in different dimensions of language such as pro-
nunciation or grammar. In our data, there was also 
a substantial number of non-native speakers of 
English, presenting additional challenges. We used 
targeted training and adaptation of our ASR sys-
tems to achieve reasonable word accuracies. While 
for text passages, the word accuracy on unseen 
speakers was about 72%, it was only about 50% 
for word lists, which was due in part to a higher 
percentage of non-native speakers in this data set, 
to the fact that various sources of noise often pre-
vented the recognizer from correctly locating the 
spoken words in the signal, and also due to our 
choice of a uniform language model since conven-
tional n-gram models did not work on this data 
with many silences and noises between words. 
 
The remainder of this paper is organized as fol-
lows: in Section 2 we review related work, fol-
lowed by a description of our data in Section 3. 
Section 4 provides a brief description of our speech 
recognizer as well as the experimental setup. Sec-
tion 5 provides the results of our experiments, fol-
lowed by a discussion in Section 6 and conclusions 
and future work in Section 7. 
 
2 Related work 
Following the seminal paper about the LISTEN 
project (Mostow et al 1994), a number of studies 
have been conducted on using automatic speech 
recognition technology to score children?s read 
speech. 
 
Similar to automated assessment of adults? speech 
(Neumeyer, Franco et al 2000; Witt, 1999), the 
likelihood computed in the Hidden Markov Model 
(HMM) decoding and some measurements of  flu-
ency, e.g., speaking rate, are widely used as fea-
tures for predicting children?s speaking 
proficiency. Children?s speech is different than 
adults?. For example, children?s speech exhibits 
higher fundamental frequencies (F0) than adults on 
average. Also, children?s more limited knowledge 
of vocabulary and grammar results in more errors 
when reading printed text. Therefore, to achieve 
high-quality recognition on children?s speech, 
modifications have to be made on recognizers that 
otherwise work well for adults. 
In the LISTEN project (Mostow et al, 1994), the 
basic technology is to use speech recognition to 
classify each word of text as correctly read or not. 
Such a classification task is hard in that the chil-
dren?s speaking deviations from the text may in-
clude arbitrary words and non-words. In a study, 
they modeled variations by the modification of the 
lexicon and the language model of the Sphinx1 
speech recognizer.  
 
Recently, the Technology Based Assessment  of 
Language and Literacy project (TBALL,  (Alwan, 
2007)) has been attempting to assess and evaluate 
the language and literacy skills of young children 
automatically. In the TBALL project, a variety of 
tests including word verification, syllable blending, 
letter naming, and reading comprehension, are 
jointly used. Word verification is an assessment 
that measures the child?s pronunciation of read-
aloud target words. A traditional pronunciation 
verification method based on log-likelihoods from 
HMM models is used initially (Tepperman et al, 
2006). Then an improvement based on a Bayesian 
network classifier (Tepperman et al, 2007) is em-
                                                          
1 See http://cmusphinx.sourceforge.net/html/cmusphinx.php 
11
ployed to handle complicated errors such as pro-
nunciation variations and other reading mistakes. 
 
Many other approaches have been developed to 
further improve recognition performance on chil-
dren?s speech. For example, one highly accurate 
recognizer of children?s speech has been developed 
by Hagen et al (2007). Vocal tract length normali-
zation (VTLN) has been utilized to cope with the 
children?s different acoustic properties. Some spe-
cial processing techniques, e.g., using a general 
garbage model to model all miscues in speaking, 
have been devised to improve the language model  
used in the recognition of children?s speech (Li et 
al., 2007). 
 
3 Data 
For both system training and evaluation, we use a 
data set containing 3 passages read by the same 
265 speakers (Set1) and a fourth passage (a longer 
version of Passage 1), read by a different set of 55 
speakers (Set2). Further, we have word lists read 
by about 500 different speakers (Set3). All speak-
ers from Set12 and most (84%) from the third set 
were U. S. middle school students in grades 6-8 
(age 10-14). A smaller number of older students in 
grades 10-12 (age 15-18) was also included in the 
third set (16%).3 4  
In terms of native language, about 15% of Set1 and 
about 76% of Set35 are non-native speakers of 
English or list a language different from English as 
their preferred language. 
Table 1 provides the details of these data sets. In 
the word lists data set, there are 178 different word 
lists containing 212 different word types in total 
(some word lists were read by several different 
students). 
 
All data was manually transcribed using a spread-
sheet where each word is presented in one line and 
the annotator, who listens to the audio file, has to 
                                                          
2 For Set1, we have demographics for 254 of 265 speakers 
(both for grade level and native language). 
3 Grade demographics are available for 477 speakers of Set3. 
4 We do not have demographic data for the small Set2 (55 
speakers). 
5 This set (Set 3) has information on native language for 165 
speakers. 
mark-up any insertions, substitutions or deletions 
by the student.  
 
Name Recordings Length in 
words 
Passage 1 
(?Bed?, Set1-A) 
265 158 
Passage 2 
(?Girls?, Set1-B) 
265 74 
Passage 3 
(?Keen?, Set1-C) 
265 100 
Passage 4 
(?Bed*?) (Set2) 
55 197 
Word lists (Set3) 590 62 (average) 
Table 1. Text passages and word lists data sets. 
 
For ASR system training only, we additionally 
used parts of the OGI (Oregon Graduate Institute) 
and CMU (Carnegie Mellon University) Kids data 
sets as well (CSLU, 2008; LDC, 1997). 
 
4 ASR system and experiments 
The ASR system?s acoustic model (AM) was 
trained using portions of the OGI and CMU Kids? 
corpora as well as a randomly selected sub-set of 
our own passage and word list data sets described 
in the previous section. About 90% of each data set 
(Set1, Set2, Set3) was used for that purpose. Since 
the size of our own data set was too small for AM 
training, we had to augment it with the two men-
tioned corpora (OGI, CMU Kids), although they 
were not a perfect match in age range and accent. 
All recordings were first converted and down-
sampled to 11 kHz, mono, 16 bit resolution, PCM 
format. There was no speaker overlap between 
training and test sets. 
 
For the language model (LM), two different mod-
els were created: for passages, we built an interpo-
lated trigram LM where 90% of the weight is 
assigned to a LM trained only on the 4 passages 
from the training set (Set1, Set2) and 10% to a ge-
neric LM using the Linguistic Data Consortium 
(LDC) Broadcast News corpus (LDC, 1997). The 
dictionary contains all words from the transcribed 
passages in the training set, augmented with the 
1,000 most frequent words from the Broadcast 
News corpus. That way, the LM is not too restric-
tive and allows the recognizer to hypothesize some 
12
reading mistakes not already encountered in the 
human transcriptions of the training set. 
 
For the word lists, a trigram LM was found to be 
not working well since the words were spoken in 
isolation with sometimes significant pauses in be-
tween and automatic removal of these silences 
proved too hard given other confounding factors 
such as microphone, speaker, or background noise. 
Therefore it was decided to implement a grammar 
LM for the word list decoder where all possible 
words are present in a network that allows them to 
occur at any time and in any sequence, allowing 
for silence and/or noises in between words. This 
model with uniform priors, however, has the dis-
advantage of not including any words not present 
in the word list training set, such as common mis-
pronunciations and is therefore more restrictive 
than the LM for text passages. 
 
One could make the argument of using forced 
alignment instead of a statistical LM to determine 
reading errors. In fact, this approach is typically 
used when assessing the pronunciation of read 
speech. However, in our case, the interest is more 
in determining how many words were read cor-
rectly in the sequence of the text (and how fast 
they were read) as opposed to details in pronuncia-
tion. Further, even if we had confidence scores 
attached to words in forced alignment, deciding on 
which of the words obtained low confidence due to 
poor pronunciation or due to substitution would 
not be an easy decision. Finally, word deletions 
and insertions, if too frequent, might prevent the 
forced alignment algorithm from terminating. 
 
After training was complete, we tested the recog-
nizer on the held-out passage and word list data. 
After recognizing, we computed our target meas-
ure of ?correct words per minute? (cwpm) accord-
ing to the following formula (W= all words in a 
text, S= substitutions, D= deletions, T= reading 
time in minutes), performing a string alignment 
between the recognizer hypothesis and the passage 
or word list to be read: 
 
(1) 
W S D
cwpm
T
? ?=   
The reason that insertions are not considered here 
is that they contribute to an increase in reading 
time and therefore can be considered to be ac-
counted for already in the formula.  
 
Next, we performed an experiment that looks at 
whether automatic scoring of read-aloud speech 
allows for accurate predictions of student place-
ments in broad cohorts of reading proficiency. 
 
We then also look more closely at typical errors 
made by human readers and the speech recognizer. 
All these experiments are described and discussed 
in the following section. 
 
Table 2 describes the set-up of the experiments. 
Note that Passage4 (Set2) was included only in the 
training but not in the evaluation set since this set 
was very small. As mentioned in the previous sec-
tion, most speakers from the passage sets read 
more than one passage and a few speakers from the 
word lists set read more than one word list. 
 
Data set Recordings Speakers Language 
model 
type 
Passages1-
3 
101 37 Trigram  
Word lists 42 38 Grammar 
Table 2. Experiment set-up (evaluation sets). 
 
5 Results 
5.1 Overall results 
Table 3 depicts the results of our evaluation run 
with the ASR system described above. Word accu-
racy is measured against the transcribed speaker 
reference (not against the true text that was read). 
Word accuracy is computed according to Equation 
(2), giving equal weight to reference and ASR hy-
pothesis (c=correct, s=substitutions, d=deletions, 
i=insertions). This way, the formula is unbiased 
with respect to insertions or deletions: 
 
(2)  
0.5 100.0
c c
wacc
c s d c s i
? ?= ? ? +? ?+ + + +? ?  
 
 
13
 
 
Data set Recordings Speakers Average word 
Accuracy over all 
speech sample 
Minimum word 
accuracy on a 
speech sample 
Maximum word  
accuracy on a speech 
sample 
All Passages  
(1-3) 
101 37 72.2 20.4 93.8 
Passage1 
(?Bed?) 
28 28 70.8 20.4 83.6 
Passage2 
(?Girls?) 
36 36 64.1 25.4 85.7 
Passage3 
(?Keen?) 
37 37 77.7 27.4 93.8 
Word lists 42 38 49.6 10.8 78.9 
Table 3. ASR experiment results (word accuracies in percent) 
 
 
The typical run-time on a 3.2GHz Pentium proces-
sor was less than 30 seconds for a recording (faster 
than real time). 
 
We next compute cwpm measures for both human 
annotations (transcripts, ?gold standard?) and ma-
chine (ASR) hypotheses  
Human annotators went over each read passage 
and word list and marked all reading errors of the 
speakers (here, only deletions and substitutions are 
relevant). The reading time is computed directly 
from the speech sample, so machine and human 
cwpm scores only differ in error counts of dele-
tions and substitutions. Currently we only have one 
human annotation available per speech sample, but 
we aim to obtain a second annotation for the pur-
pose of determining inter-annotator agreement. 
 
Table 4 presents the overall results of comparing 
machine and human cwpm scoring. We performed 
both Pearson correlation as well as Spearman rank 
correlation. While the former provides a more ge-
neric measure of cwpm correlation, the latter fo-
cuses more on the question of the relative 
performance of different speakers compared to 
their peers which is usually the more interesting 
question in practical applications of reading as-
sessment. Note that unlike for Table 3, the ASR 
hypotheses are now aligned with the text to be read 
since in a real-world application, no human tran-
scriptions would be available. 
We can see that despite the less than perfect recog-
nition rate of the ASR system which causes a much 
lower average estimate for cwpm or cw (for word-
lists), both Pearson and Spearman correlation coef-
ficients are quite high, all above 0.7 for Spearman 
rank correlation and equal to 0.8 or higher for the 
Pearson product moment correlation. This is en-
couraging as it indicates that while current ASR 
technology is not yet able to exactly transcribe 
children?s read speech, it is 
 
Data set Gold 
cwpm
ASR-
based 
cwpm 
Pearson 
r corre-
lation 
Spearman 
rank cor-
relation 
All Pas-
sages  
(1-3) 
152.0 109.8 0.86 NA 
Passage1 
(Bed) 
174.3 123.5 0.87 0.72 
Passage2 
(Girls) 
133.1 86.5 0.86 0.73 
Passage3 
(Keen) 
153.4 122.2 0.86 0.77 
Word 
lists* 
 
48.0 29.4 0.80 0.81 
Table 4. CWPM results for passages and word 
lists. All correlations are significant at p<0.01. 
*For word lists, we use ?cw? (correct words, nu-
merator of Equation (1)) as the measure, since stu-
dents were not told to be rewarded for faster 
reading time here. 
 
possible to use its output to compute reasonable 
read-aloud performance measures such as cwpm 
14
which can help to quickly and automatically assess 
reading proficiencies of students. 
5.2 Cohort assignment experiment 
To follow up on the encouraging results with basic 
and rank correlation, we conducted an experiment 
to explore the question of practical importance 
whether the automatic system can assign students 
to reading proficiency cohorts automatically. 
For better comparison, we selected those 27 stu-
dents from 37 total who read all 3 passages (Set 1) 
and grouped them into three cohorts of 9 students 
each, based on their human generated cwpm score 
for all passages combined: (a) proficient 
(cwpm>190), (b) intermediate (135<cwpm<190), 
and (c) low proficient (cwpm<135). 
We then had the automatic system predict each 
student?s cohort based on the cwpm computed 
from ASR. Since ASR-based cwpm values are co-
nsistently lower than human annotator based cwpm 
values, the automatic cohort assignment is not 
based on the cwpm values but rather on their rank-
ing. 
The outcome of this experiment is very encourag-
ing in that there were no cohort prediction errors 
by the automatic system. While the precise ranking 
differs, the system is very well able to predict 
overall cohort placement of students based on 
cwpm. 
5.3 Overall comparison of students? reading er-
rors and ASR recognition errors 
To look into more detail of what types of reading 
errors children make and to what extent they are 
reflected by the ASR system output, we used the 
sclite-tool by the National Institute for Standards 
and Technology (NIST, 2008) and performed two 
alignments on the evaluation set: 
1. TRANS-TRUE: Alignment between human 
transcription and true passage or word list text to 
be read: this alignment informs us about the kinds 
of reading errors made by the students. 
2. HYPO-TRANS: Alignment between the ASR 
hypotheses and the human transcriptions; this 
alignment informs us of ASR errors. (Note that this 
is different from the experiments reported in Table 
4 above where we aligned the ASR hypotheses 
with the true reference texts to compute cwpm.) 
 
Table 5 provides general statistics on these two 
alignments. 
 
Data set Alignment SUB DEL INS 
Passages
1-3 
TRANS-
TRUE 
2.0% 6.1% 1.8% 
Pas-
sages1-3 
HYPO-
TRANS 
18.7% 9.6% 8.1% 
Word 
lists 
TRANS-
TRUE 
5.6% 6.2% 0.6% 
Word 
lists 
HYPO-
TRANS 
42.0%  8.9% 6.4% 
Table 5. Word error statistics on TRANS-TRUE 
and HYPO-TRANS alignments for both evaluation 
data sets. 
 
From Table 5 we can see that while for students, 
deletions occur more frequently than substitutions 
and, in particular, insertions, the ASR system, due 
to its imperfect recognition, generates mostly sub-
stitutions, in particular for the word lists where the 
word accuracy is only around 50%. 
Further, we observe that the students? average 
reading word error rate (only taking into account 
substitutions and deletions as we did above for the 
cwpm and cw measures) lies around 8% for pas-
sages and 12% for wordlists (all measured on the 
held-out evaluation data). 
5.4 Specific examples 
Next, we look at some examples of frequent confu-
sion pairs for those 4 combinations of data sets and 
alignments. Table 6 lists the top 5 most frequent 
confusion pairs (i.e., substitutions).  
 
For passages, all of the most frequent reading er-
rors by students are morphological variants of the 
target words, whereas this is only true for some of 
the ASR errors, while other ASR errors can be far 
off the target words. For word lists, student errors 
are sometimes just orthographically related to the 
target word (e.g., ?liner? instead of ?linear?), and 
sometimes of different part-of-speech (e.g., 
?equally? instead of ?equality?). ASR errors are 
typically related to the target word by some pho-
netic similarity (e.g., ?example? instead of ?sim-
ple?). 
 
15
 
Finally, we look at a comparison between errors 
made by the students and the fraction of those cor-
rectly identified by the ASR system in the recogni-
tion hypotheses. Table 7 provides the statistics on 
these matched errors for text passages and word 
lists.  
 
Data 
set 
Align-
ment 
Refer-
ence 
Spoken/ 
recog-
nized 
Count 
Pas-
sages
1-3 
TRANS
-TRUE 
asks 
savings 
projects 
teacher?s 
time 
ask 
saving 
project 
teacher 
times 
6 
5 
4 
4 
4 
Pas-
sages
1-3 
HYPO-
TRANS 
storm 
lee?s 
lee?s 
observer 
thousand 
storms 
be 
we 
and 
the 
11 
6 
6 
6 
6 
Word 
lists 
TRANS
-TRUE 
nature 
over-
sleep 
equality 
linear 
ware-
housed 
Natural 
overslept 
 
equally 
liner 
ware-
house 
6 
5 
 
4 
4 
3 
Word 
lists 
HYPO-
TRANS 
plan      
see  
simple 
unoffi-
cial   
loud 
planned 
season 
example 
competi-
tion 
through-
out 
8 
6 
6 
5 
 
4 
Table 6. Top 5 most frequent confusion pairs for 
passages and word list evaluation sets in two dif-
ferent alignments. For passages, substitutions 
among closed class words such as determiners or 
prepositions are omitted. 
 
Table 7 shows that while for text passages, almost 
half of the relevant errors (substitutions and dele-
tions) were correctly identified by the recognizer, 
for word lists, this percentage is substantially 
smaller. 
 
 
 
 
6 Discussion 
The goal of this paper is to evaluate the possibility 
of creating a system for automatic oral reading as-
sessment for middle school children, based on text 
passages and word lists. 
We decided to use the common reading profi-
ciency measure of ?correct words per minute? 
which enables us to align ASR word hypotheses 
with the correct texts, estimate cwpm based on this 
alignment and the reading time, and then compare 
the automatically estimated cwpm with human an-
notations of the same texts. 
 
 
Data set / error type Percentage of correctly 
identified errors 
Passages 1-3 ? SUB 20.6 
Passages 1-3 ? DEL 56.4 
Passages 1-3 ? 
SUB+DEL 
47.7 
Word lists ? SUB 2.7 
Word lists ? DEL 29.4 
Word lists ? 
SUB+DEL 
16.8 
Table 7. Statistics on matched errors: percentage of  
students? reading errors (substitutions and dele-
tions) that were also correctly identified by the 
ASR system. 
 
We built a recognizer with an acoustic model 
based on CMU and OGI kids? corpora as well as 
about 90% of our own text passages and word list 
data (Sets 1-3). For the in-context reading (text 
passages) we trained a trigram model focused 
mostly on transcriptions of the passages. For the 
out-of-context isolated word reading, we used a 
grammar language model where every possible 
word of the word lists in the training set can follow 
any other word at any time, with silence and/or 
noise between words. (While this was not our pre-
ferred choice, standard n-gram language models 
performed very poorly given the difficulty of re-
moving inter-word silences or noise automati-
cally.) 
Given how hard ASR for children?s speech is and 
given our small matched data sets, the word accu-
racy of 72% for text passages was not unreason-
able and was acceptable, particularly in a first 
development cycle. The word accuracy of only 
about 50% for word lists, however, is more prob-
16
lematic and we conjecture that the two main rea-
sons for the worse performance were (a) the ab-
sence of time stamps for the location of words 
which made it sometimes hard for the recognizer to 
locate the correct segment in the signal for word 
decoding (given noises in between), and (b) the 
sometimes poor recording conditions where vol-
umes were set too high or too low, too much back-
ground or speaker noise was present etc. Further, 
the high relative number of non-native speakers in 
that data set may also have contributed to the lower 
word accuracy of the word lists. 
While the current data collection had not been 
done with speech recognition in mind, in future 
data collection efforts, we will make sure that the 
sound quality of recordings is better monitored, 
with some initial calibration, and that we store time 
stamps when words are presented on the screen to 
facilitate the recognition task and to allow the rec-
ognizer to expect one particular word at one par-
ticular point in time. 
Despite imperfect word accuracies, however, for 
both passages and word lists we found encourag-
ingly high correlations between human and auto-
matic cwpm measures (cw measures for word 
lists). Obviously, the absolute values of cwpm dif-
fer greatly as the ASR system generates many 
more errors on average than the readers, but both 
Pearson correlation as well as Spearman rank cor-
relation measures are all above 0.7. This means 
that if we would use our automatic scoring results 
to rank students? reading proficiency, the ranking 
order would be overall quite similar to an order 
produced by human annotators. This observation 
about the rank, rather than the absolute value of 
cwpm, is important in so far as it is often the case 
that educators are interested in separating ?co-
horts? of readers with similar proficiency and in 
particular to identify the lowest performing cohort 
for additional reading practice and tutoring. 
An experiment testing the ability of the system to 
place students into three reading proficiency co-
horts based on cwpm was very encouraging in that 
all 27 students of the test set were placed in the 
correct cohort by the system. 
When we compare frequent student errors with 
those made by the machine (Table 6), we see that 
often times, students just substitute slight morpho-
logical variants (e.g., ?ask? for ?asks?), whereas in 
the ASR system, errors are typically more complex 
than just simple substitutions of morphological 
variants. However, in the case of word lists, we do 
find substitutions with related phonological content 
in the ASR output (e.g., ?example? for ?simple?). 
Finally, we observed that, only for the text pas-
sages, the ASR system could correctly identify a 
substantial percentage of readers? substitutions and 
deletions (about 48%, see Table 7). This is also 
encouraging as it is a first step towards meaningful 
feedback in a potential interactive setting. How-
ever, we here only look at recall ? because of the 
much larger number of ASR substitutions, preci-
sion is much lower and therefore the risk of over-
correction (false alarms) is still quite high. 
Despite all of the current shortcomings, we feel 
that we were able to demonstrate a ?proof-of-
concept? with our initial system in that we can use 
our trained ASR system to make reliable estimates 
on students? reading proficiency as measured with 
?correct words per minute?, where correlations 
between human and machine scores are in the 
0.80-0.86 range for text passages and word lists. 
 
7 Conclusions and future work 
This paper demonstrates the feasibility of building 
an automatic scoring system for middle school stu-
dents? reading proficiency, using a targeted trained 
speech recognition system and the widely used 
measure of ?correctly read words per minute? 
(cwpm). 
The speech recognizer was trained both on external 
data (OGI and CMU kids? corpora) and internal 
data (text passages and word lists), yielding two 
different modes for text passages (trigram language 
model) and word lists (grammar language model). 
Automatically estimated cwpm measures agreed 
closely with human cwpm measures, achieving 0.8 
and higher correlation with Pearson and 0.7 and 
higher correlation with Spearman rank correlation 
measures. 
Future work includes an improved set-up for re-
cordings such as initial calibration and on-line 
sound quality monitoring, adding time stamps to 
recordings of word lists, adding more data for 
training/adaptation of the ASR system, and explor-
ing other features (such as fluency features) and 
their potential role in cwpm prediction. 
 
 
17
Acknowledgements 
The authors would like to acknowledge the contri-
butions of Kathy Sheehan, Tenaha O?Reilly and 
Kelly Bruce to this work. We further are grateful 
for the useful feedback and suggestions from our 
colleagues at ETS and the anonymous reviewers 
that greatly helped improve our paper. 
 
References 
Alwan, A. (2007). A System for Technology Based 
Assessment of Language and Literacy in Young 
Children: the Role of Multiple Information 
Sources. Proceedings of MMSP, Greece. 
Center for Spoken Language Understanding 
(CSLU), 2008. Kids? Speech Corpus, 
http://www.cslu.ogi.edu/corpora/kids/.LDC, BN. 
Deno, S. L., Fuchs, L. S., Marston, D., & Shin, J. 
(2001). Using curriculum-based measurements 
to establish growth standards for students with 
learning disabilities. School Psychology Re-
view, 30(4), 507-524. 
Deno, S. L. and D. Marsten (2006). Curriculum-
based measurement of oral reading: An indicator 
of growth in fluency. What Research Has to Say 
about Fluency Instruction. S. J. Samuels and A. 
E. Farstrup. Newark, DE, International Reading 
Association: 179-203. 
Hagen, A., B. Pellom, & R. Cole. (2007). "Highly 
accurate children?s speech recognition for inter-
active reading tutors using subword units." 
Speech Communication 49(6): 861-873. 
Lee, S., A. Potamianos, & S. Narayanan. (1999). 
"Acoustics of children's speech: developmental 
changes of temporal and spectral parameters." 
Journal of Acoustics Society of American 
(JASA) 105: 1455-1468. 
Li, X., Y. C. Ju, L. Deng & A. Acero. (2007). Effi-
cient and Robust Language Modeling in an 
Automatic Children's Reading Tutor System. 
Proc. IEEE International Conference on Acous-
tics, Speech and Signal Processing ICASSP 
2007. 
Li, Q. and M. Russell (2002). An analysis of the 
causes of increased error rates in children's 
speech recognition. ICSLP. Denver, CO. 
Linguistic Data Consortium (LDC), 1997. 1996 
English Broadcast News Speech (HUB4), 
LDC97S44. 
Linguistic Data Consortium (LDC), 1997. The 
CMU Kids Corpus, LDC97S63. 
Mostow, J., S. F. Roth, G. Hauptmann & M. Kane. 
(1994). A prototype reading coach that listens. 
AAAI '94: Proceedings of the twelfth national 
conference on Artificial intelligence, Menlo 
Park, CA, USA, American Association for Arti-
ficial Intelligence. 
National Center of Educational Statistics. (2006). 
National Assessment of Educational Progress. 
Washington DC: U.S. Government Printing Of-
fice. 
National Institute for Standards and Technology 
(NIST), 2008. Sclite software package. 
http://www.nist.gov/speech/tools/ 
Neumeyer, L., H. Franco, V. Digalakis & M. 
Weintraub. (2000). "Automatic Scoring of Pro-
nunciation Quality." Speech Communication 6. 
No Child Left Behind Act of 2001, Pub. L. No. 
107-110, 115 Stat. 1425 (2002). 
Tepperman, J., J. Silva, A. Kazemzadeh, H. You, 
S. Lee, A. Alwan & S. Narayanan. (2006). Pro-
nunciation verification of children's speech for 
automatic literacy assessment. INTERSPEECH-
2006. Pittsburg, PA. 
Tepperman, J., M. Black, P. Price, S. Lee, A. Ka-
zemzadeh, M. Gerosa, M. Heritage, A. Alwan & 
S. Narayanan.(2007). A bayesian network clas-
sifier for word-level reading assessment. Pro-
ceedings of ICSLP, Antwerp, Belgium. 
Wayman, M. M., Wallace, T., Wiley, H. I., Ticha, 
R., & Espin, C. A. (2007). Literature synthesis 
on curriculum-based measurement in reading. 
The Journal of Special Education, 41(2), 85-120. 
Witt, S. M. (1999). Use of Speech Recognition in 
Computer-assisted Language Learning, Univer-
sity of Cambridge. 
 
18
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 211?214,
New York, June 2006. c?2006 Association for Computational Linguistics
Incorporating Gesture and Gaze into Multimodal Models of
Human-to-Human Communication
Lei Chen
Dept. of Electrical and Computer Engineering
Purdue University
West Lafayette, IN 47907
chenl@ecn.purdue.edu
Abstract
Structural information in language is im-
portant for obtaining a better understand-
ing of a human communication (e.g., sen-
tence segmentation, speaker turns, and
topic segmentation). Human communica-
tion involves a variety of multimodal be-
haviors that signal both propositional con-
tent and structure, e.g., gesture, gaze, and
body posture. These non-verbal signals
have tight temporal and semantic links to
spoken content. In my thesis, I am work-
ing on incorporating non-verbal cues into
a multimodal model to better predict the
structural events to further improve the
understanding of human communication.
Some research results are summarized in
this document and my future research plan
is described.
1 Introduction
In human communication, ideas tend to unfold in
a structured way. For example, for an individual
speaker, he/she organizes his/her utterances into sen-
tences. When a speaker makes errors in the dy-
namic speech production process, he/she may cor-
rect these errors using a speech repair scheme. A
group of speakers in a meeting organize their ut-
terances by following a floor control scheme. All
these structures are helpful for building better mod-
els of human communication but are not explicit in
the spontaneous speech or the corresponding tran-
scription word string. In order to utilize these struc-
tures, it is necessary to first detect them, and to do
so as efficiently as possible. Utilization of various
kinds of knowledge is important; For example, lex-
ical and prosodic knowledge (Liu, 2004; Liu et al,
2005) have been used to detect structural events.
Human communication tends to utilize not only
speech but also visual cues such as gesture, gaze,
and so on. Some studies (McNeill, 1992; Cassell
and Stone, 1999) suggest that gesture and speech
stem from a single underlying mental process, and
they are related both temporally and semantically.
Gestures play an important role in human commu-
nication but use quite different expressive mecha-
nisms than spoken language. Gaze has been found
to be widely used in coordinating multi-party con-
versations (Argyle and Cook, 1976; Novick, 2005).
Given the close relationship between non-verbal
cues and speech and the special expressive capac-
ity of non-verbal cues, we believe that these cues
are likely to provide additional important informa-
tion that can be exploited when modeling structural
events. Hence, in my Ph.D thesis, I have been in-
vestigating the combination of lexical, prosodic, and
non-verbal cues for detection of the following struc-
tural events: sentence units, speech repairs, and
meeting floor control.
This paper is organized as follows: Section 1 has
described the research goals of my thesis. Section 2
summarizes the efforts made related to these goals.
Section 3 lays out the research work needed to com-
plete my thesis.
2 Completed Works
Our previous research efforts related to multimodal
analysis of human communication can be roughly
grouped to three fields: (1) multimodal corpus col-
211
Figure 1: VACE meeting corpus production
lection, annotation, and data processing, (2) mea-
surement studies to enrich knowledge of non-verbal
cues to structural events, and (3) model construc-
tion using a data-driven approach. Utilizing non-
verbal cues in human communication processing is
quite new and there is no standard data or off-the-
shelf evaluation method. Hence, the first part of my
research has focused on corpus building. Through
measurement investigations, we then obtain a bet-
ter understanding of the non-verbal cues associated
with structural events in order to model those struc-
tural events more effectively.
2.1 Multimodal Corpus Collection
Under NSF KDI award (Quek and et al, ), we col-
lected a multimodal dialogue corpus. The corpus
contains calibrated stereo video recordings, time-
aligned word transcriptions, prosodic analyses, and
hand positions tracked by a video tracking algo-
rithm (Quek et al, 2002). To improve the speed
of producing a corpus while maintaining its qual-
ity, we have investigated factors impacting the ac-
curacy of the forced alignment of transcriptions to
audio files (Chen et al, 2004a).
Meetings, in which several participants commu-
nicate with each other, play an important role in our
daily life but increase the challenges to current infor-
mation processing techniques. Understanding hu-
man multimodal communicative behavior, and how
witting and unwitting visual displays (e.g., gesture,
head orientation, gaze) relate to spoken content is
critical to the analysis of meetings. These multi-
modal behaviors may reveal static and dynamic so-
cial structure of the meeting participants, the flow
of topics being discussed, the control of floor of
the meeting, and so on. For this purpose, we have
been collecting a multimodal meeting corpus un-
der the sponsorship of ARDA VACE II (Chen et al,
2005). In a room equipped with synchronized mul-
tichannel audio,video and motion-tracking record-
ing devices, participants (from 5 to 8 civilian, mil-
itary, or mixed) engage in planning exercises, such
as managing rocket launch emergency, exploring a
foreign weapon component, and collaborating to se-
lect awardees for fellowships. we have collected and
continued to do multichannel time synchronized au-
dio and video recordings. Using a series of audio
and video processing techniques, we obtain the word
transcriptions and prosodic features, as well as head,
torso and hand 3D tracking traces from visual track-
ers and Vicon motion capture device. Figure 1 de-
picts our meeting corpus collection process.
2.2 Gesture Patterns during Speech Repairs
In the dynamic speech production process, speak-
ers may make errors or totally change the content
of what is being expressed. In either of these cases,
speakers need refocus or revise what they are saying
212
and therefore speech repairs appear in overt speech.
A typical speech repair contains a reparandum, an
optional editing phrase, and a correction. Based
on the relationship between the reparandum and the
correction, speech repairs can be classified into three
types: repetitions, content replacements, and false
starts. Since utterance content has been modified
in last two repair types, we call them content mod-
ification (CM) repairs. We carried out a measure-
ment study (Chen et al, 2002) to identify patterns of
gestures that co-occur with speech repairs that can
be exploited by a multimodal processing system to
more effectively process spontaneous speech. We
observed that modification gestures (MGs), which
exhibit a change in gesture state during speech re-
pair, have a high correlation with content modifica-
tion (CM) speech repairs, but rarely occur with con-
tent repetitions. This study does not only provide ev-
idence that gesture and speech are tightly linked in
production, but also provides evidence that gestures
provide an important additional cue for identifying
speech repairs and their types.
2.3 Incorporating Gesture in SU Detection
A sentence unit (SU) is defined as the complete ex-
pression of a speaker?s thought or idea. It can be ei-
ther a complete sentence or a semantically complete
smaller unit. We have conducted an experiment that
integrates lexical, prosodic and gestural cues in or-
der to more effectively detect sentence unit bound-
aries in conversational dialog (Chen et al, 2004b).
As can be seen in Figure 2, our multimodal model
combines lexical, prosodic, and gestural knowl-
edge sources, with each knowledge source imple-
mented as a separate model. A hidden event lan-
guage model (LM) was trained to serve as lexical
model (P (W,E)). Using a direct modeling ap-
proach (Shriberg and Stolcke, 2004), prosodic fea-
tures were extracted using the SRI prosodic fea-
ture extraction tool1 by collaborators at ICSI and
then were used to train a CART decision tree as the
prosodic model (P (E|F )). Similarly to the prosodic
model, we computed gesture features directly from
visual tracking measurements (Quek et al, 1999;
Bryll et al, 2001): 3D hand position, Hold (a state
when there is no hand motion beyond some adaptive
1A similar prosody feature extraction tool has been devel-
oped in our lab (Huang et al, 2006) using Praat.
threshold results), and Effort (analogous to the ki-
netic energy of hand movement). Using gestural fea-
tures, we trained a CART tree to serve as the gestu-
ral model (P (E|G)). Finally, an HMM based model
combination scheme was used to integrate predic-
tions from individual models to obtain an overall SU
prediction (argmax(E|W,F,G)). In our investiga-
tions, we found that gesture features complement the
prosodic and lexical knowledge sources; by using
all of the knowledge sources, the model is able to
achieve the lowest overall detection error rate.
Figure 2: Data flow diagram of multimodal SU
model using lexical, prosodic and gestural cues
2.4 Floor Control Investigation on Meetings
An underlying, auto-regulatory mechanism known
as ?floor control?, allows participants communicate
with each other coherently and smoothly. A person
controlling the floor bears the burden of moving the
discourse along. By increasing our understanding of
floor control in meetings, there is a potential to im-
pact two active research areas: human-like conver-
sational agent design and automatic meeting analy-
sis. We have recently investigated floor control in
multi-party meetings (Chen et al, 2006). In particu-
lar, we analyzed patterns of speech (e.g., the use of
discourse markers) and visual cues (e.g., eye gaze
exchange, pointing gesture for next speaker) that are
often involved in floor control changes. From this
analysis, we identified some multimodal cues that
will be helpful for predicting floor control events.
Discourse markers are found to occur frequently at
the beginning of a floor. During floor transitions, the
213
previous holder often gazes at the next floor holder
and vice verse. The well-known mutual gaze break
pattern in dyadic conversations is also found in some
meetings. A special participant, an active meeting
manager, is found to play a role in floor transitions.
Gesture cues are also found to play a role, especially
with respect to floor capturing gestures.
3 Research Directions
In the next stage of my research, I will focus on inte-
grating previous efforts into a complete multimodal
model for structural event detection. In particular, I
will improve current gesture feature extraction, and
expand the non-verbal features to include both eye
gaze and body posture. I will also investigate alter-
native integration architectures to the HMM shown
in Figure 2. In my thesis, I hope to better understand
the role that the non-verbal cues play in assisting
structural event detection. My research is expected
to support adding multimodal perception capabili-
ties to current human communication systems that
rely mostly on speech. I am also interested in inves-
tigating mutual impacts among the structural events.
For example, we will study SUs and their relation-
ship to floor control structure. Given progress in
structural event detection in human communication,
I also plan to utilize the detected structural events
to further enhance meeting understanding. A par-
ticularly interesting task is to locate salient portions
of a meeting from multimodal cues (Chen, 2005) to
summarize it.
References
M. Argyle and M. Cook. 1976. Gaze and Mutual Gaze.
Cambridge Univ. Press.
R. Bryll, F. Quek, and A. Esposito. 2001. Automatic
hand hold detection in natural conversation. In IEEE
Workshop on Cues in Communication, Kauai,Hawaii,
Dec.
J. Cassell and M. Stone. 1999. Living Hand to Mouth:
Psychological Theories about Speech and Gesture in
Interactive Dialogue Systems. In AAAI.
L. Chen, M. Harper, and F. Quek. 2002. Gesture pat-
terns during speech repairs. In Proc. of Int. Conf. on
Multimodal Interface (ICMI), Pittsburg, PA, Oct.
L. Chen, Y. Liu, M. Harper, E. Maia, and S. McRoy.
2004a. Evaluating factors impacting the accuracy of
forced alignments in a multimodal corpus. In Proc. of
Language Resource and Evaluation Conference, Lis-
bon, Portugal, June.
L. Chen, Y. Liu, M. Harper, and E. Shriberg. 2004b.
Multimodal model integration for sentence unit detec-
tion. In Proc. of Int. Conf. on Multimodal Interface
(ICMI), University Park, PA, Oct.
L. Chen, T.R. Rose, F. Parrill, X. Han, J. Tu, Z.Q. Huang,
I. Kimbara, H. Welji, M. Harper, F. Quek, D. McNeill,
S. Duncan, R. Tuttle, and T. Huang. 2005. VACE
multimodal meeting corpus. In Proceeding of MLMI
2005 Workshop.
L. Chen, M. Harper, A. Franklin, T. R. Rose, I. Kimbara,
Z. Q. Huang, and F. Quek. 2006. A multimodal anal-
ysis of floor control in meetings. In Proc. of MLMI 06,
Washington, DC, USA, May.
L. Chen. 2005. Locating salient portions of meeting us-
ing multimodal cues. Research proposal submitted to
AMI training program, Dec.
Z. Q. Huang, L. Chen, and M. Harper. 2006. An open
source prosodic feature extraction tool. In Proc. of
Language Resource and Evaluation Conference, May
2006.
Y. Liu, E. Shriberg, A. Stolcke, B. Peskin, J. Ang, Hillard
D., M. Ostendorf, M. Tomalin, P. Woodland, and
M. Harper. 2005. Structural Metadata Research in
the EARS Program. In Proc. of ICASSP.
Y. Liu. 2004. Structural Event Detection for Rich Tran-
scription of Speech. Ph.D. thesis, Purdue University.
D. McNeill. 1992. Hand and Mind: What Gestures Re-
veal about Thought. Univ. Chicago Press.
D. G. Novick. 2005. Models of gaze in multi-party dis-
course. In Proc. of CHI 2005 Workshop on the Virtu-
ality Continuum Revisted, Portland OR, April 3.
F. Quek and et al KDI: Cross-model Analysis
Signal and Sense- Data and Computational Re-
sources for Gesture, Speech and Gaze Research,
http://vislab.cs.vt.edu/kdi.
F. Quek, R. Bryll, and X. F. Ma. 1999. A parallel algo-
righm for dynamic gesture tracking. In ICCV Work-
shop on RATFG-RTS, Gorfu,Greece.
F. Quek, D. McNeill, R. Bryll, S. Duncan, X. Ma, C. Kir-
bas, K. E. McCullough, and R. Ansari. 2002. Mul-
timodal human discourse: gesture and speech. ACM
Trans. Comput.-Hum. Interact., 9(3):171?193.
E. Shriberg and A. Stolcke. 2004. Direct modeling of
prosody: An overview of applications in automatic
speech processing. In International Conference on
Speech Prosody.
214
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 278?282,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Building Comparable Corpora Based on Bilingual LDA Model 
 Zede Zhu 
University of Science and Technology 
of China, Institute of Intelligent Ma-
chines Chinese Academy of Sciences 
Hefei, China 
zhuzede@mail.ustc.edu.cn 
Miao Li, Lei Chen, Zhenxin Yang 
Institute of Intelligent Machines Chinese 
Academy of Sciences 
Hefei, China 
mli@iim.ac.cn,alan.cl@163.com, 
xinzyang@mail.ustc.edu.cn 
 
  
Abstract 
Comparable corpora are important basic re-
sources in cross-language information pro-
cessing. However, the existing methods of 
building comparable corpora, which use inter-
translate words and relative features, cannot 
evaluate the topical relation between document 
pairs. This paper adopts the bilingual LDA 
model to predict the topical structures of the 
documents and proposes three algorithms of 
document similarity in different languages. 
Experiments show that the novel method can 
obtain similar documents with consistent top-
ics own better adaptability and stability per-
formance.  
1 Introduction 
Comparable corpora can be mined fine-grained 
translation equivalents, such as bilingual termi-
nologies, named entities and parallel sentences, 
to support the bilingual lexicography, statistical 
machine translation and cross-language infor-
mation retrieval (AbduI-Rauf et al, 2009). Com-
parable corpora are defined as pairs of monolin-
gual corpora selected according to the criteria of 
content similarity but non-direct translation in 
different languages, which reduces limitation of 
matching source language and target language 
documents. Thus comparable corpora have the 
advantage over parallel corpora in which they are 
more up-to-date, abundant and accessible (Ji, 
2009). 
Many works, which focused on the exploita-
tion of building comparable corpora, were pro-
posed in the past years. Tao et al (2005) ac-
quired comparable corpora based on the truth 
that terms are inter-translation in different lan-
guages if they have similar frequency correlation 
at the same time periods. Talvensaari et al (2007) 
extracted appropriate keywords from the source 
language documents and translated them into the 
target language, which were regarded as the que-
ry words to retrieve similar target documents. 
Thuy et al (2009) analyzed document similarity 
based on the publication dates, linguistic inde-
pendent units, bilingual dictionaries and word 
frequency distributions. Otero et al (2010) took 
advantage of the translation equivalents inserted 
in Wikipedia by means of interlanguage links to 
extract similar articles. Bo et al (2010) proposed 
a comparability measure based on the expecta-
tion of finding the translation for each word.  
The above studies rely on the high coverage of 
the original bilingual knowledge and a specific 
data source together with the translation vocabu-
laries, co-occurrence information and language 
links. However, the severest problem is that they 
cannot understand semantic information. The 
new studies seek to match similar documents on 
topic level to solve the traditional problems. Pre-
iss (2012) transformed the source language topi-
cal model to the target language and classified 
probability distribution of topics in the same lan-
guage, whose shortcoming is that the effect of 
model translation seriously hampers the compa-
rable corpora quality. Ni et al (2009) adapted 
monolingual topic model to bilingual topic mod-
el in which the documents of a concept unit in 
different languages were assumed to share iden-
tical topic distribution. Bilingual topic model is 
widely adopted to mine translation equivalents 
from multi-language documents (Mimno et al, 
2009; Ivan et al, 2011).  
Based on the bilingual topic model, this paper 
predicts the topical structure of documents in 
different languages and calculates the similarity 
of topics over documents to build comparable 
corpora. The paper concretely includes: 1) Intro-
duce the Bilingual LDA (Latent Dirichlet Alloca-
tion) model  which builds comparable corpora 
and improves the efficiency of matching similar 
documents; 2) Design a novel method of TFIDF 
(Topic Frequency-Inverse Document Frequency) 
to enhance the distinguishing ability of topics 
from different documents; 3) Propose a tailored 
278
method of conditional probability to calculate 
document similarity; 4) Address a language-
independent study which isn?t limited to a par-
ticular data source in any language. 
2 Bilingual LDA Model 
2.1 Standard LDA 
LDA model (Blei et al, 2003) represents the la-
tent topic of the document distribution by Di-
richlet distribution with a K-dimensional implicit 
random variable, which is transformed into a 
complete generative model when ?  is exerted to 
Dirichlet distribution (Griffiths et al, 2004) 
(Shown in Fig. 1), 
? m? ,m n? ,m n? ?
[1, ]mn N?
[1, ]m M?
k?
[1, ]k K?  
Figure 1: Standard LDA model 
where ? and ?  denote the parameters distribut-
ed by Dirichlet; K denotes the topic numbers; k?  denotes the vocabulary probability distribution in 
the topic k; M denotes the document number; m?  denotes the topic probability distribution in the 
document m; Nm denotes the length of m; ,m n?  and ?m,n denote the topic and the word in m re-spectively. 
2.2 Bilingual LDA 
Bilingual LDA is a bilingual extension of a 
standard LDA model. It takes advantage of the 
document alignment which shares the same topic 
distribution m?  and uses different word distribu-
tions for each topic (Shown in Fig. 2), where S 
and T denote source language and target lan-
guage respectively.  
?
,
S
m n?
,
T
m n?
,
S
m n?
,
T
m n? Tk?
S
k? S?
m?
[1, ]m M?
[1, ]S Smn N?
[1, ]T Tmn N? [1, ]k K?
T?
 
Figure 2: Bilingual LDA model 
For each language l ( { , }l S T? ), ,lm n? and 
,
l
m n? are drawn using , ( | )l lm n n mP ?? ??  and 
, ,( | , )l l l lm n n m nP? ? ??? . 
Giving the comparable corpora M, the distri-
bution ,k v?  can be obtained by sampling a new 
token as word v from a topic k. For new collec-
tion of documents M? , keeping ,k v? , the distri-
bution ,lm k? ? of sampling a topic k from document 
m?  can be obtained as follows: 
( )
, ( )
1
( | )
( )
l
l
l
k
kl m
k Km k k
kmk
nP m
n
??
?
?
?? ? ?
??
?
?
?
? ,     (1) 
where ( )lkmn?  denotes the total number of times that the document m?  is assigned to the topic k. 
3 Building comparable corpora  
Based on the bilingual LDA model, building 
comparable corpora includes several steps to 
generate the bilingual topic model ,k v?  from the given bilingual corpora, predict the topic distri-bution ,lm k? ? of the new documents, calculate the 
similarity of documents and select the largest 
similar document pairs. The key step is that the 
document similarity is calculated to align the 
source language document Sm?  with relevant 
target language document Tm? .  
As one general way of expressing similarity, 
the Kullback-Leibler (KL) Divergence is adopted 
to measure the document similarity by topic dis-
tributions ? ,Sm k? and ? ,Tm k? as follows: 
? ? ?? ?, , ,1
( , ) [ ( | ), ( | )]
log . (2)S S T
S T S T
KLK
m k m k m kk
Sim m m KL P m P m
? ? ?
?
? ? ?
? ?? ? ?? ??
? ? ? ?
 
The remainder section focuses on other two 
methods of calculating document similarity. 
3.1 Cosine Similarity 
The similarity between Sm? and Tm? can be meas-
ured by Topic Frequency-Inverse Document 
Frequency. It gives high weights to the topic 
which appears frequently in a specific document 
and rarely appears in other documents. Then the 
relation between ,SmTFIDF ?? and ,TmTFIDF ??  is 
measured by Cosine Similarity (CS). 
Similar to Term Frequency-Inverse Document 
Frequency (Manning et al,1999), Topic Fre-
quency (TF) denoting frequency of topic ?  for 
the document lm? is denoted by ( | )lP m? ? . Given 
a constant value? , Inverse Document Frequency 
(IDF) is defined as the total number of docu-
ments M? divided by the number of documents 
279
: ( | )l lm P m ?? ?? ?  containing a particular topic, and then taking the logarithm, which is calculat-
ed as follows: 
log1 : ( | )l l
MIDF m P m ?? ? ? ?
?
? ? .      (3) 
The TFIDF is calculated as follows: 
*
( | ) log1 : ( | )
l
l l
TFIDF TF IDF
MP m m P m ?
?
? ? ? ? ?
?
? ? ?
.   (4) 
Thus, the TFIDF score of the topic k over 
document lm?  is given by: 
,
,
,
( | ) log 1 : ( | )
log . (5)1 :
l
l
l
m k
l
k l l
k
m k l
m k
TFIDF
MP m m P m
M
m
?
? ? ?
? ? ? ? ?
? ? ?
?
?
?
?
? ? ?
?
?  
The similarity between Sm? and Tm? is given by: 
, ,
, ,1
2 2
, ,1 1
( , ) ( , )
. (6)
S T
S T
S T
S T
CS m mK
m k m kk
K K
m k m kk k
Sim m m Cos TFIDF TFIDF
TFIDF TFIDF
TFIDF TFIDF
? ?
?
? ?
?
?
?
? ?
? ?
? ?
? ?
? ?
 3.2 Conditional Probability 
The similarity between Sm? and Tm? is defined as 
the Conditional Probability (CP) of documents 
( | )T SP m m? ?  that Tm? will be generated as a re-
sponse to the cue Sm? . 
( )P ?  as prior topic distribution is assumed a 
uniform distribution and satisfied the condition 
( ) ( )kP P? ? ? . According to the total probabil-
ity formula, the document Tm? is given as: 
1
1
( ) ( | ) ( )
( ) ( | ).
KT T
k k
kK T
k
k
P m P m P
P P m
?
?
? ? ?
? ? ?
?
?
? ?
?          (7)
 
Based on the Bayesian formula, the probabil-
ity that a given topic ?  is assigned to a particu-
lar target language document Tm?  is expressed: 
1
( | ) ( | ) ( ) ( )
= ( | ) ( | ).
T T T
KT T
k
k
P m P m P m P
P Z m P m
?
? ?? ? ? ?? ?
??
? ? ?
? ?   (8)
 
The sum of all probabilities 
1
( | )
K T
k
k
P m
?
?? ?
 that all topics ?  are assigned to a particular doc-
ument Tm?  is a constant? , thus equation (8) is 
converted as follows: 
( | ) ( | )T TP m P m? ? ? ?? ? .             (9) 
According to the total probability formula, the 
similarity between Sm? and Tm?  is given by: 
? ?
1
1
, ,1
(10)
( , ) ( | )
[ ( | ) ( | )]
[ ( | ) ( | )]
[ ].S T
S T T S
CPK T S
k k
k K T S
k k
kK
m k m kk
Sim m m P m m
P m P m
P m P m
? ?
?
?
?
?
? ? ?
? ? ? ?
? ?
?
?
?
? ? ? ?
? ?
? ?  
4 Experiments and analysis 
4.1 Datasets and Evaluation 
The experiments are conducted on two sets of 
Chinese-English comparable corpora. The first 
dataset is news corpora with 3254 comparable 
document pairs, from which 200 pairs are ran-
domly selected as the test dataset News-Test and 
the remainder is the training dataset News-Train. 
The second dataset contains 8317 bilingual Wik-
ipedia entry pairs, from which 200 pairs are ran-
domly selected as the test dataset Wiki-Test and 
the remainder is the training dataset Wiki-Train. 
Then News-Train and Wiki-Train are merged 
into the training dataset NW-Train. And the 
hand-labeled gold standard namely NW-Test is 
composed of News-Test and Wiki-Test.  
Braschler et al (1998) used five levels of rele-
vance to assess the alignments as follows: Same 
Story, Related Story, Shared Aspect, Common 
Terminology and Unrelated. The paper selects 
the documents with Same Story and Related Sto-
ry as comparable corpora. Let Cp be the compa-
rable corpora in the building result and Cl be the 
comparable corpora in the labeled result. The 
Precision (P), Recall (R) and F-measure (F) are 
defined as: 
= ,p l p l
lp
C C C CP R CC ?
? ? , 2PRF P R? ? . (11) 
4.2 Results and analysis 
Two groups of validation experiments are set 
with sampling frequency of 1000, parameter ?  
280
of 50/K, parameter ?  of 0.01 and topic number K of 600. 
Group 1: Different data source 
We learn bilingual LDA models by taking differ-
ent training datasets. The performance of three 
approaches (KL, CS and CP) is examined on dif-
ferent test datasets. Tab. 1 demonstrates these 
results with the winners for each algorithm in 
bold. 
Train Test KL CS CP P F P F P F 
News News 0.62 0.52 0.73 0.59 0.69 0.56
News Wiki 0.60 0.47 0.68 0.56 0.66 0.52
Wiki News 0.61 0.48 0.71 0.58 0.68 0.55
Wiki Wiki 0.63 0.50 0.75 0.60 0.71 0.59
NW NW 0.66 0.55 0.76 0.62 0.73 0.60
Table 1: Sensitivity of Data Source 
The results indicate the robustness and effec-
tiveness of these algorithms. The performance of 
algorithms on Wiki-Train is much better than 
News-Train. The main reason is that Wiki-Train 
is an extensive snapshot of human knowledge 
which can cover most topics talked in News-
Train. The probability of vocabularies among the 
test dataset which have not appeared in the train-
ing data is very low. And then the document top-
ic can effectively concentrate all the vocabular-
ies? expressions. The topic model slightly faces 
with the problem of knowledge migration issue, 
so the performance of the topic model trained by 
Wiki-Train shows a slight decline in the experi-
ments on News-Test.  
CS shows the strongest performance among 
the three algorithms to recognize the document 
pairs with similar topics. CP has almost equiva-
lent performance with CS. Comparing the equa-
tion (5) and (6) with (10), we can find out that 
CP is similar to a simplified CS. CP can improve 
the operating efficiency and decrease the perfor-
mance. The performance achieved by KL is the 
weakest and there is a large gap between KL and 
others. In addition, the shortage of KL is that 
when the exchange between the source language 
and the target language documents takes place, 
different evaluations will occur in the same doc-
ument pairs. 
Group 2: Existing Methods Comparison 
We adopt the NW-Train and NW-Test as training 
set and test set respectively, and utilize the CS 
algorithm to calculate the document similarity to 
verify the excellence of methods in the study. 
Then we compare its performance with the exist-
ing representative approaches proposed by Thuy 
et al (2009) and Preiss (2012) (Shown in Tab. 2).  
Algorithm P R F 
Thuy 0.45 0.32 0.37 
Preiss 0.67 0.44 0.53 
CS 0.76 0.53 0.62 
Table 2: Existing Methods Comparison 
The table shows CS outperforms other algo-
rithms, which indicates that bilingual LDA is 
valid to construct comparable corpora. Thuy et al 
(2009) matches similar documents in the view of 
inter-translated vocabulary and co-occurrence 
information features, which cannot understand 
the content effectively. Preiss (2012) uses mono-
lingual training dataset to generate topic model 
and translates source language topic model into 
target language topic model respectively. Yet the 
translation accuracy constrains the matching ef-
fectiveness of similar documents, and the cosine 
similarity is directly used to calculate document-
topic similarity failing to highlight the topic con-
tributions of different documents. 
5 Conclusion  
This study proposes a new method of using bi-
lingual topic to match similar documents. When 
CS is used to match the documents, TFIDF is 
proposed to enhance the topic discrepancies 
among different documents. The method of CP is 
also addressed to measure document similarity. 
Experimental results show that the matching 
algorithm is superior to the existing algorithms. 
It can utilize comprehensively large scales of 
document information in training set to avoid the 
information deficiency of the document itself and 
over-reliance on bilingual knowledge. The algo-
rithm makes the document match on the basis of 
understanding the document. This study does not 
calculate similar contents existed in the monolin-
gual documents. However, a large number of 
documents in the same language describe the 
same event. We intend to incorporate monolin-
gual document similarity into bilingual topics 
analysis to match multi-documents in different 
languages perfectly. 
Acknowledgments 
The work is supported by the National Natural 
Science Foundation of China under No. 
61070099 and the project of MSR-CNIC Win-
dows Azure Theme. 
281
References  
AbduI-Rauf S, Schwenk H. On the use of comparable 
corpora to improve SMT perfor-
mance[C]//Proceedings of the 12th Conference of 
the European Chapter of the Association for Com-
putational Linguistics. Association for Computa-
tional Linguistics, 2009: 16-23. 
Ji H. Mining name translations from comparable cor-
pora by creating bilingual information networks[C] 
// Proceedings of BUCC 2009. Suntec, Singapore, 
2009: 34-37. 
Braschler M, Schauble P. Multilingual Information 
Retrieval based on document alignment tech-
niques[C] // Proceedings of the Second European 
Conference on Research and Advanced Technolo-
gy for Digital Libraries. Heraklion, Greece. 1998: 
183-197. 
Tao Tao, Chengxiang Zhai. Mining comparable bilin-
gual text corpora for cross-language information 
integration[C] // Proceedings of ACM SIGKDD, 
Chicago, Illinois, USA. 2005:691-696. 
Talvensaari T, Laurikkala J, Jarvelin K, et al Creating 
and Exploiting a Comparable Corpus in Cross-
Language Information Retrieval[J]. ACM Transac-
tions on Information Systems. 2007, 25(1): 322-
334. 
Thuy Vu, Ai Ti Aw, Min Zhang. Feature-based meth-
od for document alignment in comparable news 
corpora[C] // Proceedings of the 12th Conference 
of the European Chapter of the ACL, Athens, 
Greece. 2009: 843-851. 
Otero P G, L?opez I G. Wikipedia as Multilingual 
Source of Comparable Corpora[C] // Proceedings 
of the 3rd Workshop on BUCC, LREC2010. Malta. 
2010: 21-25. 
Li B, Gaussier E. Improving corpus comparability for 
bilingual lexicon extraction from comparable cor-
pora[C]//Proceedings of the 23rd International 
Conference on Computational Linguistics. Associ-
ation for Computational Linguistics, 2010: 644-652. 
Judita Preiss. Identifying Comparable Corpora Using 
LDA[C]//2012 Conference of the North American 
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies. Mon-
tre?al, Canada, June 3-8, 2012: 558-562. 
Mimno D, Wallach H, Naradowsky J et al Polylin-
gual topic models[C]//Proceedings of the EMNLP. 
Singapore, 2009: 880-889. 
Vulic I, De Smet W, Moens M F, et al Identifying 
word translations from comparable corpora using 
latent topic models[C]//Proceedings of ACL. 2011: 
479-484. 
Ni X, Sun J T, Hu J, et al Mining multilingual topics 
from wikipedia[C]//Proceedings of the 18th inter-
national conference on World wide web. ACM, 
2009: 1155-1156. 
Blei D M, Ng A Y, Jordan M I. Latent dirichlet alo-
cation[J]. the Journal of machine Learning research, 
2003, 3: 993-1022. 
Griffiths T L, Steyvers M. Finding scientific topics[J]. 
Proceedings of the National academy of Sciences 
of the United States of America, 2004, 101: 5228-
5235. 
Manning C D, Sch?tze H. Foundations of statistical 
natural language processing[M]. MIT press, 1999. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
282
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 74?79,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Towards Using Structural Events To Assess Non-native Speech
Lei Chen, Joel Tetreault, Xiaoming Xi
Educational Testing Service (ETS)
Princeton, NJ 08540, USA
{LChen,JTetreault,XXi}@ets.org
Abstract
We investigated using structural events, e.g.,
clause and disfluency structure, from tran-
scriptions of spontaneous non-native speech,
to compute features for measuring speaking
proficiency. Using a set of transcribed au-
dio files collected from the TOEFL Practice
Test Online (TPO), we conducted a sophisti-
cated annotation of structural events, includ-
ing clause boundaries and types, as well as
disfluencies. Based on words and the anno-
tated structural events, we extracted features
related to syntactic complexity, e.g., the mean
length of clause (MLC) and dependent clause
frequency (DEPC), and a feature related to
disfluencies, the interruption point frequency
per clause (IPC). Among these features, the
IPC shows the highest correlation with holis-
tic scores (r = ?0.344). Furthermore, we in-
creased the correlation with human scores by
normalizing IPC by (1) MLC (r = ?0.386),
(2) DEPC (r = ?0.429), and (3) both (r =
?0.462). In this research, the features derived
from structural events of speech transcriptions
are found to predict holistic scores measuring
speaking proficiency. This suggests that struc-
tural events estimated on speech word strings
provide a potential way for assessing non-
native speech.
1 Introduction
In the last decade, a breakthrough in speech pro-
cessing is the emergence of a lot of active research
work on automatic estimation of structural events,
e.g., sentence structure and disfluencies, on sponta-
neous speech (Shriberg et al, 2000; Liu, 2004; Os-
tendorf et al, 2008). The detected structural events
have been successfully used in many natural lan-
guage processing (NLP) applications (Ostendorf et
al., 2008).
However, the structural events in speech data
haven?t been largely utilized by the research on us-
ing automatic speech recognition (ASR) technology
to assess speech proficiency (Neumeyer et al, 2000;
Zechner et al, 2007), which mainly used cues de-
rived at the word level, such as timing information
of spoken words. The information beyond the word
level, e.g., clause/sentence structure of utterances
and disfluency structure, has not been or is poorly
represented. For example, in Zechner et al (2007),
only special words for filled pauses such as um and
uh were obtained from ASR results to represent dis-
fluencies.
Given the successful usage of structural events
on a wide range of NLP applications and the fact
that the usage of these events is missing in the auto-
matic speech assessment research, a research ques-
tion emerges: Can we use structural events of spon-
taneous speech to assess non-native speech profi-
ciency?
We will address this question in this paper. The
paper is organized as follows: Section 2 reviews
previous research. Section 3 describes our annota-
tion convention. Section 4 reports on the data col-
lection, annotation, and quality control. Section 5
reports on features based on structural event anno-
tations. Section 6 reports on our experiments. Sec-
tion 7 discusses our findings and plans for future re-
search work.
74
2 Previous Work
In the last decade, a large amount of research (Os-
tendorf et al, 2008) has been conducted on detection
of structural events, e.g., sentence structure and dis-
fluency structure, in spontaneous speech. In these
research works, the structural events were detected
with a quite high accuracy. Furthermore, the de-
tected sentence and disfluency structures have been
found to help many of the following NLP tasks,
e.g., speech parsing, information retrieval, machine
translation, and extractive speech summary (Osten-
dorf et al, 2008).
In the second language acquisition (SLA) and
child language development research fields, the lan-
guage development is measured according to flu-
ency, accuracy, and complexity (Iwashita, 2006).
The syntactic complexity of learners? writing data
has been extensively studied in the SLA commu-
nity (Ortega, 2003). Recently, this study has been
extended to the learner?s speaking data (Iwashita,
2006). Typical metrics for examining syntactic com-
plexity include: length of production unit (e.g., T-
unit, which is defined as essentially a main clause
plus any other clauses which are dependent upon
it (Hunt, 1970), clauses, verb phrases, and sen-
tences), amount of embedding, subordination and
coordination, range of structural types, and structure
sophistication.
Iwashita (2006) investigated several measures for
syntactic complexity on the data from learners of
Japanese. The author reported that some measure-
ments, e.g., T-unit length, the number of clauses per
T-unit, and the number of independent clauses per T-
Unit, were good at predicting learners? proficiency
levels.
In addition, some previous studies used measure-
ments related to disfluencies to assess speaking pro-
ficiency. For example, Lennon (1990) used a dozen
features related to speed, pauses, and several dis-
fluency markers, such as filler pauses per T-unit,
to measure four German-speaking women?s English
improvement during a half year study in England.
He found a significant change in filled pauses per
T-unit during the studying process.
The features related to syntactic complexity and
the features related to ?smoothness? (disfluency) of
speech were jointly used in some previous stud-
ies. For example, Mizera (2006) used fluency fac-
tors related to speed, voiced smoothness (frequen-
cies of repetitions or self-corrections), pauses, syn-
tactic complexity (mean length of T-units), and
accuracy, to measure speaking proficiency on 20
non-native English speakers. In this experiment,
disfluency-related factors, such as total voiced dis-
fluencies, had a high correlation with fluency (r =
?0.45). However, the syntactic complexity factor
only showed a moderate correlation (r = 0.310).
Yoon (2009) implemented an automated disfluency
detection method and found that the disfluency-
related features lead to the moderate improvement
in the automated speech proficiency scoring.
There were limitations on using the features re-
ported in these SLA studies on standard language
tests. For example, only a very limited number of
subjects (from 20 to 30 speakers) were used in these
studies. Second, the speaking content was narra-
tions of picture books or cartoon videos rather than
standard test questions. Therefore, we conducted a
study using a much larger data set obtained from real
speech tests to address these limitations.
3 Structural Event Annotation Convention
To annotate structural events of speech content, we
have developed a convention based on previous stud-
ies and our observations on the TOEFL Practice On-
line (TPO) test data. Defining clauses is a relatively
simple task; however, defining clause boundaries
and specifying which elements fall within a particu-
lar clause is a much more challenging task for spo-
ken discourse, due to the presence of grammatical
errors, fragments, repetitions, self corrections, and
conversation fillers.
Foster et al (Foster et al, 2000) review various
units for analyzing spoken language, including syn-
tactic, semantic and intonational units, and propose
a new analysis of speech unit (AS-Unit) that they
claim is appropriate for many different purposes. In
this study, we focused on clauses given the charac-
teristics of spontaneous speech. Also, we defined
clause types based on grammar books such as (Azar,
2003). The following clause types were defined:
? Simple sentence (SS) contains a subject and a
verb, and expresses a complete thought.
75
? Independent clause (I) is the main clause that
can stand along syntactically as a complete sen-
tence. It consists minimally a subject and a fi-
nite verb (a verb that shows tense, person, or
singular/plural, e.g., he goes, I went, and I was).
? Subordinate clause is a clause in a complex
sentence that cannot stand alone as a complete
sentence and that functions within the sentence
as a noun, a verb complement, an adjective or
an adverb. There are three types of subordi-
nate clauses: noun clause (NC), relative clause
that functions as an adjective (ADJ), adverbial
clause that functions as an adverb (ADV).
? Coordinate clause (CC) is a clause in a com-
pound sentence that is grammatically equiva-
lent to the main clause and that performs the
same grammatical function.
? Adverbial phrase (ADVP) is a separate clause
from the main clause that contains a non-finite
verb (a verb that does not show tense, person,
or singular/plural).
The clause boundaries and clause types were an-
notated on the word transcriptions. Round brack-
ets were used to indicate the beginning and end of a
clause. Then, the abbreviations described above for
clause types were added. Also, if a specific bound-
ary serves as the boundaries for both the local and
global clause, the abbreviation of the local clause
was followed by that of the global. Some examples
of clause boundaries and types are reported in Ta-
ble 1.
In our annotation manual, a speech disfluency
contains three parts:
? Reparandum: the speech portion that will be
repeated, corrected, or even abandoned. The
end of the reparandum is called the interruption
point (IP), which indicates the stop of a normal
fluent speech stream.
? Editing phrase: optional inserted words, e.g.,
um.
? Correction: the speech portion that repeats,
corrects, or even starts new content.
In our annotation manual, the reparandum was en-
closed by ?*?, the editing phrase was enclosed by
?%?, and the correction was enclosed by ?$?. For
example, in the following utterance, ?He is a * very
mad * % er % $ very bad $ cop?, ?very mad? was
corrected by ?very bad? and an editing phrase, er,
was inserted.
4 Data Collection and Annotation
4.1 Audio data collection and scoring
About 1300 speech responses from the TPO test
were collected and transcribed. Each item was
scored by two experienced human raters indepen-
dently using a 4-point holistic score based on the
scoring rubrics designed for the test.
In the TPO test, some tasks required test-takers to
provide information or opinions on familiar topics
based on their personal experience or background
knowledge. Others required them to summarize and
synthesize information presented in listening and/or
reading materials. Each test-taker was required to
finish six items in one test session. Each item has a
45 or 60 seconds response time.
4.2 Annotation procedure
Two annotators (who were not the human raters
mentioned above) with a linguistics background and
past linguistics annotation experience were first pre-
sented with a draft of the annotation convention.
After reading through it, the annotators, as well as
the second and third author completed four iterative
loops of rating 4 or 5 responses per meeting. All four
discussed differences in annotations and the conven-
tion was refined as needed. After the final iteration
of comparisons, the raters seemed to have very few
disagreement and thus began annotating sets of re-
sponses. Each set consisted of roughly 50-75 re-
sponses and then a kappa set of 30-50 responses
which both annotators completed. Accordingly, be-
tween the two annotators, a set comprised roughly
130 to 200 responses. Each response takes roughly
3-8 minutes to annotate. The annotators were in-
structed to listen to the corresponding audio file if
they needed the prosodic information to annotate a
particular speech disfluency event.
76
Clause type Example
SS (That?s right |SS)
I (He turned away |I) as soon as he saw me |ADV)
NC ((What he did |NC) shocked me |I)
ADJ (She is the woman (I told you about |ADJ)|I)
ADV (As soon as he saw me |ADV) (he turned away |I)
CC (I will go home |I) (and he will go to work |CC)
ADVP (While walking to class |ADVP) (I ran into a friend |I)
Table 1: Examples of clause boundary and type annotation
4.3 Evaluation of annotation
To evaluate the quality of structural event anno-
tation, we measured the inter-rater agreement on
clause boundary (CB) annotation and interruption
point (IP) of disfluencies1.
We used Cohen?s ? to calculate the annotator
agreement on each kappa set. ? is calculated on the
absence or presence of a boundary marker (either a
clause boundary (CB) or an interruption point (IP)
between consecutive words). For each consecutive
pair of words, we check for the existence of one or
more boundaries, and collapse the set into one term
?boundary? and then compute the agreement on this
reduced annotation.
In Table 2, we list the annotator agreement for
both boundary events over 4 kappa sets. The second
column refers to the number of speech responses in
the kappa set, the next two columns refer to the an-
notator agreement using the Cohen?s ? value on CB
and IP annotation results.
Set N ? CB ? IP
Set1 54 0.886 0.626
Set2 71 0.847 0.687
Set3 35 0.855 0.695
Set4 34 0.899 0.833
Table 2: Between-rater agreement of structural event an-
notation
In general, a ? of 0.8-1.0 represents excellent
agreement, 0.6-0.8 represents good agreement, and
so forth. Over each kappa set, ? for CB annota-
tions ranges between 0.8 and 0.9, which is an ex-
1Measurement on CBs and IPs can provide a rough qual-
ity measurement of annotations. In addition, doing so is more
important to us since automatic detection of these two types of
events will be investigated in future.
cellent agreement; ? for IP annotation ranges be-
tween 0.6 and 0.8, which is a good agreement. Com-
pared to annotating clauses, marking disfluencies is
more challenging. As a result, a lower between-rater
agreement is expected.
5 Features Derived On Structural Events
Based on the structural event annotations, including
clause boundaries and their types, as well as disflu-
encies, some features measuring syntactic complex-
ity and disfluency profile were derived.
Since simple sentence (SS), independent clause
(I), and conjunct clause (CC) represent a complete
idea, we treat them as an approximate to a T-unit (T).
The clauses that have no complete idea, are depen-
dent clauses (DEP), including noun clauses (N), rel-
ative clauses that function as adjective (ADJ), adver-
bial clauses (ADV), and adverbial phrases (ADVP).
The total number of clauses is a summation of the
number of T-units (T), dependent clauses (DEP), and
fragments2 (denoted as F). Therefore,
NT = NSS +NI +NCC
NDEP = NNC +NADJ +NADV +NADV P
NC = NT +NDEP +NF
Assuming Nw is the total number of words in
the speech response (without pruning speech re-
pairs), the following features, including mean length
of clause (MLC), dependent clauses per clause
(DEPC), and interruption points per clause (IPC),
are derived:
MLC = Nw/NC
2It is either a subordinate clause that does not have a cor-
responding independent clause or a string of words without a
subject or a verb that does not express a complete thought.
77
DEPC = NDEP /NC
IPC = NIP /NC
Furthermore, we elaborated the IPC feature. Dis-
fluency is a complex behavior and is influenced by
a variety of factors, such as proficiency level, speak-
ing rate, and familiarity with speaking content. The
complexity of utterances is also an important fac-
tor on the disfluency pattern. For example, Roll
et al (Roll et al, 2007) found that complexity of
expression computed based on the language?s pars-
ing tree structure influenced the frequency of disflu-
encies in their experiment on Swedish. Therefore,
since disfluency frequency was not only influenced
by the test-takers? speaking proficiency but also by
the utterance?s syntactic structure?s difficulty, we re-
duced the impact from the syntactic structure so that
we can focus on speakers? ability. For this purpose,
we normalized IPC by dividing by some features re-
lated to syntactic-structure?s complexity, including
MLC, DEPC, and both. Therefore, the following
elaborated disfluency-related features were derived:
IPCn1 = IPC/MLC
IPCn2 = IPC/DEPC
IPCn3 = IPC/MLC/DEPC
6 Experiment
For each item, two human raters rated it separately
with a score from 1 to 4. If these two scores are
consistent (the difference between two scores is ei-
ther zero or one), we put this item in an item-pool.
Finally, a total of 1, 257 audio items were included
in the pool. Following the score-handling protocol
used in the TPO test, we used the first human rater?s
score as the item score. From the obtained item-
pool, we selected speakers with more than three
items so that the averaged score per speaker can be
estimated on several items to achieve a robust score
computation3. As a result, 175 speakers4 were se-
lected.
3The mean holistic score of these speakers is 2.786, which
is close to the mean holistic score of the selected item-pool
(2.785), indicating that score distribution was kept after focus-
ing on speakers with more than three items.
4If a speaker was assigned in a Kappa set in the annotation
as described in Section 4, this speaker would have as many as 12
annotated items. Therefore, the minimum number of speakers
from the item-pool was about 105 (1257/12).
For each speaker, his or her annotations of words
and structural events were used to extract the fea-
tures described in Section 5. Then, we computed
the Pearson correlation among the obtained features
with the averaged holistic scores per speaker.
Feature r
MLC 0.211
DEPC 0.284
IPC -0.344
IPCn1 -0.386
IPCn2 -0.429
IPCn3 -0.462
Table 3: Correlation coefficients (rs) between the fea-
tures derived from structural events with human scores
averaged on test takers
Table 3 reports on the correlation coefficient
(r) between the proposed features derived from
structural events with holistic scores. Relying
on three simple structural event annotations, i.e.,
clause boundaries, dependent clauses, and interrup-
tion points in speech disfluencies, some promising
correlations between features with holistic scores
were found. Between the two syntactic complex-
ity features, the DEPC has a higher correlation with
holistic scores than the MLC (0.284 > 0.211). It ap-
pears that a measurement about clauses? embedding
profile is more informative about a speaker?s profi-
ciency level. Second, compared to features measur-
ing syntactic complexity, the feature measuring the
disfluency profile is better to predict human holis-
tic scores on this non-native data set. For example,
IPC has a r of ?0.344, which is better than the fea-
tures about clause lengths or embedding. Finally, by
jointly using the structural events related to clauses
and disfluencies, we can further achieve a further
improved r. Compared to IPC, IPCn3 has a relative
34.30% correlation increase. This is consistent with
our idea of reducing utterance-complexity?s impact
on disfluency-related features.
7 Discussion
In most current automatic speech assessment sys-
tems, features derived from recognized words, such
as delivery features about speaking rate, pause infor-
mation, and accuracy related to word identities, have
been widely used to assess non-native speech from
78
fluency and accuracy points of view. However, in-
formation beyond recognized words, e.g., the struc-
ture of clauses and disfluencies, has only received
limited attention. Although several previous SLA
studies used features derived from structural events
to measure speaking proficiency, these studies were
limited and the findings from them were difficult to
directly apply to on large-scale standard tests.
In this paper, using a large-sized data set col-
lected in the TPO speaking test, we conducted an
sophisticated annotation of structural events, includ-
ing boundaries and types of clauses and disfluen-
cies, from transcriptions of spontaneous speech test
responses. A series of features were derived from
these structural event annotations and were eval-
uated according to their correlations with holistic
scores. We found that disfluency-related features
have higher correlations to human holistic scores
than features about syntactic complexity, which con-
firms the result reported in (Mizera, 2006). In spon-
taneous speech utterances, simple syntactic structure
tends to be utilized by speakers. This is in contrast to
sophisticated syntactic structure appearing in writ-
ing. This may cause that complexity-related features
are poor at predicting fluency scores. On the other
hand, disfluencies, a pattern unique to spontaneous
speech, were found to play a more important role in
indicating speaking proficiency levels.
Although syntactic complexity features were not
highly indicative of holistic scores, they were useful
to further improve disfluency-related features? corre-
lation with holistic scores. By normalizing IPC us-
ing measurements representing syntactic complex-
ity, we can highlight contributions from speakers?
proficiency levels. Therefore, in our experiment,
IPCn3 shows a 34.30% relative improvement in its
correlation coefficient with human holistic scores
over the original IPC.
The study reported in this paper suggests promise
that structural events beyond speech recognition re-
sults can be utilized to measure non-native speaker
proficiency levels. Recently, in the NLP research
field, an increasing amount of effort has been
made on structural event detection in spontaneous
speech (Ostendorf et al, 2008). Therefore, such
progress can benefit the study of automatic estima-
tion of structural events on non-native speech data.
For our future research plan, first, we will inves-
tigate automatically detecting these structural events
from speech transcriptions and recognition hypothe-
ses. Second, the features derived from the obtained
structural events will be used to augment the features
in automatic speech assessment research to provide
a wider construct coverage than fluency and pronun-
ciation features do.
References
B. Azar. 2003. Fundamentals of English grammar.
Pearson Longman, White Plains, NY, 3rd edition.
P. Foster, A. Tonkyn, and G. Wigglesworth. 2000. Mea-
suring spoken language: A unit for all reasons. Ap-
plied Linguistics, 21(3):354.
K. W. Hunt. 1970. Syntactic maturity in school chil-
dren and adults. In Monographs of the Society for Re-
search in Child Development. University of Chicago
Press, Chicago, IL.
N. Iwashita. 2006. Syntactic complexity measures and
their relation to oral proficiency in Japanese as a for-
eign language. Language Assessment Quarterly: An
International Journal, 3(2):151?169.
P. Lennon. 1990. Investigating fluency in EFL: A quanti-
tative approach. Language Learning, 40(3):387?417.
Y. Liu. 2004. Structural Event Detection for Rich Tran-
scription of Speech. Ph.D. thesis, Purdue University.
G. J. Mizera. 2006. Working memory and L2 oral flu-
ency. Ph.D. thesis, University of Pittsburgh.
L. Neumeyer, H. Franco, V. Digalakis, and M. Weintraub.
2000. Automatic Scoring of Pronunciation Quality.
Speech Communication, 30:83?93.
L. Ortega. 2003. Syntactic complexity measures and
their relationship to L2 proficiency: A research syn-
thesis of college-level L2 writing. Applied Linguistics,
24(4):492.
M. Ostendorf et al 2008. Speech segmentation and spo-
ken document processing. Signal Processing Maga-
zine, IEEE, 25(3):59?69, May.
M. Roll, J. Frid, and M. Horne. 2007. Measuring syntac-
tic complexity in spontaneous spoken Swedish. Lan-
guage and Speech, 50(2):227.
E. Shriberg, A. Stolcke, D. Hakkani-Tur, and G. Tur.
2000. Prosody-based automatic segmentation of
speech into sentences and topics. Speech Communi-
cation, 32(1-2):127?154.
S. Yoon. 2009. Automated assessment of speech fluency
for L2 English learners. Ph.D. thesis, University of
Illinois at Urbana-Champaign.
K. Zechner, D. Higgins, and Xiaoming Xi. 2007.
SpeechRater: A Construct-Driven Approach to Scor-
ing Spontaneous Non-Native Speech. In Proc. SLaTE.
79
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 38?45,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Detecting Structural Events for Assessing Non-Native Speech
Lei Chen
Educational Testing Service
Princeton NJ USA
LChen@ets.org
Su-Youn Yoon
Educational Testing Service
Princeton NJ USA
SYoon@ets.org
Abstract
Structural events, (i.e., the structure of clauses
and disfluencies) in spontaneous speech, are
important components of human speaking and
have been used to measure language devel-
opment. However, they have not been ac-
tively used in automated speech assessment
research. Given the recent substantial progress
on automated structural event detection on
spontaneous speech, we investigated the de-
tection of clause boundaries and interruption
points of edit disfluencies on transcriptions
of non-native speech data and extracted fea-
tures from the detected events for speech
assessment. Compared to features com-
puted on human-annotated events, the features
computed on machine-generated events show
promising correlations to holistic scores that
reflect speaking proficiency levels.
1 Introduction
Spontaneous speech utterances are organized in a
structured way and generated dynamically with op-
tional disfluencies. In second language acquisition
(SLA) research, information related to the structure
of utterances and profile of disfluencies has been
widely used to monitor speakers? language develop-
ment processes (Iwashita, 2006). However, struc-
tural events in human conversations have not been
actively used in the automated speech assessment re-
search. For example, most research that used Auto-
matic Speech Recognition (ASR) technology to au-
tomatically score speaking proficiency (Neumeyer
et al, 2000; Zechner et al, 2007) focused on word-
level cues for fluency and accuracy.
In the last decade, a large amount of research (Go-
toh and Renals, 2000; Shriberg et al, 2000; Liu,
2004; Ostendorf et al, 2008) has been conducted
on structural event detection (i.e., sentence and dis-
fluency structure). This research has resulted in
better models for structural event detection. The
detected structural events have been found to help
many of the following natural language processing
(NLP) tasks: speech parsing, information retrieval,
machine translation, and extractive speech summa-
rization (Ostendorf et al, 2008).
Because structural event information: (1) is im-
portant for understanding/processing speech, (2)
has been successfully used in monitoring language
development, which will be summarized in Sec-
tion 2, (3) has received limited attention in auto-
mated speech assessment, and (4) has been actively
investigated in the speech research domain in the
past decade, it is worthwhile investigating the util-
ity of using structural event detection on automated
speech assessment. Because of the fairly low word
accuracy currently achieved when recognizing spon-
taneous non-native speech of mixed proficiency lev-
els and native language backgrounds, this study will
focus on the transcribed words rather than speech
recognition outputs.
This paper is organized as follows: Section 2 re-
views previous research; Section 3 reports on the
data used in the paper, including the collection, scor-
ing, transcription, and annotation processes; Sec-
tion 4 discusses the methods we utilized for struc-
tural event detection; Section 5 describes the exper-
iments of structural event detection; Section 6 de-
scribed the features derived from the event sequence
38
for assessing speech and evaluation results on these
features; Section 7 discusses the findings of our re-
search and plans for future directions.
2 Previous Research
In the SLA and child language development research
fields, language development is measured accord-
ing to fluency, accuracy, and complexity (Iwashita,
2006). Structural events are used to derive the fea-
tures measuring syntactic complexity. For example,
typical metrics for measuring syntactic complexity
include: length of production units (e.g., T-units1,
clauses, verb phrases, and sentences), amount of
embedding, subordination and coordination, range
of structural types, and structural sophistication.
Iwashita (2006) investigated several measures of
syntactic complexity on data generated by learners
of Japanese. The author reported that some mea-
surements (e.g., T-unit length, the number of clauses
per T-unit, and the number of independent clauses
per T-unit) were good at predicting learners? profi-
ciency levels.
In addition, speech disfluencies are used to mea-
sure language development. For example, Lennon
(1990) used a dozen features related to speed,
pauses, and several disfluency markers, such as
filled pauses per T-unit, to measure the improvement
of English proficiency for four German-speaking
women during a six-month study in England. He
found a significant change in filled pauses per T-unit
during the study process.
These two types of features derived from struc-
tural events were combined in other previous stud-
ies. For example, Mizera (2006) used fluency fac-
tors related to speed, voiced smoothness (frequency
of repetitions or self-corrections), pauses, syntactic
complexity (mean length of T-units), and accuracy,
to measure speaking proficiency on 20 non-native
English speakers. In this experiment, disfluency-
related factors, such as the total number of voiced
disfluencies, correlated strongly with the fluency
score (r = ?0.45); however, the syntactic com-
plexity factor only showed a moderate correlation
(r = 0.310).
There have been previous efforts in using NLP
1A T-unit is defined as essentially a main clause plus any
other clauses which are dependent upon it (Hunt, 1970).
technology to automatically calculate syntactic com-
plexity metrics on learners? writing data. For exam-
ple, Lu (2009) and Sagae et al (2005) used parsing
to get structural information on written texts; how-
ever, such efforts have not been undertaken in as-
sessing speech data.
Chen et al (2010) annotated structural events
(such as clause structure and disfluencies) on En-
glish language learners? speech transcriptions and
extracted features based on the structural event pro-
file. They found that the features derived from struc-
tural event profile show promising correlation to hu-
man holistic scores. Berstein et al (2010) also com-
puted the features related to sentence lengths and
the counts of syntactic entities. They found the ex-
tracted features were highly correlated to holistic
scores measuring test-takers? language proficiency
in both English and Spanish.
In the speech research domain, a large amount
of research has been conducted to detect struc-
tural events in speech transcriptions and recognized
words using lexical and prosodic cues. Using a lan-
guage model (LM) trained on words combined with
the events of interest is a popular technique for us-
ing textual information for structural event detec-
tion. For example, Heeman and Allen (1999) devel-
oped a LM including part of speech (POS) tags, dis-
course markers (e.g., right, anyway), speech repairs,
and intonational phrases. In this way, structural in-
formation (e.g., speech repairs), could be predicted
using a traditional speech recognition approach.
Prosodic information has been widely used to fur-
ther improve textual models. For example, a sim-
ple prosodic feature, pause duration between words,
was used in Gotoh and Renals (2000) to detect sen-
tence boundaries. It was found that the pause dura-
tion model alone was better than using an LM alone,
and the combination of the two models further im-
proved the performance.
More advanced prosody models were used in
other research on sentence boundary and speech re-
pair detections (Shriberg et al, 2000; Shriberg and
Stolcke, 2004). A general framework was built com-
bining textual and prosodic cues to detect various
kinds of structural events in speech, including sen-
tence boundaries, disfluencies, topic boundaries, di-
alog acts, emotion, etc. Shriberg and Stolcke (2004)
extracted prosodic features such as pause, phone du-
39
ration, rhyme duration, and F0 features. Using all
of these features, a decision tree was built to de-
tect possible structural events. An LM augmented
with structural event tokens was also used to de-
tect structural events based on textual cues. Fi-
nally, a Hidden Markov Model (HMM) was used
to combine estimations from the textual model (an
augmented LM with structural events) and prosodic
model (decision-tree based on prosodic features).
Research on structural event detection has been
strongly affected by the DARPA EARS pro-
gram (EARS, 2002). As in Shriberg et al (2000), the
structural event detection (e.g., sentence units (SUs)
and speech repairs) investigated in EARS was a clas-
sification task utilizing both prosodic and textual
knowledge sources. New approaches for combin-
ing the two knowledge sources, including maximum
entropy (MaxEnt) and conditional random fields
(CRFs), were studied to address the weaknesses of
the generative HMM approach (Liu et al, 2004). Liu
et al (2005) concluded that ?adding textual infor-
mation, building a more robust prosodic model, us-
ing conditional modeling approaches (Maxent and
CRF), and system combination all yield perfor-
mance gains.?
3 Non-native Structural Event Corpus
Non-native speech data were collected from the
TOEFL Practice Test Online (TPO) (ETS, 2006).
In each TPO test, test-takers were required to re-
spond to six speaking test items, in which they were
required to provide information or opinions on fa-
miliar topics, based on their personal experience or
background knowledge. For example, the test-takers
were asked to describe their opinions about living on
or off campus.
A total of 1066 responses were collected from ex-
aminees. Then, a group of experienced human raters
scored these items based on the scoring rubrics de-
signed for scoring the TPO test. For each item, two
human raters independently assigned 4-point holis-
tic scores for test-takers? English proficiency levels.
The speaking content was transcribed by a pro-
fessional transcribing agency. On the transcrip-
tions, structural event annotations were added, in-
cluding (1) locations of clause boundaries, (2) types
of clauses (e.g., noun clauses, adjective clauses, ad-
verb clauses, etc.), and (3) disfluencies.
Disfluencies can further be sub-classified into sev-
eral groups: silent pauses, filled pauses (e.g., uh and
um), false starts, repetitions, and repairs. The repeti-
tions and repairs were denoted as ?edit disfluency?,
which were comprised of a reparandum, an optional
editing term, and a correction. The reparandum is
the part of an utterance that a speaker wants to re-
peat or change, while the correction contains the
speaker?s correction. The editing term can be a
filled pause (e.g., um) or an explicit expression (e.g.,
sorry). The interruption point (IP), occurring at the
end of the reparandum, is where the fluent speech is
interrupted to prepare for the correction.
For the research reported in this paper, we focus
on two structural events: the locations of clause-
ending boundaries (CBs) and interruption points
(IPs) of edit disfluencies. Note that if several clauses
(in different layers of a clause hierarchy) end at the
same word boundary, these clause boundaries were
collapsed into one CB event.
Two persons annotated the corpus separately and
their annotation quality was monitored by using sev-
eral Kappa computations. For CBs, ? ranges from
0.85 to 0.90; for IPs, ? ranges from 0.63 to 0.83.
Generally, a ? greater than 0.8 indicates a good
between-rater agreement and ? in the range of 0.6
to 0.8 indicates acceptable agreement (Landis and
Koch, 1977). Therefore, we believe that our human
annotations are sufficiently reliable to be used in the
following experiments.
4 Methods of Structural Event Detection
4.1 Features for structural event detection
In previous research (Gotoh and Renals, 2000;
Shriberg et al, 2000; Liu, 2004), prosodic cues
were found to be helpful, however, such findings
on native speech data may not work well with non-
native speech data. Anderson-Hsieh and Venkata-
giri (1994) compared the pause frequencies of three
groups of speakers (native, high-scoring, and low-
scoring non-native speakers). They found that pause
frequency was higher for groups of speakers with
lower speaking skills. For native speakers, a long
pause after a word-ending boundary is an impor-
tant cue for signaling the existence of a sentence or
clause boundary. However, the fact that there are
40
more frequent pauses in non-native speech obscures
this relationship.
On our non-native speech corpus, we conducted
a pilot study on a widely-used prosodic feature, the
pause duration2 after a word, for its predictive abil-
ity to detect clause boundaries. If the duration of the
pause after a word boundary is longer than 0.15 sec-
ond, we call it a long pause. We measured the likeli-
hood of being a CB event on the words followed by a
long pause. For each score level, the likelihoods are:
15% for a score of 1, 22% for a score of 2, 28% for
a score of 3, and 35% for a score of 4. Clearly, for
low-proficiency speakers (i.e., speakers with a score
of 1), long pauses in their utterances are not tightly
linked to CBs. Therefore, more research is needed
to utilize prosodic cues on non-native speech; in this
paper, we focus on lexical features.
4.2 Statistical models
Based on lexical features, the structural event detec-
tion task can be generalized as follows:
E? = argmax
E
P (E|W )
Given that E denotes the between-word event se-
quence and W denotes the corresponding lexical
cues, the goal is to find the event sequence that has
the greatest probability, given the observed features.
Recently, conditional modeling approaches were
successfully used in sentence units (SUs) and speech
repairs detection (Liu, 2004). Hence, we use the
Maximum Entropy (MaxEnt) (Berger et al, 1996)
and Conditional Random Fields (CRFs) (Lafferty et
al., 2001) approaches to build statistical models for
structural event detection.
5 Structural Event Detection Experiment
5.1 Setup
In our experiment, the whole corpus described in
Section 3 was split into a training set (train), a devel-
opment test set (dev), and testing set (test), without
speaker overlap between any pair of sets. Table 1
summarizes the numbers of items and words, as well
as structural events of each dataset.
2Pause durations were obtained by running forced alignment
using speech and transcriptions on a tri-phone HMM speech
recognizer
train dev test
# item 664 101 301
# word 71523 10509 33754
# CB 6121 918 2852
# IP 1767 267 1112
Table 1: The number of items, words, and structural
events of the three sets in the TPO corpus
On average, each item contains about 108.6
words, 9.3 CBs, and 3.0 IPs. 9% of the word bound-
aries are associated with a CB event and 3% of the
word boundaries are associated with an IP event.
Clearly, these CB and IP events are sparse and such
a skewed distribution of structural events increases
the difficulty of structural event detection.
5.2 Models
The following two conditional models were built to
detect CB and IP events:
? MaxEnt: Given wi as the word token at po-
sition i, the word n-gram features include:
?wi?, ?wi?1, wi?, ?wi, wi+1?, ?wi?2, wi?1, wi?,
?wi, wi+1, wi+2?, and ?wi?1, wi, wi+1?. Given
ti as the POS tag3 at position i, the POS
n-gram features include: ?ti?, ?ti?1, ti?,
?ti, ti+1?, ?ti?2, ti?1, ti?, ?ti, ti+1, ti+2?, and
?ti?1, ti, ti+1?.
For IP detection, in addition to the n-gram fea-
tures described above, another four features
that capture syntactic pattern of disfluencies are
utilized:
? filled pause adjacency: This feature has
a binary value showing whether a filled
pause such as uh or um was adjacent to
the current word (wi).
? word repetition: This feature has a binary
value showing whether the current word
(wi) was repeated in the following 5 words
or not.
3POS tags were obtained by tagging words using a MaxEnt
POS tagger, which was implemented in the OpenNLP toolkit
and trained on the Switchboard (SWBD) corpus. This POS tag-
ger was trained on about 528K word/tag pairs and achieved an
tagging accuracy of 96.3% on a test set of 379K words.
41
? similarity: This feature has a continuous
value which measures the similarity be-
tween the reparandum and correction. As-
suming that wi was the end of the reparan-
dum, the start point and the end point of
the reparandum and correction were es-
timated, and the string edit distance be-
tween the reparandum and correction was
calculated. The start point and the end
point of the reparandum and correction
were estimated as follows; if wi appeared
in the following 5 words, the second oc-
currence was defined as the end of the cor-
rection. Otherwise, wi+5 was defined as
the end of correction. Secondly, N , the
length of the correction was calculated,
and wi?N+1 was defined as the start point
of the reparandum. During the calculation
of the string edit distance, a word frag-
ment was considered to be the same as
a word whose initial character sequences
matched it.
? length of correction: This feature counts
the number of words in the correction.
The first two features are similar to the features
used in (Liu, 2004) while the last two features
provide important keys in distinguishing edit
disfluencies from fluent speech. Since the cor-
rection is composed of word sequences that are
similar to the reparandum, these two features
are higher than zero when the target word is a
part of the edit disfluency. In addition, these
two numeric features were discretized by using
an equal-distance binning approach.
Using n-gram features for CB detection and all
these lexical features for IP detection, we used
the Maxent toolkit designed by Zhang (2005) to
build MaxEnt models. The L-BFGS parameter
estimation method is used, with the Gaussian-
prior smoothing technique to avoid over-fitting.
The Gaussian prior is estimated on the dev set.
? CRF: All features which were described in
building MaxEnt models were used in the CRF
model. We used the Java-based NLP package
Mallet (McCallum, 2005) to build CRF mod-
els. Similar to MaxEnt models, Gaussian-prior
smoothing was used with the priors estimated
on the dev set.
These models were trained using the train set. Be-
sides Gaussian priors, other parameters in the model
training (i.e., the training iteration number as well as
the cutting-point for event decisions) were estimated
using the dev set. Finally, the trained models were
evaluated on the test set.
5.3 Evaluation of event detection
Since structural event detection was treated as a clas-
sification task in this paper, four standard evaluation
metrics were used:
accuracy =
TP + TN
TP + FP + TN + FN
precision =
TP
TP + FP
recall =
TP
TP + FN
F1 = 2?
recall ? precision
recall + precision
where, TP and FP denote the number of true pos-
itives and false positives, and TN and FN denote
the number of true negatives and false negatives. A
structural event (a CB or IP boundary) is treated as a
positive class. In our experiment, since we treated
precision and recall as equally important, the F1
measurement was used.
For each model, if the estimated probability,
P (Ei|W ), is larger than a threshold, the correspond-
ing word boundary will be estimated to be a positive
class. The threshold was chosen when a maximal
F1 score was achieved on the dev set.
A model that always predicts the majority class
(a no-event in this study) was treated as a baseline
model. For CB detection, this type of baseline model
resulted in an accuracy of 91.6%; for IP detection,
this type of baseline model resulted in an accuracy
of 96.7%.
5.4 Results of structural event detection
Table 2 summarizes the performance of the two
models on the CB and IP detection tasks.
For CB detection, two conditional models are su-
perior to the baseline CB detection (with an accuracy
of 91.6%); they achieved relatively high F1 scores
42
Acc. Pre. Rec. F1
CB
MaxEnt 94.5 66.1 71.8 0.689
CRF 96.1 82.3 68.6 0.749
IP
MaxEnt 98.1 61.8 55.2 0.583
CRF 98.4 76.9 48.0 0.591
Table 2: Experimental results of the CB and IP detection
measurement using accuracy (Acc.), precision (Pre.), re-
call (Rec.) and F1 measurement (F1) on the TPO data
ranging from 0.689 to 0.749. Between the two mod-
els, the CRF model achieved the higher F1 score
at 0.749, The lower F-score of the MaxEnt model
may be caused by the fact that the MaxEnt model
does not use event history information in its decod-
ing process.
However, these two models achieved lower per-
formance on the task of detecting IPs for editing dis-
fluencies. F-scores became about 0.58 to 0.59 for
IP detections. The degraded performance may be
caused by the extremely low IP distribution (only
3%) in our data. Between the two modeling ap-
proaches, consistent with the result shown for CB
detection, the CRF model achieved a higher F1
score (0.591).
6 Using Detected Structural Events for
Speech Assessment
6.1 Features assessing proficiency
Many previous SLA studies used the length of pro-
duction units and frequency of disfluencies as met-
rics to measure language development (Iwashita,
2006; Lennon, 1990; Mizera, 2006). Our automated
structural event detection provides the locations of
CBs and IPs, which can be used to compute these
features for use in speech assessment.
Using Nw to represent the total number of words
in the spoken response (without pruning the reparan-
dums and edit terms in the edit disfluencies), NC
as the total number of CBs, and NIP as the total
number of IPs detected on transcriptions of speech
streams, the following features (i.e, mean length of
clause (MLC), interruption points per clause (IPC),
and interruption points per word (IPW)) were de-
rived:
MLC = Nw/NC
IPC = NIP /NC
IPW = NIP /Nw
The IPW can be treated as the IPC normalized
by the MLC. The reason for this normalization is
that disfluency behavior is influenced by various fac-
tors, such as speakers? proficiency levels as well as
the difficulty of utterances? structure. For example,
Roll et al (2007) found that the complexity of ex-
pression, computed based on the language?s parsing-
tree structure, influenced the frequency of disflu-
encies in their experiment on Swedish responses.
Therefore, the fact that IPW is the IPC normalized
by MLC (a feature related to complexity of utter-
ances? structure) helps to reduce the impact of utter-
ances? structure and to highlight contributions from
the speaker?s proficiency.
6.2 Results of measuring the derived features
On the test set, we produced CB and IP event se-
quences estimated by the MaxEnt and CRF models,
respectively. These machine-generated events were
evaluated by comparison with human annotations,
which were denoted as REF.
The proposed features described in Section 6.1
were computed on the word/event sequence of each
item. In addition, given the fact that each item only
covers approximately one-minute of speech and the
content is quite limited, we also extracted features
on the test-taker level by combining the detected
events of all of the items spoken by each test-taker.
Then, according to the score handling protocol used
in TPO, the human-holistic scores from the first hu-
man rater were used as item scores to compute Pear-
son correlation coefficients (rs) with the features.
For the test-taker level evaluation, we used the aver-
age score for each test-taker from all of his/her item
scores.
Table 3 reports on the evaluation results of the
features derived from the structural event estima-
tions. Compared to rs computed on the speaker
level using multiple (as many as 6) items, rs com-
puted on the item level are generally lower. This
is because words and events are limited in this one-
minute long response. Among the three features, the
43
Model rMLC rIPC rIPW
Per item
REF 0.003 ?0.369 ?0.402
MaxEnt ?0.012 ?0.329 ?0.343
CRF ?0.042 ?0.328 ?0.335
Per speaker
REF 0.066 ?0.453 ?0.516
MaxEnt 0.055 ?0.396 ?0.417
CRF 0.043 ?0.355 ?0.366
Table 3: Correlation coefficients (rs) between the fea-
tures derived from structural events with human scores
on the item and speaker levels
MLC shows the lowest r to human holistic scores. In
contrast, the two features derived from interruption
points show promising rs to human holistic scores.
Between them, the IPW always shows a higher r
than the IPC. Compared to the features extracted on
human annotations, the features derived from struc-
tural events automatically estimated by the two NLP
models show a lower but sufficiently high r. The
features derived from the MaxEnt model?s estima-
tions on the test-taker level show a greater r than the
features derived from the CRF model estimations.
7 Discussion
Three features measuring syntactic complexity and
disfluency profile of speaking, MLC, IPC, and IPW,
were extracted on the structural event sequences es-
timated by the developed models. Compared to the
features extracted from the human-annotated struc-
tural events, the features derived from machine-
generated event sequences show promisingly close
correlations.
Applying automated structural event detection to
spontaneous speech brings many benefits for auto-
matic speech assessment. First, obtaining informa-
tion beyond the word level, such as the structure of
clauses and disfluencies, can expand and improve
the construct4 coverage of speech features. Second,
knowing the structure of utterances helps to facili-
tate the application of more NLP processing meth-
ods (e.g., collocation detection that requires infor-
mation about sentence boundaries), to speech con-
4A construct is the set of knowledge, skills, and abilities
measured by a test.
tent. In this study, using only simple word and
POS based n-gram features, CBs can be detected
relatively well (with an F1 score of approximately
0.70). More lexical features reflecting repair proper-
ties were found to help improve IP detection perfor-
mance. In addition, IP-based features derived from
machine-generated event sequences show promis-
ing correlation with human holistic scores. Results
in detection of clause boundaries and interruption
points support the approach of utilizing automated
structural event detection on speech assessment.
We plan to continue our research in the following
three directions. First, we will investigate integrat-
ing prosodic cues to further improve the structural
event detection performance on non-native speech.
Second, we will investigate estimating structural
events directly on speech recognition results. Third,
other aspects of syntactic complexity, such as the
embedding of clauses, will be studied to provide a
broader set of features for speech assessment.
References
J. Anderson-Hsieh and H. Venkatagiri. 1994. Syllable
duration and pausing in the speech of chinese ESL
speakers. TESOL Quarterly, pages 807?812.
A. Berger, S. Pietra, and V. Pietra. 1996. A maximum en-
tropy approach to natural language processing. Com-
putational Linguistics, 22:39?72.
J. Berstein, J. Cheng, and M. Suzuki. 2010. Fluency and
Structural Complexity as Predictors of L2 Oral Profi-
ciency. In Proc. of InterSpeech.
L. Chen, J. Tetreault, and X. Xi. 2010. Towards using
structural events to assess non-native speech. In Fifth
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, page 74.
EARS. 2002. DARPA EARS Program. http://
projects.ldc.upenn.edu/EARS/.
ETS. 2006. TOEFL Practice Online Test (TPO).
Y. Gotoh and S. Renals. 2000. Sentence boundary de-
tection in broadcast speech transcript. In Proceed-
ings of the International Speech Communication As-
sociation (ISCA) Workshop: Automatic Speech Recog-
nition: Challenges for the new Millennium ASR-2000.
P. Heeman and J. Allen. 1999. Speech repairs, in-
tonational phrased and discourse markers: Modeling
speakers? utterances in spoken dialogue. Computa-
tional Linguistics.
K. W. Hunt. 1970. Syntactic maturity in school chil-
dren and adults. In Monographs of the Society for Re-
44
search in Child Development. University of Chicago
Press, Chicago, IL.
N. Iwashita. 2006. Syntactic complexity measures and
their relation to oral proficiency in Japanese as a for-
eign language. Language Assessment Quarterly: An
International Journal, 3(2):151?169.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random field: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of the International Conference on Machine Learning
(ICML).
J. R Landis and G. G Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
pages 159?174.
P. Lennon. 1990. Investigating fluency in EFL: A quanti-
tative approach. Language Learning, 40(3):387?417.
Y. Liu, A. Stolcke, E. Shriberg, and M. Harper. 2004.
Comparing and combining generative and poste-
rior probability models: Some advances in sentence
boundary detection in speech. In Proceedings of the
Empirical Methods in Natural Language Processing
(EMNLP).
Y. Liu, E. Shriberg, A. Stolcke, B. Peskin, J. Ang, Hillard
D., M. Ostendorf, M. Tomalin, P. Woodland, and
M. Harper. 2005. Structural Metadata Research in the
EARS Program. In Proceedings of the International
Conference of Acoustics, Speech, and Signal Process-
ing (ICASSP).
Y. Liu. 2004. Structural Event Detection for Rich Tran-
scription of Speech. Ph.D. thesis, Purdue University.
X. Lu. 2009. Automatic measurement of syntactic com-
plexity in child language acquisition. International
Journal of Corpus Linguistics, 14(1):3?28.
A. McCallum. 2005. Mallet: A machine learning toolkit
for language. http://mallet.cs.umass.edu.
G. J. Mizera. 2006. Working memory and L2 oral flu-
ency. Ph.D. thesis, University of Pittsburgh.
L. Neumeyer, H. Franco, V. Digalakis, and M. Weintraub.
2000. Automatic Scoring of Pronunciation Quality.
Speech Communication, 30:83?93.
M. Ostendorf, B. Favre, R. Grishman, D. Hakkani-
Tur, M. Harper, D. Hillard, J. Hirschberg, Heng
Ji, J.G. Kahn, Yang Liu, S. Maskey, E. Matusov,
H. Ney, A. Rosenberg, E. Shriberg, Wen Wang, and
C. Woofers. 2008. Speech segmentation and spoken
document processing. Signal Processing Magazine,
IEEE, 25(3):59?69, May.
M. Roll, J. Frid, and M. Horne. 2007. Measuring syntac-
tic complexity in spontaneous spoken Swedish. Lan-
guage and Speech, 50(2):227.
K. Sagae, A. Lavie, and B. MacWhinney. 2005. Auto-
matic measurement of syntactic development in child
language. In Proc. of ACL, volume 100.
E. Shriberg and A. Stolcke. 2004. Direct modeling of
prosody: An overview of applications in automatic
speech processing. In Proceedings of the International
Conference on Speech Prosody.
E. Shriberg, A. Stolcke, D. Hakkani-Tur, and G. Tur.
2000. Prosody-based automatic segmentation of
speech into sentences and topics. Speech Communi-
cation, 32(1-2):127?154.
K. Zechner, D. Higgins, and Xiaoming Xi. 2007.
SpeechRater: A Construct-Driven Approach to Scor-
ing Spontaneous Non-Native Speech. In Proc. SLaTE.
L. Zhang. 2005. Maximum Entropy Model-
ing Toolkit for Python and C++. http:
//homepages.inf.ed.ac.uk/s0450736/
maxent_toolkit.html.
45
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 73?79,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Utilizing Cumulative Logit Models and Human Computation on Automated
Speech Assessment
Lei Chen
Educational Testing Service (ETS)
Princeton, NJ, 08541
lchen@ets.org
Abstract
We report two new approaches for building
scoring models used by automated speech
scoring systems. First, we introduce the Cu-
mulative Logit Model (CLM), which has been
widely used in modeling categorical outcomes
in statistics. On a large set of responses
to an English proficiency test, we systemati-
cally compare the CLM with two other scor-
ing models that have been widely used, i.e.,
linear regression and decision trees. Our ex-
periments suggest that the CLM has advan-
tages in its scoring performance and its robust-
ness to limited-sized training data. Second, we
propose a novel way to utilize human rating
processes in automated speech scoring. Ap-
plying accurate human ratings on a small set
of responses can improve the whole scoring
system?s performance while meeting cost and
score-reporting time requirements. We find
that the scoring difficulty of each speech re-
sponse, which could be modeled by the degree
to which it challenged human raters, could
provide a way to select an optimal set of re-
sponses for the application of human scor-
ing. In a simulation, we show that focusing
on challenging responses can achieve a larger
scoring performance improvement than sim-
ply applying human scoring on the same num-
ber of randomly selected responses.
1 Introduction
Automated assessment is a process by which com-
puter algorithms are used to score test-taker inputs,
which could be essays, short-text descriptions, read-
aloud sentences, or spontaneous speech responses
to open-end questions. Until recently, human scor-
ing has been predominantly used for scoring these
types of inputs. Several limitations of the human
scoring process have been identified in previous re-
search (Bennett, 2006). First, the human scoring
process is influenced by many hidden factors, such
as human raters? mood and fatigue conditions. In
addition, human raters may not strictly follow the
rubrics designed to guide the scoring process in their
practical scoring sessions. Furthermore, human rat-
ing is also an expensive and slow process, especially
for large-scale tests.
There has been an increasing number of studies
concerning the use of speech processing and natu-
ral language processing (NLP) technologies to auto-
matically score spoken responses (Eskenazi, 2009).
In these machine scoring systems, a set of features
related to multiple aspects of human speaking capa-
bilities, e.g., fluency, pronunciation, intonation, vo-
cabulary usage, grammatical accuracy, and content,
is extracted automatically. Then, statistical mod-
els, such as the widely used linear regression mod-
els, classification and regression trees (CART), are
trained based on human ratings and these features.
For new responses, the trained statistical models are
applied to predict machine scores.
The performance of current automated speech
scoring systems, especially for spontaneous speech
responses, still lags markedly behind the perfor-
mance of human scoring. To improve the perfor-
mance of automated speech scoring, an increas-
ing number of research studies have been under-
taken (Jang, 2009; Chen and Zechner, 2011; Chen
and Yoon, 2011). However, these studies have
mostly focused on exploring additional speech fea-
tures, not on building alternative scoring models.
Hence, in this paper, we will report on two new lines
of research focusing on the scoring model part of au-
73
tomated speech scoring systems. In particular, we
will introduce the Cumulative Logit Model (CLM),
which is not widely used in NLP, and compare it sys-
tematically with other widely-used modeling meth-
ods. In addition, we will propose a hybrid scoring
system inspired by the recent trend of involving hu-
man computation in machine learning tasks (Quinn
et al, 2010), which consists of both human scoring
and machine scoring to achieve a balance of scoring
accuracy, speed, and cost.
The remainder of the paper is organized as fol-
lows: Section 2 reviews the previous research ef-
forts; Section 3 describes both the test from which
our experimental data were collected and the auto-
mated speech scoring system; Section 4 introduces
the Cumulative Logit Model (CLM) and reports a
systematic comparison with two other widely used
modeling approaches; Section 5 proposes using both
human scoring and machine scoring to achieve a
trade-off between scoring accuracy, speed, and cost,
and shows a simulation. Finally, Section 6 con-
cludes the paper and describes our plans for future
research.
2 Related Work
In the language testing field, it is critical how easily a
score can be interpreted by test takers and stakehold-
ers. Therefore, ?white-box? machine learning meth-
ods (mostly from the field of statistics) are favored
over black-box systems (e.g., neural networks) and
widely used in automated scoring systems. For ex-
ample, SRI?s EduSpeak system (Franco et al, 2010)
used a decision-tree model to automatically produce
a speaking score from a set of discrete score la-
bels. Linear Discrimination Analysis (LDA) has
been used in pronunciation evaluation (Hacker et
al., 2005). In a speech scoring system described by
Zechner et al (2009), a linear regression (LR) model
was used to predict human scores.
Applying linear regression, which is designed for
continuous outcomes, on ordinal outcomes, such as
discrete human rated scores, is questioned by some
statisticians.
A linear regression model does not ex-
ploit the fact that the scores can assume
only a limited number of values and hence
may provide inefficient approximations to
essay scores obtained by raters. Conse-
quently, estimation based on a model that
assumes that the response is categorical
will be more accurate than linear regres-
sion. A cumulative logit model, some-
times called a proportional odds model, is
one such model (Haberman and Sinharay,
2010).
The CLM was compared systematically with an
ordinary linear regression model in terms of au-
tomated essay scoring (Haberman and Sinharay,
2010). Based on their experiment on a large variety
of TOEFL prompts, they suggested that the CLM
should be considered a very attractive alternative to
regression analysis.
In recent years, a new trend of research in the ma-
chine learning field is to use human computation to
provide additional help, especially on difficult tasks.
For example, after the ESP game (Von Ahn, 2006),
an increasing number of human computation based
games emerged to use a large number of human par-
ticipants to solve many machine learning problems,
such as human identification for image processing
and sentiment annotation in natural language pro-
cessing (NLP). Quinn and Bederson (2011) review
research in this area. Furthermore, Quinn et al
(2010) proposed a hybrid mechanism to integrate
both human computation and machine learning to
achieve a balance between speed, cost, and quality.
In this paper, we will follow the advances in the
two directions mentioned above, including using
CML as a modeling method and obtaining comple-
mentary computing by integrating machine scoring
with human scoring to further improve the scoring
models in automated speech scoring systems.
3 Data and Automated Scoring System
3.1 Data
AEST is a large-scale English test for assessing test-
takers? English proficiency in reading, writing, lis-
tening, and speaking. The data used in our exper-
iments was collected from operational AEST tests.
In each test session, test takers were required to re-
spond to six speaking test questions to provide in-
formation or express their opinions.
Each spoken response was assigned a score in the
range of 1 to 4, or 0 if the candidate either made no
74
attempt to answer the item or produced a few words
totally unrelated to the topic. Each spoken response
could also receive a ?technical difficulty? (TD) label
when technical issues may have degraded the audio
quality to such degree that a fair evaluation was not
possible. Note that in the experiments reported in
this paper, we excluded both 0 and TD responses
from our analyses. The human scoring process used
the scoring rules designed for the AEST test. From
a large pool of certified human raters, two human
raters were randomly selected to score each response
in parallel. If two raters? scores had a discrepancy
larger than one point, a third rater with more expe-
rience in human scoring was asked to give a final
score. Otherwise, the final scores used were taken
from the first human rater in each rater pair.
The Pearson correlation r among human raters
was calculated as 0.64. The second human scores
had a correlation of 0.63 to the final scores while the
first human scores had a correlation of 0.99. This
is due to the fact that only in about 2% of the cases,
two human scores have a discrepancy larger than one
point. Table 1 describes the data size and final score
distribution of the four score levels.
N 1(%) 2(%) 3(%) 4 (%)
49813 4.56 37.96 47.74 9.74
Table 1: Human score distribution of the AEST datasets
3.2 Automated scoring system
To automatically score spontaneous speech, we used
the method proposed in Chen et al (2009). In this
method, a speech recognizer is used to recognize
non-native speech and a forced alignment is con-
ducted based on the obtained recognition hypothe-
ses. From the recognition and alignment outputs,
a number of features were extracted from multi-
ple aspects, such as the timing profiles, recogni-
tion confidence scores, alignment likelihoods, etc.
For speech recognition and forced alignment, we
used a gender-independent, fully continuous Hid-
den Markov Model (HMM) speech recognizer. Our
ASR system was trained from about 800 hours of
non-native speech data and its corresponding word
transcriptions. We extracted the following two types
of features, including (1) fluency and intonation
features based on the speech recognition output as
described in Xi et al (2008) and (2) pronuncia-
tion features that indicated the quality of phonemes
and phoneme durations as described in Chen et al
(2009).
4 A comparison of three machine learning
methods in automated speech scoring
We will briefly introduce CLM and then compare
it with two other widely used scoring methods, i.e.,
linear regression and CART. In most of the related
previous investigations, several machine learning al-
gorithms were compared using a fixed number of in-
stances. However, as shown in recent studies, such
as Rozovskaya and Roth (2011), judging an algo-
rithm requires consideration of the impact of the size
of the training data set. Therefore, in our exper-
iment, we compared three algorithms on different
sizes of training samples.
Let the response?s holistic score be Y = 1, 2, ...J
(J is 4 in our study on the AEST data) and let the
associated probabilities be pi1, pi2, ...piJ . Therefore
the probability of a predicted score is not larger than
j
P (Y ? j) = pi1 + pi2 + ...+ pij (1)
The logit of this probability can be estimated as
log
P (Y ? j)
1? P (Y ? j)
= ?j +
K?
k=1
?kXk (2)
where K is the number of speech features. We can
see that a CLM contains K ?s where each ? is asso-
ciated with one feature. In addition, for each score j,
there is an intercept ?j . The CLM is a special case
of multinomial logistic regression, which is named
Maximum Entropy (MaxEnt) model (Berger et al,
1996) and is well known by NLP researchers. In
CLM, the ranking order of the labels being predicted
is emphasized. However, in MaxEnt models, there
is no assumption about the relationship of the labels
being predicted.
For CLM, we used the Ye?s VGAM R pack-
age (Yee, 2010) as our implementation. For or-
dinary linear regression and CART methods, we
used corresponding implementations in the WEKA
toolkit (Hall et al, 2009), i.e., lm and J48 tree,
through the RWeka package (Hornik et al, 2009)
so that we could run these three algorithms inside R.
75
From the available speech features, we first
run an inter-correlation analysis among these fea-
tures. Then, two feature selection approaches imple-
mented in the caret R package (Kuhn, 2008) were
used to select useful features from about 80 fea-
tures. First, all feature-pairs whose inter-correlation
was higher than 0.80 were analyzed and one feature
for each pair was removed. Next, a recursive fea-
ture elimination (RFE) based on a linear regression
model was utilized to reduce the feature size to just
20.
Using a stratified sampling based on the final
scores, the whole data set was split into a training set
(with 44, 830 instances) and a test set (with 4, 980
instances). Then, on a log10 scale, we tried using
increasing number of training samples from 100 to
104.5. For each training data set size, we randomly
selected the size of training samples from the train-
ing set, built the three models, and evaluated the
models on the entire test data. For each data set size,
such process was repeated 10 times. The evaluation
result is the averaged values from these 10 iterations.
We repeated the same experiment on the top 5, 10,
15, and 20 features. The evaluation metrics include
widely used measures in the field of automated scor-
ing, including Pearson correlation r and quadratic
weighted Kappa ? (hereafter weighted ?) between
the machine predicted scores and human final scores
in this data set.
Figure 1 shows the Pearson r and weighted ? val-
ues of the three methods vs. an increasing numbers
of training samples. We find that the CLM always
has the highest weighted ? value among these three
methods for each data size level. The CART per-
forms poorly, especially facing a limited number of
training samples. However, when the training data
size is large enough, the performance gap between
the CART and other regression models becomes
smaller. For two regression models, when work-
ing on 20 features, both Pearson r and weighted ?
values plateaued after reaching 1000 training sam-
ples. More importantly, we find that the CLM still
can provide a quite high value of weighted ? even
just using 100 training samples. This is very impor-
tant for automated assessments in cases where there
are not enough pre-test responses to fully train the
scoring model. When using other feature selections
(5, 10, and 15), we also observed the same trend as
shown in the Figure 1.
log10(dSize)
corr
0.38
0.40
0.42
0.44
0.46
0.48
0.50
l
l
l l l l
2.5 3.0 3.5 4.0 4.5
MLl CLM
J48
MR
log10(dSize)
kappa
0.35
0.40
0.45
l
l
l l l l
2.5 3.0 3.5 4.0 4.5
MLl CLM
J48
MR
Figure 1: Weighted ? and Pearson correlation r of LR,
CART, and CLM vs. an increasing number of training
samples when using 20 features.
5 Utilizing human computation to support
automated speech scoring
On spontaneous speech responses, the performance
of automated scoring still lags behind human rat-
ings. For example, on the test set (4, 098 samples),
among human raters both the Pearson r and the
weighted ? values are about 0.6, much higher than
the best automated scoring results we saw in the pre-
vious section (around 0.5). There are many possi-
ble reasons for such a big performance gap between
automated speech scoring and human scoring. For
example, the automated features? lack of a measure-
ment of content accuracy and relevance might pro-
vide an explanation for part of the performance gap.
As a result, to our knowledge, there has not been any
commercial application of automated speech scoring
on high-stakes speaking tests to open-ended ques-
tions.
To further improve the speech scoring system?s
performance, inspired by Quinn et al (2010), we
76
propose to include human computation ? human
rating of speech responses ? in the automated
speech scoring system. Previously, there have been
some efforts to use human computation in auto-
mated speech scoring systems. For example, it is
well known that human scores were used to train au-
tomated scoring models. For essay scoring, an auto-
mated scoring system, e-rater, has been used to val-
idate the human rating process (Enright and Quin-
lan, 2010). One advantage of using both human and
e-rater to score is that about 10% of human rating
requests for double-scoring required in operational
essay scoring could be saved. However, there has
been no previous work investigating the joint use of
human scoring and machine scoring. By using these
two scoring methods together, we hope to achieve a
balance among scoring accuracy, speed, and cost.
From a total of N test responses, we need ask
humans to score m, where m << N . Therefore,
an important question concerning the joint use of
human scoring and machine scoring is how to find
these m responses so that the expensive and slow
human scoring process can provide a large perfor-
mance gain. In this paper, we will report our prelim-
inary research results of focusing on the responses
challenging to machine scoring process.
Since the responses used in this paper were se-
lected to be double-scored responses from a very
large pool of AEST responses, we use the rating
condition of each doubly-scored response to pre-
dict how challenging any given response is. For
speech responses for which two human raters gave
different holistic scores, we assumed that these re-
sponses were not only difficult to score for human
beings, but also for the machine learning method,
which has been trained from human scores in a su-
pervised learning way. We call the responses on
which two human raters agreed easy-case responses
and the responses on which two human raters dis-
agreed hard-case ones. Table 2 reports on the appli-
cation of trained automated speech assessment sys-
tems to these two types of responses. From the en-
tire testing set, human raters agreed on 3, 128 re-
sponses, but disagreed on 1, 852 responses. From
the training set described in the previous section,
we randomly sampled 1, 000 responses to train a
CLM model using those 20 features used in Sec-
tion 4. Then, the trained CLM model was evalu-
ated on these two types of responses, respectively.
Table 2 reports the evaluation metrics averaged on
20 trials of using different training set portions. We
can clearly see that the machine scoring has a sig-
nificantly better performance on the easy-case re-
sponses than the hard-case responses. Therefore, it
is natural to focus expensive/slow human computa-
tion efforts on these hard-case responses.
metric easy-case hard-case
agreement(%) 68.16 48.08
r 0.594 0.377
weighted ? 0.582 0.355
Table 2: Evaluation of automated speech assessment sys-
tems on two types of speech responses. For the responses
on which two human raters agreed, the machine has a sta-
tistically significantly better performance.
Suppose that we can obtain the type of each re-
sponse, hard-case vs. easy-case, in some way, we
then can focus our human scoring efforts on hard-
case responses only since machine scoring performs
much worse on them. Figure 2 depicts the re-
sults of one trial of using human scoring to replace
an increasing number of machine scores. Among
4, 980 responses in the test set, the blue curve shows
the weighted ? values after replacing an increasing
number of machine scores with human scores. Here,
we used the scores provided by the second rater from
each rater pair. This set of human scores had a Pear-
son r of 0.626 with the final scores. We also re-
placed the same number of responses, but without
distinguishing easy- and hard-case responses by the
corresponding human scores. The results are shown
in the red curve. We can observe that the weighted
? values increased from about 0.50, which was ob-
tained by using only machine scoring, to about 0.58
by asking humans to score all hard-case responses,
about 33% of all responses. Among the two meth-
ods to select the responses for using human scoring,
we can clearly see that the strategy of focusing on
hard-case responses can achieve higher weighted ?
when spending the same amount of human efforts as
the strategy of randomly selecting responses.
6 Discussions
In this paper, we reported on two experiments for
improving the scoring model in automated sponta-
77
# items scored by human
kappa
0.52
0.53
0.54
0.55
0.56
0.57
l
l
l l
l
l
l l
l
l
500 1000 1500
methodl hard
random
Figure 2: Weighted ? values when using human rating
results to replace machine-predicted scores on hard-case
responses or a similar number of responses that are ran-
domly selected.
neous speech assessment. In the first experiment, we
systematically compared a new modeling method,
Cumulative Logit Model (CLM), which has been
widely used in statistics, with other two widely used
modeling methods, linear regression and CART.
We compared these three modeling methods on
a large test data set (containing 4, 980 responses)
and evaluated these methods on a series of train-
ing data sizes. The experimental results suggest
that the CLM model consistently achieves the best
performance (measured in Pearson r and quadratic
weighted ? between the predicted scores and human
rated scores). More importantly, we find that the
CLM can work quite well even when just using hun-
dreds of responses in the training stage. This finding
is especially important for building scoring models
when pre-test data is limited.
Although automated scoring has been designed to
overcome several disadvantages of the human rating
process, our experiments are meant to initiate sci-
entific debate on how best to combine the strengths
of human and automated scoringto achieve an opti-
mal compromise of scoring accuracy, cost, and time.
At least for current automated scoring systems for
spontaneous speech, the machine performance lags
behind the reliability of the human rating process.
We also found that the automated system performed
worse on hard-case responses on which even two hu-
man raters did not agree. In a simulation study, we
showed that jointly using human scoring and ma-
chine scoring can further improve the scoring per-
formance obtained by just using automated speech
scoring. By focusing human scoring, which is ex-
pensive, slow, but more accurate, on a set of re-
sponses specially selected from the entire set of re-
sponses, we can achieve larger gains of scoring per-
formance than randomly assigning the same amount
of responses for human scoring. Therefore, from an
engineering point of view of building more accurate
scoring systems, it is promising to design a hybrid
system consisting of both human scoring and ma-
chine scoring.
For future research, given the automated speech
scoring system?s large performance variation on two
types of responses, it is worthwhile finding a reli-
able way to automatically predict a responses? con-
dition, i.e., whether it is hard or easy to score for
humans or for machines. We need to consider both
proficiency features we used in this paper and other
features measuring audio quality. Finding such in-
formation can help us decide when to use machine
scoring and when to rely on human raters. In addi-
tion, other applications of human computation, such
as asking humans to adjust machine predicted scores
or using human rated scores accumulated in scoring
operations to routinely update the machine scoring
system will be explored.
References
R.E. Bennett. 2006. Moving the field forward: Some
thoughts on validity and automated scoring. Auto-
mated scoring of complex tasks in computer-based
testing, pages 403?412.
A. Berger, S. Pietra, and V. Pietra. 1996. A maximum en-
tropy approach to natural language processing. Com-
putational Linguistics, 22:39?72.
L. Chen and S. Yoon. 2011. Detecting structural event
for assessing non-native speech. In 6th Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, page 74.
78
Miao Chen and Klaus Zechner. 2011. Computing and
evaluating syntactic complexity features for automated
scoring of spontaneous non-native speech. In ACL?11,
pages 722?731.
L. Chen, K. Zechner, and X Xi. 2009. Improved pro-
nunciation features for construct-driven assessment of
non-native spontaneous speech. In NAACL-HLT.
M.K. Enright and T. Quinlan. 2010. Complement-
ing human judgment of essays written by english lan-
guage learners with e-rater scoring. Language Testing,
27(3):317?334.
M. Eskenazi. 2009. An overview of spoken language
technology for education. Speech Communication,
51(10):832?844.
H. Franco, H. Bratt, R. Rossier, V. Rao Gadde,
E. Shriberg, V. Abrash, and K. Precoda. 2010. EduS-
peak: a speech recognition and pronunciation scoring
toolkit for computer-aided language learning applica-
tions. Language Testing, 27(3):401.
S.J. Haberman and S. Sinharay. 2010. The application of
the cumulative logistic regression model to automated
essay scoring. Journal of Educational and Behavioral
Statistics, 35(5):586.
C. Hacker, T. Cincarek, R. Grubn, S. Steidl, E. Noth, and
H. Niemann. 2005. Pronunciation Feature Extraction.
In Proceedings of DAGM 2005.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H Witten. 2009. The WEKA data min-
ing software: An update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
K. Hornik, C. Buchta, and A. Zeileis. 2009. Open-
source machine learning: R meets weka. Computa-
tional Statistics, 24(2):225?232.
T. Y Jang. 2009. Automatic assessment of non-native
prosody using rhythm metrics: Focusing on korean
speakers? english pronunciation. In Proc. of the 2nd
International Conference on East Asian Linguistics.
M. Kuhn. 2008. Building predictive models in r us-
ing the caret package. Journal of Statistical Software,
28(5):1?26.
A.J. Quinn and B.B. Bederson. 2011. Human compu-
tation: a survey and taxonomy of a growing field. In
Proceedings of the 2011 annual conference on Human
factors in computing systems, page 14031412.
A.J. Quinn, B.B. Bederson, T. Yeh, and J. Lin. 2010.
CrowdFlow: integrating machine learning with me-
chanical turk for speed-cost-quality flexibility. Better
performance over iterations.
A. Rozovskaya and D. Roth. 2011. Algorithm selection
and model adaptation for ESL correction tasks. Ur-
bana, 51:61801.
L. Von Ahn. 2006. Games with a purpose. Computer,
39(6):92?94.
X. Xi, D. Higgins, K. Zechner, and D. Williamson.
2008. Automated Scoring of Spontaneous Speech Us-
ing SpeechRater v1.0. Technical report, Educational
Testing Service.
Thomas W. Yee. 2010. The VGAM package for categor-
ical data analysis. J. Statist. Soft., 32(10):1?34.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken en-
glish. Speech Communication, 51:883?895, October.
79
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 122?126,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Scoring Spoken Responses Based on Content Accuracy
Fei Huang
CS Dept. Temple Univ.
Philadelphia, PA, 19122
tub58431@temple.edu
Lei Chen
Educational Testing Service (ETS)
Princeton, NJ, 08541
lchen@ets.org
Jana Sukkarieh
ETS
JSukkarieh@ets.org
Abstract
Accuracy of content have not been fully uti-
lized in the previous studies on automated
speaking assessment. Compared to writing
tests, responses in speaking tests are noisy
(due to recognition errors), full of incomplete
sentences, and short. To handle these chal-
lenges for doing content-scoring in speaking
tests, we propose two new methods based
on information extraction (IE) and machine
learning. Compared to using an ordinary
content-scoring method based on vector anal-
ysis, which is widely used for scoring written
essays, our proposed methods provided con-
tent features with higher correlations to human
holistic scores.
1 Introduction
In recent years, there is an increasing interest of
using speech processing and natural language pro-
cessing (NLP) technologies to automatically score
speaking tests (Eskenazi, 2009). A set of features
related to speech delivery, such as fluency, pronun-
ciation, and intonation, has been utilized in these
studies. However, accuracy of an answer?s content
to the question being asked, important factors to be
considered during the scoring process, have not been
fully utilized. In this paper, we will report our ini-
tial efforts exploring content scoring in an automated
speaking assessment task. To start, we will briefly
describe the speaking test questions in our research.
In the test we used for evaluation, there were
two types of questions. The first type, survey,
requires a test-taker to provide answers specific
to one or several key points in a survey ques-
tion without any background reading/listening re-
lated to the topic of the survey. Typical questions
could be ?how frequently do you go shopping?? or
?what kind of products did you purchase recently??
In contrast, the second type, opinion, requires a test-
taker to speak as long as 60 seconds to present his
or her opinions about some topic. An example of
such questions could be, ?Do you agree with the
statement that online shopping will be dominant in
future or not?? Compared to the essays in writing
tests, these spoken responses could just be incom-
plete sentences. For example, for the survey ques-
tions, test-takers could just say several words. For
the questions described above, some test-takers may
just use phrases like ?once a week? or ?books?. In
addition, given short responding durations, the num-
ber of words in test-takers? responses is limited. Fur-
thermore, since scoring speech responses requires
speech recognition, more noisy inputs are expected.
To tackle these challenges, we propose two novel
content scoring methods in this paper.
The remainder of the paper is organized as fol-
lows: Section 2 reviews the related previous re-
search efforts; Section 3 proposes the two content-
scoring methods we designed for two types of ques-
tions described above; Section 4 reports the experi-
mental results of applying the proposed methods; fi-
nally, Section 5 concludes our reported research and
describes our plans for future research.
2 Related Work
For writing tests, previous content scoring investiga-
tions can be divided into the following three groups.
The first group relies on obtaining and matching pat-
terns associated with the correct answers (Leacock
and Chodorow, 2003; Sukkarieh and Blackmore,
2009).
The second group of methods, also mostly used
122
for content-scoring, is to rely on a variety of text
similarity measurements to compare a response with
either pre-defined correct answers or a group of re-
sponses rated with a high score (Mohler and Mihal-
cea, 2009). Compared to the first group, such meth-
ods can bypass a labor intensive pattern-building
step. A widely used approach to measuring text
similarity between two text strings is to convert
each text string into a word vector and then use
the angle between these two vectors as a similar-
ity metric. For example, Content Vector Analy-
sis (CVA) has been successfully utilized to detect
off-topic essays (Higgins et al, 2006) and to pro-
vide content-related features for essay scoring (At-
tali and Burstein, 2004). For this group of meth-
ods, measuring the semantics similarity between two
terms is a key question. A number of metrics have
been proposed, including metrics (Courley and Mi-
halcea, 2005) derived from WordNet, a semantics
knowledge database (Fellbaum, 1998), and metrics
related to terms? co-occurrence in corpora or on the
Web (Turney, 2001).
The third group of methods treats content scor-
ing as a Text Categorization (TC) task, which treats
the responses being scored on different score levels
as different categories. Therefore, a large amount
of previous TC research, such as the many machine
learning approaches proposed for the TC task, can
be utilized. For example, Furnkranz et al (1998)
compared the performance of applying two machine
learning methods on a web-page categorization task
and found that the Repeated Incremental Pruning to
Produce Error Reduction algorithm (RIPPER) (Co-
hen, 1995) shows an advantage concerning the fea-
ture sparsity issue.
3 Methodology
As described in Section 1, for the two types of ques-
tions considered, the number of words appearing
in a response is quite limited given the short re-
sponse time. Therefore, compared to written es-
says, when applying the content-scoring methods
based on vector analysis, e.g., CVA, feature sparsity
becomes a major factor negatively influencing the
performance of these methods. Furthermore, there
are more challenges when applying vector analysis
on survey questions because test-takers could just
use words/phrases rather than completed sentences.
Also, some survey questions could have a very large
range of correct answers. For example, if a question
is about the name of a book, millions of book ti-
tles could be potential answers. Therefore, a simple
phrase-matching solution cannot work.
3.1 Semi-Automatic Information Extraction
For survey responses, the answers should be related
to the key points mentioned in the questions. For
example, for the question, ?What kind of TV pro-
grams do you like to watch??, possible correct an-
swers should be related to TV programs. Moreover,
it should be the instances of specific TV programs,
like news, comedy, talk shows, etc. Note that the ac-
ceptable answers may be infinite, so it is not realis-
tic to enumerate all possible answers. Therefore, we
proposed a method to extract the potential answer
candidates and then measure their semantic similar-
ities to the answer keys that could be determined
manually. In particular, the answer keys were deter-
mined by the first author based on her analysis of the
test prompts. For example, for the question ?What
kind of books do you like to read??, two answer keys,
?book? and ?reading? were selected. After a fur-
ther analysis of the questions, we found that most of
the survey questions are about ?when? ?where? and
?what?, and the answers in the responses were usu-
ally nouns or noun phrases. Therefore, we decided
to extract the noun phrases from each response and
use them as potential candidates.
We use two semantic similarity metrics (SSMs)
to evaluate how each candidate relates to an answer
key, including PMI-IR (Turney, 2001) and a word-
to-word similarity metric from WordNet (Courley
and Mihalcea, 2005). The PMI-IR is a measure
based on web query analysis using Pointwise Mutual
Information (PMI) and Information Retrieval (IR).
For an answer candidate (c) and an answer key (k),
their PMI-IR is computed as:
SSMPMI-IR(c, k) =
hits(cNEARk)
hits(c)
where the hits(x) function obtains the count of term
x returned by a web search engine and NEAR is a
query operator for proximity search, searching the
pages on which both k and c appear within a spec-
ified distance. Among many WordNet (WN) based
SSMs summarized in Courley and Mihalcea (2005),
123
we found that the Wu-Palmer metric proposed by
Wu and Palmer (1994) worked the best in our pilot
study. This metric is a score denoting how similar
two word senses are, based on the depth of the two
word senses in the taxonomy and their Least Com-
mon Subsumer 1 (LCS):
SSMWN(c, k) =
2 ? depth(LCS)
depth(c) + depth(k)
For each answer key, we calculated two sets of
SSMs (SSMPMI-IR and SSMWN , respectively)
from all candidates. Then, we selected the largest
SSMPMI-IR and SSMWN as the final SSMs for this
particular answer key. For each test question, using
the corresponding responses in the training set, we
built a linear regression model between these SSMs
for all answer keys and the human judged scores.
The learned regression model was applied to the re-
sponses to this particular testing question in the test-
ing set to convert a set of SSMs to predictions of
human scores. The predicted scores were then used
as a content feature. Since answer keys were deter-
mined manually, we refer to this method as semi-
automatic information extraction (Semi-IE).
3.2 Machine Learning Using Smoothed Inputs
For the opinion responses, inspired by Furnkranz
et al (1998), we decided to try sophisticated ma-
chine learning methods instead of the simple vector-
distance computation used in CVA. Due to short
response-time in the speaking test being considered,
the ordinary vector analysis may face a problem that
the obtained vectors are too short to be reliably used.
In addition, using other non-CVA machine learning
methods can enable us to try other types of linguis-
tic features. To address the feature sparsity issue, a
smoothing method, which converts word-based text
features into features based on other entities with
a much smaller vocabulary size, is used. We use
a Hidden Markov Model (HMM) based smooth-
ing method (Huang and Yates, 2009), which in-
duces classes, corresponding to hidden states in the
HMM model, from the observed word strings. This
smoothing method can use contextual information
of the word sequences due to the nature of HMM.
Then, we convert word-entity vectors to the vec-
tors based on the induced classes. TF-IDF (term
1Most specific ancestor node
frequency and inverse document frequency) weight-
ing is applied on the new class vectors. Finally,
the processed class vectors are used as input fea-
tures (smoothed) to a machine learning method. In
this research, after comparing several widely used
machine learning approaches, such as Naive Bayes,
CART, etc., we decided to use RIPPER proposed by
Cohen (1995), a rule induction method, similar to
Furnkranz et al (1998).
4 Experiments
Our experimental data was from a test for interna-
tional workplace English. Six testing papers were
used in our study and each individual test contains
three survey questions (1, 2, and 3) and two opin-
ion questions (4 and 5). Table 1 lists examples
for these question types. From the real test, we
collected spoken responses from a total of 1, 838
test-takers. 1, 470 test-takers were used for training
and 368 were used for testing. Following scoring
rubrics developed for this test by considering speak-
ers? various language skill aspects, such as fluency,
pronunciation, vocabulary, as well as content accu-
racy, the survey and opinion responses were scored
by a group of experienced human raters by using a
3-point scale and a 5-point scale respectively. For
the survey responses, the human judged scores were
centered on 2; for the opinion responses, the human
judged scores were centered on 3 and 4.
Qs. Example
1 How frequently do you go shopping?
2 What kinds of products do you buy often?
3 How should retailers improve their services?
4 Make a purchase decision based on the chart
provided and justify your decision.
5 Do you agree with the statement that online
shopping will be dominant in the future or
not? Please justify your point.
Table 1: Examples of the five kinds of questions investi-
gated in the study
All of these non-native speech responses were
manually transcribed. A state-of-the-art HMM Au-
tomatic Speech Recognition (ASR) system which
was trained from a large set of non-native speech
data was used. For each type of test question, acous-
tic and language model adaptations were applied
to further lower the recognition error rate. Finally,
124
a word error rate around 30% to 40% could be
achieved on the held-out speech data. In our exper-
iments, we used speech transcriptions in the model
training stage and used ASR outputs in the testing
stage. Note that we decided to use speech transcrip-
tions, instead of noisy ASR outputs that match to
the testing condition, to make sure that the learned
content-scoring model are based on correct word en-
tities related to content accuracy.
For the survey responses, we manually selected
the key points from the testing questions. Then,
using a Part-Of-Speech (POS) tagger and a sen-
tence chunker implemented by using the OpenNLP 2
toolkit, we found all possible nouns and noun-
phrases that could serve as answer candidates and
applied the Semi-IE method described in Sec-
tion 3.1. For opinion questions, based on Huang and
Yates (2009), we used 80 hidden states and applied
the method described in Section 3.2 for content scor-
ing. We used JRip, a Java implementation of the
RIPPER (Cohen, 1995) algorithm in the Weka (Hall
et al, 2009) machine learning toolkit, in our experi-
ments.
When measuring performance of content-related
features, following many automated assessment
studies (Attali and Burstein, 2004; Leacock and
Chodorow, 2003; Sukkarieh and Blackmore, 2009),
we used the Pearson correlation r between the con-
tent features and human scores as an evaluation met-
ric. We compared the proposed methods with a base-
line method, CVA. It works as follows: it first groups
all the training responses by scores, then it calculates
a TF vector from all the responses under a score
level. Also, an IDF matrix is generated from all
the training responses. After that, for each testing
response, CVA first converts it into a TF-IDF vec-
tor and then calculates the cosine similarity between
this vector with each score-level vector respectively
and uses the largest cosine similarity as the content
feature for that response. The experimental results,
including content-features? correlations r to human
scores from each proposed method and the correla-
tion increases measured on CVA results, are shown
in Table 2. First, we find that CVA, which is de-
signed for scoring lengthy written essays, does not
work well for the survey questions, especially on
2http://opennlp.sourceforge.net
Question rCV A rSemi?IE r ?
1 0.12 0.30 150%
2 0.15 0.27 80%
3 0.21 0.26 23.8%
Question rCV A rRipperHMM r ?
4 0.47 0.54 14.89%
5 0.33 0.39 18.18%
Table 2: Comparisons of the proposed content-scoring
methods with CVA on survey and opinion responses
first two questions, which are mostly phrases (not
completed sentences). By contrast, our proposed
Semi-IE method can provide more informative con-
tent measurements, indicated by substantially in-
creased r. Second, CVA works better on opinion
questions than on survey questions. This is because
that opinion questions can be treated as short spo-
ken essays and therefore are closer to the data on
which the CVA method was originally designed to
work. However, even on such a well-performing
CVA baseline, the HMM smoothing method allows
the Ripper algorithm to outperform the CVA method
in content-features? correlations to human scores.
For example, on question 4, on which either a table
or a chart has been provided to test-takers, the CVA
achieves a r of 0.47. The proposed method can still
improve the r by about 15%.
5 Conclusions and Future Works
In this paper, we proposed two content-scoring
methods for the two types of test questions in an
automated speaking assessment task. For particu-
lar properties of these two question types, we uti-
lized information extraction (IE) and machine learn-
ing technologies to better score them on content
accuracy. In our experiments, we compared these
two methods, Semi-IE and machine learning us-
ing smoothed inputs, with an ordinary word-based
vector analysis method, CVA. The content features
computed using the proposed methods show higher
correlations to human scores than what was obtained
by using the CVA method.
For the Semi-IE method, one direction of investi-
gation will be how to find the expected answer keys
automatically from testing questions. In addition,
we will investigate better ways to integrate many se-
125
mantic similarly measurements (SSMs) into a single
content feature. For the machine learning approach,
inspired by Furnkranz et al (1998), we will inves-
tigate how to use some linguistic features related to
response structures rather than just TF-IDF weights.
References
Y. Attali and J. Burstein. 2004. Automated essay scoring
with e-rater v.2.0. In Presented at the Annual Meet-
ing of the International Association for Educational
Assessment.
W. Cohen. 1995. Text categorization and relational
learning. In In Proceedings of the 12th International
Conference on Machine Learning.
C. Courley and R. Mihalcea. 2005. Measuring the se-
mantic similarity of texts. In Proceedings of the ACL
Workshop on Empirical Modeling of Semantic Equiv-
alence and Entailment, pages 13?18.
M. Eskenazi. 2009. An overview of spoken language
technology for education. Speech Communication,
51(10):832?844.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. Bradford Books.
J. Furnkranz, T. Mitchell, and E. Riloff. 1998. A case
study in using linguistic phrases for text categorization
on the WWW. In Proceedings from the AAAI/ICML
Workshop on Learning for Text Categorization, page
512.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H Witten. 2009. The WEKA data min-
ing software: An update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
D. Higgins, J. Burstein, and Y. Attali. 2006. Identifying
off-topic student essays without topic-specific training
data. Natural Language Engineering, 12.
F. Huang and A. Yates. 2009. Distributional represen-
tations for handling sparsity in supervised sequence-
labeling. In Proceedings of ACL.
C. Leacock and M. Chodorow. 2003. C-rater: Auto-
mated scoring of short-answer questions. Computers
and the Humanities, 37(4):385?405.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 567?575.
J. Z. Sukkarieh and J. Blackmore. 2009. c-rater: Auto-
matic content scoring for short constructed responses.
In Paper presented at the Florida Artificial Intelli-
gence Research Society (FLAIRS) Conference, Sani-
bel, FL.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Procs. of the
Twelfth European Conference on Machine Learning
(ECML), pages 491?502, Freiburg, Germany.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexi-
cal selection. In Proceeding ACL ?94 Proceedings of
the 32nd annual meeting on Association for Computa-
tional Linguistics.
126
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 58?62,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Applying Unsupervised Learning To Support Vector Space Model Based
Speaking Assessment
Lei Chen
Educational Testing Service
600 Rosedale Rd
Princeton, NJ
LChen@ets.org
Abstract
Vector Space Models (VSM) have been
widely used in the language assessment field
to provide measurements of students? vocab-
ulary choices and content relevancy. How-
ever, training reference vectors (RV) in a VSM
requires a time-consuming and costly human
scoring process. To address this limitation, we
applied unsupervised learning methods to re-
duce or even eliminate the human scoring step
required for training RVs. Our experiments
conducted on data from a non-native English
speaking test suggest that the unsupervised
topic clustering is better at selecting responses
to train RVs than random selection. In addi-
tion, we conducted an experiment to totally
eliminate the need of human scoring. Instead
of using human rated scores to train RVs, we
used used the machine-predicted scores from
an automated speaking assessment system for
training RVs. We obtained VSM-derived fea-
tures that show promisingly high correlations
to human-holistic scores, indicating that the
costly human scoring process can be elimi-
nated.
Index Terms: Vector Space Model (VSM), speech
assessment, unsupervised learning, document clus-
tering
1 Introduction
A Vector Space Model (VSM) is a simple, yet effec-
tive, method to measure similarities between doc-
uments or utterances, which has been utilized in
the educational testing field. For example, VSM
has been applied to detect students? off-topic es-
says (Higgins et al, 2006) and to automatically
score essays (Attali and Burstein, 2004).
The following three steps are required to use
VSM for automated assessment: (1) a collection
of responses are selected from each score category
to construct reference vectors (RV); (2) for an in-
put response under scoring, the same vectorization
method used for constructing RVs is applied to com-
pute an input vector (IV); (3) similarities between
this IV and the RVs for all score categories are com-
puted as features reflecting vocabulary usage and
content relevancy, including a widely used feature,
the cosine similarity between the IV and the RV for
the highest score category.
Clearly, the quality of VSM-derived features de-
pends on the proper training of RVs. In language
assessment, we tend to use a large number of man-
ually scored responses to build RVs for each testing
question (called item in the assessment field). How-
ever, this raises an issue: the requirement of manual
scoring of these responses by human raters. Also,
for large-scale assessments administrated globally,
a high number of items are typically administered
to both ensure the assessment security and support
the large volume of test-takers. To address this chal-
lenge of application of VSM, we will describe our
solutions based on applying unsupervised learning
methods in this paper.
The rest of the paper is organized as follows: Sec-
tion 2 reviews the related previous research; Sec-
tion 3 describes the English assessment, the data
used in our experiments, and the Automatic Speech
Recognition (ASR) system used; Section 4 reports
58
the three experiments we conducted; and Section 5
discusses our findings and plans for future research.
2 Previous Work
Attali and Burstein (2004) used the VSM method
to measure non-native English writers? vocabulary
choices when scoring their essays by comparing
the words contained in an student?s response to the
words found in a sample of essays from each score
category. One belief behind this methodology is that
good essays will resemble each other in terms of the
word choice. In particular, two VSM-derived fea-
tures were used, including the maximum cosine sim-
ilarity and cosine similarity to the top score category.
Higgins et al (2006) applied the VSM technology to
detect students? off-topic essays whereby the word-
based IV from a student?s essay was compared to an
RV built from a collection of on-topic essays. When
the difference was larger than a pre-defined thresh-
old, the essay was marked as off-topic. Zechner and
Xi (2008) applied VSM as a content relevancy mea-
surement to score non-native English speaking re-
sponses. Recently, Xie et al (2012) explored the
VSM technology on automated speech scoring. Us-
ing a superior ASR to the one used in (Zechner and
Xi, 2008), they found that the VSM-derived features
had moderately high correlations with human profi-
ciency scores.
Dimension reduction, a critical step in apply-
ing VSM, removes the noises and minor details in
word-based vectors and keeps a concise semantic
structure. Latent Semantic Analysis (LSA) (Deer-
wester et al, 1990) and Latent Dirichlet Alloca-
tion (LDA) (Blei et al, 2003) are two widely used
dimension-reduction methods. Kakkonen et al
(2005) systematically investigated the dimension re-
duction methods used in the VSM methods for es-
say grading. Their experiments showed that LSA
slightly out-performs LDA.
Compared to supervised learning, unsupervised
learning can skip the time-consuming and costly
manual labeling process and has been widely used
in many machine-learning tasks. Both LSA and
LDA have been utilized in unsupervised document
clustering (Hofmann, 2001) to automatically sep-
arate a collection of documents into several sets
without any human intervention. Co-training is a
type of semi-supervised learning method (Blum and
Mitchell, 1998), consisting of two classifiers trained
from independent sets of features to predict the same
labels. It uses automatically predicted labels from
one classifier to train the other classifier.
3 Data
The data used in our experiments were collected
from the speaking section of Test Of English as a
Foreign Language (TOEFL R?), an English speak-
ing test used to evaluate students? basic English-
speaking skills for use in academic institutions that
use English as their primary teaching language. Our
data contains the speech responses for a total of 24
test items. For each item, both the stimulus mate-
rial and question were presented to test-takers fol-
lowed by a short amount of preparation time. The
test-takers were then given up to 60 seconds to pro-
vide their spoken responses. These responses were
scored by using carefully developed rating rubrics
by a group of experienced human raters. The scor-
ing rubrics covered a comprehensive list of differ-
ent aspects of speaking ability, such as pronuncia-
tion, prosody, vocabulary, content organization, etc.
A 4-point holistic scoring scale was used where the
score of 4 marks the most advanced English speak-
ers in the TOEFL R? test. Table 1 summarizes the re-
sponses across these 24 items, including mean, sd,
and sample size (n) of the total number of responses
and the number of responses per each score level.
Overall SC1 SC2 SC3 SC4
mean 1969.63 81.88 701.96 963.46 222.33
sd 12.92 30.02 62.36 67.24 37.79
n 47271 1965 16847 23123 5336
Table 1: Summary statistics of the number of total re-
sponses and the number of responses per each score level
measured in mean, sd, and sample size n across 24 items
The transcriptions of these spoken responses were
obtained by running a state-of-the-art non-native
ASR system. This ASR system uses a cross-word
tri-phone acoustic model (AM) and n-gram lan-
guage models (LMs) that were trained on approx-
imately 800 hours of spoken data and the corre-
sponding transcriptions. When being evaluated on
an held-out data set transcribed by humans from the
same test, a 33.0% word error rate was obtained.
59
4 Experiments
The three experiments described below shared the
same procedure: (1) for each item, available re-
sponses were divided into two sets - a set for train-
ing RVs and a set for evaluating the VSM-derived
features; (2) RVs were trained by using different re-
sponse selection methods investigated in this paper;
(3) the trained RVs were used to compute the VSM-
derived features; and (4) Pearson correlation coeffi-
cients (rs) between the VSM-derived features and
human-holistic scores were computed to measure
these features? predictive abilities in speech scoring.
This experimental procedure was conducted on all
24 items and was repeated in 10 iterations by using
varied training/evaluation-splitting plans and the av-
erages of these results across the items and iterations
are reported. Note that we removed some common
function words, such as a, the, etc., and some noise
words from ASR outputs, such as uh and um, when
applying the VSM method and always used LSA di-
mension reduction. We used the Gensim (R?ehu?r?ek
and Sojka, 2010) Python package to implement the
VSM-related computations in this paper. Also, in
this paper, we focused on one VSM-derived fea-
ture cos4, the cosine distance between an IV to the
RV representing the highest-score category (4) for
TOEFL R? test.
4.1 Data size for training RVs
In previous studies, researchers typically used a
large number of responses to construct RVs. For ex-
ample, Zechner and Xi (2008) used 1, 000 responses
while Xie et al (2012) increased the RV training
data to 2, 000 responses for each item. We ask, is
it possible to use fewer responses so that we would
not be forced to manually score so many responses?
To answer this question, we have investigated the re-
lationship between the size of the RV training data
and cos4?s predictive ability.
For each item, we first randomly selected 1, 800
responses as the RV training data and used the re-
maining responses as the evaluation set. We then
gradually reduced the RV training set to 1, 000, 500,
200, and even 50 responses and trained a series of
RVs. On the evaluation set, using these trained RVs,
we extracted cos4 VSM feature and calculated the
rcos4 for human-holistic scores. Table 2 reports the
average rcos4, which will de denoted as rcos4 there-
after, for the different-sized RV training sets. Table 2
shows that rcos4 continuously increases with the in-
crease of the dataset size for training RVs. However,
it is worth noting that using just 50 responses to train
RVs still provides a reasonably high rcos4 (0.383).
Between the two sizeRV conditions: 200 vs. 1800,
rcos4 did not show a statistically significant increase
based on a t-test (p = 0.314).
sizeRV 50 200 500 1000 1800
rcos4 0.383 0.428 0.435 0.439 0.440
Table 2: rcos4, a measurement of VSM features? scoring
performance, from different RV training data sizes
4.2 Using document clustering for training RVs
In the experiment described in section 4.1, we found
that using even a limited number of human-scored
responses can provide useful VSM features with a
reasonably high r to human-holistic scores. If we
can intelligently select such a small-sized dataset,
we think that the VSM-derived features will show
further improved predicting power. Armed with
this idea, we proposed a solution to use unsuper-
vised document clustering technology to find the re-
sponses for training RVs.
In particular, for each item, of the 1, 800 re-
sponses used for training the RVs, we run an LDA
document-clustering process to split all of responses
into K clusters. Then, for each cluster, we ran-
domly selected M responses. Therefore, we se-
lected K ?M responses for human scoring and for
training the RVs. Note that K ? M can be much
smaller than the original dataset size (n = 1800).
We believed that comprehensive coverage of all of
the latent topics would produce a better VSM that,
in turn, would provide more effective VSM-derived
features for scoring.
In our experiment, based upon a pilot study, we
decided to use K = 10 and M = 5 to control
the total scoring demand to be 50 responses per
item. Compared to the rcos4 value obtained from
randomly selecting 50 responses for training RVs
(0.383 in Table 2), the response selection based on
the document clustering improved the rcos4 to be
0.411. Furthermore, a t-test showed that such an in-
crease in rcos4 is statistically significant (p < 0.05).
60
4.3 Using machine predicted scores for
training RVs
Many of the previous automated speaking scoring
systems focused on the features measuring fluency,
pronunciation, and prosody (Witt, 1999; Franco et
al., 2010; Bernstein et al, 2010; Chen et al, 2009).
The scores predicted by these systems show promis-
ingly high correlations with human rated scores. In
order to eliminate the time-consuming and costly
human scoring step required by applications of
VSM, we considered using the scores automatically
scored by algorithms (AS) instead of the scores rated
by humans (HS).
In our experiment, we used a set of speech fea-
tures following (Chen et al, 2009) for automated
speech scoring. To estimate AS, a five-fold cross-
validation was applied on the entire dataset. For
each fold, a linear regression model was trained
from 80% of responses by using their HS and was
used to predict regression results on the remaining
20% of responses. The continuous scores produced
by the regression model were rounded to the four
discrete score levels (1 to 4) to serve as AS. Between
the obtained AS and HS, a Pearson r 0.56 was ob-
served.
Using the predicted scores, we re-ran our VSM
feature experiment by using the 1, 800 responses to
train the RVs. When the dataset sizes for training the
RVs was at 1, 800, we found that the rcos4 was 0.410
when using machine-predicted scores. Although it
was lower than the rcos4 value obtained by using
human-rated scores (0.440), a feature with such cor-
relational magnitude is still useful for building an
automatic scoring model.
4.4 A summary of experiments
HS1800 HS50 HScluster50 AS1800
rcos4 0.440 0.383 0.411 0.410
Table 3: A summary of rcos4 using different RV training
sizes, unsupervised-response clustering, and automated-
predicted scores
Table 3 summarizes the three experiments de-
scribed above. HS1800 refers to using 1, 800 re-
sponses with human scores (HS) to train RVs for
each item. HS50 refers to using only 50 responses
with human rated scores. HScluster50 refers to us-
ing 50 responses that were selected to cover 10 la-
tent topics detected by using an LDA unsupervised
topic clustering method. Compared to HS50, we
find that the unsupervised topic clustering method
helped to improve rcos4. AS1800 refers to using
1, 800 responses with automatically predicted scores
(AS) to train RVs for each item. Compared to
HS1800, AS1800 that avoids using a time-consuming
and costly human scoring process, shows a reason-
ably high rcos4.
5 Conclusions and Future Work
Vector Space Models (VSMs) have been widely
used in essay and speech assessment tasks to provide
vocabulary usage and content relevance measure-
ments. However, applying VSM on the assessments
with many items requires a lot of work by human
raters. To make the application of VSM in assess-
ments more economical and efficient, we propose
the use of unsupervised learning methods to reduce
and even eliminate the time-consuming and costly
human-scoring process. First, we found that it was
possible to just use hundreds rather than thousands
of responses to train RVs when applying VSM. In
our experiments with TOEFL R? data, we found that
using a minimum 200 responses to train RVs for
each item, was not statistically significantly different
from using 1, 800 responses. Next, we used an LDA
document-clustering method to identify latent top-
ics from all of the items and used the topic informa-
tion to select responses for training RVs. Our exper-
iments clearly suggest that such a method of selec-
tion provides more effective VSM features than ran-
dom selection. Finally, we used the scores predicted
by an automated speech scoring system that mostly
uses fluency and pronunciation features to replace
human-rated scores in building the VSM. Our exper-
iments suggest that the features derived from such a
VSM that can be constructed without the need of hu-
man scoring show promisingly high correlations to
human-holistic scores.
This research can be extended in several new di-
rections. First, we will apply the proposed methods
on other language assessment tasks, such as on long
(written) essays, to fully test that the proposed meth-
ods are universally helpful. Second, we are consid-
ering doing the third experiment in more iterations
? adding the VSM-derived features into the auto-
61
mated scoring model so that more accurate machine-
predicted scores can be used for building further im-
proved VSM.
References
Y. Attali and J. Burstein. 2004. Automated essay scoring
with e-rater v.2.0. In Presented at the Annual Meet-
ing of the International Association for Educational
Assessment.
J. Bernstein, A. Van Moere, and J. Cheng. 2010. Val-
idating automated speaking tests. Language Testing,
27(3):355.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the eleventh annual conference on Computa-
tional learning theory, page 92100.
L. Chen, K. Zechner, and X Xi. 2009. Improved pro-
nunciation features for construct-driven assessment of
non-native spontaneous speech. In NAACL-HLT.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American society for information science,
41(6):391407.
H. Franco, H. Bratt, R. Rossier, V. Rao Gadde,
E. Shriberg, V. Abrash, and K. Precoda. 2010. EduS-
peak: a speech recognition and pronunciation scoring
toolkit for computer-aided language learning applica-
tions. Language Testing, 27(3):401.
D. Higgins, J. Burstein, and Y. Attali. 2006. Identifying
off-topic student essays without topic-specific training
data. Natural Language Engineering, 12.
Thomas Hofmann. 2001. Unsupervised learning by
probabilistic latent semantic analysis. Machine Learn-
ing, 42(1):177?196.
Tuomo Kakkonen, Niko Myller, Erkki Sutinen, and Jari
Timonen. 2005. Comparison of dimension reduction
methods for automated essay grading. Natural Lan-
guage Engineering, 1:1?16.
Radim R?ehu?r?ek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45?50, Valletta,
Malta, May. ELRA.
S. M. Witt. 1999. Use of Speech Recognition in
Computer-assisted Language Learning. Ph.D. thesis,
University of Cambridge.
S. Xie, K. Evanini, and K. Zechner. 2012. Exploring
content features for automated speech scoring. Pro-
ceedings of the NAACL-HLT, Montreal, July.
Klaus Zechner and Xiaoming Xi. 2008. Towards auto-
matic scoring of a test of spoken language with het-
erogeneous task types. In Proceedings of the Third
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, pages 98?106. Association for
Computational Linguistics.
62
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 288?292,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Evaluating Unsupervised Language Model Adaptation Methods for
Speaking Assessment
Shasha Xie
Microsoft
1020 Enterprise Way
Sunnyvale, CA 94089
shxie@microsoft.com
Lei Chen
Educational Testing Service
600 Rosedale Rd
Princeton, NJ
LChen@ets.org
Abstract
In automated speech assessment, adaptation of
language models (LMs) to test questions is im-
portant to achieve high recognition accuracy
However, for large-scale language tests, the
ordinary supervised training, which uses an
expensive and time-consuming manual tran-
scription process, is hard to utilize for LM
adaptation. In this paper, several LM adap-
tation methods that require either no manual
transcription process or just a small amount of
transcriptions have been evaluated. Our ex-
periments suggest that these LM adaptation
methods can allow us to obtain considerable
recognition accuracy gain with no or low hu-
man transcription cost.
Index Terms: language model adaptation, unsuper-
vised training, Web as a corpus
1 Introduction
Automated speech assessment, a fast-growing area
in the speech research field (Eskenazi, 2009), typ-
ically uses an automatic speech recognition (ASR)
system to recognize spontaneous speech responses
and use the recognition outputs to generate the fea-
tures for scoring. Since the recognition accuracy di-
rectly influences the quality of the speech features,
especially for the features related to word entities,
such as those measuring grammar accuracy and vo-
cabulary richness, it is important to use ASR sys-
tems with high recognition accuracy.
Adaptation of language models (LMs) to test re-
sponses is an effective method to improve recogni-
tion accuracy. However, it is difficult to only use
the ordinary supervised training to adapt LMs to test
questions. First, for high-stake tests administered
globally, a very large pool of test questions have to
be used to strengthen the tests? security and validity.
Since a large number of test questions have many
possible answers for each question, a large set of au-
dio files needs to be transcribed to cover response
content. Second, due to time and cost constraints,
it may not be practical to have a pre-test to collect
enough speech responses for adaptation purposes.
Therefore, it is important to pursue other methods to
obtain LM adaptation data in a faster and lower-cost
way than the ordinary supervised training.
As we will review in Section 2, some promising
technologies, such as unsupervised training, active
learning, and LM adaptation based on Web data,
have been utilized in broadcast news recognition, di-
alog system, and so on. In this paper on the LM
adaptation task used in automated speech scoring
systems, we will report our experiments to obtain
LM adaptation data in a faster and more economical
way that requires little human involvement. To our
knowledge, this is the first such work reported in the
automated speech assessment area.
The rest of the paper is organized as follows: Sec-
tion 2 reviews the related previous research results;
Section 3 describes the English test, the data used
in our experiments, and the ASR system used; Sec-
tion 4 reports the experiments of different methods
we tried to obtain LM adaptation data; Section 5 dis-
cusses our findings and plans for future research.
288
2 Previous Work
Unsupervised training is the method of using untran-
scribed audio to adapt a language model (LM). An
initial ASR model (seed model) is used to recognize
the untranscribed audio, and the obtained ASR out-
puts are used in the follow-up LM adaptation. (Chen
et al, 2003) utilized unsupervised LM adaptation
on broadcast news (BN) recognition. The unsuper-
vised adaptation method reduces the word error rate
(WER) by 2% relative to using the baseline LM.
(Bacchiani and Roark, 2003) reported that unsuper-
vised LM adaptation provided an absolute error rate
reduction of 3.9% over the un-adapted baseline per-
formance by using 17 hours of untranscribed adap-
tation data. This was 51% of the 7.7% adaptation
error rate reduction obtained by using an ordinary
supervised adaptation method.
Active learning is used to reduce the number of
training examples to be annotated by automatically
processing the unlabeled examples and then select-
ing the most informative ones with respect to a given
cost function. (Riccardi and Hakkani-Tur, 2003;
Tur et al, 2005) proposed using a combination of
unsupervised and active learning for ASR training
to minimize the workload of human transcription.
Their experiments showed that the amount of la-
beled data needed for a given recognition accuracy
can be reduced by 75% when combining these two
training approaches.
A recent trend in Natural Language Processing
(NLP) and speech recognition research is utilizing
Web data to improve the LMs, especially when in-
domain training material is limited. (Ng et al,
2005) investigated LM topic adaptation using Web
data. Experiments in recognizing Mandarin tele-
phone conversations showed that use of filtered Web
data leads to a 7% reduction in the character recog-
nition error rate. (Sarikaya et al, 2005) used Web
data to adapt LMs used in a spoken dialog system.
From a limited in-domain data set, they generated
a series of search queries and retrieved Web pages
from Google using these queries. In their recog-
nition experiment done on a dialog system, they
achieved a 5.2% word error reduction by using the
Web data, compared to a baseline LM trained on
1700 in-domain utterances.
3 Test, Data, and ASR
Our in-domain data was from The Test of English
for International Communication, TOEIC R?, which
tests non-native English speakers? basic speaking
ability required in international business communi-
cations. In our experiments, we focused on opinion
testing questions. An example question is: ?Do you
agree with the statement that a company should only
hire experienced employees? Use specific reasons to
support your answer?.
A state-of-the-art HMM LVCSR system, which
was provided by a leading ASR vendor, was used in
our experiments. It contains a cross-word tri-phone
acoustic model (AM) and a combination of bi-gram,
tri-gram, and up to four-gram LMs. The AM and
LM are trained by supervised training from about
800 hours of audio and manual transcriptions of
non-native English speaking data collected from the
Test Of English as a Foreign Language (TOEFL R?).
TOEFL R? is targeted to assess test-takers? ability
to use English to study in an institution using En-
glish as its primary teaching language. Speaking
content from TOEFL R? data is quite different from
the content shown in TOEIC R? data. When testing
this recognizer on a held-out evaluation set extracted
from the TOEFL R? test, a word error rate (WER) of
33.0% 1 is observed. This recognizer was used as
the seed recognizer in our experiments.
4 Experiments
We collected a set of audio responses from the
TOEIC R? test, focusing on opinion questions. This
data set was randomly selected from different first-
language (L1) and English speaking proficiency lev-
els. Then, these audio files were manually tran-
scribed. In our experiments, 1470 responses were
used for LM adaptation and the remaining 184 re-
sponses were used to evaluate speech recognition
1ASR on non-native speech is more difficult than on native
speech for various reasons (Livescu and Glass, 2000). How-
ever, a high WER does not rule out the possibility of using
ASR outputs for automated scoring, especially when relying
on delivery related features. For example, (Chen et al, 2009)
shows that several pronunciation features? contributions for as-
sessment, measured as Pearson correlations between the feat-
uers and human scores, only drop about 10% to 20% when us-
ing ASR outputs with a WER as high as 50% compared to using
human transcriptions.
289
accuracy. When using the seed recognizer with-
out any adaptation, the WER on the evaluation set
is 42.8%, which is much higher than the accuracy
achieved on the TOEFL R? data (33.0%). Using the
ordinary supervised training, adapting LMs using
these 1470 manual transcriptions, the WER is re-
duced to 34.7%, close to the performance on the
in-domain TOEFL R? data. Note that a fixed dictio-
nary with a vocabulary size of about 20, 000 words,
which in general is much larger than the vocabulary
mastered by non-native test takers, was used in our
experiment.
4.1 Unsupervised LM adaptation
Using the seed recognizer trained on the TOEFL R?
data, we recognized 1470 adaptation responses and
selected varying amounts of ASR outputs for LM
adaptation. From ASR outputs of all responses, we
selected the responses with high confidence scores
estimated by the seed recognizer so that we could
use the ASR outputs with higher recognition accu-
racy on the LM adaptation task. We used two meth-
ods to measure the confidence score for each re-
sponse from word-level confidence scores. First, we
took the average of all word confidence scores a re-
sponse contains, as shown in Equation 1.
ConfperWord =
1
N
N?
i=1
conf(wi) (1)
where conf(wi) is the confidence score of word, wi.
The other method we used considers each word?s du-
ration, as shown in Equation 2.
ConfperSec =
?N
i=1 d(wi) ? conf(wi)
?N
i=1 d(wi)
(2)
where d(wi) is the duration of wi.
In Figure 1, we showed the WER after running
unsupervised LM adaptation, where the adaptation
responses were selected if they had high word-based
(ConfperWord) or duration-based (ConfperSec)
confidence scores. The data sizes used for adapta-
tion vary from 0% (without any adaptation) to 100%
(using all adaptation data). We observe continuous
reduction of WER when using more and more adap-
tation data. Selecting responses by the word-based
confidence scores performs a little better than the se-
lection method based on the confidence scores nor-
malized by corresponding word durations. However,
there is no significant difference between these two
selection criteria.
Figure 1: Unsupervised LM adaptation performance us-
ing different sizes of development set data.
ASR accuracy may vary within each response.
Therefore, instead of using entire responses, we also
explored using smaller units for LM adaptation. All
of the ASR outputs were split into word sequences
with fixed lengths (10-15 words), and the ones with
higher per-word confidence scores (ConfperWord)
were extracted for model adaptation. Our experi-
ment shows that using word-sequence pieces rather
than entire responses leads to a faster WER reduc-
tion. When only using 5% of the adaptation data, we
obtained 3.5% absolute WER reduction compared to
the baseline result without adaptation. Note that we
only obtained 2.5% absolute WER reduction when
using entire responses in adaptation.
4.2 Web data LM adaptation
Given around 40% WER when using our seed ASR,
unsupervised learning faces the issue that many
recognition errors were included in model adapta-
tion. Can we find another source to obtain LM
adaptation inputs with fewer errors? To address
this question, we explored building a training cor-
pus from Web data based on test questions. We
used BootCat (Baroni and Bernardini, 2004), a cor-
pus building tool designed to collect data from the
Web, to collect our LM adaptation data. Based on
test prompts in the TOEIC R? test, we manually gen-
erated search queries. After receiving the search
queries, the BootCat tool searched the Web using
the Microsoft Bing search engine. Then, top-ranked
290
Web pages were downloaded and texts on these Web
pages were extracted. We examined the Web search
results (including URLs and texts) returned by the
BootCat tool. The returned Web data has varied
matching rates among these prompts and are gen-
erally noisy.
By using only the default setup provided by the
BootCat tool, we collected 5312 sentences in total.
After a simple text normalization, we used the ob-
tained Web data for LM adaptation, and the WER
on the evaluation data was 38.5%. This WER result
is a little higher than the WER result achieved by
unsupervised LM adaptation (38.1%). Without tran-
scribing any response from test-takers, the language
model adaptation using Web data already helps to
improve recognition accuracy. Then, we tried us-
ing both the Web data and the ASR hypotheses for
adaptation, and we can further decreased the WER
to 37.6%. This is lower than using the two LM adap-
tation data sets separately.
4.3 Semi-supervised approaches for LM
adaptation
For semi-supervised LM adaptation, we replaced the
speech responses of lower confidence scores with
their corresponding human transcripts. We hoped
that by using the responses with high confidence
scores together with a small amount of human tran-
scripts, we could get better performance by intro-
ducing less noise during adaptation. We set differ-
ent thresholds for selecting the low confidence re-
sponses and replacing them with human transcripts.
We find that just manually transcribing a limited
amount of audio data gives us further WER reduc-
tion, compared to using unsupervised learning. Af-
ter transcribing just 100 responses, 6.8% of 1470 re-
sponses in the adaptation data set, semi-supervised
learning can achieve 61.73% of the WER reduction
(8.1%) obtained by using the ordinary supervised
training that requires transcription of all 1470 re-
sponses.
4.4 Discussion
In Table 1, we compared the performance of all the
adaptation methods mentioned in this paper, includ-
ing two unsupervised methods adapted using the
ASR hypotheses and ?related? Web data, and one
semi-supervised method 2, replacing the ASR hy-
potheses of lower confidence scores with their corre-
sponding human transcripts. For a convenient com-
parison, we also include the baseline (without LM
adaptation) and the result of using the supervised
adaptation. All the proposed unsupervised/semi-
supervised methods can significantly improve the
ASR performance compared to the baseline result.
For projects with time limits, we can use these
unsupervised/semi-supervised methods to help us
get relatively good ASR outputs.
Table 1: The WER on the evaluation set using different
LM adaptation methods.
baseline
unsupervised
semi super.
ASR Web ASR&Web
42.8 38.1 38.5 37.6 37.8 34.7
5 Conclusions and Future Work
In this paper, we reported our experiments in ap-
plying several LM adaptation methods to automated
speech scoring systems that require few, if any, hu-
man transcripts, which are expensive and slow to
obtain for large-sized adaptation data sets. The un-
supervised training (using ASR transcriptions from
a seed ASR system) clearly shows higher accuracy
than a ASR system without any domain adaptation.
We also used test questions to collect related texts
from Web. Even though such Web data may be noisy
and its relatedness to real test responses is not al-
ways guaranteed, text data collected from the Web
is helpful to adapt LMs to better fit the responses to
test questions. To better cope with recognition er-
rors brought on by using the unsupervised training
method, we proposed using human transcriptions on
a small amount of poorly recognized responses. Us-
ing such little human involvement further helps to
obtain a lower WER. Therefore, based on the ex-
periments described in this paper, we conclude that
these novel LM adaptation methods provide promis-
ing solutions to let us skip the ordinary supervised
training for LM adaptation tasks frequently used in
automated speech scoring.
2The semi-supervised result was from replacing 100 low-
confidence responses with human transcripts.
291
The reported experiments in this paper were con-
ducted on a limited-size data set. We plan to increase
the testing data to a larger size and hope to cover
more types of test questions and spoken tests. In ad-
dition, we plan to investigate how to automatically
generate Web search queries based on test questions.
References
M. Bacchiani and B. Roark. 2003. Unsupervised lan-
guage model adaptation. In 2003 IEEE International
Conference on Acoustics, Speech, and Signal Process-
ing, 2003. Proceedings.(ICASSP?03).
M. Baroni and S. Bernardini. 2004. BootCaT: bootstrap-
ping corpora and terms from the web. In Proceedings
of LREC, volume 2004, page 13131316.
L. Chen, J. L Gauvain, L. Lamel, and G. Adda. 2003.
Unsupervised language model adaptation for broad-
cast news. In 2003 IEEE International Conference on
Acoustics, Speech, and Signal Processing, 2003. Pro-
ceedings.(ICASSP?03).
L. Chen, K. Zechner, and X Xi. 2009. Improved pro-
nunciation features for construct-driven assessment of
non-native spontaneous speech. In NAACL-HLT.
M. Eskenazi. 2009. An overview of spoken language
technology for education. Speech Communication,
51(10):832?844.
K. Livescu and J. Glass. 2000. Lexical modeling
of non-native speech for automatic speech recogni-
tion. In Acoustics, Speech, and Signal Processing,
2000. ICASSP?00. Proceedings. 2000 IEEE Interna-
tional Conference on, volume 3, pages 1683?1686.
T. Ng, M. Ostendorf, M. Y Hwang, M. Siu, I. Bulyko, and
X. Lei. 2005. Web-data augmented language mod-
els for mandarin conversational speech recognition. In
Proc. ICASSP, volume 1.
G. Riccardi and D. Z Hakkani-Tur. 2003. Active and un-
supervised learning for automatic speech recognition.
In Proc. 8th European Conference on Speech Commu-
nication and Technology.
R. Sarikaya, A. Gravano, and Y. Gao. 2005. Rapid lan-
guage model development using external resources for
new spoken dialog domains. In Proc. ICASSP, vol-
ume 1, pages 573?576.
G. Tur, D. Hakkani-Tur, and R. E Schapire. 2005. Com-
bining active and semi-supervised learning for spo-
ken language understanding. Speech Communication,
45(2):171?186.
292
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 68?78,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Automatic evaluation of spoken summaries: the case of language
assessment
Anastassia Loukina, Klaus Zechner, Lei Chen
Educational Testing Service (ETS)
Princeton, NJ 08541, USA
aloukina@ets.org, kzechner@ets.org, lchen@ets.org
Abstract
This paper investigates whether ROUGE, a
popular metric for the evaluation of au-
tomated written summaries, can be ap-
plied to the assessment of spoken sum-
maries produced by non-native speakers
of English. We demonstrate that ROUGE,
with its emphasis on the recall of infor-
mation, is particularly suited to the as-
sessment of the summarization quality of
non-native speakers? responses. A stan-
dard baseline implementation of ROUGE-
1 computed over the output of the au-
tomated speech recognizer has a Spear-
man correlation of ? = 0.55 with experts?
scores of speakers? proficiency (? = 0.51
for a content-vector baseline). Further in-
creases in agreement with experts? scores
can be achieved by using types instead of
tokens for the computation of word fre-
quencies for both candidate and reference
summaries, as well as by using multiple
reference summaries instead of a single
one. These modifications increase the cor-
relation with experts? scores to a Spear-
man correlation of ? = 0.65. Furthermore,
we found that the choice of reference sum-
maries does not have any impact on per-
formance, and that the adjusted metric is
also robust to errors introduced by auto-
mated speech recognition (? = 0.67 for hu-
man transcriptions vs. ? = 0.65 for speech
recognition output).
1 Introduction
In this paper we explore whether metrics com-
monly used for the automated evaluation of writ-
ten summaries can be used to evaluate spoken
summaries in the context of language assessment.
The performance of automatic summarization
systems is routinely evaluated using content met-
rics such as ROUGE (Lin and Rey, 2004), which
measures the n-gram overlap between the candi-
date summary and a set of reference summaries
(see also Rankel et al. (2013) for historical back-
ground). ROUGE is a recall-oriented metric in-
spired by its precision-oriented counterpart BLEU,
developed to evaluate machine translations (Pap-
ineni et al., 2002). Recent research in this area has
been focused on identifying the most reliable vari-
ants of ROUGE and best practices in the application
of the metric (Owczarzak et al., 2012; Rankel et
al., 2013). These studies (reviewed in more detail
in Section 2.1) showed that less commonly used
variants of ROUGE may in fact be more consistent
with human judgments, at least in the context of
automatic summary evaluation.
Beyond the research in automatic summariza-
tion systems, ROUGE has also been used to eval-
uate written summaries in the context of educa-
tional assessment. Madnani et al. (2013) showed
that one of the variants of ROUGE, in combination
with other metrics, performed consistently well
for the automated scoring of written responses to
summary tasks produced by middle- and high-
school students. They did not investigate the effect
of using other variants of ROUGE.
In this paper, we explore whether ROUGE can be
used to automatically evaluate the content cover-
age of spoken summaries produced by non-native
speakers in the context of language assessment.
As in case of automatic text summaries, the hu-
man raters who score these responses are asked
to assess whether the summary accurately con-
veys the information contained in the stimulus.
While the length of the spoken responses is more
loosely constrained than in case of automatic text
summaries, human raters do not penalize for ex-
traneously irrelevant language. Therefore recall-
oriented ROUGE is an attractive evaluation metric
for this task.
At the same time, unlike automatic text sum-
68
maries, spoken summaries are abstractive and of-
ten contain ungrammatical sequences, repetitions,
repairs, and other disfluencies. Further ?noise?
is introduced by transcription errors generated by
the automated speech recognition system. In this
study, we assess whether (a) ROUGE is robust
against this type of noise; (b) how many refer-
ence summaries are necessary to obtain reliable
evaluation; and (c) how the choice of specific ref-
erence summaries affects the performance of the
metric (Section 4.1). We also assess which vari-
ants of ROUGE have the most agreement with hu-
man judgments on this type of summary and what
adjustments can be made to mitigate the effects
of disfluencies and errors introduced by automated
speech recognition (Section 4.2). Finally, we test
how well our adjusted variant of ROUGE can pre-
dict the human scores on unseen data (Section
4.3).
2 Related work
2.1 The application of ROUGE to evaluation
of automatic text summarization
There exist various versions of ROUGE which dif-
fer in terms of the length of their n-grams, the use
of skip-bigrams, the application of stemming, and
the exclusion of stop-words. Several studies have
compared these variants to identify those most
consistent with human judgments. In earlier work,
Lin (2004) reported that variants based on uni-
grams and skip-bigrams (ROUGE-SU4) or bigrams
alone (ROUGE-2) performed best. ROUGE-2 was
also identified as the best variant more recently
by Owczarzak et al. (2012). Rankel et al. (2013)
found that linear combinations of these metrics
with ROUGE based on longer n-grams are more ac-
curate in finding significantly different systems.
Previous work also explored various methods
of text pre-processing prior to the computation of
ROUGE, including stemming and the removal of
stop-words, neither of which had any substantial
effect on the performance of ROUGE (Lin and Rey,
2004; Owczarzak et al., 2012). Owczarzak et al.
(2012) reported that the agreement with human
judgments was, in fact, higher if the stop-words
were retained.
All applications discussed so far used ROUGE
to evaluate the textual summarization of written
texts. There have also been attempts to apply
this metric to text summaries of speech data with
mixed results (see Nenkova and McKeown (2011)
for a review). ROUGE performed reasonably well
for the evaluation of text summaries of spoken pre-
sentations (Hirohata et al., 2005), but was not cor-
related with the summary accuracy of summaries
of meetings or conversations (although see (Penn
and Zhu, 2008)).
Most of this work was performed on extractive
summaries produced by summarization systems
that used multiple summaries to evaluate each sys-
tem. In this study, we explore the application of
ROUGE to the evaluation of abstractive summaries
produced by students in a language assessment
context with an aim of producing a separate evalu-
ation for each summary. Furthermore, the fact that
these are spoken responses adds an extra layer of
complexity to the analysis, therefore the results of
previous studies cannot directly be applied to this
new context.
2.2 Previous approaches to the content
evaluation of spoken summaries for
assessment purposes
The research on the automated scoring of con-
tent accuracy in a language assessment has pri-
marily focused on the evaluation of written essays.
Most previous approaches in this area have used
so-called ?bag-of-words?-based models, gleaned
from the discipline of information retrieval. The
basic idea is that an essay is considered to be
highly content relevant to a given topic when it
contains words that are similar to those seen in
previously collected essays with high human-rater
scores. For instance, Attali and Burstein (2006)
used a vector-space model to compute the co-
sine similarities between word vectors found in
an essay to be automatically scored and word vec-
tors comprising previously scored essays with the
same human-rater score. In a similar vein, Foltz
et al. (1999) computed a compressed vector space
based on singular value decomposition for a set
of document-word vectors, called latent semantic
analysis, and then computed similarity scores for
essays based on this more compact representation.
It should be noted, though, that since all of these
models do not take word sequences into account,
they must be considered knowledge-poor in that
they cannot distinguish between syntactic roles
or a list of random words versus a well-formed
sentence. In operational systems, such bag-of-
words similarity features are combined with fea-
tures which evaluate grammar and other aspects
69
of language use; therefore a random list of con-
tent words is unlikely to lead to a high overall
score. However, finer-grained distinctions such as
negations or subject-object relationships between
words are often lost.
Applications of these methods to spontaneous
speech in spoken-language assessments have been
conducted much more recently as this domain of
language assessment relies on the output of Au-
tomatic Speech Recognition systems (ASR) that
typically have a fairly high word-error rate. These
errors can negatively affect the accuracy of the
methods developed for written responses. Fur-
thermore, spoken responses differ in many proper-
ties from written ones (Biber et al., 2004) and the
validity of existing methods for assessing speech
needs to be established before they can be used
for operational scoring.
Xie et al. (2012) presented experiments using
content features on spontaneous-speech data based
on vector-space models, latent semantic analysis,
as well as point-wise mutual information. Some
of these content features showed higher correla-
tions with human scores than features measuring
other aspects of speaking proficiency, such as flu-
ency or pronunciation. Chen and Zechner (2012)
also used a vector space model for the scoring of
spontaneous speech, but extended it by using the
ontological information contained in WordNet. Fi-
nally, Xiong et al. (2013) used a variety of ap-
proaches to capture the content of spontaneous re-
sponses from the same corpus that we are investi-
gating in this paper. Approaches varied from com-
puting the overlap between key words in the stim-
uli and responses to a more traditional vector space
model based on content vector analysis.
While these approaches have good correlations
with human scores, they have a number of short-
comings. The best performing method suggested
by Xiong et al. (2013) requires the manual annota-
tion of the relevant key words for each prompt be-
fore the computation of the metric. Vector space
models do not have this limitation, but they require
a substantial number of reference summaries to
achieve consistent results. Supporting this point,
Chen (2013) showed that at least 50 reference re-
sponses were necessary to obtain moderate agree-
ment between the cosine similarity measure and
human judgments, with further improvement in
agreement as the number of reference responses
is increased to 200. These limitations pose prac-
tical difficulties when new items are added to the
tests: the computation of content metrics for each
new item requires either a manual annotation or a
relatively large number of reference responses.
ROUGE appears promising in this context since
it does not have either of these limitations. First,
the computation of ROUGE does not require man-
ual annotation. Second, research on the evalua-
tion of written summaries suggests that relatively
few reference summaries may be necessary to ob-
tain reliable results, e.g., only four references were
used for the summary evaluation at the Text Anal-
ysis Conference (Rankel et al., 2013). In addition,
the recall-based nature of ROUGE is well-aligned
with the evaluation criteria for these responses.
Therefore in this paper, we explore whether any of
the variants of ROUGE can be successfully applied
to the content scoring of spoken summaries and
what modifications may be necessary to achieve
optimal performance.
3 Data and methodology
3.1 Description of the corpus
The study is based on a corpus of responses
collected during the pilot administration of the
TOEFL
R
?Junior
TM
Comprehensive test, an inter-
national assessment of English proficiency tar-
geted at middle-school students aged from 11 to
15 (see also Xiong et al. (2013) who used a subset
of this corpus).
The corpus used in this study included 5,934
spoken responses produced by 1,611 speakers; all
learners of English as a foreign language residing
in different countries. In addition to a read-aloud
task that was not relevant for this paper, the speak-
ers were presented with four other tasks. First, the
speakers were asked to describe a sequence of six
pictures. For the remaining three taks, the speak-
ers listened to one announcement and two frag-
ments from a lecture and were then asked to sum-
marize the content of what they heard. The stu-
dents were provided with a list of concepts that test
takers were expected to cover in their responses.
For example, a student may have listened to
a teacher giving an assignment in history class.
1
This assignment required the class to go to the li-
brary, look up information about the water supply
in old and modern cities, answer the questions on
their worksheet, and write a short paragraph about
1
http://toefljr.caltesting.org/sampletest/s-
historylesson.html
70
their findings. The students were then asked to re-
spond to the following prompt:
Imagine that your classmate was not
in class today. Tell your classmate
about what the history teacher asked
the students to do. Be sure to talk about
the following:
- the library
- the worksheet
- the homework
The corpus contained responses to 24 different
prompts with 6 different sets of prompts. Each
speaker only answered one set of prompts giving
4 responses per speaker. The recording time for
each response was limited to 60 seconds. The ac-
tual number of words varied between participants
with an average 72 words per response (? = 29).
From the originally recorded 6,444 responses,
we excluded from further analysis 510 responses
(about 8%), which contained either no speech or
where the quality of the recording was too low for
further analysis. All remaining 5,934 responses
were scored on a scale of 1-4 by two expert human
raters on a holistic scale that reflects all aspects
of speaking proficiency, including pronunciation,
grammar, and content coverage.
2
For content cov-
erage, the raters were asked to consider whether
the key information contained in the prompt was
conveyed accurately or, in case of the picture de-
scription prompt, whether the story was complete.
When the difference in the scores assigned by the
two raters was greater than 1, the final score was
assigned by an adjudicator.
The corpus was divided into non-overlapping
training and testing partitions. The training par-
tition contained 3,337 responses from 915 speak-
ers and the test partition contained 2,597 spoken
responses from 696 speakers. Both partitions in-
cluded responses for the same prompts but there
was no speaker overlap.
All responses were converted to text using
a state-of-the-art automatic speech recognizer
(ASR) with constrained vocabulary (see Evanini
and Wang (2013) for further details). To evalu-
ate the effect of the errors that may have been in-
troduced by the ASR system, all responses were
2
see http://www.ets.org/s/toefl junior/pdf/toefl junior
comprehensive speaking scoring guides.pdf for the scoring
rubrics
transcribed manually by professional human tran-
scribers. Comparison with the human transcrip-
tion showed that the ASR word error rate for this
corpus was 26.5% for picture narration tasks and
29.4% for the summarization tasks.
3.2 Computation of the metrics
Evaluation metrics. ROUGE was computed using
equation (1) as an n-gram (gr
n
) overlap between
candidate summary and each summary (S) from
the set of reference summaries (RS).
ROUGE
N
=
?
S?RS
?
gr
n
?S
Count
overlap
(gr
n
)
?
S?RS
?
gr
n
?S
Count(gr
n
)
(1)
We used n-grams whereby n was in a range
from 1 to 4 (ROUGE 1-4) and a combination
of unigrams with skip-bigrams with maximum
step of four words (ROUGE-SU1-4). Finally,
we also computed a combined measure ROUGE-
ALL which is the geometrical mean of ROUGE-1?
ROUGE-4, computed by using the same smoothing
procedure as for BLEU (Papineni et al., 2002).
We used the cosine distance (CVA) between the
response and reference summaries as a baseline
metric as this metric is commonly used for eval-
uating document similarity in the context of lan-
guage assessment. CVA was computed as the co-
sine distance between candidate responses and the
same reference responses as used for the com-
putation of ROUGE. All term frequencies were
weighted using tf-idf where tf is the frequency of
a term in a given response and idf is the inverse
document frequency. idf frequencies were com-
puted based on all of the responses in the corpus.
Reference summaries. The reference sum-
maries were selected from responses with the
highest human rater final score (4). This approach
is similar to using system outputs as pseudo-
models for the evaluation of machine-translation
or automatic-summarization systems (cf. Louis
and Nenkova (2013)). It has also been success-
fully applied to the content assessment of written
answers by Madnani et al. (2013) who used one
randomly selected highly scored summary as a ref-
erence summary.
Since previous work on summarization eval-
uation showed that multiple summaries increase
the reliability of evaluations (Louis and Nenkova,
2013; Nenkova and McKeown, 2011), we tested
71
how many summaries were necessary to achieve
consistent results. We therefore computed ROUGE
for each response using up to 10 randomly se-
lected responses with final score of 4. To inves-
tigate the effect that different choices of reference
summaries may have on the metrics, we repeated
the analysis for 20 randomly selected sets of refer-
ence responses.
The corpus did not contain a sufficient num-
ber of responses with the maximum score for each
prompt. Therefore, this part of the analysis was
based on a subset of 1,784 responses selected from
the training partition. This set included only 12
prompts for which human raters assigned a score
of 4 to more than 11 responses.
Text preprocessing. For the evaluation of writ-
ten summaries, ROUGE is usually computed using
the raw counts of all of the terms. In addition to us-
ing this classical approach using unstemmed terms
(?all?), we also computed ROUGE using three other
approaches: (1) excluding all stop-words (?Non-
stop?); (2) setting the frequency of all n-grams
within each summary to 1, that is, counting types
instead of tokens (?Types?); (3) excluding all stop-
words and counting types only (?Non-stop types?).
Finally, we computed all of these ROUGE variants
using raw text as well as lemmatized text. As a re-
sult, we computed 72 different variants of ROUGE
for each response and each combination of refer-
ence summaries: nine different types of ROUGE
(eight different n-gram lengths and ROUGE-ALL)
computed using four different methods of text pro-
cessing and two possible approaches to lemmati-
zation. All of the computations were done both on
ASR and manual transcriptions.
3.3 Evaluation
We computed the Spearman?s rank correlation be-
tween the metric and the holistic score assigned
by the first rater to identify the best method of
computing ROUGE and the optimal number of ref-
erences. Performance of the metric may be af-
fected by properties of the prompt (cf. (Nenkova
and Louis, 2008)), therefore we first analyzed each
prompt separately and then selected the variants
that achieved the highest performance across all
of the prompts. Since correlation coefficients are
not normally distributed, we used several non-
parametric methods to identify significant differ-
ences including non-parametric bootstrapping and
non-parametric ANOVAs. These analyses were
done using the data from the training partition of
the corpus.
We then evaluated how well the selected vari-
ants of ROUGE predicted human scores using a lin-
ear regression model trained on all of the data from
the training partition using pooled data from all of
the prompts. The model was tested on an unseen
test partition that had not been used for any of the
analyses.
Finally, we tested whether the new metrics im-
proved the performance of the automated scoring
engine for spoken responses. The current system
assigns scores based on the linear combination of
features with empirical weights obtained by train-
ing scoring models on scores assigned by expert
raters (Zechner et al., 2009; Higgins et al., 2011).
Current features measure various aspects of speak-
ing proficiency such as fluency, pronunciation, and
grammar usage. The performance of the system is
evaluated with correlations and quadratic kappas
between the scores assigned by the human raters
and rounded predicted scores.
4 Results
All analyses were performed twice: each for met-
rics computed using ASR and manual transcrip-
tions. We found that although the exact values
of the correlation coefficients differed across these
two transcriptions, the overall pattern of results
remained the same. There was also a high cor-
relation in metric values between the two types
of transcription (Pearson?s r for different types of
ROUGE varied between 0.81 for ROUGE-4 and 0.9
for ROUGE-1). Since automated scoring relies on
the output of automatic speech recognition, all nu-
merical results reported in the main text of this
section are based on ASR output. The tables re-
port the numbers for both ASR and manual tran-
scriptions.
4.1 Number and choice of reference
responses
Number of references. To identify the optimal
number of references for each prompt and met-
rics, we first found N
best
, which had the high-
est correlation with human scores and then iden-
tified the lowest number of reference summaries
for which the correlation coefficient was not sig-
nificantly lower than the correlation coefficient for
N
best
.
Comparisons between different correlations
72
were performed using the general method sug-
gested by Zou (2007) for comparing overlapping
correlations as implemented by Baguley (2012,
p.224) but we used bootstrapped confidence inter-
vals (Wilcox, 2009). Confidence intervals for each
correlation coefficient were constructed using pi-
geonhole bootstrapping (Owen, 2007) with 1,000
samples. For each N reference, we pooled the
values computed for 20 randomly selected sets of
different reference summaries. We then indepen-
dently sampled responses and sets of references
and selected values at each bootstrap repetition
at the intersection of the two samples. The con-
fidence intervals were constructed using the ad-
justed percentile method (Davison and Hinkley,
1997, p. 203-213). Since this analysis is more sen-
sitive to Type II errors (?false negatives?), we set
the significance threshold at ? = 0.15.
The optimal number of references varied be-
tween prompts, metrics, and methods of compu-
tation, but never exceeded 8. On average, opti-
mal performance was achieved with 3 references.
More references were required to achieve optimal
performance for ROUGE based on longer n-grams
(using the Kruskal-Wallis test, a non-parametric
analysis of variance, p < 2.2 ? 10
?16
). For ex-
ample, two references on average were required
to achieve reliable results for ROUGE-1, but for
ROUGE-4 this number was four references. The
required number of references was also signifi-
cantly dependent on the prompt (Kruskal-Wallis
test, p < 2.2 ? 10
?16
) with averages varying be-
tween two and four. When the number of ref-
erences was equal to or greater than the optimal
number, there were no significant differences in
the correlation coefficients across the different ref-
erence models.
For the analysis in the following section each
of the 72 variants of ROUGE for each prompt was
computed using the optimal N references identi-
fied for this variant and prompt.
4.2 Types of ROUGE and different methods of
computation
The correlation coefficients between the summa-
rization metrics and human ratings depended on
the length of n-grams (Kruskal-Wallis test p <
2.2 ? 10
?16
). While all types of ROUGE were pos-
itively correlated with human ratings, the corre-
lation coefficients were the highest for ROUGE-1
and ROUGE-SU2-4, which performed significantly
better than ROUGE-3-4 and the combined mea-
sures ROUGE-ALL (post-hoc Tukey HSD test on
ranked observations, p varied from p < 1 ? 10
?10
to 2.804 ? 10
?4
). The average correlations across
the different types of text pre-processing for ASR
and manual transcriptions are shown in Table 1.
Metrics ASR output Manual
ROUGE-1 0.616 0.637
ROUGE-SU4 0.592 0.608
ROUGE-SU3 0.595 0.609
ROUGE-SU2 0.594 0.613
ROUGE-SU1 0.598 0.619
ROUGE-ALL 0.523 0.527
ROUGE-2 0.553 0.560
ROUGE-3 0.468 0.461
ROUGE-4 0.366 0.357
Table 1: Average correlation coefficient with hu-
man scores (Spearman?s ?) across different meth-
ods of computation for ROUGE based on n-grams
of different lengths. The table shows the results
for metrics computed based on ASR and manual
transcriptions.
The effect of text pre-processing differed
across the metrics: for metrics that relied on
consecutive n-grams with n>2, the removal of
stop-words led to further drops in performance
(Kruskal-Wallis test p = 4.4 ? 10
?5
). For ROUGE
based on unigrams and skip-bigrams, counting
only type frequencies led to a significant im-
provement in performance (Kruskal-Wallis test,
p = 0.00017). Correlations for the different types
of pre-processing for the measures that performed
the best are given in Table 2. Lemmatization did
not make a significant difference to metric perfor-
mance.
Pre-processing ASR ouput Manual
All 0.573 0.606
Non-stop 0.585 0.600
Non-stop types 0.601 0.617
Types 0.622 0.634
Table 2: Average correlation coefficient with
human proficiency score (Spearman?s ?) across
ROUGE-1 and ROUGE-SU1-4 for different meth-
ods of text processing. The table shows the results
for metrics computed based on ASR output and
manual transcriptions.
73
Finally, a summarization metric performed bet-
ter on tasks that required the test takers to
summarize an announcement or lecture (average
?? = 0.653 for ROUGE-1 and ROUGE-SU1-4) rather
than on tasks that required them to describe a pic-
ture sequence (average ?? = 0.437, Mann-Whitney-
Wilcox test, a non-parametric test for comparing
two independent samples, p < 2.2 ? 10
?16
).
4.3 Evaluation of the final model
Analysis by prompt showed that the variants of
ROUGE that included unigram counts (ROUGE and
ROUGE-SU1-4) had the best correlations with hu-
man scores across all prompts. Further improve-
ment in their performance was obtained by count-
ing type frequencies only and by using several ref-
erence summaries. The optimal N references for
these variants of ROUGE varied between prompts,
but never exceeded four which was therefore se-
lected as the optimal N references for this corpus.
Based on these results we computed ROUGE-
1 metrics for all responses in the original train-
ing partition using four randomly selected, highly
scored responses for each prompt and ?types?
method of pre-processing. We then compared
it with two baselines: (1) cosine distance (CVA)
computed using type frequencies only and the
same four references, and (2) na??ve implementa-
tion of ROUGE-1 computed using one randomly
selected reference summary and raw frequencies
(tokens). The newly adjusted version of ROUGE-
1 metrics performed significantly above the base-
lines (using Zou?s method for the comparison of
overlapping correlations with confidence intervals
constructed at ? = 0.001). The correlation coeffi-
cients are shown in Table 3.
Metric ASR output Manual
New ROUGE-1 0.652 0.673
Base ROUGE-1 0.55 0.589
CVA 0.508 0.451
Table 3: Correlation coefficients with human
scores (Spearman?s ?) for the entire training parti-
tion for the newly adjusted version of ROUGE and
the baseline metrics. The table shows the results
for metrics computed based on ASR and manual
transcriptions.
We then trained a standard linear regression
model using the human scores as the dependent
variables and summarization metrics as indepen-
dent variables. The accuracy of prediction was
evaluated using two metrics as suggested, for ex-
ample, by Williamson et al. (2012): quadratic
weighted kappa (?) and Pearson?s correlation co-
efficient (r) between the observed and predicted
scores. For computation of ?, the predicted scores
were trimmed to the range of human scores and
rounded to the nearest integer.
Repeated 10-fold cross-validation on the train-
ing partition showed that a model based on
ROUGE-1 produced averages of r? = 0.65
(? = 0.031) and ?? = 0.54 (? = 0.036). The model
based on a linear combination of several ROUGE
variants using longer n-grams and a recursive fea-
ture elimination (Kuhn and Johnson, 2013, p. 480)
did not show any improvement in the performance
as compared to a model based on a single ROUGE-
1.
Finally, we tested the performance of the met-
rics on an unseen test set that had not been used
for any previous analyses. We tested both the
model based solely on the content metric as well
as on the performance of the content metrics in
combination with 11 other features used for the
automated scoring of spoken responses that mea-
sure pronunciation accuracy, prosody, fluency, and
grammar. These results are presented in Table
4. Note that the performance of the content-only
model based on the new ROUGE-1 was in line
with the estimates obtained on the training set.
Zou?s method for comparing overlapping correla-
tions showed that in all cases, the difference be-
tween the model based on an adjusted ROUGE and
the baselines was significant at ? = 0.001. In line
with previous results, the models based on manual
transcriptions showed better agreement with hu-
man scores than the models based on ASR output.
Table 4 shows that the addition of content met-
rics lead to relatively small increase in the perfor-
mance of the integrated models. This is due to the
fact that for most speakers different aspects of pro-
ficiency tend to be correlated. For example, more
fluent speakers also achieve higher ROUGE scores
(the correlation between ROUGE and pronuncia-
tion accuracy (Chen et al., 2009) is r = 0.62). As
a result, a model which measures only one as-
pect of performance such as fluency may some-
times reach near optimal performance and adding
further predictors leads to a relatively small gain.
When interpreting these results, it is important to
bear in mind that empirical performance is only
74
Model
ASR Manual
r ? r ?
Content only
CVA 0.492 0.340 0.469 0.303
Base ROUGE 0.587 0.440 0.632 0.489
New ROUGE 0.655 0.540 0.700 0.590
Integrated model
No content 0.678 0.565 0.678 0.565
CVA 0.691 0.600 0.698 0.602
Base ROUGE 0.700 0.597 0.719 0.610
New ROUGE 0.715 0.617 0.738 0.652
Table 4: Performance of the linear regression
model based on one content metric and an ?inte-
grated? model based on 11 features that measure
pronunciation, fluency, and grammar before and
after the addition of ?Base ROUGE,? ?CVA? or ?New
ROUGE.? The table shows the correlation coeffi-
cients (Pearson?s r) and quadratic weighted kappa
kappas (?) between the predicted scores and hu-
man ratings for the unseen test set. The agree-
ment between the two expert raters on this dataset
is ? = 0.69.
one aspect of evaluation of automated scoring sys-
tems. In addition to high agreement with hu-
man scores, operational automatic scoring systems
also need to show good construct representation
by covering different aspects of speaker perfor-
mance (Williamson et al., 2012). This requirement
ensures the validity of automated scores and pre-
vents future test-takers from fine-tuning their per-
formance to one particular feature measured by the
scoring system. Therefore the addition of ROUGE
to the automated scoring model serves both goals:
it improves the agreement with human raters and
also expands the construct coverage of the model.
5 Discussion
Summarization metrics can be successfully used
to evaluate spoken summaries in the context of
language assessment. Although the na??ve imple-
mentation of ROUGE had good agreement with the
scores assigned by human raters, several modifica-
tions led to a further increase in the performance.
Some of our findings show common patterns
with what has previously been reported for written
summaries. ROUGE-1, ROUGE-SU4 and ROUGE-
2 are the three variants of ROUGE most com-
monly used for the evaluation of automatic text
summaries. Our results showed that the first two
of these measures (ROUGE-1 and ROUGE-SU4)
were also most suitable for content assessment
of spoken responses. We note that both of these
measures include unigram counts. More recently,
Rankel et al. (2013) and Owczarzak et al. (2012)
reported that metrics based on longer consecutive
n-grams or linear combinations of different vari-
ants are more accurate. We did not find this for our
data. Since our data represents abstractive sum-
maries, poor performances of longer n-grams is
not surprising. Finally, as in the case of written
summaries, there was no effect of lemmatization
while the removal of stop-words sometimes led to
a decrease in performance.
Similar to written summaries, the use of more
than one reference summary improved the perfor-
mance. We found that the optimal number of refer-
ence summaries varied between prompts and met-
rics. For ROUGE-1, this number never exceeded
four across all prompts in our corpus. Further-
more, we found that the choice of reference sum-
maries from the pool of highly scored responses
had no significant effect on the performance of the
metric.
In addition to good agreement with human
scores, metrics used for automated scoring also
need to match the construct of interest, as defined
by the assessment program (Williamson et al.,
2012). The scoring guidelines for the tasks used
in this paper ask raters to judge whether the key
information contained in the prompt has been con-
veyed accurately. A notable difference between
ROUGE and previously used metrics is that as a
recall measure, ROUGE does not penalize for the
lack of precision. Our results suggest that a recall-
oriented approach has better agreement with hu-
man judgments than cosine distance which com-
bines both precision and recall.
Recall-based approaches are sensitive to the
length of candidate responses. In the case of auto-
matic summary evaluation, the length of the sum-
maries is limited to a predefined number of words.
In this data, the length of the responses is limited
more loosely by the time available to record the
response and the actual number of words varied
between the responses. Therefore, a recall-based
approach may produce inflated scores by assign-
ing higher metric values to a response which con-
tains multiple repetitions of the same n-gram as
long as the n-gram occurs several times in the ref-
erence response. The common occurrence of re-
75
pairs and repetitions in spoken speech further ag-
gravates this problem further. We addressed this
issue by only counting type frequencies, which
also improved agreement with human judgments.
The adjusted metric had better agreement with
human judgments than other ?bag-of-words? ap-
proaches such as the cosine-based measure com-
monly used for content scoring that requires a
much larger set of model responses than ROUGE.
It also performed equally well on human and ASR
transcriptions and did not require any manual an-
notation of the data. We also found that the per-
formance of ROUGE depended on the task: we
obtained better agreement for tasks that required
the student to summarize a stimulus rather than
tasks that required the student to describe a se-
quence of pictures. While in both cases the stu-
dents produced short summary-like texts, the pic-
ture description task allowed for greater variabil-
ity between the responses than the summarization
task and, therefore, recall-oriented comparisons
with highly-scored responses showed less agree-
ment with human scores.
As a ?bag-of-words? approach, ROUGE-1 has
the same shortcomings as other methods discussed
in Section 2.2 in that it doesn?t distinguish be-
tween syntactic roles. While variants based on
longer n-grams could in theory address this, our
results showed that neither a linear nor a geomet-
ric combination of these variants with ROUGE-1
improved agreement with human scores. This is-
sue has also been acknowledged in the context of
non-extractive text summarization and new met-
rics such as AutoSummEng (Giannakopoulos and
Karkaletsis, 2011) have been developed to address
it. Future research will include the conceptualiza-
tion and development of metrics that can address
the content accuracy of spoken summaries beyond
the ?bag-of-words? approach.
6 Conclusion
In this paper we applied ROUGE, a recall-based
metrics for evaluation of written summaries to the
automatic assessment of spoken summaries pro-
duced by non-native speakers of English. We per-
formed a thorough evaluation of different types of
ROUGE by varying the length of n-grams, vari-
ous methods of frequency computation, and text-
preprocessing. We also explored the effect of the
number of reference summaries. We found that
the standard baseline implementation of ROUGE-1
computed over the output of the automated speech
recognizer showed good agreement with expert
ratings and performed better than the cosine sim-
ilarity measure commonly used for the evaluation
content of spoken responses. A further increase in
agreement with human ratings could be achieved
by using types instead of tokens for the frequency
computation of both candidate and reference sum-
maries. We also found that the use of several refer-
ence summaries improves the performance of the
metric, but only four reference summaries were
necessary to achieve reliable results.
Acknowledgments
We would like to thank Keelan Evanini, Nitin
Madnani, Xinhao Wang, Derrick Higgins and
three anonymous reviewers for their helpful com-
ments and suggestions and Ren?e Lawless for edit-
ing assistance.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater V. 2. The Journal of Technology,
Learning and Assessment, 4(3):1?30.
Thomas Baguley. 2012. Serious Stats: A guide to ad-
vanced statistics for the behavioral sciences. Pal-
grave Macmillan.
Douglas Biber, Susan M. Conrad, Randi Reppen, Pat
Byrd, Marie Helt, Victoria Clark, Viviana Cortes,
Eniko Csomay, and Alfredo Urzua. 2004. Rep-
resenting language use in the university: analysis
of TOEFL 2000 Spoken and Written academic lan-
guage corpus. Educational Testing Service, Prince-
ton.
Miao Chen and Klaus Zechner. 2012. Using an on-
tology for improved automated content scoring of
spontaneous non-native speech. In Proceedings of
the 7th Workshop on Building Educational Applica-
tions Using NLP, pages 86?94, Stroudsburg, PA. As-
sociation for Computational Linguistics.
Lei Chen, Klaus Zechner, and Xiaoming Xi. 2009. Im-
proved pronunciation features for construct-driven
assessment of non-native spontaneous speech. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL ?09, pages 442?449, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Lei Chen. 2013. Applying unsupervised learning to
support vector space model based speaking assess-
ment. Proceedings of the 8th Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, Atlanta, Georgia, pages 58?62.
76
Anthony C. Davison and David V. Hinkley. 1997.
Bootstrap Methods and their Application (Cam-
bridge Series in Statistical and Probabilistic Mathe-
matics). Cambridge University Press.
Keelan Evanini and Xinhao Wang. 2013. Automated
speech scoring for non-native middle school stu-
dents with multiple task types. Proceedings of In-
terspeech 2013, Lyon, France, pages 2435?2439.
Peter W. Foltz, Darrell Laham, and Thomas K. Lan-
dauer. 1999. Automated essay scoring: applica-
tions to educational technology. In B. Collis and
R. Oliver, editors, Proceedings of World Confer-
ence on Educational Multimedia, Hypermedia and
Telecommunications 1999, pages 939?944.
George Giannakopoulos and Vangelis Karkaletsis.
2011. AutoSummENG and MeMoG in Evaluat-
ing Guided Summaries. In TAC 2011 Workshop,
Gaithersburg, MD, USA. NIST.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and
David Williamson. 2011. A three-stage approach
to the automated scoring of spontaneous spoken re-
sponses. Computer Speech & Language, 25(2):282?
306, April.
Makoto Hirohata, Yousuke Shinnaka, Koji Iwano, and
Sadaoke Furui. 2005. Sentence extraction-based
presentation summarization techniques and evalua-
tion metrics. In Acoustics, Speech, and Signal Pro-
cessing, 2005. Proceedings. (ICASSP ?05). IEEE In-
ternational Conference on, volume 1, pages 1065?
1068.
Max Kuhn and Kjell Johnson. 2013. Applied Predic-
tive Modeling. Springer.
Chin-Yew Lin and Marina Rey. 2004. ROUGE:
A package for automatic evaluation of summaries.
In Stan Szpakowicz Marie-Francine Moens, edi-
tor, Text Summarization Branches Out: Proceedings
of the ACL-04 Workshop, pages 74?81, Barcelona,
Spain. Association for Computational Linguistics.
Chin-Yew Lin. 2004. Looking for a few good metrics:
Automatic summarization evaluation - how many
samples are enough. In Proceedings of the NTCIR
Workshop, pages 1765?1776, Tokyo.
Annie Louis and A Nenkova. 2013. Automatically
assessing machine summary content without a gold
standard. Computational Linguistics, 39(2):267?
300.
Nitin Madnani, Jill Burstein, John Sabatini, and Tenaha
O?Reilly. 2013. Automated scoring of a summary-
writing task designed to measure reading compre-
hension. In Proceedings of the 8th Workshop on In-
novative Use of NLP for Building Educational Ap-
plications, pages 163?168, Atlanta, Georgia. Asso-
ciation for Computational Linguistics.
Ani Nenkova and Annie Louis. 2008. Can you sum-
marize this? Identifying correlates of input difficulty
for generic multi-document summarization. In Pro-
ceedings of the ACL-08: HLT, pages 825?833. As-
sociation for Computational Linguistics.
Ani Nenkova and Kathleen McKeown. 2011. Auto-
matic summarization. Foundations and Trends in
Information Retrieval, 5(2-3):103?233.
Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An assessment of
the accuracy of automatic evaluation in summariza-
tion. In Proceedings of workshop on evaluation met-
rics and system comparison for automatic summa-
rization., pages 1?9, Stroudsburg, PA. Association
for Computational Linguistics.
Art B. Owen. 2007. The pigeonhole bootstrap. The
Annals of Applied Statistics, 1(2):386?411.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU : a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 40th Annual Meeting of the ACL, pages
311?318, Philadelphia, PA. Association for Compu-
tational Linguistics.
Gerald Penn and Xiaodan Zhu. 2008. A Critical
Reassessment of Evaluation Baselines for Speech
Summarization. In in Proceedings of RANLP work-
shop on Crossing Barriers in Text Summarization
Research, number June, pages 470?478, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Peter A. Rankel, John. M. Conroy, Hoa Trang Dang,
and Ani Nenkova. 2013. A decade of automatic
content evaluation of news summaries: reassessing
the state of the art. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics, Sofia, Bulgaria, August 4-9, 2013, pages
131?136, Sofia. Association for Computational Lin-
guistics.
Rand R. Wilcox. 2009. Comparing Pearson correla-
tions: dealing with heteroscedasticity and nonnor-
mality. Communications in Statistics - Simulation
and Computation, 38(10):2220?2234.
David M. Williamson, Xiaoming Xi, and F. Jay Breyer.
2012. A framework for evaluation and use of au-
tomated scoring. Educational measurement: issues
and practice, 31(1):2?13.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech
scoring. In NAACL HLT ?12 Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 103?111.
Wenting Xiong, Keelan Evanini, Klaus Zechner, and
Lei Chen. 2013. Automated content scoring of
77
spoken responses containing multiple parts with fac-
tual information. In Pierre Badin, Thomas Hue-
ber, G?erard Bailly, Didier Demolin, and Franc?oise
Raby, editors, Proceedings of SLaTE 2013, Greno-
ble, France, pages 137?142, Grenoble.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring
of non-native spontaneous speech in tests of spoken
English. Speech Communication, 51(10):883?895.
Guang Yong Zou. 2007. Toward using confidence
intervals to compare correlations. Psychological
methods, 12(4):399?413.
78
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 134?142,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Automated Scoring of Speaking Items in an Assessment for Teachers of
English as a Foreign Language
Klaus Zechner, Keelan Evanini, Su-Youn Yoon, Lawrence Davis,
Xinhao Wang, Lei Chen, Chong Min Lee, Chee Wee Leong
Educational Testing Service (ETS)
Princeton, NJ 08541, USA
{kzechner,kevanini,syoon,ldavis,xwang002,lchen,clee001,cleong}@ets.org
Abstract
This paper describes an end-to-end proto-
type system for automated scoring of spo-
ken responses in a novel assessment for
teachers of English as a Foreign Language
who are not native speakers of English.
The 21 speaking items contained in the as-
sessment elicit both restricted and moder-
ately restricted responses, and their aim is
to assess the essential speaking skills that
English teachers need in order to be effec-
tive communicators in their classrooms.
Our system consists of a state-of-the-art
automatic speech recognizer; multiple fea-
ture generation modules addressing di-
verse aspects of speaking proficiency, such
as fluency, pronunciation, prosody, gram-
matical accuracy, and content accuracy; a
filter that identifies and flags problematic
responses; and linear regression models
that predict response scores based on sub-
sets of the features. The automated speech
scoring system was trained and evaluated
on a data set involving about 1,400 test
takers, and achieved a speaker-level cor-
relation (when scores for all 21 responses
of a speaker are aggregated) with human
expert scores of 0.73.
1 Introduction
As English has become increasingly important as a
language of international business, trade, science,
and communication, efforts to promote teaching
English as a Foreign Language (EFL) have seen
substantially more emphasis in many non-English-
speaking countries worldwide in recent years. In
addition, the prevailing trend in English pedagogy
has been to promote the use of spoken English in
the classroom, as opposed to the respective native
languages of the EFL learners. However, due to
the high demand for EFL teachers in many coun-
tries, the training of these teachers has not always
caught up with these high expectations, so there is
a need for both governmental and private institu-
tions involved in the employment and training of
EFL teachers to assess their competence in the En-
glish language, as well as in English pedagogy.
Against this background, we developed a lan-
guage assessment for EFL teachers who are not
native speakers of English that addresses the four
basic English language skills of Reading, Listen-
ing, Writing and Speaking. This paper focuses
only on the speaking portion of the English assess-
ment, and, in particular, on the system that we de-
veloped to automatically compute scores for test
takers? spoken responses.
Several significant challenges needed to be ad-
dressed during the course of building this auto-
mated speech scoring system, including, but not
limited to:
? The 21 Speaking items belong to 8 differ-
ent task types with different characteristics;
therefore, we had to select features and build
scoring models for each task type separately.
? The test takers speak a variety of native lan-
guages, and thus have very different non-
native accents in their spoken English. Fur-
thermore, the test takers also exhibit a wide
range of speaking proficiency levels, which
contributes to the diversity of their spoken re-
sponses. Our speech recognizer therefore had
to be trained and adapted to a large database
of non-native speech.
? Since content accuracy is very important for
the types of tasks contained in the test, even
small error rates by the automatic speech
recognition (ASR) system can lead to a no-
ticeable impact on feature performance. This
fact motivated the development of a set of
134
features that are robust to speech recognition
errors.
? A significant amount of responses (more than
7%) exhibit issues that make them hard or
impossible to score automatically, e.g., high
noise levels, background speech, etc. We
therefore implemented a filter to identify
these non-scorable responses automatically.
The paper is organized as follows: Section 2
discusses related work; in Section 3, we present
the data used for system training and evaluation;
Section 4 describes the system architecture of the
automated speech scoring system. We detail the
methods we used to build our system in Section 5,
followed by an overview of the results in Section
6. Section 7 discusses our findings; finally, Sec-
tion 8 concludes the paper.
2 Related Work
Automated speech processing and scoring tech-
nology has been applied to a variety of domains
over the course of the past two decades, includ-
ing evaluation and tutoring of children?s literacy
skills (Mostow et al., 1994), preparation for high
stakes English proficiency tests for institutions of
higher education (Zechner et al., 2009), evalua-
tion of English skills of foreign-based call center
agents (Chandel et al., 2007), and evaluation of
aviation English (Pearson Education, Inc., 2011),
to name a few (for a comprehensive overview, see
(Eskenazi, 2009)).
Most of these applications elicit restricted
speech from the participants, and the most com-
mon item type by far is the Read Aloud, in which
the speaker reads a sentence or collection of sen-
tences out loud. Due to the constrained nature
of this task, it is possible to develop ASR sys-
tems that are relatively accurate, even with heav-
ily accented non-native speech. Several types of
features related to a non-native speaker?s ability
to produce English sounds and speech patterns
effectively have been extracted from these types
of responses. Some of the best performing of
these types of features include pronunciation fea-
tures, such as a phone?s spectral match to na-
tive speaker acoustic models (Witt, 1999) and a
phone?s duration compared to native speaker mod-
els (Neumeyer et al., 2000); fluency features, such
as the rate of speech, mean pause length, and num-
ber of disfluencies (Cucchiarini et al., 2000); and
prosody features, such as F0 and intensity slope
(Hoenig, 2002).
In addition to the large majority of applications
that elicit restricted speech, a small number of ap-
plications have also investigated automated scor-
ing of non-native spontaneous speech, in order
to more fully evaluate a speaker?s communicative
competence (e.g., (Cucchiarini et al., 2002) and
(Zechner et al., 2009)). In these systems, the same
types of pronunciation, fluency, and prosody fea-
tures can be extracted; furthermore, features re-
lated to additional aspects of a speaker?s profi-
ciency in the non-native language can be extracted,
such as vocabulary usage (Yoon et al., 2012), syn-
tactic complexity (Bernstein et al., 2010a; Chen
and Zechner, 2011), and topical content (Xie et al.,
2012).
As described in Section 1, the domain for the
automated speaking assessment investigated in
this study is teachers of EFL around the world.
Based on the fact that many of the item types are
designed to assess the test taker?s ability to pro-
ductively use English constructions and linguis-
tic units that commonly recur in English teach-
ing environments, several of the item types elicit
semi-restricted speech (see Table 1 below for a de-
scription of the different item types). These types
of responses fall somewhere between the heavily
restricted speech elicited by a Read Aloud task
and unconstrained spontaneous speech. In these
semi-restricted responses, the test taker may be
provided with a set of lexical items that should
be used to form a sentence; in addition, the test
taker is often asked to make the sentence conform
to a given grammatical template. Thus, the re-
sponses provided for a given prompt of this type
by multiple different speakers will often overlap
with each other; however, it is not possible to
specify a complete list of all possible responses.
These types of items have only infrequently been
examined in the context of automated speech scor-
ing. Some related item types that have been
explored previously include the Sentence Build
and Short item types described in (Bernstein et
al., 2010b); however, those item types typically
elicited a much narrower range of responses than
the semi-restricted ones in this study.
3 Data
The data used in this study was drawn from a pilot
administration of a language assessment for teach-
135
ers of English as a Foreign Language. This test
is designed to assess the ability of a non-native
teacher of English to use English in classroom set-
tings. The language forms and functions included
in this test are based on the materials included in a
curriculum that the test takers studied prior to tak-
ing the assessment. The assessment includes items
that cover the four language skills: Reading, Lis-
tening, Writing, and Speaking. There are a total of
8 different types of Speaking items included in the
assessment. These can be divided into the follow-
ing two categories, depending on how constrained
the test taker?s response is:
? Restricted Speech: In these item types, all
of the linguistic content expected in the
test taker?s response is presented in the test
prompt, and the test taker is asked to read or
repeat it aloud.
? Semi-restricted Speech: In these item types, a
portion of the linguistic content is presented
in the prompt, and the test taker is required to
provide the remaining content to formulate a
complete response.
Sets of 7 Speaking items are presented to the
test taker in thematic units, called ?lessons?, based
on their instructional goals; in total, each test taker
completed three lessons, and thus responded to 21
Speaking items. Table 1 presents descriptions of
the 8 different item types included in the assess-
ment.
The numbers of responses provided by the test
takers to each type (along with their respective re-
sponse durations) are as follows: four Multiple
Choice (10 seconds each), six Read Aloud (four 40
second responses and two 60 second responses),
two Repeat Aloud (15 seconds each), one Incom-
plete Sentence (20 seconds), one Key Words (15
seconds), five Chart (four 20 seconds and one 40
seconds), one Keyword Chart (15 seconds), and
one Visuals (15 seconds). Thus, each test taker
provided a total of approximately 9 minutes of au-
dio.
The responses were all double-scored by trained
human raters on a three-point scale (1 - 3). For
the Restricted Speech items, the raters assessed
the test taker?s pronunciation, pacing, and intona-
tion. For the Semi-restricted Speech items, the re-
sponses were also scored holistically on a 3-point
scale, but raters were also asked to take into ac-
count the appropriateness of the language used
Restricted Speech
Type Description
Multiple
Choice
(MC)
The test taker selects the correct
option and reads it aloud
Read Aloud
(RA)
The test taker reads aloud a set
of classroom instructions
Repeat
Aloud (RP)
The test taker listens to a student
utterance twice and then repeats
it
Semi-restricted Speech
Type Description
Incomplete
Sentence
(IS)
The test taker is given a sentence
fragment and completes the sen-
tence according to the instruc-
tions
Key Words
(KW)
The test taker uses the key words
provided to speak a sentence as
instructed
Chart (CH) The test taker uses an example
from a language chart and then
formulates a similar sentence us-
ing a given grammatical pattern
Keyword
Chart (KC)
The test taker constructs a sen-
tence using keywords provided
and information in a chart
Visuals (VI) The test taker is given two visu-
als and is asked to give instruc-
tions to students based on the
graphical information
Table 1: Types of speaking items included in the
assessment
(e.g., grammatical accuracy and content correct-
ness) in addition to aspects of fluency and pronun-
ciation. For some responses, the raters were not
able to provide a score on the 1 - 3 scale, e.g.,
because the audio response contained no speech
input, the test taker responded in their native lan-
guage, etc. These responses are labeled NS for
Non-Scoreable.
After receiving scores, all of the responses
were transcribed using standard English orthogra-
phy (disfluencies, such as filled pauses and par-
tial words are also included in the transcriptions).
Then, the responses were partitioned (with no
speaker overlap) into five sets for the training and
evaluation of the ASR system and the linear re-
gression scoring models. The amount of data and
136
human score distributions in each of these parti-
tions are displayed in Table 2.
4 System Architecture
The automated scoring system used for the teach-
ers? spoken language assessment consists of the
following four components, which are invoked
one after the other in a pipeline fashion (ETS
SpeechRater
SM
, (Zechner et al., 2009; Higgins et
al., 2011)):
? an automated speech recognizer, generating
word hypotheses from input audio recordings
of the test takers? responses
? a feature computation module that generates
features based on the ASR output, e.g., mea-
suring fluency, pronunciation, prosody, and
content accuracy
? a filtering model that flags responses that
should not be scored automatically due to is-
sues with audio quality, empty responses, etc.
? linear regression scoring models that predict
the score for each response based on a set of
selected features
Furthermore, we use Praat (Boersma and
Weenick, 2012) to extract power and pitch from
the speech signal; this information is used for
some of the feature computation modules, as well
as for the filtering model.
The ASR is an HMM-based triphone system
trained on approximately 800 hours of non-native
speech from a different data set; a background
Language Model (LM) was also trained on the
same data set. Subsequently, 8 adapted LMs were
trained (with an interpolation weight of 0.9 for the
in-domain data) using the responses in the ASR
Training partition for the 8 different item types
listed in Table 1. The ASR system obtained an
overall word error rate (WER) of 13.0% on the
ASR Evaluation partition and 15.6% on the Model
Evaluation partition. As would be expected, the
ASR system performed best on the responses that
were most restricted by the test item and per-
formed worse on the responses that were less re-
stricted. The WER ranged from 11.4% for the
RA responses to 41.4% for the IS responses in the
Model Evaluation partition.
5 Methodology
5.1 Speech features
The feature computation components of our
speech scoring system compute more than 100
features based on a speaker?s response. They be-
long to the following broad dimensions of speak-
ing proficiency: fluency, pronunciation, prosody,
vocabulary usage, grammatical complexity and
accuracy, and content accuracy (Zechner et al.,
2009; Chen and Yoon, 2012; Chen et al., 2009;
Zechner et al., 2011; Yoon et al., 2012; Yoon and
Bhat, 2012; Zechner and Wang, 2013).
After initial feature generation, we selected a set
of about 10 features for each of the 8 item types,
based on the following considerations
1
(Zechner
et al., 2009; Xi et al., 2008):
? empirical performance, i.e., feature correla-
tion with human scores
? construct
2
relevance, i.e., to what extent the
feature measures aspects of speaking profi-
ciency that are considered to be relevant and
important by content experts
? overall construct coverage, i.e., the feature set
should include features from all relevant con-
struct dimensions
? feature independence, i.e., the inter-
correlation between any two features of the
set should be low
Furthermore, some features were transformed
(e.g., by applying the inverse or log function), in
order to increase the normality of their distribu-
tions (an assumption of linear regression classi-
fiers). All feature values that exceeded a thresh-
old of 4 standard deviations from the mean were
replaced by the respective threshold (outlier trun-
cation).
The composition of feature sets is slightly dif-
ferent for the two item type categories: for the 3
restricted item types, features related to fluency,
pronunciation, prosody and read/repeat accuracy
were chosen, whereas for the 5 semi-restricted
item types, vocabulary and grammar features were
also added to the set. Further, while accuracy
1
While automated feature selection is conceivable in prin-
ciple, in our experience it typically does not result in a feature
set that meets all of these criteria well.
2
A construct is the set of knowledge, skills, and abilities
measured by a test.
137
Partition Spk. Resp. Dur. 1 2 3 NS
ASR Training 773 16,049 116.7 1,587 (9.9) 4,086 (25.5) 8,796 (54.8) 1,580 (9.8)
ASR Development 25 525 3.8 53 (10.1) 133 (25.3) 327 (62.3) 12 (2.3)
ASR Evaluation 25 525 3.8 31 (5.9) 114 (21.7) 326 (62.1) 54 (10.3)
Model Training 300 6,300 45.8 675 (10.7) 1,715 (27.2) 3,577 (56.8) 333 (5.3)
Model Evaluation 300 6,300 45.7 647 (10.3) 1,637 (26.0) 3,487 (55.3) 529 (8.4)
Total 1,423 29,699 215.8 2,993 (9.38) 7,685 (25.14) 16,513 (58.26) 2,508 (7.22)
Table 2: Amount of data contained in each partition (speakers, responses, hours of speech) and distribu-
tion of human scores (percentages of scores per partition in brackets).
features for the restricted items were based only
on string alignment measures, content accuracy
features for the semi-restricted items were more
diverse, e.g., based on regular expressions, key-
words, and language model scores (Zechner and
Wang, 2013). Table 3 lists the features that were
used in the scoring models for restricted and semi-
restricted item types, along with sub-constructs
they measure and their description.
5.2 Filtering model
In order to automatically identify responses that
have technical issues (e.g., loud background noise)
or are otherwise not scorable (e.g., empty re-
sponses), a decision tree-based filtering model was
developed using a combination of features derived
from ASR output and from pitch and energy in-
formation (Yoon et al., 2011; Jeon and Yoon,
2012). The filtering model was tested on the scor-
ing model evaluation data, and obtained an ac-
curacy rate (the exact agreement between the fil-
tering model and a human rater concerning the
distinction between scorable and non-scorable re-
sponses) of 97%; it correctly identified 90% of the
non-scorable responses in the data set with a false
positive rate of 21% (recall=0.90, precision=0.79,
F-score=0.84).
5.3 Scoring models
We used the Model Training set to train 8 linear
regression models for the 8 different item types,
using the previously determined feature sets. We
used the features as independent variables in these
models and the summed scores of two human
raters as the dependent variable. These trained
scoring models were then employed to score re-
sponses of the Model Evaluation data (exclud-
ing responses marked as non-scorable by human
raters) and rounded to the nearest integer to predict
the final scores for each response. These scores
were then evaluated against the first human rater
score (H1).
Item N S-H1 H1-H2 WER (%)
RA 1653 0.34 0.51 11.4
RP 543 0.41 0.73 21.8
MC 1036 0.67 0.83 17.1
CH 1372 0.44 0.67 26.3
KW 275 0.45 0.67 28.7
KC 274 0.57 0.74 28.8
IS 260 0.46 0.69 41.4
VI 272 0.43 0.80 30.4
Table 4: Correlations between system and first hu-
man rater (S-H1) and between two human raters
(H1-H2), for all responses of each item type in the
Model Evaluation partition (N). The last column
provides the average ASR word error rate (WER)
in percent.
Additionally, for responses flagged as non-
scorable by the automatic filtering model, the sec-
ond human rater score (H2) was used as final
item score in order to mimic the operational sce-
nario where human raters score responses that are
flagged by the filtering model.
We also compute the agreement between sys-
tem and human raters based on a set of all 21 re-
sponses of a speaker. Score imputation was used
for responses that were labeled as non-scorable by
both the system and H2; in this case, the response
was given the mean score of the total scorable
responses from the same speaker. Similarly, the
same score imputation rule was applied to the H1
scores.
6 Results
Table 4 presents the Pearson correlation coeffi-
cients between human and automated scores for
the responses from the 8 different item types along
with the human-human correlation for each item
type. Furthermore, we also provide the word error
rates of the ASR system for the same 8 item types
in the last column of the table.
138
Feature Sub-construct Description
Content Ed1 Read/repeat accu-
racy / Fluency
Correctly read words per minute
Content Ed2 Read/repeat accu-
racy
Read/repeat word error rate
Content RegEx Content accuracy Matching of regular expressions
Content WER Content accuracy Response discrepancy from high scoring responses
Content NGram Content accuracy N-grams in response matching high scoring response n-
grams
Fluency Rate Fluency Speaking rate
Fluency Chunk Fluency Average length of contiguous word chunks
Fluency Sil1 Fluency Frequency of long silences
Fluency Sil2 Fluency / Grammar Proportion of long within-clause-silences to all within-
clause-silences
Fluency Sil3 Fluency Mean length of silences within a clause
Fluency Disfl1 Fluency Frequency of interruption points (repair, repetition, false
start)
Fluency Disfl2 Fluency Number of disfluencies per second
Fluency Disfl3 Fluency Frequency of repetitions
Pron Vowels Pronunciation Average vowel duration differences relative to a native-
speaker model
Prosody1 Prosody Percentage of stressed syllables
Prosody2 Prosody Mean deviation of time intervals between stressed syllables
Prosody3 Prosody Mean distance between stressed syllables
Vocab1 Vocabulary / Flu-
ency
Number of word types divided by utterance duration
Grammar POS Grammar Part-of-speech based distributional similarity score be-
tween a response and responses with different score levels
Grammar LM Grammar Global language model score (normalized by response
length)
Table 3: List of features used for item type scoring models, with the sub-constructs they represent and
descriptions.
139
Comparison Pearson r
S-H1 0.725
S-H2 0.742
H1-H2 0.934
Table 5: Speaker-level performance (Pearson r
correlations) computed over the sum of all 21
scores from each speaker, N=272
Sub-construct Restricted Semi-restricted
Content 0.33?0.67 0.34?0.61
Fluency 0.19?0.33 0.20?0.33
Pronunciation 0.20?0.22 0.13?0.31
Prosody 0.18?0.24 0.12?0.27
Grammar ? 0.23?0.49
Vocabulary ? 0.21?0.32
Table 6: Range of Pearson r correlations for dif-
ferent features with human scores (H1) by sub-
construct for restricted and semi-restricted item
types.
Table 5 presents the Pearson correlation coeffi-
cients between the speaker-level scores produced
by the automated scoring system (S) and the two
sets of human scores (H1 and H2). These speaker-
level scores were computed based on the sum of
all 21 scores from each speaker in the Model Eval-
uation partition. Responses that received a non-
scorable rating from the human raters were im-
puted, as described above. Furthermore, 28 speak-
ers were excluded from this analysis because they
had more than 7 non-scorable responses each.
3
Finally, Table 6 provides an overview of Pear-
son correlation ranges with human rater scores
(H1) for the different features used in the scoring
models, summarized by the sub-constructs that the
features represent.
7 Discussion
When looking at Table 4, we see that the inter-rater
reliability for human raters ranges between 0.51
(for RA items) and 0.83 (for MC items). Inter-
rater reliability varies less for the 5 semi-restricted
item types (0.67?0.80), compared to the 3 re-
stricted item types (0.51?0.83). As for automated
score correlations with human raters, the Pearson
r coefficients range from 0.34 (RA) to 0.67 (MC).
3
In an operational setting, these test takers would not re-
ceive a test score; instead, they would have the opportunity to
take the test again.
Again, the variability of Pearson r coefficients is
larger for the 3 restricted item types (0.34?0.67)
than for the 5 semi-restricted item types (0.43?
0.57). The degradation in correlation between the
inter-human results and the machine-human re-
sults varies from 0.16 (MC) to 0.37 (VI).
Speech recognition word error rate does not
seem to have a strong influence on model perfor-
mance (RA items have the lowest WER with S-
H1 r=0.34, but r=0.46 for IS items that have the
highest WER). However, we found other factors
that affect model performance negatively; for ex-
ample, multiple repeats of responses by test tak-
ers contribute to the large performance difference
between S-H1 and H1-H2 for the RP items. In
general, we conjecture that using features for a
larger set of sub-construct areas?in the case of
semi-restricted item types?may contribute to the
lower variation of scoring model performance for
this subset of the data.
As for speaker-level results (Table 5), the over-
all degradation between the inter-human correla-
tion and the system-human correlations is of a
similar magnitude (around 0.2) as observed for
most of the individual item types. Still, the
speaker-level correlation of 0.73 is 0.26 higher
than the average item type correlation between the
system and H1.
When we look into more detail at the Pearson
r correlations between individual features used in
the item type scoring models and human scores
(Table 6), we can see that features related to con-
tent accuracy exhibit a substantially stronger per-
formance (r=0.33?0.67) than features related to
most other sub-constructs of speaking proficiency,
namely fluency, pronunciation, prosody, and vo-
cabulary (r ? 0.2). One exception is features
related to grammar, where correlations with hu-
man scores are as high as 0.49. Since related work
on scoring speech using features indicative of flu-
ency, pronunciation, etc. showed higher correla-
tions (e.g., (Cucchiarini et al., 1997; Franco et al.,
2000; Zechner et al., 2009)), we conjecture that
the reason behind this difference is likely to be
found in the fact that the responses in this assess-
ment for teachers of English are quite short (6?
14 words on average for all items except for Read
Aloud items that are about 46 words on average).
Since content features are less reliant on longer
stretches of speech, they still work fairly well for
most items in our corpus.
140
Finally, while the proportion of words contained
in responses in restricted items is much larger than
those contained in responses in semi-restricted
items, these two item type categories are more
evenly distributed over the whole test, i.e., each
test taker responds to 9 semi-restricted and 12 re-
stricted items, and the item scores are then aggre-
gated for a final score with equal weight given to
each item score.
8 Conclusion
This paper presented an overview of an automated
speech scoring system that was developed for a
language assessment for teachers of English as a
Foreign Language (EFL) whose native language
is not English. We described the main compo-
nents of this prototype system and their perfor-
mance: the ASR system, features generated from
ASR output, a filtering model to flag non-scorable
responses, and finally a set of linear regression
models, one for each of 8 different types of test
items.
We found that overall, the correlation between
our speech scoring system?s predicted scores and
human rater scores range between 0.34 and 0.67,
evaluated on responses from 8 item types. Further-
more, we found that correlations based on com-
plete sets of 21 spoken responses per test taker im-
prove to around r = 0.73.
Given the many significant challenges of this
work, including 8 different item types in the as-
sessment, responses from speakers from different
native languages and speaking proficiency levels,
sub-optimal audio conditions for a part of the data,
and a relatively small data set for both ASR system
adaptation and linear regression model training,
we find that the overall performance achieved by
our automated speech scoring system was a good
starting point for an eventual deployment in a low-
stakes assessment context.
Future work will aim at improving the perfor-
mance of the prediction models by the addition of
more features addressing different aspects of the
construct as well as an improved filtering model
for flagging the different types of problematic re-
sponses. Furthermore, agreement between human
raters, in particular for read-aloud items, could be
improved by refining rater rubrics and additional
rater training and monitoring.
Acknowledgments
The authors would like to thank Anastassia Louk-
ina and Jidong Tao for their comments on an ear-
lier version of this paper, and are also indebted
to the anonymous reviewers of BEA-9 and ASRU
2013 for their valuable comments and suggestions.
References
Jared Bernstein, Jian Cheng, and Masanori Suzuki.
2010a. Fluency and structural complexity as pre-
dictors of L2 oral proficiency. In Proceedings of In-
terspeech.
Jared Bernstein, Alistair Van Moere, and Jian Cheng.
2010b. Validating automated speaking tests. Lan-
guage Testing, 27(3):355?377.
Paul Boersma and David Weenick. 2012. Praat: Doing
phonetics by computer, version 5.3.32. http://
www.praat.org.
Abhishek Chandel, Abhinav Parate, Maymon Ma-
dathingal, Himanshu Pant, Nitendra Rajput, Shajith
Ikbal, Om Deshmuck, and Ashish Verma. 2007.
Sensei: Spoken language assessment for call cen-
ter agents. In Proceedings of the IEEE Workshop on
Automatic Speech Recognition and Understanding
(ASRU).
Lei Chen and Su-Youn Yoon. 2012. Application of
structural events detected on ASR outputs for auto-
mated speaking assessment. In Proceedings of In-
terspeech.
Miao Chen and Klaus Zechner. 2011. Computing
and evaluating syntactic complexity features for au-
tomated scoring of spontaneous non-native speech.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, pages 722?
731.
Lei Chen, Klaus Zechner, and Xiaoming Xi. 2009. Im-
proved pronunciation features for construct-driven
assessment of non-native spontaneous speech. In
Proceedings of NAACL-HLT.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 1997.
Automatic evaluation of Dutch pronunciation by us-
ing speech recognition technology. In Proceedings
of the IEEE Workshop on Auotmatic Speech Recog-
nition and Understanding (ASRU).
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learn-
ers? fluency by means of automatic speech recogni-
tion technology. Journal of the Acoustical Society of
America, 107(2):989?999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learn-
ers? fluency: Comparisons between read and spon-
taneous speech. Journal of the Acoustical Society of
America, 111(6):2862?2873.
141
Maxine Eskenazi. 2009. An overview of spoken lan-
guage technology for education. Speech Communi-
cation, 51(10):832?844.
Horacio Franco, Leonardo Neumeyer, Vassilios Di-
galakis, and Orith Ronen. 2000. Combination of
machine scores for automatic grading of pronuncia-
tion quality. Speech Communication, 30(1-2):121?
130.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and
David M. Williamson. 2011. A three-stage ap-
proach to the automated scoring of spontaneous spo-
ken responses. Computer Speech and Language,
25(2):282?306.
Florian Hoenig. 2002. Automatic assessment of non-
native prosody ? Annotation, modelling, and evalu-
ation. In Proceedings of the International Sympo-
sium on Automatic Detection of Errors in Pronun-
ciation Training (ISADEPT), pages 21?30, Stock-
holm, Sweden.
Je Hun Jeon and Su-Youn Yoon. 2012. Acoustic
feature-based non-scorable response detection for an
automated speaking proficiency assessment. In Pro-
ceedings of Interspeech.
Jack Mostow, Steven F. Roth, Alexander G. Haupt-
mann, and Matthew Kane. 1994. A prototype read-
ing coach that listens. In Proceedings of the Twelfth
National Conference on Artificial Intelligence.
Leonardo Neumeyer, Horacio Franco, Vassilios Di-
galakis, and Mitchel Weintraub. 2000. Automatic
scoring of pronunciation quality. Speech Communi-
cation, 30:83?93.
Pearson Education, Inc. 2011. Versant
TM
Aviation English Test. http://www.
versanttest.com/technology/
VersantAviationEnglishTestValidation.
pdf.
Silke Witt. 1999. Use of speech recognition in
computer-assisted language learning. Ph.D. thesis,
Cambridge University.
Xiaoming Xi, Derrick Higgins, Klaus Zechner, and
David M. Williamson. 2008. Automated scoring of
spontaneous speech using SpeechRater v1.0. Edu-
cational Testing Service Research Report RR-08-62.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech
scoring. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 103?111, Montr?eal, Canada. Asso-
ciation for Computational Linguistics.
Su-Youn Yoon and Suma Bhat. 2012. Assessment of
ESL learners? syntactic competence based on sim-
ilarity measures. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 600?608, Jeju Island, Korea.
Association for Computational Linguistics.
Su-Youn Yoon, Keelan Evanini, and Klaus Zechner.
2011. Non-scorable response detection for auto-
mated speaking proficiency assessment. In Proceed-
ings of NAACL-HLT Workshop on Innovative Use of
NLP for Building Educational Applications.
Su-Youn Yoon, Suma Bhat, and Klaus Zechner. 2012.
Vocabulary profile as a measure of vocabulary so-
phistication. In Proceedings of the 7th Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, NAACL-HLT, Montr?eal, Canada. Associ-
ation for Computational Linguistics.
Klaus Zechner and Xinhao Wang. 2013. Automated
content scoring of spoken responses in an assess-
ment for teachers of english. In Proceedings of
the 8th Workshop on Innovative Use of NLP for
Building Educational Applications, NAACL-HLT,
Atlanta. Association for Computational Linguistics.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring
of non-native spontaneous speech in tests of spoken
English. Speech Communication, 51(10):883?895.
Klaus Zechner, Xiaoming Xi, and Lei Chen. 2011.
Evaluating prosodic features for automated scoring
of non-native read speech. In Proceedings of the
IEEE Workshop on Automatic Speech Recognition
and Understanding (ASRU).
142
