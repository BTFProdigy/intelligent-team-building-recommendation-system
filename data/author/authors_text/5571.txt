Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 100?108,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Bayesian Unsupervised Word Segmentation with
Nested Pitman-Yor Language Modeling
Daichi Mochihashi Takeshi Yamada Naonori Ueda
NTT Communication Science Laboratories
Hikaridai 2-4, Keihanna Science City, Kyoto, Japan
{daichi,yamada,ueda}@cslab.kecl.ntt.co.jp
Abstract
In this paper, we propose a new Bayesian
model for fully unsupervised word seg-
mentation and an efficient blocked Gibbs
sampler combined with dynamic program-
ming for inference. Our model is a nested
hierarchical Pitman-Yor language model,
where Pitman-Yor spelling model is em-
bedded in the word model. We confirmed
that it significantly outperforms previous
reported results in both phonetic tran-
scripts and standard datasets for Chinese
and Japanese word segmentation. Our
model is also considered as a way to con-
struct an accurate word n-gram language
model directly from characters of arbitrary
language, without any ?word? indications.
1 Introduction
?Word? is no trivial concept in many languages.
Asian languages such as Chinese and Japanese
have no explicit word boundaries, thus word seg-
mentation is a crucial first step when processing
them. Even in western languages, valid ?words?
are often not identical to space-separated tokens.
For example, proper nouns such as ?United King-
dom? or idiomatic phrases such as ?with respect
to? actually function as a single word, and we of-
ten condense them into the virtual words ?UK?
and ?w.r.t.?.
In order to extract ?words? from text streams,
unsupervised word segmentation is an important
research area because the criteria for creating su-
pervised training data could be arbitrary, and will
be suboptimal for applications that rely on seg-
mentations. It is particularly difficult to create
?correct? training data for speech transcripts, col-
loquial texts, and classics where segmentations are
often ambiguous, let alne is impossible for un-
known languages whose properties computational
linguists might seek to uncover.
From a scientific point of view, it is also inter-
esting because it can shed light on how children
learn ?words? without the explicitly given bound-
aries for every word, which is assumed by super-
vised learning approaches.
Lately, model-based methods have been intro-
duced for unsupervised segmentation, in particu-
lar those based on Dirichlet processes on words
(Goldwater et al, 2006; Xu et al, 2008). This
maximizes the probability of word segmentation
w given a string s :
w? = argmax
w
p(w|s) . (1)
This approach often implicitly includes heuristic
criteria proposed so far1, while having a clear sta-
tistical semantics to find the most probable word
segmentation that will maximize the probability of
the data, here the strings.
However, they are still na??ve with respect to
word spellings, and the inference is very slow ow-
ing to inefficient Gibbs sampling. Crucially, since
they rely on sampling a word boundary between
two neighboring words, they can leverage only up
to bigram word dependencies.
In this paper, we extend this work to pro-
pose a more efficient and accurate unsupervised
word segmentation that will optimize the per-
formance of the word n-gram Pitman-Yor (i.e.
Bayesian Kneser-Ney) language model, with an
accurate character ?-gram Pitman-Yor spelling
model embedded in word models. Further-
more, it can be viewed as a method for building
a high-performance n-gram language model di-
rectly from character strings of arbitrary language.
It is carefully smoothed and has no ?unknown
words? problem, resulting from its model struc-
ture.
This paper is organized as follows. In Section 2,
1For instance, TANGO algorithm (Ando and Lee, 2003)
essentially finds segments such that character n-gram proba-
bilities are maximized blockwise, averaged over n.
100
(a) Generating n-gram distributions G hierarchically
from the Pitman-Yor process. Here, n = 3.
(b) Equivalent representation using a hierarchical Chinese
Restaurant process. Each word in a training text is a ?customer?
shown in italic, and added to the leaf of its two words context.
Figure 1: Hierarchical Pitman-Yor Language Model.
we briefly describe a language model based on the
Pitman-Yor process (Teh, 2006b), which is a gen-
eralization of the Dirichlet process used in previ-
ous research. By embedding a character n-gram
in word n-gram from a Bayesian perspective, Sec-
tion 3 introduces a novel language model for word
segmentation, which we call the Nested Pitman-
Yor language model. Section 4 describes an ef-
ficient blocked Gibbs sampler that leverages dy-
namic programming for inference. In Section 5 we
describe experiments on the standard datasets in
Chinese and Japanese in addition to English pho-
netic transcripts, and semi-supervised experiments
are also explored. Section 6 is a discussion and
Section 7 concludes the paper.
2 Pitman-Yor process and n-gram
models
To compute a probability p(w|s) in (1), we adopt
a Bayesian language model lately proposed by
(Teh, 2006b; Goldwater et al, 2005) based on
the Pitman-Yor process, a generalization of the
Dirichlet process. As we shall see, this is a
Bayesian theory of the best-performing Kneser-
Ney smoothing of n-grams (Kneser and Ney,
1995), allowing an integrated modeling from a
Bayesian perspective as persued in this paper.
The Pitman-Yor (PY) process is a stochastic
process that generates discrete probability distri-
bution G that is similar to another distribution G0,called a base measure. It is written as
G ? PY(G0, d, ?) , (2)
where d is a discount factor and ? controls how
similar G is to G0 on average.
Suppose we have a unigram word distribution
G1 ={ p(?) } where ? ranges over each word in thelexicon. The bigram distribution G2 = { p(?|v) }
given a word v is different from G1, but will besimilar to G1 especially for high frequency words.Therefore, we can generate G2 from a PY pro-cess of base measure G1, as G2 ? PY(G1, d, ?).Similarly, trigram distribution G3 = { p(?|v?v) }given an additional word v? is generated as G3 ?
PY(G2, d, ?), and G1, G2, G3 will form a treestructure shown in Figure 1(a).
In practice, we cannot observe G directly be-
cause it will be infinite dimensional distribution
over the possible words, as we shall see in this
paper. However, when we integrate out G it is
known that Figure 1(a) can be represented by an
equivalent hierarchical Chinese Restaurant Pro-
cess (CRP) (Aldous, 1985) as in Figure 1(b).
In this representation, each n-gram context h
(including the null context  for unigrams) is
a Chinese restaurant whose customers are the
n-gram counts c(w|h) seated over the tables
1 ? ? ? thw. The seatings has been incrementallyconstructed by choosing the table k for each count
in c(w|h) with probability proportional to
{
chwk ? d (k = 1, ? ? ? , thw)
? + d?th? (k = new) ,
(3)
where chwk is the number of customers seated attable k thus far and th? = ?w thw is the total num-ber of tables in h. When k = new is selected,
thw is incremented, and this means that the countwas actually generated from the shorter context h?.
Therefore, in that case a proxy customer is sent to
the parent restaurant and this process will recurse.
For example, if we have a sentence ?she will
sing? in the training data for trigrams, we add each
word ?she? ?will? ?sing? ?$? as a customer to its
two preceding words context node, as described
in Figure 1(b). Here, ?$? is a special token rep-
resenting a sentence boundary in language model-
101
ing (Brown et al, 1992).
As a result, the n-gram probability of this hier-
archical Pitman-Yor language model (HPYLM) is
recursively computed as
p(w|h) = c(w|h)?d?thw?+c(h) +
?+d?th?
?+c(h) p(w|h
?),
(4)
where p(w|h?) is the same probability using a
(n?1)-gram context h?. When we set thw ? 1, (4)recovers a Kneser-Ney smoothing: thus a HPYLM
is a Bayesian Kneser-Ney language model as well
as an extension of the hierarchical Dirichlet Pro-
cess (HDP) used in Goldwater et al (2006). ?, d
are hyperparameters that can be learned as Gamma
and Beta posteriors, respectively, given the data.
For details, see Teh (2006a).
The inference of this model interleaves adding
and removing a customer to optimize thw, d, and
? using MCMC. However, in our case ?words?
are not known a priori: the next section describes
how to accomplish this by constructing a nested
HPYLM of words and characters, with the associ-
ated inference algorithm.
3 Nested Pitman-Yor Language Model
Thus far we have assumed that the unigram G1is already given, but of course it should also be
generated as G1 ? PY(G0, d, ?).
Here, a problem occurs: What should we use for
G0, namely the prior probabilities over words2?If a lexicon is finite, we can use a uniform prior
G0(w) = 1/|V | for every word w in lexicon V .However, with word segmentation every substring
could be a word, thus the lexicon is not limited but
will be countably infinite.
Building an accurate G0 is crucial for wordsegmentation, since it determines how the possi-
ble words will look like. Previous work using a
Dirichlet process used a relatively simple prior for
G0, namely an uniform distribution over charac-ters (Goldwater et al, 2006), or a prior solely de-
pendent on word length with a Poisson distribution
whose parameter is fixed by hand (Xu et al, 2008).
In contrast, in this paper we use a simple but
more elaborate model, that is, a character n-gram
language model that also employs HPYLM. This
is important because in English, for example,
words are likely to end in ??tion? and begin with
2Note that this is different from unigrams, which are pos-
terior distribution given data.
Figure 2: Chinese restaurant representation of our
Nested Pitman-Yor Language Model (NPYLM).
?re??, but almost never end in ??tio? nor begin with
?sre?? 3.
Therefore, we use
G0(w) = p(c1 ? ? ? ck) (5)
=
k
?
i=1
p(ci|c1 ? ? ? ci?1) (6)
where string c1 ? ? ? ck is a spelling of w, and
p(ci|c1 ? ? ? ci?1) is given by the character HPYLMaccording to (4).
This language model, which we call Nested
Pitman-Yor Language Model (NPYLM) hereafter,
is the hierarchical language model shown in Fig-
ure 2, where the character HPYLM is embedded
as a base measure of the word HPYLM.4 As the
final base measure for the character HPYLM, we
used a uniform prior over the possible characters
of a given language. To avoid dependency on n-
gram order n, we actually used the ?-gram lan-
guage model (Mochihashi and Sumita, 2007), a
variable order HPYLM, for characters. However,
for generality we hereafter state that we used the
HPYLM. The theory remains the same for ?-
grams, except sampling or marginalizing over n
as needed.
Furthermore, we corrected (5) so that word
length will have a Poisson distribution whose pa-
rameter can now be estimated for a given language
and word type. We describe this in detail in Sec-
tion 4.3.
Chinese Restaurant Representation
In our NPYLM, the word model and the charac-
ter model are not separate but connected through
a nested CRP. When a word w is generated from
its parent at the unigram node, it means that w
3Imagine we try to segment an English character string
?itisrecognizedasthe? ? ? .?
4Strictly speaking, this is not ?nested? in the sense of a
Nested Dirichlet process (Rodriguez et al, 2008) and could
be called ?hierarchical HPYLM?, which denotes another
model for domain adaptation (Wood and Teh, 2008).
102
is drawn from the base measure, namely a char-
acter HPYLM. Then we divide w into characters
c1 ? ? ? ck to yield a ?sentence? of characters andfeed this into the character HPYLM as data.
Conversely, when a table becomes empty, this
means that the data associated with the table are
no longer valid. Therefore we remove the corre-
sponding customers from the character HPYLM
using the inverse procedure of adding a customer
in Section 2.
All these processes will be invoked when a
string is segmented into ?words? and customers
are added to the leaves of the word HPYLM. To
segment a string into ?words?, we used efficient
dynamic programming combined with MCMC, as
described in the next section.
4 Inference
To find the hidden word segmentation w of a string
s = c1 ? ? ? cN , which is equivalent to the vector ofbinary hidden variables z = z1 ? ? ? zN , the sim-plest approach is to build a Gibbs sampler that ran-
domly selects a character ci and draw a binary de-cision zi as to whether there is a word boundary,and then update the language model according to
the new segmentation (Goldwater et al, 2006; Xu
et al, 2008). When we iterate this procedure suf-
ficiently long, it becomes a sample from the true
distribution (1) (Gilks et al, 1996).
However, this sampler is too inefficient since
time series data such as word segmentation have a
very high correlation between neighboring words.
As a result, the sampler is extremely slow to con-
verge. In fact, (Goldwater et al, 2006) reports that
the sampler would not mix without annealing, and
the experiments needed 20,000 times of sampling
for every character in the training data.
Furthermore, it has an inherent limitation that
it cannot deal with larger than bigrams, because it
uses only local statistics between directly contigu-
ous words for word segmentation.
4.1 Blocked Gibbs sampler
Instead, we propose a sentence-wise Gibbs sam-
pler of word segmentation using efficient dynamic
programming, as shown in Figure 3.
In this algorithm, first we randomly select a
string, and then remove the ?sentence? data of its
word segmentation from the NPYLM. Sampling
a new segmentation, we update the NPYLM by
adding a new ?sentence? according to the new seg-
1: for j = 1 ? ? ? J do
2: for s in randperm (s1, ? ? ? , sD) do
3: if j >1 then
4: Remove customers of w(s) from ?
5: end if
6: Draw w(s) according to p(w|s,?)
7: Add customers of w(s) to ?
8: end for
9: Sample hyperparameters of ?
10: end for
Figure 3: Blocked Gibbs Sampler of NPYLM ?.
mentation. When we repeat this process, it is ex-
pected to mix rapidly because it implicitly consid-
ers all possible segmentations of the given string
at the same time.
This is called a blocked Gibbs sampler that sam-
ples z block-wise for each sentence. It has an ad-
ditional advantage in that we can accommodate
higher-order relationships than bigrams, particu-
larly trigrams, for word segmentation. 5
4.2 Forward-Backward inference
Then, how can we sample a segmentation w for
each string s? In accordance with the Forward fil-
tering Backward sampling of HMM (Scott, 2002),
this is achieved by essentially the same algorithm
employed to sample a PCFG parse tree within
MCMC (Johnson et al, 2007) and grammar-based
segmentation (Johnson and Goldwater, 2009).
Forward Filtering. For this purpose, we main-
tain a forward variable ?[t][k] in the bigram case.
?[t][k] is the probability of a string c1 ? ? ? ct withthe final k characters being a word (see Figure 4).
Segmentations before the final k characters are
marginalized using the following recursive rela-
tionship:
?[t][k] =
t?k
?
j=1
p(ctt?k+1|ct?kt?k?j+1)??[t?k][j] (7)
where ?[0][0] = 1 and we wrote cn ? ? ? cm as cmn .6The rationale for (7) is as follows. Since main-
taining binary variables z1, ? ? ? , zN is equivalentto maintaining a distance to the nearest backward
5In principle fourgrams or beyond are also possible, but
will be too complex while the gain will be small. For this
purpose, Particle MCMC (Doucet et al, 2009) is promising
but less efficient in a preliminary experiment.
6As Murphy (2002) noted, in semi-HMM we cannot use a
standard trick to avoid underflow by normalizing ?[t][k] into
p(k|t), since the model is asynchronous. Instead we always
compute (7) using logsumexp().
103
Figure 4: Forward filtering of ?[t][k] to marginal-
ize out possible segmentations j before t?k.
1: for t = 1 to N do
2: for k = max(1, t?L) to t do
3: Compute ?[t][k] according to (7).
4: end for
5: end for
6: Initialize t? N , i? 0, w0 ? $
7: while t > 0 do
8: Draw k ? p(wi|ctt?k+1,?) ? ?[t][k]
9: Set wi ? ctt?k+1
10: Set t? t? k, i? i + 1
11: end while
12: Return w = wi, wi?1, ? ? ? , w1.
Figure 5: Forward-Backward sampling of word
segmentation w. (in bigram case)
word boundary for each t as qt, we can write
?[t][k]=p(ct1, qt =k) (8)
=
?
j
p(ct1, qt =k, qt?k =j) (9)
=
?
j
p(ct?k1 , ctt?k+1, qt =k, qt?k =j)(10)
=
?
j
p(ctt?k+1|ct?k1 )p(ct?k1 , qt?k =j)(11)
=
?
j
p(ctt?k+1|ct?k1 )?[t?k][j] , (12)
where we used conditional independency of qtgiven qt?k and uniform prior over qt in (11) above.
Backward Sampling. Once the probability ta-
ble ?[t][k] is obtained, we can sample a word seg-
mentation backwards. Since ?[N ][k] is a marginal
probability of string cN1 with the last k charac-ters being a word, and there is always a sentence
boundary token $ at the end of the string, with
probability proportional to p($|cNN?k)??[N ][k] wecan sample k to choose the boundary of the final
word. The second final word is similarly sampled
using the probability of preceding the last word
just sampled: we continue this process until we
arrive at the beginning of the string (Figure 5).
Trigram case. For simplicity, we showed the
algorithm for bigrams above. For trigrams, we
maintain a forward variable ?[t][k][j], which rep-
resents a marginal probability of string c1 ? ? ? ctwith both the final k characters and further j
characters preceding it being words. Forward-
Backward algorithm becomes complicated thus
omitted, but can be derived following the extended
algorithm for second order HMM (He, 1988).
Complexity This algorithm has a complexity of
O(NL2) for bigrams and O(NL3) for trigrams
for each sentence, where N is the length of the
sentence and L is the maximum allowed length of
a word (? N ).
4.3 Poisson correction
As Nagata (1996) noted, when only (5) is used in-
adequately low probabilities are assigned to long
words, because it has a largely exponential dis-
tribution over length. To correct this, we assume
that word length k has a Poisson distribution with
a mean ?:
Po(k|?) = e?? ?
k
k! . (13)
Since the appearance of c1 ? ? ? ck is equivalentto that of length k and the content, by making the
character n-gram model explicit as ? we can set
p(c1 ? ? ? ck) = p(c1 ? ? ? ck, k) (14)
= p(c1 ? ? ? ck, k|?)p(k|?) Po(k|?) (15)
where p(c1 ? ? ? ck, k|?) is an n-gram probabil-ity given by (6), and p(k|?) is a probability
that a word of length k will be generated from
?. While previous work used p(k|?) = (1 ?
p($))k?1p($), this is only true for unigrams. In-
stead, we employed a Monte Carlo method that
generates words randomly from ? to obtain the
empirical estimates of p(k|?).
Estimating ?. Of course, we do not leave ? as a
constant. Instead, we put a Gamma distribution
p(?) = Ga(a, b) = b
a
?(a)?
a?1e?b? (16)
to estimate ? from the data for given language
and word type.7 Here, ?(x) is a Gamma function
and a, b are the hyperparameters chosen to give a
nearly uniform prior distribution.8
7We used different ? for different word types, such as dig-
its, alphabets, hiragana, CJK characters, and their mixtures.
W is a set of words of each such type, and (13) becomes a
mixture of Poisson distributions in this case.
8In the following experiments, we set a=0.2, b=0.1.
104
Denoting W as a set of ?words? obtained from
word segmentation, the posterior distribution of ?
used for (13) is
p(?|W ) ? p(W |?)p(?)
= Ga
(
a+
?
w?W
t(w)|w|, b+
?
w?W
t(w)
)
, (17)
where t(w) is the number of times word w is gen-
erated from the character HPYLM, i.e. the number
of tables tw for w in word unigrams. We sampled
? from this posterior for each Gibbs iteration.
5 Experiments
To validate our model, we conducted experiments
on standard datasets for Chinese and Japanese
word segmentation that are publicly available, as
well as the same dataset used in (Goldwater et al,
2006). Note that NPYLM maximizes the probabil-
ity of strings, equivalently, minimizes the perplex-
ity per character. Therefore, the recovery of the
?ground truth? that is not available for inference is
a byproduct in unsupervised learning.
Since our implementation is based on Unicode
and learns all hyperparameters from the data, we
also confirmed that NPYLM segments the Arabic
Gigawords equally well.
5.1 English phonetic transcripts
In order to directly compare with the previously
reported result, we first used the same dataset
as Goldwater et al (2006). This dataset con-
sists of 9,790 English phonetic transcripts from
CHILDES data (MacWhinney and Snow, 1985).
Since our algorithm converges rather fast, we
ran the Gibbs sampler of trigram NPYLM for 200
iterations to obtain the results in Table 1. Among
the token precision (P), recall (R), and F-measure
(F), the recall is especially higher to outperform
the previous result based on HDP in F-measure.
Meanwhile, the same measures over the obtained
lexicon (LP, LR, LF) are not always improved.
Moreover, the average length of words inferred
was surprisingly similar to ground truth: 2.88,
while the ground truth is 2.87.
Table 2 shows the empirical computational time
needed to obtain these results. Although the con-
vergence in MCMC is not uniquely identified, im-
provement in efficiency is also outstanding.
5.2 Chinese and Japanese word segmentation
To show applicability beyond small phonetic tran-
scripts, we used standard datasets for Chinese and
Model P R F LP LR LF
NPY(3) 74.8 75.2 75.0 47.8 59.7 53.1
NPY(2) 74.8 76.7 75.7 57.3 56.6 57.0
HDP(2) 75.2 69.6 72.3 63.5 55.2 59.1
Table 1: Segmentation accuracies on English pho-
netic transcripts. NPY(n) means n-gram NPYLM.
Results for HDP(2) are taken from Goldwater et
al. (2009), which corrects the errors in Goldwater
et al (2006).
Model time iterations
NPYLM 17min 200
HDP 10h 55min 20000
Table 2: Computations needed for Table 1. Itera-
tions for ?HDP? is the same as described in Gold-
water et al (2009). Actually, NPYLM approxi-
mately converged around 50 iterations, 4 minutes.
Japanese word segmentation, with all supervised
segmentations removed in advance.
Chinese For Chinese, we used a publicly avail-
able SIGHAN Bakeoff 2005 dataset (Emerson,
2005). To compare with the latest unsupervised
results (using a closed dataset of Bakeoff 2006),
we chose the common sets prepared by Microsoft
Research Asia (MSR) for simplified Chinese, and
by City University of Hong Kong (CITYU) for
traditional Chinese. We used a random subset of
50,000 sentences from each dataset for training,
and the evaluation was conducted on the enclosed
test data. 9
Japanese For Japanese, we used the Kyoto Cor-
pus (Kyoto) (Kurohashi and Nagao, 1998): we
used random subset of 1,000 sentences for evalua-
tion and the remaining 37,400 sentences for train-
ing. In all cases we removed all whitespaces to
yield raw character strings for inference, and set
L = 4 for Chinese and L = 8 for Japanese to run
the Gibbs sampler for 400 iterations.
The results (in token F-measures) are shown in
Table 3. Our NPYLM significantly ourperforms
the best results using a heuristic approach reported
in Zhao and Kit (2008). While Japanese accura-
cies appear lower, subjective qualities are much
higher. This is mostly because NPYLM segments
inflectional suffixes and combines frequent proper
names, which are inconsistent with the ?correct?
9Notice that analyzing a test data is not easy for character-
wise Gibbs sampler of previous work. Meanwhile, NPYLM
easily finds the best segmentation using the Viterbi algorithm
once the model is learned.
105
Model MSR CITYU Kyoto
NPY(2) 80.2 (51.9) 82.4 (126.5) 62.1 (23.1)
NPY(3) 80.7 (48.8) 81.7 (128.3) 66.6 (20.6)
ZK08 66.7 (?) 69.2 (?) ?
Table 3: Accuracies and perplexities per character
(in parentheses) on actual corpora. ?ZK08? are the
best results reported in Zhao and Kit (2008). We
used?-gram for characters.
MSR CITYU Kyoto
Semi 0.895 (48.8) 0.898 (124.7) 0.913 (20.3)
Sup 0.945 (81.4) 0.941 (194.8) 0.971 (21.3)
Table 4: Semi-supervised and supervised results.
Semi-supervised results used only 10K sentences
(1/5) of supervised segmentations.
segmentations. Bigram and trigram performances
are similar for Chinese, but trigram performs bet-
ter for Japanese. In fact, although the difference
in perplexity per character is not so large, the per-
plexity per word is radically reduced: 439.8 (bi-
gram) to 190.1 (trigram). This is because trigram
models can leverage complex dependencies over
words to yield shorter words, resulting in better
predictions and increased tokens.
Furthermore, NPYLM is easily amenable to
semi-supervised or even supervised learning. In
that case, we have only to replace the word seg-
mentation w(s) in Figure 3 to the supervised one,
for all or part of the training data. Table 4
shows the results using 10,000 sentences (1/5) or
complete supervision. Our completely generative
model achieves the performance of 94% (Chinese)
or even 97% (Japanese) in supervised case. The
result also shows that the supervised segmenta-
tions are suboptimal with respect to the perplex-
ity per character, and even worse than unsuper-
vised results. In semi-supervised case, using only
10K reference segmentations gives a performance
of around 90% accuracy and the lowest perplexity,
thanks to a combination with unsupervised data in
a principled fashion.
5.3 Classics and English text
Our model is particularly effective for spoken tran-
scripts, colloquial texts, classics, or unknown lan-
guages where supervised segmentation data is dif-
ficult or even impossible to create. For example,
we are pleased to say that we can now analyze (and
build a language model on) ?The Tale of Genji?,
the core of Japanese classics written 1,000 years
ago (Figure 6). The inferred segmentations are
 	
ffLearning Nonstructural Distance Metric
by Minimum Cluster Distortions
Daichi Mochihashi, Genichiro Kikui
ATR Spoken Language Translation
research laboratories
Hikaridai 2-2-2, Keihanna Science City
Kyoto 619-0288, Japan
daichi.mochihashi@atr.jp
genichiro.kikui@atr.jp
Kenji Kita
Center for Advanced Information
Technology, Tokushima University
Minamijosanjima 2-1
Tokushima 770-8506, Japan
kita@is.tokushima-u.ac.jp
Abstract
Much natural language processing still depends on
the Euclidean (cosine) distance function between
two feature vectors, but this has severe problems
with regard to feature weightings and feature cor-
relations. To answer these problems, we propose an
optimal metric distance that can be used as an alter-
native to the cosine distance, thus accommodating
the two problems at the same time. This metric is
optimal in the sense of global quadratic minimiza-
tion, and can be obtained from the clusters in the
training data in a supervised fashion.
We confirmed the effect of the proposed metric
distance by a synonymous sentence retrieval task,
document retrieval task and the K-means clustering
of general vectorial data. The results showed con-
stant improvement over the baseline method of Eu-
clid and tf.idf, and were especially prominent for
the sentence retrieval task, showing a 33% increase
in the 11-point average precision.
1 Introduction
Natural language processing involves many kinds of
linguistic expressions, such as sentences, phrases,
documents and the collection of documents. Com-
paring these expressions based on semantic proxim-
ity is a fundamental task and has many applications.
Generally, two basic approaches exist to compare
two expressions: (a) structural and (b) nonstruc-
tural. Structural approaches make use of syntactic
parsing or dependency analysis to make a rigorous
comparison; nonstructural approaches use vector
representation and provide a rough but fast compar-
ison that is required for search/retrieval from a vast
amount of corpora. While structural approaches
have recently become available in a kernel-based
sophisticated treatment (Collins and Duffy, 2001;
Suzuki et al, 2003), here we concentrate on non-
structural comparison. This is not only because non-
structural comparison constitutes an integral part
in structural methods (that is, even in hierarchi-
cal methods the leaf comparison is still atomic),
but because it is frequently embedded in many ap-
plications where structural parsings are not avail-
able or computationally too expensive. For exam-
ple, information retrieval has long used the ?bag
of words? approach (Baeza-Yates and Ribeiro-Neto,
1999; Schu?tze, 1992) mainly due to a lack of scal-
able segmentation algorithms and the huge amount
of data involved. While segmentation algorithms,
such as TEXTTILING (Hearst, 1994) and its recent
successors using the inter-paragraph similarity ma-
trix (Choi, 2000), all themselves use nonstructural
cosine similarity as a measure of semantic proxim-
ity between paragraphs.
However, the distance function so far has been
largely defined and used ad hoc, usually by a tf.idf
weighting scheme (Salton and Yang, 1973) and a
simple cosine similarity, equivalently, an Euclidean
dot product. In this paper, we propose an optimal
distance function that is parameterized by a global
metric matrix. This metric is optimal in the sense of
global quadratic minimization, and can be learned
from the given clusters in the training data. These
clusters are often attributable with many forms, such
as paragraphs, documents or document collections,
as long as the items in the training data are not com-
pletely independent.
This paper is organized as follows. In section 2
we describe the issue of traditional Euclidean dis-
tances, and section 3 places it into general perspec-
tive with related works in machine learning. Section
4 introduces the proposed metric, and section 5 vali-
dates its effect on the task of sentence retrieval, doc-
ument retrieval and the K-means clustering. Sec-
tions 6 and 7 present discussions and the conclusion.
2 Issues with Euclidean distances
When we address nonstructural matching, linguis-
tic expressions are often modeled by a feature vec-
tor ~x ? Rn, with its elements x1 . . . xn correspond-
ing to the number of occurrences of i?th feature. If
features are simply words, this is called a ?bag of
words?; but in general, features are not restricted to
this kind, and we will use the general term ?feature?
in the rest of the paper.
To measure the distance between two vectors
~u,~v, a dot product or Euclidean distance
d(~u,~v)2 = (~u ? ~v)T (~u ? ~v) (1)
= ?ni=1(ui ? vi)2
(where T denotes a transposition) has been em-
ployed so far 1, with a heuristic feature weighting
such as tf.idf in a preprocessing stage.
However, there are two main problems with this
distance:
(1) The correlation between features is ignored.
(2) Feature weighting is inevitably arbitrary.
Problem (1) is especially important in languages,
because linguistic features (e.g., words) generally
have strong correlations between them, such as col-
locations or typical constructions. But this correla-
tion cannot be considered in a simple dot product.
While it is possible to address this with a specific
kernel function, such as polynomials (Mu?ller et al,
2001), this is not available for many problems, such
as information retrieval or question answering, that
do not fit classifications or cannot be easily ?kernel-
ized?. Problem (2) is a more subtle but inherent one:
while tf.idf often works properly in practice, there
are several options, especially in tf such as logs or
square roots, but we have no principle with which
to choose from. Further, it has no theoretical basis
that gives any optimality as a distance function.
3 Related Works
The issues above of feature correlations and fea-
ture weightings can be summarized as a problem of
defining an appropriate metric in the feature space,
based on the distribution of data. This problem has
recently been highlighted in the field of machine
learning research. (Xing et al, 2002) has an ob-
jective that is quite similar to that of this paper, and
gives a metric matrix that resembles ours based on
sample pairs of ?similar points? as training data.
(Bach and Jordan, 2004) and (Schultz and Joachims,
2004) seek to answer the same problem with an ad-
ditional scenario of spectral clustering and relative
comparisons in Support Vector Machines, respec-
tively. In this aspect, our work is a straight succes-
sor of (Xing et al, 2002) where its general usage
in vector space is preserved. We offer a discussion
on the similarity to our method and our advantages
1When we normalize the length of the vectors |~u| = |~v| = 1
as commonly adopted, (~u ? ~v)T (~u ? ~v) = |~u|2 + |~v|2 ? 2~u ?
~v ? ?~u ? ~v = ? cos(~u,~v) ; therefore, this includes a cosine
similarity (Manning and Sch u?tze, 1999).
in section 6. Finally, we note that the Fisher ker-
nel of (Jaakkola and Haussler, 1999) has the same
concept that gives an appropriate similarity of two
data through the Fisher information matrix obtained
from the empirical distribution of data. However, it
is often approximated by a unit matrix because of
its heavy computational demand.
In the field of information retrieval, (Jiang and
Berry, 1998) proposes a Riemannian SVD (R-SVD)
from the viewpoint of relevance feedback. This
work is close in spirit to our work, but is not aimed
at defining a permanent distance function and does
not utilize cluster structures existent in the training
data.
4 Defining an Optimal Metric
To solve the problems in section 2, we note the func-
tion that synonymous clusters play. There are many
levels of (more or less) synonymous clusters in lin-
guistic data: phrases, sentences, paragraphs, docu-
ments, and, in a web environment, the site that con-
tains the document. These kinds of clusters can of-
ten be attributed to linguistic expressions because
they nest in general so that each expression has a
parent cluster.
Since these clusters are synonymous, we can ex-
pect the vectors in each cluster to concentrate in the
ideal feature space. Based on this property, we can
introduce an optimal weighting and correlation in a
supervised fashion. We will describe this method
below.
4.1 The Basic Idea
As stated above, vectors in the same cluster must
have a small distance between each other in the ideal
geometry. When we measure an L2-distance be-
tween ~u and ~v by a Mahalanobis distance param-
eterized by M :
dM (~u,~v)2 = (~u ? ~v)T M(~u ? ~v) (2)
= ?ni=1
?n
j=1 mij(ui ? vi)(uj ? vj),
where symmetric metric matrix M gives both cor-
responding feature weights and feature correlations.
When we take M = I (unit matrix), we recover the
original Euclidean distance (1).
Equation (2) can be rewritten as (3) because M is
symmetric:
dM (~u,~v)2 = (M1/2(~u?~v))T (M1/2(~u?~v)). (3)
Therefore, this distance amounts to a Euclidean dis-
tance in M 1/2-mapped space (Xing et al, 2002).
Note that this distance is global, and different
from the ordinary Mahalanobis distance in pattern
recognition (for example, (Duda et al, 2000)) that is
defined for each cluster one by one, using a cluster-
specific covariance matrix. That type of distance
cannot be generalized to new kinds of data; there-
fore, it has been used for local classifications. What
we want is a global distance metric that is generally
useful, not a measure for classification to predefined
clusters. In this respect, (Xing et al, 2002) shares
the same objective as ours.
Therefore, we require an optimization over all the
clusters in the training data. Generally, data in the
clusters are distributed as in figure 1(a), comprising
ellipsoidal forms that have high (co)variances for
some dimensions and low (co)variances for other di-
mensions. Further, the cluster is not usually aligned
to the axes of coordinates. When we find a global
metric matrix M that minimizes the cluster distor-
tions, namely, one that reduces high variances and
expands low variances for the data to make a spher-
ical form as good as possible in the M 1/2-mapped
space (figure 1(b)), we can expect it to capture nec-
essary and unnecessary variations and correlations
on the features, combining information from many
clusters to produce a more reliable metric that is not
locally optimal. We will find this optimal M below.
xn
x1
x2
High
variance
High
covariance
Low
variance
(a) Original space
x1
x2
xn
(b) Mapped space
Figure 1: Geometry of feature space.
4.2 Global optimization over clusters
Suppose that each data (for example, sentences or
documents) is a vector ~s ? Rn, and the whole cor-
pus can be divided into N clusters, X1 . . . XN . That
is, each vector has a dimension n, and the number of
clusters is N . For each cluster Xi, cluster centroid
ci is calculated as ~ci = 1/|Xi|
?
~s?Xi ~s , where |X|
denotes the number of data in X . When necessary,
each element in ~sj or ~ci is referenced as sjk or cik
(k = 1 . . . n).
The basic idea above is formulated as follows.
We seek the metric matrix M that minimizes the
distance between each data ~sj and the cluster cen-
troid ~ci, dM (~sj ,~ci) for all clusters X1 . . . XN .
Mathematically, this is formulated as a quadratic
minimization problem
M = arg min
M
N
?
i=1
?
~sj?Xi
dM (~sj,~ci)2
= arg min
M
N
?
i=1
?
~sj?Xi
(~sj ? ~ci)T M(~sj ? ~ci) (4)
under a scale constraint (| ? | means determinant)
|M | = 1. (5)
Scale constraint (5) is necessary for excluding a
degenerate solution M = O. 1 is an arbitrary con-
stant: when we replace 1 by c, c2M becomes a new
solution. This minimization problem is an exten-
sion to the method of MindReader (Ishikawa et al,
1998) to multiple clusters, and has a unique solution
below.
Theorem The matrix that solves the minimization
problem (4,5) is
M = |A|1/nA?1, (6)
where A = [akl] is defined by
akl =
N
?
i=1
?
sj?Xi
(sjl ? cil)(sjk ? cik) . (7)
Proof: See Appendix A.
When A is singular, we can use as A?1 a Moore-
Penrose matrix pseudoinverse A+. Generally, A
consists of linguistic features and is very sparse, and
often singular. Therefore, A+ is nearly always nec-
essary for the above computation. For details, see
Appendix B.
4.3 Generalization
While we assumed through the above construction
that each cluster is equally important, this is not
the case in general. For example, clusters with a
small number of data may be considered weak, and
in the hierarchical clustering situation, a ?grand-
mother? cluster may be weaker. If we have con-
fidences ?1 . . . ?N for the strength of clustering for
each cluster X1 . . . XN , this information can be in-
corporated into (4) by a set of normalized cluster
weights ??i :
M = arg min
M
N
?
i=1
??i
?
~sj?Xi
(~sj ? ~ci)T M(~sj ? ~ci),
where ??i = ?i/
?N
j=1 ?j , and we obtain a respec-
tively weighted solution in (7). Further, we note that
when N = 1, this metric recovers the ordinary Ma-
halanobis distance in pattern recognition. However,
we used equal weights for the experiments below
because the number of data in each cluster was ap-
proximately equal.
5 Experiments
We evaluated our metric distance on the three tasks
of synonymous sentence retrieval, document re-
trieval, and the K-means clustering of general vec-
torial data. After calculating M on the training data
of clusters, we applied it to the test data to see how
well its clusters could be recovered. As a measure of
cluster recovery, we use 11-point average precision
and R-precision for the distribution of items of the
same cluster in each retrieval result. Here, R equals
the cardinality of the cluster; therefore, R-precision
shows the precision of cluster recovery.
5.1 Synonymous sentence retrieval
5.1.1 Sentence cluster corpus
We used a paraphrasing corpus of travel conversa-
tions (Sugaya et al, 2002) for sentence retrieval.
This corpus consists of 33,723,164 Japanese trans-
lations, each of which corresponds to one of the
original English sentences. By way of this cor-
respondence, Japanese sentences are divided into
10,610 clusters. Therefore, each cluster consists
of Japanese sentences that are possible translations
from the same English seed sentence that the clus-
ter has. From this corpus, we constructed 10 sets
of data. Each set contains random selection of 200
training clusters and 50 test clusters, and each clus-
ter contains a maximum of 100 sentences 2. Ex-
periments were conducted on these 10 datasets for
each level of dimensionality reduction (see below)
to produce average statistics.
5.1.2 Features and dimensionality reduction
As a feature of a sentence, we adopted unigrams of
all words and bigrams of functional words from the
part-of-speech tags, because the sequence of func-
tional words is important in the conversational cor-
pus.
While the lexicon is limited for travel conversa-
tions, the number of features exceeds several thou-
sand or more. This may be prohibitive for the calcu-
lation of the metric matrix, therefore, we addition-
ally compressed the features with SVD, the same
method used in Latent Semantic Indexing (Deer-
wester et al, 1990).
5.1.3 Sentence retrieval results
Qualitative result Figure 5 (last page) shows a sam-
ple retrieval result. A sentence with (*) mark at
the end is the correct answer, that is, a sentence
from the same original cluster as the query. We can
see that the results with the metric distance contain
2When the number of data in the cluster exceeds this limit,
100 sentences are randomly sampled. All sampling are made
without replacement.
less noise than a standard Euclid baseline with tf.idf
weighting, achieving a high-precision retrieval. Al-
though the high rate of dimensionality reduction in
figure 6 shows degradation due to the dimension
contamination, the effect of metric distance is still
apparent despite bad conditions.
Quantitative result Figure 2 shows the averaged
precision-recall curves of retrieval and figure 3
shows 11-point average precisions, for each rate
of dimensionality reduction. Clearly, our method
achieves higher precision than the standard method,
and does not degrade much with feature compres-
sions unless we reduce the dimension too much, i.e.,
to < 5%.
0
0.2
0.4
0.6
0.8
1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Pr
ec
is
io
n
Recall
1%
5%
10%
20%
50%
(a) Metric distance +
idf
0
0.2
0.4
0.6
0.8
1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Pr
ec
is
io
n
Recall
1%
5%
10%
20%
50%
(b) Euclidean + idf
Figure 2: Precision-recall of sentence retrieval.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0 5 10 15 20 25 30 35 40 45 50
Pr
ec
is
io
n
Dimension Reduction(%)
Metric distance +idf
Euclidean distance +idf
Figure 3: 11-point average precision.
5.2 Document retrieval
As a method of tackling clusters of texts, the text
classification task has recently made great advances
with a Na??ve Bayes or SVM classifiers (for exam-
ple, (Joachims, 1998)). However, they all aim at
classifying texts into a few predefined clusters, and
cannot deal with a document that fits neither of the
clusters. For example, when we regard a website as
a cluster of documents, the possible clusters are nu-
merous and constantly increasing, which precludes
classificatory approaches. For these circumstances,
document clustering or retrieval will benefit from a
global distance metric that exploits the multitude of
cluster structures themselves.
5.2.1 Newsgroup text dataset
For this purpose, we used the 20-Newsgroup dataset
(Lang, 1995). This is a standard text classification
dataset that has a relatively large number of classes,
20. Among the 20 newsgroups, we selected 16 clus-
ters of training data and 4 clusters of test data, and
performed 5-fold cross validation. The maximum
number of documents per cluster is 100, and when
it exceeds this limit, we made a random sampling of
100 documents as the sentence retrieval experiment.
Because our proposed metric is calculated from
the distribution of vectors in high-dimensional fea-
ture space, it becomes inappropriate if the norm
of the vectors (largely proportional to document
length) differs much from document to document.
3 Therefore, we used subsampling/oversampling to
form a median length (130 words) on training docu-
ments. Further, we preprocessed them with tf.idf as
a baseline method.
5.2.2 Results
Table 1 shows R-precision and 11-point average
precision. Since the test data contains 4 clusters,
the baselines of precision are 0.25. We can see from
both results that metric distance produces a better
retrieval over the tf.idf and dot product. However,
refinements in precision are certain (average p =
0.0243) but subtle.
This can be thought of as the effect of the dimen-
sionality reduction performed. We first decompose
data matrix X by SVD: X = USV ?1 and build
a k-dimensional compressed representation Xk =
VkX; where Vk denotes a k-largest submatrix of V .
From the equation (3), this means a Euclidean dis-
tance of M 1/2Xk = M1/2VkX . Therefore, Vk may
subsume the effect of M in a preprocessing stage.
Close inspection of table 1 shows this effect as a
tradeoff between M and Vk. To make the most of
metric distance, we should consider metric induc-
tion and dimensionality reduction simultaneously,
or reconsider the problem in kernel Hilbert space.
Dim. R-precision 11-pt Avr. Prec.
Red. Metric Euclid Metric Euclid
0.5% 0.421 0.399 0.476 0.455
1% 0.388 0.368 0.450 0.430
2% 0.359 0.343 0.425 0.409
3% 0.344 0.330 0.411 0.399
4% 0.335 0.323 0.402 0.392
5% 0.329 0.318 0.397 0.388
10% 0.316 0.307 0.379 0.376
20% 0.343 0.297 0.397 0.365
Table 1: Newsgroup text retrieval results.
3Normalizing documents to unit length effectively maps
them to a high-dimensional hypersphere; this proved to pro-
duce an unsatisfactory result. Defining metrics that work on
a hypersphere like spherical K-means (Dhillon and Modha,
2001) requires further research.
5.3 K-means clustering and general vectorial
data
Metric distance can also be used for clustering or
general vectorial data. Figure 4 shows the K-means
clustering result of applying our metric distance to
some of the UCI Machine Learning datasets (Blake
and Merz, 1998). K-means clustering was con-
ducted 100 times with a random start, where K
equals the known number of classes in the data 4.
Clustering precision was measured as an average
probability that a randomly picked pair of data will
conform to the true clustering (Xing et al, 2002).
We also conducted the same clustering for doc-
uments of the 20-Newsgroup dataset to get a small
increase in precision like the document retrieval ex-
periment in section 5.2.
0.6
0.7
0.8
0.9
1
1 2 5 10 13
Dimension
Pre
cisi
on
(a) ?wine? dataset
0.6
0.7
0.8
0.9
1 2 5 10 15 20
Dimension
Pre
cisi
on
(b) ?protein? dataset
0.7
0.8
0.9
1
1 2 3 4
Dimension
Pre
cisi
on
(c) ?iris? dataset
0.6
0.7
0.8
0.9
1
1 2 5 10 20 3 5
Dimension
Pre
cisi
on
(d) ?soybean? dataset
Figure 4: K-means clustering of UCI Machine
Learning dataset results. The horizontal axis shows
compressed dimensions (rightmost is original). The
right bar shows clustering precision using Metric
distance, and the left bar shows that using Euclidean
distance.
6 Discussion
In this paper, we proposed an optimal distance met-
ric based on the idea of minimum cluster distortion
in training data. Although vector distances have fre-
quently been used in natural language processing,
this is a rather neglected but recently highlighted
problem. Unlike recently proposed methods with
spectral methods or SVMs, our method assumes no
such additional scenarios and can be considered as
4Because of the small size of the dataset, we did not apply
cross-validation as in other experiments.
a straight successor to (Xing et al, 2002)?s work.
Their work has the same perspective as ours, and
they calculate a metric matrix A that is similar to
ours based on a set S of vector pairs (~xi, ~xj) that can
be regarded as similar. They report that the effec-
tiveness of A increases as the number of the training
pairs S increases; this requires O(n2) sample points
from n training data, and must be optimized by a
computationally expensive Newton-Raphson itera-
tion. On the other hand, our method uses only linear
algebra, and can induce an ideal metric using all the
training data at the same time. We believe this met-
ric can be useful for many vector-based language
processing methods that have used cosine similar-
ity.
There remains some future directions for re-
search. First, as we stated in section 4.3, the effect
of a cluster weighted generalized metric must be in-
vestigated and optimal weighting must be induced.
Second, as noted in section 5.2.1, the dimensional-
ity reduction required for linguistic data may con-
strain the performance of the metric distance. To
alleviate this problem, simultaneous dimensionality
reduction and metric induction may be necessary, or
the same idea in a kernel-based approach is worth
considering. The latter obviates the problem of di-
mensionality, while it restricts the usage to a situa-
tion where the kernel-based approach is available.
7 Conclusion
We proposed a global metric distance that is use-
ful for clustering or retrieval where Euclidean dis-
tance has been used. This distance is optimal in the
sense of quadratic minimization over all the clus-
ters in the training data. Experiments on sentence
retrieval, document retrieval and K-means cluster-
ing all showed improvements over Euclidean dis-
tance, with a significant refinement with tight train-
ing clusters in sentence retrieval.
Acknowledgement
The research reported here was supported in part by
a contract with the National Institute of Information
and Communications Technology entitled ?A study
of speech dialogue translation technology based on
a large corpus?.
References
Francis R. Bach and Michael I. Jordan. 2004.
Learning Spectral Clustering. In Advances in
Neural Information Processing Systems 16. MIT
Press.
Ricardo A. Baeza-Yates and Berthier A. Ribeiro-
Neto. 1999. Modern Information Retrieval.
ACM Press / Addison-Wesley.
C. L. Blake and C. J. Merz. 1998. UCI
Repository of machine learning databases.
http://www.ics.uci.edu/?mlearn/MLRepository.html.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In Proceedings
of NAACL-00.
Michael Collins and Nigel Duffy. 2001. Convo-
lution Kernels for Natural Language. In NIPS
2001.
S. Deerwester, Susan T. Dumais, and George W.
Furnas. 1990. Indexing by Latent Semantic
Analysis. Journal of the American Society of In-
formation Science, 41(6):391?407.
Inderjit S. Dhillon and Dharmendra S. Modha.
2001. Concept Decompositions for Large Sparse
Text Data Using Clustering. Machine Learning,
42(1/2):143?175.
Richard O. Duda, Peter E. Hart, and David G. Stork.
2000. Pattern Classification *Second Edition.
John Wiley & Sons.
Marti Hearst. 1994. Multi-paragraph segmentation
of expository text. In 32nd. Annual Meeting of
the Association for Computational Linguistics,
pages 9?16.
Yoshiharu Ishikawa, Ravishankar Subramanya, and
Christos Faloutsos. 1998. MindReader: Query-
ing Databases Through Multiple Examples. In
Proc. 24th Int. Conf. Very Large Data Bases,
pages 218?227.
Tommi S. Jaakkola and David Haussler. 1999. Ex-
ploiting generative models in discriminative clas-
sifiers. In Proc. of the 1998 Conference on Ad-
vances in Neural Information Processing Sys-
tems, pages 487?493.
Eric P. Jiang and Michael W. Berry. 1998. Infor-
mation Filtering Using the Riemannian SVD (R-
SVD). In Proc. of IRREGULAR ?98, pages 386?
395.
Thorsten Joachims. 1998. Text categorization with
support vector machines: learning with many
relevant features. In Proceedings of ECML-98,
number 1398, pages 137?142.
Ken Lang. 1995. Newsweeder: Learning to filter
netnews. In Proceedings of the Twelfth Interna-
tional Conference on Machine Learning, pages
331?339.
Christopher D. Manning and Hinrich Schu?tze.
1999. Foundations of Statistical Natural Lan-
guage Processing. MIT Press.
K. R. Mu?ller, S. Mika, G. Ratsch, and K. Tsuda.
2001. An introduction to kernel-based learning
algorithms. IEEE Neural Networks, 12(2):181?
201.
G. Salton and C. S. Yang. 1973. On the specifica-
tion of term values in automatic indexing. Jour-
nal of Documentation, 29:351?372.
Matthew Schultz and Thorsten Joachims. 2004.
Learning a Distance Metric from Relative Com-
parisons. In Advances in Neural Information
Processing Systems 16. MIT Press.
Hinrich Schu?tze. 1992. Dimensions of Mean-
ing. In Proceedings of Supercomputing?92, pages
787?796.
F. Sugaya, T. Takezawa, G. Kikui, and S. Ya-
mamoto. 2002. Proposal for a very-large-corpus
acquisition method by cell-formed registration.
In Proc. LREC-2002, volume I, pages 326?328.
Jun Suzuki, Tsutomu Hirao, Yutaka Sasaki, and
Eisaku Maeda. 2003. Hierarchical Directed
Acyclic Graph Kernel: Methods for Structured
Natural Language Data. In Proc. of the 41th An-
nual Meeting of Association for Computational
Linguistics (ACL2003), pages 32?39.
Eric W. Weisstein. 2004. Moore-Penrose Matrix
Inverse. http://mathworld.wolfram.com/Moore-
PenroseMatrixInverse.html.
Eric P. Xing, Andrew Y. Ng, Michael I. Jordan,
and Stuart Russell. 2002. Distance metric learn-
ing, with application to clustering with side-
information. In NIPS 2002.
Appendix A.
Derivation of the metric matrix
Here we prove theorem 1, namely deriving M that
satisfies the condition
min
M
n
?
i=1
?
~sj?Xi
(~sj ? ~ci)T M(~sj ? ~ci) , (8)
under the constraint
|M | = 1. (9)
Expanding (8), we get
?
i
?
~sj
[ n
?
k=1
n
?
l=1
(sjk ? cik)mkl(sjl ? cil)
]
, (10)
and from (9), for all k
n
?
l=1
(?1)k+lmkl|Mkl| = 1 .
Therefore
n
?
k=1
n
?
l=1
(?1)k+lmkl|Mkl| = n, (11)
where Mkl denotes an adjugate matrix of mkl.
Therefore, we come to minimize (10) under the
constraint (11).
By introducing the Lagrange multiplier ?, we de-
fine
L =
N
?
i=1
?
~sj
[
?
k
?
l
(sjk ? cik)mkl(sjl ? cil)
]
??
[
?
k
?
l
(?1)k+lmkl|Mkl| ? n
]
.
Differentiating by mkl and setting to zero, we obtain
?L
?mkl
=
?
i
?
~sj
(sjk ? cik)(sjl ? cil)
? ?(?1)k+l|Mkl| = 0
? |Mkl| =
?
i
?
~sj (sjk ? cik)(sjl ? cil)
?(?1)k+l . (12)
Let us define M?1 = [m?1kl ]. Then,
m?1kl =
(?1)k+l|Mkl|
|M |
= (?1)k+l|Mkl| (... (9))
=
?
i
?
~sj (sjk ? cik)(sjl ? cil)
? (13)
(... (12))
Therefore, when we define
A = [akl] (14)
as
akl =
N
?
i=1
?
~sj?Xi
(sjl ? cil)(sjk ? cik) , (15)
from (13),
A = ?M?1
... |A| = ?n|M?1| = ?n
... ? = |A|1/n ,
where A is defined by (14), (15).
Appendix B.
Moore-Penrose Matrix Pseudoinverse
The Moore-Penrose matrix pseudoinverse A+ of A
is a unique matrix that has a property of normal in-
verse in that x = A+y is a shortest length least
squares solution to Ax = y even if A is singular
(Weisstein, 2004).
A+ can be calculated simply by a MATLAB
function pinv. Or alternatively (Ishikawa et al,
1998), we can decompose A as
A = U?UT ,
where U is an orthonormal n ? n matrix and ? =
diag(?1, . . . , ?R, 0, . . . , 0) (R = rank(A)). Then,
A+ is calculated as
A+ = U?+UT ,
where ?+ = diag(1/?1, . . . , 1/?R, 0, . . . , 0).
Therefore,
M = (?1?2 ? ? ? ?R)1/RA+.
Query: ?  
	 ?
(?How much is the total??)
Metric distance:
distance synonymous sentence
0.2712 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1180?1190,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Improvements to the Bayesian Topic N -gram Models
Hiroshi Noji??
noji@nii.ac.jp
Daichi Mochihashi??
daichi@ism.ac.jp
?Graduate University for Advanced Studies
?National Institute of Informatics, Tokyo, Japan
?The Institute of Statistical Mathematics, Tokyo, Japan
Yusuke Miyao??
yusuke@nii.ac.jp
Abstract
One of the language phenomena that n-gram
language model fails to capture is the topic in-
formation of a given situation. We advance the
previous study of the Bayesian topic language
model by Wallach (2006) in two directions:
one, investigating new priors to alleviate the
sparseness problem caused by dividing all n-
grams into exclusive topics, and two, develop-
ing a novel Gibbs sampler that enables moving
multiple n-grams across different documents
to another topic. Our blocked sampler can
efficiently search for higher probability space
even with higher order n-grams. In terms of
modeling assumption, we found it is effective
to assign a topic to only some parts of a docu-
ment.
1 Introduction
N -gram language model is still ubiquitous in NLP,
but due to its simplicity it fails to capture some im-
portant aspects of language, such as difference of
word usage in different situations, sentence level
syntactic correctness, and so on. Toward language
model that can consider such a more global con-
text, many extensions have been proposed from
lexical pattern adaptation, e.g., adding cache (Je-
linek et al, 1991) or topic information (Gildea and
Hofmann, 1999; Wallach, 2006), to grammaticality
aware models (Pauls and Klein, 2012).
Topic language models are important for use in
e.g., unsupervised language model adaptation: we
want a language model that can adapt to the do-
main or topic of the current situation (e.g., a doc-
ument in SMT or a conversation in ASR) automat-
ically and select the appropriate words using both
topic and syntactic context. Wallach (2006) is one
such model, which generate each word based on lo-
cal context and global topic information to capture
the difference of lexical usage among different top-
ics.
However, Wallach?s experiments were limited to
bigrams, a toy setting for language models, and ex-
periments with higher-order n-grams have not yet
been sufficiently studied, which we investigate in
this paper. In particular, we point out the two funda-
mental problems caused when extending Wallach?s
model to a higher-order: sparseness caused by di-
viding all n-grams into exclusive topics, and local
minima caused by the deep hierarchy of the model.
On resolving these problems, we make several con-
tributions to both computational linguistics and ma-
chine learning.
To address the first problem, we investigate incor-
porating a global language model for ease of sparse-
ness, along with some priors on a suffix tree to cap-
ture the difference of topicality for each context,
which include an unsupervised extension of the dou-
bly hierarchical Pitman-Yor language model (Wood
and Teh, 2009), a Bayesian generative model for su-
pervised language model adaptation. For the sec-
ond inference problem, we develop a novel blocked
Gibbs sampler. When the number of topics is K
and vocabulary size is V , n-gram topic model has
O(KV n) parameters, which grow exponentially to
n, making the local minima problem even more se-
vere. Our sampler resolves this problem by moving
many customers in the hierarchical Chinese restau-
rant process at a time.
We evaluate various models by incremental cal-
culation of test document perplexity on 3 types of
corpora having different size and diversity. By com-
bining the proposed prior and the sampling method,
our Bayesian model achieve much higher accura-
cies than the naive extension of Wallach (2006) and
shows results competitive with the unigram rescal-
ing (Gildea and Hofmann, 1999), which require
1180
huge computational cost at prediction, with much
faster prediction time.
2 Basic Models
All models presented in this paper are based on the
Bayesian n-gram language model, the hierarchical
Pitman-Yor process language model (HPYLM). In
the following, we first introduce the HPYLM, and
then discuss the topic model extension of Wallach
(2006) with HPYLM.
2.1 HPYLM
Let us first define some notations. W is a vocabulary
set, V = |W | is the size of that set, and u, v, w ?W
represent the word type.
The HPYLM is a Bayesian treatment of the n-
gram language model. The generative story starts
with the unigram word distribution G?, which is
a V -dimensional multinomial where G?(w) repre-
sents the probability of word w. The model first
generates this distribution from the PYP as G? ?
PYP(a, b,G0), where G0 is a V -dimensional uni-
form distribution (G0(u) = 1V ;?u ? W ) and
acts as a prior for G? and a, b are hyperparameters
called discount and concentration, respectively. It
then generates all bigram distributions {Gu}u?W as
Gu ? PYP(a, b,G?). Given this distributions, it
successively generates 3-gram distributions Guv ?
PYP(a, b,Gu) for all (u, v) ? W 2 pairs, which
encode a natural assumption that contexts having
common suffix have similar word distributions. For
example, two contexts ?he is? and ?she is?, which
share the suffix ?is?, are generated from the same
(bigram) distribution Gis, so they would have simi-
lar word distributions. This process continues until
the context length reaches n ? 1 where n is a pre-
specified n-gram order (if n = 3, the above example
is a complete process). We often generalize this pro-
cess using two contexts h and h? as
Gh ? PYP(a, b,Gh?), (1)
where h = ah?, in which a is a leftmost word of h.
We are interested in the posterior word distribu-
tion following a context h. Our training corpus w
is a collection of n-grams, from which we can cal-
culate the posterior p(w|h,w), which is often ex-
plained with the Chinese restaurant process (CRP):
p(w|h,w) = chw ? athwch? + b
+
ath? + b
ch? + b
p(w|h?,w),
(2)
where chw is an observed count of n-gram hw called
customers, while thw is a hidden variable called ta-
bles. ch? and th? represents marginal counts: ch? =?
w chw and th? =
?
w thw. This form is very
similar to the well-known Kneser-Ney smoothing,
and actually the Kneser-Ney can be understood as a
heuristic approximation of the HPYLM. This char-
acteristic enables us to build the state-of-the-art lan-
guage model into a more complex generative model.
2.2 Wallach (2006) with HPYLM
Wallach (2006) is a generative model for a docu-
ment collection that combines the topic model with
a Bayesian n-gram language model. The latent
Dirichlet alocation (LDA) (Blei et al, 2003) is the
most basic topic model, which generates each word
in a document based on a unigram word distribution
defined by a topic allocated to that word. The bi-
gram topic model of Wallach (2006) simply replaces
this unigram word distribution (a multinomial) for
each topic with a bigram word distribution 1. In
other words, ordinary LDA generates word condi-
tioning only on the latent topic, whereas the bigram
topic model generates conditioning on both the la-
tent topic and the previous word, as in the bigram
language model. Extending this model with a higher
order n-gram is trivial; all we have to do is to replace
the bigram language model for each topic with an n-
gram language model.
The formal description of the generative story of
this n-gram topic model is as follows. First, for
each topic k ? 1, ? ? ? ,K, where K is the num-
ber of topics, the model generates an n-gram lan-
guage model Gkh.2 These n-gram models are gen-
erated by the PYP, so Gkh ? PYP(a, b,Gkh?) holds.
The model then generate a document collection. For
each document j ? 1, ? ? ? , D, it generates a K-
1This is the model called prior 2 in Wallach (2006); it con-
sistently outperformed the other prior. Wallach used the Dirich-
let language model as each topic, but we only explore the model
with HPYLM because its superiority to the Dirichlet language
model has been well studied (Teh, 2006b).
2We sometimes denote Gkh to represent a language model of
topic k, not a specific multinomial for some context h, depend-
ing on the context.
1181
dimensional topic distribution ?j by a Dirichlet dis-
tribution Dir(?) where ? = (?1, ?2, ? ? ? , ?K) is a
prior. Finally, for each word position i ? 1, ? ? ? , Nj
where Nj is the number of words in document j, i-
th word?s topic assignment zji is chosen according
to ?j , then a word type wji is generated from Gzjihji
where hji is the last n? 1 words preceding wji. We
can summarize this process as follows:
1. Generate topics:
For each h ? ?, {W}, ? ? ? , {W}n?1:
For each k ? 1, ? ? ? ,K:
Gkh ? PYP(a, b,Gkh?)
2. Generate corpora:
For each document j ? 1, ? ? ?D:
?j ? Dir(?)
For each word position i ? 1, ? ? ? , Nj :
zji ? ?j
wji ? Gzjihji
3 Extended Models
One serious drawback of the n-gram topic model
presented in the previous section is sparseness. At
inference, as in LDA, we assign each n-gram a topic,
resulting in an exclusive clustering of n-grams in
the corpora. Roughly speaking, when the number
of topics is K and the number of all n-grams in the
training corpus is N , a language model of topic k,
Gkh is learned using only about O(N/K) instances
of the n-grams assigned the topic k, making each
Gkh much sparser and unreliable distribution.
One way to alleviate this problem is to place an-
other n-gram model, say G0h, which is shared with
all topic-specific n-gram models {Gkh}Kk=1. How-
ever, what is the best way to use this special distribu-
tion? We explore two different approaches to incor-
porate this distribution in the model presented in the
previous section. In one model, the HIERARCHICAL
model, G0h is used as a prior for all other n-gram
models, where G0h exploits global statistics across
all topics {Gkh}. In the other model, the SWITCH-
ING model, no statistics are shared across G0h and
{Gkh}, but some words are directly generated from
G0h regardless of the topic distribution.
3.1 HIERARCHICAL Model
Informally, what we want to do is to establish hier-
archies among the global G0h and other topics {Gkh}.
In Bayesian formalism, we can explain this using an
???
???
???
???
???
???
Figure 1: Variable dependencies of the HIERARCHICAL
model. {u, v} are word types, k is a topic and each Gkh
is a multinomial word distribution. For example, G2uv
represents a word distribution following the context uv
in topic 2.
abstract distribution F as Gkh ? F(G0h). The prob-
lem here is making the appropriate choice for the
distribution F . Each topic word distribution already
has hierarchies among n? 1-gram and n-gram con-
texts as Gkh ? PYP(a, b,Gkh?). A natural solution
to this problem is the doubly hierarchical Pitman-
Yor process (DHPYP) proposed in Wood and Teh
(2009). Using this distribution, the new generative
process of Gkh is
Gkh ? PYP(a, b, ?Gkh? + (1? ?)G0h), (3)
where ? is a new hyperparameter that determines
mixture weight. The dependencies among G0h and
{Gkh} are shown in Figure 1. Note that the genera-
tive process of G0h is the same as the HPYLM (1).
Let us clarify the DHPYP usage differences be-
tween our model and the previous work of Wood and
Teh (2009). A key difference is the problem setting:
Wood and Teh (2009) is aimed at the supervised
adaptation of a language model for a specific do-
main, whereas our goal is unsupervised adaptation.
In Wood and Teh (2009), each Gkh for k ? 1, 2, ? ? ?
corresponds to a language model of a specific do-
main and the training corpus for each k is pre-
specified and fixed. For ease of data sparseness of
domain-specific corpora, latent model G0h exploits
shared statistics amongGkh for k = 1, 2, ? ? ? . In con-
trast, with our model, each Gkh is a topic, so it must
perform the clustering of n-grams in addition to ex-
1182
ploiting the latent G0h. This makes inference harder
and requires more careful design of ?.
Modeling of ? We can better understand the role
of ? in (3) by considering the posterior predictive
form corresponds to (2), which is written as
p(w|h, k,w) = c
k
hw ? atkhw
ckh? + b
+
atkh? + b
ckh? + b
q(w|h, k,w),
(4)
q(w|h, k,w) = ?p(w|h?, k,w) + (1? ?)p(w|h, 0,w),
where c, t with superscript k corresponds to the
count existing in topic k. This shows us that ? de-
termines the back-off behavior: which probability
we should take into account: the shorter context of
the same topic Gkh? or the full context of the global
model G0h. Wood and Teh (2009) shares this vari-
able across all contexts of the same length, for each
k, but this assumption may not be the best. For ex-
ample, after the context ?in order?, we can predict
the word ?to? or ?that?, and this tendency is unaf-
fected by the topic. We call this property of context
the topicality and say that ?in order? has weak topi-
cality. Therefore, we place ? as a distinct value for
each context h, which we share across all topics. We
designate this ? determined by h ?h in the follow-
ing. Moreover, similar contexts may have similar
values of ?h. For example, the two contexts ?of the?
and ?in the?, which share the suffix ?the?, both have
a strong topicality3. We encode this assumption by
placing hierarchical Beta distributions on the suffix
tree across all topics:
?h ? Beta(??h? , ?(1? ?h?)) = DP(?, ?h?), (5)
where DP is the hierarchical Dirichlet process (Teh
et al, 2006), which has only two atoms in {0,1} and
? is a concentration parameter. As in HPYLM, we
place a uniform prior ?0 = 1/2 on the base distribu-
tion of the top node (?? ? DP(?, ?0)).
Having generated the topic component of the
model, the corpus generating process is the same as
the previous model because we only change the gen-
erating process of Gkh for k = 1, ? ? ? ,K.
3These words can be used very differently depending on the
context. For example, in a teen story, ?in the room? or ?in the
school? seems more dominant than ?in the corpora? or ?in the
topic?, which is likely to appear in this paper.
3.2 SWITCHING Model
Our second extension also exploits the globalG0h, al-
beit differently than the HIERARCHICAL model. In
this model, the relationship of G0h to the other {Gkh}
is flat, not hierarchical: G0h is a special topic that can
generate a word. The model first generates each lan-
guage model of k = 0, 1, 2, ? ? ? ,K independently
as Gkh ? PYP(a, b,Gkh?). When generating a word,
it first determines whether to use global model G0h
or topic model {Gkh}Kk=1. Here, we use the ?h in-
troduced above in a similar way: the probability of
selecting k = 0 for the next word is determined by
the previous context. This assumption seems natu-
ral; we expect theG0h to mainly generate common n-
grams, and the topicality of each context determines
how common that n-gram might be. The complete
generative process of this model is written as fol-
lows:
1. Generate topics:
For each h ? ?, {V }, ? ? ? , {V }n?1:
?h ? DP(?, ??h)
For each k ? 0, ? ? ? ,K:
Gkh ? PYP(a, b,Gkh?)
2. Generate corpora:
For each document j ? 1, ? ? ?D:
?j ? Dir(?)
For each word position i ? 1, ? ? ? , Nj :
lji ? Bern(?hji)
If lji = 0: zji = 0
If lji = 1: zji ? ?j
wji ? Gzjihji
The difference between the two models is their
usage of the global model G0h. For a better under-
standing of this, we provide a comparison of their
graphical models in Figure 2.
4 Inference
For posterior inference, we use the collapsed Gibbs
sampler. In our models, all the latent variables are
{Gkh, ?h, ?j , z,?}, where z is the set of topic assign-
ments and ? = {a, b, ?,?} are hyperparameters,
which are treated later. We collapse all multinomials
in the model, i.e., {Gkh, ?h, ?j}, in which Gkh and ?h
are replaced with the Chinese restaurant process of
PYP and DP respectively. Given the training corpus
w, the target posterior distribution is p(z,S|w,?),
where S is the set of seating arrangements of all
restaurants. To distinguish the two types of restau-
rant, in the following, we refer the restaurant to indi-
1183
(a) HIERARCHICAL (b) SWITCHING
Figure 2: Graphical model representations of our two models in the case of a 3-gram model. Edges that only exist in
one model are colored.
cate the collapsed state of Gkh (PYP), while we refer
the restaurant of ?h to indicates the collapsed state
of ?h (DP). We present two different types of sam-
pler: a token-based sampler and a table-based sam-
pler. For both samplers, we first explain in the case
of our basic model (Section 2.2), and later discuss
some notes on our extended models.
4.1 Token-based Sampler
The token-based sampler is almost identical to
the collapsed sampler of the LDA (Griffiths and
Steyvers, 2004). At each iteration, we consider the
following conditional distribution of zji given all
other topic assignments z?ji and S?ji, which is the
set of seating arrangements with a customer corre-
sponds to wji removed, as
p(zji|z?ji,S?ji) ? p(zji|z?ji)p(wji|zji, hji,S?ji),
(6)
where p(wji|zji, hji,S?ji) =
ckhw ? atkhw
ckh? + b
+
atkh? + b
ckh? + b
p(wji|zji, hji,S?ji) (7)
is a predictive word probability under the topic zji,
and
p(zji|z?ji) =
n?jijk + ?k
Nj ? 1 +
?
k? ?k?
, (8)
where n?jijk is the number of words that is assigned
topic k in document j excluding wji, which is the
same as the LDA. Given the sampled topic zji, we
update the language model of topic zji, by adding
customer wji to the restaurant specified by zji and
context hji. See Teh (2006a) for details of these cus-
tomer operations.
HIERARCHICAL Adding customer operation is
slightly changed: When a new table is added to a
restaurant, we must track the label l ? {0, 1} indi-
cating the parent restaurant of that table, and add the
customer corresponding to l to the restaurant of ?h.
See Wood and Teh (2009) for details of this opera-
tion.
SWITCHING We replace p(zji|z?ji) with
p(zji|z?ji) =
?
?
?
p(lji = 0|hji) (zji = 0)
p(lji = 1|hji) ?
n?jijk +?k
?
k 6=0 n
?ji
jk +
?
k? ?k?
(zji 6= 0),
(9)
where p(lji|hji) is a predictive of lji given by the
CRP of ?hji . We need not assign a label to a new
table, but rather we always add a customer to the
restaurant of ?h according to whether the sampled
topic is 0 or not.
4.2 Table-based Sampler
One problem with the token-based sampler is that
the seating arrangement of the internal restaurant
would never be changed unless a new table is cre-
ated (or an old table is removed) in its child restau-
rant. This probability is very low, particularly in
the restaurants of shallow depth (e.g., unigram or
1184
vConstruct a block
Move the block to the sampled topic
: customer
: table
Figure 3: Transition of the state of restaurants in the
table-based sampler when the number of topics is 2.
{u, v, w} are word types. Each box represents a restau-
rant where the type in the upper-right corner indicates the
context. In this case, we can change the topic of the three
3-grams (vvw, vvw, uvw) in some documents from 1 to
2 at the same time.
bigram restaurants) because these restaurants have
a larger number of customers and tables than those
of deep depth, leading to get stack in undesirable
local minima. For example, imagine a table in
the restaurant of context ?hidden? (depth is 2) and
some topic, served ?unit?. This table is connected
to tables in its child restaurants corresponding to
some 3-grams (e.g., ?of hidden unit? or ?train hid-
den unit?), whereas similar n-grams, such as those
of ?of hidden units? or ?train hidden units? might
be gathered in another topic, but collecting these n-
grams into the same topic might be difficult under
the token-based sampler. The table-based sampler
moves those different n-grams having common suf-
fixes jointly into another topic.
Figure 3 shows a transition of state by the table-
based sampler and Algorithm 4.2 depicts a high-
level description of one iteration. First, we select
a table in a restaurant, which is shown with a dotted
line in the figure. Next, we descend the tree to col-
lect the tables connected to the selected table, which
are pointed by arrows. Because this connection can-
not be preserved in common data structures for a
restaurant described in Teh (2006a) or Blunsom et
al. (2009), we select the child tables randomly. This
is correct because customers in CRP are exchange-
Algorithm 1 Table-based sampler
for all table in all restaurants do
Remove a customer from the parent restaurant.
Construct a block of seating arrangement S by de-
scending the tree recursively.
Sample topic assignment zS ? p(zS |S,S?S , z?S).
Move S to sampled topic, and add a customer to the
parent restaurant of the first selected table.
end for
able, so we can restore the parent-child relations ar-
bitrarily. We continue this process recursively until
reaching the leaf nodes, obtaining a block of seat-
ing arrangement S. After calculating the conditional
distribution, we sample new topic assignment for
this block. Finally, we move this block to the sam-
pled topic, which potentially changes the topic of
many words across different documents, which are
connected to customers in a block at leaf nodes (this
connection is also arbitrary).
Conditional distribution Let zS be the block of
topic assignments connected to S and zS be a vari-
able indicating the topic assignment. Thanks to the
exchangeability of all customers and tables in one
restaurant (Teh, 2006a), we can imagine that cus-
tomers and tables in S have been added to the restau-
rants last. We are interested in the following condi-
tional distribution: (conditioning ? is omitted)
p(zS = k?|S,S?S , z?S) ? p(S|S?S , k?)p(zS = k?|z?S),
where p(S|S?S , k?) is a product of customers? ac-
tions moving to another topic, which can be decom-
posed as:
p(S|S?S , k?) = p(w|k?, h)
?
s?S
p(s|k?) (10)
p(s|k?) =
?ts?1
i=0 (b+a(t
k?(?s)
hsw
+i))
?csi
j=1(j?a)
(b+ck
?(?s)
hsw?
)cs?
(11)
?
?ts?1
i=0 (b+a(t
k?(?s)
hsw
+i))
(b+ck
?(?s)
hsw?
)cs?
. (12)
Let us define some notations used above. Each
s ? S is a part of seating arrangements in a restau-
rant, there being ts tables, i-th of which with csi
customers, with hs as the corresponding context. A
restaurant of context h and topic k has tkhw tables
served dish w, i-th of which with ckhwi customers.
Superscripts ?s indicate excluding the contribution
1185
of customers in s, and xn = x(x+1) ? ? ? (x+n?1)
is the ascending factorial. In (10) p(w|k?, h) is the
parent distribution of the first selected table, and
the other p(s|k?) is the seating arrangement of cus-
tomers. The likelihood for changing topic assign-
ments across documents must also be considered,
which is p(zS = k?|z?S) and decomposed as:
p(zS = k?|z?S) =
?
j
(n?S
jk?
+?k? )
nj(S)
(N?Sj +
?
k ?k)
nj(S)
, (13)
where nj(S) is the number of word tokens con-
nected with S in document j.
HIERARCHICAL We skip tables on restaurants of
k = 0, because these tables are all from other topics
and we cannot construct a block. The effects of ?
can be ignored because these are shared by all topics.
SWITCHING In the SWITCHING, p(zS = k?|z?S)
cannot be calculated in a closed form because
p(lji|hji) in (9) would be changed dynamically
when adding customers. This problem is the same
one addressed by Blunsom and Cohn (2011), and we
follow the same approximation in which, when we
calculate the probability, we fractionally add tables
and customers recursively.
4.3 Inference of Hyperparameters
We also place a prior on each hyperparameter and
sample value from the posterior distribution for ev-
ery iteration. As in Teh (2006a), we set different
values of a and b for each depth of PYP, but share
across all topics and sample values with an auxiliary
variable method. We also set different value of ? for
each depth, on which we place Gamma(1, 1). We
make the topic prior ? asymmetric: ? = ??0;? ?
Gamma(1, 1),?0 ? Dir(1).
5 Related Work
HMM-LDA (Griffiths et al, 2005) is a composite
model of HMM and LDA that assumes the words
in a document are generated by HMM, where only
one state has a document-specific topic distribution.
Our SWITCHING model can be understood as a lex-
ical extension of HMM-LDA. It models the topical-
ity by context-specific binary random variables, not
by hidden states. Other n-gram topic models have
focused mainly on information retrieval. Wang et
min. training set test set
Corpus appear # types # docs # tokens # docs # tokens
Brown 4 19,759 470 1,157,225 30 70,795
NIPS 4 22,705 1500 5,088,786 50 167,730
BNC 10 33,071 6,162 12,783,130 100 202,994
Table 1: Corpus statistics after the pre-processing: We
replace words appearing less than min.appear times in
training + test documents, or appearing only in a test set
with an unknown token. All numbers are replaced with
#, while punctuations are remained.
al. (2007) is a topic model on automatically seg-
mented chunks. Lindsey et al (2012) extended this
model with the hierarchical Pitman-Yor prior. They
also used switching variables, but for a different pur-
pose: to determine the segmenting points. They treat
these variables completely independently, while our
model employs a hierarchical prior to share statisti-
cal strength among similar contexts.
Our primary interest is language model adapta-
tion, which has been studied mainly in the area of
speech processing. Conventionally, this adaptation
has relied on a heuristic combination of two sep-
arately trained models: an n-gram model p(w|h)
and a topic model p(w|d). The unigram rescal-
ing, which is a product model of these two mod-
els, perform better than more simpler models such
as linear interpolation (Gildea and Hofmann, 1999).
There are also some extensions to this method (Tam
and Schultz, 2009; Huang and Renals, 2008), but
these methods have one major drawback: at predic-
tion, the rescaling-based method requires normaliza-
tion across vocabulary at each word, which prohibits
use on applications requiring dynamic (incremental)
adaptation, e.g., settings where we have to update
the topic distribution as new inputs come in. Tam
and Schultz (2005) studied on this incremental set-
tings, but they employ an interpolation. The practi-
cal interest here is whether our Bayesian models can
rival the rescaling-based method in terms of predic-
tion power. We evaluate this in the next section.
6 Experiments
6.1 Settings
We test the effectiveness of presented models and
the blocked sampling method on unsupervised lan-
guage model adaptation settings. Specifically we
1186
0 2 4 6 8time (hr.)7.3e+06
7.5e+067.7e+067.9e+06
8.1e+06
negativel
og-likeliho
od
(a) Brown
0 8 16 24 32time (hr.)2.9e+07
3.1e+073.3e+073.5e+07
3.7e+07
negativel
og-likeliho
od
(b) NIPS
0 15 30 45 60time (hr.)8.0e+07
8.3e+078.6e+078.9e+07
9.2e+07
negativel
og-likeliho
od 3-gram Hpytmtoken3-gram Hpytm4-gram Hpytmtoken4-gram Hpytm
(c) BNC
10 50 100# topics205210
215220225230
235240245
testperpl
exity
(d) Brown
10 50 100# topics100105
110115120
125
testperpl
exity
(e) NIPS
10 50 100# topics130140
150160170
180190
testperpl
exity
HpylmHpytmtokenHpytmRescalingSwitchingHierarchical
(f) BNC
Figure 4: (a)?(c): Comparison of negative log-likelihoods at training of HPYTM (K = 50). Lower is better. HPYTM
is trained on both token- and table-based samplers, while HPYTMtoken is trained only on the token-based sampler.
(d)?(f): Test perplexity of various 3-gram models as a function of number of topics on each corpus.
concentrate on the dynamic adaptation: We update
the posterior of language model given previously ob-
served contexts, which might be decoded transcripts
at that point in ASR or MT.
We use three corpora: the Brown, BNC and NIPS.
The Brown and BNC are balanced corpora that con-
sist of documents of several genres from news to
romance. The Brown corpus comprises 15 cate-
gories. We selected two documents from each cate-
gory for the test set, and use other 470 documents for
the training set. For the NIPS, we randomly select
1,500 papers for training and 50 papers for testing.
For BNC, we first randomly selected 400 documents
from a written corpus and then split each document
into smaller documents every 100 sentences, leading
to 6,262 documents, from which we randomly se-
lected 100 documents for testing, and other are used
for training. See Table 1 for the pre-processing of
unknown types and the resulting corpus statistics.
For comparison, besides our proposed HIERAR-
CHICAL and SWITCHING models, we prepare vari-
ous models for baseline. HPYLM is a n-gram lan-
guage model without any topics. We call the model
without the global G0h introduced in Section 2.2
HPYTM. To see the effect of the table-based sam-
pler, we also prepare HPYTMtoken, which is trained
only on the token-based sampler. RESCALING is
the unigram rescaling. This is a product model of
an n-gram model p(w|h) and a topic model p(w|d),
where we learn each model separately and then com-
bine them by:
p(w|h, d) ?
(p(w|d)
p(w)
)?
p(w|h). (14)
We set ? in (14) to 0.7, which we tuned with the
Brown corpus.
6.2 Effects of Table-based Sampler
We first evaluate the effects of our blocked sam-
pler at training. For simplicity, we concentrate on
the HPYTM with K = 50. Table 4(a)?(c) shows
negative likelihoods of the model during training.
On all corpora, the model with the table-based sam-
pler reached the higher probability space with much
faster speed on both 3-gram and 4-gram models.
1187
6.3 Perplexity Results
Training For burn-in, we ran the sampler as fol-
lows: For HPYLM, we ran 100 Gibbs iterations. For
RESCALING, we ran 900 iterations on LDA and 100
iterations on HPYLM. For all other models, we ran
500 iterations of the Gibbs; HPYTMtoken is trained
only on the token-based sampler, while for other
models, the table-based sampler is performed after
the token-based sampler.
Evaluation We have to adapt to the topic dis-
tribution of unseen documents incrementally. Al-
though previous works have employed incremental
EM (Gildea and Hofmann, 1999; Tam and Schultz,
2005) because their inference is EM/VB-based, we
use the left-to-right method (Wallach et al, 2009),
which is a kind of particle filter updating the poste-
rior topic distribution of a test document. We set the
number of particles to 10 and resampled each parti-
cle every 10 words for all experiments. To get the
final perplexity, after burn-in, we sampled 10 sam-
ples every 10 iterations of Gibbs, calculated a test
perplexity for each sample, and averaged the results.
Comparison of 3-grams Figure 4(d)?(f) shows
perplexities when varying the number of top-
ics. Generally, compared to the HPYTMtoken, the
HPYTM got much perplexity gains, which again
confirm the effectiveness of our blocked sampler.
Both our proposed models, the HIERARCHICAL and
the SWITCHING, got better performances than the
HPYTM, which does not place the global model
G0h. Our SWITCHING model consistently performed
the best. The HIERARCHICAL performed somewhat
worse than the RESCALING when K become large,
but the SWITCHING outperformed that.
Comparison of 4-grams and beyond We sum-
marize the results with higher order n-grams in Ta-
ble 2, where we also show the time for prediction.
We fixed the number of topics K = 100 because
we saw that all models but HPYTMtoken performed
best at K = 100 when n = 3. Generally, the
results are consistent with those of n = 3. The
models with n = ? indicate a model extension
using the Bayesian variable-order language model
(Mochihashi and Sumita, 2008), which can naturally
be integrated with our generative models. By this
extension, we can prune unnecessary nodes stochas-
NIPS BNC
Model n PPL time PPL time
HPYLM 4 117.2 59 169.2 74
HPYLM ? 117.9 61 173.1 59
RESCALING 4 101.4 19009 130.3 36323
HPYTM 4 107.0 1004 133.1 980
HPYTM ? 107.2 1346 133.6 1232
HIERARCHICAL 4 106.3 1038 129.0 993
HIERARCHICAL ? 105.7 1337 129.3 1001
SWITCHING 4 100.0 1059 125.5 991
SWITCHING ? 100.4 1369 125.7 1006
Table 2: Comparison of perplexity and the time require
for prediction (in seconds). The number of topics is fixed
to 100 on all topic-based models.
tically during training. We can see that this ?-
gram did not hurt performances, but the sampled
model get much more compact; in BNC, the number
of nodes of the SWITCHING with 4-gram is about
7.9M, while the one with ?-gram is about 3.9M.
Note that our models require no explicit normaliza-
tion, thereby drastically reducing the time for pre-
diction compared to the RESCALING. This differ-
ence is especially remarkable when the vocabulary
size becomes large.
We can see that our SWITCHING performed con-
sistently better than the HIERARCHICAL. One rea-
son for this result might be the mismatch of pre-
diction of the topic distribution in the HIERARCHI-
CAL. The HIERARCHICAL must allocate some (not
global) topics to every word in a document, so even
the words to which the SWITCHING might allocate
the global topic (mainly function words; see below)
must be allocated to some other topics, causing a
mismatch of allocations of topic.
6.4 Qualitative Results
To observe the behavior in which the SWITCHING
allocates some words to the global topic, in Figure
5, we show the posterior of allocating the topic 0
or not at each word in a part of the NIPS training
corpus. We can see that the model elegantly identi-
fied content and function words, learning the topic
distribution appropriately using only semantic con-
texts. These same results in the HIERARCHICAL are
presented in Table 3, where we show some relations
between ?h and context h. Contexts that might be
likely to precede nouns have a higher value of ?h,
1188
there has been much recent work on measuring image statistics
and on learning probability distributions on images . we observe
that the mapping from images to statistics is many-to-one and
show it can be quantified by a phase space factor .
Figure 5: The posterior for assigning topic 0 or not in
NIPS by the ?-gram SWITCHING. Darker words indi-
cate a higher probability of not being assigned topic 0.
?h h
0.0?0.1 in spite, were unable, a sort, on behalf, . regardless
0.5?0.6 assumed it, rand mines, plans was, other excersises
0.9?1.0 that the, the existing, the new, their own, and spatial
Table 3: Some contexts h for various values of ?h in-
duced by the 3-gram HIERARCHICAL in BNC.
while prefixes of idioms have a lower value. The?-
gram extension gives us the posterior of n-gram or-
der p(n|h), which can be used to calculate the proba-
bility of a word ordering composing a phrase in topic
k as p(w, n|k, h) ? p(n|h)p(w|k, n, h). In Table
4, we show some higher probability topic-specific
phrases from the model trained on the NIPS.
7 Conclusion
We have presented modeling and algorithmic con-
tributions to the existing Bayesian n-gram topic
model. We explored two different priors to incor-
porate a global model, and found the effectiveness
of the flat structured model. We developed a novel
blocked Gibbs move for these types of models to ac-
celerate inference. We believe that this Gibbs op-
eration can be incorporated with other models hav-
ing a similar hierarchical structure. Empirically, we
demonstrate that by a careful model design and effi-
cient inference, a well-defined Bayesian model can
rival the conventional heuristics.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. The Journal of Ma-
chine Learning Research, 3:993?1022.
Phil Blunsom and Trevor Cohn. 2011. A hierarchi-
cal pitman-yor process hmm for unsupervised part of
speech induction. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 865?874,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
0 46
according to ? support vectors
? ( # ) in high dimentional
? section # as decision function
techniques such as set of # observations
? ( b ) original data set
83 89
the hierarchical mixtures ? linear discriminant
the rbf units images per class
the gating networks multi-class classification
grown hme ? decision boundaries
the modular architecture references per class
Table 4: Topical phrases from NIPS induced by the ?-
gram SWITCHING model. ? is a symbol for the beginning
of a sentence and # represents a number.
Phil Blunsom, Trevor Cohn, Sharon Goldwater, and Mark
Johnson. 2009. A note on the implementation of hi-
erarchical dirichlet processes. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
337?340, Suntec, Singapore, August. Association for
Computational Linguistics.
Daniel Gildea and Thomas Hofmann. 1999. Topic-based
language models using em. In In Proceedings of EU-
ROSPEECH, pages 2167?2170.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America,
101(Suppl 1):5228?5235.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In In Advances in Neural Information Pro-
cessing Systems 17, pages 537?544. MIT Press.
Songfang Huang and Steve Renals. 2008. Unsupervised
language model adaptation based on topic and role in-
formation in multiparty meetings. In in Proc. Inter-
speech08, pages 833?836.
F. Jelinek, B. Merialdo, S. Roukos, and M. Strauss. 1991.
A dynamic language model for speech recognition. In
Proceedings of the workshop on Speech and Natural
Language, HLT ?91, pages 293?295, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Robert Lindsey, William Headden, and Michael Stipice-
vic. 2012. A phrase-discovering topic model using hi-
erarchical pitman-yor processes. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 214?222, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Daichi Mochihashi and Eiichiro Sumita. 2008. The infi-
nite markov model. In J.C. Platt, D. Koller, Y. Singer,
and S. Roweis, editors, Advances in Neural Informa-
tion Processing Systems 20, pages 1017?1024. MIT
Press, Cambridge, MA.
1189
Adam Pauls and Dan Klein. 2012. Large-scale syntac-
tic language modeling with treelets. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics: Long Papers - Volume 1, pages
959?968. Association for Computational Linguistics.
Yik-Cheung Tam and Tanja Schultz. 2005. Dynamic lan-
guage model adaptation using variational bayes infer-
ence. In INTERSPEECH, pages 5?8.
Yik-Cheung Tam and Tanja Schultz. 2009. Correlated
bigram lsa for unsupervised language model adapta-
tion. In D. Koller, D. Schuurmans, Y. Bengio, and
L. Bottou, editors, Advances in Neural Information
Processing Systems 21, pages 1633?1640.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
Yee Whye Teh. 2006a. A Bayesian Interpretation of
Interpolated Kneser-Ney. NUS School of Computing
Technical Report TRA2/06.
Yee Whye Teh. 2006b. A hierarchical bayesian language
model based on pitman-yor processes. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 985?
992, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluation methods for
topic models. In Proceedings of the 26th Annual In-
ternational Conference on Machine Learning, ICML
?09, pages 1105?1112, New York, NY, USA. ACM.
Hanna M. Wallach. 2006. Topic modeling: beyond bag-
of-words. In Proceedings of the 23rd international
conference on Machine learning, ICML ?06, pages
977?984.
Xuerui Wang, Andrew McCallum, and Xing Wei. 2007.
Topical n-grams: Phrase and topic discovery, with an
application to information retrieval. In Proceedings
of the 2007 Seventh IEEE International Conference on
Data Mining, ICDM ?07, pages 697?702, Washington,
DC, USA. IEEE Computer Society.
Frank Wood and Yee Whye Teh. 2009. A hierarchi-
cal nonparametric Bayesian approach to statistical lan-
guage model domain adaptation. In Proceedings of the
International Conference on Artificial Intelligence and
Statistics, volume 12.
1190
Proceedings of the ACL 2010 Conference Short Papers, pages 184?188,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning Common Grammar from Multilingual Corpus
Tomoharu Iwata Daichi Mochihashi
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan
{iwata,daichi,sawada}@cslab.kecl.ntt.co.jp
Hiroshi Sawada
Abstract
We propose a corpus-based probabilis-
tic framework to extract hidden common
syntax across languages from non-parallel
multilingual corpora in an unsupervised
fashion. For this purpose, we assume a
generative model for multilingual corpora,
where each sentence is generated from a
language dependent probabilistic context-
free grammar (PCFG), and these PCFGs
are generated from a prior grammar that
is common across languages. We also de-
velop a variational method for efficient in-
ference. Experiments on a non-parallel
multilingual corpus of eleven languages
demonstrate the feasibility of the proposed
method.
1 Introduction
Languages share certain common proper-
ties (Pinker, 1994). For example, the word order
in most European languages is subject-verb-object
(SVO), and some words with similar forms are
used with similar meanings in different languages.
The reasons for these common properties can be
attributed to: 1) a common ancestor language,
2) borrowing from nearby languages, and 3) the
innate abilities of humans (Chomsky, 1965).
We assume hidden commonalities in syntax
across languages, and try to extract a common
grammar from non-parallel multilingual corpora.
For this purpose, we propose a generative model
for multilingual grammars that is learned in an
unsupervised fashion. There are some computa-
tional models for capturing commonalities at the
phoneme and word level (Oakes, 2000; Bouchard-
Co?te? et al, 2008), but, as far as we know, no at-
tempt has been made to extract commonalities in
syntax level from non-parallel and non-annotated
multilingual corpora.
In our scenario, we use probabilistic context-
free grammars (PCFGs) as our monolingual gram-
mar model. We assume that a PCFG for each
language is generated from a general model that
are common across languages, and each sentence
in multilingual corpora is generated from the lan-
guage dependent PCFG. The inference of the gen-
eral model as well as the multilingual PCFGs can
be performed by using a variational method for
efficiency. Our approach is based on a Bayesian
multitask learning framework (Yu et al, 2005;
Daume? III, 2009). Hierarchical Bayesian model-
ing provides a natural way of obtaining a joint reg-
ularization for individual models by assuming that
the model parameters are drawn from a common
prior distribution (Yu et al, 2005).
2 Related work
The unsupervised grammar induction task has
been extensively studied (Carroll and Charniak,
1992; Stolcke and Omohundro, 1994; Klein and
Manning, 2002; Klein and Manning, 2004; Liang
et al, 2007). Recently, models have been pro-
posed that outperform PCFG in the grammar in-
duction task (Klein and Manning, 2002; Klein and
Manning, 2004). We used PCFG as a first step
for capturing commonalities in syntax across lan-
guages because of its simplicity. The proposed
framework can be used for probabilistic grammar
models other than PCFG.
Grammar induction using bilingual parallel cor-
pora has been studied mainly in machine transla-
tion research (Wu, 1997; Melamed, 2003; Eisner,
2003; Chiang, 2005; Blunsom et al, 2009; Sny-
der et al, 2009). These methods require sentence-
aligned parallel data, which can be costly to obtain
and difficult to scale to many languages. On the
other hand, our model does not require sentences
to be aligned. Moreover, since the complexity of
our model increases linearly with the number of
languages, our model is easily applicable to cor-
184
pora of more than two languages, as we will show
in the experiments. To our knowledge, the only
grammar induction work on non-parallel corpora
is (Cohen and Smith, 2009), but their method does
not model a common grammar, and requires prior
information such as part-of-speech tags. In con-
trast, our method does not require any such prior
information.
3 Proposed Method
3.1 Model
Let X = {X l}l?L be a non-parallel and non-
annotated multilingual corpus, where X l is a set
of sentences in language l, and L is a set of lan-
guages. The task is to learn multilingual PCFGs
G = {Gl}l?L and a common grammar that gen-
erates these PCFGs. Here, Gl = (K,W l,?l)
represents a PCFG of language l, where K is a
set of nonterminals, W l is a set of terminals, and
?l is a set of rule probabilities. Note that a set of
nonterminals K is shared among languages, but
a set of terminals W l and rule probabilities ?l
are specific to the language. For simplicity, we
consider Chomsky normal form grammars, which
have two types of rules: emissions rewrite a non-
terminal as a terminal A ? w, and binary pro-
ductions rewrite a nonterminal as two nontermi-
nalsA? BC, whereA,B,C ?K and w ?W l.
The rule probabilities for each nonterminal
A of PCFG Gl in language l consist of: 1)
?Al = {?lAt}t?{0,1}, where ?lA0 and ?lA1 repre-
sent probabilities of choosing the emission rule
and the binary production rule, respectively, 2)
?lA = {?lABC}B,C?K , where ?lABC repre-
sents the probability of nonterminal production
A ? BC, and 3) ?lA = {?lAw}w?W l , where
?lAw represents the probability of terminal emis-
sion A? w. Note that ?lA0 + ?lA1 = 1, ?lAt ? 0,
?
B,C ?lABC = 1, ?lABC ? 0,
?
w ?lAw = 1,
and ?lAw ? 0. In the proposed model, multino-
mial parameters ?lA and ?lA are generated from
Dirichlet distributions that are common across lan-
guages: ?lA ? Dir(??A) and ?lA ? Dir(?
?
A),
since we assume that languages share a common
syntax structure. ??A and ?
?
A represent the param-
eters of a common grammar. We use the Dirichlet
prior because it is the conjugate prior for the multi-
nomial distribution. In summary, the proposed
model assumes the following generative process
for a multilingual corpus,
1. For each nonterminal A ?K :
?
??Aa,b
a,b
|L|
?A? lA
lA
|K|
? ?
?
? lA |L|
z 1
z 2 z 3
x2 x3
? 
? 
? ?
Figure 1: Graphical model.
(a) For each rule type t ? {0, 1}:
i. Draw common rule type parameters
??At ? Gam(a?, b?)
(b) For each nonterminal pair (B,C):
i. Draw common production parameters
??ABC ? Gam(a
?, b?)
2. For each language l ? L:
(a) For each nonterminal A ?K :
i. Draw rule type parameters
?lA ? Dir(??A)
ii. Draw binary production parameters
?lA ? Dir(?
?
A)
iii. Draw emission parameters
?lA ? Dir(??)
(b) For each node i in the parse tree:
i. Choose rule type
tli ? Mult(?lzi)
ii. If tli = 0:
A. Emit terminal
xli ? Mult(?lzi)
iii. Otherwise:
A. Generate children nonterminals
(zlL(i), zlR(i)) ? Mult(?lzi),
where L(i) and R(i) represent the left and right
children of node i. Figure 1 shows a graphi-
cal model representation of the proposed model,
where the shaded and unshaded nodes indicate ob-
served and latent variables, respectively.
3.2 Inference
The inference of the proposed model can be ef-
ficiently computed using a variational Bayesian
method. We extend the variational method to
the monolingual PCFG learning of Kurihara and
Sato (2004) for multilingual corpora. The goal
is to estimate posterior p(Z,?,?|X), where Z
is a set of parse trees, ? = {?l}l?L is a
set of language dependent parameters, ?l =
{?lA,?lA,?lA}A?K , and ? = {?
?
A,?
?
A}A?K
is a set of common parameters. In the variational
method, posterior p(Z,?,?|X) is approximated
by a tractable variational distribution q(Z,?,?).
185
We use the following variational distribution,
q(Z,?,?) =
?
A
q(??A)q(?
?
A)
?
l,d
q(zld)
?
?
l,A
q(?lA)q(?lA)q(?lA), (1)
where we assume that hyperparameters q(??A) and
q(??A) are degenerated, or q(?) = ???(?), and
infer them by point estimation instead of distribu-
tion estimation. We find an approximate posterior
distribution that minimizes the Kullback-Leibler
divergence from the true posterior. The variational
distribution of the parse tree of the dth sentence in
language l is obtained as follows,
q(zld) ?
?
A?BC
(
pi?lA1pi
?
lABC
)C(A?BC;zld,l,d)
?
?
A?w
(
pi?lA0pi
?
lAw
)C(A?w;zld,l,d)
, (2)
where C(r; z, l, d) is the count of rule r that oc-
curs in the dth sentence of language l with parse
tree z. The multinomial weights are calculated as
follows,
pi?lAt = exp
(
Eq(?lA)
[
log ?lAt
])
, (3)
pi?lABC = exp
(
Eq(?lA)
[
log ?lABC
])
, (4)
pi?lAw = exp
(
Eq(?lA)
[
log?lAw
])
. (5)
The variational Dirichlet parameters for q(?lA) =
Dir(??lA), q(?lA) = Dir(?
?
lA), and q(?lA) =
Dir(??lA), are obtained as follows,
??lAt = ??At +
?
d,zld
q(zld)C(A, t; zld, l, d), (6)
??lABC = ?
?
ABC+
?
d,zld
q(zld)C(A?BC; zld, l, d),
(7)
??lAw = ?
? +
?
d,zld
q(zld)C(A? w; zld, l, d),
(8)
where C(A, t; z, l, d) is the count of rule type t
that is selected in nonterminal A in the dth sen-
tence of language l with parse tree z.
The common rule type parameter ??At that min-
imizes the KL divergence between the true pos-
terior and the approximate posterior can be ob-
tained by using the fixed-point iteration method
described in (Minka, 2000). The update rule is as
follows,
??(new)At ?
a??1+??AtL
(
?(
?
t? ??At?)??(??At)
)
b? +
?
l
(
?(
?
t? ??lAt?)??(??lAt)
) ,
(9)
where L is the number of languages, and ?(x) =
? log ?(x)
?x is the digamma function. Similarly, the
common production parameter ??ABC can be up-
dated as follows,
??(new)ABC ?
a? ? 1 + ??ABCLJABC
b? +
?
l J ?lABC
, (10)
where JABC = ?(
?
B?,C? ?
?
AB?C?) ? ?(?
?
ABC),
and J ?lABC = ?(
?
B?,C? ?
?
lAB?C?)??(?
?
lABC).
Since factored variational distributions depend
on each other, an optimal approximated posterior
can be obtained by updating parameters by (2) -
(10) alternatively until convergence. The updat-
ing of language dependent distributions by (2) -
(8) is also described in (Kurihara and Sato, 2004;
Liang et al, 2007) while the updating of common
grammar parameters by (9) and (10) is new. The
inference can be carried out efficiently using the
inside-outside algorithm based on dynamic pro-
gramming (Lari and Young, 1990).
After the inference, the probability of a com-
mon grammar rule A ? BC is calculated by
??A?BC = ??1??ABC , where ??1 = ??1/(??0 + ??1)
and ??ABC = ??ABC/
?
B?,C? ?
?
AB?C? represent
the mean values of ?l0 and ?lABC , respectively.
4 Experimental results
We evaluated our method by employing the Eu-
roParl corpus (Koehn, 2005). The corpus con-
sists of the proceedings of the European Parlia-
ment in eleven western European languages: Dan-
ish (da), German (de), Greek (el), English (en),
Spanish (es), Finnish (fi), French (fr), Italian (it),
Dutch (nl), Portuguese (pt), and Swedish (sv), and
it contains roughly 1,500,000 sentences in each
language. We set the number of nonterminals at
|K| = 20, and omitted sentences with more than
ten words for tractability. We randomly sampled
100,000 sentences for each language, and ana-
lyzed them using our method. It should be noted
that our random samples are not sentence-aligned.
Figure 2 shows the most probable terminals of
emission for each language and nonterminal with
a high probability of selecting the emission rule.
186
2: verb and auxiliary verb (V)
5: noun (N)
7: subject (SBJ)
9: preposition (PR)
11: punctuation (.)
13: determiner (DT)
Figure 2: Probable terminals of emission for each
language and nonterminal.
0? 16 11 (R? S . ) 0.11
16? 7 6 (S? SBJ VP) 0.06
6? 2 12 (VP? V NP) 0.04
12? 13 5 (NP? DT N) 0.19
15? 17 19 (NP? NP N) 0.07
17? 5 9 (NP? N PR) 0.07
15? 13 5 (NP? DT N) 0.06
Figure 3: Examples of inferred common gram-
mar rules in eleven languages, and their proba-
bilities. Hand-provided annotations have the fol-
lowing meanings, R: root, S: sentence, NP: noun
phrase, VP: verb phrase, and others appear in Fig-
ure 2.
We named nonterminals by using grammatical cat-
egories after the inference. We can see that words
in the same grammatical category clustered across
languages as well as within a language. Fig-
ure 3 shows examples of inferred common gram-
mar rules with high probabilities. Grammar rules
that seem to be common to European languages
have been extracted.
5 Discussion
We have proposed a Bayesian hierarchical PCFG
model for capturing commonalities at the syntax
level for non-parallel multilingual corpora. Al-
though our results have been encouraging, a num-
ber of directions remain in which we must extend
our approach. First, we need to evaluate our model
quantitatively using corpora with a greater diver-
sity of languages. Measurement examples include
the perplexity, and machine translation score. Sec-
ond, we need to improve our model. For ex-
ample, we can infer the number of nonterminals
with a nonparametric Bayesian model (Liang et
al., 2007), infer the model more robustly based
on a Markov chain Monte Carlo inference (John-
son et al, 2007), and use probabilistic grammar
models other than PCFGs. In our model, all the
multilingual grammars are generated from a gen-
eral model. We can extend it hierarchically using
the coalescent (Kingman, 1982). That model may
help to infer an evolutionary tree of languages in
terms of grammatical structure without the etymo-
logical information that is generally used (Gray
and Atkinson, 2003). Finally, the proposed ap-
proach may help to indicate the presence of a uni-
versal grammar (Chomsky, 1965), or to find it.
187
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2009.
Bayesian synchronous grammar induction. In D. Koller,
D. Schuurmans, Y. Bengio, and L. Bottou, editors, Ad-
vances in Neural Information Processing Systems 21,
pages 161?168.
Alexandre Bouchard-Co?te?, Percy Liang, Thomas Griffiths,
and Dan Klein. 2008. A probabilistic approach to lan-
guage change. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information Pro-
cessing Systems 20, pages 169?176, Cambridge, MA.
MIT Press.
Glenn Carroll and Eugene Charniak. 1992. Two experiments
on learning probabilistic dependency grammars from cor-
pora. In Working Notes of the Workshop Statistically-
Based NLP Techniques, pages 1?13. AAAI.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In ACL ?05: Proceedings
of the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263?270, Morristown, NJ, USA.
Association for Computational Linguistics.
Norm Chomsky. 1965. Aspects of the Theory of Syntax. MIT
Press.
Shay B. Cohen and Noah A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsuper-
vised grammar induction. In NAACL ?09: Proceedings of
Human Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Association
for Computational Linguistics, pages 74?82, Morristown,
NJ, USA. Association for Computational Linguistics.
Hal Daume? III. 2009. Bayesian multitask learning with la-
tent hierarchies. In Proceedings of the Twenty-Fifth An-
nual Conference on Uncertainty in Artificial Intelligence
(UAI-09), pages 135?142, Corvallis, Oregon. AUAI Press.
Jason Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In ACL ?03: Proceedings of the
41st Annual Meeting on Association for Computational
Linguistics, pages 205?208, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Russell D. Gray and Quentin D. Atkinson. 2003. Language-
tree divergence times support the Anatolian theory of
Indo-European origin. Nature, 426(6965):435?439,
November.
Mark Johnson, Thomas Griffiths, and Sharon Goldwater.
2007. Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Human Language Technologies 2007:
The Conference of the North American Chapter of the
Association for Computational Linguistics; Proceedings
of the Main Conference, pages 139?146, Rochester, New
York, April. Association for Computational Linguistics.
J. F. C. Kingman. 1982. The coalescent. Stochastic Pro-
cesses and their Applications, 13:235?248.
Dan Klein and Christopher D. Manning. 2002. A generative
constituent-context model for improved grammar induc-
tion. In ACL ?02: Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics, pages
128?135, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of depen-
dency and constituency. In ACL ?04: Proceedings of the
42nd Annual Meeting on Association for Computational
Linguistics, page 478, Morristown, NJ, USA. Association
for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of the 10th
Machine Translation Summit, pages 79?86.
Kenichi Kurihara and Taisuke Sato. 2004. An applica-
tion of the variational Bayesian approach to probabilistic
context-free grammars. In International Joint Conference
on Natural Language Processing Workshop Beyond Shal-
low Analysis.
K. Lari and S.J. Young. 1990. The estimation of stochastic
context-free grammars using the inside-outside algorithm.
Computer Speech and Language, 4:35?56.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical dirichlet pro-
cesses. In EMNLP ?07: Proceedings of the Empirical
Methods on Natural Language Processing, pages 688?
697.
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In NAACL ?03: Proceedings of the 2003
Conference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Language
Technology, pages 79?86, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Thomas Minka. 2000. Estimating a Dirichlet distribution.
Technical report, M.I.T.
Michael P. Oakes. 2000. Computer estimation of vocabu-
lary in a protolanguage from word lists in four daughter
languages. Journal of Quantitative Linguistics, 7(3):233?
243.
Steven Pinker. 1994. The Language Instinct: How the Mind
Creates Language. HarperCollins, New York.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction. In
Proceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint Con-
ference on Natural Language Processing of the AFNLP,
pages 73?81, Suntec, Singapore, August. Association for
Computational Linguistics.
Andreas Stolcke and Stephen M. Omohundro. 1994. In-
ducing probabilistic grammars by Bayesian model merg-
ing. In ICGI ?94: Proceedings of the Second International
Colloquium on Grammatical Inference and Applications,
pages 106?118, London, UK. Springer-Verlag.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Comput.
Linguist., 23(3):377?403.
Kai Yu, Volker Tresp, and Anton Schwaighofer. 2005.
Learning gaussian processes from multiple tasks. In
ICML ?05: Proceedings of the 22nd International Confer-
ence on Machine Learning, pages 1012?1019, New York,
NY, USA. ACM.
188
