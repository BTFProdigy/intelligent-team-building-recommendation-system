Classifying Biological Full-Text Articles for Multi-Database Curation 
Wen-Juan Hou, Chih Lee and Hsin-Hsi Chen
Department of Computer Science and Information Engineering, 
National Taiwan University, Taipei, Taiwan 
{wjhou, clee}@nlg.csie.ntu.edu.tw; hhchen@csie.ntu.edu.tw
Abstract
In this paper, we propose an approach 
for identifying curatable articles from a 
large document set.  This system 
considers three parts of an article (title 
and abstract, MeSH terms, and captions) 
as its three individual representations 
and utilizes two domain-specific 
resources (UMLS and a tumor name list) 
to reveal the deep knowledge contained 
in the article.  An SVM classifier is 
trained and cross-validation is employed 
to find the best combination of 
representations.  The experimental 
results show overall high performance. 
1 Introduction 
Organism databases play a crucial role in 
genomic and proteomic research.  It stores the 
up-to-date profile of each gene of the species 
interested.  For example, the Mouse Genome 
Database (MGD) provides essential integration 
of experimental knowledge for the mouse 
system with information annotated from both 
literature and online sources (Bult et al, 2004).  
To provide biomedical scientists with easy 
access to complete and accurate information, 
curators have to constantly update databases 
with new information.  With the rapidly 
growing rate of publication, it is impossible for 
curators to read every published article.  Since 
fully automated curation systems have not met 
the strict requirement of high accuracy and recall, 
database curators still have to read some (if not 
all) of the articles sent to them.  Therefore, it 
will be very helpful if a classification system can 
correctly identify the curatable or relevant 
articles in a large number of biological articles. 
Recently, several attempts have been made to 
classify documents from biomedical domain 
(Hirschman et al, 2002).  Couto et al (2004) 
used the information extracted from related web 
resources to classify biomedical literature.  Hou 
et al (2005) used the reference corpus to help 
classifying gene annotation.  The Genomics 
Track (http://ir.ohsu.edu/genomics) of TREC 
2004 and 2005 organized categorization tasks.  
The former focused on simplified GO terms 
while the latter included the triage for "tumor 
biology", "embryologic gene expression", 
"alleles of mutant phenotypes" and "GO" articles.  
The increase of the numbers of participants at 
Genomics Track shows that biological 
classification problems attracted much attention. 
This paper employs the domain-specific 
knowledge and knowledge learned from full-text 
articles to classify biological text.  Given a 
collection of articles, various methods are 
explored to extract features to represent a 
document.  We use the experimental data 
provided by the TREC 2005 Genomics Track to 
evaluate different methods. 
The rest of this paper is organized as follows.  
Section 2 sketches the overview of the system 
architecture.  Section 3 specifies the test bed 
used to evaluate the proposed methods.  The 
details of the proposed system are explained in 
Section 4.  The experimental results are shown 
and discussed in Section 5.  Finally, we make 
conclusions and present some further work. 
2 System Overview 
Figure 1 shows the overall architecture of the 
proposed system.  At first, we preprocess each 
training article, and divide it into three parts, 
including (1) title and abstract, (2) MeSH terms 
assigned to this article, and (3) captions of 
figures and tables.  They are denoted as 
"Abstract", "MeSH", and "Caption" in this paper, 
respectively.  Each part is considered as a 
representation of an article.  With the help of 
domain-specific knowledge, we obtain more 
detail representations of an article.  In the 
model selection phase, we perform feature 
ranking on each representation of an article and 
employ cross-validation to determine the 
number of features to be kept.  Moreover, we 
use cross-validation to obtain the best 
combination of all the representations.  Finally, 
a support vector machine (SVM) (Vapnik, 1995; 
Hsu et al, 2003) classifier is obtained. 
159
3 Experimental Data
We train classifiers for classifying biomedical 
articles on the Categorization Task of the TREC 
2005 Genomics Track. The task uses data from 
the Mouse Genome Informatics (MGI) system
(http://www.informatics.jax.org/) for four
categorization tasks, including tumor biology,
embryologic gene expression, alleles of mutant
phenotypes and GO annotation. Given a 
document and a category, we have to identify
whether it is relevant to the given category.
The document set consists of some full-text 
data obtained from three journals, i.e., Journal of
Biological Chemistry, Journal of Cell Biology
and Proceedings of the National Academy of 
Science in 2002 and 2003.  There are 5,837
training documents and 6,043 testing documents.
4 Methods 
4.1 Document Preprocessing
In the preprocessing phase, we perform acronym
expansion on the articles, remove the remaining
tags from the articles and extract three parts of 
interest from each article.  Abbreviations are 
often used to replace long terms in writing 
articles, but it is possible that several long terms
share the same short form, especially for
gene/protein names. To avoid ambiguity and
enhance clarity, the acronym expansion 
operation replaces every tagged abbreviation 
with its long form followed by itself in a pair of 
parentheses.
4.2 Employing Domain-Specific Knowledge 
With the help of domain-specific knowledge, we 
can extract the deeper knowledge in an article. 
For example, with a gene name dictionary, we
can identify the gene names contained in an 
article.  Moreover, by further consulting
organism databases, we can get the properties of
the genes. Two domain-specific resources are
exploited in this study.  One is the Unified 
Medical Language System (UMLS) (Humphreys
et al, 1998) and the other is a list of tumor
names obtained from Mouse Tumor Biology
Database (MTB)1.
UMLS contains a huge dictionary of
biomedical terms ? the UMLS Metathesaurus
and defines a hierarchy of semantic types ? the 
UMLS Semantic Network. Each concept in the
Metathesaurus contains a set of strings, which
are variants of each other and belong to one or
more semantic types in the Semantic Network.
Therefore, given a string, we can obtain a set of 
semantic types to which it belongs. Then we 
obtain another representation of the article by 
gathering the semantic types found in the part of 
the article. Consequently, we get another three 
much deeper representations of an article after
this step. They are denoted as "AbstractSEM",
"MeSHSEM" and "CaptionSEM". 
We use the list of tumor names on the Tumor
task. We first tokenize all the tumor names and 
stem each unique token. With the resulting list 
of unique stemmed tokens, we use it as a filter to 
remove the tokens not in the list from the 
"Abstract" and "Caption", which produce 
"AbstractTM" and "CaptionTM".
4.3 Model Selection
As mentioned above, we generate several 
representations for an article. In this section, 
we explain how feature selection is done and
how the best combination of the representations 
1 http://tumor.informatics.jax.org/mtbwi/tumorSearch.do
A New 
Full-Text
Article
Full-Text
Training
Articles
Abstract
MeSH
Caption
Model
Selection
AbsSEM/TM
Preprocessing
MeSHSEM
CapSEM/TM
Domain-Specific
Knowledge
SVM
Classifier
Yes/NoPartsSEM/TMPreprocessing Multiple
Parts
Figure 1. System Architecture
160
of an article is obtained. 
For each representation, we first rank all the 
tokens in the training documents via the 
chi-square test of independence.  Postulating 
the ranking perfectly reflects the effectiveness of 
the tokens in classification, we then decide the 
number of tokens to be used in SVM 
classification by 4-fold cross-validation.  In 
cross-validation, we use the TF*IDF weighting 
scheme.  Each feature vector is then 
normalized to a unit vector.  We set C+ to ur* C-
because of the relatively small number of 
positive examples, where C+ and C- are the 
penalty constants on positive and negative 
examples in SVMs.  After that, we obtain the 
optimal number of tokens and the corresponding 
SVM parameters C- and gamma, a parameter in 
the radial basis kernel.  In the rest of this paper, 
"Abstract30" denotes the "Abstract" 
representation with top-30 tokens, 
"CaptionSEM10" denotes "CaptionSEM" with 
top-10 tokens, and so forth. 
After feature selection is done for each 
representation, we try to find the best 
combination by the following algorithm. 
Given the candidate representations with 
selected features, we start with an initial set 
containing some or zero representation.  For 
each iteration, we add one representation to the 
set by picking the one that enhances the 
cross-validation performance the most.  The 
iteration stops when we have exhausted all the 
representations or adding more representation to 
the set doesn?t improve the cross-validation 
performance. 
For classifying the documents with better 
features, we run the algorithm twice.  We first 
start with an empty set and obtain the best 
combination of the basic three representations, 
e.g., "Abstract10", "MeSH30" and "Caption10".  
Then, starting with this combination, we attempt 
to incorporate the three semantic representations, 
e.g., "Abstract30SEM", "MeSH30SEM" and 
"Caption10SEM", and obtain the final 
combination.  Instead of using this algorithm to 
incorporate the "AbstractTM" and "CaptionTM" 
representations, we use them to replace their 
unfiltered counterparts "Abstract" and "Caption" 
when the cross-validation performance is better. 
5 Results and Discussions 
Table 1 lists the cross-validation results of each 
representation for each category (in Normalized 
Utility (NU)2 measure).  For category Allele, 
"Caption" and "AbstractSEM" perform the best 
among the basic and semantic representations, 
respectively.  For category Expression, 
"Caption" plays an important role in identifying 
relevant documents, which agrees with the 
finding by the winner of KDD CUP 2002 task 1 
(Regev et al, 2002).  Similarly, MeSH terms 
are crucial to the GO category, which are used 
by top-performing teams (Dayanik et al, 2004; 
Fujita, 2004) in TREC Genomics 2004.  For 
category Tumor, MeSH terms are important, but 
after semantic type extraction, "AbstractSEM" 
exhibits relatively high cross-validation 
performance.  Since only 10 features are 
selected for the "AbstractSEM", using this 
representation alone may be susceptible to 
over-fitting.  Finally, by comparing the 
performance of the "AbstractTM" and 
"Abstract", we find the list of tumor names 
helpful for filtering abstracts. 
We list the results for the test data in Table 2.  
Column "Experiment" identifies our proposed 
methods.  We show six experiments in Table 2: 
one for Allele (AL), one for Expression (EX), 
one for GO (GO) and three for Tumor (TU, TN 
and TS).  Column "cv NU" shows the 
cross-validation NU measure, "NU" shows the 
performance on the test data and column 
"Combination" lists the combination of the 
representations used for each experiment.  In 
this table, "M30" is the abbreviation for 
"MeSH30", "CS10" is for "CaptionSEM10", and 
so on.  The combinations for the first 4 
experiments, i.e., AL, EX, GO and TU, are 
obtained by the algorithm described in Section 
4.3, while the combination for TN is obtained by 
substituting "AbstractTM30" for "Abstract30" in 
the combination for TU.  The experiment TS 
only uses the "AbstractSEM10" because its 
cross-validation performance beats all other 
combinations for the Tumor category. 
The combinations of the first 5 experiments 
illustrate that adding other inferior 
representations to the best one enhances the 
performance, which implies that the inferior 
ones may contain important exclusive 
information.  The cross-validation performance 
fairly predicts the performance on the test data, 
except for the last experiment TS, which relies 
on only 10 features and is therefore susceptible 
to over-fitting. 
                                                 
2 Please refer to the TREC 2005 Genomics Track Protocol 
(http://ir.ohsu.edu/genomics/2005protocol.html).
161
Allele Expression GO Tumor 
# Tokens / NU # Tokens / NU # Tokens / NU # Tokens / NU 
Abstract 10 / 0.7707 10 / 0.5586 10 / 0.4411 10 / 0.8055 
MeSH 10 / 0.7965 10 / 0.6044 10 / 0.4968 30 / 0.8106 
Caption 10 / 0.8179 10 / 0.7192 10 / 0.4091 10 / 0.7644 
AbstractSEM 10 / 0.7209 10 / 0.4811 10 / 0.3493 10 / 0.8814 
MeSHSEM 10 / 0.6942 10 / 0.4563 10 / 0.4403 10 / 0.7047 
CaptionSEM 30 / 0.6789 10 / 0.5433 10 / 0.2551 30 / 0.7160 
AbstractTM 30 / 0.8325 
CaptionTM 10 / 0.7498 
Table 1. Partial Cross-validation Results. 
Experiment cv NU NU Recall Precision F-score Combination 
AL (for Allele) 0.8717 0.8423 0.9488 0.3439 0.5048 M30+C10+A10+CS10+AS10+MS10 
EX (for Expression) 0.7691 0.7515 0.8190 0.1593 0.2667 M10+C10+CS10+MS10 
GO (for GO) 0.5402 0.5332 0.8803 0.1873 0.3089 M10+C10+MS10 
TU (for Tumor) 0.8742 0.8299 0.9000 0.0526 0.0994 M30+C30+A30+AS10+CS30 
TN (for Tumor) 0.8764 0.8747 0.9500 0.0518 0.0982 M30+C30+AT30+AS10+CS30 
TS (for Tumor) 0.8814 0.5699 0.6500 0.0339 0.0645 AS10
Table 2. Evaluation Results. 
Subtask NU (Best/Median) Recall (Best/Median) Precision (Best/Median) F-score (Best/Median) 
Allele 0.8710/0.7773 0.9337/0.8720 0.4669/0.3153 0.6225/0.5010 
Expression 0.8711/0.6413 0.9333/0.7286 0.1899/0.1164 0.3156/0.2005 
GO Annotation 0.5870/0.4575 0.8861/0.5656 0.2122/0.3223 0.3424/0.4107 
Tumor 0.9433/0.7610 1.0000/0.9500 0.0709/0.0213 0.1325/0.0417 
Table 3. Best and Median Results for Each Subtask on TREC 2005 (Hersh et al, 2005). 
To compare with our performance, we list the 
best and median results for each subtask on the 
genomics classification task of TREC 2005 in 
Table 3.  Comparing to Tables 2 and 3, it shows 
our experimental results have overall high 
performance. 
6 Conclusions and Further Work 
In this paper, we demonstrate how our system is 
constructed.  Three parts of an article are 
extracted to represent its content.  We 
incorporate two domain-specific resources, i.e., 
UMLS and a list of tumor names.  For each 
categorization work, we propose an algorithm to 
get the best combination of the representations 
and train an SVM classifier out of this 
combination.  Evaluation results show overall 
high performance in this study. 
Except for MeSH terms, we can try other 
sections in the article, e.g., Results, Discussions 
and Conclusions as targets of feature extraction 
besides the abstract and captions in the future.  
Finally, we will try to make use of other 
available domain-specific resources in hope of 
enhancing the performance of this system. 
Acknowledgements
Research of this paper was partially supported by 
National Science Council, Taiwan, under the 
contracts NSC94-2213-E-002-033 and 
NSC94-2752-E-001-001-PAE. 
References 
Bult, C.J., Blake, J.A., Richardson, J.E., Kadin, J.A., Eppig, 
J.T. and the Mouse Genome Database Group. The Mouse 
Genome Database (MGD): Integrating Biology with the 
Genome. Nucleic Acids Research, 32, D476?D481, 2004. 
Couto, F.M., Martins, B. and Silva, M.J. Classifying Biological 
Articles Using Web Resources. Proceedings of the 2004 
ACM Symposium on Applied Computing, 111-115, 2004. 
Dayanik, A., Fradkin, D., Genkin, A., Kantor, P., Lewis, D.D., 
Madigan, D. and Menkov, V. DIMACS at the TREC 2004 
Genomics Track. Proceedings of the Thirteenth Text 
Retrieval Conference, 2004. 
Fujita, S., Revisiting Again Document Length Hypotheses 
TREC-2004 Genomics Track Experiments at Patolis. 
Proceedings of the Thirteenth Text Retrieval Conference,
2004.
Hersh, W., Cohen, A., Yang, J., Bhuptiraju, R.T., Toberts, P. 
and Hearst, M. TREC 2005 Genomics Track Overview. 
Proceedings of the Fourteenth Text Retrieval Conference,
2005.
Hirschman, L., Park, J., Tsujii, J., Wong, L. and Wu, C.H. 
Accomplishments and Challenges in Literature Data 
Mining for Biology. Bioinformatics, 18(12): 1553-1561, 
2002.
Hou, W.J., Lee, C., Lin, K.H.Y. and Chen, H.H. A Relevance 
Detection Approach to Gene Annotation. Proceedings of the 
First International Symposium on Semantic Mining in 
Biomedicine, http://ceur-ws.org, 148: 15-23, 2005. 
Hsu, C.W., Chang, C.C. and Lin, C.J. A Practical Guide to 
Support Vector Classification. http://www.csie.ntu.edu.tw 
/~cjlin/libsvm/index.html, 2003. 
Humphreys, B.L., Lindberg, D.A., Schoolman, H.M. and 
Barnett, G.O. The Unified Medical Language System: an 
Informatics Research Collaboration. Journal of American 
Medical Information Association, 5(1):1-11, 1998. 
Regev, Y., Finkelstein-Landau, M. and Feldman, R. Rule-based 
Extraction of Experimental Evidence in the Biomedical 
Domain - the KDD Cup (Task 1). SIGKDD Explorations,
4(2):90-92, 2002. 
Vapnik, V. The Nature of Statistical Learning Theory,
Springer-Verlag, 1995. 
162
Support Vector Machine Approach to Extracting  
Gene References into Function from Biological Documents 
Chih Lee, Wen-Juan Hou and Hsin-Hsi Chen 
Natural Language Processing Laboratory 
Department of Computer Science and Information Engineering 
National Taiwan University 
1 Roosevelt Road, Section 4, Taipei, Taiwan, 106 
{clee, wjhou}@nlg.csie.ntu.edu.tw, hh_chen@csie.ntu.edu.tw 
 
Abstract 
In the biological domain, extracting newly 
discovered functional features from the 
massive literature is a major challenging issue.  
To automatically annotate Gene References 
into Function (GeneRIF) in a new literature is 
the main goal of this paper.  We tried to find 
GRIF words in a training corpus, and then 
applied these informative words to annotate the 
GeneRIFs in abstracts with several different 
weighting schemes.  The experiments showed 
that the Classic Dice score is at most 50.18%, 
when the weighting schemes proposed in the 
paper (Hou et al, 2003) were adopted.  In 
contrast, after employing Support Vector 
Machines (SVMs) and the definition of classes 
proposed by Jelier et al (2003), the score 
greatly improved to 56.86% for Classic Dice 
(CD).  Adopting the same features, SVMs 
demonstrated advantage over the Na?ve Bayes 
Classifier.  Finally, the combination of the 
former two models attained a score of 59.51% 
for CD. 
1 Introduction 
Text Retrieval Conference (TREC) has been 
dedicated to information retrieval and information 
extraction for years.  TREC 2003 introduced a new 
track called Genomics Track (Hersh and 
Bhupatiraju, 2003) to address the information 
retrieval and information extraction issues in the 
biomedical domain.  For the information extraction 
part, the goal was to automatically reproduce the 
Gene Reference into Function (GeneRIF) resource 
in the LocusLink database (Pruitt et al, 2000.) 
GeneRIF associated with a gene is a sentence 
describing the function of that gene, and is currently 
manually generated. 
This paper presents the post-conference work on 
the information extraction task (i.e., secondary task).  
In the official runs, our system (Hou et al, 2003) 
adopted several weighting schemes (described in 
Section 3.2) to deal with this problem.  However, 
we failed to beat the simple baseline approach, 
which always picks the title of a publication as the 
candidate GeneRIF.  Bhalotia et al (2003) 
converted this task into a binary classification 
problem and trained a Na?ve  Bayes classifier with 
kernels, achieving 53.04% for CD.  In their work, 
the title and last sentence of an abstract were 
concatenated and features were then extracted from 
the resulting string.  Jelier et al (2003) observed the 
distribution of target GeneRIFs in 9 sentence 
positions and converted this task into a 9-class 
classification problem, attaining 57.83% for CD.  
Both works indicated that the sentence position is 
of great importance.  We therefore modified our 
system to incorporate the position information with 
the help of SVMs and we also investigated the 
capability of SVMs versus Na?ve  Bayes on this 
problem. 
The rest of this paper is organized as follows.  
Section 2 presents the architecture of our extracting 
procedure.  The basic idea and the experimental 
methods in this study are introduced in Section 3.  
Section 4 shows the results and makes some 
discussions.  Finally, Section 5 concludes the 
remarks and lists some future works. 
2 Architecture Overview 
A complete annotation system may be done at two 
stages, including (1) extraction of molecular 
function for a gene from a publication and (2) 
alignment of this function with a GO term.  Figure 
1 shows an example.  The left part is an MEDLINE 
abstract with the function description highlighted.  
The middle part is the corresponding GeneRIF.  
The matching words are in bold, and the similar 
words are underlined.  The right part is the GO 
annotation.  This figure shows a possible solution of 
maintaining the knowledge bases and ontology 
using natural language processing technology.  We 
addressed automation of the first stage in this paper. 
The overall architecture is shown in Figure 2.  
First, we constructed a training corpus in such a 
way that GeneRIFs were collected from LocusLink 
and the corresponding abstracts were retrieved from 
54
MEDLINE.  ?GRIF words? and their weights were 
derived from the training corpus.  Then Support 
Vector Machines were trained using the derived 
corpus.  Given a new abstract, a sentence is selected 
from the abstract to be the candidate GeneRIF. 
3 Methods  
We adopted several weighting schemes to locate the 
GeneRIF sentence in an abstract in the official runs 
(Hou et al, 2003).  Inspired by the work by Jelier et 
al. (2003), we incorporated their definition of 
classes into our weighting schemes, converting this 
task into a classification problem using SVMs as 
the classifier.  We ran SVMs on both sets of 
features proposed by Hou et al (2003) and Jelier et 
al. (2003), respectively.  Finally, all the features 
were combined and some feature selection methods 
were applied to train the classifier. 
3.1 Training and test material preparation 
Since GeneRIFs are often cited verbatim from 
abstracts, we decided to reproduce the GeneRIF by 
selecting one sentence in the abstract.  Therefore, 
for each abstract in our training corpus, the sentence 
most similar to the GeneRIF was labelled as the 
GeneRIF sentence using Classic Dice coefficient as 
similarity measure.  Totally, 259,244 abstracts were 
used, excluding the abstracts for testing.  The test 
data for evaluation are the 139 abstracts used in 
TREC 2003 Genomics track. 
3.2 GRIF words extraction and weighting 
scheme  
We called the matched words between GeneRIF 
and the selected sentence as GRIF words in this 
paper.  GRIF words represent the favorite 
vocabulary that human experts use to describe gene 
functions.  After stop word removal and stemming 
operation, 10,506 GRIF words were extracted. 
In our previous work (Hou et al, 2003), we first 
generated the weight for each GRIF word.  Given 
an abstract, the score of each sentence is the sum of 
weights of all the GRIF words in this sentence.  
Finally, the sentence with the highest score is 
selected as the  candidate GeneRIF.  This method is 
denoted as OUR weighting scheme, and several 
heuristic weighting schemes were investigated.  
Here, we only present the weighting scheme used in 
SVMs classification.  The weighting scheme is as 
follows. For GRIF word i, the number of 
occurrence Gin  in all the GeneRIF sentences and the 
number of occurrence Ain  in all the abstracts were 
computed and AiGi nn /  was assigned to GRIF word i 
as its weight. 
3.3 Classification 
3.3.1 Class definition and feature extraction 
The distribution of GeneRIF sentences showed that 
the position of a sentence in an abstract is an 
important clue to where the answer sentence is.  
Jelier et al (2003) considered only the title, the first 
three and the last five sentences, achieving the best 
performance in TREC official runs.  Their Na?ve 
Bayes model is as follows.  An abstract a is 
assigned a class vj by calculating vNB: 
 
Existing 
GeneRIFs 
on 
LocusLink 
Corresponding 
Medline 
Abstracts  
GRIF Word 
Extractor 
Weighted GRIF 
Words 
Generating 
Training Data 
Training SVMs 
New 
Abstract 
GeneRIF 
 Sentence Locator  
Candidate 
GeneRIF 
 
Figure 2: Architecture of Extracting Candidate GeneRIF 
Figure 1: An Example of Complete Annotation from a Literature to Gene Ontology 
 
extraction 
alignm
ent 
The Bcl10 gene was recently isolated 
from the breakpoint region of 
t(1;14)(p22;q32) in mucosa-associated 
lymphoid tissue (MALT) lymphomas. 
Somatic mutations of Bcl10 were found 
in not only t(1;14)-bearing MALT 
lymphomas, but also a wide range of 
other tumors. ? ? Our results strongly 
suggest that somatic mutations  of Bcl10 
are extremely rare in malignant 
cartilaginous tumors  and do not 
commonly contribute to their molecular 
pathogenesis. 
PMID: 11836626 
MEDLINE abstract 
Mutations, 
relatively 
common in 
lymphomas, 
are extremely 
rare in 
malignant 
cartilaginous 
tumors. 
GeneRIF 
l GO:0005515  
term: protein binding 
definition: Interacting selectively with any protein, or 
protein complex (a complex of two or more proteins that 
may include other nonprotein molecules). 
l GO:0008181  
term: tumor suppressor 
l GO:0006917  
term: induction of apoptosis 
l GO:0005622 
term: intracellular 
l GO:0016329  
term: apoptosis regulator activity 
definition: The function held by products which directly 
regulate any step in the process of apoptosis. 
l GO:0045786  
term: negative regulation of cell cycle 
GO annotation 
55
  CD MUD MBD MBDP 
1 Jelier (Sentence-wise bag of words + Na?ve  Bayes) 57.83% 59.63% 46.75% 49.11% 
2 Sentence-wise bag of words + SVMs 58.92% 61.46% 47.86% 50.84% 
3 OUR Weighting scheme 50.18% 46.71% 33.47% 38.83% 
4 OUR Weighting scheme + SVMs 56.86% 58.81% 45.08% 48.10% 
5 Combined 59.51% 62.16% 48.17% 51.25% 
6 Combined + gene/protein names 57.59% 59.95% 46.69% 49.68% 
7 Combined + BWRatio feature selection 57.59% 59.90% 47.11% 50.08% 
8 Combined + Graphical feature selection 58.81% 61.09% 47.98% 50.92% 
9 Optimal Classifier 67.60% 70.74% 59.28% 62.09% 
10 Baseline 50.47% 52.60% 34.82% 37.91% 
Table 2: Comparison of performances on the 139 abstracts 
,
,argmax ( ) ( | )
j a i
NB j k i j
v V i S k W
v P v P w v
? ? ?
= ?? ?
 
where vj is one of the nine positions aforementioned, 
S is the set of 9 sentence positions, Wa,i is the set of 
all word positions in sentence i in abstract a, wk,i is 
the occurrence of the normalized word at position k 
in sentence i and V is the set of 9 classes. 
We, therefore, represented each abstract by a 
feature vector composed of the scores of 9 
sentences.  Furthermore, with a list of our 10,506 
GRIF words at hand, we also computed the 
occurrences of these words in each sentence, given 
an abstract.  Each abstract is then represented by the 
number of occurrences of these words in the 9 
sentences respectively, i.e., the feature vector is 
94,554 in length.  Classification based on this type 
of features is denoted the sentence-wise bag of 
words model in the rest of this paper.  Combining 
these two models, we got totally 94,563 features. 
Since we are extracting sentences discussing gene 
functions, it?s reasonable to expect gene or protein 
names in the GeneRIF sentence.  Therefore, we 
employed Yapex (Olsson et al, 2002) and 
GAPSCORE (Chang et al, 2004) protein/gene 
name detectors to count the number of protein/gene 
names in each of the 9 sentences, resulting in 
94,581 features.  
3.3.2 Training SVMs 
The whole process related to SVM was done via 
LIBSVM ? A Library for Support Vector Machines 
(Hsu et al, 2003).  Radial basis kernel was adopted 
based on our previous experience.  However, 
further verification showed that the combined 
model with either linear or polynomial kernel only 
slightly surpassed the baseline, attaining 50.67% for 
CD.  In order to get the best-performing classifier, 
we tuned two parameters, C and gamma.  They are 
the penalty coefficient in optimization and a 
parameter for the radial basis kernel, respectively.  
Four-fold cross validation accuracy was used to 
select the best parameter pair. 
3.3.3 Picking up the answe r sentence  
Test instances were first fed to the classifier to get 
the predicted positions of GeneRIF sentences.  In 
case that the predicted position doesn?t have a 
sentence, which would happen when the abstract 
doesn?t have enough sentences, the sentence with 
the highest score is picked for the weighting 
scheme and the combined model, otherwise the title 
is picked for the sentence-wise bag of words model. 
4 Results and Discussions  
The performance measures are based on Dice 
coefficient, which calculates the overlap between 
the candidate GeneRIF and actual GeneRIF.  
Classic Dice (CD) is the classic Dice formula using 
a common stop word list and the Porter stemming 
algorithm.  Due to lack of space, we referred you to 
the Genomics track overview for the other three 
modifications of CD (Hersh and Bhupatiraju, 2003). 
The evaluation results are shown in Table 2.  The 
1st row shows the official run of Jelier?s team, the 
first place in the official runs.  The 2nd row shows 
the performance when the Na?ve Bayes classifier 
adopted by Jelier is replaced with SVMs.  The 3rd 
row is the performance of our weighting scheme 
without a classifier.  The 4th row then lists the 
performance when our weighting scheme is 
combined with SVMs.  The 5th row is the result 
when our weighting scheme and the sentence-wise 
bag of words model are combined together.  The 6th 
row is the result when two gene/protein name 
detectors are incorporated into the combined model.  
The next two rows were obtained after two feature 
selection methods were applied.  The 9th row shows 
the performance when the classifier always 
proposes a sentence most similar to the actual 
GeneRIF.  The last row lists the baseline, i.e., title 
is always picked. 
A comparative study on text categorization 
(Joachims, 1998) showed that SVMs outperform 
other classification methods, such as Na?ve  Bayes, 
C4.5, and k-NN.  The reasons would be that SVMs 
are capable of handling large feature space, text 
categorization has few irrelevant features, and 
document vectors are sparse.  The comparison 
56
between SVMs and the Na?ve  Bayes classifier again 
demonstrated the superiority of SVMs in text 
categorization (rows 1, 2). 
The performance greatly improved after 
introducing position information (rows 3, 4), 
showing the sentence position plays an important 
role in locating the GeneRIF sentence.  The 2% 
difference between rows 2 and 4 indicates that the 
features under sentence-wise bag of words model 
are more informative than those under our 
weighting scheme.  However, with only 9 features, 
our weighting scheme with SVMs performed fairly 
well.  Comparing the performance before and after 
combining our weighting scheme and the sentence-
wise bag of words model (rows 2, 5 and rows 4, 5), 
we can infer from the performance differences that 
both models provide mutually exclusive 
information in the combined model.  The result 
shown in row 6 indicates that the information of 
gene/protein name occurrences did not help identify 
the GeneRIF sentences in these 139 test abstracts. 
We performed feature selection on the combined 
model to reduce the dimension of feature space.  
There were two methods applied: a supervised 
heuristic method (denoted as BWRatio feature 
selection in Table 2) (S. Dutoit et al, 2002) and 
another unsupervised method (denoted as Graphical 
feature selection in Table 2) (Chang et al, 2002).  
The number of features was then reduced to about 
4,000 for both methods.  Unfortunately, the 
performance did not improve after either method 
was applied.  This may be attributed to over-fitting 
training data, because the cross-validation 
accuracies are indeed higher than those without 
feature selection.  The result may also imply there 
are little irrelevant features in this case. 
5 Conclusion and Future work 
This paper proposed an automatic approach to 
locate the GeneRIF sentence in an abstract with the 
assistance of SVMs, reducing the human effort in 
updating and maintaining the GeneRIF field in the 
LocusLink database. 
We have to admit that the 139 abstracts provided 
in TREC 2003 are too few to verify the 
performance among models, and the results based 
on these 139 abstracts may be slightly biased.  Our 
next step would aim at measuring the cross-
validation performances using Dice coefficient. 
The syntactic  information is worth exploring, 
since the sentences describing gene functions may 
share some common structural patterns.  Moreover, 
how the weighting scheme affects the performance 
is also very interesting.  We are currently trying to 
obtain a weighting scheme that can best distinguish 
GeneRIF sentence from non-GeneRIF sentence 
without classifiers. 
References  
G. Bhalotia, P.I. Nakov, A.S. Schwartz, and M.A. 
Hearst. 2003. BioText Team Report for the TREC 
2003 Genomics Track. TREC 2003 work notes: 
158-166. 
Y.C. I. Chang, H. Hsu and L.Y. Chou. 2002. 
Graphical Features Selection Method. Intelligent 
Data Engineering and Automated Learning, 
Edited by H. Yin, N. Allinson, R. Freeman, J. 
Keane, and S. Hubband. 
J.T. Chang, H. Schutze, R.B. Altman. 2004. 
GAPSCORE: finding gene and protein names one 
word at a time. Bioinformatics, 20(2):216-225. 
S. Dutoit, Y.H. Yang, M.J. Callow and T.P. Speed. 
2002. Statistical methods for identifying 
differentially expressed genes in replicated cDNA 
microarray experiments. J. Amer. Statis. Assoc. 
97:77-86. 
W. Hersh and Ravi Teja Bhupatiraju. 2003. TREC 
Genomics Track Overview. TREC 2003 work 
notes. 
W.J. Hou, C.Y. Teng, C. Lee and H.H. Chen. 2003. 
SVM Approach to GeneRIF Annotation. 
Proceedings of TREC 2003. 
C.W. Hsu, C.C Chang and C.J. Lin. 2003. A 
Practical Guide to Support Vector Classification.  
http://www.csie.ntu.edu.tw/~cjlin/libsvm/index.ht
ml. 
R. Jelier, M. Schuemie, C.V.E. Eijk, M. Weeber, 
E.V. Mulligen, B. Schijvenaars, B. Mons and J. 
Kors. 2003. Searching for geneRIFs: concept-
based query expansion and Bayes classification. 
TREC 2003 work notes: 167-174. 
T. Joachims. 1998. Text Categorization with 
Support Vector Machines: Learning with Many 
Relevant Features. Proceedings of ECML-98, 
137-142. 
F. Olsson, G. Eriksson, K. Franz?n, L. Asker and P. 
Lid?n. 2002. Notions of Correctness when 
Evaluating Protein Name Taggers. Proceedings of 
the 19th International Conference on 
Computational Linguistics 2002, 765-771. 
K.D. Pruitt, K.S. Katz, H. Sicotte and D.R. Maglott. 
2000. Introducing RefSeq and LocusLink: 
Curated Human Genome Resources at the NCBI. 
Trends Genet, 16(1):44-47. 
T. Sekimizu, H.S. Park and J. Tsujji. 1998. 
Identifying the Interaction Between Genes and 
Gene Products Based on Frequently Seen Verbs 
in Medline Abstracts. Genome Information, 9:62-
71 
57
Annotating Multiple Types of Biomedical Entities:  
A Single Word Classification Approach 
Chih Lee, Wen-Juan Hou and Hsin-Hsi Chen 
Natural Language Processing Laboratory 
Department of Computer Science and Information Engineering 
National Taiwan University  
1 Roosevelt Road, Section 4, Taipei, Taiwan, 106 
{clee, wjhou}@nlg.csie.ntu.edu.tw, hh_chen@csie.ntu.edu.tw 
 
Abstract 
Named entity recognition is a fundamental 
task in biomedical data mining.  Multiple -class 
annotation is more challenging than single -
class annotation.  In this paper, we took a 
single word classification approach to dealing 
with the multiple -class annotation problem 
using Support Vector Machines (SVMs).  
Word attributes, results of existing 
gene/protein name taggers, context, and other 
information are important features for 
classification.  During training, the size of 
training data and the distribution of named 
entities are considered.  The preliminary 
results showed that the approach might be 
feasible when more training data is used to 
alleviate the data imbalance problem. 
1 Introduction 
The volumn of on-line material in the biomedical 
field has been growing steadily for more than 20 
years.  Several attempts have been made to mine 
knowledge from biomedical documents, such as 
identifying gene/protein names, recognizing 
protein interactions, and capturing specific 
relations in databases.  Among these, named entity 
recognition is a fundamental step to mine 
knowledge from biological articles. 
Previous approaches on biological named entity 
extraction can be classified into two types ? rule-
based (Fukuda et al, 1998; Olsson et al, 2002; 
Tanabe and Wilbur, 2002) and corpus-based 
(Collier et al, 2000; Chang et al, 2004).  Yapex 
(Olsson et al, 2002) implemented some heuristic 
steps described by Fukuda, et al, and applied 
filters and knowledge bases to remove false alarms.  
Syntactic information obtained from the parser was 
incorporated as well.  GAPSCORE (Chang et al, 
2004) scored words on the basis of statistical 
models that quantified their appearance, 
morphology and context.  The models includes 
Naive Bayes (Manning and Schutze, 1999), 
Maximum Entropy (Ratnaparkhi, 1998) and 
Support Vector Machines (Burges, 1998).  
GAPSCORE also used Brill?s tagger (Brill, 1994) 
to get the POS tag to filter out some words that are 
clearly not gene or protein names.  Efforts have 
been made (Hou and Chen, 2002, 2003; Tsuruoka 
and Tsujii, 2003) to improve the performance.  The 
nature of classification makes it possible to 
integrate existing approaches by extracting good 
features from them.  Several works employing 
SVM classifier have been done (Kazama et al, 
2002; Lee et al, 2003; Takeuchi and Collier, 2003; 
Yamamoto et al, 2003), and will be discussed 
further in the rest of this paper. 
Collocation denotes two or more words having 
strong relationships (Manning and Schutze, 1999).  
Hou and Chen (2003) showed that protein/gene 
collocates are capable of assisting existing 
protein/gene taggers.  In this paper, we addressed 
this task as a multi-class classification problem 
with SVMs and extended the idea of collocation to 
generate features at word and pattern level in our 
method.  Existing protein/gene recognizers were 
used to perform feature extraction as well. 
The rest of this paper is organized as follows.  
The methods used in this study are introduced in 
Section 2.  The experimental results are shown and 
discussed in Section 3.  Finally, Section 4 
concludes the remarks and lists some future works. 
2 Methods  
Most of the works in the past on recognizing 
named entities in the biomedical domain focused 
on identifying a single type of entities like protein 
and/or gene names.  It is obviously more 
challenging to annotate multiple types of named 
entities simultaneously.  Intuitively, one can 
develop a specific recognizer for each type of 
named entities, run the recognizers one by one to 
annotate all types of named entities, and merge the 
results.  The problem results from the boundary 
decision and the annotation conflicts.  Instead of 
constructing five individual recognizers, we 
regarded the multiple -class annotation as a 
classification problem, and tried to learn a 
80
classifier capable of identifying all the five types of 
named entities. 
Before classification, we have to decide the unit 
of classification.  Since it is difficult to correctly 
mark the boundary of a name to be identified, the 
simplest way is to consider an individual word as 
an instance and assign a type to it.  After the type 
assignment, continuous words of the same type 
will be marked as a complete named entity of that 
type.  The feature extraction process will be 
described in the following subsections. 
2.1 Feature Extraction 
The first step in classification is to extract 
informative and useful features to represent an 
instance to be classified.  In our work, one word is 
represented by the attributes carried per se, the 
attributes contributed by two surrounding words, 
and other contextual information.  The details are 
as follows. 
2.1.1 Word Attributes 
The word ?attribute? is sometimes used 
interchangeably with ?feature?, but in this article 
they denote two different concepts.  Features are 
those used to represent a classification instance, 
and the information enclosed in the features is not 
necessarily contributed by the word itself.  
Attributes are defined to be the information that 
can be derived from the word alone in this paper. 
The attributes assigned to each word are whether 
it is part of a gene/protein name, whether it is part 
of a species name, whether it is part of a tissue 
name, whether it is a stop word, whether it is a 
number, whether it is punctuation, and the part of 
speech of this word.  Instead of using a lexicon for 
gene/protein name annotation, we employed two 
gene/protein name taggers, Yapex and 
GAPSCORE, to do this job.  As for part of speech 
tagging, Brill?s part of speech tagger was adopted. 
2.1.2 Context Information Preparation 
Contextual information has been shown helpful in 
annotating gene/protein names, and therefore two 
strategies for extracting contextual information at 
different levels are used.  One is the usual practice 
at a word level, and the other is at a pattern level.  
Since the training data released in the beginning 
does not define the abstract boundary, we have to 
assume that sentences are independent of each 
other, and the contextual information extraction 
was thus limited to be within a sentence. 
For contextual information extraction at word 
level (Hou and Chen, 2003), collocates along with 
4 statistics including frequency, the average and 
standard error of distance between word and entity 
and t-test score, were extracted.  The frequency 
and t-test score were normalized to [0, 1].  Five 
lists of collocates were obtained for cell-line, cell-
type, DNA, RNA, and protein, respectively. 
As for contextual information extraction at 
pattern level, we first gathered a list of words 
constituting a specific type of named entities.  
Then a hierarchical clustering with cutoff threshold 
was performed on the words. Edit distance was 
adopted as the measure of dissimilarity (see Figure 
1). Afterwards, common substrings were obtained 
to form the list of patterns.  With a list of patterns 
at hand, we estimated the pattern distribution, the 
occurrence frequencies at and around the current 
position, given the type of word at the current 
position.  Figure 2 showed an example of the 
estimated distribution.  The average KL-
Divergence between any two distributions was 
computed to discriminate the power of each pattern.  
The formula is as follows: 
1 1,
1 ( || )
( 1)
n n
i j
i j j i
D p p
n n = = ?- ? ? , where pi and pj 
are the distributions of a pattern given the word at 
position 0 being type i and j, respectively. 
 
Figure 1: Example of common substring extraction 
 
Figure 2: Pattern distributions given the type of 
word at position 0 
2.2 Constructing Training Data 
For each word in a sentence, the attributes of the 
word and the two adjacent words are put into the 
feature vector.  Then, the left five and the right five 
words are searched for previously extracted 
collocates.  The 15 variables thus added are shown 
below. 
5
5, 0
( | )i
i i
Freq w type
=- ?
?
 
5
5, 0
_ ( | )i
i i
t test score w type
=- ?
-?  
81
5, ,
5, 0
??( | , )
i iw type w type
i i
f i m s
=- ?
? , where f is the pdf of 
normal distribution, type is one of the five types, wi 
denotes the surrounding words,
,
?
itypew
m and 
,
?
itypew
s are 
the maximum likelihood estimates of mean and 
standard deviation for wi given the type. Next, the 
left three and right three words along with the 
current word are searched for patterns, adding 6 
variables to the feature vector. 
3
3
Prob ( | )
wi
p
i p P
i type
=- ?
? ? , where type is one of the 
six types including ?O?, 
iwP is the set of patterns 
matching wi, Prob p  denotes the pmf for pattern p.  
Finally, the type of the previous word is added to 
the feature vector, mimicking the concept of a 
stochastic model. 
2.3 Classification 
Support Vector Machines classification with radial 
basis kernel was adopted in this task, and the 
package LIBSVM ? A Library for Support Vector 
Machines (Hsu et al, 2003) was used for training 
and prediction. The penalty coefficient C in 
optimization and gamma in kernel function were 
tuned using a script provided in this package. 
The constructed training data contains 492,551 
instances, which is too large for training.  Also, the 
training data is extremely unbalanced (see Table 1) 
and this is a known problem in SVMs 
classification.  Therefore, we performed stratified 
sampling to form a smaller and balanced data set 
for training. 
Type # of instances (words) 
cell-type 15,466 
DNA 25,307 
cell-line 11,217 
RNA 2,481 
protein 55,117 
O 382,963 
Table 1: Number of instances for each type 
3 Results and Discussion 
Since there is a huge amount of training instances 
and we do not have enough time to tune the 
parameters and train a model with all the training 
instances available, we first randomly selected one 
tenth and one fourth of the complete training data.  
The results, as we expected, showed that model 
trained with more instances performed better (see 
Table 2).  However, we noticed that the 
performances vary among the 6 types and one of 
the possible causes is the imbalance of training 
data among classes (see Table 1). Therefore we 
decided to balance the training data. 
First, the training data was constructed to 
comprise equal number of instances from each 
class.  However, it didn?t perform well and lots of 
type ?O? words were misclassified, indicating that 
using only less than 1% of type ?O? training 
instances is not sufficient to train a good model.  
Thus two more models were trained to see if the 
performance can be enhanced.  One model has 
slightly more type ?O? instances than the equally 
balanced one, and the other model has the ratio 
among classes being 4:8:4:1:8:16.  The results 
showed increase in recall but drop in precision. 
Kazama et al (2002) addressed the data 
imbalance problem and sped up the training 
process by splitting the type ?O? instances into sub-
classes using part-of-speech information.  However, 
we missed their work while we were doing this 
task, and hence didn?t have the chance to use and 
extend this idea. 
After carefully examining the classification 
results, we found that many of the ?DNA? 
instances were classified as ?protein? and many of 
the ?protein? instances were classified as ?DNA?.  
For example, 904 out of 2,845 ?DNA? instances 
were categorized as ?protein? under ?model 1/4?.  
The reason may be that Yapex and GAPSCORE do 
not distinguish gene name from protein names.  
Even humans don?t do very well at this 
(Krauthammer et al, 2002). 
We originally planned to verify the contribution 
of each type of features. For example, how much 
noise was introduced by using existing taggers 
instead of lexicons. This would have helped gain 
more insights into the proposed features. 
4 Conclusion and Future work 
This paper presented the preliminary results of our 
study.  We introduced the use of existing taggers 
and presented a way to collect common substrings 
shared by entities.  Due to lack of time, the models 
were not well tuned against the two parameters ? C 
and gamma, influencing the capabilities of the 
models.  Further, not all of the training instances 
provided were used to train the model, and it will 
be interesting and worthwhile to investigate.  How 
to deal with data imbalance is another important 
issue.  By solving this problem, further evaluation 
of feature effectiveness would be facilitated.  We 
believe there is much left for our approach to 
improve and it may perform better if more time is 
given. 
82
References  
E. Brill. 1994. Some Advances in Transformation-
Based Part of Speech Tagging. Proceedings of 
the National Conference on Artificial 
Intelligence. AAAI Press; 722-727. 
C. Burges. 1998. A Tutorial on Support Vector 
Machines for Pattern Recognition. Data Mining 
and Knowledge Discovery, 2: 121-167. 
J.T. Chang, H. Schutze and R.B. Altman. 2004. 
GAPSCORE: Finding Gene and Protein Names 
One Word at a Time. Bioinformatics, 20(2): 216-
225. 
N. Collier, C. Nobata and J.I. Tsujii. 2000. 
Extracting the Names of Genes and Gene 
Products with a Hidden Markov Model. 
Proceedings of 18 th International Conference on 
Computational Linguistics, 201-207. 
K. Fukuda, T. Tsunoda, A. Tamura and T. Takagi. 
1998. Toward Information Extraction: 
Identifying Protein Names from Biological 
Papers. Proceedings of Pacific Symposium on 
Biocomputing, 707-718. 
W.J. Hou and H.H. Chen 2002. Extracting 
Biological Keywords from Scientific Text. 
Proceedings of 13 th International Conference on 
Genome Informatics; 571-573. 
W.J. Hou and H.H. Chen. 2003. Enhancing 
Performance of Protein Name Recognizers 
Using Collocation. Proceedings of the ACL 2003 
Workshop on NLP in Biomedicine, 25-32. 
C.W. Hsu, C.C Chang and C.J. Lin. 2003. A 
Practical Guide to Support Vector Classification. 
http://www.csie.ntu.edu.tw/~cjlin/libsvm/index.h
tml. 
J. Kazama, T. Makino, Y. Ohta and J. Tsujii. 2002. 
Tuning Support Vector Machines for Biomedical 
Named Entity Recognition. Proceedings of the 
ACL 2002 workshop on NLP in the Biomedical 
Domain , 1-8. 
M. Krauthammer, P. Kra, I. Iossifov, S.M. Gomez, 
G. Hripcsak, V. Hatzivassiloglou, C. Friedman 
and A. Rzhetsky. 2002. Of truth and pathways: 
chasing bits of information through myriads of 
articles. Bioinformatics, 18(sup.1):S249-S257. 
K.J. Lee, Y.S. Hwang and H.C. Rim. 2003. Two-
Phase Biomedical NE Recognition based on 
SVMs. Proceedings of the ACL 2003 Workshop 
on NLP in Biomedicine, 33-40. 
C.D. Manning and H. Schutze. 1999. Foundations 
of Statistical Natural Language Processing. MIT 
Press. 
F. Olsson, G. Eriksson, K. Franzen, L. Asker and P. 
Liden. 2002. Notions of Correctness when 
Evaluating Protein Name Taggers. Proceedings 
of the 19th International Conference on 
Computational Linguistics, 765-771. 
A. Ratnaparkrhi. 1998. Maximum Entropy Models 
for Natural Language Ambiguity Resolution. 
PhD Thesis, University of Pennsylvania. 
K. Takeuchi and N. Collier. 2003. Bio-Medical 
Entity Extraction using Support Vector 
Machines. Proceedings of the ACL 2003 
workshop on NLP in Biomedicine, 57-64. 
L. Tanabe and W.J. Wilbur. 2002. Tagging Gene 
and Protein Names in Biomedical Text. 
Bioimformatics, 18(8) : 1124-1132. 
Y. Tsuruoka and J. Tsujii. 2003. Boosting 
Precision and Recall of Dictionary-based Protein 
Name Recognition. Proceedings of the ACL 
2003 Workshop on NLP in Biomedicine, 41-48. 
K. Yamamoto, T. Kudo, A. Konagaya and Y. 
Matsumoto. 2003. Protein Name Tagging for 
Biomedical Annotation in Text. Proceedings of 
the ACL 2003 workshop on NLP in Biomedicine, 
65-72.
 Model 1/10 Model 1/4    
 Recall Prec. F-score Recall Prec. F-score Recall Prec. F-score 
Full (Object) 0.4756 0.4399 0.4571 0.5080 0.4759 0.4914    
Full (protein) 0.5846 0.4392 0.5016 0.6213 0.4614 0.5296    
Full (cell-line) 0.2420 0.2909 0.2642 0.2820 0.3341 0.3059    
Full (DNA) 0.2784 0.3249 0.2998 0.2888 0.4479 0.3512    
Full (cell-type) 0.3863 0.5752 0.4622 0.4196 0.6115 0.4977    
Full (RNA) 0.0085 0.1000 0.0156 0.0000 0.0000 0.0000    
 Model balanced equally Model slightly more ?O? Model 4:8:4:1:8:16 
Full (Object) 0.1480 0.0990 0.1186 0.1512 0.1002 0.1206 0.5036 0.3936 0.4419 
Full (protein) 0.1451 0.1533 0.1491 0.1458 0.1527 0.1492 0.5629 0.4280 0.4863 
Full (cell-line) 0.1580 0.0651 0.0922 0.2280 0.0319 0.0560 0.4060 0.2261 0.2904 
Full (DNA) 0.1326 0.0466 0.0690 0.1591 0.0582 0.0852 0.3759 0.2457 0.2972 
Full (cell-type) 0.1650 0.1375 0.1500 0.1494 0.1908 0.1676 0.4701 0.4900 0.4798 
Full (RNA) 0.0932 0.0067 0.0126 0.0169 0.0075 0.0104 0.0593 0.1148 0.0782 
Table 2: Performance of each model (only FULL is shown) 
83
