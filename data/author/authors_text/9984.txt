Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1259?1269,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet
Marco Guerini
Trento RISE
Via Sommarive 18
38123 Povo in Trento, Italy
m.guerini@trentorise.eu
Lorenzo Gatti
Trento RISE
Via Sommarive 18
38123 Povo in Trento, Italy
l.gatti@trentorise.eu
Marco Turchi
Fondazione Bruno Kessler
Via Sommarive 18
38123 Povo in Trento, Italy
turchi@fbk.eu
Abstract
Assigning a positive or negative score to a
word out of context (i.e. a word?s prior polar-
ity) is a challenging task for sentiment analy-
sis. In the literature, various approaches based
on SentiWordNet have been proposed. In this
paper, we compare the most often used tech-
niques together with newly proposed ones and
incorporate all of them in a learning frame-
work to see whether blending them can fur-
ther improve the estimation of prior polarity
scores. Using two different versions of Sen-
tiWordNet and testing regression and classifi-
cation models across tasks and datasets, our
learning approach consistently outperforms
the single metrics, providing a new state-of-
the-art approach in computing words? prior
polarity for sentiment analysis. We conclude
our investigation showing interesting biases
in calculated prior polarity scores when word
Part of Speech and annotator gender are con-
sidered.
1 Introduction
Many approaches to sentiment analysis make use of
lexical resources ? i.e. lists of positive and neg-
ative words ? often deployed as baselines or as
features for other methods (usually machine learn-
ing based) for sentiment analysis research (Liu and
Zhang, 2012). In these lexica, words are associated
with their prior polarity, i.e. if that word out of con-
text evokes something positive or something nega-
tive. For example, wonderful has a positive connota-
tion ? prior polarity ? while horrible has a negative
one. These approaches have the advantage of not
needing deep semantic analysis or word sense dis-
ambiguation to assign an affective score to a word
and are domain independent (they are thus less pre-
cise but more portable).
SentiWordNet (henceforth SWN) is one of these
resources and has been widely adopted since it pro-
vides a broad-coverage lexicon ? built in a semi-
automatic manner ? for English (Esuli and Sebas-
tiani, 2006). Given that SWN provides polarities
scores for each word sense (also called ?posterior
polarities?), it is necessary to derive prior polarities
from the posteriors. For example, the word cold has
a posterior polarity for the meaning ?having a low
temperature? ? like in ?cold beer? ? that is different
from the one in ?cold person? which refers to ?being
emotionless?. This information must be considered
when reconstructing the prior polarity of cold.
Several formulae to compute prior polarities start-
ing from posterior polarities scores have been used
in the literature. However, their performance varies
significantly depending on the adopted variant. We
show that researchers have not paid sufficient atten-
tion to this posterior-to-prior polarity issue. Indeed,
we show that some variants outperform others on
different datasets and can represent a fairer state-of-
the-art approach using SWN. On top of this, we at-
tempt to outperform the state-of-the-art formula us-
ing a learning framework that combines the various
formulae together.
In detail, we will address five main research
questions: (i) is there any relevant difference in
the posterior-to-prior polarity formulae performance
(both in regression and classification tasks), (ii) is
there any relevant variation in prior polarity values
1259
if we use different releases of SWN (i.e. SWN1 or
SWN3), (iii) can a learning framework boost per-
formance of such formulae, (iv) considering word
Part of Speech (PoS), is there any relevant difference
in formulae performance, (v) considering the gender
dimension of the annotators (male/female) and the
sentiment dimension (positive/negative), is there any
relevant difference in SWN performance.
In Section 2 we briefly describe our approach and
how it differentiates from similar sentiment analysis
tasks. Then, in Sections 3 and 4, we present Sen-
tiWordNet and overview various posterior-to-prior
polarity formulae based on this resource that ap-
peared in the literature (included some new ones
we identified as potentially relevant). In Section 5
we describe the learning approach adopted on prior-
polarity formulae. In Section 6 we introduce the
ANEW and General Inquirer resources that will be
used as gold standards. Finally, in the two last sec-
tions, we present a series of experiments, both in
regression and classification tasks, that give an an-
swer to the aforementioned research questions. The
results support the hypothesis that using a learning
framework we can improve on state-of-the-art per-
formance and that there are some interesting phe-
nomena connected to PoS and annotator gender.
2 Proposed Approach
In the broad field of Sentiment Analysis we will fo-
cus on the specific problem of posterior-to-prior po-
larity assessment, using both regression and classifi-
cation experiments. A general overview on the field
and possible approaches can be found in (Pang and
Lee, 2008) or (Liu and Zhang, 2012).
For the regression task, we tackled the problem
of assigning affective scores (along a continuum be-
tween -1 and 1) to words using the posterior-to-prior
polarity formulae. For the classification task (assess-
ing whether a word is either positive or negative) we
used the same formulae, but considering just the sign
of the result. In these experiments we will also use a
learning framework which combines the various for-
mulae together. The underlying hypothesis is that by
blending these formulae, and looking at the same in-
formation from different perspectives (i.e. the pos-
terior polarities provided by SWN combined in var-
ious ways), we can give a better prediction.
The regression task is harder than binary classifi-
cation, since we want to assess not only that pretty,
beautiful and gorgeous are positive words, but also
to define a partial or total order so that gorgeous
is more positive than beautiful which, in turn, is
more positive than pretty. This is fundamental for
tasks such as affective modification of existing texts,
where words? polarity together with their score are
necessary for creating multiple graded variations of
the original text (Guerini et al, 2008). Some of
the work that addresses the problem of sentiment
strength are presented in (Wilson et al, 2004; Pal-
toglou et al, 2010), however, their approach is mod-
eled as a multi-class classification problem (neutral,
low, medium or high sentiment) at the sentence level,
rather than a regression problem at the word level.
Other works such as (Neviarouskaya et al, 2011)
use a fine grained classification approach too, but
they consider emotion categories (anger, joy, fear,
etc.), rather than sentiment strength categories. On
the other hand, even if approaches that go beyond
pure prior polarities ? e.g. using word bigram fea-
tures (Wang and Manning, 2012) ? are better for
sentiment analysis tasks, there are tasks that are in-
trinsically based on the notion of words? prior polar-
ity. Consider copywriting, where evocative names
are a key element to a successful product (O?zbal and
Strapparava, 2012; O?zbal et al, 2012). In such cases
no context is given and the brand name alone, with
its perceived prior polarity, is responsible for stating
the area of competition and evoking semantic asso-
ciations. For example Mitsubishi changed the name
of one of its SUV for the Spanish market, since the
original name Pajero had a very negative prior po-
larity, as it meant ?wanker? in Spanish (Piller, 2003).
To our knowledge, the only work trying to address
the SWN posterior-to-prior polarity issue, compar-
ing some of the approaches appeared in the literature
is (Gatti and Guerini, 2012). However, in our previ-
ous study we only considered a regression frame-
work, we did not use machine learning and we only
tested SWN1. So, we took this work as a starting
point for our analysis and expanded on it.
3 SentiWordNet
SentiWordNet (Esuli and Sebastiani, 2006) is a
lexical resource in which each entry is a set of
1260
lemma-PoS pairs sharing the same meaning, called
?synset?. Each synset s is associated with the nu-
merical scores Pos(s) and Neg(s), which range
from 0 to 1. These scores ? automatically as-
signed starting from a bunch of seed terms ? rep-
resent the positive and negative valence (or pos-
terior polarity) of the synset and are inherited by
each lemma-PoS in the synset. According to the
structure of SentiWordNet, each pair can have more
than one sense and each of them takes the form of
lemma#PoS#sense-number, where the small-
est sense-number corresponds to the most frequent
sense.
Obviously, different senses can have different po-
larities. In Table 1, the first 5 senses of cold#a
present all possible combinations, included mixed
scores (cold#a#4), where positive and negative
valences are assigned to the same sense. Intuitively,
mixed scores for the same sense are acceptable, as
in ?cold beer? (positive) vs. ?cold pizza? (negative).
PoS Offset Pos(s) Neg(s) SynsetTerms
a 1207406 0.0 0.75 cold#a#1
a 1212558 0.0 0.75 cold#a#2
a 1024433 0.0 0.0 cold#a#3
a 2443231 0.125 0.375 cold#a#4
a 1695706 0.625 0.0 cold#a#5
Table 1: First five SentiWordNet entries for cold#a
In our experiments we use two different versions
of SWN: SentiWordNet 1.0 (SWN1), the first re-
lease of SWN, and its updated version SentiWord-
Net 3.0 (Baccianella et al, 2010) ? SWN3. In
SWN3 the annotation algorithm used in SWN1
was revised, leading to an increase in the accuracy
of posterior polarities over the previous version.
4 Prior Polarities Formulae
In this section we review the main strategies for
computing prior polarities used in previous stud-
ies. All the proposed approaches try to estimate
the prior polarity score from the posterior polari-
ties of all the senses for a single lemma-PoS. Given
a lemma-PoS with n senses (lemma#PoS#n), ev-
ery formula f is independently applied to all the
Pos(s) and Neg(s) . This produces two scores,
f(posScore) and f(negScore), for each lemma-
PoS. To obtain a unique prior polarity for each
lemma-PoS, f(posScore) and f(negScore) can be
mapped according to different strategies:
fm =
?
??
??
f(posScore) if f(posScore) ?
f(negScore)
?f(negScore) otherwise
fd = f(posScore)? f(negScore)
where fm computes the absolute maximum of
the two scores, while fd computes the difference
between them. It is worth noting that f(negScore)
is always positive by construction. To obtain
a final prior polarity that ranges from -1 to 1,
the negative sign is imposed. So, consider-
ing the first 5 senses of cold#a in Table 1,
f(posScore) will be derived from the Pos(s) val-
ues <0.0, 0.0, 0.0, 0.125, 0.625>, while f(negScore)
from <0.750, 0.750, 0.0, 0.375, 0.0>. Then, the fi-
nal polarity strength returned will be either fm or fd.
The formulae (f ) we tested are the following:
fs. In this formula only the first (and thus
most frequent) sense is considered for the given
lemma#PoS. This is equivalent to considering only
the SWN score for lemma#PoS#1. Based on
(Neviarouskaya et al, 2009; Agrawal and Siddiqui,
2009; Guerini et al, 2008; Chowdhury et al, 2013),
this is the most basic form of prior polarities.
mean. It calculates the mean of the positive
and negative scores for all the senses of the given
lemma#PoS. This formula has been used in (Thet
et al, 2009; Denecke, 2009; Devitt and Ahmad,
2007; Sing et al, 2012).
uni. Based on (Neviarouskaya et al, 2009), it
considers only those senses that have a Pos(s)
greater than or equal to the corresponding Neg(s),
and greater than 0 (the stronglyPos set). In case
posScore is equal to negScore, the one with the
highest weight is returned, where weights are de-
fined as the cardinality of stronglyPos divided by
the total number of senses. The same applies for the
negative senses. This is the only method, together
with rnd, for which we cannot apply fd, as it returns
a positive or negative score according to the weight.
uniw. Like uni but without the weighting system.
w1. This formula weighs each sense with a geo-
metric series of ratio 1/2. The rationale behind this
choice is based on the assumption that more frequent
1261
senses should bear more ?affective weight? than rare
senses when computing the prior polarity of a word.
The system presented in (Chaumartin, 2007) uses a
similar approach of weighted mean.
w2. Similar to the previous one, this formula
weighs each lemma with a harmonic series, see for
example (Denecke, 2008).
On top of these formulae, we implemented some
new formulae that were relevant to our task and
have not been implemented before. These for-
mulae mimic the ones discussed previously, but
they are built under a different assumption: that
the saliency (Giora, 1997) of a word?s prior polar-
ity might be more related to its posterior polari-
ties score, rather than to sense frequencies. Thus
we ordered posScore and negScore by strength,
giving more relevance to ?valenced? senses. For
instance, in Table 1, posScore and negScore
for cold#a become<0.625, 0.125, 0.0, 0.0, 0.0> and
<0.750, 0.750, 0.375, 0.0, 0.0> respectively.
w1s and w1n. Like w1 and w2, but senses are
ordered by strength (sorting Pos(s) and Neg(s) in-
dependently).
w1n and w2n. Like w1 and w2 respectively, but
without considering senses that have a 0 score for
both Pos(s) and Neg(s). Our motivation is that
?empty? senses are mostly noise.
w1sn and w2sn. Like w1s and w2s, but with-
out considering senses that have a 0 score for both
Pos(s) and Neg(s).
median: return the median of the senses ordered
by polarity score.
All these prior polarities formulae are compared
against two gold standards (one for regression, one
for classification) both one by one, as in the works
mentioned above, and combined together in a learn-
ing framework (to see whether combining these fea-
tures ? that capture different aspect of prior polari-
ties ? can further improve the results).
Finally, we implemented two variants of a prior
polarity random baseline to asses possible advan-
tages of approaches using SWN:
rnd. This formula represents the basic baseline
random approach. It simply returns a random num-
ber between -1 and 1 for any given lemma#PoS.
swnrnd. This formula represents an advanced
random approach that incorporates some ?knowl-
edge? from SWN. It takes the scores of a random
sense for the given lemma#PoS. We believe this
is a fairer baseline than rnd since SWN informa-
tion can possibly constrain the values. A similar ap-
proach has been used in (Qu et al, 2008).
5 Learning Algorithms
We used two non-parametric learning approaches,
Support Vector Machines (SVMs) (Shawe-Taylor
and Cristianini, 2004) and Gaussian Processes (GPs)
(Rasmussen and Williams, 2006), to test the perfor-
mance of all the metrics in conjunction. SVMs are
non-parametric deterministic algorithms that have
been widely used in several fields, in particular in
NLP where they are the state-of-the-art for various
tasks. GPs, on the other hand, are an extremely flex-
ible non-parametric probabilistic framework able to
explicitly model uncertainty, that, despite being con-
sidered state-of-the-art in regression, have rarely
been used in NLP. To our knowledge only two pre-
vious works did so (Polajnar et al, 2011; Cohn and
Specia, 2013).
Both methods take advantage of the kernel trick,
a technique used to embed the original feature space
into an alternative space where data may be linearly
separable. This is performed by the kernel func-
tion that transforms the input data in a new structure,
called kernel. How it is used to produce the predic-
tion is one of the main differences between SVMs
and GPs. In classification SVMs use the geomet-
ric mean to discriminate between the positive and
negative classes, while the GP model uses the pos-
terior probability distribution over each class. Both
frameworks support learning algorithms for regres-
sion and classification. An exhaustive explanation
of the two methodologies can be found in (Shawe-
Taylor and Cristianini, 2004) and (Rasmussen and
Williams, 2006).
In the SVM experiments, we use C-SVM and -
SVM implemented in the LIBSVM toolbox (Chang
and Lin, 2011). The selection of the kernel (linear,
polynomial, radial basis function and sigmoid) and
the optimization of the parameters are carried out
through grid search in 10-fold cross-validation.
GP regression models with Gaussian noise are a
rare exception where the exact inference with like-
lihood functions is tractable, see ?2 in (Rasmussen
1262
and Williams, 2006). Unfortunately, this is not valid
for the classification task ? see ?3 in (Rasmussen and
Williams, 2006) ? where an approximation method
is required. In this work, we use the Laplace ap-
proximation method proposed in (Williams and Bar-
ber, 1998). Different kernels are tested (covariance
for constant functions, linear with and without au-
tomatic relevance determination (ARD)1, Matern,
neural network, etc.2) and the linear logistic (lll)
and probit regression (prl) likelihood functions are
evaluated in classification. In our classification ex-
periments we tried all possible combinations of ker-
nels and likelihood functions, while in the regression
tests we ranged only on different kernels. All the GP
models were implemented using the GPML Matlab
toolbox3. Unlike SVMs, the optimization of the ker-
nel parameters can be performed without using grid
search, but the optimal parameters can be obtained
iteratively, by maximizing the marginal likelihood
(or in classification, the Laplace approximation of
the marginal likelihood). We fix at 100 the maxi-
mum number of iterations.
An interesting property of the GPs is their capa-
bility of weighting the features differently accord-
ing to their importance in the data. This is referred
to as the automatic variance determination kernel.
As demonstrated in (Weston et al, 2000), SVMs
can benefit from the application of feature selec-
tion techniques especially when there are highly re-
dundant features. Since the prior polarities formu-
lae tend to cluster in groups that provide similar re-
sults (Gatti and Guerini, 2012) ? creating noise for
the learner ? we want to understand whether feature
selection approaches can boost the performance of
SVMs. For this reason, we also test feature selection
prior to the SVM training. For that we used Ran-
domized Lasso, or stability selection (Meinshausen
and Bu?hlmann, 2010). Re-sampling of the training
data is performed several times and a Lasso regres-
sion model is fit on each sample. Features that ap-
pear in a given number of samples are retained. Both
the fraction of the data to be sampled and the thresh-
old to select the features can be configured. In our
1linone and linard in the result tables, respectively.
2More detailed information on the available kernels are in
?4 (Rasmussen and Williams, 2006)
3http://www.gaussianprocess.org/gpml/
code/matlab/doc/
experiments we set the sampling fraction to 75%,
the selection threshold to 25% and the number of re-
samples to 1,000. We refer to these as SVMfs.
6 Gold Standards
To assess how well prior polarity formulae perform,
a gold standard with word polarities provided by hu-
man annotators is needed. There are many such re-
sources in the literature, each with different cover-
age and annotation characteristics. ANEW (Bradley
and Lang, 1999) rates the valence score of 1,034
words, which were presented in isolation to anno-
tators. The SO-CAL entries (Taboada et al, 2011)
were collected from corpus data and then manu-
ally tagged by a small number of annotators with
a multi-class label. These ratings were further vali-
dated through crowdsourcing. Other resources, such
as the General Inquirer lexicon (Stone et al, 1966),
provide a binomial classification (either positive or
negative) of sentiment-bearing words. The resource
presented in (Wilson et al, 2005) uses a similar bi-
nomial annotation for single words; another inter-
esting resource is WordNetAffect (Strapparava and
Valitutti, 2004) but it labels words senses and it can-
not be used for the prior polarity validation task.
In the following we describe in detail the two
resources we used for our experiments, namely
ANEW for the regression experiments and the Gen-
eral Inquirer (GI) for the classification ones.
6.1 ANEW
ANEW (Bradley and Lang, 1999) is a resource de-
veloped to provide a set of normative emotional rat-
ings for a large number of words (roughly 1 thou-
sand) in the English language. It contains a set of
words that have been rated in terms of pleasure (af-
fective valence), arousal, and dominance. In par-
ticular for our task we considered the valence di-
mension. Since words were presented to subjects
in isolation (i.e. no context was provided) this re-
source represents a human validation of prior polar-
ities scores for the given words, and can be used as a
gold standard. For each word ANEW provides two
main metrics: anew?, which correspond to the av-
erage of annotators votes, and anew?, which gives
the variance in annotators scores for the given word.
In the same way these metrics are also provided for
1263
the male/female annotator groups.
6.2 General Inquirer
The Harvard General Inquirer dictionary is a widely
used resource, built for automatic text analysis
(Stone et al, 1966). Its latest revision4 contains
11789 words, tagged with 182 semantic and prag-
matic labels, as well as with their part of speech.
Words and their categories were initially taken
from the Harvard IV-4 Psychosociological Dictio-
nary (Dunphy et al, 1974) and the Lasswell Value
Dictionary (Lasswell and Namenwirth, 1969). For
this paper we consider the Positiv and Negativ
categories (1,915 words the former, 2,291 words the
latter, for a total of 4,206 affective words).
7 Experiments
In order to use the ANEW dataset to measure
prior polarities formulae performance, we had to
assign a PoS to all the words to obtain the SWN
lemma#PoS format. To do so, we proceeded as
follows: for each word, check if it is present among
both SWN1 and SWN3 lemmas; if not, lemmatize
the word with the TextPro tool suite (Pianta et al,
2008) and check if the lemma is present instead5.
If it is not found (i.e., the word cannot be aligned
automatically), remove the word from the list (this
was the case for 30 words of the 1,034 present in
ANEW). The remaining 1,004 lemmas were then
associated with all the PoS present in SWN to get
the final lemma#PoS. Note that a lemma can have
more than one PoS, for example, writer is present
only as a noun (writer#n), while yellow is present
as a verb, a noun and an adjective (yellow#v,
yellow#n, yellow#a). This gave us a list of
1,484 words in the lemma#PoS format.
In a similar way we pre-processed the GI words
that uses the generic modif label to indicate ei-
ther adjective or adverb (noun and verb PoS were
instead consistently used). Finally, all the sense-
disambiguated words in the lemma#PoS#n format
were discarded (1,114 words out of the 4,206 words
with positive or negative valence).
4http://www.wjh.harvard.edu/?inquirer/
5We did not lemmatize everything to avoid duplications (for
example, if we lemmatize the ANEW entry addicted, we obtain
addict, which is already present in ANEW).
After the two datasets were built this way, we
removed the words for which the posScore and
negScore contained all 0 in both SWN1 and
SWN3 (523 lemma#PoS for ANEW and 484 for
the GI dataset), since these words are not informa-
tive for our experiments. The final dataset included
961 entries for ANEW and 2,557 for GI. For each
lemma#PoS in GI and ANEW, we then applied the
prior polarity formulae described in Section 4, using
both SWN1 and SWN3 and annotated the results.
According to the nature of the human labels (real
numbers or -1/1), we ran several regression and clas-
sification experiments. In both cases, each dataset
was randomly split into 70% for training and the re-
maining for test. This process was repeated 5 times
to generate different splits. For each partition, opti-
mization of the learning algorithm parameters was
performed on the training data (in 10-fold cross-
validation for SVMs). Training and test sets were
normalized using the z-score.
To evaluate the performance of our regression ex-
periments on ANEW we used the Mean Absolute
Error (MAE), that averages the error over a given
test set. Accuracy was used for the classification ex-
periments on GI instead. We opted for accuracy ?
rather than F1 ? since for us True Negatives have
same importance as True Positives. For each experi-
ments we reported the average performance and the
standard deviation over the 5 random splits. In the
following sections, to check if there was a statisti-
cally significant difference in the results, we used
Student?s t-test for regression experiments, while
an approximate randomization test (Yeh, 2000) was
used for the classification experiments.
In Tables 2 and 3, the results of regression exper-
iments over the ANEW dataset, using SWN1 and
SWN3, are presented. The results of the classifica-
tion experiments over the GI dataset, using SWN1
and SWN3 are shown in Tables 4 and 5. For the
sake of interpretability, results are divided accord-
ing to the main approaches: randoms, posterior-to-
prior formulae, learning algorithms. Note that for
classification we report the generics f and not the
fm and fd variants. In fact, both versions always
return the same classification answer (we are clas-
sifying according to the sign of f result and not its
strength). For the GPs, we report the two best con-
figurations only.
1264
MAE ? MAE ?
rnd 0.652 0.026
swnrndm 0.427 0.011
swnrndd 0.426 0.009
uniwm 0.420 0.009
maxm 0.419 0.009
fsd 0.413 0.011
fsm 0.412 0.009
uni 0.410 0.010
uniwd 0.406 0.007
w1snm 0.405 0.011
maxd 0.404 0.005
w2snm 0.402 0.011
mediand 0.401 0.014
w1d 0.401 0.010
w1nd 0.399 0.008
meand 0.398 0.010
w2d 0.398 0.010
medianm 0.397 0.015
w1snd 0.397 0.008
w2snd 0.397 0.008
w2nd 0.397 0.008
w1sm 0.396 0.010
w1m 0.396 0.010
w1nm 0.394 0.009
meanm 0.393 0.011
w2sd 0.393 0.008
w1sd 0.393 0.009
w2sm 0.392 0.010
w2m 0.391 0.011
w2nm 0.391 0.012
GPlinard 0.398 0.014
GPlinone 0.398 0.014
SVM 0.367 0.010
SVMfs 0.366 0.011
AVERAGE 0.398 0.010
Table 2: MAE results for metrics using SWN1
8 General Discussion
In this section we sum up the main results of our
analysis, providing an answer to the various ques-
tions we introduced at the beginning of the paper:
SentiWordNet improves over random. One of
the first things worth noting ? in Tables 2, 3, 4 and
5 ? is that the random approach (rnd), as expected,
is the worst performing metric, while all other ap-
proaches, based on SWN, have statistically signif-
icant improvements both for MAE and for Accu-
racy (p < 0.001). So, using SWN for posterior-
to-prior polarity computation brings benefits, since
it increases the performance above the baseline in
words? prior polarity assessment.
SWN3 is better than SWN1. With respect to
MAE ? MAE ?
rnd 0.652 0.026
swnrndd 0.404 0.013
swnrndm 0.402 0.010
maxm 0.393 0.009
fsd 0.382 0.008
uniwm 0.382 0.015
fsm 0.381 0.010
medianm 0.377 0.008
uniwd 0.377 0.012
mediand 0.377 0.011
uni 0.376 0.010
maxd 0.372 0.011
meand 0.371 0.010
w1snm 0.371 0.011
w2snm 0.369 0.010
w1d 0.368 0.010
w2d 0.367 0.010
meanm 0.367 0.010
w1m 0.365 0.010
w2snd 0.364 0.011
w1snd 0.364 0.010
w1sm 0.363 0.009
w1nd 0.362 0.009
w2sd 0.362 0.010
w2m 0.362 0.010
w1sd 0.362 0.009
w1nm 0.362 0.007
w2nd 0.361 0.010
w2sm 0.360 0.009
w2nm 0.359 0.009
GPlinone 0.356 0.008
GPlinard 0.355 0.008
SVM 0.333 0.004
SVMfs 0.333 0.003
AVERAGE 0.366 0.009
Table 3: MAE results for regression using SWN3
SWN1, using SWN3 enhances performance, both
in regression (MAE ? 0.398 vs. 0.366, p < 0.001)
and classification (Accuracy ? 0.710 vs. 0.771,
p < 0.001) tasks. Since many of the approaches
described in the literature use SWN1 their results
should be revised and SWN3 should be used as
standard. This difference in performance can be
partially explained by the fact that, even after pre-
processing, for the ANEW dataset 137 lemma#PoS
have all senses equal to 0 in SWN1, while in SWN3
they are just 48. In the GI lexicon the numbers are
233 for SWN1 and 69 for SWN3.
Not all formulae are created equal. The formu-
lae described in Section 4 have very different results,
along a continuum. While inspecting every differ-
1265
Acc. ? Acc. ?
rnd 0.447 0.019
swn rndm 0.639 0.026
swn rndd 0.646 0.021
fs m 0.659 0.020
uni 0.684 0.017
median 0.686 0.022
uniw 0.702 0.019
max 0.710 0.022
w1 0.712 0.021
w1n 0.713 0.022
w2n 0.714 0.023
w2 0.715 0.021
mean 0.718 0.023
w2s 0.719 0.023
w2sn 0.719 0.023
w1s 0.719 0.023
w1sn 0.719 0.023
GP llllinard 0.721 0.026
GP prllinard 0.722 0.025
SVM 0.733 0.021
SVMfs 0.743 0.021
Average 0.710 0.022
Table 4: Accuracy results for classification using SWN1
ence in performance is out of the scope of the present
paper, we can see that there is a strong difference be-
tween best and worst performing formulae both in
regression (in Table 2 w2nm is better than uniwm,
in Table 3 w2nm is better than maxm) and classifi-
cation (in Table 4 w1snm is better than fsm,in Ta-
ble 5 w2m is better than fsm) and these differences
are all statistically significant (p < 0.001). Again,
these results indicate that the previous experiments
in the literature that use SWN as a baseline should
be revised to take these results into account. Further-
more, the new formulae we introduced, based on the
?posterior polarities saliency? hypothesis, proved to
be among the best performing in all experiments.
This entails that there is room for inspecting new
formulae variants other than those already proposed
in the literature.
Selecting just one sense is not a good choice.
On a side note, the approaches that rely on only one
sense polarity (namely fs, median and max) have
similar results which do not differ significantly from
swnrnd (for maxm, fsd and fsm in Table 2, and
for maxm in Table 3). These same approaches are
also far from the best performing formulae: in Ta-
ble 3, mediand differs from w2nm (p < 0.05), as
do maxm, maxd, fsm and fsd (p < 0.001); in Ta-
Acc. ? Acc. ?
rnd 0.447 0.019
swn rndd 0.700 0.030
swn rndm 0.706 0.034
fs 0.723 0.014
medianm 0.742 0.016
uni 0.750 0.015
uniw 0.762 0.023
max 0.769 0.019
w2s 0.777 0.017
w2sn 0.777 0.017
w1s 0.777 0.017
w1sn 0.777 0.017
w1n 0.780 0.021
w2n 0.780 0.022
mean 0.781 0.018
w1 0.781 0.021
w2 0.781 0.021
SVM 0.779 0.016
GPl 0.779 0.018
GPg 0.781 0.018
SVMfs 0.792 0.014
Average 0.771 0.018
Table 5: Accuracy results for classification using SWN3
ble 3, fs, max and median in both their fm and fd
variants are significantly different from the best per-
forming w2nm (p < 0.001). For classification, in
Table 4 and 5 the difference between the correspond-
ing best performing formula and the single senses
formulae is always significant (at least p < 0.01).
Among other things, this finding entails, surpris-
ingly, that taking the first sense of a lemma#PoS in
some cases has no improvement over taking a ran-
dom sense, and that in all cases it is one of the worst
approaches with SWN . This is surprising since in
many NLP tasks, such as word sense disambigua-
tion, algorithms based on most frequent sense repre-
sent a very strong baseline6.
Learning improvements. Combining the formu-
lae in a learning framework further improves the
results over the best performing formulae, both in
regression (MAE? with SWN1 0.366 vs. 0.391,
p < 0.001; MAE? with SWN3 0.333 vs. 0.359,
p < 0.001) and in classification (Accuracy? for
SWN1 is 0.743 vs. 0.719, p < 0.001; Accuracy?
for SWN3 is 0.792 vs. 0.781, not significant p =
0.07). Another thing worth noting is that, in re-
gression, GPs are outperformed by both versions of
6In SemEval 2010, only 5 participants out of 29 performed
better than the most frequent threshold (Agirre et al, 2010).
1266
SVM (p < 0.001), see Tables 2 and 3. This is in
contrast with the results presented in (Cohn and Spe-
cia, 2013), where GPs on the single task are on av-
erage better than SVMs. In classification, GPs have
similar performance to SVM without feature selec-
tion, and in some cases (see Table 5) even slightly
better. Analyzing the selected kernels for GPs and
SVMs, we notice that in most of the splits SVMs
prefer the radial based function, while the best per-
formance with the GPs are obtained with linear ker-
nels with and without ARD. There is no significant
difference in using linear logistic and probit regres-
sion likelihoods. In all our experiments, SVM with
feature selection leads to the best performance. This
is not surprising due the high level of redundancy
in the formulae scores. Interestingly, inspecting the
most frequent selected features by SVMfs, we see
that features from different groups are selected, and
even the worst performing formulae can add infor-
mation, confirming the idea that viewing the same
information from different perspectives (i.e. the pos-
terior polarities provided by SWN combined in var-
ious ways) can give better predictions.
To sum up: the new state-of-the-art performance
level in prior-polarity computation is represented
by the SVMfs approach using SWN3, and this
should be used as the reference from now on.
9 PoS and Gender Experiments
Next, we wanted to understand if the performance of
our approach, using SWN3, was consistent across
word PoS. In Table 6 we report the results for the
best performing formulae and learning algorithm on
the GI PoS classes. In particular for ADJ there are
1,073 words, 922 for NOUN and 508 for VERB. We
discarded adverbs since the class was too small to
allow reliable evaluation and efficient learning (only
54 instances). The results show a greater accuracy
for adjectives (p < 0.01), while performance for
nouns and verbs are similar.
SVMfs best f
Acc. ? Acc. ? Acc. ? Acc. ?
ADJ 0.829 0.019 0.821 0.016
NOUN 0.784 0.021 0.765 0.023
VERBS 0.782 0.052 0.744 0.046
Table 6: Accuracy results for PoS using SWN3
Finally we test against the male and female ratings
provided by ANEW. As can be seen from Table 7,
SWN approaches are far more precise in predicting
Male judgments rather than Female ones (MAE?
goes from 0.392 to 0.323 with the best formula and
from 0.369 to 0.292 with SVMfs, both differences
are significant p < 0.001). Instead, in Table 8 ?
which displays the results along gender and polarity
dimensions ? there is no statistically significant dif-
ference in MAE on positive words between male
and female, while there is a strong statistical signifi-
cance for negative words (p < 0.001).
Interestingly, there is also a large difference be-
tween positive and negative affective words (both
for male and female dimensions). This difference
is maximum for male scores on positive words com-
pared to female scores on negative words (0.283 vs.
0.399, p < 0.001). Recent work by Warriner et al
(2013) inspected the differences in prior polarity as-
sessment due to gender.
At this stage we can only note that prior polari-
ties calculated with SWN are closer to ANEW male
annotations than female ones. Understanding why
this happens would require an accurate examination
of the methods used to create WordNet and SWN
(which will be the focus of our future work).
Male female
MAE ? MAE ? MAE ? MAE ?
SVMfs 0.292 0.020 0.369 0.008
best f 0.323 0.022 0.392 0.010
Table 7: MAE results for Male vs Female using SWN3
Male female
MAE ? MAE ? MAE ? MAE ?
Pos 0.283 0.022 0.340 0.009
Neg 0.301 0.029 0.399 0.013
Table 8: MAE for Male/Female - Pos/Neg using SWN3
10 Conclusions
We have presented a study on the posterior-to-prior
polarity issue, i.e. the problem of computing words?
prior polarity starting from their posterior polarities.
Using two different versions of SentiWordNet and
30 different approaches that have been proposed in
the literature, we have shown that researchers have
not paid sufficient attention to this issue. Indeed, we
1267
showed that the better variants outperform the oth-
ers on different datasets both in regression and clas-
sification tasks, and that they can represent a fairer
state-of-art baseline approach using SentiWordNet.
On top of this, we also showed that these state-of-
the-art formulae can be further outperformed using
a learning framework that combines the various for-
mulae together. We conclude our analysis with some
experiments investigating the impact of word PoS
and annotator gender in gold standards, showing in-
teresting phenomena that requires further investiga-
tion.
Acknowledgments
The authors thanks Jose? Camargo De Souza for his
help with feature selection. This work has been par-
tially supported by the Trento RISE PerTe project.
References
E. Agirre, O.L. De Lacalle, C. Fellbaum, S.K. Hsieh,
M. Tesconi, M. Monachini, P. Vossen, and R. Segers.
2010. Semeval-2010 task 17: All-words word sense
disambiguation on a specific domain. In Proceedings
of the 5th International Workshop on Semantic Evalu-
ation (IWSE ?10), pages 75?80, Uppsala, Sweden.
S. Agrawal and T.J. Siddiqui. 2009. Using syntactic and
contextual information for sentiment polarity analysis.
In Proceedings of the 2nd International Conference on
Interaction Sciences: Information Technology, Culture
and Human (ICIS ?09), pages 620?623, Seoul, Repub-
lic of Korea.
S. Baccianella, A. Esuli, and F. Sebastiani. 2010. Senti-
WordNet 3.0: An enhanced lexical resource for sen-
timent analysis and opinion mining. In Proceed-
ings of the 7th Conference on International Language
Resources and Evaluation (LREC ?10), pages 2200?
2204, Valletta, Malta.
M.M. Bradley and P.J. Lang. 1999. Affective norms for
English words (ANEW): Instruction manual and af-
fective ratings. Technical Report C-1, University of
Florida.
C.C. Chang and C.J. Lin. 2011. LIBSVM: A library
for support vector machines. ACM Transactions on
Intelligent Systems and Technology, 2:27:1?27:27.
F.R. Chaumartin. 2007. UPAR7: A knowledge-based
system for headline sentiment tagging. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations (IWSE ?07), pages 422?425, Prague, Czech Re-
public.
F.M. Chowdhury, M. Guerini, S. Tonelli, and A. Lavelli.
2013. Fbk: Sentiment analysis in twitter with tweet-
sted. In Second Joint Conference on Lexical and Com-
putational Semantics (*SEM): Proceedings of the Sev-
enth International Workshop on Semantic Evaluation
(SemEval ?13), volume 2, pages 466?470, Atlanta,
Georgia, USA, June.
T. Cohn and L. Specia. 2013. Modelling annotator bias
with multi-task gaussian processes: An application to
machine translation quality estimation. In Proceed-
ings of the 51th Annual Meeting of the Association
for Computational Linguistics (ACL ?13), pages 32?
42, Sofia, Bulgaria.
K. Denecke. 2008. Accessing medical experiences
and information. In Proceedings of the 18th Euro-
pean Conference on Artificial Intelligence, Workshop
on Mining Social Data (MSoDa ?08), Patras, Greece.
K. Denecke. 2009. Are SentiWordNet scores suited for
multi-domain sentiment classification? In Proceed-
ings of the 4th International Conference on Digital
Information Management (ICDIM ?09), pages 32?37,
Ann Arbor, MI, USA.
A. Devitt and K. Ahmad. 2007. Sentiment polarity
identification in financial news: A cohesion-based ap-
proach. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics (ACL
?07), pages 984?991, Prague, Czech Republic.
D.C. Dunphy, C.G. Bullard, and E.E.M. Crossing. 1974.
Validation of the General Inquirer Harvard IV Dictio-
nary. Paper presented at the Pisa Conference on Con-
tent Analysis.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion min-
ing. In Proceedings of the 5th Conference on Inter-
national Language Resources and Evaluation (LREC
?06), pages 417?422, Genova, Italy.
L. Gatti and M. Guerini. 2012. Assessing sentiment
strength in words prior polarities. In Proceedings of
the 24th International Conference on Computational
Linguistics (COLING ?12), pages 361?370, Mumbai,
India.
R. Giora. 1997. Understanding figurative and literal lan-
guage: The graded salience hypothesis. Cognitive Lin-
guistics, 8:183?206.
M. Guerini, O. Stock, and C. Strapparava. 2008.
Valentino: A tool for valence shifting of natural lan-
guage texts. In Proceedings of the 6th International
Conference on Language Resources and Evaluation
(LREC ?08), pages 243?246, Marrakech, Morocco.
H.D. Lasswell and J.Z. Namenwirth. 1969. The Lasswell
value dictionary. New Haven.
B. Liu and L. Zhang. 2012. A survey of opinion mining
and sentiment analysis. Mining Text Data, pages 415?
463.
1268
N. Meinshausen and P. Bu?hlmann. 2010. Stability selec-
tion. Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 72(4):417?473.
A. Neviarouskaya, H. Prendinger, and M. Ishizuka.
2009. Sentiful: Generating a reliable lexicon for
sentiment analysis. In Proceedings of the 3rd Affec-
tive Computing and Intelligent Interaction (ACII ?09),
pages 363?368, Amsterdam, Netherlands.
A. Neviarouskaya, H. Prendinger, and M. Ishizuka.
2011. Affect analysis model: novel rule-based ap-
proach to affect sensing from text. Natural Language
Engineering, 17(1):95.
G. O?zbal and C. Strapparava. 2012. A computational ap-
proach to the automation of creative naming. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL ?12), pages 703?
711, Jeju Island, Korea.
G. O?zbal, C. Strapparava, and M. Guerini. 2012. Brand
Pitt: A corpus to explore the art of naming. In Pro-
ceedings of the 8th International Conference on Lan-
guage Resources and Evaluation (LREC ?12), pages
1822?1828, Istanbul, Turkey.
G. Paltoglou, M. Thelwall, and K. Buckley. 2010. On-
line textual communications annotated with grades of
emotion strength. In Proceedings of the 3rd Interna-
tional Workshop of Emotion: Corpora for research on
Emotion and Affect (satellite of LREC ?10), pages 25?
31, Valletta, Malta.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
E. Pianta, C. Girardi, and R. Zanoli. 2008. The TextPro
tool suite. In Proceedings of the 6th International
Conference on Language Resources and Evaluation
(LREC ?08), pages 2603?2607, Marrakech, Morocco.
I. Piller. 2003. Advertising as a site of language contact.
Annual Review of Applied Linguistics, 23:170?183.
T. Polajnar, S. Rogers, and M. Girolami. 2011. Protein
interaction detection in sentences via gaussian pro-
cesses: a preliminary evaluation. International jour-
nal of data mining and bioinformatics, 5(1):52?72.
L. Qu, C. Toprak, N. Jakob, and I. Gurevych. 2008.
Sentence level subjectivity and sentiment analysis ex-
periments in NTCIR-7 MOAT challenge. In Proceed-
ings of the 7th NTCIR Workshop Meeting (NTCIR ?08),
pages 210?217, Tokyo, Japan.
C.E. Rasmussen and C.K.I. Williams. 2006. Gaussian
processes for machine learning. MIT Press.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel meth-
ods for pattern analysis. Cambridge university press.
J.K. Sing, S. Sarkar, and T.K. Mitra. 2012. Devel-
opment of a novel algorithm for sentiment analysis
based on adverb-adjective-noun combinations. In Pro-
ceedings of the 3rd National Conference on Emerging
Trends and Applications in Computer Science (NC-
ETACS ?12), pages 38?40, Shillong, India.
P.J. Stone, D.C. Dunphy, and M.S. Smith. 1966. The
General Inquirer: A Computer Approach to Content
Analysis. MIT press.
C. Strapparava and A. Valitutti. 2004. WordNet-Affect:
an affective extension of WordNet. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation (LREC ?04), pages 1083 ?
1086, Lisbon, Portugal.
M. Taboada, J. Brooke, M. Tofiloski, K. Voll, and
M. Stede. 2011. Lexicon-based methods for senti-
ment analysis. Computational linguistics, 37(2):267?
307.
T.T. Thet, J.C. Na, C.S.G. Khoo, and S. Shakthikumar.
2009. Sentiment analysis of movie reviews on dis-
cussion boards using a linguistic approach. In Pro-
ceedings of the 1st international CIKM workshop on
Topic-sentiment analysis for mass opinion (TSA ?09),
pages 81?84, Hong Kong.
S. Wang and C.D. Manning. 2012. Baselines and bi-
grams: Simple, good sentiment and topic classifica-
tion. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (ACL ?12),
pages 90?94, Jeju Island, Korea.
A.B. Warriner, V. Kuperman, and M. Brysbaert. 2013.
Norms of valence, arousal, and dominance for 13,915
english lemmas. Behavior research methods, pages 1?
17.
J. Weston, S. Mukherjee, O. Chapelle, M. Pontil, T. Pog-
gio, and V. Vapnik. 2000. Feature selection for SVMs.
In Proceedings of the 14th Conference on Neural In-
formation Processing Systems (NIPS ?00), pages 668?
674, Denver, CO, USA.
C.K.I. Williams and D. Barber. 1998. Bayesian clas-
sification with gaussian processes. Pattern Analy-
sis and Machine Intelligence, IEEE Transactions on,
20(12):1342?1351.
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad
are you? Finding strong and weak opinion clauses. In
Proceedings of the 19th National Conference on Artifi-
cial Intelligence (AAAI ?04), pages 761?769, San Jose,
CA, USA.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing (HLT/EMNLP ?05), pages
347?354, Vancouver, Canada.
A. Yeh. 2000. More accurate tests for the statistical sig-
nificance of result differences. In Proceedings of the
18th International Conference on Computational Lin-
guistics (COLING ?00), pages 947?953, Saarbru?cken,
Germany.
1269
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 988?996,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Ecological Evaluation of Persuasive Messages Using Google AdWords
Marco Guerini
Trento-Rise
Via Sommarive 18, Povo
Trento ? Italy
marco.guerini@trentorise.eu
Carlo Strapparava
FBK-Irst
Via Sommarive 18, Povo
Trento ? Italy
strappa@fbk.eu
Oliviero Stock
FBK-Irst
Via Sommarive 18, Povo
Trento ? Italy
stock@fbk.eu
Abstract
In recent years there has been a growing in-
terest in crowdsourcing methodologies to be
used in experimental research for NLP tasks.
In particular, evaluation of systems and theo-
ries about persuasion is difficult to accommo-
date within existing frameworks. In this paper
we present a new cheap and fast methodology
that allows fast experiment building and eval-
uation with fully-automated analysis at a low
cost. The central idea is exploiting existing
commercial tools for advertising on the web,
such as Google AdWords, to measure message
impact in an ecological setting. The paper in-
cludes a description of the approach, tips for
how to use AdWords for scientific research,
and results of pilot experiments on the impact
of affective text variations which confirm the
effectiveness of the approach.
1 Introduction
In recent years there has been a growing interest in
finding new cheap and fast methodologies to be used
in experimental research, for, but not limited to, NLP
tasks. In particular, approaches to NLP that rely on
the use of web tools - for crowdsourcing long and
tedious tasks - have emerged. Amazon Mechani-
cal Turk, for example, has been used for collecting
annotated data (Snow et al, 2008). However ap-
proaches a la Mechanical Turk might not be suitable
for all tasks.
In this paper we focus on evaluating systems and
theories about persuasion, see for example (Fogg,
2009) or the survey on persuasive NL generation
studies in (Guerini et al, 2011a). Measuring the
impact of a message is of paramount importance in
this context, for example how affective text varia-
tions can alter the persuasive impact of a message.
The problem is that evaluation experiments repre-
sent a bottleneck: they are expensive and time con-
suming, and recruiting a high number of human par-
ticipants is usually very difficult.
To overcome this bottleneck, we present a specific
cheap and fast methodology to automatize large-
scale evaluation campaigns. This methodology al-
lows us to crowdsource experiments with thousands
of subjects for a few euros in a few hours, by tweak-
ing and using existing commercial tools for adver-
tising on the web. In particular we make reference
to the AdWords Campaign Experiment (ACE) tool
provided within the Google AdWords suite. One
important aspect of this tool is that it allows for real-
time fully-automated data analysis to discover sta-
tistically significant phenomena. It is worth noting
that this work originated in the need to evaluate the
impact of short persuasive messages, so as to assess
the effectiveness of different linguistic choices. Still,
we believe that there is further potential for opening
an interesting avenue for experimentally exploring
other aspects of the wide field of pragmatics.
The paper is structured as follows: Section 2 dis-
cusses the main advantages of ecological approaches
using Google ACE over traditional lab settings and
state-of-the-art crowdsourcing methodologies. Sec-
tion 3 presents the main AdWords features. Section
4 describes how AdWords features can be used for
defining message persuasiveness metrics and what
kind of stimulus characteristics can be evaluated. Fi-
nally Sections 5 and 6 describe how to build up an
988
experimental scenario and some pilot studies to test
the feasibility of our approach.
2 Advantages of Ecological Approaches
Evaluation of the effectiveness of persuasive sys-
tems is very expensive and time consuming, as the
STOP experience showed (Reiter et al, 2003): de-
signing the experiment, recruiting subjects, making
them take part in the experiment, dispensing ques-
tionnaires, gathering and analyzing data.
Existing methodologies for evaluating persuasion
are usually split in two main sets, depending on the
setup and domain: (i) long-term, in the field eval-
uation of behavioral change (as the STOP example
mentioned before), and (ii) lab settings for evaluat-
ing short-term effects, as in (Andrews et al, 2008).
While in the first approach it is difficult to take into
account the role of external events that can occur
over long time spans, in the second there are still
problems of recruiting subjects and of time consum-
ing activities such as questionnaire gathering and
processing.
In addition, sometimes carefully designed exper-
iments can fail because: (i) effects are too subtle to
be measured with a limited number of subjects or
(ii) participants are not engaged enough by the task
to provoke usable reactions, see for example what
reported in (Van Der Sluis and Mellish, 2010). Es-
pecially the second point is awkward: in fact, sub-
jects can actually be convinced by the message to
which they are exposed, but if they feel they do not
care, they may not ?react? at all, which is the case in
many artificial settings. To sum up, the main prob-
lems are:
1. Time consuming activities
2. Subject recruitment
3. Subject motivation
4. Subtle effects measurements
2.1 Partial Solution - Mechanical Turk
A recent trend for behavioral studies that is emerg-
ing is the use of Mechanical Turk (Mason and Suri,
2010) or similar tools to overcome part of these limi-
tations - such as subject recruitment. Still we believe
that this poses other problems in assessing behav-
ioral changes, and, more generally, persuasion ef-
fects. In fact:
1. Studies must be as ecological as possible, i.e.
conducted in real, even if controlled, scenarios.
2. Subjects should be neither aware of being ob-
served, nor biased by external rewards.
In the case of Mechanical Turk for example, sub-
jects are willingly undergoing a process of being
tested on their skills (e.g. by performing annota-
tion tasks). Cover stories can be used to soften this
awareness effect, nonetheless the fact that subjects
are being paid for performing the task renders the
approach unfeasible for behavioral change studies.
It is necessary that the only reason for behavior in-
duction taking place during the experiment (filling
a form, responding to a questionnaire, clicking on
an item, etc.) is the exposition to the experimental
stimuli, not the external reward. Moreover, Mechan-
ical Turk is based on the notion of a ?gold standard?
to assess contributors reliability, but for studies con-
cerned with persuasion it is almost impossible to de-
fine such a reference: there is no ?right? action the
contributor can perform, so there is no way to assess
whether the subject is performing the action because
induced to do so by the persuasive strategy, or just
in order to receive money. On the aspect of how to
handle subject reliability in coding tasks, see for ex-
ample the method proposed in (Negri et al, 2010).
2.2 Proposed Solution - Targeted Ads on the
Web
Ecological studies (e.g. using Google AdWords) of-
fer a possible solution to the following problems:
1. Time consuming activities: apart from experi-
mental design and setup, all the rest is automat-
ically performed by the system. Experiments
can yield results in a few hours as compared to
several days/weeks.
2. Subject recruitment: the potential pool of sub-
jects is the entire population of the web.
3. Subject motivation: ads can be targeted exactly
to those persons that are, in that precise mo-
ment throughout the world, most interested in
the topic of the experiment, and so potentially
more prone to react.
4. Subject unaware, unbiased: subjects are totally
unaware of being tested, testing is performed
during their ?natural? activity on the web.
989
5. Subtle effects measurements: if the are not
enough subjects, just wait for more ads to be
displayed, or focus on a subset of even more
interested people.
Note that similar ecological approaches are begin-
ning to be investigated: for example in (Aral and
Walker, 2010) an approach to assessing the social ef-
fects of content features on an on-line community is
presented. A previous approach that uses AdWords
was presented in (Guerini et al, 2010), but it crowd-
sourced only the running of the experiment, not data
manipulation and analysis, and was not totally con-
trolled for subject randomness.
3 AdWords Features
Google AdWords is Google?s advertising program.
The central idea is to let advertisers display their
messages only to relevant audiences. This is done
by means of keyword-based contextualization on the
Google network, divided into:
? Search network: includes Google search pages,
search sites and properties that display search
results pages (SERPs), such as Froogle and
Earthlink.
? Display network: includes news pages, topic-
specific websites, blogs and other properties -
such as Google Mail and The New York Times.
When a user enters a query like ?cruise? in the
Google search network, Google displays a variety of
relevant pages, along with ads that link to cruise trip
businesses. To be displayed, these ads must be asso-
ciated with relevant keywords selected by the adver-
tiser.
Every advertiser has an AdWords account that is
structured like a pyramid: (i) account, (ii) campaign
and (iii) ad group. In this paper we focus on ad
groups. Each grouping gathers similar keywords to-
gether - for instance by a common theme - around
an ad group. For each ad group, the advertiser sets a
cost-per-click (CPC) bid. The CPC bid refers to the
amount the advertiser is willing to pay for a click on
his ad; the cost of the actual click instead is based
on its quality score (a complex measure out of the
scope of the present paper).
For every ad group there could be multiple ads
to be served, and there are many AdWords measure-
ments for identifying the performance of each single
ad (its persuasiveness, from our point of view):
? CTR, Click Through Rate: measures the num-
ber of clicks divided by the number of impres-
sions (i.e. the number of times an ad has been
displayed in the Google Network).
? Conversion Rate: if someone clicks on an ad,
and buys something on your site, that click is
a conversion from a site visit to a sale. Con-
version rate equals the number of conversions
divided by the number of ad clicks.
? ROI: Other conversions can be page views or
signups. By assigning a value to a conversion
the resulting conversions represents a return on
investment, or ROI.
? Google Analytics Tool: Google Analytics is a
web analytics tool that gives insights into web-
site traffic, like number of visited pages, time
spent on the site, location of visitors, etc.
So far, we have been talking about text ads, -
Google?s most traditional and popular ad format -
because they are the most useful for NLP analysis.
In addition there is also the possibility of creating
the following types of ads:
? Image (and animated) ads
? Video ads
? Local business ads
? Mobile ads
The above formats allow for a greater potential
to investigate persuasive impact of messages (other
than text-based) but their use is beyond the scope of
the present paper1.
4 The ACE Tool
AdWords can be used to design and develop vari-
ous metrics for fast and fully-automated evaluation
experiments, in particular using the ACE tool.
This tool - released in late 2010 - allows testing,
from a marketing perspective, if any change made to
a promotion campaign (e.g. a keyword bid) had a
statistically measurable impact on the campaign it-
self. Our primary aim is slightly different: we are
1For a thorough description of the AdWords tool see:
https://support.google.com/adwords/
990
interested in testing how different messages impact
(possibly different) audiences. Still the ACE tool
goes exactly in the direction we aim at, since it in-
corporates statistically significant testing and allows
avoiding many of the tweaking and tuning actions
which were necessary before its release.
The ACE tool also introduces an option that was
not possible before, that of real-time testing of sta-
tistical significance. This means that it is no longer
necessary to define a-priori the sample size for the
experiment: as soon as a meaningful statistically
significant difference emerges, the experiment can
be stopped.
Another advantage is that the statistical knowl-
edge to evaluate the experiment is no longer nec-
essary: the researcher can focus only on setting up
proper experimental designs2.
The limit of the ACE tool is that it only allows
A/B testing (single split with one control and one ex-
perimental condition) so for experiments with more
than two conditions or for particular experimental
settings that do not fit with ACE testing bound-
aries (e.g. cross campaign comparisons) we suggest
taking (Guerini et al, 2010) as a reference model,
even if the experimental setting is less controlled
(e.g. subject randomness is not equally guaranteed
as with ACE).
Finally it should be noted that even if ACE allows
only A/B testing, it permits the decomposition of al-
most any variable affecting a campaign experiment
in its basic dimensions, and then to segment such
dimensions according to control and experimental
conditions. As an example of this powerful option,
consider Tables 3 and 6 where control and experi-
mental conditions are compared against every single
keyword and every search network/ad position used
for the experiments.
5 Evaluation and Targeting with ACE
Let us consider the design of an experiment with 2
conditions. First we create an ad Group with 2 com-
peting messages (one message for each condition).
Then we choose the serving method (in our opin-
ion the rotate option is better than optimize, since it
2Additional details about ACE features and statistics can be
found at http://www.google.com/ads/innovations/ace.html
guarantees subject randomness and is more transpar-
ent) and the context (language, network, etc.). Then
we activate the ads and wait. As soon as data begins
to be collected we can monitor the two conditions
according to:
? Basic Metrics: the highest CTR measure in-
dicates which message is best performing. It
indicates which message has the highest initial
impact.
? Google Analytics Metrics: measures how much
the messages kept subjects on the site and how
many pages have been viewed. Indicates inter-
est/attitude generated in the subjects.
? Conversion Metrics: measures how much the
messages converted subjects to the final goal.
Indicates complete success of the persuasive
message.
? ROI Metrics: by creating specific ROI values
for every action the user performs on the land-
ing page. The more relevant (from a persuasive
point of view) the action the user performs, the
higher the value we must assign to that action.
In our view combined measurements are better:
for example, there could be cases of messages
with a lower CTR but a higher conversion rate.
Furthermore, AdWords allows very complex tar-
geting options that can help in many different evalu-
ation scenarios:
? Language (see how message impact can vary in
different languages).
? Location (see how message impact can vary in
different cultures sharing the same language).
? Keyword matching (see how message impact
can vary with users having different interests).
? Placements (see how message impact can vary
among people having different values - e.g. the
same message displayed on Democrat or Re-
publican web sites).
? Demographics (see how message impact can
vary according to user gender and age).
5.1 Setting up an Experiment
To test the extent to which AdWords can be ex-
ploited, we focused on how to evaluate lexical varia-
tions of a message. In particular we were interested
991
in gaining insights about a system for affective varia-
tions of existing commentaries on medieval frescoes
for a mobile museum guide that attracts the attention
of visitors towards specific paintings (Guerini et al,
2008; Guerini et al, 2011b). The various steps for
setting up an experiment (or a series of experiments)
are as follows:
Choose a Partner. If you have the opportunity
to have a commercial partner that already has the in-
frastructure for experiments (website, products, etc.)
many of the following steps can be skipped. We as-
sume that this is not the case.
Choose a scenario. Since you may not be
equipped with a VAT code (or with the commercial
partner that furnishes the AdWords account and in-
frastructure), you may need to ?invent something to
promote? without any commercial aim. If a ?social
marketing? scenario is chosen you can select ?per-
sonal? as a ?tax status?, that do not require a VAT
code. In our case we selected cultural heritage pro-
motion, in particular the frescoes of Torre Aquila
(?Eagle Tower?) in Trento. The tower contains a
group of 11 frescoes named ?Ciclo dei Mesi? (cy-
cle of the months) that represent a unique example
of non-religious medieval frescoes in Europe.
Choose an appropriate keyword on which to
advertise, ?medieval art? in our case. It is better
to choose keywords with enough web traffic in or-
der to speed up the experimental process. In our
case the search volume for ?medieval art? (in phrase
match) was around 22.000 hits per month. Another
suggestion is to restrict the matching modality on
Keywords in order to have more control over the
situations in which ads are displayed and to avoid
possible extraneous effects (the order of control
for matching modality is: [exact match], ?phrase
match? and broad match).
Note that such a technical decision - which key-
word to use - is better taken at an early stage of de-
velopment because it affects the following steps.
Write messages optimized for that keyword (e.g.
including it in the title or the body of the ad). Such
optimization must be the same for control and exper-
imental condition. The rest of the ad can be designed
in such a way to meet control and experimental con-
dition design (in our case a message with slightly
affective terms and a similar message with more af-
fectively loaded variations)
Build an appropriate landing page, according
to the keyword and populate the website pages with
relevant material. This is necessary to create a ?cred-
ible environment? for users to interact with.
Incorporate meaningful actions in the website.
Users can perform various actions on a site, and they
can be monitored. The design should include ac-
tions that are meaningful indicators of persuasive ef-
fect/success of the message. In our case we decided
to include some outbound links, representing:
? general interest: ?Buonconsiglio Castle site?
? specific interest: ?Eagle Tower history?
? activated action: ?Timetable and venue?
? complete success: ?Book a visit?
Furthermore, through new Google Analytics fea-
tures, we set up a series of time spent on site and
number of visited pages thresholds to be monitored
in the ACE tool.
5.2 Tips for Planning an Experiment
There are variables, inherent in the Google AdWords
mechanism, that from a research point of view we
shall consider ?extraneous?. We now propose tips
for controlling such extraneous variables.
Add negative matching Keywords: To add more
control, if in doubt, put the words/expressions of the
control and experimental conditions as negative key-
words. This will prevent different highlighting be-
tween the two conditions that can bias the results. It
is not strictly necessary since one can always control
which queries triggered a click through the report
menu. An example: if the only difference between
control and experimental condition is the use of the
adjectives ?gentle knights? vs. ?valorous knights?,
one can use two negative keyword matches: -gentle
and -valorous. Obviously if you are using a key-
word in exact matching to trigger your ads, such as
[knight], this is not necessary.
Frequency capping for the display network: if
you are running ads on the display network, you can
use the ?frequency capping? option set to 1 to add
more control to the experiment. In this way it is as-
sured that ads are displayed only one time per user
on the display network.
Placement bids for the search network: unfor-
tunately this option is no longer available. Basically
the option allowed to bid only for certain positions
992
on the SERPs to avoid possible ?extraneous vari-
ables effect? given by the position. This is best ex-
plained via an example: if, for whatever reason, one
of the two ads gets repeatedly promoted to the pre-
mium position on the SERPs, then the CTR differ-
ence between ads would be strongly biased. From
a research point of view ?premium position? would
then be an extraneous variable to be controlled (i.e.
either both ads get an equal amount of premium po-
sition impressions, or both ads get no premium po-
sition at all). Otherwise the difference in CTR is de-
termined by the ?premium position? rather than by
the independent variable under investigation (pres-
ence/absence of particular affective terms in the text
ad). However even if it is not possible to rule out this
?position effect? it is possible to monitor it by using
the report (Segment > Top vs. other + Experiment)
and checking how many times each ad appeared in
a given position on the SERPs, and see if the ACE
tool reports any statistical difference in the frequen-
cies of ads positions.
Extra experimental time: While planning an ex-
periment, you should also take into account the ads
reviewing time that can take up to several days, in
worst case scenarios. Note that when ads are in eli-
gible status, they begin to show on the Google Net-
work, but they are not approved yet. This means that
the ads can only run on Google search pages and can
only show for users who have turned off SafeSearch
filtering, until they are approved. Eligible ads cannot
run on the Display Network. This status will provide
much less impressions than the final ?approved? sta-
tus.
Avoid seasonal periods: for the above reason,
and to avoid extra costs due to high competition,
avoid seasonal periods (e.g. Christmas time).
Delivery method: if you are planning to use the
Accelerated Delivery method in order to get the re-
sults as quick as possible (in the case of ?quick and
dirty? experiments or ?fast prototyping-evaluation
cycles?) you should consider monitoring your ex-
periment more often (even several times per day) to
avoid running out of budget during the day.
6 Experiments
We ran two pilot experiments to test how affective
variations of existing texts alter their persuasive im-
pact. In particular we were interested in gaining
initial insights about an intelligent system for affec-
tive variations of existing commentaries on medieval
frescoes.
We focused on adjective variations, using a
slightly biased adjective for the control conditions
and a strongly biased variation for the experimen-
tal condition. In these experiments we took it for
granted that affective variations of a message work
better than a neutral version (Van Der Sluis and Mel-
lish, 2010), and we wanted to explore more finely
grained tactics that involve the grade of the vari-
ation (i.e. a moderately positive variation vs. an
extremely positive variation). Note that this is a
more difficult task than the one proposed in (Van
Der Sluis and Mellish, 2010), where they were test-
ing long messages with lots of variations and with
polarized conditions, neutral vs. biased. In addition
we wanted to test how quickly experiments could be
performed (two days versus the two week sugges-
tion of Google).
Adjectives were chosen according to MAX bi-
gram frequencies with the modified noun, using the
Web 1T 5-gram corpus (Brants and Franz, 2006).
Deciding whether this is the best metric for choosing
adjectives to modify a noun or not (e.g. also point-
wise mutual-information score can be used with a
different rationale) is out of the scope of the present
paper, but previous work has already used this ap-
proach (Whitehead and Cavedon, 2010). Top ranked
adjectives were then manually ordered - according to
affective weight - to choose the best one (we used a
standard procedure using 3 annotators and a recon-
ciliation phase for the final decision).
6.1 First Experiment
The first experiment lasted 48 hour with a total of 38
thousand subjects and a cost of 30 euros (see Table
1 for the complete description of the experimental
setup). It was meant to test broadly how affective
variations in the body of the ads performed. The two
variations contained a fragment of a commentary of
the museum guide; the control condition contained
?gentle knight? and ?African lion?, while in the ex-
perimental condition the affective loaded variations
were ?valorous knight? and ?indomitable lion? (see
Figure 1, for the complete ads). As can be seen from
Table 2, the experiment did not yield any significant
993
result, if one looks at the overall analysis. But seg-
menting the results according to the keyword that
triggered the ads (see Table 3) we discovered that
on the ?medieval art? keyword, the control condition
performed better than the experimental one.
Starting Date: 1/2/2012
Ending Date: 1/4/2012
Total Time: 48 hours
Total Cost: 30 euros
Subjects: 38,082
Network: Search and Display
Language: English
Locations: Australia; Canada; UK; US
KeyWords: ?medieval art?, pictures middle ages
Table 1: First Experiment Setup
ACE split Clicks Impr. CTR
Control 31 18,463 0.17%
Experiment 20 19,619 0.10%
Network Clicks Impr. CTR
Search 39 4,348 0.90%
Display 12 34,027 0.04%
TOTAL 51 38,082 0.13%
Table 2: First Experiment Results
Keyword ACE split Impr. CTR
?medieval art? Control 657 0.76%
?medieval art? Experiment 701 0.14%*
medieval times history Control 239 1.67%
medieval times history Experiment 233 0.86%
pictures middle ages Control 1114 1.35%
pictures middle ages Experiment 1215 0.99%
Table 3: First Experiment Results Detail. * indicates a
statistically significant difference with ? < 0.01
Discussion. As already discussed, user moti-
vation is a key element for success in such fine-
grained experiments: while less focused keywords
did not yield any statistically significant differences,
the most specialized keyword ?medieval art? was the
one that yielded results (i.e. if we display messages
like those in Figure 1, that are concerned with me-
dieval art frescoes, only those users really interested
in the topic show different reaction patterns to the af-
fective variations, while those generically interested
in medieval times behave similarly in the two con-
ditions). In the following experiment we tried to see
whether such variations have different effects when
modifying a different element in the text.
Figure 1: Ads used in the first experiment
6.2 Second Experiment
The second experiment lasted 48 hours with a to-
tal of one thousand subjects and a cost of 17 euros
(see Table 4 for the description of the experimen-
tal setup). It was meant to test broadly how affec-
tive variations introduced in the title of the text Ads
performed. The two variations were the same as in
the first experiment for the control condition ?gentle
knight?, and for the experimental condition ?valor-
ous knight? (see Figure 2 for the complete ads). As
can be seen from Table 5, also in this case the experi-
ment did not yield any significant result, if one looks
at the overall analysis. But segmenting the results
according to the search network that triggered the
ads (see Table 6) we discovered that on the search
partners at the ?other? position, the control condition
performed better than the experimental one. Unlike
the first experiment, in this case we segmented ac-
cording to the ad position and search network typol-
ogy since we were running our experiment only on
one keyword in exact match.
Starting Date: 1/7/2012
Ending Date: 1/9/2012
Total Time: 48 hours
Total Cost: 17.5 euros
Subjects: 986
Network: Search
Language: English
Locations: Australia; Canada; UK; US
KeyWords: [medieval knights]
Table 4: Second Experiment Setup
994
Figure 2: Ads used in the second experiment
ACE split Clicks Impr. CTR
Control 10 462 2.16%
Experiment 8 524? 1.52%
TOTAL 18 986 1.82%
Table 5: Second Experiment Results. ? indicates a statis-
tically significant difference with ? < 0.05
Top vs. Other ACE split Impr. CTR
Google search: Top Control 77 6.49%
Google search: Top Experiment 68 2.94%
Google search: Other Control 219 0.00%
Google search: Other Experiment 277* 0.36%
Search partners: Top Control 55 3.64%
Search partners: Top Experiment 65 6.15%
Search partners: Other Control 96 3.12%
Search partners: Other Experiment 105 0.95%?
Total - Search ? 986 1.82%
Table 6: Second Experiment Results Detail. ? indicates a
statistical significance with ? < 0.05, * indicates a sta-
tistical significance with ? < 0.01
Discussion. From this experiment we can confirm
that at least under some circumstances a mild af-
fective variation performs better than a strong varia-
tion. This mild variations seems to work better when
user attention is high (the difference emerged when
ads are displayed in a non-prominent position). Fur-
thermore it seems that modifying the title of the ad
rather than the content yields better results: 0.9% vs.
1.83% CTR (?2 = 6.24; 1 degree of freedom; ? <
0,01) even if these results require further assessment
with dedicated experiments.
As a side note, in this experiment we can see
the problem of extraneous variables: according to
AdWords? internal mechanisms, the experimental
condition was displayed more often in the Google
search Network on the ?other? position (277 vs. 219
impressions - and overall 524 vs. 462), still from a
research perspective this is not a interesting statisti-
cal difference, and ideally should not be present (i.e.
ads should get an equal amount of impressions for
each position).
Conclusions and future work
AdWords gives us an appropriate context for evalu-
ating persuasive messages. The advantages are fast
experiment building and evaluation, fully-automated
analysis, and low cost. By using keywords with a
low CPC it is possible to run large-scale experiments
for just a few euros. AdWords proved to be very ac-
curate, flexible and fast, far beyond our expectations.
We believe careful design of experiments will yield
important results, which was unthinkable before this
opportunity for studies on persuasion appeared.
The motivation for this work was exploration of
the impact of short persuasive messages, so to assess
the effectiveness of different linguistic choices. The
experiments reported in this paper are illustrative ex-
amples of the method proposed and are concerned
with the evaluation of the role of minimal affective
variations of short expressions. But there is enor-
mous further potential in the proposed approach to
ecological crowdsourcing for NLP: for instance, dif-
ferent rhetorical techniques can be checked in prac-
tice with large audiences and fast feedback. The as-
sessment of the effectiveness of a change in the title
as opposed to the initial portion of the text body pro-
vides a useful indication: one can investigate if vari-
ations inside the given or the new part of an expres-
sion or in the topic vs. comment (Levinson, 1983)
have different effects. We believe there is potential
for a concrete extensive exploration of different lin-
guistic theories in a way that was simply not realistic
before.
Acknowledgments
We would like to thank Enrique Alfonseca and
Steve Barrett, from Google Labs, for valuable hints
and discussion on AdWords features. The present
work was partially supported by a Google Research
Award.
995
References
P. Andrews, S. Manandhar, and M. De Boni. 2008. Ar-
gumentative human computer dialogue for automated
persuasion. In Proceedings of the 9th SIGdial Work-
shop on Discourse and Dialogue, pages 138?147. As-
sociation for Computational Linguistics.
S. Aral and D. Walker. 2010. Creating social contagion
through viral product design: A randomized trial of
peer influence in networks. In Proceedings of the 31th
Annual International Conference on Information Sys-
tems.
T. Brants and A. Franz. 2006. Web 1t 5-gram corpus
version 1.1. Linguistic Data Consortium.
BJ Fogg. 2009. Creating persuasive technologies: An
eight-step design process. Proceedings of the 4th In-
ternational Conference on Persuasive Technology.
M. Guerini, O. Stock, and C. Strapparava. 2008.
Valentino: A tool for valence shifting of natural lan-
guage texts. In Proceedings of LREC 2008, Mar-
rakech, Morocco.
M. Guerini, C. Strapparava, and O. Stock. 2010. Evalu-
ation metrics for persuasive nlp with google adwords.
In Proceedings of LREC-2010.
M. Guerini, O. Stock, M. Zancanaro, D.J. O?Keefe,
I. Mazzotta, F. Rosis, I. Poggi, M.Y. Lim, and
R. Aylett. 2011a. Approaches to verbal persuasion in
intelligent user interfaces. Emotion-Oriented Systems,
pages 559?584.
M. Guerini, C. Strapparava, and O. Stock. 2011b. Slant-
ing existing text with Valentino. In Proceedings of the
16th international conference on Intelligent user inter-
faces, pages 439?440. ACM.
S.C. Levinson. 1983. Pragmatics. Cambridge Univer-
sity Press.
W. Mason and S. Suri. 2010. Conducting behavioral
research on amazon?s mechanical turk. Behavior Re-
search Methods, pages 1?23.
M. Negri, L. Bentivogli, Y. Mehdad, D. Giampiccolo, and
A. Marchetti. 2010. Divide and conquer: Crowd-
sourcing the creation of cross-lingual textual entail-
ment corpora. Proc. of EMNLP 2011.
E. Reiter, R. Robertson, and L. Osman. 2003. Lesson
from a failure: Generating tailored smoking cessation
letters. Artificial Intelligence, 144:41?58.
R. Snow, B. O?Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 254?263. Association for
Computational Linguistics.
I. Van Der Sluis and C. Mellish. 2010. Towards empir-
ical evaluation of affective tactical nlg. In Empirical
methods in natural language generation, pages 242?
263. Springer-Verlag.
S. Whitehead and L. Cavedon. 2010. Generating shifting
sentiment for a conversational agent. In Proceedings
of the NAACL HLT 2010 Workshop on Computational
Approaches to Analysis and Generation of Emotion in
Text, pages 89?97, Los Angeles, CA, June. Associa-
tion for Computational Linguistics.
996
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 427?433,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
DepecheMood:
a Lexicon for Emotion Analysis from Crowd-Annotated News
Jacopo Staiano
University of Trento
Trento - Italy
staiano@disi.unitn.it
Marco Guerini
Trento RISE
Trento - Italy
marco.guerini@trentorise.eu
Abstract
While many lexica annotated with words
polarity are available for sentiment anal-
ysis, very few tackle the harder task of
emotion analysis and are usually quite
limited in coverage. In this paper, we
present a novel approach for extracting
? in a totally automated way ? a high-
coverage and high-precision lexicon of
roughly 37 thousand terms annotated with
emotion scores, called DepecheMood.
Our approach exploits in an original way
?crowd-sourced? affective annotation im-
plicitly provided by readers of news ar-
ticles from rappler.com. By provid-
ing new state-of-the-art performances in
unsupervised settings for regression and
classification tasks, even using a na??ve ap-
proach, our experiments show the benefi-
cial impact of harvesting social media data
for affective lexicon building.
1 Introduction
Sentiment analysis has proved useful in several ap-
plication scenarios, for instance in buzz monitor-
ing ? the marketing technique for keeping track
of consumer responses to services and products ?
where identifying positive and negative customer
experiences helps to assess product and service de-
mand, tackle crisis management, etc.
On the other hand, the use of finer-grained mod-
els, accounting for the role of individual emotions,
is still in its infancy. The simple division in ?pos-
itive? vs. ?negative? comments may not suffice, as
in these examples: ?I?m so miserable, I dropped
my IPhone in the water and now it?s not working
anymore? (SADNESS) vs. ?I am very upset, my new
IPhone keeps not working!? (ANGER). While both
texts express a negative sentiment, the latter, con-
nected to anger, is more relevant for buzz monitor-
ing. Thus, emotion analysis represents a natural
evolution of sentiment analysis.
Many approaches to sentiment analysis make
use of lexical resources ? i.e. lists of positive and
negative words ? often deployed as baselines or as
features for other methods, usually machine learn-
ing based (Liu and Zhang, 2012). In these lexica,
words are associated with their prior polarity, i.e.
whether such word out of context evokes some-
thing positive or something negative. For exam-
ple, wonderful has a positive connotation ? prior
polarity ? while horrible has a negative one.
The quest for a high precision and high cov-
erage lexicon, where words are associated with
either sentiment or emotion scores, has several
reasons. First, it is fundamental for tasks such
as affective modification of existing texts, where
words? polarity together with their score are nec-
essary for creating multiple graded variations of
the original text (Inkpen et al, 2006; Guerini et
al., 2008; Whitehead and Cavedon, 2010).
Second, considering word order makes a differ-
ence in sentiment analysis. This calls for a role of
compositionality, where the score of a sentence is
computed by composing the scores of the words
up in the syntactic tree. Works worth mention-
ing in this connection are: Socher et al (2013),
which uses recursive neural networks to learn
compositional rules for sentiment analysis, and
(Neviarouskaya et al, 2009; Neviarouskaya et al,
2011) which exploit hand-coded rules to compose
the emotions expressed by words in a sentence. In
this respect, compositional approaches represent a
new promising trend, since all other approaches,
either using semantic similarity or Bag-of-Words
(BOW) based machine-learning, cannot handle,
for example, cases of texts with same wording
but different words order: ?The dangerous killer
escaped one month ago, but recently he was ar-
rested? (RELIEF, HAPPYNESS) vs. ?The danger-
ous killer was arrested one month ago, but re-
427
cently he escaped? (FEAR). The work in (Wang
and Manning, 2012) partially accounts for this
problem and argues that using word bigram fea-
tures allows improving over BOW based meth-
ods, where words are taken as features in isola-
tion. This way it is possible to capture simple
compositional phenomena like polarity reversing
in ?killing cancer?.
Finally, tasks such as copywriting, where evoca-
tive names are a key element to a successful prod-
uct (Ozbal and Strapparava, 2012; Ozbal et al,
2012) require exhaustive lists of emotion related
words. In such cases no context is given and the
brand name alone, with its perceived prior polar-
ity, is responsible for stating the area of compe-
tition and evoking semantic associations. For ex-
ample Mitsubishi changed the name of one of its
SUVs for the Spanish market, since the original
name Pajero had a very negative prior polarity, as
it means ?wanker? in Spanish (Piller, 2003). Evok-
ing emotions is also fundamental for a successful
name: consider names of a perfume like Obses-
sion, or technological products like MacBook air.
In this work, we aim at automatically producing
a high coverage and high precision emotion lex-
icon using distributional semantics, with numer-
ical scores associated with each emotion, like it
has already been done for sentiment analysis. To
this end, we take advantage in an original way of
massive crowd-sourced affective annotations as-
sociated with news articles, obtained by crawl-
ing the rappler.com social news network. We
also evaluate our lexicon by integrating it in unsu-
pervised classification and regression settings for
emotion recognition. Results indicate that the use
of our resource, even if automatically acquired, is
highly beneficial in affective text recognition.
2 Related Work
Within the broad field of sentiment analysis, we
hereby provide a short review of research efforts
put towards building sentiment and emotion lex-
ica, regardless of the approach in which such lists
are then used (machine learning, rule based or
deep learning). A general overview can be found
in (Pang and Lee, 2008; Liu and Zhang, 2012;
Wilson et al, 2004; Paltoglou et al, 2010).
Sentiment Lexica. In recent years there has
been an increasing focus on producing lists of
words (lexica) with prior polarities, to be used in
sentiment analysis. When building such lists, a
trade-off between coverage of the resource and its
precision is to be found.
One of the most well-known resources is Senti-
WordNet (SWN) (Esuli and Sebastiani, 2006; Bac-
cianella et al, 2010), in which each entry is as-
sociated with the numerical scores Pos(s) and
Neg(s), ranging from 0 to 1. These scores ?
automatically assigned starting from a bunch of
seed terms ? represent the positive and negative
valence (or posterior polarity) of each entry, that
takes the form lemma#pos#sense-number.
Starting from SWN, several prior polarities for
words (SWN-prior), in the form lemma#PoS,
can be computed (e.g. considering only the first-
sense, averaging on all the senses, etc.). These ap-
proaches, detailed in (Guerini et al, 2013), pro-
duce a list of 155k words, where the lower pre-
cision given by the automatic scoring of SWN is
compensated by the high coverage.
Another widely used resource is ANEW
(Bradley and Lang, 1999), providing valence
scores for 1k words, which were manually as-
signed by several annotators. This resource has
a low coverage, but the precision is maximized.
Similarly, the SO-CAL entries (Taboada et al,
2011) were manually tagged by a small num-
ber of annotators with a multi-class label (from
very negative to very positive). These
ratings were further validated through crowd-
sourcing, ending up with a list of roughly 4k
words. More recently, a resource that repli-
cated ANEW annotation approach using crowd-
sourcing, was released (Warriner et al, 2013), pro-
viding sentiment scores for 14k words. Interest-
ingly, this resource annotates the most frequent
words in English, so, even if lexicon coverage is
still far lower than SWN-prior, it grants a high cov-
erage, with human precision, of language use.
Finally, the General Inquirer lexicon (Stone
et al, 1966) provides a binary classifica-
tion (positive/negative) of 4k sentiment-
bearing words, while the resource in (Wilson et al,
2005) expands the General Inquirer to 6k words.
Emotion Lexica. Compared to sentiment
lexica, far less emotion lexica have been pro-
duced, and all have lower coverage. One of the
most used resources is WordNetAffect (Strappa-
rava and Valitutti, 2004) which contains manu-
ally assigned affective labels to WordNet synsets
(ANGER, JOY, FEAR, etc.). It currently provides
900 annotated synsets and 1.6k words in the form
428
AFRAID AMUSED ANGRY ANNOYED DONT CARE HAPPY INSPIRED SAD
doc 10002 0.75 0.00 0.00 0.00 0.00 0.00 0.25 0.00
doc 10003 0.00 0.50 0.00 0.16 0.17 0.17 0.00 0.00
doc 10004 0.52 0.02 0.03 0.02 0.02 0.06 0.02 0.31
doc 10011 0.40 0.00 0.00 0.20 0.00 0.20 0.20 0.00
doc 10028 0.00 0.30 0.08 0.00 0.00 0.23 0.31 0.08
Table 1: An excerpt of the Document-by-Emotion Matrix - M
DE
lemma#PoS#sense, corresponding to roughly
1 thousand lemma#PoS.
AffectNet, part of the SenticNet project (Cam-
bria and Hussain, 2012), contains 10k words (out
of 23k entries) taken from ConceptNet and aligned
withWordNetAffect. This resource extendsWord-
NetAffect labels to concepts like ?have breakfast?.
Fuzzy Affect Lexicon (Subasic and Huettner, 2001)
contains roughly 4k lemma#PoS manually an-
notated by one linguist using 80 emotion labels.
EmoLex (Mohammad and Turney, 2013) contains
almost 10k lemmas annotated with an intensity la-
bel for each emotion using Mechanical Turk. Fi-
nally Affect database is an extension of SentiFul
(Neviarouskaya et al, 2007) and contains 2.5K
words in the form lemma#PoS. The latter is the
only lexicon providing words annotated also with
emotion scores rather than only with labels.
3 Dataset Collection
To build our emotion lexicon we harvested all the
news articles from rappler.com, as of June
3rd 2013: the final dataset consists of 13.5 M
words over 25.3 K documents, with an average
of 530 words per document. For each document,
along with the text we also harvested the informa-
tion displayed by Rappler?s Mood Meter, a small
interface offering the readers the opportunity to
click on the emotion that a given Rappler story
made them feel. The idea behind the Mood Me-
ter is actually ?getting people to crowdsource the
mood for the day?
1
, and returning the percentage
of votes for each emotion label for a given story.
This way, hundreds of thousands votes have been
collected since the launch of the service. In our
novel approach to ?crowdsourcing?, as compared
to other NLP tasks that rely on tools like Ama-
zon?s Mechanical Turk (Snow et al, 2008), the
subjects are aware of the ?implicit annotation task?
but they are not paid. From this data, we built a
document-by-emotion matrixM
DE
, providing the
voting percentages for each document in the eight
1
http://nie.mn/QuD17Z
affective dimensions available in Rappler. An ex-
cerpt is provided in Table 1.
The idea of using documents annotated with
emotions is not new (Strapparava and Mihalcea,
2008; Mishne, 2005; Bellegarda, 2010), but these
works had the limitation of providing a single
emotion label per document, rather than a score for
each emotion, and, moreover, the annotation was
performed by the author of the document alone.
Table 2 reports the average percentage of votes
for each emotion on the whole corpus: HAPPI-
NESS has a far higher percentage of votes (at least
three times). There are several possible explana-
tions, out of the scope of the present paper, for this
bias: (i) it is due to cultural characteristics of the
audience (ii) the bias is in the dataset itself, being
formed mainly by ?positive? news; (iii) it is a psy-
chological phenomenon due to the fact that peo-
ple tend to express more positive moods on social
networks (Quercia et al, 2011; Vittengl and Holt,
1998; De Choudhury et al, 2012). In any case, the
predominance of happy mood has been found in
other datasets, for instance LiveJournal.com
posts (Strapparava and Mihalcea, 2008). In the
following section we will discuss how we handled
this problem.
EMOTION Votes
?
EMOTION Votes
?
AFRAID 0.04 DONT CARE 0.05
AMUSED 0.10 HAPPY 0.32
ANGRY 0.10 INSPIRED 0.10
ANNOYED 0.06 SAD 0.11
Table 2: Average percentages of votes.
4 Emotion Lexicon Creation
As a next step we built a word-by-emotion matrix
starting from M
DE
using an approach based on
compositional semantics. To do so, we first lem-
matized and PoS tagged all the documents (where
PoS can be adj., nouns, verbs, adv.) and kept
only those lemma#PoS present also in Word-
Net, similar to SWN-prior and WordNetAffect re-
sources, to which we want to align. We then com-
puted the term-by-document matrices using raw
429
Word AFRAID AMUSED ANGRY ANNOYED DONT CARE HAPPY INSPIRED SAD
awe#n 0.08 0.12 0.04 0.11 0.07 0.15 0.38 0.05
comical#a 0.02 0.51 0.04 0.05 0.12 0.17 0.03 0.06
crime#n 0.11 0.10 0.23 0.15 0.07 0.09 0.09 0.15
criminal#a 0.12 0.10 0.25 0.14 0.10 0.11 0.07 0.11
dead#a 0.17 0.07 0.17 0.07 0.07 0.05 0.05 0.35
funny#a 0.04 0.29 0.04 0.11 0.16 0.13 0.15 0.08
future#n 0.09 0.12 0.09 0.12 0.13 0.13 0.21 0.10
game#n 0.06 0.15 0.06 0.08 0.15 0.23 0.15 0.12
kill#v 0.23 0.06 0.21 0.07 0.05 0.06 0.05 0.27
rapist#n 0.02 0.07 0.46 0.07 0.08 0.16 0.03 0.12
sad#a 0.06 0.12 0.09 0.14 0.13 0.07 0.15 0.24
warning#n 0.44 0.06 0.09 0.09 0.06 0.06 0.04 0.16
Table 3: An excerpt of the Word-by-Emotion Matrix (M
WE
) using normalized frequencies (nf ). Emo-
tions weighting more than 20% in a word are highlighted for readability purposes.
frequencies, normalized frequencies, and tf-idf
(M
WD,f
, M
WD,nf
and M
WD,tfidf
respectively),
so to test which of the three weights is better. Af-
ter that, we applied matrix multiplication between
the document-by-emotion and word-by-document
matrices (M
DE
? M
WD
) to obtain a (raw) word-
by-emotion matrix M
WE
. This method allows us
to ?merge? words with emotions by summing the
products of the weight of a word with the weight
of the emotions in each document.
Finally, we transformed M
WE
by first apply-
ing normalization column-wise (so to eliminate
the over representation for happiness as discussed
in Section 3) and then scaling the data row-wise so
to sum up to one. An excerpt of the final Matrix
M
WE
is presented in Table 3, and it can be in-
terpreted as a list of words with scores that repre-
sent how much weight a given word has in the af-
fective dimensions we consider. So, for example,
awe#n has a predominant weight in INSPIRED
(0.38), comical#a has a predominant weight in
AMUSED (0.51), while kill#v has a predomi-
nant weight in AFRAID, ANGRY and SAD (0.23,
0.21 and 0.27 respectively). This matrix, that we
call DepecheMood
2
, represents our emotion lex-
icon, it contains 37k entries and is freely available
for research purposes at http://git.io/MqyoIg.
5 Experiments
To evaluate the performance we can obtain with
our lexicon, we use the public dataset provided for
the SemEval 2007 task on ?Affective Text? (Strap-
parava and Mihalcea, 2007). The task was focused
on emotion recognition in one thousand news
headlines, both in regression and classification
settings. Headlines typically consist of a few
2
In French, ?depeche? means dispatch/news.
words and are often written with the intention to
?provoke? emotions so to attract the readers? atten-
tion. An example of headline from the dataset is
the following: ?Iraq car bombings kill 22 People,
wound more than 60?. For the regression task
the values provided are: <anger (0.32),
disgust (0.27), fear (0.84), joy
(0.0), sadness (0.95), surprise
(0.20)> while for the classification task the
labels provided are {FEAR, SADNESS}.
This dataset is of interest to us since the ?com-
positional? problem is less prominent given the
simplified syntax of news headlines, containing,
for example, fewer adverbs (like negations or in-
tensifiers) than normal sentences (Turchi et al,
2012). Furthermore, this is to our knowledge the
only dataset available providing numerical scores
for emotions. Finally, this dataset was meant for
unsupervised approaches (just a small trial sample
was provided), so to avoid simple text categoriza-
tion approaches.
As the affective dimensions present in the test
set ? based on the six basic emotions model (Ek-
man and Friesen, 1971) ? do not exactly match
with the ones provided by Rappler?s Mood Meter,
we first define a mapping between the two when
possible, see Table 4. Then, we proceed to trans-
form the test headlines to the lemma#PoS format.
SemEval Rappler SemEval Rappler
FEAR AFRAID SURPRISE INSPIRED
ANGER ANGRY - ANNOYED
JOY HAPPY - AMUSED
SADNESS SAD - DON?T CARE
Table 4: Mapping of Rappler labels on Se-
meval2007. In bold, cases of suboptimal mapping.
Only one test headline contained exclusively
words not present in DepecheMood, further indi-
430
cating the high-coverage nature of our resource. In
Table 5 we report the coverage of some Sentiment
and Emotion Lexica of different sizes on the same
dataset. Similar to Warriner et al (2013), we ob-
serve that even if the number of entries of our lex-
icon is far lower than SWN-prior approaches, the
fact that we extracted and annotated words from
documents grants a high coverage of language use.
Sentiment
Lexica
ANEW 1k entries 0.10
Warriner et. al 13k entries 0.51
SWN-prior 155k entries 0.67
Emotion
Lexica
WNAffect 1k entries 0.12
DepecheMood 37k entries 0.64
Table 5: Statistics on words coverage per headline.
Since our primary goal is to assess the quality of
DepecheMood we first focus on the regression
task. We do so by using a very na??ve approach,
similar to ?WordNetAffect presence? discussed in
(Strapparava and Mihalcea, 2008): for each head-
line, we simply compute a value, for any affective
dimension, by averaging the corresponding affec-
tive scores ?obtained from DepecheMood- of all
lemma#PoS present in the headline.
In Table 6 we report the results obtained using
the three versions of our resource (Pearson corre-
lation), along with the best performance on each
emotion of other systems
3
(best
se
); the last col-
umn contains the upper bound of inter-annotator
agreement. For all the 5 emotions we improve
over the best performing systems (DISGUST has
no alignment with our labels and was discarded).
Interestingly, even using a sub-optimal align-
ment for SURPRISE we still manage to outper-
form other systems. Considering the na??ve ap-
proach we used, we can reasonably conclude that
the quality and coverage of our resource are the
reason of such results, and that adopting more
complex approaches (i.e. compositionality) can
possibly further improve performances in text-
based emotion recognition.
As a final test, we evaluate our resource in the
classification task. The na??ve approach used in
this case consists in mapping the average of the
scores of all words in the headline to a binary de-
cision with fixed threshold at 0.5 for each emotion
(after min-max normalization on all test headlines
3
Systems participating in the ?Affective Text? task plus the
approaches in (Strapparava and Mihalcea, 2008). Other su-
pervised approaches in the classification task (Mohammad,
2012; Bellegarda, 2010; Chaffar and Inkpen, 2011), report-
ing only overall performances, are not considered.
DepecheMood best
se
upper
f nf tfidf
FEAR 0.56 0.54 0.53 0.45 0.64
ANGER 0.36 0.38 0.36 0.32 0.50
SURPRISE* 0.25 0.21 0.24 0.16 0.36
JOY 0.39 0.40 0.39 0.26 0.60
SADNESS 0.48 0.47 0.46 0.41 0.68
Table 6: Regression results ? Pearson?s correlation
scores). In Table 7 we report the results (F1 mea-
sure) of our approach along with the best perfor-
mance of other systems on each emotion (best
se
),
as in the previous case. For 3 emotions out of
5 we improve over the best performing systems,
for one emotion we obtain the same results, and
for one emotion we do not outperform other sys-
tems. In this case the difference in performances
among the various ways of representing the word-
by-document matrix is more prominent: normal-
ized frequencies (nf ) provide the best results.
DepecheMood best
se
f nf tfidf
FEAR 0.25 0.32 0.31 0.23
ANGER 0.00 0.00 0.00 0.17
SURPRISE* 0.13 0.16 0.09 0.15
JOY 0.22 0.30 0.32 0.32
SADNESS 0.36 0.40 0.38 0.30
Table 7: Classification results ? F1 measures
6 Conclusions
We presented DepecheMood, an emotion lexi-
con built in a novel and totally automated way
by harvesting crowd-sourced affective annota-
tion from a social news network. Our experi-
mental results indicate high-coverage and high-
precision of the lexicon, showing significant im-
provements over state-of-the-art unsupervised ap-
proaches even when using the resource with very
na??ve classification and regression strategies. We
believe that the wealth of information provided by
social media can be harnessed to build models and
resources for emotion recognition from text, going
a step beyond sentiment analysis. Our future work
will include testing Singular Value Decomposi-
tion on the word-by-document matrices, allowing
to propagate emotions values for a document to
similar words non present in the document itself,
and the study of perceived mood effects on viral-
ity indices and readers engagement by exploiting
tweets, likes, reshares and comments.
This work has been partially supported by the Trento
RISE PerTe project.
431
References
S. Baccianella, A. Esuli, and F. Sebastiani. 2010. Sen-
tiWordNet 3.0: An enhanced lexical resource for
sentiment analysis and opinion mining. In Proceed-
ings of the Conference on International Language
Resources and Evaluation (LREC), pages 2200?
2204, Valletta, Malta.
J. R. Bellegarda. 2010. Emotion analysis using latent
affective folding and embedding. In Proceedings of
the NAACL HLT 2010 workshop on computational
approaches to analysis and generation of emotion in
text, pages 1?9. Association for Computational Lin-
guistics.
M. Bradley and P. Lang. 1999. Affective norms for
english words (ANEW): Instruction manual and af-
fective ratings. Technical Report C-1, University of
Florida.
E. Cambria and A. Hussain. 2012. Sentic computing.
Springer.
S. Chaffar and D. Inkpen. 2011. Using a hetero-
geneous dataset for emotion analysis in text. In
Advances in Artificial Intelligence, pages 62?67.
Springer.
M. De Choudhury, S. Counts, and M. Gamon. 2012.
Not all moods are created equal! exploring human
emotional states in social media. In Proceedings of
the International Conference on Weblogs and Social
Media (ICWSM).
P. Ekman and W. V. Friesen. 1971. Constants across
cultures in the face and emotion. Journal of Person-
ality and Social Psychology, 17:124?129.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion min-
ing. In Proceedings of the Conference on Interna-
tional Language Resources and Evaluation (LREC),
pages 417?422, Genova, IT.
M. Guerini, O. Stock, and C. Strapparava. 2008.
Valentino: A tool for valence shifting of natural lan-
guage texts. In Proceedings of the Conference on
International Language Resources and Evaluation
(LREC), Marrakech, Morocco.
M. Guerini, L. Gatti, and M. Turchi. 2013. Senti-
ment analysis: How to derive prior polarities from
sentiwordnet. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1259?1269.
D. Z. Inkpen, O. Feiguina, and G. Hirst. 2006. Gener-
ating more-positive and more-negative text. InCom-
puting Attitude and Affect in Text: Theory and Appli-
cations, pages 187?198. Springer.
B. Liu and L. Zhang. 2012. A survey of opinion min-
ing and sentiment analysis. Mining Text Data, pages
415?463.
G. Mishne. 2005. Experiments with mood classifica-
tion in blog posts. In Proceedings of ACM SIGIR
2005 Workshop on Stylistic Analysis of Text for In-
formation Access, volume 19.
S. M. Mohammad and P. D. Turney. 2013. Crowd-
sourcing a word?emotion association lexicon. Com-
putational Intelligence, 29(3):436?465.
S. M. Mohammad. 2012. # Emotional tweets. In Pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics (*Sem), pages 246?
255. Association for Computational Linguistics.
A. Neviarouskaya, H. Prendinger, and M. Ishizuka.
2007. Textual affect sensing for sociable and
expressive online communication. In A. Paiva,
R. Prada, and R. Picard, editors, Affective Com-
puting and Intelligent Interaction, volume 4738 of
Lecture Notes in Computer Science, pages 218?229.
Springer Berlin Heidelberg.
A. Neviarouskaya, H. Prendinger, and M. Ishizuka.
2009. Compositionality principle in recognition of
fine-grained emotions from text. In Proceedings of
the International Conference on Weblogs and Social
Media (ICWSM).
A. Neviarouskaya, H. Prendinger, and M. Ishizuka.
2011. Affect analysis model: novel rule-based ap-
proach to affect sensing from text. Natural Lan-
guage Engineering, 17(1):95.
G. Ozbal and C. Strapparava. 2012. A computational
approach to the automation of creative naming. Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
G. Ozbal, C. Strapparava, and M. Guerini. 2012.
Brand pitt: A corpus to explore the art of naming.
In Proceedings of the Conference on International
Language Resources and Evaluation (LREC).
G. Paltoglou, M. Thelwall, and K. Buckley. 2010. On-
line textual communications annotated with grades
of emotion strength. In Proceedings of the 3rd In-
ternational Workshop of Emotion: Corpora for re-
search on Emotion and Affect, pages 25?31.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1?135.
I. Piller. 2003. 10. advertising as a site of lan-
guage contact. Annual Review of Applied Linguis-
tics, 23:170?183.
D. Quercia, J. Ellis, L. Capra, and J. Crowcroft. 2011.
In the mood for being influential on twitter. Pro-
ceedings of IEEE SocialCom?11.
R. Snow, B. O?Connor, D. Jurafsky, and A. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-
expert annotations for natural language tasks. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 254?263.
432
R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D.
Manning, A. Y. Ng, and C. Potts. 2013. Recur-
sive deep models for semantic compositionality over
a sentiment treebank. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1631?1642.
P. Stone, D. Dunphy, and M. Smith. 1966. The Gen-
eral Inquirer: A Computer Approach to Content
Analysis. MIT press.
C. Strapparava and R. Mihalcea. 2007. Semeval-
2007 task 14: Affective text. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 70?74. Association for Computational
Linguistics.
C. Strapparava and R. Mihalcea. 2008. Learning to
identify emotions in text. In Proceedings of the
2008 ACM symposium on Applied computing, pages
1556?1560. ACM.
C. Strapparava and A. Valitutti. 2004. WordNet-
Affect: an affective extension of WordNet. In Pro-
ceedings of the Conference on International Lan-
guage Resources and Evaluation (LREC), pages
1083 ? 1086, Lisbon, May.
P. Subasic and A. Huettner. 2001. Affect analysis of
text using fuzzy semantic typing. Fuzzy Systems,
IEEE Transactions on, 9(4):483?496.
M. Taboada, J. Brooke, M. Tofiloski, K. Voll, and
M. Stede. 2011. Lexicon-based methods for
sentiment analysis. Computational linguistics,
37(2):267?307.
M. Turchi, M. Atkinson, A. Wilcox, B. Crawley,
S. Bucci, R. Steinberger, and E. Van der Goot. 2012.
Onts: optima news translation system. In Proceed-
ings of the Demonstrations at the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 25?30. Association for
Computational Linguistics.
J. R. Vittengl and C. S. Holt. 1998. A time-series diary
study of mood and social interaction. Motivation
and Emotion, 22(3):255?275.
S. Wang and C. Manning. 2012. Baselines and bi-
grams: Simple, good sentiment and topic classifica-
tion. Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (ACL).
A. B. Warriner, V. Kuperman, and M. Brysbaert.
2013. Norms of valence, arousal, and dominance for
13,915 english lemmas. Behavior research methods,
45(4):1191?1207.
S. Whitehead and L. Cavedon. 2010. Generating
shifting sentiment for a conversational agent. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 89?97, Los Angeles,
CA, June. Association for Computational Linguis-
tics.
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad
are you? finding strong and weak opinion clauses.
In Proceedings of AAAI, pages 761?769.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 347?354.
433
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 466?470, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
FBK: Sentiment Analysis in Twitter with Tweetsted
Md. Faisal Mahbub Chowdhury
FBK and University of Trento, Italy
fmchowdhury@gmail.com
Marco Guerini
Trento RISE, Italy
marco.guerini@trentorise.eu
Sara Tonelli
FBK, Trento, Italy
satonelli@fbk.eu
Alberto Lavelli
FBK, Trento, Italy
lavelli@fbk.eu
Abstract
This paper presents the Tweetsted system im-
plemented for the SemEval 2013 task on Sen-
timent Analysis in Twitter. In particular, we
participated in Task B on Message Polar-
ity Classification in the Constrained setting.
The approach is based on the exploitation of
various resources such as SentiWordNet and
LIWC. Official results show that our approach
yields a F-score of 0.5976 for Twitter mes-
sages (11th out of 35) and a F-score of 0.5487
for SMS messages (8th out of 28 participants).
1 Introduction
Microblogging is currently a very popular commu-
nication tool where millions of users share opinions
on different aspects of life. For this reason it is a
valuable source of data for opinion mining and sen-
timent analysis.
Working with such type of texts presents chal-
lenges for NLP beyond those typically encountered
when dealing with more traditional texts, such as
newswire data. Tweets are short, the language used
is very informal, with creative spelling and punctua-
tion, misspellings, slang, new words, URLs, genre-
specific terminology and abbreviations, and #hash-
tags. These characteristics need to be handled with
specific approaches.
This paper presents the approach adopted for the
SemEval 2013 task on Sentiment Analysis in Twit-
ter, in particular Task B on Message Polarity Clas-
sification in the Constrained setting (i.e., using the
provided training data only).
The goal of Task B on Message Polarity Classi-
fication is the following: given a message, decide
whether it expresses a positive, negative, or neutral
sentiment. For messages conveying both a positive
and a negative sentiment, whichever is the stronger
sentiment should be chosen.
Two modalities are possible: (1) Constrained (us-
ing the provided training data only; other resources,
such as lexica, are allowed; however, it is not al-
lowed to use additional tweets/SMS messages or ad-
ditional sentences with sentiment annotations); and
(2) Unconstrained (using additional data for train-
ing, e.g., additional tweets/SMS messages or addi-
tional sentences annotated for sentiment). We par-
ticipated in the Constrained modality.
We adopted a supervised machine learning (ML)
approach based on various contextual and seman-
tic features. In particular, we exploited resources
such as SentiWordNet (Esuli and Sebastiani, 2006),
LIWC (Pennebaker and Francis, 2001), and the lex-
icons described in Mohammad et al (2009).
Critical features include: whether the mes-
sage contains intensifiers, adjectives, interjections,
presence of positive or negative emoticons, pos-
sible message polarity based on SentiWordNet
scores (Esuli and Sebastiani, 2006; Gatti and
Guerini, 2012), scores based on LIWC cate-
gories (Pennebaker and Francis, 2001), negated
words, etc.
2 System Description
Our supervised ML-based approach relies on Sup-
port Vector Machines (SVMs). The SVM imple-
mentation used in the system is LIBSVM (Chang
466
and Lin, 2001) for training SVM models and test-
ing. Moreover, in the preprocessing phase we used
TweetNLP (Owoputi et al, 2013), a POS tagger ex-
plicitly tailored for working on tweets.
We adopted a 2 stage approach: (1) during stage
1, we performed a binary classification of messages
according to the classes neutral vs subjective; (2)
in stage 2, we performed a binary classification of
subjective messages according to the classes positive
vs negative. We performed various experiments on
the training and development sets exploring the use
of different features (see Section 2.1) to find the best
configurations for the official submission.
2.1 Feature list
We implement several features divided into three
groups: contextual features, semantic features from
context and semantic features from external re-
sources. The complete list is reported in Table 1.
Contextual features are features computed by
considering only the tokens in the tweets/SMS and
the associated part of speech.
Semantic Features from Context are features
based on words polarity. Emoticons were recog-
nized through a list of emoticons extracted from
Wikipedia1 and then manually labeled as positive or
negative. Negated words (feature n. 18) are any to-
ken occurring between n?t, not, no and a comma, ex-
cluding those tagged as function words. Feature n.
19 captures tokens (or sequences of tokens) labeled
with a positive or negative polarity in the resource
described in Mohammad et al (2009). The intensi-
fiers considered for Feature n. 20 have been identi-
fied by implementing a simple algorithm that detects
tokens containing anomalously repeated characters
(e.g. happyyyyy). Feature n. 21 was computed by
training the system on the training data and predict-
ing labels for the test data, and then using these la-
bels as new features to train the system again.
Semantic Features from external resources in-
clude word classes from the Linguistic Inquiry
and Word Count (LIWC), a tool that calculates
the degree to which people use different cate-
gories of words related to psycholinguistic pro-
cesses (Pennebaker and Francis, 2001). LIWC in-
1http://en.wikipedia.org/wiki/List_of_
emoticons
cludes about 2,200 words and stems grouped into 70
broad categories relevant to psychological processes
(e.g., EMOTION, COGNITION). Sample words are
shown in Table 2.
For each non-zero valued LIWC category of a cor-
responding tweet/SMS, we added a feature for that
category and used the category score as the value
of that feature. We call this LWIC string feature.
Alternatively, we also added a separate feature for
each non-zero valued LIWC category and set 1 as
the value of that feature. This feature is called LWIC
boolean.
We also used words prior polarity - i.e. if a word
out of context evokes something positive or nega-
tive. For this, we relied on SentiWordNet, a broad-
coverage resource that provides polarities for (al-
most) every word. Since words can have multi-
ple senses, we compute the prior polarity of a word
starting from the polarity of each sense and returning
its polarity strength as an index between -1 and 1.
We tested 14 formulae that combine posterior polar-
ities in different ways to obtain a word prior polarity,
as reported in (Gatti and Guerini, 2012).
For the SWNscoresMaximum feature, we select
the prior polarity of the word in a tweet/SMS hav-
ing the maximum absolute score among all words
(of that tweet/SMS). For SWNscoresPolarityCount,
we select the polarity (positive, negative or neutral)
that is assigned to the majority of the words. As
for SWNscoresSum, it corresponds to the sum of
the prior polarities associated with all words in the
tweet/SMS.
3 Experimental Setup
In order to select the best performing feature set,
we carried out several 5-fold cross validation ex-
periments on the training data. We report in Table
3 the best performing feature set. In particular, we
adopted a 2 stage approach:
1. during the first stage we performed a binary
classification of messages according to the
classes neutral vs subjective;
2. in the second stage, we performed a binary
classification of subjective messages according
to the classes positive vs negative.
We opted for a two stage binary classification ap-
proach, since we observed that it produces slightly
467
Contextual Features
1. noOfAdjectives num
2. adjective list string
3. interjection list string
4. firstInterj string
5. lastInterj string
6. bigramList string
7. beginsWithRT boolean
8. hasRTinMiddle boolean
9. endsWithLink boolean
10. endsWithHashtag boolean
11. hasQuestion boolean
Semantic Features from Context
12. noOfPositiveEmoticons num
13. noOfNegativeEmoticons num
14. beginsWithPosEmoticon boolean
15. beginsWithNegEmoticon boolean
16. endsWithPosEmoticon boolean
17. endsWithNegEmoticon boolean
18. negatedWords string
19. indexOfChunksWithPolarity string
20. containsIntensifier boolean
21. labelPredictedBySystem pos./neg./neut.
Semantic Features from External Resources
22. LIWC string string
23. LIWC boolean string
24. SWNscoresMaximum pos./neg./neut.
25. SWNscoresPolarityCount pos./neg./neut.
26. SWNscoresSum pos./neg./neut.
Table 1: Complete feature list.
LABEL Sample words
CERTAIN all, very, fact*, exact*, certain*, completely
DISCREP but, if, expect*, should
TENTAT or, some, may, possib*, probab*
SENSES observ*, discuss*, shows, appears
SELF we, our, I, us
SOCIAL discuss*, interact*, suggest*, argu*
OPTIM best, easy*, enthus*, hope, pride
ANGER hate, kill, annoyed
INHIB block, constrain, stop
Table 2: Word categories along with sample words
better results than a single stage multi-class ap-
proach (i.e. neutral vs positive vs negative).2 Dif-
ferent combinations of classifiers were explored ob-
taining comparable results. Here we will report only
2The average F-scores (pos and neg) for two stage and single
stage approaches obtained using the official scorer, by training
on the training data and testing on the development data, are
0.5682 and 0.5611 respectively.
the best results.
STAGE 1. The best result for stage (1), neutral vs
subjective, obtained with 5-fold cross validation on
training set only, accounts for an accuracy of 69.6%.
Instead, the best result for stage (1), obtained with
training on training data and testing on development
data, accounts for an accuracy of 72.67%.
The list of best features is reported in Table 3.
Feature selection was performed by starting from a
small set of basic features, and then by adding the
remaining features incrementally.
Contextual Features
2. adjective list string
3. interjection list string
5. lastInterj string
Semantic Features from Context
12. noOfPositiveEmoticons num
13. noOfNegativeEmoticons num
18. negatedWords string
19. indexOfChunksWithPolarity string
20. containsIntensifier boolean
Semantic Features from external resources
23. LIWC boolean string
24. SWNscoresMaximum posi./neg./neut.
Table 3: Best performing feature set.
STAGE 2. In stage (2), positive vs negative, we
started from the best feature set obtained from stage
(1) and added the remaining features one by one in-
crementally. In this case, we kept SWNscoresMaxi-
mum without testing again other formulae; in partic-
ular, to compute words prior polarity, we also kept
the first sense approach, that assigns to every word
the SWN score of its most frequent sense and proved
to be the most discriminative in the first stage neutral
vs. subjective. We found that none of the feature sets
produced better results than that obtained using the
best feature set selected from stage (1). So, the best
feature set for stage (2) is unchanged. We trained
the system on the training data and tested it on the
development data, achieving an accuracy of 80.67%.
4 Evaluation
The SemEval task organizers (Wilson et al, 2013)
provided two test sets on which the systems were
to be evaluated: one included Twitter messages, i.e.
the same type of texts included in the training set,
468
while the other comprised SMS messages, i.e. texts
having more or less the same length as the Twitter
data but (supposedly) a different style. We applied
the same model, trained both on the training and the
development set, on the two types of data, without
any specific adaptation.
The Twitter test set was composed of 3,813
tweets. Official results show that our approach
yields an F-score of 0.5976 for Twitter messages
(11th out of 35), while the best performing system
obtained an F-score of 0.6902. The confusion ma-
trix is reported in Table 4, while the score details
in Table 5. The latter table shows that our system
achieves the lowest results on negative tweets, both
in terms of precision and of recall.
gs/pred positive negative neutral
positive 946 101 525
negative 90 274 237
neutral 210 70 1360
Table 4: Confusion matrix for Twitter task
class prec recall F-score
positive 0.7592 0.6018 0.6714
negative 0.6157 0.4559 0.5239
neutral 0.6409 0.8293 0.7230
average(pos and neg) 0.5976
Table 5: Detailed results for Twitter task
The SMS test set for the competition was com-
posed of 2,094 SMS. Official results provided by the
task organizers show that our approach yields an F-
score of 0.5487 for SMS messages (8th out of 28
participants), while the best performing system ob-
tained an F-score of 0.6846. The confusion matrix
is reported in Table 6, while the score details in Ta-
ble 7. Also in this case the recognition of negative
messages achieves by far the poorest performance.
A comparison of the results on the two test sets
shows that, as expected, our system performs bet-
ter on tweets than on SMS. However, precision
achieved by the system on neutral SMS is 0.12
points better on text messages than on tweets.
Interestingly, it appears from the results in Ta-
bles 5 and 7 (and from the distribution of the classes
in the data sets) that there may be a correlation be-
tween the number of tweets/SMS for a particular
class and the performance obtained for such class.
We plan to further investigate this issue.
gs/pred positive negative neutral
positive 320 44 128
negative 66 171 157
neutral 208 64 936
Table 6: Confusion matrix for SMS task
class prec recall F-score
positive 0.5387 0.6504 0.5893
negative 0.6129 0.4340 0.5082
neutral 0.7666 0.7748 0.7707
average(pos and neg) 0.5487
Table 7: Detailed results for SMS task
5 Conclusions
In this paper, we presented Tweetsted, the system de-
veloped by FBK for the SemEval 2013 task on Sen-
timent Analysis. We trained a classifier performing
a two-step binary classification, i.e. first neutral vs.
subjective data, and then positive vs. negative ones.
We implemented a set of features including contex-
tual and semantic ones. We also integrated in our
feature representation external knowledge from Sen-
tiWordNet, LIWC and the resource by Mohammad
et al (2009). On both test sets (i.e., Twitter mes-
sages and SMS) of the constrained modality of the
challenge, we achieved a good performance, being
among the top 30% of the competing systems. In
the near future, we plan to perform an error analysis
of the wrongly classified data to investigate possible
classification issues, in particular the lower perfor-
mance on negative tweets and SMS.
Acknowledgments
This work is supported by ?eOnco - Pervasive knowledge
and data management in cancer care? and ?Trento RISE
PerTe? projects.
References
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
469
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion min-
ing. In Proceedings of the 5th International Confer-
ence on Language Resources and Evaluation (LREC
2006), Genoa, Italy.
Lorenzo Gatti and Marco Guerini. 2012. Assessing sen-
timent strength in words prior polarities. In Proceed-
ings of COLING 2012: Posters, pages 361?370, Mum-
bai, India, December. The COLING 2012 Organizing
Committee.
Saif Mohammad, Bonnie Dorr, and Cody Dunne. 2009.
Generating High-Coverage Semantic Orientation Lex-
icons From Overtly Marked Words and a Thesaurus.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In Proceedings of
NAACL 2013, Atlanta, Georgia, June.
J. Pennebaker and M. Francis. 2001. Linguistic inquiry
and word count: LIWC. Erlbaum Publishers.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13, June.
470
