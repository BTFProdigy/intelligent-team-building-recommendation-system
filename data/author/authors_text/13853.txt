An Integrated Architecture for Shallow and Deep Processing
Berthold Crysmann, Anette Frank, Bernd Kiefer, Stefan Mu?ller,
Gu?nter Neumann, Jakub Piskorski, Ulrich Scha?fer, Melanie Siegel, Hans Uszkoreit,
Feiyu Xu, Markus Becker and Hans-Ulrich Krieger
DFKI GmbH
Stuhlsatzenhausweg 3
Saarbru?cken, Germany
whiteboard@dfki.de
Abstract
We present an architecture for the integra-
tion of shallow and deep NLP components
which is aimed at flexible combination
of different language technologies for a
range of practical current and future appli-
cations. In particular, we describe the inte-
gration of a high-level HPSG parsing sys-
tem with different high-performance shal-
low components, ranging from named en-
tity recognition to chunk parsing and shal-
low clause recognition. The NLP com-
ponents enrich a representation of natu-
ral language text with layers of new XML
meta-information using a single shared
data structure, called the text chart. We de-
scribe details of the integration methods,
and show how information extraction and
language checking applications for real-
world German text benefit from a deep
grammatical analysis.
1 Introduction
Over the last ten years or so, the trend in application-
oriented natural language processing (e.g., in the
area of term, information, and answer extraction)
has been to argue that for many purposes, shallow
natural language processing (SNLP) of texts can
provide sufficient information for highly accurate
and useful tasks to be carried out. Since the emer-
gence of shallow techniques and the proof of their
utility, the focus has been to exploit these technolo-
gies to the maximum, often ignoring certain com-
plex issues, e.g. those which are typically well han-
dled by deep NLP systems. Up to now, deep natural
language processing (DNLP) has not played a sig-
nificant role in the area of industrial NLP applica-
tions, since this technology often suffers from insuf-
ficient robustness and throughput, when confronted
with large quantities of unrestricted text.
Current information extractions (IE) systems
therefore do not attempt an exhaustive DNLP analy-
sis of all aspects of a text, but rather try to analyse or
?understand? only those text passages that contain
relevant information, thereby warranting speed and
robustness wrt. unrestricted NL text. What exactly
counts as relevant is explicitly defined by means
of highly detailed domain-specific lexical entries
and/or rules, which perform the required mappings
from NL utterances to corresponding domain knowl-
edge. However, this ?fine-tuning? wrt. a particular
application appears to be the major obstacle when
adapting a given shallow IE system to another do-
main or when dealing with the extraction of com-
plex ?scenario-based? relational structures. In fact,
(Appelt and Israel, 1997) have shown that the cur-
rent IE technology seems to have an upper perfor-
mance level of less than 60% in such cases. It seems
reasonable to assume that if a more accurate analy-
sis of structural linguistic relationships could be pro-
vided (e.g., grammatical functions, referential rela-
tionships), this barrier might be overcome. Actually,
the growing market needs in the wide area of intel-
ligent information management systems seem to re-
quest such a break-through.
In this paper we will argue that the quality of cur-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 441-448.
                         Proceedings of the 40th Annual Meeting of the Association for
rent SNLP-based applications can be improved by
integrating DNLP on demand in a focussed manner,
and we will present a system that combines the fine-
grained anaysis provided by HPSG parsing with a
high-performance SNLP system into a generic and
flexible NLP architecture.
1.1 Integration Scenarios
Owing to the fact that deep and shallow technologies
are complementary in nature, integration is a non-
trivial task: while SNLP shows its strength in the
areas of efficiency and robustness, these aspects are
problematic for DNLP systems. On the other hand,
DNLP can deliver highly precise and fine-grained
linguistic analyses. The challenge for integration is
to combine these two paradigms according to their
virtues.
Probably the most straightforward way to inte-
grate the two is an architecture in which shallow and
deep components run in parallel, using the results of
DNLP, whenever available. While this kind of ap-
proach is certainly feasible for a real-time applica-
tion such as Verbmobil, it is not ideal for processing
large quantities of text: due to the difference in pro-
cessing speed, shallow and deep NLP soon run out
of sync. To compensate, one can imagine two possi-
ble remedies: either to optimize for precision, or for
speed. The drawback of the former strategy is that
the overall speed will equal the speed of the slow-
est component, whereas in case of the latter, DNLP
will almost always time out, such that overall preci-
sion will hardly be distinguishable from a shallow-
only system. What is thus called for is an integrated,
flexible architecture where components can play at
their strengths. Partial analyses from SNLP can be
used to identify relevant candidates for the focussed
use of DNLP, based on task or domain-specific crite-
ria. Furthermore, such an integrated approach opens
up the possibility to address the issue of robustness
by using shallow analyses (e.g., term recognition)
to increase the coverage of the deep parser, thereby
avoiding a duplication of efforts. Likewise, integra-
tion at the phrasal level can be used to guide the
deep parser towards the most likely syntactic anal-
ysis, leading, as it is hoped, to a considerable speed-
up.
shallow
NLP
components
NLP
deep
components internal repr.
layer
multi
chart
annot.
XML
external repr.
generic OOP
component
interface
WHAM
application
specification
input and
result
Figure 1: The WHITEBOARD architecture.
2 Architecture
The WHITEBOARD architecture defines a platform
that integrates the different NLP components by en-
riching an input document through XML annota-
tions. XML is used as a uniform way of represent-
ing and keeping all results of the various processing
components and to support a transparent software
infrastructure for LT-based applications. It is known
that interesting linguistic information ?especially
when considering DNLP? cannot efficiently be
represented within the basic XML markup frame-
work (?typed parentheses structure?), e.g., linguistic
phenomena like coreferences, ambiguous readings,
and discontinuous constituents. The WHITEBOARD
architecture employs a distributed multi-level repre-
sentation of different annotations. Instead of trans-
lating all complex structures into one XML docu-
ment, they are stored in different annotation layers
(possibly non-XML, e.g. feature structures). Hyper-
links and ?span? information together support effi-
cient access between layers. Linguistic information
of common interest (e.g. constituent structure ex-
tracted from HPSG feature structures) is available in
XML format with hyperlinks to full feature struc-
ture representations externally stored in correspond-
ing data files.
Fig. 1 gives an overview of the architecture of
the WHITEBOARD Annotation Machine (WHAM).
Applications feed the WHAM with input texts and
a specification describing the components and con-
figuration options requested. The core WHAM en-
gine has an XML markup storage (external ?offline?
representation), and an internal ?online? multi-level
annotation chart (index-sequential access). Follow-
ing the trichotomy of NLP data representation mod-
els in (Cunningham et al, 1997), the XML markup
contains additive information, while the multi-level
chart contains positional and abstraction-based in-
formation, e.g., feature structures representing NLP
entities in a uniform, linguistically motivated form.
Applications and the integrated components ac-
cess the WHAM results through an object-oriented
programming (OOP) interface which is designed
as general as possible in order to abstract from
component-specific details (but preserving shallow
and deep paradigms). The interfaces of the actu-
ally integrated components form subclasses of the
generic interface. New components can be inte-
grated by implementing this interface and specifying
DTDs and/or transformation rules for the chart.
The OOP interface consists of iterators that walk
through the different annotation levels (e.g., token
spans, sentences), reference and seek operators that
allow to switch to corresponding annotations on a
different level (e.g., give all tokens of the current
sentence, or move to next named entity starting
from a given token position), and accessor meth-
ods that return the linguistic information contained
in the chart. Similarily, general methods support
navigating the type system and feature structures of
the DNLP components. The resulting output of the
WHAM can be accessed via the OOP interface or as
XML markup.
The WHAM interface operations are not only
used to implement NLP component-based applica-
tions, but also for the integration of deep and shallow
processing components itself.
2.1 Components
2.1.1 Shallow NL component
Shallow analysis is performed by SPPC, a rule-
based system which consists of a cascade of
weighted finite?state components responsible for
performing subsequent steps of the linguistic anal-
ysis, including: fine-grained tokenization, lexico-
morphological analysis, part-of-speech filtering,
named entity (NE) recognition, sentence bound-
ary detection, chunk and subclause recognition,
see (Piskorski and Neumann, 2000; Neumann and
Piskorski, 2002) for details. SPPC is capable of pro-
cessing vast amounts of textual data robustly and ef-
ficiently (ca. 30,000 words per second in standard
PC environment). We will briefly describe the SPPC
components which are currently integrated with the
deep components.
Each token identified by a tokenizer as a poten-
tial word form is morphologically analyzed. For
each token, its lexical information (list of valid read-
ings including stem, part-of-speech and inflection
information) is computed using a fullform lexicon
of about 700,000 entries that has been compiled out
from a stem lexicon of about 120,000 lemmas. Af-
ter morphological processing, POS disambiguation
rules are applied which compute a preferred read-
ing for each token, while the deep components can
back off to all readings. NE recognition is based on
simple pattern matching techniques. Proper names
(organizations, persons, locations), temporal expres-
sions and quantities can be recognized with an av-
erage precision of almost 96% and recall of 85%.
Furthermore, a NE?specific reference resolution is
performed through the use of a dynamic lexicon
which stores abbreviated variants of previously rec-
ognized named entities. Finally, the system splits
the text into sentences by applying only few, but
highly accurate contextual rules for filtering implau-
sible punctuation signs. These rules benefit directly
from NE recognition which already performs re-
stricted punctuation disambiguation.
2.1.2 Deep NL component
The HPSG Grammar is based on a large?scale
grammar for German (Mu?ller, 1999), which was
further developed in the VERBMOBIL project for
translation of spoken language (Mu?ller and Kasper,
2000). After VERBMOBIL the grammar was adapted
to the requirements of the LKB/PET system (Copes-
take, 1999), and to written text, i.e., extended with
constructions like free relative clauses that were ir-
relevant in the VERBMOBIL scenario.
The grammar consists of a rich hierarchy of
5,069 lexical and phrasal types. The core grammar
contains 23 rule schemata, 7 special verb move-
ment rules, and 17 domain specific rules. All rule
schemata are unary or binary branching. The lexicon
contains 38,549 stem entries, from which more than
70% were semi-automatically acquired from the an-
notated NEGRA corpus (Brants et al, 1999).
The grammar parses full sentences, but also other
kinds of maximal projections. In cases where no full
analysis of the input can be provided, analyses of
fragments are handed over to subsequent modules.
Such fragments consist of maximal projections or
single words.
The HPSG analysis system currently integrated
in the WHITEBOARD system is PET (Callmeier,
2000). Initially, PET was built to experiment
with different techniques and strategies to process
unification-based grammars. The resulting sys-
tem provides efficient implementations of the best
known techniques for unification and parsing.
As an experimental system, the original design
lacked open interfaces for flexible integration with
external components. For instance, in the beginning
of the WHITEBOARD project the system only ac-
cepted fullform lexica and string input. In collabora-
tion with Ulrich Callmeier the system was extended.
Instead of single word input, input items can now
be complex, overlapping and ambiguous, i.e. essen-
tially word graphs. We added dynamic creation of
atomic type symbols, e.g., to be able to add arbitrary
symbols to feature structures. With these enhance-
ments, it is possible to build flexible interfaces to
external components like morphology, tokenization,
named entity recognition, etc.
3 Integration
Morphology and POS The coupling between the
morphology delivered by SPPC and the input needed
for the German HPSG was easily established. The
morphological classes of German are mapped onto
HPSG types which expand to small feature struc-
tures representing the morphological information in
a compact way. A mapping to the output of SPPC
was automatically created by identifying the corre-
sponding output classes.
Currently, POS tagging is used in two ways. First,
lexicon entries that are marked as preferred by the
shallow component are assigned higher priority than
the rest. Thus, the probability of finding the cor-
rect reading early should increase without excluding
any reading. Second, if for an input item no entry is
found in the HPSG lexicon, we automatically create
a default entry, based on the part?of?speech of the
preferred reading. This increases robustness, while
avoiding increase in ambiguity.
Named Entity Recognition Writing HPSG gram-
mars for the whole range of NE expressions etc. is
a tedious and not very promising task. They typi-
cally vary across text sorts and domains, and would
require modularized subgrammars that can be easily
exchanged without interfering with the general core.
This can only be realized by using a type interface
where a class of named entities is encoded by a gen-
eral HPSG type which expands to a feature structure
used in parsing. We exploit such a type interface for
coupling shallow and deep processing. The classes
of named entities delivered by shallow processing
are mapped to HPSG types. However, some fine-
tuning is required whenever deep and shallow pro-
cessing differ in the amount of input material they
assign to a named entity.
An alternative strategy is used for complex syn-
tactic phrases containing NEs, e.g., PPs describ-
ing time spans etc. It is based on ideas from
Explanation?based Learning (EBL, see (Tadepalli
and Natarajan, 1996)) for natural language analy-
sis, where analysis trees are retrieved on the basis
of the surface string. In our case, the part-of-speech
sequence of NEs recognised by shallow analysis is
used to retrieve pre-built feature structures. These
structures are produced by extracting NEs from a
corpus and processing them directly by the deep
component. If a correct analysis is delivered, the
lexical parts of the analysis, which are specific for
the input item, are deleted. We obtain a sceletal
analysis which is underspecified with respect to the
concrete input items. The part-of-speech sequence
of the original input forms the access key for this
structure. In the application phase, the underspeci-
fied feature structure is retrieved and the empty slots
for the input items are filled on the basis of the con-
crete input.
The advantage of this approach lies in the more
elaborate semantics of the resulting feature struc-
tures for DNLP, while avoiding the necessity of
adding each and every single name to the HPSG lex-
icon. Instead, good coverage and high precision can
be achieved using prototypical entries.
Lexical Semantics When first applying the origi-
nal VERBMOBIL HPSG grammar to business news
articles, the result was that 78.49% of the miss-
ing lexical items were nouns (ignoring NEs). In
the integrated system, unknown nouns and NEs can
be recognized by SPPC, which determines morpho-
syntactic information. It is essential for the deep sys-
tem to associate nouns with their semantic sorts both
for semantics construction, and for providing se-
mantically based selectional restrictions to help con-
straining the search space during deep parsing. Ger-
maNet (Hamp and Feldweg, 1997) is a large lexical
database, where words are associated with POS in-
formation and semantic sorts, which are organized in
a fine-grained hierarchy. The HPSG lexicon, on the
other hand, is comparatively small and has a more
coarse-grained semantic classification.
To provide the missing sort information when re-
covering unknown noun entries via SPPC, a map-
ping from the GermaNet semantic classification to
the HPSG semantic classification (Siegel et al,
2001) is applied which has been automatically ac-
quired. The training material for this learning pro-
cess are those words that are both annotated with se-
mantic sorts in the HPSG lexicon and with synsets
of GermaNet. The learning algorithm computes a
mapping relevance measure for associating seman-
tic concepts in GermaNet with semantic sorts in the
HPSG lexicon. For evaluation, we examined a cor-
pus of 4664 nouns extracted from business news
that were not contained in the HPSG lexicon. 2312
of these were known in GermaNet, where they are
assigned 2811 senses. With the learned mapping,
the GermaNet senses were automatically mapped to
HPSG semantic sorts. The evaluation of the map-
ping accuracy yields promising results: In 76.52%
of the cases the computed sort with the highest rel-
evance probability was correct. In the remaining
20.70% of the cases, the correct sort was among the
first three sorts.
3.1 Integration on Phrasal Level
In the previous paragraphs we described strategies
for integration of shallow and deep processing where
the focus is on improving DNLP in the domain of
lexical and sub-phrasal coverage.
We can conceive of more advanced strategies for
the integration of shallow and deep analysis at the
length cover- complete LP LR 0CB   2CB
age match
  40 100 80.4 93.4 92.9 92.1 98.9
 40 99.8 78.6 92.4 92.2 90.7 98.5
Training: 16,000 NEGRA sentences
Testing: 1,058 NEGRA sentences
Figure 2: Stochastic topological parsing: results
level of phrasal syntax by guiding the deep syntac-
tic parser towards a partial pre-partitioning of com-
plex sentences provided by shallow analysis sys-
tems. This strategy can reduce the search space, and
enhance parsing efficiency of DNLP.
Stochastic Topological Parsing The traditional
syntactic model of topological fields divides basic
clauses into distinct fields: so-called pre-, middle-
and post-fields, delimited by verbal or senten-
tial markers. This topological model of German
clause structure is underspecified or partial as to
non-sentential constituent boundaries, but provides
a linguistically well-motivated, and theory-neutral
macrostructure for complex sentences. Due to its
linguistic underpinning the topological model pro-
vides a pre-partitioning of complex sentences that is
(i) highly compatible with deep syntactic structures
and (ii) maximally effective to increase parsing ef-
ficiency. At the same time (iii) partiality regarding
the constituency of non-sentential material ensures
the important aspects of robustness, coverage, and
processing efficiency.
In (Becker and Frank, 2002) we present a corpus-
driven stochastic topological parser for German,
based on a topological restructuring of the NEGRA
corpus (Brants et al, 1999). For topological tree-
bank conversion we build on methods and results
in (Frank, 2001). The stochastic topological parser
follows the probabilistic model of non-lexicalised
PCFGs (Charniak, 1996). Due to abstraction from
constituency decisions at the sub-sentential level,
and the essentially POS-driven nature of topologi-
cal structure, this rather simple probabilistic model
yields surprisingly high figures of accuracy and cov-
erage (see Fig.2 and (Becker and Frank, 2002) for
more detail), while context-free parsing guarantees
efficient processing.
The next step is to elaborate a (partial) map-
ping of shallow topological and deep syntactic struc-
tures that is maximally effective for preference-gui-
Topological Structure:
CL-V2
VF-TOPIC LK-FIN MF RK-t
NN VVFIN ADV NN PREP NN VVFIN
[ 	 [ 
	 Peter] [ 
 i?t] [ 
 gerne Wu?rstchen mit Kartoffelsalat] [ Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 255?265, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Exploring Vector Space Models to Predict the Compositionality of
German Noun-Noun Compounds
Sabine Schulte im Walde and Stefan Mu?ller and Stephen Roller
Institut fu?r Maschinelle Sprachverarbeitung
Universita?t Stuttgart
Pfaffenwaldring 5b, 70569 Stuttgart, Germany
{schulte,muellesn,roller}@ims.uni-stuttgart.de
Abstract
This paper explores two hypotheses regarding
vector space models that predict the compo-
sitionality of German noun-noun compounds:
(1) Against our intuition, we demonstrate that
window-based rather than syntax-based distri-
butional features perform better predictions,
and that not adjectives or verbs but nouns rep-
resent the most salient part-of-speech. Our
overall best result is state-of-the-art, reach-
ing Spearman?s ? = 0.65 with a word-
space model of nominal features from a 20-
word window of a 1.5 billion word web cor-
pus. (2) While there are no significant dif-
ferences in predicting compound?modifier vs.
compound?head ratings on compositionality,
we show that the modifier (rather than the
head) properties predominantly influence the
degree of compositionality of the compound.
1 Introduction
Vector space models and distributional information
have been a steadily increasing, integral part of lex-
ical semantic research over the past 20 years. On
the one hand, vector space models (see Turney and
Pantel (2010) and Erk (2012) for two recent sur-
veys) have been exploited in psycholinguistic (Lund
and Burgess, 1996) and computational linguistic re-
search (Schu?tze, 1992) to explore the notion of ?sim-
ilarity? between a set of target objects within a ge-
ometric setting. On the other hand, the distribu-
tional hypothesis (Firth, 1957; Harris, 1968) has
been exploited to determine co-occurrence features
for vector space models that best describe the words,
phrases, sentences, etc. of interest.
While the emergence of vector space models is in-
creasingly pervasive within data-intensive lexical se-
mantics, and even though useful features have been
identified in general terms:1 when it comes to a spe-
cific semantic phenomenon, we need to explore the
relevant distributional features in order to investigate
the respective phenomenon. Our research is inter-
ested in the meaning of German compounds. More
specifically, we aim to predict the degrees of compo-
sitionality of German noun-noun compounds (e.g.,
Feuerwerk ?fire works?) with regard to the mean-
ings of their constituents (e.g., Feuer ?fire? and Werk
?opus?). This prediction uses vector space models,
and our goal is to identify salient features that de-
termine the degree of compositionality of the com-
pounds by relying on the distributional similarities
between the compounds and their constituents.
In this vein, we systematically explore window-
based and syntax-based contextual clues. Since the
targets in our vector space models are all nouns
(i.e., the compound nouns, the modifier nouns, and
the head nouns), our hypothesis is that adjectives
and verbs are expected to provide salient distri-
butional properties, as adjective/verb meaning and
noun meaning are in a strong interdependent rela-
tionship. Even more, we expect adjectives and verbs
that are syntactically bound to the nouns under con-
sideration (syntax-based, i.e., attributive adjectives
and subcategorising verbs) to outperform those that
?just? appear in the window contexts of the nouns
(window-based). In order to investigate this first
1See Agirre et al (2009) and Bullinaria and Levy (2007;
2012), among others, for systematic comparisons of co-
occurrence features on various semantic relatedness tasks.
255
hypothesis, we compare window-based and syntax-
based distributional features across parts-of-speech.
Concerning a more specific aspect of compound
meaning, we are interested in the contributions of
the modifier noun versus head noun properties with
regard to the meaning of the noun-noun compounds.
While there has been prior psycholinguistic research
on the constituent contributions (e.g., Gagne? and
Spalding (2009; 2011)), computational linguistics
has not yet paid much attention to this issue, as
far as we know. Our hypothesis is that the dis-
tributional properties of the head constituents are
more salient than the distributional properties of
the modifier constituents in predicting the degree
of compositionality of the compounds. In order to
assess this second hypothesis, we compare the vec-
tor space similarities between the compounds and
their modifier constituents with those of the com-
pounds and their head constituents, with regard to
the overall most successful features.
The paper is organised as follows. Section 2 in-
troduces the compound data that is relevant for this
paper, i.e., the noun-noun compounds and the com-
positionality ratings. Section 3 performs and dis-
cusses the vector space experiments to explore our
hypotheses, and Section 4 describes related work.
2 Data
2.1 German Noun-Noun Compounds
Compounds are combinations of two or more sim-
plex words. Traditionally, a number of criteria (such
as compounds being syntactically inseparable, and
that compounds have a specific stress pattern) have
been proposed, in order to establish a border be-
tween compounds and non-compounds. However,
Lieber and Stekauer (2009a) demonstrated that none
of these tests are universally reliable to distinguish
compounds from other types of derived words.
Compounds have thus been a recurrent focus
of attention within theoretical, cognitive, and in
the last decade also within computational linguis-
tics. Recent evidence of this strong interest are the
Handbook of Compounding (Lieber and Stekauer,
2009b) on theoretical perspectives, and a series of
workshops2 and special journal issues with respect
to multi-word expressions (including various types
2www.multiword.sourceforge.net
of compounds) and the computational perspective
(Journal of Computer Speech and Language, 2005;
Language Resources and Evaluation, 2010; ACM
Transactions on Speech and Language Processing,
to appear).
Our focus of interest is on German noun-noun
compounds (see Fleischer and Barz (2012) for a de-
tailed overview and Klos (2011) for a recent de-
tailed exploration), such as Ahornblatt ?maple leaf?,
Feuerwerk ?fireworks?, and Obstkuchen ?fruit cake?
where both the grammatical head (in German, this
is the rightmost constituent) and the modifier are
nouns. More specifically, we are interested in the
degrees of compositionality of German noun-noun
compounds, i.e., the semantic relatedness between
the meaning of a compound (e.g., Feuerwerk) and
the meanings of its constituents (e.g., Feuer ?fire?
and Werk ?opus?).
Our work is based on a selection of noun com-
pounds by von der Heide and Borgwaldt (2009),
who created a set of 450 concrete, depictable Ger-
man noun compounds according to four compo-
sitionality classes: compounds that are transpar-
ent with regard to both constituents (e.g., Ahorn-
blatt ?maple leaf?); compounds that are opaque
with regard to both constituents (e.g., Lo?wenzahn
?lion+tooth ? dandelion?); compounds that are
transparent with regard to the modifier but opaque
with regard to the head (e.g., Feuerzeug ?fire+stuff
? lighter?); and compounds that are opaque with
regard to the modifier but transparent with regard to
the head (e.g., Fliegenpilz ?fly+mushroom ? toad-
stool?).
From the compound set by von der Heide and
Borgwaldt, we disregarded noun compounds with
more than two constituents (in some cases, the mod-
ifier or the head was complex itself) as well as com-
pounds where the modifiers were not nouns. Our
final set comprises a subset of their compounds in-
cluding 244 two-part noun-noun compounds.
2.2 Compositionality Ratings
von der Heide and Borgwaldt (2009) collected hu-
man ratings on compositionality for all their 450
compounds. The compounds were distributed over
5 lists, and 270 participants judged the degree of
compositionality of the compounds with respect to
their first as well as their second constituent, on
256
Compounds Mean Ratings and Standard Deviations
whole literal meanings of constituents whole modifier head mean range
Ahornblatt ?maple leaf? maple leaf 6.03 ? 1.49 5.64 ? 1.63 5.71 ? 1.70
(1) high/high
Postbote ?post man? mail messenger 6.33 ? 0.96 5.87 ? 1.55 5.10 ? 1.99
Seezunge ?sole? sea tongue 1.85 ? 1.28 3.57 ? 2.42 3.27 ? 2.32
(2) mid/mid
Windlicht ?storm lamp? wind light 3.52 ? 2.08 3.07 ? 2.12 4.27 ? 2.36
Lo?wenzahn ?dandelion? lion tooth 1.66 ? 1.54 2.10 ? 1.84 2.23 ? 1.92
(3) low/low
Maulwurf ?mole? mouth throw 1.58 ? 1.43 2.21 ? 1.68 2.76 ? 2.10
Fliegenpilz ?toadstool? fly/bow tie mushroom 2.00 ? 1.20 1.93 ? 1.28 6.55 ? 0.63
(4) low/high
Flohmarkt ?flea market? flea market 2.31 ? 1.65 1.50 ? 1.22 6.03 ? 1.50
Feuerzeug ?lighter? fire stuff 4.58 ? 1.75 5.87 ? 1.01 1.90 ? 1.03
(5) high/low
Fleischwolf ?meat chopper? meat wolf 1.70 ? 1.05 6.00 ? 1.44 1.90 ? 1.42
Table 1: Examples of compound ratings.
a scale between 1 (definitely opaque) and 7 (defi-
nitely transparent). For each compound?constituent
pair, they collected judgements from 30 participants,
and calculated the rating mean and the standard de-
viation. We refer to this set as our compound?
constituent ratings.
A second experiment collected human ratings on
compositionality for our subset of 244 noun-noun
compounds. In this case, we asked the participants
to provide a unique score for each compound as
a whole, again on a scale between 1 and 7. The
collection was performed via Amazon Mechanical
Turk (AMT)3. We randomly distributed our subset
of 244 compounds over 21 batches, with 12 com-
pounds each, in random order. In order to control for
spammers, we also included two German fake com-
pound nouns into each of the batches, in random po-
sitions of the lists. If participants did not recognise
the fake words, all of their ratings were rejected. We
collected between 27 and 34 ratings per target com-
pound. For each of the compounds we calculated the
rating mean and the standard deviation. We refer to
this second set as our compound whole ratings.
Table 1 presents example mean ratings for the
compound?constituent ratings as well as for the
compound whole ratings, accompanied by the stan-
dard deviations. We selected two examples each
for five categories of mean ratings: the compound?
constituent ratings were (1) high or (2) mid or (3)
low with regard to both constituents; the compound?
constituent ratings were (4) low with regard to the
modifier but high with regard to the head; (5) vice
versa. Roller et al (2013) performed a thorough
3www.mturk.com
Figure 1: Distribution of compound ratings.
analysis of the two sets of ratings, and assessed their
reliability from several perspectives.
Figure 1 shows how the mean ratings for the com-
pounds as a whole, for the compound?modifier pairs
as well as for the compound?head pairs are dis-
tributed over the range [1, 7]: For each set, we in-
dependently sorted the 244 values and plotted them.
The purpose of the figure is to illustrate that the rat-
ings for our 244 noun-noun compounds are not par-
ticularly skewed to any area within the range.4
Figure 2 again shows the mean ratings for the
compounds as a whole as well as for the compound?
constituent pairs, but in this case only the compound
whole ratings were sorted, and the compound?
constituent ratings were plotted against the com-
pound whole ratings. According to the plot, the
compound?modifier ratings (red) seem to correlate
better with the compound whole ratings than the
compound?head ratings (yellow) do. This intuition
will be confirmed in Section 3.1.
4The illustration idea was taken from Reddy et al (2011b).
257
Figure 2: Compounds ratings sorted by whole ratings.
3 Vector Space Models (VSMs)
The goal of our vector space models is to identify
distributional features that are salient to predict the
degree of compositionality of the compounds, by re-
lying on the similarities between the compound and
constituent properties.
In all our vector space experiments, we used co-
occurrence frequency counts as induced from Ger-
man web corpora, and calculated local mutual in-
formation (LMI)5 values (Evert, 2005), to instantiate
the empirical properties of our target nouns with re-
gard to the various corpus-based features. LMI is a
measure from information theory that compares the
observed frequencies O with expected frequencies
E, taking marginal frequencies into account:
LMI = O ? log OE ,
with E representing the product of the marginal fre-
quencies over the sample size.6 In comparison to
(pointwise) mutual information (Church and Hanks,
1990), LMI improves the problem of propagating
low-frequent events, by multiplying mutual infor-
mation by the observed frequency.
Relying on the LMI vector space models, the co-
sine determined the distributional similarity between
the compounds and their constituents, which was in
turn used to predict the compositionality between
the compound and the constituents, assuming that
the stronger the distributional similarity (i.e., the co-
sine values), the larger the degree of compositional-
ity.
5Alternatively, we also used the raw frequencies in all ex-
periments below. The insights into the various features were
identical to those based on LMI, but the predictions were worse.
6See http://www.collocations.de/AM/ for a
more detailed illustration of association measures (incl. LMI).
The vector space predictions were evaluated
against the human ratings on the degree of compo-
sitionality, using the Spearman Rank-Order Correla-
tion Coefficient ? (Siegel and Castellan, 1988). The
? correlation is a non-parametric statistical test that
measures the association between two variables that
are ranked in two ordered series. In Section 3.3 we
will compare the overall effect of the various fea-
ture types and correlate all 488 compound?modifier
and compound?head predictions against the ratings
at the same time; in Section 3.4 we will compare
the different effects of the features for compound?
modifier pairs vs. compound?head pairs and thus
correlate 244 predictions in both cases.
After introducing a baseline and an upper bound
for our vector space experiments in Section 3.1 as
well as our web corpora in Section 3.2, Section 3.3
presents window-based in comparison to syntax-
based vector space models (distinguishing various
part-of-speech features). In Section 3.4 we then fo-
cus on the contribution of modifiers vs. heads in the
vector space models, with regard to the overall most
successful features.
3.1 Baseline and Upper Bound
Table 2 presents the baseline and the upper bound
values for the vector space experiments. The
baseline in the first two lines follows a proce-
dure performed by Reddy et al (2011b), and re-
lies on a random assignment of rating values [1, 7]
to the compound?modifier and the compound?
head pairs. The 244 random values for the
compound?constituent pairs were then each corre-
lated against the compound whole ratings. The
random compound?modifier ratings show a base-
line correlation of ? = 0.0959 with the compound
whole ratings, and the random compound?head rat-
ings show a baseline correlation of ? = 0.1019 with
the compound whole ratings.
The upper bound in the first two lines shows the
correlations between the human ratings from the two
experiments, i.e., between the 244 compound whole
ratings and the respective compound?modifier and
compound?head ratings. The compound?modifier
ratings exhibit a strong correlation with the com-
pound whole ratings (? = 0.6002), while the cor-
relation between the compound?head ratings and
the compound whole ratings is not even moderate
258
Function
?
Baseline Upper Bound
modifier only .0959 .6002
head only .1019 .1385
addition .1168 .7687
multiplication .1079 .7829
Table 2: Baseline/Upper bound ? correlations.
(? = 0.1385). Obviously, the semantics of the mod-
ifiers had a much stronger impact on the semantic
judgements of the compounds, thus confirming our
intuition from Section 2.2.
The lower part of the table shows the respec-
tive baseline and upper bound values when the
compound?modifier ratings and the compound?
head ratings were combined by standard arith-
metic operations, cf. Widdows (2008) and
Mitchell and Lapata (2010), among others: the
compound?modifier and compound?head ratings
were treated as vectors, and the vector fea-
tures (i.e., the compound?constituent ratings) were
added/multiplied to predict the compound whole rat-
ings. As in the related work, the arithmetic op-
erations strengthen the predictions, and multiplica-
tion reached an upper bound of ? = 0.7829, thus
outperforming not only the head-only but also the
modifier-only upper bound.
3.2 German Web Corpora
Most of our experiments rely on the sdeWaC corpus
(Faa? et al, 2010), a cleaned version of the German
web corpus deWaC created by the WaCky group (Ba-
roni et al, 2009). The corpus cleaning had focused
mainly on removing duplicates from the deWaC, and
on disregarding sentences that were syntactically ill-
formed (relying on a parsability index provided by a
standard dependency parser (Schiehlen, 2003)). The
sdeWaC contains approx. 880 million words and can
be downloaded from http://wacky.sslmit.
unibo.it/.
While the sdeWaC is an attractive corpus choice
because it is a web corpus with a reasonable size,
and yet has been cleaned and parsed (so that we
can induce syntax-based distributional features), it
has one serious drawback for a window-based ap-
proach (and, in general, for corpus work going be-
yond the sentence border): The sentences in the cor-
pus have been sorted alphabetically, so going be-
yond the sentence border is likely to entering a sen-
tence that did not originally precede or follow the
sentence of interest. So window co-occurrence in
the sdeWaC actually refers to x words to the left and
right BUT within the same sentence. Thus, enlarg-
ing the window size does not effectively change the
co-occurrence information any more at some point.
For this reason, we additionally use WebKo, a pre-
decessor version of the sdeWaC, which comprises
more data (approx. 1.5 billion words in compari-
son to 880 million words) and is not alphabetically
sorted, but is less clean and had not been parsed (be-
cause it was not clean enough).
3.3 Window-based vs. Syntax-based VSMs
Window-based Co-Occurrence When applying
window-based co-occurrence features to our vec-
tor space models, we specified a corpus, a part-of-
speech and a window size, and then determined the
co-occurrence strengths of our compound nouns and
their constituents with regard to the respective con-
text words. For example, when restricting the part-
of-speech to adjectives and the window size to 5, we
counted how often our targets appeared with any ad-
jectives in a window of five words to the left and
to the right. We looked at lemmas, and deleted
any kind of sentence punctuation. In general, we
checked windows of sizes 1, 2, 5, 10, and 20. In one
case we extended the window up to 100 words.
The window-based models compared the effect
of varying the parts-of-speech of the co-occurring
words, motivated by the hypothesis that adjectives
and verbs were expected to provide salient distribu-
tional properties. So we checked which parts-of-
speech provided specific insight into the distribu-
tional similarity between nominal compounds and
nominal constituents: We used common nouns vs.
adjectives vs. main verbs that co-occurred with the
target nouns in the corpora. Figure 3 illustrates the
behaviour of the Spearman Rank-Order Correlation
Coefficient values ? over the window sizes 1, 2, 5,
10, and 20 within sdeWaC (sentence-internal) and
WebKo (beyond sentence borders), when restricting
and combining the co-occurring parts-of-speech. It
is clear from the figure that relying on nouns was
the best choice, even better than combining nouns
with adjectives and verbs. The differences for nouns
vs. adjectives or verbs in the 20-word windows were
259
Figure 3: Window-based sdeWaC and WebKo ? correlations across part-of-speech features.
significant.7 Furthermore, the larger WebKo data
outperformed the cleaned sdeWaC data, reaching an
optimal prediction of ? = 0.6497.8 The corpus dif-
ferences for NN and NN+ADJ+VV were significant.
As none of the window lines had reached an op-
timal correlation with a window size of 20 yet (i.e.,
the correlation values were still increasing), we en-
larged the window size up to 100 words, in order
to check on the most successful window size. We
restricted the experiment to nominal features (with
nouns representing the overall most successful fea-
tures). The correlations did not increase with larger
windows: the optimal prediction was still performed
at a window size of 20.
Syntax-based Co-Occurrence When applying
syntax-based co-occurrence features to our vector
space models, we relied only on the sdeWaC cor-
pus because WebKo was not parsed and thus did
not provide syntactic information. We specified a
syntax-based feature type and then determined the
co-occurrence strengths of our compounds and con-
stituents with regard to the respective context words.
In order to test our hypothesis that syntax-based
information is more salient than window-based in-
formation to predict the compositionality of our
compound nouns, we compared a number of po-
tentially salient syntactic features for noun similar-
ity: the syntactic functions of nouns in verb subcat-
egorisation (intransitive and transitive subjects; di-
rect and PP objects), and those categories that fre-
7All significance tests in this paper were performed by
Fisher r-to-z transformation.
8For a fair corpus comparison, we repeated the experiments
with WebKo on sentence-internal data. It still outperformed the
sdeWaC corpus.
quently modify nouns or are modified by nouns (ad-
jectives and prepositions). With regard to subcate-
gorisation functions, verbs subcategorising our tar-
get nouns represented the dimensions in the vector
space models. For example, we used all verbs as
vector dimensions that took our targets as direct ob-
jects, and vector values were based on these syntac-
tic co-occurrences. For a noun like Buch ?book?,
the strongest verb dimensions were lesen ?read?,
schreiben ?write?, and kaufen ?buy?. With regard
to modification, we considered the adjectives and
prepositions that modified our target nouns, as well
as the prepositions that were modified by our target
nouns. For the noun Buch, strong modifying adjec-
tive dimensions were neu ?new?, erschienen ?pub-
lished?, and heilig ?holy?; strong modifying prepo-
sition dimensions were in ?in?, mit ?with?, and zu
?on?; and strong modified preposition dimensions
were von ?by?, u?ber ?about?, and fu?r ?for?.
Figure 4 demonstrates that the potentially salient
syntactic functions had different effects on predict-
ing compositionality. The top part of the figure
shows the modification-based correlations, the mid-
dle part shows the subcategorisation-based corre-
lations, and at the bottom of the figure we repeat
the ? correlation values for window-based adjec-
tives and verbs (within a window of 20 words)
from the sdeWaC. The syntax-based predictions by
modification and subcategorisation were all signif-
icantly worse than the predictions by the respec-
tive window-based parts-of-speech. Furthermore,
the figure shows that there are strong differences
with regard to the types of syntactic functions,
when predicting compositionality: Relying on our
target nouns as transitive subjects of verbs is al-
260
Figure 4: Syntax-based correlations.
most useless (? = 0.1194); using the intransi-
tive subject function improves the prediction (? =
0.2121); interestingly, when abstracting over subject
(in)transitivity, i.e., when we use all verbs as vector
space features that appeared with our target nouns as
subjects ?independently whether this was an intran-
sitive or a transitive subject? was again more suc-
cessful (? = 0.2749). Relying on our noun targets as
direct objects is again slightly better (? = 0.2988);
as pp objects it is again slightly worse (? = 0.2485).
None of these differences were significant, though.
Last but not least, we concatenated all syntax-
based features to a large syntactic VSM (and we
also considered variations of syntax-based feature
set concatenations), but the results of any unified
combinations were clearly below the best individ-
ual predictions. So the best syntax-based predic-
tors were adjectives that modified our compound and
constituent nouns, with ? = 0.3455, which how-
ever just (non-significantly) outperformed the best
adjective setting in our window-based vector space
(? = 0.3394). Modification by prepositions did
not provide salient distributional information, with
? = 0.2044/0.1725 relying on modifying/modified
prepositions.
In sum, attributive adjectives and verbs that sub-
categorised our target nouns as direct objects were
the most salient syntax-based distributional fea-
tures but nevertheless predicted worse than ?just?
window-based adjectives and verbs, respectively.
3.4 Role of Modifiers vs. Heads
This section tests our hypothesis that the distribu-
tional properties of the head constituents are more
salient than the distributional properties of the mod-
ifier constituents in predicting the degree of compo-
sitionality of the compounds. Our rating data enables
us to explore the modifier/head distinction with re-
gard to two perspectives.
Perspective (i): Salient Features for Compound?
Modifier vs. Compound?Head Pairs Instead of
correlating all 488 compound?constituent predic-
tions against the ratings, we distinguished between
the 244 compound?modifier predictions and the 244
compound?head predictions. This perspective al-
lowed us to distinguish between the salience of the
various feature types with regard to the semantic
relatedness between compound?modifier pairs vs.
compound?head pairs.
Figure 5 presents the correlation values when
predicting the degrees of compositionality of
compound?modifier (M in the left panel) vs.
compound?head (H in the right panel) pairs, as
based on the window features and the various parts-
of-speech. The prediction of the parts-of-speech is
NN > NN+ADJ+VV > VV > ADJ
and ?with few exceptions? the predictions are im-
proving with increasing window sizes, as the over-
all predictions in the previous section did. But
while in smaller window sizes the predictions of
the compound?head ratings are better than those of
the compound?modifier ratings, this difference van-
ishes with larger windows. With regard to a win-
dow size of 20 there is no significant difference be-
tween predicting the semantic relatedness between
compound?modifier vs. compound?head pairs.
When using the syntactic features to predict the
degrees of compositionality of compound?modifier
vs. head?compound pairs, in all but one of the
syntactic feature types the verb subcategorisation as
well as the modification functions allowed a stronger
prediction of compound?head ratings in compari-
son to compound?modifier ratings. The only syn-
tactic feature that was significantly better to predict
compound?modifier ratings was relying on transi-
tive subjects. In sum, the predictions based on syn-
tactic features in most but not all cases behaved in
accordance with our hypothesis.
As in our original experiments in Section 3.3,
the syntax-based features were significantly outper-
formed by the window-based features. The syn-
tactic features reached an optimum of ? = 0.2224
and ? = 0.3502 for predicting modifier?compound
261
Figure 5: Window-based correlations (modifiers vs. heads).
vs. head?compound degrees of compositionality (in
both cases relying on attributive adjectives), in com-
parison to ? = 0.5698 and ? = 0.5745 when relying
on nouns in a window of 20 words.
Perspective (ii): Contribution of Modifiers/Heads
to Compound Meaning This final analysis ex-
plores the contributions of the modifiers and of the
heads with regard to the compound meaning, by cor-
relating only one type of compound?constituent pre-
dictions with the compound whole ratings. I.e., we
predicted the compositionality of the compound by
the distributional similarity between the compound
and only one of its constituents, checking if the
meaning of the compound is determined more by the
meaning of the modifier or the head. This analysis is
in accordance with the upper bound in Section 3.1,
where the compound?constituent ratings were cor-
related with the compound whole ratings.
Figure 6 presents the correlation values when
determining the compound whole ratings by
only compound?modifier predictions, or only
compound?head predictions, or by adding or multi-
plying the modifier and head predictions. The under-
lying features rely on a 20-word window (adjectives,
verbs, nouns, and across parts-of-speech). It is strik-
ing that in three out of four cases the predictions of
the compound whole ratings were performed simi-
larly well (i) by only the compound?modifier pre-
dictions, and (ii) by multiplying the compound?
modifier and the compound?head predictions. So,
as in the calculation of the upper bound, the dis-
tributional semantics of the modifiers had a much
stronger impact on the semantics of the compound
than the distributional semantics of the heads did.
Figure 6: Predicting the compound whole ratings.
3.5 Discussion
The vector space models explored two hypotheses
to predict the compositionality of German noun-
noun compounds by distributional features. Re-
garding hypothesis 1, we demonstrated that ?against
our intuitions? not adjectives or verbs whose mean-
ings are strongly interdependent with the mean-
ings of nouns provided the most salient distribu-
tional information, but that relying on nouns was
the best choice, in combination with a 20-word win-
dow, reaching state-of-the-art ? = 0.6497. The
larger but less clean web corpus WebKo outper-
formed the smaller but cleaner successor sdeWaC.
Furthermore, the syntax-based predictions by adjec-
tive/preposition modification and by verb subcate-
gorisation (as well as various concatenations of syn-
tactic VSMs) were all worse than the predictions by
the respective window-based parts-of-speech.
Regarding hypothesis 2, we distinguished the
contributions of modifiers vs. heads to the com-
pound meaning from two perspectives. (i) The pre-
dictions of the compound?modifier vs. compound?
262
head ratings did not differ significantly when us-
ing features from increasing window sizes, but with
small window sizes the compound?head ratings
were predicted better than the compound?modifier
ratings. This insight fits well to the stronger im-
pact of syntax-based features on compound?head
in comparison to compound?modifier predictions
because ?even though German is a language with
comparably free word order? we can expect many
syntax-based features (especially attributive adjec-
tives and prepositions) to appear in close vicinity
to the nouns they depend on or subcategorise. We
conclude that the features that are salient to predict
similarities between the compound?modifier vs. the
compound?head pairs are different, and that based
on small windows the distributional similarity be-
tween compounds and heads is stronger than be-
tween compounds and modifiers, but based on larger
contexts this difference vanishes. (ii) With regard
to the overall meaning of the compound, the influ-
ence of the modifiers was not only much stronger
in the human ratings (cf. Section 2) and in the up-
per bound (cf. Section 3.1), but also in the vector
space models (cf. Figure 6). While this insight con-
tradicts our second hypothesis (that the head proper-
ties are more salient than the modifier properties in
predicting the compositionality of the compound),
it fits into a larger picture that has primarily been
discussed in psycholinguistic research on compound
meaning, where various factors such as the semantic
relation between the modifier and the head (Gagne?
and Spalding, 2009) and the modifier properties, in-
ferential processing and world knowledge (Gagne?
and Spalding, 2011) were taken into account. How-
ever, also in psycholinguistic studies that explore
the semantic role of modifiers and heads in noun
compounds there is no agreement about which con-
stituent properties are inherited by the compound.
4 Related Work
Most computational approaches to model the mean-
ing or compositionality of compounds have been
performed for English, including work on parti-
cle verbs (McCarthy et al, 2003; Bannard, 2005;
Cook and Stevenson, 2006); adjective-noun com-
binations (Baroni and Zamparelli, 2010; Boleda et
al., 2013); and noun-noun compounds (Reddy et
al., 2011b; Reddy et al, 2011a). Most closely re-
lated to our work is Reddy et al (2011b), who
relied on window-based distributional models to
predict the compositionality of English noun-noun
compounds. Their gold standard also comprised
compound?constituent ratings as well as compound
whole ratings, but the resources had been cleaned
more extensively, and they reached ? = 0.714.
Concerning vector space explorations and seman-
tic relatedness in more general terms, Bullinaria and
Levy (2007; 2012) also systematically assessed a
range of factors in VSMs (corpus type and size,
window size, association measures, and corpus pre-
processing, among others) against four semantic
tasks, however not including compositionality rat-
ings. Similarly, Agirre et al (2009) compared and
combined a WordNet-based and various distribu-
tional models to predict the pair similarity of the 65
Rubenstein and Goodenough word pairs and the 353
word pairs in WordSim353. They varied window
sizes, dependency relations and raw words in the
models. On WordSim353, they reached ? = 0.66,
which is slightly better than our best result, but at
the same time the dataset is smaller.
Concerning computational models of German
compounds, there is not much previous work. Our
own work (Schulte im Walde, 2005; Ku?hner and
Schulte im Walde, 2010) has addressed the degrees
of compositionality of German particle verbs. Zins-
meister and Heid (2004) are most closely related to
our current study. They suggested a distributional
model to identify lexicalised German noun com-
pounds by comparing the verbs that subcategorise
the noun compound with those that subcategorise
the head noun as direct objects.
5 Conclusion
This paper presented experiments to predict the
compositionality of German noun-noun compounds.
Our overall best result is state-of-the-art, reaching
Spearman?s ? = 0.65 with a word-space model of
nominal features from a 20-word window of a 1.5
billion word web corpus. Our experiments demon-
strated that (1) window-based features outperformed
syntax-based features, and nouns outperformed ad-
jectives and verbs; (2) the modifier properties pre-
dominantly influenced the compositionality.
263
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009. A
Study on Similarity and Relatedness Using Distribu-
tional and WordNet-based Approaches. In Proceed-
ings of the North American Chapter of the Association
for Computational Linguistics and Human Language
Technologies Conference, pages 19?27, Boulder, Col-
orado.
Collin Bannard. 2005. Learning about the Meaning of
Verb?Particle Constructions from Corpora. Computer
Speech and Language, 19:467?478.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are Vectors, Adjectives are Matrices: Represent-
ing Adjective-Noun Constructions in Semantic Space.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1183?1193, Cambridge, MA, October.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky Wide Web:
A Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation, 43(3):209?226.
Gemma Boleda, Marco Baroni, Nghia The Pham, and
Louise McNally. 2013. On Adjective-Noun Compo-
sition in Distributional Semantics. In Proceedings of
the 10th International Conference on Computational
Semantics, Potsdam, Germany.
John A. Bullinaria and Joseph P. Levy. 2007. Extracting
Semantic Representations from Word Co-Occurrence
Statistics: A Computational Study. Behavior Research
Methods, 39(3):510?526.
John A. Bullinaria and Joseph P. Levy. 2012. Extracting
Semantic Representations from Word Co-Occurrence
Statistics: Stop-Lists, Stemming, and SVD. Behavior
Research Methods, 44:890?907.
Kenneth W. Church and Patrick Hanks. 1990. Word As-
sociation Norms, Mutual Information, and Lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Paul Cook and Suzanne Stevenson. 2006. Classifying
Particle Semantics in English Verb-Particle Construc-
tions. In Proceedings of the ACL/COLING Workshop
on Multiword Expressions: Identifying and Exploiting
Underlying Properties, Sydney, Australia.
Katrin Erk. 2012. Vector Space Models of Word Mean-
ing and Phrase Meaning: A Survey. Language and
Linguistics Compass, 6(10):635?653.
Stefan Evert. 2005. The Statistics of Word Co-
Occurrences: Word Pairs and Collocations. Ph.D.
thesis, Institut fu?r Maschinelle Sprachverarbeitung,
Universita?t Stuttgart.
Gertrud Faa? Ulrich Heid, and Helmut Schmid. 2010.
Design and Application of a Gold Standard for Mor-
phological Analysis: SMOR in Validation. In Pro-
ceedings of the 7th International Conference on Lan-
guage Resources and Evaluation, pages 803?810, Val-
letta, Malta.
John R. Firth. 1957. Papers in Linguistics 1934-51.
Longmans, London, UK.
Wolfgang Fleischer and Irmhild Barz. 2012. Wortbil-
dung der deutschen Gegenwartssprache. de Gruyter.
Christina L. Gagne? and Thomas L. Spalding. 2009.
Constituent Integration during the Processing of Com-
pound Words: Does it involve the Use of Relational
Structures? Journal of Memory and Language, 60:20?
35.
Christina L. Gagne? and Thomas L. Spalding. 2011. In-
ferential Processing and Meta-Knowledge as the Bases
for Property Inclusion in Combined Concepts. Journal
of Memory and Language, 65:176?192.
Zellig Harris. 1968. Distributional Structure. In Jerold J.
Katz, editor, The Philosophy of Linguistics, Oxford
Readings in Philosophy, pages 26?47. Oxford Univer-
sity Press.
Verena Klos. 2011. Komposition und Kompositionalita?t.
Number 292 in Reihe Germanistische Linguistik. Wal-
ter de Gruyter, Berlin.
Natalie Ku?hner and Sabine Schulte im Walde. 2010. De-
termining the Degree of Compositionality of German
Particle Verbs by Clustering Approaches. In Proceed-
ings of the 10th Conference on Natural Language Pro-
cessing, pages 47?56, Saarbru?cken, Germany.
Rochelle Lieber and Pavol Stekauer. 2009a. Intro-
duction: Status and Definition of Compounding. In
The Oxford Handbook on Compounding (Lieber and
Stekauer, 2009b), chapter 1, pages 3?18.
Rochelle Lieber and Pavol Stekauer, editors. 2009b. The
Oxford Handbook of Compounding. Oxford Univer-
sity Press.
Kevin Lund and Curt Burgess. 1996. Producing
High-Dimensional Semantic Spaces from Lexical Co-
Occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28(2):203?208.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a Continuum of Compositionality in Phrasal
Verbs. In Proceedings of the ACL-SIGLEX Workshop
on Multiword Expressions: Analysis, Acquisition and
Treatment, Sapporo, Japan.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive Sci-
ence, 34:1388?1429.
Siva Reddy, Ioannis P. Klapaftis, Diana McCarthy, and
Suresh Manandhar. 2011a. Dynamic and Static Pro-
totype Vectors for Semantic Composition. In Pro-
ceedings of the 5th International Joint Conference on
264
Natural Language Processing, pages 705?713, Chiang
Mai, Thailand.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011b. An Empirical Study on Compositionality in
Compound Nouns. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language Process-
ing, pages 210?218, Chiang Mai, Thailand.
Stephen Roller, Sabine Schulte im Walde, and Silke
Scheible. 2013. The (Un)expected Effects of Apply-
ing Standard Cleansing Models to Human Ratings on
Compositionality. In Proceedings of the 9th Workshop
on Multiword Expressions, Atlanta, GA.
Michael Schiehlen. 2003. A Cascaded Finite-State
Parser for German. In Proceedings of the 10th Con-
ference of the European Chapter of the Association for
Computational Linguistics, pages 163?166, Budapest,
Hungary.
Sabine Schulte im Walde. 2005. Exploring Features to
Identify Semantic Nearest Neighbours: A Case Study
on German Particle Verbs. In Proceedings of the In-
ternational Conference on Recent Advances in Natural
Language Processing, pages 608?614, Borovets, Bul-
garia.
Hinrich Schu?tze. 1992. Dimensions of Meaning. In Pro-
ceedings of Supercomputing, pages 787?796.
Sidney Siegel and N. John Castellan. 1988. Non-
parametric Statistics for the Behavioral Sciences.
McGraw-Hill, Boston, MA.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Claudia von der Heide and Susanne Borgwaldt. 2009.
Assoziationen zu Unter-, Basis- und Oberbegriffen.
Eine explorative Studie. In Proceedings of the 9th
Norddeutsches Linguistisches Kolloquium, pages 51?
74.
Dominic Widdows. 2008. Semantic Vector Products:
Some Initial Investigations. In Proceedings of the 2nd
Conference on Quantum Interaction, Oxford, UK.
Heike Zinsmeister and Ulrich Heid. 2004. Collocations
of Complex Nouns: Evidence for Lexicalisation. In
Proceedings of Konvens, Vienna, Austria.
265
Proceedings of the EACL 2012 Workshop on Computational Linguistics and Writing, pages 9?18,
Avignon, France, April 23, 2012. c?2012 Association for Computational Linguistics
From Drafting Guideline to Error Detection:
Automating Style Checking for Legislative Texts
Stefan H?fler
University of Zurich, Institute
of Computational Linguistics
Binzm?hlestrasse 14
8050 Z?rich, Switzerland
hoefler@cl.uzh.ch
Kyoko Sugisaki
University of Zurich, Institute
of Computational Linguistics
Binzm?hlestrasse 14
8050 Z?rich, Switzerland
sugisaki@cl.uzh.ch
Abstract
This paper reports on the development of
methods for the automated detection of vi-
olations of style guidelines for legislative
texts, and their implementation in a pro-
totypical tool. To this aim, the approach
of error modelling employed in automated
style checkers for technical writing is en-
hanced to meet the requirements of legisla-
tive editing. The paper identifies and dis-
cusses the two main sets of challenges that
have to be tackled in this process: (i) the
provision of domain-specific NLP methods
for legislative drafts, and (ii) the concretisa-
tion of guidelines for legislative drafting so
that they can be assessed by machine. The
project focuses on German-language legisla-
tive drafting in Switzerland.
1 Introduction
This paper reports on work in progress that is
aimed at providing domain-specific automated
style checking to support German-language legisla-
tive editing in the Swiss federal administration. In
the federal administration of the Swiss Confedera-
tion, drafts of new acts and ordinances go through
several editorial cycles. In a majority of cases, they
are originally written by civil servants in one of
the federal offices concerned, and then reviewed
and edited both by legal experts (at the Federal
Office of Justice) and language experts (at the Fed-
eral Chancellery). While the former ensure that
the drafts meet al relevant legal requirements, the
latter are concerned with the formal and linguistic
quality of the texts. To help this task, the author-
ities have drawn up style guidelines specifically
geared towards Swiss legislative texts (Bundeskan-
zlei, 2003; Bundesamt f?r Justiz, 2007).
Style guidelines for laws (and other types of
legal texts) may serve three main purposes: (i) im-
proving the understandability of the texts (Lerch,
2004; Wydick, 2005; Mindlin, 2005; Butt and
Castle, 2006; Eichhoff-Cyrus and Antos, 2008),
(ii) enforcing their consistency with related texts,
and (iii) facilitating their translatability into other
languages. These aims are shared with writing
guidelines developed for controlled languages in
the domain of technical documentation (Lehrndor-
fer, 1996; Reuther, 2003; Muegge, 2007).
The problem is that the manual assessment of
draft laws for their compliance with all relevant
style guidelines is time-consuming and easily in-
consistent due to the number of authors and editors
involved in the drafting process. The aim of the
work presented in this paper is to facilitate this
process by providing methods for a consistent au-
tomatic identification of some specific guideline
violations.
The remainder of the paper is organised as fol-
lows. We first delineate the aim and scope of the
project presented in the paper (section 2) and the
approach we are pursuing (section 3). In the main
part of the paper, we then identify and discuss
the two main challenges that have to be tackled:
the technical challenge of providing NLP methods
for legislative drafts (section 4) and the linguis-
tic challenge of concretising the existing drafting
guidelines for legislative texts (section 5).
2 Aim and Scope
The aim of the project to be presented in this paper
is to develop methods of automated style checking
specifically geared towards legislative editing, and
to implement these methods in a prototypical tool
(cf. sections 3 and 4). We work towards automat-
9
XML
<...><...><...><...><...><...><...>
DetectionRules
Pre-processing
ErrorDetection
LegislativeDraft
Enriched Draft
Error Report
PredefinedHelptexts
ID 203
Help Text
 ID  span
 80  [...]
135  [...]
203  [...]
OutputGeneration
2)
1)
3)
HighlightedDraft
1)
2)
3)
Documentation/Help Text
+
Error ID
Token IDs
Figure 1: Architecture of the style checking system.
ically detecting violations of existing guidelines,
and where these guidelines are very abstract, we
concretise them so that they become detectable by
machine (cf. section 5). However, it is explicitly
not the goal of our project to propose novel style
rules.
We have adopted a broad conception of ?style
checking? that is roughly equivalent to how the
term, and its variant ?controlled language check-
ing,? have been used in the context of technical
writing (Geldbach, 2009). It comprises the assess-
ment of various aspects of text composition con-
trolled by specific writing guidelines (typographi-
cal conventions, lexical preferences, syntax-related
recommendations, constraints on discourse and
document structure), but it does not include the
evaluation of spelling and grammar.
While our project focuses on style checking for
German-language Swiss federal laws (the federal
constitution, acts of parliament, ordinances, fed-
eral decrees, cantonal constitutions), we believe
that the challenges arising from the task are in-
dependent of the chosen language and legislative
system but pertain to the domain in general.
3 Approach
The most important innovative contribution of our
project is the enhancement of the method of er-
ror modelling to meet the requirements of legisla-
tive editing. Error modelling means that texts are
searched for specific features that indicate a style
guideline violation: the forms of specific ?errors?
are thus anticipated and modelled.
The method of error modelling has mainly been
developed for automated style checking in the do-
main of technical writing. Companies often con-
trol the language used in their technical documen-
tation in order to improve the understandability,
readability and translatability of these texts. Con-
trolled language checkers are tools that evaluate
input texts for compliance with such style guide-
lines set up by a company.1
State-of-the-art controlled language checkers
work along the following lines. In a pre-processing
step, they first perform an automatic analysis of the
input text (tokenisation, text segmentation, mor-
phological analysis, part-of-speech tagging, pars-
ing) and enrich it with the respective structural
and linguistic information. They then apply a
number of pre-defined rules that model potential
?errors? (i.e. violations of individual style guide-
lines) and aim at detecting them in the analysed
text. Most checkers give their users the option to
choose which rules the input text is to be checked
for. Once a violation of the company?s style guide-
lines has been detected, the respective passage is
highlighted and an appropriate help text is made
available to the user (e.g. as a comment in the orig-
inal document or in an extra document generated
by the system). The system we are working on is
constructed along the same lines; its architecture
is outlined in Fig. 1.
Transferring the described method to the do-
main of legislative editing has posed challenges
to both pre-processing and error modelling. The
peculiarities of legal language and legislative texts
have necessitated a range of adaptations in the NLP
procedures devised, and the guidelines for legisla-
tive drafting have required highly domain-specific
1Examples of well-developed commercial tools that offer
such style checking for technical texts are acrolinx IQ by
Acrolinx and CLAT by IAI.
10
error modelling, which needed to be backed up
by substantial linguistic research. We will detail
these two sets of challenges in the following two
sections.
4 Pre-Processing
4.1 Tokenisation
The legislative drafters and editors we are target-
ing exclusively work with MS Word documents.
Drafters compose the texts in Word, and legisla-
tive editors use the commenting function of Word
to add their suggestions and corrections to the
texts they receive. We make use of the XML
representation (WordML) underlying these doc-
uments. In a first step, we tokenise the text con-
tained therein and assign each token an ID directly
in the WordML structure. We then extract the
text material (including the token IDs and some
formatting information that proves useful in the
processing steps to follow) for further processing.
The token IDs are used again at the end of the
style checking process when discovered styleguide
violations are highlighted by inserting a Word com-
ment at the respective position in the WordML rep-
resentation of the original document. The output
of our style checker is thus equivalent to how leg-
islative editors make their annotations to the drafts
? a fact that proves essential with regard to the tool
being accepted by its target users.
4.2 Text Segmentation
After tokenisation, the input text is then segmented
into its structural units. Legislative texts exhibit a
sophisticated domain-specific structure. Our text
segmentation tool detects the boundaries of chap-
ters, sections, articles, paragraphs, sentences and
enumeration elements, and marks them by adding
corresponding XML tags to the text.
There are three reasons why text segmentation
is crucial to our endeavour:
1. Proper text segmentation ensures that only
relevant token spans are passed on to further
processing routines (e.g. sentences contained
in articles must to be passed on to the parser,
whereas article numbers or section headings
must not).
2. Most structural units are themselves the ob-
ject of style rules (e.g. ?sections should not
contain more than twelve articles, articles
should not contain more than three para-
graphs and paragraphs should not contain
more than one sentence?). The successful
detection of violations of such rules depends
on the correct delimitation of the respective
structural units in the text.
3. Certain structural units constitute the context
for other style rules (e.g. ?the sentence right
before the first element of an enumeration has
to end in a colon?; ?the antecedent of a pro-
noun must be within the same article?). Here
too, correct text segmentation constitutes the
prerequisite for an automated assessment of
the respective style rules.
We have devised a line-based pattern-matching al-
gorithm with look-around to detect the boundaries
of the structural units of legislative drafts (H?fler
and Piotrowski, 2011). The algorithm also exploits
formatting information extracted together with the
text from the Word documents. However, not all
formatting information has proven equally reliable:
as the Word documents in which the drafts are com-
posed do only make use of style environments to
a very limited extent, formatting errors are rela-
tively frequent. Font properties such as italics or
bold face, or the use of list environments are fre-
quently erroneous and can thus not be exploited for
the purpose of delimiting text segments; headers
and newline information, on the other hand, have
proven relatively reliable.
Figure 2 illustrates the annotation that our tool
yields for the excerpt shown in the following ex-
ample:
(1) Art. 14 Amtsenthebung 2
Die Wahlbeh?rde kann eine Richterin oder
einen Richter vor Ablauf der Amtsdauer des
Amtes entheben, wenn diese oder dieser:
a. vors?tzlich oder grobfahrl?ssig
Amtspflichten schwer verletzt hat; oder
b. die F?higkeit, das Amt auszu?ben, auf
Dauer verloren hat.
Art. 14 Removal from office
The electoral authorities may remove a judge
from office before he or she has completed
his or her term where he or she:
2Patentgerichtsgesetz (Patent Court Act), SR 173.41; for
the convenience of readers, examples are also rendered in the
(non-authoritative) English version published at
http://www.admin.ch/ch/e/rs/rs.html.
11
<article>
<article_head>
<article_type>Art.</article_type>
<article_nr>14</article_nr>
<article_header>Amtsenthebung</article_header>
</article_head>
<article_body>
<paragraph>
<sentence>
Die Wahlbeh?rde kann eine Richterin oder einen Richter vor Ablauf der Amtsdauer
des Amtes entheben, wenn diese oder dieser:
<enumeration>
<enumeration_element>
<element_nr type="letter">a.</element_nr>
<element_text>
vors?tzlich oder grobfahrl?ssig Amtspflichten schwer verletzt hat;
oder
</element_text>
</enumeration_element>
<enumeration_element>
<element_nr type="letter">b.</element_nr>
<element_text>
die F?higkeit, das Amt auszu?ben, auf Dauer verloren hat.
</element_text>
</enumeration_element>
</enumeration>
</sentence>
</paragraph>
</article_body>
</article>
Figure 2: Illustration of the text segmentation provided by the tool. Excerpt: Article 14 of the Patent Court Act.
(Token delimiters and any other tags not related to text segmentation have been omitted in the example.)
a. wilfully or through gross negligence
commits serious breaches of his or her
official duties; or
b. has permanently lost the ability to
perform his or her official duties.
As our methods must be robust in the face of input
texts that are potentially erroneous, the text seg-
mentation provided by our tool does not amount to
a complete document parsing; our text segmenta-
tion routine rather performs a document chunking
by trying to detect as many structural units as pos-
sible.
Another challenge that arises from the fact that
the input texts may be erroneous is that features
whose absence we later need to mark as an error
cannot be exploited for the purpose of detecting
the boundaries of the respective contextual unit. A
colon, for instance, cannot be used as an indicator
for the beginning of an enumeration since we must
later be able to search for enumerations that are not
preceded by a sentence ending in a colon as this
constitutes a violation of the respective style rule.
Had the colon been used as an indicator for the de-
tection of enumeration boundaries, only enumera-
tions preceded by a colon would have been marked
as such in the first place. The development of ad-
equate pre-processing methods constantly faces
such dilemmas. It is thus necessary to always an-
ticipate the specific guideline violations that one
later wants to detect on the basis of the information
added by any individual pre-processing routine.
Special challenges also arise with regard to the
task of sentence boundary detection. Legislative
texts contain special syntactic structures that off-
the-shelf tools cannot process and that therefore
need special treatment. Example (1) showed a sen-
tence that runs throughout a whole enumeration;
colon and semicolons do not mark sentence bound-
aries in this case. To complicate matters even
further, parenthetical sentences may be inserted
behind individual enumeration items, as shown in
example (2).
(2) Art. 59 Abschirmung 3
1 Der Raum oder Bereich, in dem station?re
Anlagen oder radioaktive Strahlenquellen
betrieben oder gelagert werden, ist so zu
3Strahlenschutzverordnung (Radiological Protection Or-
dinance), SR 814.50; emphasis added.
12
konzipieren oder abzuschirmen, dass unter
Ber?cksichtigung der Betriebsfrequenz:
a. an Orten, die zwar innerhalb des
Betriebsareals, aber ausserhalb von
kontrollierten Zonen liegen und an
denen sich nichtberuflich
strahlenexponierte Personen aufhalten
k?nnen, die Ortsdosis 0,02 mSv pro
Woche nicht ?bersteigt. Dieser Wert
kann an Orten, wo sich Personen
nicht dauernd aufhalten, bis zum
F?nffachen ?berschritten werden;
b. an Orten ausserhalb des Betriebsareals
die Immissionsgrenzwerte nach
Artikel 102 nicht ?berschritten werden.
2 [...]
Art. 59 Shielding
1 The room or area in which stationary
radiation generators or radioactive sources
are operated or stored shall be designed and
shielded in such a way that, taking into
account the frequency of use:
a. in places situated within the premises
but outside controlled areas, where
non-occupationally exposed persons
may be present, the local dose does not
exceed 0.02 mSv per week. In places
where people are not continuously
present, this value may be exceeded
by up to a factor of five;
b. in places outside the premises, the
off-site limits specified in Article102
are not exceeded.
2 [...]
In this example, a parenthetical sentence (marked
in bold face) has been inserted at the end of the
first enumeration item. A full stop has been put
where the main sentence is interrupted, whereas
the inserted sentence is ended with a semicolon
to indicate that after it, the main sentence is con-
tinued. The recognition of sentential insertions as
the one shown in (2) is important for two reasons:
(i) sentential parentheses are themselves the object
of style rules (in general, they are to be avoided)
and should thus be marked by a style checker, and
(ii) a successful parsing of the texts depends on a
proper recognition of the sentence boundaries. As
off-the-shelf tools cannot cope with such domain-
specific structures, we have had to devise highly
specialised algorithms for sentence boundary de-
tection in our texts.
4.3 Linguistic Analysis
Following text segmentation, we perform a lin-
guistic analysis of the input text which consists of
three components: part-of-speech tagging, lemma-
tisation and chunking/parsing. The information
added by these pre-processing steps is later used
in the detection of violations of style rules that
pertain to the use of specific terms (e.g. ?the modal
sollen ?should? is to be avoided?), syntactic con-
structions (e.g. ?complex participial constructions
preceding a noun should be avoided?) or combina-
tions thereof (e.g. ?obligations where the subject
is an authority must be put as assertions and not
contain a modal verb?).
For the tasks of part-of-speech tagging and lem-
matisation, we employ TreeTagger (Schmid, 1994).
We have adapted TreeTagger to the peculiarities
of Swiss legislative language. Domain-specific
token types are pre-tagged in a special routine to
avoid erroneous part-of-speech analyses. An ex-
ample of a type of tokens that needs pre-tagging
are domain-specific cardinal numbers: i.e. cardi-
nal numbers augmented with letters (Article 2a)
or with Latin ordinals (Paragraph 4bis) as well as
ranges of such cardinal numbers (Articles 3c?6).
Furthermore, TreeTagger?s recognition of sentence
boundaries is overwritten by the output of our text
segmentation routine. We have also augmented
TreeTagger?s domain-general list of abbreviations
with a list of domain-specific abbreviations and
acronyms provided by the Swiss Federal Chan-
cellery. The lemmatisation provided by TreeTag-
ger usually does not recognise complex compound
nouns (e.g. G?terverkehrsverlagerung ?freight traf-
fic transfer?); such compound nouns are frequent
in legislative texts (Nussbaumer, 2009). To solve
the problem, we combine the output of TreeTag-
ger?s part-of-speech tagging with the lemma infor-
mation delivered by the morphology analysis tool
GERTWOL (Haapalainen and Majorin, 1995).
Some detection tasks (e.g. the detection of legal
definitions discussed in section 4.4 below) addi-
tionally require chunking or even parsing. For
chunking, we also employ TreeTagger; for pars-
ing, we have begun to adapt ParZu to legislative
language, a robust state-of-art dependency parser
13
(Sennrich et al, 2009). Like most off-the-shelf
parsers, ParZu was trained on a corpus of newspa-
per articles. As a consequence, it struggles with
analysing constructions that are rare in that do-
main but frequent in legislative texts, such as com-
plex coordinations of prepositional phrases and
PP-attachment chains (Venturi, 2008), parenthe-
ses (as illustrated in example 2 above) or subject
clauses (as shown in example 3 below).
(3) Art. 17 Rechtfertigender Notstand 4
Wer eine mit Strafe bedrohte Tat begeht,
um ein eigenes oder das Rechtsgut einer
anderen Person aus einer unmittelbaren,
nicht anders abwendbaren Gefahr zu
retten, handelt rechtm?ssig, wenn er
dadurch h?herwertige Interessen wahrt.
Art. 17 Legitimate act in a situation of
necessity
Whoever carries out an act that carries a
criminal penalty in order to save a legal
interest of his own or of another from
immediate and not otherwise avertable
danger, acts lawfully if by doing so he
safeguards interests of higher value.
As the adaptation of ParZu to legislative texts is
still in its early stages, we cannot yet provide an
assessment of how useful the output of the parser,
once properly modified, will be to our task.
4.4 Context Recognition
The annotations that the pre-processing routines
discussed so far add to the text serve as the basis
for the automatic recognition of domain-specific
contexts. Style rules for legislative drafting often
only apply to special contexts within a law. An
example is the rule pertaining to the use of the
modal sollen (?should?). The drafting guidelines
forbid the use of this modal except in statements
of purpose. Statements of purpose thus consti-
tute a special context inside which the detection
of an instance of sollen is not to trigger an error
message. Other examples of contexts in which
special style rules apply are transitional provisions
(?bergangsbestimmungen), repeals and amend-
ments of current legislation (Aufhebungen und ?n-
derungen bisherigen Rechts), definitions of the
4Strafgesetzbuch (Criminal Code), SR 311.0; emphasis
added.
subject of a law (Gegenstandsbestimmungen), def-
initions of the scope of a law (Geltungsbereichsbe-
stimmungen), definitions of terms (Begriffsbestim-
mungen), as well as preambles (Pr?ambeln) and
commencement clauses (Ingresse).
A number of these contexts can be identified
automatically by assessing an article?s position
in the text and certain keywords contained in its
header. A statements of purpose, for instance, is
usually the first article of a law, and its header usu-
ally contains the words Zweck (?purpose?) or Ziel
(?aim?). Similar rules can be applied to recognise
transitional provisions, repeals and amendments of
current legislation, and definitions of the subject
and the scope of a law.
Other contexts have to be detected at the senten-
tial level. Definitions of terms, for instance, do not
only occur as separate articles at the beginning of a
law; they can also appear in the form of individual
sentences throughout the text. As there is a whole
range of style rules pertaining to legal definitions
(e.g. ?a term must only be defined if it occurs at
least three times in the text?; ?a term must only be
defined once within the same text?; ?a term must
not be defined by itself?), the detection of this par-
ticular context (and its components: the term and
the actual definition) is crucial to a style checker
for legislative texts.5
To identify legal definitions in the text, we have
begun to adopt strategies developed in the con-
text of legal information retrieval: Walter and
Pinkal (2009) and de Maat and Winkels (2010),
for instance, show that definitions in German court
decisions and in Dutch laws respectively can be
detected by searching for combinations of key
words and sentence patterns typically used in these
domain-specific contexts. In H?fler et al (2011)
we have argued that this approach is also feasible
with regard to Swiss legislative texts: our pilot
study has shown that a substantial number of legal
definitions can be detected even without resort-
ing to syntactic analyses, merely by searching for
typical string patterns such as ?X im Sinne dieser
Verordnung ist/sind Y? (?X in the sense of this ordi-
nance is/are Y?). We are currently working towards
refining and extending the detection of legal defini-
tions by including additional syntactic information
yielded by the processes of chunking and parsing
into the search patterns.
5Further rules for the use of legal definitions in Swiss law
texts are provided by Bratschi (2009).
14
Once the legal definitions occurring in a draft
have been marked, the aforementioned style rules
can be checked automatically (e.g. by searching
the text for terms that are defined in a definition
but occur less than three times in the remainder
of the text; by checking if there are any two legal
definitions that define the same term; by assessing
if there are definitions where the defined term also
occurs in the actual definition).
After having outlined some of the main chal-
lenges that the peculiarities of legal language and
legislative texts pose to the various pre-processing
tasks, we now turn to the process of error mod-
elling, i.e. the effort of transferring the guidelines
for legislative drafting into concrete error detection
mechanisms operating on the pre-processed texts.
5 Error Modelling
5.1 Sources
The first step towards error modelling consists in
collecting the set of style rules that shall be ap-
plied to the input texts. The main source that we
use for this purpose are the compilations of draft-
ing guidelines published by the Swiss Federal Ad-
ministration (Bundeskanzlei, 2003; Bundesamt f?r
Justiz, 2007). However, especially when it comes
to linguistic issues, these two documents do not
claim to provide an exhaustive set of writing rules.
Much more so than the writing rules that are put
in place in the domain of technical documenta-
tion, the rules used in legislative drafting are based
on historically grown conventions, and there may
well be conventions beyond what is explicitly writ-
ten down in the Federal Administration?s official
drafting guidelines.
Consequently, we have also been collect-
ing rule material from three additional sources.
A first complementary source are the various
drafting guidelines issued by cantonal govern-
ments (Regierungsrat des Kantons Z?rich, 2005;
Regierungsrat des Kantons Bern, 2000) and, to a
lesser extent, the drafting guidelines of the other
German-speaking countries (Bundesministerium
f?r Justiz, 2008; Bundeskanzleramt, 1990; Rechts-
dienst der Regierung, 1990) and the European
Union (Europ?ische Kommission, 2003). A sec-
ond source are academic papers dealing with spe-
cific issues of legislative drafting, such as Eisen-
berg (2007), Bratschi (2009).
Finally, legislative editors themselves constitute
an invaluable source of expert knowledge. In or-
der to learn of their unwritten codes of practice,
we have established a regular exchange with the
Central Language Services of the Swiss Federal
Chancellery. Including the editors in the process
is likely to prove essential for the acceptability of
the methods that we develop.
5.2 Concretisation and Formalisation
The next error modelling step consists in concretis-
ing and formalising the collected rules so that spe-
cific algorithms can be developed to search for
violations of the rules in the pre-processed texts.
Depending on the level of abstraction of a rule,
this task is relatively straight-forward or it requires
more extensive preliminary research:
Concrete Rules A number of rules for legisla-
tive drafting define concrete constraints and can
thus be directly translated into detection rules. Ex-
amples of such concrete rules are rules that pro-
hibit the use of specific abbreviations (e.g. bzw.
?respectively?; z.B. ?e.g.?; d.h. ?i.e.?) and of certain
terms and phrases (e.g. grunds?tzlich ?in princi-
ple?; in der Regel ?as a general rule?). In such
cases, error detection simply consists in searching
for the respective items in the input text.
Some rules first need to be spelled out but can
then also be formalised more or less directly: the
rule stating that units of measurement must always
be written out rather than abbreviated, for instance,
requires that a list of such abbreviations of mea-
suring units (e.g. m for meter, kg for kilogram, %
for percent) is compiled whose entries can then be
searched for in the text.
The formalisation of some other rules is some-
what more complicated but can still be derived
more or less directly. The error detection strate-
gies for these rules include accessing tags that
were added during pre-processing or evaluating
the environment of a potential error. For exam-
ple, the rule stating that sentences introducing an
enumeration must end in a colon can be checked
by searching the text for <enumeration> tags that
are not preceded by a colon; violations of the rule
stating that an article must not contain more than
three paragraphs can be detected by counting for
each <article_body> environment, the number of
<paragraph> elements it contains.
15
Abstract Rules However, guidelines for legisla-
tive drafting frequently contain rules that define
relatively abstract constraints. In order to be able
to detect violations of such constraints, a linguistic
concretisation of the rules is required.
An example is the oft-cited rule that a sentence
should only convey one statement or proposition
(Bundesamt f?r Justiz, 2007, p. 358). The er-
ror modelling for this rule is not straightforward:
it is neither clear what counts as a statement in
the context of a legislative text, nor is it obvious
what forms sentences violating this rule exhibit.
Linguistic indicators for the presence of a multi-
propositional sentence first need to be determined
in in-depth analyses of legislative language. In
H?fler (2011), we name a number of such indica-
tors: among other things, sentence coordination,
relative clauses introduced by the adverb wobei
(?whereby?), and certain prepositions (e.g. vorbe-
h?ltlich ?subject to? or mit Ausnahme von ?with the
exception of?) can be signs that a sentence contains
more than one statement.
Even drafting rules that look fairly specific at
first glance may turn out to be in need of further lin-
guistic concretisation. An example is the rule that
states that in an enumeration, words that are shared
between all enumeration elements should be brack-
eted out into the introductory sentence of the enu-
meration. If, for instance, each element of an
enumeration starts with the preposition f?r (?for?),
then that preposition belongs in the introductory
sentence. The rule seems straight enough, but in
reality, the situation is somewhat more compli-
cated. Example (4) shows a case where a word
that occurs at the beginning of all elements of an
enumeration (the definite article die ?the?) cannot
be bracketed out into the introductory sentence:
(4) Art. 140 Obligatorisches Referendum 6
[...]
2 Dem Volk werden zur Abstimmung
unterbreitet:
a. die Volksinitiativen auf Totalrevision
der Bundesverfassung;
b. die Volksinitiativen auf Teilrevision der
Bundesverfassung in der Form der
allgemeinen Anregung, die von der
Bundesversammlung abgelehnt worden
sind;
6Bundesverfassung (Federal Constitution), SR 101; em-
phasis added.
c. die Frage, ob eine Totalrevision der
Bundesverfassung durchzuf?hren ist,
bei Uneinigkeit der beiden R?te.
Art. 140 Mandatory referendum
[...]
2 The following shall be submitted to a vote
of the People:
a. the popular initiatives for a complete
revision of the Federal Constitution;
b. the popular initiatives for a partial
revision of the Federal Constitution in
the form of a general proposal that have
been rejected by the Federal Assembly;
c. the question of whether a complete
revision of the Federal Constitution
should be carried out, in the event that
there is disagreement between the two
Councils.
Even if one ignores the fact that the definite article
in letters a and b is in fact not the same as the
one in letter c (the former being plural, the latter
singular), it is quite apparent that articles cannot
be extracted from the elements of an enumeration
without the nouns they specify. Even the seem-
ingly simple rule in question is thus in need of a
more linguistically informed concretisation before
it can be effectively checked by machine.
The examples illustrate that style guidelines for
legislative writing are often kept at a level of ab-
straction that necessitates concretisations if one
is to detect violations of the respective rules au-
tomatically. Besides the development of domain-
specific pre-processing algorithms, the extensive
and highly specialised linguistic research required
for such concretisations constitutes the main task
being tackled in this project.
Conflicting Rules A further challenge to error
modelling arises from the fact that a large propor-
tion of drafting guidelines for legislative texts do
not constitute absolute constraints but rather have
the status of general writing principles and rules
of thumb. This fact has to be reflected in the feed-
back messages that the system gives to its users:
what the tool detects are often not ?errors? in the
proper sense of the word but merely passages that
the author or editor may want to reconsider.
The fact that many style rules only define soft
constraints also means that there may be conflict-
ing rules. Consider, for instance, sentence (5):
16
(5) Art. 36 Ersatzfreiheitsstrafe 7
[...]
5 Soweit der Verurteilte die Geldstrafe trotz
verl?ngerter Zahlungsfrist oder
herabgesetztem Tagessatz nicht bezahlt oder
die gemeinn?tzige Arbeit trotz Mahnung
nicht leistet, wird die Ersatzfreiheitsstrafe
vollzogen.
Art. 36 Alternative custodial sentence
[...]
5 As far as the offender fails to pay the
monetary penalty despite being granted an
extended deadline for payment or a reduced
daily penalty unit or fails to perform the
community service despite being warned of
the consequences, the alternative custodial
sentence is executed.
On the one hand, this sentence must be consid-
ered a violation of the style rule that states that
the main verb of a sentence (here execute) should
be introduced as early as possible (Regierungsrat
des Kantons Z?rich, 2005, p. 73). On the other
hand, if the sentence was re-arranged in compli-
ance with this rule ? by switching the order of the
main clause and the subsidiary clause ? it would
violate the rule stating that information is to be pre-
sented in temporal and causal order (Bundesamt
f?r Justiz, 2007, p. 354). This latter rule entails
that the condition precedes its consequence.
To be able to deal with such conflicting con-
straints, error detection strategies have to be as-
signed weights. However, one and the same rule
may have different weights under different cir-
cumstances. In conditional sentences like the one
shown above, the causality principle obviously
weighs more than the rule that the main verb must
be introduced early in the sentence. Such context-
dependent rankings for individual style rules have
to be inferred and corroborated by tailor-made
corpus-linguistic studies.
5.3 Testing and Evaluation
The number of drafts available to us is very lim-
ited ? too limited to be used to test and refine the
error models we develop. However, due to the
complexity of the drafting process (multiple au-
thors and editors, political intervention), laws that
7Strafgesetzbuch (Criminal Code), SR 311.0
have already come into force still exhibit viola-
tions of specific style rules. We therefore resort
to such already published laws to test and refine
the error models we develop. To this aim, we have
built a large corpus of legislative texts automati-
cally annotated by the pre-processing routines we
have described earlier in the paper (H?fler and
Piotrowski, 2011). The corpus contains the entire
current federal legislation of Switzerland, i.e. the
federal constitution, all cantonal constitutions, all
federal acts and ordinances, federal decrees and
treaties between the Confederation and individual
cantons and municipalities. It allows us to try out
and evaluate novel error detection strategies by
assessing the number and types of true and false
positives returned.
6 Conclusion
In this paper, we have discussed the development
of methods for the automated detection of viola-
tions of domain-specific style guidelines for leg-
islative texts, and their implementation in a proto-
typical tool. We have illustrated how the approach
of error modelling employed in automated style
checkers for technical writing can be enhanced to
meet the requirements of legislative editing. Two
main sets of challenges are tackled in this process.
First, domain-specific NLP methods for legisla-
tive drafts have to be provided. Without extensive
adaptations, off-the-shelf NLP tools that have been
trained on corpora of newspaper articles are not
adequately equipped to deal with the peculiarities
of legal language and legislative texts. Second,
the error modelling for a large number of draft-
ing guidelines requires a concretisation step before
automated error detection strategies can be put in
place. The substantial linguistic research that such
concretisations require constitutes a core task to be
carried out in the development of a style checker
for legislative texts.
Acknowledgments
The project is funded under SNSF grant 134701.
The authors wish to thank the Central Language
Services of the Swiss Federal Chancellery for their
continued advice and support.
References
Rebekka Bratschi. 2009. ?Frau im Sinne dieser Bade-
ordnung ist auch der Bademeister.? Legaldefinitio-
17
nen aus redaktioneller Sicht. LeGes, 20(2):191?213.
Bundesamt f?r Justiz, editor. 2007. Gesetzgebungs-
leitfaden: Leitfaden f?r die Ausarbeitung von Er-
lassen des Bundes. Bern, 3. edition.
Bundeskanzlei, editor. 2003. Gesetzestechnische
Richtlinien. Bern.
Bundeskanzleramt, editor. 1990. Handbuch der Recht-
setzungstechnik, Teil 1: Legistische Leitlinien. Wien.
Bundesministerium f?r Justiz, editor. 2008. Handbuch
der Rechtsf?rmlichkeit, Empfehlungen zur Gestal-
tung von Gesetzen und Rechtsverordnungen. Bunde-
sanzeiger Verlag, K?ln.
Peter Butt and Richard Castle. 2006. Modern Legal
Drafting. Cambridge University Press, Cambridge,
UK, 2nd edition.
Emile de Maat and Radboud Winkels. 2010. Auto-
mated classification of norms in sources of law. In
Semantic Processing of Legal Texts. Springer, Berlin.
Karin M. Eichhoff-Cyrus and Gerd Antos, editors.
2008. Verst?ndlichkeit als B?rgerrecht? Die Rechts-
und Verwaltungssprache in der ?ffentlichen Diskus-
sion. Duden, Mannheim, Germany.
Peter Eisenberg. 2007. Die Grammatik der Gesetzes-
sprache: Was ist eine Verbesserung? In Andreas
L?tscher and Markus Nussbaumer, editors, Denken
wie ein Philosoph und schreiben wie ein Bauer,
pages 105?122. Schulthess, Z?rich.
Europ?ische Kommission, editor. 2003. Gemein-
samer Leitfaden des Europ?ischen Parlaments, des
Rates und der Kommission f?r Personen, die in den
Gemeinschaftsorganen an der Abfassung von Rechts-
texten mitwirken. Amt f?r Ver?ffentlichungen der
Europ?ischen Gemeinschaften, Luxemburg.
Stephanie Geldbach. 2009. Neue Werkzeuge zur Au-
torenunterst?tzung. MD?, 4:10?19.
Mariikka Haapalainen and Ari Majorin. 1995. GER-
TWOL und morphologische Desambiguierung f?r
das Deutsche. In Proceedings of the 10th Nordic
Conference of Computational Linguistics. University
of Helsinki, Department of General Linguistics.
Stefan H?fler and Michael Piotrowski. 2011. Build-
ing corpora for the philological study of Swiss legal
texts. Journal for Language Technology and Com-
putational Linguistics (JLCL), 26(2):77?90.
Stefan H?fler, Alexandra B?nzli, and Kyoko Sugisaki.
2011. Detecting legal definitions for automated
style checking in draft laws. Technical Report CL-
2011.01, University of Zurich, Institute of Computa-
tional Linguistics, Z?rich.
Stefan H?fler. 2011. ?Ein Satz ? eine Aussage.? Multi-
propositionale Rechtss?tze an der Sprache erkennen.
LeGes, 22(2):259?279.
Anne Lehrndorfer. 1996. Kontrolliertes Deutsch: Lin-
guistische und sprachpsychologische Leitlinien f?r
eine (maschinell) kontrollierte Sprache in der Tech-
nischen Dokumentation. G?nter Narr, T?bingen.
Kent D. Lerch, editor. 2004. Recht verstehen.
Verst?ndlichkeit, Missverst?ndlichkeit und Unver-
st?ndlichkeit von Recht. de Gruyter, Berlin.
Maria Mindlin. 2005. Is plain language better? A com-
parative readability study of plain language court
forms. Scribes Journal of Legal Writing, 10.
Uwe Muegge. 2007. Controlled language: The next
big thing in translation? ClientSide News Magazine,
7(7):21?24.
Markus Nussbaumer. 2009. Rhetorisch-stilistische
Eigenschaften der Sprache des Rechtswesens. In
Ulla Fix, Andreas Gardt, and Joachim Knape, ed-
itors, Rhetorik und Stilistik/Rhetoric and Stylis-
tics, Handbooks of Linguistics and Communica-
tion Science, pages 2132?2150. de Gruyter, New
York/Berlin.
Rechtsdienst der Regierung, editor. 1990. Richtli-
nien der Regierung des F?rstentums Liechtenstein
?ber die Grunds?tze der Rechtsetzung (Legistische
Richtlinien). Vaduz.
Regierungsrat des Kantons Bern, editor. 2000. Recht-
setzungsrichtlinien des Kantons Bern. Bern.
Regierungsrat des Kantons Z?rich, editor. 2005.
Richtlinien der Rechtsetzung. Z?rich.
Ursula Reuther. 2003. Two in one ? can it work? Read-
ability and translatability by means of controlled
language. In Proceedings of EAMT-CLAW 2003.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49.
Rico Sennrich, Gerold Schneider, Martin Volk, and
Martin Warin. 2009. A new hybrid dependency
parser for German. In Proceedings of the GSCL
Conference 2009, pages 115?124, T?bingen.
Giulia Venturi. 2008. Parsing legal texts: A contrastive
study with a view to knowledge managment applica-
tions. In Proceedings of the LREC 2008 Workshop
on Semantic Processing of Legal Texts, pages 1?10,
Marakesh.
Stephan Walter and Manfred Pinkal. 2009. Defini-
tions in court decisions: Automatic extraction and
ontology acquisition. In Joost Breuker, Pompeu
Casanovas, Michel Klein, and Enrico Francesconi,
editors, Law, Ontologies and the Semantic Web. IOS
Press, Amsterdam.
Richard C. Wydick. 2005. Plain English for Lawyers.
Carolina Academic Press, 5th edition.
18
