c? 2002 Association for Computational Linguistics
Automatic Labeling of Semantic Roles
Daniel Gildea? Daniel Jurafsky?
University of California, Berkeley, and
International Computer Science Institute
University of Colorado, Boulder
We present a system for identifying the semantic relationships, or semantic roles, filled by
constituents of a sentence within a semantic frame. Given an input sentence and a target word
and frame, the system labels constituents with either abstract semantic roles, such as Agent or
Patient, or more domain-specific semantic roles, such as Speaker,Message, and Topic.
The system is based on statistical classifiers trained on roughly 50,000 sentences that were
hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed
each training sentence into a syntactic tree and extracted various lexical and syntactic features,
including the phrase type of each constituent, its grammatical function, and its position in the
sentence. These features were combined with knowledge of the predicate verb, noun, or adjective,
as well as information such as the prior probabilities of various combinations of semantic roles.
We used various lexical clustering algorithms to generalize across possible fillers of roles. Test
sentences were parsed, were annotated with these features, and were then passed through the
classifiers.
Our system achieves 82% accuracy in identifying the semantic role of presegmented con-
stituents. At the more difficult task of simultaneously segmenting constituents and identifying
their semantic role, the system achieved 65% precision and 61% recall.
Our study also allowed us to compare the usefulness of different features and feature combi-
nation methods in the semantic role labeling task. We also explore the integration of role labeling
with statistical syntactic parsing and attempt to generalize to predicates unseen in the training
data.
1. Introduction
Recent years have been exhilarating ones for natural language understanding. The
excitement and rapid advances that had characterized other language-processing tasks
such as speech recognition, part-of-speech tagging, and parsing have finally begun to
appear in tasks in which understanding and semantics play a greater role. For example,
there has been widespread commercial deployment of simple speech-based natural
language understanding systems that answer questions about flight arrival times, give
directions, report on bank balances, or perform simple financial transactions. More
sophisticated research systems generate concise summaries of news articles, answer
fact-based questions, and recognize complex semantic and dialogue structure.
But the challenges that lie ahead are still similar to the challenge that the field
has faced since Winograd (1972): moving away from carefully hand-crafted, domain-
dependent systems toward robustness and domain independence. This goal is not as
? Currently at Institute for Research in Cognitive Science, University of Pennsylvania, 3401 Walnut
Street, Suite 400A, Philadelphia, PA 19104. E-mail: dgildea@cis.upenn.edu
? Departments of Linguistics and Computer Science, University of Colorado, Boulder, CO 80309. E-mail:
jurafsky@colorado.edu
246
Computational Linguistics Volume 28, Number 3
far away as it once was, thanks to the development of large semantic databases such
as WordNet (Fellbaum 1998) and progress in domain-independent machine learning
algorithms.
Current information extraction and dialogue understanding systems, however, are
still based on domain-specific frame-and-slot templates. Systems for booking airplane
information use domain-specific frames with slots like orig city, dest city, or de-
part time (Stallard 2000). Systems for studying mergers and acquisitions use slots
like products, relationship, joint venture company, and amount (Hobbs et al
1997). For natural language understanding tasks to proceed beyond these specific do-
mains, we need semantic frames and semantic understanding systems that do not
require a new set of slots for each new application domain.
In this article we describe a shallow semantic interpreter based on semantic roles
that are less domain specific than to airport or joint venture company. These
roles are defined at the level of semantic frames of the type introduced by Fillmore
(1976), which describe abstract actions or relationships, along with their participants.
For example, the Judgement frame contains roles like judge, evaluee, and reason,
and the Statement frame contains roles like speaker, addressee, and message, as
the following examples show:
(1) [Judge She ] blames [Evaluee the Government ] [Reason for failing to do
enough to help ] .
(2) [Message ?I?ll knock on your door at quarter to six? ] [Speaker Susan] said.
These shallow semantic roles could play an important role in information extrac-
tion. For example, a semantic role parse would allow a system to realize that the ruling
that is the direct object of change in (3) plays the same Theme role as the ruling that
is the subject of change in (4):
(3) The canvassing board changed its ruling on Wednesday.
(4) The ruling changed because of the protests.
The fact that semantic roles are defined at the frame level means, for example, that
the verbs send and receive would share the semantic roles (sender, recipient, goods,
etc.) defined with respect to a common Transfer frame. Such common frames might
allow a question-answering system to take a question like (5) and discover that (6) is
relevant in constructing an answer to the question:
(5) Which party sent absentee ballots to voters?
(6) Both Democratic and Republican voters received absentee ballots from
their party.
This shallow semantic level of interpretation has additional uses outside of gen-
eralizing information extraction, question answering, and semantic dialogue systems.
One such application is in word sense disambiguation, where the roles associated with
a word can be cues to its sense. For example, Lapata and Brew (1999) and others have
shown that the different syntactic subcategorization frames of a verb such as serve can
be used to help disambiguate a particular instance of the word. Adding semantic role
subcategorization information to this syntactic information could extend this idea to
247
Gildea and Jurafsky Automatic Labeling of Semantic Roles
use richer semantic knowledge. Semantic roles could also act as an important interme-
diate representation in statistical machine translation or automatic text summarization
and in the emerging field of text data mining (TDM) (Hearst 1999). Finally, incorpo-
rating semantic roles into probabilistic models of language may eventually yield more
accurate parsers and better language models for speech recognition.
This article describes an algorithm for identifying the semantic roles filled by con-
stituents in a sentence. We apply statistical techniques that have been successful for the
related problems of syntactic parsing, part-of-speech tagging, and word sense disam-
biguation, including probabilistic parsing and statistical classification. Our statistical
algorithms are trained on a hand-labeled data set: the FrameNet database (Baker, Fill-
more, and Lowe 1998; Johnson et al 2001). The FrameNet database defines a tag set
of semantic roles called frame elements and included, at the time of our experiments,
roughly 50,000 sentences from the British National Corpus hand-labeled with these
frame elements.
This article presents our system in stages, beginning in Section 2 with a more de-
tailed description of the data and the set of frame elements or semantic roles used. We
then introduce (in Section 3) the statistical classification technique used and examine
in turn the knowledge sources of which our system makes use. Section 4 describes
the basic syntactic and lexical features used by our system, which are derived from
a Penn Treebank?style parse of individual sentences to be analyzed. We break our
task into two subproblems: finding the relevant sentence constituents (deferred until
Section 5), and giving them the correct semantic labels (Sections 4.2 and 4.3). Section 6
adds higher-level semantic knowledge to the system, attempting to model the selec-
tional restrictions on role fillers not directly captured by lexical statistics. We compare
hand-built and automatically derived resources for providing this information. Sec-
tion 7 examines techniques for adding knowledge about systematic alternations in
verb argument structure with sentence-level features. We combine syntactic parsing
and semantic role identification into a single probability model in Section 8. Section 9
addresses the question of generalizing statistics from one target predicate to another,
beginning with a look at domain-independent thematic roles in Section 9.1. Finally we
draw conclusions and discuss future directions in Section 10.
2. Semantic Roles
Semantic roles are one of the oldest classes of constructs in linguistic theory, dating
back thousands of years to Panini?s ka?raka theory (Misra 1966; Rocher 1964; Dahiya
1995). Longevity, in this case, begets variety, and the literature records scores of pro-
posals for sets of semantic roles. These sets of roles range from the very specific to
the very general, and many have been used in computational implementations of one
type or another.
At the specific end of the spectrum are domain-specific roles such as the from air-
port, to airport, or depart time discussed above, or verb-specific roles such as
eater and eaten for the verb eat. The opposite end of the spectrum consists of theo-
ries with only two ?proto-roles? or ?macroroles?: Proto-Agent and Proto-Patient
(Van Valin 1993; Dowty 1991). In between lie many theories with approximately 10
roles, such as Fillmore?s (1971) list of nine: Agent, Experiencer, Instrument, Ob-
ject, Source, Goal, Location, Time, and Path.1
1 There are scores of other theories with slightly different sets of roles, including those of Fillmore (1968),
Jackendoff (1972), and Schank (1972); see Somers (1987) for an excellent summary.
248
Computational Linguistics Volume 28, Number 3
banter?v
debate?v
converse?v
gossip?v
dispute?n
discussion?n
tiff?n
ConversationFrame:
Protagonist?1
Protagonist?2
Protagonists
Topic
Medium
Frame Elements:
argue?v
Domain: Communication Domain: Cognition
Frame: Questioning
Topic
Medium
Frame Elements: Speaker
Addressee
Message
Frame:
Topic
Medium
Frame Elements: Speaker
Addressee
Message
Statement
Frame:
Frame Elements:
Judgment
Judge
Evaluee
Reason
Role
dispute?n
blame?v fault?n
admire?v
admiration?n disapprove?v
blame?n
appreciate?v
Frame:
Frame Elements:
Categorization
Cognizer
Item
Category
Criterion
Figure 1
Sample domains and frames from the FrameNet lexicon.
Many of these sets of roles have been proposed by linguists as part of theories
of linking, the part of grammatical theory that describes the relationship between
semantic roles and their syntactic realization. Other sets have been used by computer
scientists in implementing natural language understanding systems. As a rule, the
more abstract roles have been proposed by linguists, who are more concerned with
explaining generalizations across verbs in the syntactic realization of their arguments,
whereas the more specific roles have more often been proposed by computer scientists,
who are more concerned with the details of the realization of the arguments of specific
verbs.
The FrameNet project (Baker, Fillmore, and Lowe 1998) proposes roles that are
neither as general as the 10 abstract thematic roles, nor as specific as the thousands of
potential verb-specific roles. FrameNet roles are defined for each semantic frame. A
frame is a schematic representation of situations involving various participants, props,
and other conceptual roles (Fillmore 1976). For example, the frame Conversation,
shown in Figure 1, is invoked by the semantically related verbs argue, banter, debate,
converse, and gossip, as well as the nouns dispute, discussion, and tiff, and is defined as
follows:
(7) Two (or more) people talk to one another. No person is construed as
only a speaker or only an addressee. Rather, it is understood that both
(or all) participants do some speaking and some listening: the process is
understood to be symmetrical or reciprocal.
The roles defined for this frame, and shared by all its lexical entries, include
Protagonist-1 and Protagonist-2 or simply Protagonists for the participants
in the conversation, as well as Medium and Topic. Similarly, the Judgment frame
mentioned above has the roles Judge, Evaluee, and Reason and is invoked by verbs
such as blame, admire, and praise and nouns such as fault and admiration. We refer to
the roles for a given frame as frame elements. A number of hand-annotated exam-
ples from the Judgment frame are included below to give a flavor of the FrameNet
database:
(8) [Judge She ] blames [Evaluee the Government ] [Reason for failing to do
enough to help ] .
(9) Holman would characterise this as blaming [Evaluee the poor ] .
249
Gildea and Jurafsky Automatic Labeling of Semantic Roles
(10) The letter quotes Black as saying that [Judge white and Navajo ranchers ]
misrepresent their livestock losses and blame [Reason everything ] [Evaluee
on coyotes ] .
(11) The only dish she made that we could tolerate was [Evaluee syrup tart
which2 ] [Judge we ] praised extravagantly with the result that it became
our unhealthy staple diet.
(12) I?m bound to say that I meet a lot of [Judge people who ] praise [Evaluee
me ] [Reason for speaking up ] but don?t speak up themselves.
(13) Specimens of her verse translations of Tasso (Jerusalem Delivered) and
Verri (Roman Nights) circulated to [ Manner warm ] [Judge critical ] praise;
but ?unforeseen circumstance? prevented their publication.
(14) And if Sam Snort hails Doyler as monumental is he perhaps erring on
the side of being excessive in [ Judge his ] praise?
Defining semantic roles at this intermediate frame level helps avoid some of
the well-known difficulties of defining a unique small set of universal, abstract the-
matic roles while also allowing some generalization across the roles of different verbs,
nouns, and adjectives, each of which adds semantics to the general frame or high-
lights a particular aspect of the frame. One way of thinking about traditional ab-
stract thematic roles, such as Agent and Patient, in the context of FrameNet is to
conceive them as frame elements defined by abstract frames, such as action and mo-
tion, at the top of an inheritance hierarchy of semantic frames (Fillmore and Baker
2000).
The examples above illustrate another difference between frame elements and
thematic roles as commonly described in the literature. Whereas thematic roles tend
to be arguments mainly of verbs, frame elements can be arguments of any predicate,
and the FrameNet database thus includes nouns and adjectives as well as verbs.
The examples above also illustrate a few of the phenomena that make it hard to
identify frame elements automatically. Many of these are caused by the fact that there
is not always a direct correspondence between syntax and semantics. Whereas the
subject of blame is often the Judge, the direct object of blame can be an Evaluee (e.g.,
the poor in ?blaming the poor?) or a Reason (e.g., everything in ?blame everything
on coyotes?). The identity of the Judge can also be expressed in a genitive pronoun,
(e.g., his in ?his praise?) or even an adjective (e.g., critical in ?critical praise?).
The corpus used in this project is perhaps best described in terms of the method-
ology used by the FrameNet team. We outline the process here; for more detail see
Johnson et al (2001). As the first step, semantic frames were defined for the general
domains chosen; the frame elements, or semantic roles for participants in a frame,
were defined; and a list of target words, or lexical predicates whose meaning includes
aspects of the frame, was compiled for each frame. Example sentences were chosen
by searching the British National Corpus for instances of each target word. Separate
searches were performed for various patterns over lexical items and part-of-speech
sequences in the target words? context, producing a set of subcorpora for each tar-
get word, designed to capture different argument structures and ensure that some
examples of each possible syntactic usage of the target word would be included in
2 The FrameNet annotation includes both the relative pronoun and its antecedent in the target word?s
clause.
250
Computational Linguistics Volume 28, Number 3
the final database. Thus, the focus of the project was on completeness of examples for
lexicographic needs, rather than on statistically representative data. Sentences from
each subcorpus were then annotated by hand, marking boundaries of each frame el-
ement expressed in the sentence and assigning tags for the annotated constituent?s
frame semantic role, syntactic category (e.g., noun phrase or prepositional phrase),
and grammatical function in relation to the target word (e.g., object or complement
of a verb). In the final phase of the process, the annotated sentences for each tar-
get word were checked for consistency. In addition to the tags just mentioned, the
annotations include certain other information, which we do not make use of in this
work, such as word sense tags for some target words and tags indicating metaphoric
usages.
Tests of interannotator agreement were performed for data from a small num-
ber of predicates before the final consistency check. Interannotator agreement at the
sentence level, including all frame element judgments and boundaries for one predi-
cate, varied from .66 to .82 depending on the predicate. The kappa statistic (Siegel
and Castellan 1988) varied from .67 to .82. Because of the large number of pos-
sible categories when boundary judgments are considered, kappa is nearly iden-
tical to the interannotator agreement. The system described in this article (which
gets .65/.61 precision/recall on individual frame elements; see Table 15) correctly
identifies all frame elements in 38% of test sentences. Although this .38 is not di-
rectly comparable to the .66?.82 interannotator agreements, it?s clear that the per-
formance of our system still falls significantly short of human performance on the
task.
The British National Corpus was chosen as the basis of the FrameNet project
despite differences between British and American usage because, at 100 million words,
it provides the largest corpus of English with a balanced mixture of text genres. The
British National Corpus includes automatically assigned syntactic part-of-speech tags
for each word but does not include full syntactic parses. The FrameNet annotators did
not make use of, or produce, a complete syntactic parse of the annotated sentences,
although some syntactic information is provided by the grammatical function and
phrase type tags of the annotated frame elements.
The preliminary version of the FrameNet corpus used for our experiments con-
tained 67 frame types from 12 general semantic domains chosen for annotation. A
complete list of the semantic domains represented in our data is shown in Table 1,
along with representative frames and predicates. Within these frames, examples of a
total of 1,462 distinct lexical predicates, or target words, were annotated: 927 verbs,
339 nouns, and 175 adjectives. There are a total of 49,013 annotated sentences and
99,232 annotated frame elements (which do not include the target words them-
selves).
How important is the particular set of semantic roles that underlies our sys-
tem? For example, could the optimal choice of semantic roles be very dependent on
the application that needs to exploit their information? Although there may well be
application-specific constraints on semantic roles, our semantic role classifiers seem
in practice to be relatively independent of the exact set of semantic roles under con-
sideration. Section 9.1 describes an experiment in which we collapsed the FrameNet
roles into a set of 18 abstract thematic roles. We then retrained our classifier and
achieved roughly comparable results; overall performance was 82.1% for abstract the-
matic roles, compared to 80.4% for frame-specific roles. Although this doesn?t show
that the detailed set of semantic roles is irrelevant, it does suggest that our statistical
classification algorithm, at least, is relatively robust to even quite large changes in role
identities.
251
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Table 1
Semantic domains with sample frames and predicates from the FrameNet lexicon.
Domain Sample Frames Sample Predicates
Body Action flutter, wink
Cognition Awareness attention, obvious
Judgment blame, judge
Invention coin, contrive
Communication Conversation bicker, confer
Manner lisp, rant
Emotion Directed angry, pleased
Experiencer-Obj bewitch, rile
General Imitation bogus, forge
Health Response allergic, susceptible
Motion Arriving enter, visit
Filling annoint, pack
Perception Active glance, savour
Noise snort, whine
Society Leadership emperor, sultan
Space Adornment cloak, line
Time Duration chronic, short
Iteration daily, sporadic
Transaction Basic buy, spend
Wealthiness broke, well-off
3. Related Work
Assignment of semantic roles is an important part of language understanding, and
the problem of how to assign such roles has been attacked by many computational
systems. Traditional parsing and understanding systems, including implementations of
unification-based grammars such as Head-Driven Phrase Structure Grammar (HPSG)
(Pollard and Sag 1994), rely on hand-developed grammars that must anticipate each
way in which semantic roles may be realized syntactically. Writing such grammars is
time consuming, and typically such systems have limited coverage.
Data-driven techniques have recently been applied to template-based semantic
interpretation in limited domains by ?shallow? systems that avoid complex feature
structures and often perform only shallow syntactic analysis. For example, in the
context of the Air Traveler Information System (ATIS) for spoken dialogue, Miller et
al. (1996) computed the probability that a constituent such as Atlanta filled a semantic
slot such as Destination in a semantic frame for air travel. In a data-driven approach
to information extraction, Riloff (1993) builds a dictionary of patterns for filling slots
in a specific domain such as terrorist attacks, and Riloff and Schmelzenbach (1998)
extend this technique to derive automatically entire ?case frames? for words in the
domain. These last systems make use of a limited amount of hand labor to accept or
reject automatically generated hypotheses. They show promise for a more sophisticated
approach to generalizing beyond the relatively small number of frames considered in
the tasks. More recently, a domain-independent system has been trained by Blaheta
and Charniak (2000) on the function tags, such as Manner and Temporal, included
in the Penn Treebank corpus. Some of these tags correspond to FrameNet semantic
roles, but the Treebank tags do not include all the arguments of most predicates. In this
article, we aim to develop a statistical system for automatically learning to identify all
semantic roles for a wide variety of predicates in unrestricted text.
252
Computational Linguistics Volume 28, Number 3
4. Probability Estimation for Roles
In this section we describe the first, basic version of our statistically trained system
for automatically identifying frame elements in text. The system will be extended in
later sections. We first describe in detail the sentence- and constituent-level features
on which our system is based and then use these features to calculate probabilities
for predicting frame element labels in Section 4.2. In this section we give results for
a system that labels roles using the human-annotated boundaries for the frame ele-
ments within the sentence; we return to the question of automatically identifying the
boundaries in Section 5.
4.1 Features Used in Assigning Semantic Roles
Our system is a statistical one, based on training a classifier on a labeled training set
and testing on a held-out portion of the data. The system is trained by first using an
automatic syntactic parser to analyze the 36,995 training sentences, matching annotated
frame elements to parse constituents and extracting various features from the string
of words and the parse tree. During testing, the parser is run on the test sentences
and the same features are extracted. Probabilities for each possible semantic role r are
then computed from the features. The probability computation is described in the next
section; here we discuss the features used.
The features used represent various aspects of the syntactic structure of the sen-
tence as well as lexical information. The relationship between such surface manifes-
tations and semantic roles is the subject of linking theory (see Levin and Rappaport
Hovav [1996] for a synthesis of work in this area). In general, linking theory argues
that the syntactic realization of arguments of a predicate is predictable from semantics;
exactly how this relationship works, however, is the subject of much debate. Regardless
of the underlying mechanisms used to generate syntax from semantics, the relation-
ship between the two suggests that it may be possible to learn to recognize semantic
relationships from syntactic cues, given examples with both types of information.
4.1.1 Phrase Type. Different semantic roles tend to be realized by different syntactic
categories. For example, in communication frames, the Speaker is likely to appear
as a noun phrase, Topic as a prepositional phrase or noun phrase, and Medium as a
prepositional phrase, as in: ?[Speaker We ] talked [Topic about the proposal ] [Medium over
the phone ] .?
The phrase type feature we used indicates the syntactic category of the phrase
expressing the semantic roles, using the set of syntactic categories of the Penn Treebank
project, as described in Marcus, Santorini, and Marcinkiewicz (1993). In our data, frame
elements are most commonly expressed as noun phrases (NPs, 47% of frame elements
in the training set), and prepositional phrases (PPs, 22%). The next most common
categories are adverbial phrases (ADVPs, 4%), particles (e.g. ?make something up?;
PRTs, 2%) and clauses (SBARs, 2%, and Ss, 2%). (Tables 22 and 23 in the Appendix
provides a listing of Penn Treebank?s part-of-speech tags and constituent labels.)
We used Collins? (1997) statistical parser trained on examples from the Penn Tree-
bank to generate parses of the same format for the sentences in our data. Phrase types
were derived automatically from parse trees generated by the parser, as shown in Fig-
ure 2. Given the automatically generated parse tree, the constituent spanning the same
set of words as each annotated frame element was found, and the constituent?s nonter-
minal label was taken as the phrase type. In cases in which more than one constituent
matches because of a unary production in the parse tree, the higher constituent was
chosen.
253
Gildea and Jurafsky Automatic Labeling of Semantic Roles
He
PRP
NP
heard
VBD
the sound of liquid slurping in a metal container
NP
as
IN
Farrell
NNP
NP
approached
VBD
him
PRP
NP
from
IN
behind
NN
NP
PP
VP
S
SBAR
VP
S
target SourceGoalTheme
Figure 2
A sample sentence with parser output (above) and FrameNet annotation (below). Parse
constituents corresponding to frame elements are highlighted.
The matching was performed by calculating the starting and ending word po-
sitions for each constituent in the parse tree, as well as for each annotated frame
element, and matching each frame element with the parse constituent with the same
beginning and ending points. Punctuation was ignored in this computation. Because
of parsing errors, or, less frequently, mismatches between the parse tree formalism and
the FrameNet annotation standards, for 13% of the frame elements in the training set,
there was no parse constituent matching an annotated frame element. The one case of
systematic mismatch between the parse tree formalism and the FrameNet annotation
standards is the FrameNet convention of including both a relative pronoun and its
antecedent in frame elements, as in the first frame element in the following sentence:
(15) In its rough state he showed it to [Agt the Professor, who ] bent [BPrt his
grey beard ] [Path over the neat script ] and read for some time in silence.
Mismatch caused by the treatment of relative pronouns accounts for 1% of the frame
elements in the training set.
During testing, the largest constituent beginning at the frame element?s left bound-
ary and lying entirely within the element was used to calculate the frame element?s
features. We did not use this technique on the training set, as we expected that it
would add noise to the data, but instead discarded examples with no matching parse
constituent. Our technique for finding a near match handles common parse errors
such as a prepositional phrase being incorrectly attached to a noun phrase at the
right-hand edge, and it guarantees that some syntactic category will be returned: the
part-of-speech tag of the frame element?s first word in the limiting case.
4.1.2 Governing Category. The correlation between semantic roles and syntactic re-
alization as subject or direct object is one of the primary facts that linking theory at-
tempts to explain. It was a motivation for the case hierarchy of Fillmore (1968), which
254
Computational Linguistics Volume 28, Number 3
allowed such rules as ?If there is an underlying Agent, it becomes the syntactic sub-
ject.? Similarly, in his theory of macroroles, Van Valin (1993) describes the Actor as
being preferred in English for the subject. Functional grammarians consider syntactic
subjects historically to have been grammaticalized agent markers. As an example of
how such a feature can be useful, in the sentence ?He drove the car over the cliff,?
the subject NP is more likely to fill the Agent role than the other two NPs. We will
discuss various grammatical-function features that attempt to indicate a constituent?s
syntactic relation to the rest of the sentence, for example, as a subject or object of a
verb.
The first such feature, which we call ?governing category,? or gov, has only two
values, S and VP, corresponding to subjects and objects of verbs, respectively. This
feature is restricted to apply only to NPs, as it was found to have little effect on other
phrase types. As with phrase type, the feature was read from parse trees returned by
the parser. We follow links from child to parent up the parse tree from the constituent
corresponding to a frame element until either an S or VP node is found and assign
the value of the feature according to whether this node is an S or a VP. NP nodes
found under S nodes are generally grammatical subjects, and NP nodes under VP
nodes are generally objects. In most cases the S or VP node determining the value of
this feature immediately dominates the NP node, but attachment errors by the parser
or constructions such as conjunction of two NPs can cause intermediate nodes to
be introduced. Searching for higher ancestor nodes makes the feature robust to such
cases. Even given good parses, this feature is not perfect in discriminating grammatical
functions, and in particular it confuses direct objects with adjunct NPs such as temporal
phrases. For example, town in the sentence ?He left town? and yesterday in the sentence
?He left yesterday? will both be assigned a governing category of VP. Direct and
indirect objects both appear directly under the VP node. For example, in the sentence
?He gave me a new hose,? me and a new hose are both assigned a governing category
of VP. More sophisticated handling of such cases could improve our system.
4.1.3 Parse Tree Path. Like the governing-category feature described above, the parse
tree path feature (path) is designed to capture the syntactic relation of a constituent
to the rest of the sentence. The path feature, however, describes the syntactic relation
between the target word (that is, the predicate invoking the semantic frame) and the
constituent in question, whereas the gov feature is independent of where the target
word appears in the sentence; that is, it identifies all subjects whether they are the
subject of the target word or not.
The path feature is defined as the path from the target word through the parse
tree to the constituent in question, represented as a string of parse tree nonterminals
linked by symbols indicating upward or downward movement through the tree, as
shown in Figure 3. Although the path is composed as a string of symbols, our system
treats the string as an atomic value. The path includes, as the first element of the
string, the part of speech of the target word and, as the last element, the phrase type
or syntactic category of the sentence constituent marked as a frame element. After
some experimentation, we settled on a version of the path feature that collapses the
various part-of-speech tags for verbs, including past-tense verb (VBD), third-person
singular present-tense verb (VBZ), other present-tense verb (VBP), and past participle
(VBN), into a single verb tag denoted ?VB.?
Our path feature is dependent on the syntactic representation used, which in our
case is the Treebank-2 annotation style (Marcus et al 1994), as our parser is trained
on this later version of the Treebank data. Figure 4 shows the annotation for the
sentence ?They expect him to cut costs throughout the organization,? which exhibits
255
Gildea and Jurafsky Automatic Labeling of Semantic Roles
S
NP VP
NP
He ate some pancakes
PRP
DT NN
VB
Figure 3
In this example, the path from the target word ate to the frame element He can be represented
as VB?VP?S?NP, with ? indicating upward movement in the parse tree and ? downward
movement. The NP corresponding to He is found as described in Section 4.1.1.
Figure 4
Treebank annotation of raising constructions.
the syntactic phenomenon known as subject-to-object raising, in which the main verb?s
object is interpreted as the embedded verb?s subject. The Treebank-2 style tends to be
generous in its usage of S nodes to indicate clauses, a decision intended to make
possible a relatively straightforward mapping from S nodes to predications. In this
example, the path from cut to the frame element him would be VB?VP?VP?S?NP,
which typically indicates a verb?s subject, despite the accusative case of the pronoun
him. For the target word of expect in the sentence of Figure 4, the path to him would
be VB?VP?S?NP, rather than the typical direct-object path of VB?VP?NP.
An example of Treebank-2 annotation of an ?equi? construction, in which a noun
phrase serves as an argument of both the main and subordinate verbs, is shown in
Figure 5. Here, an empty category is used in the subject position of the subordinate
clause and is co-indexed with the NP Congress in the direct-object position of the main
clause. The empty category, however, is not used in the statistical model of the parser
or shown in its output and is also not used by the FrameNet annotation, which would
mark the NP Congress as a frame element of raise in this example. Thus, the value
of our path feature from the target word raise to the frame element Congress would
256
Computational Linguistics Volume 28, Number 3
Figure 5
Treebank annotation of equi constructions. An empty category is indicated by an asterisk, and
co-indexing by superscript numeral.
Table 2
Most frequent values of the path feature in the training data.
Frequency Path Description
14.2% VB?VP?PP PP argument/adjunct
11.8 VB?VP?S?NP Subject
10.1 VB?VP?NP Object
7.9 VB?VP?VP?S?NP Subject (embedded VP)
4.1 VB?VP?ADVP Adverbial adjunct
3.0 NN?NP?NP?PP Prepositional complement of noun
1.7 VB?VP?PRT Adverbial particle
1.6 VB?VP?VP?VP?S?NP Subject (embedded VP)
14.2 No matching parse constituent
31.4 Other
be VB?VP?VP?S?VP?NP, and from the target word of persuaded the path to Congress
would be the standard direct-object path VB?VP?NP.
Other changes in annotation style from the original Treebank style were specifi-
cally intended to make predicate argument structure easy to read from the parse trees
and include new empty (or null) constituents, co-indexing relations between nodes,
and secondary functional tags such as subject and temporal. Our parser output, how-
ever, does not include this additional information, but rather simply gives trees of
phrase type categories. The sentence in Figure 4 is one example of how the change in
annotation style of Treebank-2 can affect this level of representation; the earlier style
assigned the word him an NP node directly under the VP of expect.
The most common values of the path feature, along with interpretations, are shown
in Table 2.
For the purposes of choosing a frame element label for a constituent, the path
feature is similar to the gov feature defined above. Because the path captures more
information than the governing category, it may be more susceptible to parser errors
and data sparseness. As an indication of this, our path feature takes on a total of
2,978 possible values in the training data when frame elements with no matching
257
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Figure 6
Example of target word renting in a small clause.
parse constituent are not counted and 4,086 possible values when paths are found to
the best-matching constituent in these cases. The governing-category feature, on the
other hand, which is defined only for NPs, has only two values (S, corresponding to
subjects, and VP, corresponding to objects). In cases in which the path feature includes
an S or VP ancestor of an NP node as part of the path to the target word, the gov
feature is a function of the path feature. This is the case most of the time, including for
our prototypical subject (VB?VP?S?NP) and object (VB?VP?NP) paths. Of the 35,138
frame elements identified as NPs by the parser, only 4% have a path feature that does
not include a VP or S ancestor. One such example is shown in Figure 6, where the
small clause ?the remainder renting . . .? has no S node, giving a path feature from
renting to the remainder of VB?VP?NP?NP. The value of the gov feature here is VP, as
the algorithm finds the VP of the sentence?s main clause as it follows parent links up
the tree. The feature is spurious in this case, because the main VP is not headed by,
or relevant to, the target word renting.
Systems based on the path and gov features are compared in Section 4.3. The
differences between the two are relatively small for the purpose of identifying semantic
roles when frame element boundaries are known. The path feature will, however, be
important in identifying which constituents are frame elements for a given target word,
as it gives us a way of navigating through the parse tree to find the frame elements
in the sentence.
4.1.4 Position. To overcome errors due to incorrect parses, as well as to see how
much can be done without parse trees, we introduced position as a feature. The position
feature simply indicates whether the constituent to be labeled occurs before or after the
predicate defining the semantic frame. We expected this feature to be highly correlated
with grammatical function, since subjects will generally appear before a verb and
objects after.
Although we do not have hand-checked parses against which to measure the per-
formance of the automatic parser on our corpus, the result that 13% of frame elements
have no matching parse constituent gives a rough idea of the parser?s accuracy. Al-
258
Computational Linguistics Volume 28, Number 3
most all of these cases in which no matching parse constituent was found are due to
parser error. Other parser errors include cases in which a constituent is found, but
with the incorrect label or internal structure. This result also considers only the indi-
vidual constituent representing the frame element: the parse for the rest of the sentence
may be incorrect, resulting in an incorrect value for the grammatical function features
described in the previous two sections. Collins (1997) reports 88% labeled precision
and recall on individual parse constituents on data from the Penn Treebank, roughly
consistent with our finding of at least 13% error.
4.1.5 Voice. The distinction between active and passive verbs plays an important role
in the connection between semantic role and grammatical function, since direct objects
of active verbs often correspond in semantic role to subjects of passive verbs. From
the parser output, verbs were classified as active or passive by building a set of 10
passive-identifying patterns. Each of the patterns requires both a passive auxiliary
(some form of to be or to get) and a past participle. Roughly 5% of the examples were
identified as passive uses.
4.1.6 Head Word. As previously noted, we expected lexical dependencies to be ex-
tremely important in labeling semantic roles, as indicated by their importance in re-
lated tasks such as parsing. Head words of noun phrases can be used to express
selectional restrictions on the semantic types of role fillers. For example, in a commu-
nication frame, noun phrases headed by Bill, brother, or he are more likely to be the
Speaker, whereas those headed by proposal, story, or question are more likely to be the
Topic. (We did not attempt to resolve pronoun references.)
Since the parser we used assigns each constituent a head word as an integral
part of the parsing model, we were able to read the head words of the constituents
from the parser output, employing the same set of rules for identifying the head child
of each constituent in the parse tree. The rules for assigning a head word are listed
in Collins (1999). Prepositions are considered to be the head words of prepositional
phrases. The rules for assigning head words do not attempt to distinguish between
cases in which the preposition expresses the semantic content of a role filler, such as
Path frame elements expressed by prepositional phrases headed by along, through, or
in, and cases in which the preposition might be considered to be purely a case marker,
as in most uses of of, where the semantic content of the role filler is expressed by
the preposition?s object. Complementizers are considered to be heads, meaning that
infinitive verb phrases are always headed by to and subordinate clauses such as in the
sentence ?I?m sure that he came? are headed by that.
4.2 Probability Estimation
For our experiments, we divided the FrameNet corpus as follows: one-tenth of the
annotated sentences for each target word were reserved as a test set, and another one-
tenth were set aside as a tuning set for developing our system. A few target words
where fewer than 10 examples had been chosen for annotation were removed from the
corpus. (Section 9 will discuss generalization to unseen predicates.) In our corpus, the
average number of sentences per target word is only 34, and the number of sentences
per frame is 732, both relatively small amounts of data on which to train frame element
classifiers.
To label the semantic role of a constituent automatically, we wish to estimate a
probability distribution indicating how likely the constituent is to fill each possible
259
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Table 3
Distributions calculated for semantic role identification: r indicates semantic role, pt phrase
type, gov grammatical function, h head word, and t target word, or predicate.
Distribution Coverage Accuracy Performance
P(r | t) 100.0% 40.9% 40.9%
P(r | pt, t) 92.5 60.1 55.6
P(r | pt, gov, t) 92.0 66.6 61.3
P(r | pt, position, voice) 98.8 57.1 56.4
P(r | pt, position, voice, t) 90.8 70.1 63.7
P(r | h) 80.3 73.6 59.1
P(r | h, t) 56.0 86.6 48.5
P(r | h, pt, t) 50.1 87.4 43.8
role, given the features described above and the predicate, or target word, t:
P(r | h, pt , gov, position , voice, t)
where r indicates semantic role, h head word, and pt phrase type. It would be possible
to calculate this distribution directly from the training data by counting the number
of times each role appears with a combination of features and dividing by the total
number of times the combination of features appears:
P(r | h, pt , gov , position , voice, t) = #(r, h, pt , gov, position , voice, t)
#(h, pt , gov, position , voice, t)
In many cases, however, we will never have seen a particular combination of features
in the training data, and in others we will have seen the combination only a small
number of times, providing a poor estimate of the probability. The small number
of training sentences for each target word and the large number of values that the
head word feature in particular can take (any word in the language) contribute to the
sparsity of the data. Although we expect our features to interact in various ways, we
cannot train directly on the full feature set. For this reason, we built our classifier by
combining probabilities from distributions conditioned on a variety of subsets of the
features.
Table 3 shows the probability distributions used in the final version of the system.
Coverage indicates the percentage of the test data for which the conditioning event had
been seen in training data. Accuracy is the proportion of covered test data for which
the correct role is given the highest probability, and Performance, which is the product
of coverage and accuracy, is the overall percentage of test data for which the correct
role is predicted.3 Accuracy is somewhat similar to the familiar metric of precision
in that it is calculated over cases for which a decision is made, and performance is
similar to recall in that it is calculated over all true frame elements. Unlike in a tra-
ditional precision/recall trade-off, however, these results have no threshold to adjust,
and the task is a multiway classification rather than a binary decision. The distribu-
tions calculated were simply the empirical distributions from the training data. That is,
occurrences of each role and each set of conditioning events were counted in a table,
and probabilities calculated by dividing the counts for each role by the total number
3 Ties for the highest-probabilty role are resolved at random.
260
Computational Linguistics Volume 28, Number 3
Table 4
Sample probabilities for P(r | pt, gov, t) calculated from training data for the verb abduct. The
variable gov is defined only for noun phrases. The roles defined for the removing frame in the
motion domain are Agent (Agt), Theme (Thm), CoTheme (CoThm) (?. . . had been
abducted with him?), and Manner (Manr).
P(r | pt, gov, t) Count in training data
P(r = Agt | pt = NP, gov = S, t = abduct) = .46 6
P(r = Thm | pt = NP, gov = S, t = abduct) = .54 7
P(r = Thm | pt = NP, gov = VP, t = abduct) = 1 9
P(r = Agt | pt = PP, t = abduct) = .33 1
P(r = Thm | pt = PP, t = abduct) = .33 1
P(r = CoThm | pt = PP, t = abduct) = .33 1
P(r =Manr | pt = ADVP, t = abduct) = 1 1
of observations for each conditioning event. For example, the distribution P(r | pt, t)
was calculated as follows:
P(r | pt , t) = #(r, pt , t)
#(pt , t)
Some sample probabilities calculated from the training are shown in Table 4.
As can be seen from Table 3, there is a trade-off between more-specific distri-
butions, which have high accuracy but low coverage, and less-specific distributions,
which have low accuracy but high coverage. The lexical head word statistics, in par-
ticular, are valuable when data are available but are particularly sparse because of the
large number of possible head words.
To combine the strengths of the various distributions, we merged them in various
ways to obtain an estimate of the full distribution P(r | h, pt , gov, position , voice, t). The
first combination method is linear interpolation, which simply averages the probabil-
ities given by each of the distributions:
P(r | constituent) = ?1P(r | t) + ?2P(r | pt , t)
+ ?3P(r | pt , gov, t) + ?4P(r | pt , position , voice)
+ ?5P(r | pt , position , voice, t) + ?6P(r | h)
+ ?7P(r | h, t) + ?8P(r | h, pt , t)
where
?
i ?i = 1. The geometric mean, when expressed in the log domain, is similar:
P(r | constituent) = 1Z exp{ ?1 log P(r | t) + ?2 log P(r | pt , t)
+ ?3 log P(r | pt , gov, t) + ?4 log P(r | pt , position , voice)
+ ?5 log P(r | pt , position , voice, t) + ?6 log P(r | h)
+ ?7 log P(r | h, t) + ?8 log P(r | h, pt , t) }
where Z is a normalizing constant ensuring that
?
r P(r | constituent) = 1.
Results for systems based on linear interpolation are shown in the first row of
Table 5. These results were obtained using equal values of ? for each distribution
defined for the relevant conditioning event (but excluding distributions for which the
conditioning event was not seen in the training data). As a more sophisticated method
of choosing interpolation weights, the expectation maximization (EM) algorithm was
261
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Table 5
Results on development set, 8,167 observations.
Combining Method Correct
Equal linear interpolation 79.5%
EM linear interpolation 79.3
Geometric mean 79.6
Backoff, linear interpolation 80.4
Backoff, geometric mean 79.6
Baseline: Most common role 40.9
Table 6
Results on test set, 7,900 observations.
Combining Method Correct
EM linear interpolation 78.5%
Backoff, linear interpolation 76.9
Baseline: Most common role 40.6
used to estimate the likelihood of the observed role?s being produced by each of the
distributions in the general techniques of Jelinek and Mercer (1980). Because a number
of the distributions used may have no training data for a given set of variables, the
data were divided according to the set of distributions available, and a separate set of
interpolation weights was trained for each set of distributions. This technique (line 2
of Table 5) did not outperform equal weights even on the data used to determine
the weights. Although the EM algorithm is guaranteed to increase the likelihood of
the training data, that likelihood does not always correspond to our scoring, which is
based only on whether the correct outcome is assigned the highest probability. Results
of the EM interpolation on held-out test data are shown in Table 6.
Experimentation has shown that the weights used have relatively little impact in
our interpolation scheme, no doubt because the evaluation metric depends only on the
ranking of the probabilities and not on their exact values. Changing the interpolation
weights rarely changes the probabilities of the roles enough to change their ranking.
What matters most is whether a combination of variables has been seen in the training
data or not.
Results for the geometric mean are shown in row 3 of Table 5. As with linear
interpolation, the exact weights were found to have little effect, and the results shown
reflect equal weights. An area we have not explored is the use of the maximum-entropy
techniques of, for example, Pietra, Pietra, and Lafferty (1997), to set weights for the
log-linear model, either at the level of combining our probability distributions or at
the level of calculating weights for individual values of the features.
In the ?backoff? combination method, a lattice was constructed over the distribu-
tions in Table 3 from more-specific conditioning events to less-specific, as shown in
Figure 7. The lattice is used to select a subset of the available distributions to com-
bine. The less-specific distributions were used only when no data were present for any
more-specific distribution. Thus, the distributions selected are arranged in a cut across
the lattice representing the most-specific distributions for which data are available. The
selected probabilities were combined with both linear interpolation and a geometric
mean, with results shown in Table 5. The final row of the table represents the baseline
262
Computational Linguistics Volume 28, Number 3
P(r | h, t) P(r | pt, t) P(r | pt, position, voice)
P(r | pt, position, voice, t)P(r | pt, gf, t)
P(r | t)P(r | h)
P(r | h, pt, t)
Figure 7
Lattice organization of the distributions from Table 3, with more-specific distributions toward
the top.
of always selecting the most common role of the target word for all its constituents,
that is, using only P(r | t).
Although this lattice is reminiscent of techniques of backing off to less specific
distributions commonly used in n-gram language modeling, it differs in that we use the
lattice only to select distributions for which the conditioning event has been seen in the
training data. Discounting and deleted interpolation methods in language modeling
typically are used to assign small, nonzero probability to a predicted variable unseen in
the training data even when a specific conditioning event has been seen. In our case,
we are perfectly willing to assign zero probability to a specific role (the predicted
variable). We are interested only in finding the role with the highest probability, and
a role given a small, nonzero probability by smoothing techniques will still not be
chosen as the classifier?s output.
The lattice presented in Figure 7 represents just one way of choosing subsets of
features for our system. Designing a feature lattice can be thought of as choosing
a set of feature subsets: once the probability distributions of the lattice have been
chosen, the graph structure of the lattice is determined by the subsumption relations
among the sets of conditioning variables. Given a set of N conditioning variables,
there are 2N possible subsets, and 22
N
possible sets of subsets, giving us a doubly
exponential number of possible lattices. The particular lattice of Figure 7 was chosen to
represent some expected interaction between features. For example, we expect position
and voice to interact, and they are always used together. We expect the head word h
and the phrase type pt to be relatively independent predictors of the semantic role
and therefore include them separately as roots of the backoff structure. Although we
will not explore all the possibilities for our lattice, some of the feature interactions are
examined more closely in Section 4.3.
The final system performed at 80.4% accuracy, which can be compared to the 40.9%
achieved by always choosing the most probable role for each target word, essentially
chance performance on this task. Results for this system on test data, held out during
development of the system, are shown in Table 6. Surprisingly, the EM-based interpo-
lation performed better than the lattice-based system on the held-out test set, but not
on the data used to set the weights in the EM-based system. We return to an analysis
of which roles are hardest to classify in Section 9.1.
4.3 Interaction of Features
Three of our features, position, gov, and path, attempt to capture the syntactic relation
between the target word and the constituent to be labeled, and in particular to dif-
ferentiate the subjects from objects of verbs. To compare these three features directly,
experiments were performed using each feature alone in an otherwise identical sys-
263
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Table 7
Different estimators of grammatical function. The columns of the table correspond to
Figures 8a, 8b, and 8c.
Feature W/o voice Independent In conjunction
voice feature with voice
path 79.4% 79.2% 80.4%
gov 79.1 79.2 80.7
position 79.9 79.7 80.5
? 76.3 76.0 76.0
P(r | h, t) P(r | pt, t)
P(r | t)P(r | h)
P(r | h, pt, t) P(r | pt, GF, voice, t)
P(r | pt, GF, voice)
P(r | h, t) P(r | pt, t)
P(r | t)P(r | h)
P(r | h, pt, t) P(r | pt, voice, t)
P(r | pt, voice)
P(r | pt, GF, t)
P(r | h, t) P(r | pt, t)
P(r | t)P(r | h)
P(r | h, pt, t) P(r | pt, GF, t)a)
b)
c)
Figure 8
Lattice structures for comparing grammatical-function features.
tem. Results are shown in Table 7. For the first set of experiments, corresponding to
the first column of Table 7, no voice information was used, with the result that the
remaining distributions formed the lattice of Figure 8a. (?GF? (grammatical function)
in the figure represents one of the features position, gov, and path.) Adding voice infor-
mation back into the system independently of the grammatical-function feature results
264
Computational Linguistics Volume 28, Number 3
P(r | h, t)
P(r | t)
P(r | pt, path, t)
Figure 9
Minimal lattice.
in the lattice of Figure 8b, corresponding to the second column of Table 7. Choosing
distributions such that the grammatical function and voice features are always used
together results in Figure 8c, corresponding to the third column of Table 7. In each
case, as in previous results, the grammatical function feature was used only when
the candidate constituent was an NP. The last row of Table 7 shows results using no
grammatical-function feature: the distributions making use of GF are removed from
the lattices of Figure 8.
As a guideline for interpreting these results, with 8,167 observations, the threshold
for statistical significance with p < .05 is a 1.0% absolute difference in performance.
It is interesting to note that looking at a constituent?s position relative to the target
word performed as well as either of our features that read grammatical function off
the parse tree, both with and without passive information. The gov and path features
seem roughly equivalent in performance.
Using head word, phrase type, and target word without either position or gram-
matical function yielded only 76.3% accuracy, indicating that although the two features
accomplish a similar goal, it is important to include some measure of the constituent?s
relationship to the target word, whether relative position or either of the syntactic
features.
Use of the active/passive voice feature seems to be beneficial only when the feature
is tied to grammatical function: the second column in Table 7 shows no improvement
over the first, while the right-hand column, where grammatical function and voice are
tied, shows gains (although only trends) of at least 0.5% in all cases. As before, our
three indicators of grammatical function seem roughly equivalent, with the best result
in this case being the gov feature. The lattice of Figure 8c performs as well as our
system of Figure 7, indicating that including both position and either of the syntactic
relations is redundant.
As an experiment to see how much can be accomplished with as simple a system
as possible, we constructed the minimal lattice of Figure 9, which includes just two
distributions, along with a prior for the target word to be used as a last resort when no
data are available. This structure assumes that head word and grammatical function
are independent. It further makes no use of the voice feature. We chose the path feature
as the representation of grammatical function in this case. This system classified 76.3%
of frame elements correctly, indicating that one can obtain roughly nine-tenths the
performance of the full system with a simple approach. (We will return to a similar
system for the purposes of cross-domain experiments in Section 9.)
5. Identification of Frame Element Boundaries
In this section we examine the system?s performance on the task of locating the frame
elements in a sentence. Although our probability model considers the question of
finding the boundaries of frame elements separately from the question of finding the
correct label for a particular frame element, similar features are used to calculate both
265
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Table 8
Sample probabilities of a constituent?s being a frame element.
Distribution Sample Probability Count in training data
P(fe | path) P(fe | path = VB?VP?ADJP?ADVP) = 1 1
P(fe | path = VB?VP?NP) = .73 3,963
P(fe | path = VB?VP?NP?PP?S) = 0 22
P(fe | path, t) P(fe | path = JJ?ADJP?PP, t = apparent) = 1 10
P(fe | path = NN?NP?PP?VP?PP, t = departure) = .4 5
P(fe | h, t) P(fe | h = sudden, t = apparent) = 0 2
P(fe | h = to, t = apparent) = .11 93
P(fe | h = that, t = apparent) = .21 81
probabilities. In the experiments below, the system is no longer given frame element
boundaries but is still given as inputs the human-annotated target word and the frame
to which it belongs. We do not address the task of identifying which frames come into
play in a sentence but envision that existing word sense disambiguation techniques
could be applied to the task.
As before, features are extracted from the sentence and its parse and are used to
calculate probability tables, with the predicted variable in this case being fe, a binary
indicator of whether a given constituent in the parse tree is or is not a frame element.
The features used were the path feature of Section 4.1.3, the identity of the target
word, and the identity of the constituent?s head word. The probability distributions
calculated from the training data were P(fe | path), P(fe | path , t), and P(fe | h, t), where
fe indicates an event where the parse constituent in question is a frame element, path
the path through the parse tree from the target word to the parse constituent, t the
identity of the target word, and h the head word of the parse constituent. Some sample
values from these distributions are shown in Table 8. For example, the path VB?VP?NP,
which corresponds to the direct object of a verbal target word, had a high probability
of being a frame element. The table also illustrates cases of sparse data for various
feature combinations.
By varying the probability threshold at which a decision is made, one can plot a
precision/recall curve as shown in Figure 10. P(fe | path , t) performs relatively poorly
because of fragmentation of the training data (recall that only about 30 sentences are
available for each target word). Although the lexical statistic P(fe | h, t) alone is not
useful as a classifier, using it in linear interpolation with the path statistics improves
results. The curve labeled ?interpolation? in Figure 10 reflects a linear interpolation of
the form
P(fe | p, h, t) = ?1P(fe | p) + ?2P(fe | p, t) + ?3P(fe | h, t) (16)
Note that this method can identify only those frame elements that have a correspond-
ing constituent in the automatically generated parse tree. For this reason, it is in-
teresting to calculate how many true frame elements overlap with the results of the
system, relaxing the criterion that the boundaries must match exactly. Results for par-
tial matching are shown in Table 9. Three types of overlap are possible: the identified
constituent entirely within the true frame element, the true frame element entirely
within the identified constituent, and each sequence partially contained by the other.
An example of the first case is shown in Figure 11, where the true Message frame ele-
ment is Mandarin by a head, but because of an error in the parser output, no constituent
exactly matches the frame element?s boundaries. In this case, the system identifies
266
Computational Linguistics Volume 28, Number 3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
re
ca
ll
precision
P(fe|path)
P(fe|path, t)
interpolation
Figure 10
Plot of precision/recall curve for various methods of identifying frame elements. Recall is
calculated over only frame elements with matching parse constituents.
Table 9
Results on identifying frame elements (FEs), including partial matches. Results obtained using
P(fe | path) with threshold at 0.5. A total of 7,681 constituents were identified as FEs, and 8,167
FEs were present in hand-annotations, of which matching parse constituents were present for
7,053 (86%).
Type of Overlap Identified Constituents Number
Exactly matching boundaries 66% 5,421
Identified constituent entirely within true frame element 8 663
True frame element entirely within identified constituent 7 599
Both partially within the other 0 26
No overlap with any true frame element 13 972
two frame elements, indicated by shading, which together span the true frame ele-
ment.
When the automatically identified constituents were fed through the role-labeling
system described above, 79.6% of the constituents that had been correctly identified
in the first stage were assigned the correct role in the second, roughly equivalent
to the performance when roles were assigned to constituents identified by hand. A
more sophisticated integrated system for identifying and labeling frame elements is
described in Section 7.1.
6. Generalizing Lexical Statistics
As can be seen from Table 3, information about the head word of a constituent
is valuable in predicting the constituent?s role. Of all the distributions presented,
267
Gildea and Jurafsky Automatic Labeling of Semantic Roles
As the horses were led back... ,
SBAR
the
DT
result
NN
NP
was
VBD
announced :
VBN
Mandarin
NN
NP
by
IN
a
DT
head
NN
NP
PP
VP
VP
S
target Message
Figure 11
An example of overlap between identified frame elements and the true boundaries, caused by
parser error. In this case two frame elements identified by the classifier (shaded subtrees) are
entirely within the human annotation (indicated below the sentence), contributing two
instances to row 2 of Table 9.
P(r | h, pt , t) predicts the correct role most often (87.4% of the time) when training
data for a particular head word have been seen. Because of the large vocabulary of
possible head words, however, it also has the smallest coverage, meaning that it is
likely that, for a given case in the test data, no frame element with the same head
word will have been seen in the set of training sentences for the target word in ques-
tion. To capitalize on the information provided by the head word, we wish to find
a way to generalize from head words seen in the training data to other head words.
In this section we compare three different approaches to the task of generalizing over
head words: automatic clustering of a large vocabulary of head words to identify
words with similar semantics; use of a hand-built ontological resource, WordNet, to
organize head words in a semantic hierarchy; and bootstrapping to make use of un-
labeled data in training the system. We will focus on frame elements filled by noun
phrases, which constitute roughly half the total.
6.1 Automatic Clustering
To find groups of head words that are likely to fill the same semantic roles, an auto-
matic clustering of nouns was performed using word co-occurrence data from a large
corpus. This technique is based on the expectation that words with similar semantics
will tend to co-occur with the same other sets of words. For example, nouns describing
foods will tend to occur as direct objects of verbs such as eat devour, and savor. The
clustering algorithm attempts to find such patterns of co-occurrence from the counts
of grammatical relations between pairs of specific words in the corpus, without the
use of any external knowledge or semantic representation.
We extracted verb?direct object relations from an automatically parsed version of
the British National Corpus, using the parser of Carroll and Rooth (1998).4 Clustering
4 We are indebted to Mats Rooth and Sabine Schulte im Walde for providing us with the parsed corpus.
268
Computational Linguistics Volume 28, Number 3
was performed using the probabilistic model of co-occurrence described in detail by
Hofmann and Puzicha (1998). (For other natural language processing [NLP] applica-
tions of the probabilistic clustering algorithm, see, e.g., Rooth [1995], Rooth et al [1999];
for application to language modeling, see Gildea and Hofmann [1999]. According to
this model, the two observed variables, in this case the verb and the head noun of its
object, can be considered independent given the value of a hidden cluster variable, c:
P(n, v) =
?
c
P(c)P(n | c)P(v | c)
One begins by setting a priori the number of values that c can take and using the
EM algorithm to estimate the distributions P(c), P(n | c), and P(v | c). Deterministic
annealing was used to prevent overfitting of the training data.
We are interested only in the clusters of nouns given by the distribution P(n | c):
the verbs and the distribution P(v | c) are thrown away once training is complete. Other
grammatical relations besides direct object could be used, as could a set of relations.
We used the direct object (following other clustering work such as Pereira, Tishby, and
Lee [1993]) because it is particularly likely to exhibit semantically significant selectional
restrictions.
A total of 2,610,946 verb-object pairs were used as training data for the clustering,
with a further 290,105 pairs used as a cross-validation set to control the parameters of
the clustering algorithm. Direct objects were identified as noun phrases directly under
a verb phrase node?not a perfect technique, since it also finds nominal adjuncts such
as ?I start today.? Forms of the verb to be were excluded from the data, as its co-
occurrence patterns are not semantically informative. The number of values possible
for the latent cluster variable was set to 256. (Comparable results were found with 64
clusters; the use of deterministic annealing prevents large numbers of clusters from
resulting in overfitting.)
The soft clustering of nouns thus generated is used as follows: for each example
in the frame element?annotated training data, probabilities for values of the hidden
cluster variable were calculated using Bayes? rule:
P(c | h) = P(h | c)P(c)?
c? P(h | c?)P(c?)
The clustering was applied only to noun phrase constituents; the distribution P(n | c)
from the clustering is used as a distribution P(h | c) over noun head words.
Using the cluster probabilities, a new estimate of P(r | c, pt , t) is calculated for
cases where pt , the phrase type or syntactic category of the constituent, is NP:
P(r | c, pt , t) =
?
j:pt j = pt ,tj = t,rj = r
P(cj | hj)
?
j:pt j = pt ,tj = t
P(cj | hj)
where j is an index ranging over the frame elements in the training set and their
associated features pt , t, h and their semantic roles r.
During testing, a smoothed estimate of the head word?based role probability is
calculated by marginalizing over cluster values:
P(r | h, pt , t) =
?
c
P(r | c, pt , t)P(c | h)
again using P(c | h) = P(h|c)P(c)?
c?
P(h|c?)P(c?) .
269
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Table 10
Clustering results on NP constituents only, 4,086 instances.
Distribution Coverage Accuracy Performance
P(r | h, pt, t) 41.6% 87.0% 36.1%
?
c P(r | c, pt, t)P(c | h) 97.9 79.7 78.0
Interpolation of unclustered distributions 100.0 83.4 83.4
Unclustered distributions + clustering 100.0 85.0 85.0
As with the other methods of generalization described in this section, automatic
clustering was applied only to noun phrases, which represent 50% of the constituents
in the test data. We would not expect head word to be as valuable for other phrase
types. The second most common category is prepositional phrases. The head of a
prepositional phrase (PP) is considered to be the preposition, according to the rules
we use, and because the set of prepositions is small, coverage is not as great a problem.
Furthermore, the preposition is often a direct indicator of the semantic role. (A more
complete model might distinguish between cases in which the preposition serves as a
case or role marker and others in which it is semantically informative, with clustering
performed on the preposition?s object in the former case. We did not attempt to make
this distinction.) Phrase types other than NP and PP make up only a small proportion
of the data.
Table 10 shows results for the use of automatic clustering on constituents identified
by the parser as noun phrases. As can be seen in the table, the vocabulary used for
clustering includes almost all (97.9%) of the test data, and the decrease in accuracy from
direct lexical statistics to clustered statistics is relatively small (from 87.0% to 79.7%).
When combined with the full system described above, clustered statistics increase
performance on NP constituents from 83.4% to 85.0% (statistically significant at p <
.05). Over the entire test set, this translates into an improvement from 80.4% to 81.2%.
6.2 Using a Semantic Hierarchy: WordNet
The automatic clustering described above can be seen as an imperfect method of
deriving semantic classes from the vocabulary, and we might expect a hand-developed
set of classes to do better. We tested this hypothesis using WordNet (Fellbaum 1998), a
freely available semantic hierarchy. The basic technique, when presented with a head
word for which no training examples had been seen, was to ascend the type hierarchy
until reaching a level for which training data are available. To do this, counts of
training data were percolated up the semantic hierarchy in a technique similar to that
of, for example, McCarthy (2000). For each training example, the count #(r, s, pt , t)
was incremented in a table indexed by the semantic role r, WordNet sense s, phrase
type pt , and target word t, for each WordNet sense s above the head word h in the
hypernym hierarchy. In fact, the WordNet hierarchy is not a tree, but rather includes
multiple inheritance. For example, person has as hypernyms both life form and causal
agent. In such cases, we simply took the first hypernym listed, effectively converting
the structure into a tree. A further complication is that several WordNet senses are
possible for a given head word. We simply used the first sense listed for each word; a
word sense disambiguation module capable of distinguishing WordNet senses might
improve our results.
As with the clustering experiments reported above, the WordNet hierarchy was
used only for noun phrases. The WordNet hierarchy does not include pronouns; to
270
Computational Linguistics Volume 28, Number 3
Table 11
WordNet results on NP constituents only, 4,086 instances.
Distribution Coverage Accuracy Performance
P(r | h, pt, t) 41.6% 87.0% 36.1%
WordNet: P(r | s, pt, t) 80.8 79.5 64.1
Interpolation of unclustered distributions 100.0 83.4 83.4
Unclustered distributions + WordNet 100.0 84.3 84.3
increase coverage, the personal pronouns I, me, you, he, she, him, her, we, and us were
added as hyponyms of person. Pronouns that refer to inanimate, or both animate and
inanimate, objects were not included. In addition, the CELEX English lexical database
(Baayen, Piepenbrock, and Gulikers 1995) was used to convert plural nouns to their
singular forms.
As shown in Table 11, accuracy for the WordNet technique is roughly the same as
that in the automatic clustering results in Table 10: 84.3% on NPs, as opposed to 85.0%
with automatic clustering. This indicates that the error introduced by the unsupervised
clustering is roughly equivalent to the error caused by our arbitrary choice of the
first WordNet sense for each word and the first hypernym for each WordNet sense.
Coverage for the WordNet technique is lower, however, largely because of the absence
of proper nouns from WordNet, as well as the absence of nonanimate pronouns (both
personal pronouns such as it and they and indefinite pronouns such as something and
anyone). A dictionary of proper nouns would likely help improve coverage, and a
module for anaphora resolution might help cases with pronouns, with or without
the use of WordNet. The conversion of plural forms to singular base forms was an
important part of the success of the WordNet system, increasing coverage from 71.0%
to 80.8%. Of the remaining 19.2% of all noun phrases not covered by the combination of
lexical and WordNet sense statistics, 22% consisted of head words defined in WordNet,
but for which no training data were available for any hypernym, and 78% consisted
of head words not defined in WordNet.
6.3 Bootstrapping from Unannotated Data
A third way of attempting to improve coverage of the lexical statistics is to ?bootstrap,?
or label unannotated data with the automatic system described in Sections 4 and
5 and use the (imperfect) result as further training data. This can be considered a
variant of the EM algorithm, although we use the single most likely hypothesis for
the unannotated data, rather than calculating the expectation over all hypotheses. Only
one iteration of training on the unannotated data was performed.
The unannotated data used consisted of 156,590 sentences containing the target
words of our corpus, increasing the total amount of data available to roughly six times
the 36,995 annotated training sentences.
Table 12 shows results on noun phrases for the bootstrapping method. The accu-
racy of a system trained only on data from the automatic labeling (Pauto) is 81.0%, rea-
sonably close to the 87.0% for the system trained only on annotated data (Ptrain ). Com-
bining the annotated and automatically labeled data increases coverage from 41.6%
to 54.7% and performance to 44.5%. Because the automatically labeled data are not
as accurate as the annotated data, we can do slightly better by using the automatic
data only in cases where no training data are available, backing off to the distribution
Pauto from Ptrain . The fourth row of Table 12 shows results with Pauto incorporated
271
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Table 12
Bootstrapping results on NP constituents only, 4,086 instances.
Distribution Coverage Accuracy Performance
Ptrain(r | h, pt, t) 41.6% 87.0% 36.1%
Pauto(r | h, pt, t) 48.2 81.0 39.0
Ptrain+auto(r | h, pt, t) 54.7 81.4 44.5
Ptrain, backoff to Pauto 54.7 81.7 44.7
Interpolation of unclustered distributions 100.0 83.4 83.4
Unclustered distributions + Pauto 100.0 83.2 83.2
into the backoff lattice of all the features of Figure 7, which actually resulted in a slight
decrease in performance from the system without the bootstrapped data, shown in the
third row. This is presumably because, although the system trained on automatically
labeled data performed with reasonable accuracy, many of the cases it classifies cor-
rectly overlap with the training data. In fact our backing-off estimate of P(r | h, pt , t)
classifies correctly only 66% of the additional cases that it covers over Ptrain(r | h, pt , t).
6.4 Discussion
The three methods of generalizing lexical statistics each had roughly equivalent accu-
racy on cases for which they were able to derive an estimate of the role probabilities
for unseen head words. The differences between the three were primarily due to how
much they could improve the coverage of the estimator, that is, how many new noun
heads they were able to handle. The automatic-clustering method performed by far
the best on this metric; only 2.1% of test cases were unseen in the data used for the
automatic clustering. This indicates how much can be achieved with unsupervised
methods given very large training corpora. The bootstrapping technique described
here, although it has a similar unsupervised flavor, made use of much less data than
the corpus used for noun clustering. Unlike probabilistic clustering, the bootstrapping
technique can make use of only those sentences containing the target words in ques-
tion. The WordNet experiment, on the other hand, indicates both the usefulness of
hand-built resources when they apply and the difficulty of attaining broad coverage
with such resources. Combining the three systems described would indicate whether
their gains are complementary or overlapping.
7. Verb Argument Structure
One of the primary difficulties in labeling semantic roles is that one predicate may
be used with different argument structures: for example, in the sentences ?He opened
the door? and ?The door opened,? the verb open assigns different semantic roles to
its syntactic subject. In this section we compare two strategies for handling this type
of alternation in our system: a sentence-level feature for frame element groups and a
subcategorization feature for the syntactic uses of verbs. Then a simple system using
the predicate?s argument structure, or syntactic signature, as the primary feature will
be contrasted with previous systems based on local, independent features.
7.1 Priors on Frame Element Groups
The system described in previous sections for classifying frame elements makes an
important simplifying assumption: it classifies each frame element independent of the
decisions made for the other frame elements in the sentence. In this section we remove
272
Computational Linguistics Volume 28, Number 3
Table 13
Sample frame element groups for the verb blame.
Frame Element Group Example Sentences
{Evaluee} Holman would characterize this
as blaming [Evaluee the poor ] .
{Judge, Evaluee, Reason} The letter quotes Black as
saying that [Judge white and Navajo ranchers ]
misrepresent their livestock losses and
blame [Reason everything ] [Evaluee on coyotes ] .
[Judge She ] blames [Evaluee the Government ]
[Reason for failing to do enough to help ] .
{Judge, Evaluee} The only dish she made that we could tolerate was
[Evaluee syrup tart which ] [Judge we ]
praised extravagantly with the result that it became
our unhealthy staple diet.
Table 14
Frame element groups for the verb blame in the Judgment frame.
Frame Element Group Probability
{Evaluee, Judge, Reason} 0.549
{Evaluee, Judge} 0.160
{Evaluee, Reason} 0.167
{Evaluee} 0.097
{Evaluee, Judge, Role } 0.014
{Judge} 0.007
{Judge, Reason} 0.007
this assumption and present a system that can make use of the information that, for
example, a given target word requires that one role always be present or that having
two instances of the same role is extremely unlikely.
To capture this information, we introduce the notion of a frame element group,
which is the set of frame element roles present in a particular sentence (technically a
multiset, as duplicates are possible, though quite rare). Frame element groups (FEGs)
are unordered: examples are shown in Table 13. Sample probabilities from the training
data for the frame element groups of the target word blame are shown in Table 14.
The FrameNet corpus recognizes three types of ?null-instantiated? frame elements
(Fillmore 1986), which are implied but do not appear in the sentence. An example of
null instantiation is the sentence ?Have you eaten?? where food is understood. We
did not attempt to identify such null elements, and any null-instantiated roles are not
included in the sentence?s FEG. This increases the variability of observed FEGs, as a
predicate may require a certain role but allow it to be null instantiated.
Our system for choosing the most likely overall assignment of roles for all the
frame elements of a sentence uses an approximation that we derive beginning with
the true probability of the optimal role assignment r?:
r? = argmaxr1...n P(r1...n | t, f1...n)
where P(r1...n | t, f1...n) represents the probability of an overall assignment of roles ri
to each of the n constituents of a sentence, given the target word t and the various
features fi of each of the constituents. In the first step we apply Bayes? rule to this
273
Gildea and Jurafsky Automatic Labeling of Semantic Roles
quantity,
r? = argmaxr1...n P(r1...n | t)
P(f1...n | r1...n, t)
P(f1...n | t)
and in the second we make the assumption that the features of the various constituents
of a sentence are independent given the target word and each constituent?s role and
discard the term P(f1...n | t), which is constant with respect to r:
r? = argmaxr1...n P(r1...n | t)
?
i
P(fi | ri, t)
We estimate the prior over frame element assignments as the probability of the frame
element groups, represented with the set operator {}:
r? = argmaxr1...n P({r1...n} | t)
?
i
P(fi | ri, t)
We then apply Bayes? rule again,
r? = argmaxr1...n P({r1...n} | t)
?
i
P(ri | fi, t)P(fi | t)
P(ri | t)
and finally discard the feature prior P(fi | t) as being constant over the argmax expres-
sion:
r? = argmaxr1...n P({r1...n} | t)
?
i
P(ri | fi, t)
P(ri | t)
This leaves us with an expression in terms of the prior for frame element groups of
a particular target word P({r1...n} | t), the local probability of a frame element given
a constituent?s features P(ri | fi, t) on which our previous system was based, and the
individual priors for the frame elements chosen P(ri | t). This formulation can be used
to assign roles either when the frame element boundaries are known or when they
are not, as we will discuss later in this section.
Calculating empirical FEG priors from the training data is relatively straightfor-
ward, but the sparseness of the data presents a problem. In fact, 15% of the test
sentences had an FEG not seen in the training data for the target word in question.
Using the empirical value for the FEG prior, these sentences could never be correctly
classified. For this reason, we introduce a smoothed estimate of the FEG prior con-
sisting of a linear interpolation of the empirical FEG prior and the product, for each
possible frame element, of the probability of being present or not present in a sentence
given the target word:
?P({r1...n} | t) + (1 ? ?)
?
?
?
r?r1...n
P(r ? FEG | t)
?
r/?r1...n
P(r /? FEG | t)
?
?
The value of ? was empirically set to maximize performance on the development set;
a value of 0.6 yielded performance of 81.6%, a significant improvement over the 80.4%
of the baseline system. Results were relatively insensitive to the exact value of ?.
Up to this point, we have considered separately the problems of labeling roles
given that we know where the boundaries of the frame elements lie (Section 4, as
well as Section 6) and finding the constituents to label in the sentence (Section 5).
274
Computational Linguistics Volume 28, Number 3
Table 15
Combined results on boundary identification and role labeling.
Unlabeled Labeled
Method Precision Recall Precision Recall
Boundary id. + baseline role labeler 72.6 63.1 67.0 46.8
Boundary id. + labeler w/FEG priors 72.6 63.1 65.9 46.2
Integrated boundary id. and labeling 74.0 70.1 64.6 61.2
We now turn to combining the two systems described above into a complete role
labeling system. We use equation (16), repeated below, to estimate the probability that
a constituent is a frame element:
P(fe | p, h, t) = ?1P(fe | p) + ?2P(fe | p, t) + ?3P(fe | h, t)
where p is the path through the parse tree from the target word to the constituent, t
is the target word, and h is the constituent?s head word.
The first two rows of Table 15 show the results when constituents are determined
to be frame elements by setting the threshold on the probability P(fe | p, h, t) to 0.5 and
then running the labeling system of Section 4 on the resulting set of constituents. The
first two columns of results show precision and recall for the task of identifying frame
element boundaries correctly. The second pair of columns gives precision and recall
for the combined task of boundary identification and role labeling; to be counted as
correct, the frame element must both have the correct boundary and be labeled with
the correct role.
Contrary to our results using human-annotated boundaries, incorporating FEG
priors into the system based on automatically identified boundaries had a negative
effect on labeled precision and recall. No doubt this is due to introducing a dependency
on other frame element decisions that may be incorrect: the use of FEG priors causes
errors in boundary identification to be compounded.
One way around this problem is to integrate boundary identification with role
labeling, allowing the FEG priors and the role-labeling decisions to affect which con-
stituents are frame elements. This was accomplished by extending the formulation
argmaxr1...n P({r1...n} | t)
?
i
P(ri | fi, t)
P(ri | t)
to include frame element identification decisions:
argmaxr1...n P({r1...n} | t)
?
i
P(ri | fi, fei, t)P(fei | fi)
P(ri | t)
where fei is a binary variable indicating that a constituent is a frame element and
P(fei | fi) is calculated as above. When fei is true, role probabilities are calculated as
before; when fei is false, ri assumes an empty role with probability one and is not
included in the FEG represented by {r1...n}.
One caveat in using this integrated approach is its exponential complexity: each
combination of role assignments to constituents is considered, and the number of
combinations is exponential in the number of constituents. Although this did not pose
a problem when only the annotated frame elements were under consideration, now we
275
Gildea and Jurafsky Automatic Labeling of Semantic Roles
S
NP VP
NP
He
PRP VB
DT NN
doorthe
opened
S
VP
NP
VBDT NN
door openedThe
Figure 12
Two subcategorizations for the target word open. The relevant production in the parse tree is
highlighted. On the left, the value of the feature is ?VP ? VB NP?; on the right it is ?VP ?
VB.?
must include every parse constituent with a nonzero probability for P(fei | fi). To make
the computation tractable, we implement a pruning scheme: hypotheses are extended
by choosing assignments for one constituent at a time, and only the top m hypotheses
are retained for extension by assignments to the next constituent. Here we set m = 10
after experimentation showed that increasing m yielded no significant improvement.
Results for the integrated approach are shown in the last row of Table 15. Allowing
role assignments to influence boundary identification improves results both on the
unlabeled boundary identification task and on the combined identification and labeling
task. The integrated approach puts us in a different portion of the precision/recall
curve from the results in the first two rows, as it returns a higher number of frame
elements (7,736 vs. 5,719). A more direct comparison can be made by lowering the
probability threshold for frame element identification from 0.5 to 0.35 to force the
nonintegrated system to return the same number of frame elements as the integrated
system. This yields a frame element identification precision of 71.3% and recall of
67.6% and a labeled precision of 60.8% and recall of 57.6%, which is dominated by
the result for the integrated system. The integrated system does not have a probability
threshold to set; nonetheless it comes closer to identifying the correct number of frame
elements (8,167) than does the independent boundary identifier when the theoretically
optimal threshold of 0.5 is used with the latter.
7.2 Subcategorization
Recall that use of the FEG prior was motivated by the tendency of verbs to assign dif-
fering roles to the same syntactic position. For example, the verb open assigns different
roles to the syntactic subject in He opened the door and The door opened. In this section
we consider a different feature motivated by these problems: the syntactic subcate-
gorization of the verb. For example, the verb open seems to be more likely to assign
the role Patient to its subject in an intransitive context and Agent to its subject in a
transitive context. Our use of a subcategorization feature was intended to differentiate
between transitive and intransitive uses of a verb.
The feature used was the identity of the phrase structure rule expanding the target
word?s parent node in the parse tree, as shown in Figure 12. For example, for He closed
the door, with close as the target word, the subcategorization feature would be ?VP ?
VB NP.? The subcategorization feature was used only when the target word was a
276
Computational Linguistics Volume 28, Number 3
verb. The various part-of-speech tags for verb forms (VBD for past-tense verb forms,
VBZ for third-person singular present tense, VBP for other present tense, VBG for
present participles, and VBN for past participles) were collapsed into a single tag VB.
It is important to note that we are not able to distinguish complements from adjuncts,
and our subcategorization feature could be sabotaged by cases such as The door closed
yesterday. In the Penn Treebank style, yesterday is considered an NP with tree structure
equivalent to that of a direct object. Our subcategorization feature is fairly specific: for
example, the addition of an ADVP to a verb phrase will result in a different value. We
tested variations of the feature that counted the number of NPs in a VP or the total
number of children of the VP, with no significant change in results.
The subcategorization feature was used in conjunction with the path feature, which
represents the sequence of nonterminals along the path through the parse tree from
the target word to the constituent representing a frame element. Making use of the
new subcategorization (subcat) feature by adding the distribution P(r | subcat , path , t)
to the lattice of distributions in the baseline system resulted in a slight improvement
to 80.8% performance from 80.4%. As with the gov feature in the baseline system, it
was found beneficial to use the subcat feature only for NP constituents.
7.3 Discussion
Combining the FEG priors and subcategorization feature into a single system resulted
in performance of 81.6%, no improvement over using FEG priors without subcatego-
rization. We suspect that the two seemingly different approaches in fact provide similar
information. For example, in our hypothetical example of the sentence He opened the
door vs. the sentence The door opened, the verb open would have high priors for the FEGs
{Agent, Theme} and {Theme}, but a low prior for {Agent}. In sentences with only
one candidate frame element (the subject in The door closed), the use of the FEG prior
will cause it to be labeled Theme, even when the feature probabilities prefer labeling a
subject as Agent. Thus the FEG prior, by representing the set of arguments the predi-
cate is likely to take, essentially already performs the function of the subcategorization
feature.
The FEG prior allows us to introduce a dependency between the classifications
of the sentence?s various constituents with a single parameter. Thus, it can handle
the alternation of our example without, for example, introducing the role chosen for
one constituent as an additional feature in the probability distribution for the next
constituent?s role. It appears that because introducing additional features can further
fragment our already sparse data, it is preferable to have a single parameter for the
FEG prior.
An interesting result reinforcing this conclusion is that some of the argument-
structure features that aided the system when individual frame elements were consid-
ered independently are unnecessary when using FEG priors. Removing the features
passive and position from the system and using a smaller lattice of only the distribu-
tions not employing these features yields an improved performance of 82.8% on the
role-labeling task using hand-annotated boundaries. We believe that, because these
features pertain to syntactic alternations in how arguments are realized, they overlap
with the function of the FEG prior. Adding unnecessary features to the system can
reduce performance by fragmenting the training data.
8. Integrating Syntactic and Semantic Parsing
In the experiments reported in previous sections, we have used the parse tree returned
by a statistical parser as input to the role-labeling system. In this section, we explore
277
Gildea and Jurafsky Automatic Labeling of Semantic Roles
the interaction between semantic roles and syntactic parsing by integrating the parser
with the semantic-role probability model. This allows the semantic-role assignment
to affect the syntactic attachment decisions made by the parser, with the hope of
improving the accuracy of the complete system.
Although most statistical parsing work measures performance in terms of syntac-
tic trees without semantic information, an assignment of role fillers has been incor-
porated into a statistical parsing model by Miller et al (2000) for the domain-specific
templates of the Message Understanding Conference (Defense Advanced Research
Projects Agency 1998) task. A key finding of Miller et al?s work was that a system
developed by annotating role fillers in text and training a statistical system performed
at the same level as one based on writing a large system of rules, which requires much
more highly skilled labor to design.
8.1 Incorporating Roles into the Parsing Model
We use as the baseline of all our parsing experiments the model described in Collins
(1999). The algorithm is a form of chart parsing, which uses dynamic programming to
search through the exponential number of possible parses by considering subtrees for
each subsequence of the sentence independently. To apply chart parsing to a proba-
bilistic grammar, independence relations must be assumed to hold between the prob-
abilities of a parse tree and the internal structure of its subtrees.
In the case of stochastic context-free grammar, the probability of a tree is inde-
pendent of the internal structure of its subtrees, given the topmost nonterminal of
the subtree. The chart-parsing algorithm can simply find the highest-probability parse
for each nonterminal for each substring of the input sentence. No lower-probability
subtrees will ever be used in a complete parse, and they can be thrown away. Recent
lexicalized stochastic parsers such as Collins (1999), Charniak (1997), and others add
additional features to each constituent, the most important being the head word of
the parse constituent.
The statistical system for assigning semantic roles described in the previous sec-
tions does not fit easily into the chart-parsing framework, as it relies on long-distance
dependencies between the target word and its frame elements. In particular, the path
feature, which is used to ?navigate? through the sentence from the target word to its
likely frame elements, may be an arbitrarily long sequence of syntactic constituents.
A path feature looking for frame elements for a target word in another part of the
sentence may examine the internal structure of a constituent, violating the indepen-
dence assumptions of the chart parser. The use of priors over FEGs further complicates
matters by introducing sentence-level features dependent on the entire parse.
For these reasons, we use the syntactic parsing model without frame element
probabilities to generate a number of candidate parses, compute the best frame element
assignment for each, and then choose the analysis with the highest overall probability.
The frame element assignments are computed as in Section 7.1, with frame element
probabilities being applied to every constituent in the parse.
To return a large number of candidate parses, the parser was modified to include
constituents in the chart even when they were equivalent, according to the parsing
model, to a higher-probability constituent. Rather than choosing a fixed n and keeping
the n best constituents for each entry in the chart, we chose a probability threshold
and kept all constituents within a margin of the highest-probability constituent. Thus
the mechanism is similar to the beam search used to prune nonequivalent edges, but
a lower threshold was used for equivalent edges ( 1e vs.
1
100 ).
Using these pruning parameters, an average of 14.9 parses per sentence were
obtained. After rescoring with frame element probabilities, 18% of the sentences were
278
Computational Linguistics Volume 28, Number 3
Table 16
Results on rescoring parser output.
Frame Frame Labeled Labeled
Method Element Precision Element Recall Precision Recall
Single-best parse 74.0 70.1 64.6 61.2
Rescoring parses 73.8 70.7 64.6 61.9
assigned a parse different from the original best parse. Nevertheless, the impact on
identification of frame elements was small; results are shown in Table 16. The results
show a slight, but not statistically significant, increase in recall of frame elements. One
possible reason that the improvement is not greater is the relatively small number of
parses per sentence available for rescoring. Unfortunately, the parsing algorithm used
to generate n-best parses is inefficient, and generating large numbers of parses seems
to be computationally intractable. In theory, the complexity of n-best variations of the
Viterbi chart-parsing algorithm is quadratic in n. One can simply expand the dynamic
programming chart to have n slots for the best solutions to each subproblem, rather
than one. As our grammar forms new constituents from pairs of smaller constituents
(that is, it internally uses a binarized grammar), for each pair of constituents considered
in a single-best parser, up to n2 pairs would be present in the n-best variant. The
beam search used by modern parsers, however, makes the analysis more complex.
Lexicalization of parse constituents dramatically increases the number of categories
that must be stored in the chart, and efficient parsing requires that constituents below
a particular probability threshold be dropped from further consideration. In practice,
returning a larger number of parses with our algorithm seems to require increasing
the pruning beam size to a degree that makes run times prohibitive.
In addition to the robustness of even relatively simple parsing models, one expla-
nation for the modest improvement may be the fact that even our integrated system
includes semantic information for only one word in the sentence. As the coverage of
our frame descriptions increases, it may be possible to do better and to model the
interactions between the frames invoked by a text.
9. Generalizing to Unseen Predicates
Most of the statistics used in the system as described above are conditioned on the
target word, or predicate, for which semantic roles are being identified. This limits the
applicability of the system to words for which training data are available. In Section 6,
we attempted to generalize across fillers for the roles of a single predicate. In this
section, we turn to the related but somewhat more difficult question of generalizing
from seen to unseen predicates.
Many ways of attempting this generalization are possible, but the simplest is
provided by the frame-semantic information of the FrameNet database. We can use
data from target words in the same frame to predict behavior for an unseen word,
or, if no data are available for the frame in question, we can use data from the same
broad semantic domain into which the frames are grouped.
9.1 Thematic Roles
To investigate the degree to which our system is dependent on the set of semantic
roles used, we performed experiments using abstract, general semantic roles such as
279
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Agent, Patient, and Goal. Such roles were proposed in theories of linking such as
Fillmore (1968) and Jackendoff (1972) to explain the syntactic realization of semantic
arguments. This level of roles, often called thematic roles, was seen as useful for
expressing generalizations such as ?If a sentence has anAgent, theAgentwill occupy
the subject position.? Such correlations might enable a statistical system to generalize
from one semantic domain to another.
Recent work on linguistic theories of linking has attempted to explain syntactic
realization in terms of the fundamentals of verbs? meaning (see Levin and Rappaport
Hovav [1996] for a survey of a number of theories). Although such an explanation is
desirable, our goal is more modest: an automatic procedure for identifying semantic
roles in text. We aim to use abstract roles as a means of generalizing from limited
training data in various semantic domains. We see this effort as consistent with various
theoretical accounts of the underlying mechanisms of argument linking, since the
various theories all postulate some sort of generalization between the roles of specific
predicates.
To this end, we developed a correspondence from frame-specific roles to a set
of abstract thematic roles. For each frame, an abstract thematic role was assigned to
each frame element in the frame?s definition. Since there is no canonical set of abstract
semantic roles, we decided upon the list shown in Table 17. We are interested in
adjuncts as well as arguments, leading to roles such as Degree not found in many
theories of verb-argument linking. The difficulty of fitting many relations into standard
categories such as Agent and Patient led us to include other roles such as Topic.
In all, we used 18 roles, a somewhat richer set than is often used, but still much more
restricted than the frame-specific roles. Even with this enriched set, not all frame-
specific roles fit neatly into one category.
An experiment was performed replacing each role tag in the training and test
data with the corresponding thematic role and training the system as described above
on the new dataset. Results were roughly comparable for the two types of semantic
roles: overall performance was 82.1% for thematic roles, compared to 80.4% for frame-
specific roles. This reflects the fact that most frames had a one-to-one mapping from
frame-specific to abstract roles, so the tasks were largely equivalent. We expect abstract
roles to be most useful when one is generalizing to predicates and frames not found
in the training data, the topic of the following sections.
One interesting consequence of using abstract roles is that they allow us to compare
more easily the system?s performance on different roles because of the smaller number
of categories. This breakdown is shown in Table 18. Results are given for two systems:
the first assumes that the frame element boundaries are known and the second finds
them automatically. The second system, which is described in Section 7.1, corresponds
to the rightmost two columns in Table 18. The ?Labeled Recall? column shows how
often the frame element is correctly identified, whereas the ?Unlabeled Recall? column
shows how often a constituent with the given role is correctly identified as being a
frame element, even if it is labeled with the wrong role.
Experiencer and Agent, two similar roles generally found as the subject for
complementary sets of verbs, are the roles that are correctly identified the most often.
The ?Unlabeled Recall? column shows that these roles are easy to find in the sentence,
as a predicate?s subject is almost always a frame element, and the ?Known Boundaries?
column shows that they are also not often confused with other roles when it is known
that they are frame elements. The two most difficult roles in terms of unlabeled recall,
Manner and Degree, are typically realized by adverbs or prepositional phrases and
considered adjuncts. It is interesting to note that these are considered in FrameNet to
be general frame elements that can be used in any frame.
280
Computational Linguistics Volume 28, Number 3
Table 17
Abstract semantic roles, with representative examples from the FrameNet corpus.
Role Example
Agent Henry pushed the door open and went in.
Cause Jeez, that amazes me as well as riles me.
Degree I rather deplore the recent manifestation of Pop; it doesn?t seem to me to
have the intellectual force of the art of the Sixties.
Experiencer It may even have been that John anticipating his imminent doom ratified
some such arrangement perhaps in the ceremony at the Jordan.
Force If this is the case can it be substantiated by evidence from the history of
developed societies?
Goal Distant across the river the towers of the castle rose against the sky strad-
dling the only land approach into Shrewsbury.
Instrument In the children with colonic contractions fasting motility did not differentiate
children with and without constipation.
Location These fleshy appendages are used to detect and taste food amongst the
weed and debris on the bottom of a river.
Manner His brow arched delicately.
Null Yet while she had no intention of surrendering her home, it would be foolish
to let the atmosphere between them become too acrimonious.
Path The dung-collector ambled slowly over, one eye on Sir John.
Patient As soon as a character lays a hand on this item, the skeletal Cleric grips it
more tightly.
Percept What is apparent is that this manual is aimed at the non-specialist techni-
cian, possibly an embalmer who has good knowledge of some medical
procedures.
Proposition It says that rotation of partners does not demonstrate independence.
Result All the arrangements for stay-behind agents in north-west Europe collapsed,
but Dansey was able to charm most of the governments in exile in London
into recruiting spies.
Source He heard the sound of liquid slurping in a metal container as Farrell ap-
proached him from behind.
State Rex spied out Sam Maggott hollering at all and sundry and making good
use of his over-sized red gingham handkerchief.
Topic He said, ?We would urge people to be aware and be alert with fireworks
because your fun might be someone else?s tragedy.?
This section has shown that our system can use roles defined at a more abstract
level than the corpus?s frame-level roles and in fact that when we are looking at a
single predicate, the choice has little effect. In the following sections, we attempt to
use the abstract roles to generalize the behavior of semantically related predicates.
9.2 Unseen Predicates
We will present results at different, successively broader levels of generalization, mak-
ing use of the categorization of FrameNet predicates into frames and more general
semantic domains. We first turn to using data from the appropriate frame when no
data for the target word are available.
Table 19 shows results for various probability distributions using a division of
training and test data constructed such that no target words are in common. Every
tenth target word was included in the test set. The amount of training data available
for each frame varied, from just one target word in some cases to 167 target words
in the ?perception/noise? frame. The training set contained a total of 75,919 frame
elements and the test set 7,801 frame elements.
281
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Table 18
Performance broken down by abstract role. The third column represents accuracy when frame
element boundaries are given to the system, and the fourth and fifth columns reflect finding
the boundaries automatically. Unlabeled recall includes cases that were identified as a frame
element but given the wrong role.
Known Boundaries Unknown Boundaries
Role Number % Correct Labeled Recall Unlabeled Recall
Agent 2401 92.8 76.7 80.7
Experiencer 333 91.0 78.7 83.5
Source 503 87.3 67.4 74.2
Proposition 186 86.6 56.5 64.5
State 71 85.9 53.5 62.0
Patient 1161 83.3 63.1 69.1
Topic 244 82.4 64.3 72.1
Goal 694 82.1 60.2 69.6
Cause 424 76.2 61.6 73.8
Path 637 75.0 63.1 63.4
Manner 494 70.4 48.6 59.7
Percept 103 68.0 51.5 65.1
Degree 61 67.2 50.8 60.7
Null 55 65.5 70.9 85.5
Result 40 65.0 55.0 70.0
Location 275 63.3 47.6 63.6
Force 49 59.2 40.8 63.3
Instrument 30 43.3 30.0 73.3
(other) 406 57.9 40.9 63.1
Total 8167 82.1 63.6 72.1
Table 19
Cross-frame performance of various distributions. f represents the FrameNet semantic frame.
Distribution Coverage Accuracy Performance
P(r | path) 95.3% 44.5% 42.4%
P(r | path, f ) 87.4 68.7 60.1
P(r | h) 91.7 54.3 49.8
P(r | h, f ) 74.1 81.3 60.3
P(r | pt, position, voice) 100.0 43.9 43.9
P(r | pt, position, voice, f ) 98.7 68.3 67.4
The results show a familiar trade-off between coverage and accuracy. Conditioning
both the head word and path features on the frame reduces coverage but improves
accuracy. A linear interpolation,
?1P(r | path , f ) + ?2P(r | h, f ) + ?3P(r | pt , position , voice, f )
achieved 79.4% performance on the test set, significantly better than any of the in-
dividual distributions and approaching the result of 82.1% for the original system,
using target-specific statistics and thematic roles. This result indicates that predicates
in the same frame behave similarly in terms of their argument structure, a finding
generally consistent with theories of linking that claim that the syntactic realization
of verb arguments can be predicted from their semantics. We would expect verbs in
the same frame to be semantically similar and to have the same patterns of argument
structure. The relatively high performance of frame-level statistics indicates that the
282
Computational Linguistics Volume 28, Number 3
Table 20
Cross-frame performance of various distributions. d represents the FrameNet semantic domain.
Distribution Coverage Accuracy Performance
P(r | path) 96.2% 41.2% 39.7%
P(r | path, d) 85.7 42.7 36.6
P(r | h) 91.0 44.7 40.6
P(r | h, d) 75.2 54.3 40.9
P(r | d) 95.1 29.9 28.4
P(r) 100.0 28.7 28.7
P(r | h, d) P(r | pt, path, d)
P(r | d)
Figure 13
Minimal lattice for cross-frame generalization.
frames defined by FrameNet are fine-grained enough to capture the relevant semantic
similarities.
This result is encouraging in that it indicates that a relatively small amount of data
can be annotated for a few words in a semantic frame and used to train a system that
can then bootstrap to a larger number of predicates.
9.3 Unseen Frames
More difficult than the question of unseen predicates in a known frame are frames
for which no training data are present. The 67 frames in the current data set cover
only a fraction of the English language, and the high cost of annotation makes it
difficult to expand the data set to cover all semantic domains. The FrameNet project is
defining additional frames and annotating data to expand the scope of the database.
The question of how many frames exist, however, remains unanswered for the time
being; a full account of frame semantics is expected to include multiple frames being
invoked by many words, as well as an inheritance hierarchy of frames and a more
detailed representation of each frame?s meaning.
In this section, we examine the FrameNet data by holding out an entire frame for
testing and using other frames from the same general semantic domain for training.
Recall from Figure 1 that domains like Communication include frames like Conver-
sation, Questioning, and Statement. Because of the variation in difficulty between
different frames and the dependence of the results on which frames are held out for
testing, we used a jackknifing methodology. Each frame was used in turn as test data,
with all other frames used as training data. The results in Table 20 show average
results over the entire data set.
Combining the distributions gives a system based on the (very restricted) backoff
lattice of Figure 13. This system achieves performance of 51.0%, compared to 82.1%
for the original system and 79.4% for the within-frame generalization task. The results
show that generalizing across frames, even within a domain, is more difficult than
generalizing across target words within a frame. There are several factors that may
account for this: the FrameNet domains were intended primarily as a way of orga-
nizing the project, and their semantics have not been formalized. Thus, it may not be
283
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Table 21
Cross-domain performance of various distributions.
Distribution Coverage Accuracy Performance
P(r | path) 96.5% 35.3% 33.4%
P(r | h) 88.8 36.0 31.9
P(r) 100.0 28.7 28.7
surprising that they do not correspond to significant generalizations about argument
structure. The domains are fairly broad, as indicated by the fact that always choosing
the most common role for a given domain (the baseline for cross-frame, within-domain
generalization, given as P(r | d) in Table 20, classifies 28.4% of frame elements cor-
rectly) does not do better than the cross-domain baseline of always choosing the most
common role from the entire database regardless of domain (P(r) in Table 20, which
yields 28.7% correct). This contrasts with a 40.9% baseline for P(r | t), that is, always
choosing the most common role for a particular target word (Table 5, last line). Do-
main information does not seem to help a great deal, given no information about the
frame.
Furthermore, the cross-frame experiments here are dependent on the mapping of
frame-level roles to abstract thematic roles. This mapping was done at the frame level;
that is, FrameNet roles with the same label in two different frames may be translated
into two different thematic roles, but all target words in the same frame make use of
the same mapping. The mapping of roles within a frame is generally one to one, and
therefore the choice of mapping has little effect when using statistics conditioned on
the target word and on the frame, as in the previous section. When we are attempting
to generalize between frames, the mapping determines which roles from the training
frame are used to calculate probabilities for the roles in the test frames, and the choice
of mapping is much more significant. The mapping used is necessarily somewhat
arbitrary.
It is interesting to note that the path feature performs better when not conditioned
on the domain. The head word, however, seems to be more domain-specific: although
coverage declines when the context is restricted to the semantic domain, accuracy
improves. This seems to indicate that the identity of certain role fillers is domain-
specific, but that the syntax/semantics correspondence captured by the path feature is
more general, as predicted by theories of syntactic linking.
9.4 Unseen Domains
As general as they are, the semantic domains of the current FrameNet database cover
only a small portion of the language. The domains are defined at the level of, for
example, Communication and Emotion; a list of the 12 domains in our corpus is
given in Table 1. Whether generalization is possible across domains is an important
question for a general language-understanding system.
For these experiments, a jackknifing protocol similar to that of the previous section
was used, this time holding out one entire domain at a time and using all the others as
training material. Results for the path and head word feature are shown in Table 21.
The distributions P(r | path), P(r | h), and P(r) of Table 21 also appeared in Table 20;
the difference between the experiments is only in the division of training and test sets.
A linear interpolation, ?1P(r | path)+?2P(r | h), classifies 39.8% of frame elements
correctly. This is no better than our result of 40.9% (Table 3) for always choosing a
284
Computational Linguistics Volume 28, Number 3
predicate?s most frequent role; however, the cross-domain system does not have role
frequencies for the test predicates.
9.5 Discussion
As one might expect, as we make successively broader generalizations to semantically
more distant predicates, performance degrades. Our results indicate that frame seman-
tics give us a level at which generalizations relevant to argument linking can be made.
Our results for unseen predicates within the same frame are encouraging, indicating
that the predicates are semantically similar in ways that result in similar argument
structure, as the semantically based theories of linking advocated by Levin (1993) and
Levin and Rappaport Hovav (1996) would predict. We hope that corpus-based sys-
tems such as ours can provide a way of testing and elaborating such theories in the
future. We believe that some level of skeletal representation of the relevant aspects
of a word?s meaning, along the lines of Kipper et al (2000) and of the frame hierar-
chy being developed by the FrameNet project, could be used in the future to help a
statistical system generalize from similar words for which training data are available.
10. Conclusion
Our system is able to label semantic roles automatically with fairly high accuracy,
indicating promise for applications in various natural language tasks. Semantic roles
do not seem to be simple functions of a sentence?s syntactic tree structure, and lexical
statistics were found to be extremely valuable, as has been the case in other natural
language processing applications. Although lexical statistics are quite accurate on the
data covered by observations in the training set, the sparsity of their coverage led
us to introduce semantically motivated knowledge sources, which in turn allowed us
to compare automatically derived and hand-built semantic resources. Various meth-
ods of extending the coverage of lexical statistics indicated that the broader coverage
of automatic clustering outweighed its imprecision. Carefully choosing sentence-level
features for representing alternations in verb argument structure allowed us to intro-
duce dependencies between frame element decisions within a sentence without adding
too much complexity to the system. Integrating semantic interpretation and syntactic
parsing yielded only the slightest gain, showing that although probabilistic models
allow easy integration of modules, the gain over an unintegrated system may not be
large because of the robustness of even simple probabilistic systems.
Many aspects of our system are still quite preliminary. For example, our system
currently assumes knowledge of the correct frame type for the target word to deter-
mine the semantic roles of its arguments. A more complete semantic analysis system
would thus require a module for frame disambiguation. It is not clear how difficult
this problem is and how much it overlaps with the general problem of word-sense
disambiguation.
Much else remains to be done to apply the system described here to the inter-
pretation of general text. One technique for dealing with the sparseness of lexical
statistics would be the combination of FrameNet data with named-entity systems for
recognizing times, dates, and locations, the effort that has gone into recognizing these
items, typically used as adjuncts, should complement the FrameNet data, which is
more focused on arguments. Generalization to predicates for which no annotated data
are available may be possible using other lexical resources or automatic clustering of
predicates. Automatically learning generalizations about the semantics and syntactic
behavior of predicates is an exciting problem for the years to come.
285
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Appendix
Table 22
Penn Treebank part-of-speech tags (including punctuation).
Tag Description Example Tag Description Example
CC Coordin. Conjunction and, but, or SYM Symbol +,%, &
CD Cardinal number one, two, three TO ?to? to
DT Determiner a, the UH Interjection ah, oops
EX Existential ?there? there VB Verb, base form eat
FW Foreign word mea culpa VBD Verb, past tense ate
IN Preposition/sub-conj of, in, by VBG Verb, gerund eating
JJ Adjective yellow VBN Verb, past participle eaten
JJR Adj., comparative bigger VBP Verb, non-3sg pres eat
JJS Adj., superlative wildest VBZ Verb, 3sg pres eats
LS List item marker 1, 2, One WDT Wh-determiner which, that
MD Modal can, should WP Wh-pronoun what, who
NN Noun, sing. or mass llama WP$ Possessive wh- whose
NNS Noun, plural llamas WRB Wh-adverb how, where
NNP Proper noun, singular IBM $ Dollar sign $
NNPS Proper noun, plural Carolinas # Pound sign #
PDT Predeterminer all, both ? Left quote (? or ?)
POS Possessive ending ?s ? Right quote (? or ?)
PRP Personal pronoun I, you, he ( Left parenthesis ( [, (, {, <)
PRP$ Possessive pronoun your, one?s ) Right parenthesis ( ], ), }, >)
RB Adverb quickly, never , Comma ,
RBR Adverb, comparative faster . Sentence-final punc (. ! ?)
RBS Adverb, superlative fastest : Mid-sentence punc (: ; ... ? -)
RP Particle up, off
Table 23
Penn Treebank constituent (or nonterminal) labels.
Label Description
ADJP Adjective Phrase
ADVP Adverb Phrase
CONJP Conjunction Phrase
FRAG Fragment
INTJ Interjection
NAC Not a constituent
NP Noun Phrase
NX Head subphrase of complex noun phrase
PP Prepositional Phrase
QP Quantifier Phrase
RRC Reduced Relative Clause
S Simple declarative clause (sentence)
SBAR Clause introduced by complementizer
SBARQ Question introduced by wh-word
SINV Inverted declarative sentence
SQ Inverted yes/no question
UCP Unlike Co-ordinated Phrase
VP Verb Phrase
WHADJP Wh-adjective Phrase
WHADVP Wh-adverb Phrase
WHNP Wh-noun Phrase
WHPP Wh-prepositional Phrase
286
Computational Linguistics Volume 28, Number 3
Acknowledgments
We are grateful to Chuck Fillmore, Andreas
Stolcke, Jerry Feldman, and three
anonymous reviewers for their comments
and suggestions, to Collin Baker for his
assistance with the FrameNet data, and to
Mats Rooth and Sabine Schulte
im Walde for making available their parsed
corpus. This work was primarily funded by
National Science Foundation grant ITR/HCI
#0086132 to the FrameNet project.
References
Baayen, R. H., R. Piepenbrock, and
L. Gulikers. 1995. The CELEX Lexical
Database (Release 2) [CD-ROM]. Linguistic
Data Consortium, University of
Pennsylvania [Distributor], Philadelphia,
PA.
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. ?The Berkeley
FrameNet project.? In Proceedings of
COLING/ACL, pages 86?90, Montreal,
Canada.
Blaheta, Don and Eugene Charniak. 2000.
?Assigning function tags to parsed text.?
In Proceedings of the First Annual Meeting of
the North American Chapter of the ACL
(NAACL), pages 234?240, Seattle,
Washington.
Carroll, Glenn and Mats Rooth. 1998.
?Valence induction with a
head-lexicalized PCFG.? In Proceedings of
the Third Conference on Empirical Methods in
Natural Language Processing (EMNLP 3),
Granada, Spain.
Charniak, Eugene. 1997. ?Statistical parsing
with a context-free grammar and word
statistics.? In AAAI-97, pages 598?603,
Menlo Park, August. AAAI Press, Menlo
Park, California.
Collins, Michael. 1997. ?Three generative,
lexicalised models for statistical parsing.?
In Proceedings of the 35th Annual Meeting of
the ACL, pages 16?23, Madrid, Spain.
Collins, Michael. 1999. Head-Driven
Statistical Models for Natural Language
Parsing. Ph.D. dissertation, University of
Pennsylvania, Philadelphia.
Dahiya, Yajan Veer. 1995. Panini as a Linguist:
Ideas and Patterns. Eastern Book Linkers,
Delhi, India.
Defense Advanced Research Projects
Agency, editor. 1998. Proceedings of the
Seventh Message Understanding Conference.
Dowty, David R. 1991. Thematic proto-roles
and argument selection. Language
67(3):547?619.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, Massachusetts.
Fillmore, Charles J. 1968. ?The case for
case.? In Emmon W. Bach and Robert T.
Harms, editors, Universals in Linguistic
Theory. Holt, Rinehart & Winston, New
York, pages 1?88.
Fillmore, Charles J. 1971. ?Some problems
for case grammar.? In R. J. O?Brien,
editor, 22nd Annual Round Table.
Linguistics: Developments of the
Sixties?Viewpoints of the Seventies.
Volume 24 of Monograph Series on
Language and Linguistics. Georgetown
University Press, Washington, D.C.,
pages 35?56.
Fillmore, Charles J. 1976. ?Frame semantics
and the nature of language.? In Annals of
the New York Academy of Sciences:
Conference on the Origin and Development of
Language and Speech, Volume 280,
pages 20?32. New York Academy of
Sciences, New York.
Fillmore, Charles J. 1986. ?Pragmatically
controlled zero anaphora.? In Proceedings
of Berkeley Linguistics Society, pages 95?107,
Berkeley, California.
Fillmore, Charles J. and Collin F. Baker.
2000. ?FrameNet: Frame semantics meets
the corpus.? Poster presentation, 74th
Annual Meeting of the Linguistics Society
of America.
Gildea, Daniel and Thomas Hofmann. 1999.
?Probabilistic topic analysis for language
modeling.? In Eurospeech-99,
pages 2167?2170, Budapest.
Hearst, Marti. 1999. ?Untangling text data
mining.? In Proceedings of the 37th Annual
Meeting of the ACL, pages 3?10, College
Park, Maryland.
Hobbs, Jerry R., Douglas Appelt, John Bear,
David Israel, Megumi Kameyama,
Mark E. Stickel, and Mabry Tyson. 1997.
?FASTUS: A cascaded finite-state
transducer for extracting information
from natural-language text.? In
Emmanuel Roche and Yves Schabes,
editors, Finite-State Language Processing.
MIT Press, Cambridge, Massachusetts,
pages 383?406.
Hofmann, Thomas and Jan Puzicha. 1998.
?Statistical models for co-occurrence
data.? Memorandum, Massachussetts
Institute of Technology Artificial
Intelligence Laboratory, Cambridge,
Massachusetts.
Jackendoff, Ray. 1972. Semantic Interpretation
in Generative Grammar. MIT Press,
287
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Cambridge, Massachusetts.
Jelinek, Frederick and Robert L. Mercer.
1980. ?Interpolated estimation of Markov
source parameters from sparse data.? In
Proceedings: Workshop on Pattern Recognition
in Practice, pages 381?397. Amsterdam.
North Holland.
Johnson, Christopher R., Charles J. Fillmore,
Esther J. Wood, Josef Ruppenhofer,
Margaret Urban, Miriam R. L. Petruk, and
Collin F. Baker. 2001. The FrameNet
project: Tools for lexicon building. Version
0.7. Available at http://www.icsi.
berkeley.edu/?framenet/book.html.
Kipper, Karin, Hoa Trang Dang, William
Schuler, and Martha Palmer. 2000.
?Building a class-based verb lexicon using
TAGs.? In TAG+5 Fifth International
Workshop on Tree Adjoining Grammars and
Related Formalisms, Paris, May.
Lapata, Maria and Chris Brew. 1999. ?Using
subcategorization to resolve verb class
ambiguity.? In Joint SIGDAT Conference on
Empirical Methods in NLP and Very Large
Corpora, pages 266?274, College Park,
Maryland.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago.
Levin, Beth and Malka Rappaport Hovav.
1996. ?From lexical semantics to
argument realization.? Unpublished
manuscript.
Marcus, Mitchell P., Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann
Bies, Mark Ferguson, Karen Katz, and
Britta Schasberger. 1994. ?The Penn
Treebank: Annotating predicate argument
structure.? In ARPA Human Language
Technology Workshop, pages 114?119,
Plainsboro, New Jersey. Morgan
Kaufmann, San Francisco.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn treebank. Computational Linguistics
19(2):313?330.
McCarthy, Diana. 2000. ?Using semantic
preferences to identify verbal
participation in role switching
alternations.? In Proceedings of the First
Annual Meeting of the North American
Chapter of the ACL (NAACL),
pages 256?263, Seattle, Washington.
Miller, Scott, Heidi Fox, Lance Ramshaw,
and Ralph Weischedel. 2000. ?A novel use
of statistical parsing to extract
information from text.? In Proceedings of
the First Annual Meeting of the North
American Chapter of the ACL (NAACL),
pages 226?233, Seattle, Washington.
Miller, Scott, David Stallard, Robert Bobrow,
and Richard Schwartz. 1996. ?A fully
statistical approach to natural language
interfaces.? In Proceedings of the 34th
Annual Meeting of the ACL, pages 55?61,
Santa Cruz, California.
Misra, Vidya Niwas. 1966. The Descriptive
Technique of Panini. Mouton, The
Hague.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. ?Distributional
clustering of English words.? In
Proceedings of the 31st Annual Meeting of the
ACL, pages 183?190, Columbus, Ohio.
Pietra, Stephen Della, Vincent Della Pietra,
and John Lafferty. 1997. Inducing features
of random fields. IEEE Transactions on
Pattern Analysis and Machine Intelligence
19(4):380?393.
Pollard, Carl and Ivan A. Sag. 1994.
Head-Driven Phrase Structure Grammar.
University of Chicago Press, Chicago.
Riloff, Ellen. 1993. Automatically
constructing a dictionary for information
extraction tasks. In Proceedings of the 11th
National Conference on Artificial Intelligence
(AAAI), pages 811?816, Washington, D.C.
Riloff, Ellen and Mark Schmelzenbach. 1998.
?An empirical approach to conceptual
case frame acquisition.? In Proceedings of
the Sixth Workshop on Very Large Corpora,
pages 49?56, Montreal, Canada.
Rocher, Rosane. 1964. ?Agent? et ?Objet?
chez Panini. Journal of the American
Oriental Society 84:44?54.
Rooth, Mats. 1995. ?Two-dimensional
clusters in grammatical relations.? In
AAAI Symposium on Representation and
Acquisition of Lexical Knowledge, Stanford,
California.
Rooth, Mats, Stefan Riezler, Detlef Prescher,
Glenn Carroll, and Franz Beil. 1999.
?Inducing a semantically annotated
lexicon via EM-based clustering.? In
Proceedings of the 37th Annual Meeting of the
ACL, pages 104?111, College Park,
Maryland.
Schank, Roger C. 1972. Conceptual
dependency: A theory of natural
language understanding. Cognitive
Psychology 3:552?631.
Siegel, Sidney and N. John Castellan, Jr.
1988. Nonparametric Statistics for the
Behavioral Sciences. 2nd ed. McGraw-Hill,
New York.
Somers, Harold L. 1987. Valency and Case in
Computational Linguistics. Edinburgh
University Press, Edinburgh, Scotland.
Stallard, David. 2000. ?Talk?n?travel: A
conversational system for air travel
planning.? In Proceedings of the Sixth
288
Computational Linguistics Volume 28, Number 3
Applied Natural Language Processing
Conference (ANLP?00), pages 68?75.
Van Valin, Robert D. 1993. A synopsis of
role and reference grammar. In Robert D.
Van Valin, editor, Advances in Role and
Reference Grammar. John Benjamins
Publishing Company, Amsterdam,
pages 1?166.
Winograd, Terry. 1972. Understanding
natural language. Cognitive Psychology,
3(1). Reprinted as a book by Academic
Press, 1972.
Automatic Labeling of Semantic Roles
Daniel Gildea
University of California, Berkeley, and
International Computer Science Institute
gildea@cs.berkeley.edu
Daniel Jurafsky
Department of Linguistics
University of Colorado, Boulder
jurafsky@colorado.edu
Abstract
We present a system for identify-
ing the semantic relationships, or se-
mantic roles, lled by constituents of
a sentence within a semantic frame.
Various lexical and syntactic fea-
tures are derived from parse trees
and used to derive statistical clas-
siers from hand-annotated training
data.
1 Introduction
Identifying the semantic roles lled by con-
stituents of a sentence can provide a level of
shallow semantic analysis useful in solving a
number of natural language processing tasks.
Semantic roles represent the participants in
an action or relationship captured by a se-
mantic frame. For example, the frame for one
sense of the verb \crash" includes the roles
Agent, Vehicle and To-Location.
This shallow semantic level of interpreta-
tion can be used for many purposes. Cur-
rent information extraction systems often use
domain-specic frame-and-slot templates to
extract facts about, for example, nancial
news or interesting political events. A shal-
low semantic level of representation is a more
domain-independent, robust level of represen-
tation. Identifying these roles, for example,
could allow a system to determine that in
the sentence \The rst one crashed" the sub-
ject is the vehicle, but in the sentence \The
rst one crashed it" the subject is the agent,
which would help in information extraction in
this domain. Another application is in word-
sense disambiguation, where the roles associ-
ated with a word can be cues to its sense. For
example, Lapata and Brew (1999) and others
have shown that the dierent syntactic sub-
catgorization frames of a verb like \serve" can
be used to help disambiguate a particular in-
stance of the word \serve". Adding seman-
tic role subcategorization information to this
syntactic information could extend this idea
to use richer semantic knowledge. Semantic
roles could also act as an important inter-
mediate representation in statistical machine
translation or automatic text summarization
and in the emerging eld of Text Data Mining
(TDM) (Hearst, 1999). Finally, incorporat-
ing semantic roles into probabilistic models of
language should yield more accurate parsers
and better language models for speech recog-
nition.
This paper proposes an algorithm for au-
tomatic semantic analysis, assigning a se-
mantic role to constituents in a sentence.
Our approach to semantic analysis is to
treat the problem of semantic role labeling
like the similar problems of parsing, part of
speech tagging, and word sense disambigua-
tion. We apply statistical techniques that
have been successful for these tasks, including
probabilistic parsing and statistical classica-
tion. Our statistical algorithms are trained
on a hand-labeled dataset: the FrameNet
database (Baker et al, 1998). The FrameNet
database denes a tagset of semantic roles
called frame elements, and includes roughly
50,000 sentences from the British National
Corpus which have been hand-labeled with
these frame elements. The next section de-
scribes the set of frame elements/semantic
roles used by our system. In the rest of this
paper we report on our current system, as well
as a number of preliminary experiments on
extensions to the system.
2 Semantic Roles
Historically, two types of semantic roles have
been studied: abstract roles such as Agent
and Patient, and roles specic to individual
verbs such as Eater and Eaten for \eat".
The FrameNet project proposes roles at an in-
termediate level, that of the semantic frame.
Frames are dened as schematic representa-
tions of situations involving various partici-
pants, props, and other conceptual roles (Fill-
more, 1976). For example, the frame \conver-
sation", shown in Figure 1, is invoked by the
semantically related verbs \argue", \banter",
\debate", \converse", and \gossip" as well
as the nouns \argument", \dispute", \discus-
sion" and \ti". The roles dened for this
frame, and shared by all its lexical entries,
include Protagonist1 and Protagonist2
or simply Protagonists for the participants
in the conversation, as well as Medium, and
Topic. Example sentences are shown in Ta-
ble 1. Dening semantic roles at the frame
level avoids some of the diculties of at-
tempting to nd a small set of universal, ab-
stract thematic roles, or case roles such as
Agent, Patient, etc (as in, among many
others, (Fillmore, 1968) (Jackendo, 1972)).
Abstract thematic roles can be thought of
as being frame elements dened in abstract
frames such as \action" and \motion" which
are at the top of in inheritance hierarchy of
semantic frames (Fillmore and Baker, 2000).
The preliminary version of the FrameNet
corpus used for our experiments contained 67
frames from 12 general semantic domains cho-
sen for annotation. Examples of domains (see
Figure 1) include \motion", \cognition" and
\communication". Within these frames, ex-
amples of a total of 1462 distinct lexical pred-
icates, or target words, were annotated: 927
verbs, 339 nouns, and 175 adjectives. There
are a total of 49,013 annotated sentences, and
99,232 annotated frame elements (which do
not include the target words themselves).
3 Related Work
Assignment of semantic roles is an impor-
tant part of language understanding, and has
been attacked by many computational sys-
tems. Traditional parsing and understand-
ing systems, including implementations of
unication-based grammars such as HPSG
(Pollard and Sag, 1994), rely on hand-
developed grammars which must anticipate
each way in which semantic roles may be real-
ized syntactically. Writing such grammars is
time-consuming, and typically such systems
have limited coverage.
Data-driven techniques have recently been
applied to template-based semantic interpre-
tation in limited domains by \shallow" sys-
tems that avoid complex feature structures,
and often perform only shallow syntactic
analysis. For example, in the context of
the Air Traveler Information System (ATIS)
for spoken dialogue, Miller et al (1996) com-
puted the probability that a constituent such
as \Atlanta" lled a semantic slot such as
Destination in a semantic frame for air
travel. In a data-driven approach to infor-
mation extraction, Rilo (1993) builds a dic-
tionary of patterns for lling slots in a spe-
cic domain such as terrorist attacks, and
Rilo and Schmelzenbach (1998) extend this
technique to automatically derive entire case
frames for words in the domain. These last
systems make use of a limited amount of hand
labor to accept or reject automatically gen-
erated hypotheses. They show promise for
a more sophisticated approach to generalize
beyond the relatively small number of frames
considered in the tasks. More recently, a do-
main independent system has been trained on
general function tags such as Manner and
Temporal by Blaheta and Charniak (2000).
4 Methodology
We divide the task of labeling frame elements
into two subtasks: that of identifying the
boundaries of the frame elements in the sen-
tences, and that of labeling each frame ele-
ment, given its boundaries, with the correct
role. We rst give results for a system which
confer?v
debate?v
converse?v
gossip?v
dispute?n
discussion?n
tiff?n
ConversationFrame:
Protagonist?1
Protagonist?2
Protagonists
Topic
Medium
Frame Elements:
talk?v
Domain: Communication Domain: Cognition
Frame: Questioning
Topic
Medium
Frame Elements: Speaker
Addressee
Message
Frame:
Topic
Medium
Frame Elements: Speaker
Addressee
Message
Statement
Frame:
Frame Elements:
Judgment
Judge
Evaluee
Reason
Role
dispute?n
blame?v fault?n
admire?v
admiration?n disapprove?v
blame?n
appreciate?v
Frame:
Frame Elements:
Categorization
Cognizer
Item
Category
Criterion
Figure 1: Sample domains and frames from the FrameNet lexicon.
Frame Element Example (in italics) with target verb Example (in italics) with target noun
Protagonist 1 Kim argued with Pat Kim had an argument with Pat
Protagonist 2 Kim argued with Pat Kim had an argument with Pat
Protagonists Kim and Pat argued Kim and Pat had an argument
Topic Kim and Pat argued about politics Kim and Pat had an argument about politics
Medium Kim and Pat argued in French Kim and pat had an argument in French
Table 1: Examples of semantic roles, or frame elements, for target words \argue" and \argu-
ment" from the \conversation" frame
labels roles using human-annotated bound-
aries, returning to the question of automat-
ically identifying the boundaries in Section
5.3.
4.1 Features Used in Assigning
Semantic Roles
The system is a statistical one, based on train-
ing a classier on a labeled training set, and
testing on an unlabeled test set. The sys-
tem is trained by rst using the Collins parser
(Collins, 1997) to parse the 36,995 train-
ing sentences, matching annotated frame el-
ements to parse constituents, and extracting
various features from the string of words and
the parse tree. During testing, the parser is
run on the test sentences and the same fea-
tures extracted. Probabilities for each possi-
ble semantic role r are then computed from
the features. The probability computation
will be described in the next section; the fea-
tures include:
Phrase Type: This feature indicates the
syntactic type of the phrase expressing
the semantic roles: examples include
noun phrase (NP), verb phrase (VP), and
clause (S). Phrase types were derived au-
tomatically from parse trees generated by
the parser, as shown in Figure 2. The
parse constituent spanning each set of
words annotated as a frame element was
found, and the constituent's nonterminal
label was taken as the phrase type. As
an example of how this feature is useful,
in communication frames, the Speaker
is likely appear a a noun phrase, Topic
as a prepositional phrase or noun phrase,
and Medium as a prepostional phrase, as
in: \We talked about the proposal over
the phone." When no parse constituent
was found with boundaries matching
those of a frame element during testing,
the largest constituent beginning at the
frame element's left boundary and lying
entirely within the element was used to
calculate the features.
Grammatical Function: This feature at-
tempts to indicate a constituent's syntac-
tic relation to the rest of the sentence,
SNP
PRP
VP
VBD
NP
SBAR
IN
S
NNP
VP
VBD
NP PP
PRP IN
NP
NN
Goal SourceTheme Target
NP
He heard the sound of liquid slurping in a metal container as approached him from behindFarrell
Figure 2: A sample sentence with parser output (above) and FrameNet annotation (below).
Parse constituents corresponding to frame elements are highlighted.
for example as a subject or object of a
verb. As with phrase type, this feature
was read from parse trees returned by
the parser. After experimentation with
various versions of this feature, we re-
stricted it to apply only to NPs, as it was
found to have little eect on other phrase
types. Each NP's nearest S or VP ances-
tor was found in the parse tree; NPs with
an S ancestor were given the grammati-
cal function subject and those with a VP
ancestor were labeled object. In general,
agenthood is closely correlated with sub-
jecthood. For example, in the sentence
\He drove the car over the cli", the rst
NP is more likely to ll the Agent role
than the second or third.
Position: This feature simply indicates
whether the constituent to be labeled oc-
curs before or after the predicate den-
ing the semantic frame. We expected
this feature to be highly correlated with
grammatical function, since subjects will
generally appear before a verb, and
objects after. Moreover, this feature
may overcome the shortcomings of read-
ing grammatical function from a con-
stituent's ancestors in the parse tree, as
well as errors in the parser output.
Voice: The distinction between active and
passive verbs plays an important role
in the connection between semantic role
and grammatical function, since direct
objects of active verbs correspond to sub-
jects of passive verbs. From the parser
output, verbs were classied as active or
passive by building a set of 10 passive-
identifying patterns. Each of the pat-
terns requires both a passive auxiliary
(some form of \to be" or \to get") and a
past participle.
Head Word: As previously noted, we ex-
pected lexical dependencies to be ex-
tremely important in labeling semantic
roles, as indicated by their importance
in related tasks such as parsing. Since
the parser used assigns each constituent
a head word as an integral part of the
parsing model, we were able to read the
head words of the constituents from the
parser output. For example, in a commu-
nication frame, noun phrases headed by
\Bill", \brother", or \he" are more likely
to be the Speaker, while those headed
by \proposal", \story", or \question" are
more likely to be the Topic.
For our experiments, we divided the
FrameNet corpus as follows: one-tenth of the
annotated sentences for each target word were
reserved as a test set, and another one-tenth
were set aside as a tuning set for developing
our system. A few target words with fewer
than ten examples were removed from the cor-
pus. In our corpus, the average number of
sentences per target word is only 34, and the
number of sentences per frame is 732 | both
relatively small amounts of data on which to
train frame element classiers.
Although we expect our features to inter-
act in various ways, the data are too sparse
to calculate probabilities directly on the full
set of features. For this reason, we built our
classier by combining probabilities from dis-
tributions conditioned on a variety of combi-
nations of features.
An important caveat in using the FrameNet
database is that sentences are not chosen for
annotation at random, and therefore are not
necessarily statistically representative of the
corpus as a whole. Rather, examples are cho-
sen to illustrate typical usage patterns for
each word. We intend to remedy this in fu-
ture versions of this work by bootstrapping
our statistics using unannotated text.
Table 2 shows the probability distributions
used in the nal version of the system. Cov-
erage indicates the percentage of the test data
for which the conditioning event had been
seen in training data. Accuracy is the propor-
tion of covered test data for which the correct
role is predicted, and Performance, simply
the product of coverage and accuracy, is the
overall percentage of test data for which the
correct role is predicted. Accuracy is some-
what similar to the familiar metric of pre-
cision in that it is calculated over cases for
which a decision is made, and performance is
similar to recall in that it is calculated over all
true frame elements. However, unlike a tradi-
tional precision/recall trade-o, these results
have no threshold to adjust, and the task is a
multi-way classication rather than a binary
decision. The distributions calculated were
simply the empirical distributions from the
training data. That is, occurrences of each
role and each set of conditioning events were
counted in a table, and probabilities calcu-
lated by dividing the counts for each role by
the total number of observations for each con-
ditioning event. For example, the distribution
P (rjpt; t) was calculated sas follows:
P (rjpt; t) =
#(r; pt; t)
#(pt; t)
Some sample probabilities calculated from
the training are shown in Table 3.
5 Results
Results for dierent methods of combining
the probability distributions described in the
previous section are shown in Table 4. The
linear interpolation method simply averages
the probabilities given by each of the distri-
butions in Table 2:
P (rjconstituent) = 
1
P (rjt) +

2
P (rjpt; t) + 
3
P (rjpt; gf; t) +

4
P (rjpt; position; voice) +

5
P (rjpt; position; voice; t) + 
6
P (rjh) +

7
P (rjh; t) + 
8
P (rjh; pt; t)
where
P
i

i
= 1. The geometric mean, ex-
pressed in the log domain, is similar:
P (rjconstituent) =
1
Z
expf
1
logP (rjt) +

2
logP (rjpt; t) + 
3
logP (rjpt; gf; t) +

4
logP (rjpt; position; voice) +

5
logP (rjpt; position; voice; t) +

6
logP (rjh) + 
7
logP (rjh; t) +

8
logP (rjh; pt; t)g
where Z is a normalizing constant ensuring
that
P
r
P (rjconstituent) = 1.
The results shown in Table 4 reect equal
values of  for each distribution dened for
the relevant conditioning event (but exclud-
ing distributions for which the conditioning
event was not seen in the training data).
Distribution Coverage Accuracy Performance
P (rjt) 100% 40.9% 40.9%
P (rjpt; t) 92.5 60.1 55.6
P (rjpt; gf; t) 92.0 66.6 61.3
P (rjpt; position; voice) 98.8 57.1 56.4
P (rjpt; position; voice; t) 90.8 70.1 63.7
P (rjh) 80.3 73.6 59.1
P (rjh; t) 56.0 86.6 48.5
P (rjh; pt; t) 50.1 87.4 43.8
Table 2: Distributions Calculated for Semantic Role Identication: r indicates semantic role,
pt phrase type, gf grammatical function, h head word, and t target word, or predicate.
P (rjpt; gf; t) Count in training data
P (r =Agtjpt =NP; gf =Subj; t =abduct) = :46 6
P (r =Thmjpt =NP; gf =Subj; t =abduct) = :54 7
P (r =Thmjpt =NP; gf =Obj; t =abduct) = 1 9
P (r =Agtjpt =PP; t =abduct) = :33 1
P (r =Thmjpt =PP; t =abduct) = :33 1
P (r =CoThmjpt =PP; t =abduct) = :33 1
P (r =Manrjpt =ADVP; t =abduct) = 1 1
Table 3: Sample probabilities for P (rjpt; gf; t) calculated from training data for the verb abduct.
The variable gf is only dened for noun phrases. The roles dened for the removing frame in
the motion domain are: Agent, Theme, CoTheme (\... had been abducted with him") and
Manner.
Other schemes for choosing values of , in-
cluding giving more weight to distributions
for which more training data was available,
were found to have relatively little eect. We
attribute this to the fact that the evaluation
depends only the the ranking of the probabil-
ities rather than their exact values.
P(r | h, t) P(r | pt, t) P(r | pt, position, voice)
P(r | pt, position, voice, t)P(r | pt, gf, t)
P(r | t)P(r | h)
P(r | h, pt, t)
Figure 3: Lattice organization of the distri-
butions from Table 2, with more specic dis-
tributions towards the top.
In the \backo" combination method, a
lattice was constructed over the distributions
in Table 2 from more specic conditioning
events to less specic, as shown in Figure
3. The less specic distributions were used
only when no data was present for any more
specic distribution. As before, probabilities
were combined with both linear interpolation
and a geometric mean.
Combining Method Correct
Linear Interpolation 79.5%
Geometric Mean 79.6
Backo, linear interpolation 80.4
Backo, geometric mean 79.6
Baseline: Most common role 40.9
Table 4: Results on Development Set, 8148
observations
The nal system performed at 80.4% ac-
curacy, which can be compared to the 40.9%
achieved by always choosing the most prob-
able role for each target word, essentially
chance performance on this task. Results for
this system on test data, held out during de-
velopment of the system, are shown in Table
Linear
Backo Baseline
Development Set 80.4% 40.9%
Test Set 76.9 40.6%
Table 5: Results on Test Set, using backo
linear interpolation system. The test set con-
sists of 7900 observations.
5.
5.1 Discussion
It is interesting to note that looking at a con-
stituent's position relative to the target word
along with active/passive information per-
formed as well as reading grammatical func-
tion o the parse tree. A system using gram-
matical function, along with the head word,
phrase type, and target word, but no passive
information, scored 79.2%. A similar system
using position rather than grammatical func-
tion scored 78.8% | nearly identical perfor-
mance. However, using head word, phrase
type, and target word without either position
or grammatical function yielded only 76.3%,
indicating that while the two features accom-
plish a similar goal, it is important to include
some measure of the constituent's syntactic
relationship to the target word. Our nal sys-
tem incorporated both features, giving a fur-
ther, though not signicant, improvement. As
a guideline for interpreting these results, with
8176 observations, the threshold for statisti-
cal signifance with p < :05 is a 1.0% absolute
dierence in performance.
Use of the active/passive feature made a
further improvement: our system using po-
sition but no grammatical function or pas-
sive information scored 78.8%; adding passive
information brought performance to 80.5%.
Roughly 5% of the examples were identied
as passive uses.
Head words proved to be very accurate in-
dicators of a constituent's semantic role when
data was available for a given head word,
conrming the importance of lexicalization
shown in various other tasks. While the dis-
tribution P (rjh; t) can only be evaluated for
56.0% of the data, of those cases it gets 86.7%
correct, without use of any of the syntactic
features.
5.2 Lexical Clustering
In order to address the sparse coverage of lex-
ical head word statistics, an experiment was
carried out using an automatic clustering of
head words of the type described in (Lin,
1998). A soft clustering of nouns was per-
formed by applying the co-occurrence model
of (Hofmann and Puzicha, 1998) to a large
corpus of observed direct object relationships
between verbs and nouns. The clustering was
computed from an automatically parsed ver-
sion of the British National Corpus, using the
parser of (Carroll and Rooth, 1998). The ex-
periment was performed using only frame el-
ements with a noun as head word. This al-
lowed a smoothed estimate of P (rjh; nt; t) to
be computed as
P
c
P (rjc; nt; t)P (cjh), sum-
ming over the automatically derived clusters c
to which a nominal head word h might belong.
This allows the use of head word statistics
even when the headword h has not been seen
in conjunction was the target word t in the
training data. While the unclustered nominal
head word feature is correct for 87.6% of cases
where data for P (rjh; nt; t) is available, such
data was available for only 43.7% of nominal
head words. The clustered head word alone
correctly classied 79.7% of the cases where
the head word was in the vocabulary used
for clustering; 97.9% of instances of nominal
head words were in the vocabulary. Adding
clustering statistics for NP constituents into
the full system increased overall performance
from 80.4% to 81.2%.
5.3 Automatic Identication of
Frame Element Boundaries
The experiments described above have used
human annotated frame element boundaries
| here we address how well the frame ele-
ments can be found automatically. Exper-
iments were conducted using features simi-
lar to those described above to identify con-
stituents in a sentence's parse tree that were
likely to be frame elements. The system
was given the human-annotated target word
and the frame as inputs, whereas a full lan-
guage understanding system would also iden-
tify which frames come into play in a sen-
tence | essentially the task of word sense
disambiguation. The main feature used was
the path from the target word through the
parse tree to the constituent in question, rep-
resented as a string of parse tree nonterminals
linked by symbols indicating upward or down-
ward movement through the tree, as shown in
Figure 4.
S
NP VP
V NP
Det N
Pro
He
ate
some
target
word
frame
element pancakes
Figure 4: In this example, the path from the
frame element \He" to the target word \ate"
can be represented as NP " S # VP # V, with
" indicating upward movement in the parse
tree and # downward movement.
The other features used were the iden-
tity of the target word and the identity of
the constituent's head word. The probabil-
ity distributions calculated from the train-
ing data were P (fejpath), P (fejpath; t), and
P (fejh; t), where fe indicates an event where
the parse constituent in question is a frame el-
ement, path the path through the parse tree
from the target word to the parse constituent,
t the identity of the target word, and h the
head word of the parse constituent. By vary-
ing the probability threshold at which a deci-
sion is made, one can plot a precision/recall
curve as shown in Figure 5. P (fejpath; t)
performs relatively poorly due to fragmenta-
tion of the training data (recall only about 30
sentences are available for each target word).
While the lexical statistic P (fejh; t) alone is
not useful as a classier, using it in linear in-
terpolation with the path statistics improves
results. Note that this method can only iden-
tify frame elements that have a correspond-
ing constituent in the automatically gener-
ated parse tree. For this reason, it is inter-
esting to calculate how many true frame el-
ements overlap with the results of the sys-
tem, relaxing the criterion that the bound-
aries must match exactly. Results for partial
matching are shown in Table 6.
When the automatically identied con-
stituents were fed through the role labeling
system described above, 79.6% of the con-
stituents which had been correctly identied
in the rst stage were assigned the correct role
in the second, roughly equivalent to the per-
formance when assigning roles to constituents
identied by hand.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
re
ca
ll
precision
P(fe|path)
P(fe|path, t)
.75*P(fe | path)+.25*P(fe | h, t)
Figure 5: Precison/Recall plot for various
methods of identifying frame elements. Recall
is calculated over only frame elements with
matching parse constituents.
6 Conclusion
Our preliminary system is able to automati-
cally label semantic roles with fairly high ac-
curacy, indicating promise for applications in
various natural language tasks. Lexical statis-
tics computed on constituent head words were
found to be the most important of the fea-
tures used. While lexical statistics are quite
accurate on the data covered by observations
in the training set, the sparsity of the data
when conditioned on lexical items meant that
combining features was the key to high over-
all performance. While the combined sys-
tem was far more accurate than any feature
Type of Overlap Identied Constituents Number
Exactly Matching Boundaries 66% 5421
Identied constituent entirely within true frame element 8 663
True frame element entirely within identied constituent 7 599
Partial overlap 0 26
No match to true frame element 13 972
Table 6: Results on Identifying Frame Elements (FEs), including partial matches. Results
obtained using P (fejpath) with threshold at .5. A total of 7681 constituents were identied as
FEs, 8167 FEs were present in hand annotations, of which matching parse constituents were
present for 7053 (86%).
taken alone, the specic method of combina-
tion used was less important.
We plan to continue this work by integrat-
ing semantic role identication with parsing,
by bootstrapping the system on larger, and
more representative, amounts of data, and by
attempting to generalize from the set of pred-
icates chosen by FrameNet for annotation to
general text.
References
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The berkeley framenet project.
In Proceedings of the COLING-ACL, Montreal,
Canada.
Dan Blaheta and Eugene Charniak. 2000. As-
signing function tags to parsed text. In Pro-
ceedings of the 1st Annual Meeting of the North
American Chapter of the ACL (NAACL), Seat-
tle, Washington.
Glenn Carroll and Mats Rooth. 1998. Va-
lence induction with a head-lexicalized pcfg. In
Proceedings of the 3rd Conference on Empir-
ical Methods in Natural Language Processing
(EMNLP 3), Granada, Spain.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the
ACL.
Charles J. Fillmore and Collin F. Baker. 2000.
Framenet: Frame semantics meets the corpus.
In Linguistic Society of America, January.
Charles Fillmore. 1968. The case for case. In
Bach and Harms, editors, Universals in Lin-
guistic Theory, pages 1{88. Holt, Rinehart, and
Winston, New York.
Charles J. Fillmore. 1976. Frame semantics
and the nature of language. In Annals of the
New York Academy of Sciences: Conference on
the Origin and Development of Language and
Speech, volume 280, pages 20{32.
Marti Hearst. 1999. Untangling text data mining.
In Proceedings of the 37rd Annual Meeting of
the ACL.
Thomas Hofmann and Jan Puzicha. 1998. Sta-
tistical models for co-occurrence data. Memo,
Massachussetts Institute of Technology Arti-
cial Intelligence Laboratory, February.
Ray Jackendo. 1972. Semantic Interpretation in
Generative Grammar. MIT Press, Cambridge,
Massachusetts.
Maria Lapata and Chris Brew. 1999. Using
subcategorization to resolve verb class ambigu-
ity. In Joint SIGDAT Conference on Empiri-
cal Methods in NLP and Very Large Corpora,
Maryland.
Dekang Lin. 1998. Automatic retrieval and clus-
tering of similar words. In Proceedings of the
COLING-ACL, Montreal, Canada.
Scott Miller, David Stallard, Robert Bobrow, and
Richard Schwartz. 1996. A fully statistical
approach to natural language interfaces. In
Proceedings of the 34th Annual Meeting of the
ACL.
Carl Pollard and Ivan A. Sag. 1994. Head-
Driven Phrase Structure Grammar. University
of Chicago Press, Chicago.
Ellen Rilo and Mark Schmelzenbach. 1998. An
empirical approach to conceptual case frame ac-
quisition. In Proceedings of the Sixth Workshop
on Very Large Corpora.
Ellen Rilo. 1993. Automatically constructing
a dictionary for information extraction tasks.
In Proceedings of the Eleventh National Con-
ference on Articial Intelligence (AAAI).
Corpus Variation and Parser Performance
Daniel Gildea
University of California, Berkeley, and
International Computer Science Institute
gildea@cs.berkeley.edu
Abstract
Most work in statistical parsing has focused on a
single corpus: the Wall Street Journal portion of the
Penn Treebank. While this has allowed for quanti-
tative comparison of parsing techniques, it has left
open the question of how other types of text might
aect parser performance, and how portable pars-
ing models are across corpora. We examine these
questions by comparing results for the Brown and
WSJ corpora, and also consider which parts of the
parser's probability model are particularly tuned to
the corpus on which it was trained. This leads us
to a technique for pruning parameters to reduce the
size of the parsing model.
1 Introduction
The past several years have seen great progress in
the eld of natural language parsing, through the use
of statistical methods trained using large corpora of
hand-parsed training data. The techniques of Char-
niak (1997), Collins (1997), and Ratnaparkhi (1997)
achieved roughly comparable results using the same
sets of training and test data. In each case, the cor-
pus used was the Penn Treebank's hand-annotated
parses of Wall Street Journal articles. Relatively
few quantitative parsing results have been reported
on other corpora (though see Stolcke et al (1996)
for results on Switchboard, as well as Collins et
al. (1999) for results on Czech and Hwa (1999) for
bootstrapping from WSJ to ATIS). The inclusion of
parses for the Brown corpus in the Penn Treebank
allows us to compare parser performance across cor-
pora. In this paper we examine the following ques-
tions:
 To what extent is the performance of statistical
parsers on the WSJ task due to its relatively
uniform style, and how might such parsers fare
on the more varied Brown corpus?
 Can training data from one corpus be applied
to parsing another?
 What aspects of the parser's probability model
are particularly tuned to one corpus, and which
are more general?
Our investigation of these questions leads us to
a surprising result about parsing the WSJ corpus:
over a third of the model's parameters can be elim-
inated with little impact on performance. Aside
from cross-corpus considerations, this is an impor-
tant nding if a lightweight parser is desired or mem-
ory usage is a consideration.
2 Previous Comparisons of Corpora
A great deal of work has been done outside of the
parsing community analyzing the variations between
corpora and dierent genres of text. Biber (1993)
investigated variation in a number syntactic fea-
tures over genres, or registers, of language. Of
particular importance to statistical parsers is the
investigation of frequencies for verb subcategoriza-
tions such as Roland and Jurafsky (1998). Roland
et al (2000) nd that subcategorization frequen-
cies for certain verbs vary signicantly between the
Wall Street Journal corpus and the mixed-genre
Brown corpus, but that they vary less so between
genre-balanced British and American corpora. Ar-
gument structure is essentially the task that auto-
matic parsers attempt to solve, and the frequencies
of various structures in training data are reected in
a statistical parser's probability model. The varia-
tion in verb argument structure found by previous
research caused us to wonder to what extent a model
trained on one corpus would be useful in parsing an-
other. The probability models of modern parsers
include not only the number and syntactic type of
a word's arguments, but lexical information about
their llers. Although we are not aware of previous
comparisons of the frequencies of argument llers,
we can only assume that they vary at least as much
as the syntactic subcategorization frames.
3 The Parsing Model
We take as our baseline parser the statistical model
of Model 1 of Collins (1997). The model is a history-
based, generative model, in which the probability for
a parse tree is found by expanding each node in the
tree in turn into its child nodes, and multiplying the
probabilities for each action in the derivation. It can
be thought of as a variety of lexicalized probabilis-
tic context-free grammar, with the rule probabilities
factored into three distributions. The rst distribu-
tion gives probability of the syntactic category H
of the head child of a parent node with category
P , head word Hhw with the head tag (the part of
speech tag of the head word) Hht:
P
h
(H jP;Hht;Hhw)
The head word and head tag of the new node H are
dened to be the same as those of its parent. The
remaining two distributions generate the non-head
children one after the other. A special #STOP#
symbol is generated to terminate the sequence of
children for a given parent. Each child is gener-
ated in two steps: rst its syntactic category C and
head tag Cht are chosen given the parent's and head
child's features and a function  representing the
distance from the head child:
P
c
(C;ChtjP;H;Hht;Hhw;)
Then the new child's head word Chw is chosen:
P
cw
(ChwjP;H;Hht;Hhw;; C; Cht)
For each of the three distributions, the empirical dis-
tribution of the training data is interpolated with
less specic backo distributions, as we will see in
Section 5. Further details of the model, including
the distance features used and special handling of
punctuation, conjunctions, and base noun phrases,
are described in Collins (1999).
The fundamental features of used in the proba-
bility distributions are the lexical heads and head
tags of each constituent, the co-occurrences of par-
ent nodes and their head children, and the co-
occurrences of child nodes with their head siblings
and parents. The probability models of Charniak
(1997), Magerman (1995) and Ratnaparkhi (1997)
dier in their details but are based on similar fea-
tures. Models 2 and 3 of Collins (1997) add some
slightly more elaborate features to the probability
model, as do the additions of Charniak (2000) to
the model of Charniak (1997).
Our implementation of Collins' Model 1 performs
at 86% precision and recall of labeled parse con-
stituents on the standard Wall Street Journal train-
ing and test sets. While this does not reect
the state-of-the-art performance on the WSJ task
achieved by the more the complex models of Char-
niak (2000) and Collins (2000), we regard it as a
reasonable baseline for the investigation of corpus
eects on statistical parsing.
4 Parsing Results on the Brown
Corpus
We conducted separate experiments using WSJ
data, Brown data, and a combination of the two
as training material. For the WSJ data, we ob-
served the standard division into training (sections
2 through 21 of the treebank) and test (section 23)
sets. For the Brown data, we reserved every tenth
sentence in the corpus as test data, using the other
nine for training. This may underestimate the dif-
culty of the Brown corpus by including sentences
from the same documents in training and test sets.
However, because of the variation within the Brown
corpus, we felt that a single contiguous test section
might not be representative. Only the subset of the
Brown corpus available in the Treebank II bracket-
ing format was used. This subset consists primarily
of various ction genres. Corpus sizes are shown in
Table 1.
Training Set Test Set
Corpus Sentences Words Sentences Words
WSJ 39,832 950,028 2245 48,665
Brown 21,818 413,198 2282 38,109
Table 1: Corpus sizes. Both test sets were restricted
to sentences of 40 words or less. The Brown test
set's average sentence was shorter despite the length
restriction.
Training Data Test Set Recall Prec.
WSJ WSJ 86.1 86.6
WSJ Brown 80.3 81.0
Brown Brown 83.6 84.6
WSJ+Brown Brown 83.9 84.8
WSJ+Brown WSJ 86.3 86.9
Table 2: Parsing results by training and test corpus
Results for the Brown corpus, along with WSJ
results for comparison, are shown in Table 2. The
basic mismatch between the two corpora is shown
in the signicantly lower performance of the WSJ-
trained model on Brown data than on WSJ data
(rows 1 and 2). A model trained on Brown data only
does signicantly better, despite the smaller size of
the training set. Combining the WSJ and Brown
training data in one model improves performance
further, but by less than 0.5% absolute. Similarly,
adding the Brown data to the WSJ model increased
performance on WSJ by less than 0.5%. Thus, even
a large amount of additional data seems to have rel-
atively little impact if it is not matched to the test
material.
The more varied nature of the Brown corpus also
seems to impact results, as all the results on Brown
are lower than the WSJ result.
5 The Eect of Lexical
Dependencies
The parsers cited above all use some variety of lexical
dependency feature to capture statistics on the co-
occurrence of pairs of words being found in parent-
child relations within the parse tree. These word
pair relations, also called lexical bigrams (Collins,
1996), are reminiscent of dependency grammars such
as Me

lcuk (1988) and the link grammar of Sleator
and Temperley (1993). In Collins' Model 1, the word
pair statistics occur in the distribution
P
cw
(ChwjP;H;Hht;Hhw;; C; Cht)
whereHhw represent the head word of a parent node
in the tree and Chw the head word of its (non-head)
child. (The head word of a parent is the same as the
head word of its head child.) Because this is the only
part of the model that involves pairs of words, it is
also where the bulk of the parameters are found. The
large number of possible pairs of words in the vocab-
ulary make the training data necessarily sparse. In
order to avoid assigning zero probability to unseen
events, it is necessary to smooth the training data.
The Collins model uses linear interpolation to es-
timate probabilities from empirical distributions of
varying specicities:
P
cw
(ChwjP;H;Hht;Hhw;; C; Cht) =

1
~
P (ChwjP;H;Hht;Hhw;; C; Cht) +
(1  
1
)


2
~
P (ChwjP;H;Hht;; C; Cht)+
(1  
2
)
~
P (ChwjCht)

(1)
where
~
P represents the empirical distribution de-
rived directly from the counts in the training data.
The interpolation weights 
1
, 
2
are chosen as a
function of the number of examples seen for the con-
ditioning events and the number of unique values
seen for the predicted variable. Only the rst distri-
bution in this interpolation scheme involves pairs of
words, and the third component is simply the prob-
ability of a word given its part of speech.
Because the word pair feature is the most spe-
cic in the model, it is likely to be the most corpus-
specic. The vocabularies used in corpora vary, as
do the word frequencies. It is reasonable to ex-
pect word co-occurrences to vary as well. In or-
der to test this hypothesis, we removed the distribu-
tion
~
P (ChwjP;H;Hht;Hhw;C;Cht) from the pars-
ing model entirely, relying on the interpolation of the
two less specic distributions in the parser:
P
cw2
(ChwjP;H;Hht;; C; Cht) =

2
~
P (ChwjP;H;Hht;; C; Cht) +
(1  
2
)
~
P (ChwjCht) (2)
We performed cross-corpus experiments as before
to determine whether the simpler parsing model
might be more robust to corpus eects. Results are
shown in Table 3.
Perhaps the most striking result is just how little
the elimination of lexical bigrams aects the baseline
system: performance on the WSJ corpus decreases
by less than 0.5% absolute. Moreover, the perfor-
mance of a WSJ-trained system without lexical bi-
grams on Brown test data is identical to the WSJ-
trained system with lexical bigrams. Lexical co-
occurrence statistics seem to be of no benet when
attempting to generalize to a new corpus.
6 Pruning Parser Parameters
The relatively high performance of a parsing model
with no lexical bigram statistics on the WSJ task
led us to explore whether it might be possible to
signicantly reduce the size of the parsing model
by selectively removing parameters without sacri-
cing performance. Such a technique reduces the
parser's memory requirements as well as the over-
head of loading and storing the model, which could
be desirable for an application where limited com-
puting resources are available.
Signicant eort has gone into developing tech-
niques for pruning statistical language models for
speech recognition, and we borrow from this work,
using the weighted dierence technique of Seymore
and Rosenfeld (1996). This technique applies to any
statistical model which estimates probabilities by
backing o, that is, using probabilities from a less
specic distribution when no data are available are
available for the full distribution, as the following
equations show for the general case:
P (ejh) = P
1
(ejh) if e 62 BO(h)
= (h)P
2
(ejh
0
) if e 2 BO(h)
Here e is the event to be predicted, h is the set of
conditioning events or history,  is a backo weight,
and h
0
is the subset of conditioning events used for
the less specic backo distribution. BO is the back-
o set of events for which no data are present in the
specic distribution P
1
. In the case of n-gram lan-
guage modeling, e is the next word to be predicted,
and the conditioning events are the n  1 preceding
words. In our case the specic distribution P
1
of the
backo model is P
cw
of equation 1, itself a linear in-
terpolation of three empirical distributions from the
training data. The less specic distribution P
2
of the
backo model is P
cw2
of equation 2, an interpolation
of two empirical distributions. The backo weight 
is simply 1   
1
in our linear interpolation model.
The Seymore/Rosenfeld pruning technique can be
used to prune backo probability models regardless
of whether the backo weights are derived from lin-
ear interpolation weights or discounting techniques
such as Good-Turing. In order to ensure that the
model's probabilities still sum to one, the backo
w/ bigrams w/o bigrams
Training Data Test Set Recall Prec. Recall Prec.
WSJ WSJ 86.1 86.6 85.6 86.2
WSJ Brown 80.3 81.0 80.3 81.0
Brown Brown 83.6 84.6 83.5 84.4
WSJ+Brown Brown 83.9 84.8 83.4 84.3
WSJ+Brown WSJ 86.3 86.9 85.7 86.4
Table 3: Parsing results by training and test corpus
weight  must be adjusted whenever a parameter is
removed from the model. In the Seymore/Rosenfeld
approach, parameters are pruned according to the
following criterion:
N(e; h)(log p(ejh)  log p
0
(ejh
0
)) (3)
where p
0
(ejh
0
) represents the new backed o proba-
bility estimate after removing p(ejh) from the model
and adjusting the backo weight, and N(e; h) is the
count in the training data. This criterion aims to
prune probabilities that are similar to their back-
o estimates, and that are not frequently used. As
shown by Stolcke (1998), this criterion is an approx-
imation of the relative entropy between the original
and pruned distributions, but does not take into ac-
count the eect of changing the backo weight on
other events' probabilities.
Adjusting the threshold  below which parameters
are pruned allows us to successively remove more
and more parameters. Results for dierent values of
 are shown in Table 4.
The complete parsing model derived from the
WSJ training set has 735,850 parameters in a to-
tal of nine distributions: three levels of backo for
each of the three distributions P
h
, P
c
and P
cw
. The
lexical bigrams are contained in the most specic
distribution for P
cw
. Removing all these parameters
reduces the total model size by 43%. The results
show a gradual degradation as more parameters are
pruned.
The ten lexical bigrams with the highest scores for
the pruning metric are shown in Table 5 for WSJ
and Table 6. The pruning metric of equation 3 has
been normalized by corpus size to allow compari-
son between WSJ and Brown. The only overlap
between the two sets is for pairs of unknown word
tokens. The WSJ bigrams are almost all specic
to nance, are all word pairs that are likely to ap-
pear immediately adjacent to one another, and are
all children of the base NP syntactic category. The
Brown bigrams, which have lower correlation val-
ues by our metric, include verb/subject and prepo-
sition/object relations and seem more broadly ap-
plicable as a model of English. However, the pairs
are not strongly related semantically, no doubt be-
cause the rst term of the pruning criterion favors
the most frequent words, such as forms of the verbs
\be" and \have".
Child word Head word Parent Pruning
Chw Hhw P Metric
New York NPB .0778
Stock Exchange NPB .0336
< unk > < unk > NPB .0313
vice president NPB .0312
Wall Street NPB .0291
San Francisco NPB .0291
York Stock NPB .0243
Mr. < unk > NPB .0241
third quarter NPB .0227
Dow Jones NPB .0227
Table 5: Ten most signicant lexical bigrams from
WSJ, with parent category (other syntactic context
variables not shown) and pruning metric
. NPB is Collins' \base NP" category.
Child word Head word Parent Pruning
Chw Hhw P Metric
It was S .0174
it was S .0169
< unk > of PP .0156
< unk > in PP .0097
course Of PP .0090
been had VP .0088
< unk > < unk > NPB .0079
they were S .0077
I 'm S .0073
time at PP .0073
Table 6: Ten most signicant lexical bigrams from
Brown
7 Conclusion
Our results show strong corpus eects for statistical
parsing models: a small amount of matched train-
ing data appears to be more useful than a large
amount of unmatched data. The standard WSJ
task seems to be simplied by its homogenous style.
Adding training data from from an unmatched cor-
pus doesn't hurt, but doesn't help a great deal either.
In particular, lexical bigram statistics appear to
be corpus-specic, and our results show that they
Threshold # parameters % reduction
 removed model size Recall Prec.
0 (full model) 0 0 86.1 86.6
1 96K 13 86.0 86.4
2 166K 23 85.9 86.2
3 213K 29 85.7 86.2
1 316K 43 85.6 86.2
Table 4: Parsing results with pruned probability models. The complete parsing model contains 736K pa-
rameters in nine distributions. Removing all lexical bigram parameters reducing the size of the model by
43%.
are of no use when attempting to generalize to new
training data. In fact, they are of surprisingly little
benet even for matched training and test data |
removing them from the model entirely reduces per-
formance by less than 0.5% on the standard WSJ
parsing task. Our selective pruning technique al-
lows for a more ne grained tuning of parser model
size, and would be particularly applicable to cases
where large amounts of training data are available
but memory usage is a consideration. In our im-
plementation, pruning allowed models to run within
256MB that, unpruned, required larger machines.
The parsing models of Charniak (2000) and
Collins (2000) add more complex features to the
parsing model that we use as our baseline. An
area for future work is investigation of the degree
to which such features apply across corpora, or, on
the other hand, further tune the parser to the pe-
culiarities of the Wall Street Journal. Of particu-
lar interest are the automatic clusterings of lexical
co-occurrences used in Charniak (1997) and Mager-
man (1995). Cross-corpus experiments could reveal
whether these clusters uncover generally applicable
semantic categories for the parser's use.
Acknowledgments This work was undertaken as
part of the FrameNet project at ICSI, with funding
from National Science Foundation grant ITR/HCI
#0086132.
References
Douglas Biber. 1993. Using register-diversied cor-
pora for general language studies. Computational
Linguistics, 19(2):219{241, June.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
AAAI97, Brown University, Providence, Rhode
Island, August.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Annual
Meeting of the North American Chapter of the
ACL (NAACL), Seattle, Washington.
Michael Collins, Jan Hajic, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
czech. In Proceedings of the 37th Annual Meeting
of the ACL, College Park, Maryland.
Michael Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of the 34th Annual Meeting of the ACL.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of
the 35th Annual Meeting of the ACL.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
ICML.
Rebecca Hwa. 1999. Supervised grammar induction
using training data with limited constituent infor-
mation. In Proceedings of the 37th Annual Meet-
ing of the ACL, College Park, Maryland.
David Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd An-
nual Meeting of the ACL.
Ivan A. Me

lcuk. 1988. Dependency Syntax: Theory
and Practice. State University of New York Press.
Adwait Ratnaparkhi. 1997. A linear observed time
statistical parser based on maximum entropy
models. In Proceedings of the Second Conference
on Empirical Methods in Natural Language Pro-
cessing.
Douglas Roland and Daniel Jurafsky. 1998. How
verb subcategorization frequencies are aected by
corpus choice. In Proceedings of COLING/ACL,
pages 1122{1128.
Douglas Roland, Daniel Jurafsky, Lise Menn, Su-
sanne Gahl, Elizabeth Elder, and Chris Riddoch.
2000. Verb subcategorization frequency dier-
ences between business-news and balanced cor-
pora: the role of verb sense. In Proceedings of the
Association for Computational Linguistics (ACL-
2000) Workshop on Comparing Corpora.
Kristie Seymore and Roni Rosenfeld. 1996. Scalable
backo language models. In ICSLP-96, volume 1,
pages 232{235, Philadelphia.
Daniel Sleator and Davy Temperley. 1993. Pars-
ing english with a link grammar. In Third Inter-
national Workshop on Parsing Technologies, Au-
gust.
A. Stolcke, C. Chelba, D. Engle, V. Jimenez,
L. Mangu, H. Printz, E. Ristad, R. Rosenfeld,
D. Wu, F. Jelinek, and S. Khudanpur. 1996. De-
pendency language modeling. Summer Workshop
Final Report 24, Center for Language and Speech
Processing, Johns Hopkins University, Baltimore,
April.
Andreas Stolcke. 1998. Entropy-based pruning
of backo language models. In Proc. DARPA
Broadcast News Transcription and Understanding
Workshop, pages 270{274, Lansdowne, Va.
Probabilistic Models of Verb-Argument Structure
Daniel Gildea
Dept. of Computer and Information Science
University of Pennsylvania
dgildea@cis.upenn.edu
Abstract
We evaluate probabilistic models of verb argument
structure trained on a corpus of verbs and their syn-
tactic arguments. Models designed to represent pat-
terns of verb alternation behavior are compared with
generic clustering models in terms of the perplexity
assigned to held-out test data. While the special-
ized models of alternation do not perform as well,
closer examination reveals alternation behavior rep-
resented implicitly in the generic models.
1 Introduction
Recent research into verb-argument structure has
has attempted to acquire the syntactic alternation
behavior of verbs directly from large corpora. Mc-
Carthy (2000), Merlo and Stevenson (2001), and
Schulte im Walde (2000) have evaluated their sys-
tems? accuracy against human judgments of verb
classification, with the comprehensive verb classes
of Levin (1993) often serving as a gold standard.
Another area of research has focused on automatic
clustering algorithms for verbs and their arguments
with the goal of finding groups of semantically re-
lated words (Pereira et al, 1993; Rooth et al, 1999),
without focusing specifically on alternation behav-
ior. We aim to bring these strands of research to-
gether with a unified probabilistic model of verb ar-
gument structure incorporating alternation behavior.
Unraveling the mapping between syntactic func-
tions such as subject and object and semantic roles
such as agent and patient is an important piece
of the language understanding problem. Learn-
ing the alternation behavior of verbs automatically
from unannotated text would significantly reduce
the amount of labor needed to create text under-
standing systems, whether that labor takes the form
of writing lexical entries or of annotating semantic
information to train statistical systems.
Our use of generative probabilistic models of ar-
gument structure also allows for language modeling
applications independent of semantic interpretation.
Language models based on head-modifier lexical
dependencies in syntactic trees have been shown to
have lower perplexity than n-gram language models
and to reduce word-error rates for speech recogni-
tion (Chelba and Jelinek, 1999; Roark, 2001). In-
corporating semantic classes and verb alternation
behavior could improve such models? performance.
Automatically derived word clusters are used in the
statistical parsers of Charniak (1997) and Mager-
man (1995). Incorporating alternation behavior into
such models might improve parsing results as well.
This paper focuses on evaluating probabilistic
models of verb-argument structure in terms of how
well they model unseen test data, as measured by
perplexity. We will examine maximum likelihood
bigram and trigram models, clustering models based
on those of Rooth et al (1999), as well as a new
probabilistic model designed to capture alternations
in verb-argument structure.
2 Capturing Alternation Behavior
Automatic clustering of co-occurrences of verbs and
their direct objects was first used to induce se-
mantically related classes of both verbs and nouns
(Pereira et al, 1993). Rooth et al (1999) used
the Expectation Maximization algorithm to perform
soft clustering by optimizing the parameters of a
fairly simple probability model, which considers the
verb and noun to be independent given the unob-
served cluster variable c:
P (v; n) =
X
c
P (c)P (vjc)P (njc)
In Rooth et al (1999), the variable v represented not
only the lexical verb but also its syntactic relation to
the noun: either direct object, subject of an intransi-
tive, or subject of a transitive verb.
However, the relationship between the underly-
ing, semantic arguments of a verb and the syntac-
tic roles in a sentence is not always straightforward.
Many verbs exhibit alternations in their syntactic
behavior, as shown by the following examples:
(1) The Federal Reserve increased rates by 1/4%.
(2) Interest rates have increased sharply over the
past year.
The noun rates appears as the syntactic object
of the verb increase in the first sentence, but
as its subject in the second sentence, where the
verb is used intransitively, that is, without an ob-
ject. One of the clusters found by the model of
Rooth et al (1999) corresponded to ?verb of scalar
change? such as increase, rise, and decrease. The
model places both subject-of-intransitive-increase
and direct-object-of-increase in this class, but does
not explicitly capture the fact that these to values
represent different uses of the same verb.
The phenomenon of verb argument alternations
has been most comprehensively studied by Levin
(1993), who catalogs over 3,000 verbs into classes
according to which alternations they participate in.
A central thesis of Levin?s work is that a verb?s syn-
tactic alternations are related to its semantics, and
that semantically related verb will share the same
alternations. For example, the alternation of exam-
ples 1 and 2 is shared by verbs such as decrease and
diminish.
Table 1 gives the most common nouns occurring
as arguments of selected verbs in our corpus, show-
ing how alternation behavior shows up in corpus
statistics. The verbs open and increase, classified
by Levin and others as exhibiting a causative al-
ternation between transitive and intransitive usages,
share many of the same nouns in direct object and
subject-of-intransitive positions, as we would ex-
pect. For example, number, cost, and rate occur
among the ten most common nouns in both posi-
tions for increase, and themselves seem semanti-
cally related. For open, the first three words in either
position are the same. For the verb play, on the other
hand, classified as an ?object-drop? verb by Merlo
and Stevenson (2001), we would expect overlap be-
tween the subject of transitive and intransitive uses.
This is in fact the case, with child, band, and team
appearing among the top ten nouns for both posi-
tions. However, play also exhibits an alternation be-
tween the direct object and subject of intransitive
positions for music, role, and game. These two sets
of nouns seem to fill different semantic roles of the
verb, the first set being agents and the second be-
ing themes. This example illustrate the complex in-
teraction between verb sense and alternation behav-
ior: ?The band played? and the ?The music played?
are considered to belong to different senses of play
by WordNet (Fellbaum, 1998) and other word sense
inventories. However, it is interesting to note that
nouns from both the broad senses of play, ?play a
game? and ?play music?, participate in both alter-
nations. An advantage of our EM-based soft clus-
tering algorithm is that it can assign a verb to mul-
tiple clusters; ideally, we would hope that a verb?s
clusters would correspond to its senses.
We expect verbs which take similar sets of argu-
ment fillers to be semantically related, and to par-
ticipate in the same alternations. This idea has been
used by McCarthy (2000) to identify verbs partici-
pating in specific alternations by looking for over-
lap between nouns used in different positions, and
by using WordNet to classify role fillers into se-
mantic categories. Schulte im Walde (2000) uses
an EM-based automatic clustering of verbs to at-
tempt to derive Levin classes from unlabeled data.
As in McCarthy (2000), the nouns are classified us-
ing WordNet. However, the appearance of the same
noun in different syntactic positions is not explicitly
captured by the probability model used for cluster-
ing.
This observation motivated a new probabilistic
model of verb argument structure designed to ex-
plicitly capture alternation behavior. In addition to
an unobserved cluster variable c, we introduce a sec-
ond unobserved variable r for the semantic role of
an argument. The role r is dependent on both the
cluster c to which our verb-noun pair belongs, and
the syntactic slot s in which the noun is found, and
the probability of an observed triple P (v; s; n) is es-
timated as:
X
c;r
P (c)P (vjc)P (sjc)P (rjc; s)P (njr; c)
The noun is independent of the verb given the clus-
ter variable, as before, and the noun is independent
of the syntactic slot s given the cluster c and the se-
mantic role r. The semantic role variable r can take
two values, with P (rjc; s) representing the mapping
from syntax to semantic role for a cluster of verbs.
We expect the clusters to consist of verbs that not
only appear with the same set of nouns, but share the
same mapping from syntactic position to semantic
role. For example increase and decrease might be-
long to same cluster as they both appear frequently
Verb Object Subj of Intransitive Subj of Transitive
close door door troop
eyes eyes door
mouth mouth police
firebreak exhibition gunman
way shop woman
possibility show man
gate trial guard
account conference soldier
window window one
shop gate company
increase risk number government
number proportion increase
share population use
profit rate effect
lead pressure sale
pressure amount level
rate cost presence
likelihood sale Party
chance rates Labour
cost profit bank
play part child band
role band factor
game team England
host role child
music player people
card game woman
piano smile man
tennis people team
parts music all
guitar boy group
Table 1: Examples from the corpus: most common arguments for selected verbs
with rate, number, and price in both the direct ob-
ject and subject of intransitive slots, and would as-
sign the same value of r to both positions. The verb
lower might belong to a different cluster because,
although it appears with the same nouns, they ap-
pear as the direct object but not as the subject.
The Expectation Maximization algorithm is used
to train the model from the corpus, iterating over an
Expectation step in which expected values for the
two unobserved variables c and r are calculated for
each observation in the training data, and a Maxi-
mization step in which the parameter of each of the
five distributions P (c), P (vjc), P (sjc), P (rjc; s),
and P (njn; c) are set to maximize the likelihood of
the data given the expectations for c and r.
3 The Data
For our experiments we used a version of the British
National Corpus parsed with the statistical parser of
Collins (1997). Subject and direct object relations
were extracted by searching for NP nodes domi-
nated by S and VP nodes respectively. The head
words of the resulting subject and object nodes were
found using the deterministic headword rules em-
ployed by the parsing model. The individual obser-
vations of our dataset are noun-verb pairs of three
types: direct object, subject of a verb with an ob-
ject, and subject of a verb without an object. As a
result, the subject and object relations of the same
original sentence are considered independently by
all of the models we examine.
Direct object noun phrases were assigned the
function tags of the Treebank-2 annotation style
(Marcus et al, 1994) in order to distinguish noun
phrases such as temporal adjuncts from true direct
objects. For example, in the sentence ?He ate yes-
terday?, yesterday would be assigned the Temporal
tag, and therefore not considered a direct object for
our purposes. Similarly, in the sentence ?Interest
rates rose 2%?, 2% would be assigned the Extent
tag, and this instance of rise would be considered
intransitive.
Function tags were assigned using a simple prob-
ability model trained on the Wall Street Journal data
from the Penn Treebank, in a technique similar to
that of Blaheta and Charniak (2000). The model
predicts the function tag conditioned on the verb and
head noun of the noun phrase:
P (f jv; n) =
(
~
P (f jv; n) (v; n) 2 T
1
2
~
P (f jv) +
1
2
~
P (f jn) otherwise
where f ranges over the function tags defined (Mar-
cus et al, 1994), or the null tag. Only cases assigned
the null tag by this model were considered true di-
rect objects. Evaluated on the binary task of whether
to assign a function tag to noun phrases in object
position, this classifier was correct 95% of the time
on held-out data from the Wall Street Journal. By
never assigning a function tag, one would achieve
85% accuracy. While we have no way to evaluate
its accuracy on the British National Corpus, certain
systematic errors are apparent. For example, while
it classifies 2% as an Extent in ?Interest rates in-
creased 2%?, it assigns no tag to crack in ?The door
opened a crack?. This type of error leads to the ap-
pearance of door as a subject on transitive uses of
open in Table 1.
Both verbs and nouns were lemmatized using the
XTAG morphological dictionary (XTAG Research
Group, 2001). As we wished to focus on alternation
behavior, verbs that were used intransitively than
90% of the time were excluded from the data; we
envision that they would be handled by a separate
probability model. Pronouns were excluded from
the dataset, as were verbs and nouns that occurred
fewer than 10 times, resulting in a vocabulary of
4,456 verbs and 17,345 nouns. The resulting dataset
consisted of 1,372,111 triples of verb, noun, and
syntactic relation. Of these, 90% were used as train-
ing material, 5% were used as a cross-validation set
for setting linear interpolation and deterministic an-
nealing parameters, and 5% were used as test data
for the results reported below.
4 The Models
We compare performance of a number of probabil-
ity models for our verb argument data in order to
explore the dependencies of the data and the impact
of clustering. Graphical representations of the clus-
tering models are shown in Figure 1.
Unigram Baseline: This model assumes complete
independence of the verb, syntactic slot, and
noun, and serves to provide a baseline for the
complexity of the task:
P
1
(v; s; n) = P (v)P (s)P (n)
Bigram: This model predicts both the noun and
syntactic slot conditioned on the verb, but in-
dependently of one another:
P
2
(v; s; n) = P (v)P (sjv)P (njv)
Trigram: This is simply the empirical distribution
over triples of verb, slot, and noun:
P
3
(v; s; n) = P (v; s; n)
Three-way Aspect: Following Hofmann and
Puzicha (1998), we refer to EM-based cluster-
ing as the aspect model, where different values
of the cluster variable are intended to represent
abstract ?aspects? of the data. The simplest
version of the clustering model predicts verb,
slot, and noun independently given the cluster
variable c:
P
c
(v; s; n) = P (c)P (vjc)P (sjc)P (njc)
with all four component distributions being es-
timated by EM training.
Verb-Slot Aspect: This is the model of Rooth et al
(1999), in which the verb and slot are com-
bined into one atomic variable before the as-
pect model is trained:
P
c
vs
= P (c)P (v; sjc)P (njc)
Noun-Slot Aspect: A variation on the above model
combines the slot with the noun, rather than the
verb:
P
c
ns
= P (c)P (vjc)P (n; sjc)
Alternation: This model, described in more detail
above, introduces a new unobserved variable r
for the semantic role of the noun, which can
take two values:
P
alt
= P (c)P (vjc)P (sjc)P (rjs; c)P (njr; c)
      
      
      
      
      





     
     
     
     
     





     
     
     
     
     





cluster
verb slot noun      
     
     
     
     
     






     
     
     
     
     





cluster
verb,slot noun
     
     
     
     
     





     
     
     
     
     





     
     
     
     
     





verb
noun
slot
role
cluster
     
     
     
     
     





     
     
     
     
     





cluster
verb noun,slot
Alternation
Verb-Slot AspectThree-way Aspect
Noun-Slot Aspect
Figure 1: Graphical models: shading represents observed variables, arrows probabilistic dependencies.
Fixed Alternation: This model is designed to in-
corporate the assumption that the semantic
roles of the subject and object of the same verb
must be different. The independence assump-
tions are identical to those of the simple alter-
nation model:
P
alt
2
= P (c)P (vjc)P (sjc)P (rjs; c)P (njr; c)
but the probability P (rjs; c) is only trained for
s = subj-intrans. The model is constrained to
assign one value of the role variable to direct
objects, P (r = 0js = obj) = 1 and the other
role to subjects of transitives: P (r = 1js =
subj-trans) = 1.
5 Results
Perplexity results on held-out test data for each of
the models are shown in Table 2. Because models
2, 3, 5, and 6 will assign zero probability to certain
pairs of values not seen in the training data, they
were combined with the unigram baseline model in
order to obtain a perplexity over the entire test set
comparable to the other models. This was done
using linear interpolation, with the interpolation
weight optimized on the cross-validation data. Per-
plexity is the geometric mean of the reciprocal of
the probability assigned by the model to each triple
of verb, noun, and slot in the test data:
PP = e
 
1
N
P
i
log P (v
i
;n
i
;s
i
)
For the single-variable clustering models (4, 5
and 6) 128 values were allowed for the cluster
variable c. For the two-variable clustering mod-
els (7 and 8), 64 values for c and 2 values for the
unobserved semantic roles variable r were used,
making for a total of 128 distributions over nouns
(P (njr; c)) but only 64 over verbs (P (vjc)). The to-
tal number of parameters for each model is shown in
Table 2. Because deterministic annealing was used
to smooth the probability distributions for each clus-
ter and prevent overfitting the training data, the per-
plexities obtained were relatively insensitive to the
number of clusters used.
Of the clustering models, the Verb-Slot Aspect
model did the best, with a perplexity of 2.31M. It is
perhaps surprising how close the Three-way Aspect
model came, with a perplexity of 2.41M, despite the
fact that it models the noun as being independent
of the syntactic position for a given verb. One ex-
planation for this is that nouns in fact occur in all
three positions more frequently than we would ex-
pect from traditional accounts of alternation behav-
ior. This is shown in our corpus examples of Table
1 by the high frequency of door as a subject of an
transitive use of open. Even in the traditional al-
ternation pattern where a noun occurs in two of the
three positions, the Three-way Aspect model may
do better at capturing this overlap, even though it
will mistakenly assign probability mass to the same
nouns appearing in the third syntactic position, than
do models 5 and 6, which are not able to generalize
Model Test Perplexity Total Parameters
1. Unigram Baseline 5.50M 20,651
2. Bigram 2.95M 57.64M
3. Trigram 2.55M 172.88M
4. Three-way Aspect 2.41M 2.64M
5. Verb-Slot Aspect 2.31M 3.47M
6. Noun-Slot Aspect 2.66M 6.56M
7. Alternation 2.57M 2.43M
8. Fixed Alternation 2.60M 2.43M
9. Trigram+Verb-Slot Aspect 2.06M 176.36M
Table 2: Comparison of probability models
at all across the different arguments of a given verb.
The models specifically designed to capture alter-
nation behavior (7 and 8) did not do as well as the
generic clustering models. One explanation is that
the unconstrained models are able to fit the data bet-
ter by clustering together specific arguments of dif-
ferent verbs even when the two verbs do not share
the same alternation behavior. Examining the clus-
ters found by the Verb-Slot Aspect shows that it in
fact seems to find alternation behavior for specific
verbs despite the model?s inability to explicitly rep-
resent alternation. In many cases, two roles of the
same verb are assigned to the same cluster. Exam-
ples of the top ten members of sample clusters are
shown in Table 3. Examining the sample verbs of
Table 1, we see that the model assigns the direct
object and subject of intransitive slots of open to
the same cluster, implicitly representing the verb?s
alternation behavior, and in fact does the same for
the semantically related verbs close and shut. Sim-
ilarly, the direct object and subject of intransitive
slots of increase are assigned to the same cluster.
However, in an example of how the model can clus-
ter semantically related verbs that do not share the
same alternation behavior, the direct object slot of
reduce and the subject of transitive slot of exceed
are groups together with increase. Of particular in-
terest is the verb play, for which the model assigns
one cluster to each of the alternation patterns noted
in Table 1. Cluster 18 represents the alternation be-
tween direct object and subject of intransitive seen
with part, game, and music, while cluster 92 rep-
resents the agent relation expressed by subjects of
both transitive and intransitive sentences.
The final line of Table 2 represents an interpola-
tion of the best n-gram and best clustering model,
which further reduces perplexity to 2.06 million.
6 Conclusion
We have attempted to learn the mapping from syn-
tactic position to semantic role in an unsupervised
manner, and have evaluated the results in terms of
our systems? success as language model for unseen
data. The models designed to explicit represent verb
alternation behavior did not perform as well by this
metric as other, simpler probability models.
A perspective on this work can be gained by com-
parison with attempts at unsupervised learning of
other natural language phenomena including part-
of-speech tagging (Merialdo, 1994) and syntactic
dependencies (Carroll and Charniak, 1992; Paskin,
2001). While models trained using the Expectation
Maximization algorithm do well at fitting the data,
the results may not correspond to the human analy-
ses they were intended to learn. Language does not
exist in the abstract, but conveys information about
the world, and the ultimate goal of grammar induc-
tion is not just to model strings but to extract this
information. This suggests that although the proba-
bility models constrained to represent verb alterna-
tion behavior did not achieve the best perplexity re-
sults, they may be useful as part of an understanding
system which assigns semantic roles to arguments.
The implicit representation of alternation behavior
in our generic clustering model also suggests using
its clusters to initialize a more complex model capa-
ble of assigning semantic roles.
Acknowledgments This work was undertaken
with funding from the Institute for Research in Cog-
nitive Science at the University of Pennsylvania and
DoD Grant MDA904-00C-2136.
References
Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In Proceedings of the 1st
Annual Meeting of the North American Chapter of the
ACL (NAACL), pages 234?240, Seattle, Washington.
Cluster Id Verb-Slot Noun Cluster Id Verb-Slot Noun
57 door open-obj 18 part play-obj
mouth open-subj-intrans role form-obj
eyes close-obj lip take-obj
firebreak close-subj-intrans game bite-obj
gate shut-subj-intrans basis play-subj-intrans
shop slam-subj-intrans host lick-obj
window shut-obj parts curl-subj-intrans
way knock-obj music see-obj
exhibition reach-obj card constitute-obj
47 number increase-subj-intrans 92 people play-subj-intrans
amount require-obj man win-subj-intrans
supply reduce-obj child take-subj-trans
level increase-obj woman make-subj-trans
rate exceed-subj-trans one need-subj-trans
tooth need-obj the play-subj-trans
income include-obj band see-subj-trans
risk affect-obj group get-subj-intrans
activity show-obj team manage-subj-intrans
Table 3: Sample Clusters from Verb-Slot Aspect Model
Glenn Carroll and Eugene Charniak. 1992. Two experi-
ments on learning probabilistic dependency grammars
from corpora. In Workshop Notes for Statistically-
Based NLP Techniques, pages 1?13. AAAI.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In AAAI-
97, pages 598?603, Menlo Park, August. AAAI Press.
Ciprian Chelba and Frederick Jelinek. 1999. Recogni-
tion performance of a structured language model. In
EUROSPEECH.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th ACL, pages 16?23, Madrid, Spain.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Massachusetts.
Thomas Hofmann and Jan Puzicha. 1998. Statistical
models for co-occurrence data. Memo, Massachus-
setts Institute of Technology Artificial Intelligence
Laboratory, February.
Beth Levin. 1993. English Verb Classes And Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago.
David Magerman. 1995. Statistical decision-tree models
for parsing. In Proceedings of the 33rd ACL, Cam-
bridge, Massachusetts.
Mitchell P. Marcus, Grace Kim, Mary Ann Marcin-
kiewicz, Robert MacIntyre, Ann Bies, Mark Fergu-
son, Karen Katz, and Britta Schasberger. 1994. The
Penn Treebank: Annotating predicate argument struc-
ture. In ARPA Human Language Technology Work-
shop, pages 114?119, Plainsboro, NJ. Morgan Kauf-
mann.
Diana McCarthy. 2000. Using semantic preferences to
identify verbal participation in role switching alterna-
tions. In Proceedings of the 1st NAACL, pages 256?
263, Seattle, Washington.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155?172.
Paola Merlo and Suzanne Stevenson. 2001. Auto-
matic verb classification based on statistical distribu-
tion of argument structure. Computational Linguis-
tics, 27(3), September.
Mark Paskin. 2001. Grammatical bigrams. In T. Di-
etterich, S. Becker, and Z. Gharahmani, editors, Ad-
vances in Neural Information Processing Systems
(NIPS) 14. MIT Press.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Pro-
ceedings of the 31st ACL, pages 183?190, Columbus,
Ohio. ACL.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantically
annotated lexicon via EM-based clustering. In Pro-
ceedings of the 37th Annual Meeting of the ACL, pages
104?111, College Park, Maryland.
Sabine Schulte im Walde. 2000. Clustering verbs se-
mantically according to their alternation behaviour. In
In Proceedings of the 18th International Conference
on Computational Linguistics (COLING-00), pages
747?753, Saarbru?cken, Germany.
XTAG Research Group. 2001. A lexicalized tree adjoin-
ing grammar for English. Technical Report IRCS-01-
03, IRCS, University of Pennsylvania.
The Necessity of Parsing for Predicate Argument Recognition
Daniel Gildea and Martha Palmer
University of Pennsylvania
dgildea,mpalmer@cis.upenn.edu
Abstract
Broad-coverage corpora annotated with
semantic role, or argument structure, in-
formation are becoming available for the
rst time. Statistical systems have been
trained to automatically label seman-
tic roles from the output of statistical
parsers on unannotated text. In this pa-
per, we quantify the eect of parser accu-
racy on these systems' performance, and
examine the question of whether a at-
ter \chunked" representation of the in-
put can be as eective for the purposes
of semantic role identication.
1 Introduction
Over the past decade, most work in the eld of
information extraction has shifted from complex
rule-based, systems designed to handle a wide
variety of semantic phenomena including quan-
tication, anaphora, aspect and modality (e.g.
Alshawi (1992)), to simpler nite-state or sta-
tistical systems such as Hobbs et al (1997) and
Miller et al (1998). Much of the evaluation of
these systems has been conducted on extracting
relations for specic semantic domains such as
corporate acquisitions or terrorist events in the
framework of the DARPA Message Understand-
ing Conferences.
Recently, attention has turned to creating cor-
pora annotated for argument structure for a
broader range of predicates. The Propbank
project at the University of Pennsylvania (Kings-
bury and Palmer, 2002) and the FrameNet project
at the International Computer Science Institute
(Baker et al, 1998) share the goal of document-
ing the syntactic realization of arguments of the
predicates of the general English lexicon by an-
notating a corpus with semantic roles. Even for
a single predicate, semantic arguments often have
multiple syntactic realizations, as shown by the
following paraphrases:
(1) John will meet with Mary.
John will meet Mary.
John and Mary will meet.
(2) The door opened.
Mary opened the door.
Correctly identifying the semantic roles of the
sentence constituents is a crucial part of interpret-
ing text, and in addition to forming an important
part of the information extraction problem, can
serve as an intermediate step in machine trans-
lation or automatic summarization. In this pa-
per, we examine how the information provided by
modern statistical parsers such as Collins (1997)
and Charniak (1997) contributes to solving this
problem. We measure the eect of parser accu-
racy on semantic role prediction from parse trees,
and determine whether a complete tree is indeed
necessary for accurate role prediction.
Gildea and Jurafsky (2002) describe a statisti-
cal system trained on the data from the FrameNet
project to automatically assign semantic roles.
The system rst passed sentences through an au-
tomatic parser, extracted syntactic features from
the parses, and estimated probabilities for seman-
tic roles from the syntactic and lexical features.
Both training and test sentences were automat-
ically parsed, as no hand-annotated parse trees
were available for the corpus. While the errors
introduced by the parser no doubt negatively af-
fected the results obtained, there was no direct
way of quantifying this eect. Of the systems
evaluated for the Message Understanding Confer-
ence task, Miller et al (1998) made use of an inte-
grated syntactic and semantic model producing a
full parse tree, and achieved results comparable to
other systems that did not make use of a complete
parse. As in the FrameNet case, the parser was
not trained on the corpus for which semantic an-
notations were available, and the eect of better,
or even perfect, parses could not be measured.
One of the dierences between the two semantic
annotation projects is that the sentences chosen
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 239-246.
                         Proceedings of the 40th Annual Meeting of the Association for
for annotation for Propbank are from the same
Wall Street Journal corpus chosen for annotation
for the original Penn Treebank project, and thus
hand-checked syntactic parse trees are available
for the entire dataset. In this paper, we com-
pare the performance of a system based on gold-
standard parses with one using automatically gen-
erated parser output. We also examine whether it
is possible that the additional information con-
tained in a full parse tree is negated by the errors
present in automatic parser output, by testing a
role-labeling system based on a at or \chunked"
representation of the input.
2 The Data
The results in this paper are primarily derived
from the Propbank corpus, and will be compared
to earlier results from the FrameNet corpus. Be-
fore proceeding to the experiments, this section
will briey describe the similarities and dierences
between the two sets of data.
While the goals of the two projects are similar in
many respects, their methodologies are quite dif-
ferent. FrameNet is focused on semantic frames,
which are dened as schematic representation of
situations involving various participants, props,
and other conceptual roles (Fillmore, 1976). The
project methodology has proceeded on a frame-
by-frame basis, that is by rst choosing a semantic
frame, dening the frame and its participants or
frame elements, and listing the various lexical
predicates which invoke the frame, and then nd-
ing example sentences of each predicate in the cor-
pus (the British National Corpus was used) and
annotating each frame element. The example sen-
tences were chosen primarily for coverage of all
the syntactic realizations of the frame elements,
and simple examples of these realizations were
preferred over those involving complex syntactic
structure not immediate relevant to the lexical
predicate itself. From the perspective of an auto-
matic classication system, the overrepresentation
of rare syntactic realizations may cause the system
to perform more poorly than it might on more sta-
tistically representative data. On the other hand,
the exclusion of complex examples may make the
task articially easy. Only sentences where the
lexical predicate was used \in frame" were anno-
tated. A word with multiple distinct senses would
generally be analyzed as belonging to dierent
frames in each sense, but may only be found in the
FrameNet corpus in the sense for which a frame
has been dened. It is interesting to note that the
semantic frames are a helpful way of generalizing
between predicates; words in the same frame have
been found frequently to share the same syntactic
argument structure. A more complete description
of the FrameNet project can be found in (Baker
et al, 1998; Johnson et al, 2001), and the rami-
cations for automatic classication are discussed
more thoroughly in (Gildea and Jurafsky, 2002).
The philosophy of the Propbank project can be
likened to FrameNet without frames. While the
semantic roles of FrameNet are dened at the level
of the frame, in Propbank, roles are dened on a
per-predicate basis. The core arguments of each
predicate are simply numbered, while remaining
arguments are given labels such as \temporal" or
\locative". While the two types of label names are
reminiscent of the traditional argument/adjunct
distinction, this is primarily as a convenience in
dening roles, and no claims are intended as to
optionality or other traditional argument/adjunct
tests. To date, Propbank has addressed only
verbs, where FrameNet includes nouns and ad-
jectives. Propbank's annotation process has pro-
ceeded from the most to least common verbs, and
all examples of each verb from the corpus are an-
notated. Thus, the data for each predicate are
statistically representative of the corpus, as are
the frequencies of the predicates themselves. An-
notation takes place with reference to the Penn
Treebank trees | not only are annotators shown
the trees when analyzing a sentence, they are con-
strained to assign the semantic labels to portions
of the sentence corresponding to nodes in the tree.
Propbank annotators tag all examples of a given
verb, regardless of word sense. The tagging guide-
lines for a verb may contain many \rolesets", cor-
responding to word sense at a relatively coarse-
grained level. The need for multiple rolesets is
determined by the roles themselves, that is, uses
of the verb with dierent arguments are given sep-
arate rolesets. However, the preliminary version
of the data used in the experiments below are
not tagged for word sense, or for the roleset used.
Sense tagging is planned for a second pass through
the data. In many cases the roleset can be deter-
mined from the argument annotations themselves.
However, we did not make any attempt to distin-
guish sense in our experiments, and simply at-
tempted to predict argument labels based on the
identity of the lexical predicate.
3 The Experiments
In previous work using the FrameNet corpus,
Gildea and Jurafsky (2002) developed a system to
predict semantic roles from sentences and their
parse trees as determined by the statistical parser
of Collins (1997). We will briey review their
probability model before adapting the system to
handle unparsed data.
Probabilities of a parse constituent belonging
to a given semantic role were calculated from the
following features:
Phrase Type: This feature indicates the syntac-
tic type of the phrase expressing the semantic
roles: examples include noun phrase (NP),
verb phrase (VP), and clause (S). Phrase
types were derived automatically from parse
trees generated by the parser, as shown in
Figure 1. The parse constituent spanning
each set of words annotated as an argument
was found, and the constituent's nonterminal
label was taken as the phrase type. As an
example of how this feature is useful, in com-
munication frames, the Speaker is likely to
appear as a noun phrase, Topic as a prepo-
sitional phrase or noun phrase, and Medium
as a prepositional phrase, as in: \We talked
about the proposal over the phone." When
no parse constituent was found with bound-
aries matching those of an argument during
testing, the largest constituent beginning at
the argument's left boundary and lying en-
tirely within the element was used to calcu-
late the features.
Parse Tree Path: This feature is designed to
capture the syntactic relation of a constituent
to the predicate. It is dened as the path
from the predicate through the parse tree
to the constituent in question, represented
as a string of parse tree nonterminals linked
by symbols indicating upward or downward
movement through the tree, as shown in Fig-
ure 2. Although the path is composed as a
string of symbols, our systems will treat the
string as an atomic value. The path includes,
as the rst element of the string, the part of
speech of the predicate, and, as the last ele-
ment, the phrase type or syntactic category
of the sentence constituent marked as an ar-
gument.
Position: This feature simply indicates whether
the constituent to be labeled occurs before
or after the predicate dening the semantic
frame. This feature is highly correlated with
grammatical function, since subjects will gen-
erally appear before a verb, and objects after.
This feature may overcome the shortcom-
ings of reading grammatical function from the
parse tree, as well as errors in the parser out-
put.
S
NP VP
NP
He ate some pancakes
PRP
DT NN
VB
Figure 2: In this example, the path from the pred-
icate ate to the argument He can be represented
as VB"VP"S#NP, with " indicating upward move-
ment in the parse tree and # downward movement.
Voice: The distinction between active and pas-
sive verbs plays an important role in the con-
nection between semantic role and grammat-
ical function, since direct objects of active
verbs correspond to subjects of passive verbs.
From the parser output, verbs were classied
as active or passive by building a set of 10
passive-identifying patterns. Each of the pat-
terns requires both a passive auxiliary (some
form of \to be" or \to get") and a past par-
ticiple.
Head Word: Lexical dependencies provide im-
portant information in labeling semantic
roles, as one might expect from their use
in statistical models for parsing. Since the
parser used assigns each constituent a head
word as an integral part of the parsing model,
the head words of the constituents can be
read from the parser output. For example, in
a communication frame, noun phrases headed
by \Bill", \brother", or \he" are more likely
to be the Speaker, while those headed by
\proposal", \story", or \question" are more
likely to be the Topic.
To predict argument roles in new data, we
wish to estimate the probability of each role
given these ve features and the predicate p:
P (rjpt; path; position; voice; hw; p). Due to the
sparsity of the data, it is not possible to estimate
this probability from the counts in the training.
Instead, we estimate probabilities from various
subsets of the features, and interpolate a linear
combination of the resulting distributions. The
interpolation is performed over the most specic
distributions for which data are available, which
can be thought of as choosing the topmost distri-
butions available from a backo lattice, shown in
Figure 3.
He
PRP
NP
heard
VBD
the sound of liquid slurping in a metal container
NP
as
IN
Farrell
NNP
NP
approached
VBD
him
PRP
NP
from
IN
behind
NN
NP
PP
VP
S
SBAR
VP
S
predicate SourceGoalTheme
Figure 1: A sample sentence with parser output (above) and argument structure annotation (below).
Parse constituents corresponding to frame elements are highlighted.
P(r | h)
P(r | pt, voice)
P(r | h, pt, p) P(r | pt, voice, p)
P(r | pt, p)
P(r | p)
P(r | pt, path, p)
P(r | h, p)
Figure 3: Backo lattice with more specic distri-
butions towards the top.
We applied the same system, using the same
features to a preliminary release of the Propbank
data. The dataset used contained annotations for
26,138 predicate-argument structures containing
65,364 individual arguments and containing exam-
ples from 1,527 lexical predicates (types). In order
to provide results comparable with the statistical
parsing literature, annotations from Section 23 of
the Treebank were used as the test set; all other
sections were included in the training set.
The system was tested under two conditions,
one in which it is given the constituents which
are arguments to the predicate and merely has to
predict the correct role, and one in which it has to
both nd the arguments in the sentence and label
them correctly. Results are shown in Tables 1 and
2.
Although results for Propbank are lower than
for FrameNet, this appears to be primarily due to
the smaller number of training examples for each
predicate, rather than the dierence in annotation
style between the two corpora. The FrameNet
data contained at least ten examples from each
predicate, while 17% of the Propbank data had
fewer than ten training examples. Removing these
examples from the test set gives 84.1% accuracy
with gold-standard parses and 80.5% accuracy
with automatic parses.
As our path feature is a somewhat unusual way
of looking at parse trees, its behavior in the sys-
tem warrants a closer look. The path feature is
most useful as a way of nding arguments in the
unknown boundary condition. Removing the path
feature from the known-boundary system results
in only a small degradation in performance, from
82.3% to 81.7%. One reason for the relatively
small impact may be sparseness of the feature |
7% of paths in the test set are unseen in training
data. The most common values of the feature are
shown in Table 3, where the rst two rows cor-
respond to standard subject and object positions.
One reason for sparsity is seen in the third row:
in the Treebank, the adjunction of an adverbial
phrase or modal verb can cause an additional VP
node to appear in our path feature. We tried two
variations of the path feature to address this prob-
lem. The rst collapses sequences of nodes with
Accuracy
FrameNet Propbank Propbank
> 10 ex.
Gold-standard parses 82.8 84.1
Automatic parses 82.0 79.2 80.5
Table 1: Accuracy of semantic role prediction for known boundaries | the system is given the con-
stituents to classify.
FrameNet Propbank Propbank > 10
Precision Recall Precision Recall Precision Recall
Gold-standard parses 71.1 64.4 73.5 71.7
Automatic parses 64.6 61.2 57.7 50.0 59.0 55.4
Table 2: Accuracy of semantic role prediction for unknown boundaries | the system must identify the
correct constituents as arguments and give them the correct roles.
Path Frequency
VB"VP#NP 17:6%
VB"VP"S#NP 16:4
VB"VP"VP"S#NP 7:8
VB"VP#PP 7:6
VB"VP#PP#NP 7:3
VB"VP#SBAR#S 4:3
VB"VP#S 4:3
VB"VP#ADVP 2:4
1031 others 76:0
Table 3: Common values for parse tree path in
Propbank data, using gold-standard parses.
the same label, for example combining rows 2 and
3 of Table 3. The second variation uses only two
values for the feature: NP under S (subject posi-
tion), and NP under VP (object position). Nei-
ther variation improved performance in the known
boundary condition. As a gauge of how closely the
Propbank argument labels correspond to the path
feature overall, we note that by always assigning
the most common role for each path, for example
always assigning ARG0 to the subject position,
and using no other features, we obtain the correct
role 69.4% of the time, vs. 82.3% for the complete
system.
4 Is Parsing Necessary?
Many recent information extraction systems for
limited domains have relied on nite-state systems
that do not build a full parse tree for the sentence
being analyzed. Among such systems, (Hobbs et
al., 1997) built nite-state recognizers for vari-
ous entities, which were then cascaded to form
recognizers for higher-level relations, while (Ray
and Craven, 2001) used low-level \chunks" from
a general-purpose syntactic analyzer as observa-
tions in a trained Hidden Markov Model. Such
an approach has a large advantage in speed, as
the extensive search of modern statistical parsers
is avoided. It is also possible that this approach
may be more robust to error than parsers. Al-
though we expect the attachment decisions made
by a parser to be relevant to determining whether
a constituent of a sentence is an argument of a
particular predicate, and what its relation to the
predicate is, those decisions may be so frequently
incorrect that a much simpler system can do just
as well. In this section we test this hypothesis
by comparing a system which is given only a at,
\chunked" representation of the input sentence to
the parse-tree-based systems described above. In
this representation, base-level constituent bound-
aries and labels are present, but there are no de-
pendencies between constituents, as shown by the
following sample sentence:
(3) [
NP
Big investment banks] [
V P
refused to
step] [
ADV P
up] [
PP
to] [
NP
the plate]
[
V P
to support] [
NP
the beleaguered oor
traders] [
PP
by] [
V P
buying] [
NP
big blocks]
[
PP
of] [
NP
stock] , [
NP
traders] [
V P
say] .
Our chunks were derived from the Tree-
bank trees using the conversion described by
Tjong Kim Sang and Buchholz (2000). Thus,
the experiments were carried out using \gold-
standard" rather than automatically derived
chunk boundaries, which we believe will provide
an upper bound on the performance of a chunk-
based system.
The information provided by the parse tree can
be decomposed into three pieces: the constituent
boundaries, the grammatical relationship between
predicate and argument, expressed by our path
feature, and the head word of each candidate con-
stituent. We will examine the contribution of each
of these information sources, beginning with the
problem of assigning the correct role in the case
where the boundaries of the arguments in the sen-
tence are known, and then turning to the problem
of nding arguments in the sentence.
When the argument boundaries are known, the
grammatical relationship of the the constituent
to the predicate turns out to be of little value.
Removing the path feature from the system de-
scribed above results in only a small degradation
in performance, from 82.3% to 81.7%. While the
path feature serves to distinguish subjects from
objects, the combination of the constituent po-
sition before or after the predicate and the ac-
tive/passive voice feature serves the same purpose.
However, this result still makes use of the parser
output for nding the constituent's head word.
We implemented a simple algorithm to guess the
argument's head word from the chunked output: if
the argument begins at a chunk boundary, taking
the last word of the chunk, and in all other cases,
taking the rst word of the argument. This heuris-
tic matches the head word read from the parse tree
77% of the the time, as it correctly identies the
nal word of simple noun phrases as the head, the
preposition as the head of prepositional phrases,
and the complementizer as the head of sentential
complements. Using this process for determining
head words, the system drops to 77.0% accuracy,
indicating that identifying the relevant head word
from semantic role prediction is in itself an impor-
tant function of the parser. This chunker-based re-
sult is only slightly lower than the 79.2% obtained
using automatic parses in the known boundary
condition. These results for the known boundary
condition are summarized in Table 4.
Path Head Accuracy
gold parse gold parse 82.3
auto parse auto parse 79.2
not used gold parse 81.7
not used chunks 77.0
Table 4: Summary of results for known boundary
condition
We might expect the information provided by
the parser to be more important in identifying the
arguments in the sentence than in assigning them
the correct role. While it is easy to guess whether
a noun phrase is a subject or object given only
its position relative to the predicate, identifying
complex noun phrases and determining whether
they are arguments of a verb may be more di?cult
without the attachment information provided by
the parser.
To test this, we implemented a system in which
the argument labels were assigned to chunks, with
the path feature used by the parse-tree-based sys-
tem replaced by a number expressing the distance
in chunks to the left or right of the predicate.
Of the 3990 arguments in our test set, only
39.8% correspond to a single chunk in the at-
tened sentence representation, giving an upper
bound to the performance of this system. In par-
ticular, sentential complements (which comprise
11% of the data) and prepositional phrases (which
comprise 10%) always correspond to more than
one chunk, and therefore cannot be correctly la-
beled by our system which assigns roles to single
chunks. In fact, this system achieves 27.6% preci-
sion and 22.0% recall.
In order to see how much of the performance
degradation is caused by the di?culty of nding
exact argument boundaries in the chunked rep-
resentation, we can relax the scoring criteria to
count as correct all cases where the system cor-
rectly identies the rst chunk belonging to an
argument. For example, if the system assigns the
correct label to the preposition beginning a prepo-
sitional phrase, the argument will be counted as
correct, even though the system does not nd
the argument's righthand boundary. With this
scoring regime, the chunk-based system performs
at 49.5% precision and 35.1% recall, still signi-
cantly lower than the 57.7% precision/50.0% recall
for exact matches using automatically generated
parses. Results for the unknown boundary condi-
tion are summarized in Table 5.
Precision Recall
gold parse 71.1 64.4
auto parse 57.7 50.0
chunk 27.6 22.0
chunk, relaxed scoring 49.5 35.1
Table 5: Summary of results for unknown bound-
ary condition
As an example for comparing the behavior of
the tree-based and chunk-based systems, consider
the following sentence, with human annotations
showing the arguments of the predicate support:
(4) [
ARG0
Big investment banks] refused to step
up to the plate to support [
ARG1
the
beleaguered oor traders] [
MNR
by buying
big blocks of stock] , traders say .
Our tree-based system assigned the following anal-
ysis:
(5) Big investment banks refused to step up to
the plate to support [
ARG1
the beleaguered
oor traders] [
MNR
by buying big blocks of
stock] , traders say .
In this case, the system failed to nd the predi-
cate's ARG0 relation, because it is syntactically
distant from the verb support. The original Tree-
bank syntactic tree contains a trace which would
allow one to recover this relation, co-indexing the
empty subject position of support with the noun
phrase \Big investment banks". However, our
automatic parser output does not include such
traces, nor does our system make use of them.
The chunk-based system assigns the following ar-
gument labels:
(6) Big investment banks refused to step up to
[
ARG0
the plate] to support [
ARG1
the
beleaguered oor traders] by buying big
blocks of stock , traders say .
Here, as before, the true ARG0 relation is not
found, and it would be di?cult to imagine iden-
tifying it without building a complete syntactic
parse of the sentence. But now, unlike in the
tree-based output, the ARG0 label is mistakenly
attached to a noun phrase immediately before the
predicate. The ARG1 relation in direct object po-
sition is fairly easily identiable in the chunked
representation as a noun phrase directly follow-
ing the verb. The prepositional phrase expressing
the Manner relation, however, is not identied by
the chunk-based system. The tree-based system's
path feature for this constituent is VB"VP#PP,
which identies the prepositional phrase as at-
taching to the verb, and increases its probability
of being assigned an argument label. The chunk-
based system sees this as a prepositional phrase
appearing as the second chunk after the predi-
cate. Although this may be a typical position for
the Manner relation, the fact that the preposition
attaches to the predicate rather than to its direct
object is not represented.
In interpreting these results, it is important to
keep in mind the dierences between this task
and other information extraction datasets. In
comparison to the domain-specic relations eval-
uated by the Message Understanding Conference
(MUC) tasks, we have a wider variety of relations
but fewer training instances for each. The rela-
tions may themselves be less susceptible to nite
state methods. For example, a named-entity sys-
tem which indenties corporation names can go
a long way towards nding the \employment" re-
lation of MUC, and similarly systems for tagging
genes and proteins help a great deal for relations
in the biomedical domain. Both Propbank and
FrameNet tend to include longer arguments with
internal syntactic structure, making parsing deci-
sions more important in nding argument bound-
aries. They also involve abstract relations, with a
wide variety of possible llers for each role.
5 Conclusion
Our chunk-based system takes the last word of
the chunk as its head word for the purposes of
predicting roles, but does not make use of the
identities of the chunk's other words or the inter-
vening words between a chunk and the predicate,
unlike Hidden Markov Model-like systems such as
Bikel et al (1997), McCallum et al (2000) and
Laerty et al (2001). While a more elaborate
nite-state system might do better, it is possible
that additional features would not be helpful given
the small amount of data for each predicate. By
using a gold-standard chunking representation, we
have obtained higher performance over what could
be expected from an entirely automatic system
based on a at representation of the data.
We feel that our results show that statistical
parsers, although computationally expensive, do
a good job of providing relevant information for
semantic interpretation. Not only the constituent
structure but also head word information, pro-
duced as a side product, are important features.
Parsers, however, still have a long way to go.
Our results using hand-annotated parse trees show
that improvements in parsing should translate di-
rectly into better semantic interpretations.
Acknowledgments This work was undertaken
with funding from the Institute for Research in
Cognitive Science at the University of Pennsylva-
nia and from the Propbank project, DoD Grant
MDA904-00C-2136.
References
Hiyan Alshawi, editor. 1992. The Core Language
Engine. MIT Press, Cambridge, MA.
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet project.
In Proceedings of COLING/ACL, pages 86{90,
Montreal, Canada.
D. M. Bikel, S. Miller, R. Schwartz, and
R. Weischedel. 1997. Nymble: a high-
performance learning name-nder. In Proceed-
ings of the Fifth Conference on Applied Natural
Language Processing.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
AAAI-97, pages 598{603, Menlo Park, August.
AAAI Press.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the ACL,
pages 16{23, Madrid, Spain.
Charles J. Fillmore. 1976. Frame semantics
and the nature of language. In Annals of the
New York Academy of Sciences: Conference on
the Origin and Development of Language and
Speech, volume 280, pages 20{32.
Daniel Gildea and Daniel Jurafsky. 2002. Auto-
matic labeling of semantic roles. Computational
Linguistics, in press.
Jerry R. Hobbs, Douglas Appelt, John Bear,
David Israel, Megumi Kameyama, Mark E.
Stickel, and Mabry Tyson. 1997. FASTUS:
A cascaded nite-state transducer for extract-
ing information from natural-language text. In
Emmanuel Roche and Yves Schabes, editors,
Finite-State Language Processing, pages 383{
406. MIT Press, Cambridge, MA.
Christopher R. Johnson, Charles J. Fillmore,
Esther J. Wood, Josef Ruppenhofer, Mar-
garet Urban, Miriam R. L. Petruk, and
Collin F. Baker. 2001. The FrameNet
project: Tools for lexicon building. Version 0.7,
http://www.icsi.berkeley.edu/~framenet/book.html.
Paul Kingsbury and Martha Palmer. 2002. From
Treebank to PropBank. In Proceedings of the
3rd International Conference on Language Re-
sources and Evaluation (LREC-2002), Las Pal-
mas, Canary Islands, Spain.
John Laerty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random elds:
Probabilistic models for segmenting and label-
ing sequence data. In Machine Learning: Pro-
ceedings of the Eighteenth International Confer-
ence (ICML 2001), Stanford, California.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy Markov mod-
els for information extraction and segmenta-
tion. In Machine Learning: Proceedings of the
Seventeenth International Conference (ICML
2000), pages 591{598, Stanford, California.
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone,
Ralph Weischedel, and the Annotation Group.
1998. Algorithms that learn to extract informa-
tion { BBN: Description of the SIFT system as
used for MUC-7. In Proceedings of the Seventh
Message Understanding Conference (MUC-7),
April.
Soumya Ray and Mark Craven. 2001. Represent-
ing sentence structure in hidden markov model
for information extraction. In Seventeenth In-
ternational Joint Conference on Articial Intel-
ligence (IJCAI-01), Seattle, Washington.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the conll-2000 shared
task: Chunking. In Proceedings of CoNLL-2000
and LLL-2000, Lisbon, Portugal.
Loosely Tree-Based Alignment for Machine Translation
Daniel Gildea
University of Pennsylvania
dgildea@cis.upenn.edu
Abstract
We augment a model of translation based
on re-ordering nodes in syntactic trees in
order to allow alignments not conforming
to the original tree structure, while keep-
ing computational complexity polynomial
in the sentence length. This is done by
adding a new subtree cloning operation to
either tree-to-string or tree-to-tree align-
ment algorithms.
1 Introduction
Systems for automatic translation between lan-
guages have been divided into transfer-based ap-
proaches, which rely on interpreting the source
string into an abstract semantic representation
from which text is generated in the target lan-
guage, and statistical approaches, pioneered by
Brown et al (1990), which estimate parameters for
a model of word-to-word correspondences and word
re-orderings directly from large corpora of par-
allel bilingual text. Only recently have hybrid
approaches begun to emerge, which apply prob-
abilistic models to a structured representation of
the source text. Wu (1997) showed that restrict-
ing word-level alignments between sentence pairs
to observe syntactic bracketing constraints signif-
icantly reduces the complexity of the alignment
problem and allows a polynomial-time solution.
Alshawi et al (2000) also induce parallel tree struc-
tures from unbracketed parallel text, modeling the
generation of each node?s children with a finite-state
transducer. Yamada and Knight (2001) present an
algorithm for estimating probabilistic parameters for
a similar model which represents translation as a se-
quence of re-ordering operations over children of
nodes in a syntactic tree, using automatic parser out-
put for the initial tree structures. The use of explicit
syntactic information for the target language in this
model has led to excellent translation results (Ya-
mada and Knight, 2002), and raises the prospect of
training a statistical system using syntactic informa-
tion for both sides of the parallel corpus.
Tree-to-tree alignment techniques such as prob-
abilistic tree substitution grammars (Hajic? et al,
2002) can be trained on parse trees from parallel
treebanks. However, real bitexts generally do not
exhibit parse-tree isomorphism, whether because of
systematic differences between how languages ex-
press a concept syntactically (Dorr, 1994), or simply
because of relatively free translations in the training
material.
In this paper, we introduce ?loosely? tree-based
alignment techniques to address this problem. We
present analogous extensions for both tree-to-string
and tree-to-tree models that allow alignments not
obeying the constraints of the original syntactic tree
(or tree pair), although such alignments are dispre-
ferred because they incur a cost in probability. This
is achieved by introducing a clone operation, which
copies an entire subtree of the source language syn-
tactic structure, moving it anywhere in the target
language sentence. Careful parameterization of the
probability model allows it to be estimated at no ad-
ditional cost in computational complexity. We ex-
pect our relatively unconstrained clone operation to
allow for various types of structural divergence by
providing a sort of hybrid between tree-based and
unstructured, IBM-style models.
We first present the tree-to-string model, followed
by the tree-to-tree model, before moving on to align-
ment results for a parallel syntactically annotated
Korean-English corpus, measured in terms of align-
ment perplexities on held-out test data, and agree-
ment with human-annotated word-level alignments.
2 The Tree-to-String Model
We begin by summarizing the model of
Yamada and Knight (2001), which can be thought
of as representing translation as an Alexander
Calder mobile. If we follow the process of an
English sentence?s transformation into French,
the English sentence is first given a syntactic tree
representation by a statistical parser (Collins, 1999).
As the first step in the translation process, the
children of each node in the tree can be re-ordered.
For any node with m children, m! re-orderings are
possible, each of which is assigned a probability
Porder conditioned on the syntactic categories of
the parent node and its children. As the second
step, French words can be inserted at each node
of the parse tree. Insertions are modeled in two
steps, the first predicting whether an insertion to
the left, an insertion to the right, or no insertion
takes place with probability Pins , conditioned on
the syntactic category of the node and that of its
parent. The second step is the choice of the inserted
word P
t
(f jNULL), which is predicted without
any conditioning information. The final step, a
French translation of each original English word,
at the leaves of the tree, is chosen according to a
distribution P
t
(f je). The French word is predicted
conditioned only on the English word, and each
English word can generate at most one French
word, or can generate a NULL symbol, representing
deletion. Given the original tree, the re-ordering,
insertion, and translation probabilities at each node
are independent of the choices at any other node.
These independence relations are analogous to those
of a stochastic context-free grammar, and allow for
efficient parameter estimation by an inside-outside
Expectation Maximization (EM) algorithm. The
computation of inside probabilities ?, outlined
below, considers possible reordering of nodes in the
original tree in a bottom-up manner:
for all nodes ?
i
in input tree T do
for all k, l such that 1 < k < l < N do
for all orderings ? of the children ?
1
...?
m
of ?
i
do
for all partitions of span k, l into k
1
, l
1
...k
m
, l
m
do
?(?
i
, k, l)+= Porder (?|?i)
?
m
j=1
?(?
j
, k
j
, l
j
)
end for
end for
end for
end for
This algorithm has computational complexity
O(jT jNm+2), where m is the maximum number of
children of any node in the input tree T , and N
the length of the input string. By storing partially
completed arcs in the chart and interleaving the in-
ner two loops, complexity of O(jT jn3m!2m) can be
achieved. Thus, while the algorithm is exponential
in m, the fan-out of the grammar, it is polynomial in
the size of the input string. Assuming jT j = O(n),
the algorithm is O(n4).
The model?s efficiency, however, comes at a cost.
Not only are many independence assumptions made,
but many alignments between source and target sen-
tences simply cannot be represented. As a minimal
example, take the tree:
A
B
X Y
Z
Of the six possible re-orderings of the three ter-
minals, the two which would involve crossing the
bracketing of the original tree (XZY and YZX)
are not allowed. While this constraint gives us a
way of using syntactic information in translation,
it may in many cases be too rigid. In part to deal
with this problem, Yamada and Knight (2001) flat-
ten the trees in a pre-processing step by collapsing
nodes with the same lexical head-word. This allows,
for example, an English subject-verb-object (SVO)
structure, which is analyzed as having a VP node
spanning the verb and object, to be re-ordered as
VSO in a language such as Arabic. Larger syntactic
divergences between the two trees may require fur-
ther relaxation of this constraint, and in practice we
expect such divergences to be frequent. For exam-
ple, a nominal modifier in one language may show
up as an adverbial in the other, or, due to choices
such as which information is represented by a main
verb, the syntactic correspondence between the two
SVP
NP
NNC
NNC
Kyeo-ul
PAD
e
PAU
Neun
VP
NP
NNC
NNC
Su-Kap
PCA
eul
VP
NP
NNU
Myeoch
NNX
NNX
Khyeol-Re
XSF
Ssik
VP
NP
NNC
Ci-Keup
LV
VV
VV
Pat
EFN
Ci
S
VP
VP
VP
VP
NP
NNC
Ci-Keup
NULL
LV
VV
VV
Pat
NULL
EFN
Ci
NULL
NP
NNU
Myeoch
how
NNX
XSF
Ssik
many
NNX
Khyeol-Re
pairs
NP
NNC
NNC
Su-Kap
gloves
PCA
eul
NULL
NP
VP
LV
VV
VV
Pat
each
EFN
Ci
you
NP
NNC
Ci-Keup
issued
NNC
PAD
e
in
PAU
Neun
NULL
NNC
Kyeo-ul
winter
Figure 1: Original Korean parse tree, above, and transformed tree after reordering of children, subtree
cloning (indicated by the arrow), and word translation. After the insertion operation (not shown), the tree?s
English yield is: How many pairs of gloves is each of you issued in winter?
sentences may break down completely.
2.1 Tree-to-String Clone Operation
In order to provide some flexibility, we modify the
model in order to allow for a copy of a (translated)
subtree from the English sentences to occur, with
some cost, at any point in the resulting French sen-
tence. For example, in the case of the input tree
A
B
X Y
Z
a clone operation making a copy of node 3 as a new
child of B would produce the tree:
A
B
X Z Y
Z
This operation, combined with the deletion of the
original node Z, produces the alignment (XZY)
that was disallowed by the original tree reorder-
ing model. Figure 1 shows an example from our
Korean-English corpus where the clone operation al-
lows the model to handle a case of wh-movement in
the English sentence that could not be realized by
any reordering of subtrees of the Korean parse.
The probability of adding a clone of original node
?
i
as a child of node ?
j
is calculated in two steps:
first, the choice of whether to insert a clone under
?
j
, with probability Pins(clonej?j), and the choice
of which original node to copy, with probability
P
clone
(?
i
jclone = 1) = Pmakeclone(?i)?
k
Pmakeclone(?k)
where Pmakeclone is the probability of an original
node producing a copy. In our implementation, for
simplicity, Pins(clone) is a single number, estimated
by the EM algorithm but not conditioned on the par-
ent node ?
j
, and Pmakeclone is a constant, meaning
that the node to be copied is chosen from all the
nodes in the original tree with uniform probability.
It is important to note that Pmakeclone is not de-
pendent on whether a clone of the node in ques-
tion has already been made, and thus a node may
be ?reused? any number of times. This indepen-
dence assumption is crucial to the computational
tractability of the algorithm, as the model can be
estimated using the dynamic programming method
above, keeping counts for the expected number of
times each node has been cloned, at no increase in
computational complexity. Without such an assump-
tion, the parameter estimation becomes a problem
of parsing with crossing dependencies, which is ex-
ponential in the length of the input string (Barton,
1985).
3 The Tree-to-Tree Model
The tree-to-tree alignment model has tree transfor-
mation operations similar to those of the tree-to-
string model described above. However, the trans-
formed tree must not only match the surface string
of the target language, but also the tree structure as-
signed to the string by the treebank annotators. In or-
der to provide enough flexibility to make this possi-
ble, additional tree transformation operations allow
a single node in the source tree to produce two nodes
in the target tree, or two nodes in the source tree to
be grouped together and produce a single node in
the target tree. The model can be thought of as a
synchronous tree substitution grammar, with proba-
bilities parameterized to generate the target tree con-
ditioned on the structure of the source tree.
The probability P (T
b
jT
a
) of transforming the
source tree T
a
into target tree T
b
is modeled in a
sequence of steps proceeding from the root of the
target tree down. At each level of the tree:
1. At most one of the current node?s children is
grouped with the current node in a single ele-
mentary tree, with probability P
elem
(t
a
j?
a
)
children(?
a
)), conditioned on the current
node ?
a
and its children (ie the CFG produc-
tion expanding ?
a
).
2. An alignment of the children of the current
elementary tree is chosen, with probability
Palign(?j?a ) children(ta)). This alignment
operation is similar to the re-order operation
in the tree-to-string model, with the extension
that 1) the alignment ? can include insertions
and deletions of individual children, as nodes
in either the source or target may not corre-
spond to anything on the other side, and 2) in
the case where two nodes have been grouped
into t
a
, their children are re-ordered together in
one step.
In the final step of the process, as in the tree-to-
string model, lexical items at the leaves of the tree
are translated into the target language according to a
distribution P
t
(f je).
Allowing non-1-to-1 correspondences between
nodes in the two trees is necessary to handle the
fact that the depth of corresponding words in the
two trees often differs. A further consequence of
allowing elementary trees of size one or two is that
some reorderings not allowed when reordering the
children of each individual node separately are now
possible. For example, with our simple tree
A
B
X Y
Z
if nodes A and B are considered as one elementary
tree, with probability P
elem
(t
a
jA ) BZ), their col-
lective children will be reordered with probability
Palign(f(1, 1)(2, 3)(3, 2)gjA ) XYZ)
A
X Z Y
giving the desired word ordering XZY. However,
computational complexity as well as data sparsity
prevent us from considering arbitrarily large ele-
mentary trees, and the number of nodes considered
at once still limits the possible alignments. For ex-
ample, with our maximum of two nodes, no trans-
formation of the tree
A
B
W X
C
Y Z
is capable of generating the alignment WYXZ.
In order to generate the complete target tree, one
more step is necessary to choose the structure on the
target side, specifically whether the elementary tree
has one or two nodes, what labels the nodes have,
and, if there are two nodes, whether each child at-
taches to the first or the second. Because we are
ultimately interested in predicting the correct target
string, regardless of its structure, we do not assign
probabilities to these steps. The nonterminals on the
target side are ignored entirely, and while the align-
ment algorithm considers possible pairs of nodes as
elementary trees on the target side during training,
the generative probability model should be thought
of as only generating single nodes on the target side.
Thus, the alignment algorithm is constrained by the
bracketing on the target side, but does not generate
the entire target tree structure.
While the probability model for tree transforma-
tion operates from the top of the tree down, prob-
ability estimation for aligning two trees takes place
by iterating through pairs of nodes from each tree in
bottom-up order, as sketched below:
for all nodes ?
a
in source tree T
a
in bottom-up order do
for all elementary trees t
a
rooted in ?
a
do
for all nodes ?
b
in target tree T
b
in bottom-up order do
for all elementary trees t
b
rooted in ?
b
do
for all alignments ? of the children of t
a
and t
b
do
?(?
a
, ?
b
) +=
Pelem(ta|?a)Palign(?|?i)
?
(i,j)??
?(?
i
, ?
j
)
end for
end for
end for
end for
end for
The outer two loops, iterating over nodes in each
tree, require O(jT j2). Because we restrict our el-
ementary trees to include at most one child of the
root node on either side, choosing elementary trees
for a node pair is O(m2), where m refers to the max-
imum number of children of a node. Computing the
alignment between the 2m children of the elemen-
tary tree on either side requires choosing which sub-
set of source nodes to delete, O(22m), which subset
of target nodes to insert (or clone), O(22m), and how
to reorder the remaining nodes from source to target
tree, O((2m)!). Thus overall complexity of the algo-
rithm is O(jT j2m242m(2m)!), quadratic in the size
of the input sentences, but exponential in the fan-out
of the grammar.
3.1 Tree-to-Tree Clone Operation
Allowing m-to-n matching of up to two nodes
on either side of the parallel treebank allows for
limited non-isomorphism between the trees, as in
Hajic? et al (2002). However, even given this flexi-
bility, requiring alignments to match two input trees
rather than one often makes tree-to-tree alignment
more constrained than tree-to-string alignment. For
example, even alignments with no change in word
order may not be possible if the structures of the
two trees are radically mismatched. This leads us
to think it may be helpful to allow departures from
Tree-to-String Tree-to-Tree
elementary tree grouping P
elem
(t
a
j?
a
) children(?
a
))
re-order Porder (?j? ) children(?)) Palign(?j?a ) children(ta))
insertion Pins(left, right, nonej?) ? can include ?insertion? symbol
lexical translation P
t
(f je) P
t
(f je)
with cloning Pins(clonej?) ? can include ?clone? symbol
Pmakeclone(?) Pmakeclone(?)
Table 1: Model parameterization
the constraints of the parallel bracketing, if it can
be done in without dramatically increasing compu-
tational complexity.
For this reason, we introduce a clone operation,
which allows a copy of a node from the source tree to
be made anywhere in the target tree. After the clone
operation takes place, the transformation of source
into target tree takes place using the tree decomposi-
tion and subtree alignment operations as before. The
basic algorithm of the previous section remains un-
changed, with the exception that the alignments ?
between children of two elementary trees can now
include cloned, as well as inserted, nodes on the tar-
get side. Given that ? specifies a new cloned node
as a child of ?
j
, the choice of which node to clone is
made as in the tree-to-string model:
P
clone
(?
i
jclone 2 ?) = Pmakeclone(?i)?
k
Pmakeclone(?k)
Because a node from the source tree is cloned with
equal probability regardless of whether it has al-
ready been ?used? or not, the probability of a clone
operation can be computed under the same dynamic
programming assumptions as the basic tree-to-tree
model. As with the tree-to-string cloning operation,
this independence assumption is essential to keep
the complexity polynomial in the size of the input
sentences.
For reference, the parameterization of all four
models is summarized in Table 1.
4 Data
For our experiments, we used a parallel Korean-
English corpus from the military domain (Han et al,
2001). Syntactic trees have been annotated by hand
for both the Korean and English sentences; in this
paper we will be using only the Korean trees, mod-
eling their transformation into the English text. The
corpus contains 5083 sentences, of which we used
4982 as training data, holding out 101 sentences for
evaluation. The average Korean sentence length was
13 words. Korean is an agglutinative language, and
words often contain sequences of meaning-bearing
suffixes. For the purposes of our model, we rep-
resented the syntax trees using a fairly aggressive
tokenization, breaking multimorphemic words into
separate leaves of the tree. This gave an average
of 21 tokens for the Korean sentences. The aver-
age English sentence length was 16. The maximum
number of children of a node in the Korean trees
was 23 (this corresponds to a comma-separated list
of items). 77% of the Korean trees had no more
than four children at any node, 92% had no more
than five children, and 96% no more than six chil-
dren. The vocabulary size (number of unique types)
was 4700 words in English, and 3279 in Korean ?
before splitting multi-morphemic words, the Korean
vocabulary size was 10059. For reasons of compu-
tation speed, trees with more than 5 children were
excluded from the experiments described below.
5 Experiments
We evaluate our translation models both in terms
agreement with human-annotated word-level align-
ments between the sentence pairs. For scoring
the viterbi alignments of each system against gold-
standard annotated alignments, we use the alignment
error rate (AER) of Och and Ney (2000), which
measures agreement at the level of pairs of words:1
AER = 1 ? 2jA \ Gj
jAj + jGj
1While Och and Ney (2000) differentiate between sure and
possible hand-annotated alignments, our gold standard align-
ments come in only one variety.
Alignment
Error Rate
IBM Model 1 .37
IBM Model 2 .35
IBM Model 3 .43
Tree-to-String .42
Tree-to-String, Clone .36
Tree-to-String, Clone Pins = .5 .32
Tree-to-Tree .49
Tree-to-Tree, Clone .36
Table 2: Alignment error rate on Korean-English corpus
where A is the set of word pairs aligned by the au-
tomatic system, and G the set algned in the gold
standard. We provide a comparison of the tree-based
models with the sequence of successively more com-
plex models of Brown et al (1993). Results are
shown in Table 2.
The error rates shown in Table 2 represent the
minimum over training iterations; training was
stopped for each model when error began to in-
crease. IBM Models 1, 2, and 3 refer to
Brown et al (1993). ?Tree-to-String? is the model
of Yamada and Knight (2001), and ?Tree-to-String,
Clone? allows the node cloning operation of Section
2.1. ?Tree-to-Tree? indicates the model of Section 3,
while ?Tree-to-Tree, Clone? adds the node cloning
operation of Section 3.1. Model 2 is initialized from
the parameters of Model 1, and Model 3 is initialized
from Model 2. The lexical translation probabilities
P
t
(f je) for each of our tree-based models are initial-
ized from Model 1, and the node re-ordering proba-
bilities are initialized uniformly. Figure 1 shows the
viterbi alignment produced by the ?Tree-to-String,
Clone? system on one sentence from our test set.
We found better agreement with the human align-
ments when fixing Pins(left) in the Tree-to-String
model to a constant rather than letting it be deter-
mined through the EM training. While the model
learned by EM tends to overestimate the total num-
ber of aligned word pairs, fixing a higher probability
for insertions results in fewer total aligned pairs and
therefore a better trade-off between precision and
recall. As seen for other tasks (Carroll and Char-
niak, 1992; Merialdo, 1994), the likelihood crite-
rion used in EM training may not be optimal when
evaluating a system against human labeling. The
approach of optimizing a small number of metapa-
rameters has been applied to machine translation by
Och and Ney (2002). It is likely that the IBM mod-
els could similarly be optimized to minimize align-
ment error ? an open question is whether the opti-
mization with respect to alignment error will corre-
spond to optimization for translation accuracy.
Within the strict EM framework, we found
roughly equivalent performance between the IBM
models and the two tree-based models when making
use of the cloning operation. For both the tree-to-
string and tree-to-tree models, the cloning operation
improved results, indicating that adding the flexibil-
ity to handle structural divergence is important when
using syntax-based models. The improvement was
particularly significant for the tree-to-tree model, be-
cause using syntactic trees on both sides of the trans-
lation pair, while desirable as an additional source of
information, severely constrains possible alignments
unless the cloning operation is allowed.
The tree-to-tree model has better theoretical com-
plexity than the tree-to-string model, being quadratic
rather than quartic in sentence length, and we found
this to be a significant advantage in practice. This
improvement in speed allows longer sentences and
more data to be used in training syntax-based mod-
els. We found that when training on sentences of up
60 words, the tree-to-tree alignment was 20 times
faster than tree-to-string alignment. For reasons of
speed, Yamada and Knight (2002) limited training
to sentences of length 30, and were able to use only
one fifth of the available Chinese-English parallel
corpus.
6 Conclusion
Our loosely tree-based alignment techniques allow
statistical models of machine translation to make use
of syntactic information while retaining the flexibil-
ity to handle cases of non-isomorphic source and tar-
get trees. This is achieved with a clone operation pa-
rameterized in such a way that alignment probabili-
ties can be computed with no increase in asymptotic
computational complexity.
We present versions of this technique both for
tree-to-string models, making use of parse trees for
one of the two languages, and tree-to-tree models,
which make use of parallel parse trees. Results in
terms of alignment error rate indicate that the clone
operation results in better alignments in both cases.
On our Korean-English corpus, we found roughly
equivalent performance for the unstructured IBM
models, and the both the tree-to-string and tree-to-
tree models when using cloning. To our knowl-
edge these are the first results in the literature for
tree-to-tree statistical alignment. While we did not
see a benefit in alignment error from using syntactic
trees in both languages, there is a significant practi-
cal benefit in computational efficiency. We remain
hopeful that two trees can provide more information
than one, and feel that extensions to the ?loosely?
tree-based approach are likely to demonstrate this
using larger corpora.
Another important question we plan to pursue is
the degree to which these results will be borne out
with larger corpora, and how the models may be re-
fined as more training data is available. As one ex-
ample, our tree representation is unlexicalized, but
we expect conditioning the model on more lexical
information to improve results, whether this is done
by percolating lexical heads through the existing
trees or by switching to a strict dependency repre-
sentation.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45?60.
G. Edward Barton, Jr. 1985. On the complexity of ID/LP
parsing. Computational Linguistics, 11(4):205?218.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Frederick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16(2):79?85, June.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Glenn Carroll and Eugene Charniak. 1992. Two experi-
ments on learning probabilistic dependency grammars
from corpora. In Workshop Notes for Statistically-
Based NLP Techniques, pages 1?13. AAAI.
Michael John Collins. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Bonnie J. Dorr. 1994. Machine translation divergences:
A formal description and proposed solution. Compu-
tational Linguistics, 20(4):597?633.
Jan Hajic?, Martin ?Cmejrek, Bonnie Dorr, Yuan Ding, Ja-
son Eisner, Daniel Gildea, Terry Koo, Kristen Parton,
Gerald Penn, Dragomir Radev, and Owen Rambow.
2002. Natural language generation in the context of
machine translation. Technical report, Center for Lan-
guage and Speech Processing, Johns Hopkins Univer-
sity, Baltimore. Summer Workshop Final Report.
Chung-hye Han, Na-Rae Han, and Eon-Suk Ko. 2001.
Bracketing guidelines for Penn Korean treebank.
Technical Report IRCS-01-010, IRCS, University of
Pennsylvania.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155?172.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL-
00, pages 440?447, Hong Kong, October.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of ACL-02,
Philadelphia, PA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):3?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of ACL-
01, Toulouse, France.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical MT. In Proceedings of ACL-
02, Philadelphia, PA.
Identifying Semantic Roles Using Combinatory Categorial Grammar
Daniel Gildea and Julia Hockenmaier
University of Pennsylvania
{dgildea,juliahr}@cis.upenn.edu
Abstract
We present a system for automatically
identifying PropBank-style semantic roles
based on the output of a statistical parser
for Combinatory Categorial Grammar.
This system performs at least as well as
a system based on a traditional Treebank
parser, and outperforms it on core argu-
ment roles.
1 Introduction
Correctly identifying the semantic roles of sentence
constituents is a crucial part of interpreting text, and
in addition to forming an important part of the infor-
mation extraction problem, can serve as an interme-
diate step in machine translation or automatic sum-
marization. Even for a single predicate, semantic
arguments can have multiple syntactic realizations,
as shown by the following paraphrases:
(1) John will meet with Mary.
John will meet Mary.
John and Mary will meet.
(2) The door opened.
Mary opened the door.
Recently, attention has turned to creating cor-
pora annotated with argument structures. The
PropBank (Kingsbury and Palmer, 2002) and the
FrameNet (Baker et al, 1998) projects both doc-
ument the variation in syntactic realization of the
arguments of predicates in general English text.
Gildea and Palmer (2002) developed a system to
predict semantic roles (as defined in PropBank) from
sentences and their parse trees as determined by the
statistical parser of Collins (1999). In this paper, we
examine how the syntactic representations used by
different statistical parsers affect the performance
of such a system. We compare a parser based on
Combinatory Categorial Grammar (CCG) (Hocken-
maier and Steedman, 2002b) with the Collins parser.
As the CCG parser is trained and tested on a cor-
pus of CCG derivations that have been obtained by
automatic conversion from the Penn Treebank, we
are able to compare performance using both gold-
standard and automatic parses for both CCG and the
traditional Treebank representation. The Treebank-
parser returns skeletal phrase-structure trees with-
out the traces or functional tags in the original Penn
Treebank, whereas the CCG parser returns word-
word dependencies that correspond to the under-
lying predicate-argument structure, including long-
range dependencies arising through control, raising,
extraction and coordination.
2 Predicate-argument relations in
PropBank
The Proposition Bank (Kingsbury and Palmer,
2002) provides a human-annotated corpus of
semantic verb-argument relations. For each verb
appearing in the corpus, a set of semantic roles is
defined. Roles for each verb are simply numbered
Arg0, Arg1, Arg2, etc. As an example, the entry-
specific roles for the verb offer are given below:
Arg0 entity offering
Arg1 commodity
Arg2 price
Arg3 benefactive or entity offered to
These roles are then annotated for every instance
of the verb appearing in the corpus, including the
following examples:
(3) [ARG0 the company] to offer [ARG1 a 15% stake]
to [ARG2 the public].
(4) [ARG0 Sotheby?s] ... offered [ARG2 the Dorrance
heirs] [ARG1 a money-back guarantee]
(5) [ARG1 an amendment] offered by [ARG0 Rep.
Peter DeFazio]
(6) [ARG2 Subcontractors] will be offered [ARG1 a
settlement]
A variety of additional roles are assumed
to apply across all verbs. These secondary
roles can be thought of as being adjuncts,
rather than arguments, although no claims are
made as to optionality or other traditional argu-
ment/adjunct tests. The secondary roles include:
Location in Tokyo, outside
Time last week, on Tuesday, never
Manner easily, dramatically
Direction south, into the wind
Cause due to pressure from Washington
Discourse however, also, on the other hand
Extent 15%, 289 points
Purpose to satisfy requirements
Negation not, n?t
Modal can, might, should, will
Adverbial (none of the above)
and are represented in PropBank as ?ArgM? with an
additional function tag, for example ArgM-TMP for
temporal. We refer to PropBank?s numbered argu-
ments as ?core? arguments. Core arguments repre-
sent 75% of the total labeled roles in the PropBank
data. Our system predicts all the roles, including
core arguments as well as the ArgM labels and their
function tags.
3 Predicate-argument relations in CCG
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000), is a grammatical theory which provides
a completely transparent interface between surface
syntax and underlying semantics, such that each
syntactic derivation corresponds directly to an in-
terpretable semantic representation which includes
long-range dependencies that arise through control,
raising, coordination and extraction.
In CCG, words are assigned atomic cate-
gories such as NP, or functor categories like
(S[dcl]\NP)/NP (transitive declarative verb) or S/S
(sentential modifier). Adjuncts are represented
as functor categories such as S/S which expect
and return the same type. We use indices to
number the arguments of functor categories, eg.
(S[dcl]\NP
1
)/NP
2
, or S/S
1
, and indicate the word-
word dependencies in the predicate-argument struc-
ture as tuples ?w
h
, c
h
, i, w
a
?, where c
h
is the lexical
category of the head word w
h
, and w
a
is the head
word of the constituent that fills the ith argument of
c
h
.
Long-range dependencies can be projected
through certain types of lexical categories or
through rules such as coordination of functor
categories. For example, in the lexical category of a
relative pronoun, (NP\NP
i
)/(S[dcl]/NP
i
), the head
of the NP that is missing from the relative clause
is unified with (as indicated by the indices i) the
head of the NP that is modified by the entire relative
clause.
Figure 1 shows the derivations of an ordinary
sentence, a relative clause and a right-node-raising
construction. In all three sentences, the predicate-
argument relations between London and denied and
plans and denied are the same, which in CCG is
expressed by the fact that London fills the first (ie.
subject) argument slot of the lexical category of de-
nied, (S[dcl]\NP
1
)/NP
2
, and plans fills the second
(object) slot. The relations extracted from the CCG
derivation for the sentence ?London denied plans on
Monday? are shown in Table 1.
The CCG parser returns the local and long-range
word-word dependencies that express the predicate-
argument structure corresponding to the derivation.
These relations are recovered with an accuracy of
around 83% (labeled recovery) or 91% (unlabeled
recovery) (Hockenmaier, 2003). By contrast, stan-
dard Treebank parsers such as (Collins, 1999) only
return phrase-structure trees, from which non-local
dependencies are difficult to recover.
S[dcl]
NP
1
N
London
S[dcl]\NP
1
S[dcl]\NP
1
(S[dcl]\NP
1
)/NP
2
denied
NP
2
N
plans
(S\NP)\(S\NP)
((S\NP)\(S\NP))/NP
on
NP
N
Monday
NP
NP
2
N
plans
NP\NP
2
(NP\NP
i
)/(S[dcl]/NP
i
)
that
S[dcl]/NP
2
S/(S\NP
1
)
NP
N
London
(S[dcl]\NP
1
)/NP
2
denied
S[dcl]
S[dcl]/NP
2
S[dcl]/NP
2
S/(S\NP
1
)
NP
N
London
(S[dcl]\NP
1
)/NP
2
denied
S[dcl]/NP[conj]
conj
but
S[dcl]/NP
S/(S\NP)
NP
N
Paris
(S[dcl]\NP)/NP
admitted
NP
2
N
plans
Figure 1: CCG derivation trees for three clauses containing the same predicate-argument relations.
w
h
c
h
i w
a
denied (S[dcl]\NP
1
)/NP
2
1 London
denied (S[dcl]\NP
1
)/NP
2
2 plans
on ((S\NP
1
)\(S\NP)
2
)/NP
3
2 denied
on ((S\NP
1
)\(S\NP)
2
)/NP
3
3 Monday
Table 1: CCG predicate-argument relations for the
sentence ?London denied plans on Monday?
The CCG parser has been trained and tested on
CCGbank (Hockenmaier and Steedman, 2002a), a
treebank of CCG derivations obtained from the Penn
Treebank, from which we also obtain our training
data.
4 Mapping between PropBank and
CCGbank
Our aim is to use CCG derivations as input to a sys-
tem for automatically producing the argument labels
of PropBank. In order to do this, we wish to cor-
relate the CCG relations above with PropBank ar-
guments. PropBank argument labels are assigned
to nodes in the syntactic trees from the Penn Tree-
bank. While the CCGbank is derived from the Penn
Treebank, in many cases the constituent structures
do not correspond. That is, there may be no con-
stituent in the CCG derivation corresponding to the
same sequence of words as a particular constituent
in the Treebank tree. For this reason, we compute
the correspondence between the CCG derivation and
the PropBank labels at the level of head words. For
each role label for a verb?s argument in PropBank,
we first find the head word for its constituent accord-
ing to the the head rules of (Collins, 1999). We then
look for the label of the CCG relation between this
head word and the verb itself.
5 The Experiments
In previous work using the PropBank corpus,
Gildea and Palmer (2002) developed a system to
predict semantic roles from sentences and their
parse trees as determined by the statistical parser of
Collins (1999). We will briefly review their proba-
bility model before adapting the system to incorpo-
rate features from the CCG derivations.
5.1 The model of Gildea and Palmer (2002)
For the Treebank-based system, we use the proba-
bility model of Gildea and Palmer (2002). Proba-
bilities of a parse constituent belonging to a given
semantic role are calculated from the following fea-
tures:
The phrase type feature indicates the syntactic
type of the phrase expressing the semantic roles: ex-
amples include noun phrase (NP), verb phrase (VP),
and clause (S).
The parse tree path feature is designed to capture
the syntactic relation of a constituent to the pred-
icate. It is defined as the path from the predicate
through the parse tree to the constituent in question,
represented as a string of parse tree nonterminals
linked by symbols indicating upward or downward
movement through the tree, as shown in Figure 2.
Although the path is composed as a string of sym-
bols, our systems will treat the string as an atomic
value. The path includes, as the first element of the
string, the part of speech of the predicate, and, as the
last element, the phrase type or syntactic category of
the sentence constituent marked as an argument.
S
NP VP
NP
He ate some pancakes
PRP
DT NN
VB
Figure 2: In this example, the path from the predi-
cate ate to the argument NP He can be represented as
VB?VP?S?NP, with ? indicating upward movement
in the parse tree and ? downward movement.
The position feature simply indicates whether the
constituent to be labeled occurs before or after the
predicate. This feature is highly correlated with
grammatical function, since subjects will generally
appear before a verb, and objects after. This feature
may overcome the shortcomings of reading gram-
matical function from the parse tree, as well as errors
in the parser output.
The voice feature distinguishes between active
and passive verbs, and is important in predicting se-
mantic roles because direct objects of active verbs
correspond to subjects of passive verbs. An instance
of a verb was considered passive if it is tagged as
a past participle (e.g. taken), unless it occurs as a
descendent verb phrase headed by any form of have
(e.g. has taken) without an intervening verb phrase
headed by any form of be (e.g. has been taken).
The head word is a lexical feature, and provides
information about the semantic type of the role filler.
Head words of nodes in the parse tree are determined
using the same deterministic set of head word rules
used by Collins (1999).
The system attempts to predict argument roles
in new data, looking for the highest probabil-
ity assignment of roles r
i
to all constituents i
in the sentence, given the set of features F
i
=
{pt
i
, path
i
, pos
i
, v
i
, h
i
} at each constituent in the
parse tree, and the predicate p:
argmax
r
1..n
P (r
1..n
|F
1..n
, p)
We break the probability estimation into two
parts, the first being the probability P (r
i
|F
i
, p) of
a constituent?s role given our five features for the
consituent, and the predicate p. Due to the sparsity
of the data, it is not possible to estimate this proba-
bility from the counts in the training data. Instead,
probabilities are estimated from various subsets of
the features, and interpolated as a linear combina-
tion of the resulting distributions. The interpolation
is performed over the most specific distributions for
which data are available, which can be thought of as
choosing the topmost distributions available from a
backoff lattice, shown in Figure 3.
P(r | h)
P(r | h, pt, p)
P(r | pt, p)
P(r | p)
P(r | pt, path, p)
P(r | h, p)
P(r | pt, pos, v, p)
P(r | pt, pos, v)
Figure 3: Backoff lattice with more specific distri-
butions towards the top.
The probabilities P (r
i
|F
i
, p) are combined with
the probabilities P ({r
1..n
}|p) for a set of roles ap-
pearing in a sentence given a predicate, using the
following formula:
P (r
1..n
|F
1..n
, p) ? P ({r
1..n
}|p)
?
i
P (r
i
|F
i
, p)
P (r
i
|p)
This approach, described in more detail in
Gildea and Jurafsky (2002), allows interaction be-
tween the role assignments for individual con-
stituents while making certain independence as-
sumptions necessary for efficient probability estima-
tion. In particular, we assume that sets of roles ap-
pear independent of their linear order, and that the
features F of a constituents are independent of other
constituents? features given the constituent?s role.
5.2 The model for CCG derivations
In the CCG version, we replace the features above
with corresponding features based on both the sen-
tence?s CCG derivation tree (shown in Figure 1)
and the CCG predicate-argument relations extracted
from it (shown in Table 1).
The parse tree path feature, designed to capture
grammatical relations between constituents, is re-
placed with a feature defined as follows: If there is
a dependency in the predicate-argument structure of
the CCG derivation between two words w and w?,
the path feature from w to w? is defined as the lexical
category of the functor, the argument slot i occupied
by the argument, plus an arrow (? or?) to indicate
whether w or w? is the categorial functor. For exam-
ple, in our sentence ?London denied plans on Mon-
day?, the relation connecting the verb denied with
plans is (S[dcl]\NP)/NP.2.?, with the left arrow
indicating the lexical category included in the rela-
tion is that of the verb, while the relation connecting
denied with on is ((S\NP)\(S\NP))/NP.2.?, with
the right arrow indicating the the lexical category in-
cluded in the relation is that of the modifier.
If the CCG derivation does not define a predicate-
argument relation between the two words, we use
the parse tree path feature described above, defined
over the CCG derivation tree. In our training data,
77% of PropBank arguments corresponded directly
to a relation in the CCG predicate-argument repre-
sentation, and the path feature was used for the re-
maining 23%. Most of these mismatches arise be-
cause the CCG parser and PropBank differ in their
definition of head words. For instance, the CCG
parser always assumes that the head of a PP is
the preposition, whereas PropBank roles can be as-
signed to the entire PP (7), or only to the NP argu-
ment of the preposition (8), in which case the head
word comes from the NP:
(7) ... will be offered [PPARGM-LOC in the U.S].
(8) to offer ...[PP to [NPARG2 the public]].
In embedded clauses, CCG assumes that the head is
the complementizer, whereas in PropBank, the head
comes from the embedded sentence itself. In com-
plex verb phrases (eg. ?might not have gone?), the
CCG parser assumes that the first auxiliary (might)
is head, whereas PropBank assumes it is the main
verb (gone). Therefore, CCG assumes that not mod-
ifies might, whereas PropBank assumes it modi-
fies gone. Although the head rules of the parser
could in principle be changed to reflect more di-
rectly the dependencies in PropBank, we have not
attempted to do so yet. Further mismatches occur
because the predicate-argument structure returned
by the CCG parser only contains syntactic depen-
dencies, whereas the PropBank data also contain
some anaphoric dependencies, eg.:
(9) [ARG0 Realist ?s] negotiations to acquire
Ammann Laser Technik AG...
(10) When properly applied, [ARG0 the adhesive] is
designed to...
Such dependencies also do not correspond to a rela-
tion in the predicate-argument structure of the CCG
derivation, and cause the path feature to be used.
The phrase type feature is replaced with the lex-
ical category of the maximal projection of the Prop-
Bank argument?s head word in the CCG derivation
tree. For example, the category of plans is N, and
the category of denied is (S[dcl]\NP)/NP.
The voice feature can be read off the CCG cate-
gories, since the CCG categories of past participles
carry different features in active and passive voice
(eg. sold can be (S[pt]\NP)/NP or S[pss]\NP).
The head word of a constituent is indicated in the
derivations returned by the CCG parser.
SARG0
NP
NNP
London
VP
VBD
denied
ARG1
NP
NNS
plans
ARGM-TMP
PP
IN
on
NP
NNP
Monday
S[dcl]ARG0
NP
N
London
S[dcl]\NP
S[dcl]\NP
(S[dcl]\NP)/NP
denied
ARG1
NP
N
plans
ARGM-TMP
(S\NP)\(S\NP)
((S\NP)\(S\NP))/NP
on
NP
N
Monday
Figure 4: A sample sentence as produced by the Treebank parser (left) and by the CCG parser (right). Nodes
are annotated with PropBank roles ARG0, ARG1 and ARGM-TMP.
Treebank-based CCG-based
Features extracted from Args Precision Recall F-score Precision Recall F-score
Automatic parses core 75.9 69.6 72.6 76.1 73.5 74.8
all 72.6 61.2 66.4 71.0 63.1 66.8
Gold-standard parses core 85.5 81.7 83.5 82.4 78.6 80.4
all 78.8 69.9 74.1 76.3 67.8 71.8
Gold-standard w/o traces core 77.6 75.2 76.3
all 74.4 66.5 70.2
Table 2: Accuracy of semantic role prediction
5.3 Data
We use data from the November 2002 release of
PropBank. The dataset contains annotations for
72,109 predicate-argument structures with 190,815
individual arguments (of which 75% are core, or
numbered, arguments) and has includes examples
from 2462 lexical predicates (types). Annotations
from Sections 2 through 21 of the Treebank were
used for training; Section 23 was the test set. Both
parsers were trained on Sections 2 through 21.
6 Results
Because of the mismatch between the constituent
structures of CCG and the Treebank, we score both
systems according to how well they identify the head
words of PropBank?s arguments. Table 2 gives the
performance of the system on both PropBank?s core,
or numbered, arguments, and on all PropBank roles
including the adjunct-like ArgM roles. In order to
analyze the impact of errors in the syntactic parses,
we present results using features extracted from both
automatic parser output and the gold standard parses
in the Penn Treebank (without functional tags) and
in CCGbank. Using the gold standard parses pro-
vides an upper bound on the performance of the sys-
tem based on automatic parses. Since the Collins
parser does not provide trace information, its up-
per bound is given by the system tested on the
gold-standard Treebank representation with traces
removed. In Table 2, ?core? indicates results on
PropBank?s numbered arguments (ARG0...ARG5)
only, and ?all? includes numbered arguments as well
as the ArgM roles. Most of the numbered argu-
ments (in particular ARG0 and ARG1) correspond
to arguments that the CCG category of the verb di-
rectly subcategorizes for. The CCG-based system
outperforms the system based on the Collins parser
on these core arguments, and has comparable perfor-
mance when all PropBank labels are considered. We
believe that the superior performance of the CCG
system on this core arguments is due to its ability to
recover long-distance dependencies, whereas we at-
tribute its lower performance on non-core arguments
mainly to the mismatches between PropBank and
CCGbank.
The importance of long-range dependencies for
our task is indicated by the fact that the performance
on the Penn Treebank gold standard without traces
Treebank-based CCG-based
Scoring Precision Recall F-score Precision Recall F-score
Automatic parses Head word 72.6 61.2 66.4 71.0 63.1 66.8
Boundary 68.6 57.8 62.7 55.7 49.5 52.4
Gold-standard parses Head word 77.6 75.2 76.3 76.3 67.8 71.8
(Treebank: w/o traces) Boundary 74.4 66.5 70.2 67.5 60.0 63.5
Table 3: Comparison of scoring regimes, using automatic parser output and gold standard parses. The first
row in this table corresponds to the second row in Table 2.
is significantly lower than that on the Penn Treebank
with trace information. Long-range dependencies
are especially important for core arguments, shown
by the fact that removing trace information from the
Treebank parses results in a bigger drop for core
arguments (83.5 to 76.3 F-score) than for all roles
(74.1 to 70.2). The ability of the CCG parser to re-
cover these long-range dependencies accounts for its
higher performance, and in particular its higher re-
call, on core arguments.
The CCG gold standard performance is below
that of the Penn Treebank gold standard with traces.
We believe this performance gap to be caused by
the mismatches between the CCG analyses and the
PropBank annotations described in Section 5.2. For
the reasons described, the head words of the con-
stituents that have PropBank roles are not necessar-
ily the head words that stand in a predicate-argument
relation in CCGbank. If two words do not stand in a
predicate-argument relation, the CCG system takes
recourse to the path feature. This feature is much
sparser in CCG: since CCG categories encode sub-
categorization information, the number of categories
in CCGbank is much larger than that of Penn Tree-
bank labels. Analysis of our system?s output shows
that the system trained on the Penn Treebank gold
standard obtains 55.5% recall on those relations that
require the CCG path feature, whereas the system
using CCGbank only achieves 36.9% recall on these.
Also, in CCG, the complement-adjunct distinction
is represented in the categories for the complement
(eg. PP) or adjunct (eg. (S\NP)\(S\NP) and in
the categories for the head (eg. (S[dcl]\NP)/PP
or S[dcl]\NP). In generating the CCGbank, various
heuristics were used to make this distinction. In par-
ticular, for PPs, it depends on the ?closely-related?
(CLR) function tag, which is known to be unreli-
able. The decisions made in deriving the CCGbank
often do not match the hand-annotated complement-
adjunct distinctions in PropBank, and this inconsis-
tency is likely to make our CCGbank-based features
less predictive. A possible solution is to regenerate
the CCGbank using the Propbank annotations.
The impact of our head-word based scoring is an-
alyzed in Table 3, which compares results when only
the head word must be correctly identified (as in Ta-
ble 2) and to results when both the beginning and
end of the argument must be correctly identified in
the sentence (as in Gildea and Palmer (2002)). Even
if the head word is given the correct label, the bound-
aries of the entire argument may be different from
those given in the PropBank annotation. Since con-
stituents in CCGbank do not always match those in
PropBank, even the CCG gold standard parses ob-
tain comparatively low scores according to this met-
ric. This is exacerbated when automatic parses are
considered.
7 Conclusion
Our CCG-based system for automatically labeling
verb arguments with PropBank-style semantic roles
outperforms a system using a traditional Treebank-
based parser for core arguments, which comprise
75% of the role labels, but scores lower on adjunct-
like roles such as temporals and locatives. The CCG
parser returns predicate-argument structures that in-
clude long-range dependencies; therefore, it seems
inherently better suited for this task. However, the
performance of our CCG system is lowered by the
fact that the syntactic analyses in its training corpus
differ from those that underlie PropBank in impor-
tant ways (in particular in the notion of heads and the
complement-adjunct distinction). We would expect
a higher performance for the CCG-based system if
the analyses in CCGbank resembled more closely
those in PropBank.
Our results also indicate the importance of recov-
ering long-range dependencies, either through the
trace information in the Penn Treebank, or directly,
as in the predicate-argument structures returned by
the CCG parser. We speculate that much of the
performance improvement we show could be ob-
tained with traditional (ie. non-CCG-based) parsers
if they were designed to recover more of the infor-
mation present in the Penn Treebank, in particular
the trace co-indexation. An interesting experiment
would be the application of our role-labeling sys-
tem to the output of the trace recovery system of
Johnson (2002). Our results also have implications
for parser evaluation, as the most frequently used
constituent-based precision and recall measures do
not evaluate how well long-range dependencies can
be recovered from the output of a parser. Measures
based on dependencies, such as those of Lin (1995)
and Carroll et al (1998), are likely to be more rele-
vant to real-world applications of parsing.
Acknowledgments This work was supported by the In-
stitute for Research in Cognitive Science at the University of
Pennsylvania, the Propbank project (DoD Grant MDA904-00C-
2136), an EPSRC studentship and grant GR/M96889, and NSF
ITR grant 0205 456. We thank Mark Steedman, Martha Palmer
and Alexandra Kinyon for their comments on this work.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of COLING/ACL, pages 86?90, Montreal.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998.
Parser evaluation: a survey and a new proposal. In
Proceedings of the 1st International Conference on
Language Resources and Evaluation, pages 447?454,
Granada, Spain.
Michael John Collins. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The necessity
of syntactic parsing for predicate argument recogni-
tion. In Proceedings of the 40th Annual Conference of
the Association for Computational Linguistics (ACL-
02), Philadelphia, PA.
Julia Hockenmaier and Mark Steedman. 2002a. Acquir-
ing Compact Lexicalized Grammars from a Cleaner
Treebank. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 1974?1981, Las Palmas.
Julia Hockenmaier and Mark Steedman. 2002b. Gener-
ative models for statistical parsing with Combinatory
Categorial Grammar. In Proceedings of the 40th An-
nual Conference of the Association for Computational
Linguistics (ACL-02), Philadelphia, PA.
Julia Hockenmaier. 2003. Data and models for statis-
tical parsing with Combinatory Categorial Grammar.
Ph.D. thesis, School of Informatics, University of Ed-
inburgh.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Annual Confer-
ence of the Association for Computational Linguistics
(ACL-02), Philadelphia, PA.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to PropBank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2002), Las Palmas.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings of
the 19th International Joint Conference on Artificial
Intelligence (IJCAI-95), pages 1420?1425, Montreal.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge Mass.
Skeletons in the parser: Using a shallow parser to improve deep parsing
Mary Swift, James Allen, and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
 swift,allen,gildea@cs.rochester.edu
Abstract
We describe a simple approach for integrating
shallow and deep parsing. We use phrase struc-
ture bracketing obtained from the Collins parser
as filters to guide deep parsing. Our exper-
iments demonstrate that our technique yields
substantial gains in speed along with modest
improvements in accuracy.
1 Introduction
The detailed linguistic analyses generated by deep
parsing are an essential component of spoken dia-
log systems that collaboratively perform tasks with
users (e.g., (Allen et al, 2001)). For example, inter-
pretation in the TRIPS collaborative dialog assistant
relies on the representation produced by its parser
for word sense disambiguation, constituent depen-
dencies, and semantic roles such as agent, theme,
goal, etc. Broad coverage unification-based deep
parsers, however, unavoidably have problems meet-
ing the very high accuracy and efficiency require-
ments needed for real-time dialog. On the other
hand, parsers based on lexicalized probabilistic con-
text free grammars such those of Collins (1999) and
Charniak (1997), which we call shallow parsers1,
are robust and efficient, but the structural represen-
tations obtained with such parsers are insufficient as
input for intelligent reasoning. In addition, they are
not accurate when exact match is considered as op-
posed to constituent recall and precision and bracket
crossing. For example, the standard Collins parser
yields an exact match on only 36% on the standard
test set (section 23) of the Wall Street Journal Cor-
pus.
In this paper we explore the question of whether
preprocessing with a shallow parser can produce
analyses that are good enough to help improve the
speed and accuracy of deep parsing. Previous work
on German (Frank et al, 2002) pursued a similar
strategy and showed promising results after consid-
erable effort transforming the output of the shal-
1We do not intend the ?chunking? sense of shallow parsing
? all our parsers return tree structures.
low parser into useful guidance to the deep parser.
We were interested in seeing if we could take a
shallow parser off the shelf, namely the Collins
parser, and use its output fairly directly to improve
the performance of the TRIPS parser. It has been
reported that stochastic parsers degrade in perfor-
mance on domains different than what they were
trained on (Hwa, 1999; Gildea, 2001), so there re-
ally was an issue whether the output would be good
enough. In particular, we are taking the Collins
parser trained on the Wall Street Journal and ap-
plying it unchanged to spontaneous human-human
dialog in an emergency rescue task domain. We
have found that there are islands of reliability in the
results from the Collins parser that can be used to
substantially improve the performance of the TRIPS
parser.
The remainder of the paper is organized as fol-
lows. Section 2.1 provides background on the Mon-
roe corpus, a set of task-oriented dialogs that is the
basis for the parser evaluations. In section 2.2 we
describe the TRIPS parser and the representation it
produces for reasoning. In section 3 we describe the
preliminary evaluations we carried out by running
the Collins parser over the Monroe corpus. We then
describe our experiments in combining the parsers
under different conditions. We look at different con-
ditions, first seeing how this method can improve
overall parsing of our corpus, and then with real-
time parsing conditions, as required for spoken di-
alog systems. We find we can get substantial effi-
ciency improvements on the corpus parsing, which
mostly disappear when we look at the semi-real-
time case. In the latter, however, we do see some
improvement in coverage.
2 Background
2.1 The Monroe Corpus
Our data consists of transcribed dialogs between
two humans engaged in carefully designed tasks
in simulated emergency management situations in
Monroe County, New York (Stent, 2001). The sce-
nario was designed to encourage collaborative prob-
U We also have to send a road crew there as well
S So we probably can?t actually send the ambulance over the bridge
U You?re probably right
U Because it?s going to take another two hours
U So we?ll actually run out of time if we wait for that
U So I guess we?ll need to send them
U Actually could we send them up fifteen across two fifty two down three eighty three
U Take that way around
S Wait
S The generator?s going downtown
S Right
U The generator is going to two fifty two
S Oh oh I see the problem
U So if we go up fifteen or go south on fifteen
S And then go up three eighty three
U Two fifty two
S Three eighty three
U And that?ll get us all the way over to the person with pneumonia or the person who needs the generator
U Say at the most it takes an hour
U It should take no more than an hour to get the generator over to that person
S Okay
S So we have the people taken care of
Figure 1: Excerpt from Monroe dialog
lem solving and mixed initiative interaction involv-
ing complex planning and coordination between the
participants, so the communication is very sponta-
neous and interactive. The corpus is split into utter-
ances, and the speech repairs are marked and auto-
matically removed for these tests. Utterances that
are incomplete or uninterpretable (by humans) are
also marked and eliminated from the corpus. The
remaining utterances form the set on which we have
been developing and testing the grammar. Figure 1
shows an excerpt from one of the dialogs.
The entire Monroe corpus consists of 20 dialogs
ranging from about 7 minutes up to 40 minutes in
length. Our tests here focus on a subset of five di-
alogs that have been used to drive the grammar de-
velopment: s2, s4, s12, s16 and s17 (henceforth di-
alogs 1, 2, 3, 4 and 5), constituting 1556 parseable
utterances.2
2.2 The TRIPS Parser
The deep parser we used is a robust parsing sys-
tem developed in the TRIPS system over the past
five years being driven from five different domains.
The grammatical formalism and parsing framework
is essentially a lexicalized version of the formalism
described in (Allen, 1995). It is a GPSG/HPSG
(Pollard and Sag, 1994) inspired unification gram-
mar of approximately 1300 rules with a rich model
of semantic features (Dzikovska, 2004). The parser
2Parseable utterances exclude utterances that are incom-
plete or ungrammatical (see (Tetreault et al, 2004).)
is an agenda-driven best-first chart parser that sup-
ports experimentation with different parsing strate-
gies, although in practice we almost always use a
straightforward bi-directional bottom-up algorithm.
As an illustration of its flexibility, the modifications
required to perform this experiment required adding
only one function of ten lines of code. The grammar
used for these experiments is the same TRIPS gram-
mar used in all our applications, and the rules have
hand-tuned weights. The weights of newly derived
constituents are computed exactly as in a PCFG al-
gorithm, the only difference being that the weights
don?t necessarily add to 1 and so are not probabil-
ities.3 The TRIPS parser does not use a maximum
entropy model (cf. the XLE system (Kaplan et al,
2004)) because there is insufficient training data and
it is as yet unclear how such as model would per-
form at the detailed level of semantic representation
produced by the TRIPS parser (see Figure 2 and dis-
cussion below).
The rules, lexicon, and semantic ontology are in-
dependent of any specific domain but tailored to
human-computer practical dialog. The grammar
is fairly extensive in coverage (and still growing),
and has quite good coverage of a corpus of human-
human dialogs in the Monroe domain, an emer-
gency management domain (Swift et al, 2004). The
3We have a version of the grammar that uses a non-
lexicalized PCFG model, but it was not used here as it does
not perform as well. Thus we are using our best model, making
it the most challenging to show improvement.
SA_TELL LF::FILL-CONTAINER
:content
A (SET-OF LF::FRUIT)
:theme
:goal
THE (SET-OF LF::FRUIT):subset
:of
THE LF::LAND-VEHICLE
LF::WEIGHT-UNIT POUND
:QUANTITY
LF::QMODIFIER MIN
:quan
300:is
LF::NUMBER
:mods
(SPEECHACT V38109 SA_TELL :CONTENT V37618)
(F V37618 (LF::FILL-CONTAINER LOAD) :GOAL V37800 :THEME V38041
:TMA ((TENSE PAST) (PASSIVE +)))
(THE V37800 (LF::LAND-VEHICLE TRUCK))
(A V38041 (SET-OF (LF::FRUIT ORANGE)) :QUANTITY V37526 :SUBSET V37539)
(QUANTITY-TERM V37526 (LF::WEIGHT-UNIT POUND) :QUAN V37479)
(QUANTITY-TERM V37479 LF::NUMBER :MODS (V38268))
(F V38268 (LF::QMODIFIER MIN) :OF V37479 :IS V37523)
(QUANTITY-TERM V37523 LF::NUMBER :VALUE 300)
(THE V37539 (SET-OF (LF::FRUIT ORANGE)))
Figure 2: Parser logical form (together with a graphical approximation of the semantic content) for At least
three hundred pounds of the oranges were put in the truck.
system is in active use in our spoken dialog un-
derstanding work in several different domains. It
operates in close to real-time for short utterances,
but degrades in performance as utterances become
longer than 8 or 9 words. As one way to control
ambiguity, the grammar makes use of selectional re-
strictions. Our semantic model utilizes two related
mechanisms: first, an ontology of the predicates
that are used to create the logical forms, and sec-
ond, a vector of semantic features associated with
these predicates that are used for selectional restric-
tions. The grammar computes a flattened and un-
scoped logical form using reified events (see also
(Copestake et al, 1997) for a flat semantic represen-
tation), with many of its word senses derived from
FrameNet frames (Johnson and Fillmore, 2000) and
semantic roles (Fillmore, 1968). An example of the
logical form representation produced by the parser
is shown in Figure 2, in both a dependency graph
(upper) and the actual parser output (lower).4
4Term constructors appearing at the leftmost edge of terms
in the parser output are F (relation), A (indefinite entity),
THE (definite entity) and QUANTITY-TERM (numeric ex-
pressions).
3 Collins Parser Evaluation
As a pilot experiment, we evaluated the perfor-
mance of the Collins parser on a single dialog of
167 sentences from the Monroe corpus, dialog 3.
We extracted context-free grammar backbones from
our TRIPS gold standard parses to score the Collins?
output against. The evaluation was complicated by
difference in tree formats, illustrated in Figure 3.
The two parsers use a different (though closely re-
lated) set of syntactic categories. The TRIPS struc-
ture generally has more levels of structure (roughly
corresponding to levels in X-bar theory) than the
Penn Treebank analyses (Marcus et al, 1993), in
particular for base noun phrases.
We converted the TRIPS category labels to their
nearest equivalent in Penn Treebank inventory be-
fore scoring the Collins parser in terms of la-
beled precision and recall of constituents, the stan-
dard measures in the statistical parsing community.
Overall recall was 32%, while precision was 64%.
While we expect the Collins parser to have low
recall (it generates fewer constituents overall), the
low precision indicates that simply relabeling con-
stituents on a one-for-one basis is not sufficient to
resolve the differences in the two formalisms. Pre-
cision and recall broken down by constituent type is
shown in Table 1.
TOP
S
NP
PRP
I
VP
VBP
have
NP
NP
DT
a
NN
bomb
NN
attack
PP
IN
at
NP
DT
the
NN
airport
UTT
S
NP
PRO
I
VP
VP-
V
have
NP
SPEC
DET
ART
a
N1
N1
N
bomb
N1
N1
N
attack
ADVBL
ADV
at
NP
SPEC
DET
ART
the
N1
N
airport
Figure 3: Skeleton tree output from the Collins parser (left) and the TRIPS parser (right) for I have a bomb
attack at the airport.
However, 82% of the sentences have no cross-
ing brackets in the Collins parse. That is, while the
parser may not generate the same set of constituents,
it generates very few constituents that straddle the
boundaries of any constituent in the TRIPS parse.
At this level, the parsers agree about the structure
of the sentences to a degree that is perhaps surpris-
ing given the very different domain on which the
Collins parser is trained. This indicates that the
low performance on the other measures has more
to do with differences in the annotation style than
real mistakes by the Collins parser.
The high level of agreement on unlabeled brack-
etings led us to believe that the Collins structure
could be used as a filter for constituents generated
by the TRIPS parser. We tested this strategy in ex-
periments reported in the following section.
4 Experiments
In all the experiments, we used a subset of five di-
alogs (consisting of 1326 utterances) from the Mon-
roe corpus, described in 2.1. Pilot trials were con-
ducted on dialog 3 (167 utterances), and the exper-
iments were run with the remaining dialogs (1, 2, 4
and 5).
4.1 Method
The first experiment evaluates whether we can ex-
tract information from the Collins output that is reli-
able enough to provide significant improvements to
the TRIPS parser. In order to compare our perfor-
mance with (Frank et al, 2002), the test only uses
utterances for which we have a gold-standard. In
addition, we report our experiments only on utter-
ances 6 words or longer (with an average of 10.3
words per utterance), as shorter utterances pose lit-
tle problem for the TRIPS parser and thus running
the Collins pre-processing step would not be pro-
ductive.
We parsed dialogs 1, 2, 4 and 5 with the Collins
parser, and extracted the phrase-level bracketing for
the most reliable constituents (those which has a
precision of at least 60%) in our pilot study: NP, VP
and ADVP.5 From this information we constructed
a parse skeleton for each utterance, such as the one
shown in Figure 4.
For our experiments we modified the TRIPS
parser so that when a constituent is to be added to
the chart, if the constituent type and its start and end
positions are found in the skeleton then the ranking
for that constituent is boosted by a small amount. In
pilot trials we determined the optimal boost weight
to be 3% (see Table 2).
With a broad coverage grammar, it is possible that
the parser could run almost indefinitely on sentences
that are difficult to parse. Thus we set an upper limit
on the number of constituents that can be added to
the chart before the parser quits. The parser runs
until it finds a complete analysis or hits this upper
5The Collins parse time for the 309 utterances of 6 words or
longer was 30 seconds.
label gold recall produced precision crossing
ADJ 2 0.0% 0 0.0% 0.0%
ADJP 17 17.6% 7 42.9% 28.6%
ADVP 106 23.6% 35 71.4% 11.4%
CD 17 0.0% 0 0.0% 0.0%
DT 39 0.0% 0 0.0% 0.0%
FRAG 0 0.0% 2 0.0% 0.0%
INTJ 0 0.0% 19 0.0% 0.0%
N 5 0.0% 0 0.0% 0.0%
NNP 5 0.0% 0 0.0% 0.0%
NP 170 79.4% 225 60.0% 8.9%
NPSEQ 5 0.0% 0 0.0% 0.0%
NX 106 0.0% 0 0.0% 0.0%
PP 4 50.0% 37 5.4% 13.5%
PRED 6 0.0% 0 0.0% 0.0%
PRT 0 0.0% 2 0.0% 0.0%
QP 16 0.0% 1 0.0% 100.0%
RB 5 0.0% 0 0.0% 0.0%
S 75 42.7% 83 38.6% 6.0%
SBAR 18 50.0% 17 52.9% 23.5%
SBARQ 0 0.0% 1 0.0% 0.0%
SINV 0 0.0% 2 0.0% 0.0%
SPEC 61 0.0% 0 0.0% 0.0%
SQ 0 0.0% 2 0.0% 0.0%
UTT 185 0.0% 0 0.0% 0.0%
UTTWORD 15 0.0% 0 0.0% 0.0%
VB 6 0.0% 0 0.0% 0.0%
VP 235 43.8% 124 83.1% 7.3%
WHNP 0 0.0% 3 0.0% 0.0%
Table 1: Breakdown of Collins parser performance by constituent type. Recall refers to how many of the
gold-standard TRIPS constituents were produced by Collins, precision to how many of the produced con-
stituents matched TRIPS, and crossing brackets to the percentage of TRIPS constituents that were violated
by any bracketing produced by Collins.
So [NP I] [VP guess that if [NP we] [VP send [NP one ambulance] to [NP the airport]] [NP we] [VP can [VP get [NP
more people off] [ADVP quickly]]]
Figure 4: Skeleton filter for the utterance So I guess that if we send one ambulance to the airport we can get
more people off quickly.
Boost weight 1% 2% 3% 4% 5%
Speedup factor 1.1 1.3 2.4 2.0 1.2
Table 2: Pilot trials on dialog 3 to determine boost
factor.
limit.In the first experiment, this upper limit is set at
10000 constituents. In addition, we performed the
same experiments with lower upper limits to explore
the question of how much of the parser time is spent
on the sentences that hit the maximum chart size
limit. In the second experiment we used an upper
limit of 5000, and in the third we used an upper limit
of 1500 (the standard value for use in our real-time
dialog system to avoid long delays in responding).
4.2 Results
Results show significant improvements in the speed
of parsing. Table 3 shows the exact match sen-
tence accuracy and timing results for parsing with
and without skeletons with a maximum chart size
of 10000. The first row shows how many utterances
of 6 words or longer were parsed in each dialog.
The next two rows show exact match sentence ac-
curacy results for parses obtained with and without
Dialog 1 2 4 5 Total
Utts (6+ words) 83 78 78 70 309
Sentence accu-
racy w/ skeleton
57.8 50 37.2 52.9 49.5
Sentence accu-
racy no skeleton
56.6 48.7 35.9 52.9 48.5
Time w/ skeleton 46 85 127 45 303
Time no skeleton 90 190 321 60 661
Speedup Factor 1.9 2.2 2.5 1.3 2.0
Table 3: Sentence accuracy and timing results with
maximum chart size 10000 for utterances of 6 or
more words.
skeletons. The next two rows show the total time
(in seconds) to parse the dialogs with and without
the skeletons. The last row shows the speed up fac-
tor (computed as time-without-skeletons/time-with-
skeletons).6
We see substantial speed-ups in the parser using
this technique. The parser using skeletons com-
pleted the parses in less than half of the time of the
original parser. Looking at individual utterances,
70% were parsed more quickly with the skeletons,
while 25% were slower. Overall, our simple ap-
proach appears to provide a substantial payoff in
speed along with a small improvement in accuracy.
Note that we use a strict criterion for accuracy,
so both the correct logical form as well as the cor-
rect syntactic structure must be computed by the
parser for an analysis to be considered correct in
our evaluation. A correct logical form requires cor-
rect word sense disambiguation, constituent depen-
dencies, and semantic role assignment (see section
2.2). For example, in some cases the parser pro-
duces a structurally correct parse, but selects an in-
appropriate word sense, in which case the analysis
is considered incorrect. One such case is the utter-
ance You know where the little loop is, in which the
where is assigned the sense TO-LOC (which should
only be used for trajectories, as in Where did he go),
when in this utterance the correct sense for where is
SPATIAL-LOC.
To explore the question of how much of the speed
increase is the result of time spent on difficult sen-
tences that cause the parser to reach the maximum
chart size limit, we performed the same experiment
with a smaller maximum chart size of 5000, shown
in Table 4. As expected the speed-up gain declined
to 1.8, still quite a respectable gain, and again there
6These experiments were run with CMU Common LISP
18e and a Linux 2.4.20 kernel on a 2 GHz Xeon dual processor
with 1.0 GB total memory.
Dialog 1 2 4 5 Total
Utts (6+ words) 83 78 78 70 309
Sentence accu-
racy w/ skeleton
57.8 50 37.2 52.9 49.5
Sentence accu-
racy no skeleton
55.4 48.7 35.9 52.9 48.2
Time w/ skeleton 46 82 126 45 299
Time no skeleton 90 148 286 59 583
Speedup Factor 1.9 1.8 2.3 1.3 1.8
Table 4: Sentence accuracy and timing results with
maximum chart size 5000 for utterances of 6 or
more words.
Dialog 1 2 4 5 Total
Utts (6+ words) 83 78 78 70 309
Sentence accu-
racy w/ skeleton
57.8 48.7 37.2 52.9 49.2
Sentence accu-
racy no skeleton
55.4 47.4 35.9 52.9 47.9
Time w/ skeleton 47 76 109 45 277
Time no skeleton 74 92 150 59 375
Speedup Factor 1.6 1.2 1.4 1.3 1.4
Table 5: Sentence accuracy and timing results with
maximum chart size 1500 for utterances of 6 more
words.
is no loss of accuracy.
As we drop the chart size to 1500, the speed-up
drops to just 1.4, as shown in Table 5. However,
we have improvements in accuracy using skeletons
when we parse with low upper limits. In certain
cases the skeleton guides the parser to the correct
parse more quickly, so it can be found even when
the maximum chart size is reduced. For example,
for the utterance And meanwhile we send two am-
bulances from the Strong Hospital to take the six
wounded people from the airport (from dialog 1),
a correct full sentence analysis is found with the
larger maximum chart sizes (5000 or more), but
with a maximum chart size of 1500 the correct anal-
ysis for this utterance is found only with the help of
the skeleton.
Our best results are similar to those reported in
(Frank et al, 2002), who show a speed-up factor
of 2.26, although they use a much larger maximum
chart size (70,000). Because of the differences in
grammars and parsers, it is not clear how to fairly
compare the chart sizes.
5 Conclusion
With minimal modifications to our deep parser, we
have been able to achieve a substantial increase in
parsing speed with this technique along with a small
increase in accuracy. The experiments reported here
investigated this technique using off-line methods.
Given our promising results, we are currently work-
ing to integrate an on-line shallow parsing filter into
our collaborative dialog assistant.
Acknowledgments
We thank Micha Elsner, David Ganzhorn and Alli-
son Rosenberg for verification of accuracy results,
and Myroslava Dzikovska and Joel Tetreault for
helpful comments and discussion. This research
was partially supported by NSF grant IIS-0328810
and DARPA grant NBCH-D-03-0010.
References
James F. Allen, Donna K. Byron, Myroslava O.
Dzikovska, George Ferguson, Lucien Galescu,
and Amanda Stent. 2001. Towards conversa-
tional human-computer interaction. AI Maga-
zine, 22(4):27?35.
James F. Allen. 1995. Natural Language Under-
standing. Benjamin Cummings, Redwood City,
CA.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Pro-
ceedings of the Fourteenth National Conference
on Artificial Intelligence (AAAI-97), pages 598?
603, Menlo Park, August. AAAI Press.
Michael Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania.
Ann Copestake, Dan Flickinger, and Ivan A. Sag.
1997. Minimal Recursion Semantics: An intro-
duction. Technical report, CSLI, Stanford Uni-
versity, CA.
Myroslava O. Dzikovska. 2004. A Practical Se-
mantic Representation for Natural Language
Parsing. Ph.D. thesis, University of Rochester.
Charles J. Fillmore. 1968. The case for case. In
Emmon Bach and Robert Harms, editors, Uni-
versals in Linguistic Theory, pages 1?90. Holt,
Rinehart and Winston.
Anette Frank, Markus Becker, Berthold Crysmann,
Bernd Kiefer, and Ulrich Schaefer. 2002. Inte-
grated shallow and deep parsing: TopP meets
HPSG. In COLING?02, Taipei.
Daniel Gildea. 2001. Corpus variation and parser
performance. In 2001 Conference on Empiri-
cal Methods in Natural Language Processing
(EMNLP), pages 167?202, Pittsburgh, PA.
Rebecca Hwa. 1999. Supervised grammar induc-
tion using training data with limited constituent
information. In Proceedings of the 37th Annual
Meeting of the ACL, College Park, Maryland.
Christopher Johnson and Charles J. Fillmore. 2000.
The FrameNet tagset for frame-semantic and syn-
tactic coding of predicate-argument structure. In
Proceedings ANLP-NAACL 2000.
Ronald M. Kaplan, Stefan Riezler, Tracy Holloway
King, John T. Maxwell III, Alexander Vasser-
man, and Richard Crouch. 2004. Speed and ac-
curacy in shallow and deep stochastic parsing. In
NAACL?04, Boston.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19:313?
330.
Carl Pollard and Ivan Sag. 1994. Head-driven
Phrase Structure Grammar. Chicago University
Press.
Amanda J. Stent. 2001. Dialogue Systems as
Conversational Partners: Applying Conversation
Acts Theory to Natural Language Generation for
Task-Oriented Mixed-Initiative Spoken Dialogue.
Ph.D. thesis, University of Rochester.
Mary Swift, Myroslava Dzikovska, Joel Tetreault,
and James Allen. 2004. Semi-automatic syntac-
tic and semantic corpus annotation with a deep
parser. In LREC?04, Lisbon.
Joel Tetreault, Mary Swift, Preethum Prithviraj,
Myroslava Dzikovska, and James Allen. 2004.
Discourse annotation in the Monroe corpus.
In ACL?04 Workshop on Discourse Annotation,
Barcelona.
Syntax-Based Alignment: Supervised or Unsupervised?
Hao Zhang and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
Tree-based approaches to alignment model
translation as a sequence of probabilistic op-
erations transforming the syntactic parse tree
of a sentence in one language into that of the
other. The trees may be learned directly from
parallel corpora (Wu, 1997), or provided by a
parser trained on hand-annotated treebanks (Ya-
mada and Knight, 2001). In this paper, we
compare these approaches on Chinese-English
and French-English datasets, and find that au-
tomatically derived trees result in better agree-
ment with human-annotated word-level align-
ments for unseen test data.
1 Introduction
Statistical approaches to machine translation, pio-
neered by Brown et al (1990), estimate parame-
ters for a probabilistic model of word-to-word cor-
respondences and word re-orderings directly from
large corpora of parallel bilingual text. In re-
cent years, a number of syntactically motivated ap-
proaches to statistical machine translation have been
proposed. These approaches assign a parallel tree
structure to the two sides of each sentence pair, and
model the translation process with reordering oper-
ations defined on the tree structure. The tree-based
approach allows us to represent the fact that syn-
tactic constituents tend to move as unit, as well as
systematic differences in word order in the gram-
mars of the two languages. Furthermore, the tree
structure allows us to make probabilistic indepen-
dence assumptions that result in polynomial time
algorithms for estimating a translation model from
parallel training data, and for finding the highest
probability translation given a new sentence.
Wu (1997) modeled the reordering process with
binary branching trees, where each production
could be either in the same or in reverse order going
from source to target language. The trees of Wu?s
Inversion Transduction Grammar were derived by
synchronously parsing a parallel corpus, using a
grammar with lexical translation probabilities at the
leaves and a simple grammar with a single nonter-
minal providing the tree structure. While this gram-
mar did not represent traditional syntactic categories
such as verb phrases and noun phrases, it served
to restrict the word-level alignments considered by
the system to those allowable by reordering opera-
tions on binary trees. This restriction corresponds
to intuitions about the alignments that could be pro-
duced by systematic differences between the two
language?s grammars, and allows for a polynomial
time algorithm for finding the highest-probability
alignment, and for re-estimation of the lexical trans-
lation and grammar probabilities using the Expecta-
tion Maximization algorithm.
Yamada and Knight (2001) present an algorithm
for estimating probabilistic parameters for a simi-
lar model which represents translation as a sequence
of re-ordering operations over children of nodes in
a syntactic tree, using automatic parser output for
the initial tree structures. This gives the translation
model more information about the structure of the
source language, and further constrains the reorder-
ings to match not just a possible bracketing as in Wu
(1997), but the specific bracketing of the parse tree
provided.
In this paper, we make a direct comparison
of a syntactically unsupervised alignment model,
based on Wu (1997), with a syntactically super-
vised model, based on Yamada and Knight (2001).
We use the term syntactically supervised to indicate
that the syntactic structure in one language is given
to the training procedure. It is important to note,
however, that both algorithms are unsupervised in
that they are not provided any hand-aligned train-
ing data. Rather, they both use Expectation Maxi-
mization to find an alignment model by iteratively
improving the likelihood assigned to unaligned par-
allel sentences. Our evaluation is in terms of agree-
ment with word-level alignments created by bilin-
gual human annotators. We describe each of the
models used in more detail in the next two sections,
including the clone operation of Gildea (2003). The
reader who is familiar with these models may pro-
ceed directly to our experiments in Section 4, and
further discussion in Section 5.
2 The Inversion Transduction Grammar
The Inversion Transduction Grammar of Wu (1997)
can be thought as a a generative process which si-
multaneously produces strings in both languages
through a series of synchronous context-free gram-
mar productions. The grammar is restricted to bi-
nary rules, which can have the symbols in the right
hand side appear in the same order in both lan-
guages, represented with square brackets:
X ? [Y Z]
or the symbols may appear in reverse order in the
two languages, indicated by angle brackets:
X ? ?Y Z?
Individual lexical translations between English
words e and French words f take place at the leaves
of the tree, generated by grammar rules with a single
right hand side symbol in each language:
X ? e/f
Given a bilingual sentence pair, a synchronous
parse can be built using a two-dimensional exten-
sion of chart parsing, where chart items are indexed
by their nonterminal Y and beginning and ending
positions l, m in the source language string, and be-
ginning and ending positions i, j in the target lan-
guage string. For Expectation Maximization train-
ing, we compute inside probabilities ?(Y, l, m, i, j)
from the bottom up as outlined below:
for all l,m, n such that 1 ? l < m < n < Ns do
for all i, j, k such that 1 < i < j < k < Nt do
for all rules X ? Y Z ? G do
?(X, l, n, i, k)+=
P ([Y Z]|X)?(Y, l,m, i, j)?(Z,m, n, j, k)
?(X, l, n, i, k)+=
P (?Y Z?|X)?(Y,m, n, i, j)?(Z, l,m, j, k)
end for
end for
end for
A similar recursion is used to compute outside
probabilities for each chart item, and the inside
and outside probabilities are combined to derive ex-
pected counts for occurrence of each grammar rule,
including the rules corresponding to individual lex-
ical translations. In our experiments we use a gram-
mar with a start symbol S, a single preterminal C,
and two nonterminals A and B used to ensure that
only one parse can generate any given word-level
alignment (ignoring insertions and deletions) (Wu,
1997; Zens and Ney, 2003). The individual lexical
translations produced by the grammar may include
a NULL word on either side, in order to represent
insertions and deletions.
3 The Tree-To-String Model
The model of Yamada and Knight (2001) can be
thought of as a generative process taking a tree in
one language as input and producing a string in
the other through a sequence of probabilistic oper-
ations. If we follow the process of an English sen-
tence?s transformation into French, the English sen-
tence is first given a syntactic tree representation by
a statistical parser (Collins, 1999). As the first step
in the translation process, the children of each node
in the tree can be re-ordered. For any node with
m children, m! re-orderings are possible, each of
which is assigned a probability Porder conditioned
on the syntactic categories of the parent node and its
children. As the second step, French words can be
inserted at each node of the parse tree. Insertions are
modeled in two steps, the first predicting whether an
insertion to the left, an insertion to the right, or no
insertion takes place with probability Pins , condi-
tioned on the syntactic category of the node and that
of its parent. The second step is the choice of the in-
serted word Pt(f |NULL), which is predicted with-
out any conditioning information. The final step,
a French translation of each original English word,
at the leaves of the tree, is chosen according to a
distribution Pt(f |e). The French word is predicted
conditioned only on the English word, and each En-
glish word can generate at most one French word,
or can generate a NULL symbol, representing dele-
tion. Given the original tree, the re-ordering, inser-
tion, and translation probabilities at each node are
independent of the choices at any other node. These
independence relations are analogous to those of a
stochastic context-free grammar, and allow for effi-
cient parameter estimation by an inside-outside Ex-
pectation Maximization algorithm. The computa-
tion of inside probabilities ?, outlined below, con-
siders possible reorderings of nodes in the original
tree in a bottom-up manner:
for all nodes ?i in input tree T do
for all k, l such that 1 < k < l < N do
for all orderings ? of the children ?1...?m of ?i
do
for all partitions of span k, l into
k1, l1...km, lm do
?(?i, k, l)+=
Porder (?|?i)
?m
j=1 ?(?j , kj , lj)
end for
end for
end for
end for
As with Inversion Transduction Grammar, many
alignments between source and target sentences are
not allowed. As a minimal example, take the tree:
A
B
X Y
Z
Of the six possible re-orderings of the three ter-
minals, the two which would involve crossing the
bracketing of the original tree (XZY and YZX)
are not allowed. While this constraint gives us a
way of using syntactic information in translation,
it may in many cases be too rigid. In part to deal
with this problem, Yamada and Knight (2001) flat-
ten the trees in a pre-processing step by collapsing
nodes with the same lexical head-word. This allows,
for example, an English subject-verb-object (SVO)
structure, which is analyzed as having a VP node
spanning the verb and object, to be re-ordered as
VSO in a language such as Arabic. Larger syntactic
divergences between the two trees may require fur-
ther relaxation of this constraint, and in practice we
expect such divergences to be frequent. For exam-
ple, a nominal modifier in one language may show
up as an adverbial in the other, or, due to choices
such as which information is represented by a main
verb, the syntactic correspondence between the two
sentences may break down completely. While hav-
ing flatter trees can make more reorderings possible
than with the binary Inversion Transduction Gram-
mar trees, fixing the tree in one language generally
has a much stronger opposite effect, dramatically re-
stricting the number of permissible alignments.
3.1 Tree-to-String With Cloning
In order to provide more flexibility in alignments, a
cloning operation was introduced for tree-to-string
alignment by Gildea (2003). The model is modified
to allow for a copy of a (translated) subtree from the
English sentences to occur, with some cost, at any
point in the resulting French sentence. For example,
in the case of the input tree
A
B
X Y
Z
a clone operation making a copy of node 3 as a new
child of B would produce the tree:
A
B
X Z Y
Z
This operation, combined with the deletion of the
original node Z, produces the alignment (XZY)
that was disallowed by the original tree reordering
model.
The probability of adding a clone of original node
?i as a child of node ?j is calculated in two steps:
first, the choice of whether to insert a clone under
?j , with probability Pins(clone|?j), and the choice
of which original node to copy, with probability
Pclone(?i|clone = 1) =
Pmakeclone(?i)
?
k Pmakeclone(?k)
where Pmakeclone is the probability of an original
node producing a copy. In our implementation,
Pins(clone) is estimated by the Expectation Max-
imization algorithm conditioned on the label of the
parent node ?j , and Pmakeclone is a constant, mean-
ing that the node to be copied is chosen from all the
nodes in the original tree with uniform probability.
4 Experiments
We trained our translation models on a parallel
corpus of Chinese-English newswire text. We re-
stricted ourselves to sentences of no more than 25
words in either language, resulting in a training cor-
pus of 18,773 sentence pairs with a total of 276,113
Chinese words and 315,415 English words. The
Chinese data were automatically segmented into to-
kens, and English capitalization was retained. We
replace words occurring only once with an unknown
word token, resulting in a Chinese vocabulary of
23,783 words and an English vocabulary of 27,075
words. Our hand-aligned data consisted of 48 sen-
tence pairs also with less than 25 words in either
language, for a total of 788 English words and 580
Chinese words. A separate development set of 49
sentence pairs was used to control overfitting. These
sets were the data used by Hwa et al (2002). The
hand aligned test data consisted of 745 individual
aligned word pairs. Words could be aligned one-
to-many in either direction. This limits the perfor-
mance achievable by our models; the IBM models
allow one-to-many alignments in one direction only,
while the tree-based models allow only one-to-one
alignment unless the cloning operation is used.
Our French-English experiments were based on
data from the Canadian Hansards made available by
Ulrich German. We used as training data 20,000
sentence pairs of no more than 25 words in ei-
ther language. Our test data consisted of 447 sen-
tence pairs of no more than 30 words, hand aligned
by Och and Ney (2000). A separate development
set of 37 sentences was used to control overfitting.
We used of vocabulary of words occurring at least
10 times in the entire Hansard corpus, resulting in
19,304 English words and 22,906 French words.
Our test set is that used in the alignment evalua-
tion organized by Mihalcea and Pederson (2003),
though we retained sentence-initial capitalization,
used a closed vocabulary, and restricted ourselves
to a smaller training corpus. We parsed the English
side of the data with the Collins parser. As an ar-
tifact of the parser?s probability model, it outputs
sentence-final punctuation attached at the lowest
level of the tree. We raised sentence-final punctu-
ation to be a daughter of the tree?s root before train-
ing our parse-based model. As our Chinese-English
test data did not include sentence-final punctuation,
we also removed it from our French-English test set.
We evaluate our translation models in terms of
agreement with human-annotated word-level align-
ments between the sentence pairs. For scoring
the viterbi alignments of each system against gold-
standard annotated alignments, we use the align-
ment error rate (AER) of Och and Ney (2000),
which measures agreement at the level of pairs of
words:
AER = 1 ? |A ? GP | + |A ? GS ||A| + |GS |
where A is the set of word pairs aligned by the
automatic system, GS is the set marked in the
gold standard as ?sure?, and GP is the set marked
as ?possible? (including the ?sure? pairs). In our
Chinese-English data, only one type of alignment
was marked, meaning that GP = GS . For a better
understanding of how the models differ, we break
this figure down into precision:
P = |A ? GP ||A|
and recall:
R = |A ? GS ||GS |
Since none of the systems presented in this com-
parison make use of hand-aligned data, they may
differ in the overall proportion of words that are
aligned, rather than inserted or deleted. This affects
the precision/recall tradeoff; better results with re-
spect to human alignments may be possible by ad-
justing an overall insertion probability in order to
optimize AER.
Table 1 provides a comparison of results using the
tree-based models with the word-level IBM models.
IBM Models 1 and 4 refer to Brown et al (1993).
We used the GIZA++ package, including the HMM
model of Och and Ney (2000). We ran Model 1 for
three iterations, then the HMM model for three iter-
ations, and finally Model 4 for two iterations, train-
ing each model until AER began to increase on our
held-out cross validation data. ?Inversion Transduc-
tion Grammar? (ITG) is the model of Wu (1997),
?Tree-to-String? is the model of Yamada and Knight
(2001), and ?Tree-to-String, Clone? allows the node
cloning operation described above. Our tree-based
models were initialized from uniform distributions
for both the lexical translation probabilities and the
tree reordering operations, and were trained until
AER began to rise on our held-out cross-validation
data, which turned out to be four iterations for the
tree-to-string models and three for the Inversion
Transduction Grammar. French-English results are
shown in Table 2. Here, IBM Model 1 was trained
for 12 iterations, then the HMM model for 5 iter-
ations and Model 4 for 5 iterations. The ITG and
tree-to-string models were both trained for 5 itera-
tions. A learning curve for the Inversion Transduc-
tion Grammar, is shown in Figure 1, showing both
perplexity on held-out data and alignment error rate.
In general we found that while all models would in-
crease in AER if trained for too many iterations, the
increases were of only a few percent.
5 Discussion
The Inversion Transduction Grammar significantly
outperforms the syntactically supervised tree-to-
string model of Yamada and Knight (2001). The
tree-to-string and IBM models are roughly equiva-
lent. Adding the cloning operation improves tree-
to-string results by 2% precision and recall. It is
particularly significant that the ITG gets higher re-
call than the other models, when it is the only model
entirely limited to one-to-one alignments, bounding
the maximum recall it can achieve.
Our French-English experiments show only small
differences between the various systems. Overall,
performance on French-English is much better than
for Chinese-English. French-English has less re-
ordering overall, as shown by the percentage of pro-
ductions in the viterbi ITG parses that are inverted:
14% for French-English in comparison to 23% for
Chinese-English.
One possible explanation for our results is parser
error. While we describe our system as ?syntacti-
Alignment
Precision Recall Error Rate
IBM Model 1 .56 .42 .52
IBM Model 4 .67 .43 .47
Inversion Transduction Grammar .68 .52 .40
Tree-to-String w/ Clone .65 .43 .48
Tree-to-String w/o Clone .63 .41 .50
Table 1: Alignment results on Chinese-English corpus. Higher precision and recall correspond to lower
alignment error rate.
Alignment
Precision Recall Error Rate
IBM Model 1 .63 .71 .34
IBM Model 4 .83 .83 .17
Inversion Transduction Grammar .82 .87 .16
Tree-to-String w/ Clone .84 .85 .15
Table 2: French-English results.
cally supervised?, in fact this supervision comes in
the form of the annotation of the Wall Street Journal
treebank on which the parser is trained, rather than
parses for our parallel training corpus. In particular,
the text we are parsing has a different vocabulary
and style of prose from the WSJ treebank, and often
the fluency of the English translations leaves some-
thing to be desired. While both corpora consist of
newswire text, a typical WSJ sentence
Pierre Vinken, 61 years old, will join the
board as a nonexecutive director Nov. 29.
contrasts dramatically with
In the past when education on opposing
Communists and on resisting Russia was
stressed, retaking the mainland and uni-
fying China became a slogan for the au-
thoritarian system, which made the uni-
fication under the martial law a tool for
oppressing the Taiwan people.
a typical sentence from our corpus.
While we did not have human-annotated gold-
standard parses for our training data, we did have
human annotated parses for the Chinese side of our
test data, which was taken from the Penn Chinese
Treebank (Xue et al, 2002). We trained a second
tree-to-string model in the opposite direction, us-
ing Chinese trees and English strings. The Chi-
nese training data was parsed with the Bikel (2002)
parser, and used the Chinese Treebank parses for
our test data. Results are shown in Table 3. Because
the ITG is a symmetric, generative model, the ITG
results in Table 3 are identical to those in Table 1.
While the experiment does not show a significant
improvement, it is possible that better parses for the
training data might be equally important.
Even when the automatic parser output is correct,
the tree structure of the two languages may not cor-
respond. Dorr (1994) categorizes sources of syntac-
tic divergence between languages, and Fox (2002)
analyzed a parallel French-English corpus, quanti-
fying how often parse dependencies cross when pro-
jecting an English tree onto a French string. Even
in this closely related language pair with gener-
ally similar word order, crossed dependencies were
caused by such common occurrences as adverb
modification of a verb, or the correspondence of
?not? to ?ne pas?. Galley et al (2004) extract trans-
lation rules from a large parsed parallel corpus that
extend in scope to tree fragments beyond a single
node; we believe that adding such larger-scale op-
erations to the translation model is likely to signifi-
cantly improve the performance of syntactically su-
pervised alignment.
The syntactically supervised model has been
found to outperform the IBM word-level alignment
models of Brown et al (1993) for translation by
Yamada and Knight (2002). An evaluation for the
alignment task, measuring agreement with human
judges, also found the syntax-based model to out-
perform the IBM models. However, a relatively
small corpus was used to train both models (2121
Japanese-English sentence pairs), and the evalua-
tions were performed on the same data for training,
meaning that one or both models might be signifi-
cantly overfitting.
Zens and Ney (2003) provide a thorough analy-
sis of alignment constraints from the perspective of
decoding algorithms. They train the models of Wu
1 2 3 4 5 6 7 8 9
400
500
600
700
Pe
rp
le
xi
ty
0.4
0.45
0.5
0.55
Iterations
AER
Figure 1: Training curve for ITG model, showing perplexity on cross-validation data, and alignment error
rate on a separate hand-aligned dataset.
Alignment
Precision Recall Error Rate
Inversion Transduction Grammar .68 .52 .40
Tree-to-String, automatic parses .61 .48 .46
Tree-to-String, gold parses .61 .52 .44
Table 3: Chinese Tree to English String
(1997) as well as Brown et al (1993). Decoding,
meaning exact computation of the highest probabil-
ity translation given a foreign sentence, is not pos-
sible in polynomial time for the IBM models, and
in practice decoders search through the space of hy-
pothesis translations using a set of additional, hard
alignment constraints. Zens and Ney (2003) com-
pute the viterbi alignments for German-English and
French-English sentences pairs using IBM Model
5, and then measure how many of the resulting
alignments fall within the hard constraints of both
Wu (1997) and Berger et al (1996). They find
higher coverage for an extended version of ITG than
for the IBM decoding constraint for both language
pairs, with the unmodified ITG implementation cov-
ering about the same amount of German-English
data as IBM, and significantly less French-English
data. These results show promise for ITG as a ba-
sis for efficient decoding, but do not address which
model best aligns the original training data, as IBM-
derived alignments were taken as the gold standard,
rather than human alignments. We believe that our
results show that syntactically-motivated models are
a promising general approach to training translation
models as well to searching through the resulting
probability space.
Computational complexity is an issue for the tree-
based models presented here. While training the
IBM models with the GIZA++ software takes min-
utes, the tree-based EM takes hours. With our C im-
plementation, one iteration of the syntactically su-
pervised model takes 50 CPU hours, which can be
parallelized across machines. Our tree-based mod-
els are estimated with complete EM, while the train-
ing procedure for the IBM models samples from a
number of likely alignments when accumulating ex-
pected counts. Because not every alignment is legal
with the tree-based models, the technique of sam-
pling by choosing likely alignments according to a
simpler model is not straightforward. Nonetheless,
we feel that training times can be improved with the
right pruning and sampling techniques, as will be
necessary to train on the much larger amounts data
now available, and on longer sentences.
6 Conclusion
We present a side-by-side comparison of syntacti-
cally supervised and unsupervised tree-based align-
ment, along with the non tree-based IBM Model 4.
For Chinese-English, using trees helps the align-
ment task, but a data-derived tree structure gives
better results than projecting automatic English
parser output onto the Chinese string. The French-
English task is easier overall, and exhibits smaller
differences between the systems.
Acknowledgments We are very grateful to Re-
becca Hwa for assistance with the Chinese-English
data, and to everyone who helped make the re-
sources we used available to the research commu-
nity. This work was partially supported by NSF ITR
IIS-09325646.
References
Adam Berger, Peter Brown, Stephen Della Pietra,
Vincent Della Pietra, J. R. Fillett, Andrew Kehler,
and Robert Mercer. 1996. Language transla-
tion apparatus and method of using context-
based tanslation models. United States patent
5,510,981.
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In
Proceedings ARPA Workshop on Human Lan-
guage Technology.
Peter F. Brown, John Cocke, Stephen A. Della
Pietra, Vincent J. Della Pietra, Frederick Je-
linek, John D. Lafferty, Robert L. Mercer, and
Paul S. Roossin. 1990. A statistical approach to
machine translation. Computational Linguistics,
16(2):79?85, June.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation:
Parameter estimation. Computational Linguis-
tics, 19(2):263?311.
Michael John Collins. 1999. Head-driven Statisti-
cal Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania, Philadelphia.
Bonnie J. Dorr. 1994. Machine translation diver-
gences: A formal description and proposed solu-
tion. Computational Linguistics, 20(4):597?633.
Heidi J. Fox. 2002. Phrasal cohesion and statisti-
cal machine translation. In In Proceedings of the
2002 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP 2002), pages
304?311.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation
rule? In Proceedings of the Human Language
Technology Conference/North American Chapter
of the Association for Computational Linguistics
(HLT/NAACL).
Daniel Gildea. 2003. Loosely tree-based alignment
for machine translation. In Proceedings of the
41th Annual Conference of the Association for
Computational Linguistics (ACL-03), pages 80?
87, Sapporo, Japan.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and
Okan Kolak. 2002. Evaluating translational cor-
respondence using annotation projection. In Pro-
ceedings of the 40th Annual Conference of the
Association for Computational Linguistics (ACL-
02).
Rada Mihalcea and Ted Pederson. 2003. An eval-
uation exercise for word alignment. In HLT-
NAACL 2003 Workshop on Building and Using
Parallel Texts: Data Driven Machine Translation
and Beyond, pages 1?10, Edmonton, Alberta.
Franz Josef Och and Hermann Ney. 2000. Im-
proved statistical alignment models. In Proceed-
ings of the 38th Annual Conference of the Asso-
ciation for Computational Linguistics (ACL-00),
pages 440?447, Hong Kong, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?403.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese
corpus. In Proceedings of the 19th. International
Conference on Computational Linguistics (COL-
ING 2002), Taipei, Taiwan.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceed-
ings of the 39th Annual Conference of the Asso-
ciation for Computational Linguistics (ACL-01),
Toulouse, France.
Kenji Yamada and Kevin Knight. 2002. A de-
coder for syntax-based statistical MT. In Pro-
ceedings of the 40th Annual Conference of the
Association for Computational Linguistics (ACL-
02), Philadelphia, PA.
Richard Zens and Hermann Ney. 2003. A compar-
ative study on reordering constraints in statistical
machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computa-
tional Linguistics, Sapporo, Japan.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1081?1088
Manchester, August 2008
Extracting Synchronous Grammar Rules
From Word-Level Alignments in Linear Time
Hao Zhang and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627, USA
David Chiang
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292, USA
Abstract
We generalize Uno and Yagiura?s algo-
rithm for finding all common intervals of
two permutations to the setting of two
sequences with many-to-many alignment
links across the two sides. We show how
to maximally decompose a word-aligned
sentence pair in linear time, which can be
used to generate all possible phrase pairs
or a Synchronous Context-Free Grammar
(SCFG) with the simplest rules possible.
We also use the algorithm to precisely
analyze the maximum SCFG rule length
needed to cover hand-aligned data from
various language pairs.
1 Introduction
Many recent syntax-based statistical machine
translation systems fall into the general formalism
of Synchronous Context-Free Grammars (SCFG),
where the grammar rules are found by first align-
ing parallel text at the word level. From word-
level alignments, such systems extract the gram-
mar rules consistent either with the alignments
and parse trees for one of languages (Galley et
al., 2004), or with the the word-level alignments
alone without reference to external syntactic anal-
ysis (Chiang, 2005), which is the scenario we ad-
dress here.
In this paper, we derive an optimal, linear-time
algorithm for the problem of decomposing an ar-
bitrary word-level alignment into SCFG rules such
that each rule has at least one aligned word and is
minimal in the sense that it cannot be further de-
composed into smaller rules. Extracting minimal
rules is of interest both because rules with fewer
words are more likely to generalize to new data,
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
and because rules with lower rank (the number of
nonterminals on the right-hand side) can be parsed
more efficiently.
This algorithm extends previous work on factor-
ing permutations to the general case of factoring
many-to-many alignments. Given two permuta-
tions of n, a common interval is a set of numbers
that are consecutive in both. The breakthrough
algorithm of Uno and Yagiura (2000) computes
all K common intervals of two length n permu-
tations in O(n + K) time. This is achieved by
designing data structures to index possible bound-
aries of common intervals as the computation pro-
ceeds, so that not all possible pairs of beginning
and end points need to be considered. Landau et
al. (2005) and Bui-Xuan et al (2005) show that all
common intervals can be encoded in O(n) space,
and adapt Uno and Yagiura?s algorithm to produce
this compact representation in O(n) time. Zhang
and Gildea (2007) use similar techniques to factor-
ize Synchronous Context Free Grammars in linear
time.
These previous algorithms assume that the input
is a permutation, but in machine translation it is
common to work with word-level alignments that
are many-to-many; in general any set of pairs of
words, one from each language, is a valid align-
ment for a given bilingual sentence pair. In this
paper, we consider a generalized concept of com-
mon intervals given such an alignment: a common
interval is a pair of phrases such that no word pair
in the alignment links a word inside the phrase
to a word outside the phrase. Extraction of such
phrases is a common feature of state-of-the-art
phrase-based and syntax-based machine transla-
tion systems (Och and Ney, 2004a; Chiang, 2005).
We generalize Uno and Yagiura?s algorithm to this
setting, and demonstrate a linear time algorithm
for a pair of aligned sequences. The output is a tree
representation of possible phrases, which directly
provides a set of minimal synchronous grammar
1081
rules for an SCFG-based machine translation sys-
tem. For phrase-based machine translation, one
can also read all phrase pairs consistent with the
original alignment off of the tree in time linear in
the number of such phrases.
2 Alignments and Phrase Pairs
Let [x, y] denote the sequence of integers between
x and y inclusive, and [x, y) the integers between
x and y ? 1 inclusive. An aligned sequence pair
or simply an alignment is a tuple (E,F,A), where
E = e
1
? ? ? e
n
and F = f
1
? ? ? f
m
are strings, and
A is a set of links (x, y), where 1 ? x ? n and
1 ? y ? m, connecting E and F . For most of this
paper, since we are not concerned with the identity
of the symbols in E and F , we will assume for
simplicity that e
i
= i and f
j
= j, so that E =
[1, n] and F = [1,m].
In the context of statistical machine translation
(Brown et al, 1993), we may interpretE as an En-
glish sentence, F its translation in French, and A
a representation of how the words correspond to
each other in the two sentences. A pair of sub-
strings [s, t] ? E and [u, v] ? F is a phrase pair
(Och and Ney, 2004b) if and only if the subset of
links emitted from [s, t] in E is equal to the sub-
set of links emitted from [u, v] in F , and both are
nonempty.
Figure 1a shows an example of a many-to-
many alignment, where E = [1, 6], F =
[1, 7], and A = {(1, 6), (2, 5), (2, 7), (3, 4),
(4, 1), (4, 3), (5, 2), (6, 1), (6, 3)}. The eight
phrase pairs in this alignment are:
([1, 1], [6, 6]), ([1, 2], [5, 7]),
([3, 3], [4, 4]), ([1, 3], [4, 7]),
([5, 5], [2, 2]), ([4, 6], [1, 3]),
([3, 6], [1, 4]), ([1, 6], [1, 7]).
In Figure 1b, we show the alignment matrix rep-
resentation of the given alignment. By default, the
columns correspond to the tokens in E, the rows
correspond to the tokens in F , and the black cells
in the matrix are the alignment links in A. Using
the matrix representation, the phrase pairs can be
viewed as submatrices as shown with the black-
lined boundary boxes. Visually, a submatrix rep-
resents a phrase pair when it contains at least one
alignment link and there are no alignment links di-
rectly above, below, or to the right or left of it.
e
1
e
2
e
3
e
4
e
5
e
6
f
1
f
2
f
3
f
4
f
5
f
6
f
7
1
1
2
2
3
3
4
4
5
5
6
6
7
(a) (b)
Figure 1: An example of (a) a many-to-many
alignment and (b) the same alignment as a matrix,
with its phrase pairs marked.
2.1 Number of Phrase Pairs
In this section, we refine our definition of phrase
pairs with the concept of tightness and give an
asymptotic upper bound on the total number of
such phrase pairs as the two sequences? lengths
grow. In the original definition, the permissive
many-to-many constraint allows for unaligned to-
kens in both sequences E and F . If there is an un-
aligned token adjacent to a phrase pair, then there
is also a phrase pair that includes the unaligned
token. We say that a phrase pair ([s, t], [u, v]) is
tight if none of e
s
, e
t
, f
u
and f
v
is unaligned. By
focusing on tight phrase pairs, we eliminate the
non-tight ones that share the same set of alignment
links with their tight counterpart.
Given [s, t] in E, let l be the first member of
F that any position in [s, t] links to, and let u be
the last. According to the definition of tight phrase
pair, [l, u] is the only candidate phrase in F to pair
up with [s, t] in E. So, the total number of tight
phrase pairs is upper-bounded by the total number
of intervals in each sequence, which is O(n
2
).
If we do not enforce the tightness constraint, the
total number of phrase pairs can grow much faster.
For example, if a sentence contains only a single
alignment link between the midpoint of F and the
midpoint of E, then there will be O(n
2
m
2
) possi-
ble phrase pairs, but only a single tight phrase pair.
From now on, term phrase pair always refers to a
tight phrase pair.
2.2 Hierarchical Decomposition of Phrase
Pairs
In this section, we show how to encode all the tight
phrase pairs of an alignment in a tree of sizeO(n).
Lemma 2.1. When two phrase pairs overlap, the
intersection, the differences, and the union of the
two are also phrase pairs.
The following picture graphically represents the
two possible overlapping structures of two phrase
1082
([1, 6], [1, 7])
([1, 3], [4, 7])
([1, 2], [5, 7])
([1, 1], [6, 6])
([3, 3], [4, 4])
([4, 6], [1, 3])
([5, 5], [2, 2])
Figure 2: The normalized decomposition tree of
the alignment in Figure 1.
pairs: ([s, t], [u, v]) and ([s
?
, t
?
], [u
?
, v
?
]).
s s? t t?
u
u?
v
v?
s s? t t?
u?
u
v?
v
Let AB and BC be two overlapping English
phrases, with B being their overlap. There are six
possible phrases, A, B, C, AB, BC, and ABC,
but if we omit BC, the remainder are nested and
can be represented compactly by ((AB)C), from
which BC can easily be recovered. If we system-
atically apply this to the whole sentence, we obtain
a hierarchical representation of all the phrase pairs,
which we call the normalized decomposition tree.
The normalized decomposition tree for the exam-
ple is shown in Figure 2.
Bui-Xuan et al (2005) show that the family of
common intervals is weakly partitive, i.e. closed
under intersection, difference and union. This al-
lows the family to be represented as a hierarchi-
cal decomposition. The normalized decomposi-
tion focuses on the right strong intervals, those
that do not overlap with any others on the right.
Lemma 2.1 shows that the family of phrase pairs
is also a weakly partitive family and can be hierar-
chically decomposed after normalization. A minor
difference is we prefer left strong intervals since
our algorithms scan F from left to right. Another
difference is that we binarize a linearly-arranged
sequence of non-overlapping phrase pairs instead
of grouping them together.
In the following sections, we show how to pro-
duce the normalized hierarchical analysis of a
given alignment.
3 Shift-Reduce Algorithm
In this section, we present anO(n
2
+m+|A|) algo-
rithm that is similar in spirit to a shift-reduce algo-
rithm for parsing context-free languages. This al-
gorithm is not optimal, but its left-to-right bottom-
up control will form the basis for the improved al-
gorithm in the next section.
First, we can efficiently test whether a span
[x, y] is a phrase as follows. Define a pair of func-
tions l(x, y) and u(x, y) that record the minimum
and maximum, respectively, of the positions on the
French side that are linked to the positions [x, y]:
l(x, y) = min{j | (i, j) ? A, i ? [x, y]}
u(x, y) = max{j | (i, j) ? A, i ? [x, y]}
Note that l(?, y) is monotone increasing and u(?, y)
is monotone decreasing. Define a step of l(?, y)
(or u(?, y)) to be a maximal interval over which
l(?, y) (resp., u(?, y)) is constant. We can compute
u(x, y) in constant time from its value on smaller
spans:
u(x, y) = max{u(x, z), u(z + 1, y)}
and similarly for l(x, y).
We define the following functions to count the
number of links emitted from prefixes of F and E:
F
c
(j) = |{(i
?
, j
?
) ? A | j
?
? j}|
E
c
(i) = |{(i
?
, j
?
) ? A | i
?
? i}|
Then the difference F
c
(u) ? F
c
(l ? 1) counts the
total number of links to positions in [l, u], and
E
c
(y)?E
c
(x?1) counts the total number of links
to positions in [x, y]. E
c
and F
c
can be precom-
puted in O(n + m + |A|) time.
Finally, let
f(x, y) = F
c
(u(x, y))? F
c
(l(x, y)? 1)
? (E
c
(y)? E
c
(x? 1))
Note that f is non-negative, but not monotonic in
general. Figure 4 provides a visualization of u, l,
and f for the example alignment from Section 2.
This gives us our phrase-pair test:
Lemma 3.1. [x, y] and [l(x, y), u(x, y)] are a
phrase pair if and only if f(x, y) = 0.
This test is used in the following shift-reduce-
style algorithm:
X ? {1}
for y ? [2, n] from left to right do
append y to X
for x ? X from right to left do
compute u(x, y) from u(x + 1, y)
compute l(x, y) from l(x + 1, y)
if f(x, y) = 0 then
[x, y] is a phrase
1083
remove [x+ 1, y] from X
end if
end for
end for
In the worst case, at each iteration we traverse
the entire stack X without a successful reduction,
indicating that the worst case time complexity is
O(n
2
).
4 A Linear Algorithm
In this section, we modify the shift-reduce algo-
rithm into a linear-time algorithm that avoids un-
necessary reduction attempts. It is a generalization
of Uno and Yagiura?s algorithm.
4.1 Motivation
The reason that our previous algorithm is quadratic
is that for each y, we try every possible combina-
tion with the values in X . Uno and Yagiura (2000)
point out that in the case of permutations, it is not
necessary to examine all spans, because it is pos-
sible to delete elements from X so that f(?, y) is
monotone decreasing on X . This means that all
the x ? X such that f(x, y) = 0 can always be
conveniently found at the end of X . That this can
be done safely is guaranteed by the following:
Lemma 4.1. If x
1
< x
2
< y and f(x
1
, y) <
f(x
2
, y), then for all y
?
? y, f(x
2
, y
?
) > 0 (i.e.,
[x
2
, y
?
] is not a phrase).
Let us say that x
2
violates monotonicity if x
1
is the predecessor of x
2
in X and f(x
1
, y) <
f(x
2
, y). Then by Lemma 4.1, we can safely re-
move x
2
from X .
Furthermore, Uno and Yagiura (2000) show that
we can enforce monotonicity at all times in such a
way that the whole algorithm runs in linear time.
This is made possible with a shortcut based on the
following:
Lemma 4.2. If x
1
< x
2
< y and u(x
1
, y ? 1) >
u(x
2
, y ? 1) but u(x
1
, y) = u(x
2
, y), then for all
y
?
? y, f(x
2
, y
?
) > 0 (i.e., [x
2
, y
?
] is not a phrase).
The same holds mutatis mutandis for l.
Let us say that y updates a step [x
?
, y
?
] of u (or
l) if u(x
?
, y) > u(x
?
, y ? 1) (resp., l(x
?
, y) <
l(x
?
, y?1)). By Lemma 4.2, if [x
1
, y
1
] and [x
2
, y
2
]
are different steps of u(?, y ? 1) (resp., l(?, y ? 1))
and y updates both of them, then we can remove
from X all x
?
such that x
2
? x
?
< y.
u(?, y ? 1)
l(?, y ? 1)
u(?, y)
l(?, y)
x
?
1
y
?
2
y
x
?
2
y
?
1
Figure 3: Illustration of step (3) of the algorithm.
The letters indicate substeps of (3).
4.2 Generalized algorithm
These results generalize to the many-to-many
alignment case, although we must introduce a few
nuances. The new algorithm proceeds as follows:
Initialize X = {1}. For y ? [2, n] from left to
right:
1. Append y to X .
2. Update u and l:
(a) Traverse the steps of u(?, y ? 1) from
right to left and compute u(?, y) until we
have found the leftmost step [x
?
, y
?
] of
u(?, y ? 1) that gets updated by y.
(b) Do the same for l.
We have computed two values for x
?
; let x
?
1
be the smaller and x
?
2
be the larger. Similarly,
let y
?
1
be the smaller y
?
.
3. Enforce monotonicity of f(?, y) (see Fig-
ure 3):
(a) The positions left of the smaller x
?
al-
ways satisfy monotonicity, so do noth-
ing.
(b) For x ? [x
?
1
, x
?
2
) ? X while x violates
monotonicity, remove x from X .
(c) For x ? [x
?
2
, y
?
1
] ? X while x violates
monotonicity, remove x from X .
(d) The steps right of y
?
1
may or may not
violate monotonicity, but we use the
stronger Lemma 4.2 to delete all of them
(excluding y).
1
1
In the special case where [x
?
, y
?
] is updated by y to the
1084
y = 1 :
1
1
2
2
3
3
4
4
5
5
6
6
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
6
f
x
y = 2 :
1
1
2
2
3
3
4
4
5
5
6
6
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
6
f
x
y = 3 :
1
1
2
2
3
3
4
4
5
5
6
6
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
6
f
x
y = 4 :
1
1
2
2
3
3
4
4
5
5
6
6
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
6
f
x
y = 5 :
1
1
2
2
3
3
4
4
5
5
6
6
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
6
f
x
y = 6 :
1
1
2
2
3
3
4
4
5
5
6
6
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
6
f
x
Figure 4: The evolution of u(x, y) , l(x, y), and f(x, y) as y goes from 1 to 6 for the example alignment.
Each pair of diagrams shows the state of affairs between steps (3) and (4) of the algorithm. Light grey
boxes are the steps of u, and darker grey boxes are the steps of l. We use solid boxes to plot the values
of remaining x?s on the list but also show the other values in empty boxes for completeness.
(e) Finally, if y violates monotonicity, re-
move it from X .
4. For x ? X from right to left until f(x, y) >
0, output [x, y] and remove x?s successor in
X .
2
An example of the algorithm?s execution is
shown in Figure 4. The evolution of u(x, y),
l(x, y), and f(x, y) is displayed for increasing y
(from 1 to 6). We point out the interesting steps.
When y = 2, position 2 is eliminated due to step
(3e) of our algorithm to ensure monotonicity of
f at the right end, and [1, 2] is reduced. When
same value as the step to its left, we can use Lemma 4.2 to
delete [x
?
, y
?
] and y as well, bypassing steps (3b),(3c), and
(3e).
2
If there are any such x, they must lie to the left of x
?
1
.
Therefore a further optimization would be to perform step (4)
before step (3), starting with the predecessor of x
?
1
. If a re-
duction is made, we can jump to step (3e).
y = 3, two reductions are made: one on [3, 3] and
the other on [1, 3]. Because of leftmost normaliza-
tion, position 3 is deleted. When y = 6, we have
x
?
1
= x
?
2
= y
?
1
= 5, so that position 5 is deleted by
step (3c) and position 6 is deleted by step (3e).
4.3 Correctness
We have already argued in Section 4.1 that the
deletion of elements fromX does not alter the out-
put of the algorithm. It remains to show that step
(3) guarantees monotonicity:
Claim 4.3. For all y, at the end of step (3), f(?, y)
is monotone decreasing.
Proof. By induction on y. For y = 1, the claim
is trivially true. For y > 1, we want to show
that for x
1
, x
2
adjacent in X such that x
1
< x
2
,
f(x
1
, y) ? f(x
2
, y). We consider the five regions
of X covered by step (3) (cf. Figure 3), and then
1085
the boundaries between them.
Region (a): x
1
, x
2
? [1, x
?
1
]. Since u(x
i
, y) =
u(x
i
, y ? 1) and l(x
i
, y) = l(x
i
, y ? 1), we have:
f(x
i
, y)?f(x
i
, y?1) = 0? (E
c
(y)?E
c
(y?1))
i.e., in this region, f shifts down uniformly from
iteration y ? 1 to iteration y. Hence, if f(?, y ?
1) was monotonic, then f(?, y) is also monotonic
within this region.
Region (b): x
1
, x
2
? [x
?
1
, x
?
2
). Since u(x
1
, y ?
1) = u(x
2
, y ? 1) and u(x
1
, y) = u(x
2
, y) and
similarly for l, we have:
f(x
1
, y)? f(x
1
, y? 1) = f(x
2
, y)? f(x
2
, y? 1)
i.e., in this region, f shifts up or down uniformly.
3
Hence, if f(?, y ? 1) was monotonic, then f(?, y)
is also monotonic within this region.
Region (c): x
1
, x
2
? [x
?
2
, y
?
1
]. Same as Case 2.
Region (d) and (e): Vacuous (these regions have at
most one element).
The remaining values of x
1
, x
2
are those that
straddle the boundaries between regions. But
step (3) of the algorithm deals with each of
these boundaries explicitly, deleting elements until
f(x
1
) ? f(x
2
). Thus f(?, y) is monotonic every-
where.
4.4 Implementation and running time
X should be implemented in a way that allows
linear-time traversal and constant-time deletion;
also, u and l must be implemented in a way that
allows linear-time traversal of their steps. Doubly-
linked lists are appropriate for all three functions.
Claim 4.4. The above algorithm runs in O(n +
m + |A|) time.
We can see that the algorithm runs in linear time
if we observe that whenever we traverse a part of
X , we delete it, except for a constant amount of
work per iteration (that is, per value of y): the steps
traversed in (2) are all deleted in (3d) except four
(two for u and two for l); the positions traversed in
(3b), (3c), and (4) are all deleted except one.
4.5 SCFG Rule extraction
The algorithm of the previous section outputs the
normalized decomposition tree depicted in Fig-
ure 2. From this tree, it is straightforward to obtain
3
It can be shown further that in this region, f shifts up or
is unchanged. Therefore any reductions in step (4) must be in
region (a).
A? B
(1)
C
(2)
, C
(2)
B
(1)
B ? D
(1)
E
(2)
, E
(2)
D
(1)
D ? G
(1)
e
2
, f
5
G
(1)
f
6
G? e
1
, f
6
E ? e
3
, f
4
C ? e
4
F
(1)
e
6
, f
1
F
(1)
f
3
F ? e
5
, f
2
Figure 5: Each node from the normalized decom-
position tree of Figure 2 is converted into an SCFG
rule.
a set of maximally-decomposed SCFG rules. As
an example, the tree of Figure 2 produces the rules
shown in Figure 5.
We adopt the SCFG notation of Satta and Pe-
serico (2005). Each rule has a right-hand side se-
quence for both languages, separated by a comma.
Superscript indices in the right-hand side of gram-
mar rules such as:
A? B
(1)
C
(2)
, C
(2)
B
(1)
indicate that the nonterminals with the same index
are linked across the two languages, and will even-
tually be rewritten by the same rule application.
The example above inverts the order of B and C
when translating from the source language to the
target language.
The SCFG rule extraction proceeds as follows.
Assign a nonterminal label to each node in the tree.
Then for each node (S, T ) in the tree top-down,
where S and T are sequences of positions,
1. For each child (S
?
, T
?
), S
?
and T
?
must be
subsequences of S and T , respectively. Re-
place their occurrences in S and T with a pair
of coindexed nonterminals X
?
, where X
?
is
the nonterminal assigned to the child.
2. For each remaining position i in S, replace i
with e
i
.
3. For each remaining position j in T , replace j
with f
j
.
4. Output the rule X ? S, T , where X is the
nonterminal assigned to the parent.
As an example, consider the node ([4, 6], [1, 3])
in Figure 2. After step 1, it becomes
(4F
(1)
6, 1F
(1)
3)
and after steps 2 and 3, it becomes
(e
4
F
(1)
e
6
, f
1
F
(1)
f
3
)
1086
0 1 2 3 4 5 6
Hindi/English 52.8 53.5 99.9 99.9 100.0
Chinese/English 51.0 52.4 99.7 99.8 100.0 100.0 100.0
French/English 52.1 53.5 99.9 100.0 100.0 100.0
Romanian/English 50.8 52.6 99.9 99.9 100.0 100.0
Spanish/English 50.7 51.8 99.9 100.0 100.0 100.0
Table 1: Cumulative percentages of rule tokens by number of nonterminals in right-hand side. A blank
indicates that no rules were found with that number of nonterminals.
Finally, step 4 outputs
C ? e
4
F
(1)
e
6
, f
1
F
(1)
f
3
A few choices are available to the user depend-
ing on the application intended for the SCFG ex-
traction. The above algorithm starts by assigning
a nonterminal to each node in the decomposition
tree; one could assign a unique nonterminal to each
node, so that the resulting grammar produces ex-
actly the set of sentences given as input. But for
machine translation, one may wish to use a single
nonterminal, such that the extracted rules can re-
combine freely, as in Chiang (2005).
Unaligned words in either language (an empty
row or column in the alignment matrix, not present
in our example) will be attached as high as possi-
ble in our tree. However, other ways of handling
unaligned words are possible given the decompo-
sition tree. One can produce all SCFG rules con-
sistent with the alignment by, for each unaligned
word, looping through possible attachment points
in the decomposition tree. In this case, the num-
ber of SCFG rules produced may be exponential
in the size of the original input sentence; however,
even in this case, the decomposition tree enables a
rule extraction algorithm that is linear in the output
length (the number of SCFG rules).
4.6 Phrase extraction
We briefly discuss the process of extracting all
phrase pairs consistent with the original alignment
from the normalized decomposition tree. First of
all, every node in the tree gives a valid phrase
pair. Then, in the case of overlapping phrase pairs
such as the example in Section 2.1, the decom-
position tree will contain a left-branching chain
of binary nodes all performing the same permuta-
tion. While traversing the tree, whenever we iden-
tify such a chain, let ?
1
, . . . , ?
k
be the sequence of
all the children of the nodes in the chain. Then,
each of the subsequences {?
i
, . . . , ?
j
| 1 < i <
j ? k} yields a valid phrase pair. In our exam-
ple, the root of the tree of Figure 2 and its left
child form such a chain, with three children; the
subsequence {([3, 3], [4, 4]), ([4, 6], [1, 3])} yields
the phrase ([3, 6], [1, 4]). In the case of unaligned
words, we can also consider all combinations of
their attachments, as discussed for SCFG rule ex-
traction.
5 Experiments on Analyzing Word
Alignments
One application of our factorization algorithm
is analyzing human-annotated word alignments.
Wellington et al (2006) argue for the necessity
of discontinuous spans (i.e., for a formalism be-
yond Synchronous CFG) in order for synchronous
parsing to cover human-annotated word alignment
data under the constraint that rules have a rank
of no more than two. In a related study, Zhang
and Gildea (2007) analyze the rank of the Syn-
chronous CFG derivation trees needed to parse the
same data. The number of discontinuous spans
and the rank determine the complexity of dynamic
programming algorithms for synchronous parsing
(alignment) or machine translation decoding.
Both studies make simplifying assumptions on
the alignment data to avoid dealing with many-to-
many word links. Here, we apply our alignment
factorization algorithm directly to the alignments
to produce a normalized decomposition tree for
each alignment and collect statistics on the branch-
ing factors of the trees.
We use the same alignment data for the
five language pairs Chinese-English, Romanian-
English, Hindi-English, Spanish-English, and
French-English as Wellington et al (2006). Ta-
ble 1 reports the number of rules extracted by the
rank, or number of nonterminals on the right-hand
side. Almost all rules are binary, implying both
that binary synchronous grammars are adequate
for MT, and that our algorithm can find such gram-
mars. Table 2 gives similar statistics for the num-
ber of terminals in each rule. The phrases we ex-
tract are short enough that they are likely to gener-
alize to new sentences. The apparent difficulty of
1087
0 1 2 3 4 5 6 7 8 9 ?10 max
Hindi/English 39.6 92.2 97.7 99.5 99.7 99.9 99.9 100.0 7
Chinese/English 39.8 87.2 96.2 99.0 99.7 99.9 100.0 100.0 100.0 100.0 100.0 12
French/English 44.5 89.0 93.4 95.8 97.5 98.4 99.0 99.3 99.6 99.8 100.0 18
Romanian/English 42.9 89.8 96.9 98.9 99.5 99.8 99.9 100.0 100.0 9
Spanish/English 47.5 91.8 97.7 99.4 99.9 99.9 100.0 100.0 100.0 9
Table 2: Cumulative percentages of rule tokens by number of terminals in right-hand side. A blank
indicates that no rules were found with that number of terminals.
the French-English pair is due to the large number
of ?possible? alignments in this dataset.
6 Conclusion
By extending the algorithm of Uno and Yagiura
(2000) from one-to-one mappings to many-to-
many mappings, we have shown how to construct a
hierarchical representation of all the phrase pairs in
a given aligned sentence pair in linear time, which
yields a set of minimal SCFG rules. We have also
illustrated how to apply the algorithm as an analyt-
ical tool for aligned bilingual data.
Acknowledgments Thanks to Bob Moore for
suggesting the extension to phrase extraction at
SSST 2007. This work was supported in part
by NSF grants IIS-0546554 and ITR-0428020,
and DARPA grant HR0011-06-C-0022 under BBN
Technologies subcontract 9500008412.
References
Brown, Peter F., Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Bui-Xuan, Binh Minh, Michel Habib, and Christophe
Paul. 2005. Revisiting T. Uno and M. Yagiura?s al-
gorithm. In The 16th Annual International Sympo-
sium on Algorithms and Computation (ISAAC ?05),
pages 146?155.
Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL 2005, pages 263?270.
Galley, Michel, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of NAACL 2004.
Landau, Gad M., Laxmi Parida, and Oren Weimann.
2005. Gene proximity analysis across whole
genomes via PQ trees. Journal of Computational Bi-
ology, 12(10):1289?1306.
Och, Franz Josef and Hermann Ney. 2004a. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4).
Och, Franz Josef and Hermann Ney. 2004b. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30:417?449.
Satta, Giorgio and Enoch Peserico. 2005. Some
computational complexity results for synchronous
context-free grammars. In Proceedings of EMNLP
2005, pages 803?810, Vancouver, Canada, October.
Uno, Takeaki and Mutsunori Yagiura. 2000. Fast al-
gorithms to enumerate all common intervals of two
permutations. Algorithmica, 26(2):290?309.
Wellington, Benjamin, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the
complexity of translational equivalence. In Proceed-
ings of COLING-ACL 2006.
Zhang, Hao and Daniel Gildea. 2007. Factorization
of synchronous context-free grammars in linear time.
In Proceedings of the NAACL Workshop on Syntax
and Structure in Statistical Translation (SSST).
1088
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 718?726,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Unsupervised Tokenization for Machine Translation
Tagyoung Chung and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
Training a statistical machine translation
starts with tokenizing a parallel corpus.
Some languages such as Chinese do not in-
corporate spacing in their writing system,
which creates a challenge for tokenization.
Moreover, morphologically rich languages
such as Korean present an even bigger
challenge, since optimal token boundaries
for machine translation in these languages
are often unclear. Both rule-based solu-
tions and statistical solutions are currently
used. In this paper, we present unsuper-
vised methods to solve tokenization prob-
lem. Our methods incorporate informa-
tion available from parallel corpus to de-
termine a good tokenization for machine
translation.
1 Introduction
Tokenizing a parallel corpus is usually the first
step of training a statistical machine translation
system. With languages such as Chinese, which
has no spaces in its writing system, the main chal-
lenge is to segment sentences into appropriate to-
kens. With languages such as Korean and Hun-
garian, although the writing systems of both lan-
guages incorporate spaces between ?words?, the
granularity is too coarse compared with languages
such as English. A single word in these lan-
guages is composed of several morphemes, which
often correspond to separate words in English.
These languages also form compound nouns more
freely. Ideally, we want to find segmentations for
source and target languages that create a one-to-
one mapping of words. However, this is not al-
ways straightforward for two major reasons. First,
what the optimal tokenization for machine trans-
lation should be is not always clear. Zhang et al
(2008b) and Chang et al (2008) show that get-
ting the tokenization of one of the languages in
the corpus close to a gold standard does not nec-
essarily help with building better machine trans-
lation systems. Second, even statistical methods
require hand-annotated training data, which means
that in resource-poor languages, good tokenization
is hard to achieve.
In this paper, we explore unsupervised methods
for tokenization, with the goal of automatically
finding an appropriate tokenization for machine
translation. We compare methods that have ac-
cess to parallel corpora to methods that are trained
solely using data from the source language. Unsu-
pervised monolingual segmentation has been stud-
ied as a model of language acquisition (Goldwater
et al, 2006), and as model of learning morphol-
ogy in European languages (Goldsmith, 2001).
Unsupervised segmentation using bilingual data
has been attempted for finding new translation
pairs (Kikui and Yamamoto, 2002), and for finding
good segmentation for Chinese in machine trans-
lation using Gibbs sampling (Xu et al, 2008). In
this paper, further investigate the use of bilingual
information to find tokenizations tailored for ma-
chine translation. We find a benefit not only for
segmentation of languages with no space in the
writing system (such as Chinese), but also for the
smaller-scale tokenization problem of normaliz-
ing between languages that include more or less
information in a ?word? as defined by the writ-
ing system, using Korean-English for our exper-
iments. Here too, we find a benefit from using
bilingual information, with unsupervised segmen-
tation rivaling and in some cases surpassing su-
pervised segmentation. On the modeling side,
we use dynamic programming-based variational
Bayes, making Gibbs sampling unnecessary. We
also develop and compare various factors in the
model to control the length of the tokens learned,
and find a benefit from adjusting these parame-
ters directly to optimize the end-to-end translation
quality.
718
2 Tokenization
Tokenization is breaking down text into lexemes
? a unit of morphological analysis. For relatively
isolating languages such as English and Chinese, a
word generally equals a single token, which is usu-
ally a clearly identifiable unit. English, especially,
incorporates spaces between words in its writing
system, which makes tokenization in English usu-
ally trivial. The Chinese writing system does not
have spaces between words, but there is less am-
biguity where word boundaries lie in a given sen-
tence compared to more agglutinative languages.
In languages such as Hungarian, Japanese, and
Korean, what constitutes an optimal token bound-
ary is more ambiguous. While two tokens are usu-
ally considered two separate words in English, this
may be not be the case in agglutinative languages.
Although what is considered a single morpholog-
ical unit is different from language to language,
if someone were given a task to align words be-
tween two languages, it is desirable to have one-
to-one token mapping between two languages in
order to have the optimal problem space. For ma-
chine translation, one token should not necessarily
correspond to one morphological unit, but rather
should reflect the morphological units and writing
system of the other language involved in transla-
tion.
For example, consider a Korean ?word? meok-
eoss-da, which means ate. It is written as a sin-
gle word in Korean but consists of three mor-
phemes eat-past-indicative. If one uses morpho-
logical analysis as the basis for Korean tokeniza-
tion, meok-eoss-da would be split into three to-
kens, which is not desirable if we are translat-
ing Korean to English, since English does not
have these morphological counterparts. However,
a Hungarian word szekr?enyemben, which means in
my closet, consists of three morphemes closet-my-
inessive that are distinct words in English. In this
case, we do want our tokenizer to split this ?word?
into three morphemes szekr?eny em ben.
In this paper, we use segmentation and to-
kenization interchangeably as blanket terms to
cover the two different problems we have pre-
sented here. The problem of segmenting Chinese
sentences into words and the problem of segment-
ing Korean or Hungarian ?words? into tokens of
right granularity are different in their nature. How-
ever, our models presented in section 3 handle the
both problems.
3 Models
We present two different methods for unsuper-
vised tokenization. Both are essentially unigram
tokenization models. In the first method, we try
learning tokenization from word alignments with
a model that bears resemblance to Hidden Markov
models. We use IBMModel 1 (Brown et al, 1993)
for the word alignment model. The second model
is a relatively simpler monolingual tokenization
model based on counts of substrings which serves
as a baseline of unsupervised tokenization.
3.1 Learning tokenization from alignment
We use expectation maximization as our primary
tools in learning tokenization form parallel text.
Here, the observed data provided to the algorithm
are the tokenized English string e
n
1
and the unto-
kenized string of foreign characters c
m
1
. The un-
observed variables are both the word-level align-
ments between the two strings, and the tokeniza-
tion of the foreign string. We represent the tok-
enization with a string s
m
1
of binary variables, with
s
i
= 1 indicating that the ith character is the final
character in a word. The string of foreign words
f
?
1
can be thought of as the result of applying the
tokenization s to the character string c:
f = s ? c where ? =
m
?
i=1
s
i
We use IBM Model 1 as our word-level align-
ment model, following its assumptions that each
foreign word is generated independently from one
English word:
P (f |e) =
?
a
P (f ,a | e)
=
?
a
?
i
P (f
i
| e
a
i
)P (a)
=
?
i
?
j
P (f
i
| e
j
)P (a
i
= j)
and that all word-level alignments a are equally
likely: P (a) =
1
n
for all positions. While Model 1
has a simple EM update rule to compute posteri-
ors for the alignment variables a and from them
learn the lexical translation parameters P (f | e),
we cannot apply it directly here because f itself is
unknown, and ranges over an exponential number
of possibilities depending on the hidden segmenta-
tion s. This can be addressed by applying dynamic
programing over the sequence s. We compute the
719
posterior probability of a word beginning at posi-
tion i, ending at position j, and being generated by
English word k:
P (s
i...j
= (1, 0, . . . , 0, 1), a = k | e)
=
?(i)P (f | e
k
)P (a = k)?(j)
P (c | e)
where f = c
i
. . . c
j
is the word formed by con-
catenating characters i through j, and a is a vari-
able indicating which English position generated
f . Here ? and ? are defined as:
?(i) = P (c
i
1
, s
i
= 1 | e)
?(j) = P (c
m
j+1
, s
j
= 1 | e)
These quantities resemble forward and backward
probabilities of hidden Markov models, and can
be computed with similar dynamic programming
recursions:
?(i) =
L
?
?=1
?(i? ?)
?
a
P (a)P (c
i
i??
| e
a
)
?(j) =
L
?
?=1
?
a
P (a)P (c
j+?
j
| e
a
)?(j + ?)
where L is the maximum character length for a
word.
Then, we can calculate the expected counts of
individual word pairs being aligned (c
j
i
, e
k
) by ac-
cumulating these posteriors over the data:
ec(c
j
i
, e
k
) +=
?(i)P (a)P (c
j
i
| e
k
)?(j)
?(m)
The M step simply normalizes the counts:
?
P (f | e) =
ec(f, e)
?
e
ec(f, e)
Our model can be compared to a hiddenMarkov
model in the following way: a target word gen-
erates a source token which spans a zeroth order
Markov chain of characters in source sentence,
where a ?transition? represents a segmentation and
a ?emission? represents an alignment. The model
uses HMM-like dynamic programming to do in-
ference. For the convenience, we refer to this
model as the bilingual model in the rest of the
paper. Figure 1 illustrates our first model with
an small example. Under this model we are not
learning segmentation directly, but rather we are
learning alignments between two sentences. The
c
1
c
2
c
3
c
4
f
1
f
2
e
1
e
2
Figure 1: The figure shows a source sentence
f = f
1
, f
2
= s ? c
1
. . . c
4
where s = (0, 0, 1, 1)
and a target sentence e = e
1
, e
2
. There is a seg-
mentation between c
3
and c
4
; thus c
1
, c
2
, c
3
form
f
1
and c
3
forms f
2
. f
1
is generated by e
2
and f
2
is
generated by e
1
.
segmentation is by-product of learning the align-
ment. We can find the optimal segmentation of
a new source language sentence using the Viterbi
algorithm. Given two sentences e and f ,
a
?
= argmax
a
P (f ,a | e)
and segmentation s
?
implied by alignment a
?
is
the optimal segmentation of f found by this model.
3.2 Learning tokenization from substring
counts
The second tokenization model we propose is
much simpler. More sophisticated unsupervised
monolingual tokenization models using hierarchi-
cal Bayesian models (Goldwater et al, 2006)
and using the minimum description length prin-
ciple (Goldsmith, 2001; de Marcken, 1996) have
been studied. Our model is meant to serve as
a computationally efficient baseline for unsuper-
vised monolingual tokenization. Given a corpus
of only source language of unknown tokenization,
we want to find the optimal s given c ? s that
gives us the highest P (s | c). According to Bayes?
rule,
P (s | c) ? P (c | s)P (s)
Again, we assume that all P (s) are equally likely.
Let f = s?c = f
1
. . . f
?
, where f
i
is a word under
some possible segmentation s. We want to find the
s that maximizes P (f). We assume that
P (f) = P (f
1
)? . . .? P (f
?
)
To calculate P (f
i
), we count every possible
720
substring ? every possible segmentation of char-
acters ? from the sentences. We assume that
P (f
i
) =
count(f
i
)
?
k
count(f
k
)
We can compute these counts by making a sin-
gle pass through the corpus. As in the bilingual
model, we limit the maximum size of f for prac-
tical reasons and to prevent our model from learn-
ing unnecessarily long f . With P (f), given a se-
quence of characters c, we can calculate the most
likely segmentation using the Viterbi algorithm.
s
?
= argmax
s
P (f)
Our rationale for this model is that if a span of
characters f = c
i
. . . c
j
is an independent token, it
will occur often enough in different contexts that
such a span of characters will have higher prob-
ability than other spans of characters that are not
meaningful. For the rest of the paper, this model
will be referred to as the monolingual model.
3.3 Tokenizing new data
Since the monolingual tokenization only uses in-
formation from a monolingual corpus, tokenizing
new data is not a problem. However, with the
bilingual model, we are learning P (f | e). We are
relying on information available from e to get the
best tokenization for f. However, the parallel sen-
tences will not be available for new data we want
to translate. Therefore, for the new data, we have
to rely only on P (f) to tokenize any new data,
which can be obtained by calculating
P (f) =
?
e
P (f | e)P (e)
With P (f) from the bilingual model, we can run
the Viterbi algorithm in the same manner as mono-
lingual tokenization model for monolingual data.
We hypothesize that we can learn valuable infor-
mation on which token boundaries are preferable
in language f when creating a statistical machine
translation system that translates from language f
to language e.
4 Preventing overfitting
We introduce two more refinements to our word-
alignment induced tokenization model and mono-
lingual tokenization model. Since we are consid-
ering every possible token f that can be guessed
from our corpus, the data is very sparse. For the
bilingual model, we are also using the EM algo-
rithm to learn P (f | e), which means there is a
danger of the EM algorithm memorizing the train-
ing data and thereby overfitting. We put a Dirichlet
prior on our multinomial parameter for P (f | e)
to control this situation. For both models, we also
want a way to control the distribution of token
length after tokenization. We address this problem
by adding a length factor to our models.
4.1 Variational Bayes
Beal (2003) and Johnson (2007) describe vari-
ational Bayes for hidden Markov model in de-
tail, which can be directly applied to our bilingual
model. With this Bayesian extension, the emission
probability of our first model can be summarized
as follows:
?
e
| ? ? Dir(?),
f
i
| e
i
= e ? Multi(?
e
).
Johnson (2007) and Zhang et al (2008a) show
having small ? helps to control overfitting. Fol-
lowing this, we set our Dirichlet prior to be as
sparse as possible. It is set at ? = 10
?6
, the num-
ber we used as floor of our probability.
For the model incorporating the length factor,
which is described in the next section, we do not
place a prior on our transition probability, since
there are only two possible states, i.e. P (s = 1)
and P (s = 0). This distribution is not as sparse as
the emission probability.
Comparing variational Bayes to the traditional
EM algorithm, the E step stays the same but the
M step for calculating the emission probability
changes as follows:
?
P (f | e) =
exp(?(ec(f, e) + ?))
exp(?(
?
e
ec(f, e) + s?))
where ? is the digamma function, and s is the size
of the vocabulary from which f is drawn. Since
we do not accurately know s, we set s to be the
number of all possible tokens. As can be seen from
the equation, by setting ? to a small value, we are
discounting the expected count with help of the
digamma function. Thus, having lower ? leads to
a sparser solution.
4.2 Token length
We now add a parameter that can adjust the to-
kenizer?s preference for longer or shorter tokens.
721
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 1  2  3  4  5  6
ref
P(s)=0.55
lambda=3.16
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 1  2  3  4  5  6
ref
P(s)=0.58
lambda=2.13
Figure 2: Distribution of token length for (from left to right) Chinese, and Korean. ?ref? is the empirical
distribution from supervised tokenization. Two length factors ? ?
1
and ?
2
are also shown. For ?
1
, the
parameter to geometric distribution P (s) is set to the value learned from our bilingual model. For ?
2
, ?
is set using the criterion described in the experiment section.
This parameter is beneficial because we want our
distribution of token length after tokenization to
resemble the real distribution of token length. This
parameter is also useful because we also want to
incorporate information on the number of tokens
in the other language in the parallel corpus. This is
based on the assumption that, if tokenization cre-
ates a one-to-one mapping, the number of tokens
in both languages should be roughly the same. We
can force the two languages to have about the same
number of tokens by adjusting this parameter. The
third reason is to further control overfitting. Our
observation is that certain morphemes are very
common, such that they will be always observed
attached to other morphemes. For example, in Ko-
rean, a noun attached with nominative case marker
is very common. Our model is likely to learn a
noun attached with the morpheme ? nominative
case marker ? rather than noun itself. This is not
desirable when the noun occurs with less common
morphemes; in these cases the morpheme will be
split off creating inconsistencies.
We have experimented with two different length
factors, each with one adjustable parameter:
?
1
(?) = P (s)(1? P (s))
??1
?
2
(?) = 2
??
?
The first, ?
1
, is the geometric distribution, where
l is length of a token and P (s) is probability of
segmentation between two characters. The second
length factor ?
2
was acquired through several ex-
periments and was found to work well. As can
been seen from Figure 2, the second factor dis-
counts longer tokens more heavily than the geo-
metric distribution. We can adjust the value of ?
and P (s) to increase or decrease number of tokens
after segmentation.
For our monolingual model, incorporating these
factors is straightforward. We assume that
P (f) ? P (f
1
)?(?
1
)? . . .? P (f
n
)?(?
n
)
where ?
i
is the length of f
i
. Then, we use the same
Viterbi algorithm to select the f
1
. . . f
n
that max-
imizes P (f), thereby selecting the optimal s ac-
cording to our monolingual model with a length
factor. We pick the value of ? and P (s) that
produces about the same number of tokens in the
source side as in the target side, thereby incorpo-
rating some information about the target language.
For our bilingual model, we modify our model
slightly to incorporate ?
1
, creating a hybrid
model. Now, our forward probability of forward-
backward algorithm is:
?(i) =
L
?
?=1
?(i? l)?
1
(?)
?
a
P (a)P (c
i
i??
| e
a
)
and the expected count of (c
j
i
, e
k
) is
ec(c
j
i
, e
k
) +=
?(i)P (a)P (c
j
i
| e
k
)?(j)?
1
(j ? i)
?(m)
For ?
1
, we can learn P (s) for the geometric dis-
tribution from the model itself:
1
P (s) =
1
m
m
?
i
?(i)?(i)
?(m)
1
The equation is for one sentence, but in practice, we sum
over all sentences in the training data to calculate P (s).
722
We can also fix P (s) instead of learning it through
EM. We incorporate ?
2
into the bilingual model
as follows: after learning P (f) from the bilingual
model, we pick the ? in the same manner as the
monolingual model and run the Viterbi algorithm.
After applying the length factor, what we have
is a log-linear model for tokenization, with two
feature functions with equal weights: the length
factor and P (f) learned from model.
5 Experiments
5.1 Data
We tested our tokenization methods on two differ-
ent language pairs: Chinese-English, and Korean-
English. For Chinese-English, we used FBIS
newswire data. The Korean-English parallel data
was collected from news websites and sentence-
aligned using two different tools described by
Moore (2002) and Melamed (1999). We used sub-
sets of each parallel corpus consisting of about 2M
words and 60K sentences on the English side. For
our development set and test set, Chinese-English
had about 1000 sentences each with 10 reference
translations taken from the NIST 2002 MT eval-
uation. For Korean-English, 2200 sentence pairs
were randomly sampled from the parallel corpus,
and held out from the training data. These were
divided in half and used for test set and develop-
ment set respectively. For all language pairs, very
minimal tokenization ? splitting off punctuation
? was done on the English side.
5.2 Experimental setup
We used Moses (Koehn et al, 2007) to train
machine translation systems. Default parameters
were used for all experiments except for the num-
ber of iterations for GIZA++ (Och and Ney, 2003).
GIZA++ was run until the perplexity on develop-
ment set stopped decreasing. For practical rea-
sons, the maximum size of a token was set at three
for Chinese, and four for Korean.
2
Minimum error
rate training (Och, 2003) was run on each system
afterwards and BLEU score (Papineni et al, 2002)
was calculated on the test sets.
For the monolingual model, we tested two ver-
sions with the length factor ?
1
, and ?
2
. We picked
? and P (s) so that the number of tokens on source
side (Chinese, and Korean) will be about the same
2
In the Korean writing system, one character is actually
one syllable block. We do not decompose syllable blocks
into individual consonants and vowels.
as the number of tokens in the target side (En-
glish).
For the bilingual model, as explained in the
model section, we are learning P (f | e), but only
P (f) is available for tokenizing any new data. We
compared two conditions: using only the source
data to tokenize the source language training data
according to P (f) (which is consistent with the
conditions at test time), and using both the source
and English data to tokenize the source language
training data (which might produce better tok-
enization by using more information). For the first
length factor ?
1
, we ran an experiment where the
model learns P (s) as described in the model sec-
tion, and we also had experiments where P (s)was
pre-set at 0.9, 0.7, 0.5, and 0.3 for comparison. We
also ran an experiment with the second length fac-
tor ?
2
where ? was picked as the same manner as
the monolingual model.
We varied tokenization of development set and
test set to match the training data for each ex-
periment. However, as we have implied in the
previous paragraph, in the one experiment where
P (f | e) was used to segment training data, di-
rectly incorporating information from target cor-
pus, tokenization for test and development set is
not exactly consistent with tokenization of train-
ing corpus. Since we assume only source corpus
is available at the test time, the test and the devel-
opment set was tokenized only using information
from P (f).
We also trained MT systems using supervised
tokenizations and tokenization requiring a mini-
mal effort for the each language pair. For Chinese-
English, the minimal effort tokenization is maxi-
mal tokenization where every Chinese character is
segmented. Since a number of Chinese tokeniz-
ers are available, we have tried four different to-
kenizations for the supervised tokenizations. The
first one is the LDC Chinese tokenizer available at
the LDC website
3
, which is compiled by Zhibiao
Wu. The second tokenizer is a maxent-based to-
kenizer described by Xue (2003). The third and
fourth tokenizations come from the CRF-based
Stanford Chinese segmenter described by Chang
et al (2008). The difference between third and
fourth tokenization comes from the different gold
standard, the third one is based on Beijing Uni-
versity?s segmentation (pku) and the fourth one is
based on Chinese Treebank (ctb). For Korean-
3
http://projects.ldc.upenn.edu/Chinese/LDC ch.htm
723
Chinese Korean
BLEU F-score BLEU
Supervised
Rule-based morphological analyzer 7.27
LDC segmenter 20.03 0.94
Xue?s segmenter 23.02 0.96
Stanford segmenter (pku) 21.69 0.96
Stanford segmenter (ctb) 22.45 1.00
Unsupervised
Splitting punctuation only 6.04
Maximal (Character-based MT) 20.32 0.75
Bilingual P (f | e) with ?
1
P (s) = learned 19.25 6.93
Bilingual P (f) with ?
1
P (s) = learned 20.04 0.80 7.06
Bilingual P (f) with ?
1
P (s) = 0.9 20.75 0.87 7.46
Bilingual P (f) with ?
1
P (s) = 0.7 20.59 0.81 7.31
Bilingual P (f) with ?
1
P (s) = 0.5 19.68 0.80 7.18
Bilingual P (f) with ?
1
P (s) = 0.3 20.02 0.79 7.38
Bilingual P (f) with ?
2
22.31 0.88 7.35
Monolingual P (f) with ?
1
20.93 0.83 6.76
Monolingual P (f) with ?
2
20.72 0.85 7.02
Table 1: BLEU score results for Chinese-English and Korean-English experiments and F-score of seg-
mentation compared against Chinese Treebank standard. The highest unsupervised score is highlighted.
English, the minimal effort tokenization splitting
off punctuation and otherwise respecting the spac-
ing in the Korean writing system. A Korean mor-
phological analysis tool
4
was used to create the su-
pervised tokenization.
For Chinese-English, since a gold standard for
Chinese segmentation is available, we ran an addi-
tional evaluation of tokenization from each meth-
ods we have tested. We tokenized the raw text
of Chinese Treebank (Xia et al, 2000) using all
of the methods (supervised/unsupervised) we have
described in this section except for the bilingual
tokenization using P (f | e) because the English
translation of the Chinese Treebank data was not
available. We compared the result against the gold
standard segmentation and calculated the F-score.
6 Results
Results from Chinese-English and Korean-English
experiments are presented in Table 1. Note that
nature of data and number of references are dif-
ferent for the two language pairs, and therefore
the BLEU scores are not comparable. For both
language pairs, our models perform equally well
as supervised baselines, or even better. We can
4
http://nlp.kookmin.ac.kr/HAM/eng/main-e.html
observe three things from the result. First, tok-
enization of training data using P (f | e) tested on
a test set tokenized with P (f) performed worse
than any other experiments. This affirms our be-
lief that consistency in tokenization is important
for machine translation, which was alsomentioned
by Chang et al (2008). Secondly, we are learning
valuable information by looking at the target lan-
guage. Compare the result of the bilingual model
with ?
2
as the length factor to the result of the
monolingual model with the same length factor.
The bilingual version consistently performed bet-
ter than the monolingual model in all language
pairs. This tells us we can learn better token
boundaries by using information from the target
language. Thirdly, our hypothesis on the need
for heavy discount for longer tokens is confirmed.
The value for P (s) learned by the model was 0.55,
and 0.58 for Chinese, and Korean respectively. For
both language pairs, this accurately reflects the
empirical distribution of token length, as can be
seen in Figure 2. However, experiments where
P (s) was directly optimized performed better, in-
dicating that this parameter should be optimized
within the context of a complete system. The sec-
ond length factor ?
2
, which discounts longer to-
kens even more heavily, generally performed bet-
724
English the two presidents will hold a joint press conference at the end of their summit talks .
Untokenized Korean ??????????????????????????????? .
Supervised ???? ??? ??? ???????? ?? ????? ????? ? ?? .
Bilingual P (f | e) with ?
1
??????? ?????????? ??????? ?????? ? .
Bilingual P (f) with ?
2
???? ??? ?????????? ??????? ????? ?? .
Monolingual P (f) with ?
1
??? ? ??? ?????????? ????????????? ? .
Monolingual P (f) with ?
2
???? ??? ?????????? ???????????? ?? .
Figure 3: Sample tokenization results for Korean-English data. The underscores are added to clearly
visualize where the breaks are.
ter than the first length factor when used in con-
junction with the bilingual model. Lastly, F-scores
of Chinese segmentations compared against the
gold standard shows higher segmentation accuracy
does not necessarily lead to higher BLEU score.
F-scores presented in Table 1 are not directly com-
parable for all different experiments because the
test data (Chinese Treebank) is used in training for
some of the supervised segmenters, but these num-
bers do show how close unsupervised segmenta-
tions are to the gold standard. It is interesting to
note that our highest unsupervised segmentation
result does make use of bilingual information.
Sample tokenization results for Korean-English
experiments are presented in Figure 3. We observe
that different configurations produce different tok-
enizations, and the bilingual model produced gen-
erally better tokenizations for translation com-
pared to the monolingual models or the super-
vised tokenizer. In this example, the tokenization
obtained from the supervised tokenizer, although
morphologically correct, is too fine-grained for the
purpose of translation to English. For example,
it correctly tokenized the attributive suffix ? -n
however, this is not desirable since English has no
such counterpart. Both variations of the monolin-
gual tokenization have errors such as incorrectly
not segmenting ??? gyeol-gwa-reul, which is
a compound of a noun and a case marker, into?
? ? gyeol-gwa reul as the bilingual model was
able to do.
6.1 Conclusion and future work
We have shown that unsupervised tokenization for
machine translation is feasible and can outperform
rule-based methods that rely on lexical analysis,
or supervised statistical segmentations. The ap-
proach can be applied both to morphological anal-
ysis of Korean and the segmentation of sentences
into words for Chinese, which may at first glace
appear to be quite different problems. We have
only shown how our methods can be applied to
one language of the pair, where one language is
generally isolating and the other is generally syn-
thetic. However, our methods could be extended
to tokenization for both languages by iterating be-
tween languages. We also used the most simple
word-alignment model, but more complex word
alignment models could be incorporated into our
bilingual model.
Acknowledgments This work was supported by
NSF grants IIS-0546554 and ITR-0428020.
References
Matthew J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Univer-
sity College London.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Pi-Chuan Chang, Michel Galley, and Christopher Man-
ning. 2008. Optimizing Chinese word segmentation
for machine translation performance. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 224?232.
Carl de Marcken. 1996. Linguistic structure as compo-
sition and perturbation. In Meeting of the Associa-
tion for Computational Linguistics, pages 335?341.
Morgan Kaufmann Publishers.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153?198.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the International Conference on Computational Lin-
guistics/Association for Computational Linguistics
(COLING/ACL-06), pages 673?680.
725
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In 2007 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 296?305, Prague, Czech Republic,
June. Association for Computational Linguistics.
Genichiro Kikui and Hirofumi Yamamoto. 2002.
Finding translation pairs from english-japanese un-
tokenized aligned corpora. In Proceedings of the
40th Annual Conference of the Association for
Computational Linguistics (ACL-02) workshop on
Speech-to-speech translation: algorithms and sys-
tems, pages 23?30. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL-07), Demonstration Session, pages 177?
180.
I. Dan Melamed. 1999. Bitext maps and alignment
via pattern recognition. Computational Linguistics,
25:107?130.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In AMTA ?02: Pro-
ceedings of the 5th Conference of the Association for
Machine Translation in the Americas on Machine
Translation: From Research to Real Users, pages
135?144, London, UK. Springer-Verlag.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of the 41th Annual Conference of the Association for
Computational Linguistics (ACL-03).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Conference of the Association for
Computational Linguistics (ACL-02).
Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen
Okurowski, John Kovarik, Shizhe Huang, Tony
Kroch, and Mitch Marcus. 2000. Developing
Guidelines and Ensuring Consistency for Chinese
Text Annotation. In Proc. of the 2nd International
Conference on Language Resources and Evaluation
(LREC-2000), Athens, Greece.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 1017?1024, Manchester, UK, August.
Coling 2008 Organizing Committee.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. In International Journal of Com-
putational Linguistics and Chinese Language Pro-
cessing, volume 8, pages 29?48.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008a. Bayesian learning of non-
compositional phrases with synchronous parsing. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-08),
pages 97?105, Columbus, Ohio.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008b. Improved statistical machine translation by
multiple Chinese word segmentation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 216?223.
726
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1308?1317,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Bayesian Learning of Phrasal Tree-to-String Templates
Ding Liu and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
{dliu, gildea}@cs.rochester.edu
Abstract
We examine the problem of overcoming
noisy word-level alignments when learn-
ing tree-to-string translation rules. Our
approach introduces new rules, and re-
estimates rule probabilities using EM. The
major obstacles to this approach are the
very reasons that word-alignments are
used for rule extraction: the huge space
of possible rules, as well as controlling
overfitting. By carefully controlling which
portions of the original alignments are re-
analyzed, and by using Bayesian infer-
ence during re-analysis, we show signifi-
cant improvement over the baseline rules
extracted from word-level alignments.
1 Introduction
Non-parametric Bayesian methods have been suc-
cessfully applied to directly learn phrase pairs
from a bilingual corpus with little or no depen-
dence on word alignments (Blunsom et al, 2008;
DeNero et al, 2008). Because such approaches di-
rectly learn a generative model over phrase pairs,
they are theoretically preferable to the standard
heuristics for extracting the phrase pairs from the
many-to-one word-level alignments produced by
the IBM series models (Brown et al, 1993) or
the Hidden Markov Model (HMM) (Vogel et al,
1996). We wish to apply this direct, Bayesian ap-
proach to learn better translation rules for syntax-
based statistical MT (SSMT), by which we specif-
ically refer to MT systems using Tree-to-String
(TTS) translation templates derived from syntax
trees (Liu et al, 2006; Huang et al, 2006; Gal-
ley et al, 2006; May and Knight, 2007), as op-
posed to formally syntactic systems such as Hi-
ero (Chiang, 2007). The stumbling block pre-
venting us from taking this approach is the ex-
tremely large space of possible TTS templates
when no word alignments are given. Given a sen-
tence pair and syntax tree over one side, there
are an exponential number of potential TTS tem-
plates and a polynomial number of phrase pairs.
In this paper, we explore methods for restricting
the space of possible TTS templates under con-
sideration, while still allowing good templates to
emerge directly from the data as much as possible.
We find an improvement in translation accuracy
through, first, using constraints to limit the number
of new templates, second, using Bayesian methods
to limit which of these new templates are favored
when re-analyzing the training data with EM, and,
third, experimenting with different renormaliza-
tion techniques for the EM re-analysis.
We introduce two constraints to limit the num-
ber of TTS templates that we extract directly from
tree/string pairs without using word alignments.
The first constraint is to limit direct TTS tem-
plate extraction to the part of the corpus where
word alignment tools such as GIZA++ do poorly.
There is no reason not to re-use the good align-
ments from GIZA++, which holds a very compet-
itive baseline performance. As already mentioned,
the noisy alignments from GIZA++ are likely
to cross the boundaries of the tree constituents,
which leads to comparatively big TTS templates.
We use this fact as a heuristic to roughly distin-
guish noisy from good word alignments.
1
Here
we define big templates as those with more than
8 symbols in their right hand sides (RHSs). The
word alignments in big templates are considered
to be noisy and will be recomposed by extracting
smaller TTS templates. Another reason to do ex-
traction on big templates is that the applicability
of big templates to new sentences is very limited
due to their size, and the portion of the training
data from which they are extracted is effectively
wasted. The second constraint, after choosing the
1
Precisely differentiating the noisy/good word alignments
is as hard as correctly aligning the words.
1308
extraction site, is to extract the TTS templates all
the way down to the leaves of the hosting tem-
plates. This constraint limits the number of possi-
ble left hand sides (LHSs) to be equal to the num-
ber of tree nodes in the hosting templates. The
entire extraction process can be summarized in 3
steps:
1. Compute word alignments using GIZA++,
and generate the basic TTS templates.
2. Select big templates from the basic TTS tem-
plates in step 1, and extract smaller TTS tem-
plates all the way down to the bottom from
big templates, without considering the pre-
computed word alignments.
3. Combine TTS templates from step 1 and step
2 and estimate their probabilities using Vari-
ational Bayes with a Dirichlet Process prior.
In step 2, since there are no constraints from the
pre-computed word alignments, we have complete
freedom in generating all possible TTS templates
to overcome noisy word alignments. We use vari-
ational EM to approximate the inference of our
Bayesian model and explore different normaliza-
tion methods for the TTS templates. A two-stage
normalization is proposed by combining LHS-
based normalization with normalization based on
the root of the LHS, and is shown to be the best
model when used with variational EM.
Galley et al (2006) recompose the TTS tem-
plates by inserting unaligned target words and
combining small templates into bigger ones. The
recomposed templates are then re-estimated using
the EM algorithm described in Graehl and Knight
(2004). This approach also generates TTS tem-
plates beyond the precomputed word alignments,
but the freedom is only granted over unaligned tar-
get words, and most of the pre-computed word
alignments remain unchanged. Other prior ap-
proaches towards improving TTS templates fo-
cus on improving the word alignment performance
over the classic models such as IBM series mod-
els and Hidden Markov Model (HMM), which do
not consider the syntactic structure of the align-
ing languages and produce syntax-violating align-
ments. DeNero and Klein (2007) use a syntax-
based distance in an HMM word alignment model
to favor syntax-friendly alignments. Fossum et al
(2008) start from the GIZA++ alignment and in-
crementally delete bad links based on a discrim-
S
NP
VP
?
NN
?
AUX
issu
e
has
??
??
?
?
S
NP
VP
?
NN
?
AUX
issu
e
has
??
??
?
?
Figure 1: 5 small TTS templates are extracted based on the
correct word alignments (left), but only 1 big TTS template
(right) can be extracted when the cross-boundary noisy align-
ments are added in.
inative model with syntactic features. This ap-
proach can only find a better subset of the GIZA++
alignment and requires a parallel corpus with gold-
standard word alignment for training the discrim-
inative model. May and Knight (2007) factorize
the word alignment into a set of re-orderings rep-
resented by the TTS templates and build a hierar-
chical syntax-based word alignment model. The
problem is that the TTS templates are generated
by the word alignments from GIZA++, which lim-
its the potential of the syntactic re-alignment. As
shown by these prior approaches, directly improv-
ing the word alignment either falls into the frame-
work of many-to-one alignment, or is substantially
confined by the word alignment it builds upon.
The remainder of the paper focuses on the
Bayesian approach to learning TTS templates and
is organized as follows: Section 2 describes the
procedure for generating the candidate TTS tem-
plates; Section 3 describes the inference methods
used to learn the TTS templates; Section 4 gives
the empirical results, Section 5 discusses the char-
acteristics of the learned TTS templates, and Sec-
tion 6 presents the conclusion.
2 Extracting Phrasal TTS Templates
The Tree-to-String (TTS) template, the most im-
portant component of a SSMT system, usually
contains three parts: a fragment of a syntax tree
in its left hand side (LHS), a sequence of words
and variables in its right hand side (RHS), and
a probability indicating how likely the template
is to be used in translation. The RHS of a TTS
template shows one possible translation and re-
ordering of its LHS. The variables in a TTS tem-
plate are further transformed using other TTS tem-
plates, and the recursive process continues until
there are no variables left. There are two ways
1309
SNP VP
NP1 PP ADJAUX
in isNP3
of
NP3 ? NP1 ??
beautiful
2
1
3 4
Figure 2: Examples of valid and invalid templates extracted
from a big Template. Template 1, invalid, doesn?t go all the
way down to the bottom. Template 2 is valid. Template 3, in-
valid, doesn?t have the same set of variables in its LHS/RHS.
Template 4, invalid, is not a phrasal TTS template.
that TTS templates are commonly used in ma-
chine translation. The first is synchronous pars-
ing (Galley et al, 2006; May and Knight, 2007),
where TTS templates are used to construct syn-
chronous parse trees for an input sentence, and
the translations will be generated once the syn-
chronous trees are built up. The other way is
the TTS transducer (Liu et al, 2006; Huang et
al., 2006), where TTS templates are used just as
their name indicates: to transform a source parse
tree (or forest) into the proper target string. Since
synchronous parsing considers all possible syn-
chronous parse trees of the source sentence, it is
less constrained than TTS transducers and hence
requires more computational power. In this paper,
we use a TTS transducer to test the performance of
different TTS templates, but our techniques could
also be applied to SSMT systems based on syn-
chronous parsing.
2.1 Baseline Approach: TTS Templates
Obeying Word Alignment
TTS templates are commonly generated by de-
composing a pair of aligned source syntax tree
and target string into smaller pairs of tree frag-
ments and target string (i.e., the TTS templates).
To keep the number of TTS templates to a manage-
able scale, only the non-decomposable TTS tem-
plates are generated. This algorithm is referred to
as GHKM (Galley et al, 2004) and is widely used
in SSMT systems (Galley et al, 2006; Liu et al,
2006; Huang et al, 2006). The word alignment
used in GHKM is usually computed independent
of the syntactic structure, and as DeNero and Klein
(2007) and May and Knight (2007) have noted,
Ch-En En-Ch Union Heuristic
28.6% 33.0% 45.9% 20.1%
Table 1: Percentage of corpus used to generate big templates,
based on different word alignments
9-12 13-20 ?21
Ch-En 18.2% 17.4% 64.4%
En-Ch 15.9% 20.7% 63.4%
Union 9.8% 15.1% 75.1%
Heuristic 24.6% 27.9% 47.5%
Table 2: In the selected big templates, the distribution of
words in the templates of different sizes, which are measured
based on the number of symbols in their RHSs
is not the best for SSMT systems. In fact, noisy
word alignments cause more damage to a SSMT
system than to a phrase based SMT system, be-
cause the TTS templates can only be derived from
tree constituents. If some noisy alignments happen
to cross over the boundaries of two constituents,
as shown in Figure 2, a much bigger tree frag-
ment will be extracted as a TTS template. Even
though the big TTS templates still carry the orig-
inal alignment information, they have much less
chance of getting matched beyond the syntax tree
where they were extracted, as we show in Sec-
tion 4. In other words, a few cross-boundary noisy
alignments could disable a big portion of a training
syntax tree, while for a phrase-based SMT system,
their effect is limited to the phrases they align. As
a rough measure of how the training corpus is af-
fected by the big templates, we calculated the dis-
tribution of target words in big and non-big TTS
templates. The word alignment is computed using
GIZA++
2
for the selected 73,597 sentence pairs in
the FBIS corpus in both directions and then com-
bined using union and heuristic diagonal growing
(Koehn et al, 2003). Table 1 shows that big
templates consume 20.1% to 45.9% of the training
corpus depending on different types of word align-
ments. The statistics indicate that a significant
portion of the training corpus is simply wasted,
if the TTS templates are extracted based on word
alignments from GIZA++. On the other hand, it
shows the potential for improving an SSMT sys-
tem if we can efficiently re-use the wasted train-
ing corpus. By further examining the selected big
templates, we find that the most common form of
big templates is a big skeleton template starting
2
GIZA++ is available at
http://www.fjoch.com/GIZA++.html
1310
from the root of the source syntax tree, and hav-
ing many terminals (words) misaligned in the bot-
tom. Table 2 shows, in the selected big templates,
the distribution of words in the templates of differ-
ent sizes (measured based on the number of sym-
bols in their RHS). We can see that based on ei-
ther type of word alignment, the most common
big templates are the TTS templates with more
than 20 symbols in their RHSs, which are gen-
erally the big skeleton templates. The advantage
of such big skeleton templates is that they usually
have good marginal accuracy
3
and allow accurate
smaller TTS templates to emerge.
2.2 Liberating Phrasal TTS Templates From
Noisy Word Alignments
To generate better TTS templates, we use a more
direct way than modifying the underlying word
alignment: extract smaller phrasal TTS tem-
plates from the big templates without looking at
their pre-computed word alignments. We define
phrasal TTS templates as those with more than
one symbol (word or non-terminal) in their LHS.
The reason to consider only phrasal TTS tem-
plates is that they are more robust than the word-
level TTS templates in addressing the complicated
word alignments involved in big templates, which
are usually not the simple type of one-to-many or
many-to-one. Abandoning the pre-computed word
alignments in big templates, an extracted smaller
TTS template can have many possible RHSs, as
long as the two sides have the same set of vari-
ables. Note that the freedom is only given to the
alignments of the words; for the variables in the
big templates, we respect the pre-computed word
alignments. To keep the extracted smaller TTS
templates to a manageable scale, the following two
constraints are applied:
1. The LHS of extracted TTS templates should
go all the way down to the bottom of the LHS
of the big templates. This constraint ensures
that at most N LHSs can be extracted from
one big Template, where N is the number of
tree nodes in the big Template?s LHS.
2. The number of leaves (including both words
and variables) in an extracted TTS template?s
LHS should not exceed 6. This constraint
limits the size of the extracted TTS templates.
3
Here, marginal accuracy means the correctness of the
TTS template?s RHS corresponding to its LHS.
( VP ( AUX is ) ( ADJ  beautiful ) )         ??( PP ( IN of )  NP3 )          NP3  ?( NP NP1 ( PP ( IN of )  NP3 ) )          NP3  ? NP1( NP NP1 ( PP ( IN of )  NP3 ) )          NP3  ? NP1  ??
Figure 3: All valid templates that can be extracted from the
example in Figure 2.1
for all template t do
if size(t.rhs) > 8 then
for all tree node s in t.lhs do
subt = subtree(s, t.lhs);
if leaf num(subt) ? 6 then
for i=1:size(t.rhs) do
for j=i:size(t.rhs) do
if valid(subt, i, j) then
create template(subt, i, j);
Figure 4: Algorithm that liberates smaller TTS Templates
from big templates
As we show in Section 4, use of bigger TTS
templates brings very limited performance
gain.
Figure 2.2 describes the template liberating algo-
rithm running in O(NM
2
), where N denotes the
number of tree nodes in the LHS of the input big
Template andM denotes the length of the RHS. In
the algorithm, function valid returns true if there
are the same set of variables in the left/right hand
side of an extracted TTS template; subtree(x, y)
denotes the sub-tree in y which is rooted at x and
goes all the way down to y?s bottom. Figure 2.1
shows valid and invalid TTS templates which can
be extracted from an example hosting TTS tem-
plate. Note that, in order to keep the example
simple, the hosting TTS template only has 4 sym-
bols in its RHS, which does not qualify as a big
template according to our definition. Figure 2.2
shows the complete set of valid TTS templates
which can be extracted from the example TTS
template. The subscripts of the non-terminals are
used to differentiate identical non-terminals in dif-
ferent positions. The extraction process blindly
releases smaller TTS templates from the big tem-
plates, among which only a small fraction are cor-
rect TTS templates. Therefore, we need an infer-
ence method to raise the weight of the correct tem-
plates and decrease the weight of the noisy tem-
plates.
1311
3 Estimating TTS Template Probability
The Expectation-Maximization (EM) algorithm
(Dempster et al, 1977) can be used to estimate
the TTS templates? probabilities, given a genera-
tive model addressing how a pair of source syn-
tax tree and target string is generated. There are
two commonly used generative models for syntax-
based MT systems, each of which corresponds to
a normalization method for the TTS templates.
The LHS-based normalization (LHSN) (Liu et al,
2006; Huang et al, 2006), corresponds to the
generative process where the source syntax sub-
tree is first generated, and then the target string
is generated given the source syntax subtree. The
other one is normalization based on the root of
the LHS (ROOTN) (Galley et al, 2006), corre-
sponding to the generative process where, given
the root of the syntax subtree, the LHS syntax sub-
tree and the RHS string are generated simultane-
ously. By omitting the decomposition probability
in the LHS-based generative model, the two gen-
erative models share the same formula for comput-
ing the probability of a training instance:
Pr(T, S) =
?
R
Pr(T, S,R) =
?
R
(
?
t?R
Pr(t)
)
where T and S denote the source syntax tree and
target string respectively, R denotes the decompo-
sition of (T, S), and t denotes the TTS template.
The expected counts of the TTS templates can then
be efficiently computed using an inside-outside-
like dynamic programming algorithm (May and
Knight, 2007).
LHSN, as shown by Galley et al (2006), cannot
accurately restore the true conditional probabili-
ties of the target sentences given the source sen-
tences in the training corpus. This indicates that
LHSN is not good at predicting unseen sentences
or at translating new sentences. But this deficiency
does not affect its ability to estimate the expected
counts of the TTS templates, because the posteri-
ors of the TTS templates only depend on the com-
parative probabilities of the different derivations
of a training instance (a pair of tree and string).
In fact, as we show in Section 4, LHSN is bet-
ter than ROOTN in liberating smaller TTS tem-
plates out of the big templates, since it is less bi-
ased to the big templates in the EM training.
4
Be-
cause the two normalization methods have their
4
Based on LHSN, the difference between the probabil-
ity of a big Template and the product of the probabilities of
E-step:
for all pair of syntax tree T and target string S do
for all TTS Template t do
EC(t)+ =
P
R:t?R
Pr(T,S,R)
?
P
R
?
Pr(T,S,R
?
)
?
;
Increase ?;
M-step:
for all TTS Template t do
if it is the last iteration then
Pr(t) =
EC(t)
P
t
?
:t
?
.root=t.root
EC(t
?
)
;
else
Pr(t) =
EC(t)
P
t
?
:t
?
.lhs=t.lhs
EC(t
?
)
;
Figure 5: EM Algorithm For Estimating TTS Templates
own strength and weakness, both of them are used
in our EM algorithm: LHSN is used in all EM
iterations except the last one to compute the ex-
pected counts of the TTS templates, and ROOTN
is used in the last EM iteration to compute the final
probabilities of the TTS templates. This two-stage
normalization method is denoted as MIXN in this
paper.
Deterministic Annealing (Rose et al, 1992) is
is used in our system to speed up the training
process, similar to Goldwater et al (2006). We
start from a high temperature and gradually de-
crease the temperature to 1; we find that the ini-
tial high temperature can also help small templates
to survive the initial iterations. The complete EM
framework is sketched in Figure 3, where ? is the
inverse of the specified temperature, and EC de-
notes the expected count.
3.1 Bayesian Inference with the Dirichlet
Process Prior
Bayesian inference plus the Dirichlet Process (DP)
have been shown to effectively prevent MT mod-
els from overfitting the training data (DeNero et
al., 2008; Blunsom et al, 2008). A similar ap-
proach can be applied here for SSMT by consider-
ing each TTS template as a cluster, and using DP
to adjust the number of TTS templates according
to the training data. Note that even though there
is a size limitation on the liberated phrasal TTS
templates, standard EM will still tend to overfit
the training data by pushing up the probabilities of
the big templates from the noisy word alignments.
The complete generative process, integrating the
DP prior and the generative models described in
its decomposing TTS templates is much less than the one
based on ROOTN, thus LHSN gives comparably more ex-
pected counts to the smaller TTS templates than ROOTN.
1312
for all TTS Template t do
if it is the last iteration then
Pr(t) =
exp(?(EC(t)+?G
0
(t)))
exp
(
?
((
P
t
?
:t
?
.root=t.root
EC(t
?
)
)
+?
))
;
else
Pr(t) =
exp(?(EC(t)+?G
0
(t)))
exp
(
?
((
P
t
?
:t
?
.lhs=t.lhs
EC(t
?
)
)
+?
))
;
Figure 6: M-step of the Variational EM
Section 3.1, is given below:
?
r
| {?
r
, G
r
0
} ? DP (?
r
, G
r
0
)
t | ?
t.root
? ?
t.root
(T, S) | {SG, {t}, ?} ? SG({t}, ?)
where G
0
is a base distribution of the TTS tem-
plates, t denotes a TTS template, ?
t.root
denotes
the multinomial distribution over TTS templates
with the same root as t, SG denotes the generative
model for a pair of tree and string in Section 3.1,
and ? is a free parameter which adjusts the rate at
which new TTS templates are generated.
It is intractable to do exact inference under the
Bayesian framework, even with a conjugate prior
such as DP. Two methods are commonly used
for approximate inference: Markov chain Monte
Carlo (MCMC) (DeNero et al, 2008), and Vari-
ational Bayesian (VB) inference (Blunsom et al,
2008). In this paper, the latter approach is used be-
cause it requires less running time. The E-step of
VB is exactly the same as standard EM, and in the
M-step the digamma function ? and the base dis-
tributionG
0
are used to increase the uncertainty of
the model. Similar to standard EM, both LHS- and
root-based normalizations are used in the M-step,
as shown in Figure 3.1. For the TTS templates,
which are also pairs of subtrees and strings, a natu-
ral choice ofG
0
is the generative models described
in Section 3.1. BecauseG
0
estimates the probabil-
ity of the new TTS templates, the root-based gen-
erative model is superior to the LHS-based gener-
ative model and used in our approach.
3.2 Initialization
Since the EM algorithm only converges to a lo-
cal minimum, proper initializations are needed to
achieve good performance for both standard EM
and variational EM. For the baseline templates
derived from word alignments, the initial counts
are set to the raw counts in the training corpus.
For the templates blindly extracted from big tem-
plates, the raw count of a LHS tree fragment is
distributed among their RHSs based on the like-
lihood of the template, computed by combining
for all big template t do
for all template g extracted from t do
g.count = g.lhs.count = 0;
for all template g extracted from t do
g.count += w in(g)?w out(g, t);
g.lhs.count += w in(g)?w out(g, t);
for all template g extracted from t do
g.init +=
g.count
g.lhs.count
;
Figure 7: Compute the initial counts of the liberated TTS
templates
the word-based inside/outside scores. The algo-
rithm is sketched in Figure 3.2, where the inside
score w in(g) is the product of the IBM Model 1
scores in both directions, computed based on the
words in g?s LHS and RHS. The outside score
w out(g, t) is computed similarly, except that the
IBM Model 1 scores are computed based on the
words in the hosting template t?s LHS/RHS ex-
cluding the words in g?s LHS/RHS. The initial
probabilities of the TTS templates are then com-
puted by normalizing their initial counts using
LHSN or ROOTN.
4 Experiments
We train an English-to-Chinese translation sys-
tem using the FBIS corpus, where 73,597 sentence
pairs are selected as the training data, and 500 sen-
tence pairs with no more than 25 words on the Chi-
nese side are selected for both the development
and test data.
5
Charniak (2000)?s parser, trained
on the Penn Treebank, is used to generate the En-
glish syntax trees. Modified Kneser-Ney trigram
models are trained using SRILM (Stolcke, 2002)
upon the Chinese portion of the training data. The
trigram language model, as well as the TTS tem-
plates generated based on different methods, are
used in the TTS transducer. The model weights
of the transducer are tuned based on the develop-
ment set using a grid-based line search, and the
translation results are evaluated based on a single
Chinese reference
6
using BLEU-4 (Papineni et al,
2002). Huang et al (2006) used character-based
BLEU as a way of normalizing inconsistent Chi-
nese word segmentation, but we avoid this prob-
lem as the training, development, and test data are
from the same source.
5
The total 74,597 sentence pairs used in experiments are
those in the FBIS corpus whose English part can be parsed
using Charniak (2000)?s parser.
6
BLEU-4 scores based on a single reference are much
lower than the ones based on multiple references.
1313
E2C C2E Union Heuristic
w/ Big 13.37 12.66 14.55 14.28
w/o Big 13.20 12.62 14.53 14.21
Table 3: BLEU-4 scores (test set) of systems based on
GIZA++ word alignments
? 5 ? 6 ? 7 ? 8 ? ?
BLEU-4 14.27 14.42 14.43 14.45 14.55
Table 4: BLEU-4 scores (test set) of the union alignment, us-
ing TTS templates up to a certain size, in terms of the number
of leaves in their LHSs
4.1 Baseline Systems
GHKM (Galley et al, 2004) is used to generate
the baseline TTS templates based on the word
alignments computed using GIZA++ and different
combination methods, including union and the di-
agonal growing heuristic (Koehn et al, 2003). We
also tried combining alignments from GIZA++
based on intersection, but it is worse than both
single-direction alignments, due to its low cover-
age of training corpus and the incomplete transla-
tions it generates. The baseline translation results
based on ROOTN are shown in Table 4.1. The first
two columns in the table show the results of the
two single direction alignments. e2c and c2e de-
note the many English words to one Chinese word
alignment and the many Chinese words to one En-
glish word alignment, respectively. The two rows
show the results with and without the big tem-
plates, from which we can see that removing the
big templates does not affect performance much;
this verifies our postulate that the big templates
have very little chance of being used in the trans-
lation. Table 4.1, using the union alignments as
the representative and measuring a template?s size
by the number of leaves in its LHS, also demon-
strates that using big TTS templates brings very
limited performance gain.
The result that the union-based combination
outperforms either single direction alignments and
even the heuristic-based combination, combined
with the statistics of the disabled corpus in Sec-
tion 2.2, shows that more disabled training cor-
pus actually leads to better performance. This can
be explained by the fact that the union alignments
have the largest number of noisy alignments gath-
ered together in the big templates, and thus have
the least amount of noisy alignments which lead
to small and low-quality TTS templates.
 17
 17.5
 18
 18.5
 19
 19.5
 20
 20.5
 21
 1  2  3  4  5  6  7  8  9  10
1.01.01.01.00.90.80.70.50.30.1
iteration
temperature parameter ?
MIXN-EM
LHSN-VB
LHSN-EM
ROOTN-EM
ROOTN-VB
MIXN-VB
Figure 8: BLEU-4 scores (development set) of annealing EM
and annealing VB in each iteration.
4.2 Learning Phrasal TTS Templates
To test our learning methods, we start with the
TTS templates generated based on e2c, c2e, and
union alignments using GHKM. This gives us
0.98M baseline templates. We use the big tem-
plates from the union alignments as the basis
and extract 10.92M new phrasal TTS templates,
which, for convenience, are denoted by NEW-
PHR. Because based on Table 1 and Table 2
the union alignment has the greatest number of
alignment links and therefore produces the largest
rules, this gives us the greatest flexibility in re-
aligning the input sentences. The baseline TTS
templates as well as NEW-PHR are initialized us-
ing the method in Section 3.3 for both annealing
EM and annealing VB. To simplify the experi-
ments, the same Dirichlet Process prior is used for
all multinomial distributions of the TTS templates
with different roots. G
0
in the Dirichlet prior is
computed based on the 1-level TTS templates se-
lected from the baseline TTS templates, so that the
big templates are efficiently penalized. The train-
ing algorithms follow the same annealing sched-
ule, where the temperature parameter ? is initial-
ized to 0.1, and gradually increased to 1.
We experiment with the two training algo-
rithms, annealing EM and annealing VB, with dif-
ferent normalization methods. The experimental
results based on the development data are shown
in Figure 4.2, where the free parameter ? of an-
nealing VB is set to 1, 100, and 100 respec-
tively for ROOTN, LHSN, and MIXN. The re-
sults verify that LHSN is worse than ROOTN in
predicting the translations, since MIXN outper-
forms LHSN with both annealing EM and VB.
ROOTN is on par with MIXN and much better
1314
Max Likelihood Annealing EM Annealing VB
w/o new-phr with new-phr w/o new-phr with new-phr w/o new-phr with new-phr
LHSN 14.05 13.16 14.31 15.33 14.82 16.15
ROOTN 14.50 13.49 14.90 16.06 14.76 16.12
MIXN NA NA 14.82 16.37 14.93 16.84
Table 5: BLEU-4 scores (test set) of different systems.
Initial Template Final Template
number new-phr% number new-phr%
ROOTN 11.9M 91.8% 408.0K 21.9%
LHSN 11.9M 91.8% 557.2K 29.8%
MIXN 11.9M 91.8% 500.5K 27.6%
Table 6: The total number of templates and the percentage of
NEW-PHR, in the beginning and end of annealing VB
than LHSN when annealing EM is used; but with
annealing VB, it is outperformed by MIXN by
a large margin and is even slightly worse than
LHSN. This indicates that ROOTN is not giv-
ing large expected counts to NEW-PHR and leaves
very little space for VB to further improve the re-
sults. For all the normalization methods, anneal-
ing VB outperforms annealing EM and maintains
a longer ascending path, showing better control of
overfitting for the Bayesian models. Figure 4.2
shows the optimized results of the development
set based on annealing VB with different ?. The
best performance is achieved as ? approaches 1,
100, and 100 for ROOTN, LHSN and MIXN re-
spectively. The ? parameter can be viewed as a
weight used to balance the expected counts and
the probabilities from G
0
. Thus it is reasonable
for LHSN and MIXN to have bigger optimal ?
than ROOTN, since ROOTN gives lower expected
counts to NEW-PHR than LHSN and MIXN do.
To see the contribution of the phrasal template
extraction in the performance gain, MT experi-
ments are conducted by turning this component
on and off. Results on the test set, obtained by
using parameters optimized on the development
set, are shown in Table 4.2. The template counts
used in the Max-Likelihood training are the same
as the ones used in the initialization of anneal-
ing EM and VB. Results show that for annealing
EM and VB, use of NEW-PHR greatly improves
performance, while for the Max-Likelihood train-
ing, use of NEW-PHR hurts performance. This
is not surprising, because Max-Likelihood train-
ing cannot efficiently filter out the noisy phrasal
templates introduced in the initial NEW-PHR. An-
other observation is that annealing VB does not al-
ways outperform annealing EM. With NEW-PHR
 19.8
 20
 20.2
 20.4
 20.6
 20.8
 21
 0.1  1  10  100  1000
?
MIXN
ROOTN
LHSN
Figure 9: BLEU-4 scores (development set) of annealing VB
with different ?.
turned on, annealing VB shows consistent supe-
riority over annealing EM; while without NEW-
PHR, it only outperforms annealing EM based on
LHSN and MIXN, and the improvement is not as
big as when NEW-PHR is turned on. This indi-
cates that without NEW-PHR, there is less need
to use VB to shrink down the size of the tem-
plate set. Table 4.2 shows the statistics of the ini-
tial template set including NEW-PHR and the final
TTS template set after annealing VB is conducted,
where we can see annealing VB efficiently re-
duces NEW-PHR to a relatively small size and re-
sults in much more compact systems than the sys-
tem based on the baseline templates from GIZA++
alignments. Comparing with the best GIZA++-
based system union, our best system, utilizing
NEW-PHR and the two-stage template normaliza-
tion, demonstrates the strength of annealing VB
by an absolute improvement of 2.29% in BLEU-
4 score, from 14.55 to 16.84. This improvement
is significant at p < 0.005 based on 2000 itera-
tions of paired bootstrap re-sampling of the test
set (Koehn, 2004).
5 Discussion
Our experimental results are obtained based on
a relatively small training corpus, the improved
performance may be questionable when a larger
training corpus is used. Someone may wonder if
the performance gain primarily comes from the
1315
Many-to-one Alignment
( VP ( VB make ) ( NP ( DT a ) ( JJ complete ) ( NN statement ) ) ) ????
( S ( VP VBG ( NP ( DT the ) ( NN mass ) ( NN line ) ) PP ) ) PP VBG ????
( PP ( TO to ) ( NP ( DT the ) ( JJS greatest ) ( NN extent ) ) ) ???? ?
( PP ( IN of ) ( NP ( JJ peaceful ) ( NNP coexistence ) ) ) ????
Many-to-many Alignment
( VP ( VBN based ) ( PP ( IN on ) ( NP ( JJ actual ) ( NNS needs ) ) ) ) ? ?? ??
( PP ( IN into ) ( NP ( NP ( DT the ) ( NNS hands ) ) PP ) ) ?? ? PP ??
( VP ( VBP exercise ) ( NP ( JJ strict ) ( NN self-discipline ) ) ) ? ? ? ?
( SBAR ( S ( NP ( DT the ) ( VBG aging ) NN ) ( VP ( aux is ) NP ) ) ) NN ? ? ? ? NP
( NP NP1 PP ( , , ) ( VP ( VBN centered ) ( PP ( IN around ) NP2 ) ) ) ? NP2 ? ?? ? NP1 PP
Allowance of Bad Word Segmentation
( NP ( NP ( NNP japan ) ( POS 's ) ) ( NNP sdf ) ( NNP navy ) ) ??? ? ???
( NP ( PDT all ) ( NP ( NNS people ) ( POS 's ) ) ( NNS organizations ) ) ?? ?? ?
Figure 10: Examples of the learned TTS templates
reduced out of vocabulary (OOV) ratio. We ex-
amined the OOV ratio of the test set with/without
the learned TTS templates, and found the differ-
ence was very small. In fact, our method is de-
signed to learn the phrasal TTS templates, and ex-
plictly avoids lexical pairs. To further understand
the characteristics of the learned TTS templates,
we list some representative templates in Figure 4.2
classified in 3 groups. The group Many-to-one
Alignment and Many-to-many Alignment show the
TTS templates based on complicated word align-
ments, which are difficult to compute based on the
existing word alignment models. These templates
do not have rare English words, whose translation
cannot be found outside the big templates. The
difficulty lies in the non-literal translation of the
source words, which are unlikely to learnt by soly
increasing the size of the training corpus. One
other interesting observation is that our learning
method is tolerant to noisy Chinese word segmen-
tation, as shown in group Allowance of Bad Word
Segmentation.
6 Conclusion
This paper proposes a Bayesian model for extract-
ing the Tree-to-String templates directly from the
data. By limiting the extraction to the big tem-
plates from the pre-computed word alignments
and applying a set of constraints, we restrict the
space of possible TTS templates under consider-
ation, while still allowing new and more accurate
templates to emerge from the training data. The
empirical results demonstrate the strength of our
approach, which outperforms the GIZA++-based
systems by a large margin. This encourages a
move from word-alignment-based systems to sys-
tems based on consistent, end-to-end probabilistic
modeling. Because our Bayesian model employs
a very simple prior, more sophisticated generative
models provide a possible direction for further ex-
perimentation.
Acknowledgments This work was supported by
NSF grants IIS-0546554 and ITR-0428020.
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian synchronous grammar induction. In Neu-
ral Information Processing Systems (NIPS).
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL-01, pages
132?139.
1316
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Soci-
ety, 39(1):1?21.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of ACL-07, pages 17?24.
John DeNero, Alexandre Bouchard-Cote, and Dan
Klein. 2008. Sampling alignment structure under
a bayesian translation model. In EMNLP08.
Victoria Fossum, Kevin Knight, and Steven Abney.
2008. Using syntax to improveword alignment pre-
cision for syntax-based machine translation. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, Columbus, Ohio. ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of NAACL-04, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING/ACL-06, pages 961?968, July.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the Human Language Technology Conference/North
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL).
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In Proceedings of NAACL-04.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Bi-
ennial Conference of the Association for Machine
Translation in the Americas (AMTA), Boston, MA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL-03, Edmonton, Alberta.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388?395, Barcelona, Spain, July.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of COLING/ACL-06,
Sydney, Australia, July.
J. May and K. Knight. 2007. Syntactic re-alignment
models for machine translation. In Proceedings of
EMNLP.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of ACL-02.
K. Rose, E. Gurewitz, and G. C. Fox. 1992. Vec-
tor quantization by deterministic annealing. IEEE
Transactions on Information Theory, 38(4):1249?
1257, July.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In International Conference on
Spoken Language Processing, volume 2, pages 901?
904.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In COLING-96, pages 836?841.
1317
The Proposition Bank: An Annotated
Corpus of Semantic Roles
Martha Palmer
University of Pennsylvania
Daniel Gildea.
University of Rochester
Paul Kingsbury
University of Pennsylvania
The Proposition Bank project takes a practical approach to semantic representation, adding a
layer of predicate-argument information, or semantic role labels, to the syntactic structures of
the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not
represent coreference, quantification, and many other higher-order phenomena, but also broad,
in that it covers every instance of every verb in the corpus and allows representative statistics to
be calculated.
We discuss the criteria used to define the sets of semantic roles used in the annotation process
and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an
automatic system for semantic role tagging trained on the corpus and discuss the effect on its
performance of various types of information, including a comparison of full syntactic parsing
with a flat representation and the contribution of the empty ??trace?? categories of the treebank.
1. Introduction
Robust syntactic parsers, made possible by new statistical techniques (Ratnaparkhi
1997; Collins 1999, 2000; Bangalore and Joshi 1999; Charniak 2000) and by the
availability of large, hand-annotated training corpora (Marcus, Santorini, and
Marcinkiewicz 1993; Abeille? 2003), have had a major impact on the field of natural
language processing in recent years. However, the syntactic analyses produced by
these parsers are a long way from representing the full meaning of the sentences that
are parsed. As a simple example, in the sentences
(1) John broke the window.
(2) The window broke.
a syntactic analysis will represent the window as the verb?s direct object in the first
sentence and its subject in the second but does not indicate that it plays the same
underlying semantic role in both cases. Note that both sentences are in the active voice
* 2005 Association for Computational Linguistics
 Department of Computer and Information Science, University of Pennsylvania, 3330 Walnut Street,
Philadelphia, PA 19104. Email: mpalmer@cis.upenn.edu.
. Department of Computer Science, University of Rochester, PO Box 270226, Rochester, NY 14627. Email:
gildea@cs.rochester.edu.
Submission received: 9th December 2003; Accepted for publication: 11th July 2004
and that this alternation in subject between transitive and intransitive uses of the verb
does not always occur; for example, in the sentences
(3) The sergeant played taps.
(4) The sergeant played.
the subject has the same semantic role in both uses. The same verb can also undergo
syntactic alternation, as in
(5) Taps played quietly in the background.
and even in transitive uses, the role of the verb?s direct object can differ:
(6) The sergeant played taps.
(7) The sergeant played a beat-up old bugle.
Alternation in the syntactic realization of semantic arguments is widespread,
affecting most English verbs in some way, and the patterns exhibited by specific verbs
vary widely (Levin 1993). The syntactic annotation of the Penn Treebank makes it
possible to identify the subjects and objects of verbs in sentences such as the above
examples. While the treebank provides semantic function tags such as temporal and
locative for certain constituents (generally syntactic adjuncts), it does not distinguish
the different roles played by a verb?s grammatical subject or object in the above
examples. Because the same verb used with the same syntactic subcategorization can
assign different semantic roles, roles cannot be deterministically added to the treebank
by an automatic conversion process with 100% accuracy. Our semantic-role annotation
process begins with a rule-based automatic tagger, the output of which is then hand-
corrected (see section 4 for details).
The Proposition Bank aims to provide a broad-coverage hand-annotated corpus of
such phenomena, enabling the development of better domain-independent language
understanding systems and the quantitative study of how and why these syntactic
alternations take place. We define a set of underlying semantic roles for each verb and
annotate each occurrence in the text of the original Penn Treebank. Each verb?s roles
are numbered, as in the following occurrences of the verb offer from our data:
(8) . . . [
Arg0
the company] to . . . offer [
Arg1
a 15% to 20% stake] [
Arg2
to the public]
(wsj_0345)1
(9) . . . [
Arg0
Sotheby?s] . . . offered [
Arg2
the Dorrance heirs] [
Arg1
a money-back
guarantee] (wsj_1928)
(10) . . . [
Arg1
an amendment] offered [
Arg0
by Rep. Peter DeFazio] . . . (wsj_0107)
(11) . . . [
Arg2
Subcontractors] will be offered [
Arg1
a settlement] . . . (wsj_0187)
We believe that providing this level of semantic representation is important for
applications including information extraction, question answering, and machine
72
1 Example sentences drawn from the treebank corpus are identified by the number of the file in which they
occur. Constructed examples usually feature John.
Computational Linguistics Volume 31, Number 1
73
translation. Over the past decade, most work in the field of information extraction has
shifted from complex rule-based systems designed to handle a wide variety of
semantic phenomena, including quantification, anaphora, aspect, and modality (e.g.,
Alshawi 1992), to more robust finite-state or statistical systems (Hobbs et al 1997;
Miller et al 1998). These newer systems rely on a shallower level of semantic
representation, similar to the level we adopt for the Proposition Bank, but have also
tended to be very domain specific. The systems are trained and evaluated on corpora
annotated for semantic relations pertaining to, for example, corporate acquisitions or
terrorist events. The Proposition Bank (PropBank) takes a similar approach in that we
annotate predicates? semantic roles, while steering clear of the issues involved in
quantification and discourse-level structure. By annotating semantic roles for every
verb in our corpus, we provide a more domain-independent resource, which we hope
will lead to more robust and broad-coverage natural language understanding systems.
The Proposition Bank focuses on the argument structure of verbs and provides a
complete corpus annotated with semantic roles, including roles traditionally viewed as
arguments and as adjuncts. It allows us for the first time to determine the frequency of
syntactic variations in practice, the problems they pose for natural language
understanding, and the strategies to which they may be susceptible.
We begin the article by giving examples of the variation in the syntactic realization
of semantic arguments and drawing connections to previous research into verb alter-
nation behavior. In section 3 we describe our approach to semantic-role annotation,
including the types of roles chosen and the guidelines for the annotators. Section 5
compares our PropBank methodology and choice of semantic-role labels to those of
another semantic annotation project, FrameNet. We conclude the article with a dis-
cussion of several preliminary experiments we have performed using the PropBank
annotations, and discuss the implications for natural language research.
2. Semantic Roles and Syntactic Alternation
Our work in examining verb alternation behavior is inspired by previous research into
the linking between semantic roles and syntactic realization, in particular, the
comprehensive study of Levin (1993). Levin argues that syntactic frames are a direct
reflection of the underlying semantics; the sets of syntactic frames associated with a
particular Levin class reflect underlying semantic components that constrain allowable
arguments. On this principle, Levin defines verb classes based on the ability of
particular verbs to occur or not occur in pairs of syntactic frames that are in some
sense meaning-preserving (diathesis alternations). The classes also tend to share
some semantic component. For example, the break examples above are related by a
transitive/intransitive alternation called the causative/inchoative alternation. Break
and other verbs such as shatter and smash are also characterized by their ability to
appear in the middle construction, as in Glass breaks/shatters/smashes easily. Cut, a
similar change-of-state verb, seems to share in this syntactic behavior and can also
appear in the transitive (causative) as well as the middle construction: John cut the
bread, This loaf cuts easily. However, it cannot also occur in the simple intransitive: The
window broke/*The bread cut. In contrast, cut verbs can occur in the conative? John
valiantly cut/hacked at the frozen loaf, but his knife was too dull to make a dent in it?whereas
break verbs cannot: *John broke at the window. The explanation given is that cut describes
a series of actions directed at achieving the goal of separating some object into pieces.
These actions consist of grasping an instrument with a sharp edge such as a knife and
applying it in a cutting fashion to the object. It is possible for these actions to be
Palmer, Gildea, and Kingsbury The Proposition Bank
performed without the end result being achieved, but such that the cutting manner can
still be recognized, for example, John cut at the loaf. Where break is concerned, the only
thing specified is the resulting change of state, in which the object becomes separated
into pieces.
VerbNet (Kipper, Dang, and Palmer 2000; Kipper, Palmer, and Rambow 2002)
extends Levin?s classes by adding an abstract representation of the syntactic frames for
each class with explicit correspondences between syntactic positions and the semantic
roles they express, as in Agent REL Patient or Patient REL into pieces for break.2 (For other
extensions of Levin, see also Dorr and Jones [2000] and Korhonen, Krymolowsky, and
Marx [2003].) The original Levin classes constitute the first few levels in the hierarchy,
with each class subsequently refined to account for further semantic and syntactic
differences within a class. The argument list consists of thematic labels from a set of 20
such possible labels (Agent, Patient, Theme, Experiencer, etc.). The syntactic frames
represent a mapping of the list of schematic labels to deep-syntactic arguments.
Additional semantic information for the verbs is expressed as a set (i.e., conjunction) of
semantic predicates, such as motion, contact, transfer_info. Currently, all Levin verb
classes have been assigned thematic labels and syntactic frames, and over half the
classes are completely described, including their semantic predicates. In many cases,
the additional information that VerbNet provides for each class has caused it to
subdivide, or use intersections of, Levin?s original classes, adding an additional level
to the hierarchy (Dang et al 1998). We are also extending the coverage by adding new
classes (Korhonen and Briscoe 2004).
Our objective with the Proposition Bank is not a theoretical account of how and
why syntactic alternation takes place, but rather to provide a useful level of repre-
sentation and a corpus of annotated data to enable empirical study of these issues. We
have referred to Levin?s classes wherever possible to ensure that verbs in the same
classes are given consistent role labels. However, there is only a 50% overlap between
verbs in VerbNet and those in the Penn TreeBank II, and PropBank itself does not
define a set of classes, nor does it attempt to formalize the semantics of the roles it
defines.
While lexical resources such as Levin?s classes and VerbNet provide information
about alternation patterns and their semantics, the frequency of these alternations and
their effect on language understanding systems has never been carefully quantified.
While learning syntactic subcategorization frames from corpora has been shown to be
possible with reasonable accuracy (Manning 1993; Brent 1993; Briscoe and Carroll
1997), this work does not address the semantic roles associated with the syntactic
arguments. More recent work has attempted to group verbs into classes based on
alternations, usually taking Levin?s classes as a gold standard (McCarthy 2000; Merlo
and Stevenson 2001; Schulte im Walde 2000; Schulte im Walde and Brew 2002). But
without an annotated corpus of semantic roles, this line of research has not been able
to measure the frequency of alternations directly, or more generally, to ascertain how
well the classes defined by Levin correspond to real-world data.
We believe that a shallow labeled dependency structure provides a feasible level of
annotation which, coupled with minimal coreference links, could provide the
foundation for a major advance in our ability to extract salient relationships from
text. This will in turn improve the performance of basic parsing and generation
74
2 These can be thought of as a notational variant of tree-adjoining grammar elementary trees or tree-
adjoining grammar partial derivations (Kipper, Dang, and Palmer 2000).
Computational Linguistics Volume 31, Number 1
75
components, as well as facilitate advances in text understanding, machine translation,
and fact retrieval.
3. Annotation Scheme: Choosing the Set of Semantic Roles
Because of the difficulty of defining a universal set of semantic or thematic roles
covering all types of predicates, PropBank defines semantic roles on a verb-by-verb
basis. An individual verb?s semantic arguments are numbered, beginning with zero.
For a particular verb, Arg0 is generally the argument exhibiting features of a Pro-
totypical Agent (Dowty 1991), while Arg1 is a Prototypical Patient or Theme. No
consistent generalizations can be made across verbs for the higher-numbered
arguments, though an effort has been made to consistently define roles across mem-
bers of VerbNet classes. In addition to verb-specific numbered roles, PropBank defines
several more general roles that can apply to any verb. The remainder of this section
describes in detail the criteria used in assigning both types of roles.
As examples of verb-specific numbered roles, we give entries for the verbs accept
and kick below. These examples are taken from the guidelines presented to the
annotators and are also available on the Web at http://www.cis.upenn.edu/? cotton/
cgi-bin/pblex_fmt.cgi.
(12) Frameset accept.01 ??take willingly??
Arg0: Acceptor
Arg1: Thing accepted
Arg2: Accepted-from
Arg3: Attribute
Ex:[
Arg0
He] [
ArgM-MOD
would][
ArgM-NEG
n?t] accept [
Arg1
anything of value]
[
Arg2
from those he was writing about]. (wsj_0186)
(13) Frameset kick.01 ??drive or impel with the foot??
Arg0: Kicker
Arg1: Thing kicked
Arg2: Instrument (defaults to foot)
Ex1: [
ArgM-DIS
But] [
Arg0
two big New York banks
i
] seem [
Arg0
*trace*
i
]
to have kicked [
Arg1
those chances] [
ArgM-DIR
away], [
ArgM-TMP
for the
moment], [
Arg2
with the embarrassing failure of Citicorp and
Chase Manhattan Corp. to deliver $7.2 billion in bank financing
for a leveraged buy-out of United Airlines parent UAL Corp].
(wsj_1619)
Ex2: [
Arg0
John
i
] tried [
Arg0
*trace*
i
] to kick [
Arg1
the football], but Mary
pulled it away at the last moment.
A set of roles corresponding to a distinct usage of a verb is called a roleset and can
be associated with a set of syntactic frames indicating allowable syntactic variations in
the expression of that set of roles. The roleset with its associated frames is called a
Palmer, Gildea, and Kingsbury The Proposition Bank
frameset. A polysemous verb may have more than one frameset when the differences
in meaning are distinct enough to require a different set of roles, one for each
frameset. The tagging guidelines include a ??descriptor?? field for each role, such as
??kicker?? or ??instrument,?? which is intended for use during annotation and as
documentation but does not have any theoretical standing. In addition, each frameset
is complemented by a set of examples, which attempt to cover the range of syntactic
alternations afforded by that usage. The collection of frameset entries for a verb is
referred to as the verb?s frames file.
The use of numbered arguments and their mnemonic names was instituted for a
number of reasons. Foremost, the numbered arguments plot a middle course among
many different theoretical viewpoints.3 The numbered arguments can then be mapped
easily and consistently onto any theory of argument structure, such as traditional theta
role (Kipper, Palmer, and Rambow 2002), lexical-conceptual structure (Rambow et al
2003), or Prague tectogrammatics (Hajic?ova and Kuc?erova? 2002).
While most rolesets have two to four numbered roles, as many as six can appear,
in particular for certain verbs of motion:4
(14) Frameset edge.01 ??move slightly??
Arg0: causer of motion Arg3: start point
Arg1: thing in motion Arg4: end point
Arg2: distance moved Arg5: direction
Ex: [
Arg0
Revenue] edged [
Arg5
up] [
Arg2-EXT
3.4%] [
Arg4
to $904 million]
[
Arg3
from $874 million] [
ArgM-TMP
in last year?s third quarter]. (wsj_1210)
Because of the use of Arg0 for agency, there arose a small set of verbs in which an
external force could cause the Agent to execute the action in question. For example, in
the sentence . . . Mr. Dinkins would march his staff out of board meetings and into his private
office . . . (wsj_0765), the staff is unmistakably the marcher, the agentive role. Yet
Mr. Dinkins also has some degree of agency, since he is causing the staff to do the
marching. To capture this, a special tag, ArgA, is used for the agent of an induced
action. This ArgA tag is used only for verbs of volitional motion such as march and
walk, modern uses of volunteer (e.g., Mary volunteered John to clean the garage, or more
likely the passive of that, John was volunteered to clean the garage), and, with some
hesitation, graduate based on usages such as Penn only graduates 35% of its students.
(This usage does not occur as such in the Penn Treebank corpus, although it is evoked
in the sentence No student should be permitted to be graduated from elementary school
without having mastered the 3 R?s at the level that prevailed 20 years ago. (wsj_1286))
In addition to the semantic roles described in the rolesets, verbs can take any of a
set of general, adjunct-like arguments (ArgMs), distinguished by one of the function
tags shown in Table 1. Although they are not considered adjuncts, NEG for verb-level
negation (e.g., John didn?t eat his peas) and MOD for modal verbs (e.g., John would eat
76
3 By following the treebank, however, we are following a very loose government-binding framework.
4 We make no attempt to adhere to any linguistic distinction between arguments and adjuncts. While many
linguists would consider any argument higher than Agr2 or Agr3 to be an adjunct, such arguments occur
frequently enough with their respective verbs, or classes of verbs, that they are assigned a number in
order to ensure consistent annotation.
Computational Linguistics Volume 31, Number 1
77
everything else) are also included in this list to allow every constituent surrounding the
verb to be annotated. DIS is also not an adjunct but is included to ease future discourse
connective annotation.
3.1 Distinguishing Framesets
The criteria for distinguishing framesets are based on both semantics and syntax. Two
verb meanings are distinguished as different framesets if they take different numbers
of arguments. For example, the verb decline has two framesets:
(15) Frameset decline.01 ??go down incrementally??
Arg1: entity going down
Arg2: amount gone down by, EXT
Arg3: start point
Arg4: end point
Ex: . . . [
Arg1
its net income] declining [
Arg2-EXT
42%] [
Arg4
to $121 million]
[
ArgM-TMP
in the first 9 months of 1989]. (wsj_0067)
(16) Frameset decline.02 ??demure, reject??
Arg0: agent
Arg1: rejected thing
Ex: [
Arg0
A spokesman
i
] declined [
Arg1
*trace*
i
to elaborate] (wsj_0038)
However, alternations which preserve verb meanings, such as causative/inchoative or
object deletion, are considered to be one frameset only, as shown in the example (17).
Both the transitive and intransitive uses of the verb open correspond to the same
frameset, with some of the arguments left unspecified:
(17) Frameset open.01 ??cause to open??
Arg0: agent
Arg1: thing opened
Arg2: instrument
Ex1: [
Arg0
John] opened [
Arg1
the door]
Table 1
Subtypes of the ArgM modifier tag.
LOC: location CAU: cause
EXT: extent TMP: time
DIS: discourse connectives PNC: purpose
ADV: general purpose MNR: manner
NEG: negation marker DIR: direction
MOD: modal verb
Palmer, Gildea, and Kingsbury The Proposition Bank
Ex2: [
Arg1
The door] opened
Ex3: [
Arg0
John] opened [
Arg1
the door] [
Arg2
with his foot]
Moreover, differences in the syntactic type of the arguments do not constitute
criteria for distinguishing among framesets. For example, see.01 allows for either an NP
object or a clause object:
(18) Frameset see.01 ??view??
Arg0: viewer
Arg1: thing viewed
Ex1: [
Arg0
John] saw [
Arg1
the President]
Ex2: [
Arg0
John] saw [
Arg1
the President collapse]
Furthermore, verb-particle constructions are treated as separate from the
corresponding simplex verb, whether the meanings are approximately the same or
not. Example (19-21) presents three of the framesets for cut:
(19) Frameset cut.01 ??slice??
Arg0: cutter
Arg1: thing cut
Arg2: medium, source
Arg3: instrument
Ex: [
Arg0
Longer production runs] [
ArgM-MOD
would] cut [
Arg1
inefficiencies
from adjusting machinery between production cycles]. (wsj_0317)
(20) Frameset cut.04 ??cut off = slice??
Arg0: cutter
Arg1: thing cut (off)
Arg2: medium, source
Arg3: instrument
Ex: [
Arg0
The seed companies] cut off [
Arg1
the tassels of each plant].
(wsj_0209)
(21) Frameset cut.05 ??cut back = reduce??
Arg0: cutter
Arg1: thing reduced
Arg2: amount reduced by
78
Computational Linguistics Volume 31, Number 1
79
Arg3: start point
Arg4: end point
Ex: ??Whoa,?? thought John, ?[
Arg0
I
i
]?ve got [
Arg0
*trace*
i
] to start
[
Arg0
*trace*
i
] cutting back [
Arg1
my intake of chocolate].
Note that the verb and particle do not need to be contiguous; (20) above could just as
well be phrased The seed companies cut the tassels of each plant off.
For the WSJ text, there are frames for over 3,300 verbs, with a total of just over
4,500 framesets described, implying an average polysemy of 1.36. Of these verb frames,
only 21.6% (721/3342) have more than one frameset, while less than 100 verbs have
four or more. Each instance of a polysemous verb is marked as to which frameset it
belongs to, with interannotator (ITA) agreement of 94%. The framesets can be viewed
as extremely coarse-grained sense distinctions, with each frameset corresponding to
one or more of the Senseval 2 WordNet 1.7 verb groupings. Each grouping in turn
corresponds to several WordNet 1.7 senses (Palmer, Babko-Malaya, and Dang 2004).
3.2 Secondary Predications
There are two other functional tags which, unlike those listed above, can also be
associated with numbered arguments in the frames files. The first one, EXT (extent),
indicates that a constituent is a numerical argument on its verb, as in climbed 15%
or walked 3 miles. The second, PRD (secondary predication), marks a more subtle
relationship. If one thinks of the arguments of a verb as existing in a dependency tree,
all arguments depend directly on the verb. Each argument is basically independent of
the others. There are those verbs, however, which predict that there is a predicative
relationship between their arguments. A canonical example of this is call in the sense of
??attach a label to,?? as in Mary called John an idiot. In this case there is a relationship
between John and an idiot (at least in Mary?s mind). The PRD tag is associated with the
Arg2 label in the frames file for this frameset, since it is predictable that the Arg2
predicates on the Arg1 John. This helps to disambiguate the crucial difference between
the following two sentences:
predicative reading ditransitive reading
Mary called John a doctor. Mary called John a doctor.5
(LABEL) (SUMMON)
Arg0: Mary Arg0: Mary
Rel: called Rel: called
Arg1: John (item being labeled) Arg2: John (benefactive)
Arg2-PRD: a doctor (attribute) Arg1: a doctor (thing summoned)
It is also possible for ArgMs to predicate on another argument. Since this must be
decided on a case-by-case basis, the PRD function tag is added to the ArgM by the
annotator, as in example (28).
5 This sense could also be stated in the dative: Mary called a doctor for John.
Palmer, Gildea, and Kingsbury The Proposition Bank
3.3 Subsumed Arguments
Because verbs which share a VerbNet class are rarely synonyms, their shared argument
structure occasionally takes on odd characteristics. Of primary interest among these are
the cases in which an argument predicted by one member of a class cannot be attested
by another member of the same class. For a relatively simple example, consider the verb
hit, in VerbNet classes 18.1 and 18.4. This takes three very obvious arguments:
(22) Frameset hit ??strike??
Arg0: hitter
Arg1: thing hit, target
Arg2: instrument of hitting
Ex1: Agentive subject: ??[
Arg0
He
i
] digs in the sand instead of [
Arg0
*trace*
i
]
hitting [
Arg1
the ball], like a farmer,?? said Mr. Yoneyama. (wsj_1303)
Ex2: Instrumental subject: Dealers said [
Arg1
the shares] were hit [
Arg2
by
fears of a slowdown in the U.S. economy]. (wsj_1015)
Ex3: All arguments: [
Arg0
John] hit [
Arg1
the tree] [
Arg2
with a stick].6
VerbNet classes 18.1 and 18.4 are filled with verbs of hitting, such as beat, hammer,
kick, knock, strike, tap, and whack. For some of these the instrument of hitting is
necessarily included in the semantics of the verb itself. For example, kick is essentially
??hit with the foot?? and hammer is exactly ??hit with a hammer.?? For these verbs, then,
the Arg2 might not be available, depending on how strongly the instrument is
incorporated into the verb. Kick, for example, shows 28 instances in the treebank but
only one instance of a (somewhat marginal) instrument:
(23) [
ArgM-DIS
But] [
Arg0
two big New York banks] seem to have kicked [
Arg1
those
chances] [
ArgM-DIR
away], [
ArgM-TMP
for the moment], [
Arg2
with the embarrassing
failure of Citicorp and Chase Manhattan Corp. to deliver $7.2 billion in
bank financing for a leveraged buy-out of United Airlines parent UAL
Corp]. (wsj_1619)
Hammer shows several examples of Arg2s, but these are all metaphorical hammers:
(24) Despite the relatively strong economy, [
Arg1
junk bond prices
i
] did
nothing except go down, [
Arg1
*trace*
i
] hammered [
Arg2
by a seemingly
endless trail of bad news]. (wsj_2428)
Another perhaps more interesting case is that in which two arguments can be
merged into one in certain syntactic situations. Consider the case of meet, which
canonically takes two arguments:
(25) Frameset meet ??come together??
Arg0: one party
80
6 The Wall Street Journal corpus contains no examples with both an agent and an instrument.
Computational Linguistics Volume 31, Number 1
81
Arg1: the other party
Ex: [
Arg0
Argentine negotiator Carlos Carballo] [
ArgM-MOD
will] meet
[
Arg1
with banks this week]. (wsj_0021)
It is perfectly possible, of course, to mention both meeting parties in the same
constituent:
(26) [
Arg0
The economic and foreign ministers of 12 Asian and Pacific
nations] [
ArgM-MOD
will] meet [
ArgM-LOC
in Australia] [
ArgM-TMP
next week]
[
ArgM-PRP
to discuss global trade as well as regional matters such as
transportation and telecommunications]. (wsj_0043)
In these cases there is an assumed or default Arg1 along the lines of ??each other??:
(27) [
Arg0
The economic and foreign ministers of 12 Asian and Pacific
nations] [
ArgM-MOD
will] meet [
Arg1-REC
(with) each other] . . .
Similarly, verbs of attachment (attach, tape, tie, etc.) can express the things being
attached as either one constituent or two:
(28) Frameset connect.01 ??attach??
Arg0: agent, entity causing two objects to be attached
Arg1: patient
Arg2: attached-to
Arg3: instrument
Ex1: The subsidiary also increased reserves by $140 million, however,
and set aside an additional $25 million for [
Arg1
claims] connected
[
Arg2
with Hurricane Hugo]. (wsj_1109)
Ex2: Machines using the 486 are expected to challenge higher-priced
work stations and minicomputers in applications such as [
Arg0
so-called
servers
i
], [
Arg0
which
i
] [
Arg0
*trace*
i
] connect [
Arg1
groups of computers]
[
ArgM-PRD
[together], and in computer-aided design. (wsj_0781)
3.4 Role Labels and Syntactic Trees
The Proposition Bank assigns semantic roles to nodes in the syntactic trees of the Penn
Treebank. Annotators are presented with the roleset descriptions and the syntactic tree
and mark the appropriate nodes in the tree with role labels. The lexical heads of
constituents are not explicitly marked either in the treebank trees or in the semantic
labeling layered on top of them. Annotators cannot change the syntactic parse, but
they are not otherwise restricted in assigning the labels. In certain cases, more than
one node may be assigned the same role. The annotation software does not require that
the nodes being assigned labels be in any syntactic relation to the verb. We discuss
the ways in which we handle the specifics of the treebank syntactic annotation style in
this section.
Palmer, Gildea, and Kingsbury The Proposition Bank
3.4.1 Prepositional Phrases. The treatment of prepositional phrases is complicated by
several factors. On one hand, if a given argument is defined as a ??destination,?? then in
a sentence such as John poured the water into the bottle, the destination of the water is
clearly the bottle, not ??into the bottle.?? The fact that the water is going into the bottle is
inherent in the description ??destination??; the preposition merely adds the specific
information that the water will end up inside the bottle. Thus arguments should
properly be associated with the NP heads of prepositional phrases. On the other hand,
however, ArgMs which are prepositional phrases are annotated at the PP level, not the
NP level. For the sake of consistency, then, numbered arguments are also tagged at the
PP level. This also facilitates the treatment of multiword prepositions such as out of,
according to, and up to but not including.7
(29) [
Arg1
Its net income] declining [
Arg2-EXT
42%] [to
Arg4
$121 million]
[
ArgM-TMP
in the first 9 months of 1989] (wsj_0067)
3.4.2 Traces and Control Verbs. The Penn Treebank contains empty categories known
as traces, which are often coindexed with other constituents in the tree. When a trace is
assigned a role label by an annotator, the coindexed constituent is automatically added
to the annotation, as in
(30) [
Arg0
John
i
] tried [
Arg0
*trace*
i
] to kick [
Arg1
the football], but Mary pulled
it away at the last moment.
Verbs such as cause, force, and persuade, known as object control verbs, pose a
problem for the analysis and annotation of semantic structure. Consider a sentence
such as Commonwealth Edison said the ruling could force it to slash its 1989 earnings by
$1.55 a share. (wsj_0015). The Penn Treebank?s analysis assigns a single sentential (S)
constituent to the entire string it to slash . . . a share, making it a single syntactic
argument to the verb force. In the PropBank annotation, we split the sentential
complement into two semantic roles for the verb force, assigning roles to the noun
phrase and verb phrase but not to the S node which subsumes them:
(31) Frameset cause, force, persuade, etc. ??impelled action??
Arg0: agent
Arg1: impelled agent
Arg2: impelled action
Ex: Commonwealth Edison said [
Arg0
the ruling] [
ArgM-MOD
could] force
[
Arg1
it] [
Arg2-PRD
to slash its 1989 earnings by $1.55 a share]. (wsj_0015)
In such a sentence, the object of the control verb will also be assigned a semantic role
by the subordinate clause?s verb:
(32) Commonwealth Edison said the ruling could force [
Arg0
it] to slash
[
Arg1
its 1989 earnings] by [
Arg2-by
$1.55 a share]. (wsj_0015)
82
7 Note that out of is exactly parallel to into, but one is spelled with a space in the middle and the other isn?t.
Computational Linguistics Volume 31, Number 1
83
While it is the Arg0 of force, it is the Arg1 of slash. Similarly, subject control verbs such as
promise result in the subject of the main clause being assigned two roles, one for each verb:
(33) [
Arg0
Mr. Bush?s legislative package] promises [
Arg2
to cut emissions by
10 million tons?basically in half?by the year 2000]. (wsj_0146)
(34) [
Arg0
Mr. Bush?s legislative package
i
] promises [
Arg0
*trace*
i
] to cut
[
Arg1
emissions] [
Arg2
by 10 million tons?basically in half?]
[
ARGM-TMP
by the year 2000].
We did not find a single case of a subject control verb used with a direct object and an
infinitival clause (e.g., John promised Mary to come) in the Penn Treebank.
The cases above must be contrasted with verbs such as expect, often referred as
exceptional case marking (ECM) verbs, where an infinitival subordinate clause is a
single semantic argument:
(35) Frameset expect ??look forward to, anticipate??
Arg0: expector
Arg1: anticipated event
Ex: Mr. Leinonen said [
Arg0
he] expects [
Arg1
Ford to meet the deadline
easily]. (wsj_0064)
While Ford is given a semantic role for the verb meet, it is not given a role for expect.
3.4.3 Split Constituents. Most verbs of saying (say, tell, ask, report, etc.) have the
property that the verb and its subject can be inserted almost anywhere within another
of the verb?s arguments. While the canonical realization is John said (that) Mary was
going to eat outside at lunchtime today, it is common to say Mary, John said, was going to eat
outside at lunchtime today or Mary was going to eat outside, John said, at lunchtime today. In
this situation, there is no constituent holding the whole of the utterance while not also
holding the verb of saying. We annotate these cases by allowing a single semantic role
to point to the component pieces of the split constituent in order to cover the correct,
discontinuous substring of the sentence.
(36) Frameset say
Arg0: speaker
Arg1: utterance
Arg2: listener
Ex: [
Arg1
By addressing those problems], [
Arg0
Mr. Maxwell] said,
[
Arg1
the new funds have become ??extremely attractive to Japanese
and other investors outside the U.S.??] (wsj_0029)
In the flat structure we have been using for example sentences, this looks like a case of
repeated role labels. Internally, however, there is one role label pointing to multiple
constituents of the tree, shown in Figure 1.
Palmer, Gildea, and Kingsbury The Proposition Bank
4. The Propbank Development Process
Since the Proposition Bank consists of two portions, the lexicon of frames files and the
annotated corpus, the process is similarly divided into framing and annotation.
4.1 Framing
The process of creating the frames files, that is, the collection of framesets for each
lexeme, begins with the examination of a sample of the sentences from the corpus
containing the verb under consideration. These instances are grouped into one or more
major senses, and each major sense is turned into a single frameset. To show all the
possible syntactic realizations of the frameset, many sentences from the corpus are
included in the frames file, in the same format as the examples above. In many cases a
particular realization will not be attested within the Penn Treebank corpus; in these
cases, a constructed sentence is used, usually identified by the presence of the
characters of John and Mary. Care was taken during the framing process to make
synonymous verbs (mostly in the sense of ??sharing a VerbNet Class??) have the same
framing, with the same number of roles and the same descriptors on those roles.
Generally speaking, a given lexeme/sense pair required 10?15 minutes to frame,
although highly polysemous verbs could require longer. With the 4,500+ framesets
currently in place for PropBank, this is clearly a substantial time investment, and the
frames files represent an important resource in their own right. We were able to use
membership in a VerbNet class which already had consistent framing to project
accurate frames files for up to 300 verbs. If the overlap between VerbNet and
PropBank had been more than 50%, this number might have been higher.
4.2 Annotation
We begin the annotation process by running a rule-based argument tagger (Palmer,
Rosenzweig, and Cotton 2001) on the corpus. This tagger incorporates an extensive
lexicon, entirely separate from that used by PropBank, which encodes class-based
84
Figure 1
Split constituents: In this case, a single semantic role label points to multiple nodes in the original
treebank tree.
Computational Linguistics Volume 31, Number 1
85
mappings between grammatical and semantic roles. The rule-based tagger achieved
83% accuracy on pilot data, with many of the errors due to differing assumptions
made in defining the roles for a particular verb. The output of this tagger is then
corrected by hand. Annotators are presented with an interface which gives them access
to both the frameset descriptions and the full syntactic parse of any sentence from the
treebank and allows them to select nodes in the parse tree for labeling as arguments of
the predicate selected. For any verb they are able to examine both the descriptions of
the arguments and the example tagged sentences, much as they have been presented
here. The tagging is done on a verb-by-verb basis, known as lexical sampling, rather
than all-words annotation of running text.
The downside of this approach is that it does not quickly provide a stretch of fully
annotated text, needed for early assessment of the usefulness of the resource (see
subsequent sections). For this reason a domain-specific subcorpus was automatically
extracted from the entirety of the treebank, consisting of texts roughly primarily
concerned with financial reporting and identified by the presence of a dollar sign
anywhere in the text. This ??financial?? subcorpus comprised approximately one-third
of the treebank and served as the initial focus of annotation.
The treebank as a whole contains 3,185 unique verb lemmas, while the financial
subcorpus contains 1,826. These verbs are arrayed in a classic Zipfian distribution,
with a few verbs occurring very often (say, for example, is the most common verb, with
over 10,000 instances in its various inflectional forms) and most verbs occurring two or
fewer times. As with the distribution of the lexical items themselves, the framesets also
display a Zipfian distribution: A small number of verbs have many framesets ( go has
20 when including phrasal variants, and come, get, make, pass, take, and turn each have
more than a dozen) while the majority of verbs (2581/3342) have only one frameset.
For polysemous verbs annotators had to determine which frameset was appropriate
for a given usage in order to assign the correct argument structure, although this
information was explicitly marked only during a separate pass.
Annotations were stored in a stand-off notation, referring to nodes within the Penn
Treebank without actually replicating any of the lexical material or structure of that
corpus. The process of annotation was a two-pass, blind procedure followed by an
adjudication phase to resolve differences between the two initial passes. Both role
labeling decisions and the choice of frameset were adjudicated.
The annotators themselves were drawn from a variety of backgrounds, from
undergraduates to holders of doctorates, including linguists, computer scientists, and
others. Undergraduates have the advantage of being inexpensive but tend to work for
only a few months each, so they require frequent training. Linguists make the best
overall judgments although several of our nonlinguist annotators also had excellent
skills. The learning curve for the annotation task tended to be very steep, with most
annotators becoming comfortable with the process within three days of work. This
contrasts favorably with syntactic annotation, which has a much longer learning curve
(Marcus, personal communication), and indicates one of the advantages of using
a corpus already syntactically parsed as the basis of semantic annotation. Over
30 annotators contributed to the project, some for just a few weeks, some for up to
three years. The framesets were created and annotation disagreements were adju-
dicated by a small team of highly trained linguists: Paul Kingsbury created the frames
files and managed the annotators, and Olga Babko-Malaya checked the frames files for
consistency and did the bulk of the adjudication.
We measured agreement between the two annotations before the adjudication step
using the kappa statistic (Siegel and Castellan 1988), which is defined with respect to
Palmer, Gildea, and Kingsbury The Proposition Bank
the probability of interannotator agreement, P?A?, and the agreement expected by
chance, P?E?:
k ? P?A?  P?E?
1  P?E?
Measuring interannotator agreement for PropBank is complicated by the large num-
ber of possible annotations for each verb. For role identification, we expect agree-
ment between annotators to be much higher than chance, because while any node in
the parse tree can be annotated, the vast majority of arguments are chosen from the
small number of nodes near the verb. In order to isolate the role classification decisions
from this effect and avoid artifically inflating the kappa score, we split role
identification (role vs. nonrole) from role classification (Arg0 vs. Arg1 vs. . . .) and
calculate kappa for each decision separately. Thus, for the role identification kappa,
the interannotator agreement probability P?A? is the number of node observation
agreements divided by the total number of nodes considered, which is the number of
nodes in each parse tree multiplied by the number of predicates annotated in the
sentence. All the PropBank data were annotated by two people, and in calculating
kappa we compare these two annotations, ignoring the specific identities of the
annotators for the predicate (in practice, agreement varied with the training and skill
of individual annotators). For the role classification kappa, we consider only nodes
that were marked as arguments by both annotators and compute kappa over the
choices of possible argument labels. For both role identification and role classification,
we compute kappa for two ways of treating ArgM labels. The first is to treat ArgM
labels as arguments like any other, in which case ArgM-TMP, ArgM-LOC, and so on
are considered separate labels for the role classification kappa. In the second scenario,
we ignore ArgM labels, treating them as unlabeled nodes, and calculate agreement for
identification and classification of numbered arguments only.
Kappa statistics for these various decisions are shown in Table 2. Agreement
on role identification is very high (.99 under both treatments of ArgM), given the large
number of obviously irrelevant nodes. Reassuringly, kappas for the more difficult
role classification task are also high: .93 including all types of ArgM and .96 con-
sidering only numbered arguments. Kappas on the combined identification and
classication decision, calculated over all nodes in the tree, are .91 including all sub-
types of ArgM and .93 over numbered arguments only. Interannotator agreement
among nodes that either annotator identified as an argument was .84, including ArgMs
and .87, excluding ArgMs.
Discrepancies between annotators tended to be less on numbered arguments than
on the selection of function tags, as shown in the confusion matrices of Tables 3 and 4.
86
Table 2
Interannotator agreement.
P?A? P?E? k
Including ArgM Role identification .99 .89 .93
Role classification .95 .27 .93
Combined decision .99 .88 .91
Excluding ArgM Role identification .99 .91 .94
Role classification .98 .41 .96
Combined decision .99 .91 .93
Computational Linguistics Volume 31, Number 1
87
Certain types of functions, particularly those represented by the tags ADV, MNR, and
DIS, can be difficult to distinguish. For example, in the sentence Also, substantially lower
Dutch corporate tax rates helped the company keep its tax outlay flat relative to earnings
growth (wsj_0132), the phrase relative to earnings growth could be interpreted as a
manner adverbial (MNR), describing how the tax outlays were kept flat, or as a
general-purpose adverbial (ADV), merely providing more information on the keeping
event. Similarly, a word such as then can have several functions. It is canonically a
temporal adverb marking time or a sequence of events (. . . the Senate then broadened the
list further . . . (wsj_0101)) but can also mark a consequence of another action (. . . if for
any reason I don?t have the values, then I won?t recommend it. (wsj_0331)) or simply serve as
a placeholder in conversation (It?s possible then that Santa Fe?s real estate . . . could one day
fetch a king?s ransom (wsj_0331)). These three usages require three different taggings
(TMP, ADV, and DIS, respectively) and can easily trip up an annotator.
The financial subcorpus was completely annotated and given a preadjudication
release in June 2002. The fully annotated and adjudicated corpus was completed in
March 2004. Both of these are available through the Linguistic Data Consortium,
although because of the use of the stand-off notation, prior possession of the treebank
is also necessary. The frames files are distributed separately and are available through
the project Web site at http://www.cis.upenn.edu/?ace/.
Table 3
Confusion matrix for argument labels, with ArgM labels collapsed into one category. Entries are
a fraction of total annotations; true zeros are omitted, while other entries are rounded to zero.
Arg0 Arg1 Arg2 Arg3 Arg4 ArgM
Arg0 0.288 0.006 0.001 0.000 0.000
Arg1 0.364 0.006 0.001 0.000 0.002
Arg2 0.074 0.001 0.001 0.003
Arg3 0.013 0.000 0.001
Arg4 0.011 0.000
ArgM 0.228
Table 4
Confusion matrix among subtypes of ArgM, defined in Table 1. Entries are fraction of all ArgM
labels. Entries are a fraction of all ArgM labels; true zeros are omitted, while other entries are
rounded to zero.
ADV CAU DIR DIS EXT LOC MNR MOD NEG PNC TMP
ADV 0.087 0.003 0.001 0.017 0.001 0.004 0.016 0.001 0.000 0.003 0.007
CAU 0.018 0.000 0.000 0.001 0.001 0.002 0.002
DIR 0.014 0.000 0.001 0.001 0.000
DIS 0.055 0.000 0.000 0.002 0.000 0.000 0.000 0.005
EXT 0.007 0.000 0.001 0.000 0.000
LOC 0.106 0.006 0.000 0.000 0.000 0.003
MNR 0.085 0.000 0.000 0.001 0.002
MOD 0.161 0.000 0.000
NEG 0.061 0.001
PNC 0.026 0.000
TMP 0.286
Palmer, Gildea, and Kingsbury The Proposition Bank
5. FrameNet and PropBank
The PropBank project and the FrameNet project at the International Computer Science
Institute (Baker, Fillmore, and Lowe 1998) share the goal of documenting the syntactic
realization of arguments of the predicates of the general English lexicon by annotating
a corpus with semantic roles. Despite the two projects? similarities, their methodol-
ogies are quite different. FrameNet is focused on semantic frames,8 which are defined
as a schematic representation of situations involving various participants, props, and
other conceptual roles (Fillmore 1976). The project methodology has proceeded on a
frame-by-frame basis, that is, by first choosing a semantic frame (e.g., Commerce),
defining the frame and its participants or frame elements (BUYER, GOODS, SELLER,
MONEY), listing the various lexical predicates which invoke the frame (buy, sell, etc.),
and then finding example sentences of each predicate in a corpus (the British National
Corpus was used) and annotating each frame element in each sentence. The example
sentences were chosen primarily to ensure coverage of all the syntactic realizations of
the frame elements, and simple examples of these realizations were preferred over
those involving complex syntactic structure not immediately relevant to the lexical
predicate itself. Only sentences in which the lexical predicate was used ??in frame??
were annotated. A word with multiple distinct senses would generally be analyzed as
belonging to different frames in each sense but may only be found in the FrameNet
corpus in the sense for which a frame has been defined. It is interesting to note that the
semantic frames are a helpful way of generalizing between predicates; words in the
same frame have been found frequently to share the same syntactic argument
structure (Gildea and Jurafsky 2002). A more complete description of the FrameNet
project can be found in Baker, Fillmore, and Lowe (1998) and Johnson et al (2002), and
the ramifications for automatic classification are discussed more thoroughly in Gildea
and Jurafsky (2002).
In contrast with FrameNet, PropBank is aimed at providing data for training
statistical systems and has to provide an annotation for every clause in the Penn
Treebank, no matter how complex or unexpected. Similarly to FrameNet, PropBank
also attempts to label semantically related verbs consistently, relying primarily on
VerbNet classes for determining semantic relatedness. However, there is much less
emphasis on the definition of the semantics of the class that the verbs are associated
with, although for the relevant verbs additional semantic information is provided
through the mapping to VerbNet. The PropBank semantic roles for a given VerbNet
class may not correspond to the semantic elements highlighted by a particular
FrameNet frame, as shown by the examples of Table 5. In this case, FrameNet?s
COMMERCE frame includes roles for Buyer (the receiver of the goods) and Seller (the
receiver of the money) and assigns these roles consistently to two sentences describing
the same event:
FrameNet annotation:
(37) [
Buyer
Chuck] bought [
Goods
a car] [
Seller
from Jerry] [
Payment
for $1000].
(38) [
Seller
Jerry] sold [
Goods
a car] [
Buyer
to Chuck] [
Payment
for $1000].
88
8 The authors apologize for the ambiguity between PropBank?s ??syntactic frames?? and Framenet?s
??semantic frames.?? Syntactic frames refer to syntactic realizations. Semantic frames will appear herein in
boldface.
Computational Linguistics Volume 31, Number 1
89
PropBank annotation:
(39) [
Arg0
Chuck] bought [
Arg1
a car] [
Arg2
from Jerry] [
Arg3
for $1000].
(40) [
Arg0
Jerry] sold [
Arg1
a car] [
Arg2
to Chuck] [
Arg3
for $1000].
PropBank requires an additional level of inference to determine who has possession of
the car in both cases. However, FrameNet does not indicate that the subject in both
sentences is an Agent, represented in PropBank by labeling both subjects as Arg0.9
Note that the subject is not necessarily an agent, as in, for instance, the passive
construction:
FrameNet annotation:
(41) [
Goods
A car] was bought [
Buyer
by Chuck].
(42) [
Goods
A car] was sold [
Buyer
to Chuck] [
Seller
by Jerry].
(43) [
Buyer
Chuck] was sold [
Goods
a car] [
Seller
by Jerry].
PropBank annotation:
(44) [
Arg1
A car] was bought [
Arg0
by Chuck].
(45) [
Arg1
A car] was sold [
Arg2
to Chuck] [
Arg0
by Jerry].
(46) [
Arg2
Chuck] was sold [
Arg1
a car] [
Arg0
by Jerry].
To date, PropBank has addressed only verbs, whereas FrameNet includes nouns
and adjectives.10 PropBank annotation also differs in that it takes place with reference
to the Penn Treebank trees; not only are annotators shown the trees when analyzing
a sentence, they are constrained to assign the semantic labels to portions of the
sentence corresponding to nodes in the tree. Parse trees are not used in FrameNet;
annotators mark the beginning and end points of frame elements in the text and add
Table 5
Comparison of frames.
PropBank FrameNet
buy sell COMMERCE
Arg0: buyer Arg0: seller Buyer
Arg1: thing bought Arg1: thing sold Seller
Arg2: seller Arg2: buyer Payment
Arg3: price paid Arg3: price paid Goods
Arg4: benefactive Arg4: benefactive Rate/Unit
9 FrameNet plans ultimately to represent agency is such examples using multiple inheritance of frames
(Fillmore and Atkins 1998; Fillmore and Baker 2001).
10 New York University is currently in the process of annotating nominalizations in the Penn Treebank
using the PropBank frames files and annotation interface, creating a resource to be known as NomBank.
Palmer, Gildea, and Kingsbury The Proposition Bank
a grammatical function tag expressing the frame element?s syntactic relation to the
predicate.
6. A Quantitative Analysis of the Semantic-Role Labels
The stated aim of PropBank is the training of statistical systems. It also provides a rich
resource for a distributional analysis of semantic features of language that have
hitherto been somewhat inaccessible. We begin this section with an overview of
general characteristics of the syntactic realization of the different semantic-role labels
and then attempt to measure the frequency of syntactic alternations with respect to
verb class membership. We base this analysis on previous work by Merlo and
Stevenson (2001). In the following section we discuss the performance of a system
trained to automatically assign the semantic-role labels.
6.1 Associating Role Labels with Specific Syntactic Constructions
We begin by simply counting the frequency of occurrence of roles in specific syntactic
positions. In all the statistics given in this section, we do not consider past- or present-
participle uses of the predicates, thus excluding any passive-voice sentences. The
syntactic positions used are based on a few heuristic rules: Any NP under an S node in
the treebank is considered a syntactic subject, and any NP under a VP is considered an
object. In all other cases, we use the syntactic category of the argument?s node in the
treebank tree: for example, SBAR for sentential complements and PP for prepositional
phrases. For prepositional phrases, as well as for noun phrases that are the object of
a preposition, we include the preposition as part of our syntactic role: for example,
PP-in, PP-with. Table 6 shows the most frequent semantic roles associated with var-
ious syntactic positions, while Table 7 shows the most frequent syntactic positions for
various roles.
Tables 6 and 7 show overall statistics for the corpus, and some caution is needed in
interpreting the results, as the semantic-role labels are defined on a per-frameset basis
and do not necessarily have corpus-wide definitions. Nonetheless, a number of trends
are apparent. Arg0, when present, is almost always a syntactic subject, while the
subject is Arg0 only 79% of the time. This provides evidence for the notion of a
thematic hierarchy in which the highest-ranking role present in a sentence is given the
90
Table 6
Most frequent semantic roles for each syntactic position.
Position Total Four most common roles (%) Other
roles (%)
Sub 37,364 Arg0 79.0 Arg1 16.8 Arg2 2.4 TMP 1.2 0.6
Obj 21,610 Arg1 84.0 Arg2 9.8 TMP 4.6 Arg3 0.8 0.8
S 10,110 Arg1 76.0 ADV 8.5 Arg2 7.5 PRP 2.4 5.5
NP 7,755 Arg2 34.3 Arg1 23.6 Arg4 18.9 Arg3 12.9 10.4
ADVP 5,920 TMP 30.3 MNR 22.2 DIS 19.8 ADV 10.3 17.4
MD 4,167 MOD 97.4 ArgM 2.3 Arg1 0.2 MNR 0.0 0.0
PP-in 3,134 LOC 46.6 TMP 35.3 MNR 4.6 DIS 3.4 10.1
SBAR 2,671 ADV 36.0 TMP 30.4 Arg1 16.8 PRP 7.6 9.2
RB 1,320 NEG 91.4 ArgM 3.3 DIS 1.6 DIR 1.4 2.3
PP-at 824 EXT 34.7 LOC 27.4 TMP 23.2 MNR 6.1 8.6
Computational Linguistics Volume 31, Number 1
91
honor of subjecthood. Going from syntactic position to semantic role, the numbered
arguments are more predictable than the non-predicate-specific adjunct roles. The two
exceptions are the roles of ??modal?? (MOD) and ??negative?? (NEG), which as previously
discussed are not syntactic adjuncts at all but were simply marked as ArgMs as the
best means of tracking their important semantic contributions. They are almost always
realized as auxiliary verbs and the single adverb (part-of-speech tag RB) not,
respectively.
6.2 Associating Verb Classes with Specific Syntactic Constructions
Turning to the behavior of individual verbs in the PropBank data, it is interesting to
see how much correspondence there is between verb classes proposed in the literature
Table 7
Most frequent syntactic positions for each semantic role.
Roles Total Four most common syntactic positions (%) Other
positions
(%)
Arg1 35,112 Obj 51.7 S 21.9 Subj 17.9 NP 5.2 3.4
Arg0 30,459 Subj 96.9 NP 2.4 S 0.2 Obj 0.2 0.2
Arg2 7,433 NP 35.7 Obj 28.6 Subj 12.1 S 10.2 13.4
TMP 6,846 ADVP 26.2 PP-in 16.2 Obj 14.6 SBAR 11.9 31.1
MOD 4,102 MD 98.9 ADVP 0.8 NN 0.1 RB 0.0 0.1
ADV 3,137 SBAR 30.6 S 27.4 ADVP 19.4 PP-in 3.1 19.5
LOC 2,469 PP-in 59.1 PP-on 10.0 PP-at 9.2 ADVP 6.4 15.4
MNR 2,429 ADVP 54.2 PP-by 9.6 PP-with 7.8 PP-in 5.9 22.5
Arg3 1,762 NP 56.7 Obj 9.7 Subj 8.9 ADJP 7.8 16.9
DIS 1,689 ADVP 69.3 CC 10.6 PP-in 6.2 PP-for 5.4 8.5
Table 8
Semantic roles of verbs? subjects, for the verb classes of Merlo and Stevenson (2001).
Relative frequency of semantic roleVerb Count
Arg0 Arg1 Arg2 ArgA TMP
Unergative
float 14 35.7 64.3
hurry 2 100.0
jump 125 97.6 2.4
leap 11 90.9 9.1
march 8 87.5 12.5
race 4 75.0 25.0
rush 31 6.5 90.3 3.2
vault 1 100.0
wander 3 100.0
glide 1 100.0
hop 34 97.1 2.9
jog 1 100.0
scoot 1 100.0
scurry 2 100.0
skip 5 100.0
tiptoe 2 100.0
Palmer, Gildea, and Kingsbury The Proposition Bank
and the annotations in the corpus. Table 8 shows the PropBank semantic role labels for
the subjects of each verb in each class. Merlo and Stevenson (2001) aim to
automatically classify verbs into one of three categories: unergative, unaccusative,
and object-drop. These three categories, more coarse-grained than the classes of Levin
or VerbNet, are defined by the semantic roles they assign to a verb?s subjects and
objects in both transitive and intransitive sentences, as illustrated by the following
examples:
Unergative: [
Causal Agent
The jockey] raced [
Agent
the horse] past the barn.
[
Agent
The horse] raced past the barn.
92
Table 8
(cont.)
Relative frequency of semantic roleVerb Count
Arg0 Arg1 Arg2 ArgA TMP
Unaccusative
boil 1 100.0
dissolve 4 75.0 25.0
explode 7 100.0
flood 5 80.0 20.0
fracture 1 100.0
melt 4 25.0 50.0 25.0
open 80 72.5 21.2 2.5 3.8
solidify 6 83.3 16.7
collapse 36 94.4 5.6
cool 9 66.7 33.3
widen 29 27.6 72.4
change 148 65.5 33.8 0.7
clear 14 78.6 21.4
divide 1 100.0
simmer 5 100.0
stabilize 33 45.5 54.5
Object-Drop
dance 2 100.0
kick 5 80.0 20.0
knit 1 100.0
paint 4 100.0
play 67 91.0 9.0
reap 10 100.0
wash 4 100.0
yell 5 100.0
borrow 36 100.0
inherit 6 100.0
organize 11 100.0
sketch 1 100.0
clean 4 100.0
pack 7 100.0
study 40 100.0
swallow 5 80.0 20.0
call 199 97.0 1.5 1.0 0.5
Computational Linguistics Volume 31, Number 1
93
Unaccusative: [
Causal Agent
The cook] melted [
Theme
the butter] in the pan.
[
Theme
The butter] melted in the pan.
Object-Drop: [
Agent
The boy] played [
Theme
soccer].
[
Agent
The boy] played.
6.2.1 Predictions. In our data, the closest analogs to Merlo and Stevenson?s three roles
of Causal Agent, Agent, and Theme are ArgA, Arg0, and Arg1, respectively. We
hypothesize that PropBank data will confirm
1. that the subject can take one of two roles (Arg0 or Arg1) for
unaccusative and unergative verbs but only one role (Arg0) for
object-drop verbs;
2. that Arg1s appear more frequently as subjects for intransitive
unaccusatives than they do for intransitive unergatives.
In Table 8 we show counts for the semantic roles of the subjects of the Merlo
and Stevenson verbs which appear in PropBank (80%), regardless of transitivity, in
order to measure whether the data in fact reflect the alternations between syntactic and
semantic roles that the verb classes predict. For each verb, we show counts only for
occurrences tagged as belonging to the first frameset, reflecting the predominant or
unmarked sense.
6.2.2 Results of Prediction 1. The object-drop verbs of Merlo and Stevenson do in fact
show little variability in our corpus, with the subject almost always being Arg0. The
unergative and unaccusative verbs show much more variability in the roles that can
appear in the subject position, as predicted, although some individual verbs always
have Arg0 as subject, presumably as a result of the small number of occurrences.
6.2.3 Results of Prediction 2. As predicted, there is in general a greater preponderance
of Arg1 subjects for unaccusatives than for unergatives, with the striking exception of a
few unergative verbs, such as jump and rush, whose subjects are almost always Arg1.
Jump is being affected by the predominance of a financial-subcorpus sense used for
stock reportage (79 out of 82 sentences), which takes jump as rise dramatically: Jaguar
shares jumped 23 before easing to close at 654, up 6. (wsj_1957) Rush is being affected by a
framing decision, currently being reconsidered, wherein rush was taken to mean cause
to move quickly. Thus the entity in motion is tagged Arg1, as in Congress in Congress would
have rushed to pass a private relief bill. (wsj_0946) The distinction between unergatives
and unaccusatives is not apparent from the PropBank data in this table, since we are
not distinguishing between transitives and intransitives, which is left for future
experiments.
In most cases, the first frameset (numbered 1 in the PropBank frames files) is the
most common, but in a few cases this is not the case because of the domain of the text.
For example, the second frameset for kick, corresponding to the phrasal usage kick in,
meaning begin, accounted for seven instances versus the five instances for frameset 1.
Palmer, Gildea, and Kingsbury The Proposition Bank
The phrasal frameset has a very different pattern, with the subject always corres-
ponding to Arg1, as in
(47) [
Arg1
Several of those post-crash changes] kicked in [
ArgM-TMP
during
Friday?s one-hour collapse] and worked as expected, even though
they didn?t prevent a stunning plunge. (wsj_2417)
Statistics for all framesets of kick are shown in Table 9; the first row in Table 9
corresponds to the entry for kick in the ??Object-Drop?? section of Table 8.
Overall, these results support our hypotheses and also highlight the important
role played by even the relatively coarse-grained sense tagging exemplified by the
framesets.
7. Automatic Determination of Semantic-Role Labels
The stated goal of the PropBank is to provide training data for supervised automatic
role labelers, and the project description cannot be considered complete without a
discussion of PropBank?s suitability for this purpose. One of PropBank?s important
features as a practical resource is that the sentences chosen for annotation are from the
same Wall Street Journal corpus used for the original Penn Treebank project, and thus
hand-checked syntactic parse trees are available for the entire data set. In this section,
we examine the importance of syntactic information for semantic-role labeling by
comparing the performance of a system based on gold-standard parses with one using
automatically generated parser output. We then examine whether it is possible that the
additional information contained in a full parse tree is negated by the errors present in
automatic parser output, by testing a role-labeling system based on a flat or ??chunked??
representation of the input.
Gildea and Jurafsky (2002) describe a statistical system trained on the data from
the FrameNet project to automatically assign semantic roles. The system first passed
sentences through an automatic parser (Collins 1999), extracted syntactic features from
the parses, and estimated probabilities for semantic roles from the syntactic and lexical
features. Both training and test sentences were automatically parsed, as no hand-
annotated parse trees were available for the corpus. While the errors introduced by the
parser no doubt negatively affected the results obtained, there was no direct way of
quantifying this effect. One of the systems evaluated for the Message Understanding
Conference task (Miller et al 1998) made use of an integrated syntactic and semantic
model producing a full parse tree and achieved results comparable to other systems
that did not make use of a complete parse. As in the FrameNet case, the parser was not
94
Table 9
Semantic roles for different frame sets of kick.
Frame set Count Relative frequency of semantic role
Arg0 Arg1 Arg2 ArgA TMP
Unergative
kick.01: drive or impel with the foot 5 80.0 20.0
kick.02: kick in, begin 7 100.0
kick.04: kick off, begin, inaugurate 3 100.0
Computational Linguistics Volume 31, Number 1
95
trained on the corpus for which semantic annotations were available, and the effect of
better, or even perfect, parses could not be measured.
In our first set of experiments, the features and probability model of the Gildea and
Jurafsky (2002) system were applied to the PropBank corpus. The existence of the
hand-annotated treebank parses for the corpus allowed us to measure the
improvement in performance offered by gold-standard parses.
7.1 System Description
Probabilities of a parse constituent belonging to a given semantic role are calculated
from the following features:
The phrase type feature indicates the syntactic type of the phrase expressing the
semantic roles: Examples include noun phrase (NP), verb phrase (VP), and clause (S).
The parse tree path feature is designed to capture the syntactic relation of a
constituent to the predicate.11 It is defined as the path from the predicate through the
parse tree to the constituent in question, represented as a string of parse tree
nonterminals linked by symbols indicating upward or downward movement through
the tree, as shown in Figure 2. Although the path is composed as a string of symbols,
our systems treat the string as an atomic value. The path includes, as the first element
of the string, the part of speech of the predicate, and as the last element, the phrase
type or syntactic category of the sentence constituent marked as an argument.
The position feature simply indicates whether the constituent to be labeled occurs
before or after the predicate. This feature is highly correlated with grammatical
function, since subjects will generally appear before a verb and objects after. This
feature may overcome the shortcomings of reading grammatical function from the
parse tree, as well as errors in the parser output.
The voice feature distinguishes between active and passive verbs and is important
in predicting semantic roles, because direct objects of active verbs correspond to
subjects of passive verbs. An instance of a verb is considered passive if it is tagged as a
past participle (e.g., taken), unless it occurs as a descendent verb phrase headed by any
form of have (e.g., has taken) without an intervening verb phrase headed by any form of
be (e.g., has been taken).
11 While the treebank has a ??subject?? marker on noun phrases, this is the only such grammatical function
tag. The treebank does not explicitly represent which verb?s subject the node is, and the subject tag is not
typically present in automatic parser output.
Figure 2
In this example, the path from the predicate ate to the argument NP He can be represented as
VBjVPjS,NP, with j indicating upward movement in the parse tree and , downward
movement.
Palmer, Gildea, and Kingsbury The Proposition Bank
The headword is a lexical feature and provides information about the semantic
type of the role filler. Headwords of nodes in the parse tree are determined using the
same deterministic set of headword rules used by Collins (1999).
The system attempts to predict argument roles in new data, looking for the
highest-probability assignment of roles ri to all constituents i in the sentence, given the
set of features Fi ? fpti, pathi, posi, vi, hig at each constituent in the parse tree, and the
predicate p:
argmaxr1 : : : n P?r1 : : : njF1 : : : n, p?
We break the probability estimation into two parts, the first being the probability
P?ri j Fi,p? of a constituent?s role given our five features for the constituent and the
predicate p. Because of the sparsity of the data, it is not possible to estimate this
probability from the counts in the training data. Instead, probabilities are estimated
from various subsets of the features and interpolated as a linear combination of the
resulting distributions. The interpolation is performed over the most specific
distributions for which data are available, which can be thought of as choosing the
topmost distributions available from a back-off lattice, shown in Figure 3.
Next, the probabilities P?ri j Fi,p? are combined with the probabilities P?fr1 : : : ngjp?
for a set of roles appearing in a sentence given a predicate, using the following formula:
P?r1 : : : n jF1 : : : n, p? , P?fr1 : : : ngjp?
Y
i
P?ri jFi, p?
P?ri jp?
This approach, described in more detail in Gildea and Jurafsky (2002), allows
interaction among the role assignments for individual constituents while making
certain independence assumptions necessary for efficient probability estimation. In
particular, we assume that sets of roles appear independent of their linear order and
that the features F of a constituent are independent of other constituents? features
given the constituent?s role.
7.1.1 Results. We applied the same system, using the same features, to a preliminary
release of the PropBank data. The data set used contained annotations for 72,109
predicate-argument structures containing 190,815 individual arguments and examples
96
Figure 3
Back-off lattice with more specific distributions towards the top.
Computational Linguistics Volume 31, Number 1
97
from 2,462 lexical predicates (types). In order to provide results comparable with the
statistical parsing literature, annotations from section 23 of the treebank were used as
the test set; all other sections were included in the training set. The preliminary
version of the data used in these experiments was not tagged for WordNet word
sense or PropBank frameset. Thus, the system neither predicts the frameset nor uses it
as a feature.
The system was tested under two conditions, one in which it is given the
constituents which are arguments to the predicate and merely has to predict the
correct role, and one in which it has to both find the arguments in the sentence and
label them correctly. Results are shown in Tables 10 and 11. Results for FrameNet are
based on a test set of 8,167 individual labels from 4,000 predicate-argument structures.
As a guideline for interpreting these results, with 8,167 observations, the threshold for
statistical significance with p < .05 is a 1.0% absolute difference in performance (Gildea
and Jurafsky 2002). For the PropBank data, with a test set of 8,625 individual labels, the
threshold for significance is similar. There are 7,574 labels for which the predicate has
been seen 10 or more times in training (third column of the tables).
Results for PropBank are similar to those for FrameNet, despite the smaller
number of training examples for many of the predicates. The FrameNet data contained
at least 10 examples from each predicate, while 12% of the PropBank data had fewer
than 10 training examples. Removing these examples from the test set gives 82.8%
accuracy with gold-standard parses and 80.9% accuracy with automatic parses.
7.1.2 Adding Traces. The gold-standard parses of the Penn Treebank include several
types of information not typically produced by statistical parsers or included in their
evaluation. Of particular importance are traces, empty syntactic categories which
generally occupy the syntactic position in which another constituent could be
interpreted and include a link to the relevant constituent. Traces are used to indicate
cases of wh-extraction, antecedents of relative clauses, and control verbs exhibiting the
syntactic phenomena of raising and ??equi.?? Traces are intended to provide hints as to
the semantics of individual clauses, and the results in Table 11 show that they do so
effectively. When annotating syntactic trees, the PropBank annotators marked the
traces along with their antecedents as arguments of the relevant verbs. In line 2 of
Table 11, along with all our experiments with automatic parser output, traces were
ignored, and the semantic-role label was assigned to the antecedent in both training
and test data. In line 3 of Table 11, we assume that the system is given trace
information, and in cases of trace chains, the semantic-role label is assigned to the trace
in training and test conditions. Trace information boosts the performance of the system
by roughly 5%. This indicates that systems capable of recovering traces (Johnson 2002;
Dienes and Dubey 2003) could improve semantic-role labeling.
Table 10
Accuracy of semantic-role prediction (in percentages) for known boundaries (the system is given
the constituents to classify).
Accuracy
FrameNet PropBank PropBank > 10 examples
Automatic parses 82.0 79.9 80.9
Gold-standard parses 82.0 82.8
Palmer, Gildea, and Kingsbury The Proposition Bank
As our path feature is a somewhat unusual way of looking at parse trees, its
behavior in the system warrants a closer look. The path feature is most useful as a way
of finding arguments in the unknown boundary condition. Removing the path feature
from the known-boundary system results in only a small degradation in performance,
from 82.0% to 80.1%. One reason for the relatively small impact may be sparseness of
the feature: 7% of paths in the test set are unseen in training data. The most common
values of the feature are shown in Table 12, in which the first two rows correspond to
standard subject and object positions. One reason for sparsity is seen in the third row:
In the treebank, the adjunction of an adverbial phrase or modal verb can cause an
additional VP node to appear in our path feature. We tried two variations of the path
feature to address this problem. The first collapses sequences of nodes with the same
label, for example, combining rows 2 and 3 of Table 12. The second variation uses only
two values for the feature: NP under S (subject position) and NP under VP (object
position). Neither variation improved performance in the known-boundary condition.
As a gauge of how closely the PropBank semantic-role labels correspond to the path
feature overall, we note that by always assigning the most common role for each path
(for example, always assigning Arg0 to the subject position), and using no other
features, we obtain the correct role 64.0% of the time, versus 82.0% for the complete
system. Conditioning on the path and predicate, which allows the subject of different
verbs to receive different labels but does not allow for alternation behavior within a
verb?s argument structure, yields an accuracy rate of 76.6%.
Table 13 shows the performance of the system broken down by the argument types
in the gold standard. Results are shown for the unknown-boundaries condition, using
gold-standard parses and traces (last row, middle two columns of Table 11). The
98
Table 12
Common values (in percentages) for parse tree path in PropBank data, using gold-standard
parses.
Path Frequency
VBjVP,NP 17.6
VBjVPjS,NP 16.4
VBjVPjVPjS,NP 7.8
VBjVP,PP 7.6
VBjVP,PP,NP 7.3
VBjVP,SBAR,S 4.3
VBjVP,S 4.3
VBjVP,ADVP 2.4
Others (n = 1,031) 76.0
Table 11
Accuracy of semantic-role prediction (in percentages) for unknown boundaries (the system must
identify the correct constituents as arguments and give them the correct roles).
FrameNet PropBank PropBank > 10 examples
Precision Recall Precision Recall Precision Recall
Automatic parses 64.6 61.2 68.6 57.8 69.9 61.1
Gold-standard parses 74.3 66.4 76.0 69.9
Gold-standard with traces 80.6 71.6 82.0 74.7
Computational Linguistics Volume 31, Number 1
99
??Labeled Recall?? column shows how often the semantic-role label is correctly iden-
tified, while the ??Unlabeled recall?? column shows how often a constituent with the
given role is correctly identified as being a semantic role, even if it is labeled with the
wrong role. The more central, numbered roles are consistently easier to identify than
the adjunct-like ArgM roles, even when the ArgM roles have preexisting Treebank
function tags.
7.2 The Relation of Syntactic Parsing and Semantic-Role Labeling
Many recent information extraction systems for limited domains have relied on finite-
state systems that do not build a full parse tree for the sentence being analyzed.
Among such systems, Hobbs et al (1997) built finite-state recognizers for various
entities, which were then cascaded to form recognizers for higher-level relations, while
Ray and Craven (2001) used low-level ??chunks?? from a general-purpose syntactic
analyzer as observations in a trained hidden Markov model. Such an approach has a
large advantage in speed, as the extensive search of modern statistical parsers is
avoided. It is also possible that this approach may be more robust to error than parsers.
Our experiments working with a flat, ??chunked?? representation of the input sentence,
described in more detail in Gildea and Palmer (2002), test this finite-state hypothesis.
In the chunked representation, base-level constituent boundaries and labels are pres-
ent, but there are no dependencies between constituents, as shown by the following
sample sentence:
(48) [
NP
Big investment banks] [
VP
refused to step] [
ADVP
up] [
PP
to]
[
NP
the plate] [
VP
to support] [
NP
the beleaguered floor traders] [
PP
by]
[
VP
buying] [
NP
bigblocks] [
PP
of] [
NP
stock], [
NP
traders] [
VP
say]. (wsj_2300)
Our chunks were derived from the treebank trees using the conversion described
by Tjong Kim Sang and Buchholz (2000). Thus, the experiments were carried out using
Table 13
Accuracy of semantic-role prediction for unknown boundaries (the system must identify the
correct constituents as arguments and give them the correct roles).
Role Number Precision Labeled recall Unlabeled recall
Arg0 1,197 94.2% 88.9% 92.2%
Arg1 1,436 95.4 82.5 88.9
Arg2 229 79.0 64.2 77.7
Arg3 61 71.4 49.2 54.1
Arg4 31 91.7 71.0 83.9
ArgM 127 59.6 26.8 52.0
ArgM-ADV 85 59.1 30.6 55.3
ArgM-DIR 49 76.7 46.9 61.2
ArgM-DIS 65 40.0 18.5 55.4
ArgM-EXT 18 81.2 72.2 77.8
ArgM-LOC 95 60.7 38.9 62.1
ArgM-MNR 80 62.7 40.0 63.8
ArgM-MOD 95 77.6 40.0 43.2
ArgM-NEG 40 63.6 17.5 40.0
ArgM-PRD 3 0.0 0.0 33.3
ArgM-PRP 54 70.0 25.9 37.0
ArgM-TMP 325 72.4 45.2 64.6
Palmer, Gildea, and Kingsbury The Proposition Bank
gold-standard rather than automatically derived chunk boundaries, which we believe
will provide an upper bound on the performance of a chunk-based system. Distance in
chunks from the predicate was used in place of the parser-based path feature.
The results in Table 14 show that full parse trees are much more effective than the
chunked representation for labeling semantic roles. This is the case even if we relax
the scoring criteria to count as correct all cases in which the system correctly identifies
the first chunk belonging to an argument (last row of Table 14).
As an example for comparing the behavior of the tree-based and chunk-based
systems, consider the following sentence, with human annotations showing the
arguments of the predicate support:
(49) [
Arg0
Big investment banks] refused to step up to the plate to support
[
Arg1
the beleaguered floor traders] [
MNR
by buying big blocks of stock],
traders say.
Our system based on automatic parser output assigned the following analysis:
(50) Big investment banks refused to step up to the plate to support
[
Arg1
the beleaguered floor traders] [
MNR
by buying big blocks of stock],
traders say.
In this case, the system failed to find the predicate?s Arg0 relation, because it is
syntactically distant from the verb support. The original treebank syntactic tree contains
a trace which would allow one to recover this relation, coindexing the empty subject
position of support with the noun phrase Big investment banks. However, our automatic
parser output does not include such traces. The system based on gold-standard trees
and incorporating trace information produced exactly the correct labels:
(51) [
Arg0
Big investment banks] refused to step up to the plate to support
[
Arg1
the beleaguered floor traders] [
MNR
by buying big blocks of stock],
traders say.
The system based on (gold-standard) chunks assigned the following semantic-role labels:
(52) Big investment banks refused to step up to [
Arg0
the plate] to support
[
Arg1
the beleaguered floor traders] by buying big blocks of stock,
traders say.
Here, as before, the true Arg0 relation is not found, and it would be difficult to imagine
identifying it without building a complete syntactic parse of the sentence. But now,
100
Table 14
Summary of results for unknown-boundary condition.
Precision Recall
Gold parse 74.3% 66.4%
Auto parse 68.6 57.8
Chunk 27.6 22.0
Chunk, relaxed scoring 49.5 35.1
Computational Linguistics Volume 31, Number 1
101
unlike in the tree-based output, the Arg0 label is mistakenly attached to a noun phrase
immediately before the predicate. The Arg1 relation in direct-object position is fairly
easily identifiable in the chunked representation as a noun phrase directly following
the verb. The prepositional phrase expressing the Manner relation, however, is not
identified by the chunk-based system. The tree-based system?s path feature for this
constituent is VBjVP,PP, which identifies the prepositional phrase as attaching to the
verb and increases its probability of being assigned an argument label. The chunk-
based system sees this as a prepositional phrase appearing as the second chunk after
the predicate. Although this may be a typical position for the Manner relation, the fact
that the preposition attaches to the predicate rather than to its direct object is not
represented.
Participants in the 2004 CoNLL semantic-labeling shared task (Carreras and
Ma`rquez 2004) have reported higher results for chunk-based systems, but to date
chunk-based systems have not closed the gap with the state-of-the-art results based on
parser output.
7.2.1 Parsing and Models of Syntax. While treebank parsers such as that of Collins
(1999) return much richer representations than a chunker, they do not include a great
deal of the information present in the original Penn Treebank. Specifically, long-
distance dependencies indicated by traces in the treebank are crucial for seman-
tic interpretation but do not affect the constituent recall and precision metrics most
often used to evaluate parsers and are not included in the output of the standard
parsers.
Gildea and Hockenmaier (2003) present a system for labeling PropBank?s semantic
roles based on a statistical parser for combinatory categorial grammar (CCG)
(Steedman 2000). The parser, described in detail in Hockenmaier and Steedman
(2002), is trained on a version of the Penn Treebank automatically converted to CCG
representations. The conversion process uses the treebank?s trace information to make
underlying syntactic relations explicit. For example, the same CCG-level relation
appears between a verb and its direct object whether the verb is used in a simple
transitive clause, a relative clause, or a question with wh-extraction. Using the CCG-
based parser, Gildea and Hockenmaier (2003) find a 2% absolute improvement over
the Collins parser in identifying core or numbered PropBank arguments. This points to
the shortcomings of evaluating parsers purely on constituent precision and recall; we
feel that a dependency-based evaluation (e.g., Carroll, Briscoe, and Sanfilippo 1998) is
more relevant to real-world applications.
8. Conclusion
The Proposition Bank takes the comprehensive corpus annotation of the Penn
Treebank one step closer to a detailed semantic representation by adding semantic-role
labels. On analyzing the data, the relationships between syntax and semantic
structures are more complex than one might at first expect. Alternations in the
realization of semantic arguments of the type described by Levin (1993) turn out to be
common in practice as well as in theory, even in the limited genre of Wall Street Journal
articles. Even so, by using detailed guidelines for the annotation of each individual
verb, rapid consistent annotation has been achieved, and the corpus is available
through the Linguistic Data Consortium. For information on obtaining the frames file,
please consult http://www.cis.upenn.edu/?ace/.
Palmer, Gildea, and Kingsbury The Proposition Bank
The broad-coverage annotation has proven to be suitable for training automatic
taggers, and in addition to ourselves there is a growing body of researchers engaged in
this task. Chen and Rambow (2003) make use of extracted tree-adjoining grammars.
Most recently, the Gildea and Palmer (2002) scores presented here have been improved
markedly through the use of support-vector machines as well as additional features for
named entity tags, headword POS tags, and verb clusters for back-off (Pradhan et al
2003) and using maximum-entropy classifiers (He and Gildea 2004, Xue and Palmer
2004). This group also used Charniak?s parser instead of Collins?s and tested the
system on TDT data. The performance on a new genre is lower, as would be expected.
Despite the complex relationship between syntactic and semantic structures, we
find that statistical parsers, although computationally expensive, do a good job of
providing information relevant for this level of semantic interpretation. In addition to
the constituent structure, the headword information, produced as a side product, is an
important feature. Automatic parsers, however, still have a long way to go. Our results
using hand-annotated parse trees including traces show that improvements in parsing
should translate directly into more accurate semantic representations.
There has already been a demonstration that a preliminary version of these data
can be used to simplify the effort involved in developing information extraction (IE)
systems. Researchers were able to construct a reasonable IE system by simply mapping
specific Arg labels for a set of verbs to template slots, completely avoiding the
necessity of building explicit regular expression pattern matchers (Surdeanu et al
2003). There is equal hope for advantages for machine translation, and proposition
banks in Chinese (Xue and Palmer 2003) and Korean are already being built, focusing
where possible on parallel data. The general approach ports well to new languages,
with the major effort continuing to go into the creation of frames files for verbs.
There are many directions for future work. Our preliminary linguistic analyses
have merely scratched the surface of what is possible with the current annotation,
and yet it is only a first approximation at capturing the richness of semantic repre-
sentation. Annotation of nominalizations and other noun predicates is currently being
added by New York University, and a Phase II (Babko-Malaya et al) that will include
eventuality variables, nominal references, additional sense tagging, and discourse
connectives is underway.
We have several plans for improving the performance of our automatic semantic-
role labeling. As a first step we are producing a version of PropBank that uses more in-
formative thematic labels based on VerbNet thematic labels (Kipper, Palmer, and
Rambow 2002). We are also working with FrameNet to produce a mapping between
our annotation and theirs which will allow us to merge the two annotated data sets.
Finally, we will explore alternative machine-learning approaches and closer integra-
tion of semantic-role labeling and sense tagging with the parsing process.
102
Acknowledgments
This work was funded by DOD grant
MDA904-00C-2136, NSF grant IIS-9800658,
and the Institute for Research in Cognitive
Science at the University of Pennsylvania
NSF-STC grant SBR-89-20230. Any
opinions, findings, and conclusions or
recommendations expressed in this material
are those of the authors and do not
necessarily reflect the views of the National
Science Foundation. We are indebted to Scott
Cotton, our staff programmer, for his untiring
efforts and his lovely annotation tool, to
Joseph Rosenzweig for the initial automatic
semantic-role labeling of the Penn Treebank,
and to Ben Snyder for programming support
and computing agreement figures. We would
also like to thank Mitch Marcus, Aravind
Joshi, Olga Babko-Malaya, Hoa Trang Dang,
Christiane Fellbaum, and Betsy Klipple for
their extremely useful and insightful
guidance and our many hard-working
Computational Linguistics Volume 31, Number 1
103
annotators, especially Kate Forbes, Ilana
Streit, Ann Delilkin, Brian Hertler, Neville
Ryant, and Jill Flegg, for all of their help.
References
Abeille?, Anne, editor. 2003. Building and Using
Parsed Corpora. Language and Speech
series. Kluwer, Dordrecht.
Alshawi, Hiyan, editor. 1992. The Core
Language Engine. MIT Press, Cambridge,
MA.
Baker, Collin F., Charles J. Fillmore, and John
B. Lowe. 1998. The Berkeley FrameNet
project. In Proceedings of COLING/ACL,
pages 86?90, Montreal.
Bangalore, Srinivas and Aravind K. Joshi.
1999. Supertagging: An approach to almost
parsing. Computational Linguistics,
25(2):237?265.
Brent, Michael R. 1993. From grammar to
lexicon: Unsupervised learning of lexical
syntax. Computational Linguistics, 19(2):
243?262.
Briscoe, Ted and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. In Fifth Conference on Applied
Natural Language Processing, pages 356?363,
Washington, DC. ACL.
Carreras, Xavier and LluN?s Ma`rquez.
2004. Introduction to the CoNLL-2004
shared task: Semantic role labeling. In
HLT-NAACL 2004 Workshop: Eighth
Conference on Computational Natural
Language Learning (CoNLL-2004),
pages 89?97, Boston.
Carroll, John, Ted Briscoe, and Antonio
Sanfilippo. 1998. Parser evaluation:
A survey and a new proposal. In
Proceedings of the First International
Conference on Language Resources and
Evaluation, pages 447?454, Granada, Spain.
Charniak, Eugene. 2000. A
maximum-entropy-inspired parser.
In Proceedings of the First Annual Meeting
of the North American Chapter of the ACL
(NAACL), pages 132?139, Seattle.
Chen, John and Owen Rambow. 2003.
Use of deep linguistic features for the
recognition and labeling of semantic
arguments. In Michael Collins and
Mark Steedman, editors, Proceedings
of the 2003 Conference on Empirical
Methods in Natural Language Processing,
Sapporo, Japan, pages 41?48.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania,
Philadelphia.
Collins, Michael. 2000. Discriminative
reranking for natural language parsing.
In Proceedings of the International
Conference on Machine Learning (ICML),
Stanford, CA.
Dang, Hoa Trang, Karin Kipper, Martha
Palmer, and Joseph Rosenzweig. 1998.
Investigating regular sense extensions
based on intersective Levin classes. In
COLING/ACL-98, pages 293?299,
Montreal. ACL.
Dienes, Peter and Amit Dubey. 2003.
Antecedent recovery: Experiments with a
trace tagger. In 2003 Conference on Empirical
Methods in Natural Language Processing
(EMNLP), Sapporo, Japan.
Dorr, Bonnie J. and Douglas Jones. 2000.
Acquisition of semantic lexicons: Using
word sense disambiguation to improve
precision. In Evelyn Viegas, editor,
Breadth and Depth of Semantic Lexicons.
Kluwer Academic, Norwell, MA,
pages 79?98.
Dowty, David R. 1991. Thematic proto-roles
and argument selection. Language,
67(3):547?619.
Fillmore, Charles J. 1976. Frame semantics
and the nature of language. In Annals of the
New York Academy of Sciences: Conference on
the Origin and Development of Language and
Speech, volume 280, pages 20?32.
Fillmore, Charles J. and B. T. S. Atkins. 1998.
FrameNet and lexicographic relevance. In
Proceedings of the First International
Conference on Language Resources and
Evaluation, Granada, Spain.
Fillmore, Charles J. and Collin F. Baker. 2001.
Frame semantics for text understanding. In
Proceedings of NAACL WordNet and Other
Lexical Resources Workshop, Pittsburgh, June.
Gildea, Daniel and Julia Hockenmaier. 2003.
Identifying semantic roles using
combinatory categorial grammar. In 2003
Conference on Empirical Methods in Natural
Language Processing (EMNLP), Sapporo,
Japan.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Gildea, Daniel and Martha Palmer. 2002. The
necessity of syntactic parsing for predicate
argument recognition. In Proceedings of the
40th Annual Conference of the Association for
Computational Linguistics (ACL-02),
pages 239?246, Philadelphia.
Hajic?ova?, Eva and Ivona Kuc?erova?. 2002.
Argument/valency structure in PropBank,
LCS Database and Prague Dependency
Treebank: A comparative pilot study. In
Palmer, Gildea, and Kingsbury The Proposition Bank
104
Proceedings of the Third International
Conference on Language Resources and
Evaluation (LREC 2002), Las Palmas, Spain,
pages 846?851. ELRA.
He, Shan and Daniel Gildea. 2004. Semantic
roles labeling by maximum entropy model.
Technical Report 847, University of
Rochester.
Hobbs, Jerry R., Douglas Appelt, John Bear,
David Israel, Megumi Kameyama,
Mark E. Stickel, and Mabry Tyson. 1997.
FASTUS: A cascaded finite-state
transducer for extracting information
from natural-language text. In Emmanuel
Roche and Yves Schabes, editors,
Finite-State Language Processing. MIT Press,
Cambridge, MA, pages 383?406.
Hockenmaier, Julia and Mark Steedman.
2002. Generative models for statistical
parsing with combinatory categorial
grammar. In Proceedings of the 40th Annual
Conference of the Association for
Computational Linguistics (ACL-02),
pages 335?342, Philadelphia.
Johnson, Christopher R., Charles J. Fillmore,
Miriam R. L. Petruck, Collin F. Baker,
Michael Ellsworth, Josef Ruppenhofer, and
Esther J. Wood. 2002. FrameNet: Theory
and practice. Version 1.0, available at
http://www.icsi.berkeley.edu/framenet/.
Johnson, Mark. 2002. A simple
pattern-matching algorithm for recovering
empty nodes and their antecedents. In
Proceedings of the 40th Annual Conference of
the Association for Computational Linguistics
(ACL-02), Philadelphia.
Kipper, Karin, Hoa Trang Dang, and Martha
Palmer. 2000. Class-based construction of a
verb lexicon. In Proceedings of the Seventh
National Conference on Artificial Intelligence
(AAAI-2000), Austin, TX, July?August.
Kipper, Karin, Martha Palmer, and Owen
Rambow. 2002. Extending PropBank
with VerbNet semantic predicates.
Paper presented at Workshop on
Applied Interlinguas, AMTA-2002,
Tiburon, California, October.
Korhonen, Anna and Ted Briscoe. 2004.
Extended lexical-semantic classification of
English verbs. In Proceedings of the HLT/
NAACL Workshop on Computational Lexical
Semantics, Boston.
Korhonen, Anna, Yuval Krymolowsky, and
Zvika Marx. 2003. Clustering polysemic
subcategorization frame distributions
semantically. In Proceedings of the 41st
Annual Conference of the Association for
Computational Linguistics (ACL-03),
Sapporo, Japan.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago.
Manning, Christopher D. 1993. Automatic
acquisition of a large subcategorization
dictionary from corpora. In Proceedings
of the 31st Annual Meeting of the Association
for Computational Linguistics, pages 235?242,
Ohio State University, Columbus.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
McCarthy, Diana. 2000. Using semantic
preferences to identify verbal participation
in role switching alternations. In Proceedings
of the 1st Annual Meeting of the North
American Chapter of the ACL (NAACL),
pages 256?263, Seattle.
Merlo, Paola and Suzanne Stevenson. 2001.
Automatic verb classification based on
statistical distribution of argument
structure. Computational Linguistics,
27(3):373?408.
Miller, Scott, Michael Crystal, Heidi Fox,
Lance Ramshaw, Richard Schwartz,
Rebecca Stone, Ralph Weischedel, and the
Annotation Group. 1998. Algorithms that
learn to extract information?BBN:
Description of the SIFT system as used
for MUC-7. In Proceedings of the Seventh
Message Understanding Conference
(MUC-7), April.
Babko-Malaya, Olga, Martha Palmer,
Nianwen Xue, Aravind Joshi, Seth Kulick,
Proposition Bank II: Delving deeper,
frontiers in corpus annotation, Workshop
in conjunction with HLT/NAACL 2004,
Boston, MA, May 6, 2004.
Palmer, Martha, Olga Babko-Malaya, and
Hoa Trang Dang. 2004. Different sense
granularities for different applications. In
Second Workshop on Scalable Natural
Language Understanding Systems at HLT/
NAACL-04, Boston.
Palmer, Martha, Joseph Rosenzweig, and
Scott Cotton. 2001. Predicate argument
analysis of the Penn Treebank. In
Proceedings of HLT 2001, First International
Conference on Human Language Technology
Research, San Diego, CA, March.
Pradhan, S., K. Hacioglu, W. Ward,
J. Martin, and Daniel Jurafsky. 2003.
Semantic role parsing: Adding
semantic structure to unstructured
text. In Proceedings of the International
Conference on Data Mining (ICDM-2003),
Melbourne, FL.
Computational Linguistics Volume 31, Number 1
105
Rambow, Owen, Bonnie J. Dorr, Karin
Kipper, Ivona Kuc?erova?, and Martha
Palmer. 2003. Automatically deriving
tectogrammatical labels from other
resources: A comparison of semantic
labels across frameworks. Prague
Bulletin of Mathematical Linguistics,
vol. 79?80, pages 23?35.
Ratnaparkhi, Adwait. 1997. A linear
observed time statistical parser based
on maximum entropy models. In
Proceedings of the Second Conference on
Empirical Methods in Natural Language
Processing, pages 1?10, Providence, ACL.
Ray, Soumya and Mark Craven. 2001.
Representing sentence structure in
hidden Markov model for information
extraction. In Seventeenth International Joint
Conference on Artificial Intelligence
(IJCAI-01), Seattle.
Schulte im Walde, Sabine. 2000.
Clustering verbs semantically according
to their alternation behaviour. In
Proceedings of the 18th International
Conference on Computational Linguistics
(COLING-00), pages 747?753,
Saarbru?cken, Germany.
Schulte im Walde, Sabine and Chris Brew.
2002. Inducing German semantic verb
classes from purely syntactic
subcategorisation information. In
Proceedings of the 40th Annual Conference
of the Association for Computational
Linguistics (ACL-02), pages 223?230,
Philadelphia.
Siegel, Sidney and N. John Castellan Jr.
1988. Nonparametric Statistics for the
Behavioral Sciences. 2nd ed. McGraw-Hill,
New York.
Steedman, Mark. 2000. The Syntactic Process.
MIT Press, Cambridge, MA.
Surdeanu, Mihai, Sanda Harabagiu,
John Williams, and Paul Aarseth. 2003.
Using predicate-argument structures
for information extraction. In Proceedings
of the 41st Annual Conference of the
Association for Computational Linguistics
(ACL-03), Sapporo, Japan, pages 8?15.
Tjong Kim Sang, Erik F. and Sabine Buchholz.
2000. Introduction to the CoNLL-2000
shared task: Chunking. In Proceedings
of CoNLL-2000 and LLL-2000, Lisbon,
Portugal.
Xue, Nianwen, and Martha Palmer. 2004.
Calibrating Features for Semantic Role
Labeling, Empirical Methods in Natural
Language Processing Conference, in
conjunction with the 42nd Meeting of the
Association for Computational Linguistics
(ACL-04), Barcelona, Spain, July 21?26.
Xue, Nianwen, and Martha Palmer. 2004.
Annotating the Propositions in the Penn
Chinese Treebank Second SIGHAN
Workshop on Chinese Language
Processing, held in conjunction with
ACL-03, Sapporo, Japan, pages 47?54, July.
Palmer, Gildea, and Kingsbury The Proposition Bank

A Smorgasbord of Features for Statistical Machine Translation
Franz Josef Och
USC/ISI
Daniel Gildea
U. of Rochester
Sanjeev Khudanpur
Johns Hopkins U.
Anoop Sarkar
Simon Fraser U.
Kenji Yamada
Xerox/XRCE
Alex Fraser
USC/ISI
Shankar Kumar
Johns Hopkins U.
Libin Shen
U. of Pennsylvania
David Smith
Johns Hopkins U.
Katherine Eng
Stanford U.
Viren Jain
U. of Pennsylvania
Zhen Jin
Mt. Holyoke
Dragomir Radev
U. of Michigan
Abstract
We describe a methodology for rapid exper-
imentation in statistical machine translation
which we use to add a large number of features
to a baseline system exploiting features from a
wide range of levels of syntactic representation.
Feature values were combined in a log-linear
model to select the highest scoring candidate
translation from an n-best list. Feature weights
were optimized directly against the BLEU eval-
uation metric on held-out data. We present re-
sults for a small selection of features at each
level of syntactic representation.
1 Introduction
Despite the enormous progress in machine translation
(MT) due to the use of statistical techniques in recent
years, state-of-the-art statistical systems often produce
translations with obvious errors. Grammatical errors in-
clude lack of a main verb, wrong word order, and wrong
choice of function words. Frequent problems of a less
grammatical nature include missing content words and
incorrect punctuation.
In this paper, we attempt to address these problems by
exploring a variety of new features for scoring candidate
translations. A high-quality statistical translation system
is our baseline, and we add new features to the exist-
ing set, which are then combined in a log-linear model.
To allow an easy integration of new features, the base-
line system provides an n-best list of candidate transla-
tions which is then reranked using the new features. This
framework allows us to incorporate different types of fea-
tures, including features based on syntactic analyses of
the source and target sentences, which we hope will ad-
dress the grammaticality of the translations, as well as
lower-level features. As we work on n-best lists, we can
easily use global sentence-level features.
We begin by describing our baseline system and the
n-best rescoring framework within which we conducted
our experiments. We then present a selection of new fea-
tures, progressing from word-level features to those based
to part-of-speech tags and syntactic chunks, and then to
features based on Treebank-based syntactic parses of the
source and target sentences.
2 Log-linear Models for Statistical MT
The goal is the translation of a text given in some source
language into a target language. We are given a source
(?Chinese?) sentence f = fJ1 = f1, . . . , fj , . . . , fJ ,
which is to be translated into a target (?English?) sentence
e = eI1 = e1, . . . , ei, . . . , eI Among all possible target
sentences, we will choose the sentence with the highest
probability:
e?I1 = argmax
eI1
{Pr(eI1|f
J
1 )} (1)
As an alternative to the often used source-channel ap-
proach (Brown et al, 1993), we directly model the pos-
terior probability Pr(eI1|fJ1 ) (Och and Ney, 2002) us-
ing a log-linear combination of feature functions. In
this framework, we have a set of M feature functions
hm(eI1, f
J
1 ),m = 1, . . . ,M . For each feature function,
there exists a model parameter ?m,m = 1, . . . ,M . The
direct translation probability is given by:
Pr(eI1|f
J
1 ) =
exp[
?M
m=1 ?mhm(e
I
1, f
J
1 )]
?
e?I1
exp[
?M
m=1 ?mhm(e
?I
1, f
J
1 )]
(2)
We obtain the following decision rule:
e?I1 = argmax
eI1
{ M?
m=1
?mhm(e
I
1, f
J
1 )
}
(3)
The standard criterion for training such a log-linear
model is to maximize the probability of the parallel train-
ing corpus consisting of S sentence pairs {(fs, es) : s =
1, . . . , S}. However, this does not guarantee optimal per-
formance on the metric of translation quality by which
our system will ultimately be evaluated. For this reason,
we optimize the parameters directly against the BLEU
metric on held-out data. This is a more difficult optimiza-
tion problem, as the search space is no longer convex.
Figure 1: Example segmentation of Chinese sentence and
its English translation into alignment templates.
However, certain properties of the BLEU metric can be
exploited to speed up search, as described in detail by
Och (2003). We use this method of optimizing feature
weights throughout this paper.
2.1 Baseline MT System: Alignment Templates
Our baseline MT system is the alignment template system
described in detail by Och, Tillmann, and Ney (1999)
and Och and Ney (2004). In the following, we give a
short description of this baseline model.
The probability model of the alignment template sys-
tem for translating a sentence can be thought of in distinct
stages. First, the source sentence words fJ1 are grouped to
phrases f?K1 . For each phrase f? an alignment template z is
chosen and the sequence of chosen alignment templates
is reordered (according to piK1 ). Then, every phrase f?
produces its translation e? (using the corresponding align-
ment template z). Finally, the sequence of phrases e?K1
constitutes the sequence of words eI1.
Our baseline system incorporated the following feature
functions:
Alignment Template Selection Each alignment
template is chosen with probability p(z|f?), estimated by
relative frequency. The corresponding feature function in
our log-linear model is the log probability of the product
of p(z|f?) for all used alignment templates used.
Word Selection This feature is based on the lexical
translation probabilities p(e|f), estimated using relative
frequencies according to the highest-probability word-
level alignment for each training sentence. A translation
probability conditioned on the source and target position
within the alignment template p(e|f, i, j) is interpolated
with the position-independent probability p(e|f).
Phrase Alignment This feature favors monotonic
alignment at the phrase level. It measures the ?amount
of non-monotonicity? by summing over the distance (in
the source language) of alignment templates which are
consecutive in the target language.
Language Model Features As a language model
feature, we use a standard backing off word-based tri-
gram language model (Ney, Generet, and Wessel, 1995).
The baseline system actually includes four different lan-
guage model features trained on four different corpora:
the news part of the bilingual training data, a large Xin-
hua news corpus, a large AFP news corpus, and a set of
Chinese news texts downloaded from the web.
Word/Phrase Penalty This word penalty feature
counts the length in words of the target sentence. Without
this feature, the sentences produced tend to be too short.
The phrase penalty feature counts the number of phrases
produced, and can allow the model to prefer either short
or long phrases.
Phrases from Conventional Lexicon The baseline
alignment template system makes use of the Chinese-
English lexicon provided by LDC. Each lexicon entry is
a potential phrase translation pair in the alignment tem-
plate system. To score the use of these lexicon entries
(which have no normal translation probability), this fea-
ture function counts the number of times such a lexicon
entry is used.
Additional Features A major advantage of the log-
linear modeling approach is that it is easy to add new
features. In this paper, we explore a variety of features
based on successively deeper syntactic representations of
the source and target sentences, and their alignment. For
each of the new features discussed below, we added the
feature value to the set of baseline features, re-estimated
feature weights on development data, and obtained re-
sults on test data.
3 Experimental Framework
We worked with the Chinese-English data from the recent
evaluations, as both large amounts of sentence-aligned
training corpora and multiple gold standard reference
translations are available. This is a standard data set,
making it possible to compare results with other systems.
In addition, working on Chinese allows us to use the ex-
isting Chinese syntactic treebank and parsers based on it.
For the baseline MT system, we distinguish the fol-
lowing three different sentence- or chunk-aligned parallel
training corpora:
? training corpus (train): This is the basic training
corpus used to train the alignment template transla-
tion model (word lexicon and phrase lexicon). This
corpus consists of about 170M English words. Large
parts of this corpus are aligned on a sub-sentence
level to avoid the existence of very long sentences
which would be filtered out in the training process
to allow a manageable word alignment training.
? development corpus (dev): This is the training cor-
pus used in discriminative training of the model-
parameters of the log-linear translation model. In
most experiments described in this report this cor-
pus consists of 993 sentences (about 25K words) in
both languages.
? test corpus (test): This is the test corpus used to
assess the quality of the newly developed feature
functions. It consists of 878 sentences (about 25K
words).
For development and test data, we have four English (ref-
erence) translations for each Chinese sentence.
3.1 Reranking, n-best lists, and oracles
For each sentence in the development, test, and the blind
test corpus a set of 16,384 different alternative transla-
tions has been produced using the baseline system. For
extracting the n-best candidate translations, an A* search
is used. These n-best candidate translations are the basis
for discriminative training of the model parameters and
for re-ranking.
We used n-best reranking rather than implementing
new search algorithms. The development of efficient
search algorithms for long-range dependencies is very
complicated and a research topic in itself. The rerank-
ing strategy enabled us to quickly try out a lot of new
dependencies, which would not have been be possible if
the search algorithm had to be changed for each new de-
pendency.
On the other hand, the use of n-best list rescoring lim-
its the possibility of improvements to what is available
in the n-best list. Hence, it is important to analyze the
quality of the n-best lists by determining how much of an
improvement would be possible given a perfect reranking
algorithm. We computed the oracle translations, that is,
the set of translations from our n-best list that yields the
best BLEU score.1
We use the following two methods to compute the
BLEU score of an oracle translation:
1. optimal oracle (opt): We select the oracle sentences
which give the highest BLEU score compared to the
set of 4 reference translations. Then, we compute
BLEU score of oracle sentences using the same set
of reference translations.
2. round-robin oracle (rr): We select four differ-
ent sets of oracle sentences which give the highest
BLEU score compared to each of the 4 references
translations. Then, we compute for each set of or-
acle sentences a BLEU score using always those
three references to score that have not been cho-
sen to select the oracle. Then, these 4 3-reference
BLEU scores are averaged.
1Note that due to the corpus-level holistic nature of the
BLEU score it is not trivial to compute the optimal set of oracle
translations. We use a greedy search algorithm for the oracle
translations that might find only a local optimum. Empirically,
we do not observe a dependence on the starting point, hence we
believe that this does not pose a significant problem.
Table 1: Oracle BLEU scores for different sizes of the
n-best list. The avBLEUr3 scores are computed with
respect to three reference translations averaged over the
four different choices of holding out one reference.
avBLEUr3[%] BLEUr4
n rr opt opt
human 35.8 -
1 28.3 28.3 31.6
4 29.1 30.8 34.5
16 29.9 33.2 37.3
64 30.6 35.6 38.7
256 31.3 37.8 42.8
1024 31.7 40.0 45.3
4096 32.0 41.8 47.3
The first method provides the theoretical upper bound of
what BLEU score can be obtained by rescoring a given n-
best list. Using this method with a 1000-best list, we ob-
tain oracle translations that outperform the BLEU score
of the human translations. The oracle translations achieve
113% against the human BLEU score on the test data
(Table 1), while the first best translations obtain 79.2%
against the human BLEU score. The second method uses
a different references for selection and scoring. Here, us-
ing an 1000-best list, we obtain oracle translations with a
relative human BLEU score of 88.5%.
Based on the results of the oracle experiment, and
in order to make rescoring computationally feasible for
features requiring significant computation for each hy-
pothesis, we used the top 1000 translation candidates for
our experiments. The baseline system?s BLEU score is
31.6% on the test set (equivalent to the 1-best oracle in
Table 1). This is the benchmark against which the contri-
butions of the additional features described in the remain-
der of this paper are to be judged.
3.2 Preprocessing
As a precursor to developing the various syntactic fea-
tures described in this report, the syntactic represen-
tations on which they are based needed to be com-
puted. This involved part-of-speech tagging, chunking,
and parsing both the Chinese and English side of our
training, development, and test sets.
Applying the part-of-speech tagger to the often un-
grammatical MT output from our n-best lists sometimes
led to unexpected results. Often the tagger tries to ?fix
up? ungrammatical sentences, for example by looking for
a verb when none is present:
China NNP 14 CD open JJ border NN
cities NNS achievements VBZ remarkable JJ
Here, although achievements has never been seen as a
verb in the tagger?s training data, the prior for a verb
in this position is high enough to cause a present tense
verb tag to be produced. In addition to the inaccura-
cies of the MT system, the difference in genre from the
tagger?s training text can cause problems. For example,
while our MT data include news article headlines with no
verb, headlines are not included in the Wall Street Journal
text on which the tagger is trained. Similarly, the tagger
is trained on full sentences with normalized punctuation,
leading it to expect punctuation at the end of every sen-
tence, and produce a punctuation tag even when the evi-
dence does not support it:
China NNP ?s POS economic JJ
development NN and CC opening VBG
up RP 14 CD border NN cities NNS
remarkable JJ achievements .
The same issues affect the parser. For example the
parser can create verb phrases where none exist, as in the
following example in which the tagger correctly did not
identify a verb in the sentence:
These effects have serious implications for designing
syntactic feature functions. Features such ?is there a verb
phrase? may not do what you expect. One solution would
be features that involve the probability of a parse subtree
or tag sequence, allowing us to ask ?how good a verb
phrase is it?? Another solution is more detailed features
examining more of the structure, such as ?is there a verb
phrase with a verb??
4 Word-Level Feature Functions
These features, directly based on the source and target
strings of words, are intended to address such problems as
translation choice, missing content words, and incorrect
punctuation.
4.1 Model 1 Score
We used IBM Model 1 (Brown et al, 1993) as one of the
feature functions. Since Model 1 is a bag-of-word trans-
lation model and it gives the sum of all possible alignment
probabilities, a lexical co-occurrence effect, or triggering
effect, is expected. This captures a sort of topic or seman-
tic coherence in translations.
As defined by Brown et al (1993), Model 1 gives a
probability of any given translation pair, which is
p(f |e; M1) =

(l + 1)m
m?
j=1
l?
i=0
t(fj |ei).
We used GIZA++ to train the model. The training data is
a subset (30 million words on the English side) of the en-
tire corpus that was used to train the baseline MT system.
For a missing translation word pair or unknown words,
where t(fj |ei) = 0 according to the model, a constant
t(fj |ei) = 10?40 was used as a smoothing value.
The average %BLEU score (average of the best four
among different 20 search initial points) is 32.5. We also
tried p(e|f ; M1) as feature function, but did not obtain
improvements which might be due to an overlap with the
word selection feature in the baseline system.
The Model 1 score is one of the best performing fea-
tures. It seems to ?fix? the tendency of our baseline sys-
tem to delete content words and it improves word selec-
tion coherence by the triggering effect. It is also possible
that the triggering effect might work on selecting a proper
verb-noun combination, or a verb-preposition combina-
tion.
4.2 Lexical Re-ordering of Alignment Templates
As shown in Figure 1 the alignment templates (ATs)
used in the baseline system can appear in various con-
figurations which we will call left/right-monotone and
left/right-continuous. We built 2 out of these 4 models to
distinguish two types of lexicalized re-ordering of these
ATs:
The left-monotone model computes the total proba-
bility of all ATs being left monotone: where the lower
left corner of the AT touches the upper right corner of the
previous AT. Note that the first word in the current AT
may or may not immediately follow the last word in the
previous AT. The total probability is the product over all
alignment templates i, either P (ATi is left-monotone) or
1 ? P (ATi is left-monotone).
The right-continuous model computes the total prob-
ability of all ATs being right continuous: where the
lower left corner of the AT touches the upper right cor-
ner of the previous AT and the first word in the cur-
rent AT immediately follows the last word in the pre-
vious AT. The total probability is the product over all
alignment templates i, either P (ATi is right-continuous)
or 1 ? P (ATi is right-continuous).
In both models, the probabilities P have been esti-
mated from the full training data (train).
5 Shallow Syntactic Feature Functions
By shallow syntax, we mean the output of the part-of-
speech tagger and chunkers. We hope that such features
can combine the strengths of tag- and chunk-based trans-
lation systems (Schafer and Yarowsky, 2003) with our
baseline system.
5.1 Projected POS Language Model
This feature uses Chinese POS tag sequences as surro-
gates for Chinese words to model movement. Chinese
words are too sparse to model movement, but an attempt
to model movement using Chinese POS may be more
successful. We hope that this feature will compensate for
a weak model of word movement in the baseline system.
Chinese POS sequences are projected to English us-
ing the word alignment. Relative positions are indicated
for each Chinese tag. The feature function was also tried
without the relative positions:
CD +0 M +1 NN +3 NN -1 NN +2 NN +3
14 (measure) open border cities
The table shows an example tagging of an English hy-
pothesis showing how it was generated from the Chinese
sentence. The feature function is the log probability out-
put by a trigram language model over this sequence. This
is similar to the HMM Alignment model (Vogel, Ney, and
Tillmann, 1996) but in this case movement is calculated
on the basis of parts of speech.
The Projected POS feature function was one of the
strongest performing shallow syntactic feature functions,
with a %BLEU score of 31.8. This feature function can
be thought of as a trade-off between purely word-based
models, and full generative models based upon shallow
syntax.
6 Tree-Based Feature Functions
Syntax-based MT has shown promise in the
work of, among others, Wu and Wong (1998) and
Alshawi, Bangalore, and Douglas (2000). We hope that
adding features based on Treebank-based syntactic
analyses of the source and target sentences will address
grammatical errors in the output of the baseline system.
6.1 Parse Tree Probability
The most straightforward way to integrate a statistical
parser in the system would be the use of the (log of the)
parser probability as a feature function. Unfortunately,
this feature function did not help to obtain better results
(it actually seems to significantly hurt performance).
To analyze the reason for this, we performed an ex-
periment to test if the used statistical parser assigns a
higher probability to presumably grammatical sentences.
The following table shows the average log probability as-
signed by the Collins parser to the 1-best (produced), or-
acle and the reference translations:
Hypothesis 1-best Oracle Reference
log(parseProb) -147.2 -148.5 -154.9
We observe that the average parser log-probability of
the 1-best translation is higher than the average parse
log probability of the oracle or the reference translations.
Hence, it turns out that the parser is actually assigning
higher probabilities to the ungrammatical MT output than
to the presumably grammatical human translations. One
reason for that is that the MT output uses fewer unseen
words and typically more frequent words which lead to
a higher language model probability. We also performed
experiments to balance this effect by dividing the parser
probability by the word unigram probability and using
this ?normalized parser probability? as a feature function,
but also this did not yield improvements.
6.2 Tree-to-String Alignment
A tree-to-string model is one of several syntax-
based translation models used. The model is a
conditional probability p(f |T (e)). Here, we used
a model defined by Yamada and Knight (2001) and
Yamada and Knight (2002).
Internally, the model performs three types of opera-
tions on each node of a parse tree. First, it reorders the
child nodes, such as changing VP ? VB NP PP into
VP ? NP PP VB. Second, it inserts an optional word at
each node. Third, it translates the leaf English words into
Chinese words. These operations are stochastic and their
probabilities are assumed to depend only on the node, and
are independent of other operations on the node, or other
nodes. The probability of each operation is automatically
obtained by a training algorithm, using about 780,000 En-
glish parse tree-Chinese sentence pairs. The probability
of these operations ?(eki,j) is assumed to depend on the
edge of the tree being modified, eki,j , but independent of
everything else, giving the following equation,
p(f |T (e)) =
?
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j) (4)
where ? varies over the possible alignments between the
f and e and ?(eki,j) is the particular operations (in ?) for
the edge eki,j .
The model is further extended to incorporate phrasal
translations performed at each node of the input parse
tree (Yamada and Knight, 2002). An English phrase cov-
ered by a node can be directly translated into a Chinese
phrase without regular reorderings, insertions, and leaf-
word translations.
The model was trained using about 780,000 English
parse tree-Chinese sentence pairs. There are about 3 mil-
lion words on the English side, and they were parsed by
Collins? parser.
Since the model is computationally expensive, we
added some limitations on the model operations. As the
base MT system does not produce a translation with a
big word jump, we restrict the model not to reorder child
nodes when the node covers more than seven words. For
a node that has more than four children, the reordering
probability is set to be uniform. We also introduced prun-
ing, which discards partial (subtree-substring) alignments
if the probability is lower than a threshold.
The model gives a sum of all possible alignment prob-
abilities for a pair of a Chinese sentence and an English
parse tree. We also calculate the probability of the best
alignment according to the model. Thus, we have the fol-
lowing two feature functions:
hTreeToStringSum(e, f) = log(
?
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j))
hTreeToStringViterbi(e, f) = log(max
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j))
As the model is computationally expensive, we sorted the
n-best list by the sentence length, and processed them
from the shorter ones to the longer ones. We used 10
CPUs for about five days, and 273/997 development sen-
tences and 237/878 test sentences were processed.
The average %BLEU score (average of the best four
among different 20 search initial points) was 31.7 for
both hTreeToStringSum and hTreeToStringViterbi. Among the pro-
cessed development sentences, the model preferred the
oracle sentences over the produced sentence in 61% of
the cases.
The biggest problem of this model is that it is compu-
tationally very expensive. It processed less than 30% of
the n-best lists in long CPU hours. In addition, we pro-
cessed short sentences only. For long sentences, it is not
practical to use this model as it is.
6.3 Tree-to-Tree Alignment
A tree-to-tree translation model makes use of syntac-
tic tree for both the source and target language. As in
the tree-to-string model, a set of operations apply, each
with some probability, to transform one tree into another.
However, when training the model, trees for both the
source and target languages are provided, in our case
from the Chinese and English parsers.
We began with the tree-to-tree alignment model pre-
sented by Gildea (2003). The model was extended to han-
dle dependency trees, and to make use of the word-level
alignments produced by the baseline MT system. The
probability assigned by the tree-to-tree alignment model,
given the word-level alignment with which the candidate
translation was generated, was used as a feature in our
rescoring system.
We trained the parameters of the tree transformation
operations on 42,000 sentence pairs of parallel Chinese-
English data from the Foreign Broadcast Information Ser-
vice (FBIS) corpus. The lexical translation probabili-
ties Pt were trained using IBM Model 1 on the 30 mil-
lion word training corpus. This was done to overcome
the sparseness of the lexical translation probabilities es-
timated while training the tree-to-tree model, which was
not able to make use of as much training data.
As a test of the tree-to-tree model?s discrimination, we
performed an oracle experiment, comparing the model
scores on the first sentence in the n-best list with candi-
date giving highest BLEU score. On the 1000-best list for
the 993-sentence development set, restricting ourselves
to sentences with no more than 60 words and a branching
factor of no more than five in either the Chinese or En-
glish tree, we achieved results for 480, or 48% of the 993
sentences. Of these 480, the model preferred the pro-
duced over the oracle 52% of the time, indicating that
it does not in fact seem likely to significantly improve
BLEU scores when used for reranking. Using the prob-
ability of the source Chinese dependency parse aligning
with the n-best hypothesis dependency parse as a feature
function, making use of the word-level alignments, yields
a 31.6 %BLEU score ? identical to our baseline.
6.4 Markov Assumption for Tree Alignments
The tree-based feature functions described so far have the
following limitations: full parse tree models are expen-
sive to compute for long sentences and for trees with flat
constituents and there is limited reordering observed in
the n-best lists that form the basis of our experiments. In
addition to this, higher levels of parse tree are rarely ob-
served to be reordered between source and target parse
trees.
In this section we attack these problems using a simple
Markov model for tree-based alignments. It guarantees
tractability: compared to a coverage of approximately
30% of the n-best list by the unconstrained tree-based
models, using the Markov model approach provides 98%
coverage of the n-best list. In addition, this approach is
robust to inaccurate parse trees.
The algorithm works as follows: we start with word
alignments and two parameters: n for maximum number
of words in tree fragment and k for maximum height of
tree fragment. We proceed from left to right in the Chi-
nese sentence and incrementally grow a pair of subtrees,
one subtree in Chinese and the other in English, such that
each word in the Chinese subtree is aligned to a word in
the English subtree. We grow this pair of subtrees un-
til we can no longer grow either subtree without violat-
ing the two parameter values n and k. Note that these
aligned subtree pairs have properties similar to alignment
templates. They can rearrange in complex ways between
source and target. Figure 2 shows how subtree-pairs for
parameters n = 3 and k = 3 can be drawn for this
sentence pair. In our experiments, we use substantially
bigger tree fragments with parameters set to n = 8 and
k = 9.
Once these subtree-pairs have been obtained, we can
easily assert a Markov assumption for the tree-to-tree and
tree-to-string translation models that exploits these pair-
ings. Let consider a sentence pair in which we have dis-
covered n subtree-pairs which we can call Frag0, . . .,
Fragn. We can then compute a feature function for the
sentence pair using the tree-to-string translation model as
follows:
hMarkovTreeToString =
logPtree-to-string(Frag0) + . . . + logPtree-to-string(Fragn)
Using this Markov assumption on tree alignments with
Figure 2: Markov assumption on tree alignments.
the Tree to String model described in Section 6.2 we ob-
tain a coverage improvement to 98% coverage from the
original 30%. The accuracy of the tree to string model
also improved with a %BLEU score of 32.0 which is the
best performing single syntactic feature.
6.5 Using TAG elementary trees for scoring word
alignments
In this section, we consider another method for carving
up the full parse tree. However, in this method, instead of
subtree-pairs we consider a decomposition of parse trees
that provides each word with a fragment of the original
parse tree as shown in Figure 3. The formalism of Tree-
Adjoining Grammar (TAG) provides the definition what
each tree fragment should be and in addition how to de-
compose the original parse trees to provide the fragments.
Each fragment is a TAG elementary tree and the compo-
sition of these TAG elementary trees in a TAG deriva-
tion tree provides the decomposition of the parse trees.
The decomposition into TAG elementary trees is done by
augmenting the parse tree for source and target sentence
with head-word and argument (or complement) informa-
tion using heuristics that are common to most contempo-
rary statistical parsers and easily available for both En-
glish and Chinese. Note that we do not use the word
alignment information for the decomposition into TAG
elementary trees.
Once we have a TAG elementary tree per word,
we can create several models that score word align-
ments by exploiting the alignments between TAG ele-
mentary trees between source and target. Let tfi and
tei be the TAG elementary trees associated with the
aligned words fi and ei respectively. We experimented
with two models over alignments: unigram model over
alignments:
?
i P (fi, tfi , ei, tei) and conditional model:?
i P (ei, tei | fi, tfi) ? P (fi+1, tfi+1 | fi, tfi)
We trained both of these models using the SRI Lan-
guage Modeling Toolkit using 60K aligned parse trees.
We extracted 1300 TAG elementary trees each for Chi-
Figure 3: Word alignments with TAG elementary trees.
nese and for English. The unigram model gets a %BLEU
score of 31.7 and the conditional model gets a %BLEU
score of 31.9.
%BLEU
Baseline 31.6
IBM Model 1 p(f |e) 32.5
Tree-to-String Markov fragments 32.0
Right-continuous alignment template 32.0
TAG conditional bigrams 31.9
Left-monotone alignment template 31.9
Projected POS LM 31.8
Tree-to-String 31.7
TAG unigram 31.7
Tree-to-Tree 31.6
combination 32.9
Table 2: Results for the baseline features, each new fea-
ture added to the baseline features on its own, and a com-
bination of new features.
7 Conclusions
The use of discriminative reranking of an n-best list pro-
duced with a state-of-the-art statistical MT system al-
lowed us to rapidly evaluate the benefits of off-the-shelf
parsers, chunkers, and POS taggers for improving syntac-
tic well-formedness of the MT output. Results are sum-
marized in Table 2; the best single new feature improved
the %BLEU score from 31.6 to 32.5. The 95% confi-
dence intervals computed with the bootstrap resampling
method are about 0.8%. In addition to experiments with
single features we also integrated multiple features using
a greedy approach where we integrated at each step the
feature that most improves the BLEU score. This feature
integration produced a statistically significant improve-
ment of absolute 1.3% to 32.9 %BLEU score.
Our single best feature, and in fact the only single fea-
ture to produce a truly significant improvement, was the
IBM Model 1 score. We attribute its success that it ad-
dresses the weakness of the baseline system to omit con-
tent words and that it improves word selection by em-
ploying a triggering effect. We hypothesize that this al-
lows for better use of context in, for example, choosing
among senses of the source language word.
A major goal of this work was to find out if we can ex-
ploit annotated data such as treebanks for Chinese and
English and make use of state-of-the-art deep or shal-
low parsers to improve MT quality. Unfortunately, none
of the implemented syntactic features achieved a statisti-
cally significant improvement in the BLEU score. Poten-
tial reasons for this might be:
? As described in Section 3.2, the use of off-the-shelf
taggers and parsers has various problems due to vari-
ous mismatches between the parser training data and
our application domain. This might explain that the
use of the parser probability as feature function was
not successful. A potential improvement might be to
adapt the parser by retraining it on the full training
data that has been used by the baseline system.
? The use of a 1000-best list limits the potential im-
provements. It is possible that more improvements
could be obtained using a larger n-best list or a word
graph representation of the candidates.
? The BLEU score is possibly not sufficiently sensi-
tive to the grammaticality of MT output. This could
not only make it difficult to see an improvement in
the system?s output, but also potentially mislead the
BLEU-based optimization of the feature weights. A
significantly larger corpus for discriminative train-
ing and for evaluation would yield much smaller
confidence intervals.
? Our discriminative training technique, which di-
rectly optimizes the BLEU score on a development
corpus, seems to have overfitting problems with
large number of features. One could use a larger de-
velopment corpus for discriminative training or in-
vestigate alternative discriminative training criteria.
? The amount of annotated data that has been used
to train the taggers and parsers is two orders of
magnitude smaller than the parallel training data
that has been used to train the baseline system (or
the word-based features). Possibly, a comparable
amount of annotated data (e.g. a treebank with 100
million words) is needed to obtain significant im-
provements.
This is the first large scale integration of syntactic analy-
sis operating on many different levels with a state-of-the-
art phrase-based MT system. The methodology of using
a log-linear feature combination approach, discriminative
reranking of n-best lists computed with a state-of-the-art
baseline system allowed members of a large team to si-
multaneously experiment with hundreds of syntactic fea-
ture functions on a common platform.
Acknowledgments
This material is based upon work supported by the Na-
tional Science Foundation under Grant No. 0121285.
References
Alshawi, Hiyan, Srinivas Bangalore, and Shona Douglas. 2000.
Learning dependency translation models as collections of
finite state head transducers. Computational Linguistics,
26(1):45?60.
Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
Gildea, Daniel. 2003. Loosely tree-based alignment for ma-
chine translation. In Proc. of the 41st Annual Meeting of the
Association for Computational Linguistics (ACL), Sapporo,
Japan.
Ney, Hermann, M. Generet, and Frank Wessel. 1995. Ex-
tensions of absolute discounting for language modeling. In
Proc. of the Fourth European Conf. on Speech Communica-
tion and Technology, Madrid, Spain.
Och, Franz Josef. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. of the 41st Annual Meeting
of the Association for Computational Linguistics (ACL), Sap-
poro, Japan.
Och, Franz Josef and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical ma-
chine translation. In Proc. of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL), Philadel-
phia, PA.
Och, Franz Josef and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Computa-
tional Linguistics. Accepted for Publication.
Och, Franz Josef, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical machine
translation. In Proc. of the Joint SIGDAT Conf. on Empiri-
cal Methods in Natural Language Processing and Very Large
Corpora, College Park, MD.
Schafer, Charles and David Yarowsky. 2003. Statistical ma-
chine translation using coercive two-level syntactic transduc-
tion. In Proc. of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), Philadel-
phia, PA.
Vogel, Stephan, Hermann Ney, and Christoph Tillmann. 1996.
HMM-based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational Lin-
guistics, Copenhagen, Denmark.
Wu, Dekai and H. Wong. 1998. Machine translation with a
stochastic grammatical channel. In COLING-ACL ?98: 36th
Annual Meeting of the Association for Computational Lin-
guistics and 17th Int. Conf. on Computational Linguistics,
Montreal, Canada.
Yamada, Kenji and Kevin Knight. 2001. A syntax-based sta-
tistical translation model. In Proc. of the 39th Annual Meet-
ing of the Association for Computational Linguistics (ACL),
Toulouse, France.
Yamada, Kenji and Kevin Knight. 2002. A decoder for syntax-
based MT. In Proc. of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), Philadelphia,
PA.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 256?263,
New York, June 2006. c?2006 Association for Computational Linguistics
Synchronous Binarization for Machine Translation
Hao Zhang
Computer Science Department
University of Rochester
Rochester, NY 14627
zhanghao@cs.rochester.edu
Liang Huang
Dept. of Computer & Information Science
University of Pennsylvania
Philadelphia, PA 19104
lhuang3@cis.upenn.edu
Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
gildea@cs.rochester.edu
Kevin Knight
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
knight@isi.edu
Abstract
Systems based on synchronous grammars
and tree transducers promise to improve
the quality of statistical machine transla-
tion output, but are often very computa-
tionally intensive. The complexity is ex-
ponential in the size of individual gram-
mar rules due to arbitrary re-orderings be-
tween the two languages, and rules ex-
tracted from parallel corpora can be quite
large. We devise a linear-time algorithm
for factoring syntactic re-orderings by bi-
narizing synchronous rules when possible
and show that the resulting rule set signif-
icantly improves the speed and accuracy
of a state-of-the-art syntax-based machine
translation system.
1 Introduction
Several recent syntax-based models for machine
translation (Chiang, 2005; Galley et al, 2004) can
be seen as instances of the general framework of
synchronous grammars and tree transducers. In this
framework, both alignment (synchronous parsing)
and decoding can be thought of as parsing problems,
whose complexity is in general exponential in the
number of nonterminals on the right hand side of a
grammar rule. To alleviate this problem, we investi-
gate bilingual binarization to factor the synchronous
grammar to a smaller branching factor, although it is
not guaranteed to be successful for any synchronous
rule with arbitrary permutation. In particular:
? We develop a technique called synchronous bi-
narization and devise a fast binarization algo-
rithm such that the resulting rule set alows ef-
ficient algorithms for both synchronous parsing
and decoding with integrated n-gram language
models.
? We examine the effect of this binarization
method on end-to-end machine translation
quality, compared to a more typical baseline
method.
? We examine cases of non-binarizable rules in a
large, empirically-derived rule set, and we in-
vestigate the effect on translation quality when
excluding such rules.
Melamed (2003) discusses binarization of multi-
text grammars on a theoretical level, showing the
importance and difficulty of binarization for efficient
synchronous parsing. One way around this diffi-
culty is to stipulate that all rules must be binary
from the outset, as in inversion-transduction gram-
mar (ITG) (Wu, 1997) and the binary synchronous
context-free grammar (SCFG) employed by the Hi-
ero system (Chiang, 2005) to model the hierarchical
phrases. In contrast, the rule extraction method of
Galley et al (2004) aims to incorporate more syn-
tactic information by providing parse trees for the
target language and extracting tree transducer rules
that apply to the parses. This approach results in
rules with many nonterminals, making good bina-
rization techniques critical.
Suppose we have the following SCFG, where su-
perscripts indicate reorderings (formal definitions of
256
S
NP
Baoweier
PP
yu
Shalong
VP
juxing le
huitan
S
NP
Powell
VP
held
a meeting
PP
with
Sharon
Figure 1: A pair of synchronous parse trees in the
SCFG (1). The dashed curves indicate pairs of syn-
chronous nonterminals (and sub trees).
SCFGs can be found in Section 2):
(1)
S? NP(1) VP(2) PP(3), NP(1) PP(3) VP(2)
NP? Powell, Baoweier
VP? held a meeting, juxing le huitan
PP? with Sharon, yu Shalong
Decoding can be cast as a (monolingual) parsing
problem since we only need to parse the source-
language side of the SCFG, as if we were construct-
ing a CFG projected on Chinese out of the SCFG.
The only extra work we need to do for decoding
is to build corresponding target-language (English)
subtrees in parallel. In other words, we build syn-
chronous trees when parsing the source-language in-
put, as shown in Figure 1.
To efficiently decode with CKY, we need to bi-
narize the projected CFG grammar.1 Rules can be
binarized in different ways. For example, we could
binarize the first rule left to right or right to left:
S? VNP-PP VP
VNP-PP? NP PP or
S? NP VPP-VP
VPP-VP ? PP VP
We call those intermediate symbols (e.g. VPP-VP) vir-
tual nonterminals and corresponding rules virtual
rules, whose probabilities are all set to 1.
These two binarizations are no different in the
translation-model-only decoding described above,
just as in monolingual parsing. However, in the
source-channel approach to machine translation, we
need to combine probabilities from the translation
model (an SCFG) with the language model (an n-
gram), which has been shown to be very impor-
tant for translation quality (Chiang, 2005). To do
bigram-integrated decoding, we need to augment
each chart item (X, i, j) with two target-language
1Other parsing strategies like the Earley algorithm use an
internal binary representation (e.g. dotted-rules) of the original
grammar to ensure cubic time complexity.
boundary words u and v to produce a bigram-item
like
( u ??? vX
i j
)
, following the dynamic program-
ming algorithm of Wu (1996).
Now the two binarizations have very different ef-
fects. In the first case, we first combine NP with PP:
( Powell ??? PowellNP
1 2
)
: p
( with ??? SharonPP
2 4
)
: q
( Powell ??? Powell ??? with ??? Sharon
VNP-PP
1 4
)
: pq
where p and q are the scores of antecedent items.
This situation is unpleasant because in the target-
language NP and PP are not contiguous so we can-
not apply language model scoring when we build the
VNP-PP item. Instead, we have to maintain all fourboundary words (rather than two) and postpone the
language model scoring till the next step where VNP-PP
is combined with ( held ??? meetingVP
2 4
) to form an S item.
We call this binarization method monolingual bina-
rization since it works only on the source-language
projection of the rule without respecting the con-
straints from the other side.
This scheme generalizes to the case where we
have n nonterminals in a SCFG rule, and the decoder
conservatively assumes nothing can be done on lan-
guage model scoring (because target-language spans
are non-contiguous in general) until the real nonter-
minal has been recognized. In other words, target-
language boundary words from each child nonter-
minal of the rule will be cached in all virtual non-
terminals derived from this rule. In the case of
m-gram integrated decoding, we have to maintain
2(m ? 1) boundary words for each child nontermi-
nal, which leads to a prohibitive overall complex-
ity of O(|w|3+2n(m?1)), which is exponential in rule
size (Huang et al, 2005). Aggressive pruning must
be used to make it tractable in practice, which in
general introduces many search errors and adversely
affects translation quality.
In the second case, however:
( with ??? SharonPP
2 4
)
: r
( held ??? meetingVP
4 7
)
: s
( held ??? Sharon
VPP-VP
2 7
)
: rs ? Pr(with | meeting)
Here since PP and VP are contiguous (but
swapped) in the target-language, we can include the
257
NP
NP
PP
VP
VP
PP
target (English)
source (Chinese)
VPP-VP
NP
PP
VP
Chinese indices
English
boundary
w
o
rds 1 2 4 7Powell
Powellheld
meetingwith
Sharon
VPP-VP
Figure 2: The alignment pattern (left) and alignment
matrix (right) of the synchronous production.
language model score by adding Pr(with | meeting),
and the resulting item again has two boundary
words. Later we add Pr(held | Powell) when the
resulting item is combined with ( Powell ??? PowellNP
1 2
) to
form an S item. As illustrated in Figure 2, VPP-VP hascontiguous spans on both source and target sides, so
that we can generate a binary-branching SCFG:
(2) S? NP(1) VPP-VP(2), NP(1) VPP-VP(2)VPP-VP ? VP(1) PP(2), PP(2) VP(1)
In this case m-gram integrated decoding can be
done in O(|w|3+4(m?1)) time which is much lower-
order polynomial and no longer depends on rule size
(Wu, 1996), allowing the search to be much faster
and more accurate facing pruning, as is evidenced in
the Hiero system of Chiang (2005) where he restricts
the hierarchical phrases to be a binary SCFG. The
benefit of binary grammars also lies in synchronous
parsing (alignment). Wu (1997) shows that parsing
a binary SCFG is in O(|w|6) while parsing SCFG is
NP-hard in general (Satta and Peserico, 2005).
The same reasoning applies to tree transducer
rules. Suppose we have the following tree-to-string
rules, following Galley et al (2004):
(3)
S(x0:NP, VP(x2:VP, x1:PP))? x0 x1 x2NP(NNP(Powell))? Baoweier
VP(VBD(held), NP(DT(a) NPS(meeting)))
? juxing le huitan
PP(TO(with), NP(NNP(Sharon)))? yu Shalong
where the reorderings of nonterminals are denoted
by variables xi.Notice that the first rule has a multi-level left-
hand side subtree. This system can model non-
isomorphic transformations on English parse trees
to ?fit? another language, for example, learning that
the (S (V O)) structure in English should be trans-
formed into a (V S O) structure in Arabic, by look-
ing at two-level tree fragments (Knight and Graehl,
2005). From a synchronous rewriting point of view,
this is more akin to synchronous tree substitution
grammar (STSG) (Eisner, 2003). This larger locality
is linguistically motivated and leads to a better pa-
rameter estimation. By imagining the left-hand-side
trees as special nonterminals, we can virtually cre-
ate an SCFG with the same generative capacity. The
technical details will be explained in Section 3.2.
In general, if we are given an arbitrary syn-
chronous rule with many nonterminals, what are the
good decompositions that lead to a binary grammar?
Figure 2 suggests that a binarization is good if ev-
ery virtual nonterminal has contiguous spans on both
sides. We formalize this idea in the next section.
2 Synchronous Binarization
A synchronous CFG (SCFG) is a context-free
rewriting system for generating string pairs. Each
rule (synchronous production) rewrites a nontermi-
nal in two dimensions subject to the constraint that
the sequence of nonterminal children on one side is
a permutation of the nonterminal sequence on the
other side. Each co-indexed child nonterminal pair
will be further rewritten as a unit.2 We define the
language L(G) produced by an SCFG G as the pairs
of terminal strings produced by rewriting exhaus-
tively from the start symbol.
As shown in Section 3.2, terminals do not play
an important role in binarization. So we now write
rules in the following notation:
X ? X(1)1 ...X(n)n , X
(pi(1))
pi(1) ...X
(pi(n))
pi(n)
where each Xi is a variable which ranges over non-terminals in the grammar and pi is the permutation
of the rule. We also define an SCFG rule as n-ary
if its permutation is of n and call an SCFG n-ary if
its longest rule is n-ary. Our goal is to produce an
equivalent binary SCFG for an input n-ary SCFG.
2In making one nonterminal play dual roles, we follow the
definitions in (Aho and Ullman, 1972; Chiang, 2005), origi-
nally known as Syntax Directed Translation Schema (SDTS).
An alternative definition by Satta and Peserico (2005) allows
co-indexed nonterminals taking different symbols in two di-
mensions. Formally speaking, we can construct an equivalent
SDTS by creating a cross-product of nonterminals from two
sides. See (Satta and Peserico, 2005, Sec. 4) for other details.
258
(2,3,5,4)
(2,3)
2 3
(5,4)
5 4
(2,3,5,4)
2 (3,5,4)
3 (5,4)
5 4
(a) (b) (c)
Figure 3: (a) and (b): two binarization patterns
for (2, 3, 5, 4). (c): alignment matrix for the non-
binarizable permuted sequence (2, 4, 1, 3)
However, not every SCFG can be binarized. In
fact, the binarizability of an n-ary rule is determined
by the structure of its permutation, which can some-
times be resistant to factorization (Aho and Ullman,
1972). So we now start to rigorously define the bi-
narizability of permutations.
2.1 Binarizable Permutations
A permuted sequence is a permutation of consec-
utive integers. For example, (3, 5, 4) is a permuted
sequence while (2, 5) is not. As special cases, single
numbers are permuted sequences as well.
A sequence a is said to be binarizable if it is a
permuted sequence and either
1. a is a singleton, i.e. a = (a), or
2. a can be split into two sub sequences, i.e.
a = (b; c), where b and c are both binarizable
permuted sequences. We call such a division
(b; c) a binarizable split of a.
This is a recursive definition. Each binarizable
permuted sequence has at least one hierarchical bi-
narization pattern. For instance, the permuted se-
quence (2, 3, 5, 4) is binarizable (with two possible
binarization patterns) while (2, 4, 1, 3) is not (see
Figure 3).
2.2 Binarizable SCFG
An SCFG is said to be binarizable if the permu-
tation of each synchronous production is binariz-
able. We denote the class of binarizable SCFGs as
bSCFG. This set represents an important subclass
of SCFG that is easy to handle (parsable in O(|w|6))
and covers many interesting longer-than-two rules.3
3Although we factor the SCFG rules individually and de-
fine bSCFG accordingly, there are some grammars (the dashed
SCFG bSCFG SCFG-2
O(|w|6) parsable
Figure 4: Subclasses of SCFG. The thick arrow de-
notes the direction of synchronous binarization. For
clarity reasons, binary SCFG is coded as SCFG-2.
Theorem 1. For each grammar G in bSCFG, there
exists a binary SCFG G?, such that L(G?) = L(G).
Proof. Once we decompose the permutation of n
in the original rule into binary permutations, all
that remains is to decorate the skeleton binary parse
with nonterminal symbols and attach terminals to
the skeleton appropriately. We explain the technical
details in the next section.
3 Binarization Algorithms
We have reduced the problem of binarizing an SCFG
rule into the problem of binarizing its permutation.
This problem can be cast as an instance of syn-
chronous ITG parsing (Wu, 1997). Here the parallel
string pair that we are parsing is the integer sequence
(1...n) and its permutation (pi(1)...pi(n)). The goal
of the ITG parsing is to find a synchronous tree that
agrees with the alignment indicated by the permu-
tation. In fact, as demonstrated previously, some
permutations may have more than one binarization
patterns among which we only need one. Wu (1997,
Sec. 7) introduces a non-ambiguous ITG that prefers
left-heavy binary trees so that for each permutation
there is a unique synchronous derivation (binariza-
tion pattern).
However, this problem has more efficient solu-
tions. Shapiro and Stephens (1991, p. 277) infor-
mally present an iterative procedure where in each
pass it scans the permuted sequence from left to right
and combines two adjacent sub sequences whenever
possible. This procedure produces a left-heavy bi-
narization tree consistent with the unambiguous ITG
and runs in O(n2) time since we need n passes in the
worst case. We modify this procedure and improve
circle in Figure 4), which can be binarized only by analyzing
interactions between rules. Below is a simple example:
S? X(1) X(2) X(3) X(4), X(2) X(4) X(1) X(3)
X? a , a
259
iteration stack input action
1 5 3 4 2
1 5 3 4 2 shift
1 1 5 3 4 2 shift
2 1 5 3 4 2 shift
3 1 5 3 4 2 shift
1 5 3-4 2 reduce [3, 4]
1 3-5 2 reduce ?5, [3, 4]?
4 1 3-5 2 shift
1 2-5 reduce ?2, ?5, [3, 4]??
1-5 reduce [1, ?2, ?5, [3, 4]??]
Figure 5: Example of Algorithm 1 on the input
(1, 5, 3, 4, 2). The rightmost column shows the
binarization-trees generated at each reduction step.
it into a linear-time shift-reduce algorithm that only
needs one pass through the sequence.
3.1 The linear-time skeleton algorithm
The (unique) binarization tree bi(a) for a binariz-
able permuted sequence a is recursively defined as
follows:
? if a = (a), then bi(a) = a;
? otherwise let a = (b; c) to be the rightmost
binarizable split of a. then
bi(a) =
{
[bi(b), bi(c)] b1 < c1
?bi(b), bi(c)? b1 > c1.
For example, the binarization tree for (2, 3, 5, 4)
is [[2, 3], ?5, 4?], which corresponds to the binariza-
tion pattern in Figure 3(a). We use [] and ?? for
straight and inverted combinations respectively, fol-
lowing the ITG notation (Wu, 1997). The rightmost
split ensures left-heavy binary trees.
The skeleton binarization algorithm is an instance
of the widely used left-to-right shift-reduce algo-
rithm. It maintains a stack for contiguous subse-
quences discovered so far, like 2-5, 1. In each it-
eration, it shifts the next number from the input and
repeatedly tries to reduce the top two elements on
the stack if they are consecutive. See Algorithm 1
for details and Figure 5 for an example.
Theorem 2. Algorithm 1 succeeds if and only if the
input permuted sequence a is binarizable, and in
case of success, the binarization pattern recovered
is the binarization tree of a.
Proof. ?: it is obvious that if the algorithm suc-
ceeds then a is binarizable using the binarization
pattern recovered.
?: by a complete induction on n, the length of a.
Base case: n = 1, trivial.
Assume it holds for all n? < n.
If a is binarizable, then let a = (b; c) be its right-
most binarizable split. By the induction hypothesis,
the algorithm succeeds on the partial input b, reduc-
ing it to the single element s[0] on the stack and re-
covering its binarization tree bi(b).
Let c = (c1; c2). If c1 is binarizable and trig-gers our binarizer to make a straight combination
of (b; c1), based on the property of permutations, itmust be true that (c1; c2) is a valid straight concate-nation. We claim that c2 must be binarizable in thissituation. So, (b, c1; c2) is a binarizable split to theright of the rightmost binarizable split (b; c), which
is a contradiction. A similar contradiction will arise
if b and c1 can make an inverted concatenation.
Therefore, the algorithm will scan through the
whole c as if from the empty stack. By the in-
duction hypothesis again, it will reduce c into s[1]
on the stack and recover its binarization tree bi(c).
Since b and c are combinable, the algorithm re-
duces s[0] and s[1] in the last step, forming the bi-
narization tree for a, which is either [bi(b), bi(c)] or
?bi(b), bi(c)?.
The running time of Algorithm 1 is linear in n, the
length of the input sequence. This is because there
are exactly n shifts and at most n?1 reductions, and
each shift or reduction takes O(1) time.
3.2 Binarizing tree-to-string transducers
Without loss of generality, we have discussed how
to binarize synchronous productions involving only
nonterminals through binarizing the corresponding
skeleton permutations. We still need to tackle a few
technical problems in the actual system.
First, we are dealing with tree-to-string trans-
ducer rules. We view each left-hand side subtree
as a monolithic nonterminal symbol and factor each
transducer rule into two SCFG rules: one from
the root nonterminal to the subtree, and the other
from the subtree to the leaves. In this way we can
uniquely reconstruct the tree-to-string derivation us-
ing the two-step SCFG derivation. For example,
260
Algorithm 1 The Linear-time Binarization Algorithm
1: function BINARIZABLE(a)
2: top? 0 . stack top pointer
3: PUSH(a1, a1) . initial shift4: for i? 2 to |a| do . for each remaining element
5: PUSH(ai, ai) . shift6: while top > 1 and CONSECUTIVE(s[top], s[top? 1]) do . keep reducing if possible
7: (p, q)? COMBINE(s[top], s[top? 1])
8: top? top? 2
9: PUSH(p, q)
10: return (top = 1) . if reduced to a single element then the input is binarizable, otherwise not
11: function CONSECUTIVE((a, b), (c, d))
12: return (b = c? 1) or (d = a? 1) . either straight or inverted
13: function COMBINE((a, b), (c, d))
14: return (min(a, c), max(b, d))
consider the following tree-to-string rule:
ADJP
x0:RB JJ
responsible
PP
IN
for
NP-C
NPB
DT
the
x2:NN
x1:PP
? x0 fuze x1 de x2
We create a specific nonterminal, say, T859, whichis a unique identifier for the left-hand side subtree
and generate the following two SCFG rules:
ADJP ? T859 (1), T859 (1)
T859 ? RB
(1) resp. for the NN(2) PP(3),
RB(1) fuze PP(3) de NN(2)
Second, besides synchronous nonterminals, ter-
minals in the two languages can also be present, as
in the above example. It turns out we can attach the
terminals to the skeleton parse for the synchronous
nonterminal strings quite freely as long as we can
uniquely reconstruct the original rule from its binary
parse tree. In order to do so we need to keep track of
sub-alignments including both aligned nonterminals
and neighboring terminals.
When binarizing the second rule above, we first
run the skeleton algorithm to binarize the under-
lying permutation (1, 3, 2) to its binarization tree
[1, ?3, 2?]. Then we do a post-order traversal to the
skeleton tree, combining Chinese terminals (one at
a time) at the leaf nodes and merging English termi-
nals greedily at internal nodes:
[1, ?3, 2?]
1 ?3, 2?
3 2
?
T859 [1,?3,2?]
V[RB, fuze]1
RB fuze
V?V[PP, de], resp. for the NN??3,2?
V[PP, de]3
PP de
NN2
A pre-order traversal of the decorated binarization
tree gives us the following binary SCFG rules:
T859 ? V1(1) V2(2), V1(1) V2(2)
V1 ? RB(1), RB(1) fuze
V2 ? resp. for the NN(1) V(2)3 , V(2)3 NN(1)V3 ? PP(1), PP(1) de
where the virtual nonterminals are:
V1: V[RB, fuze]V2: V?V[PP, de], resp. for the NN?V3: V[PP, de]
Analogous to the ?dotted rules? in Earley pars-
ing for monolingual CFGs, the names we create
for the virtual nonterminals reflect the underlying
sub-alignments, ensuring intermediate states can be
shared across different tree-to-string rules without
causing ambiguity.
The whole binarization algorithm still runs in time
linear in the number of symbols in the rule (includ-
ing both terminals and nonterminals).
4 Experiments
In this section, we answer two empirical questions.
261
 0
 2e+06
 4e+06
 6e+06
 8e+06
 1e+07
 0  5  10  15  20  25  30  35  40
 0
 20
 40
 60
 80
 100
# 
of
 ru
le
s
pe
rc
en
ta
ge
 (%
)
length
Figure 6: The solid-line curve represents the distribution of all rules against permutation lengths. The
dashed-line stairs indicate the percentage of non-binarizable rules in our initial rule set while the dotted-line
denotes that percentage among all permutations.
4.1 How many rules are binarizable?
It has been shown by Shapiro and Stephens (1991)
and Wu (1997, Sec. 4) that the percentage of binariz-
able cases over all permutations of length n quickly
approaches 0 as n grows (see Figure 6). However,
for machine translation, it is more meaningful to
compute the ratio of binarizable rules extracted from
real text. Our rule set is obtained by first doing word
alignment using GIZA++ on a Chinese-English par-
allel corpus containing 50 million words in English,
then parsing the English sentences using a variant
of Collins parser, and finally extracting rules using
the graph-theoretic algorithm of Galley et al (2004).
We did a ?spectrum analysis? on the resulting rule
set with 50,879,242 rules. Figure 6 shows how the
rules are distributed against their lengths (number
of nonterminals). We can see that the percentage
of non-binarizable rules in each bucket of the same
length does not exceed 25%. Overall, 99.7% of
the rules are binarizable. Even for the 0.3% non-
binarizable rules, human evaluations show that the
majority of them are due to alignment errors. It is
also interesting to know that 86.8% of the rules have
monotonic permutations, i.e. either taking identical
or totally inverted order.
4.2 Does synchronous binarizer help decoding?
We did experiments on our CKY-based decoder with
two binarization methods. It is the responsibility of
the binarizer to instruct the decoder how to compute
the language model scores from children nontermi-
nals in each rule. The baseline method is mono-
lingual left-to-right binarization. As shown in Sec-
tion 1, decoding complexity with this method is ex-
ponential in the size of the longest rule and since we
postpone all the language model scorings, pruning
in this case is also biased.
system bleu
monolingual binarization 36.25
synchronous binarization 38.44
alignment-template system 37.00
Table 1: Syntax-based systems vs. ATS
To move on to synchronous binarization, we first
did an experiment using the above baseline system
without the 0.3% non-binarizable rules and did not
observe any difference in BLEU scores. So we
safely move a step further, focusing on the binariz-
able rules only.
The decoder now works on the binary translation
rules supplied by an external synchronous binarizer.
As shown in Section 1, this results in a simplified de-
coder with a polynomial time complexity, allowing
less aggressive and more effective pruning based on
both translation model and language model scores.
We compare the two binarization schemes in
terms of translation quality with various pruning
thresholds. The rule set is that of the previous sec-
tion. The test set has 116 Chinese sentences of no
longer than 15 words. Both systems use trigram as
the integrated language model. Figure 7 demon-
strates that decoding accuracy is significantly im-
proved after synchronous binarization. The number
of edges proposed during decoding is used as a mea-
sure of the size of search space, or time efficiency.
Our system is consistently faster and more accurate
than the baseline system.
We also compare the top result of our syn-
chronous binarization system with the state-of-the-
art alignment-template approach (ATS) (Och and
Ney, 2004). The results are shown in Table 1. Our
system has a promising improvement over the ATS
262
 33.5
 34.5
 35.5
 36.5
 37.5
 38.5
 3e+09  4e+09  5e+09  6e+09  7e+09
bl
eu
 s
co
re
s
# of edges proposed during decoding
synchronous binarization
monolingual binarization
Figure 7: Comparing the two binarization methods
in terms of translation quality against search effort.
system which is trained on a larger data-set but tuned
independently.
5 Conclusion
Modeling reorderings between languages has been a
major challenge for machine translation. This work
shows that the majority of syntactic reorderings, at
least between languages like English and Chinese,
can be efficiently decomposed into hierarchical bi-
nary reorderings. From a modeling perspective, on
the other hand, it is beneficial to start with a richer
representation that has more transformational power
than ITG or binary SCFG. Our work shows how to
convert it back to a computationally friendly form
without harming much of its expressiveness. As a
result, decoding with n-gram models can be fast and
accurate, making it possible for our syntax-based
system to overtake a comparable phrase-based sys-
tem in BLEU score. We believe that extensions of
our technique to more powerful models such as syn-
chronous tree-adjoining grammar (Shieber and Sch-
abes, 1990) is an interesting area for further work.
Acknowledgments Much of this work was done
when H. Zhang and L. Huang were visiting
USC/ISI. The authors wish to thank Wei Wang,
Jonathan Graehl and Steven DeNeefe for help with
the experiments. We are also grateful to Daniel
Marcu, Giorgio Satta, and Aravind Joshi for discus-
sions. This work was partially supported by NSF
ITR IIS-09325646 and NSF ITR IIS-0428020.
References
Albert V. Aho and Jeffery D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume 1.
Prentice-Hall, Englewood Cliffs, NJ.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL-05, pages 263?270, Ann Arbor, Michigan.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of ACL-
03, companion volume, Sapporo, Japan.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT/NAACL-04.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005. Ma-
chine translation as lexicalized parsing with hooks. In
Proceedings of IWPT-05, Vancouver, BC.
Kevin Knight and Jonathan Graehl. 2005. An overview
of probabilistic tree transducers for natural language
processing. In Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLing). LNCS.
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of NAACL-03, Ed-
monton.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4).
Giorgio Satta and Enoch Peserico. 2005. Some computa-
tional complexity results for synchronous context-free
grammars. In Proceedings of HLT/EMNLP-05, pages
803?810, Vancouver, Canada, October.
L. Shapiro and A. B. Stephens. 1991. Bootstrap percola-
tion, the Schro?der numbers, and the n-kings problem.
SIAM Journal on Discrete Mathematics, 4(2):275?
280.
Stuart Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In COLING-90, volume III,
pages 253?258.
Dekai Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In 34th Annual Meeting
of the Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
263
Proceedings of NAACL HLT 2007, pages 41?48,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Source-Language Features and Maximum Correlation Training
for Machine Translation Evaluation
Ding Liu and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We propose three new features for MT
evaluation: source-sentence constrained
n-gram precision, source-sentence re-
ordering metrics, and discriminative un-
igram precision, as well as a method of
learning linear feature weights to directly
maximize correlation with human judg-
ments. By aligning both the hypothe-
sis and the reference with the source-
language sentence, we achieve better cor-
relation with human judgments than pre-
viously proposed metrics. We further
improve performance by combining indi-
vidual evaluation metrics using maximum
correlation training, which is shown to be
better than the classification-based frame-
work.
1 Introduction
Evaluation has long been a stumbling block in the
development of machine translation systems, due to
the simple fact that there are many correct trans-
lations for a given sentence. The most commonly
used metric, BLEU, correlates well over large test
sets with human judgments (Papineni et al, 2002),
but does not perform as well on sentence-level eval-
uation (Blatz et al, 2003). Later approaches to im-
prove sentence-level evaluation performance can be
summarized as falling into four types:
? Metrics based on common loose sequences of
MT outputs and references (Lin and Och, 2004;
Liu and Gildea, 2006). Such metrics were
shown to have better fluency evaluation per-
formance than metrics based on n-grams such
BLEU and NIST (Doddington, 2002).
? Metrics based on syntactic similarities such as
the head-word chain metric (HWCM) (Liu and
Gildea, 2005). Such metrics try to improve flu-
ency evaluation performance for MT, but they
heavily depend on automatic parsers, which are
designed for well-formed sentences and cannot
generate robust parse trees for MT outputs.
? Metrics based on word alignment between MT
outputs and the references (Banerjee and Lavie,
2005). Such metrics do well in adequacy evalu-
ation, but are not as good in fluency evaluation,
because of their unigram basis (Liu and Gildea,
2006).
? Combination of metrics based on machine
learning. Kulesza and Shieber (2004) used
SVMs to combine several metrics. Their
method is based on the assumption that
higher classification accuracy in discriminat-
ing human- from machine-generated transla-
tions will yield closer correlation with human
judgment. This assumption may not always
hold, particularly when classification is diffi-
cult. Lita et al (2005) proposed a log-linear
model to combine features, but they only did
preliminary experiments based on 2 features.
Following the track of previous work, to improve
evaluation performance, one could either propose
new metrics, or find more effective ways to combine
the metrics. We explore both approaches. Much
work has been done on computing MT scores based
41
on the pair of MT output/reference, and we aim to
investigate whether some other information could
be used in the MT evaluation, such as source sen-
tences. We propose two types of source-sentence
related features as well as a feature based on part of
speech. The three new types of feature can be sum-
marized as follows:
? Source-sentence constrained n-gram precision.
Overlapping n-grams between an MT hypothe-
sis and its references do not necessarily indicate
correct translation segments, since they could
correspond to different parts of the source sen-
tence. Thus our constrained n-gram precision
counts only overlapping n-grams in MT hy-
pothesis and reference which are aligned to the
same words in the source sentences.
? Source-sentence reordering agreement. With
the alignment information, we can compare the
reorderings of the source sentence in the MT
hypothesis and in its references. Such compar-
ison only considers the aligned positions of the
source words in MT hypothesis and references,
and thus is oriented towards evaluating the sen-
tence structure.
? Discriminative unigram precision. We divide
the normal n-gram precision into many sub-
precisions according to their part of speech
(POS). The division gives us flexibility to train
the weights of each sub-precision in frame-
works such as SVM and Maximum Correla-
tion Training, which will be introduced later.
The motivation behind such differentiation is
that different sub-precisions should have dif-
ferent importance in MT evaluation, e.g., sub-
precision of nouns, verbs, and adjectives should
be important for evaluating adequacy, and
sub-precision in determiners and conjunctions
should mean more in evaluating fluency.
Along the direction of feature combination, since
indirect weight training using SVMs, based on re-
ducing classification error, cannot always yield good
performance, we train the weights by directly opti-
mizing the evaluation performance, i.e., maximizing
the correlation with the human judgment. This type
of direct optimization is known as Minimum Error
Rate Training (Och, 2003) in the MT community,
and is an essential component in building the state-
of-art MT systems. It would seem logical to apply
similar methods to MT evaluation. What is more,
Maximum Correlation Training (MCT) enables us
to train the weights based on human fluency judg-
ments and adequacy judgments respectively, and
thus makes it possible to make a fluency-oriented or
adequacy-oriented metric. It surpasses previous MT
metrics? approach, where a a single metric evaluates
both fluency and adequacy. The rest of the paper is
organized as follows: Section 2 gives a brief recap of
n-gram precision-based metrics and introduces our
three extensions to them; Section 3 introduces MCT
for MT evaluation; Section 4 describes the experi-
mental results, and Section 5 gives our conclusion.
2 Three New Features for MT Evaluation
Since our source-sentence constrained n-gram preci-
sion and discriminative unigram precision are both
derived from the normal n-gram precision, it is
worth describing the original n-gram precision met-
ric, BLEU (Papineni et al, 2002). For every MT
hypothesis, BLEU computes the fraction of n-grams
which also appear in the reference sentences, as well
as a brevity penalty. The formula for computing
BLEU is shown below:
BLEU = BPN
N
X
n=1
P
C
P
ngram?C Countclip(ngram)
P
C
P
ngram??C Count(ngram?)
where C denotes the set of MT hypotheses.
Countclip(ngram) denotes the clipped number of
n-grams in the candidates which also appear in the
references. BP in the above formula denotes the
brevity penalty, which is set to 1 if the accumulated
length of the MT outputs is longer than the arith-
metic mean of the accumulated length of the refer-
ences, and otherwise is set to the ratio of the two.
For sentence-level evaluation with BLEU, we com-
pute the score based on each pair of MT hypothe-
sis/reference. Later approaches, as described in Sec-
tion 1, use different ways to manipulate the morpho-
logical similarity between the MT hypothesis and its
references. Most of them, except NIST, consider the
words in MT hypothesis as the same, i.e., as long as
the words in MT hypothesis appear in the references,
42
they make no difference to the metrics.1 NIST com-
putes the n-grams weights as the logarithm of the ra-
tio of the n-gram frequency and its one word lower
n-gram frequency. From our experiments, NIST is
not generally better than BLEU, and the reason, we
conjecture, is that it differentiates the n-grams too
much and the frequency estimated upon the evalua-
tion corpus is not always reliable. In this section we
will describe two other strategies for differentiating
the n-grams, one of which uses the alignments with
the source sentence as a further constraint, while the
other differentiates the n-gram precisions according
to POS.
2.1 Source-sentence Constrained N-gram
Precision
The quality of an MT sentence should be indepen-
dent of the source sentence given the reference trans-
lation, but considering that current metrics are all
based on shallow morphological similarity of the
MT outputs and the reference, without really under-
standing the meaning in both sides, the source sen-
tences could have some useful information in dif-
ferentiating the MT outputs. Consider the Chinese-
English translation example below:
Source: wo bu neng zhe me zuo
Hypothesis: I must hardly not do this
Reference: I must not do this
It is clear that the word not in the MT output can-
not co-exist with the word hardly while maintain-
ing the meaning of the source sentence. None of
the metrics mentioned above can prevent not from
being counted in the evaluation, due to the simple
reason that they only compute shallow morphologi-
cal similarity. Then how could the source sentence
help in the example? If we reveal the alignment
of the source sentence with both the reference and
the MT output, the Chinese word bu neng would
be aligned to must not in the reference and must
hardly in the MT output respectively, leaving the
word not in the MT output not aligned to any word in
the source sentence. Therefore, if we can somehow
find the alignments between the source sentence and
the reference/MT output, we could be smarter in se-
lecting the overlapping words to be counted in the
1In metrics such as METEOR, ROUGE, SIA (Liu and
Gildea, 2006), the positions of words do make difference, but
it has nothing to do with the word itself.
for all n-grams wi, ..., wi+n?1 in MT hypothesis
do
max val = 0;
for all reference sentences do
for all n-grams rj , ..., rj+n?1 in current ref-
erence sentence do
val=0;
for k=0; k ? n-1; k ++ do
if wi+k equals rj+k AND MTaligni
equals REFalignj then
val += 1n ;
if val ? max val then
max val = val;
hit count += max val;
return hit countMThypothesislength ? length penalty;
Figure 1: Algorithm for Computing Source-
sentence Constrained n-gram Precision
metric: only select the words which are aligned to
the same source words. Now the question comes
to how to find the alignment of source sentence and
MT hypothesis/references, since the evaluation data
set usually does not contain alignment information.
Our approach uses GIZA++2 to construct the many-
to-one alignments between source sentences and the
MT hypothesis/references respectively.3 GIZA++
could generate many-to-one alignments either from
source sentence to the MT hypothesis, in which case
every word in MT hypothesis is aligned to a set
of (or none) words in the source sentence, or from
the reverse direction, in which case every word in
MT hypothesis is aligned to exactly one word (or
none) word in the source sentence. In either case,
using MTaligni and REFaligni to denote the po-
sitions of the words in the source sentences which
are aligned to a word in the MT hypothesis and a
word in the reference respectively, the algorithm for
computing source-sentence constrained n-gram pre-
cision of length n is described in Figure 1.
Since source-sentence constrained n-gram preci-
sion (SSCN) is a precision-based metric, the vari-
2GIZA++ is available at
http://www.fjoch.com/GIZA++.html
3More refined alignments could be got for source-hypothesis
from the MT system, and for source-references by using manual
proof-reading after the automatic alignment. Doing so, how-
ever, requires the MT system?s cooperation and some costly hu-
man labor.
43
able length penalty is used to avoid assigning a
short MT hypothesis a high score, and is computed
in the same way as BLEU. Note that in the algo-
rithm for computing the precision of n-grams longer
than one word, not all words in the n-grams should
satisfy the source-sentence constraint. The reason is
that the high order n-grams are already very sparse
in the sentence-level evaluation. To differentiate the
SSCNs based on the source-to-MT/Ref (many-to-
one) alignments and the MT/Ref-to-source (many-
to-one) alignments, we use SSCN1 and SSCN2 to
denote them respectively. Naturally, we could com-
bine the constraint in SSCN1 and SSCN2 by either
taking their union (the combined constrained is sat-
isfied if either one is satisfied) or intersecting them
(the combined constrained is satisfied if both con-
straints are satisfied). We use SSCN u and SSCN i
to denote the SSCN based on unioned constraints
and intersected constraints respectively. We could
also apply the stochastic word mapping proposed in
SIA (Liu and Gildea, 2006) to replace the hard word
matching in Figure 1, and the corresponding met-
rics are denoted as pSSCN1, pSSCN2, pSSCN u,
pSSCN i, with the suffixed number denoting differ-
ent constraints.
2.2 Metrics Based on Source Word Reordering
Most previous MT metrics concentrate on the co-
occurrence of the MT hypothesis words in the ref-
erences. Our metrics based on source sentence re-
orderings, on the contrary, do not take words identi-
ties into account, but rather compute how similarly
the source words are reordered in the MT output and
the references. For simplicity, we only consider the
pairwise reordering similarity. That is, for the source
word pair wi and wj , if their aligned positions in the
MT hypothesis and a reference are in the same order,
we call it a consistent word pair. Our pairwise re-
ordering similarity (PRS) metric computes the frac-
tion of the consistent word pairs in the source sen-
tence. Figure 2 gives the formal description of PRS.
SrcMTi and SrcRefk,i denote the aligned position
of source word wi in the MT hypothesis and the kth
reference respectively, and N denotes the length of
the source sentence.
Another criterion for evaluating the reordering of
the source sentence in the MT hypothesis is how
well it maintains the original word order in the
for all word pair wi, wj in the source sentence
such that i < j do
for all reference sentences rk do
if (SrcMTi == SrcMTj AND
SrcRefk,i == SrcRefk,j) OR
((SrcMTi ? SrcMTj) ? (SrcRefk,i ?
SrcRefk,j) > 0) then
count + +; break;
return 2?countN?(N?1) ;
Figure 2: Compute Pairwise Reordering Similarity
for all word pair wi, wj in the source sentence,
such that i < j do
if SrcMTi ? SrcMTj < 0 then
count + +;
return 2?countN?(N?1) ;
Figure 3: Compute Source Sentence Monotonic Re-
ordering Ratio
source sentence. We know that most of the time,
the alignment of the source sentence and the MT hy-
pothesis is monotonic. This idea leads to the metric
of monotonic pairwise ratio (MPR), which computes
the fraction of the source word pairs whose aligned
positions in the MT hypothesis are of the same order.
It is described in Figure 3.
2.3 Discriminative Unigram Precision Based
on POS
The Discriminative Unigram Precision Based on
POS (DUPP) decomposes the normal unigram pre-
cision into many sub-precisions according to their
POS. The algorithm is described in Figure 4.
These sub-precisions by themselves carry the
same information as standard unigram precision, but
they provide us the opportunity to make a better
combined metric than the normal unigram precision
with MCT, which will be introduced in next section.
for all unigram s in the MT hypothesis do
if s is found in any of the references then
countPOS(s) += 1
precisionx = countxmt hypothesis length
?x ? POS
Figure 4: Compute DUPP for N-gram with length n
44
Such division could in theory be generalized to work
with higher order n-grams, but doing so would make
the n-grams in each POS set much more sparse. The
preprocessing step for the metric is tagging both
the MT hypothesis and the references with POS. It
might elicit some worries about the robustness of the
POS tagger on the noise-containing MT hypothesis.
This should not be a problem for two reasons. First,
compared with other preprocessing steps like pars-
ing, POS tagging is easier and has higher accuracy.
Second, because the counts for each POS are accu-
mulated, the correctness of a single word?s POS will
not affect the result very much.
3 Maximum Correlation Training for
Machine Translation Evaluation
Maximum Correlation Training (MCT) is an in-
stance of the general approach of directly optimiz-
ing the objective function by which a model will
ultimately be evaluated. In our case, the model is
the linear combination of the component metrics, the
parameters are the weights for each component met-
ric, and the objective function is the Pearson?s corre-
lation of the combined metric and the human judg-
ments. The reason to use the linear combination of
the metrics is that the component metrics are usu-
ally of the same or similar order of magnitude, and it
makes the optimization problem easy to solve. Us-
ing w to denote the weights, and m to denote the
component metrics, the combined metric x is com-
puted as:
x(w) =
?
j
wjmj (1)
Using hi and x(w)i denote the human judgment
and combined metric for a sentence respectively, and
N denote the number of sentences in the evaluation
set, the objective function is then computed as:
Pearson(X(w), H) =
PN
i=1 x(w)ihi ?
PN
i=1 x(w)i
PN
i=1 hi
N
q
(
PN
i=1 x(w)2i ?
(PNi=1 x(w)i)2
N )(
PN
i=1 h2i ?
(PNi=1 hi)2
N )
Now our task is to find the weights for each compo-
nent metric so that the correlation of the combined
metric with the human judgment is maximized. It
can be formulated as:
w = argmax
w
Pearson(X(w), H) (2)
The function Pearson(X(w), H) is differentiable
with respect to the vector w, and we compute this
derivative analytically and perform gradient ascent.
Our objective function not always convex (one can
easily create a non-convex function by setting the
human judgments and individual metrics to some
particular value). Thus there is no guarantee that,
starting from a random w, we will get the glob-
ally optimal w using optimization techniques such
as gradient ascent. The easiest way to avoid ending
up with a bad local optimum to run gradient ascent
by starting from different random points. In our ex-
periments, the difference in each run is very small,
i.e., by starting from different random initial values
of w, we end up with, not the same, but very similar
values for Pearson?s correlation.
4 Experiments
Experiments were conducted to evaluate the perfor-
mance of the new metrics proposed in this paper,
as well as the MCT combination framework. The
data for the experiments are from the MT evalua-
tion workshop at ACL05. There are seven sets of
MT outputs (E09 E11 E12 E14 E15 E17 E22), each
of which contains 919 English sentences translated
from the same set of Chinese sentences. There are
four references (E01, E02, E03, E04) and two sets
of human scores for each MT hypothesis. Each hu-
man score set contains a fluency and an adequacy
score, both of which range from 1 to 5. We create a
set of overall human scores by averaging the human
fluency and adequacy scores. For evaluating the au-
tomatic metrics, we compute the Pearson?s correla-
tion of the automatic scores and the averaged human
scores (over the two sets of available human scores),
for overall score, fluency, and adequacy. The align-
ment between the source sentences and the MT hy-
pothesis/references is computed by GIZA++, which
is trained on the combined corpus of the evalua-
tion data and a parallel corpus of Chinese-English
newswire text. The parallel newswire corpus con-
tains around 75,000 sentence pairs, 2,600,000 En-
glish words and 2,200,000 Chinese words. The
45
stochastic word mapping is trained on a French-
English parallel corpus containing 700,000 sentence
pairs, and, following Liu and Gildea (2005), we only
keep the top 100 most similar words for each En-
glish word.
4.1 Performance of the Individual Metrics
To evaluate our source-sentence based metrics, they
are used to evaluate the 7 MT outputs, with the 4 sets
of human references. The sentence-level Pearson?s
correlation with human judgment is computed for
each MT output, and the averaged results are shown
in Table 1. As a comparison, we also show the re-
sults of BLEU, NIST, METEOR, ROUGE, WER,
and HWCM. For METEOR and ROUGE, WORD-
NET and PORTER-STEMMER are enabled, and for
SIA, the decay factor is set to 0.6. The number
in brackets, for BLEU, shows the n-gram length it
counts up to, and for SSCN, shows the length of the
n-gram it uses. In the table, the top 3 results in each
column are marked bold and the best result is also
underlined. The results show that the SSCN2 met-
rics are better than the SSCN1 metrics in adequacy
and overall score. This is understandable since what
SSCN metrics need is which words in the source
sentence are aligned to an n-gram in the MT hy-
pothesis/references. This is directly modeled in the
alignment used in SSCN2. Though we could also
get such information from the reverse alignment, as
in SSCN1, it is rather an indirect way and could con-
tain more noise. It is interesting that SSCN1 gets
better fluency evaluation results than SSCN2. The
SSCN metrics with the unioned constraint, SSCN u,
by combining the strength of SSCN1 and SSCN2,
get even better results in all three aspects. We can
see that SSCN metrics, even without stochastic word
mapping, get significantly better results than their
relatives, BLEU, which indicates the source sen-
tence constraints do make a difference. SSCN2 and
SSCN u are also competitive to the state-of-art MT
metrics such as METEOR and SIA. The best SSCN
metric, pSSCN u(2), achieves the best performance
among all the testing metrics in overall and ade-
quacy, and the second best performance in fluency,
which is just a little bit worse than the best fluency
metric SIA.
The two reordering based metrics, PRS and MPR,
are not as good as the other testing metrics, in terms
Fluency Adequacy Overall
ROUGE W 24.8 27.8 29.0
ROUGE S 19.7 30.9 28.5
METEOR 24.4 34.8 33.1
SIA 26.8 32.1 32.6
NIST 1 09.6 22.6 18.5
WER 22.5 27.5 27.7
PRS 14.2 19.4 18.7
MPR 11.0 18.2 16.5
BLEU(1) 18.4 29.6 27.0
BLEU(2) 20.4 31.1 28.9
BLEU(3) 20.7 30.4 28.6
HWCM(2) 22.1 30.3 29.2
SSCN1(1) 24.2 29.6 29.8
SSCN2(1) 22.9 33.0 31.3
SSCN u(1) 23.8 34.2 32.5
SSCN i(1) 23.4 28.0 28.5
pSSCN1(1) 24.9 30.2 30.6
pSSCN2(1) 23.8 34.0 32.4
pSSCN u(1) 24.5 34.6 33.1
pSSCN i(1) 24.1 28.8 29.3
SSCN1(2) 24.0 29.6 29.7
SSCN2(2) 23.3 31.5 31.8
SSCN u(2) 24.1 34.5 32.8
SSCN i(2) 23.1 27.8 28.2
pSSCN1(2) 24.9 30.2 30.6
pSSCN2(2) 24.3 34.4 32.8
pSSCN u(2) 25.2 35.4 33.9
pSSCN i(2) 23.9 28.7 29.1
Table 1: Performance of Component Metrics
of the individual performance. It should not be sur-
prising since they are totally different kind of met-
rics, which do not count the overlapping n-grams,
but the consistent/monotonic word pair reorderings.
As long as they capture some property of the MT
hypothesis, they might be able to boost the per-
formance of the combined metric under the MCT
framework.
4.2 Performance of the Combined Metrics
To test how well MCT works, the following scheme
is used: each set of MT outputs is evaluated by MCT,
which is trained on the other 6 sets of MT outputs
and their corresponding human judgment; the aver-
aged correlation of the 7 sets of MT outputs with the
human judgment is taken as the final result.
4.2.1 Discriminative Unigram Precision based
on POS
We first use MCT to combine the discriminative
unigram precisions. To reduce the sparseness of the
unigrams of each POS, we do not use the original
POS set, but use a generalized one by combining
46
all POS tags with the same first letter (e.g., the dif-
ferent verb forms such as VBN, VBD, and VBZ are
transformed to V). The unified POS set contains 23
POS tags. To give a fair comparison of DUPP with
BLEU, the length penalty is also added into it as a
component. Results are shown in Table 2. DUPP f,
DUPP a and DUPP o denote DUPP trained on hu-
man fluency, adequacy and overall judgment respec-
tively. This shows that DUPP achieves obvious im-
provement over BLEU, with only the unigrams and
length penalty, and DUPP f/ a/ o gets the best re-
sult in fluency/adequacy/overall evaluation, showing
that MCT is able to make a fluency- or adequacy-
oriented metric.
4.2.2 Putting It All Together
The most interesting question in this paper is, with
all these metrics, how well we can do in the MT
evaluation. To answer the question, we put all the
metrics described into the MCT framework and use
the combined metric to evaluate the 7 MT outputs.
Note that to speed up the training process, we do
not directly use 24 DUPP components, instead, we
use the 3 combined DUPP metrics. With the met-
rics shown in Table 1, we then have in total 31 met-
rics. Table 2 shows the results of the final combined
metric. We can see that MCT trained on fluency,
adequacy and overall human judgment get the best
results among all the testing metrics in fluency, ade-
quacy and overall evaluation respectively. We did a
t-test with Fisher?s z transform for the combined re-
sults and the individual results to see how significant
the difference is. The combined results in adequacy
and overall are significantly better at 99.5% confi-
dence than the best results of the individual metrics
(pSSCN u(2)), and the combined result in fluency
is significantly better at 96.9% confidence than the
best individual metric (SIA). We also give the upper
bound for each evaluation aspect by training MCT
on the testing MT outputs, e.g., we train MCT on
E09 and then use it to evaluate E09. The upper-
bound is the best we can do with the MCT based
on linear combination. Another linear framework,
Classification SVM (CSVM),4 is also used to com-
bine the testing metrics except DUPP. Since DUPP
is based on MCT, to make a neat comparison, we
rule out DUPP in the experiments with CSVM. The
4http://svmlight.joachims.org/
Fluency Adequacy Overall
DUPP f 23.6 30.1 30.1
DUPP a 22.1 32.9 30.9
DUPP o 23.2 32.8 31.3
MCT f(4) 30.3 36.7 37.2
MCT a(4) 28.0 38.9 37.4
MCT o(4) 29.4 38.8 38.0
Upper bound 35.3 43.4 42.2
MCT f(3) 29.2 34.7 35.3
MCT a(3) 27.4 38.4 36.8
MCT o(3) 28.8 38.0 37.2
CSVM(3) 27.3 36.9 35.5
Table 2: Combination of the Testing Metrics
testing scheme is the same as MCT, except that we
only use 3 references for each MT hypothesis, and
the positive samples for training CSVM are com-
puted as the scores of one of the 4 references based
on the other 3 references. The slack parameter of
CSVM is chosen so as to maximize the classifica-
tion accuracy of a heldout set of 800 negative and
800 positive samples, which are randomly selected
from the training set. The results are shown in Ta-
ble 2. We can see that MCT, with the same number
of reference sentences, is better than CSVM. Note
that the resources required by MCT and CSVM are
different. MCT uses human judgments to adjust the
weights, while CSVM needs extra human references
to produce positive training samples.
To have a rough idea of how the component met-
rics contribute to the final performance of MCT, we
incrementally add metrics into the MCT in descend-
ing order of their overall evaluation performance,
with the results shown in Figure 5. We can see that
the performance improves as the number of metrics
increases, in a rough sense. The major improvement
happens in the 3rd, 4th, 9th, 14th, and 30th metrics,
which are METEOR, SIA, DUPP a, pSSCN1(1),
and PRS. It is interesting to note that these are not
the metrics with the highest individual performance.
Another interesting observation is that there are no
two metrics belonging to the same series in the most
beneficial metrics, indicating that to get better com-
bined metrics, individual metrics showing different
sentence properties are preferred.
5 Conclusion
This paper first describes two types of new ap-
proaches to MT evaluation, which includes making
47
0 5 10 15 20 25 30 35
0.24
0.26
0.28
0.3
0.32
0.34
0.36
0.38
0.4
the number of metrics (o: adequacy, x: fluency, +: overall)
co
rr
e
la
tio
n 
w
ith
 h
um
an
 fl
ue
nc
y/
ov
er
al
l/a
de
qu
ac
y 
jud
ge
me
nts
Figure 5: Performance as a Function of the Number
of Interpolated Metrics
use of source sentences, and discriminating unigram
precisions based on POS. Among all the testing met-
rics including BLEU, NIST, METEOR, ROUGE,
and SIA, our new metric, pSSCN u(2), based on
source-sentence constrained bigrams, achieves the
best adequacy and overall evaluation results, and the
second best result in fluency evaluation. We fur-
ther improve the performance by combining the in-
dividual metrics under the MCT framework, which
is shown to be better than a classification based
framework such as SVM. By examining the contri-
bution of each component metric, we find that met-
rics showing different properties of a sentence are
more likely to make a good combined metric.
Acknowledgments This work was supported by
NSF grants IIS-0546554, IIS-0428020, and IIS-
0325646.
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved cor-
relation with human judegments. In Proceedings of
the ACL-04 workshop on Intrinsic and Extrinsic Eval-
uation Measures for Machine Translation and/or Sum-
marization, Ann Arbor, Michigan.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2003. Confidence estimation for
machine translation. Technical report, Center for Lan-
guage and Speech Processing, Johns Hopkins Univer-
sity, Baltimore. Summer Workshop Final Report.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In In HLT 2002, Human Language Technology
Conference, San Diego, CA.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
In Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation (TMI), Baltimore, MD, October.
Chin-Yew Lin and Franz Josef Och. 2004. Automatic
evaluation of machine translation quality using longest
common subsequence and skip-bigram statistics. In
Proceedings of the 42th Annual Conference of the
Association for Computational Linguistics (ACL-04),
Barcelona, Spain.
Lucian Vlad Lita, Monica Rogati, and Alon Lavie. 2005.
Blanc: Learning evaluation metrics for mt. Vancouver.
Ding Liu and Daniel Gildea. 2005. Syntactic features for
evaluation of machine translation. In ACL 2005 Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization.
Ding Liu and Daniel Gildea. 2006. Stochastic iterative
alignment for machine translation evaluation. Sydney.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of ACL-
03.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL-
02, Philadelphia, PA.
48
Proceedings of NAACL HLT 2007, pages 147?154,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Worst-Case Synchronous Grammar Rules
Daniel Gildea and Daniel ?Stefankovic?
Computer Science Dept.
University of Rochester
Rochester, NY 14627
Abstract
We relate the problem of finding the best
application of a Synchronous Context-
Free Grammar (SCFG) rule during pars-
ing to a Markov Random Field. This
representation allows us to use the the-
ory of expander graphs to show that the
complexity of SCFG parsing of an input
sentence of length N is ?(N cn), for a
grammar with maximum rule length n and
some constant c. This improves on the
previous best result of ?(N c
?n).
1 Introduction
Recent interest in syntax-based methods for statis-
tical machine translation has lead to work in pars-
ing algorithms for synchronous context-free gram-
mars (SCFGs). Generally, parsing complexity de-
pends on the length of the longest rule in the gram-
mar, but the exact nature of this relationship has only
recently begun to be explored. It has been known
since the early days of automata theory (Aho and
Ullman, 1972) that the languages of string pairs gen-
erated by a synchronous grammar can be arranged in
an infinite hierarchy, with each rule size ? 4 pro-
ducing languages not possible with grammars re-
stricted to smaller rules. For any grammar with
maximum rule size n, a fairly straightforward dy-
namic programming strategy yields an O(Nn+4) al-
gorithm for parsing sentences of length N . How-
ever, this is often not the best achievable complexity,
and the exact bounds of the best possible algorithms
are not known. Satta and Peserico (2005) showed
that a permutation can be defined for any length n
such that tabular parsing strategies must take at least
?(N c
?n), that is, the exponent of the algorithm is
proportional to the square root of the rule length.
In this paper, we improve this result, showing that
in the worst case the exponent grows linearly with
the rule length. Using a probabilistic argument, we
show that the number of easily parsable permuta-
tions grows slowly enough that most permutations
must be difficult, where by difficult we mean that the
exponent in the complexity is greater than a constant
factor times the rule length. Thus, not only do there
exist permutations that have complexity higher than
the square root case of Satta and Peserico (2005),
but in fact the probability that a randomly chosen
permutation will have higher complexity approaches
one as the rule length grows.
Our approach is to first relate the problem of
finding an efficient parsing algorithm to finding the
treewidth of a graph derived from the SCFG rule?s
permutation. We then show that this class of graphs
are expander graphs, which in turn means that the
treewidth grows linearly with the graph size.
2 Synchronous Parsing Strategies
We write SCFG rules as productions with one
lefthand side nonterminal and two righthand side
strings. Nonterminals in the two strings are linked
with superscript indices; symbols with the same in-
dex must be further rewritten synchronously. For ex-
ample,
X ? A(1) B(2) C(3) D(4), A(1) B(2) C(3) D(4)
(1)
is a rule with four children and no reordering, while
X ? A(1) B(2) C(3) D(4), B(2) D(4) A(1) C(3)
(2)
147
Algorithm 1 BottomUpParser(grammar G, input strings e, f )
for x0, xn such that 1 < x0 < xn < |e| in increasing order of xn ? x0 do
for y0, yn such that 1 < y0 < yn < |f | in increasing order of yn ? y0 do
for Rules R of form X ? X(1)1 ...X
(n)
n , X(pi(1))pi(1) ...X
(pi(n))
pi(n) in G do
p = P (R) maxx1..xn?1y1..yn?1
?
i
?(Xi, xi?1, xi, ypi(i)?1, ypi(i))
?(X,x0, xn, y0, yn) = max{?(X,x0, xn, y0, yn), p}
end for
end for
end for
expresses a more complex reordering. In general,
we can take indices in the first grammar dimen-
sion to be consecutive, and associate a permutation
? with the second dimension. If we use Xi for
0 ? i ? n as a set of variables over nonterminal
symbols (for example, X1 and X2 may both stand
for nonterminal A), we can write rules in the gen-
eral form:
X0 ? X(1)1 ...X(n)n , X
(pi(1))
pi(1) ...X
(pi(n))
pi(n)
Grammar rules also contain terminal symbols, but as
their position does not affect parsing complexity, we
focus on nonterminals and their associated permuta-
tion ? in the remainder of the paper. In a probabilis-
tic grammar, each rule R has an associated proba-
bility P (R). The synchronous parsing problem con-
sists of finding the tree covering both strings having
the maximum product of rule probabilities.1
We assume synchronous parsing is done by stor-
ing a dynamic programming table of recognized
nonterminals, as outlined in Algorithm 1. We refer
to a dynamic programming item for a given nonter-
minal with specified boundaries in each language as
a cell. The algorithm computes cells by maximiz-
ing over boundary variables xi and yi, which range
over positions in the two input strings, and specify
beginning and end points for the SCFG rule?s child
nonterminals.
The maximization in the inner loop of Algo-
rithm 1 is the most expensive part of the proce-
dure, as it would take O(N2n?2) with exhaustive
1We describe our methods in terms of the Viterbi algorithm
(using the max-product semiring), but they also apply to non-
probabilistic parsing (boolean semiring), language modeling
(sum-product semiring), and Expectation Maximization (with
inside and outside passes).
search; making this step more efficient is our fo-
cus in this paper. The maximization can be done
with further dynamic programming, storing partial
results which contain some subset of an SCFG rule?s
righthand side nonterminals that have been recog-
nized. A parsing strategy for a specific SCFG rule
consists of an order in which these subsets should
be combined, until all the rule?s children have been
recognized. The complexity of an individual parsing
step depends on the number of free boundary vari-
ables, each of which can take O(N) values. It is
often helpful to visualize parsing strategies on the
permutation matrix corresponding to a rule?s per-
mutation ?. Figure 1 shows the permutation matrix
of rule (2) with a three-step parsing strategy. Each
panel shows one combination step along with the
projections of the partial results in each dimension;
the endpoints of these projections correspond to free
boundary variables. The second step has the high-
est number of distinct endpoints, five in the vertical
dimension and three horizontally, meaning parsing
can be done in time O(N8).
As an example of the impact that the choice of
parsing strategy can make, Figure 2 shows a per-
mutation for which a clever ordering of partial re-
sults enables parsing in time O(N10) in the length
of the input strings. Permutations having this pattern
of diagonal stripes can be parsed using this strat-
egy in time O(N10) regardless of the length n of
the SCFG rule, whereas a na??ve strategy proceeding
from left to right in either input string would take
time O(Nn+3).
2.1 Markov Random Fields for Cells
In this section, we connect the maximization of
probabilities for a cell to the Markov Random Field
148
{A,B,C,D}
{A,B,C}
{A,B}
{A} {B}
{C}
{D}
x0 x1 x2 x3 x4
y0
y1
y2
y3
y4
A
B
C
D
x0 x1 x2 x3 x4
y0
y1
y2
y3
y4
A
B
C
D
x0 x1 x2 x3 x4
y0
y1
y2
y3
y4
A
B
C
D
Figure 1: The tree on the left defines a three-step parsing strategy for rule (2). In each step, the two subsets
of nonterminals in the inner marked spans are combined into a new chart item with the outer spans. The
intersection of the outer spans, shaded, has now been processed. Tic marks indicate distinct endpoints of the
spans being combined, corresponding to the free boundary variables.
(MRF) representation, which will later allow us to
use algorithms and complexity results based on the
graphical structure of MRFs. A Markov Random
Field is defined as a probability distribution2 over a
set of variables x that can be written as a product of
factors fi that are functions of various subsets xi of
x. The probability of an SCFG rule instance com-
puted by Algorithm 1 can be written in this func-
tional form:
?R(x) = P (R)
?
i
fi(xi)
where
x = {xi, yi} for 0 ? i ? n
xi = {xi?1, xi, ypi(i)?1, ypi(i)}
and the MRF has one factor fi for each child nonter-
minal Xi in the grammar rule R. The factor?s value
is the probability of the child nonterminal, which can
be expressed as a function of its four boundaries:
fi(xi) = ?(Xi, xi?1, xi, ypi(i)?1, ypi(i))
For reasons that are explained in the following
section, we augment our Markov Random Fields
with a dummy factor for the completed parent non-
terminal?s chart item. Thus there is one dummy fac-
tor d for each grammar rule:
d(x0, xn, y0, yn) = 1
expressed as a function of the four outer boundary
variables of the completed rule, but with a constant
2In our case unnormalized.
Figure 2: A parsing strategy maintaining two spans
in each dimension is O(N10) for any length permu-
tation of this general form.
value of 1 so as not to change the probabilities com-
puted.
Thus an SCFG rule with n child nonterminals al-
ways results in a Markov Random Field with 2n+2
variables and n+ 1 factors, with each factor a func-
tion of exactly four variables.
Markov Random Fields are often represented as
graphs. A factor graph representation has a node
for each variable and factor, with an edge connect-
ing each factor to the variables it depends on. An ex-
ample for rule (2) is shown in Figure 3, with round
nodes for variables, square nodes for factors, and a
diamond for the special dummy factor.
2.2 Junction Trees
Efficient computation on Markov Random Fields
is performed by first transforming the MRF into
a junction tree (Jensen et al, 1990; Shafer and
Shenoy, 1990), and then applying the standard
149
dy0 y1 y2 y3 y4
f1 f2 f3 f4
x0 x1 x2 x3 x4
Figure 3: Markov Random Field for rule (2).
message-passing algorithm for graphical models
over this tree structure. The complexity of the mes-
sage passing algorithm depends on the structure of
the junction tree, which in turn depends on the graph
structure of the original MRF.
A junction tree can be constructed from a Markov
Random Field by the following three steps:
? Connect all variable nodes that share a factor,
and remove factor nodes. This results in the
graphs shown in Figure 4.
? Choose a triangulation of the resulting graph,
by adding chords to any cycle of length greater
than three.
? Decompose the triangulated graph into a tree of
cliques.
We call nodes in the resulting tree, corresponding
to cliques in the triangulated graph, clusters. Each
cluster has a potential function, which is a function
of the variables in the cluster. For each factor in the
original MRF, the junction tree will have at least one
cluster containing all of the variables on which the
factor is defined. Each factor is associated with one
such cluster, and the cluster?s potential function is
set to be the product of its factors, for all combina-
tions of variable values. Triangulation ensures that
the resulting tree satisfies the junction tree property,
which states that for any two clusters containing the
same variable x, all nodes on the path connecting the
clusters also contain x. A junction tree derived from
the MRF of Figure 3 is shown in Figure 5.
The message-passing algorithm for graphical
models can be applied to the junction tree. The algo-
y0 y1 y2 y3 y4
x0 x1 x2 x3 x4
y0 y1 y2 y3 y4
x0 x1 x2 x3 x4
Figure 4: The graphs resulting from connecting
all interacting variables for the identity permutation
(1, 2, 3, 4) (top) and the (2, 4, 1, 3) permutation of
rule (2) (bottom).
rithm works from the leaves of the tree inward, alter-
nately multiplying in potential functions and maxi-
mizing over variables that are no longer needed, ef-
fectively distributing the max and product operators
so as to minimize the interaction between variables.
The complexity of the message-passing is O(nNk),
where the junction tree contain O(n) clusters, k is
the maximum cluster size, and each variable in the
cluster can take N values.
However, the standard algorithm assumes that the
factor functions are predefined as part of the input.
In our case, however, the factor functions themselves
depend on message-passing calculations from other
grammar rules:
fi(xi) = ?(Xi, xi?1, xi, ypi(i)?1, ypi(i))
= max
R?:Xi??,?
P (R?) max
x?:
x?0=xi?1,x?n?=xi
y?0=ypi(i?1),y?n?=ypi(i)
?R?(x?) (3)
We must modify the standard algorithm in order
to interleave computation among the junction trees
corresponding to the various rules in the grammar,
using the bottom-up ordering of computation from
Algorithm 1. Where, in the standard algorithm, each
message contains a complete table for all assign-
ments to its variables, we break these into a sepa-
rate message for each individual assignment of vari-
ables. The overall complexity is unchanged, because
each assignment to all variables in each cluster is
still considered only once.
The dummy factor d ensures that every junction
150
x0 x3 x4 y0 y2 y3 y4
x0 x2 x3 y0 y1 y2 y3 y4
x0 x1 x2 y1 y2 y3 y4
Figure 5: Junction tree for rule (2).
tree we derive from an SCFG rule has a cluster con-
taining all four outer boundary variables, allowing
efficient lookup of the inner maximization in (3).
Because the outer boundary variables need not ap-
pear throughout the junction tree, this technique al-
lows reuse of some partial results across different
outer boundaries. As an example, consider message
passing on the junction tree of shown in Figure 5,
which corresponds to the parsing strategy of Fig-
ure 1. Only the final step involves all four bound-
aries of the complete cell, but the most complex step
is the second, with a total of eight boundaries. This
efficient reuse would not be achieved by applying
the junction tree technique directly to the maximiza-
tion operator in Algorithm 1, because we would be
fixing the outer boundaries and computing the junc-
tion tree only over the inner boundaries.
3 Treewidth and Tabular Parsing
The complexity of the message passing algorithm
over an MRF?s junction tree is determined by the
treewidth of the MRF. In this section we show that,
because parsing strategies are in direct correspon-
dence with valid junction trees, we can use treewidth
to analyze the complexity of a grammar rule.
We define a tabular parsing strategy as any dy-
namic programming algorithm that stores partial re-
sults corresponding to subsets of a rule?s child non-
terminals. Such a strategy can be represented as a
recursive partition of child nonterminals, as shown
in Figure 1(left). We show below that a recursive
partition of children having maximum complexity k
at any step can be converted into a junction tree hav-
ing k as the maximum cluster size. This implies that
finding the optimal junction tree will give a parsing
strategy at least as good as the strategy of the opti-
mal recursive partition.
A recursive partition of child nonterminals can be
converted into a junction tree as follows:
? For each leaf of the recursive partition, which
represents a single child nonterminal i, cre-
ate a leaf in the junction tree with the cluster
(xi?1, xi, ypi(i)?1, ypi(i)) and the potential func-
tion fi(xi?1, xi, ypi(i)?1, ypi(i)).
? For each internal node in the recursive parti-
tion, create a corresponding node in the junc-
tion tree.
? Add each variable xi to all nodes in the junction
tree on the path from the node for child nonter-
minal i? 1 to the node for child nonterminal i.
Similarly, add each variable ypi(i) to all nodes
in the junction tree on the path from the node
for child nonterminal ?(i) ? 1 to the node for
child nonterminal ?(i).
Because each variable appears as an argument of
only two factors, the junction tree nodes in which it
is present form a linear path from one leaf of the tree
to another. Since each variable is associated only
with nodes on one path through the tree, the result-
ing tree will satisfy the junction tree property. The
tree structure of the original recursive partition im-
plies that the variable rises from two leaf nodes to
the lowest common ancestor of both leaves, and is
not contained in any higher nodes. Thus each node
in the junction tree contains variables correspond-
ing to the set of endpoints of the spans defined by
the two subsets corresponding to its two children.
The number of variables at each node in the junction
tree is identical to the number of free endpoints at
the corresponding combination in the recursive par-
tition.
Because each recursive partition corresponds to a
junction tree with the same complexity, finding the
best recursive partition reduces to finding the junc-
tion tree with the best complexity, i.e., the smallest
maximum cluster size.
Finding the junction tree with the smallest clus-
ter size is equivalent to finding the input graph?s
treewidth, the smallest k such that the graph can be
embedded in a k-tree. In general, this problem was
shown to be NP-complete by Arnborg et al (1987).
However, because the treewidth of a given rule lower
bounds the complexity of its tabular parsing strate-
gies, parsing complexity for general rules can be
151
bounded with treewidth results for worst-case rules,
without explicitly identifying the worst-case permu-
tations.
4 Treewidth Grows Linearly
In this section, we show that the treewidth of the
graphs corresponding to worst-case permutations
growths linearly with the permutation?s length. Our
strategy is as follows:
1. Define a 3-regular graph for an input permu-
tation consisting of a subset of edges from the
original graph.
2. Show that the edge-expansion of the 3-regular
graph grows linearly for randomly chosen per-
mutations.
3. Use edge-expansion to bound the spectral gap.
4. Use spectral gap to bound treewidth.
For the first step, we define H = (V,E) as a ran-
dom 3-regular graph on 2n vertices obtained as fol-
lows. Let G1 = (V1, E1) and G2 = (V2, E2) be
cycles, each on a separate set of n vertices. These
two cycles correspond to the edges (xi, xi+1) and
(yi, yi+1) in the graphs of the type shown in Fig-
ure 4. Let M be a random perfect matching be-
tween V1 and V2. The matching represents the edges
(xi, ypi(i)) produced from the input permutation ?.
Let H be the union of G1, G2, and M . While H
contains only some of the edges in the graphs de-
fined in the previous section, removing edges cannot
increase the treewidth.
For the second step of the proof, we use a proba-
bilistic argument detailed in the next subsection.
For the third step, we will use the following con-
nection between the edge-expansion and the eigen-
value gap (Alon and Milman, 1985; Tanner, 1984).
Lemma 4.1 Let G be a k-regular graph. Let ?2 be
the second largest eigenvalue of G. Let h(G) be the
edge-expansion of G. Then
k ? ?2 ?
h(G)2
2k .
Finally, for the fourth step, we use a relation be-
tween the eigenvalue gap and treewidth for regu-
lar graphs shown by Chandran and Subramanian
(2003).
Lemma 4.2 Let G be a k-regular graph. Let n be
the number of vertices of G. Let ?2 be the second
largest eigenvalue of G. Then
tw(G) ?
? n
4k (k ? ?2)
?
? 1
Note that in our setting k = 3. In order to use
Lemma 4.2 we will need to give a lower bound on
the eigenvalue gap k ? ?2 of G.
4.1 Edge Expansion
The edge-expansion of a set of vertices T is the ra-
tio of the number of edges connecting vertices in T
to the rest of the graph, divided by the number of
vertices in T ,
|E(T, V ? T )|
|T |
where we assume that |T | ? |V |/2. The edge ex-
pansion of a graph is the minimum edge expansion
of any subset of vertices:
h(G) = min
T?V
|E(T, V ? T )|
min{|T |, |V ? T |} .
Intuitively, if all subsets of vertices are highly con-
nected to the remainder of the graph, there is no way
to decompose the graph into minimally interacting
subgraphs, and thus no way to decompose the dy-
namic programming problem of parsing into smaller
pieces.
Let
(n
k
)
be the standard binomial coefficient, and
for ? ? R, let
( n
? ?
)
=
???
?
k=0
(n
k
)
.
We will use the following standard inequality valid
for 0 ? ? ? n:
( n
? ?
)
?
(ne
?
)?
(4)
Lemma 4.3 With probability at least 0.98 the graph
H has edge-expansion at least 1/50.
Proof :
Let ? = 1/50. Assume that T ? V is a set with a
small edge-expansion, i. e.,
|E(T, V ? T )| ? ?|T |, (5)
152
and |T | ? |V |/2 = n. Let Ti = T ? Vi and let
ti = |Ti|, for i = 1, 2. We will w.l.o.g. assume
t1 ? t2. We will denote as ?i the number of spans of
consecutive vertices from Ei contained in T . Thus
2?i = |E(Ti, Vi ? Ti)|, for i = 1, 2. The spans
counted by ?1 and ?2 correspond to continuous spans
counted in computing the complexity of a chart pars-
ing operation. However, unlike in the diagrams in
the earlier part of this paper, in our graph theoretic
argument there is no requirement that T select only
corresponding pairs of vertices from V1 and V2.
There are at least 2(?1+?2)+t2?t1 edges between
T and V ? T . This is because there are 2?i edges
within Vi at the left and right boundaries of the ?i
spans, and at least t2? t1 edges connecting the extra
vertices from T2 that have no matching vertex in T1.
Thus from assumption (5) we have
t2 ? t1 ? ?(t1 + t2)
which in turn implies
t1 ? t2 ?
1 + ?
1? ? t1. (6)
Similarly, using (6), we have
?1 + ?2 ?
?
2 (t1 + t2) ?
?
1? ? t1. (7)
That is, for T to have small edge expansion,
the vertices in T1 and T2 must be collected into a
small number of spans ?1 and ?2. This limit on the
number of spans allows us to limit the number of
ways of choosing T1 and T2. Suppose that t1 is
given. Any pair T1, T2 is determined by the edges
in E(T1, V1 ? T1), and E(T2, V2 ? T2), and two
bits (corresponding to the possible ?swaps? of Ti
with Vi ? Ti). Note that we can choose at most
2?1 + 2?2 ? t1 ? 2?/(1? ?) edges in total. Thus the
number of choices of T1 and T2 is bounded above by
4 ?
( 2n
? 2?1?? t1
)
. (8)
For a given choice of T1 and T2, for T to have
small edge expansion, there must also not be too
many edges that connect T1 to vertices in V2 ? T2.
Let k be the number of edges between T1 and T2.
There are at least t1 + t2 ? 2k edges between T and
V ? T and from assumption (5) we have
t1 + t2 ? 2k ? ?(t1 + t2)
Thus
k ? (1? ?) t1 + t22 ? (1? ?)t1. (9)
The probability that there are ? (1? ?)t1 edges be-
tween T1 and T2 is bounded by
( t1
? ?t1
)( t2
n
)(1??)t1
where the first term selects vertices in T1 connected
to T2, and the second term upper bounds the proba-
bility that the selected vertices are indeed connected
to T2. Using 6, we obtain a bound in terms of t1
alone:
( t1
? ?t1
)(1 + ?
1? ? ?
t1
n
)(1??)t1
, (10)
Combining the number of ways of choosing T1
and T2 (8) with the bound on the probability that the
edges M from the input permutation connect almost
all the vertices in T1 to vertices from T2 (10), and
using the union bound over values of t1, we obtain
that the probability p that there exists T ? V with
edge-expansion less than ? is bounded by:
2
?n/2?
?
t1=0
4?
( 2n
? 2?1?? t1
)( t1
? ?t1
)(1 + ?
1? ? ?
t1
n
)(1??)t1
(11)
where the factor of 2 is due to the assumption t1 ?
t2.
The graph H is connected and hence T has at least
one out-going edge. Therefore if t1 + t2 ? 1/?, the
edge-expansion of T is at least ?. Thus a set with
edge-expansion less than ? must have t1 + t2 ? 1/?,
which, by (6), implies t1 ? (1 ? ?)/(2?). Thus the
sum in (11) can be taken for t from ?(1 ? ?)/(2?)?
153
to ?n/2?. Using (4) we obtain
p ? 8
?n/2?
?
t1=? 1??2? ?
?
?
(
2ne
2?
1?? t1
)
2?
1?? t1 ( t1e
?t1
)?t1
(1 + ?
1? ? ?
t1
n
)(1??)t1
]
=
8
?n/2?
?
t1=? 1??2? ?
(
(e(1? ?)
?
)
2?
1?? (e
?
)?
(1 + ?
1? ?
)1??( t1
n
)1??? 2?1??
)t1
.
(12)
We will use t1/n ? 1/2 and plug ? = 1/50 into
(12). We obtain
p ? 8
?
?
t1=25
0.74t1 ? 0.02.

While this constant bound on p is sufficient for
our main complexity result, it can further be shown
that p approaches zero as n increases, from the fact
that the geometric sum in (12) converges, and each
term for fixed t1 goes to zero as n grows.
This completes the second step of the proof as
outlined at the beginning of this section. The con-
stant bound on the edge expansion implies a constant
bound on the eigenvalue gap (Lemma 4.1), which in
turn implies an ?(n) bound on treewidth (Lemma
4.2), yielding:
Theorem 4.4 Tabular parsing strategies for Syn-
chronous Context-Free Grammars containing rules
with all permutations of length n require time
?(N cn) in the input string length N for some con-
stant c.
We have shown our result without explicitly con-
structing a difficult permutation, but we close with
one example. The zero-based permutations of length
p, where p is prime, ?(i) = i?1 mod p for 0 <
i < p, and ?(0) = 0, provide a known family of
expander graphs (see Hoory et al (2006)).
5 Conclusion
We have shown in the exponent in the complex-
ity of polynomial-time parsing algorithms for syn-
chronous context-free grammars grows linearly with
the length of the grammar rules. While it is very
expensive computationally to test whether a speci-
fied permutation has a parsing algorithm of a certain
complexity, it turns out that randomly chosen per-
mutations are difficult with high probability.
Acknowledgments This work was supported by
NSF grants IIS-0546554, IIS-0428020, and IIS-
0325646.
References
Albert V. Aho and Jeffery D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume 1.
Prentice-Hall, Englewood Cliffs, NJ.
N. Alon and V.D. Milman. 1985. ?1, isoperimetric
inequalities for graphs and superconcentrators. J. of
Combinatorial Theory, Ser. B, 38:73?88.
Stefen Arnborg, Derek G. Corneil, and Andrzej
Proskurowski. 1987. Complexity of finding embed-
dings in a k-tree. SIAM Journal of Algebraic and Dis-
crete Methods, 8:277?284, April.
L.S. Chandran and C.R. Subramanian. 2003. A spectral
lower bound for the treewidth of a graph and its conse-
quences. Information Processing Letters, 87:195?200.
Shlomo Hoory, Nathan Linial, and Avi Wigderson. 2006.
Expander graphs and their applications. Bull. Amer.
Math. Soc., 43:439?561.
Finn V. Jensen, Steffen L. Lauritzen, and Kristian G. Ole-
sen. 1990. Bayesian updating in causal probabilis-
tic networks by local computations. Computational
Statistics Quarterly, 4:269?282.
Giorgio Satta and Enoch Peserico. 2005. Some com-
putational complexity results for synchronous context-
free grammars. In Proceedings of HLT/EMNLP, pages
803?810, Vancouver, Canada, October.
G. Shafer and P. Shenoy. 1990. Probability propaga-
tion. Annals of Mathematics and Artificial Intelli-
gence, 2:327?353.
R.M. Tanner. 1984. Explicit construction of concentra-
tors from generalized n-gons. J. Algebraic Discrete
Methods, 5:287?294.
154
Proceedings of the 43rd Annual Meeting of the ACL, pages 475?482,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Stochastic Lexicalized Inversion Transduction Grammar for Alignment
Hao Zhang and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
We present a version of Inversion Trans-
duction Grammar where rule probabili-
ties are lexicalized throughout the syn-
chronous parse tree, along with pruning
techniques for efficient training. Align-
ment results improve over unlexicalized
ITG on short sentences for which full EM
is feasible, but pruning seems to have a
negative impact on longer sentences.
1 Introduction
The Inversion Transduction Grammar (ITG) of Wu
(1997) is a syntactically motivated algorithm for
producing word-level alignments of pairs of transla-
tionally equivalent sentences in two languages. The
algorithm builds a synchronous parse tree for both
sentences, and assumes that the trees have the same
underlying structure but that the ordering of con-
stituents may differ in the two languages.
This probabilistic, syntax-based approach has in-
spired much subsequent reasearch. Alshawi et
al. (2000) use hierarchical finite-state transducers.
In the tree-to-string model of Yamada and Knight
(2001), a parse tree for one sentence of a transla-
tion pair is projected onto the other string. Melamed
(2003) presents algorithms for synchronous parsing
with more complex grammars, discussing how to
parse grammars with greater than binary branching
and lexicalization of synchronous grammars.
Despite being one of the earliest probabilistic
syntax-based translation models, ITG remains state-
of-the art. Zens and Ney (2003) found that the con-
straints of ITG were a better match to the decod-
ing task than the heuristics used in the IBM decoder
of Berger et al (1996). Zhang and Gildea (2004)
found ITG to outperform the tree-to-string model for
word-level alignment, as measured against human
gold-standard alignments. One explanation for this
result is that, while a tree representation is helpful
for modeling translation, the trees assigned by the
traditional monolingual parsers (and the treebanks
on which they are trained) may not be optimal for
translation of a specific language pair. ITG has the
advantage of being entirely data-driven ? the trees
are derived from an expectation maximization pro-
cedure given only the original strings as input.
In this paper, we extend ITG to condition the
grammar production probabilities on lexical infor-
mation throughout the tree. This model is reminis-
cent of lexicalization as used in modern statistical
parsers, in that a unique head word is chosen for
each constituent in the tree. It differs in that the
head words are chosen through EM rather than de-
terministic rules. This approach is designed to retain
the purely data-driven character of ITG, while giving
the model more information to work with. By condi-
tioning on lexical information, we expect the model
to be able capture the same systematic differences in
languages? grammars that motive the tree-to-string
model, for example, SVO vs. SOV word order or
prepositions vs. postpositions, but to be able to do
so in a more fine-grained manner. The interaction
between lexical information and word order also ex-
plains the higher performance of IBM model 4 over
IBM model 3 for alignment.
We begin by presenting the probability model in
the following section, detailing how we address is-
sues of pruning and smoothing that lexicalization in-
troduces. We present alignment results on a parallel
Chinese-English corpus in Section 3.
475
2 Lexicalization of Inversion Transduction
Grammars
An Inversion Transduction Grammar can generate
pairs of sentences in two languages by recursively
applying context-free bilingual production rules.
Most work on ITG has focused on the 2-normal
form, which consists of unary production rules that
are responsible for generating word pairs:
X ? e/f
and binary production rules in two forms that are
responsible for generating syntactic subtree pairs:
X ? [Y Z]
and
X ? ?Y Z?
The rules with square brackets enclosing the right
hand side expand the left hand side symbol into the
two symbols on the right hand side in the same order
in the two languages, whereas the rules with pointed
brackets expand the left hand side symbol into the
two right hand side symbols in reverse order in the
two languages.
One special case of ITG is the bracketing ITG that
has only one nonterminal that instantiates exactly
one straight rule and one inverted rule. The ITG we
apply in our experiments has more structural labels
than the primitive bracketing grammar: it has a start
symbol S, a single preterminal C, and two interme-
diate nonterminals A and B used to ensure that only
one parse can generate any given word-level align-
ment, as discussed by Wu (1997) and Zens and Ney
(2003).
As an example, Figure 1 shows the alignment and
the corresponding parse tree for the sentence pair Je
les vois / I see them using the unambiguous bracket-
ing ITG.
A stochastic ITG can be thought of as a stochastic
CFG extended to the space of bitext. The indepen-
dence assumptions typifying S-CFGs are also valid
for S-ITGs. Therefore, the probability of an S-ITG
parse is calculated as the product of the probabili-
ties of all the instances of rules in the parse tree. For
instance, the probability of the parse in Figure 1 is:
P (S ? A) ? P (A ? [CB])
? P (B ? ?CC?) ? P (C ? I/Je)
? P (C ? see/vois) ? P (C ? them/les)
It is important to note that besides the bottom-
level word-pairing rules, the other rules are all non-
lexical, which means the structural alignment com-
ponent of the model is not sensitive to the lexical
contents of subtrees. Although the ITG model can
effectively restrict the space of alignment to make
polynomial time parsing algorithms possible, the
preference for inverted or straight rules only pas-
sively reflect the need of bottom level word align-
ment. We are interested in investigating how much
help it would be if we strengthen the structural align-
ment component by making the orientation choices
dependent on the real lexical pairs that are passed up
from the bottom.
The first step of lexicalization is to associate a lex-
ical pair with each nonterminal. The head word pair
generation rules are designed for this purpose:
X ? X(e/f)
The word pair e/f is representative of the lexical
content of X in the two languages.
For binary rules, the mechanism of head selection
is introduced. Now there are 4 forms of binary rules:
X(e/f) ? [Y (e/f)Z]
X(e/f) ? [Y Z(e/f)]
X(e/f) ? ?Y (e/f)Z?
X(e/f) ? ?Y Z(e/f)?
determined by the four possible combinations of
head selections (Y or Z) and orientation selections
(straight or inverted).
The rules for generating lexical pairs at the leaves
of the tree are now predetermined:
X(e/f) ? e/f
Putting them all together, we are able to derive a
lexicalized bilingual parse tree for a given sentence
pair. In Figure 2, the example in Figure 1 is revisited.
The probability of the lexicalized parse is:
P (S ? S(see/vois))
? P (S(see/vois) ? A(see/vois))
? P (A(see/vois) ? [CB(see/vois)])
? P (C ? C(I/Je))
476
Isee
them
Je les vois
C
B
C
A
see/vois them/les
I/Je
S
C
Figure 1: ITG Example
I
see
them
Je les vois
S(see/vois)
C(see/vois)C(I/Je)
C
S
C(them/les)
C
B(see/vois)
A(see/vois)
Figure 2: Lexicalized ITG Example. see/vois is the headword of both the 2x2 cell and the entire alignment.
? P (B(see/vois) ? ?C(see/vois)C?)
? P (C ? C(them/les))
The factors of the product are ordered to show
the generative process of the most probable parse.
Starting from the start symbol S, we first choose
the head word pair for S, which is see/vois in the
example. Then, we recursively expand the lexical-
ized head constituents using the lexicalized struc-
tural rules. Since we are only lexicalizing rather than
bilexicalizing the rules, the non-head constituents
need to be lexicalized using head generation rules
so that the top-down generation process can proceed
in all branches. By doing so, word pairs can appear
at all levels of the final parse tree in contrast with the
unlexicalized parse tree in which the word pairs are
generated only at the bottom.
The binary rules are lexicalized rather than bilexi-
calized.1 This is a trade-off between complexity and
expressiveness. After our lexicalization, the number
of lexical rules, thus the number of parameters in the
statistical model, is still at the order of O(|V ||T |),
where |V | and |T | are the vocabulary sizes of the
1In a sense our rules are bilexicalized in that they condition
on words from both languages; however they do not capture
head-modifier relations within a language.
two languages.
2.1 Parsing
Given a bilingual sentence pair, a synchronous parse
can be built using a two-dimensional extension of
chart parsing, where chart items are indexed by their
nonterminal X , head word pair e/f if specified, be-
ginning and ending positions l,m in the source lan-
guage string, and beginning and ending positions i, j
in the target language string. For Expectation Max-
imization training, we compute lexicalized inside
probabilities ?(X(e/f), l,m, i, j), as well as un-
lexicalized inside probabilities ?(X, l,m, i, j), from
the bottom up as outlined in Algorithm 1.
The algorithm has a complexity of O(N4sN4t ),
where Ns and Nt are the lengths of source and tar-
get sentences respectively. The complexity of pars-
ing for an unlexicalized ITG is O(N3sN3t ). Lexical-
ization introduces an additional factor of O(NsNt),
caused by the choice of headwords e and f in the
pseudocode.
Assuming that the lengths of the source and target
sentences are proportional, the algorithm has a com-
plexity of O(n8), where n is the average length of
the source and target sentences.
477
Algorithm 1 LexicalizedITG(s, t)
for all l,m such that 0 ? l ? m ? Ns do
for all i, j such that 0 ? i ? j ? Nt do
for all e ? {el+1 . . . em} do
for all f ? {fi+1 . . . fj} do
for all n such that l ? n ? m do
for all k such that i ? k ? j do
for all rules X ? Y Z ? G do
?(X(e/f), l,m, i, j) +=
 straight rule, where Y is head
P ([Y (e/f)Z] | X(e/f)) ??(Y (e/f), l, n, i, k) ? ?(Z, n,m, k, j)
 inverted rule, where Y is head
+ P (?Y (e/f)Z? | X(e/f)) ??(Y (e/f), n,m, i, k) ? ?(Z, l, n, k, j)
 straight rule, where Z is head
+ P ([Y Z(e/f)] | X(e/f)) ??(Y, l, n, i, k) ? ?(Z(e/f), n,m, k, j)
 inverted rule, where Z is head
+ P (?Y Z(e/f)? | X(e/f)) ??(Y, n,m, i, k) ? ?(Z(e/f), l, n, k, j)
end for
end for
end for
 word pair generation rule
?(X, l,m, i, j) += P (X(e/f) | X) ??(X(e/f), l,m, i, j)
end for
end for
end for
end for
2.2 Pruning
We need to further restrict the space of alignments
spanned by the source and target strings to make the
algorithm feasible. Our technique involves comput-
ing an estimate of how likely each of the n4 cells in
the chart is before considering all ways of building
the cell by combining smaller subcells. Our figure
of merit for a cell involves an estimate of both the
inside probability of the cell (how likely the words
within the box in both dimensions are to align) and
the outside probability (how likely the words out-
side the box in both dimensions are to align). In
including an estimate of the outside probability, our
technique is related to A* methods for monolingual
parsing (Klein and Manning, 2003), although our
estimate is not guaranteed to be lower than com-
plete outside probabity assigned by ITG. Figure 3(a)
displays the tic-tac-toe pattern for the inside and
outside components of a particular cell. We use
IBM Model 1 as our estimate of both the inside and
outside probabilities. In the Model 1 estimate of
the outside probability, source and target words can
align using any combination of points from the four
outside corners of the tic-tac-toe pattern. Thus in
Figure 3(a), there is one solid cell (corresponding
to the Model 1 Viterbi alignment) in each column,
falling either in the upper or lower outside shaded
corner. This can be also be thought of as squeezing
together the four outside corners, creating a new cell
whose probability is estimated using IBM Model
1. Mathematically, our figure of merit for the cell
(l,m, i, j) is a product of the inside Model 1 proba-
bility and the outside Model 1 probability:
P (f (i,j) | e(l,m)) ? P (f(i,j) | e(l,m)) (1)
= ?|(l,m)|,|(i,j)|
?
t?(i,j)
?
s?{0,(l,m)}
t(ft | es)
? ?|(l,m)|,|(i,j)|
?
t?(i,j)
?
s?{0,(l,m)}
t(ft | es)
478
lm
i j i j
l
m
i j(a) (b) (c)
Figure 3: The tic-tac-toe figure of merit used for pruning bitext cells. The shaded regions in (a) show
alignments included in the figure of merit for bitext cell (l,m, i, j) (Equation 1); solid black cells show the
Model 1 Viterbi alignment within the shaded area. (b) shows how to compute the inside probability of a
unit-width cell by combining basic cells (Equation 2), and (c) shows how to compute the inside probability
of any cell by combining unit-width cells (Equation 3).
where (l,m) and (i, j) represent the complementary
spans in the two languages. ?L1,L2 is the probability
of any word alignment template for a pair of L1-
word source string and L2-word target string, which
we model as a uniform distribution of word-for-
word alignment patterns after a Poisson distribution
of target string?s possible lengths, following Brown
et al (1993). As an alternative, the ? operator can
be replaced by the max operator as the inside opera-
tor over the translation probabilities above, meaning
that we use the Model 1 Viterbi probability as our
estimate, rather than the total Model 1 probability.2
A na??ve implementation would take O(n6) steps
of computation, because there are O(n4) cells, each
of which takes O(n2) steps to compute its Model 1
probability. Fortunately, we can exploit the recur-
sive nature of the cells. Let INS(l,m, i, j) denote
the major factor of our Model 1 estimate of a cell?s
inside probability,
?
t?(i,j)
?
s?{0,(l,m)} t(ft | es). It
turns out that one can compute cells of width one
(i = j) in constant time from a cell of equal width
and lower height:
INS(l,m, j, j) =
?
t?(j,j)
?
s?{0,(l,m)}
t(ft | es)
=
?
s?{0,(l,m)}
t(fj | es)
= INS(l,m? 1, j, j)
+ t(fj | em) (2)
Similarly, one can compute cells of width greater
than one by combining a cell of one smaller width
2The experimental difference of the two alternatives was
small. For our results, we used the max version.
with a cell of width one:
INS(l,m, i, j) =
?
t?(i,j)
?
s?{0,(l,m)}
t(ft | es)
=
?
t?(i,j)
INS(l,m, t, t)
= INS(l,m, i, j ? 1)
? INS(l,m, j, j) (3)
Figure 3(b) and (c) illustrate the inductive compu-
tation indicated by the two equations. Each of the
O(n4) inductive steps takes one additive or mul-
tiplicative computation. A similar dynammic pro-
graming technique can be used to efficiently com-
pute the outside component of the figure of merit.
Hence, the algorithm takes just O(n4) steps to com-
pute the figure of merit for all cells in the chart.
Once the cells have been scored, there can be
many ways of pruning. In our experiments, we ap-
plied beam ratio pruning to each individual bucket of
cells sharing a common source substring. We prune
cells whose probability is lower than a fixed ratio be-
low the best cell for the same source substring. As a
result, at least one cell will be kept for each source
substring. We safely pruned more than 70% of cells
using 10?5 as the beam ratio for sentences up to 25
words. Note that this pruning technique is applica-
ble to both the lexicalized ITG and the conventional
ITG.
In addition to pruning based on the figure of merit
described above, we use top-k pruning to limit the
number of hypotheses retained for each cell. This
is necessary for lexicalized ITG because the number
of distinct hypotheses in the two-dimensional ITG
479
chart has increased to O(N3sN3t ) from O(N2sN2t )
due to the choice one of O(Ns) source language
words and one of O(Nt) target language words as
the head. We keep only the top-k lexicalized items
for a given chart cell of a certain nonterminal Y con-
tained in the cell l,m, i, j. Thus the additional com-
plexity of O(NsNt) will be replaced by a constant
factor.
The two pruning techniques can work for both the
computation of expected counts during the training
process and for the Viterbi-style algorithm for ex-
tracting the most probable parse after training. How-
ever, if we initialize EM from a uniform distribution,
all probabilties are equal on the first iteration, giving
us no basis to make pruning decisions. So, in our
experiments, we initialize the head generation prob-
abilities of the form P (X(e/f) | X) to be the same
as P (e/f | C) from the result of the unlexicalized
ITG training.
2.3 Smoothing
Even though we have controlled the number of pa-
rameters of the model to be at the magnitude of
O(|V ||T |), the problem of data sparseness still ren-
ders a smoothing method necessary. We use back-
ing off smoothing as the solution. The probabilities
of the unary head generation rules are in the form of
P (X(e/f) | X). We simply back them off to the
uniform distribution. The probabilities of the binary
rules, which are conditioned on lexicalized nonter-
minals, however, need to be backed off to the prob-
abilities of generalized rules in the following forms:
P ([Y (?)Z] | X(?))
P ([Y Z(?)] | X(?))
P (?Y (?)Z? | X(?))
P (?Y Z(?)? | X(?))
where ? stands for any lexical pair. For instance,
P ([Y (e/f)Z] | X(e/f)) =
(1 ? ?)PEM ([Y (e/f)Z] | X(e/f))
+ ?P ([Y (?)Z] | X(?))
where
? = 1/(1 + Expected Counts(X(e/f)))
The more often X(e/f) occurred, the more reli-
able are the estimated conditional probabilities with
the condition part being X(e/f).
3 Experiments
We trained both the unlexicalized and the lexical-
ized ITGs on a parallel corpus of Chinese-English
newswire text. The Chinese data were automati-
cally segmented into tokens, and English capitaliza-
tion was retained. We replaced words occurring only
once with an unknown word token, resulting in a
Chinese vocabulary of 23,783 words and an English
vocabulary of 27,075 words.
In the first experiment, we restricted ourselves to
sentences of no more than 15 words in either lan-
guage, resulting in a training corpus of 6,984 sen-
tence pairs with a total of 66,681 Chinese words and
74,651 English words. In this experiment, we didn?t
apply the pruning techniques for the lexicalized ITG.
In the second experiment, we enabled the pruning
techniques for the LITG with the beam ratio for the
tic-tac-toe pruning as 10?5 and the number k for the
top-k pruning as 25. We ran the experiments on sen-
tences up to 25 words long in both languages. The
resulting training corpus had 18,773 sentence pairs
with a total of 276,113 Chinese words and 315,415
English words.
We evaluate our translation models in terms of
agreement with human-annotated word-level align-
ments between the sentence pairs. For scoring the
Viterbi alignments of each system against gold-
standard annotated alignments, we use the alignment
error rate (AER) of Och and Ney (2000), which mea-
sures agreement at the level of pairs of words:
AER = 1 ? |A ?GP | + |A ?GS ||A| + |GS |
where A is the set of word pairs aligned by the
automatic system, GS is the set marked in the
gold standard as ?sure?, and GP is the set marked
as ?possible? (including the ?sure? pairs). In our
Chinese-English data, only one type of alignment
was marked, meaning that GP = GS .
In our hand-aligned data, 20 sentence pairs are
less than or equal to 15 words in both languages,
and were used as the test set for the first experiment,
and 47 sentence pairs are no longer than 25 words in
either language and were used to evaluate the pruned
480
Alignment
Precision Recall Error Rate
IBM Model 1 .59 .37 .54
IBM Model 4 .63 .43 .49
ITG .62 .47 .46
Lexicalized ITG .66 .50 .43
Table 1: Alignment results on Chinese-English corpus (? 15 words on both sides). Full ITG vs. Full LITG
Alignment
Precision Recall Error Rate
IBM Model 1 .56 .42 .52
IBM Model 4 .67 .43 .47
ITG .68 .52 .40
Lexicalized ITG .69 .51 .41
Table 2: Alignment results on Chinese-English corpus (? 25 words on both sides). Full ITG vs. Pruned
LITG
LITG against the unlexicalized ITG.
A separate development set of hand-aligned sen-
tence pairs was used to control overfitting. The sub-
set of up to 15 words in both languages was used for
cross-validating in the first experiment. The subset
of up to 25 words in both languages was used for the
same purpose in the second experiment.
Table 1 compares results using the full (unpruned)
model of unlexicalized ITG with the full model of
lexicalized ITG.
The two models were initialized from uniform
distributions for all rules and were trained until AER
began to rise on our held-out cross-validation data,
which turned out to be 4 iterations for ITG and 3
iterations for LITG.
The results from the second experiment are shown
in Table 2. The performance of the full model of un-
lexicalized ITG is compared with the pruned model
of lexicalized ITG using more training data and eval-
uation data.
Under the same check condition, we trained ITG
for 3 iterations and the pruned LITG for 1 iteration.
For comparison, we also included the results from
IBM Model 1 and Model 4. The numbers of itera-
tions for the training of the IBM models were cho-
sen to be the turning points of AER changing on the
cross-validation data.
4 Discussion
As shown by the numbers in Table 1, the full lexical-
ized model produced promising alignment results on
sentence pairs that have no more than 15 words on
both sides. However, due to its prohibitive O(n8)
computational complexity, our C++ implementation
of the unpruned lexicalized model took more than
500 CPU hours, which were distributed over multi-
ple machines, to finish one iteration of training. The
number of CPU hours would increase to a point that
is unacceptable if we doubled the average sentence
length. Some type of pruning is a must-have. Our
pruned version of LITG controlled the running time
for one iteration to be less than 1200 CPU hours, de-
spite the fact that both the number of sentences and
the average length of sentences were more than dou-
bled. To verify the safety of the tic-tac-toe pruning
technique, we applied it to the unlexicalized ITG us-
ing the same beam ratio (10?5) and found that the
AER on the test data was not changed. However,
whether or not the top-k lexical head pruning tech-
nique is equally safe remains a question. One no-
ticeable implication of this technique for training is
the reliance on initial probabilities of lexical pairs
that are discriminative enough. The comparison of
results for ITG and LITG in Table 2 and the fact that
AER began to rise after only one iteration of train-
ing seem to indicate that keeping few distinct lex-
ical heads caused convergence on a suboptimal set
481
of parameters, leading to a form of overfitting. In
contrast, overfitting did not seem to be a problem for
LITG in the unpruned experiment of Table 1, despite
the much larger number of parameters for LITG than
for ITG and the smaller training set.
We also want to point out that for a pair of long
sentences, it would be hard to reflect the inherent
bilingual syntactic structure using the lexicalized bi-
nary bracketing parse tree. In Figure 2, A(see/vois)
echoes IP (see/vois) and B(see/vois) echoes
V P (see/vois) so that it means IP (see/vois) is not
inverted from English to French but its right child
V P (see/vois) is inverted. However, for longer sen-
tences with more than 5 levels of bracketing and the
same lexicalized nonterminal repeatedly appearing
at different levels, the correspondences would be-
come less linguistically plausible. We think the lim-
itations of the bracketing grammar are another rea-
son for not being able to improve the AER of longer
sentence pairs after lexicalization.
The space of alignments that is to be considered
by LITG is exactly the space considered by ITG
since the structural rules shared by them define the
alignment space. The lexicalized ITG is designed
to be more sensitive to the lexical influence on the
choices of inversions so that it can find better align-
ments. Wu (1997) demonstrated that for pairs of
sentences that are less than 16 words, the ITG align-
ment space has a good coverage over all possibili-
ties. Hence, it?s reasonable to see a better chance
of improving the alignment result for sentences less
than 16 words.
5 Conclusion
We presented the formal description of a Stochastic
Lexicalized Inversion Transduction Grammar with
its EM training procedure, and proposed specially
designed pruning and smoothing techniques. The
experiments on a parallel corpus of Chinese and En-
glish showed that lexicalization helped for aligning
sentences of up to 15 words on both sides. The prun-
ing and the limitations of the bracketing grammar
may be the reasons that the result on sentences of up
to 25 words on both sides is not better than that of
the unlexicalized ITG.
Acknowledgments We are very grateful to Re-
becca Hwa for assistance with the Chinese-English
data, to Kevin Knight and Daniel Marcu for their
feedback, and to the authors of GIZA. This work
was partially supported by NSF ITR IIS-09325646
and NSF ITR IIS-0428020.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45?60.
Adam Berger, Peter Brown, Stephen Della Pietra, Vin-
cent Della Pietra, J. R. Fillett, Andrew Kehler, and
Robert Mercer. 1996. Language translation apparatus
and method of using context-based tanslation models.
United States patent 5,510,981.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: Fast exact viterbi parse selection. In Proceed-
ings of the 2003 Meeting of the North American chap-
ter of the Association for Computational Linguistics
(NAACL-03).
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of the 2003 Meeting
of the North American chapter of the Association for
Computational Linguistics (NAACL-03), Edmonton.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Conference of the Association for Compu-
tational Linguistics (ACL-00), pages 440?447, Hong
Kong, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Conference of the Association for Com-
putational Linguistics (ACL-01), Toulouse, France.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
Sapporo, Japan.
Hao Zhang and Daniel Gildea. 2004. Syntax-based
alignment: Supervised or unsupervised? In Proceed-
ings of the 20th International Conference on Compu-
tational Linguistics (COLING-04), Geneva, Switzer-
land, August.
482
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 279?286,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Factoring Synchronous Grammars By Sorting
Daniel Gildea
Computer Science Dept.
University of Rochester
Rochester, NY 14627
Giorgio Satta
Dept. of Information Eng?g
University of Padua
I-35131 Padua, Italy
Hao Zhang
Computer Science Dept.
University of Rochester
Rochester, NY 14627
Abstract
Synchronous Context-Free Grammars
(SCFGs) have been successfully exploited
as translation models in machine trans-
lation applications. When parsing with
an SCFG, computational complexity
grows exponentially with the length of the
rules, in the worst case. In this paper we
examine the problem of factorizing each
rule of an input SCFG to a generatively
equivalent set of rules, each having the
smallest possible length. Our algorithm
works in time O(n log n), for each rule
of length n. This improves upon previous
results and solves an open problem about
recognizing permutations that can be
factored.
1 Introduction
Synchronous Context-Free Grammars (SCFGs)
are a generalization of the Context-Free Gram-
mar (CFG) formalism to simultaneously produce
strings in two languages. SCFGs have a wide
range of applications, including machine transla-
tion, word and phrase alignments, and automatic
dictionary construction. Variations of SCFGs go
back to Aho and Ullman (1972)?s Syntax-Directed
Translation Schemata, but also include the In-
version Transduction Grammars in Wu (1997),
which restrict grammar rules to be binary, the syn-
chronous grammars in Chiang (2005), which use
only a single nonterminal symbol, and the Multi-
text Grammars in Melamed (2003), which allow
independent rewriting, as well as other tree-based
models such as Yamada and Knight (2001) and
Galley et al (2004).
When viewed as a rewriting system, an SCFG
generates a set of string pairs, representing some
translation relation. We are concerned here with
the time complexity of parsing such a pair, accord-
ing to the grammar. Assume then a pair with each
string having a maximum length of N , and con-
sider an SCFG G of size |G|, with a bound of n
nonterminals in the right-hand side of each rule in
a single dimension, which we call below the rank
of G. As an upper bound, parsing can be carried
out in time O(|G|Nn+4) by a dynamic program-
ming algorithm maintaining continuous spans in
one dimension. As a lower bound, parsing strate-
gies with discontinuous spans in both dimensions
can take time ?(|G|N c?n) for unfriendly permu-
tations (Satta and Peserico, 2005). A natural ques-
tion to ask then is: What if we could reduce the
rank of G, preserving the generated translation?
As in the case of CFGs, one way of doing this
would be to factorize each single rule into several
rules of rank strictly smaller than n. It is not diffi-
cult to see that this would result in a new grammar
of size at most 2 ? |G|. In the time complexities
reported above, we see that such a size increase
would be more than compensated by the reduction
in the degree of the polynomial in N . We thus
conclude that a reduction in the rank of an SCFG
would result in more efficient parsing algorithms,
for most common parsing strategies.
In the general case, normal forms with bounded
rank are not admitted by SCFGs, as shown in (Aho
and Ullman, 1972). Nonetheless, an SCFG with a
rank of n may not necessarily meet the worst case
of Aho and Ullman (1972). It is then reasonable
to ask if our SCFG G can be factorized, and what
is the smallest rank k < n that can be obtained
in this way. This paper answers these two ques-
tions, by providing an algorithm that factorizes the
rules of an input SCFG, resulting in a new, genera-
tively equivalent, SCFG with rank k as low as pos-
sible. The algorithm works in time O(n log n) for
each rule, regardless of the rank k of the factorized
rules. As discussed above, in this way we achieve
an improvement of the parsing time for SCFGs,
obtaining an upper bound of O(|G|N k+4) by us-
ing a parsing strategy that maintains continuous
279
1,2
1,2
2,1
2 1
1,2
3 4
3,1,4,2
7 5 8 6
4,1,3,5,2
7 1 2,4,1,3
4 6 3 5
8 2
Figure 1: Two permutation trees. The permuta-
tions associated with the leaves can be produced
by composing the permutations at the internal
nodes.
spans in one dimension.
Previous work on this problem has been pre-
sented in Zhang et al (2006), where a method is
provided for casting an SCFG to a form with rank
k = 2. If generalized to any value of k, that algo-
rithm would run in time O(n2). We thus improve
existing factorization methods by almost a factor
of n. We also solve an open problem mentioned
by Albert et al (2003), who pose the question of
whether irreducible, or simple, permutations can
be recognized in time less than ?(n2).
2 Synchronous CFGs and permutation
trees
We begin by describing the synchronous CFG for-
malism, which is more rigorously defined by Aho
and Ullman (1972) and Satta and Peserico (2005).
Let us consider strings defined over some set of
nonterminal and terminal symbols, as defined for
CFGs. We say that two such strings are syn-
chronous if some bijective relation is given be-
tween the occurrences of the nonterminals in the
two strings. A synchronous context-free gram-
mar (SCFG) is defined as a CFG, with the dif-
ference that it uses synchronous rules of the form
[A1 ? ?1, A2 ? ?2], with A1, A2 nonterminalsand ?1, ?2 synchronous strings. We can use pro-duction [A1 ? ?1, A2 ? ?2] to rewrite any syn-chronous strings [?11A1?12, ?21A2?22] into thesynchronous strings [?11?1?12, ?21?2?22], un-der the condition that the indicated occurrences
of A1 and A2 be related by the bijection asso-ciated with the source synchronous strings. Fur-
thermore, the bijective relation associated with the
target synchronous strings is obtained by compos-
ing the relation associated with the source syn-
chronous strings and the relation associated with
synchronous pair [?1, ?2], in the most obviousway.
As in standard constructions that reduce the
rank of a CFG, in this paper we focus on each
single synchronous rule and factorize it into syn-
chronous rules of lower rank. If we view the bijec-
tive relation associated with a synchronous rule as
a permutation, we can further reduce our factoriza-
tion problem to the problem of factorizing a per-
mutation of arity n into the composition of several
permutations of arity k < n. Such factorization
can be represented as a tree of composed permuta-
tions, called in what follows a permutation tree.
A permutation tree can be converted into a set of
k-ary SCFG rules equivalent to the input rule. For
example, the input rule:
[ X ? A(1)B(2)C(3)D(4)E(5)F (6)G(7)H(8),
X ? B(2)A(1)C(3)D(4)G(7)E(5)H(8)F (6) ]
yields the permutation tree of Figure 1(left). In-
troducing a new grammar nonterminal Xi for eachinternal node of the tree yields an equivalent set of
smaller rules:
[ X ? X(1)1 X
(2)
2 , X ? X
(1)
1 X
(2)
2 ]
[ X1 ? X(1)3 X
(2)
4 , X1 ? X
(1)
3 X
(2)
4 ]
[ X3 ? A(1)B(2), X3 ? B(2)A(1) ]
[ X4 ? C(1)D(2), X4 ? C(1)D(2) ]
[ X2 ? E(1)F (2)G(3)H(4),
X2 ? G(3)E(1)H(4)F (2) ]
In the case of stochastic grammars, the rule cor-
responding to the root of the permutation tree is
assigned the original rule?s probability, while all
other rules, associated with new grammar nonter-
minals, are assigned probability 1. We process
each rule of an input SCFG independently, pro-
ducing an equivalent grammar with the smallest
possible arity.
3 Factorization Algorithm
In this section we specify and discuss our factor-
ization algorithm. The algorithm takes as input a
permutation defined on the set {1, ? ? ? , n}, repre-
senting a rule of some SCFG, and provides a per-
mutation tree of arity k ? n for that permutation,
with k as small as possible.
Permutation trees covering a given input permu-
tation are unambiguous with the exception of se-
quences of binary rules of the same type (either
inverted or straight) (Albert et al, 2003). Thus,
when factorizing a permutation into a permutation
280
tree, it is safe to greedily reduce a subsequence
into a new subtree as soon as a subsequence is
found which represents a continuous span in both
dimensions of the permutation matrix1 associated
with the input permutation. For space reasons, we
omit the proof, but emphasize that any greedy re-
duction turns out to be either necessary, or equiv-
alent to the other alternatives.
Any sequences of binary rules can be rear-
ranged into a normalized form (e.g. always left-
branching) as a postprocessing step, if desired.
The top-level structure of the algorithm exploits
a divide-and-conquer approach, and is the same as
that of the well-known mergesort algorithm (Cor-
men et al, 1990). We work on subsequences of
the original permutation, and ?merge? neighbor-
ing subsequences into successively longer subse-
quences, combining two subsequences of length
2i into a subsequence of length 2i+1 until we have
built one subsequence spanning the entire permu-
tation. If each combination of subsequences can
be performed in linear time, then the entire permu-
tation can be processed in time O(n log n). As in
the case of mergesort, this is an application of the
so-called master theorem (Cormen et al, 1990).
As the algorithm operates, we will maintain the
invariant that we must have built all subtrees of
the target permutation tree that are entirely within
a given subsequence that has been processed. This
is analogous to the invariant in mergesort that all
processed subsequences are in sorted order. When
we combine two subsequences, we need only build
nodes in the tree that cover parts of both sub-
sequences, but are entirely within the combined
subsequence. Thus, we are looking for subtrees
that span the midpoint of the combined subse-
quence, but have left and right boundaries within
the boundaries of the combined subsequence. In
what follows, this midpoint is called the split
point.
From this invariant, we will be guaranteed to
have a complete, correct permutation tree at the
end of last subsequence combination. An example
of the operation of the general algorithm is shown
in Figure 2. The top-level structure of the algo-
rithm is presented in function KARIZE of Figure 3.
There may be more than one reduction neces-
sary spanning a given split point when combin-
ing two subsequences. Function MERGE in Fig-
1A permutation matrix is a way of representing a permuta-
tion, and is obtained by rearranging the row (or the columns)
of an identity matrix, according to the permutation itself.
2 1 3 4 7 5 8 6
2,1
2 1
1,2
3 4 7 5 8 6
1,2
2,1
2 1
1,2
3 4
3,1,4,2
7 5 8 6
1,2
1,2
2,1
2 1
1,2
3 4
3,1,4,2
7 5 8 6
Figure 2: Recursive combination of permutation
trees. Top row, the input permutation. Second row,
after combination into sequences of length two, bi-
nary nodes have been built where possible. Third
row, after combination into sequences of length
four; bottom row, the entire output tree.
ure 3 initializes certain data structures described
below, and then checks for reductions repeatedly
until no further reduction is possible. It looks first
for the smallest reduction crossing the split point
of the subsequences being combined. If SCAN,
described below, finds a valid reduction, it is com-
mitted by calling REDUCE. If a reduction is found,
we look for further reductions crossing either the
left or right boundary of the new reduction, repeat-
ing until no further reductions are possible. Be-
cause we only need to find reductions spanning
the original split point at a given combination step,
this process is guaranteed to find all reductions
needed.
We now turn to the problem of identifying a
specific reduction to be made across a split point,
which involves identifying the reduction?s left and
right boundaries. Given a subsequence and can-
didate left and right boundaries for that subse-
quence, the validity of making a reduction over
this span can be tested by verifying whether the
span constitutes a permuted sequence, that is,
a permutation of a contiguous sequence of inte-
gers. Since the starting permutation is defined
on a set {1, 2, ? ? ? , n}, we have no repeated in-
tegers in our subsequences, and the above condi-
281
function KARIZE(pi)
. initialize with identity mapping
h? hmin? hmax? (0..|pi|);
. mergesort core
for size? 1; size ? |pi|; size? size * 2 do
for min? 0;
min < |pi|-size+1;
min? min + 2 * size do
div = min + size - 1;
max? min(|pi|, min + 2*size - 1);
MERGE(min, div, max);
function MERGE(min, div, max)
. initialize h
sort h[min..max] according to pi[i];
sort hmin[min..max] according to pi[i];
sort hmax[min..max] according to pi[i];
. merging sorted list takes linear time
. initialize v
for i? min; i ? max; i? i + 1 do
v [ h[i] ]? i;
. check if start of new reduced block
if i = min or
hmin[i] 6= hmin[i-1] then
vmin? i;
vmin[ h[i] ]? vmin;
for i? max; i ? min; i? i - 1 do
. check if start of new reduced block
if i = max or
hmax[i] 6= hmax[i+1] then
vmax? i ;
vmax[ h[i] ]? vmax;
. look for reductions
if SCAN(div) then
REDUCE(scanned reduction);
while SCAN(left) or SCAN(right) do
REDUCE(smaller reduction);
function REDUCE(left, right, bot, top)
for i? bot..top do
hmin[i]? left;
hmax[i]? right;
for i? left..right do
vmin[i]? bot;
vmax[i]? top;
print ?reduce:? left..right ;
Figure 3: KARIZE: Top level of algorithm, iden-
tical to that of mergesort. MERGE: combines two
subsequences of size 2i into new subsequence of
size 2i+1. REDUCE: commits reduction by updat-
ing min and max arrays.
tion can be tested by scanning the span in ques-
tion, finding the minimum and maximum integers
in the span, and checking whether their difference
is equal to the length of the span minus one. Be-
low we call this condition the reduction test. As
an example of the reduction test, consider the sub-
sequence (7, 5, 8, 6), and take the last three ele-
ments, (5, 8, 6), as a candidate span. We see that
5 and 8 are the minimum and maximum integers
in the corresponding span, respectively. We then
compute 8 ? 5 = 3, while the length of the span
minus one is 2, implying that no reduction is possi-
ble. However, examining the entire subsequence,
the minimum is 5 and the maximum is 8, and
8 ? 5 = 3, which is the length of the span minus
one. We therefore conclude that we can reduce
that span by means of some permutation, that is,
parse the span by means of a node in the permuta-
tion tree. This reduction constitutes the 4-ary node
in the permutation tree of Figure 2.
A trivial implementation of the reduction test
would be to tests all combinations of left and right
boundaries for the new reduction. Unfortunately,
this would take time ?(n2) for a single subse-
quence combination step, whereas to achieve the
overall O(n log n) complexity we need linear time
for each combination step.
It turns out that the boundaries of the next re-
duction, covering a given split point, can be com-
puted in linear time with the technique shown in
function SCAN of Figure 5. We start with left and
right candidate boundaries at the two points imme-
diately to the left and right of the split point, and
then repeatedly check whether the current left and
right boundaries identify a permuted sequence by
applying the reduction test, and move the left and
right boundaries outward as necessary, as soon as
?missing? integers are identified outside the cur-
rent boundaries, as explained below. We will show
that, as we move outward, the number of possible
configurations achieved for the positions of the left
and the right boundaries is linearly bounded in the
length of the combined subsequence (as opposed
to quadratically bounded).
In order to efficiently implement the above idea,
we will in fact maintain four boundaries for the
candidate reduction, which can be visualized as
the left, right, top and bottom boundaries in the
permutation matrix. No explicit representation
of the permutation matrix itself is constructed, as
that would require quadratic time. Rather, we
282
7 1 4 6 3 5 8 2pi 4
7
2
1
1
3
2
4
4
3
6
1
6
3
8
7
5
5
8
8
6
5
2
7
v
pi
h
Figure 4: Permutation matrix for input permuta-
tion pi (left) and within-subsequence permutation
v (right) for subsequences of size four.
maintain two arrays: h, which maps from vertical
to horizontal positions within the current subse-
quence, and v which maps from horizontal to ver-
tical positions. These arrays represent the within-
subsequence permutation obtained by sorting the
elements of each subsequence according to the
input permutation, while keeping each element
within its block, as shown in Figure 4.
Within each subsequence, we alternate between
scanning horizontally from left to right, possibly
extending the top and bottom boundaries (Figure 5
lines 9 to 14), and scanning vertically from bottom
to top, possibly extending the left and right bound-
aries (lines 20 to 26). Each extension is forced
when, looking at the within-subsequence permuta-
tion, we find that some element is within the cur-
rent boundaries in one dimension but outside the
boundaries in the other. If the distance between
vertical boundaries is larger in the input permu-
tation than in the subsequence permutation, nec-
essary elements are missing from the current sub-
sequence and no reduction is possible at this step
(line 18). When all necessary elements are present
in the current subsequence and no further exten-
sions are necessary to the boundaries (line 30), we
have satisfied the reduction test on the input per-
mutation, and make a reduction.
The trick used to keep the iterative scanning lin-
ear is that we skip the subsequence scanned on the
previous iteration on each scan, in both the hori-
zontal and vertical directions. Lines 13 and 25 of
Figure 5 perform this skip by advancing the x and y
counters past previously scanned regions. Consid-
ering the horizontal scan of lines 9 to 14, in a given
iteration of the while loop, we scan only the items
between newleft and left and between right and
newright. On the next iteration of the while loop,
the newleft boundary has moved further to the left,
1: function SCAN (div)
2: left???;
3: right???;
4: newleft? div;
5: newright? div + 1 ;
6: newtop???;
7: newbot??;
8: while 1 do
. horizontal scan
9: for x? newleft; x ? newright ; do
10: newtop? max(newtop, vmax[x]);
11: newbot? min(newbot, vmin[x]);
. skip to end of reduced block
12: x? hmax[vmin[x]] + 1;
. skip section scanned on last iter
13: if x = left then
14: x? right + 1;
15: right? newright;
16: left? newleft;
. the reduction test
17: if newtop - newbot <
18: pi[h[newtop]] - pi[h[newbot]] then
19: return (0);
. vertical scan
20: for y? newbot; y ? newtop ; do
21: newright?
22: max(newright, hmax[y]);
23: newleft? min(newleft, hmin[y]);
. skip to end of reduced block
24: y? vmax[hmin[y]] + 1;
. skip section scanned on last iter
25: if y = bot then
26: y? top + 1;
27: top? newtop;
28: bot? newbot;
. if no change to boundaries, reduce
29: if newright = right
30: and newleft = left then
31: return (1, left, right, bot, top);
Figure 5: Linear time function to check for a sin-
gle reduction at split point div.
283
while the variable left takes the previous value of
newleft, ensuring that the items scanned on this it-
eration are distinct from those already processed.
Similarly, on the right edge we scan new items,
between right and newright. The same analysis
applies to the vertical scan. Because each item in
the permutation is scanned only once in the verti-
cal direction and once in the horizontal direction,
the entire call to SCAN takes linear time, regard-
less of the number of iterations of the while loop
that are required.
We must further show that each call to MERGE
takes only linear time, despite that fact that it
may involve many calls to SCAN. We accom-
plish this by introducing a second type of skipping
in the scans, which advances past any previously
reduced block in a single step. In order to skip
past previous reductions, we maintain (in func-
tion REDUCE) auxiliary arrays with the minimum
and maximum positions of the largest block each
point has been reduced to, in both the horizontal
and vertical dimensions. We use these data struc-
tures (hmin, hmax, vmin, vmax) when advancing to
the next position of the scan in lines 12 and 24 of
Figure 5. Because each call to SCAN skips items
scanned by previous calls, each item is scanned
at most twice across an entire call to MERGE,
once when scanning across a new reduction?s left
boundary and once when scanning across the right
boundary, guaranteeing that MERGE completes in
linear time.
4 An Example
In this section we examine the operation of the
algorithm on a permutation of length eight, re-
sulting in the permutation tree of Figure 1(right).
We will build up our analysis of the permutation
by starting with individual items of the input per-
mutation and building up subsequences of length
2, 4, and finally 8. In our example permutation,
(7, 1, 4, 6, 3, 5, 8, 2), no reductions can be made
until the final combination step, in which one per-
mutation of size 4 is used, and one of size 5.
We begin with the input permutation along the
bottom of Figure 6a. We represent the interme-
diate data structures h, hmin, and hmax along the
vertical axis of the figure; these three arrays are all
initialized to be the sequence (1, 2, ? ? ? , 8).
Figure 6b shows the combination of individual
items into subsequences of length two. Each new
subsequence of the h array is sorted according to
a)
7
1
1
1
111
1
2
2
2
222
4
3
3
3
333
6
4
4
4
444
3
5
5
5
555
5
6
6
6
666
8
7
7
7
777
2
8
8
8
888
pi
v
vmin
vmax
hhm
in
hm
ax
1
7
1
2
1
2
3
4
3
4
6
4
5
3
5
6
5
6
7
8
7
8
2
8
v
pi
h
b)
7
2
2
2
222
1
1
1
1
111
4
3
3
3
333
6
4
4
4
444
3
5
5
5
555
5
6
6
6
666
8
8
8
8
888
2
7
7
7
777
pi
v
vmin
vmax
hhm
in
hm
ax
2
7
2
1
1
1
3
4
3
4
6
4
5
3
5
6
5
6
8
8
8
7
2
7
v
pi
h
c)
7
4
4
4
222
1
1
1
1
333
4
2
2
2
444
6
3
3
3
111
3
6
6
6
888
5
7
7
7
555
8
8
8
8
666
2
5
5
5
777
pi
v
vmin
vmax
hhm
in
hm
ax
4
7
2
1
1
3
2
4
4
3
6
1
6
3
8
7
5
5
8
8
6
5
2
7
v
pi
h
Figure 6: Steps in an example computation,
with input permutation pi on left and within-
subsequence permutation described by v array on
right. Panel (a) shows initial blocks of unit size,
(b) shows combination of unit blocks into blocks
of size two, and (c) size two into size four. No
reductions are possible in these stages; example
continued in next figure.
284
a)
7
7
7
7
222
1
1
1
1
888
4
4
4
4
555
6
6
6
6
333
3
3
3
3
666
5
5
5
5
444
8
8
8
8
111
2
2
2
2
777
pi
v
vmin
vmax
hhm
in
hm
ax b)
7
7
7
7
222
1
1
1
1
888
4
4
3
6
536
6
6
3
6
336
3
3
3
6
636
5
5
3
6
436
8
8
8
8
111
2
2
2
2
777
pi
v
vmin
vmax
hhm
in
hm
ax
Left and right boundaries are initialized
to be adjacent to horizontal split point.
Vertical scan shows left and right bound-
aries must be extended. Permutation of
size four is reduced.
c)
7
7
7
7
222
1
1
1
1
888
4
4
3
6
536
6
6
3
6
336
3
3
3
6
636
5
5
3
6
436
8
8
8
8
111
2
2
2
2
777
pi
v
vmin
vmax
hhm
in
hm
ax d)
7
7
7
7
222
1
1
1
1
888
4
4
3
6
536
6
6
3
6
336
3
3
3
6
636
5
5
3
6
436
8
8
8
8
111
2
2
2
2
777
pi
v
vmin
vmax
hhm
in
hm
ax
Search for next reduction: left and right
boundaries initialized to be adjacent to
left edge of previous reduction.
Vertical scan shows right boundary must
be extended.
e)
7
7
7
7
222
1
1
1
1
888
4
4
3
6
536
6
6
3
6
336
3
3
3
6
636
5
5
3
6
436
8
8
8
8
111
2
2
2
2
777
pi
v
vmin
vmax
hhm
in
hm
ax f)
7
7
1
8
218
1
1
1
8
818
4
4
1
8
518
6
6
1
8
318
3
3
1
8
618
5
5
1
8
418
8
8
1
8
118
2
2
1
8
718
pi
v
vmin
vmax
hhm
in
hm
ax
Horizontal scan shows top boundary must
be extended.
Vertical scan shows left boundary must
be extended. Permutation of size five is
reduced.
Figure 7: Steps in scanning for final combination of subsequences, where v = pi. Area within current
left, right, top and bottom boundaries is shaded; darker shading indicates a reduction. In each scan, the
span scanned in the previous panel is skipped over.
285
the vertical position of the dots in the correspond-
ing columns. Thus, because pi[7] = 8 > pi[8] = 2,
we swap 7 and 8 in the h array. The algorithm
checks whether any reductions can be made at this
step by computing the difference between the in-
tegers on each side of each split point. Because
none of the pairs of integers in are consecutive, no
reductions are made at this step.
Figure 6c shows the combination the pairs
into subsequences of length four. The two split
points to be examined are between the second and
third position, and the sixth and seventh position.
Again, no reductions are possible.
Finally we combine the two subsequences of
length four to complete the analysis of the entire
permutation. The split point is between the fourth
and fifth positions of the input permutation, and
in the first horizontal scan of these two positions,
we see that pi[4] = 6 and pi[5] = 3, meaning our
top boundary will be 6 and our bottom boundary
3, shown in Figure 7a. Scanning vertically from
position 3 to 6, we see horizontal positions 5, 3,
6, and 4, giving the minimum, 3, as the new left
boundary and the maximum, 6, as the new right
boundary, shown in Figure 7b. We now perform
another horizontal scan starting at position 3, but
then jumping directly to position 6, as horizontal
positions 4 and 5 were scanned previously. Af-
ter this scan, the minimum vertical position seen
remains 3, and the maximum vertical position is
still 6. At this point, because we have the same
boundaries as on the previous scan, we can stop
and verify whether the region determined by our
current boundaries has the same length in the ver-
tical and horizontal dimensions. Both dimensions
have length four, meaning that we have found a
subsequence that is continuous in both dimensions
and can safely be reduced, as shown in Figure 6d.
After making this reduction, we update the hmin
array to have all 3?s for the newly reduced span,
and update hmax to have all sixes. We then check
whether further reductions are possible covering
this split point. We repeat the process of scan-
ning horizontally and vertically in Figure 7c-f,
this time skipping the span just reduced. One fur-
ther reduction is possible, covering the entire input
permutation, as shown in Figure 7f.
5 Conclusion
The algorithm above not only identifies whether
a permutation can be factored into a composi-
tion of permutations, but also returns the factor-
ization that minimizes the largest rule size, in time
O(n log n). The factored SCFG with rules of size
at most k can be used to synchronously parse
in time O(Nk+4) by dynamic programming with
continuous spans in one dimension.
As mentioned in the introduction, the optimal
parsing strategy for SCFG rules with a given
permutation may involve dynamic programming
states with discontinuous spans in both dimen-
sions. Whether these optimal parsing strategies
can be found efficiently remains an interesting
open problem.
Acknowledgments This work was partially sup-
ported by NSF ITR IIS-09325646 and NSF ITR
IIS-0428020.
References
Albert V. Aho and Jeffery D. Ullman. 1972. The
Theory of Parsing, Translation, and Compiling, vol-
ume 1. Prentice-Hall, Englewood Cliffs, NJ.
M. H. Albert, M. D. Atkinson, and M. Klazar. 2003.
The enumeration of simple permutations. Journal
of Integer Sequences, 6(03.4.4):18 pages.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL-05, pages 263?270.
Thomas H. Cormen, Charles E. Leiserson, and
Ronald L. Rivest. 1990. Introduction to algorithms.
MIT Press, Cambridge, MA.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT/NAACL.
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of HLT/NAACL.
Giorgio Satta and Enoch Peserico. 2005. Some com-
putational complexity results for synchronous
context-free grammars. In Proceedings of
HLT/EMNLP, pages 803?810.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of ACL-01.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of HLT/NAACL.
286
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 539?546,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Stochastic Iterative Alignment for Machine Translation Evaluation
Ding Liu and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
A number of metrics for automatic eval-
uation of machine translation have been
proposed in recent years, with some met-
rics focusing on measuring the adequacy
of MT output, and other metrics focus-
ing on fluency. Adequacy-oriented met-
rics such as BLEU measure n-gram over-
lap of MT outputs and their references, but
do not represent sentence-level informa-
tion. In contrast, fluency-oriented metrics
such as ROUGE-W compute longest com-
mon subsequences, but ignore words not
aligned by the LCS. We propose a metric
based on stochastic iterative string align-
ment (SIA), which aims to combine the
strengths of both approaches. We com-
pare SIA with existing metrics, and find
that it outperforms them in overall evalu-
ation, and works specially well in fluency
evaluation.
1 Introduction
Evaluation has long been a stumbling block in
the development of machine translation systems,
due to the simple fact that there are many correct
translations for a given sentence. Human evalu-
ation of system output is costly in both time and
money, leading to the rise of automatic evalua-
tion metrics in recent years. In the 2003 Johns
Hopkins Workshop on Speech and Language En-
gineering, experiments on MT evaluation showed
that BLEU and NIST do not correlate well with
human judgments at the sentence level, even when
they correlate well over large test sets (Blatz et
al., 2003). Liu and Gildea (2005) also pointed
out that due to the limited references for every
MT output, using the overlapping ratio of n-grams
longer than 2 did not improve sentence level eval-
uation performance of BLEU. The problem leads
to an even worse result in BLEU?S fluency eval-
uation, which is supposed to rely on the long n-
grams. In order to improve sentence-level evalu-
ation performance, several metrics have been pro-
posed, including ROUGE-W, ROUGE-S (Lin and
Och, 2004) and METEOR (Banerjee and Lavie,
2005). ROUGE-W differs from BLEU and NIST
in that it doesn?t require the common sequence be-
tween MT output and the references to be consec-
utive, and thus longer common sequences can be
found. There is a problem with loose-sequence-
based metrics: the words outside the longest com-
mon sequence are not considered in the metric,
even if they appear both in MT output and the
reference. ROUGE-S is meant to alleviate this
problem by computing the common skipped bi-
grams instead of the LCS. But the price ROUGE-
S pays is falling back to the shorter sequences and
losing the advantage of long common sequences.
METEOR is essentially a unigram based metric,
which prefers the monotonic word alignment be-
tween MT output and the references by penalizing
crossing word alignments. There are two prob-
lems with METEOR. First, it doesn?t consider
gaps in the aligned words, which is an important
feature for evaluating the sentence fluency; sec-
ond, it cannot use multiple references simultane-
ously.1 ROUGE and METEOR both use WordNet
and Porter Stemmer to increase the chance of the
MT output words matching the reference words.
Such morphological processing and synonym ex-
traction tools are available for English, but are not
always available for other languages. In order to
take advantage of loose-sequence-based metrics
and avoid the problems in ROUGE and METEOR,
we propose a new metric SIA, which is based on
loose sequence alignment but enhanced with the
following features:
1METEOR and ROUGE both compute the score based on
the best reference
539
? Computing the string alignment score based
on the gaps in the common sequence. Though
ROUGE-W also takes into consider the gaps
in the common sequence between the MT
output and the reference by giving more cred-
its to the n-grams in the common sequence,
our method is more flexible in that not only
do the strict n-grams get more credits, but
also the tighter sequences.
? Stochastic word matching. For the purpose
of increasing hitting chance of MT outputs in
references, we use a stochastic word match-
ing in the string alignment instead of WORD-
STEM and WORD-NET used in METEOR
and ROUGE. Instead of using exact match-
ing, we use a soft matching based on the sim-
ilarity between two words, which is trained
in a bilingual corpus. The corpus is aligned
in the word level using IBM Model4 (Brown
et al, 1993). Stochastic word matching is a
uniform replacement for both morphological
processing and synonym matching. More im-
portantly, it can be easily adapted for differ-
ent kinds of languages, as long as there are
bilingual parallel corpora available (which is
always true for statistical machine transla-
tion).
? Iterative alignment scheme. In this scheme,
the string alignment will be continued until
there are no more co-occuring words to be
found between the MT output and any one of
the references. In this way, every co-occuring
word between the MT output and the refer-
ences can be considered and contribute to the
final score, and multiple references can be
used simultaneously.
The remainder of the paper is organized as fol-
lows: section 2 gives a recap of BLEU, ROUGE-
W and METEOR; section 3 describes the three
components of SIA; section 4 compares the per-
formance of different metrics based on experimen-
tal results; section 5 presents our conclusion.
2 Recap of BLEU, ROUGE-W and
METEOR
The most commonly used automatic evaluation
metrics, BLEU (Papineni et al, 2002) and NIST
(Doddington, 2002), are based on the assumption
that ?The closer a machine translation is to a pro-
mt1: Life is like one nice chocolate in box
ref: Life is just like a box of tasty chocolate
ref: Life is just like a box of tasty chocolate
mt2: Life is of one nice chocolate in box 
Figure 1: Alignment Example for ROUGE-W
fessional human translation, the better it is? (Pa-
pineni et al, 2002). For every hypothesis, BLEU
computes the fraction of n-grams which also ap-
pear in the reference sentences, as well as a brevity
penalty. NIST uses a similar strategy to BLEU but
further considers that n-grams with different fre-
quency should be treated differently in the evalu-
ation (Doddington, 2002). BLEU and NIST have
been shown to correlate closely with human judg-
ments in ranking MT systems with different qual-
ities (Papineni et al, 2002; Doddington, 2002).
ROUGE-W is based on the weighted longest
common subsequence (LCS) between the MT out-
put and the reference. The common subsequences
in ROUGE-W are not necessarily strict n-grams,
and gaps are allowed in both the MT output and
the reference. Because of the flexibility, long
common subsequences are feasible in ROUGE-
W and can help to reflect the sentence-wide sim-
ilarity of MT output and references. ROUGE-W
uses a weighting strategy where the LCS contain-
ing strict n-grams is favored. Figure 1 gives two
examples that show how ROUGE-W searches for
the LCS. For mt1, ROUGE-W will choose either
life is like chocolate or life is like box as the LCS,
since neither of the sequences ?like box? and ?like
chocolate? are strict n-grams and thus make no dif-
ference in ROUGE-W (the only strict n-grams in
the two candidate LCS is life is). For mt2, there
is only one choice of the LCS: life is of chocolate.
The LCS of mt1 and mt2 have the same length and
the same number of strict n-grams, thus they get
the same score in ROUGE-W. But it is clear to us
that mt1 is better than mt2. It is easy to verify that
mt1 and mt2 have the same number of common 1-
grams, 2-grams, and skipped 2-grams with the ref-
erence (they don?t have common n-grams longer
than 2 words), thus BLEU and ROUGE-S are also
not able to differentiate them.
METEOR is a metric sitting in the middle
of the n-gram based metrics and the loose se-
540
mt1: Life is like one nice chocolate in box
ref: Life is just like a box of tasty chocolate
ref: Life is just like a box of tasty chocolate
mt2: Life is of one nice chocolate in box 
Figure 2: Alignment Example for METEOR
quence based metrics. It has several phases and
in each phase different matching techniques (EX-
ACT, PORTER-STEM, WORD-NET) are used to
make an alignment for the MT output and the ref-
erence. METEOR doesn?t require the alignment to
be monotonic, which means crossing word map-
pings (e.g. a b is mapped to b a) are allowed,
though doing so will get a penalty. Figure 2 shows
the alignments of METEOR based on the same
example as ROUGE. Though the two alignments
have the same number of word mappings, mt2 gets
more crossed word mappings than mt1, thus it will
get less credits in METEOR. Both ROUGE and
METEOR normalize their evaluation result based
on the MT output length (precision) and the ref-
erence length (recall), and the final score is com-
puted as the F-mean of them.
3 Stochastic Iterative Alignment (SIA)
for Machine Translation Evaluation
We introduce three techniques to allow more sen-
sitive scores to be computed.
3.1 Modified String Alignment
This section introduces how to compute the string
alignment based on the word gaps. Given a pair
of strings, the task of string alignment is to obtain
the longest monotonic common sequence (where
gaps are allowed). SIA uses a different weighting
strategy from ROUGE-W, which is more flexible.
In SIA, the alignments are evaluated based on the
geometric mean of the gaps in the reference side
and the MT output side. Thus in the dynamic pro-
gramming, the state not only includes the current
covering length of the MT output and the refer-
ence, but also includes the last aligned positions in
them. The algorithm for computing the alignment
score in SIA is described in Figure 3. The sub-
routine COMPUTE SCORE, which computes the
score gained from the current aligned positions, is
shown in Figure 4. From the algorithm, we can
function GET ALIGN SCORE(mt, M, ref, N)
. Compute the alignment score of the MT output mt
with length M and the reference ref with length N
for i = 1; i ? M; i = i +1 do
for j = 1; j ? N; j = j +1 do
for k = 1; k ? i; k = k +1 do
for m = 1; m ? j; m = m +1 do
scorei,j,k,m
= max{scorei?1,j,k,m,scorei,j?1,k,m } ;
end for
end for
scorei,j,i,j =
max
n=1,M ;p=1,N
{scorei,j,i,j , scorei?1,j?1,n,p
+ COMPUTE SCORE(mt,ref, i, j, n, p)};
end for
end for
return scoreM,N,M,NM ;
end function
Figure 3: Alignment Algorithm Based on Gaps
function COMPUTE SCORE(mt, ref, i, j, n, p)
if mt[i] == ref [j] then
return 1/
p
(i ? n) ? (j ? p);
else
return 0;
end if
end function
Figure 4: Compute Word Matching Score Based
on Gaps
see that not only will strict n-grams get higher
scores than non-consecutive sequences, but also
the non-consecutive sequences with smaller gaps
will get higher scores than those with larger gaps.
This weighting method can help SIA capture more
subtle difference of MT outputs than ROUGE-W
does. For example, if SIA is used to align mt1
and ref in Figure 1, it will choose life is like box
instead of life is like chocolate, because the aver-
age distance of ?box-box? to its previous mapping
?like-like? is less than ?chocolate-chocolate?. Then
the score SIA assigns to mt1 is:
( 1
1 ? 1 +
1
1 ? 1 +
1?
1 ? 2
+ 1?
2 ? 5
)
?18 = 0.399(1)
For mt2, there is only one possible alignment,
its score in SIA is computed as:
( 1
1 ? 1 +
1
1 ? 1 +
1?
1 ? 5
+ 1?
2 ? 3
)
?18 = 0.357(2)
Thus, mt1 will be considered better than mt2 in
SIA, which is reasonable. As mentioned in sec-
tion 1, though loose-sequence-based metrics give
a better reflection of the sentence-wide similarity
of the MT output and the reference, they cannot
541
make full use of word-level information. This de-
fect could potentially lead to a poor performance
in adequacy evaluation, considering the case that
the ignored words are crucial to the evaluation. In
the later part of this section, we will describe an it-
erative alignment scheme which is meant to com-
pensate for this defect.
3.2 Stochastic Word Mapping
In ROUGE and METEOR, PORTER-STEM and
WORD-NET are used to increase the chance of
the MT output words matching the references.
We use a different stochastic approach in SIA to
achieve the same purpose. The string alignment
has a good dynamic framework which allows the
stochastic word matching to be easily incorporated
into it. The stochastic string alignment can be im-
plemented by simply replacing the function COM-
PUTE SCORE with the function of Figure 5. The
function similarity(word1, word2) returns a ratio
which reflects how similar the two words are. Now
we consider how to compute the similarity ratio of
two words. Our method is motivated by the phrase
extraction method of Bannard and Callison-Burch
(2005), which computes the similarity ratio of two
words by looking at their relationship with words
in another language. Given a bilingual parallel
corpus with aligned sentences, say English and
French, the probability of an English word given
a French word can be computed by training word
alignment models such as IBM Model4. Then for
every English word e, we have a set of conditional
probabilities given each French word: p(e|f1),
p(e|f2), ... , p(e|fN ). If we consider these proba-bilities as a vector, the similarities of two English
words can be obtained by computing the dot prod-
uct of their corresponding vectors.2 The formula
is described below:
similarity(ei, ej) =
N
?
k=1
p(ei|fk)p(ej |fk) (3)
Paraphrasing methods based on monolingual par-
allel corpora such as (Pang et al, 2003; Barzilay
and Lee, 2003) can also be used to compute the
similarity ratio of two words, but they don?t have
as rich training resources as the bilingual methods
do.
2Although the marginalized probability (over all French
words) of an English word given the other English word
(PNk=1 p(ei|fk)p(fk|ej)) is a more intuitive way of measur-ing the similarity, the dot product of the vectors p(e|f) de-
scribed above performed slightly better in our experiments.
function STO COMPUTE SCORE(mt, ref, i, j, n, p)
if mt[i] == ref [j] then
return 1/
p
(i ? n) ? (j ? p);
else
return similarity(mt[i],ref [i])?
(i?n)?(j?p)
;
end if
end function
Figure 5: Compute Stochastic Word Matching
Score
3.3 Iterative Alignment Scheme
ROUGE-W, METEOR, and WER all score MT
output by first computing a score based on each
available reference, and then taking the highest
score as the final score for the MT output. This
scheme has the problem of not being able to use
multiple references simultaneously. The itera-
tive alignment scheme proposed here is meant to
alleviate this problem, by doing alignment be-
tween the MT output and one of the available ref-
erences until no more words in the MT output
can be found in the references. In each align-
ment round, the score based on each reference
is computed and the highest one is taken as the
score for the round. Then the words which have
been aligned in best alignment will not be con-
sidered in the next round. With the same num-
ber of aligned words, the MT output with fewer
alignment rounds should be considered better than
those requiring more rounds. For this reason, a
decay factor ? is multiplied with the scores of
each round. The final score of the MT output is
then computed by summing the weighted scores
of each alignment round. The scheme is described
in Figure 6.
The function GET ALIGN SCORE 1 used
in GET ALIGN SCORE IN MULTIPLE REFS
is slightly different from GET ALIGN SCORE
described in the prior subsection. The dynamic
programming algorithm for getting the best
alignment is the same, except that it has two more
tables as input, which record the unavailable po-
sitions in the MT output and the reference. These
positions have already been used in the prior best
alignments and should not be considered in the
ongoing alignment. It also returns the aligned
positions of the best alignment. The pseudocode
for GET ALIGN SCORE 1 is shown in Figure 7.
The computation of the length penalty is similar
to BLEU: it is set to 1 if length of the MT output
is longer than the arithmetic mean of length of the
542
function GET ALIGN SCORE IN MULTIPLE REFS(mt,
ref 1, ..., ref N , ?)
. Iteratively Compute the Alignment Score Based on
Multiple References and the Decay Factor ?
final score = 0;
while max score != 0 do
for i = 1, ..., N do
(score, align) =
GET ALIGN SCORE 1(mt, ref i, mt table, ref tablei);
if score > max score then
max score = score;
max align = align;
max ref = i;
end if
end for
final score += max score ??;
? ? = ?;
Add the words in align to mt table and
ref tablemax ref ;
end while
return final score? length penalty;
end function
Figure 6: Iterative Alignment Scheme
references, and otherwise is set to the ratio of the
two. Figure 8 shows how the iterative alignment
scheme works with an evaluation set containing
one MT output and two references. The selected
alignment in each round is shown, as well as the
unavailable positions in MT output and refer-
ences. With the iterative scheme, every common
word between the MT output and the reference
set can make a contribution to the metric, and
by such means SIA is able to make full use of
the word-level information. Furthermore, the
order (alignment round) in which the words are
aligned provides a way to weight them. In BLEU,
multiple references can be used simultaneously,
but the common n-grams are treated equally.
4 Experiments
Evaluation experiments were conducted to com-
pare the performance of different metrics includ-
ing BLEU, ROUGE, METEOR and SIA.3 The test
data for the experiments are from the MT evalu-
ation workshop at ACL05. There are seven sets
of MT outputs (E09 E11 E12 E14 E15 E17 E22),
all of which contain 919 English sentences. These
sentences are the translation of the same Chinese
input generated by seven different MT systems.
The fluency and adequacy of each sentence are
manually ranked from 1 to 5. For each MT output,
there are two sets of human scores available, and
3METEOR and ROUGE can be downloaded at
http://www.cs.cmu.edu/?alavie/METEOR and
http://www.isi.edu/licensed-sw/see/rouge
function GET ALIGN SCORE1(mt, ref, mttable, reftable)
. Compute the alignment score of the MT output mt
with length M and the reference ref with length N, without
considering the positions in mttable and reftable
M = |mt|; N = |ref |;
for i = 1; i ? M; i = i +1 do
for j = 1; j ? N; j = j +1 do
for k = 1; k ? i; k = k +1 do
for m = 1; m ? j; m = m +1 do
scorei,j,k,m
= max{scorei?1,j,k,m, scorei,j?1,k,m};
end for
end for
if i is not in mttable and j is not in reftable then
scorei,j,i,j = max
n=1,M ;p=1,N
{scorei,j,i,j ,
scorei?1,j?1,n,p + COMPUTE SCORE(mt, ref, i, j, n, p)};
end if
end for
end for
return scoreM,N,M,NM and the corresponding alignment;
end function
Figure 7: Alignment Algorithm Based on Gaps
Without Considering Aligned Positions
m: England with France discussed this crisis in London
r1: Britain and France consulted about this crisis in London with each other
r2: England and France discussed the crisis in London
m: England with France discussed this crisis in London
r2: England and France discussed the crisis in London
r1: Britain and France consulted about this crisis in London with each other
m: England with France discussed this crisis in London
r1: Britain and France consulted about this crisis in London with each other
r2: England and France discussed the crisis in London
Figure 8: Alignment Example for SIA
543
we randomly choose one as the score used in the
experiments. The human overall scores are calcu-
lated as the arithmetic means of the human fluency
scores and adequacy scores. There are four sets
of human translations (E01, E02, E03, E04) serv-
ing as references for those MT outputs. The MT
outputs and reference sentences are transformed to
lower case. Our experiments are carried out as fol-
lows: automatic metrics are used to evaluate the
MT outputs based on the four sets of references,
and the Pearson?s correlation coefficient of the au-
tomatic scores and the human scores is computed
to see how well they agree.
4.1 N -gram vs. Loose Sequence
One of the problems addressed in this paper is
the different performance of n-gram based metrics
and loose-sequence-based metrics in sentence-
level evaluation. To see how they really differ
in experiments, we choose BLEU and ROUGE-
W as the representative metrics for the two types,
and used them to evaluate the 6433 sentences in
the 7 MT outputs. The Pearson correlation coeffi-
cients are then computed based on the 6433 sam-
ples. The experimental results are shown in Ta-
ble 1. BLEU-n denotes the BLEU metric with
the longest n-gram of length n. F denotes flu-
ency, A denotes adequacy, and O denotes overall.
We see that with the increase of n-gram length,
BLEU?s performance does not increase monoton-
ically. The best result in adequacy evaluation is
achieved at 2-gram and the best result in fluency is
achieved at 4-gram. Using n-grams longer than 2
doesn?t buy much improvement for BLEU in flu-
ency evaluation, and does not compensate for the
loss in adequacy evaluation. This confirms Liu and
Gildea (2005)?s finding that in sentence level eval-
uation, long n-grams in BLEU are not beneficial.
The loose-sequence-based ROUGE-W does much
better than BLEU in fluency evaluation, but it does
poorly in adequacy evaluation and doesn?t achieve
a significant improvement in overall evaluation.
We speculate that the reason is that ROUGE-W
doesn?t make full use of the available word-level
information.
4.2 METEOR vs. SIA
SIA is designed to take the advantage of loose-
sequence-based metrics without losing word-level
information. To see how well it works, we choose
E09 as the development set and the sentences in
the other 6 sets as the test data. The decay fac-
B-3 R 1 R 2 M S
F 0.167 0.152 0.192 0.167 0.202
A 0.306 0.304 0.287 0.332 0.322
O 0.265 0.256 0.266 0.280 0.292
Table 2: Sentence level evaluation results of
BLEU, ROUGE, METEOR and SIA
tor in SIA is determined by optimizing the over-
all evaluation for E09, and then used with SIA
to evaluate the other 5514 sentences based on the
four sets of references. The similarity of English
words is computed by training IBM Model 4 in
an English-French parallel corpus which contains
seven hundred thousand sentence pairs. For every
English word, only the entries of the top 100 most
similar English words are kept and the similarity
ratios of them are then re-normalized. The words
outside the training corpus will be considered as
only having itself as its similar word. To com-
pare the performance of SIA with BLEU, ROUGE
and METEOR, the evaluation results based on
the same testing data is given in Table 2. B-
3 denotes BLEU-3; R 1 denotes the skipped bi-
gram based ROUGE metric which considers all
skip distances and uses PORTER-STEM; R 2 de-
notes ROUGE-W with PORTER-STEM; M de-
notes the METEOR metric using PORTER-STEM
and WORD-NET synonym; S denotes SIA.
We see that METEOR, as the other metric
sitting in the middle of n-gram based metrics
and loose sequence metrics, achieves improve-
ment over BLEU in both adequacy and fluency
evaluation. Though METEOR gets the best re-
sults in adequacy evaluation, in fluency evaluation,
it is worse than the loose-sequence-based metric
ROUGE-W-STEM. SIA is the only one among
the 5 metrics which does well in both fluency and
adequacy evaluation. It achieves the best results in
fluency evaluation and comparable results to ME-
TEOR in adequacy evaluation, and the balanced
performance leads to the best overall evaluation
results in the experiment. To estimate the signif-
icance of the correlations, bootstrap resampling
(Koehn, 2004) is used to randomly select 5514
sentences with replacement out of the whole test
set of 5514 sentences, and then the correlation co-
efficients are computed based on the selected sen-
tence set. The resampling is repeated 5000 times,
and the 95% confidence intervals are shown in Ta-
bles 3, 4, and 5. We can see that it is very diffi-
544
BLEU-1 BLEU-2 BLEU-3 BLEU-4 BLEU-5 BLEU-6 ROUGE-W
F 0.147 0.162 0.166 0.168 0.165 0.164 0.191
A 0.288 0.296 0.291 0.285 0.279 0.274 0.268
O 0.243 0.256 0.255 0.251 0.247 0.244 0.254
Table 1: Sentence level evaluation results of BLEU and ROUGE-W
low mean high
B-3 (-16.6%) 0.138 0.165 0.192 (+16.4%)
R 1 (-17.8%) 0.124 0.151 0.177 (+17.3%)
R 2 (-14.3%) 0.164 0.191 0.218 (+14.2%)
M (-15.8%) 0.139 0.166 0.191 (+15.5%)
S (-13.3%) 0.174 0.201 0.227 (+13.3%)
Table 3: 95% significance intervals for sentence-
level fluency evaluation
low mean high
B-3 (-08.2%) 0.280 0.306 0.330 (+08.1%)
R 1 (-08.5%) 0.278 0.304 0.329 (+08.4%)
R 2 (-09.2%) 0.259 0.285 0.312 (+09.5%)
M (-07.3%) 0.307 0.332 0.355 (+07.0%)
S (-07.9%) 0.295 0.321 0.346 (+07.8%)
Table 4: 95% significance intervals for sentence-
level adequacy evaluation
cult for one metric to significantly outperform an-
other metric in sentence-level evaluation. The re-
sults show that the mean of the correlation factors
converges right to the value we computed based on
the whole testing set, and the confidence intervals
correlate with the means.
While sentence-level evaluation is useful if we
are interested in a confidence measure on MT out-
puts, syste-x level evaluation is more useful for
comparing MT systems and guiding their develop-
ment. Thus we also present the evaluation results
based on the 7 MT output sets in Table 6. SIA uses
the same decay factor as in the sentence-level eval-
uation. Its system-level score is computed as the
arithmetic mean of the sentence level scores, and
low mean high
B-3 (-09.8%) 0.238 0.264 0.290 (+09.9%)
R 1 (-10.2%) 0.229 0.255 0.281 (+10.0%)
R 2 (-10.0%) 0.238 0.265 0.293 (+10.4%)
M (-09.0%) 0.254 0.279 0.304 (+08.8%)
S (-08.7%) 0.265 0.291 0.316 (+08.8%)
Table 5: 95% significance intervals for sentence-
level overall evaluation
WLS WLS WLS WLS
PROB INCS PROB
INCS
F 0.189 0.202 0.188 0.202
A 0.295 0.310 0.311 0.322
O 0.270 0.285 0.278 0.292
Table 7: Results of different components in SIA
WLS WLS WLS WLS
INCS INCS INCS INCS
STEM WN STEM
WN
F 0.188 0.188 0.187 0.191
A 0.311 0.313 0.310 0.317
O 0.278 0.280 0.277 0.284
Table 8: Results of SIA working with Porter-Stem
and WordNet
so are ROUGE, METEOR and the human judg-
ments. We can see that SIA achieves the best per-
formance in both fluency and adequacy evaluation
of the 7 systems. Though the 7-sample based re-
sults are not reliable, we can get a sense of how
well SIA works in the system-level evaluation.
4.3 Components in SIA
To see how the three components in SIA con-
tribute to the final performance, we conduct exper-
iments where one or two components are removed
in SIA, shown in Table 7. The three components
are denoted as WLS (weighted loose sequence
alignment), PROB (stochastic word matching),
and INCS (iterative alignment scheme) respec-
tively. WLS without INCS does only one round
of alignment and chooses the best alignment score
as the final score. This scheme is similar to
ROUGE-W and METEOR. We can see that INCS,
as expected, improves the adequacy evaluation
without hurting the fluency evaluation. PROB
improves both adequacy and fluency evaluation
performance. The result that SIA works with
PORTER-STEM and WordNet is also shown in
Table 8. When PORTER-STEM and WordNet are
545
B-6 R 1 R 2 M S
F 0.514 0.466 0.458 0.378 0.532
A 0.876 0.900 0.906 0.875 0.928
O 0.794 0.790 0.792 0.741 0.835
Table 6: Results of BLEU, ROUGE, METEOR and SIA in system level evaluation
both used, PORTER-STEM is used first. We can
see that they are not as good as using the stochastic
word matching. Since INCS and PROB are inde-
pendent of WLS, we believe they can also be used
to improve other metrics such as ROUGE-W and
METEOR.
5 Conclusion
This paper describes a new metric SIA for MT
evaluation, which achieves good performance by
combining the advantages of n-gram-based met-
rics and loose-sequence-based metrics. SIA uses
stochastic word mapping to allow soft or partial
matches between the MT hypotheses and the ref-
erences. This stochastic component is shown to
be better than PORTER-STEM and WordNet in
our experiments. We also analyzed the effect of
other components in SIA and speculate that they
can also be used in other metrics to improve their
performance.
Acknowledgments This work was supported
by NSF ITR IIS-09325646 and NSF ITR IIS-
0428020.
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judegments. In Proceed-
ings of the ACL-04 workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, Ann Arbor, Michigan.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Conference of the As-
sociation for Computational Linguistics (ACL-05).
Regina Barzilay and Lillian Lee. 2003. Learning
to paraphrase: An unsupervised approach using
multiple-sequence alignment. In Proceedings of the
2003 Meeting of the North American chapter of the
Association for Computational Linguistics (NAACL-
03), pages 16?23.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence es-
timation for machine translation. Technical report,
Center for Language and Speech Processing, Johns
Hopkins University, Baltimore. Summer Workshop
Final Report.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
G. Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In In HLT 2002, Human Lan-
guage Technology Conference, San Diego, CA.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In 2004 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 388?395, Barcelona,
Spain, July.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic evaluation of machine translation quality us-
ing longest common subsequence and skip-bigram
statistics. In Proceedings of the 42th Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL-04), Barcelona, Spain.
Ding Liu and Daniel Gildea. 2005. Syntactic fea-
tures for evaluation of machine translation. In ACL
2005 Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations:
Extracting paraphrases and generating new sen-
tences. In Proceedings of the 2003 Meeting of the
North American chapter of the Association for Com-
putational Linguistics (NAACL-03).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Conference of the Association for Com-
putational Linguistics (ACL-02), Philadelphia, PA.
546
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 953?960,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Inducing Word Alignments with Bilexical Synchronous Trees
Hao Zhang and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
This paper compares different bilexical
tree-based models for bilingual alignment.
EM training for the new model bene-
fits from the dynamic programming ?hook
trick?. The model produces improved de-
pendency structure for both languages.
1 Introduction
A major difficulty in statistical machine translation
is the trade-off between representational power
and computational complexity. Real-world cor-
pora for language pairs such as Chinese-English
have complex reordering relationships that are not
captured by current phrase-based MT systems, de-
spite their state-of-the-art performance measured
in competitive evaluations. Synchronous gram-
mar formalisms that are capable of modeling
such complex relationships while maintaining the
context-free property in each language have been
proposed for many years, (Aho and Ullman, 1972;
Wu, 1997; Yamada and Knight, 2001; Melamed,
2003; Chiang, 2005), but have not been scaled to
large corpora and long sentences until recently.
In Synchronous Context Free Grammars, there
are two sources of complexity, grammar branch-
ing factor and lexicalization. In this paper we fo-
cus on the second issue, constraining the gram-
mar to the binary-branching Inversion Transduc-
tion Grammar of Wu (1997). Lexicalization seems
likely to help models predict alignment patterns
between languages, and has been proposed by
Melamed (2003) and implemented by Alshawi et
al. (2000) and Zhang and Gildea (2005). However,
each piece of lexical information considered by a
model multiplies the number of states of dynamic
programming algorithms for inference, meaning
that we must choose how to lexicalize very care-
fully to control complexity.
In this paper we compare two approaches to
lexicalization, both of which incorporate bilexical
probabilities. One model uses bilexical probabil-
ities across languages, while the other uses bilex-
ical probabilities within one language. We com-
pare results on word-level alignment, and investi-
gate the implications of the choice of lexicaliza-
tion on the specifics of our alignment algorithms.
The new model, which bilexicalizes within lan-
guages, allows us to use the ?hook trick? (Eis-
ner and Satta, 1999) and therefore reduces com-
plexity. We describe the application of the hook
trick to estimation with Expectation Maximization
(EM). Despite the theoretical benefits of the hook
trick, it is not widely used in statistical monolin-
gual parsers, because the savings do not exceed
those obtained with simple pruning. We speculate
that the advantages may be greater in an EM set-
ting, where parameters to guide pruning are not
(initially) available.
In order to better understand the model, we an-
alyze its performance in terms of both agreement
with human-annotated alignments, and agreement
with the dependencies produced by monolingual
parsers. We find that within-language bilexical-
ization does not improve alignment over cross-
language bilexicalization, but does improve recov-
ery of dependencies. We find that the hook trick
significantly speeds training, even in the presence
of pruning.
Section 2 describes the generative model. The
hook trick for EM is explained in Section 3. In
Section 4, we evaluate the model in terms of align-
ment error rate and dependency error rate. We
conclude with discussions in Section 5.
953
2 Bilexicalization of Inversion
Transduction Grammar
The Inversion Transduction Grammar of Wu
(1997) models word alignment between a transla-
tion pair of sentences by assuming a binary syn-
chronous tree on top of both sides. Using EM
training, ITG can induce good alignments through
exploring the hidden synchronous trees from in-
stances of string pairs.
ITG consists of unary production rules that gen-
erate English/foreign word pairs e/f :
X ? e/f
and binary production rules in two forms that gen-
erate subtree pairs, written:
X ? [Y Z]
and
X ? ?Y Z?
The square brackets indicate the right hand side
rewriting order is the same for both languages.
The pointed brackets indicate there exists a type of
syntactic reordering such that the two right hand
side constituents rewrite in the opposite order in
the second language.
The unary rules account for the alignment links
across two sides. Either e or f may be a special
null word, handling insertions and deletions. The
two kinds of binary rules (called straight rules and
inverted rules) build up a coherent tree structure
on top of the alignment links. From a modeling
perspective, the synchronous tree that may involve
inversions tells a generative story behind the word
level alignment.
An example ITG tree for the sentence pair Je
les vois / I see them is shown in Figure 1(left). The
probability of the tree is the product rule probabil-
ities at each node:
P (S ? A)
? P (A ? [C B])
? P (C ? I/Je)
? P (B ? ?C C?)
? P (C ? see/vois)
? P (C ? them/les)
The structural constraint of ITG, which is that
only binary permutations are allowed on each
level, has been demonstrated to be reasonable
by Zens and Ney (2003) and Zhang and Gildea
(2004). However, in the space of ITG-constrained
synchronous trees, we still have choices in making
the probabilistic distribution over the trees more
realistic. The original Stochastic ITG is the coun-
terpart of Stochastic CFG in the bitext space. The
probability of an ITG parse tree is simply a prod-
uct of the probabilities of the applied rules. Thus,
it only captures the fundamental features of word
links and reflects how often inversions occur.
2.1 Cross-Language Bilexicalization
Zhang and Gildea (2005) described a model in
which the nonterminals are lexicalized by English
and foreign language word pairs so that the inver-
sions are dependent on lexical information on the
left hand side of synchronous rules. By introduc-
ing the mechanism of probabilistic head selection
there are four forms of probabilistic binary rules
in the model, which are the four possibilities cre-
ated by taking the cross-product of two orienta-
tions (straight and inverted) and two head choices:
X(e/f) ? [Y (e/f) Z]
X(e/f) ? [Y Z(e/f)]
X(e/f) ? ?Y (e/f) Z?
X(e/f) ? ?Y Z(e/f)?
where (e/f) is a translation pair.
A tree for our example sentence under this
model is shown in Figure 1(center). The tree?s
probability is again the product of rule probabil-
ities:
P (S ? A(see/vois))
? P (A(see/vois) ? [C B(see/vois)])
? P (C ? C(I/Je))
? P (B(see/vois) ? ?C(see/vois) C?)
? P (C ? C(them/les))
2.2 Head-Modifier Bilexicalization
One disadvantage of the model above is that it
is not capable of modeling bilexical dependen-
cies on the right hand side of the rules. Thus,
while the probability of a production being straight
or inverted depends on a bilingual word pair, it
does not take head-modifier relations in either lan-
guage into account. However, modeling complete
bilingual bilexical dependencies as theorized in
Melamed (2003) implies a huge parameter space
of O(|V |2|T |2), where |V | and |T | are the vo-
cabulary sizes of the two languages. So, in-
stead of modeling cross-language word transla-
tions and within-language word dependencies in
954
CB
C
A
see/vois them/les
I/Je
S
C
C(I/Je)
C
C(them/les)
C
B(see/vois)
A(see/vois)
C(see/vois)
S S
C(I)
C(them)
them/les
I/Je C(see)
see/vois
B(see)
A(see)
Figure 1: Parses for an example sentence pair under unlexicalized ITG (left), cross-language bilexicaliza-
tion (center), and head-modifier bilexicaliztion (right). Thick lines indicate head child; crossbar indicates
inverted production.
a joint fashion, we factor them apart. We lexical-
ize the dependencies in the synchronous tree using
words from only one language and translate the
words into their counterparts in the other language
only at the bottom of the tree. Formally, we have
the following patterns of binary dependency rules:
X(e) ? [Y (e) Z(e?)]
X(e) ? [Y (e?) Z(e)]
X(e) ? ?Y (e) Z(e?)?
X(e) ? ?Y (e?) Z(e)?
where e is an English head and e? is an English
modifier.
Equally importantly, we have the unary lexical
rules that generate foreign words:
X(e) ? e/f
To make the generative story complete, we also
have a top rule that goes from the unlexicalized
start symbol to the highest lexicalized nonterminal
in the tree:
S ? X(e)
Figure 1(right), shows our example sentence?s
tree under the new model. The probability of a
bilexical synchronous tree between the two sen-
tences is:
P (S ? A(see))
? P (A(see) ? [C(I) B(see)])
? P (C(I) ? I/Je)
? P (B(see) ? ?C(see) C(them)?)
? P (C(see) ? see/vois)
? P (C(them) ? them/les)
Interestingly, the lexicalized B(see) predicts
not only the existence of C(them), but also that
there is an inversion involved going from C(see)
to C(them). This reflects the fact that direct ob-
ject pronouns come after the verb in English, but
before the verb in French. Thus, despite condi-
tioning on information about words from only one
language, the model captures syntactic reordering
information about the specific language pair it is
trained on. We are able to discriminate between
the straight and inverted binary nodes in our ex-
ample tree in a way that cross-language bilexical-
ization could not.
In terms of inferencing within the framework,
we do the usual Viterbi inference to find the best
bilexical synchronous tree and treat the depen-
dencies and the alignment given by the Viterbi
parse as the best ones, though mathematically the
best alignment should have the highest probabil-
ity marginalized over all dependencies constrained
by the alignment. We do unsupervised training to
obtain the parameters using EM. Both EM and
Viterbi inference can be done using the dynamic
programming framework of synchronous parsing.
3 Inside-Outside Parsing with the Hook
Trick
ITG parsing algorithm is a CYK-style chart pars-
ing algorithm extended to bitext. Instead of build-
ing up constituents over spans on a string, an ITG
chart parser builds up constituents over subcells
within a cell defined by two strings. We use
?(X(e), s, t, u, v) to denote the inside probabil-
ity of X(e) which is over the cell of (s, t, u, v)
where (s, t) are indices into the source language
string and (u, v) are indices into the target lan-
guage string. We use ?(X(e), s, t, u, v) to de-
note its outside probability. Figure 2 shows how
smaller cells adjacent along diagonals can be com-
bined to create a large cell. We number the sub-
cells counterclockwise. To analyze the complex-
ity of the algorithm with respect to input string
955
S
       
       
       
       
       
       
      
      
      
      
      
      
 





      
      
      
      
      
      
u
U
v
s t
2 1
3 4
U






     
     
     
     
     
     
u
s S
e
Figure 2: Left: Chart parsing over the bitext cell of (s, t, u, v). Right: One of the four hooks built for
four corners for more efficient parsing.
length, without loss of generality, we ignore the
nonterminal symbols X , Y , and Z to simplify the
derivation.
The inside algorithm in the context of bilexical
ITG is based on the following dynamic program-
ming equation:
? (e, s, t, u, v)
=
?
S,U,e?
?
?
?
?
?
?1(e) ? ?3(e?) ? P ([e?e] | e)
+?2(e) ? ?4(e?) ? P (?ee?? | e)
+?3(e) ? ?1(e?) ? P ([ee?] | e)
+?4(e) ? ?2(e?) ? P (?e?e? | e)
?
?
?
?
?
So, on the right hand side, we sum up all possi-
ble ways (S, U ) of splitting the left hand side cell
and all possible head words (e?) for the non-head
subcell. e, e?, s, t, u, v, S, and U all eight vari-
ables take O(n) values given that the lengths of
the source string and the target string are O(n).
Thus the entire DP algorithm takes O(n8) steps.
Fortunately, we can reduce the maximum num-
ber of interacting variables by factorizing the ex-
pression.
Let us keep the results of the summations over
e? as:
?+1 (e) =
?
e?
?1(e?) ? P ([ee?] | e)
?+2 (e) =
?
e?
?2(e?) ? P (?e?e? | e)
?+3 (e) =
?
e?
?3(e?) ? P ([e?e] | e)
?+4 (e) =
?
e?
?4(e?) ? P (?ee?? | e)
The computation of each ?+ involves four
boundary indices and two head words. So, we can
rely on DP to compute them in O(n6). Based on
these intermediate results, we have the equivalent
DP expression for computing inside probabilities:
? (e, s, t, u, v)
=
?
S,U
?
?
?
?
?
?1(e) ? ?+3 (e)
+ ?2(e) ? ?+4 (e)
+ ?3(e) ? ?+1 (e)
+ ?4(e) ? ?+2 (e)
?
?
?
?
?
We reduced one variable from the original ex-
pression. The maximum number of interacting
variables throughout the algorithm is 7. So the im-
proved inside algorithm has a time complexity of
O(n7).
The trick of reducing interacting variables in DP
for bilexical parsing has been pointed out by Eis-
ner and Satta (1999). Melamed (2003) discussed
the applicability of the so-called hook trick for
parsing bilexical multitext grammars. The name
hook is based on the observation that we combine
the non-head constituent with the bilexical rule to
create a special constituent that matches the head
like a hook as demonstrated in Figure 2. How-
ever, for EM, it is not clear from their discussions
how we can do the hook trick in the outside pass.
The bilexical rules in all four directions are anal-
ogous. To simplify the derivation for the outside
algorithm, we just focus on the first case: straight
rule with right head word.
The outside probability of the constituent
(e, S, t, U, v) in cell 1 being a head of such rules
is:
?
s,u,e?
(
?(e) ? ?3(e?) ? P ([e?e] | e)
)
=
?
s,u
(
?(e) ?
(
?
e?
?3(e?) ? P ([e?e] | e)
))
=
?
s,u
(
?(e) ? ?+3 (e)
)
which indicates we can reuse ?+ of the lower left
neighbors of the head to make the computation
feasible in O(n7).
On the other hand, the outside probability for
(e?, s, S, u, U) in cell 3 acting as a modifier of such
956
a rule is:
?
t,v,e
(
?(e) ? ?1(e) ? P ([e?e] | e)
)
=
?
e
?
?P ([e?e] | e) ?
?
?
?
t,v
?(e) ? ?1(e)
?
?
?
?
=
?
e
(
P ([e?, e] | e) ? ?+3 (e)
)
in which we memorize another kind of intermedi-
ate sum to make the computation no more complex
than O(n7).
We can think of ?+3 as the outside probabilityof the hook on cell 3 which matches cell 1. Gener-
ally, we need outside probabilities for hooks in all
four directions.
?+1 (e) =
?
s,u
?(e) ? ?3(e)
?+2 (e) =
?
t,u
?(e) ? ?4(e)
?+3 (e) =
?
t,v
?(e) ? ?1(e)
?+4 (e) =
?
s,v
?(e) ? ?2(e)
Based on them, we can add up the outside prob-
abilities of a constituent acting as one of the two
children of each applicable rule on top of it to get
the total outside probability.
We finalize the derivation by simplifying the ex-
pression of the expected count of (e ? [e?e]).
EC(e ? [e?e])
=
?
s,t,u,v,S,U
(
P ([e?e] | e) ? ?3(e?) ? ?(e) ? ?1(e)
)
=
?
s,S,u,U
?
?P ([e?e] | e) ? ?3(e?) ?
?
?
?
t,v
? ? ?1
?
?
?
?
=
?
s,S,u,U
(
P ([e?e] | e) ? ?3(e?) ? ?+3 (e)
)
which can be computed in O(n6) as long as we
have ?+3 ready in a table. Overall we can do theinside-outside algorithm for the bilexical ITG in
O(n7), by reducing a factor of n through interme-
diate DP.
The entire trick can be understood very clearly
if we imagine the bilexical rules are unary rules
that are applied on top of the non-head con-
stituents to reduce it to a virtual lexical constituent
(a hook) covering the same subcell while sharing
the head word with the head constituent. However,
if we build hooks looking for all words in a sen-
tence whenever a complete constituent is added to
the chart, we will build many hooks that are never
used, considering that the words outside of larger
cells are fewer and pruning might further reduce
the possible outside words. Blind guessing of what
might appear outside of the current cell will off-
set the saving we can achieve. Instead of actively
building hooks, which are intermediate results, we
can build them only when we need them and then
cache them for future use. So the construction of
the hooks will be invoked by the heads when the
heads need to combine with adjacent cells.
3.1 Pruning and Smoothing
We apply one of the pruning techniques used in
Zhang and Gildea (2005). The technique is gen-
eral enough to be applicable to any parsing algo-
rithm over bitext cells. It is called tic-tac-toe prun-
ing since it involves an estimate of both the inside
probability of the cell (how likely the words within
the box in both dimensions are to align) and the
outside probability (how likely the words outside
the box in both dimensions are to align). By scor-
ing the bitext cells and throwing away the bad cells
that fall out of a beam, it can reduce over 70% of
O(n4) cells using 10?5 as the beam ratio for sen-
tences up to 25 words in the experiments, without
harming alignment error rate, at least for the un-
lexicalized ITG.
The hook trick reduces the complexity of bilex-
ical ITG from O(n8) to O(n7). With the tic-tac-
toe pruning reducing the number of bitext cells to
work with, also due to the reason that the grammar
constant is very small for ITG. the parsing algo-
rithm runs with an acceptable speed,
The probabilistic model has lots of parameters
of word pairs. Namely, there are O(|V |2) de-
pendency probabilities and O(|V ||T |) translation
probabilities, where |V | is the size of English vo-
cabulary and |T | is the size of the foreign lan-
guage vocabulary. The translation probabilities of
P (f |X(e)) are backed off to a uniform distribu-
tion. We let the bilexical dependency probabilities
back off to uni-lexical dependencies in the follow-
ing forms:
P ([Y (?) Z(e?)] | X(?))
P ([Y (e?) Z(?)] | X(?))
P (?Y (?) Z(e?)? | X(?))
P (?Y (e?) Z(?)? | X(?))
957
 0
 100
 200
 300
 400
 500
 600
 700
 0  5  10  15  20
se
co
n
ds
sentence length
without-hook
with-hook
 0
 20
 40
 60
 80
 100
 120
 140
 0  5  10  15  20  25
se
co
n
ds
sentence length
without-hook
with-hook
(a) (b)
Figure 3: Speedup for EM by the Hook Trick. (a) is without pruning. In (b), we apply pruning on the
bitext cells before parsing begins.
The two levels of distributions are interpolated
using a technique inspired by Witten-Bell smooth-
ing (Chen and Goodman, 1996). We use the ex-
pected count of the left hand side lexical nontermi-
nal to adjust the weight for the EM-trained bilexi-
cal probability. For example,
P ([Y (e) Z(e?)] | X(e)) =
(1 ? ?)PEM ([Y (e) Z(e?)] | X(e))
+ ?P ([Y (?) Z(e?)] | X(?))
where
? = 1/(1 + Expected Counts(X(e)))
4 Experiments
First of all, we are interested in finding out how
much speedup can be achieved by doing the hook
trick for EM. We implemented both versions in
C++ and turned off pruning for both. We ran the
two inside-outside parsing algorithms on a small
test set of 46 sentence pairs that are no longer than
25 words in both languages. Then we put the re-
sults into buckets of (1 ? 4), (5 ? 9), (10 ? 14),
(15?19), and (20?24) according to the maximum
length of two sentences in each pair and took av-
erages of these timing results. Figure 3 (a) shows
clearly that as the sentences get longer the hook
trick is helping more and more. We also tried to
turn on pruning for both, which is the normal con-
dition for the parsers. Both are much faster due
to the effectiveness of pruning. The speedup ratio
is lower because the hooks will less often be used
again since many cells are pruned away. Figure 3
(b) shows the speedup curve in this situation.
We trained both the unlexicalized and the lex-
icalized ITGs on a parallel corpus of Chinese-
English newswire text. The Chinese data were
automatically segmented into tokens, and English
capitalization was retained. We replaced words
occurring only once with an unknown word token,
resulting in a Chinese vocabulary of 23,783 words
and an English vocabulary of 27,075 words.
We did two types of comparisons. In the first
comparison, we measured the performance of five
word aligners, including IBM models, ITG, the
lexical ITG (LITG) of Zhang and Gildea (2005),
and our bilexical ITG (BLITG), on a hand-aligned
bilingual corpus. All the models were trained us-
ing the same amount of data. We ran the ex-
periments on sentences up to 25 words long in
both languages. The resulting training corpus had
18,773 sentence pairs with a total of 276,113 Chi-
nese words and 315,415 English words.
For scoring the Viterbi alignments of each sys-
tem against gold-standard annotated alignments,
we use the alignment error rate (AER) of Och
and Ney (2000), which measures agreement at the
level of pairs of words:
AER = 1 ? |A ? GP | + |A ? GS ||A| + |GS |
where A is the set of word pairs aligned by the
automatic system, GS is the set marked in thegold standard as ?sure?, and GP is the set markedas ?possible? (including the ?sure? pairs). In our
Chinese-English data, only one type of alignment
was marked, meaning that GP = GS .
In our hand-aligned data, 47 sentence pairs are
no longer than 25 words in either language and
were used to evaluate the aligners.
A separate development set of hand-aligned
sentence pairs was used to control overfitting. The
subset of up to 25 words in both languages was
used. We chose the number of iterations for EM
958
Alignment
Precision Recall Error Rate
IBM-1 .56 .42 .52
IBM-4 .67 .43 .47
ITG .68 .52 .41
LITG .69 .51 .41
BLITG .68 .51 .42
Dependency
Precision Recall Error Rate
ITG-lh .11 .11 .89
ITG-rh .22 .22 .78
LITG .13 .12 .88
BLITG .24 .22 .77
Table 1: Bilingual alignment and English dependency results on Chinese-English corpus (? 25 words on
both sides). LITG stands for the cross-language Lexicalized ITG. BLITG is the within-English Bilexical
ITG. ITG-lh is ITG with left-head assumption on English. ITG-rh is with right-head assumption.
Precision Recall AER
ITG .59 .60 .41
LITG .60 .57 .41
BLITG .58 .55 .44
Precision Recall DER
ITG-rh .23 .23 .77
LITG .11 .11 .89
BLITG .24 .24 .76
Table 2: Alignment and dependency results on a larger Chinese-English corpus.
training as the turning point of AER on the de-
velopment data set. The unlexicalized ITG was
trained for 3 iterations. LITG was trained for only
1 iteration, partly because it was initialized with
fully trained ITG parameters. BLITG was trained
for 3 iterations.
For comparison, we also included the results
from IBM Model 1 and Model 4. The numbers
of iterations for the training of the IBM models
were also chosen to be the turning points of AER
changing on the development data set.
We also want to know whether or not BLITG
can model dependencies better than LITG. For
this purpose, we also used the AER measurement,
since the goal is still getting higher precision/recall
for a set of recovered word links, although the de-
pendency word links are within one language. For
this reason, we rename AER to Dependency Error
Rate. Table 1(right) is the dependency results on
English side of the test data set. The dependency
results on Chinese are similar.
The gold standard dependencies were extracted
from Collins? parser output on the sentences. The
LITG and BLITG dependencies were extracted
from the Viterbi synchronous trees by following
the head words.
For comparison, we also included two base-line
results. ITG-lh is unlexicalized ITG with left-head
assumption, meaning the head words always come
from the left branches. ITG-rh is ITG with right-
head assumption.
To make more confident conclusions, we also
did tests on a larger hand-aligned data set used in
Liu et al (2005). We used 165 sentence pairs that
are up to 25 words in length on both sides.
5 Discussion
The BLITG model has two components, namely
the dependency model on the upper levels of the
tree structure and the word-level translation model
at the bottom. We hope that the two components
will mutually improve one another. The current
experiments indicate clearly that the word level
alignment does help inducing dependency struc-
tures on both sides. The precision and recall on
the dependency retrieval sub-task are almost dou-
bled for both languages from LITG which only
has a kind of uni-lexical dependency in each lan-
guage. Although 20% is a low number, given the
fact that the dependencies are learned basically
through contrasting sentences in two languages,
the result is encouraging. The results slightly im-
prove over ITG with right-head assumption for
English, which is based on linguistic insight. Our
results also echo the findings of Kuhn (2004).
They found that based on the guidance of word
alignment between English and multiple other lan-
guages, a modified EM training for PCFG on En-
glish can bootstrap a more accurate monolingual
probabilistic parser. Figure 4 is an example of the
dependency tree on the English side from the out-
put of BLITG, comparing against the parser out-
put.
We did not find that the feedback from the de-
959
are
accomplishments
Economic reform
bright
for
cities
China
?s
14 open frontier
accomplishments
Economic reform frontier
open cities 14
bright
for are
?s
China
Figure 4: Dependency tree extracted from parser output vs. Viterbi dependency tree from BLITG
pendencies help alignment. To get the reasons, we
need further and deeper analysis. One might guess
that the dependencies are modeled but are not yet
strong and good enough given the amount of train-
ing data. Since the training algorithm EM has the
problem of local maxima, we might also need to
adjust the training algorithm to obtain good pa-
rameters for the alignment task. Initializing the
model with good dependency parameters is a pos-
sible adjustment. We would also like to point out
that alignment task is simpler than decoding where
a stronger component of reordering is required to
produce a fluent English sentence. Investigating
the impact of bilexical dependencies on decoding
is our future work.
Acknowledgments This work was supported
by NSF ITR IIS-09325646 and NSF ITR IIS-
0428020.
References
Albert V. Aho and Jeffery D. Ullman. 1972. The
Theory of Parsing, Translation, and Compiling, vol-
ume 1. Prentice-Hall, Englewood Cliffs, NJ.
Hiyan Alshawi, Srinivas Bangalore, and Shona Dou-
glas. 2000. Learning dependency translation mod-
els as collections of finite state head transducers.
Computational Linguistics, 26(1):45?60.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Con-
ference of the Association for Computational Lin-
guistics (ACL-96), pages 310?318, Santa Cruz, CA.
ACL.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Conference of the As-
sociation for Computational Linguistics (ACL-05),
pages 263?270, Ann Arbor, Michigan.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In 37th Annual Meeting of the
Association for Computational Linguistics.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proceedings of the 42nd An-
nual Conference of the Association for Computa-
tional Linguistics (ACL-04).
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings
of the 43rd Annual Conference of the Association
for Computational Linguistics (ACL-05), Ann Ar-
bor, Michigan.
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of the 2003 Meet-
ing of the North American chapter of the Associ-
ation for Computational Linguistics (NAACL-03),
Edmonton.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Conference of the Association for Com-
putational Linguistics (ACL-00), pages 440?447,
Hong Kong, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of the 39th Annual Conference of the Association
for Computational Linguistics (ACL-01), Toulouse,
France.
Richard Zens and Hermann Ney. 2003. A compara-
tive study on reordering constraints in statistical ma-
chine translation. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics, Sapporo, Japan.
Hao Zhang and Daniel Gildea. 2004. Syntax-based
alignment: Supervised or unsupervised? In Pro-
ceedings of the 20th International Conference on
Computational Linguistics (COLING-04), Geneva,
Switzerland, August.
Hao Zhang and Daniel Gildea. 2005. Stochastic lex-
icalized inversion transduction grammar for align-
ment. In Proceedings of the 43rd Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL-05), Ann Arbor, MI.
960
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 184?191,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Optimizing Grammars for Minimum Dependency Length
Daniel Gildea
Computer Science Dept.
University of Rochester
Rochester, NY 14627
David Temperley
Eastman School of Music
University of Rochester
Rochester, NY 14604
Abstract
We examine the problem of choosing word
order for a set of dependency trees so as
to minimize total dependency length. We
present an algorithm for computing the op-
timal layout of a single tree as well as a
numerical method for optimizing a gram-
mar of orderings over a set of dependency
types. A grammar generated by minimizing
dependency length in unordered trees from
the Penn Treebank is found to agree surpris-
ingly well with English word order, suggest-
ing that dependency length minimization has
influenced the evolution of English.
1 Introduction
Dependency approaches to language assume that ev-
ery word in a sentence is the dependent of one other
word (except for one word, which is the global head
of the sentence), so that the words of a sentence form
an acyclic directed graph. An important principle of
language, supported by a wide range of evidence, is
that there is preference for dependencies to be short.
This has been offered as an explanation for numer-
ous psycholinguistic phenomena, such as the greater
processing difficulty of object relative clauses ver-
sus subject relative clauses (Gibson, 1998). Depen-
dency length minimization is also a factor in ambi-
guity resolution: listeners prefer the interpretation
with shorter dependencies. Statistical parsers make
use of features that capture dependency length (e.g.
an adjacency feature in Collins (1999), more explicit
length features in McDonald et al (2005) and Eisner
and Smith (2005)) and thus learn to favor parses with
shorter dependencies.
In this paper we attempt to measure the extent to
which basic English word order chooses to minimize
dependency length, as compared to average depen-
dency lengths under other possible grammars. We
first present a linear-time algorithm for finding the
ordering of a single dependency tree with shortest
total dependency length. Then, given that word or-
der must also be determined by grammatical rela-
tions, we turn to the problem of specifying a gram-
mar in terms of constraints over such relations. We
wish to find the set of ordering constraints on depen-
dency types that minimizes a corpus?s total depen-
dency length. Even assuming that dependency trees
must be projective, this problem is NP-complete,1
but we find that numerical optimization techniques
work well in practice. We reorder unordered depen-
dency trees extracted from corpora and compare the
results to English in terms of both the resulting de-
pendency length and the strings that are produced.
The optimized order constraints show a high degree
of similarity to English, suggesting that dependency
length minimization has influenced the word order
choices of basic English grammar.
2 The Dependency Length Principle
This idea that dependency length minimization may
be a general principle in language has been dis-
cussed by many authors. One example concerns the
1English has crossing (non-projective) dependencies, but
they are believed to be very infrequent. McDonald et al (2005)
report that even in Czech, commonly viewed as a non-projective
language, fewer than 2% of dependencies violate the projectiv-
ity constraint.
184
well-known principle that languages tend to be pre-
dominantly ?head-first? (in which the head of each
dependency is on the left) or ?head-last? (where it
is on the right). Frazier (1985) suggests that this
might serve the function of keeping heads and de-
pendents close together. In a situation where each
word has exactly one dependent, it can be seen that
a ?head-first? arrangement achieves minimal depen-
dency length, as each link has a length of one.
We will call a head-first dependency ?right-
branching? and a head-last dependency ?left-
branching?; a language in which most or all de-
pendencies have the same branching direction is a
?same-branching? language.
Another example of dependency length mini-
mization concerns situations where a head has mul-
tiple dependents. In such cases, dependency length
will be minimized if the shorter dependent is placed
closer to the head. Hawkins (1994) has shown that
this principle is reflected in grammatical rules across
many languages. It is also reflected in situations of
choice; for example, in cases where a verb is fol-
lowed by a prepositional phrase and a direct object
NP, the direct object NP will usually be placed first
(closer to the verb) but if it is longer than the PP, it
is often placed second.
While one might suppose that a ?same-
branching? language is optimal for dependency-
length minimization, this is not in fact the case. If
a word has several dependents, placing them all
on the same side causes them to get in the way of
each other, so that a more ?balanced? configuration
? with some dependents on each side ? has lower
total dependency length. It is particularly desirable
for one or more one-word dependent phrases to be
?opposite-branching? (in relation to the prevailing
branching direction of the language); opposite-
branching of a long phrase tends to cause a long
dependency from the head of the phrase to the
external head.
Exactly this pattern has been observed by Dryer
(1992) in natural languages. Dryer argues that,
while most languages have a predominant branch-
ing direction, phrasal (multi-word) dependents tend
to adhere to this prevailing direction much more
consistently than one-word dependents, which fre-
quently branch opposite to the prevailing direction
of the language. English reflects this pattern quite
?
w0 w1 w2 w3 w4 w5 w6 w7 w8
Figure 1: Separating a dependency link into two
pieces at a subtree boundary.
strongly: While almost all phrasal dependents are
right-branching (prepositional phrases, objects of
prepositions and verbs, relative clauses, etc.), some
1-word categories are left-branching, notably deter-
miners, noun modifiers, adverbs (sometimes), and
attributive adjectives.
This linguistic evidence strongly suggests that
languages have been shaped by principles of de-
pendency length minimization. One might won-
der how close natural languages are to being op-
timal in this regard. To address this question, we
extract unordered dependency graphs from English
and consider different algorithms, which we call De-
pendency Linearization Algorithms (DLAs), for or-
dering the words; our goal is to find the algorithm
that is optimal with regard to dependency length
minimization. We begin with an ?unlabeled? DLA,
which simply minimizes dependency length without
requiring consistent ordering of syntactic relations.
We then consider the more realistic case of a ?la-
beled? DLA, which is required to have syntactically
consistent ordering.
Once we find the optimal DLA, two questions can
be asked. First, how close is dependency length in
English to that of this optimal DLA? Secondly, how
similar is the optimal DLA to English in terms of the
actual rules that arise?
3 The Optimal Unlabeled DLA
Finding linear arrangements of graphs that minimize
total edge length is a classic problem, NP-complete
for general graphs but with an O(n1.6) algorithm for
trees (Chung, 1984). However, the traditional prob-
lem description does not take into account the pro-
jectivity constraint of dependency grammar. This
constraint simplifies the problem; in this section we
show that a simple linear-time algorithm is guaran-
teed to find an optimal result.
A natural strategy would be to apply dynamic pro-
gramming over the tree structure, observing that to-
185
tal dependency length of a linearization can be bro-
ken into the sum of links below any node w in the
tree, and the sum of links outside the node, by which
we mean all links not connected to dependents of the
node. These two quantities interact only through the
position of w relative to the rest of its descendants,
meaning that we can use this position as our dy-
namic programming state, compute the optimal lay-
out of each subtree given each position of the head
within the subtree, and combine subtrees bottom-up
to compute the optimal linearization for the entire
sentence.
This can be further improved by observing that
the total length of the outside links depends on the
position of w only because it affects the length of
the link connecting w to its parent. All other outside
links either cross above all words under w, and de-
pend only on the total size of w?s subtree, or are en-
tirely on one side of w?s subtree. The link from w to
its parent is divided into two pieces, whose lengths
add up to the total length of the link, by slicing the
link where it crosses the boundary from w?s subtree
to the rest of the sentence. In the example in Fig-
ure 1, the dependency from w1 to w6 has total length
five, and is divided in to two components of length
2.5 at the boundary of w1?s subtree. The length of
the piece over w?s subtree depends on w?s position
within that subtree, while the other piece does not
depend on the internal layout of w?s subtree. Thus
the total dependency length for the entire sentence
can be divided into:
1. the length of all links within w?s subtree plus
the length of the first piece of w?s link to its
parent, i.e. the piece that is above descendants
of w.
2. the length of the remaining piece of w?s link to
its parent plus the length of all links outside w.
where the second quantity can be optimized in-
dependently of the internal layout of w?s subtree.
While the link from w to its parent may point either
to the right or left, the optimal layout for w?s subtree
given that w attaches to its left must be the mirror
image of the optimal layout given that w attaches to
its right. Thus, only one case need be considered,
and the optimal layout for the entire sentence can
be computed from the bottom up using just one dy-
namic programming state for each node in the tree.
We now go on to show that, in computing the or-
dering of the di children of a given node, not all di!
possibilities need be considered. In fact, one can
simply order the children by adding them in increas-
ing order of size, going from the head outwards,
and alternating between adding to the left and right
edges of the constituent.
The first part of this proof is the observation that,
as we progress from the head outward, to either the
left or the right, the head?s child subtrees must be
placed in increasing order of size. If any two ad-
jacent children appear with the smaller one further
from the head, we can swap the positions of these
two children, reducing the total dependency length
of the tree. No links crossing over the two chil-
dren will change in length, and no links within ei-
ther child will change. Thus only the length of the
links from the two children will change, and as the
link connecting the outside child now crosses over a
shorter intermediate constituent, the total length will
decrease.
Next, we show that the two longest children must
appear on opposite sides of the head in the optimal
linearization. To see this, consider the case where
both child i (the longest child) and child i ? 1 (the
second longest child) appear on the same side of the
head. From the previous result, we know that i ? 1
and i must be the outermost children on their side.
If there are no children on the other side of the head,
the tree can be improved by moving either i or i ?
1 to the other side. If there is a child on the other
side of the head, it must be smaller than both i and
i? 1, and the tree can be improved by swapping the
position of the child from the other side and child
i? 1.
Given that the two largest children are outermost
and on opposite sides of the head, we observe that
the sum of the two links connecting these children
to the head does not depend on the arrangement of
the first i ? 2 children. Any rearrangement that de-
creases the length of the link to the left of the head
must increase the length of the link to the right of
the head by the same amount. Thus, the optimal lay-
out of all i children can be found by placing the two
largest children outermost and on opposite sides, the
next two largest children next outermost and on op-
186
Figure 2: Placing dependents on alternating sides
from inside out in order of increasing length.
posite sides, and so on until only one or zero chil-
dren are left. If there are an odd number of children,
the side of the final (smallest) child makes no differ-
ence, because the other children are evenly balanced
on the two sides so the last child will have the same
dependency-lengthening effect whichever side it is
on.
Our pairwise approach implies that there are
many optimal linearizations, 2?i/2? in fact, but one
simple and optimal approach is to alternate sides as
in Figure 2, putting the smallest child next to the
head, the next smallest next to the head on the op-
posite side, the next outside the first on the first side,
and so on.
So far we have not considered the piece of the link
from the head to its parent that is over the head?s
subtree. The argument above can be generalized by
considering this link as a special child, longer than
the longest real child. By making the special child
the longest child, we will be guaranteed that it will
be placed on the outside, as is necessary for a projec-
tive tree. As before, the special child and the longest
real child must be placed outermost and on oppo-
site sides, the next two longest children immediately
within the first two, and so on.
Using the algorithm from the previous section, it
is possible to efficiently compute the optimal de-
pendency length from English sentences. We take
sentences from the Wall Street Journal section of
the Penn Treebank, extract the dependency trees us-
ing the head-word rules of Collins (1999), consider
them to be unordered dependency trees, and lin-
earize them to minimize dependency length. Au-
tomatically extracting dependencies from the Tree-
bank can lead to some errors, in particular with
complex compound nouns. Fortunately, compound
nouns tend to occur at the leaves of the tree, and the
head rules are reliable for the vast majority of struc-
tures.
Results in Table 1 show that observed depen-
dency lengths in English are between the minimum
DLA Length
Optimal 33.7
Random 76.1
Observed 47.9
Table 1: Dependency lengths for unlabeled DLAs.
achievable given the unordered dependencies and
the length we would find given a random order-
ing, and are much closer to the minimum. This al-
ready suggests that minimizing dependency length
has been a factor in the development of English.
However, the optimal ?language? to which English
is being compared has little connection to linguis-
tic reality. Essentially, this model represents a free
word-order language: Head-modifier relations are
oriented without regard to the grammatical relation
between the two words. In fact, however, word order
in English is relatively rigid, and a more realistic ex-
periment would be to find the optimal algorithm that
reflects consistent syntactic word order rules. We
call this a ?labeled? DLA, as opposed to the ?unla-
beled? DLA presented above.
4 Labeled DLAs
In this section, we consider linearization algorithms
that assume fixed word order for a given grammat-
ical relation, but choose the order such as to mini-
mize dependency length over a large number of sen-
tences. We represent grammatical relations simply
by using the syntactic categories of the highest con-
stituent headed by (maximal projection of) the two
words in the dependency relation. Due to sparse
data concerns, we removed all function tags such as
TMP (temporal), LOC (locative), and CLR (closely
related) from the treebank. We made an exception
for the SBJ (subject) tag, as we thought it important
to distinguish a verb?s subject and object for the pur-
poses of choosing word order. Looking at a head and
its set of dependents, the complete ordering of all de-
pendents can be modeled as a context-free grammar
rule over a nonterminal alphabet of maximal projec-
tion categories. A fixed word-order language will
have only one rule for each set of nonterminals ap-
pearing in the right-hand side.
Searching over all such DLAs would be exponen-
tially expensive, but a simple approximation of the
187
Dep. len. /
DLA % correct order
random 76.1 / 40.5
extracted from optimal 61.6 / 55.4
weights from English 50.9 / 82.2
optimized weights 42.5 / 64.9
Table 2: Results for different methods of lineariz-
ing unordered trees from section 0 of the Wall Street
Journal corpus. Each result is given as average de-
pendency length in words, followed by the percent-
age of heads (with at least one dependent) having all
dependents correctly ordered.
optimal labeled DLA can found using the following
procedure:
1. Compute the optimal layout of all sentences in
the corpus using the unlabeled DLA.
2. For each combination of a head type and a set
of child types, count the occurrences of each
ordering.
3. Take the most frequent ordering for each set as
the order in the new DLA.
In the first step we used the alternating procedure
from the previous section, with a modification for
the fixed word-order scenario. In order to make
the order of a subtree independent of the direction
in which it attaches to its parent, dependents were
placed in order of length on alternating sides of the
head from the inside out, always starting with the
shortest dependent immediately to the left of the
head.
Results in Table 2 (first two lines) show that a
DLA using rules extracted from the optimal layout
matches English significantly better than a random
DLA, indicating that dependency length can be used
as a general principle to predict word order.
4.1 An Optimized Labeled DLA
While the DLA presented above is a good deal bet-
ter than random (in terms of minimizing dependency
length), there is no reason to suppose that it is opti-
mal. In this section we address the issue of finding
the optimal labeled DLA.
If we model a DLA as a set of context-free gram-
mar rules over dependency types, specifying a fixed
ordering for any set of dependency types attaching
to a given head, the space of DLAs is enormous, and
the problem of finding the optimal DLA is a diffi-
cult one. One way to break the problem down is
to model the DLA as a set of weights for each type
of dependency relation. Under this model the word
order is determined by placing all dependents of a
word in order of increasing weight from left to right.
This reduces the number of parameters of the model
to T , if there are T dependency types, from T k if
a word may have up to k dependents. It also al-
lows us to naturally capture statements such as ?a
noun phrase consists of a determiner, then (possi-
bly) some adjectives, the head noun, and then (pos-
sibly) some prepositional phrases?, by, for example,
setting the weight for NP?DT to -2, NP?JJ to -
1, and NP?PP to 1. We assume the head itself
has a weight of zero, meaning negatively weighted
dependents appear to the head?s left, and positively
weighted dependents to the head?s right.
4.1.1 A DLA Extracted from English
As a test of whether this model is adequate to
represent English word order, we extracted weights
for the Wall Street Journal corpus, used them to re-
order the same set of sentences, and tested how often
words with at least one dependent were assigned the
correct order. We extracted the weights by assign-
ing, for each dependency relation in the corpus, an
integer according to its position relative to the head,
-1 for the first dependent to the left, -2 for the sec-
ond to the left, and so on. We averaged these num-
bers across all occurrences of each dependency type.
The dependency types consisted of the syntactic cat-
egories of the maximal projections of the two words
in the dependency relation.
Reconstructing the word order of each sentence
from this weighted DLA, we find that 82% of all
words with at least one dependent have all depen-
dents ordered correctly (third line of Table 2). This
is significantly higher than the heuristic discussed in
the previous section, and probably as good as can be
expected from such a simple model, particularly in
light of the fact that there is some choice in the word
order for most sentences (among adjuncts for exam-
ple) and that this model does not take the lengths of
188
the individual constituents into account at all.
We now wish to find the set of weights that min-
imize the dependency length of the corpus. While
the size of the search space is still too large to search
exhaustively, numerical optimization techniques can
be applied to find an approximate solution.
4.1.2 NP-Completeness
The problem of finding the optimum weighted
DLA for a set of input trees can be shown to be NP-
complete by reducing from the problem of finding a
graph?s minimum Feedback Arc Set, one of the 21
classic problems of Karp (1972). The input to the
Feedback Arc Set problem is a directed graph, for
which we wish to find an ordering of vertices such
that the smallest number of edges point from later to
earlier vertices in the ordering. Given an instance of
this problem, we can create a set of dependency trees
such that each feedback arc in the original graph
causes total dependency length to increase by one,
if we identify each dependency type with a vertex
in the original problem, and choose weights for the
dependency types according to the vertex order.2
4.1.3 Local Search
Our search procedure is to optimize one weight at
a time, holding all others fixed, and iterating through
the set of weights to be set. The objective function
describing the total dependency length of the corpus
is piecewise constant, as the dependency length will
not change until one weight crosses another, caus-
ing two dependents to reverse order, at which point
the total length will discontinuously jump. Non-
differentiability implies that methods based on gra-
dient ascent will not apply. This setting is reminis-
cent of the problem of optimizing feature weights
for reranking of candidate machine translation out-
puts, and we employ an optimization technique sim-
ilar to that used by Och (2003) for machine trans-
lation. Because the objective function only changes
at points where one weight crosses another?s value,
the set of segments of weight values with different
values of the objective function can be exhaustively
enumerated. In fact, the only significant points are
the values of other weights for dependency types
which occur in the corpus attached to the same head
2We omit details due to space.
Test Data
Training Data WSJ Swbd
WSJ 42.5 / 64.9 12.5 / 63.6
Swbd 43.9 / 59.8 12.2 / 58.7
Table 3: Domain effects on dependency length min-
imization: each result is formatted as in Table 2.
as the dependency being optimized. We build a ta-
ble of interacting dependencies as a preprocessing
step on the data, and then when optimizing a weight,
consider the sequence of values between consecu-
tive interacting weights. When computing the total
corpus dependency length at a new weight value, we
can further speed up computation by reordering only
those sentences in which a dependency type is used,
by building an index of where dependency types oc-
cur as another preprocessing step.
This optimization process is not guaranteed to
find the global maximum (for this reason we call
the resulting DLA ?optimized? rather than ?opti-
mal?). The procedure is guaranteed to converge sim-
ply from the fact that there are a finite number of
objective function values, and the objective function
must increase at each step at which weights are ad-
justed.
We ran this optimization procedure on section 2
through 21 of the Wall Street Journal portion of the
Penn Treebank, initializing all weights to random
numbers between zero and one. This initialization
makes all phrases head-initial to begin with, and has
the effect of imposing a directional bias on the re-
sulting grammar. When optimization converges, we
obtain a set of weights which achieves an average
dependency length of 40.4 on the training data, and
42.5 on held-out data from section 0 (fourth line
of Table 2). While the procedure is unsupervised
with respect to the English word order (other than
the head-initial bias), it is supervised with respect to
dependency length minimization; for this reason we
report all subsequent results on held-out data. While
random initializations lead to an initial average de-
pendency length varying from 60 to 73 with an aver-
age of 66 over ten runs, all runs were within ?.5 of
one another upon convergence. When the order of
words? dependents was compared to the real word
order on held-out data, we find that 64.9% of words
189
Training Sents Dep. len. / % correct order
100 13.70 / 54.38
500 12.81 / 57.75
1000 12.59 / 58.01
5000 12.34 / 55.33
10000 12.27 / 55.92
50000 12.17 / 58.73
Table 4: Average dependency length and rule accu-
racy as a function of training data size, on Switch-
board data.
with at least one dependent have the correct order.
4.2 Domain Variation
Written and spoken language differ significantly in
their structure, and one of the most striking differ-
ences is the much greater average sentence length
of formal written language. The Wall Street Journal
is not representative of typical language use. Lan-
guage was not written until relatively recently in its
development, and the Wall Street Journal in particu-
lar represents a formal style with much longer sen-
tences than are used in conversational speech. The
change in the lengths of sentences and their con-
stituents could make the optimized DLA in terms of
dependency length very different for the two genres.
In order to test this effect, we performed exper-
iments using both the Wall Street Journal (written)
and Switchboard (conversational speech) portions of
the Penn Treebank, and compared results with dif-
ferent training and test data. For Switchboard, we
used the first 50,000 sentences of sections 2 and 3 as
the training data, and all of section 4 as the test data.
We find relatively little difference in dependency
length as we vary training data between written and
spoken English, as shown in Table 3. For the ac-
curacy of the resulting word order, however, train-
ing on Wall Street Journal outperforms Switchboard
even when testing on Switchboard, perhaps because
the longer sentences in WSJ provide more informa-
tion for the optimization procedure to work with.
4.3 Learning Curve
How many sentences are necessary to learn a good
set of dependency weights? Table 4 shows results
for Switchboard as we increase the number of sen-
tences provided as input to the weight optimization
procedure. While the average dependency length on
Label Interpretation Weight
S?NP verb - object NP 0.037
S?NP-SBJ verb - subject NP -0.022
S?PP verb - PP 0.193
NP?DT object noun - determiner -0.070
NP-SBJ?DT subject noun - determiner -0.052
NP?PP obj noun - PP 0.625
NP-SBJ?PP subj noun - PP 0.254
NP?SBAR obj noun - rel. clause 0.858
NP-SBJ?SBAR subject noun - rel. clause -0.110
NP?JJ obj noun - adjective 0.198
NP-SBJ?JJ subj noun - adjective -0.052
Table 5: Sample weights from optimized DLA. Neg-
atively weighted dependents appear to the left of
their head.
held-out test data slowly decreases with more data,
the percentage of correctly ordered dependents is
less well-behaved. It turns out that even 100 sen-
tences are enough to learn a DLA that is nearly as
good as one derived from a much larger dataset.
4.4 Comparing the Optimized DLA to English
We have seen that the optimized DLA matches En-
glish text much better than a random DLA and that
it achieves only a slightly lower dependency length
than English. It is also of interest to compare the
optimized DLA to English in more detail. First
we examine the DLA?s tendency towards ?opposite-
branching 1-word phrases?. English reflects this
principle to a striking degree: on the WSJ test set,
79.4 percent of left-branching phrases are 1-word,
compared to only 19.4 percent of right-branching
phrases. The optimized DLA also reflects this pat-
tern, though somewhat less strongly: 75.5 percent of
left-branching phrases are 1-word, versus 36.7 per-
cent of right-branching phrases.
We can also compare the optimized DLA to En-
glish with regard to specific rules. As explained ear-
lier, the optimal DLA?s rules are expressed in the
form of weights assigned to each relation, with pos-
itive weights indicating right-branching placement.
Table 5 shows some important rules. The middle
column shows the syntactic situation in which the
relation normally occurs. We see, first of all, that
object NPs are to the right of the verb and subject
NPs are to the left, just like in English. PPs are also
the right of verbs; the fact that the weight is greater
than for NPs indicates that they are placed further to
the right, as they normally are in English. Turning
190
to the internal structure of noun phrases, we see that
determiners are to the left of both object and sub-
ject nouns; PPs are to the right of both object and
subject nouns. We also find some differences with
English, however. Clause modifiers of nouns (these
are mostly relative clauses) are to the right of object
nouns, as in English, but to the left of subject nouns;
adjectives are to the left of subject nouns, as in En-
glish, but to the right of object nouns. Of course,
these differences partly arise from the fact that we
treat NP and NP-SBJ as distinct whereas English
does not (with regard to their internal structure).
5 Conclusion
In this paper we have presented a dependency lin-
earization algorithm which is optimized for mini-
mizing dependency length, while still maintaining
consistent positioning for each grammatical relation.
The fact that English is so much lower than the
random DLAs in dependency length gives suggests
that dependency length minimization is an important
general preference in language. The output of the
optimized DLA also proves to be much more similar
to English than a random DLA in word order. An in-
formal comparison of some important rules between
English and the optimal DLA reveals a number of
striking similarities, though also some differences.
The fact that the optimized DLA?s ordering
matches English on only 65% of words shows, not
surprisingly, that English word order is determined
by other factors in addition to dependency length
minimization. In some cases, ordering choices in
English are underdetermined by syntactic rules. For
example, a manner adverb may be placed either be-
fore the verb or after (?He ran quickly / he quickly
ran?). Here the optimized DLA requires a consistent
ordering while English does not. One might suppose
that such syntactic choices in English are guided at
least partly by dependency length minimization, and
indeed there is evidence for this; for example, people
tend to put the shorter of two PPs closer to the verb
(Hawkins, 1994). But there are also other factors in-
volved ? for example, the tendency to put ?given?
discourse elements before ?new? ones, which has
been shown to play a role independent of length
(Arnold et al, 2000).
In other cases, the optimized DLA allows more
fine-grained choices than English. For example, the
optimized DLA treats NP and NP-SBJ as different;
this allows it to have different syntactic rules for the
two cases ? a possibility that it sometimes exploits,
as seen above. No doubt this partly explains why the
optimized DLA achieves lower dependency length
than English.
Acknowledgments This work was supported by
NSF grants IIS-0546554 and IIS-0325646.
References
J. E. Arnold, T. Wasow, T. Losongco, and R. Ginstrom.
2000. Heaviness vs. newness: the effects of structural
complexity and discourse status on constituent order-
ing. Language, 76:28?55.
F. R. K. Chung. 1984. On optimal linear arrangements of
trees. Computers and Mathematics with Applications,
10:43?60.
Michael John Collins. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Matthew Dryer. 1992. The Greenbergian word order cor-
relations. Language, 68:81?138.
Jason Eisner and Noah A. Smith. 2005. Parsing with
soft and hard constraints on dependency length. In
Proceedings of the International Workshop on Parsing
Technologies (IWPT), pages 30?41.
Lyn Frazier. 1985. Syntactic complexity. In D. Dowty,
L. Karttunen, and A. Zwicky, editors, Natural Lan-
guage Parsing: Psychological, Computational, and
Theoretical Perspectives, pages 129?189. Cambridge
University Press, Cambridge.
Edward Gibson. 1998. Linguistic complexity: Locality
of syntactic dependencies. Cognition, 68:1?76.
John Hawkins. 1994. A Performance Theory of Order
and Constituency. Cambridge University Press, Cam-
bridge, UK.
Richard M. Karp. 1972. Reducibility among combina-
torial problems. In R. E. Miller and J. W. Thatcher,
editors, Complexity of Computer Computations, pages
85?103.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of ACL-
03.
191
Proceedings of ACL-08: HLT, pages 97?105,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Bayesian Learning of Non-compositional Phrases with Synchronous Parsing
Hao Zhang
Computer Science Department
University of Rochester
Rochester, NY 14627
zhanghao@cs.rochester.edu
Chris Quirk
Microsoft Research
One Microsoft Way
Redmond, WA 98052 USA
chrisq@microsoft.com
Robert C. Moore
Microsoft Research
One Microsoft Way
Redmond, WA 98052 USA
bobmoore@microsoft.com
Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
gildea@cs.rochester.edu
Abstract
We combine the strengths of Bayesian mod-
eling and synchronous grammar in unsu-
pervised learning of basic translation phrase
pairs. The structured space of a synchronous
grammar is a natural fit for phrase pair proba-
bility estimation, though the search space can
be prohibitively large. Therefore we explore
efficient algorithms for pruning this space that
lead to empirically effective results. Incorpo-
rating a sparse prior using Variational Bayes,
biases the models toward generalizable, parsi-
monious parameter sets, leading to significant
improvements in word alignment. This pref-
erence for sparse solutions together with ef-
fective pruning methods forms a phrase align-
ment regimen that produces better end-to-end
translations than standard word alignment ap-
proaches.
1 Introduction
Most state-of-the-art statistical machine transla-
tion systems are based on large phrase tables ex-
tracted from parallel text using word-level align-
ments. These word-level alignments are most of-
ten obtained using Expectation Maximization on the
conditional generative models of Brown et al (1993)
and Vogel et al (1996). As these word-level align-
ment models restrict the word alignment complex-
ity by requiring each target word to align to zero
or one source words, results are improved by align-
ing both source-to-target as well as target-to-source,
then heuristically combining these alignments. Fi-
nally, the set of phrases consistent with the word
alignments are extracted from every sentence pair;
these form the basis of the decoding process. While
this approach has been very successful, poor word-
level alignments are nonetheless a common source
of error in machine translation systems.
A natural solution to several of these issues is
unite the word-level and phrase-level models into
one learning procedure. Ideally, such a procedure
would remedy the deficiencies of word-level align-
ment models, including the strong restrictions on
the form of the alignment, and the strong inde-
pendence assumption between words. Furthermore
it would obviate the need for heuristic combina-
tion of word alignments. A unified procedure may
also improve the identification of non-compositional
phrasal translations, and the attachment decisions
for unaligned words.
In this direction, Expectation Maximization at
the phrase level was proposed by Marcu and Wong
(2002), who, however, experienced two major dif-
ficulties: computational complexity and controlling
overfitting. Computational complexity arises from
the exponentially large number of decompositions
of a sentence pair into phrase pairs; overfitting is a
problem because as EM attempts to maximize the
likelihood of its training data, it prefers to directly
explain a sentence pair with a single phrase pair.
In this paper, we attempt to address these two is-
sues in order to apply EM above the word level.
97
We attack computational complexity by adopting
the polynomial-time Inversion Transduction Gram-
mar framework, and by only learning small non-
compositional phrases. We address the tendency of
EM to overfit by using Bayesian methods, where
sparse priors assign greater mass to parameter vec-
tors with fewer non-zero values therefore favoring
shorter, more frequent phrases. We test our model
by extracting longer phrases from our model?s align-
ments using traditional phrase extraction, and find
that a phrase table based on our system improves MT
results over a phrase table extracted from traditional
word-level alignments.
2 Phrasal Inversion Transduction
Grammar
We use a phrasal extension of Inversion Transduc-
tion Grammar (Wu, 1997) as the generative frame-
work. Our ITG has two nonterminals: X and
C, where X represents compositional phrase pairs
that can have recursive structures and C is the pre-
terminal over terminal phrase pairs. There are three
rules with X on the left-hand side:
X ? [X X],
X ? ?X X?,
X ? C.
The first two rules are the straight rule and in-
verted rule respectively. They split the left-hand side
constituent which represents a phrase pair into two
smaller phrase pairs on the right-hand side and order
them according to one of the two possible permuta-
tions. The rewriting process continues until the third
rule is invoked. C is our unique pre-terminal for
generating terminal multi-word pairs:
C ? e/f .
We parameterize our probabilistic model in the
manner of a PCFG: we associate a multinomial dis-
tribution with each nonterminal, where each out-
come in this distribution corresponds to an expan-
sion of that nonterminal. Specifically, we place one
multinomial distribution ?X over the three expan-
sions of the nonterminalX , and another multinomial
distribution ?C over the expansions of C. Thus, the
parameters in our model can be listed as
?X = (P??, P[], PC),
where P?? is for the inverted rule, P[] for the straight
rule, PC for the third rule, satisfyingP??+P[]+PC =
1, and
?C = (P (e/f), P (e?/f ?), . . . ),
where
?
e/f P (e/f) = 1 is a multinomial distribu-
tion over phrase pairs.
This is our model in a nutshell. We can train
this model using a two-dimensional extension of the
inside-outside algorithm on bilingual data, assuming
every phrase pair that can appear as a leaf in a parse
tree of the grammar a valid candidate. However, it is
easy to show that the maximum likelihood training
will lead to the saturated solution where PC = 1 ?
each sentence pair is generated by a single phrase
spanning the whole sentence. From the computa-
tional point of view, the full EM algorithm runs in
O(n6) where n is the average length of the two in-
put sentences, which is too slow in practice.
The key is to control the number of parameters,
and therefore the size of the set of candidate phrases.
We deal with this problem in two directions. First
we change the objective function by incorporating
a prior over the phrasal parameters. This has the
effect of preferring parameter vectors in ?C with
fewer non-zero values. Our second approach was
to constrain the search space using simpler align-
ment models, which has the further benefit of signif-
icantly speeding up training. First we train a lower
level word alignment model, then we place hard con-
straints on the phrasal alignment space using confi-
dent word links from this simpler model. Combining
the two approaches, we have a staged training pro-
cedure going from the simplest unconstrained word
based model to a constrained Bayesian word-level
ITG model, and finally proceeding to a constrained
Bayesian phrasal model.
3 Variational Bayes for ITG
Goldwater and Griffiths (2007) and Johnson (2007)
show that modifying an HMM to include a sparse
prior over its parameters and using Bayesian esti-
mation leads to improved accuracy for unsupervised
part-of-speech tagging. In this section, we describe
a Bayesian estimator for ITG: we select parame-
ters that optimize the probability of the data given
a prior. The traditional estimation method for word
98
alignment models is the EM algorithm (Brown et
al., 1993) which iteratively updates parameters to
maximize the likelihood of the data. The drawback
of maximum likelihood is obvious for phrase-based
models. If we do not put any constraint on the dis-
tribution of phrases, EM overfits the data by mem-
orizing every sentence pair. A sparse prior over a
multinomial distribution such as the distribution of
phrase pairs may bias the estimator toward skewed
distributions that generalize better. In the context of
phrasal models, this means learning the more repre-
sentative phrases in the space of all possible phrases.
The Dirichlet distribution, which is parameter-
ized by a vector of real values often interpreted as
pseudo-counts, is a natural choice for the prior, for
two main reasons. First, the Dirichlet is conjugate
to the multinomial distribution, meaning that if we
select a Dirichlet prior and a multinomial likelihood
function, the posterior distribution will again be a
Dirichlet. This makes parameter estimation quite
simple. Second, Dirichlet distributions with small,
non-zero parameters place more probability mass on
multinomials on the edges or faces of the probabil-
ity simplex, distributions with fewer non-zero pa-
rameters. Starting from the model from Section 2,
we propose the following Bayesian extension, where
A ? Dir(B) means the random variable A is dis-
tributed according to a Dirichlet with parameter B:
?X | ?X ? Dir(?X),
?C | ?C ? Dir(?C),
[X X]
?X X?
C
X ? Multi(?X),
e/f | C ? Multi(?C).
The parameters ?X and ?C control the sparsity of
the two distributions in our model. One is the distri-
bution of the three possible branching choices. The
other is the distribution of the phrase pairs. ?C is
crucial, since the multinomial it is controlling has a
high dimension. By adjusting ?C to a very small
number, we hope to place more posterior mass on
parsimonious solutions with fewer but more confi-
dent and general phrase pairs.
Having defined the Bayesian model, it remains
to decide the inference procedure. We chose Vari-
ational Bayes, for its procedural similarity to EM
and ease of implementation. Another potential op-
tion would be Gibbs sampling (or some other sam-
pling technique). However, in experiments in un-
supervised POS tag learning using HMM structured
models, Johnson (2007) shows that VB is more ef-
fective than Gibbs sampling in approaching distribu-
tions that agree with the Zipf?s law, which is promi-
nent in natural languages.
Kurihara and Sato (2006) describe VB for PCFGs,
showing the only need is to change the M step of
the EM algorithm. As in the case of maximum like-
lihood estimation, Bayesian estimation for ITGs is
very similar to PCFGs, which follows due to the
strong isomorphism between the two models. Spe-
cific to our ITG case, the M step becomes:
P? (l+1)[] =
exp(?(E(X ? [X X]) + ?X))
exp(?(E(X) + s?X))
,
P? (l+1)?? =
exp(?(E(X ? ?X X?) + ?X))
exp(?(E(X) + s?X))
,
P? (l+1)C =
exp(?(E(X ? C) + ?X))
exp(?(E(X) + s?X))
,
P? (l+1)(e/f) = exp(?(E(e/f) + ?C))exp(?(E(C) +m?C))
,
where ? is the digamma function (Beal, 2003), s =
3 is the number of right-hand-sides for X , and m is
the number of observed phrase pairs in the data. The
sole difference between EM and VB with a sparse
prior ? is that the raw fractional counts c are re-
placed by exp(?(c + ?)), an operation that resem-
bles smoothing. As pointed out by Johnson (2007),
in effect this expression adds to c a small value that
asymptotically approaches ? ? 0.5 as c approaches
?, and 0 as c approaches 0. For small values of
? the net effect is the opposite of typical smooth-
ing, since it tends to redistribute probably mass away
from unlikely events onto more likely ones.
4 Bitext Pruning Strategy
ITG is slow mainly because it considers every pair of
spans in two sentences as a possible chart element.
In reality, the set of useful chart elements is much
99
smaller than the possible scriptO(n4), where n is
the average sentence length. Pruning the span pairs
(bitext cells) that can participate in a tree (either as
terminals or non-terminals) serves to not only speed
up ITG parsing, but also to provide a kind of ini-
tialization hint to the training procedures, encourag-
ing it to focus on promising regions of the alignment
space.
Given a bitext cell defined by the four boundary
indices (i, j, l,m) as shown in Figure 1a, we prune
based on a figure of merit V (i, j, l,m) approximat-
ing the utility of that cell in a full ITG parse. The
figure of merit considers the Model 1 scores of not
only the words inside a given cell, but also all the
words not included in the source and target spans, as
in Moore (2003) and Vogel (2005). Like Zhang and
Gildea (2005), it is used to prune bitext cells rather
than score phrases. The total score is the product of
the Model 1 probabilities for each column; ?inside?
columns in the range [l,m] are scored according to
the sum (or maximum) of Model 1 probabilities for
[i, j], and ?outside? columns use the sum (or maxi-
mum) of all probabilities not in the range [i, j].
Our pruning differs from Zhang and Gildea
(2005) in two major ways. First, we perform prun-
ing using both directions of the IBM Model 1 scores;
instead of a single figure of merit V , we have two:
VF and VB . Only those spans that pass the prun-
ing threshold in both directions are kept. Second,
we allow whole spans to be pruned. The figure of
merit for a span is VF (i, j) = maxl,m VF (i, j, l,m).
Only spans that are within some threshold of the un-
restricted Model 1 scores VF and VB are kept:
VF (i, j)
VF
? ?s and
VB(l,m)
VB
? ?s.
Amongst those spans retained by this first threshold,
we keep only those bitext cells satisfying both
VF (i, j, l,m)
VF (i, j)
? ?b and
VB(i, j, l,m)
VB(l,m)
? ?b.
4.1 Fast Tic-tac-toe Pruning
The tic-tac-toe pruning algorithm (Zhang and
Gildea, 2005) uses dynamic programming to com-
pute the product of inside and outside scores for
all cells in O(n4) time. However, even this can be
slow for large values of n. Therefore we describe an
Figure 1: (a) shows the original tic-tac-toe score for a
bitext cell (i, j, l,m). (b) demonstrates the finite state
representation using the machine in (c), assuming a fixed
source span (i, j).
improved algorithm with best case n3 performance.
Although the worst case performance is also O(n4),
in practice it is significantly faster.
To begin, let us restrict our attention to the for-
ward direction for a fixed source span (i, j). Prun-
ing bitext spans and cells requires VF (i, j), the score
of the best bitext cell within a given span, as well
as all cells within a given threshold of that best
score. For a fixed i and j, we need to search over
the starting and ending points l and m of the in-
side region. Note that there is an isomorphism be-
tween the set of spans and a simple finite state ma-
chine: any span (l,m) can be represented by a se-
quence of l OUTSIDE columns, followed bym?l+1
INSIDE columns, followed by n ? m + 1 OUT-
SIDE columns. This simple machine has the re-
stricted form described in Figure 1c: it has three
states, L, M , and R; each transition generates ei-
ther an OUTSIDE column O or an INSIDE column
I . The cost of generating an OUTSIDE at posi-
tion a is O(a) = P (ta|NULL) +
?
b 6?[i,j] P (ta|sb);
likewise the cost of generating an INSIDE column
is I(a) = P (ta|NULL) +
?
b?[i,j] P (ta|sb), with
100
O(0) = O(n+ 1) = 1 and I(0) = I(n+ 1) = 0.
Directly computing O and I would take time
O(n2) for each source span, leading to an overall
runtime of O(n4). Luckily there are faster ways to
find the inside and outside scores. First we can pre-
compute following arrays in O(n2) time and space:
pre[0, l] := P (tl|NULL)
pre[i, l] := pre[i? 1, l] + P (tl|si)
suf[n+ 1, l] := 0
suf[i, l] := suf[i+ 1, l] + P (tl|si)
Then for any (i, j), O(a) = P (ta|NULL) +
?
b 6?[i,j] P (ta|sb) = pre[i ? 1, a] + suf[j + 1, a].
I(a) can be incrementally updated as the source
span varies: when i = j, I(a) = P (ta|NULL) +
P (ta|si). As j is incremented, we add P (ta|sj) to
I(a). Thus we have linear time updates for O and I .
We can then find the best scoring sequence using
the familiar Viterbi algorithm. Let ?[a, ?] be the cost
of the best scoring sequence ending at in state ? at
time a:
?[0, ?] := 1 if ? = L; 0 otherwise
?[a, L] := ?[a? 1, L] ?O(a)
?[a,M ] := max
??L,M
{?[a? 1, ?]} ? I(a)
?[a,R] := max
??M,R
{?[a? 1, ?]} ?O(a)
Then VF (i, j) = ?[n + 1, R], using the isomor-
phism between state sequences and spans. This lin-
ear time algorithm allows us to compute span prun-
ing in O(n3) time. The same algorithm may be
performed using the backward figure of merit after
transposing rows and columns.
Having cast the problem in terms of finite state au-
tomata, we can use finite state algorithms for prun-
ing. For instance, fixing a source span we can enu-
merate the target spans in decreasing order by score
(Soong and Huang, 1991), stopping once we en-
counter the first span below threshold. In practice
the overhead of maintaining the priority queue out-
weighs any benefit, as seen in Figure 2.
An alternate approach that avoids this overhead is
to enumerate spans by position. Note that ?[m,R] ?
?n
a=m+1O(a) is within threshold iff there is a
span with right boundary m? < m within thresh-
old. Furthermore if ?[m,M ] ? ?na=m+1O(a) is
 0
 100
 200
 300
 400
 500
 600
 700
 800
 900
 10  20  30  40  50
Pr
un
in
g 
tim
e 
(th
ou
san
ds
 of
 se
co
nd
s)
Average sentence length
Baseline
k-best
Fast
Figure 2: Speed comparison of the O(n4) tic-tac-toe
pruning algorithm, the A* top-x algorithm, and the fast
tic-tac-toe pruning. All produce the same set of bitext
cells, those within threshold of the best bitext cell.
within threshold, thenm is the right boundary within
threshold. Using these facts, we can gradually
sweep the right boundary m from n toward 1 until
the first condition fails to hold. For each value where
the second condition holds, we pause to search for
the set of left boundaries within threshold.
Likewise for the left edge, ?[l,M ] ??ma=l+1 I(a) ?
?n
a=m+1O(a) is within threshold iff there is some
l? < l identifying a span (l?,m) within threshold.
Finally if V (i, j, l,m) = ?[l ? 1, L] ? ?ma=l I(a) ?
?n
a=m+1O(a) is within threshold, then (i, j, l,m)
is a bitext cell within threshold. For right edges that
are known to be within threshold, we can sweep the
left edges leftward until the first condition no longer
holds, keeping only those spans for which the sec-
ond condition holds.
The filtering algorithm behaves extremely well.
Although the worst case runtime is still O(n4), the
best case has improved to n3; empirically it seems to
significantly reduce the amount of time spent explor-
ing spans. Figure 2 compares the speed of the fast
tic-tac-toe algorithm against the algorithm in Zhang
and Gildea (2005).
101
Figure 3: Example output from the ITG using non-compositional phrases. (a) is the Viterbi alignment from the word-
based ITG. The shaded regions indicate phrasal alignments that are allowed by the non-compositional constraint; all
other phrasal alignments will not be considered. (b) is the Viterbi alignment from the phrasal ITG, with the multi-word
alignments highlighted.
5 Bootstrapping Phrasal ITG from
Word-based ITG
This section introduces a technique that bootstraps
candidate phrase pairs for phrase-based ITG from
word-based ITG Viterbi alignments. The word-
based ITG uses the same expansions for the non-
terminal X , but the expansions of C are limited to
generate only 1-1, 1-0, and 0-1 alignments:
C ? e/f,
C ? e/?,
C ? ?/f
where ? indicates that no word was generated.
Broadly speaking, the goal of this section is the same
as the previous section, namely, to limit the set of
phrase pairs that needs to be considered in the train-
ing process. The tic-tac-toe pruning relies on IBM
model 1 for scoring a given aligned area. In this
part, we use word-based ITG alignments as anchor
points in the alignment space to pin down the poten-
tial phrases. The scope of iterative phrasal ITG train-
ing, therefore, is limited to determining the bound-
aries of the phrases anchored on the given one-to-
one word alignments.
The heuristic method is based on the Non-
Compositional Constraint of Cherry and Lin (2007).
Cherry and Lin (2007) use GIZA++ intersections
which have high precision as anchor points in the
bitext space to constraint ITG phrases. We use ITG
Viterbi alignments instead. The benefit is two-fold.
First of all, we do not have to run a GIZA++ aligner.
Second, we do not need to worry about non-ITG
word alignments, such as the (2, 4, 1, 3) permutation
patterns. GIZA++ does not limit the set of permu-
tations allowed during translation, so it can produce
permutations that are not reachable using an ITG.
Formally, given a word-based ITG alignment, the
bootstrapping algorithm finds all the phrase pairs
according to the definition of Och and Ney (2004)
and Chiang (2005) with the additional constraint
that each phrase pair contains at most one word
link. Mathematically, let e(i, j) count the number of
word links that are emitted from the substring ei...j ,
and f(l,m) count the number of word links emit-
ted from the substring fl...m. The non-compositional
phrase pairs satisfy
e(i, j) = f(l,m) ? 1.
Figure 3 (a) shows all possible non-compositional
phrases given the Viterbi word alignment of the ex-
ample sentence pair.
6 Summary of the Pipeline
We summarize the pipeline of our system, demon-
strating the interactions between the three main con-
tributions of this paper: Variational Bayes, tic-tac-
toe pruning, and word-to-phrase bootstrapping. We
102
start from sentence-aligned bilingual data and run
IBM Model 1 in both directions to obtain two trans-
lation tables. Then we use the efficient bidirectional
tic-tac-toe pruning to prune the bitext space within
each of the sentence pairs; ITG parsing will be car-
ried out on only this this sparse set of bitext cells.
The first stage of training is word-based ITG, us-
ing the standard iterative training procedure, except
VB replaces EM to focus on a sparse prior. Af-
ter several training iterations, we obtain the Viterbi
alignments on the training data according to the fi-
nal model. Now we transition into the second stage
? the phrasal training. Before the training starts,
we apply the non-compositional constraints over the
pruned bitext space to further constrain the space
of phrase pairs. Finally, we run phrasal ITG itera-
tive training using VB for a certain number of itera-
tions. In the end, a Viterbi pass for the phrasal ITG is
executed to produce the non-compositional phrasal
alignments. From this alignment, phrase pairs are
extracted in the usual manner, and a phrase-based
translation system is trained.
7 Experiments
The training data was a subset of 175K sentence
pairs from the NIST Chinese-English training data,
automatically selected to maximize character-level
overlap with the source side of the test data. We put
a length limit of 35 on both sides, producing a train-
ing set of 141K sentence pairs. 500 Chinese-English
pairs from this set were manually aligned and used
as a gold standard.
7.1 Word Alignment Evaluation
First, using evaluations of alignment quality, we
demonstrate the effectiveness of VB over EM, and
explore the effect of the prior.
Figure 4 examines the difference between EM and
VB with varying sparse priors for the word-based
model of ITG on the 500 sentence pairs, both af-
ter 10 iterations of training. Using EM, because of
overfitting, AER drops first and increases again as
the number of iterations varies from 1 to 10. The
lowest AER using EM is achieved after the second
iteration, which is .40. At iteration 10, AER for EM
increases to .42. On the other hand, using VB, AER
decreases monotonically over the 10 iterations and
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 1e-009  1e-006  0.001  1
A
ER
Prior value
VB
EM
Figure 4: AER drops as ?C approaches zero; a more
sparse solution leads to better results.
stabilizes at iteration 10. When ?C is 1e ? 9, VB
gets AER close to .35 at iteration 10.
As we increase the bias toward sparsity, the AER
decreases, following a long slow plateau. Although
the magnitude of improvement is not large, the trend
is encouraging.
These experiments also indicate that a very sparse
prior is needed for machine translation tasks. Un-
like Johnson (2007), who found optimal perfor-
mance when ? was approximately 10?4, we ob-
served monotonic increases in performance as ?
dropped. The dimensionality of this MT problem is
significantly larger than that of the sequence prob-
lem, though, therefore it may take a stronger push
from the prior to achieve the desired result.
7.2 End-to-end Evaluation
Given an unlimited amount of time, we would tune
the prior to maximize end-to-end performance, us-
ing an objective function such as BLEU. Unfortu-
nately these experiments are very slow. Since we
observed monotonic increases in alignment perfor-
mance with smaller values of ?C , we simply fixed
the prior at a very small value (10?100) for all trans-
lation experiments. We do compare VB against EM
in terms of final BLEU scores in the translation ex-
periments to ensure that this sparse prior has a sig-
103
nificant impact on the output.
We also trained a baseline model with GIZA++
(Och and Ney, 2003) following a regimen of 5 it-
erations of Model 1, 5 iterations of HMM, and 5
iterations of Model 4. We computed Chinese-to-
English and English-to-Chinese word translation ta-
bles using five iterations of Model 1. These val-
ues were used to perform tic-tac-toe pruning with
?b = 1 ? 10?3 and ?s = 1 ? 10?6. Over the pruned
charts, we ran 10 iterations of word-based ITG using
EM or VB. The charts were then pruned further by
applying the non-compositional constraint from the
Viterbi alignment links of that model. Finally we ran
10 iterations of phrase-based ITG over the residual
charts, using EM or VB, and extracted the Viterbi
alignments.
For translation, we used the standard phrasal de-
coding approach, based on a re-implementation of
the Pharaoh system (Koehn, 2004). The output of
the word alignment systems (GIZA++ or ITG) were
fed to a standard phrase extraction procedure that
extracted all phrases of length up to 7 and esti-
mated the conditional probabilities of source given
target and target given source using relative fre-
quencies. Thus our phrasal ITG learns only the
minimal non-compositional phrases; the standard
phrase-extraction algorithm learns larger combina-
tions of these minimal units. In addition the phrases
were annotated with lexical weights using the IBM
Model 1 tables. The decoder also used a trigram lan-
guage model trained on the target side of the training
data, as well as word count, phrase count, and distor-
tion penalty features. Minimum Error Rate training
(Och, 2003) over BLEU was used to optimize the
weights for each of these models over the develop-
ment test data.
We used the NIST 2002 evaluation datasets for
tuning and evaluation; the 10-reference develop-
ment set was used for minimum error rate training,
and the 4-reference test set was used for evaluation.
We trained several phrasal translation systems, vary-
ing only the word alignment (or phrasal alignment)
method.
Table 1 compares the four systems: the GIZA++
baseline, the ITG word-based model, the ITG multi-
word model using EM training, and the ITG multi-
word model using VB training. ITG-mwm-VB is
our best model. We see an improvement of nearly
Development Test
GIZA++ 37.46 28.24
ITG-word 35.47 26.55
ITG-mwm (VB) 39.21 29.02
ITG-mwm (EM) 39.15 28.47
Table 1: Translation results on Chinese-English, using
the subset of training data (141K sentence pairs) that have
length limit 35 on both sides. (No length limit in transla-
tion. )
2 points dev set and nearly 1 point of improvement
on the test set. We also observe the consistent supe-
riority of VB over EM. The gain is especially large
on the test data set, indicating VB is less prone to
overfitting.
8 Conclusion
We have presented an improved and more efficient
method of estimating phrase pairs directly. By both
changing the objective function to include a bias
toward sparser models and improving the pruning
techniques and efficiency, we achieve significant
gains on test data with practical speed. In addition,
these gains were shown without resorting to external
models, such as GIZA++. We have shown that VB
is both practical and effective for use in MT models.
However, our best system does not apply VB to a
single probability model, as we found an apprecia-
ble benefit from bootstrapping each model from sim-
pler models, much as the IBM word alignment mod-
els are usually trained in succession. We find that
VB alone is not sufficient to counteract the tendency
of EM to prefer analyses with smaller trees using
fewer rules and longer phrases. Both the tic-tac-toe
pruning and the non-compositional constraint ad-
dress this problem by reducing the space of possible
phrase pairs. On top of these hard constraints, the
sparse prior of VB helps make the model less prone
to overfitting to infrequent phrase pairs, and thus
improves the quality of the phrase pairs the model
learns.
Acknowledgments This work was done while the
first author was at Microsoft Research; thanks to Xi-
aodong He, Mark Johnson, and Kristina Toutanova.
The last author was supported by NSF IIS-0546554.
104
References
Matthew Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Gatsby
Computational Neuroscience Unit, University College
London.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In Proceedings of SSST, NAACL-HLT 2007 / AMTA
Workshop on Syntax and Structure in Statistical Trans-
lation, pages 17?24, Rochester, New York, April. As-
sociation for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270, Ann Arbor, Michigan, USA.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
744?751, Prague, Czech Republic, June. Association
for Computational Linguistics.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 296?305.
Philipp Koehn. 2004. Pharaoh: A beam search de-
coder for phrase-based statistical machine translation
models. In Proceedings of the 6th Conference of the
Association for Machine Translation in the Americas
(AMTA), pages 115?124, Washington, USA, Septem-
ber.
Kenichi Kurihara and Taisuke Sato. 2006. Variational
bayesian grammar induction for natural language. In
International Colloquium on Grammatical Inference,
pages 84?96, Tokyo, Japan.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In 2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Robert C. Moore. 2003. Learning translations of named-
entity phrases from parallel corpora. In Proceedings
of EACL, Budapest, Hungary.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449, De-
cember.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167, Sapporo, Japan.
Frank Soong and Eng Huang. 1991. A tree-trellis based
fast search for finding the n best sentence hypotheses
in continuous speech recognition. In Proceedings of
ICASSP 1991.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of COLING, pages 836?741,
Copenhagen, Denmark.
Stephan Vogel. 2005. PESA: Phrase pair extraction as
sentence splitting. In MT Summit X, Phuket, Thailand.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403, Septem-
ber.
Hao Zhang and Daniel Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment. In
Proceedings of ACL.
105
Proceedings of ACL-08: HLT, pages 209?217,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Efficient Multi-pass Decoding for Synchronous Context Free Grammars
Hao Zhang and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
We take a multi-pass approach to ma-
chine translation decoding when using syn-
chronous context-free grammars as the trans-
lation model and n-gram language models:
the first pass uses a bigram language model,
and the resulting parse forest is used in the
second pass to guide search with a trigram lan-
guage model. The trigram pass closes most
of the performance gap between a bigram de-
coder and a much slower trigram decoder, but
takes time that is insignificant in comparison
to the bigram pass. An additional fast de-
coding pass maximizing the expected count
of correct translation hypotheses increases the
BLEU score significantly.
1 Introduction
Statistical machine translation systems based
on synchronous grammars have recently shown
great promise, but one stumbling block to their
widespread adoption is that the decoding, or search,
problem during translation is more computationally
demanding than in phrase-based systems. This com-
plexity arises from the interaction of the tree-based
translation model with an n-gram language model.
Use of longer n-grams improves translation results,
but exacerbates this interaction. In this paper, we
present three techniques for attacking this problem
in order to obtain fast, high-quality decoders.
First, we present a two-pass decoding algorithm,
in which the first pass explores states resulting from
an integrated bigram language model, and the sec-
ond pass expands these states into trigram-based
states. The general bigram-to-trigram technique
is common in speech recognition (Murveit et al,
1993), where lattices from a bigram-based decoder
are re-scored with a trigram language model. We ex-
amine the question of whether, given the reordering
inherent in the machine translation problem, lower
order n-grams will provide as valuable a search
heuristic as they do for speech recognition.
Second, we explore heuristics for agenda-based
search, and present a heuristic for our second pass
that combines precomputed language model infor-
mation with information derived from the first pass.
With this heuristic, we achieve the same BLEU
scores and model cost as a trigram decoder with es-
sentially the same speed as a bigram decoder.
Third, given the significant speedup in the
agenda-based trigram decoding pass, we can rescore
the trigram forest to maximize the expected count of
correct synchronous constituents of the model, us-
ing the product of inside and outside probabilities.
Maximizing the expected count of synchronous con-
stituents approximately maximizes BLEU. We find
a significant increase in BLEU in the experiments,
with minimal additional time.
2 Language Model Integrated Decoding
for SCFG
We begin by introducing Synchronous Context Free
Grammars and their decoding algorithms when an
n-gram language model is integrated into the gram-
matical search space.
A synchronous CFG (SCFG) is a set of context-
free rewriting rules for recursively generating string
pairs. Each synchronous rule is a pair of CFG rules
209
with the nonterminals on the right hand side of one
CFG rule being one-to-one mapped to the other CFG
rule via a permutation pi. We adopt the SCFG nota-
tion of Satta and Peserico (2005). Superscript in-
dices in the right-hand side of grammar rules:
X ? X(1)1 ...X(n)n , X
(pi(1))
pi(1) ...X
(pi(n))
pi(n)
indicate that the nonterminals with the same index
are linked across the two languages, and will eventu-
ally be rewritten by the same rule application. Each
Xi is a variable which can take the value of any non-
terminal in the grammar.
In this paper, we focus on binary SCFGs and
without loss of generality assume that only the pre-
terminal unary rules can generate terminal string
pairs. Thus, we are focusing on Inversion Transduc-
tion Grammars (Wu, 1997) which are an important
subclass of SCFG. Formally, the rules in our gram-
mar include preterminal unary rules:
X ? e/f
for pairing up words or phrases in the two languages
and binary production rules with straight or inverted
orders that are responsible for building up upper-
level synchronous structures. They are straight rules
written:
X ? [Y Z]
and inverted rules written:
X ? ?Y Z?.
Most practical non-binary SCFGs can be bina-
rized using the synchronous binarization technique
by Zhang et al (2006). The Hiero-style rules of
(Chiang, 2005), which are not strictly binary but bi-
nary only on nonterminals:
X ? yu X(1) you X(2); have X(2) with X(1)
can be handled similarly through either offline bi-
narization or allowing a fixed maximum number of
gap words between the right hand side nonterminals
in the decoder.
For these reasons, the parsing problems for more
realistic synchronous CFGs such as in Chiang
(2005) and Galley et al (2006) are formally equiva-
lent to ITG. Therefore, we believe our focus on ITG
for the search efficiency issue is likely to generalize
to other SCFG-based methods.
Without an n-gram language model, decoding us-
ing SCFG is not much different from CFG pars-
ing. At each time a CFG rule is applied on the in-
put string, we apply the synchronized CFG rule for
the output language. From a dynamic programming
point of view, the DP states are X[i, j], where X
ranges over all possible nonterminals and i and j
range over 0 to the input string length |w|. Each
state stores the best translations obtainable. When
we reach the top state S[0, |w|], we can get the best
translation for the entire sentence. The algorithm is
O(|w|3).
However, when we want to integrate an n-gram
language model into the search, our goal is search-
ing for the derivation whose total sum of weights
of productions and n-gram log probabilities is
maximized. Now the adjacent span-parameterized
states X[i, k] and X[k, j] can interact with each
other by ?peeping into? the leading and trailing
n ? 1 words on the output side for each state.
Different boundary words differentiate the span-
parameterized states. Thus, to preserve the dynamic
programming property, we need to refine the states
by adding the boundary words into the parameter-
ization. The LM -integrated states are represented
as X[i, j, u1,..,n?1, v1,..,n?1]. Since the number of
variables involved at each DP step has increased to
3 + 4(n ? 1), the decoding algorithm is asymptoti-
cally O(|w|3+4(n?1)). Although it is possible to use
the ?hook? trick of Huang et al (2005) to factor-
ize the DP operations to reduce the complexity to
O(|w|3+3(n?1)), when n is greater than 2, the com-
plexity is still prohibitive.
3 Multi-pass LM-Integrated Decoding
In this section, we describe a multi-pass progres-
sive decoding technique that gradually augments the
LM -integrated states from lower orders to higher
orders. For instance, a bigram-integrated state
[X, i, j, u, v] is said to be a coarse-level state of a
trigram-integrate state [X, i, j, u, u?, v?, v], because
the latter state refines the previous by specifying
more inner words.
Progressive search has been used for HMM?s in
speech recognition (Murveit et al, 1993). The gen-
210
eral idea is to use a simple and fast decoding algo-
rithm to constrain the search space of a following
more complex and slower technique. More specif-
ically, a bigram decoding pass is executed forward
and backward to figure out the probability of each
state. Then the states can be pruned based on their
global score using the product of inside and outside
probabilities. The advanced decoding algorithm will
use the constrained space (a lattice in the case of
speech recognition) as a grammatical constraint to
help it focus on a smaller search space on which
more discriminative features are brought in.
The same idea has been applied to forests for pars-
ing. Charniak and Johnson (2005) use a PCFG to do
a pass of inside-outside parsing to reduce the state
space of a subsequent lexicalized n-best parsing al-
gorithm to produce parses that are further re-ranked
by a MaxEnt model.
We take the same view as in speech recognition
that a trigram integrated model is a finer-grained
model than bigram model and in general we can do
an n ? 1-gram decoding as a predicative pass for
the following n-gram pass. We need to do inside-
outside parsing as coarse-to-fine parsers do. How-
ever, we use the outside probability or cost informa-
tion differently. We do not combine the inside and
outside costs of a simpler model to prune the space
for a more complex model. Instead, for a given finer-
gained state, we combine its true inside cost with
the outside cost of its coarse-level counter-part to
estimate its worthiness of being explored. The use
of the outside cost from a coarser-level as the out-
side estimate makes our method naturally fall in the
framework of A* parsing.
Klein and Manning (2003) describe an A* pars-
ing framework for monolingual parsing and admis-
sible outside estimates that are computed using in-
side/outside parsing algorithm on simplified PCFGs
compared to the original PCFG. Zhang and Gildea
(2006) describe A* for ITG and develop admissible
heuristics for both alignment and decoding. Both
have shown the effectiveness of A* in situations
where the outside estimate approximates the true
cost closely such as when the sentences are short.
For decoding long sentences, it is difficult to come
up with good admissible (or inadmissible) heuris-
tics. If we can afford a bigram decoding pass, the
outside cost from a bigram model is conceivably a
very good estimate of the outside cost using a tri-
gram model since a bigram language model and a
trigram language model must be strongly correlated.
Although we lose the guarantee that the bigram-pass
outside estimate is admissible, we expect that it ap-
proximates the outside cost very closely, thus very
likely to effectively guide the heuristic search.
3.1 Inside-outside Coarse Level Decoding
We describe the coarse level decoding pass in this
section. The decoding algorithms for the coarse
level and the fine level do not necessarily have to
be the same. The fine level decoding algorithm is an
A* algorithm. The coarse level decoding algorithm
can be CKY or A* or other alternatives.
Conceptually, the algorithm is finding the short-
est hyperpath in the hypergraph in which the nodes
are states like X[i, j, u1,..,n?1, v1,..,n?1], and the hy-
peredges are the applications of the synchronous
rules to go from right-hand side states to left-hand
side states. The root of the hypergraph is a special
node S?[0, |w|, ?s?, ?/s?] which means the entire in-
put sentence has been translated to a string starting
with the beginning-of-sentence symbol and ending
at the end-of-sentence symbol. If we imagine a start-
ing node that goes to all possible basic translation
pairs, i.e., the instances of the terminal translation
rules for the input, we are searching the shortest hy-
per path from the imaginary bottom node to the root.
To help our outside parsing pass, we store the back-
pointers at each step of exploration.
The outside parsing pass, however, starts from the
root S?[|w|, ?s?, ?/s?] and follows the back-pointers
downward to the bottom nodes. The nodes need to
be visited in a topological order so that whenever
a node is visited, its parents have been visited and
its outside cost is over all possible outside parses.
The algorithm is described in pseudocode in Algo-
rithm 1. The number of hyperedges to traverse is
much fewer than in the inside pass because not ev-
ery state explored in the bottom up inside pass can
finally reach the goal. As for normal outside parsing,
the operations are the reverse of inside parsing. We
propagate the outside cost of the parent to its chil-
dren by combining with the inside cost of the other
children and the interaction cost, i.e., the language
model cost between the focused child and the other
children. Since we want to approximate the Viterbi
211
outside cost, it makes sense to maximize over all
possible outside costs for a given node, to be con-
sistent with the maximization of the inside pass. For
the nodes that have been explored in the bottom up
pass but not in the top-down pass, we set their out-
side cost to be infinity so that their exploration is
preferred only when the viable nodes from the first
pass have all been explored in the fine pass.
3.2 Heuristics for Fine-grained Decoding
In this section, we summarize the heuristics for finer
level decoding.
The motivation for combining the true inside
cost of the fine-grained model and the outside es-
timate given by the coarse-level parsing is to ap-
proximate the true global cost of a fine-grained state
as closely as possible. We can make the approx-
imation even closer by incorporating local higher-
order outside n-gram information for a state of
X[i, j, u1,..,n?1, v1,..,n?1] into account. We call this
the best-border estimate. For example, the best-
border estimate for trigram states is:
hBB(X, i, j, u1, u2, v1, v2)
=
[
max
s?S(i,j)
Plm(u2 | s, u1)
]
?
[
max
s?S(i,j)
Plm(s | v1, v2)
]
where S(i, j) is the set of candidate target language
words outside the span of (i, j). hBB is the prod-
uct of the upper bounds for the two on-the-border
n-grams.
This heuristic function was one of the admissible
heuristics used by Zhang and Gildea (2006). The
benefit of including the best-border estimate is to re-
fine the outside estimate with respect to the inner
words which refine the bigram states into the trigram
states. If we do not take the inner words into consid-
eration when computing the outside cost, all states
that map to the same coarse level state would have
the same outside cost. When the simple best-border
estimate is combined with the coarse-level outside
estimate, it can further boost the search as will be
shown in the experiments. To summarize, our recipe
for faster decoding is that using
?(X[i, j, u1,..,n?1, v1,..,n?1])
+ ?(X[i, j, u1, vn?1])
+ hBB(X, i, j, u1,...,n, v1,...,n) (1)
where ? is the Viterbi inside cost and ? is the Viterbi
outside cost, to globally prioritize the n-gram inte-
grated states on the agenda for exploration.
3.3 Alternative Efficient Decoding Algorithms
The complexity of n-gram integrated decoding for
SCFG has been tackled using other methods.
The hook trick of Huang et al (2005) factor-
izes the dynamic programming steps and lowers the
asymptotic complexity of the n-gram integrated de-
coding, but has not been implemented in large-scale
systems where massive pruning is present.
The cube-pruning by Chiang (2007) and the lazy
cube-pruning of Huang and Chiang (2007) turn the
computation of beam pruning of CYK decoders into
a top-k selection problem given two columns of
translation hypotheses that need to be combined.
The insight for doing the expansion top-down lazily
is that there is no need to uniformly explore every
cell. The algorithm starts with requesting the first
best hypothesis from the root. The request translates
into requests for the k-bests of some of its children
and grandchildren and so on, because re-ranking at
each node is needed to get the top ones.
Venugopal et al (2007) also take a two-pass de-
coding approach, with the first pass leaving the lan-
guage model boundary words out of the dynamic
programming state, such that only one hypothesis is
retained for each span and grammar symbol.
4 Decoding to Maximize BLEU
The ultimate goal of efficient decoding to find the
translation that has a highest evaluation score using
the least time possible. Section 3 talks about utiliz-
ing the outside cost of a lower-order model to esti-
mate the outside cost of a higher-order model, boost-
ing the search for the higher-order model. By doing
so, we hope the intrinsic metric of our model agrees
with the extrinsic metric of evaluation so that fast
search for the model is equivalent to efficient decod-
ing. But the mismatch between the two is evident,
as we will see in the experiments. In this section,
212
Algorithm 1 OutsideCoarseParsing()
for all X[i, j, u, v] in topological order do
for all children pairs pointed to by the back-pointers do
if X ? [Y Z] then
 the two children are Y [i, k, u, u?] and Z[k, j, v?, v]
?(Y [i, k, u, u?]) = max {?(Y [i, k, u, u?]),
?(X[i, j, u, v]) + ?(Z[k, j, v?, v]) + rule(X ? [Y Z]) + bigram(u?, v?)}
?(Z[k, j, v?, v]) = max {?(Z[k, j, v?, v]),
?(X[i, j, u, v]) + ?(Y [i, k, u, u?]) + rule(X ? [Y Z]) + bigram(u?, v?)}
end if
if X ? ?Y Z? then
 the two children are Y [i, k, v?, v] and Z[k, j, u, u?]
?(Y [i, k, v?, v]) = max {?(Y [i, k, v?, v]),
?(X[i, j, u, v]) + ?(Z[k, j, u, u?]) + rule(X ? ?Y Z?) + bigram(u?, v?)}
?(Z[k, j, u, u?]) = max {?(Z[k, j, u, u?]),
?(X[i, j, u, v]) + ?(Y [i, k, v?, v]) + rule(X ? ?Y Z?) + bigram(u?, v?)}
end if
end for
end for
we deal with the mismatch by introducing another
decoding pass that maximizes the expected count
of synchronous constituents in the tree correspond-
ing to the translation returned. BLEU is based on
n-gram precision, and since each synchronous con-
stituent in the tree adds a new 4-gram to the trans-
lation at the point where its children are concate-
nated, the additional pass approximately maximizes
BLEU.
Kumar and Byrne (2004) proposed the framework
of Minimum Bayesian Risk (MBR) decoding that
minimizes the expected loss given a loss function.
Their MBR decoding is a reranking pass over an n-
best list of translations returned by the decoder. Our
algorithm is another dynamic programming decod-
ing pass on the trigram forest, and is similar to the
parsing algorithm for maximizing expected labelled
recall presented by Goodman (1996).
4.1 Maximizing the expected count of correct
synchronous constituents
We introduce an algorithm that maximizes the ex-
pected count of correct synchronous constituents.
Given a synchronous constituent specified by the
state [X, i, j, u, u?, v?, v], its probability of being cor-
rect in the model is
EC([X, i, j, u, u?, v?, v])
= ?([X, i, j, u, u?, v?, v]) ? ?([X, i, j, u, u?, v?, v]),
where ? is the outside probability and ? is the in-
side probability. We approximate ? and ? using the
Viterbi probabilities. Since decoding from bottom
up in the trigram pass already gives us the inside
Viterbi scores, we only have to visit the nodes in
the reverse order once we reach the root to compute
the Viterbi outside scores. The outside-pass Algo-
rithm 1 for bigram decoding can be generalized to
the trigram case. We want to maximize over all
translations (synchronous trees) T in the forest af-
ter the trigram decoding pass according to
max
T
?
[X,i,j,u,u?,v?,v]?T
EC([X, i, j, u, u?, v?, v]).
The expression can be factorized and computed us-
ing dynamic programming on the forest.
5 Experiments
We did our decoding experiments on the LDC 2002
MT evaluation data set for translation of Chinese
newswire sentences into English. The evaluation
data set has 10 human translation references for each
sentence. There are a total of 371 Chinese sentences
of no more than 20 words in the data set. These
sentences are the test set for our different versions
of language-model-integrated ITG decoders. We
evaluate the translation results by comparing them
against the reference translations using the BLEU
metric.
213
The word-to-word translation probabilities are
from the translation model of IBM Model 4 trained
on a 160-million-word English-Chinese parallel cor-
pus using GIZA++. The phrase-to-phrase transla-
tion probabilities are trained on 833K parallel sen-
tences. 758K of this was data made available by
ISI, and another 75K was FBIS data. The language
model is trained on a 30-million-word English cor-
pus. The rule probabilities for ITG are trained using
EM on a corpus of 18,773 sentence pairs with a to-
tal of 276,113 Chinese words and 315,415 English
words.
5.1 Bigram-pass Outside Cost as Trigram-pass
Outside Estimate
We first fix the beam for the bigram pass, and change
the outside heuristics for the trigram pass to show
the difference before and after using the first-pass
outside cost estimate and the border estimate. We
choose the beam size for the CYK bigram pass to be
10 on the log scale. The first row of Table 1 shows
the number of explored hyperedges for the bigram
pass and its BLEU score. In the rows below, we
compare the additional numbers of hyperedges that
need to be explored in the trigram pass using differ-
ent outside heuristics. It takes too long to finish us-
ing uniform outside estimate; we have to use a tight
beam to control the agenda-based exploration. Us-
ing the bigram outside cost estimate makes a huge
difference. Furthermore, using Equation 1, adding
the additional heuristics on the best trigrams that can
appear on the borders of the current hypothesis, on
average we only need to explore 2700 additional hy-
peredges per sentence to boost the BLEU score from
21.77 to 23.46. The boost is so significant that over-
all the dominant part of search time is no longer the
second pass but the first bigram pass (inside pass ac-
tually) which provides a constrained space and out-
side heuristics for the second pass.
5.2 Two-pass decoding versus One-pass
decoding
By varying the beam size for the first pass, we can
plot graphs of model scores versus search time and
BLEU scores versus search time as shown in Fig-
ure 1. We use a very large beam for the second pass
due to the reason that the outside estimate for the
second pass is discriminative enough to guide the
Decoding Method Avg. Hyperedges BLEU
Bigram Pass 167K 21.77
Trigram Pass
UNI ? ?
BO + 629.7K=796.7K 23.56
BO+BB +2.7K =169.7K 23.46
Trigram One-pass,
with Beam 6401K 23.47
Table 1: Speed and BLEU scores for two-pass decoding.
UNI stands for the uniform (zero) outside estimate. BO
stands for the bigram outside cost estimate. BB stands for
the best border estimate, which is added to BO.
Decoder Time BLEU Model Score
One-pass agenda 4317s 22.25 -208.849
One-pass CYK 3793s 22.89 -207.309
Multi-pass, CYK first
agenda second pass 3689s 23.56 -205.344
MEC third pass 3749s 24.07 -203.878
Lazy-cube-pruning 3746s 22.16 -208.575
Table 2: Summary of different trigram decoding strate-
gies, using about the same time (10 seconds per sen-
tence).
search. We sum up the total number of seconds for
both passes to compare with the baseline systems.
On average, less than 5% of time is spent in the sec-
ond pass.
In Figure 1, we have four competing decoders.
bitri cyk is our two-pass decoder, using CYK as
the first pass decoding algorithm and using agenda-
based decoding in the second pass which is guided
by the first pass. agenda is our trigram-integrated
agenda-based decoder. The other two systems are
also one-pass. cyk is our trigram-integrated CYK
decoder. lazy kbest is our top-down k-best-style de-
coder.1
Figure 1(left) compares the search efficiencies of
the four systems. bitri cyk at the top ranks first. cyk
follows it. The curves of lazy kbest and agenda cross
1In our implementation of the lazy-cube-pruning based ITG
decoder, we vary the re-ranking buffer size and the the top-k
list size which are the two controlling parameters for the search
space. But we did not use any LM estimate to achieve early
stopping as suggested by Huang and Chiang (2007). Also, we
did not have a translation-model-only pruning pass. So the re-
sults shown in this paper for the lazy cube pruning method is
not of its best performance.
214
and are both below the curves of bitri cyk and cyk.
This figure indicates the advantage of the two-pass
decoding strategy in producing translations with a
high model score in less time.
However, model scores do not directly translate
into BLEU scores. In Figure 1(right), bitri cyk is
better than CYK only in a certain time window when
the beam is neither too small nor too large. But
the window is actually where we are interested ? it
ranges from 5 seconds per sentence to 20 seconds
per sentence. Table 2 summarizes the performance
of the four decoders when the decoding speed is at
10 seconds per sentence.
5.3 Does the hook trick help?
We have many choices in implementing the bigram
decoding pass. We can do either CYK or agenda-
based decoding. We can also use the dynamic pro-
gramming hook trick. We are particularly interested
in the effect of the hook trick in a large-scale system
with aggressive pruning.
Figure 2 compares the four possible combinations
of the decoding choices for the first pass: bitri cyk,
bitri agenda, bitri cyk hook and bitri agenda hook.
bitri cyk which simply uses CYK as the first pass
decoding algorithm is the best in terms of perfor-
mance and time trade-off. The hook-based de-
coders do not show an advantage in our experiments.
Only bitri agenda hook gets slightly better than bi-
tri agenda when the beam size increases. So, it is
very likely the overhead of building hooks offsets its
benefit when we massively prune the hypotheses.
5.4 Maximizing BLEU
The bitri cyk decoder spends little time in the
agenda-based trigram pass, quickly reaching the
goal item starting from the bottom of the chart. In
order to maximize BLEU score using the algorithm
described in Section 4, we need a sizable trigram
forest as a starting point. Therefore, we keep pop-
ping off more items from the agenda after the goal
is reached. Simply by exploring more (200 times
the log beam) after-goal items, we can optimize the
Viterbi synchronous parse significantly, shown in
Figure 3(left) in terms of model score versus search
time.
However, the mismatch between model score and
BLEU score persists. So, we try our algorithm
of maximizing expected count of synchronous con-
stituents on the trigram forest. We find signifi-
cant improvement in BLEU, as shown in Figure 3
(right) by the curve of bitri cyk epass me cons. bi-
tri cyk epass me cons beats both bitri cyk and cyk
in terms of BLEU versus time if using more than
1.5 seconds on average to decode each sentence. At
each time point, the difference in BLEU between
bitri cyk epass me cons and the highest of bitri cyk
and cyk is around .5 points consistently as we vary
the beam size for the first pass. We achieve the
record-high BLEU score 24.34 using on average 21
seconds per sentence, compared to the next-highest
score of 23.92 achieved by cyk using on average 78
seconds per sentence.
6 Conclusion
We present a multi-pass method to speed up n-
gram integrated decoding for SCFG. We use an in-
side/outside parsing algorithm to get the Viterbi out-
side cost of bigram integrated states which is used as
an outside estimate for trigram integrated states. The
coarse-level outside cost plus the simple estimate for
border trigrams speeds up the trigram decoding pass
hundreds of times compared to using no outside es-
timate.
Maximizing the probability of the synchronous
derivation is not equivalent to maximizing BLEU.
We use a rescoring decoding pass that maximizes the
expected count of synchronous constituents. This
technique, together with the progressive search at
previous stages, gives a decoder that produces the
highest BLEU score we have obtained on the data in
a very reasonable amount of time.
As future work, new metrics for the final pass may
be able to better approximate BLEU. As the bigram
decoding pass currently takes the bulk of the decod-
ing time, better heuristics for this phase may speed
up the system further.
Acknowledgments This work was supported by
NSF ITR-0428020 and NSF IIS-0546554.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In ACL.
215
-224
-222
-220
-218
-216
-214
-212
-210
-208
-206
-204
 10  100  1000  10000  100000
lo
g 
sc
or
e
total secs
bitri_cyk
cyk
agenda
lazy kbest
 0.17
 0.18
 0.19
 0.2
 0.21
 0.22
 0.23
 0.24
 10  100  1000  10000  100000
bl
eu
total secs
bitri_cyk
cyk
agenda
lazy kbest
Figure 1: We compare the two-pass ITG decoder with the one-pass trigram-integrated ITG decoders in terms of both
model scores vs. time (left) and BLEU scores vs. time (right). The model score here is the log probability of the
decoded parse, summing up both the translation model and the language model. We vary the beam size (for the first
pass in the case of two-pass) to search more and more thoroughly.
-222
-220
-218
-216
-214
-212
-210
-208
-206
-204
 100  1000  10000  100000
lo
g 
sc
or
e
total secs
bitri_cyk
bitri_cyk_hook
bitri_agenda
bitri_agenda_hook
 0.17
 0.18
 0.19
 0.2
 0.21
 0.22
 0.23
 0.24
 100  1000  10000  100000
bl
eu
total secs
bitri_cyk
bitri_cyk_hook
bitri_agenda
bitri_agenda_hook
Figure 2: We use different first-pass decoding algorithms, fixing the second pass to be agenda-based which is guided
by the outside cost of the first pass. Left: model score vs. time. Right: BLEU score vs. time.
-222
-220
-218
-216
-214
-212
-210
-208
-206
-204
-202
 100  1000  10000  100000
lo
g 
sc
or
e
total secs
bitri_cyk delayed-stopping
bitri_cyk
 0.17
 0.18
 0.19
 0.2
 0.21
 0.22
 0.23
 0.24
 0.25
 10  100  1000  10000  100000
bl
eu
total secs
bitri_cyk_epass_me_cons
bitri_cyk
cyk
Figure 3: Left: improving the model score by extended agenda-exploration after the goal is reached in the best-first
search. Right: maximizing BLEU by the maximizing expectation pass on the expanded forest.
216
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Conference of the Association for
Computational Linguistics (ACL-05), pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the International Conference on Computational
Linguistics/Association for Computational Linguistics
(COLING/ACL-06), pages 961?968, July.
Joshua Goodman. 1996. Parsing algorithms and metrics.
In Proceedings of the 34th Annual Conference of the
Association for Computational Linguistics (ACL-96),
pages 177?183.
Liang Huang and David Chiang. 2007. Faster algorithms
for decoding with integrated language models. In Pro-
ceedings of ACL, Prague, June.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005.
Machine translation as lexicalized parsing with hooks.
In International Workshop on Parsing Technologies
(IWPT05), Vancouver, BC.
Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: Fast exact Viterbi parse selection. In Proceed-
ings of the 2003 Meeting of the North American chap-
ter of the Association for Computational Linguistics
(NAACL-03).
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Daniel Marcu Susan Dumais and Salim Roukos,
editors, HLT-NAACL 2004: Main Proceedings, pages
169?176, Boston, Massachusetts, USA, May 2 - May
7. Association for Computational Linguistics.
Hy Murveit, John W. Butzberger, Vassilios V. Digalakis,
and Mitchel Weintraub. 1993. Large-vocabulary dic-
tation using SRI?s decipher speech recognition system:
Progressive-search techniques. In Proceedings of the
IEEE International Conference on Acoustics, Speech,
& Signal Processing (IEEE ICASSP-93), volume 2,
pages 319?322. IEEE.
Giorgio Satta and Enoch Peserico. 2005. Some com-
putational complexity results for synchronous context-
free grammars. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 803?810, Vancouver, Canada,
October.
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In NAACL07,
Rochester, NY, April.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Hao Zhang and Daniel Gildea. 2006. Efficient search for
inversion transduction grammar. In 2006 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), Sydney.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the 2006 Meeting of the
North American chapter of the Association for Com-
putational Linguistics (NAACL-06), pages 256?263.
217
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 45?48,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Bayesian Learning of a Tree Substitution Grammar
Matt Post and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
Tree substitution grammars (TSGs) of-
fer many advantages over context-free
grammars (CFGs), but are hard to learn.
Past approaches have resorted to heuris-
tics. In this paper, we learn a TSG us-
ing Gibbs sampling with a nonparamet-
ric prior to control subtree size. The
learned grammars perform significantly
better than heuristically extracted ones on
parsing accuracy.
1 Introduction
Tree substition grammars (TSGs) have potential
advantages over regular context-free grammars
(CFGs), but there is no obvious way to learn these
grammars. In particular, learning procedures are
not able to take direct advantage of manually an-
notated corpora like the Penn Treebank, which are
not marked for derivations and thus assume a stan-
dard CFG. Since different TSG derivations can
produce the same parse tree, learning procedures
must guess the derivations, the number of which is
exponential in the tree size. This compels heuristic
methods of subtree extraction, or maximum like-
lihood estimators which tend to extract large sub-
trees that overfit the training data.
These problems are common in natural lan-
guage processing tasks that search for a hid-
den segmentation. Recently, many groups have
had success using Gibbs sampling to address the
complexity issue and nonparametric priors to ad-
dress the overfitting problem (DeNero et al, 2008;
Goldwater et al, 2009). In this paper we apply
these techniques to learn a tree substitution gram-
mar, evaluate it on the Wall Street Journal parsing
task, and compare it to previous work.
2 Model
2.1 Tree substitution grammars
TSGs extend CFGs (and their probabilistic coun-
terparts, which concern us here) by allowing non-
terminals to be rewritten as subtrees of arbitrary
size. Although nonterminal rewrites are still
context-free, in practice TSGs can loosen the in-
dependence assumptions of CFGs because larger
rules capture more context. This is simpler than
the complex independence and backoff decisions
of Markovized grammars. Furthermore, subtrees
with terminal symbols can be viewed as learn-
ing dependencies among the words in the subtree,
obviating the need for the manual specification
(Magerman, 1995) or automatic inference (Chiang
and Bikel, 2002) of lexical dependencies.
Following standard notation for PCFGs, the
probability of a derivation d in the grammar is
given as
Pr(d) =
?
r?d
Pr(r)
where each r is a rule used in the derivation. Un-
der a regular CFG, each parse tree uniquely idenfi-
fies a derivation. In contrast, multiple derivations
in a TSG can produce the same parse; obtaining
the parse probability requires a summation over
all derivations that could have produced it. This
disconnect between parses and derivations com-
plicates both inference and learning. The infer-
ence (parsing) task for TSGs is NP-hard (Sima?an,
1996), and in practice the most probable parse is
approximated (1) by sampling from the derivation
forest or (2) from the top k derivations.
Grammar learning is more difficult as well.
CFGs are usually trained on treebanks, especially
the Wall Street Journal (WSJ) portion of the Penn
Treebank. Once the model is defined, relevant
45
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  2  4  6  8  10  12  14
subtree height
Figure 1: Subtree count (thousands) across heights
for the ?all subtrees? grammar () and the supe-
rior ?minimal subset? () from Bod (2001).
events can simply be counted in the training data.
In contrast, there are no treebanks annotated with
TSG derivations, and a treebank parse tree of n
nodes is ambiguous among 2n possible deriva-
tions. One solution would be to manually annotate
a treebank with TSG derivations, but in addition
to being expensive, this task requires one to know
what the grammar actually is. Part of the thinking
motivating TSGs is to let the data determine the
best set of subtrees.
One approach to grammar-learning is Data-
Oriented Parsing (DOP), whose strategy is to sim-
ply take all subtrees in the training data as the
grammar (Bod, 1993). Bod (2001) did this, ap-
proximating ?all subtrees? by extracting from the
Treebank 400K random subtrees for each subtree
height ranging from two to fourteen, and com-
pared the performance of that grammar to that
of a heuristically pruned ?minimal subset? of it.
The latter?s performance was quite good, achiev-
ing 90.8% F
1
score1 on section 23 of the WSJ.
This approach is unsatisfying in some ways,
however. Instead of heuristic extraction we would
prefer a model that explained the subtrees found
in the grammar. Furthermore, it seems unlikely
that subtrees with ten or so lexical items will be
useful on average at test time (Bod did not report
how often larger trees are used, but did report that
including subtrees with up to twelve lexical items
improved parser performance). We expect there to
be fewer large subtrees than small ones. Repeat-
ing Bod?s grammar extraction experiment, this is
indeed what we find when comparing these two
grammars (Figure 1).
In summary, we would like a principled (model-
based) means of determining from the data which
1The harmonic mean of precision and recall: F
1
=
2PR
P+R
.
set of subtrees should be added to our grammar,
and we would like to do so in a manner that prefers
smaller subtrees but permits larger ones if the data
warrants it. This type of requirement is common in
NLP tasks that require searching for a hidden seg-
mentation, and in the following sections we apply
it to learning a TSG from the Penn Treebank.
2.2 Collapsed Gibbs sampling with a DP
prior2
For an excellent introduction to collapsed Gibbs
sampling with a DP prior, we refer the reader to
Appendix A of Goldwater et al (2009), which we
follow closely here. Our training data is a set of
parse trees T that we assume was produced by an
unknown TSG g with probability Pr(T |g). Using
Bayes? rule, we can compute the probability of a
particular hypothesized grammar as
Pr(g | T ) =
Pr(T | g) Pr(g)
Pr(T )
Pr(g) is a distribution over grammars that ex-
presses our a priori preference for g. We use a set
of Dirichlet Process (DP) priors (Ferguson, 1973),
one for each nonterminal X ? N , the set of non-
terminals in the grammar. A sample from a DP
is a distribution over events in an infinite sample
space (in our case, potential subtrees in a TSG)
which takes two parameters, a base measure and a
concentration parameter:
g
X
? DP (G
X
, ?)
G
X
(t) = Pr
$
(|t|; p
$
)
?
r?t
PrMLE(r)
The base measure G
X
defines the probability of a
subtree t as the product of the PCFG rules r ? t
that constitute it and a geometric distribution Pr
$
over the number of those rules, thus encoding a
preference for smaller subtrees.3 The parameter ?
contributes to the probability that previously un-
seen subtrees will be sampled. All DPs share pa-
rameters p
$
and ?. An entire grammar is then
given as g = {g
X
: X ? N}. We emphasize that
no head information is used by the sampler.
Rather than explicitly consider each segmen-
tation of the parse trees (which would define a
TSG and its associated parameters), we use a col-
lapsed Gibbs sampler to integrate over all possi-
2Cohn et al (2009) and O?Donnell et al (2009) indepen-
dently developed similar models.
3
G
X
(t) = 0 unless root(t) = X .
46
S1
NP
NN
ADVP
RB VBZ S2
NP
PRP
you
VP
VB
quit
Someone always makes
VP
Figure 2: Depiction of sub(S
2
) and sub(S
2
).
Highlighted subtrees correspond with our spinal
extraction heuristic (?3). Circles denote nodes
whose flag=1.
ble grammars and sample directly from the poste-
rior. This is based on the Chinese Restaurant Pro-
cess (CRP) representation of the DP. The Gibbs
sampler is an iterative procedure. At initialization,
each parse tree in the corpus is annotated with a
specific derivation by marking each node in the
tree with a binary flag. This flag indicates whether
the subtree rooted at that node (a height one CFG
rule, at minimum) is part of the subtree contain-
ing its parent. The Gibbs sampler considers ev-
ery non-terminal, non-root node c of each parse
tree in turn, freezing the rest of the training data
and randomly choosing whether to join the sub-
trees above c and rooted at c (outcome h
1
) or to
split them (outcome h
2
) according to the probabil-
ity ratio ?(h
1
)/(?(h
1
) + ?(h
2
)), where ? assigns
a probability to each of the outcomes (Figure 2).
Let sub(n) denote the subtree above and includ-
ing node n and sub(n) the subtree rooted at n; ? is
a binary operator that forms a single subtree from
two adjacent ones. The outcome probabilities are:
?(h
1
) = ?(t)
?(h
2
) = ?(sub(c)) ? ?(sub(c))
where t = sub(c) ? sub(c). Under the CRP, the
subtree probability ?(t) is a function of the current
state of the rest of the training corpus, the appro-
priate base measure G
root(t)
, and the concentra-
tion parameter ?:
?(t) =
count
z
t
(t) + ?G
root(t)
(t)
|z
t
| + ?
where z
t
is the multiset of subtrees in the frozen
portion of the training corpus sharing the same
root as t, and count
z
t
(t) is the count of subtree
t among them.
3 Experiments
3.1 Setup
We used the standard split for the Wall Street Jour-
nal portion of the Treebank, training on sections 2
to 21, and reporting results on sentences with no
more than forty words from section 23.
We compare with three other grammars.
? A standard Treebank PCFG.
? A ?spinal? TSG, produced by extracting n
lexicalized subtrees from each length n sen-
tence in the training data. Each subtree is de-
fined as the sequence of CFG rules from leaf
upward all sharing a head, according to the
Magerman head-selection rules. We detach
the top-level unary rule, and add in counts
from the Treebank CFG rules.
? An in-house version of the heuristic ?mini-
mal subset? grammar of Bod (2001).4
We note two differences in our work that ex-
plain the large difference in scores for the minimal
grammar from those reported by Bod: (1) we did
not implement the smoothed ?mismatch parsing?,
which permits lexical leaves of subtrees to act as
wildcards, and (2) we approximate the most prob-
able parse with the top single derivation instead of
the top 1,000.
Rule probabilities for all grammars were set
with relative frequency. The Gibbs sampler was
initialized with the spinal grammar derivations.
We construct sampled grammars in two ways: by
summing all subtree counts from the derivation
states of the first i sampling iterations together
with counts from the Treebank CFG rules (de-
noted (?, p
$
,?i)), and by taking the counts only
from iteration i (denoted (?, p
$
, i)).
Our standard CKY parser and Gibbs sampler
were both written in Perl. TSG subtrees were flat-
tened to CFG rules and reconstructed afterward,
with identical mappings favoring the most proba-
ble rule. For pruning, we binned nonterminals ac-
cording to input span and degree of binarization,
keeping the ten highest scoring items in each bin.
3.2 Results
Table 1 contains parser scores. The spinal TSG
outperforms a standard unlexicalized PCFG and
4All rules of height one, plus 400K subtrees sampled at
each height h, 2 ? h ? 14, minus unlexicalized subtrees of
h > 6 and lexicalized subtrees with more than twelve words.
47
grammar size LP LR F
1
PCFG 46K 75.37 70.05 72.61
spinal 190K 80.30 78.10 79.18
minimal subset 2.56M 76.40 78.29 77.33
(10, 0.7, 100) 62K 81.48 81.03 81.25
(10, 0.8, 100) 61K 81.23 80.79 81.00
(10, 0.9, 100) 61K 82.07 81.17 81.61
(100, 0.7, 100) 64K 81.23 80.98 81.10
(100, 0.8, 100) 63K 82.13 81.36 81.74
(100, 0.9, 100) 62K 82.11 81.20 81.65
(100, 0.7,?100) 798K 82.38 82.27 82.32
(100, 0.8,?100) 506K 82.27 81.95 82.10
(100, 0.9,?100) 290K 82.64 82.09 82.36
(100, 0.7, 500) 61K 81.95 81.76 81.85
(100, 0.8, 500) 60K 82.73 82.21 82.46
(100, 0.9, 500) 59K 82.57 81.53 82.04
(100, 0.7,?500) 2.05M 82.81 82.01 82.40
(100, 0.8,?500) 1.13M 83.06 82.10 82.57
(100, 0.9,?500) 528K 83.17 81.91 82.53
Table 1: Labeled precision, recall, and F
1
on
WSJ?23.
the significantly larger ?minimal subset? grammar.
The sampled grammars outperform all of them.
Nearly all of the rules of the best single iteration
sampled grammar (100, 0.8, 500) are lexicalized
(50,820 of 60,633), and almost half of them have
a height greater than one (27,328). Constructing
sampled grammars by summing across iterations
improved over this in all cases, but at the expense
of a much larger grammar.
Figure 3 shows a histogram of subtree size taken
from the counts of the subtrees (by token, not type)
actually used in parsing WSJ?23. Parsing with
the ?minimal subset? grammar uses highly lexi-
calized subtrees, but they do not improve accuracy.
We examined sentence-level F
1
scores and found
that the use of larger subtrees did correlate with
accuracy; however, the low overall accuracy (and
the fact that there are so many of these large sub-
trees available in the grammar) suggests that such
rules are overfit. In contrast, the histogram of sub-
tree sizes used in parsing with the sampled gram-
mar matches the shape of the histogram from the
grammar itself. Gibbs sampling with a DP prior
chooses smaller but more general rules.
4 Summary
Collapsed Gibbs sampling with a DP prior fits
nicely with the task of learning a TSG. The sam-
pled grammars are model-based, are simple to
specify and extract, and take the expected shape
100
101
102
103
104
105
106
 0  2  4  6  8  10  12
number of words in subtree?s frontier
(100,0.8,500), actual grammar
(100,0.8,500), used parsing WSJ23
minimal, actual grammar
minimal, used parsing WSJ23
Figure 3: Histogram of subtrees sizes used in pars-
ing WSJ?23 (filled points), as well as from the
grammars themselves (outlined points).
over subtree size. They substantially outperform
heuristically extracted grammars from previous
work as well as our novel spinal grammar, and can
do so with many fewer rules.
Acknowledgments This work was supported by
NSF grants IIS-0546554 and ITR-0428020.
References
Rens Bod. 1993. Using an annotated corpus as a
stochastic grammar. In Proc. ACL.
Rens Bod. 2001. What is the minimal set of fragments
that achieves maximal parse accuracy. In Proc. ACL.
David Chiang and Daniel M. Bikel. 2002. Recovering
latent information in treebanks. In COLING.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In Proc. NAACL.
John DeNero, Alexandre Bouchard-Co?te?, and Dan
Klein. 2008. Sampling alignment structure under
a Bayesian translation model. In EMNLP.
Thomas S. Ferguson. 1973. A Bayesian analysis of
some nonparametric problems. Annals of Mathe-
matical Statistics, 1(2):209?230.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proc. ACL.
T.J. O?Donnell, N.D. Goodman, J. Snedeker, and J.B.
Tenenbaum. 2009. Computation and reuse in lan-
guage. In Proc. Cognitive Science Society.
Khalil Sima?an. 1996. Computational complexity of
probabilistic disambiguation by means of tree gram-
mars. In COLING.
48
Dependencies vs. Constituents for Tree-Based Alignment
Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
Given a parallel parsed corpus, statistical tree-
to-tree alignment attempts to match nodes in
the syntactic trees for a given sentence in
two languages. We train a probabilistic tree
transduction model on a large automatically
parsed Chinese-English corpus, and evaluate re-
sults against human-annotated word level align-
ments. We find that a constituent-based model
performs better than a similar probability model
trained on the same trees converted to a depen-
dency representation.
1 Introduction
Statistical approaches to machine translation, pio-
neered by Brown et al (1990), estimate parame-
ters for a probabilistic model of word-to-word cor-
respondences and word re-orderings directly from
large corpora of parallel bilingual text. In re-
cent years, a number of syntactically motivated ap-
proaches to statistical machine translation have been
proposed. These approaches assign a parallel tree
structure to the two sides of each sentence pair, and
model the translation process with reordering oper-
ations defined on the tree structure. The tree-based
approach allows us to represent the fact that syn-
tactic constituents tend to move as unit, as well as
systematic differences in word order in the gram-
mars of the two languages. Furthermore, the tree
structure allows us to make probabilistic indepen-
dence assumptions that result in polynomial time
algorithms for estimating a translation model from
parallel training data, and for finding the highest
probability translation given a new sentence.
Wu (1997) modeled the reordering process with
binary branching trees, where each production
could be either in the same or in reverse order going
from source to target language. The trees of Wu?s
Inversion Transduction Grammar were derived by
synchronously parsing a parallel corpus, using a
grammar with lexical translation probabilities at the
leaves and a simple grammar with a single nonter-
minal providing the tree structure. While this gram-
mar did not represent traditional syntactic categories
such as verb phrases and noun phrases, it served to
restrict the word-level alignments considered by the
system to those allowable by reordering operations
on binary trees.
Yamada and Knight (2001) present an algorithm
for estimating probabilistic parameters for a simi-
lar model which represents translation as a sequence
of re-ordering operations over children of nodes in
a syntactic tree, using automatic parser output for
the initial tree structures. This gives the translation
model more information about the structure of the
source language, and further constrains the reorder-
ings to match not just a possible bracketing as in Wu
(1997), but the specific bracketing of the parse tree
provided.
Recent models of alignment have attempted to
exploit syntactic information from both languages
by aligning a pair of parse trees for the same sen-
tence in either language node by node. Eisner
(2003) presented such a system for transforming
semantic-level dependecy trees into syntactic-level
dependency trees for text generation. Gildea (2003)
trained a system on parallel constituent trees from
the Korean-English Treebank, evaluating agreement
with hand-annotated word alignments. Ding and
Palmer (2004) align parallel dependency trees with
a divide and conquer strategy, choosing a highly
likely word-pair as a splitting point in each tree. In
addition to providing a deeper level of representa-
tion for the transformations of the translation model
to work with, tree-to-tree models have the advan-
tage that they are much less computationally costly
to train than models which must induce tree struc-
ture on one or both sides of the translation pair.
Because Expectation Maximization for tree-to-tree
models iterates over pairs of nodes in the two trees,
it is O(n2) in the sentence length, rather than O(n6)
for Wu?s Inversion Transduction Grammar or O(n4)
for the Yamada and Knight tree-to-string model.
In this paper, we make a comparison of two tree-
to-tree models, one trained on the trees produced by
automatic parsers for both our English and Chinese
corpora, and one trained on the same parser output
converted to a dependency representation. The trees
are converted using a set of deterministic head rules
for each language. The dependency representation
equalizes some differences in the annotation style
between the English and Chinese treebanks. How-
ever, the dependency representation makes the as-
sumption that not only the bracketing structure, but
also the head word choices, will correspond in the
two trees. Our evaluation is in terms of agreement
with word-level alignments created by bilingual hu-
man annotators. Our model of alignment is that of
Gildea (2003), reviewed in Section 2 and extended
to dependency trees in Section 3. We describe our
data and experiments in Section 4, and discuss re-
sults in Section 5.
2 The Tree-to-Tree Model
A tree-to-tree alignment model has tree transforma-
tion operations for reordering a node?s children, in-
serting and deleting nodes, and translating individ-
ual words at the leaves of the parse trees. The trans-
formed tree must not only match the surface string
of the target language, but also the tree structure as-
signed to the string by the parser. In order to pro-
vide enough flexibility to make this possible, tree
transformation operations allow a single node in the
source tree to produce two nodes in the target tree,
or two nodes in the source tree to be grouped to-
gether and produce a single node in the target tree.
The model can be thought of as a synchronous tree
substitution grammar, with probabilities parameter-
ized to generate the target tree conditioned on the
structure of the source tree.
The probability P (Tb|Ta) of transforming the
source tree Ta into target tree Tb is modeled in a
sequence of steps proceeding from the root of the
target tree down. At each level of the tree:
1. At most one of the current node?s children is
grouped with the current node in a single ele-
mentary tree, with probability Pelem(ta|?a ?
children(?a)), conditioned on the current
node ?a and its children (ie the CFG produc-
tion expanding ?a).
2. An alignment of the children of the current
elementary tree is chosen, with probability
Palign(?|?a ? children(ta)). This alignment
operation is similar to the re-order operation
in the tree-to-string model, with the extension
that 1) the alignment ? can include insertions
and deletions of individual children, as nodes
in either the source or target may not corre-
spond to anything on the other side, and 2) in
the case where two nodes have been grouped
into ta, their children are re-ordered together
in one step.
In the final step of the process, as in the tree-to-
string model, lexical items at the leaves of the tree
are translated into the target language according to
a distribution Pt(f |e).
Allowing non-1-to-1 correspondences between
nodes in the two trees is necessary to handle the
fact that the depth of corresponding words in the
two trees often differs. A further consequence of
allowing elementary trees of size one or two is that
some reorderings not allowed when reordering the
children of each individual node separately are now
possible. For example, with our simple tree
A
B
X Y
Z
if nodes A and B are considered as one elementary
tree, with probability Pelem(ta|A ? BZ), their col-
lective children will be reordered with probability
Palign({(1, 1)(2, 3)(3, 2)}|A ? XYZ)
A
X Z Y
giving the desired word ordering XZY. However,
computational complexity as well as data sparsity
prevent us from considering arbitrarily large ele-
mentary trees, and the number of nodes considered
at once still limits the possible alignments. For ex-
ample, with our maximum of two nodes, no trans-
formation of the tree
A
B
W X
C
Y Z
is capable of generating the alignment WYXZ.
In order to generate the complete target tree, one
more step is necessary to choose the structure on the
target side, specifically whether the elementary tree
has one or two nodes, what labels the nodes have,
and, if there are two nodes, whether each child at-
taches to the first or the second. Because we are
Operation Parameterization
elementary tree grouping Pelem(ta|?a ? children(?a))
re-order Palign(?|?a ? children(ta))
insertion ? can include ?insertion? symbol
lexical translation Pt(f |e)
cloning Pmakeclone(?)
? can include ?clone? symbol
Table 1: The probabilistic tree-to-tree model
ultimately interested in predicting the correct target
string, regardless of its structure, we do not assign
probabilities to these steps. The nonterminals on the
target side are ignored entirely, and while the align-
ment algorithm considers possible pairs of nodes as
elementary trees on the target side during training,
the generative probability model should be thought
of as only generating single nodes on the target side.
Thus, the alignment algorithm is constrained by the
bracketing on the target side, but does not generate
the entire target tree structure.
While the probability model for tree transforma-
tion operates from the top of the tree down, proba-
bility estimation for aligning two trees takes place
by iterating through pairs of nodes from each tree in
bottom-up order, as sketched below:
for all nodes ?a in source tree Ta in bottom-up order
do
for all elementary trees ta rooted in ?a do
for all nodes ?b in target tree Tb in bottom-up or-
der do
for all elementary trees tb rooted in ?b do
for all alignments ? of the children of ta and
tb do
?(?a, ?b) +=
Pelem(ta|?a)Palign(?|?i)
?
(i,j)?? ?(?i, ?j)
end for
end for
end for
end for
end for
The outer two loops, iterating over nodes in each
tree, require O(|T |2). Because we restrict our ele-
mentary trees to include at most one child of the root
node on either side, choosing elementary trees for a
node pair is O(m2), where m refers to the maxi-
mum number of children of a node. Computing the
alignment between the 2m children of the elemen-
tary tree on either side requires choosing which sub-
set of source nodes to delete, O(22m), which sub-
set of target nodes to insert (or clone), O(22m), and
how to reorder the remaining nodes from source to
target tree, O((2m)!). Thus overall complexity of
the algorithm is O(|T |2m242m(2m)!), quadratic in
the size of the input sentences, but exponential in
the fan-out of the grammar.
2.1 Clone Operation
Both our constituent and dependency models make
use of the ?clone? operation introduced by Gildea
(2003), which allows words to be aligned even
in cases of radically mismatched trees, at a cost
in the probability of the alignment. Allowing m-
to-n matching of up to two nodes on either side
of the parallel treebank allows for limited non-
isomorphism between the trees. However, even
given this flexibility, requiring alignments to match
two input trees rather than one often makes tree-to-
tree alignment more constrained than tree-to-string
alignment. For example, even alignments with no
change in word order may not be possible if the
structures of the two trees are radically mismatched.
Thus, it is helpful to allow departures from the con-
straints of the parallel bracketing, if it can be done
in without dramatically increasing computational
complexity.
The clone operation allows a copy of a node from
the source tree to be made anywhere in the target
tree. After the clone operation takes place, the trans-
formation of source into target tree takes place using
the tree decomposition and subtree alignment oper-
ations as before. The basic algorithm of the previ-
ous section remains unchanged, with the exception
that the alignments ? between children of two ele-
mentary trees can now include cloned, as well as in-
serted, nodes on the target side. Given that ? speci-
fies a new cloned node as a child of ?j , the choice of
which node to clone is made as in the tree-to-string
model:
Pclone(?i|clone ? ?) =
Pmakeclone(?i)
?
k Pmakeclone(?k)
Because a node from the source tree is cloned with
equal probability regardless of whether it has al-
ready been ?used? or not, the probability of a clone
operation can be computed under the same dynamic
programming assumptions as the basic tree-to-tree
model. As with the tree-to-string cloning operation,
this independence assumption is essential to keep
the complexity polynomial in the size of the input
sentences.
3 Dependency Tree-to-Tree Alignments
Dependencies were found to be more consistent
than constituent structure between French and En-
glish by Fox (2002), though this study used a tree
representation on the English side only. We wish to
investigate whether dependency trees are also more
suited to tree-to-tree alignment.
Figure 1 shows a typical Xinhua newswire sen-
tence with the Chinese parser output, and the sen-
tence?s English translation with its parse tree. The
conversion to dependency representation is shown
below the original parse trees.
Examination of the trees shows both cases where
the dependency representation is more similar
across the two languages, as well as its potential
pitfalls. The initial noun phrase, ?14 Chinese open
border cities? has two subphrases with a level of
constituent structure (the QP and the lower NP)
not found in the English parse. In this case, the
difference in constituent structure derives primar-
ily from differences in the annotation style between
the original English and Chinese treebanks (Marcus
et al, 1993; Xue and Xia, 2000; Levy and Man-
ning, 2003). These differences disappear in the con-
stituent representation. In general, the number of
levels of constituent structure in a tree can be rela-
tively arbitrary, while it is easier for people (whether
professional syntacticians or not) to agree on the
word-to-word dependencies.
In some cases, differences in the number of level
may be handled by the tree-to-tree model, for ex-
ample by grouping the subject NP and its base NP
child together as a single elementary tree. How-
ever, this introduces unnecessary variability into the
alignment process. In cases with large difference
in the depths of the two trees, the aligner may not
be able to align the corresponding terminal nodes
because only one merge is possible at each level.
In this case the aligner will clone the subtree, at an
even greater cost in probability.
The rest of our example sentence, however,
shows cases where the conversion to dependency
structure can in some cases exacerbate differences
in constituent structure. For example, jingji and
jianshe are sisters in the original constituent struc-
ture, as are their English translations, economic and
construction. In the conversion to Chinese depen-
dency structure, they remain sisters both dependent
on the noun chengjiu (achievements) while in En-
glish, economic is a child of construction. The
correspondence of a three-noun compound in Chi-
nese to a noun modified by prepositional phrase
and an adjective-noun relation in English means that
the conversion rules select different heads even for
pieces of tree that are locally similar.
3.1 The Dependency Alignment Model
While the basic tree-to-tree alignment algorithm is
the same for dependency trees, a few modifications
to the probability model are necessary.
First, the lexical translation operation takes place
at each node in the tree, rather than only at the
leaves. Lexical translation probabilities are main-
tained for each word pair as before, and the lexical
translation probabilities are included in the align-
ment cost for each elementary tree. When both el-
ementary trees contain two words, either alignment
is possible between the two. The direct alignment
between nodes within the elementary tree has prob-
ability 1?Pswap. A new parameter Pswap gives the
probability of the upper node in the elementary tree
in English corresponding to the lower node in Chi-
nese, and vice versa. Thus, the probability for the
following transformation:
A
B
X Y
? B?
A?
X Y
is factored as Pelem(AB|A?B) Pswap Pt(A?|A)
Pt(B?|B) Palign({(1, 1)(2, 2)}|A ? XY ).
Our model does not represent the position of the
head among its children. While this choice would
have to be made in generating MT output, for the
purposes of alignment we simply score how many
tree nodes are correctly aligned, without flattening
our trees into a string.
We further extended the tree-to-tree alignment al-
gorithm by conditioning the reordering of a node?s
children on the node?s lexical item as well as its syn-
tactic category at the categories of its children. The
lexicalized reordering probabilities were smoothed
with the nonlexicalized probabilities (which are
themselves smoothed with a uniform distribution).
We smooth using a linear interpolation of lexical-
ized and unlexicalized probabilities, with weights
proportional to the number of observations for each
type of event.
4 Experiments
We trained our translation models on a parallel
corpus of Chinese-English newswire text. We re-
IP
NP
NP
NR
Zhongguo
QP
CD
shisi
CLP
M
ge
NP
NN
bianjing
NN
kaifang
NN
chengshi
NP
NN
jingji
NN
jianshe
NN
chengjiu
VP
VV
xianzhu
S
NP
CD
14
NNP
Chinese
JJ
open
NN
border
NNS
cities
VP
VBP
make
NP
NP
JJ
significant
NNS
achievements
PP
IN
in
NP
JJ
economic
NN
construction
VV:xianzhu
NN:chengshi
NR:Zhongguo CD:shisi
M:ge
NN:bianjing NN:kaifang
NN:chengjiu
NN:jingji NN:jianshe
VV:make
NNS:cities
CD:14 NNP:Chinese JJ:open NN:border
NNS:achievements
JJ:significant IN:in
NN:construction
JJ:economic
Figure 1: Constituent and dependency trees for a sample sentence
Alignment
Precision Recall Error Rate
IBM Model 1 .56 .42 .52
IBM Model 4 .67 .43 .47
Constituent Tree-to-Tree .51 .48 .50
Dependency Tree-to-Tree .44 .38 .60
Dependency, lexicalized reordering .41 .37 .61
Table 2: Alignment results on Chinese-English corpus. Higher precision and recall correspond to lower
alignment error rate.
stricted ourselves to sentences of no more than 25
words in either language, resulting in a training cor-
pus of 18,773 sentence pairs with a total of 276,113
Chinese words and 315,415 English words. The
Chinese data were automatically segmented into to-
kens, and English capitalization was retained. We
replace words occurring only once with an unknown
word token, resulting in a Chinese vocabulary of
23,783 words and an English vocabulary of 27,075
words. Chinese data was parsed using the parser
of Bikel (2002), and English data was parsed us-
ing Collins (1999). Our hand-aligned test data were
those used in Hwa et al (2002), and consisted of 48
sentence pairs also with less than 25 words in either
language, for a total of 788 English words and 580
Chinese words. The hand aligned data consisted of
745 individual aligned word pairs. Words could be
aligned one-to-many in either direction. This limits
the performance achievable by our models; the IBM
models allow one-to-many alignments in one direc-
tion only, while the tree-based models allow only
one-to-one alignment unless the cloning operation
is used. A separate set of 49 hand-aligned sentence
pairs was used to control overfitting in training our
models.
We evaluate our translation models in terms of
agreement with human-annotated word-level align-
ments between the sentence pairs. For scoring
the viterbi alignments of each system against gold-
standard annotated alignments, we use the align-
ment error rate (AER) of Och and Ney (2000),
which measures agreement at the level of pairs of
words:1
AER = 1 ? 2|A ? G||A| + |G|
where A is the set of word pairs aligned by the auto-
matic system, and G the set algned in the gold stan-
dard. For a better understanding of how the models
1While Och and Ney (2000) differentiate between sure and
possible hand-annotated alignments, our gold standard align-
ments come in only one variety.
differ, we break this figure down into precision:
P = |A ? G||A|
and recall:
R = |A ? G||G|
Since none of the systems presented in this com-
parison make use of hand-aligned data, they may
differ in the overall proportion of words that are
aligned, rather than inserted or deleted. This affects
the precision/recall tradeoff; better results with re-
spect to human alignments may be possible by ad-
justing an overall insertion probability in order to
optimize AER.
Table 2 provides a comparison of results using the
tree-based models with the word-level IBM models.
IBM Models 1 and 4 refer to Brown et al (1993).
We used the GIZA++ package, including the HMM
model of Och and Ney (2000). We trained each
model until AER began to increase on our held-out
cross validation data, resulting in running Model 1
for three iterations, then the HMM model for three
iterations, and finally Model 4 for two iterations
(the optimal number of iterations for Models 2 and
3 was zero). ?Constituent Tree-to-Tree? indicates
the model of Section 2 trained and tested directly
on the trees output by the parser, while ?Depen-
dency Tree-to-Tree? makes the modifications to the
model described in Section 3. For reasons of com-
putational efficiency, our constituent-based training
procedure skipped sentences for which either tree
had a node with more than five children, and the
dependency-based training skipped trees with more
than six children. Thus, the tree-based models were
effectively trained on less data than IBM Model 4:
11,422 out of 18,773 sentence pairs for the con-
stituent model and 10,662 sentence pairs for the de-
pendency model. Our tree-based models were ini-
tialized with lexical translation probabilities trained
using IBM Model 1, and uniform probabilities for
the tree reordering operations. The models were
trained until AER began to rise on our held-out
cross-validation data, though in practice AER was
nearly constant for both tree-based models after the
first iteration.
5 Discussion
The constituent-based version of the alignment
model significantly outperforms the dependency-
based model. The IBM models outperform the con-
stituent tree-to-tree model to a lesser degree, with
tree-to-tree achieving higher recall, and IBM higher
precision. It is particularly significant that the tree-
based model gets higher recall than the other mod-
els, since it is limited to one-to-one alignments un-
less the clone operation is used, bounding the recall
it can achieve.
In order to better understand the differences be-
tween the constituent and dependency representa-
tions of our data, we analyzed how well the two
representations match our hand annotated alignment
data. We looked for consistently aligned pairs of
constituents in the two parse trees. By consistently
aligned, we mean that all words within the English
constituent are aligned to words inside the Chinese
constituent (if they are aligned to anything), and
vice versa. In our example in Figure 1, the NP ?14
Chinese border cities? and the Chinese subject NP
?Zhongguo shisi ge bianjing kaifang chengshi? are
consistenly aligned, but the PP ?in economic con-
struction? has no consistently aligned constituent in
the Chinese sentence. We found that of the 2623
constituents in our English parse trees (not count-
ing unary consituents, which have the same bound-
aries as their children), for 1044, or 40%, there ex-
ists some constituent in the Chinese parse tree that
is consistently aligned. This confirms the results of
Fox (2002) and Galley et al (2004) that many trans-
lation operations must span more than one parse tree
node. For each of our consistently aligned pairs, we
then found the head word of both the Chinese and
English constituents according to our head rules.
The two head words correspond in the annotated
alignments 67% of the time (700 out of 1044 con-
sistently aligned constituent pairs). While the head-
swapping operation of our translation model will be
able to handle some cases of differing heads, it can
only do so if the two heads are adjacent in both tree
structures.
Our system is trained and test on automatically
generated parse trees, which may contribute to the
mismatches in the tree structures. As our test
data was taken from the Chinese Treebank, hand-
annotated parse trees were available for the Chinese,
but not the English, sentences. Running the analy-
sis on hand-annotated Chinese trees found slightly
better English/Chinese agreement overall, but there
were still disagreements in the head words choices
for a third of all consistently aligned constuent pairs.
Running our alignment system on gold standard
trees did not improve results. The comparison be-
tween parser output and gold standard trees is sum-
marized in Table 3.
We used head rules developed for statistical
parsers in both languages, but other rules may be
better suited to the alignment task. For example,
the tensed auxiliary verb is considered the head of
English progressive and perfect verb phrases, rather
than the present or past particple of the main verb.
Such auxiliaries carry agreement information rele-
vant to parsing, but generally have no counterpart in
Chinese. A semantically oriented dependency struc-
ture, such as Tree Adjoining Grammar derivation
trees, may be more appropriate for alignment.
6 Conclusion
We present a comparison of constituent and de-
pendency models for tree-to-tree alignment. De-
spite equalizing some mismatches in tree structure,
the dependency representation does not perform as
well, likely because it is less robust to large differ-
ences between the tree structures.
Acknowledgments We are very grateful to Re-
becca Hwa, Hao Zhang, everyone at the 2003 John
Hopkins speech and language summer research
workshop, and EMNLP?s reviewers for their assis-
tance, criticism, and data. This work was partially
supported by NSF ITR IIS-09325646, NSF research
infrastructure grant EIA-0080124, and NSF grant
0121285 to the summer workshop.
References
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In
Proceedings ARPA Workshop on Human Lan-
guage Technology.
Peter F. Brown, John Cocke, Stephen A. Della
Pietra, Vincent J. Della Pietra, Frederick Je-
linek, John D. Lafferty, Robert L. Mercer, and
Paul S. Roossin. 1990. A statistical approach to
machine translation. Computational Linguistics,
16(2):79?85, June.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation:
Parameter estimation. Computational Linguis-
tics, 19(2):263?311.
Michael John Collins. 1999. Head-driven Statisti-
Chinese Parse Trees
Automatic Treebank
Proportion of English constits w/ consistently aligned Chinese constit .40 .42
Proportion of above with heads words aligned .67 .66
Constituent-Based AER .50 .51
Dependency-Based AER .60 .62
Table 3: Comparison of automatically generated and hand-annotated Chinese parse trees.
cal Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania, Philadelphia.
Yuan Ding and Martha Palmer. 2004. Automatic
learning of parallel dependency treelet pairs. In
The First International Joint Conference on Nat-
ural Language Processing (IJCNLP).
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceed-
ings of the 41st Meeting of the Association for
Computational Linguistics, companion volume,
Sapporo, Japan.
Heidi J. Fox. 2002. Phrasal cohesion and statisti-
cal machine translation. In In Proceedings of the
2002 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP 2002), pages
304?311.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation
rule? In Proceedings of the Human Language
Technology Conference/North American Chapter
of the Association for Computational Linguistics
(HLT/NAACL).
Daniel Gildea. 2003. Loosely tree-based alignment
for machine translation. In Proceedings of the
41th Annual Conference of the Association for
Computational Linguistics (ACL-03), pages 80?
87, Sapporo, Japan.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and
Okan Kolak. 2002. Evaluating translational cor-
respondence using annotation projection. In Pro-
ceedings of the 40th Annual Conference of the
Association for Computational Linguistics (ACL-
02).
Roger Levy and Christopher Manning. 2003. Is
it harder to parse Chinese, or the Chinese Tree-
bank? In Proceedings of the 41th Annual Con-
ference of the Association for Computational Lin-
guistics (ACL-03), Sapporo, Japan.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The Penn
treebank. Computational Linguistics, 19(2):313?
330, June.
Franz Josef Och and Hermann Ney. 2000. Im-
proved statistical alignment models. In Proceed-
ings of the 38th Annual Conference of the Asso-
ciation for Computational Linguistics (ACL-00),
pages 440?447, Hong Kong, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?403.
Nianwen Xue and Fei Xia. 2000. The bracketing
guidelines for the penn chinese treebank. Tech-
nical Report IRCS-00-08, IRCS, University of
Pennsylvania.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceed-
ings of the 39th Annual Conference of the Asso-
ciation for Computational Linguistics (ACL-01),
Toulouse, France.
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 25?32, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Syntactic Features for Evaluation of Machine Translation
Ding Liu and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
Automatic evaluation of machine transla-
tion, based on computing n-gram similar-
ity between system output and human ref-
erence translations, has revolutionized the
development of MT systems. We explore
the use of syntactic information, includ-
ing constituent labels and head-modifier
dependencies, in computing similarity be-
tween output and reference. Our results
show that adding syntactic information
to the evaluation metric improves both
sentence-level and corpus-level correla-
tion with human judgments.
1 Introduction
Evaluation has long been a stumbling block in the
development of machine translation systems, due to
the simple fact that there are many correct transla-
tions for a given sentence. Human evaluation of sys-
tem output is costly in both time and money, leading
to the rise of automatic evaluation metrics in recent
years. The most commonly used automatic evalua-
tion metrics, BLEU (Papineni et al, 2002) and NIST
(Doddington, 2002), are based on the assumption
that ?The closer a machine translation is to a profes-
sional human translation, the better it is? (Papineni
et al, 2002). For every hypothesis, BLEU computes
the fraction of n-grams which also appear in the ref-
erence sentences, as well as a brevity penalty. NIST
uses a similar strategy to BLEU but further consid-
ers that n-grams with different frequency should be
treated differently in the evaluation. It introduces the
notion of information weights, which indicate that
rarely occurring n-grams count more than those fre-
quently occurring ones in the evaluation (Dodding-
ton, 2002). BLEU and NIST have been shown to
correlate closely with human judgments in ranking
MT systems with different qualities (Papineni et al,
2002; Doddington, 2002).
In the 2003 Johns Hopkins Workshop on Speech
and Language Engineering, experiments on MT
evaluation showed that BLEU and NIST do not cor-
relate well with human judgments at the sentence
level, even when they correlate well over large test
sets (Blatz et al, 2003). Kulesza and Shieber (2004)
use a machine learning approach to improve the cor-
relation at the sentence level. Their method, based
on the assumption that higher classification accuracy
in discriminating human- from machine-generated
translations will yield closer correlation with hu-
man judgments, uses support vector machine (SVM)
based learning to weight multiple metrics such as
BLEU, NIST, and WER (minimal word error rate).
The SVM is trained for differentiating the MT hy-
pothesis and the professional human translations,
and then the distance from the hypothesis?s metric
vector to the hyper-plane of the trained SVM is taken
as the final score for the hypothesis.
While the machine learning approach improves
correlation with human judgments, all the metrics
discussed are based on the same type of information:
n-gram subsequences of the hypothesis translations.
This type of feature cannot capture the grammatical-
ity of the sentence, in part because they do not take
into account sentence-level information. For exam-
ple, a sentence can achieve an excellent BLEU score
without containing a verb. As MT systems improve,
the shortcomings of n-gram based evaluation are be-
coming more apparent. State-of-the-art MT output
25
reference: S
NP
PRON
VP
V NP
ART N
hypothesis 1: S
NP
PRON
VP
V NP
ART N
hypothesis 2: S
NP
ART N
NP
PRON
VP
V
Figure 1: Syntax Trees of the Examples
often contains roughly the correct words and con-
cepts, but does not form a coherent sentence. Often
the intended meaning can be inferred; often it can-
not. Evidence that we are reaching the limits of n-
gram based evaluation was provided by Charniak et
al. (2003), who found that a syntax-based language
model improved the fluency and semantic accuracy
of their system, but lowered their BLEU score.
With the progress of MT research in recent years,
we are not satisfied with the getting correct words
in the translations; we also expect them to be well-
formed and more readable. This presents new chal-
lenges to MT evaluation. As discussed above, the
existing word-based metrics can not give a clear
evaluation for the hypothesis? fluency. For exam-
ple, in the BLEU metric, the overlapping fractions
of n-grams with more than one word are considered
as a kind of metric for the fluency of the hypothesis.
Consider the following simple example:
Reference: I had a dog.
Hypothesis 1: I have the dog.
Hypothesis 2: A dog I had.
If we use BLEU to evaluate the two sentences, hy-
pothesis 2 has two bigrams a dog and I had which
are also found in the reference, and hypothesis 1 has
no bigrams in common with the reference. Thus hy-
pothesis 2 will get a higher score than hypothesis 1.
The result is obviously incorrect. However, if we
evaluate their fluency based on the syntactic simi-
larity with the reference, we will get our desired re-
sults. Figure 1 shows syntactic trees for the example
sentences, from which we can see that hypothesis 1
has exactly the same syntactic structure with the ref-
erence, while hypothesis 2 has a very different one.
Thus the evaluation of fluency can be transformed as
computing the syntactic similarity of the hypothesis
and the references.
This paper develops a number of syntactically
motivated evaluation metrics computed by automat-
ically parsing both reference and hypothesis sen-
tences. Our experiments measure how well these
metrics correlate with human judgments, both for in-
dividual sentences and over a large test set translated
by MT systems of varying quality.
2 Evaluating Machine Translation with
Syntactic Features
In order to give a clear and direct evaluation for the
fluency of a sentence, syntax trees are used to gen-
erate metrics based on the similarity of the MT hy-
pothesis?s tree and those of the references. We can?t
expect that the whole syntax tree of the hypothesis
can always be found in the references, thus our ap-
proach is to be based on the fractions of the subtrees
26
which also appear in the reference syntax trees. This
idea is intuitively derived from BLEU, but with the
consideration of the sparse subtrees which lead to
zero fractions, we average the fractions in the arith-
metic mean, instead of the geometric mean used
in BLEU. Then for each hypothesis, the fractions
of subtrees with different depths are calculated and
their arithmetic mean is computed as the syntax tree
based metric, which we denote as ?subtree metric?
STM:
STM = 1
D
D
?
n=1
?
t?subtreesn(hyp) countclip(t)
?
t?subtreesn(hyp) count(t)
where D is the maximum depth of subtrees con-
sidered, count(t) denotes the number of times sub-
tree t appears in the candidate?s syntax tree, and
countclip(t) denotes the clipped number of times
t appears in the references? syntax trees. Clipped
here means that, for a given subtree, the count com-
puted from the hypothesis syntax tree can not exceed
the maximum number of times the subtree occurs in
any single reference?s syntax tree. A simple exam-
ple with one hypothesis and one reference is shown
in Figure 2. Setting the maximum depth to 3, we
go through the hypothesis syntax tree and compute
the fraction of subtrees with different depths. For
the 1-depth subtrees, we get S, NP, VP, PRON, V,
NP which also appear in the reference syntax tree.
Since PRON only occurs once in the reference, its
clipped count should be 1 rather than 2. Then we
get 6 out of 7 for the 1-depth subtrees. For the 2-
depth subtrees, we get S?NP VP, NP?PRON, and
VP?V NP which also appear in the reference syntax
tree. For the same reason, the subtree NP?PRON
can only be counted once. Then we get 3 out of 4
for the 2-depth subtree. Similarly, the fraction of
3-depth subtrees is 1 out of 2. Therefore, the final
score of STM is (6/7+3/4+1/2)/3=0.702.
While the subtree overlap metric defined above
considers only subtrees of a fixed depth, subtrees of
other configurations may be important for discrimi-
nating good hypotheses. For example, we may want
to look for the subtree:
S
NP VP
V NP
to find sentences with transitive verbs, while ignor-
ing the internal structure of the subject noun phrase.
In order to include subtrees of all configurations in
our metric, we turn to convolution kernels on our
trees. Using H(x) to denote the vector of counts of
all subtrees found in tree x, for two trees T1 and T2,the inner product H(T1) ?H(T2) counts the numberof matching pairs of subtrees of T1 and T2. Collinsand Duffy (2001) describe a method for efficiently
computing this dot product without explicitly com-
puting the vectors H , which have dimensionality ex-
ponential in the size of the original tree. In order to
derive a similarity measure ranging from zero to one,
we use the cosine of the vectors H:
cos(T1, T2) =
H(T1) ?H(T2)
|H(T1)||H(T2)|
Using the identity
|H(T1)| =
?
H(T1) ?H(T1)
we can compute the cosine similarity using the ker-
nel method, without ever computing the entire of
vector of counts H . Our kernel-based subtree metric
TKM is then defined as the maximum of the cosine
measure over the references:
TKM = max
t?ref
cos(hyp, t)
The advantage of using the tree kernel is that it
can capture the similarity of subtrees of different
shapes; the weak point is that it can only use the
reference trees one by one, while STM can use them
simultaneously. The dot product also weights indi-
vidual features differently than our other measures,
which compute overlap in the same way as does
BLEU. For example, if the same subtree occurs 10
times in both the hypothesis and the reference, this
contributes a term of 100 to the dot product, rather
than 10 in the clipped count used by BLEU and by
our subtree metric STM.
2.1 Dependency-Based Metrics
Dependency trees consist of trees of head-modifier
relations with a word at each node, rather than just
at the leaves. Dependency trees were found to corre-
spond better across translation pairs than constituent
trees by Fox (2002), and form the basis of the ma-
chine translation systems of Alshawi et al (2000)
27
reference: S
NP
PRON
VP
V NP
ART N
hypothesis: S
NP
PRON
VP
V NP
PRON
Figure 2: Examples for the Computation of STM
and Lin (2004). We derived dependency trees from
the constituent trees by applying the determinis-
tic headword extraction rules used by the parser of
Collins (1999). For the example of the reference
syntax tree in Figure 2, the whole tree with the root
S represents a sentence; and the subtree NP?ART N
represents a noun phrase. Then for every node in the
syntax tree, we can determine its headword by its
syntactic structure; from the subtree NP?ART N,
for example, the headword selection rules chose the
headword of NP to be word corresponding to the
POS N in the subtree, and the other child, which cor-
responds to ART, is the modifier for the headword.
The dependency tree then is a kind of structure con-
stituted by headwords and every subtree represents
the modifier information for its root headword. For
example, the dependency tree of the sentence I have
a red pen is shown as below.
have
I pen
a red
The dependency tree contains both the lexical and
syntactic information, which inspires us to use it for
the MT evaluation.
Noticing that in a dependent tree the child
nodes are the modifier of its parent, we propose
a dependency-tree based metric by extracting the
headwords chains from both the hypothesis and the
reference dependency trees. A headword chain is
a sequence of words which corresponds to a path
in the dependency tree. Take the dependency tree
in Figure 2 as the example, the 2-word headword
chains include have I, have pen, pen a, and pen
red. Before using the headword chains, we need
to extract them out of the dependency trees. Fig-
ure 3 gives an algorithm which recursively extracts
the headword chains in a dependency tree from short
to long. Having the headword chains, the headword
chain based metric is computed in a manner similar
to BLEU, but using n-grams of dependency chains
rather than n-grams in the linear order of the sen-
tence. For every hypothesis, the fractions of head-
word chains which also appear in the reference de-
pendency trees are averaged as the final score. Using
HWCM to denote the headword chain based metric,
it is computed as follows:
HWCM = 1
D
D
?
n=1
?
g?chainn(hyp) countclip(g)
?
g?chainn(hyp) count(g)
where D is chosen as the maximum length chain
considered.
We may also wish to consider dependency rela-
tions over more than two words that are contigu-
ous but not in a single ancestor chain in the depen-
dency tree. For this reason, the two methods de-
scribed in section 3.1 are used to compute the simi-
larity of dependency trees between the MT hypothe-
sis and its references, and the corresponding metrics
are denoted DSTM for dependency subtree metric
and DTKM for dependency tree kernel metric.
3 Experiments
Our testing data contains two parts. One part is a set
of 665 English sentences generated by a Chinese-
English MT system. And for each MT hypothesis,
three reference translations are associated with it.
28
Input: dependency tree T, maximum length N of the headword chain
Output: headword chains from length 1 to N
for i = 1 to N
for every node n in T
if i == 1
add n?s word to n?s 1 word headword chains;
else
for every direct child c of n
for every i-1 words headword chain hc of c
newchain = joint(n?s word, hc);
add newchain to the i words headword chains of n;
endfor
endfor
endif
endfor
endfor
Figure 3: Algorithm for Extracting the Headword Chains
The human judgments, on a scale of 1 to 5, were col-
lected at the 2003 Johns Hopkins Speech and Lan-
guage Summer Workshop, which tells the overall
quality of the MT hypotheses. The translations were
generated by the alignment template system of Och
(2003). This testing set is called JHU testing set
in this paper. The other set of testing data is from
MT evaluation workshop at ACL05. Three sets of
human translations (E01, E03, E04) are selected as
the references, and the outputs of seven MT systems
(E9 E11 E12 E14 E15 E17 E22) are used for testing
the performance of our syntactic metrics. Each set
of MT translations contains 929 English sentences,
each of which is associated with human judgments
for its fluency and adequacy. The fluency and ade-
quacy scores both range from 1 to 5.
3.1 Sentence-level Evaluation
Our syntactic metrics are motivated by a desire to
better capture grammaticality in MT evaluation, and
thus we are most interested in how well they cor-
relate with human judgments of sentences? fluency,
rather than the adequacy of the translation. To
do this, the syntactic metrics (computed with the
Collins (1999) parser) as well as BLEU were used
to evaluate hypotheses in the test set from ACL05
MT workshop, which provides both fluency and ad-
equacy scores for each sentence, and their Pearson
coefficients of correlation with the human fluency
scores were computed. For BLEU and HWCM, in
order to avoid assigning zero scores to individual
MaxLength/Depth BLEU HWCM STM DSTM
1 0.126 0.130 ?? ??
2 0.132 0.142 0.142 0.159
3 0.117 0.157 0.147 0.150
4 0.093 0.153 0.136 0.121
kernel 0.065 0.090
Table 1: Correlation with Human Fluency Judg-
ments for E14
sentences, when precision for n-grams of a particu-
lar length is zero we replace it with an epsilon value
of 10?3. We choose E14 and E15 as two repre-
sentative MT systems in the ACL05 MT workshop
data set, which have relatively high human scores
and low human scores respectively. The results are
shown in Table 1 and Table 2, with every metric
indexed by the maximum n-gram length or subtree
depth. The last row of the each table shows the tree-
kernel-based measures, which have no depth param-
eter to adjust, but implicitly consider all depths.
The results show that in both systems our syntac-
tic metrics all achieve a better performance in the
correlation with human judgments of fluency. We
also notice that with the increasing of the maximum
length of n-grams, the correlation of BLEU with hu-
man judgments does not necessarily increase, but
decreases in most cases. This is contrary to the argu-
ment in BLEU which says that longer n-grams bet-
ter represent the sentences? fluency than the shorter
29
MaxLength/Depth BLEU HWCM STM DSTM
1 0.122 0.128 ?? ??
2 0.094 0.120 0.134 0.137
3 0.073 0.119 0.144 0.124
4 0.048 0.113 0.143 0.121
kernel 0.089 0.066
Table 2: Correlation with Human Fluency Judg-
ments for E15
ones. The problem can be explained by the limi-
tation of the reference translations. In our exper-
iments, every hypothesis is evaluated by referring
to three human translations. Since the three human
translations can only cover a small set of possible
translations, with the increasing of n-gram length,
more and more correct n-grams might not be found
in the references, so that the fraction of longer n-
grams turns to be less reliable than the short ones
and hurts the final scores. In the the corpus-level
evaluation of a MT system, the sparse data problem
will be less serious than in the sentence-level evalu-
ation, since the overlapping n-grams of all the sen-
tences and their references will be summed up. So
in the traditional BLEU algorithm used for corpus-
level evaluation, a maximum n-gram of length 4 or 5
is usually used. A similar trend can be found in syn-
tax tree and dependency tree based metrics, but the
decreasing ratios are much lower than BLEU, which
indicates that the syntactic metrics are less affected
by the sparse data problem. The poor performance
of tree-kernel based metrics also confirms our argu-
ments on the sparse data problem, since the kernel
measures implicitly consider the overlapping ratios
of the sub-trees of all shapes, and thus will be very
much affected by the sparse data problem.
Though our syntactic metrics are proposed for
evaluating the sentences? fluency, we are curious
how well they do in the overall evaluation of sen-
tences. Thus we also computed each metric?s cor-
relation with human overall judgments in E14, E15
and JHU testing set. The overall human score for
each sentence in E14 and E15 is computed by sum-
ming up its fluency score and adequacy score. The
results are shown in Table 3, Table 4, and Table
5. We can see that the syntactic metrics achieve
MaxLength/Depth BLEU HWCM STM DSTM
1 0.176 0.191 ?? ??
2 0.185 0.195 0.171 0.193
3 0.169 0.202 0.168 0.175
4 0.137 0.199 0.158 0.143
kernel 0.093 0.127
Table 3: Correlation with Human Overall Judgments
for E14
MaxLength/Depth BLEU HWCM STM DSTM
1 0.146 0.152 ?? ??
2 0.124 0.142 0.148 0.152
3 0.095 0.144 0.151 0.139
4 0.067 0.137 0.144 0.137
kernel 0.098 0.084
Table 4: Correlation with Human Overall Judgments
for E15
competitive correlations in the test, among which
HWCM, based on headword chains, gives better
performances in evaluation of E14 and E15, and a
slightly worse performance in JHU testing set than
BLEU. Just as with the fluency evaluation, HWCM
and other syntactic metrics present more stable per-
formance as the n-gram?s length (subtree?s depth)
increases.
3.2 Corpus-level Evaluation
While sentence-level evaluation is useful if we are
interested in a confidence measure on MT outputs,
corpus level evaluation is more useful for comparing
MaxLength/Depth BLEU HWCM STM DSTM
1 0.536 0.502 ?? ??
2 0.562 0.555 0.515 0.513
3 0.513 0.538 0.529 0.477
4 0.453 0.510 0.497 0.450
kernel 0.461 0.413
Table 5: Correlation with Human Overall Judgments
for JHU Testing Set
30
MaxLength/Depth BLEU HWCM STM DSTM
1 0.629 0.723 ?? ??
2 0.683 0.757 0.538 0.780
3 0.724 0.774 0.597 0.780
4 0.753 0.778 0.612 0.788
5 0.781 0.780 0.618 0.778
6 0.763 0.778 0.618 0.782
kernel 0.539 0.875
Table 6: Corpus-level Correlation with Human
Overall Judgments (E9 E11 E12 E14 E15 E17 E22)
MT systems and guiding their development. Does
higher sentence-level correlation necessarily indi-
cate higher correlation in corpus-level evaluation?
To answer this question, we used our syntactic met-
rics and BLEU to evaluate all the human-scored MT
systems (E9 E11 E12 E14 E15 E17 E22) in the
ACL05 MT workshop test set, and computed the
correlation with human overall judgments. The hu-
man judgments for an MT system are estimated by
summing up each sentence?s human overall score.
Table 6 shows the results indexed by different n-
grams and tree depths.
We can see that the corpus-level correlation and
the sentence-level correlation don?t always corre-
spond. For example, the kernel dependency subtree
metric achieves a very good performance in corpus-
level evaluation, but it has a poor performance in
sentence-level evaluation. Sentence-level correla-
tion reflects the relative qualities of different hy-
potheses in a MT system, which does not indicate
any information for the relative qualities of differ-
ent systems. If we uniformly decrease or increase
every hypothesis?s automatic score in a MT sys-
tem, the sentence-level correlation with human judg-
ments will remain the same, but the corpus-level cor-
relation will be changed. So we might possibly get
inconsistent corpus-level and sentence-level correla-
tions.
From the results, we can see that with the increase
of n-grams length, the performance of BLEU and
HWCM will first increase up to length 5, and then
starts decreasing, where the optimal n-gram length
of 5 corresponds to our usual setting for BLEU algo-
rithm. This shows that corpus-level evaluation, com-
pared with the sentence-level evaluation, is much
less sensitive to the sparse data problem and thus
leaves more space for making use of comprehen-
sive evaluation metrics. We speculate this is why the
kernel dependency subtree metric achieves the best
performance among all the metrics. We can also see
that HWCM and DSTM beat BLEU in most cases
and exhibit more stable performance.
An example hypothesis which was assigned a
high score by HWCM but a low score by BLEU is
shown in Table 7. In this particular sentence, the
common head-modifier relations ?aboard? plane?
and ?plane ? the? caused a high headword chain
overlap, but did not appear as common n-grams
counted by BLEU. The hypothesis is missing the
word ?fifth?, but was nonetheless assigned a high
score by human judges. This is probably due to its
fluency, which HWCM seems to capture better than
BLEU.
4 Conclusion
This paper introduces several syntax-based metrics
for the evaluation of MT, which we find to be par-
ticularly useful for predicting a hypothesis?s fluency.
The syntactic metrics, except the kernel based ones,
all outperform BLEU in sentence-level fluency eval-
uation. For the overall evaluation of sentences for
fluency and adequacy, the metric based on headword
chain performs better than BLEU in both sentence-
level and corpus-level correlation with human judg-
ments. The kernel based metrics, though poor in
sentence-level evaluation, achieve the best results in
corpus-level evaluation, where sparse data are less
of a barrier.
Our syntax-based measures require the existence
of a parser for the language in question, however it
is worth noting that a parser is required for the tar-
get language only, as all our measures of similarity
are defined across hypotheses and references in the
same language.
Our results, in particular for the primarily struc-
tural STM, may be surprising in light of the fact
that the parser is not designed to handle ill-formed
or ungrammatical sentences such as those produced
by machine translation systems. Modern statistical
parsers have been tuned to discriminate good struc-
tures from bad rather than good sentences from bad.
31
hyp Diplomats will be aboard the plane to return home .
ref1 Diplomats are to come back home aboard the fifth plane .
ref2 Diplomatic staff would go home in a fifth plane .
ref3 Diplomatic staff will take the fifth plane home .
Table 7: An example hypothesis in the ACL05-MTE workshop which was assigned a high score by HWCM
(0.511) but a low score by BLEU (0.084). Both human judges assigned a high score (4).
Indeed, in some recent work on re-ranking machine
translation hypotheses (Och et al, 2004), parser-
produced structures were not found to provide help-
ful information, as a parser is likely to assign a good-
looking structure to even a lousy input hypothesis.
However, there is an important distinction be-
tween the use of parsers in re-ranking and evaluation
? in the present work we are looking for similarities
between pairs of parse trees rather than at features
of a single tree. This means that the syntax-based
evaluation measures can succeed even when the tree
structure for a poor hypothesis looks reasonable on
its own, as long as it is sufficiently distinct from the
structures used in the references.
We speculate that by discriminatively training
weights for the individual subtrees and headword
chains used by the syntax-based metrics, further im-
provements in evaluation accuracy are possible.
Acknowledgments We are very grateful to Alex
Kulesza for assistance with the JHU data. This work
was partially supported by NSF ITR IIS-09325646
and NSF ITR IIS-0428020.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45?60.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2003. Confidence estimation for
machine translation. Technical report, Center for Lan-
guage and Speech Processing, Johns Hopkins Univer-
sity, Baltimore. Summer Workshop Final Report.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for machine
translation. In Proc. MT Summit IX.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems.
Michael John Collins. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In In HLT 2002, Human Language Technology
Conference, San Diego, CA.
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In In Proceedings of the 2002 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2002), pages 304?311.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
In Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation (TMI), Baltimore, MD, October.
Dekang Lin. 2004. A path-based transfer model for
machine translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics
(COLING-04), pages 625?630, Geneva, Switzerland.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proceedings of the 2004 Meeting of the
North American chapter of the Association for Com-
putational Linguistics (NAACL-04), Boston.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Conference of the Association for Com-
putational Linguistics (ACL-03).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Conference of the Association for Com-
putational Linguistics (ACL-02), Philadelphia, PA.
32
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 65?73,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Machine Translation as Lexicalized Parsing with Hooks
Liang Huang
Dept. of Computer & Information Science
University of Pennsylvania
Philadelphia, PA 19104
Hao Zhang and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
We adapt the ?hook? trick for speeding up
bilexical parsing to the decoding problem
for machine translation models that are
based on combining a synchronous con-
text free grammar as the translation model
with an n-gram language model. This
dynamic programming technique yields
lower complexity algorithms than have
previously been described for an impor-
tant class of translation models.
1 Introduction
In a number of recently proposed synchronous
grammar formalisms, machine translation of new
sentences can be thought of as a form of parsing on
the input sentence. The parsing process, however,
is complicated by the interaction of the context-free
translation model with an m-gram1 language model
in the output language. While such formalisms ad-
mit dynamic programming solutions having poly-
nomial complexity, the degree of the polynomial is
prohibitively high.
In this paper we explore parallels between transla-
tion and monolingual parsing with lexicalized gram-
mars. Chart items in translation must be augmented
with words from the output language in order to cap-
ture language model state. This can be thought of as
a form of lexicalization with some similarity to that
of head-driven lexicalized grammars, despite being
unrelated to any notion of syntactic head. We show
1We speak of m-gram language models to avoid confusion
with n, which here is the length of the input sentence for trans-
lation.
that techniques for parsing with lexicalized gram-
mars can be adapted to the translation problem, re-
ducing the complexity of decoding with an inversion
transduction grammar and a bigram language model
from O(n7) to O(n6). We present background on
this translation model as well as the use of the tech-
nique in bilexicalized parsing before describing the
new algorithm in detail. We then extend the al-
gorithm to general m-gram language models, and
to general synchronous context-free grammars for
translation.
2 Machine Translation using Inversion
Transduction Grammar
The Inversion Transduction Grammar (ITG) of Wu
(1997) is a type of context-free grammar (CFG) for
generating two languages synchronously. To model
the translational equivalence within a sentence pair,
ITG employs a synchronous rewriting mechanism to
relate two sentences recursively. To deal with the
syntactic divergence between two languages, ITG
allows the inversion of rewriting order going from
one language to another at any recursive level. ITG
in Chomsky normal form consists of unary produc-
tion rules that are responsible for generating word
pairs:
X ? e/f
X ? e/
X ? /f
where e is a source language word, f is a foreign lan-
guage word, and  means the null token, and binary
production rules in two forms that are responsible
for generating syntactic subtree pairs:
X ? [Y Z]
65
and
X ? ?Y Z?
The rules with square brackets enclosing the
right-hand side expand the left-hand side symbol
into the two symbols on the right-hand side in the
same order in the two languages, whereas the rules
with angled brackets expand the left hand side sym-
bol into the two right-hand side symbols in reverse
order in the two languages. The first class of rules
is called straight rule. The second class of rules is
called inverted rule.
One special case of 2-normal ITG is the so-called
Bracketing Transduction Grammar (BTG) which
has only one nonterminal A and two binary rules
A ? [AA]
and
A ? ?AA?
By mixing instances of the inverted rule with
those of the straight rule hierarchically, BTG can
meet the alignment requirements of different lan-
guage pairs. There exists a more elaborate version
of BTG that has 4 nonterminals working together
to guarantee the property of one-to-one correspon-
dence between alignments and synchronous parse
trees. Table 1 lists the rules of this BTG. In the
discussion of this paper, we will consider ITG in 2-
normal form.
By associating probabilities or weights with the
bitext production rules, ITG becomes suitable for
weighted deduction over bitext. Given a sentence
pair, searching for the Viterbi synchronous parse
tree, of which the alignment is a byproduct, turns out
to be a two-dimensional extension of PCFG parsing,
having time complexity of O(n6), where n is the
length of the English string and the foreign language
string. A more interesting variant of parsing over bi-
text space is the asymmetrical case in which only the
foreign language string is given so that Viterbi pars-
ing involves finding the English string ?on the fly?.
The process of finding the source string given its tar-
get counterpart is decoding. Using ITG, decoding is
a form of parsing.
2.1 ITG Decoding
Wu (1996) presented a polynomial-time algorithm
for decoding ITG combined with an m-gram lan-
guage model. Such language models are commonly
used in noisy channel models of translation, which
find the best English translation e of a foreign sen-
tence f by finding the sentence e that maximizes the
product of the translation model P (f |e) and the lan-
guage model P (e).
It is worth noting that since we have specified ITG
as a joint model generating both e and f , a language
model is not theoretically necessary. Given a foreign
sentence f , one can find the best translation e?:
e? = argmax
e
P (e, f)
= argmax
e
?
q
P (e, f, q)
by approximating the sum over parses q with the
probability of the Viterbi parse:
e? = argmax
e
max
q
P (e, f, q)
This optimal translation can be computed in using
standard CKY parsing over f by initializing the
chart with an item for each possible translation of
each foreign word in f , and then applying ITG rules
from the bottom up.
However, ITG?s independence assumptions are
too strong to use the ITG probability alone for ma-
chine translation. In particular, the context-free as-
sumption that each foreign word?s translation is cho-
sen independently will lead to simply choosing each
foreign word?s single most probable English trans-
lation with no reordering. In practice it is beneficial
to combine the probability given by ITG with a local
m-gram language model for English:
e? = argmax
e
max
q
P (e, f, q)Plm(e)?
with some constant language model weight ?. The
language model will lead to more fluent output by
influencing both the choice of English words and the
reordering, through the choice of straight or inverted
rules. While the use of a language model compli-
cates the CKY-based algorithm for finding the best
translation, a dynamic programming solution is still
possible. We extend the algorithm by storing in each
chart item the English boundary words that will af-
fect the m-gram probabilities as the item?s English
string is concatenated with the string from an adja-
cent item. Due to the locality of m-gram language
66
Structural Rules Lexical Rules
S ? A
S ? B
S ? C
A ? [AB]
A ? [BB]
A ? [CB]
A ? [AC]
A ? [BC]
A ? [CC]
B ? ?AA?
B ? ?BA?
B ? ?CA?
B ? ?AC?
B ? ?BC?
B ? ?CC?
C ? ei/fj
C ? /fj
C ? ei/
Table 1: Unambiguous BTG
model, only m?1 boundary words need to be stored
to compute the new m-grams produced by combin-
ing two substrings. Figure 1 illustrates the combi-
nation of two substrings into a larger one in straight
order and inverted order.
3 Hook Trick for Bilexical Parsing
A traditional CFG generates words at the bottom of
a parse tree and uses nonterminals as abstract rep-
resentations of substrings to build higher level tree
nodes. Nonterminals can be made more specific to
the actual substrings they are covering by associ-
ating a representative word from the nonterminal?s
yield. When the maximum number of lexicalized
nonterminals in any rule is two, a CFG is bilexical.
A typical bilexical CFG in Chomsky normal form
has two types of rule templates:
A[h] ? B[h]C[h?]
or
A[h] ? B[h?]C[h]
depending on which child is the head child that
agrees with the parent on head word selection.
Bilexical CFG is at the heart of most modern statisti-
cal parsers (Collins, 1997; Charniak, 1997), because
the statistics associated with word-specific rules are
more informative for disambiguation purposes. If
we use A[i, j, h] to represent a lexicalized con-
stituent, ?(?) to represent the Viterbi score function
applicable to any constituent, and P (?) to represent
the rule probability function applicable to any rule,
Figure 2 shows the equation for the dynamic pro-
gramming computation of the Viterbi parse. The two
terms of the outermost max operator are symmetric
cases for heads coming from left and right. Contain-
ing five free variables i,j,k,h?,h, ranging over 1 to
n, the length of input sentence, both terms can be
instantiated in n5 possible ways, implying that the
complexity of the parsing algorithm is O(n5).
Eisner and Satta (1999) pointed out we don?t have
to enumerate k and h? simultaneously. The trick,
shown in mathematical form in Figure 2 (bottom) is
very simple. When maximizing over h?, j is irrele-
vant. After getting the intermediate result of maxi-
mizing over h?, we have one less free variable than
before. Throughout the two steps, the maximum
number of interacting variables is 4, implying that
the algorithmic complexity is O(n4) after binarizing
the factors cleverly. The intermediate result
max
h?,B
[?(B[i, k, h?]) ? P (A[h] ? B[h?]C[h])]
can be represented pictorially as
C[h]
A
i k . The
same trick works for the second max term in
Equation 1. The intermediate result coming from
binarizing the second term can be visualized as
A
k
B[h]
j
. The shape of the intermediate re-
sults gave rise to the nickname of ?hook?. Melamed
(2003) discussed the applicability of the hook trick
for parsing bilexical multitext grammars. The anal-
ysis of the hook trick in this section shows that it is
essentially an algebraic manipulation. We will for-
mulate the ITG Viterbi decoding algorithm in a dy-
namic programming equation in the following sec-
tion and apply the same algebraic manipulation to
produce hooks that are suitable for ITG decoding.
4 Hook Trick for ITG Decoding
We start from the bigram case, in which each de-
coding constituent keeps a left boundary word and
67
tu11 u12 v12v11 u21 u22 v22v21
X
Y Z[ ]
Ss
u21
X
Y Z
Ss t
< >
v21 v22 u11 u12 v11 v12u22
(a) (b)
Figure 1: ITG decoding using 3-gram language model. Two boundary words need to be kept on the left (u)
and right (v) of each constituent. In (a), two constituents Y and Z spanning substrings s, S and S, t of the
input are combined using a straight rule X ? [Y Z]. In (b), two constituents are combined using a inverted
rule X ? ?Y Z?. The dashed line boxes enclosing three words are the trigrams produced from combining
two substrings.
?(A[i, j, h]) = max
?
?
?
?
?
max
k,h?,B,C
[
?(B[i, k, h?]) ? ?(C[k, j, h]) ? P (A[h] ? B[h?]C[h])
]
,
max
k,h?,B,C
[
?(B[i, k, h]) ? ?(C[k, j, h?]) ? P (A[h] ? B[h]C[h?])
]
?
?
?
?
?
(1)
max
k,h?,B,C
[
?(B[i, k, h?]) ? ?(C[k, j, h]) ? P (A[h] ? B[h?]C[h])
]
= max
k,C
[
max
h?,B
[
?(B[i, k, h?]) ? P (A[h] ? B[h?]C[h])
]
? ?(C[k, j, h])
]
Figure 2: Equation for bilexical parsing (top), with an efficient factorization (bottom)
a right boundary word. The dynamic programming
equation is shown in Figure 3 (top) where i,j,k range
over 1 to n, the length of input foreign sentence, and
u,v,v1,u2 (or u,v,v2,u1) range over 1 to V , the size
of English vocabulary. Usually we will constrain the
vocabulary to be a subset of words that are probable
translations of the foreign words in the input sen-
tence. So V is proportional to n. There are seven
free variables related to input size for doing the max-
imization computation. Hence the algorithmic com-
plexity is O(n7).
The two terms in Figure 3 (top) within the first
level of the max operator, corresponding to straight
rules and inverted rules, are analogous to the two
terms in Equation 1. Figure 3 (bottom) shows how to
decompose the first term; the same method applies
to the second term. Counting the free variables en-
closed in the innermost max operator, we get five: i,
k, u, v1, and u2. The decomposition eliminates one
free variable, v1. In the outermost level, there are
six free variables left. The maximum number of in-
teracting variables is six overall. So, we reduced the
complexity of ITG decoding using bigram language
model from O(n7) to O(n6).
The hooks k
X
Zu u2
i that we have built for de-
coding with a bigram language model turn out to be
similar to the hooks for bilexical parsing if we focus
on the two boundary words v1 and u2 (or v2 and u1)
68
?(X[i, j, u, v]) = max
?
?
?
?
?
?
?
?
?
max
k,v1,u2,Y,Z
[
?(Y [i, k, u, v1]) ? ?(Z[k, j, u2, v])
? P (X ? [Y Z]) ? bigram(v1, u2)
]
,
max
k,v2,u1,Y,Z
[
?(Y [i, k, u1, v]) ? ?(Z[k, j, u, v2])
? P (X ? ?Y Z?) ? bigram(v2, u1)
]
?
?
?
?
?
?
?
?
?
(2)
max
k,v1,u2,Y,Z
[
?(Y [i, k, u, v1]) ? ?(Z[k, j, u2, v]) ? P (X ? [Y Z]) ? bigram(v1, u2)
]
= max
k,u2,Z
[
max
v1,Y
[
?(Y [i, k, u, v1]) ? P (X ? [Y Z]) ? bigram(v1, u2)
]
? ?(Z[k, j, u2, v])
]
Figure 3: Equation for ITG decoding (top), with an efficient factorization (bottom)
that are interacting between two adjacent decoding
constituents and relate them with the h? and h that
are interacting in bilexical parsing. In terms of al-
gebraic manipulation, we are also rearranging three
factors (ignoring the non-lexical rules), trying to re-
duce the maximum number of interacting variables
in any computation step.
4.1 Generalization to m-gram Cases
In this section, we will demonstrate how to use the
hook trick for trigram decoding which leads us to a
general hook trick for any m-gram decoding case.
We will work only on straight rules and use icons
of constituents and hooks to make the equations eas-
ier to interpret.
The straightforward dynamic programming equa-
tion is:
i
X
u1u2 v1v2
j = maxv11,v12,u21,u22,
k,Y,Z
u22
i k j
X
Y Z
u1u2 v2v1
][
v11v12 u21
(3)
By counting the variables that are dependent
on input sentence length on the right hand side
of the equation, we know that the straightfor-
ward algorithm?s complexity is O(n11). The max-
imization computation is over four factors that
are dependent on n: ?(Y [i, k, u1, u2, v11, v12]),
?(Z[k, j, u21, u22, v1, v2]), trigram(v11, v12, u21),
and trigram(v12, u21, u22). As before, our goal is
to cleverly bracket the factors.
By bracketing trigram(v11, v12, u21) and
?(Y [i, k, u1, u2, v11, v12]) together and maximizing
over v11 and Y , we can build the the level-1 hook:
u21
i k
X
Z
u1u2
][
v12
= max
v11,Y
u21
i k
X
Y Z
u1u2
][
v11v12
The complexity is O(n7).
Grouping the level-1 hook and
trigram(v12, u21, u22), maximizing over v12,
we can build the level-2 hook:
u21
i k
X
Z
u1u2
][
u22
= max
v12
u21
i k
X
Z
u1u2
][
v12 u22
The complexity is O(n7). Finally,
we can use the level-2 hook to com-
bine with Z[k, j, u21, u22, v1, v2] to build
X[i, j, u1, u2, v1, v2]. The complexity is O(n9)
after reducing v11 and v12 in the first two steps.
i
X
u1u2 v1v2
j = max
u21,u22,k,Z
u22
i k j
X
Z
u1u2 v2v1
][
u21
(4)
Using the hook trick, we have reduced the com-
plexity of ITG decoding using bigrams from O(n7)
to O(n6), and from O(n11) to O(n9) for trigram
69
case. We conclude that for m-gram decoding of
ITG, the hook trick can change the the time com-
plexity from O(n3+4(m?1)) to O(n3+3(m?1)). To
get an intuition of the reduction, we can compare
Equation 3 with Equation 4. The variables v11 and
v12 in Equation 3, which are independent of v1 and
v2 for maximizing the product have been concealed
under the level-2 hook in Equation 4. In general,
by building m ? 1 intermediate hooks, we can re-
duce m ? 1 free variables in the final combination
step, hence having the reduction from 4(m ? 1) to
3(m ? 1).
5 Generalization to Non-binary Bitext
Grammars
Although we have presented our algorithm as a de-
coder for the binary-branching case of Inversion
Transduction Grammar, the same factorization tech-
nique can be applied to more complex synchronous
grammars. In this general case, items in the dy-
namic programming chart may need to represent
non-contiguous span in either the input or output
language. Because synchronous grammars with in-
creasing numbers of children on the right hand side
of each production form an infinite, non-collapsing
hierarchy, there is no upper bound on the number
of discontinuous spans that may need to be repre-
sented (Aho and Ullman, 1972). One can, however,
choose to factor the grammar into binary branching
rules in one of the two languages, meaning that dis-
continuous spans will only be necessary in the other
language.
If we assume m is larger than 2, it is likely that
the language model combinations dominate com-
putation. In this case, it is advantageous to factor
the grammar in order to make it binary in the out-
put language, meaning that the subrules will only
need to represent adjacent spans in the output lan-
guage. Then the hook technique will work in the
same way, yielding O(n2(m?1)) distinct types of
items with respect to language model state, and
3(m?1) free indices to enumerate when combining
a hook with a complete constituent to build a new
item. However, a larger number of indices point-
ing into the input language will be needed now that
items can cover discontinuous spans. If the gram-
mar factorization yields rules with at most R spans
in the input language, there may be O(n2R) dis-
tinct types of chart items with respect to the input
language, because each span has an index for its
beginning and ending points in the input sentence.
Now the upper bound of the number of free in-
dices with respect to the input language is 2R + 1,
because otherwise if one rule needs 2R + 2 in-
dices, say i1, ? ? ? , i2R+2, then there are R + 1 spans
(i1, i2), ? ? ? , (i2R+1, i2R+2), which contradicts the
above assumption. Thus the time complexity at the
input language side is O(n2R+1), yielding a total al-
gorithmic complexity of O(n3(m?1)+(2R+1)).
To be more concrete, we will work through a 4-
ary translation rule, using a bigram language model.
The standard DP equation is:
i
u v
j
A
= maxv3,u1,v1,u4,v4,u2,
k1,k2,k3,
B,C,D,E
B C D E
A
v3u u1 v1 u4 v4 u2 v
i k1 k2 k3 j (5)
This 4-ary rule is a representative difficult case.
The underlying alignment pattern for this rule is as
follows:
D
C
E
B
A
It is a rule that cannot be binarized in the bitext
space using ITG rules. We can only binarize it in
one dimension and leave the other dimension having
discontinuous spans. Without applying binarization
and hook trick, decoding parsing with it according
to Equation 5 requires time complexity of O(n13).
However, we can build the following partial con-
stituents and hooks to do the combination gradually.
The first step finishes a hook by consuming one
bigram. Its time complexity is O(n5):
C D E
A
u1u
k2 k3 = max
v3,B
B C D E
A
u v3 u1
k2 k3
The second step utilizes the hook we just built and
builds a partial constituent. The time complexity is
O(n7):
70
D E
A
u v1
i k1 k2 k3 = max
u1,C
C D E
A
u u1 v1
i k1 k2 k3
By ?eating? another bigram, we build the second
hook using O(n7):
D E
A
u u4
i k1 k2 k3 = max
v1
D E
A
u v1 u4
i k1 k2 k3
We use the last hook. This step has higher com-
plexity: O(n8):
E
A
u v4
i k1 k2 j = max
u4,k3,D
v4u4
k2 k3
D E
A
jk1i
u
The last bigram involved in the 4-ary rule is com-
pleted and leads to the third hook, with time com-
plexity of O(n7):
E
A
jk2k1i
u u2
= max
v4
E
A
u v4 u2
i k1 k2 j
The final combination is O(n7):
i
u v
j
A
= max
u2,k1,k2,E
u
i k1 k2
E
A
u2
j
v
The overall complexity has been reduced to
O(n8) after using binarization on the output side and
using the hook trick all the way to the end. The result
is one instance of our general analysis: here R = 2,
m = 2, and 3(m ? 1) + (2R + 1) = 8.
6 Implementation
The implementation of the hook trick in a practi-
cal decoder is complicated by the interaction with
pruning. If we build hooks looking for all words
in the vocabulary whenever a complete constituent
is added to the chart, we will build many hooks
that are never used, because partial hypotheses with
many of the boundary words specified by the hooks
may never be constructed due to pruning. In-
stead of actively building hooks, which are inter-
mediate results, we can build them only when we
need them and then cache them for future use. To
make this idea concrete, we sketch the code for bi-
gram integrated decoding using ITG as in Algo-
rithm 1. It is worthy of noting that for clarity we
are building hooks in shape of
v
k j
v?
Z
, instead
of
X
Y v
k j
v?
as we have been showing in the
previous sections. That is, the probability for the
grammar rule is multiplied in when a complete con-
stituent is built, rather than when a hook is created.
If we choose the original representation, we would
have to create both straight hooks and inverted hooks
because the straight rules and inverted rules are to be
merged with the ?core? hooks, creating more speci-
fied hooks.
7 Conclusion
By showing the parallels between lexicalization for
language model state and lexicalization for syntac-
tic heads, we have demonstrated more efficient al-
gorithms for previously described models of ma-
chine translation. Decoding for Inversion Transduc-
tion Grammar with a bigram language model can be
done in O(n6) time. This is the same complexity
as the ITG alignment algorithm used by Wu (1997)
and others, meaning complete Viterbi decoding is
possible without pruning for realistic-length sen-
tences. More generally, ITG with an m-gram lan-
guage model is O(n3+3(m?1)), and a synchronous
context-free grammar with at most R spans in the
input language is O(n3(m?1)+(2R+1)). While this
improves on previous algorithms, the degree in n
is probably still too high for complete search to
be practical with such models. The interaction of
the hook technique with pruning is an interesting
71
Algorithm 1 ITGDecode(Nt)
for all s, t such that 0 ? s < t ? Nt do
for all S such that s < S < t do
 straight rule
for all rules X ? [Y Z] ? G do
for all (Y, u1, v1) possible for the span of (s, S) do
 a hook who is on (S, t), nonterminal as Z, and outside expectation being v1 is required
if not exist hooks(S, t, Z, v1) then
build hooks(S, t, Z, v1)
end if
for all v2 possible for the hooks in (S, t, Z, v1) do
 combining a hook and a hypothesis, using straight rule
?(s, t, X, u1, v2) =
max
{
?(s, t, X, u1, v2), ?(s, S, Y, u1, v1) ? ?+(S, t, Z, v1, v2) ? P (X ? [Y Z])
}
end for
end for
end for
 inverted rule
for all rules X ? ?Y Z? ? G do
for all (Z, u2, v2) possible for the span of (S, t) do
 a hook who is on (s, S), nonterminal as Y , and outside expectation being v2 is required
if not exist hooks(s, S, Y, v2) then
build hooks(s, S, Y, v2)
end if
for all v1 possible for the hooks in (s, S, Y, v2) do
 combining a hook and a hypothesis, using inverted rule
?(s, t, X, u2, v1) =
max
{
?(s, t, X, u2, v1), ?(S, t, Z, u2, v2) ? ?+(s, S, Y, v2, v1) ? P (X ? ?Y Z?)
}
end for
end for
end for
end for
end for
routine build hooks(s, t, X, v?)
for all (X, u, v) possible for the span of (s, t) do
 combining a bigram with a hypothesis
?+(s, t, X, v?, v) =
max
{
?+(s, t, X, v?, v), bigram(v?, u) ? ?(s, t, X, u, v)
}
end for
72
area for future work. Building the chart items with
hooks may take more time than it saves if many of
the hooks are never combined with complete con-
stituents due to aggressive pruning. However, it may
be possible to look at the contents of the chart in or-
der to build only those hooks which are likely to be
useful.
References
Aho, Albert V. and Jeffery D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume 1.
Englewood Cliffs, NJ: Prentice-Hall.
Charniak, Eugene. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of the Fourteenth National Conference on Arti-
ficial Intelligence (AAAI-97), pages 598?603, Menlo
Park, August. AAAI Press.
Collins, Michael. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Conference of the Association for Compu-
tational Linguistics (ACL-97), pages 16?23, Madrid,
Spain.
Eisner, Jason and Giorgio Satta. 1999. Efficient parsing
for bilexical context-free grammars and head automa-
ton grammars. In 37th Annual Meeting of the Associ-
ation for Computational Linguistics.
Melamed, I. Dan. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of the 2003 Meeting
of the North American chapter of the Association for
Computational Linguistics (NAACL-03), Edmonton.
Wu, Dekai. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In 34th Annual Meeting
of the Association for Computational Linguistics.
Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
73
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 198?199,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Online Statistics for a Unification-Based Dialogue Parser
Micha Elsner, Mary Swift, James Allen, and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
{melsner,swift,allen,gildea}@cs.rochester.edu
Abstract
We describe a method for augmenting
unification-based deep parsing with statis-
tical methods. We extend and adapt the
Bikel parser, which uses head-driven lex-
ical statistics, to dialogue. We show that
our augmented parser produces signifi-
cantly fewer constituents than the baseline
system and achieves comparable brack-
eting accuracy, even yielding slight im-
provements for longer sentences.
1 Introduction
Unification parsers have problems with efficiency
and selecting the best parse. Lexically-conditioned
statistics as used by Collins (1999) may provide a
solution. They have been used in three ways: as
a postprocess for parse selection (Toutanova et al,
2005; Riezler et al, 2000; Riezler et al, 2002), a
preprocess to find more probable bracketing struc-
tures (Swift et al, 2004), and online to rank each
constituent produced, as in Tsuruoka et al (2004)
and this experiment.
The TRIPS parser (Allen et al, 1996) is a unifi-
cation parser using an HPSG-inspired grammar and
hand-tuned weights for each rule. In our augmented
system (Aug-TRIPS), we replaced these weights
with a lexically-conditioned model based on the
adaptation of Collins used by Bikel (2002), allowing
more efficiency and (in some cases) better selection.
Aug-TRIPS retains the same grammar and lexicon
as TRIPS, but uses its statistical model to determine
the order in which unifications are attempted.
2 Experiments
We tested bracketing accuracy on the Monroe cor-
pus (Stent, 2001), which contains collaborative
emergency-management dialogues. Aug-TRIPS is
comparable to TRIPS in accuracy, but produces
fewer constituents (Table 1). The Bikel parser has
slightly higher precision/recall than either TRIPS
or Aug-TRIPS, since it can choose any bracketing
structure regardless of semantic coherence, while
the TRIPS systems must find a legal pattern of fea-
ture unifications. Aug-TRIPS also has better preci-
sion/recall when parsing the longer sentences (Ta-
ble 2).
(training=9282) Bikel Aug-TRIPS TRIPS
Recall 79.40 76.09 76.77
Precision 79.40 77.08 78.20
Complete Match 42.00 46.00 65.00
% Constit. Reduction - 36.96 0.00
Table 1: Bracketing accuracy for 100 random sen-
tences ? 2 words.
> 7 Aug-TRIPS > 7 TRIPS
Recall 73.25 71.00
Precision 74.78 73.44
Complete Match 22.50 37.50
Table 2: Bracketing accuracy for the 40 sentences >
7 words.
Since our motivation for unification parsing is to
reveal semantics as well as syntax, we next evalu-
ated Aug-TRIPS?s production of correct interpreta-
tions at the sentence level, which require complete
correctness not only of the bracketing structure but
of the sense chosen for each word and the thematic
198
roles of each argument (Tetreault et al, 2004).
For this task, we modified the probability model
to condition on the senses in our lexicon rather than
words. For instance, the words ?two thousand dol-
lars? are replaced with the senses ?number number-
unit money-unit?. This allows us to model lexi-
cal disambiguation explicitly. The model generates
one or more senses from each word with probability
P (sense|word, tag), and then uses sense statistics
rather than word statistics in all other calculations.
Similar but more complex models were used in the
PCFG-sem model of Toutanova et al (2005) and us-
ing WordNet senses in Bikel (2000).
We used the Projector dialogues (835 sentences),
which concern purchasing video projectors. In this
domain, Aug-TRIPS makes about 10% more inter-
pretation errors than TRIPS (Table 3), but when
parsing sentences on which TRIPS itself makes er-
rors, it can correct about 10% (Table 4).
(training=310) TRIPS Aug-TRIPS
Correct 26 21
Incorrect 49 54
% Reduction in Constituents 0% 45%
Table 3: Sentence-level accuracy on 75 random sen-
tences.
(training=396) TRIPS Aug-TRIPS
Correct 0 8
Incorrect 54 46
% Reduction in Constituents 0% 46%
Table 4: Sentence-level accuracy on 54 TRIPS error
sentences
Our parser makes substantially fewer constituents
than baseline TRIPS at only slightly lower accu-
racy. Tsuruoka et al (2004) achieved a much higher
speedup (30 times) than we did; this is partly due to
their use of the Penn Treebank, which contains much
more data than our corpora. In addition, however,
their baseline system is a classic HPSG parser with
no efficiency features, while our baseline, TRIPS, is
designed as a real-time dialogue parser which uses
hand-tuned weights to guide its search and imposes
a maximum chart size.
Acknowledgements Our thanks to Will DeBeau-
mont and four anonymous reviewers.
References
James F. Allen, Bradford W. Miller, Eric K. Ringger, and
Teresa Sikorski. 1996. A robust system for natural
spoken dialogue. In Proceedings of the 1996 Annual
Meeting of the Association for Computational Linguis-
tics (ACL?96).
Daniel Bikel. 2000. A statistical model for parsing
and word-sense disambiguation. In Proceedings of
the Joint SIGDAT Conference on Empirical Methods
in Natural Language Processing and Very Large Cor-
pora, Hong Kong.
Daniel Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Human Lan-
guage Technology Conference (HLT), San Diego.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling of
constraint-based grammars using log-linear measures
and EM training. In Proceedings of the 38th Annual
Meeting of the ACL, Hong Kong.
Stefan Riezler, Tracy H. King, Richard Crouch, and
John T. Maxwell. 2002. Parsing the Wall Street Jour-
nal using a Lexical-Functional Grammar and discrim-
inative estimation. In Proceedings of the 40th Annual
Meeting of the ACL, Philadelphia.
Amanda J. Stent. 2001. Dialogue Systems as Conversa-
tional Partners. Ph.D. thesis, University of Rochester.
Mary Swift, James Allen, and Daniel Gildea. 2004.
Skeletons in the parser: Using a shallow parser to im-
prove deep parsing. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics
(COLING-04), Geneva, Switzerland, August.
Joel Tetreault, Mary Swift, Preethum Prithviraj, My-
roslava Dzikovska, and James Allen. 2004. Discourse
annotation in the Monroe corpus. In ACL workshop on
Discourse Annotation, Barcelona, Spain, July.
Kristina Toutanova, Christopher D. Manning, Dan
Flickinger, and Stephan Oepen. 2005. Stochastic
HPSG parse disambiguation using the Redwoods cor-
pus. Journal of Logic and Computation.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi Tsujii.
2004. Towards efficient probabilistic HPSG parsing:
Integrating semantic and syntactic preference to guide
the parsing. In Proceedings of IJCNLP-04 Workshop:
Beyond Shallow Analyses- Formalisms and Statistical
Modeling for Deep Analyses, Sanya City, China.
199
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 224?231,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Efficient Search for Inversion Transduction Grammar
Hao Zhang and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
We develop admissible A* search heuris-
tics for synchronous parsing with Inver-
sion Transduction Grammar, and present
results both for bitext alignment and for
machine translation decoding. We also
combine the dynamic programming hook
trick with A* search for decoding. These
techniques make it possible to find opti-
mal alignments much more quickly, and
make it possible to find optimal transla-
tions for the first time. Even in the pres-
ence of pruning, we are able to achieve
higher BLEU scores with the same amount
of computation.
1 Introduction
The Inversion Transduction Grammar (ITG) of
Wu (1997) is a syntactically motivated algorithm
for producing word-level alignments of pairs of
translationally equivalent sentences in two lan-
guages. The algorithm builds a synchronous parse
tree for both sentences, and assumes that the trees
have the same underlying structure but that the or-
dering of constituents may differ in the two lan-
guages. ITG imposes constraints on which align-
ments are possible, and these constraints have
been shown to be a good match for real bitext data
(Zens and Ney, 2003).
A major motivation for the introduction of ITG
was the existence of polynomial-time algorithms
both for alignment and translation. Alignment,
whether for training a translation model using EM
or for finding the Viterbi alignment of test data,
is O(n6) (Wu, 1997), while translation (decod-
ing) is O(n7) using a bigram language model, and
O(n11) with trigrams. While polynomial-time al-
gorithms are a major improvement over the NP-
complete problems posed by the alignment models
of Brown et al (1993), the degree of these polyno-
mials is high, making both alignment and decod-
ing infeasible for realistic sentences without very
significant pruning. In this paper, we explore use
of the ?hook trick? (Eisner and Satta, 1999; Huang
et al, 2005) to reduce the asymptotic complexity
of decoding, and the use of heuristics to guide the
search.
Our search heuristics are a conservative esti-
mate of the outside probability of a bitext cell in
the complete synchronous parse. Some estimate
of this outside probability is a common element
of modern statistical (monolingual) parsers (Char-
niak et al, 1998; Collins, 1999), and recent work
has developed heuristics that are admissible for A*
search, guaranteeing that the optimal parse will
be found (Klein and Manning, 2003). We extend
this type of outside probability estimate to include
both word translation and n-gram language model
probabilities. These measures have been used to
guide search in word- or phrase-based MT sys-
tems (Wang and Waibel, 1997; Och et al, 2001),
but in such models optimal search is generally not
practical even with good heuristics. In this paper,
we show that the same assumptions that make ITG
polynomial-time can be used to efficiently com-
pute heuristics which guarantee us that we will
find the optimal alignment or translation, while
significantly speeding the search.
2 Inversion Transduction Grammar
An Inversion Transduction Grammar can generate
pairs of sentences in two languages by recursively
applying context-free bilingual production rules.
Most work on ITG has focused on the 2-normal
form, which consists of unary production rules
that are responsible for generating word pairs:
X ? e/f
224
and binary production rules in two forms that are
responsible for generating syntactic subtree pairs:
X ? [Y Z]
and
X ? ?Y Z?
The rules with square brackets enclosing the
right hand side expand the left hand side symbol
into the two symbols on the right hand side in the
same order in the two languages, whereas the rules
with pointed brackets expand the left hand side
symbol into the two right hand side symbols in re-
verse order in the two languages.
3 A* Viterbi Alignment Selection
A* parsing is a special case of agenda-based chart
parsing, where the priority of a node X[i, j] on the
agenda, corresponding to nonterminal X spanning
positions i through j, is the product of the node?s
current inside probability with an estimate of the
outside probability. By the current inside proba-
bility, we mean the probability of the so-far-most-
probable subtree rooted on the node X[i, j], with
leaves being iwj , while the outside probability isthe highest probability for a parse with the root
being S[0, N ] and the sequence 0wiXjwn formingthe leaves. The node with the highest priority is re-
moved from the agenda and added to the chart, and
then explored by combining with all of its neigh-
boring nodes in the chart to update the priorities
of the resulting nodes on the agenda. By using
estimates close to the actual outside probabilities,
A* parsing can effectively reduce the number of
nodes to be explored before putting the root node
onto the chart. When the outside estimate is both
admissible and monotonic, whenever a node is put
onto the chart, its current best inside parse is the
Viterbi inside parse.
To relate A* parsing with A* search for find-
ing the lowest cost path from a certain source
node to a certain destination node in a graph, we
view the forest of all parse trees as a hypergraph.
The source node in the hypergraph fans out into
the nodes of unit spans that cover the individual
words. From each group of children to their par-
ent in the forest, there is a hyperedge. The destina-
tion node is the common root node for all the parse
trees in the forest. Under the mapping, a parse is a
hyperpath from the source node to the destination
node. The Viterbi parse selection problem thus be-
comes finding the lowest-cost hyperpath from the
source node to the destination node. The cost in
this scenario is thus the negative of log probabil-
ity. The inside estimate and outside estimate natu-
rally correspond to the g? and h? for A* searching,
respectively.
A stochastic ITG can be thought of as a stochas-
tic CFG extended to the space of bitext. A node in
the ITG chart is a bitext cell that covers a source
substring and a target substring. We use the no-
tion of X[l, m, i, j] to represent a tree node in ITG
parse. It can potentially be combined with any
bitext cells at the four corners, as shown in Fig-
ure 1(a).
Unlike CFG parsing where the leaves are fixed,
the Viterbi ITG parse selection involves finding
the Viterbi alignment under ITG constraint. Good
outside estimates have to bound the outside ITG
Viterbi alignment probability tightly.
3.1 A* Estimates for Alignment
Under the ITG constraints, each source language
word can be aligned with at most one target lan-
guage word and vice versa. An ITG constituent
X[l, m, i, j] implies that the words in the source
substring in the span [l, m] are aligned with the
words in the target substring [i, j]. It further im-
plies that the words outside the span [l, m] in the
source are aligned with the words outside the span
[i, j] in the target language. Figure 1(b) displays
the tic-tac-toe pattern for the inside and outside
components of a particular cell. To estimate the
upper bound of the ITG Viterbi alignment proba-
bility for the outside component with acceptable
complexity, we need to relax the ITG constraint.
Instead of ensuring one-to-one in both directions,
we use a many-to-one constraint in one direction,
and we relax all constraints on reordering within
the outside component.
The many-to-one constraint has the same dy-
namic programming structure as IBM Model 1,
where each target word is supposed to be trans-
lated from any of the source words or the NULL
symbol. In the Model 1 estimate of the outside
probability, source and target words can align us-
ing any combination of points from the four out-
side corners of the tic-tac-toe pattern. Thus in
Figure 1(b), there is one solid cell (correspond-
ing to the Model 1 Viterbi alignment) in each col-
umn, falling either in the upper or lower outside
shaded corner. This can be also be thought of as
squeezing together the four outside corners, creat-
225
lm
0 i j N
l
m
0 i j N
l
m
n
0 i j k N
(a) (b) (c)
Figure 1: (a) A bitext cell X[l, m, i, j] (shaded) for ITG parsing. The inside cell can be combined with
adjacent cells in the four outside corners (lighter shading) to expand into larger cells. One possible
expansion to the lower left corner is displayed. (b) The tic-tac-toe pattern of alignments consistent with
a given cell. If the inner box is used in the final synchronous parse, all other alignments must come
from the four outside corners. (c) Combination of two adjacent cells shown with region for new outside
heuristic.
ing a new cell whose probability is estimated using
IBM Model 1. In contrast, the inside Viterbi align-
ment satisfies the ITG constraint, implying only
one solid cell in each column and each row. Math-
ematically, our Model 1 estimate for the outside
component is:
hM1(l, m, i, j) =
?
t<i,
t>j
max
s<l,
s>m
P (ft, es)
This Model 1 estimate is admissible. Maximiz-
ing over each column ensures that the translation
probability for each target word is greater than or
equal to the corresponding word translation prob-
ability under the ITG constraint. Model 1 virtually
assigns a probability of 1 for deleting any source
word. As a product of word-to-word translation
probabilities including deletions and insertions,
the ITG Viterbi alignment probability cannot be
higher than the product of maximal word-to-word
translation probabilities using the Model 1 esti-
mate.
The Model 1 estimate is also monotonic, a prop-
erty which is best understood geometrically. A
successor state to cell (l, m, i, j) in the search is
formed by combining the cell with a cell which
is adjacent at one of the four corners, as shown
in Figure 1(c). Of the four outside corner regions
used in calculating the search heuristic, one will
be the same for the successor state, and three will
be a subset of the old corner region. Without
loss of generality, assume we are combining a cell
(m, n, j, k) that is adjacent to (l, m, i, j) to the up-
per right. We define
HM1(l, m, i, j) = ? log hM1(l, m, i, j)
as the negative log of the heuristic in order to cor-
respond to an estimated cost or distance in search
terminology. Similarly, we speak of the cost of a
chart entry c(X[l, m, i, j]) as its negative log prob-
ability, and the cost of a cell c(l, m, i, j) as the
cost of the best chart entry with the boundaries
(l, m, i, j). The cost of the cell (m, n, j, k) which
is being combined with the old cell is guaranteed
to be greater than the contribution of the columns
j through k to the heuristic HM1(l, m, i, j). Thecontribution of the columns k through N to the
new heuristic HM1(l, n, i, k) is guaranteed to begreater in cost than their contribution to the old
heuristic. Thus,
HM1(l, m, i, j) ? c(m, n, j, k) + c(X ? Y Z)
+ HM1(l, n, i, k)
meaning that the heuristic is monotonic or consis-
tent.
The Model 1 estimate can be applied in both
translation directions. The estimates from both
directions are an upper bound of the actual ITG
Viterbi probability. By taking the minimum of the
two, we can get a tighter upper bound.
We can precompute the Model 1 outside esti-
mate for all bitext cells before parsing starts. A
na??ve implementation would take O(n6) steps of
computation, because there are O(n4) cells, each
of which takes O(n2) steps to compute its Model 1
probability. Fortunately, exploiting the recursive
226
ju v
</S><S>
i
Figure 2: The region within the dashed lines is the translation hypothesis X[i, j, u, v]. The word sequence
on the top is the Viterbi translation of the sentence on the bottom. Wide range word order change may
happen.
nature of the cells, we can compute values for the
inside and outside components of each cell using
dynamic programming in O(n4) time (Zhang and
Gildea, 2005).
4 A* Decoding
The of ITG decoding algorithm of Wu (1996) can
be viewed as a variant of the Viterbi parsing al-
gorithm for alignment selection. The task of stan-
dard alignment is to find word level links between
two fixed-order strings. In the decoding situation,
while the input side is a fixed sequence of words,
the output side is a bag of words to be linked with
the input words and then reordered. Under the ITG
constraint, if the target language substring [i, j] is
translated into s1 in the source language and thetarget substring [j, k] is translated into s2, then s1and s2 must be consecutive in the source languageas well and two possible orderings, s1s2 and s2s1,are allowed. Finding the best translation of the
substring of [i, k] involves searching over all pos-
sible split points j and two possible reorderings
for each split. In theory, the inversion probabilities
associated with the ITG rules can do the job of re-
ordering. However, a language model as simple as
bigram is generally stronger. Using an n-gram lan-
guage model implies keeping at least n?1 bound-
ary words in the dynamic programming table for a
hypothetical translation of a source language sub-
string. In the case of a bigram ITG decoder, a
translation hypothesis for the source language sub-
string [i, j] is denoted as X[i, j, u, v], where u and
v are the left boundary word and right boundary
word of the target language counterpart.
As indicated by the similarity of parsing item
notation, the dynamic programming property of
the Viterbi decoder is essentially the same as the
bitext parsing for finding the underlying Viterbi
alignment. By permitting translation from the null
target string of [i, i] into source language words as
many times as necessary, the decoder can translate
an input sentence into a longer output sentence.
When there is the null symbol in the bag of candi-
date words, the decoder can choose to translate a
word into null to decrease the output length. Both
insertions and deletions are special cases of the bi-
text parsing items.
Given the similarity of the dynamic program-
ming framework to the alignment problem, it is
not surprising that A* search can also be ap-
plied in a similar way. The initial parsing items
on the agenda are the basic translation units:
X[i, i + 1, u, u], for normal word-for-word trans-
lations and deletions (translations into nothing),
and also X[i, i, u, u], for insertions (translations
from nothing). The goal item is S[0, N, ?s?, ?/s?],
where ?s? stands for the beginning-of-sentence
symbol and ?/s? stands for the end-of-sentence
symbol. The exploration step of the A* search
is to expand the translation hypothesis of a sub-
string by combining with neighboring translation
hypotheses. When the outside estimate is admis-
sible and monotonic, the exploration is optimal
in the sense that whenever a hypothesis is taken
from the top of the agenda, it is a Viterbi transla-
tion of the corresponding target substring. Thus,
when S[0, N, ?s?, ?/s?] is added to the chart, we
have found the Viterbi translation for the entire
sentence.
227
?(X[i, j, u, v]) = max
{
???(X[i, j, u, v]), ?[](X[i, j, u, v])
}
?[](X[i, j, u, v]) = maxk,v1,u2,Y,Z
[
?(Y [i, k, u, v1]) ? ?(Z[k, j, u2, v]) ? P (X ? [Y Z]) ? Plm(u2 | v1)
]
= max
k,u2,Y,Z
[
max
v1
[
?(Y [i, k, u, v1]) ? Plm(u2 | v1)
]
? P (X ? [Y Z]) ? ?(Z[k, j, u2, v])
]
Figure 3: Top: An ITG decoding constituent can be built with either a straight or an inverted rule.
Bottom: An efficient factorization for straight rules.
4.1 A* Estimates for Translation
The key to the success of A* decoding is an out-
side estimate that combines word-for-word trans-
lation probabilities and n-gram probabilities. Fig-
ure 2 is the picture of the outside translations
and bigrams of a particular translation hypothesis
X[i, j, u, v].
Our heuristic involves precomputing two val-
ues for each word in the input string, involving
forward- and backward-looking language model
probabilities. For the forward looking value hf atinput position n, we take a maximum over the set
of words Sn that the input word tn can be trans-lated as:
hf (n) = maxs?Sn
[
Pt(s | tn) max
s??S
Plm(s? | s)
]
where:
S =
?
n
Sn
is the set of all possible translations for all words
in the input string. While hf considers lan-guage model probabilities for words following s,
the backward-looking value hb considers languagemodel probabilities for s given possible preceding
words:
hb(n) = maxs?Sn
[
Pt(s | tn) max
s??S
Plm(s | s?)
]
Our overall heuristic for a partial translation
hypothesis X[i, j, u, v] combines language model
probabilities at the boundaries of the input sub-
string with backward-looking values for the pre-
ceding words, and forward-looking values for the
following words:
h(i, j, u, v) =
[
max
s?S
Plm(u | s)
] [
max
s?S
Plm(s | v)
]
?
?
n<i,
n>j
max [hb(n), hf (n)]
Because we don?t know whether a given input
word will appear before or after the partial hypoth-
esis in the final translation, we take the maximum
of the forward and backward values for words out-
side the span [i, j].
4.2 Combining the Hook Trick with A*
The hook trick is a factorization technique for dy-
namic programming. For bilexical parsing, Eis-
ner and Satta (1999) pointed out we can reduce
the complexity of parsing from O(n5) to O(n4)
by combining the non-head constituents with the
bilexical rules first, and then combining the resul-
tant hook constituents with the head constituents.
By doing so, the maximal number of interactive
variables ranging over n is reduced from 5 to 4.
For ITG decoding, we can apply a similar factor-
ization trick. We describe the bigram-integrated
decoding case here, and refer to Huang et al
(2005) for more detailed discussion. Figure 3
shows how to decompose the expression for the
case of straight rules; the same method applies to
inverted rules. The number of free variables on the
right hand side of the second equation is 7: i, j, k,
u, v, v1, and u2.1 After factorization, counting thefree variables enclosed in the innermost max oper-
ator, we get five: i, k, u, v1, and u2. The decompo-sition eliminates one free variable, v1. In the out-ermost level, there are six free variables left. The
maximum number of interacting variables is six
overall. So, we reduced the complexity of ITG de-
coding using bigram language model from O(n7)
to O(n6). If we visualize an ITG decoding con-
stituent Y extending from source language posi-
tion i to k and target language boundary words u
and v1 with a diagram:
Y
i k
u v1
1X , Y , and Z range over grammar nonterminals, of which
there are a constant number.
228
 0
 50
 100
 150
 200
 250
 300
 350
 400
 450
 500
 0  10  20  30  40  50  60  70
se
co
n
ds
sentence length
full
uniform
ibm1encn
ibm1sym
 0
 1e+06
 2e+06
 3e+06
 4e+06
 5e+06
 6e+06
 0  10  20  30  40  50  60  70
# 
ar
cs
max sentence length
full
uniform
ibm1encn
ibm1sym
Figure 4: Speed of various techniques for finding the optimal alignment.
the hook corresponding to the innermost max op-
erator in the equation can be visualized as follows:
Y
i k
u u2
with the expected language model state u2 ?hang-ing? outside the target language string.
The trick is generic to the control strategies of
actual parsing, because the hooks can be treated
as just another type of constituent. Building hooks
is like applying special unary rules on top of non-
hooks. In terms of of outside heuristic for hooks,
there is a slight difference from that for non-hooks:
h(i, j, u, v) =
[
max
s?S
Plm(s | v)
]
?
?
n<i,
n>j
max [hb(n), hf (n)]
That is, we do not need the backward-looking es-
timate for the left boundary word u.
5 Experiments
We tested the performance of our heuristics for
alignment on a Chinese-English newswire corpus.
Probabilities for the ITG model were trained using
Expectation Maximization on a corpus of 18,773
sentence pairs with a total of 276,113 Chinese
words and 315,415 English words. For EM train-
ing, we limited the data to sentences of no more
than 25 words in either language. Here we present
timing results for finding the Viterbi alignment of
longer sentences using this fixed translation model
with different heuristics. We compute alignments
on a total of 117 test sentences, which are broken
down by length as shown in Table 1.
Length # sentences
0-9 5
10?19 26
20?29 29
30?39 22
40?49 24
50?59 10
60 1
Table 1: Length of longer sentence in each pair
from test data.
method time speedup
full 815s ?
uniform 547s 1.4
ibm1encn 269s 3.0
ibm1sym 205s 3.9
Table 2: Total time for each alignment method.
Results are presented both in terms of time and
the number of arcs added to the chart before the
optimal parse is found. Full refers to exhaus-
tive parsing, that is, building a complete chart
with all n4 arcs. Uniform refers to a best-first
parsing strategy that expands the arcs with the
highest inside probability at each step, but does
not incorporate an estimate of the outside proba-
bility. Ibm1encn denotes our heuristic based on
IBM model 1, applied to translations from English
to Chinese, while ibm1sym applies the Model 1
heuristic in both translation directions and takes
the minimum. The factor by which times were de-
creased was found to be roughly constant across
different length sentences. The alignment times
for the entire test set are shown in Table 2, the
best heuristic is 3.9 times faster than exhaustive
229
 0
 2e+06
 4e+06
 6e+06
 8e+06
 1e+07
 1.2e+07
 1.4e+07
 0  5  10  15  20
# 
hy
pe
re
dg
es
input sentence length
BI-UNIFORM
BI-HOOK-UNIFORM
BI-HOOK-A*
BI-HOOK-A*-BEAM
 13.7
 13.8
 13.9
 14
 14.1
 14.2
 14.3
 14.4
 14.5
 0  100  200  300  400  500  600  700
bl
eu
average number of arcs (unit is 1k)
BI-HOOK-A*+BEAM
BI-CYK-BEAM
Figure 5: On the left, we compare decoding speed for uniform outside estimate best-first decoding with
and without the hook trick, as well as results using our heuristic (labeled A*) and with beam pruning
(which no longer produces optimal results). On the right, we show the relationship between computation
time and BLEU scores as the pruning threshold is varied for both A* search and bottom-up CYK parsing.
dynamic programming.
We did our ITG decoding experiments on the
LDC 2002 MT evaluation data set for translation
of Chinese newswire sentences into English. The
evaluation data set has 10 human translation refer-
ences for each sentence. There are a total of 371
Chinese sentences of no more than 20 words in
the data set. These sentences are the test set for
our different versions of ITG decoders using both
a bigram language model and a trigram language
model. We evaluate the translation results by com-
paring them against the reference translations us-
ing the BLEU metric. The word-for-word transla-
tion probabilities are from the translation model
of IBM Model 4 trained on a 160-million-word
English-Chinese parallel corpus using GIZA++.
The language model is trained on a 30-million-
word English corpus. The rule probabilities for
ITG are from the same training as in the alignment
experiments described above.
We compared the BLEU scores of the A* de-
coder and the ITG decoder that uses beam ratio
pruning at each stage of bottom-up parsing. In the
case of bigram-integrated decoding, for each input
word, the best 2 translations are put into the bag of
output words. In the case of trigram-integrated de-
coding, top 5 candidate words are chosen. The A*
decoder is guaranteed to find the Viterbi transla-
tion that maximizes the product of n-grams prob-
abilities, translation probabilities (including inser-
tions and deletions) and inversion rule probabili-
ties by choosing the right words and the right word
order subject to the ITG constraint.
Figure 5 (left) demonstrates the speedup ob-
Decoder Combinations BLEU
BI-UNIFORM 8.02M 14.26
BI-HOOK-A* 2.10M 14.26
BI-HOOK-A*-BEAM 0.40M 14.43
BI-CYK-BEAM 0.20M 14.14
Table 3: Decoder speed and BLEU scores for bi-
gram decoding.
Decoder Cbns BLEU
TRI-A*-BEAM(10?3) 213.4M 17.83
TRI-A*-BEAM(10?2) 20.7M 17.09
TRI-CYK-BEAM(10?3) 21.2M 16.86
Table 4: Results for trigram decoding.
tained through the hook trick, the heuristic, and
pruning, all based on A* search. Table 3 shows the
improvement of BLEU score after applying the A*
algorithm to find the optimal translation under the
model. Figure 5 (right) investigates the relation-
ship between the search effort and BLEU score for
A* and bottom-up CYK parsing, both with prun-
ing. Pruning for A* works in such a way that we
never explore a low probability hypothesis falling
out of a certain beam ratio of the best hypothesis
within the bucket of X[i, j, ?, ?], where ? means
any word. Table 4 shows results for trigram-
integrated decoding. However, due to time con-
straint, we have not explored time/performance
tradeoff as we did for bigram decoding.
The number of combinations in the table is
the average number of hyperedges to be explored
in searching, proportional to the total number of
230
computation steps.
6 Conclusion
A* search for Viterbi alignment selection under
ITG is efficient using IBM Model 1 as an outside
estimate. The experimental results indicate that
despite being a more relaxed word-for-word align-
ment model than ITG, IBM Model 1 can serve
as an efficient and reliable approximation of ITG
in terms of Viterbi alignment probability. This is
more true when we apply Model 1 to both trans-
lation directions and take the minimum of both.
We have also tried to incorporate estimates of bi-
nary rule probabilities to make the outside esti-
mate even sharper. However, the further improve-
ment was marginal.
We are able to find the ITG Viterbi translation
using our A* decoding algorithm with an outside
estimate that combines outside bigrams and trans-
lation probabilities for outside words. The hook
trick gave us a significant further speedup; we be-
lieve this to be the first demonstrated practical ap-
plication of this technique.
Interestingly, the BLEU score for the opti-
mal translations under the probabilistic model is
lower than we achieve with our best bigram-
based system using pruning. However, this sys-
tem makes use of the A* heuristic, and our
speed/performance curve shows that the heuris-
tic allows us to achieve higher BLEU scores with
the same amount of computation. In the case of
trigram integrated decoding, there is 1 point of
BLEU score improvement by moving from a typ-
ical CYK plus beam search decoder to a decoder
using A* plus beam search.
However, without knowing what words will ap-
pear in the output language, a very sharp outside
estimate to further bring down the number of com-
binations is difficult to achieve.
The brighter side of the move towards optimal
decoding is that the A* search strategy leads us
to the region of the search space that is close to
the optimal result, where we can more easily find
good translations.
Acknowledgments This work was supported
by NSF ITR IIS-09325646 and NSF ITR IIS-
0428020.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Eugene Charniak, Sharon Goldwater, and Mark John-
son. 1998. Edge-based best-first chart parsing. In
Proceedings of the Sixth Workshop on Very Large
Corpora.
Michael John Collins. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In 37th Annual Meeting of the
Association for Computational Linguistics.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005.
Machine translation as lexicalized parsing with
hooks. In International Workshop on Parsing Tech-
nologies (IWPT05), Vancouver, BC.
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact viterbi parse selection. In Pro-
ceedings of the 2003 Meeting of the North American
chapter of the Association for Computational Lin-
guistics (NAACL-03).
Franz Josef Och, Nicola Ueffing, and Herman Ney.
2001. An efficient a* search algorithm for statis-
tical machine translation. In Proceedings of the
ACL Workshop on Data-Driven Machine Transla-
tion, pages 55?62, Toulouse, France.
Ye-Yi Wang and Alex Waibel. 1997. Decoding algo-
rithm in statistical machine translation. In 35th An-
nual Meeting of the Association for Computational
Linguistics.
Dekai Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In 34th Annual Meeting
of the Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Richard Zens and Hermann Ney. 2003. A compara-
tive study on reordering constraints in statistical ma-
chine translation. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics, Sapporo, Japan.
Hao Zhang and Daniel Gildea. 2005. Stochastic lex-
icalized inversion transduction grammar for align-
ment. In Proceedings of the 43rd Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL-05), Ann Arbor, MI.
231
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 25?32,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Factorization of Synchronous Context-Free Grammars in Linear Time
Hao Zhang and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
Factoring a Synchronous Context-Free
Grammar into an equivalent grammar with
a smaller number of nonterminals in each
rule enables synchronous parsing algo-
rithms of lower complexity. The prob-
lem can be formalized as searching for the
tree-decomposition of a given permutation
with the minimal branching factor. In this
paper, by modifying the algorithm of Uno
and Yagiura (2000) for the closely related
problem of finding all common intervals
of two permutations, we achieve a linear
time algorithm for the permutation factor-
ization problem. We also use the algo-
rithm to analyze the maximum SCFG rule
length needed to cover hand-aligned data
from various language pairs.
1 Introduction
A number of recent syntax-based approaches to
statistical machine translation make use of Syn-
chronous Context Free Grammar (SCFG) as the un-
derlying model of translational equivalence. Wu
(1997)?s Inversion Transduction Grammar, as well
as tree-transformation models of translation such as
Yamada and Knight (2001), Galley et al (2004), and
Chiang (2005) all fall into this category.
A crucial question for efficient computation in ap-
proaches based on SCFG is the length of the gram-
mar rules. Grammars with longer rules can represent
a larger set of reorderings between languages (Aho
and Ullman, 1972), but also require greater compu-
tational complexity for word alignment algorithms
based on synchronous parsing (Satta and Peserico,
2005). Grammar rules extracted from large paral-
lel corpora by systems such as Galley et al (2004)
can be quite large, and Wellington et al (2006) ar-
gue that complex rules are necessary by analyzing
the coverage of gold-standard word alignments from
different language pairs by various grammars.
However, parsing complexity depends not only
on rule length, but also on the specific permutations
represented by the individual rules. It may be possi-
ble to factor an SCFG with maximum rule length
n into a simpler grammar with a maximum of k
nonterminals in any one rule, if not all n! permuta-
tions appear in the rules. Zhang et al (2006) discuss
methods for binarizing SCFGs, ignoring the non-
binarizable grammars; in Section 2 we discuss the
generalized problem of factoring to k-ary grammars
for any k and formalize the problem as permutation
factorization in Section 3.
In Section 4, we describe an O(k ? n) left-to-
right shift-reduce algorithm for analyzing permuta-
tions that can be k-arized. Its time complexity be-
comes O(n2) when k is not specified beforehand
and the minimal k is to be discovered. Instead of
linearly shifting in one number at a time, Gildea
et al (2006) employ a balanced binary tree as the
control structure, producing an algorithm similar in
spirit to merge-sort with a reduced time complex-
ity of O(n logn). However, both algorithms rely
on reduction tests on emerging spans which involve
redundancies with the spans that have already been
tested.
25
Uno and Yagiura (2000) describe a clever algo-
rithm for the problem of finding all common inter-
vals of two permutations in time O(n + K), where
K is the number of common intervals, which can
itself be ?(n2). In Section 5, we adapt their ap-
proach to the problem of factoring SCFGs, and show
that, given this problem definition, running time can
be improved to O(n), the optimum given the time
needed to read the input permutation.
The methodology in Wellington et al (2006) mea-
sures the complexity of word alignment using the
number of gaps that are necessary for their syn-
chronous parser which allows discontinuous spans
to succeed in parsing. In Section 6, we provide a
more direct measurement using the minimal branch-
ing factor yielded by the permutation factorization
algorithm.
2 Synchronous CFG and Synchronous
Parsing
We begin by describing the synchronous CFG for-
malism, which is more rigorously defined by Aho
and Ullman (1972) and Satta and Peserico (2005).
We adopt the SCFG notation of Satta and Peserico
(2005). Superscript indices in the right-hand side of
grammar rules:
X ? X(1)1 ...X(n)n , X
(pi(1))
pi(1) ...X
(pi(n))
pi(n)
indicate that the nonterminals with the same index
are linked across the two languages, and will eventu-
ally be rewritten by the same rule application. Each
Xi is a variable which can take the value of any non-
terminal in the grammar.
We say an SCFG is n-ary if and only if the max-
imum number of co-indexed nonterminals, i.e. the
longest permutation contained in the set of rules, is
of size n.
Given a synchronous CFG and a pair of input
strings, we can apply a generalized CYK-style bot-
tom up chart parser to build synchronous parse
trees over the string pair. Wu (1997) demonstrates
the case of binary SCFG parsing, where six string
boundary variables, three for each language as in
monolingual CFG parsing, interact with each other,
yielding an O(N6) dynamic programming algo-
rithm, where N is the string length, assuming the
two paired strings are comparable in length. For an
n-ary SCFG, the parsing complexity can be as high
as O(Nn+4). The reason is even if we binarize on
one side to maintain 3 indices, for many unfriendly
permutations, at most n + 1 boundary variables in
the other language are necessary.
The fact that this bound is exponential in the rule
length n suggests that it is advantageous to reduce
the length of grammar rules as much as possible.
This paper focuses on converting an SCFG to the
equivalent grammar with smallest possible maxi-
mum rule size. The algorithm processes each rule
in the input grammar independently, and determines
whether the rule can be factored into smaller SCFG
rules by analyzing the rule?s permutation pi.
As an example, given the input rule:
[X ? A(1)B(2)C(3)D(4)E(5)F (6)G(7),
X ? E(5)G(7)D(4)F (6)C(3)A(1)B(2) ] (1)
we consider the associated permutation:
(5, 7, 4, 6, 3, 1, 2)
We determine that this permutation can be fac-
tored into the following permutation tree:
(2,1)
(2,1)
(2,4,1,3)
5 7 4 6
3
(1,2)
1 2
We define permutation trees formally in the next
section, but note here that nodes in the tree corre-
spond to subsets of nonterminals that form a sin-
gle continuous span in both languages, as shown by
the shaded regions in the permutation matrix above.
This tree can be converted into a set of output rules
that are generatively equivalent to the original rule:
[X ? X(1)1 X
(2)
2 , X ? X
(2)
2 X
(1)
1 ]
[X1 ? A(1)B(2), X1 ? A(1)B(2) ]
[X2 ? C(1)X(2)3 , X2 ? X
(2)
3 C(1) ]
[X3 ? D(1)E(2)F (3)G(4),
X3 ? E(2)G(4)D(1)F (3) ]
where X1, X2 and X3 are new nonterminals used to
represent the intermediate states in which the syn-
chronous nodes are combined. The factorized gram-
mar is only larger than the original grammar by a
constant factor.
26
3 Permutation Trees
We define the notion of permutation structure in this
section. We define a permuted sequence as a per-
mutation of n (n ? 1) consecutive natural numbers.
A permuted sequence is said to be k-ary parsable
if either of the following conditions holds:
1. The permuted sequence only has one number.
2. It has more than one number and can be seg-
mented into k? (k ? k? ? 2) permuted se-
quences each of which is k-ary parsable, and
the k? subsequences are arranged in an order
identified by one of the k?! permutations of k?.
This is a recursive definition, and we call the cor-
responding recursive structure over the entire se-
quence a k-ary permutation tree.
Our goal is to find out the k-ary permutation tree
for a given permutation, where k is minimized.
4 Shift-reduce on Permutations
In this section, we present an O(n ? k) algorithm
which can be viewed as a need-to-be-optimized ver-
sion of the linear time algorithm to be presented in
the next section.
The algorithm is based on a shift-reduce parser,
which maintains a stack for subsequences that have
been discovered so far and loops over shift and re-
duce steps:
1. Shift the next number in the input permutation
onto the stack.
2. Go down the stack from the top to the bottom.
Whenever the top m subsequences satisfy the
partition property, which says the total length
of the m (k ? m ? 2) subsequences minus 1
is equal to the difference between the smallest
number and the largest number contained in the
m segments, make a reduction by gluing the
m segments into one subsequence and restart
reducing from the top of the new stack. Stop
when no reduction is possible.
3. If there are remaining numbers in the input per-
mutation, go to 1.
When we exit from the loop, if the height of the stack
is 1, the input permutation of n has been reduced to
Stack Input Operation
5, 7, 4, 6, 3, 1, 2 shift
5 7, 4, 6, 3, 1, 2 shift
5, 7 4, 6, 3, 1, 2 shift
5, 7, 4 6, 3, 1, 2 shift
5, 7, 4, 6 3, 1, 2 reduce by (2,4,1,3)
[4...7] 3, 1, 2 shift
[4...7], 3 1, 2 reduce by (2,1)
[3...7] 1, 2 shift
[3...7], 1 2 shift
[3...7], 1, 2 reduce by (1,2)
[3...7], [1...2] reduce by (2,1)
[1...7]
Table 1: The execution trace of the shift-reduce
parser on the input permutation 5, 7, 4, 6, 3, 1, 2.
a linear sequence of 1 to n, and parsing is success-
ful. Otherwise, the input permutation of n cannot be
parsed into a k-ary permutation tree.
An example execution trace of the algorithm is
shown in Table 1.
The partition property is a sufficient and neces-
sary condition for the top m subsequences to be re-
ducible. In order to check if the property holds, we
need to compute the sum of the lengths of subse-
quences under consideration and the difference be-
tween the largest and smallest number in the cov-
ered region. We can incrementally compute both
along with each step going down the stack. If m
is bounded by k, we need O(k) operations for each
item shifted onto the stack. So, the algorithm runs in
O(n ? k).
We might also wish to compute the minimum k
for which k-arization can be successful on an input
permutation of n. We can simply keep doing reduc-
tion tests for every possible top region of the stack
while going deeper in the stack to find the minimal
reduction. In the worst case, each time we go down
to the bottom of the increasingly higher stack with-
out a successful reduction. Thus, in O(n2), we can
find the minimum k-arization.
5 Linear Time Factorization
In this section, we show a linear time algorithm
which shares the left-to-right and bottom-up control
structure but uses more book-keeping operations to
reduce unnecessary reduction attempts. The reason
that our previous algorithm is asymptotically O(n2)
27
is that whenever a new number is shifted in, we have
to try out every possible new span ending at the new
number. Do we need to try every possible span? Let
us start with a motivating example. The permuted
sequence (5, 7, 4, 6) in Table 1 can only be reduced
as a whole block. However, in the last algorithm,
when 4 is shifted in, we make an unsuccessful at-
tempt for the span on (7, 4), knowing we are miss-
ing 5, which will not appear when we expand the
span no matter how much further to the right. Yet
we repeat the same mistake to try on 7 when 6 is
scanned in by attempting on (7, 4, 6). Such wasteful
checks result in the quadratic behavior of the algo-
rithm. The way the following algorithm differs from
and outperforms the previous algorithm is exactly
that it crosses out impossible candidates for reduc-
tions such as 7 in the example as early as possible.
Now we state our problem mathematically. We
define a function whose value indicates the re-
ducibility of each pair of positions (x, y) (1 ? x ?
y ? n):
f(x, y) = u(x, y)? l(x, y)? (y ? x)
where
l(x, y) = min
i?[x,y]
pi(i)
u(x, y) = max
i?[x,y]
pi(i)
l records the minimum of the numbers that are
permuted to from the positions in the region [x, y].
u records the maximum. Figure 1 provides the vi-
sualization of u, l, and f for the example permuta-
tion (5, 7, 4, 6, 3, 1, 2). u and l can be visualized as
stairs. u goes up from the right end to the left. l
goes down. f is non-negative, but not monotonic
in general. We can make a reduction on (x, y) if
and only if f(x, y) = 0. This is the mathemati-
cal statement of the partition property in step 2 of
the shift-reduce algorithm. u and l can be computed
incrementally from smaller spans to larger spans to
guarantee O(1) operations for computing f on each
new span of [x, y] as long as we go bottom up. In the
new algorithm, we will reduce the size of the search
space of candidate position pairs (x, y) to be linear
in n so that the whole algorithm is O(n).
The algorithm has two main ideas:
? We filter x?s to maintain the invariant that
f(x, y) (x ? y) is monotonically decreasing
with respect to x, over iterations on y (from 1
to n), so that any remaining values of x corre-
sponding to valid reductions are clustered at the
point where f tails off to zero. To put it another
way, we never have to test invalid reductions,
because the valid reductions have been sorted
together for us.
? We make greedy reductions as in the shift-
reduce algorithm.
In the new algorithm, we use a doubly linked list,
instead of a stack, as the data structure that stores
the candidate x?s to allow for more flexible main-
taining operations. The steps of the algorithm are as
follows:
1. Increase the left-to-right index y by one and ap-
pend it to the right end of the list.
2. Find the pivot x? in the list which is minimum
(leftmost) among x satisfying either u(x, y ?
1) < u(x, y) (exclusively) or l(x, y ? 1) >
l(x, y).
3. Remove those x?s that yield even smaller
u(x, y ? 1) than u(x?, y ? 1) or even larger
l(x, y ? 1) than l(x?, y ? 1). Those x?s must
be on the right of x? if they exist. They must
form a sub-list extending to the right end of the
original x list.
4. Denote the x which is immediately to the left
of x? as x?. Repeatedly remove all x?s such that
f(x, y) > f(x?, y) where x is at the left end of
the sub-list of x?s starting from x? extending to
the right.
5. Go down the pruned list from the right end, out-
put (x, y) until f(x, y) > 0. Remove x?s such
that f(x, y) = 0, sparing the smallest x which
is the leftmost among all such x?s on the list.
6. If there are remaining numbers in the input per-
mutation, go to 1.
The tricks lie in step 3 and step 4, where bad can-
didate x?s are filtered out. We use the following di-
agram to help readers understand the parts of x-list
that the two steps are filtering on.
28
x1, ..., x?,
step 4
? ?? ?
x?, ..., xi, ..., xj , ..., xk
? ?? ?
step 3
, y
The steps from 2 to 4 are the operations that main-
tain the monotonic invariant which makes the reduc-
tions in step 5 as trivial as performing output. The
stack-based shift-reduce algorithm has the same top-
level structure, but lacks steps 2 to 4 so that in step 5
we have to winnow the entire list. Both algorithms
scan left to right and examine potential reduction
spans by extending the left endpoint from right to
left given a right endpoint.
5.1 Example Execution Trace
An example of the algorithm?s execution is shown
in Figure 1. The evolution of u(x, y), l(x, y), and
f(x, y) is displayed for increasing y?s (from 2 to 7).
To identify reducible spans, we can check the plot of
f(x, y) to locate the (x, y) pairs that yield zero. The
pivots found by step 2 of the algorithm are marked
with ??s on the x-axis in the plot for u and l. The x?s
that are filtered out by step 3 or 4 are marked with
horizontal bars across. We want to point out the in-
teresting steps. When y = 3, x? = 1, x = 2 needs
to be crossed out by step 3 in the algorithm. When
y = 4, x? = 3, x = 3 itself is to be deleted by step 4
in the algorithm. x = 4 is removed at step 5 because
it is the right end in the first reduction. On the other
hand, x = 4 is also a bad starting point for future
reductions. Notice that we also remove x = 5 at
step 6, which can be a good starting point for reduc-
tions. But we exclude it from further considerations,
because we want left-most reductions.
5.2 Correctness
Now we explain why the algorithm works. Both al-
gorithms are greedy in the sense that at each scan
point we exhaustively reduce all candidate spans to
the leftmost possible point. It can be shown that
greediness is safe for parsing permutations.
What we need to show is how the monotonic in-
variant holds and is valid. Now we sketch the proof.
We want to show for all xi remaining on the list,
f(xi, y) ? f(xi+1, y). When y = 1, it is trivially
true. Now we do the induction on y step by case
analysis:
Case 1: If xi < xi+1 < x?, then f(xi, y) ?
f(xi, y ? 1) = ?1. The reason is if xi is on the
left of x?, both u(xi, y) and l(xi, y) are not changed
from the y ? 1-th step, so the only difference is that
y?xi has increased by one. Graphically, the f curve
extending to the left of x? shifts down a unit of 1. So,
the monotonic property still holds to the left of x?.
Case 2: If x? ? xi < xi+1, then f(xi, y) ?
f(xi, y ? 1) = c (c ? 0). The reason is that after
executing step 3 in the algorithm, the remaining xi?s
have either their u(xi, y) shifted up uniformly with
l(xi, y) being unchanged, or the symmetric case that
l(xi, y) is shifted down uniformly without changing
u(xi, y). In both cases, the difference between u and
l increases by at least one unit to offset the one unit
increase of y ? xi. The result is that the f curve ex-
tending from x? to the right shifts up or remains the
same.
Case 3: So the half curve of f on the left of x? is
shifting down and the half right curve on the right is
shifting up, making it necessary to consider the case
that xi is on the left and xi+1 on the right. Fortu-
nately, step 4 in the algorithm deals with this case
explicitly by cutting down the head of the right half
curve to smooth the whole curve into a monotoni-
cally decreasing one.
We still need one last piece for the proof, i.e., the
validity of pruning. Is it possible we winnow off
good x?s that will become useful in later stages of
y? The answer is no. The values we remove in step
3 and 4 are similar to the points indexing into the
second and third numbers in the permuted sequence
(5, 7, 4, 6). Any span starting from these two points
will not be reducible because the element 5 is miss-
ing.1
To summarize, we remove impossible left bound-
aries and keep good ones, resulting in the mono-
tonicity of f function which in turn makes safe
greedy reductions fast.
5.3 Implementation and Time Analysis
We use a doubly linked list to implement both the u
and l functions, where list element includes a span
of x values (shaded rectangles in Figure 1). Both
lists can be doubly linked with the list of x?s so that
1Uno and Yagiura (2000) prove the validity of step 3 and
step 4 rigorously.
29
we can access the u function and l function at O(1)
time for each x. At the same time, if we search for
x based on u or l, we can follow the stair functions,
skipping many intermediate x?s.
The total number of operations that occur at step
4 and step 5 is O(n) since these steps just involve
removing nodes on the x list, and only n nodes are
created in total over the entire algorithm. To find
x?, we scan back from the right end of u list or l
list. Due to step 3, each u (and l) element that we
scan over is removed at this iteration. So the total
number of operations accountable to step 2 and step
3 is bounded by the maximum number of nodes ever
created on the u and l lists, which is also n.
5.4 Related Work
Our algorithm is based on an algorithm for finding
all common intervals of two permutations (Uno and
Yagiura, 2000). The difference2 is in step 5, where
we remove the embedded reducible x?s and keep
only the leftmost one; their algorithm will keep all of
the reducible x?s for future considerations so that in
the example the number 3 will be able to involve in
both the reduction ([4?7], 3) and (3, [1?2]). In the
worst case, their algorithm will output a quadratic
number of reducible spans, making the whole algo-
rithm O(n2). Our algorithm is O(n) in the worst
case. We can also generate all common intervals by
transforming the permutation tree output by our al-
gorithm.
However, we are not the first to specialize the Uno
and Yagiura algorithm to produce tree structures for
permutations. Bui-Xuan et al (2005) reached a lin-
ear time algorithm in the definition framework of
PQ trees. PQ trees represent families of permuta-
tions that can be created by composing operations
of scrambling subsequences according to any per-
mutation (P nodes) and concatenating subsequences
in order (Q nodes). Our definition of permutation
tree can be thought of as a more specific version of a
PQ tree, where the nodes are all labeled with a spe-
cific permutation which is not decomposable.
2The original Uno and Yagiura algorithm also has the minor
difference that the scan point goes from right to left.
6 Experiments on Analyzing Word
Alignments
We apply the factorization algorithm to analyzing
word alignments in this section. Wellington et al
(2006) indicate the necessity of introducing discon-
tinuous spans for synchronous parsing to match up
with human-annotated word alignment data. The
number of discontinuous spans reflects the struc-
tural complexity of the synchronous rules that are
involved in building the synchronous trees for the
given alignments. However, the more direct and de-
tailed analysis would be on the branching factors of
the synchronous trees for the aligned data.
Since human-aligned data has many-to-one word
links, it is necessary to modify the alignments into
one-to-one. Wellington et al (2006) treat many-to-
one word links disjunctively in their synchronous
parser. We also commit to one of the many-one links
by extracting a maximum match (Cormen et al,
1990) from the bipartite graph of the alignment. In
other words, we abstract away the alternative links
in the given alignment while capturing the backbone
using the maximum number of word links.
We use the same alignment data for the five
language pairs Chinese/English, Romanian/English,
Hindi/English, Spanish/English, and French/English
(Wellington et al, 2006). In Table 2, we report the
number of sentences that are k-ary parsable but not
k ? 1-ary parsable for increasing k?s. Our analysis
reveals that the permutations that are accountable for
non-ITG alignments include higher order permuta-
tions such as (3, 1, 5, 2, 4), albeit sparsely seen.
We also look at the number of terminals the non-
binary synchronous nodes can cover. We are in-
terested in doing so, because this can tell us how
general these unfriendly rules are. Wellington et al
(2006) did a similar analysis on the English-English
bitext. They found out the majority of non-ITG
parsable cases are not local in the sense that phrases
of length up to 10 are not helpful in covering the
gaps. We analyzed the translation data for the five
language pairs instead. Our result differs. The right-
most column in Table 2 shows that only a tiny per-
cent of the non-ITG cases are significant in the sense
that we can not deal with them through phrases or
tree-flattening within windows of size 10.
30
y = 2:
1
*
1
2
2
3
3
4
4
5
5
6
6
7
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
7
6
f
x
y = 3:
1
*
1
2?
2
3
3
4
4
5
5
6
6
7
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
7
6
f
x
y = 4:
(1
1
2?
2
3?
*
3
4)
4
5
5
6
6
7
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
7
6
f
x
y = 5:
((1
*
1
2?
2
3?
3
4?)
4
5)
5
6
6
7
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
7
6
f
x
y = 6:
((1
*
1
2?
2
3?
3
4?)
4
5)
5
6
6
7
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
7
6
f
x
y = 7:
(((1
1
2?
2
3?
3
4?)
4
5)
5
(6
*
6
7))
7
u, l
x
1
0
2
1
3
2
4
3
5
4
6
5
7
6
f
x
Figure 1: Evolution of u(x, y), l(x, y), and f(x, y) as y goes from 2 to 7 for the permutation
(5, 7, 4, 6, 3, 1, 2). We use ? under the x-axis to indicate the x??s that are pivots in the algorithm. Use-
less x?s are crossed out. x?s that contribute to reductions are marked with either ( on its left or ) on its right.
For the f function, we use solid boxes to plot the values of remaining x?s on the list but also show the other
f values for completeness.
31
Branching Factor
1 2 4 5 6 7 10 ? 4 (and covering > 10 words)
Chinese/English 451 30 4 5 1 7(1.4%)
Romanian/English 195 4 0
Hindi/English 3 85 1 1 0
Spanish/English 195 4 1(0.5%)
French/English 425 9 9 3 1 6(1.3%)
Table 2: Distribution of branching factors for synchronous trees on various language pairs.
7 Conclusion
We present a linear time algorithm for factorizing
any n-ary SCFG rule into a set of k-ary rules where
k is minimized. The algorithm speeds up an easy-
to-understand shift-reduce algorithm, by avoiding
unnecessary reduction attempts while maintaining
the left-to-right bottom-up control structure. Em-
pirically, we provide a complexity analysis of word
alignments based on the concept of minimal branch-
ing factor.
References
Albert V. Aho and Jeffery D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume 1.
Prentice-Hall, Englewood Cliffs, NJ.
Binh Minh Bui-Xuan, Michel Habib, and Christophe
Paul. 2005. Revisiting T. Uno and M. Yagiura?s algo-
rithm. In The 16th Annual International Symposium
on Algorithms and Computation (ISAAC?05), pages
146?155.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Conference of the Association for
Computational Linguistics (ACL-05), pages 263?270,
Ann Arbor, Michigan.
Thomas H. Cormen, Charles E. Leiserson, and Ronald L.
Rivest. 1990. Introduction to algorithms. MIT Press,
Cambridge, MA.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of the Human Language Technology Confer-
ence/North American Chapter of the Association for
Computational Linguistics (HLT/NAACL).
Daniel Gildea, Giorgio Satta, and Hao Zhang. 2006. Fac-
toring synchronous grammars by sorting. In Proceed-
ings of the International Conference on Computational
Linguistics/Association for Computational Linguistics
(COLING/ACL-06) Poster Session, Sydney.
Giorgio Satta and Enoch Peserico. 2005. Some com-
putational complexity results for synchronous context-
free grammars. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 803?810, Vancouver, Canada,
October.
Takeaki Uno and Mutsunori Yagiura. 2000. Fast algo-
rithms to enumerate all common intervals of two per-
mutations. Algorithmica, 26(2):290?309.
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the
complexity of translational equivalence. In Proceed-
ings of the International Conference on Computational
Linguistics/Association for Computational Linguistics
(COLING/ACL-06).
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Conference of the Association for Com-
putational Linguistics (ACL-01), Toulouse, France.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of the Human Lan-
guage Technology Conference/North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL).
32
Proceedings of the Third Workshop on Statistical Machine Translation, pages 62?69,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Improved Tree-to-string Transducer for Machine Translation
Ding Liu and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We propose three enhancements to the tree-
to-string (TTS) transducer for machine trans-
lation: first-level expansion-based normaliza-
tion for TTS templates, a syntactic align-
ment framework integrating the insertion of
unaligned target words, and subtree-based n-
gram model addressing the tree decomposi-
tion probability. Empirical results show that
these methods improve the performance of a
TTS transducer based on the standard BLEU-
4 metric. We also experiment with semantic
labels in a TTS transducer, and achieve im-
provement over our baseline system.
1 Introduction
Syntax-based statistical machine translation
(SSMT) has achieved significant progress during
recent years, with two threads developing simul-
taneously: the synchronous parsing-based SSMT
(Galley et al, 2006; May and Knight, 2007) and
the tree-to-string (TTS) transducer (Liu et al,
2006; Huang et al, 2006). Synchronous SSMT
here denotes the systems which accept a source
sentence as the input and generate the translation
and the syntactic structure for both the source and
the translation simultaneously. Such systems are
sometimes also called TTS transducers, but in this
paper, TTS transducer refers to the system which
starts with the syntax tree of a source sentence and
recursively transforms the tree to the target language
based on TTS templates.
In synchronous SSMT, TTS templates are used
similar to the context free grammar used in the stan-
dard CYK parser, thus the syntax is part of the output
and can be thought of as a constraint on the transla-
tion process. In the TTS transducer, since the parse
tree is given, syntax can be thought of as an addi-
tional feature of the input to be used in the transla-
tion. The idea of synchronous SSMT can be traced
back to Wu (1997)?s Stochastic Inversion Transduc-
tion Grammars. A systematic method for extract-
ing TTS templates from parallel corpora was pro-
posed by Galley et al (2004), and later binarized
by Zhang et al (2006) for high efficiency and ac-
curacy. In the other track, the TTS transducer orig-
inated from the tree transducer proposed by Rounds
(1970) and Thatcher (1970) independently. Graehl
and Knight (2004) generalized the tree transducer
to the TTS transducer and introduced an EM al-
gorithm to estimate the probability of TTS tem-
plates based on a bilingual corpus with one side
parsed. Liu et al (2006) and Huang et al (2006)
then used the TTS transducer on the task of Chinese-
to-English and English-to-Chinese translation, re-
spectively, and achieved decent performance.
Despite the progress SSMT has achieved, it is
still a developing field with many problems un-
solved. For example, the word alignment com-
puted by GIZA++ and used as a basis to extract
the TTS templates in most SSMT systems has been
observed to be a problem for SSMT (DeNero and
Klein, 2007; May and Knight, 2007), due to the
fact that the word-based alignment models are not
aware of the syntactic structure of the sentences and
could produce many syntax-violating word align-
ments. Approaches have been proposed recently to-
wards getting better word alignment and thus bet-
ter TTS templates, such as encoding syntactic struc-
ture information into the HMM-based word align-
ment model DeNero and Klein (2007), and build-
62
ing a syntax-based word alignment model May
and Knight (2007) with TTS templates. Unfortu-
nately, neither approach reports end-to-end MT per-
formance based on the syntactic alignment. DeN-
ero and Klein (2007) focus on alignment and do not
present MT results, while May and Knight (2007)
takes the syntactic re-alignment as an input to an EM
algorithm where the unaligned target words are in-
serted into the templates and minimum templates are
combined into bigger templates (Galley et al, 2006).
Thus the improvement they reported is rather indi-
rect, leading us to wonder how much improvement
the syntactic alignment model can directly bring to a
SSMT system. Some other issues of SSMT not fully
addressed before are highlighted below:
1. Normalization of TTS templates. Galley et
al. (2006) mentioned that with only the mini-
mum templates extracted from GHKM (Galley
et al, 2004), normalizing the template proba-
bility based on its tree pattern ?can become ex-
tremely biased?, due to the fact that bigger tem-
plates easily get high probabilities. They in-
stead use a joint model where the templates are
normalized based on the root of their tree pat-
terns and show empirical results for that. There
is no systematic comparison of different nor-
malization methods.
2. Decomposition model of a TTS transducer
(or syntactic language model in synchronous
SSMT). There is no explicit modeling for the
decomposition of a syntax tree in the TTS
transducer (or the probability of the syntactic
tree in a synchronous SSMT). Most systems
simply use a uniform model (Liu et al, 2006;
Huang et al, 2006) or implicitly consider it
with a joint model producing both syntax trees
and the translations (Galley et al, 2006).
3. Use of semantics. Using semantic features in
a SSMT is a natural step along the way to-
wards generating more refined models across
languages. The statistical approach to semantic
role labeling has been well studied (Xue and
Palmer, 2004; Ward et al, 2004; Toutanova et
al., 2005), but there is no work attempting to
use such information in SSMT, to our limited
knowledge.
This paper proposes novel methods towards solv-
ing these problems. Specifically, we compare three
ways of normalizing the TTS templates based on the
tree pattern, the root of the tree pattern, and the first-
level expansion of the tree pattern respectively, in
the context of hard counting and EM estimation; we
present a syntactic alignment framework integrating
both the template re-estimation and insertion of un-
aligned target words; we use a subtree-based n-gram
model to address the decomposition of the syntax
trees in TTS transducer (or the syntactic language
model for synchronous SSMT); we use a statistical
classifier to label the semantic roles defined by Prop-
Bank (Palmer et al, 2005) and try different ways of
using the semantic features in a TTS transducer.
We chose the TTS transducer instead of syn-
chronous SSMT for two reasons. First, the decoding
algorithm for the TTS transducer has lower compu-
tational complexity, which makes it easier to inte-
grate a complex decomposition model. Second, the
TTS Transducer can be easily integrated with se-
mantic role features since the syntax tree is present,
and it?s not clear how to do this in a synchronous
SSMT system. The remainder of the paper will
focus on introducing the improved TTS transducer
and is organized as follows: Section 2 describes the
implementation of a basic TTS transducer; Section
3 describes the components of the improved TTS
transducer; Section 4 presents the empirical results
and Section 5 gives the conclusion.
2 A Basic Tree-to-string Transducer for
Machine Translation
The TTS transducer, as a generalization to the finite
state transducer, receives a tree structure as its input
and recursively applies TTS templates to generate
the target string. For simplicity, usually only one
state is used in the TTS transducer, i.e., a TTS tem-
plate will always lead to the same outcome wher-
ever it is used. A TTS template is composed of a
left-hand side (LHS) and a right-hand side (RHS),
where LHS is a subtree pattern and RHS is a se-
quence of the variables and translated words. The
variables in the RHS of a template correspond to the
bottom level non-terminals in the LHS?s subtree pat-
tern, and their relative order indicates the permuta-
tion desired at the point where the template is ap-
63
SQ
AUX  NP1 RB  VP2  ?3 
  Is            not
NP1 ?? VP2  ?3
Figure 1: A TTS Template Example
SQ
AUX  NP  1R  BP   V 
2?
3SQ 3AUX I?s NPn 31R ot ? s BP ?   V? s ? ?  NPn  ?? BP ?  V?
NN
? t ?
? ?
V? ? ?
BRN
ot ? ? IoI?? ? ?
3NP 3? ?  ? ? ? s 3NN ? t ? ss ? ?  ??3BP BRNns ? ?  BRNn3BRN ? IoI?? ? ? s ? ?  ??3V Vs ? ?  ?
    ??     ?? ?? ?
Figure 2: Derivation Example
plied to translate one language to another. The vari-
ables are further transformed and the recursive pro-
cess goes on until there are no variables left. The
formal description of a TTS transducer is described
in Graehl and Knight (2004), and our baseline ap-
proach follows the Extended Tree-to-String Trans-
ducer defined in (Huang et al, 2006). Figure 1 gives
an example of the English-to-Chinese TTS template,
which shows how to translate a skeleton YES/NO
question from English to Chinese. NP 1 and V P 2
are the variables whose relative position in the trans-
lation are determined by the template while their ac-
tual translations are still unknown and dependent on
the subtrees rooted at them; and the English words Is
and not are translated into the Chinese word MeiYou
in the context of the template. The superscripts at-
tached on the variables are used to distinguish the
non-terminals with identical names (if there is any).
Figure 2 shows the steps of transforming the English
sentence ?Is the job not finished ?? to the corre-
sponding Chinese.
For a given derivation (decomposition) of a syn-
tax tree, the translation probability is computed as
the product of the templates which generate both
the source syntax trees and the target translations.
In theory, the translation model should sum over
all possible derivations generating the target transla-
tion, but in practice, usually only the best derivation
is considered:
Pr(S|T,D?) =
?
t?D?
Weight(t)
Here, S denotes the target translation, T denotes the
source syntax tree, and D? denotes the best deriva-
tion of T . The implementation of a TTS trans-
ducer can be done either top down with memoiza-
tion to the visited subtrees (Huang et al, 2006), or
with a bottom-up dynamic programming (DP) algo-
rithm (Liu et al, 2006). This paper uses the lat-
ter approach, and the algorithm is sketched in Fig-
ure 3. For the baseline approach, only the translation
model and n-gram model for the target language are
used:
S? = argmax
S
Pr(T |S) = argmax
S
Pr(S)Pr(S|T )
Since the n-gram model tends to favor short transla-
tions, a penalty is added to the translation templates
with fewer RHS symbols than LHS leaf symbols:
Penalty(t) = exp(|t.RHS| ? |t.LHSLeaf |)
where |t.RHS| denotes the number of symbols in
the RHS of t, and |t.LHSLeaf | denotes the num-
ber of leaves in the LHS of t. The length penalty is
analogous to the length feature widely used in log-
linear models for MT (Huang et al, 2006; Liu et al,
2006; Och and Ney, 2004). Here we distribute the
penalty into TTS templates for the convenience of
DP, so that we don?t have to generate the N -best list
and do re-ranking. To speed up the decoding, stan-
dard beam search is used.
In Figure 3, BinaryCombine denotes the target-
size binarization (Huang et al, 2006) combination.
The translation candidates of the template?s vari-
ables, as well as its terminals, are combined pair-
wise in the order they appear in the RHS of the
template. fi denotes a combined translation, whose
probability is equal to the product of the probabili-
ties of the component translations, the probability of
the rule, the n-gram probability of connecting the
component translations, and the length penalty of
64
Match(v, t): the descendant tree nodes of v, which match the variables in template t
v.sk: the stack associated with tree node v
In(cj , fi): the translation candidate of cj which is chosen to combine fi
???????????????????????????????????
for all tree node v in bottom-up order do
for all template t applicable at v do
{c1, c2, ..., cl}=Match(v, t);
{f1, f2, ..., fm} = BinaryCombine(c1.sk, c2.sk, ..., cn.sk, t);
for i=1:m do
Pr(fi) =
?l
j=1Pr(In(cj , fi)) ? Weight(t)
? ? Lang(v, t, fi)? ? Penalty(t)?;
Add (fi, P r(fi)) to v.sk;
Prune v.sk;
Figure 3: Decoding Algorithm
the template. ?, ? and ? are the weights of the length
penalty, the translation model, and the n-gram lan-
guage model, respectively. Each state in the DP
chart denotes the best translation of a tree node with
a certain prefix and suffix. The length of the pre-
fix and the suffix is equal to the length of the n-gram
model minus one. Without the beam pruning, the de-
coding algorithm runs in O(N4(n?1)RPQ), where
N is the vocabulary size of the target language, n is
the length of the n-gram model, R is the maximum
number of templates applicable to one tree node, P
is the maximum number of variables in a template,
and Q is the number of tree nodes in the syntax tree.
The DP algorithm works for most systems in the pa-
per, and only needs to be slightly modified to en-
code the subtree-based n-gram model described in
Section 3.3.
3 Improved Tree-to-string Transducer for
Machine Translation
3.1 Normalization of TTS Templates
Given the story that translations are generated based
on the source syntax trees, the weight of the template
is computed as the probability of the target strings
given the source subtree:
Weight(t) =
#(t)
#(t? : LHS(t?) = LHS(t))
Such normalization, denoted here as TREE, is used
in most tree-to-string template-based MT systems
(Liu et al, 2007; Liu et al, 2006; Huang et al,
2006). Galley et al (2006) proposed an alteration
in synchronous SSMT which addresses the proba-
bility of both the source subtree and the target string
given the root of the source subtree:
Weight(t) =
#(t)
#(t? : root(t?) = root(t))
This method is denoted as ROOT. Here, we propose
another modification:
Weight(t) =
#(t)
#(t? : cfg(t?) = cfg(t))
(1)
cfg in Equation 1 denotes the first level expansion
of the source subtree and the method is denoted as
CFG. CFG can be thought of as generating both the
source subtree and the target string given the first
level expansion of the source subtree. TREE focuses
on the conditional probability of the target string
given the source subtree, ROOT focuses on the joint
probability of both the source subtree and the target
string, while CFG, as something of a compromise
between TREE and ROOT, hopefully can achieve a
combined effect of both of them. Compared with
TREE, CFG favors the one-level context-free gram-
mar like templates and gives penalty to the templates
bigger (in terms of the depth of the source subtree)
than that. It makes sense considering that the big
templates, due to their sparseness in the corpus, are
often assigned unduly large probabilities by TREE.
3.2 Syntactic Word Alignment
The idea of building a syntax-based word alignment
model has been explored byMay and Knight (2007),
with an algorithm working from the root tree node
down to the leaves, recursively replacing the vari-
ables in the matched tree-to-string templates until
there are no such variables left. The TTS tem-
plates they use are initially gathered using GHKM
65
1. Run GIZA++ to get the initial word alignment, use
GHKM to gather translation templates, and com-
pute the initial probability as their normalized fre-
quency.
2. Collect all the one-level subtrees in the training cor-
pus containing only non-terminals and create TTS
templates addressing all the permutations of the
subtrees? leaves if its spanning factor is not greater
than four, or only the monotonic translation tem-
plate if its spanning factor is greater than four. Col-
lect all the terminal rules in the form of A ? B
where A is one source word, B is the consecutive
target word sequence up to three words long, and
A, B occurs in some sentence pairs. These extra
templates are assigned a small probability 10?6.
3. Run the EM algorithm described in (Graehl and
Knight, 2004) with templates obtained in step 1 and
step 2 to re-estimate their probabilities.
4. Use the templates from step 3 to compute the viterbi
word alignment.
5. The templates not occurring in the viterbi deriva-
tion are ignored and the probability of the remain-
ing ones are re-normalized based on their frequency
in the viterbi derivation.
Figure 4: Steps generating the refined TTS templates
(Galley et al, 2004) with the word alignment com-
puted by GIZA++ and re-estimated using EM, ig-
noring the alignment from Giza++. The refined
word alignment is then fed to the expanded GHKM
(Galley et al, 2006), where the TTS templates will
be combined with the unaligned target words and
re-estimated in another EM framework. The syn-
tactic alignment proposed here shares the essence of
May and Knight?s approach, but combines the re-
estimation of the TTS templates and insertion of the
unaligned target words into a single EM framework.
The process is described in Figure 4. The inser-
tion of the unaligned target words is done implicitly
as we include the extra terminal templates in Fig-
ure 4, and the extra non-terminal templates ensure
that we can get a complete derivation forest in the
EM training. The last viterbi alignment step may
seem unnecessary given that we already have the
EM-estimated templates, but in experiments we find
that it produces better result by cutting off the noisy
(usually very big) templates resulting from the poor
alignments of GIZA++.
3.3 Tree Decomposition Model
A deficiency of the translation model for tree-to-
string transducer is that it cannot fully address
the decomposition probability of the source syntax
trees. Though we can say that ROOT/CFG implic-
itly includes the decomposition model, a more di-
rect and explicit modeling of the decomposition is
still desired. Here we propose a novel n-gram-like
model to solve this problem. The probability of a
decomposition (derivation) of a syntax tree is com-
puted as the product of the n-gram probability of
the decomposed subtrees conditioned on their ascen-
dant subtrees. The formal description of the model
is in Equation 2, whereD denotes the derivation and
PT (st) denotes the direct parent subtree of st.
Pr(D|T ) =
?
subtrees
st?D
Pr(st|PT (st), PT (PT (st)), ...)
(2)
Now, with the decomposition model added in, the
probability of the target string given the source syn-
tax tree is computed as:
Pr(S|T ) = Pr(D?|T )? Pr(S|T,D?)
To encode this n-gram probability of the subtrees
in the decoding process, we need to expand the
state space of the dynamic programming algorithm
in Figure 3, so that each state represents not only
the prefix/suffix of the partial translation, but also
the decomposition history of a tree node. For ex-
ample, with a bigram tree model, the states should
include the different subtrees in the LHS of the tem-
plates used to translate a tree node. With bigger n-
grams, more complex history information should be
encoded in the states, and this leads to higher com-
putational complexity. In this paper, we only con-
sider the tree n-gram up to size 2. It is not practi-
cal to search the full state space; instead, we mod-
ify the beam search algorithm in Figure 3 to encode
the decomposition history information. The mod-
ified algorithm for the tree bigram creates a stack
for each tree pattern occurring in the templates ap-
plicable to a tree node. This ensures that for each
tree node, the decompositions headed with differ-
ent subtrees have equal number of translation can-
didates surviving to the upper phase. The function
66
SQAUXXS NP1URBV21URU??V?3I sRnoB
t V? n?U? VB
? ? ? RU3t V? n?U? VB ? P ? Q? VR? Q
2V? s2VR
?U? ? oU? V3? s2V?? RU? B?U? Qs? 3? 2V?
2V? s? nsBQ? s? 3? s2V?
? ? ? ? P
t V? n?U? VB
? sR? U?QAV
? o? ? ? RVV? Q? RU?
Figure 5: Flow graph of the system with all components
integrated
BinaryCombine is almost the same as in Figure 3,
except that the translation candidates (states) of each
tree node are grouped according to their associated
subtrees. The bigram probabilities of the subtrees
can be easily computed with the viterbi derivation in
last subsection. Also, a weight should be assigned
to this component. This tree n-gram model can be
easily adapted and used in synchronous SSMT sys-
tems such as May and Knight (2007), Galley et al
(2006). The flow graph of the final system with all
the components integrated is shown in Figure 5.
3.4 Use of Semantic Roles
Statistical approaches to MT have gone through
word-based systems, phrase-based systems, and
syntax-based systems. The next generation would
seem to be semantic-based systems. We use Prop-
Bank (Palmer et al, 2005) as the semantic driver in
our TTS transducer because it is built upon the same
corpus (the Penn Treebank) used to train the statisti-
cal parser, and its shallow semantic roles are more
easily integrated into a TTS transducer. A Max-
Entropy classifier, with features following Xue and
Palmer (2004) andWard et al (2004), is used to gen-
erate the semantic roles for each verb in the syntax
trees. We then replace the syntactic labels with the
semantic roles so that we have more general tree la-
bels, or combine the semantic roles with the syntac-
tic labels to generate more refined tree node labels.
Though semantic roles are associated with the verbs,
it is not feasible to differentiate the roles of different
NP VP VP NP
(S NP-agent VP) 0.983 0.017
(S NP-patient VP) 0.857 0.143
Table 1: The TREE-based weights of the skeleton tem-
plates with NP in different roles
verbs due to the data sparseness problem. If some
tree nodes are labeled different roles for different
verbs, those semantic roles will be ignored.
A simple example demonstrating the need for se-
mantics in the TTS transducer is that in English-
Chinese translation, the NP VP skeleton phrase is
more likely to be inverted when NP is in a patient
role than when it is in an agent role. Table 1 shows
the TREE-based weights of the 4 translation tem-
plates, computed based on our training corpus. This
shows that the difference caused by the roles of NP
is significant.
4 Experiment
We used 74,597 pairs of English and Chinese sen-
tences in the FBIS data set as our experimental
data, which are further divided into 500 test sen-
tence pairs, 500 development sentence pairs and
73597 training sentence pairs. The test set and de-
velopment set are selected as those sentences hav-
ing fewer than 25 words on the Chinese side. The
translation is from English to Chinese, and Char-
niak (2000)?s parser, trained on the Penn Treebank,
is used to generate the syntax trees for the English
side. The weights of the MT components are op-
timized based on the development set using a grid-
based line search. The Chinese sentence from the se-
lected pair is used as the single reference to tune and
evaluate the MT system with word-based BLEU-4
(Papineni et al, 2002). Huang et al (2006) used
character-based BLEU as a way of normalizing in-
consistent Chinese word segmentation, but we avoid
this problem as the training, development, and test
data are from the same source.
4.1 Syntax-Based System
The decoding algorithm described in Figure 3 is
used with the different normalization methods de-
scribed in Section 3.1 and the results are summa-
rized in Table 2. The TTS templates are extracted
using GHKM based on the many-to-one alignment
67
Baseline Syntactic Alignment Subtree bigram
dev test dev test dev test
TREE 12.29 8.90 13.25 9.65 14.84 10.61
ROOT 12.41 9.66 13.72 10.16 14.24 10.66
CFG 13.27 9.69 14.32 10.29 15.30 10.99
PHARAOH 9.04 7.84
Table 2: BLEU-4 scores of various systems with the syntactic alignment and subtree bigram improvements added
incrementally.
from Chinese to English obtained from GIZA++.
We have tried using alignment in the reverse direc-
tion and the union of both directions, but neither
of them is better than the Chinese-to-English align-
ment. The reason, based on the empirical result,
is simply that the Chinese-to-English alignments
lead to the maximum number of templates using
GHKM. A modified Kneser-Ney bigram model of
the Chinese sentence is trained using SRILM (Stol-
cke, 2002) using the training set. For comparison,
results for Pharaoh (Koehn, 2004), trained and tuned
under the same condition, are also shown in Table 2.
The phrases used in Pharaoh are extracted as the pair
of longest continuous spans in English and Chinese
based on the union of the alignments in both direc-
tion. We tried using alignments of different direc-
tions with Pharaoh, and find that the union gives
the maximum number of phrase pairs and the best
BLEU scores. The results show that the TTS trans-
ducers all outperform Pharaoh, and among them, the
one with CFG normalization works better than the
other two.
We tried the three normalization methods in the
syntactic alignment process in Figure 4, and found
that the initialization (step 1) and viterbi alignment
(step 3 and 4) based on the least biased model
ROOT gave the best performance. Table 2 shows
the results with the final template probability re-
normalized (step 5) using TREE, ROOT and CFG
respectively. We can see that the syntactic align-
ment brings a reasonable improvement for the TTS
transducer no matter what normalization method is
used. To test the effect of the subtree-based n-
gram model, SRILM is used to compute a modi-
fied Kneser-Ney bigram model for the subtree pat-
terns used in the viterbi alignment. The last 3 lines
in Table 2 show the improved results by further in-
corporating the subtree-based bigram model. We
can see that the difference of the three normaliza-
tion methods is lessened and TREE, the weakest nor-
malization in terms of addressing the decomposition
probability, gets the biggest improvement with the
subtree-based bigram model added in.
4.2 Semantic-Based System
Following the standard division, our max-entropy
based SRL classifier is trained and tuned using sec-
tions 2-21 and section 24 of PropBank, respectively.
The F-score we achieved on section 23 is 88.70%.
We repeated the experiments in last section with
the semantic labels generated by the SRL classi-
fier. Table 3 shows the results, comparing the non-
semantic-based systems with similar systems us-
ing the refined and general semantic labels, respec-
tively. Unfortunately, semantic based systems do
not always outperform the syntactic based systems.
We can see that for the baseline systems based on
TREE and ROOT, semantic labels improve the re-
sults, while for the other systems, they are not re-
ally better than the syntactic labels. Our approach
to semantic roles is preliminary; possible improve-
ments include associating role labels with verbs and
backing off to the syntactic-label based models from
semantic-label based TTS templates. In light of our
results, we are optimistic that more sophisticated
use of semantic features can further improve a TTS
transducer?s performance.
5 Conclusion
This paper first proposes three enhancements to the
TTS transducer: first-level expansion-based normal-
ization for TTS templates, a syntactic alignment
framework integrating the insertion of unaligned tar-
get words, and a subtree-based n-gram model ad-
dressing the tree decomposition probability. The ex-
periments show that the first-level expansion-based
68
No Semantic Labels Refined Labels General Labels
Syntactic Subtree Syntactic Subtree Syntactic Subtree
Baseline Alignment Bigram Baseline Alignment Bigram Baseline Alignment Bigram
TREE 8.90 9.65 10.61 9.40 10.25 10.42 9.40 10.02 10.47
ROOT 9.66 10.16 10.66 9.89 10.32 10.43 9.82 10.17 10.42
CFG 9.69 10.29 10.99 9.66 10.16 10.33 9.58 10.25 10.59
Table 3: BLEU-4 scores of semantic-based systems on test data. As in Table 2, the syntactic alignment and subtree
bigram improvements are added incrementally within each condition.
normalization for TTS templates is better than the
root-based one and the tree-based one; the syntactic
alignment framework and the n-gram based tree de-
composition model both improve a TTS transducer?s
performance. Our experiments using PropBank se-
mantic roles in the TTS transducer show that the ap-
proach has potential, improving on our baseline sys-
tem. However, adding semantic roles does not im-
prove our best TTS system.
References
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In The Proceedings of the North American
Chapter of the Association for Computational Linguis-
tics, pages 132?139.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of ACL-07, pages 17?24.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of NAACL-04.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING/ACL-06, pages 961?968, July.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proceedings of NAACL-04.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
Conference of the Association for Machine Translation
in the Americas (AMTA), Boston, MA.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In The Sixth Conference of the Association for
Machine Translation in the Americas, pages 115?124.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING/ACL-06, Sydney,
Australia, July.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of ACL-07, Prague.
J. May and K. Knight. 2007. Syntactic re-alignment
models for machine translation. In Proceedings of
EMNLP.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4).
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of ACL-
02.
William C. Rounds. 1970. Mappings and grammars on
trees. Mathematical Systems Theory, 4(3):257?287.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In International Conference on Spo-
ken Language Processing, volume 2, pages 901?904.
J. W. Thatcher. 1970. Generalized2 sequential machine
maps. J. Comput. System Sci., 4:339?367.
Kristina Toutanova, Aria Haghighi, and Christopher
Manning. 2005. Joint learning improves semantic role
labeling. In Proceedings of ACL-05, pages 589?596.
Wayne Ward, Kadri Hacioglu, James Martin, , and Dan
Jurafsky. 2004. Shallow semantic parsing using sup-
port vector machines. In Proceedings of EMNLP.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings of
EMNLP.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of NAACL-06, pages 256?
263.
69
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 89?98,
Paris, October 2009. c?2009 Association for Computational Linguistics
Weight pushing and binarization for fixed-grammar parsing
Matt Post and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We apply the idea of weight pushing
(Mohri, 1997) to CKY parsing with fixed
context-free grammars. Applied after
rule binarization, weight pushing takes the
weight from the original grammar rule and
pushes it down across its binarized pieces,
allowing the parser to make better prun-
ing decisions earlier in the parsing pro-
cess. This process can be viewed as gen-
eralizing weight pushing from transduc-
ers to hypergraphs. We examine its ef-
fect on parsing efficiency with various bi-
narization schemes applied to tree sub-
stitution grammars from previous work.
We find that weight pushing produces dra-
matic improvements in efficiency, espe-
cially with small amounts of time and with
large grammars.
1 Introduction
Fixed grammar-parsing refers to parsing that em-
ploys grammars comprising a finite set of rules
that is fixed before inference time. This is in
contrast to markovized grammars (Collins, 1999;
Charniak, 2000), variants of tree-adjoining gram-
mars (Chiang, 2000), or grammars with wildcard
rules (Bod, 2001), all of which allow the con-
struction and use of rules not seen in the training
data. Fixed grammars must be binarized (either
explicitly or implicitly) in order to maintain the
O(n3|G|) (n the sentence length, |G| the grammar
size) complexity of algorithms such as the CKY
algorithm.
Recently, Song et al (2008) explored different
methods of binarization of a PCFG read directly
from the Penn Treebank (the Treebank PCFG),
showing that binarization has a significant effect
on both the number of rules and new nontermi-
nals introduced, and subsequently on parsing time.
This variation occurs because different binariza-
tion schemes produce different amounts of shared
rules, which are rules produced during the bina-
rization process from more than one rule in the
original grammar. Increasing sharing reduces the
amount of state that the parser must explore. Bina-
rization has also been investigated in the context of
parsing-based approaches to machine translation,
where it has been shown that paying careful atten-
tion to the binarization scheme can produce much
faster decoders (Zhang et al, 2006; Huang, 2007;
DeNero et al, 2009).
The choice of binarization scheme will not af-
fect parsing results if the parser is permitted to ex-
plore the whole search space. In practice, how-
ever, this space is too large, so parsers use prun-
ing to discard unlikely hypotheses. This presents
a problem for bottom-up parsing algorithms be-
cause of the way the probability of a rule is dis-
tributed among its binarized pieces: The standard
approach is to place all of that probability on the
top-level binarized rule, and to set the probabilities
of lower binarized pieces to 1.0. Because these
rules are reconstructed from the bottom up, prun-
ing procedures do not have a good estimate of the
complete cost of a rule until the entire original rule
has been reconstructed. It is preferable to have this
information earlier on, especially for larger rules.
In this paper we adapt the technique of weight
pushing for finite state transducers (Mohri, 1997)
to arbitrary binarizations of context-free grammar
rules. Weight pushing takes the probability (or,
more generally, the weight) of a rule in the origi-
nal grammar and pushes it down across the rule?s
binarized pieces. This helps the parser make bet-
89
ter pruning decisions, and to make them earlier in
the bottom-up parsing process. We investigate this
algorithm with different binarization schemes and
grammars, and find that it improves the time vs.
accuracy tradeoff for parsers roughly proportion-
ally to the size of the grammar being binarized.
This paper extends the work of Song et al
(2008) in three ways. First, weight pushing fur-
ther reduces the amount of time required for pars-
ing. Second, we apply these techniques to Tree
Substitution Grammars (TSGs) learned from the
Treebank, which are both larger and more accu-
rate than the context-free grammar read directly
from the Treebank.1 Third, we examine the inter-
action between binarization schemes and the in-
exact search heuristic of beam-based and k-best
pruning.
2 Weight pushing
2.1 Binarization
Not all binarization schemes are equivalent in
terms of efficiency of representation. Consider the
grammar in the lefthand column of Figure 1 (rules
1 and 2). If this grammar is right-binarized or
left-binarized, it will produce seven rules, whereas
the optimal binarization (depicted) produces only
5 rules due to the fact that two of them are shared.
Since the complexity of parsing with CKY is a
function of the grammar size as well as the input
sentence length, and since in practice parsing re-
quires significant pruning, having a smaller gram-
mar with maximal shared substructure among the
rules is desirable.
We investigate two kinds of binarization in this
paper. The first is right binarization, in which non-
terminal pairs are collapsed beginning from the
two rightmost children and moving leftward. The
second is a greedy binarization, similar to that of
Schmid (2004), in which the most frequently oc-
curring (grammar-wide) nonterminal pair is col-
lapsed in turn, according to the algorithm given in
Figure 2.
Binarization must ensure that the product of the
probabilities of the binarized pieces is the same as
that of the original rule. The easiest way to do
this is to assign each newly-created binarized rule
a probability of 1.0, and give the top-level rule the
complete probability of the original rule. In the
following subsection, we describe a better way.
1The mean rule rank in a Treebank PCFG is 2.14, while
the mean rank in our sampled TSG is 8.51. See Table 1.
NP
a JJ NN NN PP
??JJ:NN?:NN?
?JJ:NN? NN
JJ NN
?a:??JJ:NN?:NN??
a
PP
NPE
C
B
A
1Rule
NP
the JJ NN NN
Rule 2
??JJ:NN?:NN?
?JJ:NN? NN
JJ NN
NP
the
A
B
D
Figure 1: A two-rule grammar. The greedy
binarization algorithm produces the binarization
shown, with the shared structure highlighted. Bi-
narized rules A, B, and C are initially assigned
a probability of 1.0, while rules D and E are as-
signed the original probabilities of rules 2 and 1,
respectively.
2.2 Weight pushing
Spreading the weight of an original rule across
its binarized pieces is complicated by sharing,
because of the constraint that the probability of
shared binarized pieces must be set so that the
product of their probabilities is the same as the
original rule, for each rule the shared piece partici-
pates in. Mohri (1997) introduced weight pushing
as a step in the minimization of weighted finite-
state transducers (FSTs), which addressed a sim-
ilar problem for tasks employing finite-state ma-
chinery. At a high level, weight pushing moves
the weight of a path towards the initial state, sub-
ject to the constraint that the weight of each path
in the FST is unchanged. To do weight pushing,
one first computes for each state q in the trans-
ducer the shortest distance d(q) to any final state.
Let ?(q, a) be the state transition function, deter-
ministically transitioning on input a from state q to
state ?(q, a). Pushing adjusts the weight of each
edge w(e) according to the following formula:
w?(e) = d(q)?1 ? w(e)? d(?(q, a)) (1)
Mohri (1997, ?3.7) and Mohri and Riley (2001)
discuss how these operations can be applied us-
ing various semirings; in this paper we use the
(max,?) semiring. The important observation for
our purposes is that pushing can be thought of as a
sequence of local operations on individual nodes
90
1: function GREEDYBINARIZE(P )
2: while RANK(P ) > 2 do
3: ? := UPDATECOUNTS(P )
4: for each rule X ? x1x2 ? ? ?xr do
5: b := argmaxi?(2???r) ?[xi?1, xi]
6: l := ?xb?1 : xb?
7: add l ? xb?1xb to P
8: replace xb?1xb with l in rule
9: function UPDATECOUNTS(P )
10: ? := {} ? a dictionary
11: for each rule X ? x1x2 ? ? ?xr ? P do
12: for i ? (2 ? ? ? r) do
13: ?[xi?1, xi]++
return ?
Figure 2: A greedy binarization algorithm. The
rank of a grammar is the rank of its largest rule.
Our implementation updates the counts in ? more
efficiently, but we present it this way for clarity.
q, shifting a constant amount of weight d(q)?1
from q?s outgoing edges to its incoming edges.
Klein and Manning (2003) describe an encod-
ing of context-free grammar rule binarization that
permits weight pushing to be applied. Their ap-
proach, however, works only with left or right bi-
narizations whose rules can be encoded as an FST.
We propose a form of weight pushing that works
for arbitrary binarizations. Weight pushing across
a grammar can be viewed as generalizing push-
ing from weighted transducers to a certain kind of
weighted hypergraph. To begin, we use the fol-
lowing definition of a hypergraph:
Definition. A hypergraph H is a tuple
?V,E, F,R?, where V is a set of nodes, E is a
set of hyperedges, F ? V is a set of final nodes,
and R is a set of permissible weights on the hy-
peredges. Each hyperedge e ? E is a triple
?T (e), h(e), w(e)?, where h(e) ? V is its head
node, T (e) is a sequence of tail nodes, and w(e) is
its weight.
We can arrange the binarized rules of Figure 1
into a shared hypergraph forest (Figure 3), with
nodes as nonterminals and binarized rules as hy-
peredges. We distinguish between final and non-
final nodes and hyperedges. Nonfinal nodes are
those in V ?F . Nonfinal hyperdges ENF are those
in {e : h(e) ? V ? F}, that is, all hyperedges
whose head is a nonfinal node. Because all nodes
introduced by our binarization procedure expand
deterministically, each nonfinal node is the head
of no more than one such hyperedge. Initially, all
0.6/1.0
0.4/0.67?
1.0/0.6
1.0/1.0
1.0/1.0
??JJ:NN?:NN?
?JJ:NN? NN
JJ NN
NP
the
?a:??JJ:NN?:NN??
a
PP
Figure 3: The binarized rules of Figure 1 arranged
in a shared hypergraph forest. Each hyperedge is
labeled with its weight before/after pushing.
nonfinal hyperedges have a probability of 1, and fi-
nal hyperedges have a probability equal to the that
of the original unbinarized rule. Each path through
the forest exactly identifies a binarization of a rule
in the original grammar, and hyperpaths overlap
where binarized rules are shared.
Weight pushing in this hypergraph is similar to
weight pushing in a transducer. We consider each
nonfinal node v in the graph and execute a local
operation that moves weight in some way from the
set of edges {e : v ? T (e)} (v?s outgoing hyper-
edges) to the edge eh for which v = h(e) (v?s
incoming hyperedge).
A critical difference from pushing in trans-
ducers is that a node in a hyperpath may be
used more than once. Consider adding the rule
NP?JJ NN JJ NN to the binarized two-rule gram-
mar we have been considering. Greedy binariza-
tion could2 binarize it in the following manner
NP ? ?JJ:NN? ?JJ:NN?
?JJ:NN? ? JJ NN
which would yield the hypergraph in Figure 4. In
order to maintain hyperpath weights, a pushing
procedure at the ?JJ:NN? node must pay attention
to the number of times it appears in the set of tail
nodes of each outgoing hyperedge.
2Depending on the order in which the argmax variable i
of Line 5 from the algorithm in Figure 2 is considered. This
particular binarization would not have been produced if the
values 2 . . . r were tested sequentially.
91
0.6/1.0
0.3/0.5
1.0/0.6
1.0/1.0
1.0/1.0
0.1/0.27?
??JJ:NN?:NN?
?JJ:NN? NN
JJ NN
NP
the
?a:??JJ:NN?:NN??
a
PP
Figure 4: A hypergraph containing a hyperpath
representing a rule using the same binarized piece
twice. Hyperedge weights are again shown be-
fore/after pushing.
With these similarities and differences in mind,
we can define the local weight pushing procedure.
For each nonfinal node v in the hypergraph, we
define eh as the edge for which h(e) = v (as be-
fore), P = {e : v ? T (e)} (the set of outgo-
ing hyperedges), and c(v, T (e)) as the number of
times v appears in the sequence of tail nodes T (e).
The minimum amount of probability available for
pushing is then
max{ c(v,T (e))?w(e) : e ? P} (2)
This amount can then be multiplied into w(eh) and
divided out of each edge e ? P . This max is a
lower bound because we have to ensure that the
amount of probability we divide out of the weight
of each outgoing hyperedge is at least as large as
that of the maximum weight.
While finite state transducers each have a
unique equivalent transducer on which no further
pushing is possible, defined by Equation 1, this is
not the case when operating on hypergraphs. In
this generalized setting, the choice of which tail
nodes to push weight across can result in differ-
ent final solutions. We must define a strategy for
choosing among sequences of pushing operations,
and for this we now turn to a discussion of the
specifics of our algorithm.
2.3 Algorithm
We present two variants. Maximal pushing, analo-
gous to weight pushing in weighted FSTs, pushes
the original rule?s weight down as far as pos-
sible. Analysis of interactions between pruning
1: function DIFFUSEWEIGHTS(PBIN ,?)
2: R := bottom-up sort of PBIN
3: for each rule r ? R do
4: r.pr := max{ c(r,p)?p.pr : p ? ?(r)}
5: for each rule p ? ?(r) do
6: p.pr := p.pr/r.prc(r,p)
Figure 6: Maximal weight pushing algorithm ap-
plied to a binarized grammar, PBIN . ? is a dictio-
nary mapping from an internal binary rule to a list
of top-level binary rules that it appeared under.
and maximal pushing discovered situations where
maximal pushing resulted in search error (see
?4.2). To address this, we also discuss nthroot
pushing, which attempts to distribute the weight
more evenly across its pieces, by taking advantage
of the fact that Equation 2 is a lower bound on the
amount of probability available for pushing.
The algorithm for maximal pushing is listed
in Figure 6, and works in the following manner.
When binarizing we maintain, for each binarized
piece, a list of all the original rules that share
it. We then distribute that original rule?s weight
by considering each of these binarized pieces in
bottom-up topological order and setting the prob-
ability of the piece to the maximum (remaining)
probability of these parents. This amount is then
divided out of each of the parents, and the process
continues. See Figure 5 for a depiction of this pro-
cess. Note that, although we defined pushing as a
local operation between adjacent hyperedges, it is
safe to move probability mass from the top-level
directly to the bottom (as we do here). Intuitively,
we can imagine this as a series of local pushing
operations on all intervening nodes; the end result
is the same.
For nthroot pushing, we need to maintain a dic-
tionary ? which records, for each binary piece, the
rank (number of items on the rule?s righthand side)
of the original rule it came from. This is accom-
plished by replacing line 4 in Figure 6 with
r.pr := max{ (?(p)?1)?c(r,p)?p.pr : p ? ?(r)}
Applying weight pushing to a binarized PCFG
results in a grammar that is not a PCFG, be-
cause rule probabilities for each lefthand side
no longer sum to one. However, the tree dis-
tribution, as well as the conditional distribution
P(tree|string) (which are what matter for parsing)
are unchanged. To show this, we argue from
the algorithm in Figure 6, demonstrating that, for
92
step A B C D E
0 1.0 1.0 1.0 x y
1 max(x, y) ? ? xmax(x,y) ymax(x,y)
2 ? max(z1,D, z1,E) ? z1,Dmax(z1,D,z1,E)
z1,D
max(z1,D,z1,E)
3 ? ? max(z2,D, z2,E) z2,Dmax(z2,D,z2,E)
z2,E
max(z2,D,z2,E)
4 ? ? ? ? ?
Figure 5: Stepping through the maximal weight pushing algorithm for the binarized grammar in Figure 1.
Rule labels A through E were chosen so that the binarized pieces are sorted in topological order. A (?)
indicates a rule whose value has not changed from the previous step, and the value zr,c denotes the value
in row r column c.
each rule in the original grammar, its probability
is equal to the product of the probabilities of its
pieces in the binarized grammar. This invariant
holds at the start of the algorithm (because the
probability of each original rule was placed en-
tirely at the top-level rule, and all other pieces re-
ceived a probability of 1.0) and is also true at the
end of each iteration of the outer loop. Consider
this loop. Each iteration considers a single binary
piece (line 3), determines the amount of probabil-
ity to claim from the parents that share it (line 4),
and then removes this amount of weight from each
of its parents (lines 5 and 6). There are two impor-
tant considerations.
1. A binarized rule piece may be used more than
once in the reconstruction of an original rule;
this is important because we are assigning
probabilities to binarized rule types, but rule
reconstruction makes use of binarized rule to-
kens.
2. Multiplying together two probabilities results
in a lower number: when we shift weight p
from the parent rule to (n instances of) a bi-
narized piece beneath it, we are creating a
new set of probabilities pc and pp such that
pnc ? pp = p, where pc is the weight placed on
the binarized rule type, and pp is the weight
we leave at the parent. This means that we
must choose pc from the range [p, 1.0].3
In light of these considerations, the weight re-
moved from each parent rule in line 6 must be
greater than or equal to each parent sharing the
binarized rule piece. To ensure this, line 4 takes
3The upper bound of 1.0 is set to avoid assigning a nega-
tive weight to a rule.
the maximum of the c(r, p)th root of each parent?s
probability, where c(r, p) is the number of times
binarized rule token r appears in the binarization
of p.
Line 4 breaks the invariant, but line 6 restores it
for each parent rule the current piece takes part in.
From this it can be seen that weight pushing does
not change the product of the probabilities of the
binarized pieces for each rule in the grammar, and
hence the tree distribution is also unchanged.
We note that, although Figures 3 and 4 show
only one final node, any number of final nodes can
appear if binarized pieces are shared across differ-
ent top-level nonterminals (which our implemen-
tation permits and which does indeed occur).
3 Experimental setup
We present results from four different grammars:
1. The standard Treebank probabilistic context-
free grammar (PCFG).
2. A ?spinal? tree substitution grammar (TSG),
produced by extracting n lexicalized subtrees
from each length n sentence in the training
data. Each subtree is defined as the sequence
of CFG rules from leaf upward all sharing the
same lexical head, according to the Mager-
man head-selection rules (Collins, 1999). We
detach the top-level unary rule, and add in
counts from the Treebank CFG rules.
3. A ?minimal subset? TSG, extracted and then
refined according to the process defined in
Bod (2001). For each height h, 2 ? h ? 14,
400,000 subtrees are randomly sampled from
the trees in the training data, and the counts
93
rank
grammar # rules median mean max
PCFG 46K 1 2.14 51
spinal 190K 3 3.36 51
sampled 804K 8 8.51 70
minimal 2,566K 10 10.22 62
Table 1: Grammar statistics. A rule?s rank is the
number of symbols on its right-hand side.
grammar unbinarized right greedy
PCFG 46K 56K 51K
spinal 190K 309K 235K
sampled 804K 3,296K 1,894K
minimal 2,566K 15,282K 7,981K
Table 2: Number of rules in each of the complete
grammars before and after binarization.
are summed. From these counts we remove
(a) all unlexicalized subtrees of height greater
than six and (b) all lexicalized subtrees con-
taining more than twelve terminals on their
frontier, and we add all subtrees of height one
(i.e., the Treebank PCFG).
4. A sampled TSG produced by inducing
derivations on the training data using a
Dirichlet Process prior (described below).
The sampled TSG was produced by inducing a
TSG derivation on each of the trees in the train-
ing data, from which subtree counts were read di-
rectly. These derivations were induced using a
collapsed Gibbs sampler, which sampled from the
posterior of a Dirichlet process (DP) defined over
the subtree rewrites of each nonterminal. The DP
describes a generative process that prefers small
subtrees but occasionally produces larger ones;
when used for inference, it essentially discovers
TSG derivations that contain larger subtrees only
if they are frequent in the training data, which dis-
courages model overfitting. See Post and Gildea
(2009) for more detail. We ran the sampler for 100
iterations with a stop probability of 0.7 and the DP
parameter ? = 100, accumulating subtree counts
from the derivation state at the end of all the itera-
tions, which corresponds to the (100, 0.7,? 100)
grammar from that paper.
All four grammar were learned from all sen-
tences in sections 2 to 21 of the Wall Street Journal
portion of the Penn Treebank. All trees were pre-
processed to remove empty nodes and nontermi-
NP
NP
DT
a
JJ NN NN
PP
Figure 7: Rule 1 in Figure 1 was produced by
flattening this rule from the sampled grammar.
nal annotations. Punctuation was retained. Statis-
tics for these grammars can be found in Table 1.
We present results on sentences with no more than
forty words from section 23.
Our parser is a Perl implementation of the CKY
algorithm.4 For the larger grammars, memory lim-
itations require us to remove from consideration
all grammar rules that could not possibly take part
in a parse of the current sentence, which we do by
matching the rule?s frontier lexicalization pattern
against the words in the sentence. All unlexical-
ized rules are kept. This preprocessing time is not
included in the parsing times reported in the next
section.
For pruning, we group edges into equivalence
classes according to the following features:
? span (s, t) of the input
? level of binarization (0,1,2+)
The level of binarization refers to the height of a
nonterminal in the subtree created by binarizing a
CFG rule (with the exception that the root of this
tree has a binarization level of 0). The naming
scheme used to create new nonterminals in line 6
of Figure 2 means we can determine this level by
counting the number of left-angle brackets in the
nonterminal?s name. In Figure 1, binarized rules
D and E have level 0, C has level 3, B has level 2,
and A has level 1.
Within each bin, only the ? highest-weight
items are kept, where ? ? (1, 5, 10, 25, 50) is a pa-
rameter that we vary during our experiments. Ties
are broken arbitrarily. Additionally, we maintain a
beam within each bin, and an edge is pruned if its
score is not within a factor of 10?5 of the highest-
scoring edge in the bin. Pruning takes place when
the edge is added and then again at the end of each
4It is available from http://www.cs.rochester.
edu/
?
post/.
94
span in the CKY algorithm (but before applying
unary rules).
In order to binarize TSG subtrees, we follow
Bod (2001) in first flattening each subtree to a
depth-one PCFG rule that shares the subtree?s root
nonterminal and leaves, as depicted in Figure 7.
Afterward, this transformation is reversed to pro-
duce the parse tree for scoring. If multiple TSG
subtrees have identical mappings, we take only the
most probable one. Table 2 shows how grammar
size is affected by binarization scheme.
We note two differences in our work that ex-
plain the large difference between the scores re-
ported for the ?minimal subset? grammar in Bod
(2001) and here. First, we did not implement the
smoothed ?mismatch parsing?, which introduces
new subtrees into the grammar at parsing time by
allowing lexical leaves of subtrees to act as wild-
cards. This technique reportedly makes a large
difference in parsing scores (Bod, 2009). Second,
we approximate the most probable parse with the
single most probable derivation instead of the top
1,000 derivations, which Bod also reports as hav-
ing a large impact (Bod, 2003, ?4.2).
4 Results
Figure 8 displays search time vs. model score for
the PCFG and the sampled grammar. Weight
pushing has a significant impact on search effi-
ciency, particularly for the larger sampled gram-
mar. The spinal and minimal graphs are similar to
the PCFG and sampled graphs, respectively, which
suggests that the technique is more effective for
the larger grammars.
For parsing, we are ultimately interested in ac-
curacy as measured by F1 score.5 Figure 9 dis-
plays graphs of time vs. accuracy for parses with
each of the grammars, alongside the numerical
scores used to generate them. We begin by noting
that the improved search efficiency from Figure 8
carries over to the time vs. accuracy curves for
the PCFG and sampled grammars, as we expect.
Once again, we note that the difference is less pro-
nounced for the two smaller grammars than for the
two larger ones.
4.1 Model score vs. accuracy
The tables in Figure 9 show that parser accuracy
is not always a monotonic function of time; some
of the runs exhibited peak performance as early
5F1 = 2?P ?RP+R , where P is precision and R recall.
-340
-338
-336
-334
-332
-330
-328
-326
-324
-322
-320
1 5 10 25 50
m
o
de
l s
co
re
 (th
ou
sa
nd
s)
(greedy,max)
(greedy,nthroot)
(greedy,none)
(right,max)
(right,nthroot)
(right,none)
-370
-360
-350
-340
-330
-320
-310
-300
-290
1 5 10 25 50
m
o
de
l s
co
re
 (th
ou
sa
nd
s)
mean time per sentence (s)
(greedy,max)
(greedy,nthroot)
(greedy,none)
(right,max)
(right,nthroot)
(right,none)
Figure 8: Time vs. model score for the PCFG (top)
and the sampled grammar (bottom). Note that the
y-axis range differs between plots.
as at a bin size of ? = 10, and then saw drops
in scores when given more time. We examined
a number of instances where the F1 score for a
sentence was lower at a higher bin setting, and
found that they can be explained as modeling (as
opposed to search) errors. With the PCFG, these
errors were standard parser difficulties, such as PP
attachment, which require more context to resolve.
TSG subtrees, which have more context, are able
to correct some of these issues, but introduce a dif-
ferent set of problems. In many situations, larger
bin settings permitted erroneous analyses to re-
main in the chart, which later led to the parser?s
discovery of a large TSG fragment. Because these
fragments often explain a significant portion of the
sentence more cheaply than multiple smaller rules
multiplied together, the parser prefers them. More
often than not, they are useful, but sometimes they
are overfit to the training data, and result in an in-
correct analysis despite a higher model score.
Interestingly, these dips occur most frequently
for the heuristically extracted TSGs (four of six
95
 50
 55
 60
 65
 70
 75
 80
 85
1 5 10 25 50
a
cc
u
ra
cy
mean time per sentence (s)
(greedy,max)
(greedy,none)
(right,max)
(right,none)
PCFG
run 1 5 10 25 50
 (g,m) 66.44 72.45 72.54 72.54 72.51u (g,n) 65.44 72.21 72.47 72.45 72.47
N (g,-) 63.91 71.91 72.48 72.51 72.51
 (r,m) 67.30 72.45 72.61 72.47 72.49e (r,n) 64.09 71.78 72.33 72.45 72.47
? (r,-) 61.82 71.00 72.18 72.42 72.41
 50
 55
 60
 65
 70
 75
 80
 85
1 5 10 25 50
a
cc
u
ra
cy
mean time per sentence (s)
(greedy,max)
(greedy,none)
(right,max)
(right,none)
spinal
run 1 5 10 25 50
 (g,m) 68.33 78.35 79.21 79.25 79.24u (g,n) 64.67 78.46 79.04 79.07 79.09
N (g,-) 61.44 77.73 78.94 79.11 79.20
 (r,m) 69.92 79.07 79.18 79.25 79.05e (r,n) 67.76 78.46 79.07 79.04 79.04
? (r,-) 65.27 77.34 78.64 78.94 78.90
 50
 55
 60
 65
 70
 75
 80
 85
1 5 10 25 50
a
cc
u
ra
cy
mean time per sentence (s)
(greedy,max)
(greedy,none)
(right,max)
(right,none)
sampled
run 1 5 10 25 50
 (g,m) 63.75 80.65 81.86 82.40 82.41u (g,n) 61.87 79.88 81.35 82.10 82.17
N (g,-) 53.88 78.68 80.48 81.72 81.98
 (r,m) 72.98 81.66 82.37 82.49 82.40e (r,n) 65.53 79.01 80.81 81.91 82.13
? (r,-) 61.82 77.33 79.72 81.13 81.70
 50
 55
 60
 65
 70
 75
 80
 85
1 5 10 25 50
a
cc
u
ra
cy
mean time per sentence (s)
(greedy,max)
(greedy,none)
(right,max)
(right,none)
minimal
run 1 5 10 25 50
 (g,m) 59.75 77.28 77.77 78.47 78.52u (g,n) 57.54 77.12 77.82 78.35 78.36
N (g,-) 51.00 75.52 77.21 78.30 78.13
 (r,m) 65.29 76.14 77.33 78.34 78.13e (r,n) 61.63 75.08 76.80 77.97 78.31
? (r,-) 59.10 73.42 76.34 77.88 77.91
Figure 9: Plots of parsing time vs. accuracy for each of the grammars. Each plot contains four sets of five
points (? ? (1, 5, 10, 25, 50)), varying the binarization strategy (right (r) or greedy (g)) and the weight
pushing technique (maximal (m) or none (-)). The tables also include data from nthroot (n) pushing.
96
 50
 55
 60
 65
 70
 75
 80
 85
1 5 10 25 50
a
cc
u
ra
cy
(right,max)
(right,nthroot)
(right,none)
 50
 55
 60
 65
 70
 75
 80
 85
1 5 10 25 50
a
cc
u
ra
cy
mean time per sentence (s)
(greedy,max)
(greedy,nthroot)
(greedy,none)
Figure 10: Time vs. accuracy (F1) for the sampled
grammar, broken down by binarization (right on
top, greedy on bottom).
runs for the spinal grammar, and two for the min-
imal grammar) and for the PCFG (four), and least
often for the model-based sampled grammar (just
once). This may suggest that rules selected by our
sampling procedure are less prone to overfitting on
the training data.
4.2 Pushing
Figure 10 compares the nthroot and maximal
pushing techniques for both binarizations of the
sampled grammar. We can see from this figure
that there is little difference between the two tech-
niques for the greedy binarization and a large dif-
ference for the right binarization. Our original mo-
tivation in developing nthroot pushing came as a
result of analysis of certain sentences where max-
imal pushing and greedy binarization resulted in
the parser producing a lower model score than
with right binarization with no pushing. One such
example was binarized fragment A from Fig-
ure 1; when parsing a particular sentence in the
development set, the correct analysis required the
rule from Figure 7, but greedy binarization and
maximal pushing resulted in this piece getting
pruned early in the search procedure. This pruning
happened because maximal pushing allowed too
much weight to shift down for binarized pieces of
competing analyses relative to the correct analy-
sis. Using nthroot pushing solved the search prob-
lem in that instance, but in the aggregate it does
not appear to be helpful in improving parser effi-
ciency as much as maximal pushing. This demon-
strates some of the subtle interactions between bi-
narization and weight pushing when inexact prun-
ing heuristics are applied.
4.3 Binarization
Song et al (2008, Table 4) showed that CKY pars-
ing efficiency is not a monotonic function of the
number of constituents produced; that is, enumer-
ating fewer edges in the dynamic programming
chart does not always correspond with shorter run
times. We see here that efficiency does not al-
ways perfectly correlate with grammar size, ei-
ther. For all but the PCFG, right binarization
improves upon greedy binarization, regardless of
the pushing technique, despite the fact that the
right-binarized grammars are always larger than
the greedily-binarized ones.
Weight pushing and greedy binarization both in-
crease parsing efficiency, and the graphs in Fig-
ures 8 and 9 suggest that they are somewhat com-
plementary. We also investigated left binarization,
but discontinued that exploration because the re-
sults were nearly identical to that of right bina-
rization. Another popular binarization approach
is head-outward binarization. Based on the anal-
ysis above, we suspect that its performance will
fall somewhere among the binarizations presented
here, and that pushing will improve it as well. We
hope to investigate this in future work.
5 Summary
Weight pushing increases parser efficiency, espe-
cially for large grammars. Most notably, it im-
proves parser efficiency for the Gibbs-sampled
tree substitution grammar of Post and Gildea
(2009).
We believe this approach could alo bene-
fit syntax-based machine translation. Zhang et
al. (2006) introduced a synchronous binariza-
tion technique that improved decoding efficiency
and accuracy by ensuring that rule binarization
avoided gaps on both the source and target sides
97
(for rules where this was possible). Their binariza-
tion was designed to share binarized pieces among
rules, but their approach to distributing weight was
the default (nondiffused) case found in this paper
to be least efficient: The entire weight of the orig-
inal rule is placed at the top binarized rule and all
internal rules are assigned a probability of 1.0.
Finally, we note that the weight pushing algo-
rithm described in this paper began with a PCFG
and ensured that the tree distribution was not
changed. However, weight pushing need not be
limited to a probabilistic interpretation, but could
be used to spread weights for grammars with dis-
criminatively trained features as well, with neces-
sary adjustments to deal with positively and nega-
tively weighted rules.
Acknowledgments We thank the anonymous
reviewers for their helpful comments. This work
was supported by NSF grants IIS-0546554 and
ITR-0428020.
References
Rens Bod. 2001. What is the minimal set of fragments
that achieves maximal parse accuracy? In Pro-
ceedings of the 39th Annual Conference of the As-
sociation for Computational Linguistics (ACL-01),
Toulouse, France.
Rens Bod. 2003. Do all fragments count? Natural
Language Engineering, 9(4):307?323.
Rens Bod. 2009. Personal communication.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 2000 Meet-
ing of the North American chapter of the Association
for Computational Linguistics (NAACL-00), Seattle,
Washington.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of the 38th Annual Conference of the
Association for Computational Linguistics (ACL-
00), Hong Kong.
Michael John Collins. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
John DeNero, Mohit Bansal, Adam Pauls, and Dan
Klein. 2009. Efficient parsing for transducer gram-
mars. In Proceedings of the 2009 Meeting of the
North American chapter of the Association for Com-
putational Linguistics (NAACL-09), Boulder, Col-
orado.
Liang Huang. 2007. Binarization, synchronous bi-
narization, and target-side binarization. In North
American chapter of the Association for Computa-
tional Linguistics Workshop on Syntax and Struc-
ture in Statistical Translation (NAACL-SSST-07),
Rochester, NY.
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact Viterbi parse selection. In Pro-
ceedings of the 2003 Meeting of the North American
chapter of the Association for Computational Lin-
guistics (NAACL-03), Edmonton, Alberta.
Mehryar Mohri and Michael Riley. 2001. A weight
pushing algorithm for large vocabulary speech
recognition. In European Conference on Speech
Communication and Technology, pages 1603?1606.
Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Lin-
guistics, 23(2):269?311.
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proceedings of the
47th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-09), Suntec, Singapore.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
Proceedings of the 20th International Conference on
Computational Linguistics (COLING-04), Geneva,
Switzerland.
Xinying Song, Shilin Ding, and Chin-Yew Lin. 2008.
Better binarization for the CKY parsing. In 2008
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-08), Honolulu, Hawaii.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of the 2006 Meet-
ing of the North American chapter of the Associ-
ation for Computational Linguistics (NAACL-06),
New York, NY.
98
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 716?724,
Beijing, August 2010
Semantic Role Features for Machine Translation
Ding Liu
Department of Computer Science
University of Rochester
Daniel Gildea
Department of Computer Science
University of Rochester
Abstract
We propose semantic role features for a
Tree-to-String transducer to model the re-
ordering/deletion of source-side semantic
roles. These semantic features, as well as
the Tree-to-String templates, are trained
based on a conditional log-linear model
and are shown to significantly outperform
systems trained based on Max-Likelihood
and EM. We also show significant im-
provement in sentence fluency by using
the semantic role features in the log-linear
model, based on manual evaluation.
1 Introduction
Syntax-based statistical machine translation
(SSMT) has achieved significant progress during
recent years (Galley et al, 2006; May and
Knight, 2007; Liu et al, 2006; Huang et al,
2006), showing that deep linguistic knowledge,
if used properly, can improve MT performance.
Semantics-based SMT, as a natural extension
to SSMT, has begun to receive more attention
from researchers (Liu and Gildea, 2008; Wu
and Fung, 2009). Semantic structures have two
major advantages over syntactic structures in
terms of helping machine translation. First of all,
semantic roles tend to agree better between two
languages than syntactic constituents (Fung et al,
2006). This property motivates the approach of
using the consistency of semantic roles to select
MT outputs (Wu and Fung, 2009). Secondly,
the set of semantic roles of a predicate models
the skeleton of a sentence, which is crucial to
the readability of MT output. By skeleton, we
mean the main structure of a sentence including
the verbs and their arguments. In spite of the
theoretical potential of the semantic roles, there
has not been much success in using them to
improve SMT systems.
Liu and Gildea (2008) proposed a semantic role
based Tree-to-String (TTS) transducer by adding
semantic roles to the TTS templates. Their ap-
proach did not differentiate the semantic roles of
different predicates, and did not always improve
the TTS transducer?s performance. Wu and Fung
(2009) took the output of a phrase-based SMT sys-
tem Moses (Koehn et al, 2007), and kept permut-
ing the semantic roles of the MT output until they
best matched the semantic roles in the source sen-
tence. This approach shows the positive effect of
applying semantic role constraints, but it requires
re-tagging semantic roles for every permuted MT
output and does not scale well to longer sentences.
This paper explores ways of tightly integrating
semantic role features (SRFs) into an MT system,
rather than using them in post-processing or n-
best re-ranking. Semantic role labeling (SRL) sys-
tems usually use sentence-wide features (Xue and
Palmer, 2004; Pradhan et al, 2004; Toutanova et
al., 2005); thus it is difficult to compute target-
side semantic roles incrementally during decoding.
Noticing that the source side semantic roles are
easy to compute, we apply a compromise approach,
where the target side semantic roles are generated
by projecting the source side semantic roles us-
ing the word alignments between the source and
target sentences. Since this approach does not per-
form true SRL on the target string, it cannot fully
evaluate whether the source and target semantic
structures are consistent. However, the approach
does capture the semantic-level re-ordering of the
sentences. We assume here that the MT system is
capable of providing word alignment (or equiva-
lent) information during decoding, which is gener-
ally true for current statistical MT systems.
Specifically, two types of semantic role features
are proposed in this paper: a semantic role re-
ordering feature designed to capture the skeleton-
level permutation, and a semantic role deletion fea-
716
ture designed to penalize missing semantic roles in
the target sentence. To use these features during de-
coding, we need to keep track of the semantic role
sequences (SRS) for partial translations, which can
be generated based on the source-side semantic
role sequence and the corresponding word align-
ments. Since the SRL system and the MT sys-
tem are separate, a translation rule (e.g., a phrase
pair in phrase-based SMT) could cover two partial
source-side semantic roles. In such cases partial
SRSs must be recorded in such a way that they can
be combined later with other partial SRSs. Deal-
ing with this problem will increase the complexity
of the decoding algorithm. Fortunately, Tree-to-
String transducer based MT systems (Liu et al,
2006; Huang et al, 2006) can avoid this problem
by using the same syntax tree for both SRL and
MT. Such an arrangement guarantees that a TTS
template either covers parts of one source-side se-
mantic role, or a few complete semantic roles. This
advantage motivates us to use a TTS transducer as
the MT system with which to demonstrate the use
of the proposed semantic role features. Since it is
hard to design a generative model to combine both
the semantic role features and the TTS templates,
we use a log-linear model to estimate the feature
weights, by maximizing the conditional probabil-
ities of the target strings given the source syntax
trees. The log-linear model with latent variables
has been discussed by Blunsom et al (2008); we
apply this technique to combine the TTS templates
and the semantic role features.
The remainder of the paper is organized as fol-
lows: Section 2 describes the semantic role fea-
tures proposed for machine translation; Section 3
describes how semantic role features are used and
trained in a TTS transducer; Section 4 presents
the experimental results; and Section 5 gives the
conclusion.
2 Semantic Role Features for Machine
Translation
2.1 Defining Semantic Roles
There are two semantic standards with publicly
available training data: PropBank (Palmer et al,
2005) and FrameNet (Johnson et al, 2002). Prop-
Bank defines a set of semantic roles for the verbs
in the Penn TreeBank using numbered roles. These
roles are defined individually for each verb. For
example, for the verb disappoint, the role name
arg1 means experiencer, but for the verb wonder,
role name arg1 means cause. FrameNet is moti-
vated by the idea that a certain type of verbs can
be gathered together to form a frame, and in the
same frame, a set of semantic roles is defined and
shared among the verbs. For example, the verbs
boil, bake, and steam will be in frame apply heat,
and they have the semantic roles of cook, food, and
heating instrument. Of these two semantic stan-
dards, we choose PropBank over FrameNet for the
following reasons:
1. PropBank has a simpler semantic definition
than FrameNet and thus is easier for auto-
matic labeling.
2. PropBank is built upon the Penn TreeBank
and is more consistent with statistical parsers,
most of which are trained on the Penn Tree-
Bank.
3. PropBank is a larger corpus than FrameNet.
Note that the semantic standard/corpus is not cru-
cial in this paper. Any training corpus that can be
used to automatically obtain the set of semantic
roles of a verb could be used in our approach.
2.2 Semantic Role Features
Ideally, we want to use features based on the true
semantic roles of the MT candidates. Consider-
ing there is no efficient way of integrating SRL
and MT, accurate target-side semantic roles can
only be used in post-processing and re-ranking
the MT outputs, where a limited number of MT
candidates are considered. On the other hand, it
is much easier to obtain reliable semantic roles
for the source sentences. This paper uses a com-
promise approach, where the target-side semantic
roles are projected from the source-side semantic
roles using the word alignment derived from the
translation process. More specifically, we define
two types of semantic role features:
1. Semantic Role Re-ordering (SRR) This fea-
ture describes re-ordering of the source-side
717
semantic roles (including the predicate) in the
target side. It takes the following form:
SrcPred : SrcRole1, ..., SrcRolen
? TarRole1, ..., TarRolen
where SrcPred and SrcRole denotes the
central verb and semantic roles in the source
side, and TarRole denotes the target-side
roles. The source/target SRSs do not need be
continuous, but there should be a one-to-one
alignment between the roles in the two sides.
Compared to the general re-ordering models
used in statistical MT systems, this type of
feature is capable of modeling skeleton-level
re-ordering, which is crucial to the fluency
of MT output. Because a predicate can have
different semantic role sequences in different
voices, passive/active are tagged for each oc-
currence of the verbs based on their POS and
preceding words. Figure 1 shows examples
of the feature SRR.
2. Deleted Roles (DR) are the individual source-
side semantic roles which are deleted in the
MT outputs, taking the form of:
SrcPred : SrcRole ? deleted
DR is meant to penalize the deletion of the
semantic roles. Though most statistical MT
systems have penalties for word deletion, it
is still useful to make separate features for
the deletion of semantic roles, which is con-
sidered more harmful than the deletion of
non-core components (e.g., modifiers) and
deserves more serious penalty. Examples of
the deletion features can be found in Figure 1.
Both types of features can be made non-lexicalized
by removing the actual verb but retaining its voice
information in the features. Non-lexicalized fea-
tures are used in the system to alleviate the problem
of sparse verbs.
3 Using Semantic Role Features in
Machine Translation
This section describes how to use the proposed se-
mantic role features in a Tree-to-String transducer,
I??di
d??n
ot??
see
??th
e??b
??
??
arg
0
arg
?ne
g
arg
1
SRR
: see?
acti
ve:?
arg?
neg
verb
bor
row
ed?
acti
ve:?
arg1
?a
bor
row
ed?
acti
ve:?
arg1
?v e
bor
row
ed?
acti
ve:?
arg0
?ve
bor
row
ed?
acti
ve:?
arg1
?a
DR:
see
?act
ive:
?arg
0??
dele
tboo
k??y
ou??
bor
row
ed
??
??arg1
arg
0
??a
rg?n
egv
erb
rg0
??a
rg0
?arg
1
erb
??v
erb
?arg
1
erb
??a
rg0
?ver
b
rg0
?ver
b??
arg0
?ver
b?ar
g1
ted
?
Figure 1: Examples of the semantic role features
assuming that the semantic roles have been tagged
for the source sentences. We first briefly describe
the basic Tree-to-String translation model used in
our experiments, and then describe how to modify
it to incorporate the semantic role features.
3.1 Basic Tree-to-String Transducer
A Tree-to-String transducer receives a syntax tree
as its input and, by recursively applying TTS tem-
plates, generates the target string. A TTS tem-
plate is composed of a left-hand side (LHS) and
a right-hand side (RHS), where the LHS is a sub-
tree pattern and the RHS is a sequence of variables
and translated words. The variables in the RHS
of a template correspond to the bottom level non-
terminals in the LHS?s subtree pattern, and their
relative order indicates the permutation desired at
the point where the template is applied to translate
one language to another. The variables are further
transformed, and the recursive process goes on un-
til there are no variables left. The formal descrip-
tion of a TTS transducer is given by Graehl and
Knight (2004), and our baseline approach follows
the Extended Tree-to-String Transducer defined by
Huang et al (2006). For a given derivation (de-
composition into templates) of a syntax tree, the
translation probability is computed as the product
of the templates which generate both the source
syntax trees and the target translations.
Pr(S | T,D?) =
?
t?D?
Pr(t)
Here, S denotes the target sentence, T denotes the
source syntax tree, and D? denotes the derivation
of T . In addition to the translation model, the
718
function DECODE(T )
for tree node v of T in bottom-up order do
for template t applicable at v do
{c1, c2}=match(v, t);
s.leftw = c1.leftw;
s.rightw = c2.rightw;
s.val = c1.val ? c2.val;
s.val ?= Pr(t);
s.val ?= Pr(c2.leftw|c1.rightw);
add s to v?s beam;
Figure 2: Decoding algorithm for the standard Tree-to-String
transducer. leftw/rightw denote the left/right boundary
word of s. c1, c2 denote the descendants of v, ordered based
on RHS of t.
TTS system includes a trigram language model,
a deletion penalty, and an insertion bonus. The
bottom-up decoding algorithm for the TTS trans-
ducer is sketched in Figure 2. To incorporate the
n-gram language model, states in the algorithm
denote a tree node?s best translations with different
left and right boundary words. We use standard
beam-pruning to narrow the search space. To sim-
plify the description, we assume in Figure 2 that
a bigram language model is used and all the TTS
templates are binarized. It is straightforward to
generalize the algorithm for larger n-gram models
and TTS templates with any number of children in
the bottom using target-side binarized combination
(Huang et al, 2006).
3.2 Modified Tree-to-String Transducer with
Semantic Role Features
Semantic role features can be used as an auxiliary
translation model in the TTS transducer, which
focuses more on the skeleton-level permutation.
The model score, depending on not only the in-
put source tree and the derivation of the tree, but
also the semantic roles of the source tree, can be
formulated as:
Pr(S | T,D?) =
?
f?F (S,T.role,D?)
Pr(f)
where T denotes the source syntax tree with
semantic roles, T.role denotes the seman-
tic role sequence in the source side and
F (S.role, T.role,D?) denotes the set of defined
semantic role features over T.role and the target
side semantic role sequence S.role. Note that
given T.role and the derivation D?, S.role can
VP NP
[gi
vin
g:?
VB
G
[gi
vin
g:?v
erb
]
giv
ing
VP
[gi
vin
g:?a
r g
TTS
?te
mp
lat
e:
(VP
?(V
BG
?giv
in
Tri
gge
red
??SR
R:?
?giv
ing
?ac
tive
:?a
Tri
gge
red
?DR
:????
?giv
ing
?ac
tive
:?vNP
[gi
vin
g:?
VB
G
[gi
vin
g:?v
erb
]
giv
ing
arg
2]
NP
[gi
vin
g:?a
rg1
]
g2?
arg
1]
g?)?
?NP
#1?
NP
#2?
)???
NP
#1?
NP
#2
arg
2?a
rg1
??
arg
2?a
rg1
ver
b??
de
let
ed
arg
2]
NP
[gi
vin
g:?a
rg1
]
Figure 3: An example showing the combination of the se-
mantic role sequences of the states. Above/middle is the state
information before/after applying the TTS template, and bot-
tom is the used TTS template and the triggered SRFs during
the combination.
be easily derived. Now we show how to in-
corporate the two types of semantic role features
into a TTS transducer. To use the semantic role
re-ordering feature SRR, the states in the decod-
ing algorithm need to be expanded to encode the
target-side SRSs. The SRSs are initially attached
to the translation states of the source tree con-
? PP VBZ ? ??
VP
VBZ
[bring: verb]
NP
[bring: arg1]
PP
[bring: arg3]
NNP NN
new test
0 3 4
^
Combined SRS arg3 verb arg1
Median = 3 arg1
Figure 4: An example showing how to compute the target side
position of a semantic role by using the median of its aligning
points.
719
stituents which are labeled as semantic roles for
some predicate. These semantic roles are then
accumulated with re-ordering and deletion oper-
ations specified by the TTS templates as the de-
coding process goes bottom-up. Figure 5 shows
the decoding algorithm incorporating the SRR fea-
tures. The model component corresponding to the
feature SRR is computed when combining two
translation states. I.e., the probabilities of the SRR
features composed based on the semantic roles of
the two combining states will be added into the
combined state. See Figure 3 for examples. The
theoretical upper bound of the decoding complex-
ity is O(NM4(n?1)R(?Ci=0 C!i! )V ), where N isthe number of nodes in the source syntax tree, M
is the vocabulary size of the target language, n is
the order of the n-gram language model, R is the
maximum number of TTS templates which can be
matched at a tree node, C is the maximum number
of roles of a verb, and V is the maximum number
of verbs in a sentence. In this formula, ?Ci=0 C!i!is the number of role sequences obtained by first
choosing i out of C possible roles and then per-
muting the i roles. This theoretical upper bound
is not reached in practice, because the number of
possible TTS templates applicable at a tree node
is very limited. Furthermore, since we apply beam
pruning at each tree node, the running time is con-
trolled by the beam size, and is linear in the size of
the tree.
The re-ordering of the semantic roles from
source to target is computed for each TTS template
as part of the template extraction process, using
the word-level alignments between the LHS/RHS
of the TTS template (e.g., Figure 3). This is usu-
ally straightforward, with the exception of the case
where the words that are aligned to a particular
role?s span in the source side are not continuous
in the target side, as shown in Figure 4. Since
we are primarily interested in the relative order of
the semantic roles, we approximate each seman-
tic role?s target side position by the median of the
word positions that is aligned to. If more than one
semantic role is mapped to the same position in
the target side, their source side order will be used
as their target side order, i.e., monotonic transla-
tion is assumed for those semantic roles. Figure 4
shows an example of calculating the target side
function DECODE(T )
for tree node v of T in bottom-up order do
for template t applicable at v do
{c1, c2}=match(v, t);
s.leftw = c1.leftw;
s.rightw = c2.rightw;
s.role = concatenate(c1.role, c2.role);
if v is a semantic role then
set s.role to v.role;
s.val = c1.val ? c2.val;
s.val ?= Pr(t);
s.val ?= Pr(c2.leftw|c1.rightw);
. Compute the probabilities associated with semantic roles
s.val ?= Qf?Sema(c1.role,c2.role,t) Pr(f);add s to v?s beam;
Figure 5: Decoding algorithm using semantic role features.
Sema(c1.role, c2.role, t) denotes the triggered semantic
role features when combining two children states, and ex-
amples can be found in Figure 3.
SRS based on a complicated TTS template. The
word alignments in the TTS templates are also used
to compute the deletion feature DR. Whenever a
semantic role is deleted in a TTS template?s RHS,
the corresponding deletion penalty will be applied.
3.3 Training
We describe two alternative methods for training
the weights for the model?s features, including both
the individual TTS templates and the semantic
role features. The first method maximizes data
likelihood as is standard in EM, while the second
method maximizes conditional likelihood for a log-
linear model following Blunsom et al (2008).
3.3.1 Maximizing Data Likelihood
The standard way to train a TTS translation
model is to extract the minimum TTS templates us-
ing GHKM (Galley et al, 2004), and then normal-
ize the frequency of the extracted TTS templates
(Galley et al, 2004; Galley et al, 2006; Liu et al,
2006; Huang et al, 2006). The probability of the
semantic features SRR and DR can be computed
similarly, given that SRR and DR can be derived
from the paired source/target sentences and the
word alignments between them. We refer to this
model as max-likelihood training and normalize
the counts of TTS templates and semantic features
based on their roots and predicates respectively.
We wish to overcome noisy alignments from
GIZA++ and learn better TTS rule probabilities
by re-aligning the data using EM within the TTS
720
E-step:
for all pair of syntax tree T and target string S do
for all TTS Template t, semantic features f do
EC(t) +=
P
D:t?D Pr(S,T,D)P
D? Pr(S,T,D?)
;
EC(f) +=
P
D:f?D Pr(S,T,D)P
D? Pr(S,T,D?)
;
M-step:
for all TTS Template t, semantic features f do
Pr(t) = EC(t)P
t?:t?.root=t.root EC(t?)
;
Pr(f) = EC(f)P
f?:f?.predicate=t.predicate EC(f ?)
;
Figure 6: EM Algorithm For Estimating TTS Templates and
Semantic Features
framework (May and Knight, 2007). We can es-
timate the expected counts of the TTS templates
and the semantic features by formulating the prob-
ability of a pair of source tree and target string
as:
X
D
Pr(S, T,D) =
X
D
0
@
Y
t?D
Pr(t)
Y
f?F (S,T.role,D)
Pr(f)
1
A
Though the above formulation, which makes the
total probability of all the pairs of trees and strings
less than 1, is not a strict generative model, we can
still use the EM algorithm (Dempster et al, 1977)
to estimate the probability of the TTS templates
and the semantic features, as shown in Figure 6.
The difficult part of the EM algorithm is the E-
step, which computes the expected counts of the
TTS templates and the semantic features by sum-
ming over all possible derivations of the source
trees and target strings. The standard inside-
outside algorithm (Graehl and Knight, 2004) can
be used to compute the expected counts of the TTS
templates. Similar to the modification made in the
TTS decoder, we can add the target-side semantic
role sequence to the dynamic programming states
of the inside-outside algorithm to compute the ex-
pected counts of the semantic features. This way
each state (associated with a source tree node) rep-
resents a target side span and the partial SRSs. To
speed up the training, a beam is created for each
target span and only the top rated SRSs in the beam
are kept.
3.3.2 Maximizing Conditional Likelihood
A log-linear model is another way to combine
the TTS templates and the semantic features to-
gether. Considering that the way the semantic
function COMPUTE PARTITION(T )
for tree node v of T in bottom-up order do
for template t applicable at v do
for {s1, s2}=Match(v, t) do
s.sum += s1.sum? s2.sum?
exp(?t +
P
f?Sema(s1,s2,t) ?f );
s.role = concatenate(s1.role, s2.role);
add s to v;
for state s in root do res += s.sum;
return res;
Figure 7: Computing the partition function of the conditional
probability Pr(S|T ). Sema(s1, s2, t) denotes all the seman-
tic role features generated by combining s1 and s2 using t.
role features are defined makes it impossible to
design a sound generative model to incorporate
these features, a log-linear model is also a theoreti-
cally better choice than the EM algorithm. If we
directly translate the EM algorithm into the log-
linear model, the problem becomes maximizing
the data likelihood represented by feature weights
instead of feature probabilities:
Pr(S, T ) =
P
D exp
P
i ?ifi(S, T,D)P
S?,T ?
P
D? exp
P
i ?ifi(S?, T ?, D?)
where the features f include both the TTS tem-
plates and the semantic role features. The numer-
ator in the formula above can be computed using
the same dynamic programming algorithm used to
compute the expected counts in the EM algorithm.
However, the partition function (denominator) re-
quires summing over all possible source trees and
target strings, and is infeasible to compute. In-
stead of approximating the partition function using
methods such as sampling, we change the objective
function from the data likelihood to the conditional
likelihood:
Pr(S | T ) =
P
D exp
P
i ?ifi(S, T,D)P
S??all(T )
P
D? exp
P
i ?ifi(S?, T,D?)
where all(T ) denotes all the possible target strings
which can be generated from the source tree T .
Given a set of TTS templates, the new partition
function can be efficiently computed using the dy-
namic programming algorithm shown in Figure 7.
Again, to simplify the illustration, only binary TTS
templates are used. Using the conditional proba-
bility as the objective function not only reduces
the computational cost, but also corresponds better
to the TTS decoder, where the best MT output is
721
selected only among the possible candidates which
can be generated from the input source tree using
TTS templates.
The derivative of the logarithm of the objective
function (over the entire training corpus) w.r.t. a
feature weight can be computed as:
? log
Q
S,T Pr(S | T )
??i
=
X
S,T
{ECD|S,T (fi)? ECS?|T (fi)}
where ECD|S,T (fi), the expected count of a fea-
ture over all derivations given a pair of tree and
string, can be computed using the modified inside-
outside algorithm described in Section 3.2, and
ECS?|T (fi), the expected count of a feature over
all possible target strings given the source tree,
can be computed in a similar way to the partition
function described in Figure 7. With the objective
function and its derivatives, a variety of optimiza-
tion methods can be used to obtain the best feature
weights; we use LBFGS (Zhu et al, 1994) in our
experiments. To prevent the model from overfitting
the training data, a weighted Gaussian prior is used
with the objective function. The variance of the
Gaussian prior is tuned based on the development
set.
4 Experiments
We train an English-to-Chinese translation system
using the FBIS corpus, where 73,597 sentence
pairs are selected as the training data, and 500
sentence pairs with no more than 25 words on the
Chinese side are selected for both the development
and test data.1 Charniak (2000)?s parser, trained on
the Penn Treebank, is used to generate the English
syntax trees. To compute the semantic roles for the
source trees, we use an in-house max-ent classifier
with features following Xue and Palmer (2004) and
Pradhan et al (2004). The semantic role labeler
is trained and tuned based on sections 2?21 and
section 24 of PropBank respectively. The standard
role-based F-score of our semantic role labeler is
88.70%. Modified Kneser-Ney trigram models
are trained using SRILM (Stolcke, 2002) on the
Chinese portion of the training data. The model
1The total 74,597 sentence pairs used in experiments are
those in the FBIS corpus whose English part can be parsed
using Charniak (2000)?s parser.
(n-gram language model, TTS templates, SRR,
DR) weights of the transducer are tuned based on
the development set using a grid-based line search,
and the translation results are evaluated based on a
single Chinese reference using BLEU-4 (Papineni
et al, 2002). Huang et al (2006) used character-
based BLEU as a way of normalizing inconsistent
Chinese word segmentation, but we avoid this prob-
lem as the training, development, and test data are
from the same source.
The baseline system in our experiments uses
the TTS templates generated by using GHKM
and the union of the two single-direction align-
ments generated by GIZA++. Unioning the two
single-direction alignments yields better perfor-
mance for the SSMT systems using TTS templates
(Fossum et al, 2008) than the two single-direction
alignments and the heuristic diagonal combination
(Koehn et al, 2003). The two single-direction
word alignments as well as the union are used to
generate the initial TTS template set for both the
EM algorithm and the log-linear model. The ini-
tial TTS templates? probabilities/weights are set to
their normalized counts based on the root of the
TTS template (Galley et al, 2006). To test seman-
tic role features, their initial weights are set to their
normalized counts for the EM algorithm and to 0
for the log-linear model. The performance of these
systems is shown in Table 1. We can see that the
EM algorithm, based only on TTS templates, is
slightly better than the baseline system. Adding
semantic role features to the EM algorithm actu-
ally hurts the performance, which is not surprising
since the combination of the TTS templates and
semantic role features does not yield a sound gen-
erative model. The log-linear model based on TTS
templates achieves significantly better results than
both the baseline system and the EM algorithm.
Both improvements are significant at p < 0.05
based on 2000 iterations of paired bootstrap re-
sampling of the test set (Koehn, 2004).
Adding semantic role features to the log-linear
model further improves the BLEU score. One prob-
lem in our approach is the sparseness of the verbs,
which makes it difficult for the log-linear model
to tune the lexicalized semantic role features. One
way to alleviate this problem is to make features
based on verb classes. We first tried using the verb
722
TTS Templates + SRF + Verb Class
Union 15.6 ? ?
EM 15.9 15.5 15.6
Log-linear 17.1 17.4 17.6
Table 1: BLEU-4 scores of different systems
equal better worse
With SRF vs. W/O SRF 72% 20.2% 7.8%
Table 2: Distribution of the sentences where the semantic
role features give no/positive/negative impact to the sentence
fluency in terms of the completeness and ordering of the
semantic roles.
classes in VerbNet (Dang et al, 1998). Unfortu-
nately, VerbNet only covers about 34% of the verb
tokens in our training corpus, and does not im-
prove the system?s performance. We then resorted
to automatic clustering based on the aspect model
(Hofmann, 1999; Rooth et al, 1999). The training
corpus used in clustering is the English portion of
the selected FBIS corpus. Though automatically
obtained verb clusters lead to further improvement
in BLEU score, the total improvement from the se-
mantic role features is not statistically significant.
Because BLEU-4 is biased towards the adequacy
of the MT outputs and may not effectively evaluate
their fluency, it is desirable to give a more accurate
evaluation of the sentence?s fluency, which is the
property that semantic role features are supposed
to improve. To do this, we manually compare
the outputs of the two log-linear models with and
without the semantic role features. Our evaluation
focuses on the completeness and ordering of the
semantic roles, and better, equal, worse are tagged
for each pair of MT outputs indicating the impact
of the semantic role features. Table 2 shows the
manual evaluation results based on the entire test
set, and the improvement from SRF is significant
at p < 0.005 based on a t-test. To illustrate how
SRF impacts the translation results, Figure 8 gives
3 examples of the MT outputs with and without
the SRFs.
5 Conclusion
This paper proposes two types of semantic role
features for a Tree-to-String transducer: one mod-
els the reordering of the source-side semantic role
sequence, and the other penalizes the deletion of a
source-side semantic role. These semantic features
Sou
rce
Lau
nch
ing
1N
ew
2D
ip
SRF
?On
??
1?
? 2
??
3?
? 4
SRF
?Of
f
??
2?
? 3
??
4
Sou
rce
It 1
is 2
the
ref
ore
3
ne
tra
nsf
orm
ati
on
9o
f 10
hig
h 14
tec
hn
olo
gie
s 15
SRF
?On
??
12
3?
4?
? 6,
7?
?
SRF
?Of
f
??
12
3?
4?
??
14
,15
Sou
rce
A 1
gra
tify
ing
2
cha
n
str
uct
ure
8o
f 9e
thn
ic
SRF
?On
??
??
10
,11
??
8?
4
SRF
?Of
f
??
1?
??
2?
? 3
,?pl
om
ati
c 3O
ffe
nsi
ve 4
4 ece
ssa
ry 4
to 5
spe
ed
6
up
7
the
8
tra
dit
ion
al 1
1in
du
str
ies
12
wit
h 13
5 ??
? 14
,15
??
9?
??
? 11
,12
,?
? 6,
7?
??
? 11
,12
??
9
nge
3
als
o 4
occ
urr
ed
5
in 6
the
7
10
mi
no
rity
11
cad
res
12
??
5?
??
2?
? 3
??
4?
??
? 10
,11
??
?
??
8
Figure 8: Examples of the MT outputs with and without SRFs.
The first and second example shows that SRFs improve the
completeness and the ordering of the MT outputs respectively,
the third example shows that SRFs improve both properties.
The subscripts of each Chinese phrase show their aligned
words in English.
and the Tree-to-String templates, trained based on
a conditional log-linear model, are shown to sig-
nificantly improve a basic TTS transducer?s per-
formance in terms of BLEU-4. To avoid BLEU?s
bias towards the adequacy of the MT outputs, man-
ual evaluation is conducted for sentence fluency
and significant improvement is shown by using
the semantic role features in the log-linear model.
Considering our semantic features are the most ba-
sic ones, using more sophisticated features (e.g.,
the head words and their translations of the source-
side semantic roles) provides a possible direction
for further experimentation.
Acknowledgments This work was funded by
NSF IIS-0546554 and IIS-0910611.
References
Blunsom, Phil, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics (ACL-08), Columbus, Ohio.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL-01, pages
132?139.
Dang, Hoa Trang, Karin Kipper, Martha Palmer, and
723
Joseph Rosenzweig. 1998. Investigating regu-
lar sense extensions based on intersective Levin
classes. In COLING/ACL-98, pages 293?299, Mon-
treal. ACL.
Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Soci-
ety, 39(1):1?21.
Fossum, Victoria, Kevin Knight, and Steven Abney.
2008. Using syntax to improveword alignment pre-
cision for syntax-based machine translation. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, Columbus, Ohio. ACL.
Fung, Pascale, Zhaojun Wu, Yongsheng Yang, and
Dekai Wu. 2006. Learning of Chinese/English se-
mantic structure mapping. In IEEE/ACL 2006 Work-
shop on Spoken Language Technology, Aruba.
Galley, Michel, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of NAACL-04, pages 273?280.
Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING/ACL-06, pages 961?968, July.
Graehl, Jonathan and Kevin Knight. 2004. Training
tree transducers. In Proceedings of NAACL-04.
Hofmann, Thomas. 1999. Probabilistic latent semantic
analysis. In Uncertainity in Artificial Intelligence,
UAI?99, Stockholm.
Huang, Liang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Bi-
ennial Conference of the Association for Machine
Translation in the Americas (AMTA), Boston, MA.
Johnson, Christopher R., Charles J. Fillmore, Miriam
R. L. Petruck, Collin F. Baker, Michael Ellsworth,
Josef Ruppenhofer, and Esther J. Wood. 2002.
FrameNet: Theory and practice. Version 1.0,
http://www.icsi.berkeley.edu/framenet/.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL-03, Edmonton, Alberta.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of ACL, Demonstration Session, pages
177?180.
Koehn, Philipp. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388?395, Barcelona, Spain, July.
Liu, Ding and Daniel Gildea. 2008. Improved tree-
to-string transducers for machine translation. In
ACL Workshop on Statistical Machine Translation
(ACL08-SMT), pages 62?69, Columbus, Ohio.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of COLING/ACL-06,
Sydney, Australia, July.
May, Jonathan and Kevin Knight. 2007. Syntactic
re-alignment models for machine translation. In Pro-
ceedings of EMNLP.
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
ACL-02.
Pradhan, Sameer, Wayne Ward, Kadri Hacioglu, James
Martin, , and Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In Proceed-
ings of NAACL-04.
Rooth, Mats, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via EM-based clustering. In
Proceedings of the 37th Annual Meeting of the ACL,
pages 104?111, College Park, Maryland.
Stolcke, Andreas. 2002. SRILM - an extensible lan-
guage modeling toolkit. In International Conference
on Spoken Language Processing, volume 2, pages
901?904.
Toutanova, Kristina, Aria Haghighi, and Christopher
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of ACL-05, pages 589?
596.
Wu, Dekai and Pascale Fung. 2009. Semantic roles
for smt: A hybrid two-pass model. In Proceedings
of the HLT-NAACL 2009: Short Papers, Boulder,
Colorado.
Xue, Nianwen and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of EMNLP.
Zhu, Ciyou, Richard H. Byrd, Peihuang Lu, and Jorge
Nocedal. 1994. L-BFGS-B: Fortran subroutines for
large-scale bound constrained optimization. Techni-
cal report, ACM Trans. Math. Software.
724
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596?605,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Fast Fertility Hidden Markov Model for Word Alignment Using MCMC
Shaojun Zhao and Daniel Gildea
Department of Computer Science
University of Rochester
Abstract
A word in one language can be translated to
zero, one, or several words in other languages.
Using word fertility features has been shown
to be useful in building word alignment mod-
els for statistical machine translation. We built
a fertility hidden Markov model by adding fer-
tility to the hidden Markov model. This model
not only achieves lower alignment error rate
than the hidden Markov model, but also runs
faster. It is similar in some ways to IBM
Model 4, but is much easier to understand. We
use Gibbs sampling for parameter estimation,
which is more principled than the neighbor-
hood method used in IBM Model 4.
1 Introduction
IBM models and the hidden Markov model (HMM)
for word alignment are the most influential statistical
word alignment models (Brown et al, 1993; Vogel et
al., 1996; Och and Ney, 2003). There are three kinds
of important information for word alignment mod-
els: lexicality, locality and fertility. IBM Model 1
uses only lexical information; IBM Model 2 and the
hidden Markov model take advantage of both lexi-
cal and locality information; IBM Models 4 and 5
use all three kinds of information, and they remain
the state of the art despite the fact that they were de-
veloped almost two decades ago.
Recent experiments on large datasets have shown
that the performance of the hidden Markov model is
very close to IBM Model 4. Nevertheless, we be-
lieve that IBM Model 4 is essentially a better model
because it exploits the fertility of words in the tar-
get language. However, IBM Model 4 is so com-
plex that most researches use the GIZA++ software
package (Och and Ney, 2003), and IBM Model 4 it-
self is treated as a black box. The complexity in IBM
Model 4 makes it hard to understand and to improve.
Our goal is to build a model that includes lexicality,
locality, and fertility; and, at the same time, to make
it easy to understand. We also want it to be accurate
and computationally efficient.
There have been many years of research on word
alignment. Our work is different from others in
essential ways. Most other researchers take either
the HMM alignments (Liang et al, 2006) or IBM
Model 4 alignments (Cherry and Lin, 2003) as in-
put and perform post-processing, whereas our model
is a potential replacement for the HMM and IBM
Model 4. Directly modeling fertility makes our
model fundamentally different from others. Most
models have limited ability to model fertility. Liang
et al (2006) learn the alignment in both translation
directions jointly, essentially pushing the fertility to-
wards 1. ITG models (Wu, 1997) assume the fer-
tility to be either zero or one. It can model phrases,
but the phrase has to be contiguous. There have been
works that try to simulate fertility using the hidden
Markov model (Toutanova et al, 2002; Deng and
Byrne, 2005), but we prefer to model fertility di-
rectly.
Our model is a coherent generative model that
combines the HMM and IBM Model 4. It is easier to
understand than IBM Model 4 (see Section 3). Our
model also removes several undesired properties in
IBM Model 4. We use Gibbs sampling instead of a
heuristic-based neighborhood method for parameter
596
estimation. Our distortion parameters are similar to
IBM Model 2 and the HMM, while IBM Model 4
uses inverse distortion (Brown et al, 1993). Our
model assumes that fertility follows a Poisson distri-
bution, while IBM Model 4 assumes a multinomial
distribution, and has to learn a much larger number
of parameters, which makes it slower and less reli-
able. Our model is much faster than IBM Model 4.
In fact, we will show that it is also faster than the
HMM, and has lower alignment error rate than the
HMM.
Parameter estimation for word alignment models
that model fertility is more difficult than for mod-
els without fertility. Brown et al (1993) and Och
and Ney (2003) first compute the Viterbi alignments
for simpler models, then consider only some neigh-
bors of the Viterbi alignments for modeling fertil-
ity. If the optimal alignment is not in those neigh-
bors, this method will not be able find the opti-
mal alignment. We use the Markov Chain Monte
Carlo (MCMC) method for training and decoding,
which has nice probabilistic guarantees. DeNero et
al. (2008) applied the Markov Chain Monte Carlo
method to word alignment for machine translation;
they do not model word fertility.
2 Statistical Word Alignment Models
2.1 Alignment and Fertility
Given a source sentence fJ1 = f1, f2, . . . , fJ and a
target sentence eI1 = e1, e2, . . . , eI , we define the
alignments between the two sentences as a subset of
the Cartesian product of the word positions. Fol-
lowing Brown et al (1993), we assume that each
source word is aligned to exactly one target word.
We denote as aJ1 = a1, a2, . . . , aJ the alignments
between fJ1 and eI1. When a word fj is not aligned
with any word e, aj is 0. For convenience, we add
an empty word ? to the target sentence at position 0
(i.e., e0 = ?). However, as we will see, we have
to add more than one empty word for the HMM.
In order to compute the ?jump probability? in the
HMM model, we need to know the position of the
aligned target word for the previous source word. If
the previous source word aligns to an empty word,
we could use the position of the empty word to indi-
cate the nearest previous source word that does not
align to an empty word. For this reason, we use a
total of I + 1 empty words for the HMM model1.
Moore (2004) also suggested adding multiple empty
words to the target sentence for IBM Model 1. After
we add I+1 empty words to the target sentence, the
alignment is a mapping from source to target word
positions:
a : j ? i, i = aj
where j = 1, 2, . . . , J and i = 1, 2, . . . , 2I + 1.
Words from position I + 1 to 2I + 1 in the target
sentence are all empty words.
We allow each source word to align with exactly
one target word, but each target word may align with
multiple source words.
The fertility ?i of a word ei at position i is defined
as the number of aligned source words:
?i =
J
?
j=1
?(aj , i)
where ? is the Kronecker delta function:
?(x, y) =
{
1 if x = y
0 otherwise
In particular, the fertility of all empty words in
the target sentence is
?2I+1
i=I+1 ?i. We define ?? ?
?2I+1
i=I+1 ?i. For a bilingual sentence pair e
2I+1
1 and
fJ1 , we have
?I
i=1 ?i + ?? = J .
The inverted alignments for position i in the tar-
get sentence are a set Bi, such that each element in
Bi is aligned with i, and all alignments of i are in
Bi. Inverted alignments are explicitly used in IBM
Models 3, 4 and 5, but not in our model, which is
one reason that our model is easier to understand.
2.2 IBM Model 1 and HMM
IBM Model 1 and the HMM are both generative
models, and both start by defining the probabil-
ity of alignments and source sentence given the
target sentence: P (aJ1 , fJ1 |e2I+11 ); the data likeli-
hood can be computed by summing over alignments:
1If fj?1 does not align with an empty word and fj aligns
with an empty word, we want to record the position of the target
word that fj?1 aligns with. There are I + 1 possibilities: fj is
the first word in the source sentence, or fj?1 aligns with one of
the target word.
597
P (fJ1 |e2I+11 ) =
?
aJ1
P (aJ1 , fJ1 |e2I+11 ). The align-
ments aJ1 are the hidden variables. The expectation
maximization algorithm is used to learn the parame-
ters such that the data likelihood is maximized.
Without loss of generality, P (aJ1 , fJ1 |e2I+11 ) can
be decomposed into length probabilities, distor-
tion probabilities (also called alignment probabil-
ities), and lexical probabilities (also called transla-
tion probabilities):
P (aJ1 , fJ1 |e2I+11 )
= P (J |e2I+11 )
J
?
j=1
P (aj , fj |f j?11 , a
j?1
1 , e2I+11 )
= P (J |e2I+11 )
J
?
j=1
(
P (aj |f j?11 , a
j?1
1 , e2I+11 ) ?
P (fj |f j?11 , a
j
1, e2I+11 )
)
where P (J |e2I+11 ) is a length probability,
P (aj |f j?11 , a
j?1
1 , e2I+11 ) is a distortion prob-
ability and P (fj |f j?11 , a
j
1, e2I+11 ) is a lexical
probability.
IBM Model 1 assumes a uniform distortion prob-
ability, a length probability that depends only on the
length of the target sentence, and a lexical probabil-
ity that depends only on the aligned target word:
P (aJ1 , fJ1 |e2I+11 ) =
P (J |I)
(2I + 1)J
J
?
j=1
P (fj |eaj )
The hidden Markov model assumes a length prob-
ability that depends only on the length of the target
sentence, a distortion probability that depends only
on the previous alignment and the length of the tar-
get sentence, and a lexical probability that depends
only on the aligned target word:
P (aJ1 , fJ1 |e2I+11 ) =
P (J |I)
J
?
j=1
P (aj |aj?1, I)P (fj |eaj )
In order to make the HMM work correctly, we en-
force the following constraints (Och and Ney, 2003):
P (i+ I + 1|i?, I) = p0?(i, i?)
P (i+ I + 1|i? + I + 1, I) = p0?(i, i?)
P (i|i? + I + 1, I) = P (i|i?, I)
where the first two equations imply that the proba-
bility of jumping to an empty word is either 0 or p0,
and the third equation implies that the probability of
jumping from a non-empty word is the same as the
probability of jumping from the corespondent empty
word.
The absolute position in the HMM is not impor-
tant, because we re-parametrize the distortion prob-
ability in terms of the distance between adjacent
alignment points (Vogel et al, 1996; Och and Ney,
2003):
P (i|i?, I) = c(i? i
?)
?
i?? c(i?? ? i?)
where c( ) is the count of jumps of a given distance.
In IBM Model 1, the word order does not mat-
ter. The HMM is more likely to align a source
word to a target word that is adjacent to the previ-
ous aligned target word, which is more suitable than
IBM Model 1 because adjacent words tend to form
phrases.
For these two models, in theory, the fertility for
a target word can be as large as the length of the
source sentence. In practice, the fertility for a target
word in IBM Model 1 is not very big except for rare
target words, which can become a garbage collector,
and align to many source words (Brown et al, 1993;
Och and Ney, 2003; Moore, 2004). The HMM is
less likely to have this garbage collector problem be-
cause of the alignment probability constraint. How-
ever, fertility is an inherent cross-language property
and these two models cannot assign consistent fer-
tility to words. This is our motivation for adding fer-
tility to these two models, and we expect that the re-
sulting models will perform better than the baseline
models. Because the HMM performs much better
than IBM Model 1, we expect that the fertility hid-
den Markov model will perform much better than
the fertility IBM Model 1. Throughout the paper,
?our model? refers to the fertility hidden Markov
model.
Due to space constraints, we are unable to pro-
vide details for IBM Models 3, 4 and 5; see Brown
et al (1993) and Och and Ney (2003). But we want
to point out that the locality property modeled in the
HMM is missing in IBM Model 3, and is modeled
invertedly in IBM Model 4. IBM Model 5 removes
deficiency (Brown et al, 1993; Och and Ney, 2003)
598
from IBM Model 4, but it is computationally very
expensive due to the larger number of parameters
than IBM Model 4, and IBM Model 5 often provides
no improvement on alignment accuracy.
3 Fertility Hidden Markov Model
Our fertility IBM Model 1 and fertility HMM
are both generative models and start by defin-
ing the probability of fertilities (for each
non-empty target word and all empty words),
alignments, and the source sentence given
the target sentence: P (?I1, ??,aJ1 , fJ1 |e2I+11 );
the data likelihood can be computed by
summing over fertilities and alignments:
P (fJ1 |e2I+11 ) =
?
?I1,??,aJ1
P (?I1, ??,aJ1 , fJ1 |e2I+11 ).
The fertility for a non-empty word ei is a random
variable ?i, and we assume ?i follows a Poisson dis-
tribution Poisson(?i;?(ei)). The sum of the fer-
tilities of all the empty words (??) grows with the
length of the target sentence. Therefore, we assume
that ?? follows a Poisson distribution with parameter
I?(?).
Now P (?I1, ??,aJ1 , fJ1 |e2I+11 ) can be decomposed
in the following way:
P (?I1, ??,aJ1 , fJ1 |e2I+11 )
= P (?I1|e2I+11 )P (??|?I1, e2I+11 )?
J
?
j=1
P (aj , fj |f j?11 , a
j?1
1 , e2I+11 , ?I1, ??)
=
I
?
i=1
?(ei)?ie??(ei)
?i!
?
(I?(?))?? e?I?(?)
??!
?
J
?
j=1
(
P (aj |f j?11 , a
j?1
1 , e2I+11 , ?I1, ??) ?
P (fj |f j?11 , a
j
1, e2I+11 , ?I1, ??)
)
Superficially, we only try to model the length
probability more accurately. However, we also en-
force the fertility for the same target word across the
corpus to be consistent. The expected fertility for a
non-empty word ei is ?(ei), and the expected fertil-
ity for all empty words is I?(?). Any fertility value
has a non-zero probability, but fertility values that
are further away from the mean have low probabil-
ity. IBM Models 3, 4, and 5 use a multinomial distri-
bution for fertility, which has a much larger number
of parameters to learn. Our model has only one pa-
rameter for each target word, which can be learned
more reliably.
In the fertility IBM Model 1, we assume that
the distortion probability is uniform, and the lexical
probability depends only on the aligned target word:
P (?I1, ??,aJ1 , fJ1 |e2I+11 )
=
I
?
i=1
?(ei)?ie??(ei)
?i!
?
(I?(?))?? e?(I?(?))
??!
?
1
(2I + 1)J
J
?
j=1
P (fj |eaj ) (1)
In the fertility HMM, we assume that the distor-
tion probability depends only on the previous align-
ment and the length of the target sentence, and that
the lexical probability depends only on the aligned
target word:
P (?I1, ??,aJ1 , fJ1 |e2I+11 )
=
I
?
i=1
?(ei)?ie??(ei)
?i!
?
(I?(?))?? e?(I?(?))
??!
?
J
?
j=1
P (aj |aj?1, I)P (fj |eaj ) (2)
When we compute P (fJ1 |e2I+11 ), we only sum
over fertilities that agree with the alignments:
P (fJ1 |e2I+11 ) =
?
aJ1
P (aJ1 , fJ1 |e2I+11 )
599
where
P (aJ1 , fJ1 |e2I+11 )
=
?
?I1,??
P (?I1, ??,aJ1 , fJ1 |e2I+11 )
? P (?I1, ??,aJ1 , fJ1 |e2I+11 )?
I
?
i=1
?
?
?
J
?
j=1
?(aj , i), ?i
?
??
?
?
?
2I+1
?
i=I+1
J
?
j=1
?(aj , i), ??
?
? (3)
In the last two lines of Equation 3, ?? and each
?i are not free variables, but are determined by
the alignments. Because we only sum over fer-
tilities that are consistent with the alignments, we
have
?
fJ1
P (fJ1 |e2I+11 ) < 1, and our model is de-
ficient, similar to IBM Models 3 and 4 (Brown et
al., 1993). We can remove the deficiency for fertil-
ity IBM Model 1 by assuming a different distortion
probability: the distortion probability is 0 if fertility
is not consistent with alignments, and uniform oth-
erwise. The total number of consistent fertility and
alignments is J !??!?Jj=1 ?i!
. Replacing 1(2I+1)J with
??!
?J
j=1 ?i!
J ! , we have:
P (?I1, ??,aJ1 , fJ1 |e2I+11 )
=
I
?
i=1
?(ei)?ie??(ei) ?
(I?(?))?? e?(I?(?)) ?
1
J !
J
?
j=1
P (fj |eaj )
In our experiments, we did not find a noticeable
change in terms of alignment accuracy by removing
the deficiency.
4 Expectation Maximization Algorithm
We estimate the parameters by maximizing
P (fJ1 |e2I+11 ) using the expectation maximization
(EM) algorithm (Dempster et al, 1977). The
auxiliary function is:
L(P (f |e), P (a|a?), ?(e), ?1(e), ?2(a?))
=
?
aJ1
P? (aJ1 |e2I+11 , fJ1 ) logP (aJ1 , fJ1 |e2I+11 )
?
?
e
?1(e)(
?
f
P (f |e)? 1)
?
?
a?
?2(a?)(
?
a
P (a|a?)? 1)
Because P (aJ1 , fJ1 |e2I+11 ) is in the exponential
family, we get a closed form for the parameters from
expected counts:
P (f |e) =
?
s c(f |e; f (s), e(s))
?
f
?
s c(f |e; f (s), e(s))
(4)
P (a|a?) =
?
s c(a|a?; f (s), e(s))
?
a
?
s c(a|a?; f (s), e(s))
(5)
?(e) =
?
s c(?|e; f (s), e(s))
?
s c(k|e; f (s), e(s))
(6)
where s is the number of bilingual sentences, and
c(f |e; fJ1 , e2I+11 ) =
?
aJ1
P? (aJ1 |fJ1 , e2I+11 )?
?
j
?(fj , f)?(ei, e)
c(a|a?; fJ1 , e2I+11 ) =
?
aJ1
P? (aJ1 |fJ1 , e2I+11 )?
?
j
?(aj , a)?(aj?1, a?)
c(?|e; fJ1 , e2I+11 ) =
?
aJ1
P? (aJ1 |fJ1 , e2I+11 )?
?
i
?i?(ei, e)
c(k|e; fJ1 , e2I+11 ) =
?
i
k(ei)?(ei, e)
These equations are for the fertility hidden
Markov model. For the fertility IBM Model 1, we
do not need to estimate the distortion probability.
5 Gibbs Sampling for Fertility HMM
Although we can estimate the parameters by using
the EM algorithm, in order to compute the expected
600
counts, we have to sum over all possible alignments
aJ1 , which is, unfortunately, exponential. We devel-
oped a Gibbs sampling algorithm (Geman and Ge-
man, 1984) to compute the expected counts.
For each target sentence e2I+11 and source sen-
tence fJ1 , we initialize the alignment aj for each
source word fj using the Viterbi alignments from
IBM Model 1. During the training stage, we try all
2I + 1 possible alignments for aj but fix all other
alignments.2 We choose alignment aj with probabil-
ity P (aj |a1, ? ? ? aj?1, aj+1 ? ? ? aJ , fJ1 , e2I+11 ), which
can be computed in the following way:
P (aj |a1, ? ? ? , aj?1, aj+1, ? ? ? , aJ , fJ1 , e2I+11 )
=
P (aJ1 , fJ1 |e2I+11 )
?
aj P (a
J
1 , fJ1 |e2I+11 )
(7)
For each alignment variable aj , we choose t sam-
ples. We scan through the corpus many times until
we are satisfied with the parameters we learned us-
ing Equations 4, 5, and 6. This Gibbs sampling
method updates parameters constantly, so it is an
?online learning? algorithm. However, this sampling
method needs a large amount of communication be-
tween machines in order to keep the parameters up
to date if we compute the expected counts in parallel.
Instead, we do ?batch learning?: we fix the parame-
ters, scan through the entire corpus and compute ex-
pected counts in parallel (E-step); then combine all
the counts together and update the parameters (M-
step). This is analogous to what IBM models and
the HMM do in the EM algorithms. The algorithm
for the E-step on one machine (all machines are in-
dependent) is in Algorithm 1.
For the fertility hidden Markov model, updating
P (aJ1 , fJ1 |e2I+11 ) whenever we change the alignment
aj can be done in constant time, so the complexity
of choosing t samples for all aj (j = 1, 2, . . . , J) is
O(tIJ). This is the same complexity as the HMM
if t is O(I), and it has lower complexity if t is a
constant. Surprisingly, we can achieve better results
than the HMM by computing as few as 1 sample
for each alignment, so the fertility hidden Markov
model is much faster than the HMM. Even when
choosing t such that our model is 5 times faster than
the HMM, we achieve better results.
2For fertility IBM Model 1, we only need to compute I + 1
values because e2I+1I+1 are identical empty words.
Algorithm 1: One iteration of E-step: draw
t samples for each aj for each sentence pair
(fJ1 , e2I+11 ) in the corpus
for (fJ1 , e2I+11 ) in the corpus do
Initialize aJ1 with IBM Model 1;
for t do
for j do
for i do
aj = i;
Compute P (aJ1 , fJ1 |e2I+11 );
end
Draw a sample for aj using
Equation 7;
Update counts;
end
end
end
We also consider initializing the alignments using
the HMM Viterbi algorithm in the E-step. In this
case, the fertility hidden Markov model is not faster
than the HMM. Fortunately, initializing using IBM
Model 1 Viterbi does not decrease the accuracy in
any noticeable way, and reduces the complexity of
the Gibbs sampling algorithm.
In the testing stage, the sampling algorithm is the
same as above except that we keep the alignments
aJ1 that maximize P (aJ1 , fJ1 |e2I+11 ). We need more
samples in the testing stage because it is unlikely
to get to the optimal alignments by sampling a few
times for each alignment. On the contrary, in the
above training stage, although the samples are not
accurate enough to represent the distribution defined
by Equation 7 for each alignment aj , it is accurate
enough for computing the expected counts, which
are defined at the corpus level. Interestingly, we
found that throwing away the fertility and using the
HMM Viterbi decoding achieves same results as the
sampling approach (we can ignore the difference be-
cause it is tiny), but is faster. Therefore, we use
Gibbs sampling for learning and the HMM Viterbi
decoder for testing.
Gibbs sampling for the fertility IBM Model 1 is
similar but simpler. We omit the details here.
601
Alignment Model P R AER
en ? cn
IBM1 49.6 55.3 47.8
IBM1F 55.4 57.1 43.8
HMM 62.6 59.5 39.0
HMMF-1 65.4 59.1 37.9
HMMF-5 66.8 60.8 36.2
HMMF-30 67.8 62.3 34.9
IBM4 66.8 64.1 34.5
cn ? en
IBM1 52.6 53.7 46.9
IBM1F 55.9 56.4 43.9
HMM 66.1 62.1 35.9
HMMF-1 68.6 60.2 35.7
HMMF-5 71.1 62.2 33.5
HMMF-30 71.1 62.7 33.2
IBM4 69.3 68.5 31.1
Table 1: AER results. IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM. We choose t = 1,
5, and 30 for the fertility HMM.
 0.32
 0.34
 0.36
 0.38
 0.4
 0.42
 0.44
 0.46
 0.48
AE
R
IBM1
IBM1F
HMM
HMMF-1
HMMF-5
HMMF-30
IBM4
Figure 1: AER comparison (en?cn)
602
 0.32
 0.34
 0.36
 0.38
 0.4
 0.42
 0.44
 0.46
 0.48
AE
R
IBM1
IBM1F
HMM
HMMF-1
HMMF-5
HMMF-30
IBM4
Figure 2: AER comparison (cn ?en)
 0
 1000
 2000
 3000
 4000
 5000
tra
in
in
g 
tim
e 
[se
co
nd
s]
IBM1
IBM1F
HMM
HMMF-1
HMMF-5
HMMF-30
IBM4
Figure 3: Training time comparison. The training time for each model is calculated from scratch. For example, the
training time of IBM Model 4 includes the training time of IBM Model 1, the HMM, and IBM Model 3.
603
6 Experiments
We evaluated our model by computing the word
alignment and machine translation quality. We use
the alignment error rate (AER) as the word align-
ment evaluation criterion. Let A be the alignments
output by word alignment system, P be a set of pos-
sible alignments, and S be a set of sure alignments
both labeled by human beings. S is a subset of P .
Precision, recall, and AER are defined as follows:
recall =
|A ? S|
|S|
precision =
|A ? P |
|A|
AER(S, P,A) = 1? |A ? S|+ |A ? P ||A|+ |S|
AER is an extension to F-score. Lower AER is bet-
ter.
We evaluate our fertility models on a Chinese-
English corpus. The Chinese-English data taken
from FBIS newswire data, and has 380K sentence
pairs, and we use the first 100K sentence pairs as
our training data. We used hand-aligned data as ref-
erence. The Chinese-English data has 491 sentence
pairs.
We initialize IBM Model 1 and the fertility IBM
Model 1 with a uniform distribution. We smooth
all parameters (?(e) and P (f |e)) by adding a small
value (10?8), so they never become too small. We
run both models for 5 iterations. AER results are
computed using the IBM Model 1 Viterbi align-
ments, and the Viterbi alignments obtained from the
Gibbs sampling algorithm.
We initialize the HMM and the fertility HMM
with the parameters learned in the 5th iteration of
IBM Model 1. We smooth all parameters (?(e),
P (a|a?) and P (f |e)) by adding a small value (10?8).
We run both models for 5 iterations. AER results are
computed using traditional HMM Viterbi decoding
for both models.
It is always difficult to determine how many sam-
ples are enough for sampling algorithms. However,
both fertility models achieve better results than their
baseline models using a small amount of samples.
For the fertility IBM Model 1, we sample 10 times
for each aj , and restart 3 times in the training stage;
we sample 100 times and restart 12 times in the test-
ing stage. For the fertility HMM, we sample 30
times for each aj with no restarting in the training
stage; no sampling in the testing stage because we
use traditional HMM Viterbi decoding for testing.
More samples give no further improvement.
Initially, the fertility IBM Model 1 and fertility
HMM did not perform well. If a target word e
only appeared a few times in the training corpus, our
model cannot reliably estimate the parameter ?(e).
Hence, smoothing is needed. One may try to solve
it by forcing all these words to share a same pa-
rameter ?(einfrequent). Unfortunately, this does not
solve the problem because all infrequent words tend
to have larger fertility than they should. We solve
the problem in the following way: estimate the pa-
rameter ?(enon empty) for all non-empty words, all
infrequent words share this parameter. We consider
words that appear less than 10 times as infrequent
words.
Table 1, Figure 1, and Figure 2 shows the AER
results for different models. We can see that the fer-
tility IBM Model 1 consistently outperforms IBM
Model 1, and the fertility HMM consistently outper-
forms the HMM.
The fertility HMM not only has lower AER than
the HMM, it also runs faster than the HMM. Fig-
ure 3 show the training time for different models.
In fact, with just 1 sample for each alignment, our
model archives lower AER than the HMM, and runs
more than 5 times faster than the HMM. It is pos-
sible to use sampling instead of dynamic program-
ming in the HMM to reduce the training time with
no decrease in AER (often an increase). We con-
clude that the fertility HMM not only has better AER
results, but also runs faster than the hidden Markov
model.
We also evaluate our model by computing the
machine translation BLEU score (Papineni et al,
2002) using the Moses system (Koehn et al, 2007).
The training data is the same as the above word
alignment evaluation bitexts, with alignments for
each model symmetrized using the grow-diag-final
heuristic. Our test is 633 sentences of up to length
50, with four references. Results are shown in Ta-
ble 2; we see that better word alignment results do
not lead to better translations.
604
Model BLEU
HMM 19.55
HMMF-30 19.26
IBM4 18.77
Table 2: BLEU results
7 Conclusion
We developed a fertility hidden Markov model
that runs faster and has lower AER than the
HMM. Our model is thus much faster than IBM
Model 4. Our model is also easier to understand
than IBM Model 4. The Markov Chain Monte Carlo
method used in our model is more principled than
the heuristic-based neighborhood method in IBM
Model 4. While better word alignment results do not
necessarily correspond to better translation quality,
our translation results are comparable in translation
quality to both the HMM and IBM Model 4.
Acknowledgments We would like to thank Tagy-
oung Chung, Matt Post, and the anonymous review-
ers for helpful comments. This work was supported
by NSF grants IIS-0546554 and IIS-0910611.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In Proceedings of
ACL-03.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(1):1?21.
John DeNero, Alexandre Bouchard-Cote, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of EMNLP.
Yonggang Deng and William Byrne. 2005. HMM word
and phrase alignment for statistical machine transla-
tion. In Proceedings of Human Language Technology
Conference and Conference on Empirical Methods in
Natural Language Processing, pages 169?176, Van-
couver, British Columbia, Canada, October. Associa-
tion for Computational Linguistics.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distribution, and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, PAMI-6(6):721?741,
November.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL, Demonstration Session, pages 177?180.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In North American Association for Com-
putational Linguistics (NAACL), pages 104?111.
Robert C. Moore. 2004. Improving IBM word alignment
Model 1. In Proceedings of the 42nd Meeting of the
Association for Computational Linguistics (ACL?04),
Main Volume, pages 518?525, Barcelona, Spain, July.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of ACL-
02.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D.
Manning. 2002. Extensions to HMM-based statis-
tical word alignment models. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002), pages 87?94.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In COLING-96, pages 836?841.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
605
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 636?645,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Effects of Empty Categories on Machine Translation
Tagyoung Chung and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We examine effects that empty categories have
on machine translation. Empty categories are
elements in parse trees that lack corresponding
overt surface forms (words) such as dropped
pronouns and markers for control construc-
tions. We start by training machine trans-
lation systems with manually inserted empty
elements. We find that inclusion of some
empty categories in training data improves the
translation result. We expand the experiment
by automatically inserting these elements into
a larger data set using various methods and
training on the modified corpus. We show that
even when automatic prediction of null ele-
ments is not highly accurate, it nevertheless
improves the end translation result.
1 Introduction
An empty category is an element in a parse tree
that does not have a corresponding surface word.
They include traces such as Wh-traces which indi-
cate movement operations in interrogative sentences
and dropped pronouns which indicate omission of
pronouns in places where pronouns are normally
expected. Many treebanks include empty nodes in
parse trees to represent non-local dependencies or
dropped elements. Examples of the former include
traces such as relative clause markers in the Penn
Treebank (Bies et al, 1995). An example of the lat-
ter include dropped pronouns in the Korean Tree-
bank (Han and Ryu, 2005) and the Chinese Tree-
bank (Xue and Xia, 2000).
In languages such as Chinese, Japanese, and Ko-
rean, pronouns are frequently or regularly dropped
when they are pragmatically inferable. These lan-
guages are called pro-drop languages. Dropped pro-
nouns are quite a common phenomenon in these lan-
guages. In the Chinese Treebank, they occur once
in every four sentences on average. In Korean the
Treebank, they are even more frequent, occurring
in almost every sentence on average. Translating
these pro-drop languages into languages such as En-
glish where pronouns are regularly retained could
be problematic because English pronouns have to be
generated from nothing.
There are several different strategies to counter
this problem. A special NULL word is typically
used when learning word alignment (Brown et al,
1993). Words that have non-existent counterparts
can be aligned to the NULL word. In phrase-based
translation, the phrase learning system may be able
to learn pronouns as a part of larger phrases. If the
learned phrases include pronouns on the target side
that are dropped from source side, the system may
be able to insert pronouns even when they are miss-
ing from the source language. This is an often ob-
served phenomenon in phrase-based translation sys-
tems. Explicit insertion of missing words can also
be included in syntax-based translation models (Ya-
mada and Knight, 2001). For the closely related
problem of inserting grammatical function particles
in English-to-Korean and English-to-Japanese ma-
chine translation, Hong et al (2009) and Isozaki et
al. (2010) employ preprocessing techniques to add
special symbols to the English source text.
In this paper, we examine a strategy of automat-
ically inserting two types of empty elements from
the Korean and Chinese treebanks as a preprocess-
636
Korean
*T* 0.47 trace of movement
(NP *pro*) 0.88 dropped subject or object
(WHNP *op*) 0.40 empty operator in relative
constructions
*?* 0.006 verb deletion, VP ellipsis,
and others
Chinese
(XP (-NONE- *T*)) 0.54 trace of A?-movement
(NP (-NONE- *)) 0.003 trace of A-movement
(NP (-NONE- *pro*)) 0.27 dropped subject or object
(NP (-NONE- *PRO*)) 0.31 control structures
(WHNP (-NONE- *OP*)) 0.53 empty operator in relative
constructions
(XP (-NONE- *RNR*)) 0.026 right node raising
(XP (-NONE- *?*)) 0 others
Table 1: List of empty categories in the Korean Treebank
(top) and the Chinese Treebank (bottom) and their per-
sentence frequencies in the training data of initial experi-
ments.
ing step. We first describe our experiments with data
that have been annotated with empty categories, fo-
cusing on zero pronouns and traces such as those
used in control constructions. We use these an-
notations to insert empty elements in a corpus and
train a machine translation system to see if they im-
prove translation results. Then, we illustrate differ-
ent methods we have devised to automatically insert
empty elements to corpus. Finally, we describe our
experiments with training machine translation sys-
tems with corpora that are automatically augmented
with empty elements. We conclude this paper by
discussing possible improvements to the different
methods we describe in this paper.
2 Initial experiments
2.1 Setup
We start by testing the plausibility of our idea
of preprocessing corpus to insert empty cate-
gories with ideal datasets. The Chinese Treebank
(LDC2005T01U01) is annotated with null elements
and a portion of the Chinese Treebank has been
translated into English (LDC2007T02). The Korean
Treebank version 1.0 (LDC2002T26) is also anno-
tated with null elements and includes an English
translation. We extract null elements along with
tree terminals (words) and train a simple phrase-
BLEU
Chi-Eng No null elements 19.31
w/ *pro* 19.68
w/ *PRO* 19.54
w/ *pro* and *PRO* 20.20
w/ all null elements 20.48
Kor-Eng No null elements 20.10
w/ *pro* 20.37
w/ all null elements 19.71
Table 2: BLEU score result of initial experiments.
Each experiment has different empty categories added in.
*PRO* stands for the empty category used to mark con-
trol structures and *pro* indicates dropped pronouns for
both Chinese and Korean.
based machine translation system. Both datasets
have about 5K sentences and 80% of the data was
used for training, 10% for development, and 10%
for testing.
We used Moses (Koehn et al, 2007) to train
machine translation systems. Default parameters
were used for all experiments. The same number
of GIZA++ (Och and Ney, 2003) iterations were
used for all experiments. Minimum error rate train-
ing (Och, 2003) was run on each system afterwards,
and the BLEU score (Papineni et al, 2002) was cal-
culated on the test sets.
There are several different empty categories in
the different treebanks. We have experimented with
leaving in and out different empty categories for dif-
ferent experiments to see their effect. We hypoth-
esized that nominal phrasal empty categories such
as dropped pronouns may be more useful than other
ones, since they are the ones that may be missing in
the source language (Chinese and Korean) but have
counterparts in the target (English). Table 1 summa-
rizes empty categories in Chinese and Korean tree-
bank and their frequencies in the training data.
2.2 Results
Table 2 summarizes our findings. It is clear that
not all elements improve translation results when in-
cluded in the training data. For the Chinese to En-
glish experiment, empty categories that mark con-
trol structures (*PRO*), which serve as the sub-
ject of a dependent clause, and dropped pronouns
(*pro*), which mark omission of pragmatically in-
637
word P (e | ?pro?) word P (e | ?PRO?)
the 0.18 to 0.45
i 0.13 NULL 0.10
it 0.08 the 0.02
to 0.08 of 0.02
they 0.05 as 0.02
Table 3: A lexical translation table from the Korean-
English translation system (left) and a lexical transla-
tion from the Chinese-English translation system (right).
For the Korean-English lexical translation table, the left
column is English words that are aligned to a dropped
pronoun (*pro*) and the right column is the conditional
probability of P (e | ?pro?). For the Chinese-English
lexical translation table, the left column is English words
that are aligned to a control construction marker (*PRO*)
and the right column is the conditional probability of
P (e | ?PRO?).
ferable pronouns, helped to improve translation re-
sults the most. For the Korean to English experi-
ment, the dropped pronoun is the only empty cate-
gory that seems to improve translation.
For the Korean to English experiment, we also
tried annotating whether the dropped pronouns are a
subject, an object, or a complement using informa-
tion from the Treebank?s function tags, since English
pronouns are inflected according to case. However,
this did not yield a very different result and in fact
was slightly worse. This is possibly due to data spar-
sity created when dropped pronouns are annotated.
Dropped pronouns in subject position were the over-
whelming majority (91%), and there were too few
dropped pronouns in object position to learn good
parameters.
2.3 Analysis
Table 3 and Table 4 give us a glimpse of why having
these empty categories may lead to better transla-
tion. Table 3 is the lexical translation table for the
dropped pronoun (*pro*) from the Korean to En-
glish experiment and the marker for control con-
structions (*PRO*) from the Chinese to English ex-
periment. For the dropped pronoun in the Korean
to English experiment, although there are errors,
the table largely reflects expected translations of a
dropped pronoun. It is possible that the system is in-
serting pronouns in right places that would be miss-
ing otherwise. For the control construction marker
in the Chinese to English experiment, the top trans-
lation for *PRO* is the English word to, which is ex-
pected since Chinese clauses that have control con-
struction markers often translate to English as to-
infinitives. However, as we discuss in the next para-
graph, the presence of control construction markers
may affect translation results in more subtle ways
when combined with phrase learning.
Table 4 shows how translations from the system
trained with null elements and the system trained
without null elements differ. The results are taken
from the test set and show extracts from larger sen-
tences. Chinese verbs that follow the empty node for
control constructions (*PRO*) are generally trans-
lated to English as a verb in to-infinitive form, a
gerund, or a nominalized verb. The translation re-
sults show that the system trained with this null el-
ement (*PRO*) translates verbs that follow the null
element largely in such a manner. However, it may
not be always closest to the reference. It is exempli-
fied by the translation of one phrase.
Experiments in this section showed that prepro-
cessing the corpus to include some empty elements
can improve translation results. We also identified
which empty categories maybe helpful for improv-
ing translation for different language pairs. In the
next section, we focus on how we add these ele-
ments automatically to a corpus that is not annotated
with empty elements for the purpose of preprocess-
ing corpus for machine translation.
3 Recovering empty nodes
There are a few previous works that have attempted
restore empty nodes for parse trees using the Penn
English Treebank. Johnson (2002) uses rather sim-
ple pattern matching to restore empty categories as
well as their co-indexed antecedents with surpris-
ingly good accuracy. Gabbard et al (2006) present
a more sophisticated algorithm that tries to recover
empty categories in several steps. In each step, one
or more empty categories are restored using pat-
terns or classifiers (five maximum-entropy and two
perceptron-based classifiers to be exact).
What we are trying to achieve has obvious simi-
larity to these previous works. However, there are
several differences. First, we deal with different
languages. Second, we are only trying to recover
638
Chinese English Reference System trained w/ nulls System trained w/o nulls
*PRO*?? implementing implementation implemented
*PRO*???? have gradually formed to gradually form gradually formed
*PRO*?????? attracting foreign investment attracting foreign investment attract foreign capital
Table 4: The first column is a Chinese word or a phrase that immediately follows empty node marker for Chinese
control constructions. The second column is the English reference translation. The third column is the translation
output from the system that is trained with the empty categories added in. The fourth column is the translation output
from the system trained without the empty categories added, which was given the test set without the empty categories.
Words or phrases and their translations presented in the table are part of larger sentences.
a couple of empty categories that would help ma-
chine translation. Third, we are not interested in re-
covering antecedents. The linguistic differences and
the empty categories we are interested in recovering
made the task much harder than it is for English. We
will discuss this in more detail later.
From this section on, we will discuss only
Chinese-English translation because Chinese
presents a much more interesting case, since we
need to recover two different empty categories that
are very similarly distributed. Data availability
was also a consideration since much larger datasets
(bilingual and monolingual) are available for
Chinese. The Korean Treebank has only about 5K
sentences, whereas the version of Chinese Treebank
we used includes 28K sentences.
The Chinese Treebank was used for all experi-
ments that are mentioned in the rest of this Section.
Roughly 90% of the data was used for the training
set, and the rest was used for the test set. As we have
discussed in Section 2, we are interested in recover-
ing dropped pronouns (*pro*) and control construc-
tion markers (*PRO*). We have tried three different
relatively simple methods so that recovering empty
elements would not require any special infrastruc-
ture.
3.1 Pattern matching
Johnson (2002) defines a pattern for empty node re-
covery to be a minimally connected tree fragment
containing an empty node and all nodes co-indexed
with it. Figure 1 shows an example of a pattern. We
extracted patterns according this definition, and it
became immediately clear that the same definition
that worked for English will not work for Chinese.
Table 5 shows the top five patterns that match con-
trol constructions (*PRO*) and dropped pronouns
(*pro*). The top pattern that matches *pro* and
*PRO* are both exactly the same, since the pat-
tern will be matched against parse trees where empty
nodes have been deleted.
When it became apparent that we cannot use the
same definition of patterns to successfully restore
empty categories, we added more context to the pat-
terns. Patterns needed more context for them to be
able to disambiguate between sites that need to be
inserted with *pro*s and sites that need to be in-
serted with *PRO*s. Instead of using minimal tree
fragments that matched empty categories, we in-
cluded the parent and siblings of the minimal tree
fragment in the pattern (pattern matching method
1). This way, we gained more context. However,
as can be seen in Table 5, there is still a lot of over-
lap between patterns for the two empty categories.
However, it is more apparent that at least we can
choose the pattern that will maximize matches for
one empty category and then discard that pattern for
the other empty category.
We also tried giving patterns even more context
by including terminals if preterminals are present in
the pattern (pattern matching method 2). In this way,
we are able have more context for patterns such as
(VP VV (IP ( NP (-NONE- *PRO*) ) VP)) by know-
ing what the verb that precedes the empty category
is. Instead of the original pattern, we would have
patterns such as (VP (VV??) ( IP ( NP (-NONE-
*PRO*)) VP)). We are able to gain more context be-
cause some verbs select for a control construction.
The Chinese verb ?? generally translates to En-
glish as to decide and is more often followed by
a control construction than by a dropped pronoun.
Whereas the pattern (VP (VV ??) ( IP ( NP (-
NONE- *PRO*)) VP)) occurred 154 times in the
training data, the pattern (VP (VV ??) (IP (NP
(-NONE- *pro*)) VP)) occurred only 8 times in the
639
IP
NP-SBJ
-NONE-
*pro*
VP
VV
??
NP-OBJ
PN
??
PU
?
? IP
VP
VV
??
NP-OBJ
PN
??
PU
?
(IP (NP-SBJ (-NONE- *pro*)) VP PU) (IP VP PU)
Figure 1: An example of a tree with an empty node (left), the tree stripped of an empty node (right), and a pattern that
matches the example. Sentences are parsed without empty nodes and if a tree fragment (IP VP PU) is encountered in
a parse tree, the empty node may be inserted according to the learned pattern (IP (NP-SBJ (-NONE- *pro*)) VP PU).
*PRO* *pro*
Count Pattern Count Pattern
12269 ( IP ( NP (-NONE- *PRO*) ) VP ) 10073 ( IP ( NP (-NONE- *pro*) ) VP )
102 ( IP PU ( NP (-NONE- *PRO*) ) VP PU ) 657 ( IP ( NP (-NONE- *pro*) ) VP PU )
14 ( IP ( NP (-NONE- *PRO*) ) VP PRN ) 415 ( IP ADVP ( NP (-NONE- *pro*) ) VP )
13 ( IP NP ( NP (-NONE- *PRO*) ) VP ) 322 ( IP NP ( NP (-NONE- *pro*) ) VP )
12 ( CP ( NP (-NONE- *PRO*) ) CP ) 164 ( IP PP PU ( NP (-NONE- *pro*) ) VP )
*PRO* *pro*
Count Pattern Count Pattern
2991 ( VP VV NP ( IP ( NP (-NONE- *PRO*) ) VP ) ) 1782 ( CP ( IP ( NP (-NONE- *pro*) ) VP ) DEC )
2955 ( VP VV ( IP ( NP (-NONE- *PRO*) ) VP ) ) 1007 ( VP VV ( IP ( NP (-NONE- *pro*) ) VP ) )
850 ( CP ( IP ( NP (-NONE- *PRO*) ) VP ) DEC ) 702 ( LCP ( IP ( NP (-NONE- *pro*) ) VP ) LC )
765 ( PP P ( IP ( NP (-NONE- *PRO*) ) VP ) ) 684 ( IP IP PU ( IP ( NP (-NONE- *pro*) ) VP ) PU )
654 ( LCP ( IP ( NP (-NONE- *PRO*) ) VP ) LC ) 654 ( TOP ( IP ( NP (-NONE- *pro*) ) VP PU ) )
Table 5: Top five minimally connected patterns that match *pro* and *PRO* (top). Patterns that match both *pro*
and *PRO* are shaded with the same color. The table on the bottom show more refined patterns that are given added
context by including the parent and siblings to minimally connected patterns. Many patterns still match both *pro*
and *PRO* but there is a lesser degree of overlap.
640
training data.
After the patterns are extracted, we performed
pruning similar to the pruning that was done by
Johnson (2002). The patterns that have less than
50% chance of matching are discarded. For exam-
ple, if (IP VP) occurs one hundred times in a tree-
bank that is stripped of empty nodes and if pattern
(IP (NP (-NONE- *PRO*)) VP) occurs less than
fifty times in the same treebank that is annotated
with empty nodes, it is discarded.1 We also found
that we can discard patterns that occur very rarely
(that occur only once) without losing much accu-
racy. In cases where there was an overlap between
two empty categories, the pattern was chosen for
either *pro* or *PRO*, whichever that maximized
the number of matchings and then discarded for the
other.
3.2 Conditional random field
We tried building a simple conditional random field
(Lafferty et al, 2001) to predict null elements. The
model examines each and every word boundary and
decides whether to leave it as it is, insert *pro*,
or insert *PRO*. The obvious disadvantage of this
method is that if there are two consecutive null el-
ements, it will miss at least one of them. Although
there were some cases like this in the treebank, they
were rare enough that we decided to ignore them.
We first tried using only differently sized local win-
dows of words as features (CRF model 1). We also
experimented with adding the part-of-speech tags of
words as features (CRF model 2). Finally, we exper-
imented with a variation where the model is given
each word and its part-of-speech tag and its imme-
diate parent node as features (CRF model 3).
We experimented with using different regulariza-
tions and different values for regularizations but it
did not make much difference in the final results.
The numbers we report later used L2 regularization.
3.3 Parsing
In this approach, we annotated nonterminal symbols
in the treebank to include information about empty
categories and then extracted a context free gram-
mar from the modified treebank. We parsed with
the modified grammar, and then deterministically re-
1See Johnson (2002) for more details.
*PRO* *pro*
Cycle Prec. Rec. F1 Prec Rec. F1
1 0.38 0.08 0.13 0.38 0.08 0.12
2 0.52 0.23 0.31 0.37 0.18 0.24
3 0.59 0.46 0.52 0.43 0.24 0.31
4 0.62 0.50 0.56 0.47 0.25 0.33
5 0.61 0.52 0.56 0.47 0.33 0.39
6 0.60 0.53 0.56 0.46 0.39 0.42
7 0.58 0.52 0.55 0.43 0.40 0.41
Table 6: Result using the grammars output by the Berke-
ley state-splitting grammar trainer to predict empty cate-
gories
covered the empty categories from the trees. Fig-
ure 2 illustrates how the trees were modified. For
every empty node, the most immediate ancestor of
the empty node that has more than one child was an-
notated with information about the empty node, and
the empty node was deleted. We annotated whether
the deleted empty node was *pro* or *PRO* and
where it was deleted. Adding where the child was
necessary because, even though most empty nodes
are the first child, there are many exceptions.
We first extracted a plain context free grammar af-
ter modifying the trees and used the modified gram-
mar to parse the test set and then tried to recover the
empty elements. This approach did not work well.
We then applied the latent annotation learning pro-
cedures of Petrov et al (2006)2 to refine the non-
terminals in the modified grammar. This has been
shown to help parsing in many different situations.
Although the state splitting procedure is designed to
maximize the likelihood of of the parse trees, rather
than specifically to predict the empty nodes, learning
a refined grammar over modified trees was also ef-
fective in helping to predict empty nodes. Table 6
shows the dramatic improvement after each split,
merge, and smoothing cycle. The gains leveled off
after the sixth iteration and the sixth order grammar
was used to run later experiments.
3.4 Results
Table 7 shows the results of our experiments. The
numbers are very low when compared to accuracy
reported in other works that were mentioned in the
beginning of this Section, which dealt with the Penn
English Treebank. Dropped pronouns are especially
2http://code.google.com/p/berkeleyparser/
641
IP
NP-SBJ
-NONE-
*pro*
VP
VV
??
NP-OBJ
PN
??
PU
?
? SPRO0IP
VP
VV
??
NP-OBJ
PN
??
PU
?
Figure 2: An example of tree modification
*PRO* *pro*
Prec. Rec. F1 Prec Rec. F1
Pattern 1 0.65 0.61 0.63 0.41 0.23 0.29
Pattern 2 0.67 0.58 0.62 0.46 0.24 0.31
CRF 1 0.66 0.31 0.43 0.53 0.24 0.33
CRF 2 0.68 0.46 0.55 0.58 0.35 0.44
CRF 3 0.63 0.47 0.54 0.54 0.36 0.43
Parsing 0.60 0.53 0.56 0.46 0.39 0.42
Table 7: Result of recovering empty nodes
hard to recover. However, we are dealing with a dif-
ferent language and different kinds of empty cate-
gories. Empty categories recovered this way may
still help translation. In the next section, we take the
best variation of the each method use it to add empty
categories to a training corpus and train machine
translation systems to see whether having empty cat-
egories can help improve translation in more realis-
tic situations.
3.5 Analysis
The results reveal many interesting aspects about re-
covering empty categories. The results suggest that
tree structures are important features for finding sites
where markers for control constructions (*PRO*)
have been deleted. The method utilizing patterns
that have more information about tree structure of
these sites performed better than other methods. The
fact that the method using parsing was better at pre-
dicting *PRO*s than the methods that used the con-
ditional random fields also corroborates this finding.
For predicting dropped pronouns, the method using
the CRFs did better than the others. This suggests
that rather than tree structure, local context of words
and part-of-speech tags maybe more important fea-
tures for predicting dropped pronouns. It may also
suggest that methods using robust machine learning
techniques are better outfitted for predicting dropped
pronouns.
It is interesting to note how effective the parser
was at predicting empty categories. The method us-
ing the parser requires the least amount of supervi-
sion. The method using CRFs requires feature de-
sign, and the method that uses patterns needs hu-
man decisions on what the patterns should be and
pruning criteria. There is also room for improve-
ment. The split-merge cycles learn grammars that
produce better parse trees rather than grammars that
predict empty categories more accurately. By modi-
fying this learning process, we may be able to learn
grammars that are better suited for predicting empty
categories.
4 Experiments
4.1 Setup
For Chinese-English, we used a subset of FBIS
newswire data consisting of about 2M words and
60K sentences on the English side. For our develop-
ment set and test set, we had about 1000 sentences
each with 10 reference translations taken from the
NIST 2002 MT evaluation. All Chinese data was
re-segmented with the CRF-based Stanford Chinese
segmenter (Chang et al, 2008) that is trained on
the segmentation of the Chinese Treebank for con-
sistency. The parser used in Section 3 was used to
parse the training data so that null elements could
be recovered from the trees. The same method for
recovering null elements was applied to the train-
642
BLEU BP *PRO* *pro*
Baseline 23.73 1.000
Pattern 23.99 0.998 0.62 0.31
CRF 24.69* 1.000 0.55 0.44
Parsing 23.99 1.000 0.56 0.42
Table 8: Final BLEU score result. The asterisk indicates
statistical significance at p < 0.05 with 1000 iterations
of paired bootstrap resampling. BP stands for the brevity
penalty in BLEU. F1 scores for recovering empty cate-
gories are repeated here for comparison.
ing, development, and test sets to insert empty nodes
for each experiment. The baseline system was also
trained using the raw data.
We used Moses (Koehn et al, 2007) to train
machine translation systems. Default parameters
were used for all experiments. The same number
of GIZA++ (Och and Ney, 2003) iterations were
used for all experiments. Minimum error rate train-
ing (Och, 2003) was run on each system afterwards
and the BLEU score (Papineni et al, 2002) was cal-
culated on the test set.
4.2 Results
Table 8 summarizes our results. Generally, all sys-
tems produced BLEU scores that are better than the
baseline, but the best BLEU score came from the
system that used the CRF for null element insertion.
The machine translation system that used training
data from the method that was overall the best in
predicting empty elements performed the best. The
improvement is 0.96 points in BLEU score, which
represents statistical significance at p < 0.002 based
on 1000 iterations of paired bootstrap resampling
(Koehn, 2004). Brevity penalties applied for cal-
culating BLEU scores are presented to demonstrate
that the baseline system is not penalized for produc-
ing shorter sentences compared other systems.3
The BLEU scores presented in Table 8 represent
the best variations of each method we have tried
for recovering empty elements. Although the dif-
ference was small, when the F1 score were same
for two variations of a method, it seemed that we
could get slightly better BLEU score with the varia-
tion that had higher recall for recovering empty ele-
3We thank an anonymous reviewer for tipping us to examine
the brevity penalty.
ments rather the variation with higher precision.
We tried a variation of the experiment where the
CRF method is used to recover *pro* and the pattern
matching is used to recover *PRO*, since these rep-
resent the best methods for recovering the respective
empty categories. However, it was not as successful
as we thought would be. The resulting BLEU score
from the experiment was 24.24, which is lower than
the one that used the CRF method to recover both
*pro* and *PRO*. The problem was we used a very
na?ve method of resolving conflict between two dif-
ferent methods. The CRF method identified 17463
sites in the training data where *pro* should be
added. Of these sites, the pattern matching method
guessed 2695 sites should be inserted with *PRO*
rather than *pro*, which represent more than 15%
of total sites that the CRF method decided to in-
sert *pro*. In the aforementioned experiment, wher-
ever there was a conflict, both *pro* and *PRO*
were inserted. This probably lead the experiment
to have worse result than using only the one best
method. This experiment suggest that more sophisti-
cated methods should be considered when resolving
conflicts created by using heterogeneous methods to
recover different empty categories.
Table 9 shows five example translations of source
sentences in the test set that have one of the empty
categories. Since empty categories have been auto-
matically inserted, they are not always in the cor-
rect places. The table includes the translation results
from the baseline system where the training and test
sets did not have empty categories and the transla-
tion results from the system (the one that used the
CRF) that is trained on an automatically augmented
corpus and given the automatically augmented test
set.
5 Conclusion
In this paper, we have showed that adding some
empty elements can help building machine transla-
tion systems. We showed that we can still benefit
from augmenting the training corpus with empty el-
ements even when empty element prediction is less
than what would be conventionally considered ro-
bust.
We have also shown that there is a lot of room for
improvement. More comprehensive and sophisti-
643
source ???? *PRO*????????
reference china plans to invest in the infrastructure
system trained w/ nulls china plans to invest in infrastructure
system trained w/o nulls china ?s investment in infrastructure
source ?? *PRO*????????????
reference good for consolidating the trade and shipping center of hong kong
system trained w/ nulls favorable to the consolidation of the trade and shipping center in hong kong
system trained w/o nulls hong kong will consolidate the trade and shipping center
source ?????? *PRO*??????
reference some large - sized enterprises to gradually go bankrupt
system trained w/ nulls some large enterprises to gradually becoming bankrupt
system trained w/o nulls some large enterprises gradually becoming bankrupt
source *pro*??????
reference it is not clear now
system trained w/ nulls it is also not clear
system trained w/o nulls he is not clear
source *pro*??????
reference it is not clear yet
system trained w/ nulls it is still not clear
system trained w/o nulls is still not clear
Table 9: Sample translations. The system trained without nulls is the baseline system where the training corpus and
test corpus did not have empty categories. The system trained with nulls is the system trained with the training corpus
and the test corpus that have been automatically augmented with empty categories. All examples are part of longer
sentences.
cated methods, perhaps resembling the work of Gab-
bard et al (2006) may be necessary for more accu-
rate recovery of empty elements. We can also con-
sider simpler methods where different algorithms
are used for recovering different empty elements, in
which case, we need to be careful about how recov-
ering different empty elements could interact with
each other as exemplified by our discussion of the
pattern matching algorithm in Section 3 and our ex-
periment presented in Section 4.2.
There are several other issues we may consider
when recovering empty categories that are miss-
ing in the target language. We only considered
empty categories that are present in treebanks. How-
ever, there might be some empty elements which are
not annotated but nevertheless helpful for improv-
ing machine translation. As always, preprocessing
the corpus to address a certain problem in machine
translation is less principled than tackling the prob-
lem head on by integrating it into the machine trans-
lation system itself. It may be beneficial to include
consideration for empty elements in the decoding
process, so that it can benefit from interacting with
other elements of the machine translation system.
Acknowledgments We thank the anonymous re-
viewers for their helpful comments. This work
was supported by NSF grants IIS-0546554 and IIS-
0910611.
References
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for treebank II
style. Penn Treebank Project, January.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Pi-Chuan Chang, Michel Galley, and Christopher Man-
ning. 2008. Optimizing Chinese word segmentation
for machine translation performance. In Proceedings
of the Third Workshop on Statistical Machine Transla-
tion, pages 224?232.
Ryan Gabbard, Seth Kulick, and Mitchell Marcus. 2006.
Fully parsing the Penn Treebank. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 184?191, New York
644
City, USA, June. Association for Computational Lin-
guistics.
Na-Rae Han and Shijong Ryu. 2005. Guidelines for
Penn Korean Treebank version 2.0. Technical report,
IRCS, University of Pennsylvania.
Gumwon Hong, Seung-Wook Lee, and Hae-Chang Rim.
2009. Bridging morpho-syntactic gap between source
and target sentences for English-Korean statistical ma-
chine translation. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 233?236.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head finalization: A simple re-
ordering rule for sov languages. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and Metrics, pages 244?251.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Annual Confer-
ence of the Association for Computational Linguistics
(ACL-02), Philadelphia, PA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL, Demonstration Session, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In 2004 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 388?395, Barcelona, Spain, July.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Ma-
chine Learning: Proceedings of the Eighteenth Inter-
national Conference (ICML 2001), Stanford, Califor-
nia.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Conference of the Association for Com-
putational Linguistics (ACL-03).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Conference of the Association for Com-
putational Linguistics (ACL-02).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433?440, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Nianwen Xue and Fei Xia. 2000. The bracketing guide-
lines for the Penn Chinese Treebank. Technical Report
IRCS-00-08, IRCS, University of Pennsylvania.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Conference of the Association for Com-
putational Linguistics (ACL-01), Toulouse, France.
645
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1735?1745,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Type-based MCMC for Sampling Tree Fragments from Forests
Xiaochang Peng and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
This paper applies type-based Markov
Chain Monte Carlo (MCMC) algorithms
to the problem of learning Synchronous
Context-Free Grammar (SCFG) rules
from a forest that represents all possible
rules consistent with a fixed word align-
ment. While type-based MCMC has been
shown to be effective in a number of NLP
applications, our setting, where the tree
structure of the sentence is itself a hid-
den variable, presents a number of chal-
lenges to type-based inference. We de-
scribe methods for defining variable types
and efficiently indexing variables in or-
der to overcome these challenges. These
methods lead to improvements in both log
likelihood and BLEU score in our experi-
ments.
1 Introduction
In previous work, sampling methods have been
used to learn Tree Substitution Grammar (TSG)
rules from derivation trees (Post and Gildea, 2009;
Cohn et al., 2009) for TSG learning. Here, at each
node in the derivation tree, there is a binary vari-
able indicating whether the node is internal to a
TSG rule or is a split point, which we refer to as
a cut, between two rules. The problem of extract-
ing machine translation rules from word-aligned
bitext is a similar problem in that we wish to au-
tomatically learn the best granularity for the rules
with which to analyze each sentence. The prob-
lem of rule extraction is more complex, however,
because the tree structure of the sentence is also
unknown.
In machine translation applications, most pre-
vious work on joint alignment and rule extrac-
tion models uses heuristic methods to extract rules
from learned word alignment or bracketing struc-
tures (Zhang et al., 2008; Blunsom et al., 2009;
DeNero et al., 2008; Levenberg et al., 2012).
Chung et al. (2014) present a MCMC algorithm
schedule to learn Hiero-style SCFG rules (Chiang,
2007) by sampling tree fragments from phrase de-
composition forests, which represent all possible
rules that are consistent with a set of fixed word
alignments. Assuming fixed word alignments re-
duces the complexity of the sampling problem,
and has generally been effective in most state-
of-the-art machine translation systems. The al-
gorithm for sampling rules from a forest is as
follows: from the root of the phrase decomposi-
tion forest, one samples a cut variable, denoting
whether the current node is a cut, and an edge vari-
able, denoting which incoming hyperedge is cho-
sen, at each node of the current tree in a top-down
manner. This sampling schedule is efficient in that
it only samples the current tree and will not waste
time on updating variables that are unlikely to be
used in any tree.
As with many other token-based Gibbs Sam-
pling applications, sampling one node at a time
can result in slow mixing due to the strong cou-
pling between variables. One general remedy is
to sample blocks of coupled variables. Cohn and
Blunsom (2010) and Yamangil and Shieber (2013)
used blocked sampling algorithms that sample the
whole tree structure associated with one sentence
at a time for TSG and TAG learning. However, this
kind of blocking does not deal with the coupling of
variables correlated with the same type of struc-
ture across sentences. Liang et al. (2010) intro-
duced a type-based sampling schedule which up-
dates a block of variables of the same type jointly.
The type of a variable is defined as the combina-
tion of new structural choices added when assign-
ing different values to the variable. Type-based
MCMC tackles the coupling issue by assigning the
same type to variables that are strongly coupled.
In this paper, we follow the phrase decompo-
sition forest construction procedures of Chung et
1735
al. (2014) and present a type-based MCMC algo-
rithm for sampling tree fragments from phrase de-
composition forests which samples the variables
of the same type jointly. We define the type of the
cut variable for each node in our sampling sched-
ule. While type-based MCMC has been proven
to be effective in a number of NLP applications,
our sample-edge, sample-cut setting is more com-
plicated as our tree structure is unknown. We
need additional steps to maintain the cut type in-
formation when the tree structure is changed as
we sample the edge variable. Like other type-
based MCMC applications, we need bookkeep-
ing of node sites to be sampled in order to loop
through sites of the same type efficiently. As noted
by Liang et al. (2010), indexing by the complete
type information is too expensive in some appli-
cations like TSG learning. Our setting is different
from TSG learning in that the internal structure of
each SCFG rule is abstracted away when deriving
the rule type from the tree fragment sampled.
We make the following contributions:
1. We apply type-based MCMC to the setting of
SCFG learning and have achieved better log
likelihood and BLEU score result.
2. We present an innovative way of storing the
type information by indexing on partial type
information and then filtering the retrieved
nodes according to the full type information,
which enables efficient updates to maintain
the type information while the amount of
bookkeeping is reduced significantly.
3. We replace the two-stage sampling schedule
of Liang et al. (2010) with a simpler and
faster one-stage method.
4. We use parallel programming to do inexact
type-based MCMC, which leads to a speed
up of four times in comparison with non-
parallel type-based MCMC, while the like-
lihood result of the Markov Chain does not
change. This strategy should also work with
other type-based MCMC applications.
2 MCMC for Sampling Tree Fragments
from Forests
2.1 Phrase Decomposition Forest
The phrase decomposition forest provides a com-
pact representation of all machine translation rules
?
I
??
have
?
a
?
date
?
with
??
her
today
1
Figure 1: Example word alignment, with boxes
showing valid phrase pairs. In this example, all
individual alignment points are also valid phrase
pairs.
that are consistent with our fixed input word align-
ment (Chung et al., 2014), and our sampling algo-
rithm selects trees from this forest.
As in Hiero, our grammars will make use of a
single nonterminal X , and will contain rules with
a mixture of nonterminals and terminals on the
righthand side (r.h.s.), with at most two nontermi-
nal occurrences on the r.h.s. Under this restric-
tion, the maximum number of rules that can be
extracted from an input sentence pair is O(n
12
)
with respect to the length of the sentence pair,
as the left and right boundaries of the lefthand
side (l.h.s.) nonterminal and each of the two r.h.s.
nonterminals can take O(n) positions in each of
the two languages. This complexity leads us to
explore sampling algorithms instead of using dy-
namic programming.
A span [i, j] is a set of contiguous word in-
dices {i, i + 1, . . . , j ? 1}. Given an aligned
Chinese-English sentence pair, a phrase n is a pair
of spans n = ([i
1
, j
1
], [i
2
, j
2
]) such that Chinese
words in positions [i
1
, j
1
] are aligned only to En-
glish words in positions [i
2
, j
2
], and vice versa. A
phrase forest H = ?V,E? is a hypergraph made
of a set of hypernodes V and a set of hyperedges
E. Each node n = ([i
1
, j
1
], [i
2
, j
2
]) ? V is a
tight phrase as defined by Koehn et al. (2003),
i.e., a phrase containing no unaligned words at its
boundaries. A phrase n = ([i
1
, j
1
], [i
2
, j
2
]) covers
n
?
= ([i
?
1
, j
?
1
], [i
?
2
, j
?
2
]) if
i
1
? i
?
1
? j
?
1
? j
1
? i
2
? i
?
2
? j
?
2
? j
2
Each edge in E, written as T ? n, is made of a
1736
([0 1], [0 1])X , I
([4 5], [1 2])X  , have([5 6], [3 4])X  , date([2 3], [4 5])X  , with([3 4], [5 6])X , her
([2 4], [4 6])X XX, XX([4 6], [1 4])X XX, Xa X
([1 4], [4 7])X XX, XX([2 6], [1 6])X XX, XX
([1 6], [1 7])X XX, XX X XX, XX
([0 6], [0 7])X XX, XX
([1 2], [6 7])X , today
Figure 2: A phrase decomposition forest extracted
from the sentence pair ?????????, I
have a date with her today?. Each edge is a min-
imal SCFG rule, and the rules at the bottom level
are phrase pairs. Unaligned word ?a? shows up
in the rule X ? X
1
X
2
, X
1
aX
2
after unaligned
words are put back into the alignment matrix. The
highlighted portion of the forest shows an SCFG
rule built by composing minimal rules.
set of non-intersecting tail nodes T ? V , and a
single head node n ? V that covers each tail node.
We say an edge T ? n is minimal if there does
not exist another edge T
?
? n such that T
?
covers
T . A minimal edge is an SCFG rule that cannot
be decomposed by factoring out some part of its
r.h.s. as a separate rule. We define a phrase de-
composition forest to be made of all phrases from
a sentence pair, connected by all minimal SCFG
rules. A phrase decomposition forest compactly
represents all possible SCFG rules that are consis-
tent with word alignments. For the example word
alignment shown in Figure 1, the phrase decom-
position forest is shown in Figure 2. Each boxed
phrase in Figure 1 corresponds to a node in the
forest of Figure 2, while hyperedges in Figure 2
represent ways of building phrases out of shorter
phrases.
A phrase decomposition forest has the impor-
tant property that any SCFG rule consistent with
the word alignment corresponds to a contiguous
fragment of some complete tree found in the for-
est. For example, the highlighted tree fragment
of the forest in Figure 2 corresponds to the SCFG
rule:
X ? ? X
2
? X
1
, have a X
1
with X
2
Thus any valid SCFG rule can be formed by se-
lecting a set of adjacent hyperedges from the for-
est and composing the minimal SCFG rules speci-
fied by each hyperedge. Therefore, the problem of
SCFG rules extraction can be solved by sampling
tree fragments from the phrase decomposition for-
est. We use a bottom-up algorithm to construct the
phrase decomposition forest from the word align-
ments.
2.2 Sampling Tree Fragments From Forest
We formulate the rule sampling procedure into
two phases: first we select a tree from a forest,
then we select the cuts in the tree to denote the split
points between fragments, with each fragment cor-
responding to a SCFG rule. A tree can be speci-
fied by attaching a variable e
n
to each node n in
the forest, indicating which hyperedge is turned
on at the current node. Thus each assignment will
specify a unique tree by tracing the edge variables
from the root down to the leaves. We also attach
a cut variable z
n
to each node, indicating whether
the node is a split point between two adjacent frag-
ments.
Let all the edge variables form the random vec-
tor Y and all the cut variables form the random
vector Z. Given an assignment y to the edge vari-
ables and assignment z to the cut variables, our de-
sired distribution is proportional to the product of
weights of the rules specified by the assignment:
P
t
(Y = y, Z = z) ?
?
r??(y,z)
w(r) (1)
where ?(y, z) is the set of rules identified by the
assignment. We use a generative model based on
a Dirichlet Process (DP) defined over composed
rules. We draw a distribution G over rules from a
DP, and then rules from G.
G | ?, P
0
?Dir(?, P
0
)
r | G ?G
For the base distribution P
0
, we use a uniform
distribution where all rules of the same size have
equal probability:
P
0
(r) = V
?|r
f
|
f
V
?|r
e
|
e
(2)
1737
where V
f
and V
e
are the vocabulary sizes of the
source language and the target language, and |r
f
|
and |r
e
| are the lengths of the source side and tar-
get side of rule r. By marginalizing outGwe get a
simple posterior distribution over rules which can
be described using the Chinese Restaurant Process
(CRP). For this analogy, we imagine a restaurant
has infinite number of tables that represent rule
types and customers that represent translation rule
instances. Each customer enters the restaurant and
chooses a table to sit at. Let z
i
be the table chosen
by the i-th customer, then the customer chooses a
table k either having been seated or a new table
with probability:
P (z
i
= k|z
?i
) =
{
n
k
i?1+?
1 ? k ? K
?
i?1+?
k = K + 1
(3)
where z
?i
is the current seating arrangement, n
k
is
the number of customers at the table k,K is the to-
tal number of occupied tables. If the customer sits
at a new table, the new table will be assigned a rule
label r with probability P
0
(r). We can see from
Equation 3 that the only history related to the cur-
rent table assignment is the counts in z
?i
. There-
fore, we define a table of counts N = {N
C
}
C?I
which memorizes different categories of counts in
z
?i
. I is an index set for different categories of
counts. EachN
C
is a vector of counts for category
C. We have P (r
i
= r|z
?i
) = P (r
i
= r|N). If
we marginalize over tables labeled with the same
rule, we get the following probability over rule r
given the previous count table N :
P (r
i
= r|N) =
N
R
(r) + ?P
0
(r)
n+ ?
(4)
here in the case of DP, I = {R}, where R is the
index for the category of rule counts. N
R
(r) is the
number of times that rule r has been observed in
z
?i
, n =
?
r
N
R
(r) is the total number of rules
observed.
We also define a Pitman-Yor Process (PYP)
(Pitman and Yor, 1997) over rules of each length l.
We draw the rule distribution G from a PYP, and
then rules of length l are drawn from G.
G|?, d, P
0
? PY (?, d, P
0
)
r|G ? G
The first two parameters, a concentration parame-
ter ? and a discount parameter d, control the shape
of distribution G by controlling the size and the
Algorithm 1 Top-down Sampling Algorithm
1: queue.push(root)
2: while queue is not empty do
3: n = queue.pop()
4: SAMPLEEDGE(n)
5: SAMPLECUT(n)
6: for each child c of node n do
7: queue.push(c)
8: end for
9: end while
number of clusters. Integrating over G, we have
the following PYP posterior probability:
P (r
i
= r|N) =
N
R
(r)? T
r
d+ (T
l
d+ ?)P
0
(r)
N
L
(l) + ?
(5)
here for the case of PYP, I = {R,L}. We have an
additional index L for the category of rule length
counts, and N
L
(l) is the total number of rules of
length l observed in z
?i
. T
r
is the number of ta-
bles labeled with r in z
?i
. The length of the rule is
drawn from a Poisson distribution, so a rule length
probability P (l;?) =
?
l
e
??
l!
is multiplied by this
probability to calculate the real posterior probabil-
ity for each rule. In order to simplify the tedious
book-keeping, we estimate the number of tables
using the following equations (Huang and Renals,
2010):
T
r
= N
R
(r)
d
(6)
T
l
=
?
r:|r|=l
N
R
(r)
d
(7)
We use the top-down sampling algorithm of
Chung et al. (2014) (see Algorithm 1). Starting
from the root of the forest, we sample a value for
the edge variable denoting which incoming hyper-
edge of the node is turned on in the current tree,
and then we sample a cut value for the node de-
noting whether the node is a split point between
two fragments in the tree. For each node n, we de-
note the composed rule type that we get when we
set the cut of node n to 0 as r
1
and the two split
rule types that we get when we set the cut to 1 as
r
2
, r
3
. We sample the cut value z
i
of the current
node according to the posterior probability:
P (z
i
= z|N) =
{
P (r
1
|N)
P (r
1
|N)+P (r
2
|N)P (r
3
|N
?
)
if z = 0
P (r
2
|N)P (r
3
|N
?
)
P (r
1
|N)+P (r
2
|N)P (r
3
|N
?
)
otherwise
(8)
where the posterior probability P (r
i
|N) is accord-
ing to either a DP or a PYP, andN,N
?
are tables of
counts. In the case of DP, N,N
?
differ only in the
rule counts of r
2
, where N
?
R
(r
2
) = N
R
(r
2
) + 1.
In the case of PYP, there is an extra difference that
1738
([0 1], [0 1])X , I
([4 5], [1 2])X  , have([5 6], [3 4])X , date([2 3], [4 5])X , with([3 4], [5 6])X , her
([2 4], [4 6])X XX, XX([4 6], [1 4])X XX, Xa X
([1 4], [4 7])X XX, XX([2 6], [1 6])X XX, XX
([1 6], [1 7])X XX, XX X XX, XX
([0 6], [0 7])X XX, XX
([1 2], [6 7])X , today
Figure 3: An example of cut type: Consider
the two nodes marked in bold, ([2 6], [1 6]),
([1 4], [4 7]). These two non-split nodes are
internal to the same composed rule: X ?
X
1
X
2
X
3
, X
3
X
2
X
1
. We keep these two sites with
the same index. However, when we set the cut
value of these two nodes to 1, as the rules imme-
diately above and immediately below are different
for these two sites, they are not of the same type.
N
?
L
(l) = N
L
(l) + 1, where l is the rule length of
r
2
.
As for edge variables e
i
, we refer to the set of
composed rules turned on below n including the
composed rule fragments having n as an internal
or root node as {r
1
, . . . , r
m
}. We have the follow-
ing posterior probability over the edge variable e
i
:
P (e
i
= e|N) ?
m
?
i=1
P (r
i
|N
i?1
)
?
v??(e)?in(n)
deg(v) (9)
where deg(v) is the number of incoming edges for
node v, in(n) is the set of nodes in all subtrees
under n, and ?(e) is the tree specified when we
set e
i
= e. N
0
to N
m
are tables of counts where
N
0
= N , N
i
R
(r
i
) = N
i?1
R
(r
i
) + 1 in the case of
DP and additionally N
i
L
(l
i
) = N
i?1
L
(l
i
) + 1 in the
case of PYP, where l
i
is the rule length of r
i
.
3 Type-based MCMC Sampling
Our goal in this paper is to organize blocks of vari-
ables that are strongly coupled into types and sam-
ple variables of each type jointly. One major prop-
erty of type-based MCMC is that the joint proba-
bility of variables of the same type should be ex-
changeable so that the order of the variables does
not matter. Also, the choices of the variables to
be sampled jointly should not interfere with each
other, which we define as a conflict. In this section,
we define the type of cut variables in our sampling
schedule and explain that with the two priors we
introduced before, the joint probability of the vari-
ables will satisfy the exchangeability property. We
will also discuss how to check conflict sites in our
application.
In type-based MCMC, we need bookkeeping of
sites as we need to loop through them to search for
sites having the same type efficiently. In our two-
stage sample-edge, sample-cut schedule, updating
the edge variable would change the tree structure
and trigger updates for the cut variable types in
both the old and the new subtree. We come up with
an efficient bookkeeping strategy to index on par-
tial type information which significantly reduces
the bookkeeping size, while updates are quite effi-
cient when the tree structure is changed. The detail
will become clear below.
3.1 Type-based MCMC
We refer to each node site to be sampled as a pair
(t, n), indicating node n of forest t. For each site
(t, n) and the corresponding composed rule types
r
1
obtained when we set n?s cut value to 0 and
r
2
, r
3
obtained when we set the cut value to 1, the
cut variable type of site (t, n) is:
type(t, n)
def
= (r
1
, r
2
, r
3
)
We say that the cut variables of two sites are of
the same type if the composed rule types r
1
, r
2
and
r
3
are exactly the same. For example, in Figure 3,
assume that all the nodes in the hypergraph are
currently set to be split points except for the two
nodes marked in bold, ([2 6], [1 6]), ([1 4], [4 7]).
Considering these two non-split nodes, the com-
posed rule types they are internal to (r
1
) are ex-
actly the same. However, the situation changes if
we set the cut variables of these two nodes to be 1,
i.e., all of the nodes in the hypergraph are now split
points. As the rule type immediately above and
the rule type immediately below the two nodes (r
2
and r
3
) are now different, they are not of the same
type.
We sample the cut value z
i
according to Equa-
tion 8. As each rule is sampled according to
a DP or PYP posterior and the joint probabili-
ties according to both posteriors are exchangeable,
we can see from Equation 8 that the joint prob-
1739
ability of a sequence of cut variables is also ex-
changeable. Consider a set of sites S containing
n cut variables z
S
= (z
1
, ..., z
n
) of the same type.
This exchangeability property leads to the fact that
any sequence containing same number of cuts (cut
value of 1) would have same probability. We have
the following probability distribution:
P (z
S
|N) ?
n?m
?
i=1
P (r
1
|N
i?1
)
m
?
i=1
P (r
2
|
?
N
i?1
)P (r
3
|
?
N
i?1
)
def
= g(m) (10)
where N is the count table for all the other vari-
ables except for S. m =
?
n
i=1
z
i
is the number of
cut sites. The variablesN,
?
N , and
?
N keep track of
the counts as the derivation proceeds step by step:
N
0
= N
N
i
R
(r
1
) = N
i?1
R
(r
1
) + 1
?
N
0
= N
n?m
?
N
i?1
R
(r
2
) =
?
N
i?1
R
(r
2
) + 1
?
N
i
R
(r
3
) =
?
N
i?1
R
(r
3
) + 1
For PYP, we add extra count indices for rule length
counts similarly.
Given the exchangeability property of the cut
variables, we can calculate the posterior probabil-
ity of m =
?
n
i=1
z
i
by summing over all
(
n
m
)
combinations of the cut sites:
p(m|N) ?
?
z
S
:m=
?
i
z
i
p(z
S
|N) =
(
n
m
)
g(m) (11)
3.2 Sampling Cut-types
Given Equation 11 and the exchangeability prop-
erty, our sampling strategy falls out naturally: first
we sample m according to Equation 11, then con-
ditioned on m, we pick m sites of z
S
as cut sites
out of the
(
n
m
)
combinations with uniform proba-
bility.
Now we proceed to define conflict sites. In ad-
dition to exchangeability, another important prop-
erty of type-based MCMC is that the type of each
site to be sampled should be independent of the
assignment of the other sites sampled at the same
time. That is, in our case, setting the cut value of
each site should not change the (r
1
, r
2
, r
3
) triple
of another site. We can see that the cut value of
the current site would have effect on and only on
Algorithm 2 Type-based MCMC Algorithm for
Sampling One Site
1: sample one type of sites, currently sample site
(node, parent)
2: if parent is None or node is sampled then
3: return
4: end if
5: old = node.cut
6: node.cut = 0
7: r
1
= composed rule(parent)
8: node.cut = 1
9: r
2
= composed rule(parent)
10: r
3
= composed rule(node)
11: node.cut = old
12: sites =
13: for sites s ? index[r
1
] do
14: for sites s
?
in rule rooted at s do
15: if s
?
of type (r
1
, r
2
, r
3
) and no conflict
then
16: add s
?
to sites
17: end if
18: end for
19: end for
20: for sites s ? index[r
3
] do
21: if s of type (r
1
, r
2
, r
3
) and no conflict then
22: add s to sites
23: end if
24: end for
25: sample m according to Equation 11
26: remove sites from index
27: uniformly choose m in sites to be cut sites.
28: add new cut sites to index
29: mark all nodes in sites as sampled
the nodes in the r
1
fragment. We denote nodes(r)
as the node set for all nodes within fragment r.
Then for ?z, z
?
? S, z is not in conflict with z
?
if
and only if nodes(r
1
) ? nodes(r
?
1
) = ?, where r
1
and r
?
1
are the corresponding composed rule types
when we set z, z
?
to 0.
Another crucial issue in type-based sampling is
the bookkeeping of sampling sites, as we need to
loop through all sites having the same type with
the current node. We only maintain the type in-
formation of nodes that are currently turned on in
the chosen tree of the forest, as we only sample
these nodes. It is common practice to directly use
the type value of each variable as an index and
maintain a set of sites for each type. However,
maintaining a (r
1
, r
2
, r
3
) triple for each node in
the chosen tree is too memory heavy in our appli-
1740
cation.
In our two-stage sample-edge, sample-cut
schedule, there is an additional issue that we must
deal with efficiently: when we have chosen a new
incoming edge for the current node, we also have
to update the bookkeeping index as the current tree
structure is changed. Cut variable types in the old
subtree will be turned off and a new subtree of
variable types will be turned on. In the extreme
case, when we have chosen a new incoming edge
at the root node, we have chosen a new tree in the
forest. So, we need to remove appearances of cut
variable types in the old tree and add all cut vari-
able types in the newly chosen tree.
Our strategy to deal with these two issues is to
build a small, simple index, at the cost of some
additional computation when retrieving nodes of a
specified type. To be precise, we build an index
from (single) rule types r to all occurrences of r in
the data, where each occurrence is represented as
a pointer to the root of r in the forest. Our strategy
has two important differences from the standard
strategy of building an index having the complete
type (r
1
, r
2
, r
3
) as the key and having every node
as an entry. Specifically:
1. We index only the roots of the current rules,
rather than every node, and
2. We key on a single rule type, rather than a
triple of rule types.
Differences (1) and (2) both serve to keep the in-
dex small, and the dramatic savings in memory is
essential to making our algorithm practical. Fur-
thermore, difference (1) reduces the amount of
work that needs to be done when an edge variable
is resampled. While we must still re-index the en-
tire subtree under the changed edge variable, we
need only to re-index the roots of the current tree
fragments, rather than all nodes in the subtree.
Given this indexing strategy, we now proceed
to describe the process for retrieving nodes of a
specified type (r
1
, r
2
, r
3
). These nodes fall into
one of two cases:
1. Internal nodes, i.e., nodes whose cut variable
is currently set to 0. These nodes must be
contained in a fragment of rule type r
1
, and
must furthermore have r
2
above them, and r
3
below them. We retrieve these nodes by look-
ing up r
1
in the index, iterating over all nodes
in each fragment retrieved, and retaining only
those with r
2
above and r
3
below. (Lines 13?
19 in Algorithm 2.)
2. Boundary nodes, i.e., nodes whose cut vari-
able is currently set to 1. These nodes must
form the root of a fragment r
3
, and have a
fragment r
2
above them. We retrieve these
nodes by looking up r
3
in the index, and then
checking each node retrieved to retain only
those nodes with r
2
above them in the current
tree. (Lines 20?24 in Algorithm 2.)
This process of winnowing down the nodes re-
trieved by the index adds some computational
overhead to our algorithm, but we find that it is
minimal in practice.
We still use the top-down sampling schedule of
Algorithm 1, except that in the sample-edge step,
when we choose a new incoming edge, we add
additional steps to update the bookkeeping index.
Furthermore, in the sample-cut step, we sample
all non-conflict sites having the same type with n
jointly. Our full algorithm for sampling one cut-
type is shown in Algorithm 2. When sampling
each site, we record a parent node of the near-
est cut ancestor of the current node so that we
can build r
1
and r
2
more quickly, as they are both
rooted at parent. We first identify the type of the
current site. Then we search the bookkeeping in-
dex to find possible candidate sites of the same
type, as described above. As for conflict check-
ing, we keep a set of nodes that includes all nodes
in the r
1
fragment of previous non-conflict sites. If
the r
1
fragment of the current site has any node in
common with this node set, we arrive at a conflict
site.
4 Methods of Further Optimization
4.1 One-stage Sampling Schedule
Instead of calculating the posterior of each m ac-
cording to Equation 11 and then sampling m, we
can build our real m more greedily.
P (z
S
|N) =
n
?
i=1
P (z
i
|N
i?1
) (12)
where N,N
0
, . . . , N
n
are count tables, and N
0
=
N . N
i
is the new count table after we updateN
i?1
according to the assignment of z
i
. This equation
gives us a hint to sample each z
i
according to
P (z
i
|N
i?1
) and then update the count table N
i?1
according to the assignment of z
i
. This greedy
1741
sampling saves us the effort to calculate each m
by multiplying over each posterior of cut variables
but directly samples the real m. In our exper-
iment, this one-stage sampling strategy gives us
a 1.5 times overall speed up in comparison with
the two-stage sampling schedule of Liang et al.
(2010).
4.2 Parallel Implementation
As our type-based sampler involves tedious book-
keeping and frequent conflict checking and mis-
match of cut types, one iteration of the type-based
sampler is slower than an iteration of the token-
based sampler when run on a single processor.
In order to speed up our sampling procedure, we
used a parallel sampling strategy similar to that of
Blunsom et al. (2009) and Feng and Cohn (2013),
who use multiple processors to perform inexact
Gibbs Sampling, and find equivalent performance
in comparison with an exact Gibbs Sampler with
significant speed up. In our application, we split
the data into several subsets and assign each sub-
set to a processor. Each processor performs type-
based sampling on its subset using local counts
and local bookkeeping, and communicates the up-
date of the local counts after each iteration. All
the updates are then aggregated to generate global
counts and then we refresh the local counts of
each processor. We do not communicate the up-
date on the bookkeeping of each processor. In this
implementation, we have a slightly ?out-of-date?
counts at each processor and a smaller bookkeep-
ing of sites of the same type, but we can perform
type-based sampling independently on each pro-
cessor. Our experiments show that, with proper
division of the dataset, the final performance does
not change, while the speed up is significant.
5 Experiments
We used the same LDC Chinese-English parallel
corpus as Chung et al. (2014),
1
which is composed
of newswire text. The corpus consists of 41K sen-
tence pairs, which has 1M words on the English
side. The corpus has a 392-sentence development
set with four references for parameter tuning, and
1
The data are randomly sampled from various differ-
ent sources (LDC2006E86, LDC2006E93, LDC2002E18,
LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26,
LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92,
LDC2006E24, LDC2006E92, LDC2006E24) The language
model is trained on the English side of entire data (1.65M
sentences, which is 39.3M words.)
a 428-sentence test set with four references for
testing.
2
The development set and the test set have
sentences with less than 30 words. A trigram lan-
guage model was used for all experiments. We
plotted the log likelihood graph to compare the
convergence property of each sampling schedule
and calculated BLEU (Papineni et al., 2002) for
evaluation.
5.1 Experiment Settings
We use the top-down token-based sampling al-
gorithm of Chung et al. (2014) as our baseline.
We use the same SCFG decoder for translation
with both the baseline and the grammars sam-
pled using our type-based MCMC sampler. The
features included in our experiments are differ-
ently normalized rule counts and lexical weight-
ings (Koehn et al., 2003) of each rule. Weights are
tuned using Pairwise Ranking Optimization (Hop-
kins and May, 2011) using a grammar extracted by
the standard heuristic method (Chiang, 2007) and
the development set. The same weights are used
throughout our experiments.
First we want to compare the DP likelihood
of the baseline with our type-based MCMC sam-
pler to see if type-based sampling would converge
to a better sampling result. In order to verify if
type-based MCMC really converges to a good op-
timum point, we use simulated annealing (Kirk-
patrick et al., 1983) to search possible better opti-
mum points. We sample from the real distribution
modified by an annealing parameter ?:
z ? P (z)
?
We increase our ? from 0.1 to 1.3, and then de-
crease from 1.3 to 1.0, changing by 0.1 every 3
iterations. We also run an inexact parallel ap-
proximation of type-based MCMC in comparison
with the non-parallel sampling to find out if par-
allel programming is feasible to speed up type-
based MCMC sampling without affecting the per-
formance greatly. We do not compare the PYP
likelihood because the approximation renders it
impossible to calculate the real PYP likelihood.
We also calculate the BLEU score to compare the
grammars extracted using each sampling sched-
ule. We just report the BLEU result of grammars
sampled using PYP as for all our schedules, since
PYP always performs better than DP.
2
They are from newswire portion of NIST MT evaluation
data from 2004, 2005, and 2006.
1742
As for parameter settings, we used d = 0.5 for
the Pitman-Yor discount parameter. Though we
have a separate PYP for each rule length, we used
same ? = 5 for all rule sizes in all experiments,
including experiments using DP. For rule length
probability, a Poisson distribution where ? = 2
was used for all experiments.
3
For each sentence sample, we initialize all the
nodes in the forest to be cut sites and choose an
incoming edge for each node uniformly. For each
experiment, we run for 160 iterations. For each
DP experiment, we draw the log likelihood graph
for each sampling schedule before it finally con-
verges. For each PYP experiment, we tried aver-
aging the grammars from every 10th iteration to
construct a single grammar and use this grammar
for decoding. We tune the number of grammars
included for averaging by comparing the BLEU
score on the dev set and report the BLEU score
result on the test with the same averaging of gram-
mars.
As each tree fragment sampled from the for-
est represents a unique translation rule, we do not
need to explicitly extract the rules; we merely
need to collect them and count them. However,
the fragments sampled include purely non-lexical
rules that do not conform to the rule constraints
of Hiero, and rules that are not useful for trans-
lation. In order to get rid of this type of rule,
we prune every rule that has scope (Hopkins and
Langmead, 2010) greater than two. Whereas Hi-
ero does not allow two adjacent nonterminals in
the source side, our pruning criterion allows some
rules of scope two that are not allowed by Hiero.
For example, the following rule (only source side
shown) has scope two but is not allowed by Hiero:
X ? w
1
X
1
X
2
w
2
X
3
5.2 Experiment Results
Figure 4 shows the log likelihood result of our
type-based MCMC sampling schedule and the
baseline top-down sampling. We can see that type-
based sampling converges to a much better re-
sult than non-type-based top-down sampling. This
shows that type-based MCMC escapes some local
optima that are hard for token-based methods to
escape. This further strengthens the idea that sam-
pling a block of strongly coupled variables jointly
3
The priors are the same as the work of Chung et al.
(2014). The priors are set to be the same because other priors
turn out not to affect much of the final performance and add
additional difficulty for tuning.
0 10 20 30 40 50 60
Iteration #
?6.0
?5.5
?5.0
?4.5
?4.0
?3.5
l
o
g
l
i
k
e
l
i
h
o
o
d
 
(
*
1
0
^
6
)
Log likelihood: type-based vs non-type-base vs simulated annealing
type-based + simulated annealing
type-based
non-type-based
Figure 4: Log likelihood result of type-based
MCMC sampling against non-type-based MCMC
sampling, simulated annealing is used to verify if
type-based MCMC converges to a good likelihood
0 10 20 30 40 50 60
Iteration #
?6.5
?6.0
?5.5
?5.0
?4.5
?4.0
?3.5
l
o
g
l
i
k
e
l
i
h
o
o
d
 
(
*
1
0
^
6
)
Log likelihood: parallel type-based vs non-parallel type-based
parallel type-based
non-parallel type-based
Figure 5: parallelization result for type-based
MCMC
helps solve the slow mixing problem of token-
based sampling methods. Another interesting ob-
servation is that, even though theoretically these
two sampling methods should finally converge to
the same point, in practice a worse sampling al-
gorithm is prone to get trapped at local optima,
and it will be hard for its Markov chain to es-
cape it. We can also see from Figure 4 that the
log likelihood result only improves slightly using
simulated annealing. One possible explanation is
that the Markov chain has already converged to
a very good optimum point with type-based sam-
pling and it is hard to search for a better optimum.
Figure 5 shows the parallelization result of type-
based MCMC sampling when we run the program
on five processors. We can see from the graph that
when running on five processors, the likelihood fi-
1743
Sampling Schedule iteration dev test
Non-type-based averaged (0-90) 25.62 24.98
Type-based averaged (0-100) 25.88 25.20
Parallel Type-based averaged (0-90) 25.75 25.04
Table 1: Comparisons of BLEU score results
nally converges to the same likelihood result as
non-parallel type-based MCMC sampling. How-
ever, when we use more processors, the likelihood
eventually becomes lower than with non-parallel
sampling. This is because when we increase the
number of processors, we split the dataset into
very small subsets. As we maintain the bookkeep-
ing for each subset separately and do not com-
municate the updates to each subset, the power of
type-based sampling is weakened with bookkeep-
ing for very few sites of each type. In the extreme
case, when we use too many processors in parallel,
the bookkeeping would have a singleton site for
each type. In this case, the approximation would
degrade to the scenario of approximating token-
based sampling. By choosing a proper size of divi-
sion of the dataset and by maintaining local book-
keeping for each subset, the parallel approxima-
tion can converge to almost the same point as non-
parallel sampling. As shown in our experimental
results, the speed up is very significant with the
running time decreasing from thirty minutes per
iteration to just seven minutes when running on
five processors. Part of the speed up comes from
the smaller bookkeeping since with fewer sites for
each index, there is less mismatch or conflict of
sites.
Table 1 shows the BLEU score results for type-
based MCMC and the baseline. For non-type-
based top-down sampling, the best BLEU score re-
sult on dev is achieved when averaging the gram-
mars of every 10th iteration from the 0th to the
90th iteration, while our type-based method gets
the best result by averaging over every 10th itera-
tion from the 0th to the 100th iteration. We can see
that the BLEU score on dev for type-based MCMC
and the corresponding BLEU score on test are
both better than the result for the non-type-based
method, though not significantly. This shows that
the better likelihood of our Markov Chain using
type-based MCMC does result in better transla-
tion.
We have also done experiments calculating the
BLEU score result of the inexact parallel imple-
mentation. We can see from Table 1 that, while the
likelihood of the approximation does not change
in comparison with the exact type-based MCMC,
there is a gap between the BLEU score results. We
think this difference might come from the incon-
sistency of the grammars sampled by each proces-
sor within each iteration, as they do not communi-
cate the update within each iteration.
6 Conclusion
We presented a novel type-based MCMC algo-
rithm for sampling tree fragments from phrase de-
composition forests. While the hidden tree struc-
ture in our settings makes it difficult to maintain
the constantly changing type information, we have
come up with a compact way to store the type in-
formation of variables and proposed efficient ways
to update the bookkeeping index. Under the addi-
tional hidden structure limitation, we have shown
that type-based MCMC sampling still works and
results in both better likelihood and BLEU score.
We also came with techniques to speed up the
type-based MCMC sampling schedule while not
affecting the final sampling likelihood result. A re-
maining issue with parallelization is the inconsis-
tency of the grammar within an iteration between
processors. One possible solution would be using
better averaging methods instead of simply aver-
aging over every few iterations. Another interest-
ing extension for our methods would be to also de-
fine types for the edge variables, and then sample
both cut and edge types jointly.
Acknowledgments
We gratefully acknowledge the assistance of
Licheng Fang and Tagyoung Chung. This work
was partially funded by NSF grant IIS-0910611.
References
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2, pages 782?790, Singapore.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Tagyoung Chung, Licheng Fang, Daniel Gildea, and
Daniel
?
Stefankovi?c. 2014. Sampling tree fragments
from forests. Computational Linguistics, 40:203?
229.
1744
Trevor Cohn and Phil Blunsom. 2010. Blocked in-
ference in Bayesian tree substitution grammars. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-10),
pages 225?230, Uppsala, Sweden.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 548?556,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
John DeNero, Alexandre Bouchard-Cote, and Dan
Klein. 2008. Sampling alignment structure under
a Bayesian translation model. In Proceedings of
EMNLP, pages 314?323, Honolulu, HI.
Yang Feng and Trevor Cohn. 2013. A markov
model of machine translation using non-parametric
bayesian inference. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 333?
342, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Mark Hopkins and Greg Langmead. 2010. SCFG de-
coding without binarization. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 646?655, Cambridge,
MA, October.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July.
Songfang Huang and Steve Renals. 2010. Power law
discounting for n-gram language models. In Proc.
IEEE International Conference on Acoustic, Speech,
and Signal Processing (ICASSP?10), pages 5178?
5181, Dallas, Texas, USA.
Scott Kirkpatrick, C. D. Gelatt, Jr., and Mario P. Vec-
chi. 1983. Optimization by Simulated Annealing.
Science, 220(4598):671?680.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL-03, pages 48?54, Edmonton,
Alberta.
Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012.
A Bayesian model for learning SCFGs with discon-
tiguous rules. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 223?232, Jeju Island, Korea.
Percy Liang, Michael I Jordan, and Dan Klein. 2010.
Type-based mcmc. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 573?581. Association for
Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of ACL-02, pages 311?318, Philadelphia, PA.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25(2):855?900.
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proc. Association
for Computational Linguistics (short paper), pages
45?48, Singapore.
Elif Yamangil and Stuart M Shieber. 2013. Non-
parametric bayesian inference and efficient parsing
for tree-adjoining grammars. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics. Association of Computational
Linguistics.
Hao Zhang, Daniel Gildea, and David Chiang. 2008.
Extracting synchronous grammar rules from word-
level alignments in linear time. In COLING-08,
pages 1081?1088, Manchester, UK.
1745
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1786?1791,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Comparing Representations of Semantic Roles for
String-To-Tree Decoding
Marzieh Bazrafshan and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We introduce new features for incorpo-
rating semantic predicate-argument struc-
tures in machine translation (MT). The
methods focus on the completeness of the
semantic structures of the translations, as
well as the order of the translated seman-
tic roles. We experiment with translation
rules which contain the core arguments
for the predicates in the source side of a
MT system, and observe that using these
rules significantly improves the translation
quality. We also present a new semantic
feature that resembles a language model.
Our results show that the language model
feature can also significantly improve MT
results.
1 Introduction
In recent years, there have been increasing ef-
forts to incorporate semantics in statistical ma-
chine translation (SMT), and the use of predicate-
argument structures has provided promising im-
provements in translation quality. Wu and Fung
(2009) showed that shallow semantic parsing can
improve the translation quality in a machine trans-
lation system. They introduced a two step model,
in which they used a semantic parser to rerank
the translation hypotheses of a phrase-based sys-
tem. Liu and Gildea (2010) used semantic fea-
tures for a tree-to-string syntax based SMT sys-
tem. Their features modeled deletion and reorder-
ing for source side semantic roles, and they im-
proved the translation quality. Xiong et al. (2012)
incorporated the semantic structures into phrase-
based SMT by adding syntactic and semantic fea-
tures to their translation model. They proposed
two discriminative models which included fea-
tures for predicate translation and argument re-
ordering from source to target side. Bazrafshan
and Gildea (2013) used semantic structures in
a string-to-tree translation system by extracting
translation rules enriched with semantic informa-
tion, and showed that this can improve the trans-
lation quality. Li et al. (2013) used predicate-
argument structure reordering models for hierar-
chical phrase-based translation, and they used lin-
guistically motivated constraints for phrase trans-
lation.
In this paper, we experiment with methods for
incorporating semantics in a string-to-tree MT
system. These methods are designed to model the
order of translation, as well as the completeness
of the semantic structures. We extract translation
rules that include the complete semantic structure
in the source side, and compare that with using
semantic rules for the target side predicates. We
present a method for modeling the order of seman-
tic role sequences that appear spread across multi-
ple syntax-based translation rules, in order to over-
come the problem that a rule representing the en-
tire semantic structure of a predicate is often too
large and too specific to apply to new sentences
during decoding. For this method, we compare the
verb-specific roles of PropBank and the more gen-
eral thematic roles of VerbNet.
These essential arguments of a verbal predicate
are called the core arguments. Standard syntax-
based MT is incapable of ensuring that the tar-
get translation includes all of the core arguments
of a predicate that appear in the source sentence.
To encourage the translation of the likely core ar-
guments, we follow the work of Bazrafshan and
Gildea (2013), who use special translation rules
with complete semantic structures of the predi-
cates in the target side of their MT system. Each
of these rules includes a predicate and all of its
core arguments. Instead of incorporating only the
target side semantic rules, we extract the special
rules for both the source and the target sides, and
compare the effectiveness of adding these rules to
1786
S-8
NP-7-ARG1
1
victimized by
NP-7-ARG0
2
NP-7-ARG1
1 ? NP-7-ARG0 2
Figure 1: A complete semantic rule (Bazrafshan
and Gildea (2013)).
the system separately and simultaneously.
Besides the completeness of the arguments, it is
also important for the arguments to appear in the
correct order. Our second method is designed to
encourage correct order of translation for both the
core and the non-core roles in the target sentence.
We designed a new feature that resembles the lan-
guage model feature in a standard MT system. We
train a n-gram language model on sequences of se-
mantic roles, by treating the semantic roles as the
words in what we call the semantic language. Our
experimental results show that the language model
feature significantly improves translation quality.
Semantic Role Labeling (SRL): We use se-
mantic role labelers to annotate the training data
that we use to extract the translation rules. For tar-
get side SRL, the role labels are attached to the
nonterminal nodes in the syntactic parse of each
sentence. For source side SRL, the labels annotate
the spans from the source sentence that they cover.
We train our semantic role labeler using two differ-
ent standards: Propbank (Palmer et al., 2005) and
VerbNet (Kipper Schuler, 2005).
PropBank annotates the Penn Treebank with
predicate-argument structures.It use generic labels
(such as Arg0, Arg1, etc.) which are defined
specifically for each verb. We trained a semantic
role labeler on the annotated Penn Treebank data
and used the classifier to tag our training data.
VerbNet is a verb lexicon that categorizes En-
glish verbs into hierarchical classes, and annotates
them with thematic roles for the arguments that
they accept. Since the thematic roles use more
meaningful labels (e.g. Agent, Patient, etc.), a lan-
guage model trained on VerbNet labels may be
more likely to generalize across verbs than one
trained on PropBank labels. It may also provide
more information, since VerbNet has a larger set
of labels than PropBank. To train the semantic
role labeler on VerbNet, we used the mappings
A? BC c
0
[B, i, j] c
1
[C, j, k] c
2
[A, i, k] c
0
+ c
1
+ c
2
Figure 2: A deduction step in our baseline decoder
provided by the SemLink project (Palmer, 2009)
to annotate the Penn Treebank with the VerbNet
roles. These mappings map the roles in PropBank
to the thematic roles of VerbNet. When there is no
mapping for a role, we keep the role from Prop-
bank.
2 Using Semantics in Machine
Translation
In this section, we present our techniques for in-
corporating semantics inMT: source side semantic
rules, and the semantic language model.
2.1 Source Side Semantic Rules
Bazrafshan and Gildea (2013) extracted transla-
tion rules that included a predicate and all of its
arguments from the target side, and added those
rules to the baseline rules of their string-to-tree
MT system. Figure 1 shows an example of such
rules, which we refer to as complete semantic
rules. The new rules encourage the decoder to
generate translations that include all of the seman-
tic roles that appear in the source sentence.
In this paper, we use the same idea to extract
rules from the semantic structures of the source
side. The complete semantic rules consist of the
smallest fragments of the combination of GHKM
(Galley et al., 2004) rules that include one pred-
icate and all of its core arguments that appear in
the sentence. Rather than keeping the predicate
and argument labels attached to the non-terminals,
we remove those labels from our extracted seman-
tic rules, to keep the non-terminals in the semantic
rules consistent with the non-terminals of the base-
line GHKM rules. This is also important when us-
ing both the source and the target semantic rules
(i.e. Chinese and English rules), as it has been
shown that there are cross lingual mismatches be-
tween Chinese and English semantic roles in bilin-
gual sentences (Fung et al., 2007).
We extract a complete semantic rule for each
verbal predicate of each sentence pair in the train-
ing data. To extract the target side complete se-
mantic rules, using the target side SRL anno-
1787
A? BC to space c
0
(x1 x2 Destination)
[B, i, j, (Agent, )] c
1
[C, j, k, (PRED bring, Theme, )] c
2
[A, i, k, (Agent, PRED bring,-*-, Theme, Destination)] c
0
+ c
1
+ c
+
+ LMcost(Agent, PRED bring,-*-, Theme, Destination)
Figure 3: A deduction step in the semantic language model method.
tated training data, we follow the general GHKM
method, and modify it to ensure that each fron-
tier node (Galley et al., 2004) in a rule includes ei-
ther all or none of the semantic role labels (i.e. the
predicate and all of its present core arguments) in
its descendants in the target side tree. The result-
ing rule then includes the predicate and all of its
arguments. We use the source side SRL annotated
training data to extract the source side semantic
rules. Since the annotations specify the spans of
the semantic roles, we extract the semantic rules
by ensuring that the span of the root (in the target
side) of the extracted rule covers all of the spans
of the roles in the predicate-argument structure.
The semantic rules are then used together with
the original GHKM rules. We add a binary feature
to distinguish the semantic rules from the rest. We
experiment with adding the semantic rules from
the source side, and compare that with adding se-
mantic rules of both the source and the target side.
In all of the experiments in this paper, we use
a string-to-tree decoder which uses a CYK style
parser (Yamada and Knight, 2002). Figure 2 de-
picts a deduction step in the baseline decoder. The
CFG rule in the first line is used to generate a
new item A with span (i, k) using items B and
C, which have spans (i, j) and (j, k) respectively.
The cost of each item is shown on the right. For
experimenting with complete semantic rules, in
addition having more rules, the only other modi-
fication made to the baseline system is extending
the feature vector to include the new feature. We
do not modify the decoder in any significant way.
2.2 Semantic Language Model
The semantic language model is designed to en-
courage the correct order of translation for the se-
mantic roles. While the complete translation rules
of Section 2.1 contain the order of the translation
for core semantic roles, they do not include the
non-core semantic roles, that is, semantic roles
which are not essential for the verbal predicates,
but do contribute to the meaning of the predicate.
In addition, the semantic LM can help in cases
where no specific complete semantic rule can ap-
ply, which makes the system more flexible.
The semantic language model resembles a reg-
ular language model, but instead of words, it de-
fines a probability distribution over sequences of
semantic roles. For this method we also use a se-
mantic role labeler on our training data, and use
the labeled data to train a tri-gram semantic lan-
guage model.
The rules are extracted using the baseline rule
extraction method. As opposed to the previous
method, the rules for this method are not derived
by combining GHKM rules, but rather are reg-
ular GHKM rules which are annotated with se-
mantic roles. We make a new field in each rule
to keep the ordered list of the semantic roles in
that rule. We also include the nonterminals of the
right-hand-side of the rule in that ordered list, to
be able to substitute the semantic roles from the
input translation items in the correct order. The
decoder uses this new field to save the semantic
roles in the translation items, and propagates the
semantic LM states in the same way that the reg-
ular language model states are propagated by the
decoder.
We define a new feature for the semantic lan-
guage model, and score the semantic states in each
translation item, again analogously to a regular
language model. Figure 3 depicts how the de-
duction for this method is different from our base-
line. In this example, the semantic roles ?Agent?,
?PRED bring? and ?Theme? come from the input
items, and the role ?Destination? (which tags the
terminals ?to space?) comes from the translation
rule.
We stemmed the verbs for training this feature,
and also annotated our rules with stemmed verbal
predicates. The stemming helps the training since
the argument types of a verb are normally inde-
pendent of its inflected variants.
1788
avg. BLEU Score
dev test p-value
Baseline 26.01 25.00 -
Source 26.44 25.17 0.048
Source and target 26.39 25.63 < 10
?10
Propbank LM 26.38 25.08 0.108
VerbNet LM 26.58 25.23 0.025
Table 1: Comparisons of the methods with the
baseline. The BLEU scores are calculated on the
top 3 results from 15 runs MERT for each experi-
ments. The p-values are calculated by comparing
each method against the baseline system.
3 Experiments
3.1 Experimental Setup
The data that we used for training the MT sys-
tem was a Chinese-English corpus derived from
newswire text from LDC.
1
The data consists of
250K sentences, which is 6.3M words in the En-
glish side. Our language model was trained on
the English side of the entire data, which consisted
of 1.65M sentences (39.3M words). Our develop-
ment and test sets are from the newswire portion
of NIST evaluations (2004, 2005, 2006). We used
392 sentences for the development set and 428
sentences for the test set. These sentences have
lengths smaller than 30, and they each have 4 ref-
erence translations. We used our in-house string-
to-tree decoder that uses Earley parsing. Other
than the features that we presented for our new
methods, we used a set of nine standard features.
The rules for the baseline system were extracted
using the GHKM method. Our baseline GHKM
rules also include composed rules, where larger
rules are constructed by combining two levels of
the regular GHKM rules. We exclude any unary
rules (Chung et al., 2011), and only keep rules
that have scope up to 3 (Hopkins and Langmead,
2010). For the semantic language model, we used
the SRILM package (Stolcke, 2002) and trained
a tri-gram language model with the default Good-
Turing smoothing.
Our target side semantic role labeler uses a max-
imum entropy classifier to label parsed sentences.
We used Sections 02-22 of the Penn TreeBank to
1
The data was randomly selected from the follow-
ing sources: LDC2006E86, LDC2006E93, LDC2002E18,
LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26,
LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92,
LDC2006E24, LDC2006E92, LDC2006E24
train the labeler, and sections 24 and 23 for devel-
opment set and training set respectively. The la-
beler has a precision of 90% and a recall of 88%.
We used the Chinese semantic role labeler of Wu
and Palmer (2011) for source side SRL, which
uses the LIBLINEAR (Fan et al., 2008) as a classi-
fier. Minimum Error Rate Training (MERT) (Och,
2003) was used for tuning the feature weights.
For all of our experiments, we ran 15 instances
of MERT with random initial weight vectors, and
used the weights of the top 3 results on the de-
velopment set to test the systems on the test set.
We chose to use the top 3 runs (rather than the
best run) of each system to account for the insta-
bility of MERT (Clark et al., 2011). This method
is designed to reflect the average performance of
the MT system when trained with random restarts
of MERT: we wish to discount runs in which the
optimizer is stuck in a poor region of the weight
space, but also to average across several good runs
in order not to be mislead by the high variance of
the single best run. For each of our MT systems,
we merged the results of the top 3 runs on the test
set into one file, and ran a statistical significance
test, comparing it to the merged top 3 results from
our baseline system. The 3 runs were merged by
duplicating each run 3 times, and arranging them
in the file so that the significance testing compares
each run with all the runs of the baseline. We per-
formed significance testing using paired bootstrap
resampling (Koehn, 2004). The difference is con-
sidered statistically significant if p < 0.05 using
1000 iterations of paired bootstrap resampling.
3.2 Results
Our results are shown in Table 1. The second
and the third columns contain the average BLEU
score (Papineni et al., 2002) on the top three re-
sults on the development and test sets. The fourth
column is the p-value for statistical significance
testing against the baseline. The first row shows
the results for our baseline. The second row con-
tains the results for using the source (Chinese)
side complete semantic rules of Section 2.1, and
the third row is the results for combining both
the source and the target side complete semantic
rules. As noted before, in both of these experi-
ments we also use the regular GHKM rules. The
result show that the source side complete seman-
tic rules improve the system (p = 0.048), and as
we expected, combining the source and the tar-
1789
Source Sentence ?? ,??????????????????????? .
Reference therefore , it is the international community ?s responsibility to protect the children from harms resulted
from armed conflicts .
Baseline the armed conflicts will harm the importance of the international community the responsibilities . there-
fore , from child protection
Verbet LM therefore , the importance of the international community is to protect children from the harm affected
by the armed conflicts .
Source Sentence ???????? ,??????????? ,???????? .
Reference compared with last year ?s meeting , the smell of gunpowder has disappeared in this year ?s meeting and
the two sides ? standpoints are getting closer .
Baseline disappears on gunpowder , near the stance of the two sides compared with last year ?s meeting , the
meeting of this year .
Verbet LM the smells of gunpowder has disappeared , the position in the two sides approach . compared with last
year ?s meeting , this meeting
(a) Comparison of the language model method (using VerbNet) and the baseline system.
Source Sentence ???????? ,??????????????? .
Reference scientists have boldly predicted that the british spacecraft might have been stuck in a hole .
Baseline scientists boldly expected , this vessel uk may have in the space ship in hang tung .
Semantic Rules scientists have boldly expected this vessel and the possible settlement of the space ship in hang tung .
Source Sentence ????????????????? .
Reference the us government should show goodwills to north korea ?s stand .
Baseline this position of the government of the united states to goodwill toward the dprk .
Semantic Rules this position that the us government should use goodwill toward the dprk .
(b) Comparison of the experiments with source and target side semantic rules and the baseline system.
Figure 4: Comparison of example translations from our semantic methods and the baseline system.
get side rules improves the system even more sig-
nificantly (p < 10
?10
). To measure the effect
of combining the rules, in a separate experiment
we replicated the complete semantic rules exper-
iments of Bazrafshan and Gildea (2013), and ran
statistical significance tests comparing the combi-
nation of the source and target rules with using
only the source or the target semantic rules sep-
arately. The results showed that combining the se-
mantic rules outperforms both of the experiments
that used rules from only one side (with p < 0.05
in both cases).
The results for the language model feature are
shown in the last two rows of the table. Us-
ing Propbank for language model training did not
change the system in any significant way (p =
0.108), but using VerbNet significantly improved
the results (p = 0.025). Figure 4(a) contains an
example comparing the baseline system with the
VerbNet language model. We can see how the
VerbNet language model helps the decoder trans-
late the argument in the correct order. The baseline
system has also generated the correct arguments,
but the output is in the wrong order. Figure 4(b)
compares the experiment with semantic rules of
both target and source side and the baseline sys-
tem. Translation of the word ?use? by our seman-
tic rules is a good example showing how the de-
coder uses these semantic rules to generate a more
complete predicate-argument structure.
4 Conclusions
We experimented with two techniques for incor-
porating semantics in machine translation. The
models were designed to help the decoder trans-
late semantic roles in the correct order, as well
as generating complete predicate-argument struc-
tures. We observed that using a semantic lan-
guage model can significantly improve the trans-
lations, and help the decoder to generate the se-
mantic roles in the correct order. Adding transla-
tion rules with complete semantic structures also
improved our MT system. We experimented with
using source side complete semantic rules, as well
as using rules for both the source and the target
sides. Both of our experiments showed improve-
ments over the baseline, and as expected, the sec-
ond one had a higher improvement.
Acknowledgments
Partially funded by NSF grant IIS-0910611.
1790
References
Marzieh Bazrafshan and Daniel Gildea. 2013. Seman-
tic roles for string to tree machine translation. In
Association for Computational Linguistics (ACL-13)
short paper.
Tagyoung Chung, Licheng Fang, and Daniel Gildea.
2011. Issues concerning decoding with synchronous
context-free grammar. In Proceedings of the ACL
2011 Conference Short Papers, Portland, Oregon.
Association for Computational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for opti-
mizer instability. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: Short Pa-
pers - Volume 2, HLT ?11, pages 176?181, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871?1874, June.
Pascale Fung, Zhaojun Wu, Yongsheng Yang, and
Dekai Wu. 2007. Learning Bilingual Seman-
tic Frames: Shallow Semantic Parsing vs. Seman-
tic Role Projection. In TMI-2007: Proceedings of
the 11 th International Conference on Theoretical
and Methodological Issues in Machine Translation,
Sk?ovde, Sweden.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation
rule? In Proceedings of NAACL-04, pages 273?280,
Boston, MA.
Mark Hopkins and Greg Langmead. 2010. SCFG de-
coding without binarization. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 646?655, Cambridge,
MA, October.
Karin Kipper Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388?395, Barcelona, Spain, July.
Junhui Li, Philip Resnik, and Hal Daum?e III. 2013.
Modeling syntactic and semantic structures in hier-
archical phrase-based translation. In HLT-NAACL,
pages 540?549.
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In COLING-10, Bei-
jing.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL-03, pages 160?167, Sapporo, Japan.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Martha. Palmer. 2009. SemLink: Linking PropBank,
VerbNet and FrameNet. In Proceedings of the Gen-
erative Lexicon ConferenceGenLex-09, Pisa, Italy,
Sept.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of ACL-02, pages 311?318, Philadelphia, PA.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In International Confer-
ence on Spoken Language Processing, volume 2,
pages 901?904.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
smt: A hybrid two-pass model. In Proceedings of
the HLT-NAACL 2009: Short Papers, Boulder, Col-
orado.
Shumin Wu and Martha Palmer. 2011. Semantic map-
ping using automatic word alignment and seman-
tic role labeling. In Proceedings of the Fifth Work-
shop on Syntax, Semantics and Structure in Statisti-
cal Translation, SSST-5, pages 21?30, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Mod-
eling the translation of predicate-argument structure
for smt. In ACL (1), pages 902?911.
Kenji Yamada and Kevin Knight. 2002. A decoder
for syntax-based statistical MT. In Proceedings of
ACL-02, pages 303?310, Philadelphia, PA.
1791
Binarization of Synchronous
Context-Free Grammars
Liang Huang?
USC/Information Science Institute
Hao Zhang??
Google Inc.
Daniel Gildea?
University of Rochester
Kevin Knight?
USC/Information Science Institute
Systems based on synchronous grammars and tree transducers promise to improve the quality
of statistical machine translation output, but are often very computationally intensive. The
complexity is exponential in the size of individual grammar rules due to arbitrary re-orderings
between the two languages. We develop a theory of binarization for synchronous context-free
grammars and present a linear-time algorithm for binarizing synchronous rules when possible.
In our large-scale experiments, we found that almost all rules are binarizable and the resulting
binarized rule set significantly improves the speed and accuracy of a state-of-the-art syntax-
based machine translation system. We also discuss the more general, and computationally more
difficult, problem of finding good parsing strategies for non-binarizable rules, and present an
approximate polynomial-time algorithm for this problem.
1. Introduction
Several recent syntax-based models for machine translation (Chiang 2005; Galley et al
2004) can be seen as instances of the general framework of synchronous grammars
and tree transducers. In this framework, both alignment (synchronous parsing) and
decoding can be thought of as parsing problems, whose complexity is in general ex-
ponential in the number of nonterminals on the right-hand side of a grammar rule.
To alleviate this problem, we investigate bilingual binarization as a technique to fac-
tor each synchronous grammar rule into a series of binary rules. Although mono-
lingual context-free grammars (CFGs) can always be binarized, this is not the case
? Information Science Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: lhuang@isi.edu,
liang.huang.sh@gmail.com.
?? 1600 Amphitheatre Parkway, Mountain View, CA 94303. E-mail: haozhang@google.com.
? Computer Science Dept., University of Rochester, Rochester NY 14627. E-mail: gildea@cs.rochester.edu.
? Information Science Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu.
Submission received: 14 March 2007; revised submission received: 8 January 2009; accepted for publication:
25 March 2009.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 4
for all synchronous rules; we investigate algorithms for non-binarizable rules as well.
In particular:
r We develop a technique called synchronous binarization and devise a
linear-time binarization algorithm such that the resulting rule set alows
efficient algorithms for both synchronous parsing and decoding with
integrated n-gram language models.
r We examine the effect of this binarization method on end-to-end
translation quality on a large-scale Chinese-to-English syntax-based
system, compared to a more typical baseline method, and a state-of-the-art
phrase-based system.
r We examine the ratio of binarizability in large, empirically derived rule
sets, and show that the vast majority is binarizable. However, we also
provide, for the first time, real examples of non-binarizable cases verified
by native speakers.
r In the final, theoretical, sections of this article, we investigate the general
problem of finding the most efficient synchronous parsing or decoding
strategy for arbitrary synchronous context-free grammar (SCFG) rules,
including non-binarizable cases. Although this problem is believed to be
NP-complete, we prove two results that substantially reduce the search
space over strategies. We also present an optimal algorithm that runs
tractably in practice and a polynomial-time algorithm that is a good
approximation of the former.
Melamed (2003) discusses binarization of multi-text grammars on a theoretical
level, showing the importance and difficulty of binarization for efficient synchronous
parsing. One way around this difficulty is to stipulate that all rules must be binary
from the outset, as in Inversion Transduction Grammar (ITG) (Wu 1997) and the binary
SCFG employed by the Hiero system (Chiang 2005) to model the hierarchical phrases.
In contrast, the rule extraction method of Galley et al (2004) aims to incorporate more
syntactic information by providing parse trees for the target language and extracting
tree transducer rules that apply to the parses. This approach results in rules with many
nonterminals, making good binarization techniques critical.
We explain how synchronous rule binarization interacts with n-gram language
models and affects decoding for machine translation in Section 2. We define binarization
formally in Section 3, and present an efficient algorithm for the problem in Section 4.
Experiments described in Section 51 show that binarization improves machine trans-
lation speed and quality. Some rules cannot be binarized, and we present a decoding
strategy for these rules in Section 6. Section 7 gives a solution to the general theo-
retical problem of finding optimal decoding and synchronous parsing strategies for
arbitrary SCFGs, and presents complexity results on the nonbinarizable rules from our
Chinese?English data. These final two sections are of primarily theoretical interest, as
nonbinarizable rules have not been shown to benefit real-world machine translation sys-
tems. However, the algorithms presented may become relevant as machine translation
systems improve.
1 A preliminary version of Section 1?5 appeared in Zhang et al (2006).
560
Huang et al Binarization of Synchronous Context-Free Grammars
2. Motivation
Consider the following Chinese sentence and its English translation:
(1) ?
Ba`owe?ier
Powell

yu?
with
??
Sha?lo?ng
Sharon
>L
ju?x??ng
hold
?
le
[past.]
Grammar Factorization by
Tree Decomposition
Daniel Gildea?
University of Rochester
We describe the application of the graph-theoretic property known as treewidth to the problem of
finding efficient parsing algorithms. This method, similar to the junction tree algorithm used in
graphical models for machine learning, allows automatic discovery of efficient algorithms such
as the O(n4) algorithm for bilexical grammars of Eisner and Satta. We examine the complexity
of applying this method to parsing algorithms for general Linear Context-Free Rewriting Sys-
tems. We show that any polynomial-time algorithm for this problem would imply an improved
approximation algorithm for the well-studied treewidth problem on general graphs.
1. Introduction
In this article, we describe meta-algorithms for parsing: algorithms for finding the
optimal parsing algorithm for a given grammar, with the constraint that rules in the
grammar are considered independently of one another. In order to have a common
representation for our algorithms to work with, we represent parsing algorithms
as weighted deduction systems (Shieber, Schabes, and Pereira 1995; Goodman 1999;
Nederhof 2003). Weighted deduction systems consist of axioms and rules for building
items or partial results. Items are identified by square brackets, with their weights
written to the left. Figure 1 shows a rule for deducing a new item when parsing a
context free grammar (CFG) with the rule S? AB. The item below the line, called
the consequent, can be derived if the two items above the line, called the antecedents,
have been derived. Items have types, corresponding to grammar nonterminals in this
example, and variables, whose values range over positions in the string to be parsed.
We restrict ourselves to items containing position variables directly as arguments; no
other functions or operations are allowed to apply to variables. The consequent?s weight
is the product of the weights of the two antecedents and the rule weight w0. Implicit in
the notation is the fact that we take themaximumweight over all derivations of the same
item. Thus, the weighted deduction system corresponds to the Viterbi or max-product
algorithm for parsing. Applications of the same weighted deduction system with other
semirings are also possible (Goodman 1999).
The computational complexity of parsing depends on the total number of instanti-
ations of variables in the system?s deduction rules. If the total number of instantiations
is M, parsing is O(M) if there are no cyclic dependencies among instantiations, or,
equivalently, if all instantiations can be sorted topologically. In most parsing algorithms,
? Computer Science Department, University of Rochester, Rochester NY 14627.
E-mail: gildea@cs.rochester.edu.
Submission received: 28 June 2010; revised submission received: 20 September 2010; accepted for publication:
21 October 2010.
? 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 1
w1: [A, x0, x1]
w2: [B, x1, x2]
w0w1w2: [S, x0, x2]
Figure 1
CFG parsing in weighted deduction notation.
variables range over positions in the input string. In order to determine complexity in
the length n of the input string, it is sufficient to count the number of unique position
variables in each rule. If all rules have at most k position variables, M = O(nk), and
parsing takes timeO(nk) in the length of the input string. In the remainder of this article,
we will explore methods for minimizing k, the largest number of position variables in
any rule, among equivalent deduction systems. These methods directly minimize the
parsing complexity of the resulting deduction system. Although we will assume no
cyclic dependencies among rule instantiations for the majority of the article, we will
discuss the cyclic case in Section 2.2.
It is often possible to improve the computational complexity of a deduction rule
by decomposing the computation into two or more new rules, each having a smaller
number of variables than the original rule. We refer to this process as factorization. One
straightforward example of rule factorization is the binarization of a CFG, as shown in
Figure 2. Given a deduction rule for a CFG rule with r nonterminals on the righthand
side, and a total of r+ 1 variables, an equivalent set of rules can be produced, each with
three variables, storing intermediate results that indicate that a substring of the original
rule?s righthand side has been recognized. This type of rule factorization produces an
O(n3) parser for any input CFG.
Another well-known instance of rule factorization is the hook trick of Eisner and
Satta (1999), which reduces the complexity of parsing for bilexicalized CFGs from
O(n5) to O(n4). The basic rule for bilexicalized parsing combines two CFG constituents
marked with lexical heads as shown in Figure 3a. Here items with type C indicate
constituents, with [C, x0, h, x1] indicating a constituent extending from position x0 to
position x1, headed by the word at position h. The item [D,m? h] is used to indicate
the weight assigned by the grammar to a bilexical dependency headed by the word at
a)
w1: [A, x0, x1]
w2: [B, x1, x2]
w3: [C, x2, x3]
w4: [D, x3, x4]
w0w1w2w3w4: [S, x0, x4]
b)
w1: [A, x0, x1]
w2: [B, x1, x2]
w1w2: [X, x0, x2]
w5: [X, x0, x2]
w3: [C, x2, x3]
w3w5: [Y, x0, x3]
w6: [Y, x0, x3]
w3: [D, x3, x4]
w0w3w6: [S, x0, x3]
Figure 2
Binarization of the CFG rule S? ABCD as rule factorization: The deduction rule above can be
factored into the three equivalent rule below.
232
Gildea Grammar Factorization by Tree Decomposition
a)
w: [D,m? h]
w1: [C, x0, h, x1]
w2: [C, x1,m, x2]
ww1w2: [C, x0, h, x2]
b)
w: [D,m? h]
w2: [C, x1,m, x2]
ww2: [H, h, x1, x2]
wh: [H, h, x1, x2]
w1: [C, x0, h, x1]
whw1: [C, x0, h, x2]
Figure 3
Rule factorization for bilexicalized parsing.
position h with the word at position m as a modifier. The deduction rule is broken into
two steps, one which includes the weight for the bilexical grammar rule, and another
which identifies the boundaries of the new constituent, as shown in Figure 3b. The hook
trick has also been applied to Tree Adjoining Grammar (TAG; Eisner and Satta 2000),
and has been generalized to improve the complexity of machine translation decoding
under synchronous context-free grammars (SCFGs) with an n-gram language model
(Huang, Zhang, and Gildea 2005).
Rule factorization has also been studied in the context of parsing for SCFGs. Unlike
monolingual CFGs, SCFGs cannot always be binarized; depending on the permutation
between nonterminals in the two languages, it may or may not be possible to reduce the
rank, or number of nonterminals on the righthand side, of a rule. Algorithms for finding
the optimal rank reduction of a specific rule are given by Zhang and Gildea (2007). The
complexity of synchronous parsing for a rule of rank r is O(n2r+2), so reducing rank
improves parsing complexity.
Rule factorization has also been applied to Linear Context-Free Rewriting Systems
(LCFRS), which generalize CFG, TAG, and SCFG to define a rewriting system where
nonterminals may have arbitrary fan-out, which indicates the number of continuous
spans that a nonterminal accounts for in the string (Vijay-Shankar, Weir, and Joshi 1987).
Recent work has examined the problem of factorization of LCFRS rules in order to
reduce rank without increasing grammar fan-out (Go?mez-Rodr??guez et al 2009), as well
as factorization with the goal of directly minimizing the parsing complexity of the new
grammar (Gildea 2010).
We define factorization as a process which applies to rules of the input grammar
independently. Individual rules are replaced with an equivalent set of new rules, which
must derive the same set of consequent items as the original rule given the same an-
tecedent items. While new intermediate items of distinct types may be produced, the set
of items and weights derived by the original weighted deduction system is unchanged.
This definition of factorization is broad enough to include all of the previous examples,
but does not include, for example, the fold/unfold operation applied to grammars by
Johnson (2007) and Eisner and Blatz (2007). Rule factorization corresponds to the unfold
operation of fold/unfold.
If we allow unrestricted transformations of the input deduction system, finding the
most efficient equivalent system is undecidable; this follows from the fact that it is un-
decidable whether a CFG generates the set of all strings (Bar-Hillel, Perles, and Shamir
1961), and would therefore be recognizable in constant time. Whereas the fold/unfold
operation of Johnson (2007) and Eisner and Blatz (2007) specifies a narrower class of
233
Computational Linguistics Volume 37, Number 1
grammar transformations, no general algorithms are known for identifying an optimal
series of transformations in this setting. Considering input rules independently allows
us to provide algorithms for optimal factorization.
In this article, we wish to provide a general framework for factorization of deduc-
tive parsing systems in order to minimize computational complexity. We show how
to apply the graph-theoretic property of treewidth to the factorization problem, and
examine the question of whether efficient algorithms exist for optimizing the parsing
complexity of general parsing systems in this framework. In particular, we show that
the existence of a polynomial time algorithm for optimizing the parsing complexity of
general LCFRS rules would imply an improved approximation algorithm for the well-
studied problem of treewidth of general graphs.
2. Treewidth and Rule Factorization
In this section, we introduce the graph-theoretic property known as treewidth, and
show how it can be applied to rule factorization.
A tree decomposition of a graph G = (V,E) is a type of tree having a subset of G?s
vertices at each node. We define the nodes of this tree T to be the set I, and its edges
to be the set F. The subset of V associated with node i of T is denoted by Xi. A tree
decomposition is therefore defined as a pair ({Xi | i ? I},T = (I,F)) where each Xi, i ? I
is a subset of V, and tree T has the following properties:
 Vertex cover: The nodes of the tree T cover all the vertices of G:
?
i?I Xi = V.
 Edge cover: Each edge in G is included in some node of T. That is, for all
edges (u, v) ? E, there exists an i ? I with u, v ? Xi.
 Running intersection: The nodes of T containing a given vertex of G form a
connected subtree. Mathematically, for all i, j, k ? I, if j is on the (unique)
path from i to k in T, then Xi
?
Xk ? Xj.
The treewidth of a tree decomposition ({Xi},T) is maxi |Xi| ? 1. The treewidth of a
graph is the minimum treewidth over all tree decompositions:
tw(G) = min
({Xi},T)?TD(G)
max
i
|Xi| ? 1
where TD(G) is the set of valid tree decompositions of G. We refer to a tree decomposi-
tion achieving the minimum possible treewidth as being optimal.
In general, more densely interconnected graphs have higher treewidth. Any tree
has treewidth = 1; a graph consisting of one large cycle has treewidth = 2, and a fully
connected graph of n vertices has treewidth= n? 1. Low treewidth indicates some tree-
like structure in the graph, as shown by the example with treewidth = 2 in Figure 4. As
an example of the running intersection property, note that the vertex N appears in three
adjacent nodes of the tree decomposition. Finding the treewidth of a graph is an NP-
complete problem (Arnborg, Corneil, and Proskurowski 1987). However, given a graph
of n vertices and treewidth k, a simple algorithm finds the optimal tree decomposition in
time O(nk+2) (Arnborg, Corneil, and Proskurowski 1987), and a variety of approxima-
tion algorithms and heuristics are known for the treewidth problem (Bodlaender et al
1995; Amir 2001; Feige, Hajiaghayi, and Lee 2005). Furthermore, for fixed k, optimal tree
decompositions can be computed in linear time (Bodlaender 1996).
234
Gildea Grammar Factorization by Tree Decomposition
Figure 4
A tree decomposition of a graph is a set of overlapping clusters of the graph?s vertices, arranged
in a tree. This example has treewidth = 2.
We can factorize a deduction rule by representing the rule as a graph, which we
call a dependency graph, and searching for tree decompositions of this graph. For a
rule r having n variables V = {vi | i ? {1, . . . ,n}}, m antecedent items Ai, i ? {1, . . . ,m},
and consequent C, let V(Ai) ? V be the variables appearing in antecedent Ai, and V(C)
be the variables appearing in the consequent. The dependency graph representation of
the rule is Gr = (V,E =
?
S:A1,...,Am,C
{(vi, vj) | vi, vj ? V(S)}). That is, we have a vertex for
each variable in the rule, and connect any two vertices that appear together in the same
antecedent, or that appear together in the consequent.
The dependency graph representation allows us to prove the following result con-
cerning parsing complexity:
Theorem 1
Given a deduction rule r for parsing where the input string is referenced only through
position variables appearing as arguments of antecedent and consequent items, the opti-
mal complexity of any factorization of rule r isO(ntw(Gr )+1), where Gr is the dependency
graph derived from r.
Proof
One consequence of the definition of a tree decomposition is that, for any clique appear-
ing in the original graph Gr, there must exist a node in the tree decomposition T which
contains all the vertices in the clique. We use this fact to show that there is a one-to-
one correspondence between tree decompositions of a rule?s dependency graph Gr and
factorizations of the rule.
First, we need to show that any tree decomposition of Gr can be used as a factoriza-
tion of the original deduction rule. By our earlier definition, a factorization must derive
the same set of consequent items from a given set of antecedent items as the original
rule. Because Gr includes a clique connecting all variables in the consequent C, the tree
decomposition Tmust have a node Xc such that V(C) ? Xc. We consider this node to be
the root of T. The original deduction rule can be factorized into a new set of rules, one
for each node in T. For node Xc, the factorized rule has C as a consequent, and all other
nodes Xi have a new partial result as a consequent, consisting of the variables Xi ? Xj,
where Xj is Xi?s neighbor on the path to the root node Xc. We must guarantee that the
factorized rule set yields the same result as the original rule, namely, the semiring sum
235
Computational Linguistics Volume 37, Number 1
over all variable values of the semiring product of the antecedents? weights. The tree
structure of T corresponds to a factorization of this semiring expression. For example, if
we represent the CFG rule of Figure 2a with the generalized semiring expression:
?
x1x2x3
A(x0, x1)? B(x1, x2)? C(x2, x3)?D(x3, x4)
the factorization of this expression corresponding to the binarized rule is
?
x3
(
?
x2
(
?
x1
A(x0, x1)? B(x1, x2)
)
? C(x2, x3)
)
?D(x3, x4)
where semiring operations ? and ? have been interchanged as allowed by the depen-
dency graph for this rule.
Because each antecedent Ai is represented by a clique in the graph Gr, the tree
decomposition T must contain at least one node which includes all variables V(Ai).
We can choose one such node and multiply in the weight of Ai, given the values of
variables V(Ai), at this step of the expression. The running intersection property of the
tree decomposition guarantees that each variable has a consistent value at each point
where it is referenced in the factorization.
The same properties guarantee that any valid rule factorization corresponds to a
tree decomposition of the graph Gr. We consider the tree decomposition with a set Xi
for each new rule ri, consisting of all variables used in ri, and with tree edges T defined
by the producer/consumer relation over intermediate results in the rule factorization.
Each antecedent of the original rule must appear in some new rule in the factorization,
as must the consequent of the original rule. Therefore, all edges in the original rule?s
dependency graph Gr appear in some tree node Xi. Any variable that appears in two
rules in the factorizationmust appear in all intermediate rules in order to ensure that the
variable has a consistent value in all rules that reference it. This guarantees the running
intersection property of the tree decomposition ({Xi},T). Thus any rule factorization,
when viewed as a tree of sets of variables, has the properties that make it a valid tree
decomposition of Gr.
The theorem follows as a consequence of the one-to-one correspondence between
rule factorizations and tree decompositions. 
2.1 Computational Complexity
Factorization produces, for each input rule having m antecedents, at most m? 1 new
rules, each containing at most the same number of nonterminals and the same number
of variables as the input rule. Hence, the size of the new factorized grammar is O(|G|2),
andwe avoid any possibility of an exponential increase in grammar size. Tighter bounds
can be achieved for specific classes of input grammars.
The computational complexity of optimal factorization with tree decomposition is
exponential in the size of the input rules. However, optimal factorization is generally
feasible whenever parsing with the unfactorized grammar is feasible. This is because,
for an input rule with  variables, parsing is O(n) in the sentence length n. The
treewidth of this rule is at most ? 1, and can be computed in timeO(+1); generally we
expect n to be greater than . One may also wish to accept only rules having treewidth
k and disregard the remainder, for example, when factorizing rules automatically
236
Gildea Grammar Factorization by Tree Decomposition
extracted from word-aligned bitext (Wellington, Waxmonsky, and Melamed 2006;
Huang et al 2009) or from dependency treebanks (Kuhlmann and Nivre 2006; Gildea
2010). In this setting, the rules having treewidth k can be identified in timeO(k+2) using
the simple algorithm of Arnborg, Corneil, and Proskurowski (1987), (where again 
is the number of variables in the input rules), or in time O() using the algorithm of
Bodlaender (1996).
2.2 Cyclic Dependencies
Although this article primarily addresses the case where there are no cyclic dependen-
cies between rule instantiations, we note here that our techniques carry over to the
cyclic case under certain conditions. If there are cycles in the rule dependencies, but
the semiring meets Knuth?s (1977) definition of a superior function, parsing takes time
O(M logM), where M is the number of rule instantiations, and the extra logM term
accounts for maintaining an agenda as a priority queue (Nederhof 2003). Cycles in
the rule dependencies may arise, for example, from chains of unary productions in a
CFG; the properties of superior functions guarantee that unbounded chains need not
be considered. The max-product semiring used in Viterbi parsing has this property,
assuming that all rule weights are less than one, whereas for exact computation with
the sum-product semiring, unbounded chains must be considered. As in the acyclic
case, M = O(nk) for parsing problems where rules have at most k variables. Under
the assumption of superior functions, parsing takes time O(nkk log n) with Knuth?s
algorithm. In this setting, as in the acyclic case, minimizing k with tree decomposition
minimizes parsing complexity.
2.3 Related Applications of Treewidth
The technique of using treewidth to minimize complexity has been applied to constraint
satisfaction (Dechter and Pearl 1989), graphical models in machine learning (Jensen,
Lauritzen, and Olesen 1990; Shafer and Shenoy 1990), and query optimization for
databases (Chekuri and Rajaraman 1997). Our formulation of parsing is most closely
related to logic programming; in this area treewidth has been applied to limit complex-
ity in settings where either the deduction rules or the input database of ground facts
have fixed treewidth (Flum, Frick, and Grohe 2002). Whereas Flum, Frick, and Grohe
(2002) apply treewidth to nonrecursive datalog programs, our parsing programs have
unbounded recursion, as the depth of the parse tree is not fixed in advance. Our results
for parsing can be seen as a consequence of the fact that, even in the case of unbounded
recursion, the complexity of (unweighted) datalog programs is linear in the number of
possible rule instantiations (McAllester 2002).
3. Examples of Treewidth for Parsing
In this section, we show how a few well-known parsing algorithms can be derived
automatically by finding the optimal tree decomposition of a dependency graph.
To aid in visualization of the graphical representation of deduction rules, we use a
factor graph representation based on that of Kschischang, Frey, and Loeliger (2001) for
Markov Random Fields. Our graphs have three types of nodes: variables, antecedents,
and consequents. Each antecedent node is connected to the variables it contains, and
represents the antecedent?s weight as a function of those variables. Antecedent nodes
are analogous to the factor nodes of Kschischang, Frey, and Loeliger (2001), and
237
Computational Linguistics Volume 37, Number 1
Figure 5
Factor graph for the binary CFG deduction rule of Figure 1.
consequent nodes are a new feature of this representation. We can think of consequents
as factors with weight = 1; they do not affect the weights computed, but serve to
guarantee that the consequent of the original rule can be found in one node of the tree
decomposition. We refer to both antecedent and consequent nodes as factor nodes. Re-
placing each factor node with a clique over its neighbor variables yields the dependency
graph Gr defined earlier. We represent variables with circles, antecedents with squares
labeled with the antecedent?s weight, and consequents with diamonds labeled c. An
example factor graph for the simple CFG rule of Figure 1 is shown in Figure 5.
3.1 CFG Binarization
Figure 6a shows the factor graph derived from the monolingual CFG rule with four
children in Figure 2a. The dependency graph obtained by replacing each factor with
a clique of size 2 (a single edge) is a graph with one large cycle, shown in Figure 6b.
Finding the optimal tree decomposition yields a tree with nodes of size 3, {x0, xi, xi+1}
for each i, shown in Figure 6c. Each node in this tree decomposition corresponds to one
of the factored deduction rules in Figure 2b. Thus, the tree decomposition shows us how
Figure 6
Treewidth applied to CFG binarization.
238
Gildea Grammar Factorization by Tree Decomposition
to parse in time O(n3); finding the tree decomposition of a long CFG rule is essentially
equivalent to converting to Chomsky Normal Form.
3.2 The Hook Trick
The deduction rule for bilexicalized parsing shown in Figure 3a translates into the factor
graph shown in Figure 7a. Factor nodes are created for the two existing constituents
from the chart, with the first extending from position x0 in the string to x1, and the
second from x1 to x2. Both factor nodes are connected not only to the start and end
points, but also to the constituent?s head word, h for the first constituent and m for
the second (we show the construction of a left-headed constituent in the figure). An
additional factor is connected only to h and m to represent the bilexicalized rule weight,
expressed as a function of h and m, which is multiplied with the weight of the two
existing constituents to derive the weight of the new constituent. The new constituent
is represented by a consequent node at the top of the graph?the variables that will be
relevant for its further combination with other constituents are its end points x0 and x2
and its head word h.
Placing an edge between each pair of variable nodes that share a factor, we get Fig-
ure 7b. If we compute the optimal tree decomposition for this graph, shown in Figure 7c,
each of the two nodes corresponds to one of the factored rules in Figure 3b. The largest
node of the tree decomposition has four variables, giving the O(n4) algorithm of Eisner
and Satta (1999).
3.3 SCFG Parsing Strategies
SCFGs generalize CFGs to generate two strings with isomorphic hierarchical structure
simultaneously, and have become widely used as statistical models of machine transla-
tion (Galley et al 2004; Chiang 2007). We write SCFG rules as productions with one
Figure 7
Treewidth applied to bilexicalized parsing.
239
Computational Linguistics Volume 37, Number 1
lefthand side nonterminal and two righthand side strings. Nonterminals in the two
strings are linkedwith superscript indices; symbols with the same indexmust be further
rewritten synchronously. For example,
X ? A(1) B(2) C(3)D(4), A(1) B(2) C(3)D(4) (1)
is a rule with four children and no reordering, whereas
X ? A(1) B(2) C(3)D(4), B(2)D(4) A(1) C(3) (2)
expresses a more complex reordering. In general, we can take indices in the first
righthand-side string to be consecutive, and associate a permutation ? with the second
string. If we use Xi for 0 ? i ? n as a set of variables over nonterminal symbols (for
example,X1 andX2 may both stand for nonterminalA), we canwrite rules in the general
form:
X0 ? X
(1)
1 ? ? ?X
(n)
n , X
(?(1))
?(1) ? ? ?X
(?(n))
?(n)
Unlike monolingual CFGs, SCFGs cannot always be binarized. In fact, the lan-
guages of string pairs generated by a synchronous grammar can be arranged in an
infinite hierarchy, with each rank? 4 producing languages not possible with grammars
restricted to smaller rules (Aho and Ullman 1972). For any grammar with maximum
rank r, converting each rule into a single deduction rule yields an O(n2r+2) parsing
algorithm, because there are r+ 1 boundary variables in each language. More efficient
parsing algorithms are often possible for specific permutations, and, by Theorem 1, the
best algorithm for a permutation can be found by computing the minimum-treewidth
tree decomposition of the graph derived from the SCFG deduction rule for a specific
permutation. For example, for the non-binarizable rule of Equation (2), the resulting
factor graph is shown in Figure 8a, where variables x0, . . . , x4 indicate position variables
in one language of the synchronous grammar, and y0, . . . , y4 are positions in the other
language. The optimal tree decomposition for this rule is shown in Figure 8c. For this
permutation, the optimal parsing algorithm takes time O(n8), because the largest node
in the tree decomposition of Figure 8c includes eight position variables. This result is
intermediate between the O(n6) for binarizable SCFGs, also known as Inversion Trans-
duction Grammars (Wu 1997), and theO(n10) that we would achieve by recognizing the
rule in a single deduction step.
Gildea and S?tefankovic? (2007) use a combinatorial argument to show that as the
number of nonterminals r in an SCFG rule grows, the parsing complexity grows as
?(ncr) for some constant c. In other words, some very difficult permutations exist of all
lengths.
It is interesting to note that although applying the tree decomposition technique
to long CFG rules results in a deduction system equivalent to a binarized CFG, the
individual deduction steps in the best parsing strategy for an SCFG rule do not in
general correspond to SCFG rules. This is because the intermediate results may include
more than one span in each language. These intermediate deduction steps do, however,
correspond to LCFRS rules. We now turn to examine LCFRS in more detail.
240
Gildea Grammar Factorization by Tree Decomposition
Figure 8
Treewidth applied to the SCFG rule of Equation (2).
4. LCFRS Parsing Strategies
LCFRS provides a generalization of a number of widely used formalisms in natural
language processing, including CFG, TAG, SCFG, and synchronous TAG. LCFRS has
also been used to model non-projective dependency grammars, and the LCFRS rules
extracted from dependency treebanks can be quite complex (Kuhlmann and Satta
2009), making factorization important. Similarly, LCFRS can model translation relations
beyond the power of SCFG (Melamed, Satta, and Wellington 2004), and grammars
extracted from word-aligned bilingual corpora can also be quite complex (Wellington,
Waxmonsky, and Melamed 2006). An algorithm for factorization of LCFRS rules is
presented by Gildea (2010), exploiting specific properties of LCFRS. The tree decompo-
sition method achieves the same results without requiring analysis specific to LCFRS.
In this section, we examine the complexity of rule factorization for general LCFRS
grammars.
The problem of finding the optimal factorization of an arbitrary deduction rule is
NP-complete. This follows from the NP-completeness of treewidth using the following
construction: Given a graph, create a deduction rule with a variable for each vertex in
the graph and an antecedent for each edge, containing the two variables associated with
the edge?s endpoints. The graphs produced by LCFRS grammar rules, however, have
certain properties which may make more efficient factorization algorithms possible. We
first define LCFRS precisely before examining the properties of these graphs.
241
Computational Linguistics Volume 37, Number 1
An LCFRS is defined as a tuple G = (VT,VN,P,S), where VT is a set of terminal
symbols, VN is a set of nonterminal symbols, P is a set of productions, and S ? VN is a
distinguished start symbol. Associatedwith each nonterminal B is a fan-out?(B), which
tells how many continuous spans B covers. Productions p ? P take the form:
p : A? g(B1,B2, . . . ,Br) (3)
where A,B1, . . . ,Br ? VN, and g is a function
g : (V?T )
?(B1) ? ? ? ? ? (V?T )
?(Br ) ? (V?T )
?(A)
which specifies how to assemble the
?r
i=1?(Bi) spans of the righthand side nontermi-
nals into the?(A) spans of the lefthand side nonterminal. The function gmust be linear
and non-erasing, which means that if we write
g(?s1,1, . . . , s1,?(B1 )?, . . . , ?s1,1, . . . , s1,?(Br )?) = ?t1, . . . , t?(A)?
the tuple of strings ?t1, . . . , t?(A)? on the righthand side contains each variable si,j from
the lefthand side exactly once, and may also contain terminals from VT. The process of
generating a string from an LCFRS grammar can be thought of as first choosing, top-
down, a production to expand each nonterminal, and then, bottom?up, applying the
functions associated with each production to build the string. As an example, the CFG
S? AB
A? a
B? b
corresponds to the following grammar in LCFRS notation:
S? gS(A,B) gS(?sA?, ?sB?) = ?sAsB?
A? gA() gA() = ?a?
B? gB() gB() = ?b?
Here, all nonterminals have fan-out = 1, reflected in the fact that all tuples defining the
productions? functions contain just one string. As CFG is equivalent to LCFRS with fan-
out = 1, SCFG and TAG can be represented as LCFRS with fan-out = 2. Higher values of
fan-out allow strictly more powerful grammars (Rambow and Satta 1999). Polynomial-
time parsing is possible for any fixed LCFRS grammar, but the degree of the polynomial
depends on the grammar. Parsing general LCFRS grammars, where the grammar is
considered part of the input, is NP-complete (Satta 1992).
4.1 Graphs Derived from LCFRS Rules
Given an LCFRS rule as defined previously, a weighted deduction rule for a bottom?
up parser can be derived by creating an antecedent for each righthand nonterminal,
a consequent for the lefthand side, and variables for all the boundaries of the non-
terminals in the rule. A nonterminal of fan-out f has 2f boundaries. Each boundary
242
Gildea Grammar Factorization by Tree Decomposition
variable will occur exactly twice in the deduction rule: either in two antecedents, if two
nonterminals on the rule?s righthand side are adjacent, or once in an antecedent and
once in the consequent, if the variable indicates a boundary of any segment of the rule?s
lefthand side.
Converting such deduction rules into dependency graphs, we see that the cliques
of the dependency graph may be arbitrarily large, due to the unbounded fan-out of
LCFRS nonterminals. However, each vertex appears in only two cliques, because each
boundary variable in the rule is shared by exactly two nonterminals. In the remainder of
this section, we consider whether the problem of finding the optimal tree decomposition
of this restricted set of graphs is also NP-complete, or whether efficient algorithms may
be possible in the LCFRS setting.
4.2 Approximation of Treewidth for General Graphs
We will show that an efficient algorithm for finding the factorization of an arbitrary
LCFRS production that optimizes parsing complexity would imply the existence of an
algorithm for treewidth that returns a result within a factor of 4?(G) of the optimum,
where ?(G) is the maximum degree of the input graph. Although such an approxima-
tion algorithm may be possible, it would require progress in fundamental problems in
graph theory.
Consider an arbitrary graph G = (V,E), and define k to be its treewidth, k = tw(G).
We wish to construct a new graph G? = (V?,E?) from G in such a way that tw(G?) =
tw(G) and every vertex in G? has even degree. This can be accomplished by doubling
the graph?s edges in the manner shown in Figure 9. To double the edges, for every edge
e = (u, v) in E, we add a new vertex e? to G? and add edges (u, e?) and (v, e?) to G?. We also
include every edge in the original graph G in G?. Now, every vertex v in G? has degree =
2, if it is a newly created vertex, or twice the degree of v in G otherwise, and therefore
?(G?) = 2?(G) (4)
We now show that tw(G?) = tw(G), under the assumption that tw(G) ? 3. Any tree
decomposition of G can be adapted to a tree decomposition of G? by adding a node
containing {u, v, e?} for each edge e in the original graph, as shown in Figure 10. The new
node can be attached to a node containing u and v; because u and v are connected by
an edge in G, such a node must exist in G?s tree decomposition. The vertex e? will not
occur anywhere else in the tree decomposition, and the occurrences of u and v still form
a connected subtree. For each edge e = (u, v) in G?, the tree decomposition must have a
node containing u and v; this is the case because, if e is an original edge from G, there
is already a node in the tree decomposition containing u and v, whereas if e is an edge
to a newly added vertex in G?, one of the newly added nodes in the tree decomposition
Figure 9
An example graph Gex and the result G
?
ex of doubling Gex?s edges.
243
Computational Linguistics Volume 37, Number 1
Figure 10
Tree decompositions of Gex and G
?
ex.
will contain its endpoints. We constructed the new tree decomposition by adding nodes
of size 3. Therefore, as long as the treewidth of G was at least 3, tw(G?) ? tw(G). In
the other direction, because G is a subgraph of G?, any tree decomposition of G? forms a
valid tree decomposition of G after removing the vertices in G? ? G, and hence tw(G?) ?
tw(G). Therefore,
tw(G?) = tw(G) (5)
Because every vertex in G? has even degree, G? has an Eulerian tour, that is, a
path visiting every edge exactly once, beginning and ending at the same vertex. Let
? = ??1, . . . ,?n? be the sequence of vertices along such a tour, with ?1 = ?n. Note that
the sequence ? contains repeated elements. Let ?i, i ? {1, . . . ,n} indicate how many
times we have visited ?i on the ith step of the tour: ?i = |{j | ?j = ?i, j ? {1, . . . , i}}|.
We now construct an LCFRS production P with |V?| righthand side nonterminals from
the Eulerian tour:
P : X ? g(B1, . . . ,B|V?|)
g(?s1,1, . . . , s1,?(B1)?, . . . , ?s|V?|,1, . . . , s|V?|,?(B|V?| )?) = ?s?1,?1 ? ? ? s?n,?n?
The fan-out ?(Bi) of each nonterminal Bi is the number of times vertex i is visited on the
Eulerian tour. The fan-out of the lefthand side nonterminal X is one, and the lefthand
side is constructed by concatenating the spans of each nonterminal in the order specified
by the Eulerian tour.
For the example graph in Figure 9, one valid tour is
?ex = ?A,B,C,D,F,C,E,A,G,B,H,C,A?
This tour results in the following LCFRS production:
Pex : X ? gex(A,B,C,D,E,F,G,H)
gex(?sA,1, sA,2, sA,3?, ?sB,1, sB,2?, ?sC,1, sC,2, sC,3?, ?sD,1?, ?sE,1?, ?sF,1?, ?sG,1?, ?sH,1?) =
?sA,1sB,1sC,1sD,1sF,1sC,2sE,1sA,2sG,1sB,2sH,1sC,3sA,3?
We now construct dependency graph G?? from the LCFRS production P by applying
the technique of Section 2. G?? has n+ 1 vertices, corresponding to the beginning and
244
Gildea Grammar Factorization by Tree Decomposition
end points of the nonterminals in P. The edges in G?? are formed by adding a clique for
each nonterminal in P connecting all its beginning and end points, that is,
(
2f
2
)
edges
for a nonterminal of fan-out f . We must include a clique for X, the lefthand side of
the production. However, because the righthand side of the production begins and
ends with the same nonterminal, the vertices for the beginning and end points of X
are already connected, so the lefthand side does not affect the graph structure for the
entire production. By Theorem 1, the optimal parsing complexity of P is tw(G??)+ 1.
The graphs G? and G?? are related in the following manner: Every edge in G?
corresponds to a vertex in G??, and every vertex in G? corresponds to a clique in G??.
We can identify vertices in G?? with unordered pairs of vertices {u, v} in G?. The edges in
G?? are ({u, v}, {u,w}) ?u, v,w : u = v,u = w, v = w. An example of G?? derived from our
example production Pex is shown in Figure 11.
Any tree decomposition T?? of G?? can be transformed into a valid tree decomposi-
tion T? ofG? by simply replacing each vertex in each node of T?? with both corresponding
vertices in G?. If T?? witnesses a tree decomposition of optimal width k?? = tw(G??), each
node in T?? will produce a node of size at most 2k?? in T?. For any vertex v in G?, one
node in T?? must contain the clique corresponding to v in G??. Each vertex {v,w} in G??
must be found in a contiguous subtree of T??, and these subtrees all include the node
containing the clique for v. The occurrences of v in T? are the union of these contiguous
subtrees, which must itself form a contiguous subtree. Furthermore, each edge (u, v) in
G? corresponds to some vertex in G??, so u and v must occur together in some node of
T?. Combining these two properties, we see that T? is a valid tree decomposition of G?.
From the construction, if SOL is the treewidth of T?, we are guaranteed that
SOL ? 2tw(G??) (6)
In the other direction, any tree decomposition T? of G? can be transformed into a
tree decomposition T?? of G?? by simply replacing each occurrence of vertex v in a node
of T? with all vertices {v,w} in T??. The number of such vertices is the degree of v, ?(v).
Figure 11
Dependency graph G??ex derived from the example of Figure 9. Vertex #A corresponds to the
beginning of the Eulerian tour through G?ex and A# corresponds to the end of the tour; all other
vertices correspond to edges in G?ex.
245
Computational Linguistics Volume 37, Number 1
Each vertex {v,w} occurs in a contiguous subtree of T?? because v and w occurred in
contiguous subtrees of T?, and had to co-occur in at least one node of T?. Each edge
in G?? comes from a clique for some vertex v in G?, so the edge has both its endpoints
in any node of T?? corresponding to a node of T? that contained v. Thus T?? is a valid tree
decomposition of G??. We expand each node in the tree decomposition by at most the
maximum degree of the graph?(G?), and therefore
tw(G??) ? ?(G?)tw(G?) (7)
Assume that we have an efficient algorithm for computing the optimal parsing
strategy of an arbitrary LCFRS rule. Consider the following algorithm for finding a tree
decomposition of an input graph G:
 Transform G to G? of even degree, and construct LCFRS production P from
an Eulerian tour of G?.
 Find the optimal parsing strategy for P.
 Translate this strategy into a tree decomposition of G?? of treewidth k??, and
map this into a tree decomposition of G?, and then remove all new nodes e?
to obtain a tree decomposition of G of treewidth SOL.
If tw(G??) = k??, we have SOL ? 2k?? from Equation (6), and k?? ? ?(G?)tw(G?) from
Equation (7). Putting these together:
SOL ? 2?(G?)tw(G?)
and using Equations (4) and (5) to relate our result to the original graph G,
SOL ? 4?(G)tw(G)
This last inequality proves the main result of this section
Theorem 2
An algorithm for finding the optimal parsing strategy of an arbitrary LCFRS production
would imply a 4?(G) approximation algorithm for treewidth.
Whether such an approximation algorithm for treewidth is possible is an open prob-
lem. The best-known result is the O(
?
log k) approximation result of Feige, Hajiaghayi,
and Lee (2005), which improves on theO(log k) result of Amir (2001). This indicates that,
although polynomial-time factorization of LCFRS rules to optimize parsing complexity
may be possible, it would require progress on general algorithms for treewidth.
5. Conclusion
We have demonstrated that a number of techniques used for specific parsing prob-
lems can be found algorithmically from declarative specifications of the grammar.
Our method involves finding the optimal tree decomposition of a graph, which is in
general an NP-complete problem. However, the relation to tree decomposition allows
us to exploit existing algorithms for this problem, such as the linear time algorithm
of Bodlaender (1996) for graphs of bounded treewidth. In practice, grammar rules are
246
Gildea Grammar Factorization by Tree Decomposition
typically small, and finding the tree decomposition is not computationally expensive,
and in fact is trivial in comparison to the original parsing problem. Given the special
structure of the graphs derived from LCFRS productions, however, we have explored
whether finding optimal tree decompositions of these graphs, and therefore optimal
parsing strategies for LCFRS productions, is also NP-complete. Although a polynomial
time algorithm for this problem would not necessarily imply that P = NP, it would
require progress on fundamental, well-studied problems in graph theory. Therefore, it
does not seem possible to exploit the special structure of graphs derived from LCFRS
productions.
Acknowledgments
This work was funded by NSF grants
IIS-0546554 and IIS-0910611. We are grateful
to Giorgio Satta for extensive discussions on
grammar factorization, as well as for
feedback on earlier drafts from Mehdi Hafezi
Manshadi, Matt Post, and four anonymous
reviewers.
References
Aho, Albert V. and Jeffery D. Ullman. 1972.
The Theory of Parsing, Translation, and
Compiling, volume 1. Prentice-Hall,
Englewood Cliffs, NJ.
Amir, Eyal. 2001. Efficient approximation
for triangulation of minimum treewidth.
In 17th Conference on Uncertainty in
Artificial Intelligence, pages 7?15,
Seattle, WA.
Arnborg, Stefen, Derek G. Corneil, and
Andrzej Proskurowski. 1987. Complexity
of finding embeddings in a k-tree. SIAM
Journal of Algebraic and Discrete Methods,
8:277?284.
Bar-Hillel, Yehoshua, M. Perles, and
E. Shamir. 1961. On formal properties of
simple phrase structure grammars.
Zeitschrift fu?r Phonetik, Sprachwissenschaft
und Kommunikationsforschung, 14:143?172.
Reprinted in Y. Bar-Hillel. (1964). Language
and Information: Selected Essays on Their
Theory and Application, Addison-Wesley
Reading, MA, pages 116?150.
Bodlaender, H. L. 1996. A linear time
algorithm for finding tree decompositions
of small treewidth. SIAM Journal on
Computing, 25:1305?1317.
Bodlaender, Hans L., John R. Gilbert,
Hja?lmty?r Hafsteinsson, and Ton Kloks.
1995. Approximating treewidth,
pathwidth, frontsize, and shortest
elimination tree. Journal of Algorithms,
18(2):238?255.
Chekuri, Chandra and Anand Rajaraman.
1997. Conjunctive query containment
revisited. In Database Theory ? ICDT ?97,
volume 1186 of Lecture Notes in Computer
Science. Springer, Berlin, pages 56?70.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
Dechter, Rina and Judea Pearl. 1989. Tree
clustering for constraint networks.
Artificial Intelligence, 38(3):353?366.
Eisner, Jason and John Blatz. 2007. Program
transformations for optimization of
parsing algorithms and other weighted
logic programs. In Shuly Wintner, editor,
Proceedings of FG 2006: The 11th Conference
on Formal Grammar. CSLI Publications,
pages 45?85, Malaga.
Eisner, Jason and Giorgio Satta. 1999.
Efficient parsing for bilexical context-free
grammars and head automaton grammars.
In Proceedings of the 37th Annual Conference
of the Association for Computational
Linguistics (ACL-99), pages 457?464,
College Park, MD.
Eisner, Jason and Giorgio Satta. 2000. A faster
parsing algorithm for lexicalized
tree-adjoining grammars. In Proceedings of
the 5th Workshop on Tree-Adjoining
Grammars and Related Formalisms (TAG+5),
pages 14?19, Paris.
Feige, Uriel, MohammadTaghi Hajiaghayi,
and James R. Lee. 2005. Improved
approximation algorithms for
minimum-weight vertex separators. In
STOC ?05: Proceedings of the thirty-seventh
annual ACM symposium on Theory of
computing, pages 563?572, Baltimore, MD.
Flum, Jo?rg, Markus Frick, and Martin Grohe.
2002. Query evaluation via
tree-decompositions. Journal of the ACM,
49(6):716?752.
Galley, Michel, Mark Hopkins, Kevin Knight,
and Daniel Marcu. 2004. What?s in a
translation rule? In Proceedings of the 2004
Meeting of the North American Chapter of the
Association for Computational Linguistics
(NAACL-04), pages 273?280, Boston, MA.
Gildea, Daniel. 2010. Optimal parsing
strategies for Linear Context-Free
247
Computational Linguistics Volume 37, Number 1
Rewriting Systems. In Proceedings of the
2010 Meeting of the North American Chapter
of the Association for Computational
Linguistics (NAACL-10), pages 769?776,
Los Angeles, CA.
Gildea, Daniel and Daniel S?tefankovic?. 2007.
Worst-case synchronous grammar rules. In
Proceedings of the 2007 Meeting of the North
American Chapter of the Association for
Computational Linguistics (NAACL-07),
pages 147?154, Rochester, NY.
Go?mez-Rodr??guez, Carlos, Marco
Kuhlmann, Giorgio Satta, and David Weir.
2009. Optimal reduction of rule length in
Linear Context-Free Rewriting Systems. In
Proceedings of the 2009 Meeting of the North
American Chapter of the Association for
Computational Linguistics (NAACL-09),
pages 539?547, Boulder, CO.
Goodman, Joshua. 1999. Semiring parsing.
Computational Linguistics, 25(4):573?605.
Huang, Liang, Hao Zhang, and Daniel
Gildea. 2005. Machine translation as
lexicalized parsing with hooks. In
International Workshop on Parsing
Technologies (IWPT05), pages 65?73,
Vancouver.
Huang, Liang, Hao Zhang, Daniel Gildea,
and Kevin Knight. 2009. Binarization of
synchronous context-free grammars.
Computational Linguistics, 35(4):559?595.
Jensen, Finn V., Steffen L. Lauritzen, and
Kristian G. Olesen. 1990. Bayesian
updating in causal probabilistic networks
by local computations. Computational
Statistics Quarterly, 4:269?282.
Johnson, Mark. 2007. Transforming
projective bilexical dependency grammars
into efficiently-parsable CFGs with
unfold-fold. In Proceedings of the 45th
Annual Meeting of the Association of
Computational Linguistics, pages 168?175,
Prague.
Knuth, D. 1977. A generalization of Dijkstra?s
algorithm. Information Processing Letters,
6(1):1?5.
Kschischang, F. R., B. J. Frey, and H. A.
Loeliger. 2001. Factor graphs and the
sum-product algorithm. IEEE Transactions
on Information Theory, 47(2):498?519.
Kuhlmann, Marco and Joakim Nivre. 2006.
Mildly non-projective dependency
structures. In Proceedings of the International
Conference on Computational
Linguistics/Association for Computational
Linguistics (COLING/ACL-06),
pages 507?514, Sydney.
Kuhlmann, Marco and Giorgio Satta. 2009.
Treebank grammar techniques for
non-projective dependency parsing. In
Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL-09),
pages 478?486, Athens.
McAllester, David. 2002. On the complexity
analysis of static analyses. Journal of the
ACM, 49(4):512?537.
Melamed, I. Dan, Giorgio Satta, and Ben
Wellington. 2004. Generalized multitext
grammars. In Proceedings of the 42nd
Annual Conference of the Association for
Computational Linguistics (ACL-04),
pages 661?668, Barcelona.
Nederhof, M.-J. 2003. Weighted deductive
parsing and Knuth?s algorithm.
Computational Linguistics, 29(1):135?144.
Rambow, Owen and Giorgio Satta. 1999.
Independent parallelism in finite
copying parallel rewriting systems.
Theoretical Computer Science,
223(1-2):87?120.
Satta, Giorgio. 1992. Recognition of Linear
Context-Free Rewriting Systems. In
Proceedings of the 30th Annual Conference of
the Association for Computational Linguistics
(ACL-92), pages 89?95, Newark, DE.
Shafer, G. and P. Shenoy. 1990. Probability
propagation. Annals of Mathematics and
Artificial Intelligence, 2:327?353.
Shieber, Stuart M., Yves Schabes, and
Fernando C. N. Pereira. 1995. Principles
and implementation of deductive parsing.
The Journal of Logic Programming,
24(1-2):3?36.
Vijay-Shankar, K., D. L. Weir, and A. K. Joshi.
1987. Characterizing structural
descriptions produced by various
grammatical formalisms. In Proceedings of
the 25th Annual Conference of the Association
for Computational Linguistics (ACL-87),
pages 104?111, Stanford, CA.
Wellington, Benjamin, Sonjia Waxmonsky,
and I. Dan Melamed. 2006. Empirical
lower bounds on the complexity of
translational equivalence. In Proceedings of
the International Conference on Computational
Linguistics/Association for Computational
Linguistics (COLING/ACL-06),
pages 977?984, Sydney.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
Zhang, Hao and Daniel Gildea. 2007.
Factorization of synchronous context-free
grammars in linear time. In NAACL
Workshop on Syntax and Structure in
Statistical Translation (SSST), pages 25?32,
Rochester, NY.
248
On the String Translations Produced by Multi
Bottom?Up Tree Transducers
Daniel Gildea?
University of Rochester
Tree transducers are defined as relations between trees, but in syntax-based machine translation,
we are ultimately concerned with the relations between the strings at the yields of the input and
output trees. We examine the formal power of Multi Bottom-Up Tree Transducers from this point
of view.
1. Introduction
Many current approaches to syntax-based statistical machine translation fall under
the theoretical framework of synchronous tree substitution grammars (STSGs). Tree
substitution grammars (TSGs) generalize context-free grammars (CFGs) in that each
rule expands a nonterminal to produce an arbitrarily large tree fragment, rather than
a fragment of depth one as in a CFG. Synchronous TSGs generate tree fragments in
the source and target languages in parallel, with each rule producing a tree fragment
in either language. Systems such as that of Galley et al (2006) extract STSG rules from
parallel bilingual text that has been automatically parsed in one language, and the STSG
nonterminals correspond to nonterminals in these parse trees. Chiang?s (2007) Hiero
system produces simpler STSGs with a single nonterminal.
STSGs have the advantage that they can naturally express many re-ordering and
restructuring operations necessary for machine translation (MT). They have the dis-
advantage, however, that they are not closed under composition (Maletti et al 2009).
Therefore, if one wishes to construct anMT system as a pipeline of STSG operations, the
result may not be expressible as an STSG. Recently, Maletti (2010) has argued that multi
bottom?up tree transducers (MBOTs) (Lilin 1981; Arnold and Dauchet 1982; Engelfriet,
Lilin, and Maletti 2009) provide a useful representation for natural language processing
applications because they generalize STSGs, but have the added advantage of being
closed under composition. MBOTs generalize traditional bottom?up tree transducers in
that they allow transducer states to pass more than one output subtree up to subsequent
transducer operations. The number of subtrees taken by a state is called its rank. MBOTs
are linear and non-deleting; that is, operations cannot copy or delete arbitrarily large
tree fragments.
Although STSGs and MBOTs both perform operations on trees, it is important to
note that, in MT, we are primarily interested in translational relations between strings.
Tree operations such as those provided by STSGs are ultimately tools to translate a string
? Computer Science Department, University of Rochester, Rochester NY 14627.
E-mail: gildea@cs.rochester.edu.
Submission received: 3 May 2011; revised submission received: 1 October 2011; accepted for publication:
28 October 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 3
in one natural language into a string in another. Whereas MBOTs originate in the tree
transducer literature and are defined to take a tree as input, MT systems such as those
of Galley et al (2006) and Chiang (2007) find a parse of the source language sentence
as part of the translation process, and the decoding algorithm, introduced by Yamada
and Knight (2002), has more in common with CYK parsing than with simulating a tree
transducer.
In this article, we investigate the power of MBOTs, and of compositions of STSGs
in particular, in terms of the set of string translations that they generate. We relate
MBOTs and compositions of STSGs to existing grammatical formalisms defined on
strings through five main results, which we outline subsequently. The first four results
serve to situate general MBOTs among string formalisms, and the fifth result addresses
MBOTs resulting from compositions of STSGs in particular.
Our first result is that the translations produced by MBOTs are a subset of those
produced by linear context-free rewriting systems (LCFRSs) (Vijay-Shankar, Weir, and
Joshi 1987). LCFRS provides a very general framework that subsumes CFG, tree ad-
joining grammar (TAG; Joshi, Levy, and Takahashi 1975; Joshi and Schabes 1997), and
more complex systems, as well as synchronous context-free grammar (SCFG) (Aho and
Ullman 1972) and synchronous tree adjoining grammar (STAG) (Shieber and Schabes
1990; Schabes and Shieber 1994) in the context of translation. LCFRS allows gram-
mar nonterminals to generate more than one span in the final string; the number of
spans produced by an LCFRS nonterminal corresponds to the rank of an MBOT state.
Our second result states that the translations produced by MBOTs are equivalent to
a specific restricted form of LCFRS, which we call 1-m-LCFRS. From the construction
relating MBOTs and 1-m-LCFRSs follow results about the source and target sides of the
translations produced by MBOTs. In particular, our third result is that the translations
produced by MBOTs are context-free within the source language, and hence are strictly
less powerful than LCFRSs. This implies that MBOTs are not as general as STAGs, for
example. Similarly, MBOTs are not as general as the generalized multitext grammars
proposed for machine translation by Melamed (2003), which retain the full power of
LCFRSs in each language (Melamed, Satta, and Wellington 2004). Our fourth result
is that the output of an MBOT, when viewed as a string language, does retain the
full power of LCFRSs. This fact is mentioned by Engelfriet, Lilin, and Maletti (2009,
page 586), although no explicit construction is given.
Our final result specifically addresses the string translations that result from com-
positions of STSGs, with the goal of better understanding the complexity of using such
compositions in machine translation systems. We show that the translations produced
by compositions of STSGs are more powerful than those produced by single STSGs,
or, equivalently, by SCFGs. Although it is known that STSGs are not closed under
composition, the proofs used previously in the literature rely on differences in tree
structure, and do not generate string translations that cannot be generated by STSG.
Our result implies that current approaches to machine translation decoding will need
to be extended to handle arbitrary compositions of STSGs.
We now turn to give definitions of MBOTs and LCFRSs in the next section, before
presenting our results on general MBOTs in Section 3, and our result on compositions
of STSGs in Section 4.
2. Preliminaries
A ranked alphabet is an alphabet where each symbol has an integer rank, denoting the
number of children the symbol takes in a tree. T? denotes the set of trees constructed
674
Gildea On the String Translations Produced by Multi Bottom?Up Tree Transducers
from ranked alphabet ?. We use parentheses to write trees: for example, a(b, c, d) is an
element of T? if a is an element of ? with rank 3, and b, c, and d are elements of ? with
rank 0. Similarly, given a ranked alphabet ? and a set X, ?(X) denotes the set of trees
consisting of a single symbol of ? of rank k dominating a sequence of k elements from
X. We use T?(X) to denote the set of arbitrarily sized trees constructed from ranked
alphabet ? having items from set X at some leaf positions. That is, T?(X) is the smallest
set such thatX ? T?(X) and ?(t1, . . . , tk) ? T?(X) if ? is an element of?with rank k, and
t1, . . . , tk ? T?(X). Amulti bottom?up tree transducer (MBOT) (Lilin 1981; Arnold and
Dauchet 1982; Engelfriet, Lilin, and Maletti 2009; Maletti 2010) is a system (S,?,?,F,R)
where:
 S,?, and? are ranked alphabets of states, input symbols, and output
symbols, respectively.
 F ? S is a set of accepting states.
 R is a finite set of rules l? r where, using a set of variables X,
l ? T?(S(X)), and r ? S(T?(X)) such that:
? every x ? X that occurs in l occurs exactly once in r and vice versa,
and
? l ? S(X) or r ? S(X).
One step in an MBOT transduction is performed by rewriting a local tree fragment as
specified by one of the rules in R. We replace the fragment l with r, copying the subtree
under each variable in l to the location of the corresponding variable in r. Transducer
rules apply bottom?up from the leaves of the input tree, as shown in Figure 1, and must
terminate in an accepting state. We use underlined symbols for the transducer states,
in order to distinguish them from the symbols of the input and output alphabets.
We define a translation to be a set of string pairs, and we define the yield of an
MBOT M to be the set of string pairs (s, t) such that there exist: a tree s? ? T? having s
as its yield, a tree t? ? T? having t as its yield, and a transduction from s? to t? that is
accepted byM. We refer to s as the source side and t as the target side of the translation.
We use the notation source(T) to denote the set of source strings of a translation T,
source(T) = {s | (s, t) ? T}, and we use the notation target(T) to denote the set of target
strings. We use the notation yield(MBOT) to denote the set of translations produced
by the set of all MBOTs.
A linear context-free rewriting system (LCFRS) is defined as a system
(VN,VT,P,S), where VN is a set of nonterminal symbols, VT is a set of terminal symbols,
P is a set of productions, and S ? VN is a distinguished start symbol. Associated with
each nonterminal B is a fan-out ?(B), which tells how many spans B covers in the final
string. Productions p ? P take the form: p : A? g(B1,B2, . . . ,Br), where A,B1, . . . ,Br ?
VN, and g is a function g : (V
?
T )
?(B1) ? ? ? ? ? (V?T )
?(Br ) ? (V?T )
?(A), which specifies how
to assemble the
?r
i=1?(Bi) spans of the righthand side nonterminals into the ?(A)
spans of the lefthand side nonterminal. The function gmust be linear and non-erasing,
which means that if we write
g(?x1,1, . . . , x1,?(B1 )?, . . . , ?xr,1, . . . , xr,?(Br )?) = ?t1, . . . , t?(A)?
the tuple of strings ?t1, . . . , t?(A)? on the right-hand side contains each variable xi,j from
the left-hand side exactly once, and may also contain terminals from VT. The process
of generating a string from an LCFRS grammar consists of first choosing, top?down, a
production to expand each nonterminal, and then, bottom?up, applying the functions
675
Computational Linguistics Volume 38, Number 3
Figure 1
Step-by-step example of an MBOT tree transduction. The left column shows the transducer rule
applied at each step; only the last rule contains variables, whereas the others contain alphabet
symbols of rank zero at their leaves. State VPQ has rank two, and states NP and S have rank one.
associated with each production to build the string. We refer to the tree induced by top?
down nonterminal expansions of an LCFRS as the derivation tree, or sometimes simply
as a derivation.
As an example of how the LCFRS framework subsumes grammatical formalisms
such as CFG, consider the following CFG:
S? AB
A? a
B? b
This grammar corresponds to the following grammar in LCFRS notation:
S? gS(A,B) gS(?sA?, ?sB?) = ?sAsB?
A? gA() gA() = ?a?
B? gB() gB() = ?b?
Here, all nonterminals have fan-out one, reflected in the fact that all tuples defining
the productions? functions contain just one string. Just as CFG is equivalent to LCFRS
with fan-out 1, SCFG and TAG can be represented as LCFRS with fan-out 2. Higher
values of fan-out allow strictly more powerful grammars (Rambow and Satta 1999).
Polynomial-time parsing is possible for any fixed LCFRS grammar, but the degree of
676
Gildea On the String Translations Produced by Multi Bottom?Up Tree Transducers
the polynomial depends on the grammar. Parsing general LCFRS grammars, where the
grammar is considered part of the input, is NP-complete (Satta 1992).
Following Melamed, Satta, and Wellington (2004), we represent translation in
LCFRS by using a special symbol # to separate the strings of the two languages. Our
LCFRS grammars will only generate strings of the form s#t, where s and t are strings
not containing the symbol #, and we will identify s as the source string and t as the
target string. We use the notation trans(LCFRS) to denote the set of translations that
can be produced by taking the string language of some LCFRS and splitting each string
into a pair at the location of the # symbol.
3. Translations Produced by General MBOTs
In this section, we relate the yield of general MBOTs to string rewriting systems.
To begin, we show that the translation produced by any MBOT is also produced
by an LCFRS by giving a straightforward construction for converting MBOT rules to
LCFRS rules.
We first consider MBOT rules having only variables, as opposed to alphabet
symbols of rank zero, at their leaves. For an MBOT rule l? r with l ? T?(S(X)), let
S1,S2, . . . ,Sk be the sequence of states appearing from left to right immediately above
the leaves of l. Without loss of generality, we will name the variables such that xi,j is the
jth child of the ith state, Si, and the sequence of variables at the leaves of l, read from
left to right, is: x1,1, . . . , x1,d(S1 ), . . . , xk,1, . . . , xk,d(Sk ), where d(Si) is the rank of state Si.
Let S0 be the state symbol at the root of the right-hand-side (r.h.s.) tree r ? S(T?(X)).
Let ? and ? be functions such that x?(1),?(1), x?(2),?(2), . . . , x?(n),?(n) is the sequence
of variables at the leaves of r read from left to right. We will call this sequence the yield
of r. Finally, let p(i) for 1 ? i ? d(S0) be the position in the yield of r of the rightmost
leaf of S0?s ith child. Thus, for all i, 1 ? p(i) ? n.
Given this notation, the LCFRS rule corresponding to the MBOT rule l? r is
constructed as S0 ? g(S1,S2, . . . ,Sk). The LCFRS nonterminal Si has fan-out equal
to the corresponding MBOT state?s rank plus one: ?(Si) = d(Si)+ 1. This is because
the LCFRS nonterminal has one span in the source language, and d(Si) spans in
the target language of the translation. The combination function for the LCFRS rule
S0 ? g(S1,S2, . . . ,Sk) is:
g(?e1, f1,1, . . . , f1,d(S1 )?, . . . , ?ek, fk,1, . . . , fk,d(Sk )?) =
? e1 ? ? ? ek, f?(1),?(1) ? ? ? f?(p(1)),?(p(1)),
f?(p(1)+1),?(p(1)+1) ? ? ? f?(p(2)),?(p(2)),
. . . ,
f?(p(d(S0 )?1)+1),?(p(d(S0 )?1)+1) ? ? ? f?(p(d(S0 ))),?(p(d(S0 )) ?
Here we use ei for the variables in the LCFRS rule corresponding to spans in the input
tree of the MBOT, and fi,j for variables corresponding to the output tree. The pattern in
which these spans fit together is specified by the functions ? and ? that were read off of
the MBOT rule.
Examples of the conversion of an MBOT rule to an LCFRS rule are shown in
Figure 2. The first example shows an MBOT rule derived from an STSG rule, in this
case converting SVO (as in English) to VSO (as in Arabic) word order. The states of
an MBOT rule derived from an STSG rule always have rank 1. In the resulting LCFRS
677
Computational Linguistics Volume 38, Number 3
rule, this means that every nonterminal in the grammar has fan-out 2, corresponding
to one span in the source language string and one span in the target language string of
the translation. This is what we would expect, given that, in terms of the translations
produced, STSG is equivalent to SCFG (because the internal tree structure of the rules is
irrelevant), and SCFG falls within the class of LCFRS grammars of fan-out 2. Figure 2b
shows a more general example, where the states of the MBOT rule have rank > 1.
Now we extend this construction to handle tree symbols of rank zero, which corre-
spond to terminal symbols in the LCFRS. Let ?0 be the sequence of rank zero symbols
appearing at the leaves of l to the left of x1,1, and let ?i for 1 ? i ? k be the sequence of
rank zero symbols to the right of xi,d(Si ), and to the left of xi+1,1 if i < k. Let ?i,j be the
sequence of symbols of rank zero at the leaves of r appearing in the subtree under the
ith child of S0 after the jth variable in this subtree and before the j+ 1th variable, with
?i,0 being to the left of the first variable, and ?i,p(i) being to the right of the last variable.
We can add these sequences of terminal symbols to the LCFRS rule as follows:
g(?e1, f1,1, . . . , f1,d(S1 )?, . . . , ?ek, fk,1, . . . , fk,d(Sk )?) =
? ?0e1?1 ? ? ? ek?k,
?1,0f?(1),?(1)?1,1 ? ? ? f?(p(1)),?(p(1))?1,p(1),
?2,0f?(p(1)+1),?(p(1)+1)?2,1 ? ? ? f?(p(2)),?(p(2))?2,p(2),
. . . ,
?d(S0 ),0f?(p(d(S0 )?1)+1),?(p(d(S0 )?1)+1)?d(S0 ),1 ? ? ? f?(p(d(S0 ))),?(p(d(S0 ))?d(S0 ),p(d(S0 )) ?
An example of this conversion is shown in Figure 3. In this example, ?1 = of, ?1,1 =?,
all other ? and ? values are the empty string, and d(S0) = 1. We refer to the LCFRS rule
constructed from MBOT rule l? r as pl?r.
Figure 2
Examples of the conversion of an MBOT rule to an LCFRS rule.
678
Gildea On the String Translations Produced by Multi Bottom?Up Tree Transducers
Figure 3
Conversion of an MBOT rule with symbols of rank zero to an LCFRS production with terminals.
Finally, we add a start rule rule S? g(Si), g(?e, f ?) = ?e#f ? for each Si ? F to gener-
ate all final states Si of the MBOT from the start symbol S of the LCFRS.
We now show that the language of the LCFRS constructed from a given MBOT is
identical to the yield of the MBOT. We represent MBOT transductions as derivation
trees, where each node is labeled with an MBOT rule, and each node?s children are
the rules used to produce the subtrees matched by any variables in the rule. We can
construct an LCFRS derivation tree by simply relabeling each node with the LCFRS rule
constructed from the node?s MBOT rule. Because, in the MBOT derivation tree, each
node has children which produce the states required by the the MBOT rule?s left-hand
side (l.h.s.), it also holds that, in the LCFRS derivation tree, each node has as its children
rules which expand the set of nonterminals appearing in the parent?s r.h.s. Therefore
the LCFRS tree constitutes a valid derivation.
Given the mapping from MBOT derivations to LCFRS derivations, the following
lemma relates the strings produced by the derivations:
Lemma 1
Let TMBOT be an MBOT derivation tree with I as its input tree and O as its output tree,
and construct TLCFRS by mapping each node nMBOT in TMBOT to a node nLCFRS labeled
with the LCFRS production constructed from the rule at nMBOT. Let ?t0, t1, . . . , tk? be the
string tuple returned by the LCFRS combination function at any node nLCFRS in TLCFRS.
The string t0 contains the yield of the node of I at which the MBOT rule at the node of
TMBOT corresponding to nLCFRS was applied. Furthermore, the strings t1, . . . , tk contain
the k yields of the k MBOT output subtrees (subtrees of O) that are found as children
of the root (state symbol) of the MBOT rule?s right-hand side.
Proof
When we apply the LCFRS combination functions to build the string produced by the
LCFRS derivation, the sequence of function applications corresponds exactly to the
bottom?up application ofMBOT rules to the input tree. Let us refer to the tuple returned
by one LCFRS combination function g as ?t0, t1, . . . , tk?. An MBOT rule applying at the
bottom of the input tree cannot contain any variables, and for MBOT rules of this type,
our construction produces an LCFRS rule with a combination function of the form:
g() = ??0,?1,0, . . . ,?k,0?
taking no arguments and returning string constants equal to the yield of the MBOT
rule?s l.h.s, and the sequence of yields of the k subtrees under the r.h.s.?s root. Now we
consider how further rules in the LCFRS derivation make use of the tuple ?t0, t1, . . . , tk?.
Our LCFRS combination functions always concatenate the first elements of the input
679
Computational Linguistics Volume 38, Number 3
tuple in order, adding any terminals present in the portion of the input tree matched
by the MBOT?s l.h.s. Thus the combination functions maintain the property that the
first element in the resulting tuple, t0, contains the yield of the subtree of the input
tree where the corresponding MBOT rule applied. The combination functions combine
the remaining elements in their input tuples in the same order given by the MBOT
rule?s r.h.s., again adding any terminals added to the output tree by the MBOT rule.
Thus, at each step, the strings t1, . . . , tk returned by LCFRS combination functions
contain the k yields of the k MBOT output subtrees found as children of the root (state
symbol) of the MBOT rule?s r.h.s. By induction, the lemma holds at each node in the
derivation tree. 
The correspondence between LCFRS string tuples and MBOT tree yields gives us
our first result:
Theorem 1
yield(MBOT) ? trans(LCFRS).
Proof
From a given MBOT, construct an LCFRS as described previously. For any transduction
of the MBOT, from Lemma 1, there exists an LCFRS derivation which produces a string
consisting of the yield of the MBOT?s input and output trees joined by the # symbol. In
the other direction, we note that any valid derivation of the LCFRS corresponds to an
MBOT transduction on some input tree; this input tree can be constructed by assembling
the left-hand sides of the MBOT rules from which the LCFRS rules of the LCFRS
derivation were originally constructed. Because there is a one-to-one correspondence
between LCFRS and MBOT derivations, the translation produced by the LCFRS and
the yield of the MBOT are identical.
Because we can construct an LCFRS generating the same translation as the yield of
any given MBOT, we see that yield(MBOT) ? trans(LCFRS). 
The translations produced by MBOTs are equivalent to the translations produced
by a certain restricted class of LCFRS grammars, which we now specify precisely.
Theorem 2
The class of translations yield(MBOT) is equivalent to yield(1-m-LCFRS), where 1-m-
LCFRS is defined to be the class of LCFRS grammars where each rule either is a start
rule of the form S? g(Si), g(?e, f ?) = ?e#f ?, or meets both of the following conditions:
 The combination function keeps the two sides of the translation separate.
That is, it must be possible to write
g(?e1, f1,1, . . . , f1,?1?, . . . , ?er, fr,1, . . . , fr,?r?)
as
g1(?e1?, . . . , ?er?)+ g2(? f1,1, . . . , f1,?1?, . . . , ? fr,1, . . . , fr,?r?)
where + represents tuple concatenation, for some functions g1 and g2.
 The function g1 returns a tuple of length 1.
680
Gildea On the String Translations Produced by Multi Bottom?Up Tree Transducers
Proof
Our construction for transforming an MBOT to an LCFRS produces LCFRS grammars
satisfying the given constraints, so yield(MBOT) ? trans(1-m-LCFRS).
To show the other direction, we will construct an MBOT from a 1-m-LCFRS. For
each 1-m-LCFRS rule of the form
S? g(B1, . . . ,Br)
g(?e1, f1,1, . . . , f1,?1?, . . . , ?er, fr,1, . . . , fr,?r?) =
g1(?e1?, . . . , ?er?)+ g2(? f1,1, . . . , f1,?1?, . . . , ? fr,1, . . . , fr,?r?)
g1(?e1?, . . . , ?er?) = ??0e1 ? ? ??r?1er?r?
g2(? f1,1, . . . , f1,?1?, . . . , ? fr,1, . . . , fr,?r?) = ?t1,1 ? ? ? t1,n1 , . . . , tm,1 ? ? ? tm,nm?
where each ?i is a string of terminals, and each symbol ti,j is either a variable fi?,j? , or a
single terminal, we construct the MBOT rule:
S
?0 B1
f1,1 . . . f1,?(B1 )
? ? ? ?r?1 Br
fr,1 . . . fr,?(Br )
?r
? S
S1
t1,1 ? ? ? t1,n1
? ? ? Sm
tm,1 ? ? ? tm,nm
By the same reasoning used for our construction of LCFRS grammars from MBOTs,
there is a one-to-one correspondence between derivation trees of the 1-m-LCFRS and the
constructed MBOT, and the yield strings also correspond at each node in the derivation
trees. Therefore, yield(1-m-LCFRS) ? yield(MBOT).
Because we have containment in both directions, yield(MBOT) = trans(1-m-
LCFRS). 
We now move on to consider the languages formed by the source and target
projections of MBOT translations.
Grammars of the class 1-m-LCFRS have the property that, for any nonterminal A
(other than the start symbol S) having fan-out ?(A), one span is always realized in the
source string (to the left of the # separator), and ?(A)? 1 spans are always realized
in the target language (to the right of the separator). This property is introduced by
the start rules S? g(Si), g(?e, f ?) = ?e#f ? and is maintained by all further productions
because of the condition on 1-m-LCFRS that the combination function must keep the
two sides of translation separate. For a 1-m-LCFRS rule constructed from an MBOT,
we define the rule?s source language projection to be the rule obtained by discarding
all the target language spans, as well as the separator symbol # in the case of the start
productions. The definition of 1-m-LCFRS guarantees that the combination function
returning a rule?s l.h.s. source span needs to have only the r.h.s. source spans available
as arguments.
For an LCFRS G, we define L(G) to be the language produced by G. We define
source(G) to be the LCFRS obtained by projecting each rule in G. Because more than
one rule may have the same projection, we label the rules of source(G) with their origin
rule, preserving a one-to-one correspondence between rules in the two grammars. Sim-
ilarly, we obtain a rule?s target language projection by discarding the source language
spans, and define target(G) to be the resulting grammar.
681
Computational Linguistics Volume 38, Number 3
Lemma 2
For an LCFRSG constructed from anMBOTM by the given construction, L(source(G))=
source(trans(M)), and L(target(G)) = target(trans(M)).
Proof
There is a valid derivation tree in the source language projection for each valid deriva-
tion tree in the full LCFRS, because for any expansion rewriting a nonterminal of fan-
out ?(A) in the full grammar, we can apply the projected rule to the corresponding
nonterminal of fan-out 1 in the projected derivation. In the other direction, for any
expansion in a derivation of the source projection, a nonterminal of fan-out ?(A) will
be available for expansion in the corresponding derivation of the full LCFRS. Because
there is a one-to-one correspondence between derivations in the full LCFRS and its
source projection, the language generated by the source projection is the source of the
translation generated by the original LCFRS. By the same reasoning, there is a one-to-
one correspondence between derivations in the target projection and the full LCFRS,
and the language produced by the target projection is the target side of the translation
of the full LCFRS. 
Lemma 2 implies that it is safe to evaluate the power of the source and target
projections of the LCFRS independently. This fact leads to our next result.
Theorem 3
yield(MBOT)  trans(LCFRS).
Proof
In the LCFRS generated by our construction, all nonterminals have fan-out 1 in the
source side of the translation. Therefore, the source side of the translation is a context-
free language, and an MBOT cannot represent the following translation:
{(anbncndn, anbncndn) | n ? 1}
which is produced by an STAG (Shieber and Schabes 1990; Schabes and Shieber 1994).
Because STAG is a type of LCFRS, yield(MBOT)  trans(LCFRS). 
Although the source side of the translation produced by an MBOT must be a
context-free language, we now show that the target side can be any language produced
by an LCFRS.
Theorem 4
target(yield(MBOT)) = LCFRS
Proof
Given an input LCFRS, we can construct an MBOT whose target side corresponds to
the rules in the original LCFRS, and whose source simply accepts derivation trees of the
LCFRS. To make this precise, given an LCFRS rule in the general form:
S? g(B1, . . . ,Br)
g(?x1,1, . . . , x1,?(B1 )?, . . . , ?x1,1, . . . , x1,?(Br )?) = ?t1,1 ? ? ? t1,n1 , . . . , t?(S),1 ? ? ? t?(S),n?(S)?
682
Gildea On the String Translations Produced by Multi Bottom?Up Tree Transducers
where each symbol ti,j is either some variable xi?,j? or a terminal from the alphabet of
the LCFRS, we construct the MBOT rule:
S
B1
x1,1 . . . x1,?(B1 )
? ? ? Br
xr,1 . . . xr,?(Br )
? S
S1
t1,1 ? ? ? t1,n1
? ? ? S?(S)
t?(S),1 ? ? ? t?(S),n?(S)
where the MBOT?s input alphabet contains a symbol S for each LCFRS nonterminal S,
and the MBOT?s output alphabet contains ?(S) symbols Si for each LCFRS nontermi-
nal S. This construction for converting an LCFRS to an MBOT shows that LCFRS ?
target(yield(MBOT)).
Given our earlier construction for generating the target projection of the LCFRS
derived from an MBOT, we know that target(yield(MBOT))? LCFRS. Combining these
two facts yields the theorem. 
4. Composition of STSGs
Maletti et al (2009) discuss the composition of extended top?down tree transducers,
which are equivalent to STSGs, as shown by Maletti (2010). They show that this for-
malism is not closed under composition in terms of the tree transformations that are
possible. In this article, we focus on the string yields of the formalisms under discussion,
and from this point of view we now examine the question of whether the yield of the
composition of two STSGs is itself the yield of an STSG in general. It is important to
note that, although we focus on the yield of the composition, in our notion of STSG
composition, the tree structure output by the first STSG still serves as input to the
second STSG.
Maletti et al (2009) give two tree transformations as counterexamples to the compo-
sitionality of STSG, shown in Figure 4. From the point of view of string yield, both of the
transformations are equivalent to an STSG rule that simply copies the three variables
with no re-ordering. Thus, these counterexamples are not sufficient to show that the
yield of the composition of two STSGs is not the yield of an STSG.
We now present two STSGs, shown in MBOT notation in Figures 5 and 6, whose
composition is not a translation produced by an STSG. The essence of this counterex-
ample, explained in more detail subsequently, is that rules from the two STSGs apply
in an overlapping manner to unboundedly long sequences, as in the example of Arnold
Figure 4
Examples of tree transformations not contained in STSG, from Maletti et al (2009). Here Gn
denotes a unary chain of G?s of arbitrary length.
683
Computational Linguistics Volume 38, Number 3
Figure 5
First MBOT in composition.
and Dauchet (1982, section 3.4). To this approach we add a re-ordering pattern which
results in a translation that we will show not to be possible with STSG.
The heart of each MBOT is the first rule, which reverses the order of adjacent
sequences of c?s and d?s. The MBOT of Figure 5 generates the translation:
{(a[cn2i?1dn2i ]i=1a, a[d
n2icn2i?1 ]i=1a) | ni ? 1, 
 ? 1}
where 
 is the number of times the first rule of the transducer is applied, and the notation
[xi]
n
i=1 indicates the string concatenation x1x2 ? ? ? xn. Here we have 2
 repeated sequences
of characters c and d, each occurring ni times, with each integer ni for 1 ? i ? 2
 varying
freely.
The secondMBOT reverses sequences of c?s and d?s in a pattern that is offset by one
from the pattern of the first MBOT. It produces the translation:
{(adn1 [cn2idn2i+1 ]?1i=1 c
n2a, adn1 [dn2i+1cn2i ]?1i=1 c
n2a) | ni ? 1, 
 ? 1}
When we compose the two MBOTs, the yield of the resulting transducer is the
translation:
Tcrisscross =
?
?
=1
Tcrisscross (1)
684
Gildea On the String Translations Produced by Multi Bottom?Up Tree Transducers
where
Tcrisscross = {(a[c
n2i?1dn2i ]i=1a, ad
n2 [dn2i+2cn2i?1 ]?1i=1 c
n2?1a) | ni ? 1} (2)
A visualization of the alignment pattern of this translation is shown in Figure 7. We
will show that Tcrisscross cannot be produced by any SCFG.
We define an SCFG to be a system (V,?,?,P,S) where V is a set of nonterminals,
? and ? are the terminal alphabets of the source and target language respectively,
S ? V is a distinguished start symbol, and P is a set of productions of the following
general form:
X0 ? X
1
1 ? ? ?X
n
n , X
?(1)
?(1) ? ? ?X
?(n)
?(n)
Figure 6
Second MBOT in composition.
Figure 7
Translation resulting from MBOT composition with 
 = 8.
685
Computational Linguistics Volume 38, Number 3
where ? is a permutation of length n, and the variables Xi for 0 ? i ? n range over
nonterminal symbols (for example, X1 and X2 may both stand for nonterminal A).
In SCFG productions, the l.h.s. nonterminal rewrites into a string of terminals and
nonterminals in both the source and target languages, and pairs of r.h.s. nonterminals
that are linked by the same superscript index must be further rewritten by the same
rule.
In terms of string translations, STSGs and SCFGs are equivalent, because any SCFG
is also an STSG with rules of depth 1, and any STSG can be converted to an SCFG with
the same string translation by simply removing the internal tree nodes in each rule. We
will adopt SCFG terminology for our proof because the internal structure of STSG rules
is not relevant to our result.
For a fixed value of 
, the translation Tcrisscross can be produced by an SCFG of
rank 2
, shown in Figure 8, because one rule can produce 2
 nonterminals arranged
in the permutation of Figure 7. (In the context of SCFGs, rank refers to the maxi-
mum number of nonterminals on the r.h.s. of a rule.) We will show that strings of
this form cannot be produced by any SCFG of rank less than 2
. Intuitively, factoring
the alignment pattern of Figure 7 into smaller SCFG rules would require identifying
subsequences in the two languages that are consistently aligned to one another, and, as
can be seen from the figure, no such subsequences exist. Because 
 can be unboundedly
large in our translation, the translation cannot be produced by any SCFG of fixed
rank.
We will assume, without loss of generality, that any SCFG is written in a normal
form such that each rule?s r.h.s. either contains only terminals in each language, or
contains only nonterminals. An SCFG can be transformed into this normal form by
applying the following procedure to each rule:
1. Associate each sequence of terminals with the preceding nonterminal, or
the following nonterminal in the case of initial terminals.
2. Replace each group consisting of a nonterminal and its associated
terminals with a fresh nonterminal A, and add a rule rewriting A as the
group in source and target. (Nonterminals with no associated terminals
may be left intact.)
3. In each rule created in the previous step, replace each sequence of
terminals with another fresh nonterminal B, and add a rule rewriting B as
the terminal sequence in source and target.
Figure 9 shows an example of this grammar transformation. Because we do not change
the rank of existing rules, and we add rules of rank no greater than 3, the transformation
does not increase the rank of any grammar having rank at least 3.
Figure 8
An SCFG producing translation Tcrisscross for fixed 
.
686
Gildea On the String Translations Produced by Multi Bottom?Up Tree Transducers
Figure 9
Conversion of grammar (a) to normal form (b) in which each rule has only nonterminals or only
terminals on the r.h.s.
Figure 10
An SCFG derivation produced by applying each rule in Figure 9b once, in the order given in
Figure 9b. Indices of linked nonterminals are renumbered after each step to be monotonically
increasing in the English side of the derivation. The preterminal permutation of the derivation,
(3,2,1), is the sequence of indices on the Chinese side in the last step before any terminals are
produced.
In an SCFG derivation, nonterminals in either language are linked as shown in Fig-
ure 10. We restrict derivations to apply rules producing terminals after applying all
other rules. We refer to nonterminals at the last step in which the sentential form
consists exclusively of nonterminals as preterminals, and we refer to a pair of linked
preterminals as an aligned preterminal pair. Assuming that aligned preterminal pairs
are indexed consecutively in the source side of the sentential form, we refer to the
sequence of indices in the target side as the preterminal permutation of a derivation.
For example, the preterminal permutation of the derivation in Figure 10 is (3,2,1). The
permutation of any sentential form of an SCFG of rank r can be produced by composing
permutations of length no greater than r, by induction over the length of the derivation.
Thus, while the permutation (3,2,1) of our example can be produced by composing
permutations of length 2, the preterminal permutation (2,4,1,3) can never be produced
by an SCFG of rank 2 (Wu 1997). In fact, this restriction also applies to subsequences of
the preterminal permutation.
Lemma 3
Let ? be a preterminal permutation produced by an SCFG derivation containing rules
of maximum rank r, and let ?? be a permutation obtained from ? by removing some
elements and renumbering the remaining elements with a strictly increasing function.
Then ?? falls within the class of compositions of permutations of length r.
Proof
From each rule in the derivation producing preterminal permutation ?, construct a new
rule by removing any nonterminals whose indices were removed from ?. The resulting
sequence of rules produces preterminal permutation ?? and contains rules of rank no
greater than r. 
As an example of Lemma 3, removing any element from the permutation (3,2,1) results
in the permutation (2,1), which can still (trivially) be produced by an SCFG of rank 2.
687
Computational Linguistics Volume 38, Number 3
We will make use of another general fact about SCFGs, which we derive by ap-
plying Ogden?s Lemma (Ogden 1968), a generalized pumping lemma for context-free
languages, to the source language of an SCFG.
Lemma 4 (Ogden?s Lemma)
For each context-free grammar G = (V,?,P,S) there is an integer k such that for any
word ? in L(G), if any k or more distinct positions in ? are designated as distinguished,
then there is some A in V and there are words ?, ?, ?, ?, and ? in ?? such that:
 S?? ?A? ?? ??A?? ?? ????? = ?, and hence ??m??m? ? L(G)
for all m ? 0.
 ? contains at least one of the distinguished positions.
 Either ? and ? both contain distinguished positions, or ? and ? both
contain distinguished positions.
 ??? contains at most k distinguished positions.
Ogden?s lemma can be extended as follows to apply to SCFGs.
Lemma 5
For each SCFG G = (V,?,?,P,S) having source alphabet? and target alhabet?, there
is an integer k such that for any string pair (?,??) in L(G), if any k or more distinct
positions in ? are designated as distinguished, then there is some A in V and there are
words ?, ?, ?, ?, and ? in ?? and ??, ??, ??, ??, and ?? in?? such that:
 (S
1
,S
1
)?? (?A 1 ?, ??A 1 ??)?? (??A 1 ??, ????A 1 ????)??
(?????, ??????????) = (?,??), and hence
(??m??m?, ??(??)m??(??)m??) ? L(G) for all m ? 0.
 ? contains at least one of the distinguished positions.
 Either ? and ? both contain distinguished positions, or ? and ? both
contain distinguished positions.
 ??? contains at most k distinguished positions.
Note that there are no guarantees on the form of ??, ??, ??, ??, and ??, and indeed these
may all be the empty string.
Proof
There must exist some sequence of rules in the source projection of Gwhich licenses the
derivation A?? ?A?. If we write the jth rule in this sequence as Aj ? ?j, there must
exist a synchronous rule in G of the form Aj ? ?j, ??j that rewrites the same nontermi-
nal. Thus G licenses a synchronous derivation (A
1
, A
1
)?? (?A 1 ?, ??A 1 ??) for some
?? and ??. Similarly, the source derivation S?? ?A? has a synchronous counterpart
(S
1
, S
1
)?? (?A 1 ?, ??A 1 ??) for some ?? and ??, and the source derivation A? ? has
a synchronous counterpart (A
1
, A
1
)? (?, ??) for some ??. Because the synchronous
688
Gildea On the String Translations Produced by Multi Bottom?Up Tree Transducers
derivation (A
1
, A
1
)?? (?A 1 ?, ??A 1 ??) can be repeated any number of times, the
string pairs
(??m??m?, ??(??)m??(??)m??) (3)
are generated by the SCFG for all m ? 0. The further conditions on ?, ?, ?, ?, and ?
follow directly from Ogden?s Lemma. 
We refer to a substring arising from a term cni or dni in the definition of Tcrisscross
(Equation (2)) as a run. In order to distinguish runs, we refer the run arising from cni or
dni as the ith run. We refer to the pair (cni , cni ) or (dni , dni ) consisting of the ith run in
the source and target strings as the ith aligned run. We now use Lemma 5 to show that
aligned runs must be generated from aligned preterminal pairs.
Lemma 6
Assume that some SCFG G? generates the translation Tcrisscross for some fixed 
. There
exists a constant k such that, in any derivation of grammarG? having each ni > k, for any
i, 1 ? i ? 2
, there exists at least one aligned preterminal pair among the subsequences
of source and target preterminals generating the ith aligned run.
Proof
We consider a source string ?, (?, ??) ? Tcrisscross, such that the length ni of each run
is greater than the constant k of Lemma 5. For a fixed i, 1 ? i ? 2
, we consider the
distinguished positions to be all and only the terminals in the ith run. This implies that
the run can be pumped to be arbitrarily long; indeed, this follows from the definition of
the language itself.
Because our distinguished positions are within the ith run, and because Lemma 5
guarantees that either ?, ?, and ? all contain distinguished positions or ?, ?, and ?
all contain distinguished positions, we are guaranteed that either ? or ? lies entirely
within the ith run. Consider the case where ? lies within the run. We must consider
three possibilities for the location of ? in the string:
Case 1. The string ? also lies entirely within the ith run.
Case 2. The string ? contains substrings of more than one run. This cannot occur, because
pumped strings of the form ??m??m? would contain more than 2
 runs, which is not
allowed under the definition of Tcrisscross.
Case 3. The string ? lies entirely within the jth run, where j = i. The strings ??m??m?
have the same form as ?????, with the exception that the ith and jth runs are extended
from lengths ni and nj to some greater lengths n
?
i and n
?
j . By the definition of T

crisscross,
for each source string, only one target string is permitted. For string pairs of the form of
Equation (3) to belong to Tcrisscross, ?
? and ?? must lie within the ith and jth aligned runs
in the target side. Because the permutation of Figure 7 cannot be decomposed, there
must exist some k such that the kth aligned run lies between the ith and jth aligned
runs in one side of the translation, and outside the ith and jth aligned runs in the other
side of the translation. If this were not the case, we would be able to decompose the
permutation by factoring out the subsequence between the ith and jth runs on both
sides of the translation. Consider the case where the kth aligned run lies between the ith
and jth aligned runs in the source side, and therefore is a substring of ? in the source,
689
Computational Linguistics Volume 38, Number 3
and a substring of either ?? or ?? in the target. We apply Lemma 5 a second time, with
all terminals of the kth run as the distinguished positions, to the derivation (A, A)??
(?, ??) by taking A as the start symbol of the grammar. This implies that there exist ??,
??, ??, ???, ???, and ??? such that
(?,??) = (??????????, ?????????????????)
and all strings
(??m??n????n?m?, ??(??)m(???)n???(???)n(??)m??)
are members of the translation Tcrisscross. Either ?? or ?? is a substring of source side of
the kth aligned run, so the kth aligned run can be pumped to be arbitrarily long in
the source without changing its length in the target. This contradicts the definition of
Tcrisscross. Similarly, the case where the kth aligned run lies between ?
? and ?? in the
target leads to a contradiction. Thus the assumption that j = imust be false.
Because Cases 2 and 3 are impossible, ? must lie entirely within the ith run. Simi-
larly, in the case where ? contains distinguished positions, ?must lie within the ith run.
Thus both ? and ? always lie entirely within the ith aligned run.
Because the ? and ? lie within the ith aligned run, the strings ??m??m? have the
same form as ?????, with the exception that the ith run is extended from length ni to
some greater length n?i . For the pairs of Equation (3) to be members of the translation,
?? and ?? must be substrings of the ith aligned run in the target. Because ?m??m and
(??)m??(??)m were derived from the same nonterminal, the two sequences of pretermi-
nals generating these two strings consist of aligned preterminal pairs. Because both
?m??m and (??)m??(??)m are substrings of the ith aligned run, we have at least one
aligned preterminal pair among the source and target preterminal sequences generating
the ith aligned run. 
Lemma 7
Assume that some SCFG G? generates the translation Tcrisscross for some fixed 
. There
exists a constant k such that, if (?,??) is a string pair generated by G? having each ni > k,
any derivation of (?,??) with grammar G? must contain a rule of rank at least 2
.
Proof
Because the choice of i in Lemma 6 was arbitrary, each aligned run must contain at least
one aligned preterminal pair. If we select one such preterminal pair from each run, the
associated permutation is that of Figure 7. This permutation cannot be decomposed, so,
by Lemma 3, it cannot be generated by an SCFG derivation containing only rules of
rank less than 2
. 
We will use one more general fact about SCFGs to prove our main result.
Lemma 8
Let G be an SCFG and let T = L(G) be the translation it produces. Let F be a finite state
machine, and let R = L(F) be the regular language it accepts. Let T? be the translation
derived by intersecting the source strings of T with R
T? = {(s, t) | (s, t) ? T, s ? R}
690
Gildea On the String Translations Produced by Multi Bottom?Up Tree Transducers
Then there exists an SCFG G? such that T? = L(G?).
Proof
Let V be the nonterminal set of G, and let S be the state set of F. Construct the SCFG G?
with nonterminal set V ? S? S by applying the construction of Bar-Hillel, Perles, and
Shamir (1961) for intersection of a CFG and finite state machine to the source side of
each rule in G. 
Now we are ready for our main result.
Theorem 5
SCFG = yield(STSG)  yield(STSG;STSG), where the semicolon denotes composition.
Proof
Assume that some SCFG G generates Tcrisscross. Note that T

crisscross is the result of inter-
secting the source of Tcrisscross with the regular language a[c
+d+]a. By Lemma 8, we can
construct an SCFG G generating Tcrisscross. By Lemma 7, for each 
, G
 has rank at least
2
. The intersection construction does not increase the rank of the grammar, so G has
rank at least 2
. Because 
 is unbounded in the definition of Tcrisscross, and because any
SCFG has a finite maximum rank, Tcrisscross cannot be produced by any SCFG. 
4.1 Implications for Machine Translation
The ability of MBOTs to represent the composition of STSGs is given as a motivation for
the MBOT formalism byMaletti (2010), but this raises the issue of whether synchronous
parsing and machine translation decoding can be undertaken efficiently for MBOTs
resulting from the composition of STSGs.
In discussing the complexity of synchronous parsing problems, we distinguish
the case where the grammar is considered part of the input, and the case where the
grammar is fixed, and only the source and target strings are considered part of the input.
For SCFGs, synchronous parsing is NP-complete when the grammar is considered part
of the input and can have arbitrary rank. For any fixed grammar, however, synchronous
parsing is possible in time polynomial in the lengths of the source and target strings,
with the degree of the polynomial depending on the rank of the fixed SCFG (Satta and
Peserico 2005). Because MBOTs subsume SCFGs, the problem of recognizing whether a
string pair belongs to the translation produced by an arbitrary MBOT, when the MBOT
is considered part of the input, is also NP-complete.
Given our construction for converting an MBOT to an LCFRS, we can use standard
LCFRS tabular parsing techniques to determine whether a given string pair belongs
to the translation defined by the yield of a fixed MBOT. As with arbitrary-rank SCFG,
LCFRS parsing is polynomial in the length of the input string pair, but the degree of the
polynomial depends on the complexity of the MBOT. To be precise, the degree of the
polynomial for LCFRS parsing is
?r
i=0?(Si) (Seki et al 1991), which yields
?r
i=0(1+
d(Si)) when applied to MBOTs.
If we restrict ourselves to MBOTs that are derived from the composition of STSGs,
synchronous parsing is NP-complete if the STSGs to compose are part of the input,
because a single STSG suffices. For a composition of fixed STSGs, we obtain a fixed
MBOT, and polynomial time parsing is possible. Theorem 5 indicates that we cannot
apply SCFG parsing techniques off the shelf, but rather that we must implement some
type of more general parsing system. Either of the STSGs used in our proof of Theorem 5
691
Computational Linguistics Volume 38, Number 3
can be binarized and synchronously parsed in time O(n6), but tabular parsing for the
LCFRS resulting from composition has higher complexity. Thus, composing STSGs
generally increases the complexity of synchronous parsing.
The problem of language-model?integrated decoding with synchronous grammars
is closely related to that of synchronous parsing; both problems can be seen as inter-
secting the grammar with a fixed source-language string and a finite-state machine
constraining the target-language string. The widely used decoding algorithms for SCFG
(Yamada and Knight 2002; Zollmann and Venugopal 2006; Huang et al 2009) search
for the highest-scoring translation when combining scores from a weighted SCFG and
a weighted finite-state language model. As with SCFG, language-model?integrated
decoding for weighted MBOTs can be performed by adding n-gram language model
state to each candidate target language span. This, as with synchronous parsing, gives
an algorithm which is polynomial in the length of the input sentence for a fixed MBOT,
but with an exponent that depends on the complexity of the MBOT. Furthermore,
Theorem 5 indicates that SCFG-based decoding techniques cannot be applied off the
shelf to compositions of STSGs, and that composition of STSGs in general increases
decoding complexity.
Finally, we note that finding the highest-scoring translation without incorporating
a language model is equivalent to parsing with the source or target projection of the
MBOT used to model translation. For the source language of the MBOT, this implies
time O(n3) because the problem reduces to CFG parsing. For the target language of
the MBOT, this implies polynomial-time parsing, where the degree of the polynomial
depends on the MBOT, as a result of Theorem 4.
5. Conclusion
MBOTs are desirable for natural language processing applications because they are
closed under composition and can be used to represent sequences of transforma-
tions of the type performed by STSGs. However, the string translations produced
by MBOTs representing compositions of STSGs are strictly more powerful than the
string translations produced by STSGs, which are equivalent to the translations pro-
duced by SCFGs. From the point of view of machine translation, because parsing
with general LCFRS is NP-complete, restrictions on the power of MBOTs will be
necessary in order to achieve polynomial?time algorithms for synchronous parsing
and language-model?integrated decoding. Our result on the string translations pro-
duced by compositions of STSGs implies that algorithms for SCFG-based synchronous
parsing or language-model-integrated decoding cannot be applied directly to these
problems, and that composing STSGs generally increases the complexity of these prob-
lems. Developing parsing algorithms specific to compositions of STSGs, as well as
possible restrictions on the STSGs to be composed, presents an interesting area for
future work.
Acknowledgments
We are grateful for extensive feedback
on earlier versions of this work from
Giorgio Satta, Andreas Maletti, Adam
Purtee, and three anonymous reviewers.
This work was partially funded by NSF
grant IIS-0910611.
References
Aho, Albert V. and Jeffery D. Ullman. 1972.
The Theory of Parsing, Translation, and
Compiling, volume 1. Prentice-Hall,
Englewood Cliffs, NJ.
Arnold, Andre? and Max Dauchet. 1982.
Morphismes et bimorphismes
692
Gildea On the String Translations Produced by Multi Bottom?Up Tree Transducers
d?arbres. Theoretical Computer Science,
20:33?93.
Bar-Hillel, Y., M. Perles, and E. Shamir.
1961. On formal properties of simple
phrase structure grammars. Zeitschrift
fu?r Phonetik, Sprachwissenschaft und
Kommunikationsforschung, 14:143?172.
Reprinted in Y. Bar-Hillel. 1964. Language
and Information: Selected Essays on their
Theory and Application, Addison-Wesley,
Boston, MA, pages 116?150.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
Engelfriet, J., E. Lilin, and A. Maletti.
2009. Extended multi bottom?up
tree transducers. Acta Informatica,
46(8):561?590.
Galley, Michel, Jonathan Graehl, Kevin
Knight, Daniel Marcu, Steve DeNeefe,
Wei Wang, and Ignacio Thayer. 2006.
Scalable inference and training of
context-rich syntactic translation models.
In Proceedings of the International Conference
on Computational Linguistics/Association for
Computational Linguistics (COLING/
ACL-06), pages 961?968, Sydney.
Huang, Liang, Hao Zhang, Daniel Gildea,
and Kevin Knight. 2009. Binarization of
synchronous context-free grammars.
Computational Linguistics, 35(4):559?595.
Joshi, A. K., L. S. Levy, and M. Takahashi.
1975. Tree adjunct grammars. Journal of
Computer and System Sciences, 10:136?163.
Joshi, A. K. and Y. Schabes. 1997.
Tree-adjoining grammars. In G. Rozenberg
and A. Salomaa, editors, Handbook of
Formal Languages, volume 3. Springer,
Berlin, pages 69?124.
Lilin, Eric. 1981. Proprie?te?s de clo?ture d?une
extension de transducteurs d?arbres
de?terministes. In CAAP, volume 112 of
LNCS. Springer, Berlin, pages 280?289.
Maletti, Andreas. 2010. Why synchronous
tree substitution grammars? In Proceedings
of the 2010 Meeting of the North American
Chapter of the Association for Computational
Linguistics (NAACL-10), pages 876?884,
Los Angeles, CA.
Maletti, Andreas, Jonathan Graehl, Mark
Hopkins, and Kevin Knight. 2009. The
power of extended top?down tree
transducers. SIAM Journal on Computing,
39:410?430.
Melamed, I. Dan. 2003. Multitext grammars
and synchronous parsers. In Proceedings
of the 2003 Meeting of the North American
Chapter of the Association for Computational
Linguistics (NAACL-03), pages 158?165,
Edmonton.
Melamed, I. Dan, Giorgio Satta, and
Ben Wellington. 2004. Generalized
multitext grammars. In Proceedings of the
42nd Annual Conference of the Association
for Computational Linguistics (ACL-04),
pages 661?668, Barcelona.
Ogden, William F. 1968. A helpful result for
proving inherent ambiguity.Mathematical
Systems Theory, 2(3):191?194.
Rambow, Owen and Giorgio Satta. 1999.
Independent parallelism in finite copying
parallel rewriting systems. Theoretical
Computer Science, 223(1-2):87?120.
Satta, Giorgio. 1992. Recognition of Linear
Context-Free Rewriting Systems. In
Proceedings of the 30th Annual Conference of
the Association for Computational Linguistics
(ACL-92), pages 89?95, Newark, DE.
Satta, Giorgio and Enoch Peserico. 2005.
Some computational complexity results for
synchronous context-free grammars. In
Proceedings of Human Language Technology
Conference and Conference on Empirical
Methods in Natural Language Processing
(HLT/EMNLP), pages 803?810, Vancouver.
Schabes, Yves and Stuart M. Shieber. 1994.
An alternative conception of tree-adjoining
derivation. Computational Linguistics,
20:91?124.
Seki, H., T. Matsumura, M. Fujii, and
T. Kasami. 1991. On multiple context-free
grammars. Theoretical Computer Science,
88:191?229.
Shieber, Stuart and Yves Schabes. 1990.
Synchronous tree-adjoining grammars.
In Proceedings of the 13th International
Conference on Computational Linguistics
(COLING-90), volume III, pages 253?258,
Helsinki.
Vijay-Shankar, K., D. L. Weir, and A. K.
Joshi. 1987. Characterizing structural
descriptions produced by various
grammatical formalisms. In Proceedings of
the 25th Annual Conference of the Association
for Computational Linguistics (ACL-87),
pages 104?111, Stanford, CA.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
Yamada, Kenji and Kevin Knight. 2002. A
decoder for syntax-based statistical MT. In
Proceedings of the 40th Annual Conference of
the Association for Computational Linguistics
(ACL-02), pages 303?310, Philadelphia, PA.
Zollmann, Andreas and Ashish Venugopal.
2006. Syntax augmented machine
translation via chart parsing. In Proceedings
of the Workshop on Statistical Machine
Translation, pages 138?141, New York, NY.
693

Sampling Tree Fragments from Forests
Tagyoung Chung?
University of Rochester
Licheng Fang??
University of Rochester
Daniel Gildea?
University of Rochester
Daniel S?tefankovic??
University of Rochester
We study the problem of sampling trees from forests, in the setting where probabilities for each
tree may be a function of arbitrarily large tree fragments. This setting extends recent work
for sampling to learn Tree Substitution Grammars to the case where the tree structure (TSG
derived tree) is not fixed. We develop a Markov chain Monte Carlo algorithm which corrects for
the bias introduced by unbalanced forests, and we present experiments using the algorithm to
learn Synchronous Context-Free Grammar rules for machine translation. In this application, the
forests being sampled represent the set of Hiero-style rules that are consistent with fixed input
word-level alignments. We demonstrate equivalent machine translation performance to standard
techniques but with much smaller grammars.
1. Introduction
Recent work on learning Tree Substitution Grammars (TSGs) has developed procedures
for sampling TSG rules from known derived trees (Cohn, Goldwater, and Blunsom
2009; Post and Gildea 2009). Here one samples binary variables at each node in the
tree, indicating whether the node is internal to a TSG rule or is a split point between
two rules. We consider the problem of learning TSGs in cases where the tree structure
is not known, but rather where possible tree structures are represented in a forest. For
example, we may wish to learn from text where treebank annotation is unavailable,
? Computer Science Dept., University of Rochester, Rochester NY 14627.
E-mail: chung@cs.rochester.edu.
?? Computer Science Dept., University of Rochester, Rochester NY 14627.
E-mail: lfang@cs.rochester.edu.
? Computer Science Dept., University of Rochester, Rochester NY 14627.
E-mail: gildea@cs.rochester.edu.
? Computer Science Dept., University of Rochester, Rochester NY 14627.
E-mail: stefanko@cs.rochester.edu.
Submission received: 26 October 2012; revised version received: 14 March 2013; accepted for publication:
4 May 2013.
doi:10.1162/COLI a 00170
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 1
but a forest of likely parses can be produced automatically. Another application on
which we focus our attention in this article arises in machine translation, where we
want to learn translation rules from a forest representing the phrase decompositions that
are consistent with an automatically derived word alignment. Both these applications
involve sampling TSG trees from forests, rather than from fixed derived trees.
Chappelier and Rajman (2000) present a widely used algorithm for sampling trees
from forests: One first computes an inside probability for each node bottom?up, and
then chooses an incoming hyperedge for each node top?down, sampling according to
each hyperedge?s inside probability. Johnson, Griffiths, and Goldwater (2007) use this
sampling algorithm in a Markov chain Monte Carlo framework for grammar learning.
We can combine the representations used in this algorithm and in the TSG learning
algorithm discussed earlier, maintaining two variables at each node of the forest, one
for the identity of the incoming hyperedge, and another representing whether the node
is internal to a TSG rule or is a split point. However, computing an inside probability
for each node, as in the first phase of the algorithm of Johnson, Griffiths, and Goldwater
(2007), becomes difficult because of the exponential number of TSG rules that can apply
at any node in the forest. Not only is the number of possible TSG rules that can apply
given a fixed tree structure exponentially large in the size of the tree, but the number of
possible tree structures under a node is also exponentially large. This problem is par-
ticularly acute during grammar learning, as opposed to sampling according to a fixed
grammar, because any tree fragment is a valid potential rule. Cohn and Blunsom (2010)
address the large number of valid unseen rules by decomposing the prior over TSG
rules into an equivalent probabilistic context-free grammar; however, this technique
only applies to certain priors. In general, algorithms that match all possible rules are
likely to be prohibitively slow, as well as unwieldy to implement. In this article, we
design a sampling algorithm that avoids explicitly computing inside probabilities for
each node in the forest.
In Section 2, we derive a general algorithm for sampling tree fragments from forests.
We avoid computing inside probabilities, as in the TSG sampling algorithms of Cohn,
Goldwater, and Blunsom (2009) and Post and Gildea (2009), but we must correct for
the bias introduced by the forest structure, a complication that does not arise when the
tree structure is fixed. In order to simplify the presentation of the algorithm, we first set
aside the complication of large, TSG-style rules, and describe an algorithm for sampling
trees from forests while avoiding computation of inside probabilities. This algorithm is
then generalized to learn the composed rules of TSG in Section 2.3.
As an application of our technique, we present machine translation experiments in
the remainder of the article. We learn Hiero-style Synchronous Context-Free Grammar
(SCFG) rules (Chiang 2007) from bilingual sentences for which a forest of possible
minimal SCFG rules has been constructed fromfixedword alignments. The construction
of this forest and its properties are described in Section 3. We make the assumption
that the alignments produced by a word-level model are correct in order to simplify
the computation necessary for rule learning. This approach seems safe given that the
pipeline of alignment followed by rule extraction has generally remained the state of
the art despite attempts to learn joint models of alignment and rule decomposition
(DeNero, Bouchard-Cote, and Klein 2008; Blunsom et al. 2009; Blunsom and Cohn
2010a). We apply our sampling algorithm to learn the granularity of rule decomposition
in a Bayesian framework, comparing sampling algorithms in Section 4. The end-to-end
machine translation experiments of Section 5 show that our algorithm is able to achieve
performance equivalent to the standard technique of extracting all rules, but results in
a significantly smaller grammar.
204
Chung et al. Sampling Tree Fragments from Forests
2. Sampling Trees from Forests
As a motivating example, consider the small example forest of Figure 1. This forest
contains a total of five trees, one under the hyperedge labeled A, and four under the
hyperedge labeled B (the cross-product of the two options for deriving node 4 and the
two options for deriving node 5).
Let us suppose that we wish to sample trees from this forest according to a distribu-
tion Pt, and further suppose that this distribution is proportional to the product of the
weights of each tree?s hyperedges:
Pt(t) ?
?
h?t
w(h) (1)
To simplify the example, suppose that in Figure 1 each hyperedge has weight 1,
?h w(h) = 1
giving us a uniform distribution over trees:
?t Pt(t) = 15
A tree can be specified by attaching a variable zn to each node n in the forest
indicating which incoming hyperedge is to be used in the current tree. For example,
variable z1 can take values A and B in Figure 1, whereas variables z2 and z3 can only
take a single value. We use z to refer to the entire set of variables zn in a forest. Each
assignment to z specifies a unique tree, ?(z), which can be found by following the
incoming hyperedges specified by z from the goal node of each forest down to the
terminals.
A naive sampling strategy would be to resample each of these variables zn in
order, holding all others constant, as in standard Gibbs sampling. If we choose an
incoming hyperedge according to the probability Pt(?(z)) of the resulting tree, holding
all other variable assignments fixed, we see that, because Pt is uniform, we will choose
2 3 5
1
4
A
B
Figure 1
Example forest.
205
Computational Linguistics Volume 40, Number 1
with uniform probability of 1/m among the m incoming hyperedges at each node. In
particular, we will choose among the two incoming hyperedges at the root (node 1)
with equal probability, meaning that, over the long run, the sampler will spend half its
time in the state for the single tree corresponding to nodes 2 and 3, and only one eighth
of its time in each of the four other possible trees. Our naive algorithm has failed at its
goal of sampling among the five possible trees each with probability 1/5.
Thus, we cannot adopt the simple Gibbs sampling strategy, used for TSG induction
from fixed trees, of resampling one variable at a time according to the target distribution,
conditioned on all other variables. The intuitive reason for this, as illustrated by the
example, is the bias introduced by forests that are bushier (that is, havemore derivations
for each node) in some parts than in others. The algorithm derived in the remainder of
this section corrects for this bias, while avoiding the computation of inside probabilities
in the forest.
2.1 Choosing a Stationary Distribution
We will design our sampling algorithm by first choosing a distribution Pz over the set
of variables z defined earlier. We will show correctness of our algorithm by showing
that it is a Markov chain converging to Pz, and that Pz results in the desired distribution
Pt over trees.
The tree specified by an assignment to z will be denoted ?(z) (see Table 1). For a tree
t the vector containing the variables found at nodes in t will be denoted z[t]. This is a
subvector of z: for example, in Figure 1, if t chooses A at node 1 then z[t] = (z1, z2, z3),
and if t chooses B at node 1 then z[t] = (z1, z4, z5). We use z[?t] to denote the variables
not used in the tree t. The vectors z[t] and z[?t] differ according to t, but for any tree
t, the two vectors form a partition of z. There are many values of z that correspond to
the same tree, but each tree t corresponds to a unique subvector of variables z[t] and a
unique assignment to those specific variables?we will denote this unique assignment
of variables in z[t] by ?(t). (In terms of ? and ? one has for any z and t that ?(z) = t if
and only if z[t] = ?(t).)
Let Z be the random vector generated by our algorithm. As long as Pz(?(Z) = t) =
Pt(t) for all trees t, our algorithm will generate trees from the desired distribution.
Thus for any tree t the probability Pz(Z[t] = ?(t)) is fixed to Pt(t), but any distribu-
tion over the remaining variables not contained in t will still yield the desired dis-
tribution over trees. Thus, in designing the sampling algorithm, we may choose any
distribution Pz(Z[?t] | Z[t] = ?(t)). A simple and convenient choice is to make
Pz(Z[?t] | Z[t] = ?(t)) uniform. That is, each incoming hyperedge variable with m
alternatives assigns each hyperedge probability 1/m. (In fact, our algorithm can easily
Table 1
Notation
Pt desired distribution on trees
z vector of variables
Z random vector over z
?(z) tree corresponding to setting of z
Z[t] subset of random variables that occur in tree t
?(t) setting of variables in Z[t]
Z[?t] subset of random variables that do not occur in t
206
Chung et al. Sampling Tree Fragments from Forests
be adapted to other product distributions of Pz(Z[?t] | Z[t] = ?(t)).) This choice of
P(Z[?t] | Z[t] = ?(t)) determines a unique distribution for Pz:
Pz(Z = z) = Pt(?(z))Pz(Z[??(z)] = z[??(z)] | Z[?(z)] = z[?(z)])
= Pt(?(z))
?
v?z[??(z)]
1
deg(v)
(2)
where deg(v) is the number of possible values for v.
We proceed by designing a Gibbs sampler for this Pz. The sampler resamples vari-
ables from z one at a time, according to the joint probability Pz(z) for each alternative.
The set of possible values for zn at a node n having m incoming hyperedges consists of
the hyperedges ej, 1 ? j ? m. Let sj be the vector z with the value of zn changed to ej.
Note that the ?(sj)?s only differ at nodes below n.
Let z[in(n)] be the vector consisting of the variables at nodes below n (that is,
contained in subtrees rooted at n, or ?inside? n) in the forest, and let z[in(n)] be the
vector consisting of variables not under node n. Thus the vectors z[in(n)] and z[in(n)]
partition the complete vector z. We will use the notation z[t ? in(n)] to represent the
vector of variables from z that are both in a tree t and under a node n.
For Gibbs sampling, we need to compute the relative probabilities of sj?s. We now
consider the two terms of Equation (2) in this setting. Because of our requirement that Pz
correspond to the desired distribution Pt, the first term of Equation (2) can be computed,
up to a normalization constant, by evaluating our model over trees (Equation (1)).
The second term of Equation (2) is a product of uniform distributions that can be
decomposed into nodes below n and all other nodes:
Pz(Z[?t] = z[?t] | Z[t] = z[t]) =
?
v?z[?t? in(n)]
1
deg(v)
?
v?z[?t? in(n)]
1
deg(v)
(3)
where t = ?(z). Recall that ?(sj)?s only differ at vertices below n and hence
Pz(Z[?t] = z[?t] | Z[t] = z[t]) ?
?
v?z[?t? in(n)]
1
deg(v)
(4)
where we emphasize that ? refers to the relative probabilities of the sj?s, which corre-
spond to the options from which the Gibbs sampler chooses at a given step. We can
manipulate Equation (4) into a more computationally convenient form by multiplying
by the deg(v) term for each node v inside n. Because the terms for all nodes not included
in the current tree cancel each other out, we are left with:
Pz(Z[?t] = z[?t] | Z[t] = z[t]) ?
?
v?z[t? in(n)]
deg(v) (5)
Note that we only need to consider the nodes z[t] in the current tree, without needing
to examine the remainder of the forest at all.
207
Computational Linguistics Volume 40, Number 1
Substituting Equation (5) into Equation (2) gives a simple update rule for use in our
Gibbs sampler:
Pz(Z(i+1) = sj | Z(i) = z,n is updated) ? Pt(?(sj))
?
v?z[?(sj )?in(n)]
deg(v) (6)
To make a step of the Markov chain, we compute the right-hand side of Equation (6)
for every sj and then choose the next state of the chain Z(i+1) from the corresponding
distribution on the sj?s. The second term, which we refer to as the density factor, is
equal to the total number of trees in the forest under node n. This factor compensates
for the bias introduced by forests that are bushier in some places than in others, as in
the example of Figure 1. A related factor, defined on graphs rather than hypergraphs,
can be traced back as far as Knuth (1975), who wished to estimate the sum of values at
all nodes in a large tree by sampling a small number of the possible paths from the root
to the leaves. Knuth sampled paths uniformly and independently, rather than using a
continuously evolving Markov chain as in our Gibbs sampler.
2.2 Sampling Schedule
In a standard Gibbs sampler, updates are made iteratively to each variable z1, . . . , zN,
and this general strategy can be applied in our case. However, it may be wasteful to
continually update variables that are not used by the current tree and are unlikely to be
used by any tree. We propose an alternative sampling schedule consisting of sweeping
from the root of the current tree down to its leaves, resampling variables at each node
in the current tree as we go. If an update changes the structure of the current tree, the
sweep continues along the new tree structure. This strategy is shown in Algorithm 1,
where v(z, i) denotes the ith variable in a top?down ordering of the variables of the
current tree ?(z). The top?down ordering may be depth-first or breadth-first, among
other possibilities, as long as the variables at each node have lower indices than the
variables at the node?s descendants in the current tree.
To show that this sampling schedule will converge to the desired distribution over
trees, we will first show that Pz is a stationary distribution for the transition defined by
a single step of the sweep:
Lemma 1
For any setting of variables z, any top?down ordering v(z, i), and any i, updating
variable zv(z,i) according to Equation (6) is stationary with respect to the distribution
Pz defined by Equation (2).
Algorithm 1 Sampling algorithm
Require: A function v(z, i) returning the index of the ith variable of z in a top-down ordering of
the variables of the tree ?(z).
1: i ? 1
2: while i ? |Z[?(z)]| do  Until last node of current tree.
3: Resample zv(z,i) according to Equation (6)
4: i ? i + 1
5: end while
208
Chung et al. Sampling Tree Fragments from Forests
Proof
We will show that each step of the sweep is stationary for Pz by showing that it satisfies
detailed balance. Detailed balance is the condition that, on average, for each pair of
states z and z?, the number of transitions between the two states is the same in either
direction:
Pz(Z = z)P(Z(i+1) = z? | Z(i) = z) = Pz(Z = z?)P(Z(i+1) = z | Z(i) = z?) (7)
where P(Z(i+1) = z? | Z(i) = z) is the transition performed by one step of the sweep:
P(Z(i+1) = z? | Z(i) = z) =
{
Pz(Z=z? )
?
z?? Pz(Z=z
?? )I(z???v(z,i)=z?v(z,i) )
if z??v(z,i) = z?v(z,i)
0 otherwise
(8)
It is important to observe that, because the resampling step only changes the tree
structure below the ith node, the ith node in the new tree remains the same node. That
is, after making an update from z to z?, v(z, i) = v(z?, i), and, mathematically:
z??v(z,i) = z?v(z,i) ? v(z, i) = v(z?, i) ? z??v(z?,i) = z?v(z,i)
? z??v(z?,i) = z?v(z?,i)
Thus, the condition in Equation (8) is symmetric in z and z?, and we define the predicate
match(z, z?, i) to be equivalent to this condition. Substituting Equation (8) into the left-
hand side of Equation (7), we have:
Pz(Z = z)P(Z(i+1) = z? | Z(i) = z) =
{
Pz(Z=z)Pz(Z=z? )
?
z?? Pz(Z=z
?? )I(z???v(z,i)=z?v(z,i) )
if match(z, z?, i)
0 otherwise
(9)
By symmetry of the righthand side of Equation (9) in z and z?, we see that Equa-
tion (7) is satisfied. Because detailed balance implies stationarity, Pz is a stationary
distribution of P(Z(i+1) = z? | Z(i) = z). 
This lemma allows us to prove the correctness of our main algorithm:
Theorem 1
For any top?down sampling schedule v(z, i), and any desired distribution over trees
Pt that assigns non-zero probability to all trees in the forest, Algorithm 1 will converge
to Pt.
Proof
Because Pz is stationary for each step of the sweep, it is stationary for one entire sweep
from top to bottom.
To show that the Markov chain defined by an entire sweep is ergodic, we must show
that it is aperiodic and irreducible. It is aperiodic because the chain can stay in the same
configuration with non-zero probability by selecting the same setting for each variable
in the sweep. The chain is irreducible because any configuration can be reached in a
finite number of steps by sorting the variables in topological order bottom?up in the
forest, and then, for each variable, executing one sweep that selects a tree that includes
the desired variable with the desired setting.
209
Computational Linguistics Volume 40, Number 1
Because Pz is stationary for the chain defined by entire sweeps, and this chain is
ergodic, the chain will converge to Pz. Because Equation (2) guarantees that Pz(?(Z) =
t) = Pt(t), convergence to Pz implies convergence to Pt. 
2.3 Sampling Composed Rules
Our approach to sampling was motivated by a desire to learn TSG-style grammars,
where one grammar rule is the composition of a number of hyperedges in the forest.
We extend our sampling algorithm to handle this problem by using the same methods
that are used to learn a TSG from a single, fixed tree (Cohn, Goldwater, and Blunsom
2009; Post and Gildea 2009). We attach a binary variable to each node in the forest
indicating whether the node is a boundary between two TSG rules, or is internal to a
single TSG rule. Thus, the complete set of variables used by the sampler, z, now consists
of two variables at each node in the forest: one indicating the incoming hyperedge, and
one binary boundary variable. The proof of Section 2.1 carries through, with each new
binary variable v having deg(v) = 2 in Equation (2). As before, the current setting of z
partitions z into two sets of variables, those used in the current tree, z[?(z)], and those
outside the current tree, z[??(z)]. Given a fixed assignment to z, we can read off both
the current tree and its segmentation into TSG rules. We modify the tree probability of
Equation (1) to be a product over TSG rules r:
Pt(t) ?
?
r?t
w(r) (10)
in order to emphasize that grammar rules are no longer strictly equivalent to hyper-
edges in the forest. We modify the sampling algorithm of Algorithm 1 to make use of
this definition of Pt and to resample both variables at the current node. The incoming
hyperedge variable is resampled according to Equation (6), while the segmentation
variable is simply resampled according to Pt, as the update does not change the sets
z[?(z)] and z[??(z)].
The proof that the sampling algorithm converges to the correct distribution still
applies in the TSG setting, as it makes use of the partition of z into z[?(z)] and
z[??(z)], but does not depend on the functional form of the desired distribution over
trees Pt.
3. Phrase Decomposition Forest
In the remainder of this article, we will apply the algorithm developed in the previous
section to the problem of learning rules for machine translation in the context of a
Hiero-style, SCFG-based system. As in Hiero, our grammars will make use of a single
nonterminal X, and will contain rules with a mixture of nonterminals and terminals on
the right-hand side, with at most two nonterminal occurrences in the right-hand side
of a rule. In general, many overlapping rules of varying sizes are consistent with the
input word alignments, meaning that we must address a type of segmentation problem
in order to learn rules of the right granularity. Given the restriction to two right-hand
side nonterminals, the maximum number of rules that can be extracted from an input
sentence pair is O(n12) in the sentence length, because the left and right boundaries of
the left-hand side (l.h.s.) nonterminal and each of the two right-hand side nonterminals
can take O(n) positions in each of the two languages. This complexity leads us to
explore sampling algorithms, as dynamic programming approaches are likely to be
210
Chung et al. Sampling Tree Fragments from Forests
prohibitively slow. In this section, we show that the problem of learning rules can be
analyzed as a problem of identifying tree fragments of unknown size and shape in a
forest derived from the input word alignments for each sentence. These tree fragments
are similar to the tree fragments used in TSG learning. As in TSG learning, each rule
of the final grammar consists of some number of adjacent, minimal tree fragments:
one-level treebank expansions in the case of TSG learning and minimal SCFG rules,
defined subsequently, in the case of translation. The internal structure of TSG rules is
used during parsing to determine the final tree structure to output, and the internal
structure of machine translation rules will not be used at decoding time. This distinction
is irrelevant during learning. A more significant difference from TSG learning is that the
sets of minimal tree fragments in our SCFG application come not from a single, known
tree, but rather from a forest representing the set of bracketings consistent with the input
word alignments.
We now proceed to precisely define this phrase decomposition forest and discuss
some of its theoretical properties. The phrase decomposition forest is designed to extend
the phrase decomposition tree defined by Zhang, Gildea, and Chiang (2008) in order to
explicitly represent each possible minimal rule with a hyperedge.
A span [i, j] is a set of contiguous word indices {i, i + 1, . . . , j ? 1}. Given an aligned
Chinese?English sentence pair, a phrase n is a pair of spans n = ([i1, j1], [i2, j2]) such that
Chinese words in positions [i1, j1] are aligned only to English words in positions [i2, j2],
and vice versa. A phrase forest H = ?V,E? is a hypergraph made of a set of hypernodes
V and a set of hyperedges E. Each node n = ([i1, j1], [i2, j2]) ? V is a tight phrase as
defined by Koehn, Och, and Marcu (2003), namely, a phrase containing no unaligned
words at its boundaries. A phrase n = ([i1, j1], [i2, j2]) covers n? = ([i?1, j
?
1], [i
?
2, j
?
2]) if
i1 ? i?1 ? j?1 ? j1 ? i2 ? i?2 ? j?2 ? j2
Note that every phrase covers itself. It follows from the definition of phrases that if
i1 ? i?1 ? j?1 ? j1, then i2 ? i?2 ? j?2 ? j2. That means we can determine phrase coverage by
only looking at one language side of the phrases. We are going to use this property
to simplify the discussion of our proofs. We also define coverage between two sets of
phrases. Given two sets of phrases T and T?, we say T? covers T if for all t ? T, there
exists a t? ? T? such that t? covers t. We say that two phrases overlap if they intersect,
but neither covers the other.
If two phrases n = ([i1, j1], [i2, j2]) and n? = ([i?1, j
?
1], [i
?
2, j
?
2]) intersect, we can take the
union of the two phrases by taking the union of the source and target language spans,
respectively. That is, n1 ? n2 = ([i1, j1] ? [i?1, j?1], [i2, j2] ? [i?2, j?2]). An important property
of phrases is that if two phrases intersect, their union is also a phrase. For example,
given that have a date with her and with her today are both valid phrases in Figure 2, have
a date with her today must also be a valid phrase. Given a set T of phrases, we define
the union closure of the phrase set T, denoted
??(T), to be constructed by repeatedly
joining intersecting phrases until there are no intersecting phrases left.
Each edge in E, written as T ? n, is made of a set of non-intersecting tail nodes T ?
V, and a single head node n ? V that covers each tail node. Each edge is an SCFG rule
consistent with the word alignments. Each tail node corresponds to a right-hand-side
nonterminal in the SCFG rule, and any position included in n but not included in any tail
node corresponds to a right-hand-side terminal in the SCFG rule. For example, given the
aligned sentence pair of Figure 2, the edge {([3, 4], [5, 6]), ([5, 6], [3, 4])} ? ([2, 6], [1, 6]),
corresponds to a SCFG rule X ?? X1 ? X2, have a X2 with X1.
211
Computational Linguistics Volume 40, Number 1
?
I
??
have
?
a
?
date
?
with
??
her
today
Figure 2
Example word alignment, with boxes showing valid phrase pairs. In this example, all individual
alignment points are also valid phrase pairs.
For the rest of this section, we assume that there are no unaligned words. Unaligned
words can be temporarily removed from the alignment matrix before building the
phrase decomposition forest. After extracting the forest, they are put back into the
alignment matrix. For each derivation in the phrase decomposition forest, an unaligned
word appears in the SCFG rule whose left-hand side corresponds to the lowest forest
node that covers the unaligned word.
Definition 1
An edge T ? n is minimal if there does not exist another edge T? ? n such that T?
covers T.
A minimal edge is an SCFG rule that cannot be decomposed by factoring out some
part of its right-hand side as a separate rule. We define a phrase decomposition forest
to be made of all phrases from a sentence pair, connected by all minimal SCFG rules.
A phrase decomposition forest compactly represents all possible SCFG rules that are
consistent with word alignments. For the example word alignment shown in Figure 2,
the phrase decomposition forest is shown in Figure 3. Each boxed phrase in Figure 2
corresponds to a node in the forest of Figure 3, and hyperedges in Figure 3 represent
ways of building phrases out of shorter phrases.
A phrase decomposition forest has the important property that any SCFG rule
consistent with the word alignment corresponds to a contiguous fragment of some
complete tree found in the forest. For example, the highlighted tree fragment of the
forest in Figure 3 corresponds to the SCFG rule:
X ? ? X2 ? X1, have a X1 with X2
Thus any valid SCFG rule can be formed by selecting a set of adjacent hyperedges from
the forest and composing the minimal SCFG rules specified by each hyperedge. We
will apply the sampling algorithm developed in Section 2 to this problem of selecting
hyperedges from the forest.
212
Chung et al. Sampling Tree Fragments from Forests
Figure 3
A phrase decomposition forest extracted from the sentence pair ?????????, I have
a date with her today?. Each edge is a minimal SCFG rule, and the rules at the bottom level are
phrase pairs. Unaligned word ?a? shows up in the rule X ? X1X2,X1aX2 after unaligned
words are put back into the alignment matrix. The highlighted portion of the forest shows
an SCFG rule built by composing minimal rules.
The structure and size of phrase decomposition forests are constrained by the
following lemma:
Lemma 2
When there exists more than one minimal edge leading to the same head node n =
([i1, j1], [i2, j2]), each of these minimal edges is a binary split of phrase pair n, which
gives us either a straight or inverted binary SCFG rule with no terminals.
Proof
Suppose that there exist two minimal edges T1 ? n and T2 ? n leading to node n.
Consider the node set we get by taking the union closure of the tail nodes in T1
and T2:
??
(T1 ? T2) ? n
Figure 4 shows two cases of this construction. We show only the spans on the source
language side, which is enough to determine coverage properties. Let n have span [i, j]
on the source side. In the first case (left),
??(T1 ? T2) = {n}. We know
??(T1 ? T2) ? n
is also a valid edge because the unions of intersecting phrases are phrases, too. By the
definition of union closure,
??(T1 ? T2) covers both T1 and T2. Therefore T1 ? n and
213
Computational Linguistics Volume 40, Number 1
T2 ? n cannot both be minimal. In the second case (right),
??(T1 ? T2) = {n}. This
means that the phrases in T1 ? T2 overlap one another in a chain covering the entire span
of n. There must exist a phrase n1 = [i, k1] in T1 or T2 that begins at the left boundary
i of n. Without loss of generality, assume that n1 ? T1. There must exist another phrase
n2 = [k2, j2] ? T2 that overlaps with n such that k2 < k1 and j2 > k1. The span [k2, j] is
a valid phrase, because it consists of the union closure of all phrases that begin to the
right of k2:
??
{[i?, j?] | [i?, j?] ? T1 ? T2 ? i? ? k2} = {[k2, j]}
We also know that n1 ? n2 = [i, k2] is a valid phrase because the difference of two
overlapping phrases is also a valid phrase. Therefore k2 is a valid binary split point of n,
which means that either T2 is an edge formed by this binary split, or T2 is not minimal.
The span [k2, j]? n1 = [k1, j] is also a valid phrase formed by taking the difference of
two overlapping phrases, which makes k1 a valid binary split point for n. This makes T1
either an edge formed by the binary split at k1, or not a minimal phrase. Thus, whenever
we have two minimal edges, both consist of a binary split. 
Another interesting property of phrase decomposition forests relates to the length
of derivations. A derivation is a tree of minimal edges reaching from a given node all
the way down to the forest?s terminal nodes. The length of a derivation is the number
of minimal edges it contains.
Lemma 3
All derivations under a node in a phrase decomposition forest have the same length.
Proof
This is proved by induction. As the base case, all the nodes at the bottom of the
phrase decomposition forest have only one derivation of length 1. For the induction
step, we consider the two possible cases in Lemma 2. The case where a node n has
only a single edge underneath is trivial. It can have only one derivation length be-
cause the children under that single edge already do. For the case where there are
multiple valid binary splits for a node n at span (i, j), we assume the split points are
k1, . . . , ks. Because the intersection of two phrases is also a phrase, we know that spans
(i, k1), (k1, k2), . . . , (ks, j) are all valid phrases, and so is any concatenation of consecutive
phrases in these spans. Any derivation in this sub-forest structure leading from these
s + 1 spans to n has length s, which completes the proof under the assumption of the
induction. 
??(T1 ? T2)
T2
T1
??(T1 ? T2)
T2
i k1k2 j2 j
T1
Figure 4
Sketch of proof for Lemma 2. In the first case,
??(T1 ? T2) consists of more than one span, or
consists of one span that is strictly smaller than n. In the second case,
??(T1 ? T2) = {n}.
214
Chung et al. Sampling Tree Fragments from Forests
Because all the different derivations under the same node in a minimal phrase forest
contain the same number of minimal rules, we call that number the level of a node. The
fact that nodes can be grouped by levels forms the basis of our fast iterative sampling
algorithm as described in Section 5.3.
3.1 Constructing the Phrase Decomposition Forest
Given a word-aligned sentence pair, a phrase decomposition tree can be extracted with
a shift-reduce algorithm (Zhang, Gildea, and Chiang 2008). Whereas the algorithm of
Zhang, Gildea, and Chiang (2008) constructs a single tree which compactly represents
the set of possible phrase trees, we wish to represent the set of all trees as a forest.
We now describe a bottom?up parsing algorithm, shown in Algorithm 2, for building
this forest. The algorithm considers all spans (i, j) in order of increasing length. The
CYK-like loop over split points k (line 10) is only used for the case where a phrase can
be decomposed into two phrases, corresponding to a binary SCFG rule with no right-
hand side terminals. By Lemma 2, this is the only source of ambiguity in constructing
Algorithm 2 The CYK-like algorithm for building a phrase decomposition forest from
word-aligned sentence pair ?f, e?.
1: Extract all phrase pairs in the form of ([i1, j1], [i2, j2])
2: Build a forest node for each phrase pair, and let n(i, j) be the node corresponding to the phrase
pair whose source side is [i, j]
3: for s = 1 . . . |f | do
4: for i = 0 . . . |f | ? s do
5: j ? i + s
6: if n(i, j) exists then
7: continue
8: end if
9: split ? 0
10: for k = i + 1 . . . j ? 1 do
11: if both n(i, k) and n(k, j) exist then
12: add edge {n(i, k),n(k, j)} ? n(i, j)
13: split ? split + 1
14: end if
15: end for
16: if split = 0 then
17: T ? ?
18: l ? i
19: while l < j do
20: l? ? l + 1
21: for m ? j . . . l do
22: if n(l,m) exists then
23: T ? T ? n(l,m)
24: l? ? m
25: break
26: end if
27: end for
28: l ? l?
29: end while
30: add edge T ? n(i, j)
31: end if
32: end for
33: end for
215
Computational Linguistics Volume 40, Number 1
phrase decompositions. When no binary split is found (line 16), a single hyperedge is
made that connects the current span with all its maximal children. (A child is maximal
if it is not itself covered by another child.) This section can produce SCFG rules with
more than two right-hand side nonterminals, and it also produces any rules containing
both terminals and nonterminals in the right-hand side. Right-hand side nonterminals
correspond to previously constructed nodes n(l,m) in line 23, and right-hand side
terminals correspond to advancing a position in the string in line 20.
The running time of the algorithm is O(n3) in terms of the length of the Chinese
sentence f . The size of the resulting forests depends on the input alignments. The worst
case in terms of forest size is when the input consists of a monotonic, one-to-one word
alignment. In this situation, all (i, k, j) tuples correspond to valid hyperedges, and the
size of the output forest is O(n3). At the other extreme, when given a non-decomposable
permutation as an input alignment, the output forest consists of a single hyperedge.
In practice, given Chinese?English word alignments from GIZA++, we find that the
resulting forests are highly constrained, and the algorithm?s running time is negligible
in our overall system. In fact, we find it better to rebuild every forest from a word
alignment every time we re-sample a sentence, rather than storing the hypergraphs
across sampling iterations.
4. Comparison of Sampling Methods
To empirically verify the sampling methods presented in Section 2, we construct phrase
decomposition forests over which we try to learn composed translation rules. In this
section, we use a simple probability model for the tree probability Pt in order to study
the convergence behavior of our sampling algorithm. We will use a more sophisticated
probability model for our end-to-end machine translation experiments in Section 5.
For studying convergence, we desire a simpler model with a probability that can be
evaluated in closed form.
4.1 Model
We use a very basic generative model based on a Dirichlet process defined over
composed rules. The model is essentially the same as the TSG model used by Cohn,
Goldwater, and Blunsom (2009) and Post and Gildea (2009).
We define a single Dirichlet process over the entire set of rules. We draw the rule
distribution G from a Dirichlet process, and then rules from G.
G | ?,P0 ? Dir(?,P0)
r | G ? G
For the base distribution P0, we use a very simple uniform distribution where all rules
of the same size have equal probability:
P0(r) = Vf
?|rf |Ve
?|re|
where Vf is the vocabulary size of source language, and |rf | is the length of the source
side of the rule r. Integrating over G, we get a Chinese restaurant process for the
Dirichlet process. Customers in the Chinese restaurant analogy represent translation
rule instances in the machine translation setting, and tables represent rule types. The
216
Chung et al. Sampling Tree Fragments from Forests
Chinese restaurant has an infinite number of tables, and customers enter one by one and
choose a table to sit at. Let zi be the table chosen by ith customer. Then, the probability
of the customer choosing a table which is already occupied by customers who entered
the restaurant previously, or a new table, is given by following equations:
P(zi = t | z?i) =
{
nt
i?1+? 1 ? t ? T
?
i?1+? t = T + 1
where z?i is the current seating arrangement, t is the index of the table, nt is the number
of customers at the table t, and T is the total number of occupied tables in the restaurant.
In our model, a table t has a label indicating to which rule r the table is assigned. The
label of a table is drawn from the base distribution P0.
If we marginalize over tables labeled with the same rule, we get the following
probability of choosing r given the current analysis z?i of the data:
P(ri = r | z?i) =
nr + ?P0(r)
n + ? (11)
where nr is the number of times rule r has been observed in z?i, and n is total number
of rules observed in z?i.
4.2 Sampling Methods
We wish to sample from the set of possible decompositions into rules, including com-
posed rules, for each sentence in our training data. We follow the top?down sampling
schedule discussed in Section 2 and also implement tree-level rejection sampling as a
baseline.
Our rejection sampling baseline is a form of Metropolis-Hastings where a new tree
t is resampled from a simple proposal distribution Q(t), and then either accepted or
rejected according the Metropolis-Hastings acceptance rule, as shown in Algorithm 3.
As in Algorithm 1, we use v(z, i) to denote a top?down ordering of forest variables. As in
all our experiments, Pt is the current tree probability conditioned on the current trees for
all other sentences in our corpus, using Equation (11) as the rule probability in Equation
(10).
Our proposal distribution samples each variable with uniform probability working
top?down through the forest. The proposal distribution for an entire tree is thus:
Q(t) =
?
w?z[t]
1
deg(w)
This does not correspond to a uniform distribution over entire trees for the reasons dis-
cussed in Section 2. However, the Metropolis-Hastings acceptance probability corrects
for this, and thus the algorithm is guaranteed to converge to the correct distribution in
the long term. We will show that, because the proposal distribution does not re-use any
of the variable settings from the current tree, the rejection sampling algorithm converges
more slowly in practice than the more sophisticated alternative described in Section 2.2.
We now describe in more detail our implementation of the approach of Section 2.2.
We define two operations on a hypergraph node n, SAMPLECUT and SAMPLEEDGE, to
change the sampled tree from the hypergraph. SAMPLECUT(n) chooses whether n is a
217
Computational Linguistics Volume 40, Number 1
Algorithm 3 Metropolis-Hastings sampling algorithm.
Require: A function v(z, i) returning the index of the ith variable of z in a top-down ordering of
the variables of the tree ?(z).
1: i ? 1
2: while i < |Z[?(z)]| do
3: Sample znewv(z,i) according to uniform(z
new
v(z,i) )
4: i ? i + 1
5: end while
6: z ?
{
znew w/probmin
{
1, Pt (t(z
new ))Q(t(zold ))
Pt (t(zold ))Q(t(znew ))
}
zold otherwise
segmentation point or not, deciding if two rules should merge, while SAMPLEEDGE(n)
chooses a hyperedge under n, making an entire new subtree. Algorithm 4 shows our
implementation of Algorithm 1 in terms of tree operations and the sampling operations
SAMPLEEDGE(n) and SAMPLECUT(n).
4.3 Experiments
We used a Chinese?English parallel corpus available from the Linguistic Data Consor-
tium (LDC), composed of newswire text. The corpus consists of 41K sentence pairs,
which is 1M words on the English side. We constructed phrase decomposition forests
with this corpus and ran the top?down sampling algorithm and the rejection sampling
algorithm described in Section 4.2 for one hundred iterations.We used? = 100 for every
experiment. The likelihood of the current state was calculated for every iteration. Each
setting was repeated five times, and then we computed the average likelihood for each
iteration.
Figure 5 shows a comparison of the likelihoods found by rejection sampling and
top?down sampling. As expected, we found that the likelihood converged much more
quickly with top?down sampling. Figure 6 shows a comparison between two different
versions of top?down sampling: the first experiment was run with the density factor
described in Section 2, Equation (6), and the second one was run without the density
factor. The density factor has a much smaller effect on the convergence of our algorithm
than does the move from rejection sampling to top?down sampling, such that the dif-
ference between the two curves shown in Figure 6 is not visible at the scale of Figure 5.
(The first ten iterations are omitted in Figure 6 in order to highlight the difference.) The
small difference is likely due to the fact that our trees are relatively evenly balanced,
Algorithm 4 Top?down sampling algorithm.
1: queue.push(root)
2: while queue is not empty do
3: n = queue.pop()
4: SAMPLEEDGE(n)
5: SAMPLECUT(n)
6: for each child c of node n do
7: queue.push(c)
8: end for
9: end while
218
Chung et al. Sampling Tree Fragments from Forests
Figure 5
Likelihood graphs for rejection sampling and top?down sampling.
Figure 6
Likelihood graphs for top?down sampling with and without density factor. The first ten
iterations are omitted to highlight the difference.
such that the ratio of the density factor for two trees is not significant in comparison
to the ratio of their model probabilities. Nevertheless, we do find higher likelihood
states with the density factor than without it. This shows that, in addition to providing
a theoretical guarantee that our Markov chain converges to the desired distribution Pt
in the limit, the density factor also helps us find higher probability trees in practice.
219
Computational Linguistics Volume 40, Number 1
5. Application to Machine Translation
The results of the previous section demonstrate the performance of our algorithm in
terms of the probabilities of the model it is given, but do not constitute an end-to-end
application. In this section we demonstrate its use in a complete machine translation
system, using the SCFG rules found by the sampler in a Hiero-style MT decoder. We
discuss our approach and how it relates to previous work in machine translation in
Section 5.1 before specifying the precise probability model used for our experiments in
Section 5.2, discussing a technique to speed-up the model?s burn-in in Section 5.3, and
describing our experiments in Section 5.4.
5.1 Approach
A typical pipeline for training current statistical machine translation systems consists
of the following three steps: word alignment, rule extraction, and tuning of feature
weights. Word alignment is most often performed using the models of Brown et al.
(1993) and Vogel, Ney, and Tillmann (1996). Phrase extraction is performed differently
for phrase-based (Koehn, Och, and Marcu 2003), hierarchical (Chiang 2005), and syntax-
based (Galley et al. 2004) translation models, whereas tuning algorithms are generally
independent of the translation model (Och 2003; Chiang, Marton, and Resnik 2008;
Hopkins and May 2011).
Recently, a number of efforts have been made to combine the word alignment and
rule extraction steps into a joint model, with the hope both of avoiding some of the
errors of the word-level alignment, and of automatically learning the decomposition
of sentence pairs into rules (DeNero, Bouchard-Cote, and Klein 2008; Blunsom et al.
2009; Blunsom and Cohn 2010a; Neubig et al. 2011). This approach treats both word
alignment and rule decomposition as hidden variables in an EM-style algorithm. While
these efforts have been able to match the performance of systems based on two succes-
sive steps for word alignment and rule extraction, they have generally not improved
performance enough to become widely adopted. One possible reason for this is the
added complexity and in particular the increased computation time when compared
to the standard pipeline. The accuracy of word-level alignments from the standard
GIZA++ package has proved hard to beat, in particular when large amounts of training
data are available.
Given this state of affairs, the question arises whether static word alignments can
be used to guide rule learning in a model which treats the decomposition of a sentence
pair into rules as a hidden variable. Such an approach would favor rules which are
consistent with the other sentences in the data, and would contrast with the standard
practice inHiero-style systems of simply extracting all overlapping rules consistent with
static word alignments. Constraining the search over rule decomposition with word
alignments has the potential to significantly speed up training of rule decomposition
models, overcoming one of the barriers to their widespread use. Rule decomposition
models also have the benefit of producing much smaller grammars than are achieved
when extracting all possible rules. This is desirable given that the size of translation
grammars is one of the limiting computational factors in current systems, necessitating
elaborate strategies for rule filtering and indexing.
In this section, we apply our sampling algorithm to learn rules for the Hiero trans-
lation model of Chiang (2005). Hiero is based on SCFG, with a number of constraints on
the form that rules can take. The grammar has a single nonterminal, and each rule has
220
Chung et al. Sampling Tree Fragments from Forests
at most two right-hand side nonterminals. Most significantly, Hiero allows rules with
mixed terminals and nonterminals on the right-hand side. This has the great benefit
of allowing terminals to control re-ordering between languages, but also leads to very
large numbers of valid rules during the rule extraction process. We wish to see whether,
by adding a learned model of sentence decomposition to Hiero?s original method of
leveraging fixed word-level alignments, we can learn a small set of rules in a system that
is both efficient to train and efficient to decode. Our approach of beginning with fixed
word alignments is similar to that of Sankaran, Haffari, and Sarkar (2011), although
their sampling algorithm reanalyzes individual phrases extracted with Hiero heuristics
rather than entire sentences, and produces rules with no more than one nonterminal
on the right-hand side.
Most previous works on joint word alignment and rule extraction models were
evaluated indirectly by resorting to heuristic methods to extract rules from learned
word alignment or bracketing structures (DeNero, Bouchard-Cote, and Klein 2008;
Zhang et al. 2008; Blunsom et al. 2009; Levenberg, Dyer, and Blunsom 2012), and do not
directly learn the SCFG rules that are used during decoding. In this article, we work
with lexicalized translation rules with a mix of terminals and nonterminals, and we
use the rules found by our sampler directly for decoding. Because word alignments are
fixed in our model, any improvements we observe in translation quality indicate that
our model learns how SCFG rules interplay with each other, rather than fixing word
alignment errors.
The problem of rule decomposition is not only relevant to the Hiero model.
Translation models that make use of monolingual parsing, such as string-to-tree
(Galley et al. 2004), and tree-to-string (Liu, Liu, and Lin 2006), are all known to
benefit greatly from learning composed rules (Galley et al. 2006). In the particular
case of Hiero rule extraction, although there is no explicit rule composition step, the
extracted rules are in fact ?composed rules? in the sense of string-to-tree or tree-to-
string rule extraction, because they can be further decomposed into smaller SCFG rules
that are also consistent with word alignments. Although our experiments only include
the Hiero model, the method presented in this article is also applicable to string-to-
tree and tree-to-string models, because the phrase decomposition forest presented in
Section 3 can be extended to rule learning and extraction of other syntax-based MT
models.
5.2 Model
In this section, we describe a generative model based on the Pitman-Yor process (Pitman
and Yor 1997; Teh 2006) over derivation trees consisting of composed rules. Bayesian
methods have been applied to a number of segmentation tasks in natural language pro-
cessing, including word segmentation, TSG learning, and learning machine translation
rules, as a way of controlling the overfitting produced when Expectation Maximization
would tend to prefer longer segments. However, it is important to note that the Bayesian
priors in most cases control the size and number of the clusters, but do not explicitly
control the size of rules. In many cases, this type of Bayesian prior alone is not strong
enough to overcome the preference for longer, less generalizable rules. For example,
some previous work in word segmentation (Liang and Klein 2009; Naradowsky and
Toutanova 2011) adopts a ?length penalty? to remedy this situation. Because we have
the prior knowledge that longer rules are less likely to generalize and are therefore less
likely to be a good rule, we adopt a similar scheme to control the length of rules in
our model.
221
Computational Linguistics Volume 40, Number 1
In order to explicitly control the length of our rules, we generate a rule r in two
stages. First, we draw the length of a rule |r| =  from a probability distribution defined
over positive integers. We use a Poisson distribution:
P(; ?) = ?
e??
!
Because of the factorial in the denominator, the Poisson distribution decays quickly as
 becomes larger, which is ideal for selecting rule length because we want to encourage
learning of shorter rules and learn longer rules only when there is strong evidence for
them in the data.
A separate Pitman-Yor process is defined for the rules of each length . We draw
the rule distribution G from a Pitman-Yor process, and then rules of length  are drawn
from G.
G | ?, d,P0 ? PY(?, d,P0)
r | G ? G
The first two parameters, a concentration parameter ? and a discount parameter d,
control the shape of distribution G by controlling the size and the number of clusters.
The label of the cluster is decided by the base distribution P0. Because our alignment
is fixed, we do not need a complex base distribution that differentiates better aligned
phrases from others. We use a uniform distribution where each rule of the same size
has equal probability. Since the number of possible shorter rules is smaller than that of
longer rules, we need to reflect this fact and need to have larger uniform probability
for shorter rules and smaller uniform probability for longer rules. We reuse the Poisson
probability for the base distribution, essentially assuming that the number of possible
rules of length  is 1/P(; ?).
The Pitman-Yor process gives us the following probability of choosing r of size 
given the current analysis z?i of the data:
P(ri = r | , z?i) =
nr ? Trd + (Td + ?)P0(r)
n + ?
where nr is the number of times rule r has been observed in z?i, Tr is the number of
tables (in the Chinese restaurant metaphor) labeled r, and n is the total number of rules
of length  observed in z?i. Because we have drawn the length of the rule from a Poisson
distribution, the rule length probability is multiplied by this equation in order to obtain
the probability of the rule under our model.
Keeping track of table assignments during inference requires a lot of book-keeping.
In order to simplify the implementation, instead of explicitly keeping track of the
number of tables for each rule, we estimate the number of tables using the following
equations (Huang and Renals 2010):
Tr = ndr
T =
?
r:|r|=
ndr
222
Chung et al. Sampling Tree Fragments from Forests
In order to encourage learning rules with smaller parsing complexity and rules with
mixed terminals and nonterminals, which are useful for replicating re-orderings that
are seen in the data, we made use of the concept of scope (Hopkins and Langmead
2010) in our definition of rule length. The scope of a rule is defined as the number of
pairs of adjacent nonterminals in the source language right-hand side plus the number
of nonterminals at the beginning or end of the source language right-hand side. For
example,
X ? f1X1X2f2X3, X1e1X2X3e2
has scope 2 because X1 and X2 are adjacent in the source language and X3 is at the
end of the source language right-hand side. The target side of the rule is irrelevant.
The intuition behind this definition is that it measures the number of free indices into
the source language string required during parsing, under the assumption that the
terminals provide fixed anchor points into the string. Thus a rule of scope of k can be
parsed in O(nk). We define the length of a rule to be the number of terminals in the
source and the target side plus the scope of the rule. This is equivalent to counting the
total number of symbols in the rule, but only counting a nonterminal if it contributes
to parsing complexity. For example, the length of a rule that consists only of two
consecutive nonterminals would be 3, and the length of a rule that has two consecutive
nonterminals bounded by terminals on both sides would be 3 as well. This definition
of rule length encourages rules with mixed terminals and nonterminals over rules with
only nonterminals, which tend not to provide useful guidance to the translation process
during decoding.
5.3 Stratified Sampling
We follow the same Gibbs sampler introduced in Section 4.2. The SAMPLEEDGE oper-
ation in our Gibbs sampler can be a relatively expensive operation, because the entire
subtree under a node is being changed during sampling. We observe that in a phrase
decomposition forest, lexicalized rules, which are crucial to translation quality, appear
at the bottom level of the forest. This lexicalized information propagates up the forest
as rules get composed. It is reasonable to constrain initial sampling iterations to work
only on those bottom level nodes, and then gradually lift the constraint. This not only
makes the sampler much more efficient, but also gives it a chance to focus on getting
better estimates of the more important parameters, before starting to consider nodes
at higher levels, which correspond to rules of larger size. Fortunately, as mentioned in
Section 3, each node in a phrase decomposition forest already has a unique level, with
level 1 nodes corresponding to minimal phrase pairs. We design the sampler to use a
stratified sampling process (i.e., sampling level one nodes for K iterations, then level 1
and 2 nodes for K iterations, and so on). We emphasize that when we sample for level
2 nodes, level 1 nodes are also sampled, which means parameters for the smaller rules
are given more chance to mix, and thereby settle into a more stable distribution.
In our experiments, running the first 100 iterations of sampling with regular sam-
pling techniques took us about 18 hours. However, with stratified sampling, it took
only about 6 hours. We also compared translation quality as measured by decoding
with rules from the 100th sample, and by averaging over every 10th sample. Both
sampling methods gave us roughly the same translation quality as measured in BLEU.
We therefore used stratified sampling throughout our experiments.
223
Computational Linguistics Volume 40, Number 1
5.4 Experiments
We used a Chinese?English parallel corpus available from LDC,1 composed of
newswire text. The corpus consists of 41K sentence pairs, which is 1M words on the
English side. We used a 392-sentence development set with four references for parame-
ter tuning, and a 428-sentence test set with four references for testing.2 The development
set and the test set have sentences with less than 30 words. A trigram language model
was used for all experiments. BLEU (Papineni et al. 2002) was calculated for evaluation.
5.4.1 Baseline. For our baseline system, we extract Hiero translation rules using the
heuristic method (Chiang 2007), with the standard Hiero rule extraction constraints.
We use our in-house SCFG decoder for translation with both the Hiero baseline and our
sampled grammars. Our features for all experiments include differently normalized rule
counts and lexical weightings (Koehn, Och, and Marcu 2003) of each rule. Weights are
tuned using Pairwise Ranking Optimization (Hopkins and May 2011) using the baseline
grammar and development set, then used throughout the experiments.
Because our sampling procedure results in a smaller rule table, we also establish a
no-singleton baseline to compare our results to a simple heuristic method of reducing
rule table size. The no-singleton baseline discards rules that occur only once and that
have more than one word on the Chinese side during the Hiero rule extraction process,
before counting the rules and computing feature scores.
5.4.2 Experimental Settings.
Model parameters. For all experiments, we used d = 0.5 for the Pitman-Yor discount
parameter, except where we compared the Pitman-Yor process with Dirichlet process
(d = 0). Although we have a separate Pitman-Yor process for each rule length, we used
the same ? = 5 for all rule sizes in all experiments, including Dirichlet process experi-
ments. For rule length probability, a Poisson distribution where ? = 2 was used for all
experiments.
Sampling. The samples are initialized such that all nodes in a forest are set to be seg-
mented, and a random edge is chosen under each node. For all experiments, we ran the
sampler for 100 iterations and took the sample from the last iteration to compare with
the baseline. For stratified sampling, we increased the level we sample at every 10th
iteration. We also tried ?averaging? samples, where samples from every 10th iteration
are merged to a single grammar. For averaging samples, we took the samples from the
0th iteration (initialization) to the 70th iteration at every 10th iteration.3 We decided
on the 70th iteration (last iteration of level 7 sampling) as the last iteration because
we constrained the sampler not to sample nodes whose span covers more than seven
words (for SAMPLECUT only, SAMPLECUT always segments for these nodes), and the
likelihood becomes very stable at that point.
1 We randomly sampled our data from various different sources (LDC2006E86, LDC2006E93, LDC2002E18,
LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08, LDC2005T06, LDC2005T10, LDC2005T34,
LDC2006E26, LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006E24, LDC2006E92,
LDC2006E24). The language model is trained on the English side of entire data (1.65M sentences,
which is 39.3M words).
2 They are from newswire portion of NIST MT evaluation data from 2004, 2005, and 2006.
3 Not including initialization has negligible effect on translation quality.
224
Chung et al. Sampling Tree Fragments from Forests
Rule extraction. Because every tree fragment in the sampled derivation represents a
translation rule, we do not need to explicitly extract the rules; we merely need to collect
them and count them. However, derivations include purely nonlexical rules that do not
conform to the rule constraints of Hiero, and which are not useful for translation. To
get rid of this type of rule, we prune every rule that has a scope greater than 2. Whereas
Hiero does not allow two adjacent nonterminals in the source side, our pruning criterion
allows some rules of scope 2 that are not allowed by Hiero. For example, the following
rule (only source side shown) has scope 2 but is not allowed by Hiero:
X ? w1X1X2w2X3
In order to see if these rules have any positive or negative effects on translation, we
compare a rule set that strictly conforms to the Hiero constraints and a rule set that
includes all the rules of scope 2 or less.
5.4.3 Results. Table 2 summarizes our results. As a general test of our probability model,
we compare the result from initialization and the 100th sample. The translation perfor-
mance of the grammar from the 100th iteration of sampling is much higher than that
of the initialization state. This shows that states with higher probability in our Markov
chain generally do result in better translation, and that the sampling process is able to
learn valuable composed rules.
In order to determine whether the composed rules learned by our algorithm are
particularly valuable, we compare them to the standard baseline of extracting all rules.
The size of the grammar taken from the single sample (100th sample) is only about 9% of
the baseline but still produces translation results that are not far worse than the baseline.
A simple way to reduce the number of rules in the baseline grammar is to remove all
rules that occur only once in the training data and that contain more than a single word
on the Chinese side. This ?no-singleton? baseline still leaves us with more rules than
our algorithm, with translation results between those of the standard baseline and our
algorithm.
We also wish to investigate the trade-off between grammar size and translation
performance that is induced by including rules from multiple steps of the sampling
process. It is helpful for translation quality to include more than one analysis of each
sentence in the final grammar in order to increase coverage of new sentences. Averaging
samples also better approximates the long-term behavior of the Markov chain, whereas
taking a single sample involves an arbitrary random choice. When we average eight
Table 2
Comparisons of decoding results.
iteration model pruning #rules dev test time (s)
Baseline heuristic Hiero 3.59M 25.5 25.1 809
No-singleton heuristic Hiero 1.09M 24.7 24.2 638
Sampled 0th (init) Pitman-Yor scope < 3 212K 19.9 19.1 489
Sampled 100th Pitman-Yor scope < 3 313K 23.9 23.3 1,214
Sampled averaged (0 to 70) Pitman-Yor scope < 3 885K 26.2 24.5 1,488
Sampled averaged (0 to 70) Pitman-Yor Hiero 785K 25.6 25.1 532
Sampled averaged (0 to 70) Dirichlet scope < 3 774K 24.6 23.8 930
225
Computational Linguistics Volume 40, Number 1
different samples, we get a larger number of rules than from a single sample, but still
only a quarter as many rules as in the Hiero baseline. The translation results with eight
samples are comparable to the Hiero baseline (not significantly different according
to 1,000 iterations of paired bootstrap resampling [Koehn 2004]). Translation results
are better with the sampled grammar than with the no-singleton method of reducing
grammar size, while the sampled grammar was smaller than the no-singleton rule set.
Thus, averaging samples seems to produce a good trade-off between grammar size and
quality.
The filtering applied to the final rule set affects both the grammar size and de-
coding speed, because rules with different terminal/nonterminal patterns have vary-
ing decoding complexities. We experimented with two methods of filtering the final
grammar: retaining rules of scope no greater than three, and the more restrictive the
Hiero constraints. We do not see a consistent difference in translation quality between
these methods, but there is a large impact in terms of speed. The Hiero constraints
dramatically speeds decoding. The following is the full list of the Hiero constraints,
taken verbatim from Chiang (2007):
 If there are multiple initial phrase pairs containing the same set of
alignments, only the smallest is kept. That is, unaligned words are
not allowed at the edges of phrases.
 Initial phrases are limited to a length of 10 words on either side.
 Rules are limited to five nonterminals plus terminals on the French side.
 Rules can have at most two nonterminals, which simplifies the decoder
implementation. This also makes our grammar weakly equivalent to an
inversion transduction grammar (Wu 1997), although the conversion
would create a very large number of new nonterminal symbols.
 It is prohibited for nonterminals to be adjacent on the French side,
a major cause of spurious ambiguity.
 A rule must have at least one pair of aligned words, so that translation
decisions are always based on some lexical evidence.
Of these constraints, the differences between the Hiero constraints and scope filtering
are: First, the Hiero constraints limit the number of nonterminals in a rule to no more
than two. Second, the Hiero constraints do not allow two adjacent nonterminals in
the source side of a rule. As discussed previously, these two differences limit Hiero
grammar to be a subset of scope 2 grammar, whereas the scope-filtered grammar retains
all scope 2 rules. Among grammars with the Hiero constraint, smaller grammars are
generally faster. The relationship between the number of rules and the decoding time is
less than linear. This is because the decoder never considers rules containing sequences
of terminals not present in the source sentence. As the number of rules grows, we see
rules with larger numbers of terminals that in turn apply to fewer input sentences.
The sampled grammar has a more pronounced effect of reducing rule table size than
decoding speed. Our sampling method may be particularly valuable for very large data
sets where grammar size can become a limiting factor.
Finally, we wish to investigate whether the added power of the Pitman-Yor process
gives any benefit over the simpler Dirichlet process prior, using the same modeling
of word length in both cases. We find better translation quality with the Pitman-Yor
226
Chung et al. Sampling Tree Fragments from Forests
process, indicating that the additional strength of the Pitman-Yor process in suppressing
infrequent rules helps prevent overfitting.
6. Conclusion
We presented a hypergraph sampling algorithm that overcomes the difficulties inherent
in computing inside probabilities in applications where the segmentation of the tree into
rules is not known.
Given parallel text with word-level alignments, we use this algorithm to learn
sentence bracketing and SCFG rule composition. Our rule learning algorithm is based
on a compact structure that represents all possible SCFG rules extracted from word-
aligned sentences pairs, and works directly with highly lexicalized model parameters.
We show that by effectively controlling overfitting with a Bayesian model, and design-
ing algorithms that efficiently sample that parameter space, we are able to learn more
compact grammars with competitive translation quality. Based on the framework we
built in this work, it is possible to explore other rule learning possibilities that are known
to help translation quality, such as learning refined nonterminals.
Our general sampling algorithm is likely to be useful in settings beyond machine
translation. One interesting application would be unsupervised or partially supervised
learning of (monolingual) TSGs, given text where the tree structure is completely or
partially unknown, as in the approach of Blunsom and Cohn (2010b).
Acknowledgments
This work was partially funded by NSF
grant IIS-0910611.
References
Blunsom, P., T. Cohn, C. Dyer, and
M. Osborne. 2009. A Gibbs sampler for
phrasal synchronous grammar induction.
In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 2,
pages 782?790, Singapore.
Blunsom, Phil and Trevor Cohn. 2010a.
Inducing synchronous grammars with
slice sampling. In Proceedings of the Human
Language Technology: The 11th Annual
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 238?241, Boulder, CO.
Blunsom, Phil and Trevor Cohn. 2010b.
Unsupervised induction of tree
substitution grammars for dependency
parsing. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing, pages 1,204?1,213,
Cambridge, MA.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Chappelier, Jean-Ce?dric and Martin Rajman.
2000. Monte-Carlo sampling for NP-hard
maximization problems in the framework
of weighted parsing. In Natural Language
Processing (NLP 2000), pages 106?117,
Patras.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical
machine translation. In Proceedings of the
43rd Annual Conference of the Association
for Computational Linguistics (ACL-05),
pages 263?270, Ann Arbor, MI.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
Chiang, David, Yuval Marton, and Philip
Resnik. 2008. Online large-margin training
of syntactic and structural translation
features. In Conference on Empirical
Methods in Natural Language Processing
(EMNLP-08), pages 224?233, Honolulu, HI.
Cohn, Trevor and Phil Blunsom. 2010.
Blocked inference in Bayesian tree
substitution grammars. In Proceedings of
the 48th Annual Meeting of the Association
for Computational Linguistics (ACL-10),
pages 225?230, Uppsala.
Cohn, Trevor, Sharon Goldwater, and
Phil Blunsom. 2009. Inducing compact
but accurate tree-substitution grammars.
In Proceedings of Human Language
227
Computational Linguistics Volume 40, Number 1
Technologies: The 2009 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 548?556, Boulder, CO.
DeNero, John, Alexandre Bouchard-Cote,
and Dan Klein. 2008. Sampling alignment
structure under a Bayesian translation
model. In Conference on Empirical
Methods in Natural Language Processing
(EMNLP-08), pages 314?323, Honolulu, HI.
Galley, Michel, Jonathan Graehl, Kevin
Knight, Daniel Marcu, Steve DeNeefe,
Wei Wang, and Ignacio Thayer. 2006.
Scalable inference and training of
context-rich syntactic translation models.
In Proceedings of the International Conference
on Computational Linguistics/Association
for Computational Linguistics (COLING/
ACL-06), pages 961?968, Sydney.
Galley, Michel, Mark Hopkins, Kevin Knight,
and Daniel Marcu. 2004. What?s in a
translation rule? In Proceedings of the
2004 Meeting of the North American
Chapter of the Association for Computational
Linguistics (NAACL-04), pages 273?280,
Boston, MA.
Hopkins, Mark and Greg Langmead.
2010. SCFG decoding without
binarization. In Proceedings of the
2010 Conference on Empirical Methods
in Natural Language Processing,
pages 646?655, Cambridge, MA.
Hopkins, Mark and Jonathan May. 2011.
Tuning as ranking. In Proceedings of the
2011 Conference on Empirical Methods
in Natural Language Processing,
pages 1,352?1,362, Edinburgh.
Huang, Songfang and Steve Renals. 2010.
Power law discounting for n-gram
language models. In Proceedings of the
IEEE International Conference on Acoustic,
Speech, and Signal Processing (ICASSP?10),
pages 5,178?5,181, Dallas, TX.
Johnson, Mark, Thomas L. Griffiths, and
Sharon Goldwater. 2007. Bayesian
inference for PCFGs via Markov chain
Monte Carlo. In Proceedings of the 2007
Meeting of the North American Chapter
of the Association for Computational
Linguistics (NAACL-07), pages 139?146,
Rochester, NY.
Knuth, D. E. 1975. Estimating the efficiency
of backtrack programs. Mathematics of
Computation, 29(129):121?136.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation.
In 2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 388?395, Barcelona.
Koehn, Philipp, Franz Josef Och, and
Daniel Marcu. 2003. Statistical
phrase-based translation. In Proceedings
of the 2003 Meeting of the North American
Chapter of the Association for Computational
Linguistics (NAACL-03), pages 48?54,
Edmonton.
Levenberg, Abby, Chris Dyer, and Phil
Blunsom. 2012. A Bayesian model for
learning SCFGs with discontiguous rules.
In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning, pages 223?232,
Jeju Island.
Liang, Percy and Dan Klein. 2009. Online
EM for unsupervised models. In
Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 611?619,
Boulder, CO.
Liu, Yang, Qun Liu, and Shouxun Lin.
2006. Tree-to-string alignment template
for statistical machine translation.
In Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics, pages 609?616,
Sydney.
Naradowsky, Jason and Kristina Toutanova.
2011. Unsupervised bilingual morpheme
segmentation and alignment with
context-rich hidden semi-Markov models.
In Proceedings of the 49th Annual Meeting
of the Association for Computational
Linguistics: Human Language Technologies,
pages 895?904, Portland, OR.
Neubig, Graham, Taro Watanabe, Eiichiro
Sumita, Shinsuke Mori, and Tatsuya
Kawahara. 2011. An unsupervised model
for joint phrase alignment and extraction.
In Proceedings of the 49th Annual Meeting
of the Association for Computational
Linguistics: Human Language Technologies,
pages 632?641, Portland, OR.
Och, Franz Josef. 2003. Minimum error rate
training for statistical machine translation.
In Proceedings of the 41th Annual Conference
of the Association for Computational
Linguistics (ACL-03), pages 160?167,
Sapporo.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. BLEU:
A method for automatic evaluation of
machine translation. In Proceedings of the
40th Annual Conference of the Association
for Computational Linguistics (ACL-02),
pages 311?318, Philadelphia, PA.
228
Chung et al. Sampling Tree Fragments from Forests
Pitman, Jim and Marc Yor. 1997. The
two-parameter Poisson-Dirichlet
distribution derived from a stable
subordinator. Annals of Probability,
25(2):855?900.
Post, Matt and Daniel Gildea. 2009. Bayesian
learning of a tree substitution grammar.
In Proceedings of the Association for
Computational Linguistics (short paper),
pages 45?48, Singapore.
Sankaran, Baskaran, Gholamreza Haffari,
and Anoop Sarkar. 2011. Bayesian
extraction of minimal SCFG rules for
hierarchical phrase-based translation.
In Proceedings of the Sixth Workshop
on Statistical Machine Translation,
pages 533?541, Edinburgh.
Teh, Yee Whye. 2006. A hierarchical
Bayesian language model based on
Pitman-Yor processes. In Proceedings
of the 21st International Conference on
Computational Linguistics and 44th Annual
Meeting of the Association for Computational
Linguistics, pages 985?992, Sydney.
Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. HMM-based
word alignment in statistical translation.
In Proceedings of the 16th International
Conference on Computational Linguistics
(COLING-96), pages 836?841, Copenhagen.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
Zhang, Hao, Daniel Gildea, and David
Chiang. 2008. Extracting synchronous
grammar rules from word-level
alignments in linear time. In Proceedings
of the 22nd International Conference on
Computational Linguistics (COLING-08),
pages 1,081?1,088, Manchester.
Zhang, Hao, Chris Quirk, Robert C. Moore,
and Daniel Gildea. 2008. Bayesian learning
of non-compositional phrases with
synchronous parsing. In Proceedings of
the 46th Annual Meeting of the Association
for Computational Linguistics (ACL-08),
pages 97?105, Columbus, OH.
229

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 769?776,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Optimal Parsing Strategies for Linear Context-Free Rewriting Systems
Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
Factorization is the operation of transforming
a production in a Linear Context-Free Rewrit-
ing System (LCFRS) into two simpler produc-
tions by factoring out a subset of the nontermi-
nals on the production?s righthand side. Fac-
torization lowers the rank of a production but
may increase its fan-out. We show how to
apply factorization in order to minimize the
parsing complexity of the resulting grammar,
and study the relationship between rank, fan-
out, and parsing complexity. We show that it
is always possible to obtain optimum parsing
complexity with rank two. However, among
transformed grammars of rank two, minimum
parsing complexity is not always possible with
minimum fan-out. Applying our factorization
algorithm to LCFRS rules extracted from de-
pendency treebanks allows us to find the most
efficient parsing strategy for the syntactic phe-
nomena found in non-projective trees.
1 Introduction
G?mez-Rodr?guez et al (2009a) recently examined
the problem of transforming arbitrary grammars in
the Linear Context-Free Rewriting System (LCFRS)
formalism (Vijay-Shankar et al, 1987) in order to
reduce the rank of a grammar to 2 while minimiz-
ing its fan-out. The work was motivated by the
desire to develop efficient chart-parsing algorithms
for non-projective dependency trees (Kuhlmann and
Nivre, 2006) that do not rely on the independence
assumptions of spanning tree algorithms (McDon-
ald et al, 2005). Efficient parsing algorithms for
general LCFRS are also relevant in the context of
Synchronous Context-Free Grammars (SCFGs) as a
formalism for machine translation, as well as the de-
sire to handle even more general synchronous gram-
mar formalisms which allow nonterminals to cover
discontinuous spans in either language (Melamed et
al., 2004; Wellington et al, 2006). LCFRS provides
a very general formalism which subsumes SCFGs,
the Multitext Grammars of Melamed et al (2004),
as well as mildly context-sensitive monolingual for-
malisms such as Tree Adjoining Grammar (Joshi
and Schabes, 1997). Thus, work on transforming
general LCFRS grammars promises to be widely ap-
plicable in both understanding how these formalisms
interrelate, and, from a more practical viewpoint, de-
riving efficient parsing algorithms for them.
In this paper, we focus on the problem of trans-
forming an LCFRS grammar into an equivalent
grammar for which straightforward application of
dynamic programming to each rule yields a tabular
parsing algorithm with minimum complexity. This
is closely related, but not equivalent, to the prob-
lem considered by G?mez-Rodr?guez et al (2009a),
who minimize the fan-out, rather than the parsing
complexity, of the resulting grammar. In Section 4,
we show that restricting our attention to factorized
grammars with rank no greater than 2 comes at no
cost in parsing complexity. This result may be sur-
prising, as G?mez-Rodr?guez et al (2009a) com-
ment that ?there may be cases in which one has to
find an optimal trade-off between rank and fan-out?
in order to minimize parsing complexity ? in fact,
no such trade-off is necessary, as rank 2 is always
sufficient for optimal parsing complexity. Given
this fact, we show how to adapt the factorization al-
gorithm of G?mez-Rodr?guez et al (2009a) to re-
turn a transformed grammar with minimal parsing
complexity and rank 2. In Section 5, we give a
769
counterexample to the conjecture that minimal pars-
ing complexity is possible among binarizations with
minimal fan-out.
2 Background
A linear context-free rewriting system (LCFRS) is
defined as a tuple G = (VN , VT , P, S), where VT is
a set of terminal symbols, VN is a set of nonterminal
symbols, P is a set of productions, and S ? VN is
a distinguished start symbol. Associated with each
nonterminal B is a fan-out ?(B), which tell how
many discontinuous spans B covers. Productions
p ? P take the form:
p : A? g(B1, B2, . . . , Br) (1)
where A,B1, . . . Br ? VN , and g is a function
g : (V ?T )?(B1) ? . . .? (V ?T )?(Br) ? (V ?T )?(A)
which specifies how to assemble the
?r
i=1 ?(Bi)
spans of the righthand side nonterminals into the
?(A) spans of the lefthand side nonterminal. The
function g must be linear, non-erasing, which
means that if we write
g(?x1,1, . . . , x1,?(B1)?, . . . , ?x1,1, . . . , x1,?(Br)?)
= ?t1, . . . , t?(A)?
the tuple of strings ?t1, . . . , t?(A)? on the righthand
side contains each variable xi,j from the lefthand
side exactly once, and may also contain terminals
from VT .
We call r, the number of nonterminals on the
righthand side of a production p, the rank of p, ?(p).
The fan-out of a production, ?(p) is the fan-out of its
lefthand side, ?(A). The rank of a grammar is the
maximum rank of its rules,
?(G) = max
p?P
?(p)
and similarly the fan-out of a grammar is the maxi-
mum fan-out of its rules, or equivalently, of its non-
terminals:
?(G) = max
B?VN
?(B)
3 Parsing LCFRS
A bottom-up dynamic programming parser can be
produced from an LCFRS grammar by generaliz-
ing the CYK algorithm for context-free grammars.
We convert each production of the LCFRS into a
deduction rule with variables for the left and right
endpoints of each of the ?(Bi) spans of each of the
nonterminals Bi, i ? [r] in the righthand side of the
production.
The computational complexity of the resulting
parser is polynomial in the length of the input string,
with the degree of the polynomial being the number
of distinct endpoints in the most complex produc-
tion. Thus, for input of length n, the complexity
is O(nc) for some constant c which depends on the
grammar.
For a given rule, each of the r nonterminals has
?(Bi) spans, and each span has a left and right end-
point, giving an upper bound of c ? 2?ri=1 ?(Bi).
However, some of these endpoints may be shared
between nonterminals on the righthand side. The
exact number of distinct variables for the dynamic
programming deduction rule can the written
c(p) = ?(A) +
r
?
i=1
?(Bi) (2)
where c(p) is the parsing complexity of a produc-
tion p of the form of eq. 1 (Seki et al, 1991). To
see this, consider counting the left endpoint of each
span on the lefthand side of the production, and the
right endpoint of each span on the righthand side of
the production. Any variable corresponding to the
left endpoint of a span of a righthand side nonter-
minal will either be shared with the right endpoint
of another span if two spans are being joined by the
production, or, alternatively, will form the left end-
point of a span of A. Thus, each distinct endpoint in
the production is counted exactly once by eq. 2.
The parsing complexity of a grammar, c(G), is
the maximum parsing complexity of its rules. From
eq. 2, we see that c(G) ? (?(G) + 1)?(G). While
we focus on the time complexity of parsing, it is in-
teresting to note the space complexity of the DP al-
gorithm is O(n2?(G)), since the DP table for each
nonterminal is indexed by at most 2?(G) positions
in the input string.
770
4 Binarization Minimizes Parsing
Complexity
An LCFRS production of rank r can be factorized
into two productions of the form:
p1 : A? g1(B1, . . . , Br?2, X)
p2 : X ? g2(Br?1, Br)
This operation results in new productions that have
lower rank, but possibly higher fan-out, than the
original production.
If we examine the DP deduction rules correspond-
ing to the original production p, and the first new
production p1 we find that
c(p1) ? c(p)
regardless of the function g of the original produc-
tion, or the fan-out of the production?s nonterminals.
This is because
?(X) ? ?(Br?1) + ?(Br)
that is, our newly created nonterminal X may join
spans from Br?1 and Br, but can never introduce
new spans. Thus,
c(p1) = ?(A) +
(r?2
?
i=1
?(Bi)
)
+ ?(X)
? ?(A) +
r
?
i=1
?(Bi)
= c(p)
As similar result holds for the second newly cre-
ated production:
c(p2) ? c(p)
In this case, the fan-out of the newly created nonter-
minal, ?(X) may be greater than ?(A). Let us con-
sider the left endpoints of the spans of X . Each left
endpoint is either also the left endpoint of a span of
A, or is the right endpoint of some nonterminal not
included in X , that is, one of B1, . . . Br?2. Thus,
?(X) ? ?(A) +
r?2
?
i=1
?(Bi)
and applying this inequality to the definition of c(p2)
we have:
c(p2) = ?(X) + ?(Br?1) + ?(Br?2)
? ?(A) +
r
?
i=1
?(Bi)
= c(p)
For notational convenience, we have defined the
factorization operation as factoring out the last two
nonterminals of a rule; however, the same operation
can be applied to factor out any subset of the orig-
inal nonterminals. The same argument that parsing
complexity cannot increase still applies.
We may apply the factorization operation repeat-
edly until all rules have rank 2; we refer to the re-
sulting grammar as a binarization of the original
LCFRS. The factorization operation may increase
the fan-out of a grammar, but never increases its
parsing complexity. This guarantees that, if we wish
to find the transformation of the original grammar
having the lowest parsing complexity, it is sufficient
to consider only binarizations. This is because any
transformed grammar having more than two nonter-
minals on the righthand side can be binarized with-
out increasing its parsing complexity.
5 The relationship between fan-out and
parsing complexity
G?mez-Rodr?guez et al (2009a) provide an algo-
rithm for finding the binarization of an LCFRS hav-
ing minimal fan-out. The key idea is to search over
ways of combining subsets of a rule?s righthand side
nonterminals such that subsets with low fan-out are
considered first; this results in an algorithm with
complexity polynomial in the rank of the input rule,
with the exponent depending on the resulting mini-
mum fan-out.
This algorithm can be adapted to find the binariza-
tion with minimum parsing complexity, rather than
minimum fan-out. We simply use c(p) rather than
?(p) as the score for new productions, controlling
both which binarizations we prefer and the order in
which they are explored.
An interesting question then arises: does the bina-
rization with minimal parsing complexity also have
minimal fan-out? A binarization into a grammar of
771
A? g(B1, B2, B3, B4)
g(?x1,1, x1,2?, ?x2,1, x2,2, x2,3?, ?x3,1, x3,2, x3,3, x3,4, x3,5?, ?x4,1, x4,2, x4,3?) =
?x4,1x3,1, x2,1, x4,2x1,1x2,2x4,3x3,2x2,3x3,3, x1,2x3,4, x3,5?
Figure 2: A production for which minimizing fan-out and minimizing parsing complexity are mutually exclusive.
{B3}
{B4}
{B3, B4}
{B3, B4}
{B1}
{B1, B3, B4}
{B1, B3, B4}
{B2}
{B1, B2, B3, B4}
Figure 3: The binarization of the rule from Figure 2 that minimizes parsing complexity. In each of the three steps,
we show the spans of each of the two subsets of the rule?s righthand-side nonterms being combined, with the spans of
their union (corresponding to a nonterminal created by the binarization) below.
772
1: function MINIMAL-BINARIZATION(p,?)
2: workingSet? ?;
3: agenda? priorityQueue(?);
4: for i from 1 to ?(p) do
5: workingSet? workingSet ?{Bi};
6: agenda? agenda ?{Bi};
7: while agenda 6= ? do
8: p? ? pop minimum from agenda;
9: if nonterms(p?) = {B1, . . . B?(p)} then
10: return p?;
11: for p1 ? workingSet do
12: p2 ? newProd(p?, p1);
13: find p?2 ? workingSet :
14: nonterms(p?2) = nonterms(p2);
15: if p2 ? p?2 then
16: workingSet? workingSet ?{p2}\{p?2};
17: push(agenda, p2);
Figure 1: Algorithm to compute best binarization accord-
ing to a user-specified ordering ? over productions.
fan-out f ? cannot have parsing complexity higher
than 3f ?, according to eq. 2. Thus, minimizing fan-
out puts an upper bound on parsing complexity, but
is not guaranteed to minimize it absolutely. Bina-
rizations with the same fan-out may in fact vary
in their parsing complexity; similarly binarizations
with the same parsing complexity may vary in their
fan-out. It is not immediately apparent whether, in
order to find a binarization of minimal parsing com-
plexity, it is sufficient to consider only binarizations
of minimal fan-out.
To test this conjecture, we adapted the algorithm
of G?mez-Rodr?guez et al (2009a) to use a prior-
ity queue as the agenda, as shown in Figure 1. The
algorithm takes as an argument an arbitrary partial
ordering relation on productions, and explores pos-
sible binarized rules in the order specified by this re-
lation. In Figure 1, ?workingSet? is a set of single-
ton nonterminals and binarized productions which
are guaranteed to be optimal for the subset of non-
terminals that they cover. The function ?nonterms?
returns, for a newly created production, the subset
of the original nonterminals B1, . . . Br that it gen-
erates, and returns subsets of singleton nonterminals
directly.
To find the binarization with the minimum fan-out
f ? and the lowest parsing complexity among bina-
rizations with fan-out f ?, we use the following com-
parison operation in the binarization algorithm:
p1 ??c p2 iff ?(p1) < ?(p2) ?
(?(p1) = ?(p2) ? c(p1) < c(p2))
guaranteeing that we explore binarizations with
lower fan-out first, and, among binarizations with
equal fan-out, those with lower parsing complexity
first. Similarly, we can search for the binarization
with the lowest parsing complexity c? and the lowest
fan-out among binarizations with complexity c?, we
use
p1 ?c? p2 iff c(p1) < c(p2) ?
(c(p1) = c(p2) ? ?(p1) < ?(p2))
We find that, in fact, it is sometimes necessary to
sacrifice minimum fan-out in order to achieve mini-
mum parsing complexity. An example of an LCFRS
rule for which this is the case is shown in Figure 2.
This production can be binarized to produce a set of
productions with parsing complexity 14 (Figure 3);
among binarizations with this complexity the mini-
mum fan-out is 6. However, an alternative binariza-
tion with fan-out 5 is also possible; among binariza-
tions with this fan-out, the minimum parsing com-
plexity is 15. This binarization (not pictured) first
joins B1 and B2, then adds B4, and finally adds B3.
Given the incompatibility of optimizing time
complexity and fan-out, which corresponds to space
complexity, which should we prefer? In some sit-
uations, it may be desirable to find some trade-off
between the two. It is important to note, however,
that if optimization of space complexity is the sole
objective, factorization is unnecessary, as one can
never improve on the fan-out required by the origi-
nal grammar nonterminals.
6 A Note on Generative Capacity
Rambow and Satta (1999) categorize the genera-
tive capacity of LCFRS grammars according to their
rank and fan-out. In particular, they show that
grammars can be arranged in a two-dimensional
grid, with languages of rank r and fan-out f having
greater generative capacity than both grammars of
rank r and fan-out f ?1 and grammars of rank r?1
773
nmod sbj root vc pp nmod np tmp
A hearing is scheduled on the issue today
nmod? g1 g1 = ?A ?
sbj? g2(nmod, pp) g2(?x1,1?, ?x2,1?) = ?x1,1 hearing , x2,1?
root? g3(sbj, vc) g3(?x1,1, x1,2?, ?x2,1, x2,2?) = ?x1,1 is x2,1x1,2x2,2?
vc? g4(tmp) g4(?x1,1?) = ? scheduled , x1,1?
pp? g5(tmp) g5(?x1,1?) = ? on x1,1?
nmod? g6 g6 = ? the ?
np? g7(nmod) g7(?x1,1?) = ?x1,1 issue ?
tmp? g8 g8 = ? today ?
Figure 4: A dependency tree with the LCFRS rules extracted for each word (Kuhlmann and Satta, 2009).
and fan-out f , with two exceptions: with fan-out 1,
all ranks greater than one are equivalent (context-
free languages), and with fan-out 2, rank 2 and rank
3 are equivalent.
This classification is somewhat unsatisfying be-
cause minor changes to a grammar can change both
its rank and fan-out. In particular, through factor-
izing rules, it is always possible to decrease rank,
potentially at the cost of increasing fan-out, until a
binarized grammar of rank 2 is achieved.
Parsing complexity, as defined above, also pro-
vides a method to compare the generative capacity
of LCFRS grammars. From Rambow and Satta?s
result that grammars of rank two and increasing
fan-out provide an infinite hierarchy of increasing
generative capacity, we see that parsing complexity
also provides such an infinite hierarchy. Compar-
ing grammars according to the parsing complexity
amounts to specifying a normalized binarization for
grammars of arbitrary rank and fan-out, and compar-
ing the resulting binarized grammars. This allows us
to arrange LCFRS grammars into total ordering over
generative capacity, that is a one-dimensional hier-
archy, rather than a two-dimensional grid. It also
gives a way of categorizing generative capacity that
is more closely tied to algorithmic complexity.
It is important to note, however, that parsing com-
plexity as calculated by our algorithm remains a
function of the grammar, rather than an intrinsic
function of the language. One can produce arbitrar-
ily complex grammars that generate the simple lan-
guage a?. Thus the parsing complexity of a gram-
mar, like its rank and fan-out, can be said to catego-
rize its strong generative capacity.
7 Experiments
A number of recent papers have examined dynamic
programming algorithms for parsing non-projective
dependency structures by exploring how well vari-
ous categories of polynomially-parsable grammars
cover the structures found in dependency treebanks
for various languages (Kuhlmann and Nivre, 2006;
G?mez-Rodr?guez et al, 2009b).
Kuhlmann and Satta (2009) give an algorithm for
extracting LCFRS rules from dependency structures.
One rule is extracted for each word in the depen-
dency tree. The rank of the rule is the number of
children that the word has in the dependency tree,
as shown by the example in Figure 4. The fan-out
of the symbol corresponding to a word is the num-
ber of continuous intervals in the sentence formed
by the word and its descendants in the tree. Projec-
774
complexity arabic czech danish dutch german port swedish
20 1
18 1
16 1
15 1
13 1
12 2 3
11 1 1 1
10 2 6 16 3
9 7 4 1
8 4 7 129 65 10
7 3 12 89 30 18
6 178 11 362 1811 492 59
5 48 1132 93 411 1848 172 201
4 250 18269 1026 6678 18124 2643 1736
3 10942 265202 18306 39362 154948 41075 41245
Table 1: Number of productions with specified parsing complexity
tive trees yield LCFRS rules of fan-out one and pars-
ing complexity three, while the fan-out and parsing
complexity from non-projective trees are in princi-
ple unbounded.
Extracting LCFRS rules from treebanks allows us
to study how many of the rules fall within certain
constraints. Kuhlmann and Satta (2009) give an al-
gorithm for binarizing LCRFS rules without increas-
ing the rules? fan-out; however, this is not always
possible, and the algorithm does not succeed even in
some cases for which such a binarization is possible.
Kuhlmann and Satta (2009) find that all but 0.02%
of productions in the CoNLL 2006 training data,
which includes various languages, can be binarized
by their algorithm, but they do not give the fan-out
or parsing complexity of the resulting rules. In re-
lated work, G?mez-Rodr?guez et al (2009b) define
the class of mildly ill-nested dependency structures
of varying gap degrees; gap degree is essentially fan-
out minus one. For a given gap degree k, this class of
grammars can be parsing in time O(n3k+4) for lexi-
calized grammars. G?mez-Rodr?guez et al (2009b)
study dependency treebanks for nine languages and
find that all dependency structures meet the mildly
ill-nested condition in the dependency treebanks for
some gap degree. However, they do not report the
maximum gap degree or parsing complexity.
We extracted LCFRS rules from dependency tree-
banks using the same procedure as Kuhlmann and
Satta (2009), and applied the algorithm of Figure 1
directly to calculate their minimum parsing com-
plexity. This allows us to characterize the pars-
ing complexity of the rules found in the treebank
without needing to define specific conditions on
the rules, such as well-nestedness (Kuhlmann and
Nivre, 2006) or mildly ill-nestedness, that may not
be necessary for all efficiently parsable grammars.
The numbers of rules of different complexities are
shown in Table 1.
As found by previous studies, the vast major-
ity of productions are context-free (projective trees,
parsable in O(n3)). Of non-projective rules, the
vast majority can be parsed in O(n6), including the
well-nested structures of gap degree one defined by
Kuhlmann and Nivre (2006). The single most com-
plex rule had parsing complexity of O(n20), and was
derived from a Swedish sentence which turns out to
be so garbled as to be incomprehensible (taken from
the high school essay portion of the Swedish tree-
bank). It is interesting to note that, while the bina-
rization algorithm is exponential in the worst case, it
is practical for real data: analyzing all the rules ex-
tracted from the various treebanks takes only a few
minutes. We did not find any cases in rules extracted
from Treebank data of rules where minimizing pars-
ing complexity is inconsistent with minimizing fan-
775
out, as is the case for the rule of Figure 2.
8 Conclusion
We give an algorithm for finding the optimum pars-
ing complexity for an LCFRS among grammars ob-
tained by binarization. We find that minimum pars-
ing complexity is always achievable with rank 2, but
is not always achievable with minimum fan-out. By
applying the binarization algorithm to productions
found in dependency treebanks, we can completely
characterize the parsing complexity of the extracted
LCFRS grammar.
Acknowledgments This work was funded by NSF
grants IIS-0546554 and IIS-0910611. We are grate-
ful to Joakim Nivre for assistance with the Swedish
treebank.
References
Carlos G?mez-Rodr?guez, Marco Kuhlmann, Giorgio
Satta, and David Weir. 2009a. Optimal reduction of
rule length in linear conext-free rewriting systems. In
Proceedings of the 2009 Meeting of the North Ameri-
can chapter of the Association for Computational Lin-
guistics (NAACL-09), pages 539?547.
Carlos G?mez-Rodr?guez, David Weir, and John Car-
roll. 2009b. Parsing mildly non-projective depen-
dency structures. In Proceedings of the 12th Confer-
ence of the European Chapter of the ACL (EACL-09),
pages 291?299.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining gram-
mars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, volume 3, pages 69?
124. Springer, Berlin.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In Proceed-
ings of the International Conference on Computational
Linguistics/Association for Computational Linguistics
(COLING/ACL-06), pages 507?514.
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL-09), pages 478?
486.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (HLT/EMNLP).
I. Dan Melamed, Giorgio Satta, and Ben Wellington.
2004. Generalized multitext grammars. In Proceed-
ings of the 42nd Annual Conference of the Association
for Computational Linguistics (ACL-04), Barcelona,
Spain.
Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting sys-
tems. Theor. Comput. Sci., 223(1-2):87?120.
H. Seki, T. Matsumura, M. Fujii, and T. Kasami. 1991.
On multiple context-free grammars. Theoretical Com-
puter Science, 88:191?229.
K. Vijay-Shankar, D. L. Weir, and A. K. Joshi. 1987.
Characterizing structural descriptions produced by
various grammatical formalisms. In Proceedings of
the 25th Annual Conference of the Association for
Computational Linguistics (ACL-87).
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the
complexity of translational equivalence. In Proceed-
ings of the International Conference on Computa-
tional Linguistics/Association for Computational Lin-
guistics (COLING/ACL-06), pages 977?984, Sydney,
Australia.
776
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 543?547,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Tuning as Linear Regression
Marzieh Bazrafshan, Tagyoung Chung and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We propose a tuning method for statistical ma-
chine translation, based on the pairwise rank-
ing approach. Hopkins and May (2011) pre-
sented a method that uses a binary classifier.
In this work, we use linear regression and
show that our approach is as effective as us-
ing a binary classifier and converges faster.
1 Introduction
Since its introduction, the minimum error rate train-
ing (MERT) (Och, 2003) method has been the most
popular method used for parameter tuning in ma-
chine translation. Although MERT has nice proper-
ties such as simplicity, effectiveness and speed, it is
known to not scale well for systems with large num-
bers of features. One alternative that has been used
for large numbers of features is the Margin Infused
Relaxed Algorithm (MIRA) (Chiang et al, 2008).
MIRA works well with a large number of features,
but the optimization problem is much more compli-
cated than MERT. MIRA also involves some modi-
fications to the decoder itself to produce hypotheses
with high scores against gold translations.
Hopkins and May (2011) introduced the method
of pairwise ranking optimization (PRO), which casts
the problem of tuning as a ranking problem be-
tween pairs of translation candidates. The problem
is solved by doing a binary classification between
?correctly ordered? and ?incorrectly ordered? pairs.
Hopkins and May (2011) use the maximum entropy
classifier MegaM (Daume? III, 2004) to do the binary
classification. Their method compares well to the
results of MERT, scales better for high dimensional
feature spaces, and is simpler than MIRA.
In this paper, we use the same idea for tuning, but,
instead of using a classifier, we use linear regression.
Linear regression is simpler than maximum entropy
based methods. The most complex computation that
it needs is a matrix inversion, whereas maximum en-
tropy based classifiers use iterative numerical opti-
mization methods.
We implemented a parameter tuning program
with linear regression and compared the results to
PRO?s results. The results of our experiments are
comparable to PRO, and in many cases (also on av-
erage) we get a better maximum BLEU score. We
also observed that on average, our method reaches
the maximum BLEU score in a smaller number of
iterations.
The contributions of this paper include: First, we
show that linear regression tuning is an effective
method for tuning, and it is comparable to tuning
with a binary maximum entropy classifier. Second,
we show linear regression is faster in terms of the
number of iterations it needs to reach the best re-
sults.
2 Tuning as Ranking
The parameter tuning problem in machine transla-
tion is finding the feature weights of a linear trans-
lation model that maximize the scores of the candi-
date translations measured against reference transla-
tions. Hopkins and May (2011) introduce a tuning
method based on ranking the candidate translation
pairs, where the goal is to learn how to rank pairs of
candidate translations using a gold scoring function.
543
PRO casts the tuning problem as the problem of
ranking pairs of sentences. This method iteratively
generates lists of ?k-best? candidate translations for
each sentence, and tunes the weight vector for those
candidates. MERT finds the weight vector that max-
imizes the score for the highest scored candidate
translations. In contrast, PRO finds the weight vec-
tor which classifies pairs of candidate translations
into ?correctly ordered? and ?incorrectly ordered,?
based on the gold scoring function. While MERT
only considers the highest scored candidate to tune
the weights, PRO uses the entire k-best list to learn
the ranking between the pairs, which can help pre-
vent overfitting.
Let g(e) be a scoring function that maps each
translation candidate e to a number (score) using a
set of reference translations. The most commonly
used gold scoring function in machine translation
is the BLEU score, which is calculated for the en-
tire corpus, rather than for individual sentences. To
use BLEU as our gold scoring function, we need to
modify it to make it decomposable for single sen-
tences. One way to do this is to use a variation of
BLEU called BLEU+1 (Lin and Och, 2004), which
is a smoothed version of the BLEU score.
We assume that our machine translation system
scores translations by using a scoring function which
is a linear combination of the features:
h(e) = wTx(e) (1)
where w is the weight vector and x is the feature vec-
tor. The goal of tuning as ranking is learning weights
such that for every two candidate translations e1 and
e2, the following inequality holds:
g(e1) > g(e2) ? h(e1) > h(e2) (2)
Using Equation 1, we can rewrite Equation 2:
g(e1) > g(e2) ? wT(x(e1) ? x(e2)) > 0 (3)
This problem can be viewed as a binary classifica-
tion problem for learning w, where each data point is
the difference vector between the feature vectors of
a pair of translation candidates, and the target of the
point is the sign of the difference between their gold
scores (BLEU+1). PRO uses the MegaM classifier
to solve this problem. MegaM is a binary maximum
entropy classifier which returns the weight vector
w as a linear classifier. Using this method, Hop-
kins and May (2011) tuned the weight vectors for
various translation systems. The results were close
to MERT?s and MIRA?s results in terms of BLEU
score, and the method was shown to scale well to
high dimensional feature spaces.
3 Linear Regression Tuning
In this paper, we use the same idea as PRO for tun-
ing, but instead of using a maximum entropy clas-
sifier, we use a simple linear regression to estimate
the vector w in Equation 3. We use the least squares
method to estimate the linear regression. For a ma-
trix of data points X, and a target vector g, the
weight vector can be calculated as:
w = (XTX)?1XTg (4)
Adding L2 regularization with parameter ? has the
following closed form solution:
w = (XTX + ?I)?1XTg (5)
Following the sampling method used in PRO, the
matrices X and vector g are prepared as follows:
For each sentence,
1. Generate a list containing the k best transla-
tions of the sentence, with each translation e
scored by the decoder using a function of the
form h(e) = wTx(e).
2. Use the uniform distribution to sample n ran-
dom pairs from the set of candidate transla-
tions.
3. Calculate the gold scores g for the candidates in
each pair using BLEU+1. Keep a pair of can-
didates as a potential pair if the difference be-
tween their g scores is bigger than a threshold
t.
4. From the potential pairs kept in the previous
step, keep the s pairs that have the highest dif-
ferences in g and discard the rest.
5. For each pair e1 and e2 kept in step 4, make two
data points (x(e1)? x(e2), g(e1)? g(e2)) and
(x(e2) ? x(e1), g(e2) ? g(e1)).
544
The rows of X consist of the inputs of the data points
created in step 5, i.e., the difference vectors x(e1)?
x(e2). Similarly, the corresponding rows in g are
the outputs of the data points, i.e., the gold score
differences g(e1) ? g(e2).
One important difference between the linear re-
gression method and PRO is that rather than using
the signs of the gold score differences and doing a
binary classification, we use the differences of the
gold scores directly, which allows us to use the in-
formation about the magnitude of the differences.
4 Experiments
4.1 Setup
We used a Chinese-English parallel corpus with the
English side parsed for our experiments. The cor-
pus consists of 250K sentence pairs, which is 6.3M
words on the English side. The corpus derives from
newswire texts available from LDC.1 We used a 392-
sentence development set with four references for
parameter tuning, and a 428-sentence test set with
four references for testing. They are drawn from the
newswire portion of NIST evaluations (2004, 2005,
2006). The development set and the test set only
had sentences with less than 30 words for decoding
speed.
We extracted a general SCFG (GHKM) grammar
using standard methods (Galley et al, 2004; Wang
et al, 2010) from the parallel corpus with a mod-
ification to preclude any unary rules (Chung et al,
2011). All rules over scope 3 are pruned (Hopkins
and Langmead, 2010). A set of nine standard fea-
tures was used for the experiments, which includes
globally normalized count of rules, lexical weight-
ing (Koehn et al, 2003), and length penalty. Our
in-house decoder was used for experiments with a
trigram language model. The decoder is capable
of both CNF parsing and Earley-style parsing with
cube-pruning (Chiang, 2007).
We implemented linear regression tuning using
1We randomly sampled our data from various differ-
ent sources (LDC2006E86, LDC2006E93, LDC2002E18,
LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26,
LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92,
LDC2006E24, LDC2006E92, LDC2006E24) The language
model is trained on the English side of entire data (1.65M sen-
tences, which is 39.3M words.)
Average of max BLEU Max BLEU
dev test dev test
Regression 27.7 (0.91) 26.4 (0.82) 29.0 27.6
PRO 26.9 (1.05) 25.6 (0.84) 28.0 27.2
Table 1: Average of maximum BLEU scores of the ex-
periments and the maximum BLEU score from the ex-
periments. Numbers in the parentheses indicate standard
of deviations of maximum BLEU scores.
the method explained in Section 3. Following Hop-
kins and May (2011), we used the following param-
eters for the sampling task: For each sentence, the
decoder generates the 1500 best candidate transla-
tions (k = 1500), and the sampler samples 5000
pairs (n = 5000). Each pair is kept as a potential
data point if their BLEU+1 score difference is big-
ger than 0.05 (t = 0.05). Finally, for each sentence,
the sampler keeps the 50 pairs with the highest dif-
ference in BLEU+1 (s = 50) and generates two data
points for each pair.
4.2 Results
We ran eight experiments with random initial weight
vectors and ran each experiment for 25 iterations.
Similar to what PRO does, in each iteration, we lin-
early interpolate the weight vector learned by the re-
gression (w) with the weight vector of the previous
iteration (wt?1) using a factor of 0.1:
wt = 0.1 ? w + 0.9 ? wt?1 (6)
For the sake of comparison, we also implemented
PRO with exactly the same parameters, and ran it
with the same initial weight vectors.
For each initial weight vector, we selected the iter-
ation at which the BLEU score on the development
set is highest, and then decoded using this weight
vector on the test set. The results of our experi-
ments are presented in Table 1. In the first column,
we show the average over the eight initial weight
vectors of the BLEU score achieved, while in the
second column we show the results from the ini-
tial weight vector with the highest BLEU score on
the development set. Thus, while the second col-
umn corresponds to a tuning process where the sin-
gle best result is retained, the first column shows the
expected behavior of the procedure on a single ini-
tial weight vector. The linear regression method has
545
 12
 14
 16
 18
 20
 22
 24
 26
 28
 0  5  10  15  20  25
BL
EU
Iteration
reg-avg
pro-avg
Figure 1: Average of eight runs of regression and PRO.
higher BLEU scores on both development and test
data for both the average over initial weights and the
maximum over initial weights.
Figure 1 shows the average of the BLEU scores
on the development set of eight runs of the experi-
ments. We observe that on average, the linear regres-
sion experiments reach the maximum BLEU score
in a smaller number of iterations. On average, linear
regression reached the maximum BLEU score after
14 iterations and PRO reached the maximum BLEU
score after 20 iterations. One iteration took several
minutes for both of the algorithms. The largest por-
tion of this time is spent on decoding the develop-
ment set and reading in the k-best list. The sampling
phase, which includes performing linear regression
or running MegaM, takes a negligible amount of
time compared to the rest of the operations.
We experimented with adding L2 regularization
to linear regression. As expected, the experiments
with regularization produced lower variance among
the different experiments in terms of the BLEU
score, and the resulting set of the parameters had a
smaller norm. However, because of the small num-
ber of features used in our experiments, regulariza-
tion was not necessary to control overfitting.
5 Discussion
We applied the idea of tuning as ranking and modi-
fied it to use linear regression instead of binary clas-
sification. The results of our experiments show that
tuning as linear regression is as effective as PRO,
and on average it reaches a better BLEU score in a
fewer number of iterations.
In comparison with MERT, PRO and linear re-
gression are different in the sense that the latter two
approaches take into account rankings of the k-best
list, whereas MERT is only concerned with separat-
ing the top 1-best sentence from the rest of the k-
best list. PRO and linear regression are similar in
the sense that both are concerned with ranking the
k-best list. Their difference lies in the fact that PRO
only uses the information on the relative rankings
and uses binary classification to rank the points; on
the contrary, linear regression directly uses the infor-
mation on the magnitude of the differences. This dif-
ference between PRO and linear regression explains
why linear regression converges faster and also may
explain the fact that linear regression achieves a
somewhat higher BLEU score. In this sense, lin-
ear regression is also similar to MIRA since MIRA?s
loss function also uses the information on the magni-
tude of score difference. However, the optimization
problem for linear regression is simpler, does not re-
quire any changes to the decoder, and therefore the
familiar MERT framework can be kept.
Acknowledgments We thank the anonymous re-
viewers for their helpful comments. This work was
supported by NSF grant IIS-0910611.
References
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Conference on Empirical
Methods in Natural Language Processing (EMNLP-
08).
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Tagyoung Chung, Licheng Fang, and Daniel Gildea.
2011. Issues concerning decoding with synchronous
context-free grammar. In Proceedings of the ACL
2011 Conference Short Papers, Portland, Oregon. As-
sociation for Computational Linguistics.
Hal Daume? III. 2004. Notes on CG and LM-BFGS opti-
mization of logistic regression. August.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of the 2004 Meeting of the North American
chapter of the Association for Computational Linguis-
tics (NAACL-04), pages 273?280, Boston.
Mark Hopkins and Greg Langmead. 2010. SCFG decod-
ing without binarization. In Proceedings of the 2010
546
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 646?655, Cambridge, MA,
October. Association for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Meeting of the North American chap-
ter of the Association for Computational Linguistics
(NAACL-03), Edmonton, Alberta.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In Proceedings of Coling 2004,
pages 501?507, Geneva, Switzerland, Aug 23?Aug
27. COLING.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Conference of the Association for Com-
putational Linguistics (ACL-03).
Wei Wang, Jonathan May, Kevin Knight, and Daniel
Marcu. 2010. Re-structuring, re-labeling, and re-
aligning for syntax-based machine translation. Com-
putational Linguistics, 36:247?277, June.
547
Proceedings of NAACL-HLT 2013, pages 32?40,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Simultaneous Word-Morpheme Alignment for
Statistical Machine Translation
Elif Eyigo?z
Computer Science
University of Rochester
Rochester, NY 14627
Daniel Gildea
Computer Science
University of Rochester
Rochester, NY 14627
Kemal Oflazer
Computer Science
Carnegie Mellon University
PO Box 24866, Doha, Qatar
Abstract
Current word alignment models for statisti-
cal machine translation do not address mor-
phology beyond merely splitting words. We
present a two-level alignment model that dis-
tinguishes between words and morphemes, in
which we embed an IBM Model 1 inside an
HMM based word alignment model. The
model jointly induces word and morpheme
alignments using an EM algorithm. We eval-
uated our model on Turkish-English parallel
data. We obtained significant improvement of
BLEU scores over IBM Model 4. Our results
indicate that utilizing information from mor-
phology improves the quality of word align-
ments.
1 Introduction
All current state-of-the-art approaches to SMT rely
on an automatically word-aligned corpus. However,
current alignment models do not take into account
the morpheme, the smallest unit of syntax, beyond
merely splitting words. Since morphology has not
been addressed explicitly in word alignment models,
researchers have resorted to tweaking SMT systems
by manipulating the content and the form of what
should be the so-called ?word?.
Since the word is the smallest unit of translation
from the standpoint of word alignment models, the
central focus of research on translating morphologi-
cally rich languages has been decomposition of mor-
phologically complex words into tokens of the right
granularity and representation for machine transla-
tion. Chung and Gildea (2009) and Naradowsky and
Toutanova (2011) use unsupervised methods to find
word segmentations that create a one-to-one map-
ping of words in both languages. Al-Onaizan et al
(1999), C?mejrek et al (2003), and Goldwater and
McClosky (2005) manipulate morphologically rich
languages by selective lemmatization. Lee (2004)
attempts to learn the probability of deleting or merg-
ing Arabic morphemes for Arabic to English trans-
lation. Niessen and Ney (2000) split German com-
pound nouns, and merge German phrases that cor-
respond to a single English word. Alternatively,
Yeniterzi and Oflazer (2010) manipulate words of
the morphologically poor side of a language pair
to mimic having a morphological structure similar
to the richer side via exploiting syntactic structure,
in order to improve the similarity of words on both
sides of the translation.
We present an alignment model that assumes in-
ternal structure for words, and we can legitimately
talk about words and their morphemes in line with
the linguistic conception of these terms. Our model
avoids the problem of collapsing words and mor-
phemes into one single category. We adopt a two-
level representation of alignment: the first level in-
volves word alignment, the second level involves
morpheme alignment in the scope of a given word
alignment. The model jointly induces word and
morpheme alignments using an EM algorithm.
We develop our model in two stages. Our initial
model is analogous to IBM Model 1: the first level
is a bag of words in a pair of sentences, and the sec-
ond level is a bag of morphemes. In this manner,
we embed one IBM Model 1 in the scope of another
IBM Model 1. At the second stage, by introducing
distortion probabilities at the word level, we develop
an HMM extension of the initial model.
We evaluated the performance of our model on the
32
Turkish-English pair both on hand-aligned data and
by running end-to-end machine translation experi-
ments. To evaluate our results, we created gold word
alignments for 75 Turkish-English sentences. We
obtain significant improvement of AER and BLEU
scores over IBM Model 4. Section 2.1 introduces
the concept of morpheme alignment in terms of its
relation to word alignment. Section 2.2 presents
the derivation of the EM algorithm and Section 3
presents the results of our experiments.
2 Two-level Alignment Model (TAM)
2.1 Morpheme Alignment
Following the standard alignment models of Brown
et al (1993), we assume one-to-many alignment for
both words and morphemes. A word alignment aw
(or only a) is a function mapping a set of word po-
sitions in a source language sentence to a set of
word positions in a target language sentence. A mor-
pheme alignment am is a function mapping a set of
morpheme positions in a source language sentence
to a set of morpheme positions in a target language
sentence. A morpheme position is a pair of integers
(j, k), which defines a word position j and a relative
morpheme position k in the word at position j. The
alignments below are depicted in Figures 1 and 2.
aw(1) = 1 am(2, 1) = (1, 1) aw(2) = 1
Figure 1 shows a word alignment between two sen-
tences. Figure 2 shows the morpheme alignment be-
tween same sentences. We assume that all unaligned
morphemes in a sentence map to a special null mor-
pheme.
A morpheme alignment am and a word alignment
aw are compatible if and only if they satisfy the fol-
lowing conditions: If the morpheme alignment am
maps a morpheme of e to a morpheme of f , then the
word alignment aw maps e to f . If the word align-
ment aw maps e to f , then the morpheme alignment
am maps at least one morpheme of e to a morpheme
of f . If the word alignment aw maps e to null, then
all of its morphemes are mapped to null. In sum, a
morpheme alignment am and a word alignment aw
are compatible if and only if:
? j, k,m, n ? N+, ? s, t ? N+
[am(j, k) = (m,n)? aw(j) = m] ?
[aw(j) = m? am(j, s) = (m, t)] ?
[aw(j) = null? am(j, k) = null] (1)
Please note that, according to this definition of com-
patibility, ?am(j, k) = null? does not necessarily im-
ply ?aw(j) = null?.
A word alignment induces a set of compati-
ble morpheme alignments. However, a morpheme
alignment induces a unique word alignment. There-
fore, if a morpheme alignment am and a word align-
ment aw are compatible, then the word alignment is
aw is recoverable from the morpheme alignment am.
The two-level alignment model (TAM), like
IBM Model 1, defines an alignment between words
of a sentence pair. In addition, it defines a mor-
pheme alignment between the morphemes of a sen-
tence pair.
The problem domain of IBM Model 1 is defined
over alignments between words, which is depicted
as the gray box in Figure 1. In Figure 2, the smaller
boxes embedded inside the main box depict the new
problem domain of TAM. Given the word align-
ments in Figure 1, we are presented with a new
alignment problem defined over their morphemes.
The new alignment problem is constrained by the
given word alignment. We, like IBM Model 1, adopt
a bag-of-morphemes approach to this new problem.
We thus embed one IBM Model 1 into the scope of
another IBM Model 1, and formulate a second-order
interpretation of IBM Model 1.
TAM, like IBM Model 1, assumes that words and
morphemes are translated independently of their
context. The units of translation are both words and
morphemes. Both the word alignment aw and the
morpheme alignment am are hidden variables that
need to be learned from the data using the EM algo-
rithm.
In IBM Model 1, p(e|f), the probability of trans-
lating the sentence f into e with any alignment is
computed by summing over all possible word align-
ments:
p(e|f) =
?
a
p(a, e|f)
33
Figure 1: Word alignment Figure 2: Morpheme alignment
In TAM, the probability of translating the sentence
f into e with any alignment is computed by sum-
ming over all possible word alignments and all pos-
sible morpheme alignments that are compatible with
a given word alignment aw:
p(e|f) =
?
aw
p(aw, e|f)
?
am
p(am, e|aw, f) (2)
where am stands for a morpheme alignment. Since
the morpheme alignment am is in the scope of a
given word alignment aw, am is constrained by aw.
In IBM Model 1, we compute the probability of
translating the sentence f into e by summing over
all possible word alignments between the words of f
and e:
p(e|f) = R(e, f)
|e|?
j=1
|f |?
i=0
t(ej |fi) (3)
where t(ej | fi) is the word translation probability
of ej given fi. R(e, f) substitutes
P (le|lf )
(lf+1)
le for easy
readability.1
In TAM, the probability of translating the sen-
tence f into e is computed as follows:
Word
R(e, f)
|e|?
j=1
|f |?
i=0
(
t(ej |fi)
R(ej , fi)
|ej |?
k=1
|fi|?
n=0
t(ekj |f
n
i )
)
Morpheme
where fni is the n
th morpheme of the word at po-
sition i. The right part of this equation, the con-
tribution of morpheme translation probabilities, is
1le = |e| is the number of words in sentence e and lf = |f |.
in the scope of the left part. In the right part, we
compute the probability of translating the word fi
into the word ej by summing over all possible mor-
pheme alignments between the morphemes of ej and
fi. R(ej , fi) is equivalent to R(e, f) except for the
fact that its domain is not the set of sentences but
the set of words. The length of words ej and fi in
R(ej , fi) are the number of morphemes of ej and fi.
The left part, the contribution of word transla-
tion probabilities alone, equals Eqn. 3. Therefore,
canceling the contribution of morpheme translation
probabilities reduces TAM to IBM Model 1. In
our experiments, we call this reduced version of
TAM ?word-only? (IBM). TAM with the contribu-
tion of both word and morpheme translation proba-
bilities, as the equation above, is called ?word-and-
morpheme?. Finally, we also cancel out the con-
tribution of word translation probabilities, which is
called ?morpheme-only?. In the ?morpheme-only?
version of TAM, t(ej |fi) equals 1. Bellow is the
equation of p(e|f) in the morpheme-only model.
p(e|f) =
R(e, f)
|e|?
j=1
|f |?
i=0
|ej |?
k=1
|fi|?
n=0
R(ej , fi)t(e
k
j |f
n
i ) (4)
Please note that, although this version of the two-
level alignment model does not use word translation
probabilities, it is also a word-aware model, as mor-
pheme alignments are restricted to correspond to a
valid word alignment according to Eqn. 1. When
presented with words that exhibit no morphology,
the morpheme-only version of TAM is equivalent to
IBM Model 1, as every single-morpheme word is it-
self a morpheme.
Deficiency and Non-Deficiency of TAM We
present two versions of TAM, the word-and-
34
morpheme and the morpheme-only versions. The
word-and-morpheme version of the model is defi-
cient whereas the morpheme-only model is not.
The word-and-morpheme version is deficient, be-
cause some probability is allocated to cases where
the morphemes generated by the morpheme model
do not match the words generated by the word
model. Moreover, although most languages exhibit
morphology to some extent, they can be input to the
algorithm without morpheme boundaries. This also
causes deficiency in the word-and-morpheme ver-
sion, as single morpheme words are generated twice,
as a word and as a morpheme.
Nevertheless, we observed that the deficient ver-
sion of TAM can perform as good as the non-
deficient version of TAM, and sometimes performs
better. This is not surprising, as deficient word align-
ment models such as IBM Model 3 or discriminative
word alignment models work well in practice.
Goldwater and McClosky (2005) proposed a mor-
pheme aware word alignment model for language
pairs in which the source language words corre-
spond to only one morpheme. Their word alignment
model is:
P (e|f) =
K?
k=0
P (ek|f)
where ek is the kth morpheme of the word e. The
morpheme-only version of our model is a general-
ization of this model. However, there are major dif-
ferences in their and our implementation and exper-
imentation. Their model assumes a fixed number of
possible morphemes associated with any stem in the
language, and if the morpheme ek is not present, it
is assigned a null value.
The null word on the source side is also a null
morpheme, since every single morpheme word is it-
self a morpheme. In TAM, the null word is the null
morpheme that all unaligned morphemes align to.
2.2 Second-Order Counts
In TAM, we collect counts for both word translations
and morpheme translations. Unlike IBM Model 1,
R(e, f) = P (le|lf )
(lf+1)
le does not cancel out in the counts
of TAM. To compute the conditional probability
P (le|lf ), we assume that the length of word e (the
number of morphemes of word e) varies according
to a Poisson distribution with a mean that is linear
with length of the word f .
P (le|lf ) = FPoisson(le, r ? lf )
=
exp(?r ? lf )(r ? lf )le
le!
FPoisson(le, r ? lf ) expresses the probability that there
are le morphemes in e if the expected number of
morphemes in e is r ? lf , where r =
E[le]
E[lf ]
is the rate
parameter. Since lf is undefined for null words, we
omit R(e, f) for null words.
We introduce T (e|f), the translation probability
of e given f with all possible morpheme alignments,
as it will occur frequently in the counts of TAM:
T (e|f) = t(e|f)R(e, f)
|e|?
k=1
|f |?
n=0
t(ek|fn)
The role of T (e|f) in TAM is very similar to the
role of t(e|f) in IBM Model 1. In finding the Viterbi
alignments, we do not take max over the values in
the summation in T (e|f).
2.2.1 Word Counts
Similar to IBM Model 1, we collect counts for
word translations over all possible alignments,
weighted by their probability. In Eqn. 5, the count
function collects evidence from a sentence pair
(e, f) as follows: For all words ej of the sentence e
and for all word alignments aw(j), we collect counts
for a particular input word f and an output word e
iff ej = e and faw(j) = f .
cw(e|f ; e, f , aw) =
?
1?j?|e|
s.t.
e=ej
f=faw(j)
T (e|f)
|f |?
i=0
T (e|fi)
(5)
2.2.2 Morpheme Counts
As for morpheme translations, we collect counts
over all possible word and morpheme alignments,
weighted by their probability. The morpheme count
function below collects evidence from a word pair
(e, f) in a sentence pair (e, f) as follows: For all
words ej of the sentence e and for all word align-
ments aw(j), for all morphemes ekj of the word ej
and for all morpheme alignments am(j, k), we col-
lect counts for a particular input morpheme g and an
35
output morpheme h iff ej = e and faw(j) = f and
h = ekj and g = fam(j,k).
cm(h|g; e, f , aw, am) =
?
1?j?|e|
s.t.
e=ej
f=faw(j)
?
1?k?|e|
s.t.
h=ekj
g=fam(j,k)
T (e|f)
|f |?
i=0
T (e|fi)
t(h|g)
|f |?
i=1
t(h|f i)
The left part of the morpheme count function is the
same as the word-counts in Eqn. 5. Since it does not
contain h or g, it needs to be computed only once for
each word. The right part of the equation is familiar
from the IBM Model 1 counts.
2.3 HMM Extension
We implemented TAM with the HMM extension
(Vogel et al, 1996) at the word level. We redefine
p(e|f) as follows:
R(e, f)
?
aw
|e|?
j=1
(
p(s(j ) |C (faw (j?1 ))) t(ej |faw(j))
R(ej , faw(j))
?
am
|ej |?
k=1
t(ekj |fam(j,k))
)
where the distortion probability depends on the rel-
ative jump width s(j) = aw(j ? 1) ? aw(j),
as opposed to absolute positions. The distortion
probability is conditioned on class of the previous
aligned word C (faw(j?1)). We used the mkcls
tool in GIZA (Och and Ney, 2003) to learn the word
classes.
We formulated the HMM extension of TAM only
at the word level. Nevertheless, the morpheme-only
version of TAM also has an HMM extension, as it
is also a word-aware model. To obtain the HMM
extension of the morpheme-only version, substitute
t(ej |faw(j)) with 1 in the equation above.
For the HMM to work correctly, we must han-
dle jumping to and jumping from null positions. We
learn the probabilities of jumping to a null position
from the data. To compute the jump probability from
a null position, we keep track of the nearest previous
source word that does not align to null, and use the
position of the previous non-null word to calculate
the jump width. For this reason, we use a total of
2lf ? 1 words for the HMM model, the positions
> lf stand for null positions between the words of f
(Och and Ney, 2003). We do not allow null to null
jumps. In sum, we enforce the following constraints:
P (i+ lf + 1|i
?) = p(null|i?)
P (i+ lf + 1|i
? + lf + 1) = 0
P (i|i? + lf + 1) = p(i|i
?)
In the HMM extension of TAM, we perform
forward-backward training using the word counts in
Eqn. 5 as the emission probabilities. We calculate
the posterior word translation probabilities for each
ej and fi such that 1 ? j ? le and 1 ? i ? 2lf ? 1
as follows:
?j(i) =
?j(i)?j(i)
2lf?1?
m=1
?j(m)?j(m)
where ? is the forward and ? is the backward prob-
abilities of the HMM. The HMM word counts, in
turn, are the posterior word translation probabilities
obtained from the forward-backward training:
cw(e|f ; e, f , aw) =
?
1?j?|e|
s.t.
e=ej
f=faw(j)
?j(aw(j))
Likewise, we use the posterior probabilities in HMM
morpheme counts:
cm(h|g; e, f , aw, am) =
?
1?j?|e|
s.t.
e=ej
f=faw(j)
?
1?k?|e|
s.t.
h=ekj
g=fam(j,k)
?j(aw(j))
t(h|g)
|f |?
i=1
t(h|f i)
The complexity of the HMM extension of TAM is
O(n3m2), where n is the number of words, and m
is the number of morphemes per word.
2.4 Variational Bayes
Moore (2004) showed that the EM algorithm is par-
ticularly susceptible to overfitting in the case of rare
words when training IBM Model 1. In order to pre-
vent overfitting, we use the Variational Bayes ex-
tension of the EM algorithm (Beal, 2003). This
36
(a) Kas?m 1996?da, Tu?rk makamlar?, I?c?is?leri Bakanl?g?? bu?nyesinde bir kay?p kis?ileri arama birimi olus?turdu.
(b) Kas?m+Noun 1996+Num?Loc ,+Punc Tu?rk+Noun makam+Noun?A3pl?P3sg ,+Punc I?c?is?i+Noun?A3pl?
P3sg Bakanl?k+Noun?P3sg bu?nye+Noun?P3sg?Loc bir+Det kay?p+Adj kis?i+Noun?A3pl?Acc ara+Verb?
Inf2 birim+Noun?P3sg olus?+Verb?Caus?Past .+Punc
(c) In November 1996 the Turkish authorities set up a missing persons search unit within the Ministry of the
Interior.
(d) in+IN November+NNP 1996+CD the+DT Turkish+JJ author+NN?ity+N|N.?NNS set+VB?VBD up+RP
a+DT miss+VB?VBG+JJ person+NN?NNS search+NN unit+NN within+IN the+DT minister+NN?
y+N|N. of+IN the+DT interior+NN .+.
(e) In+IN November+NNP 1996+CD the+DT Turkish+JJ authorities+NNS set+VBD up+RP a+DT missing+JJ
persons+NNS search+NN unit+NN within+IN the+DT Ministry+NNP of+IN the+DT Interior+NNP .+.
Figure 3: Turkish-English data examples
amounts to a small change to the M step of the orig-
inal EM algorithm. We introduce Dirichlet priors ?
to perform an inexact normalization by applying the
function f(v) = exp(?(v)) to the expected counts
collected in the E step, where ? is the digamma
function (Johnson, 2007).
?x|y =
f(E[c(x|y)] + ?)
f(
?
j E[c(xj |y)] + ?)
We set ? to 10?20, a very low value, to have the ef-
fect of anti-smoothing, as low values of ? cause the
algorithm to favor words which co-occur frequently
and to penalize words that co-occur rarely.
3 Experimental Setup
3.1 Data
We trained our model on a Turkish-English paral-
lel corpus of approximately 50K sentences, which
have a maximum of 80 morphemes. Our parallel
data consists mainly of documents in international
relations and legal documents from sources such as
the Turkish Ministry of Foreign Affairs, EU, etc. We
followed a heavily supervised approach in morpho-
logical analysis. The Turkish data was first morpho-
logically parsed (Oflazer, 1994), then disambiguated
(Sak et al, 2007) to select the contextually salient in-
terpretation of words. In addition, we removed mor-
phological features that are not explicitly marked by
an overt morpheme ? thus each feature symbol be-
yond the root part-of-speech corresponds to a mor-
pheme. Line (b) of Figure 3 shows an example of
a segmented Turkish sentence. The root is followed
by its part-of-speech tag separated by a ?+?. The
derivational and inflectional morphemes that follow
the root are separated by ???s. In all experiments,
we used the same segmented version of the Turkish
data, because Turkish is an agglutinative language.
For English, we used the CELEX database
(Baayen et al, 1995) to segment English words into
morphemes. We created two versions of the data:
a segmented version that involves both derivational
and inflectional morphology, and an unsegmented
POS tagged version. The CELEX database provides
tags for English derivational morphemes, which in-
dicate their function: the part-of-speech category the
morpheme attaches to and the part-of-speech cate-
gory it returns. For example, in ?sparse+ity? = ?spar-
sity?, the morpheme -ity attaches to an adjective to
the right and returns a noun. This behavior is repre-
sented as ?N|A.? in CELEX, where ?.? indicates the
attachment position. We used these tags in addition
to the surface forms of the English morphemes, in
order to disambiguate multiple functions of a single
surface morpheme.
The English sentence in line (d) of Figure 3 ex-
hibits both derivational and inflectional morphology.
For example, ?author+ity+s?=?authorities? has both
an inflectional suffix -s and a derivational suffix -ity,
whereas ?person+s? has only an inflectional suffix -s.
For both English and Turkish data, the dashes in
Figure 3 stand for morpheme boundaries, therefore
the strings between the dashes are treated as indi-
37
Words Morphemes
Tokens Types Tokens Types
English Der+Inf 1,033,726 27,758 1,368,188 19,448
English POS 1,033,726 28,647 1,033,726 28,647
Turkish Der+Inf 812,374 57,249 1,484,673 16,713
Table 1: Data statistics
visible units. Table 1 shows the number of words,
the number of morphemes and the respective vocab-
ulary sizes. The average number of morphemes in
segmented Turkish words is 2.69, and the average
length of segmented English words is 1.57.
3.2 Experiments
We initialized our baseline word-only model with 5
iterations of IBM Model 1, and further trained the
HMM extension (Vogel et al, 1996) for 5 iterations.
We call this model ?baseline HMM? in the discus-
sions. Similarly, we initialized the two versions of
TAM with 5 iterations of the model explained in
Section 2.2, and then trained the HMM extension of
it as explained in Section 2.3 for 5 iterations.
To obtain BLEU scores for TAM models and
our implementation of the word-only model, i.e.
baseline-HMM, we bypassed GIZA++ in the Moses
toolkit (Och and Ney, 2003). We also ran GIZA++
(IBM Model 1?4) on the data. We translated 1000
sentence test sets.
4 Results and Discussion
We evaluated the performance of our model in two
different ways. First, we evaluated against gold
word alignments for 75 Turkish-English sentences.
Second, we used the word Viterbi alignments of our
algorithm to obtain BLEU scores.
Table 2 shows the AER (Och and Ney, 2003) of
the word alignments of the Turkish-English pair and
the translation performance of the word alignments
learned by our models. We report the grow-diag-
final (Koehn et al, 2003) of the Viterbi alignments.
In Table 2, results obtained with different versions
of the English data are represented as follows: ?Der?
stands for derivational morphology, ?Inf? for inflec-
tional morphology, and ?POS? for part-of-speech
tags. ?Der+Inf? corresponds to the example sen-
tence in line (d) in Figure 3, and ?POS? to line (e).
?DIR? stands for models with Dirichlet priors, and
?NO DIR? stands for models without Dirichlet pri-
ors. All reported results are of the HMM extension
of respective models.
Table 2 shows that using Dirichlet priors hurts
the AER performance of the word-and-morpheme
model in all experiment settings, and benefits the
morpheme-only model in the POS tagged experi-
ment settings.
In order to reduce the effect of nondeterminism,
we run Moses three times per experiment setting,
and report the highest BLEU scores obtained. Since
the BLEU scores we obtained are close, we did a sig-
nificance test on the scores (Koehn, 2004). Table 2
visualizes the partition of the BLEU scores into sta-
tistical significance groups. If two scores within the
same column have the same background color, or the
border between their cells is removed, then the dif-
ference between their scores is not statistically sig-
nificant. For example, the best BLEU scores, which
are in bold, have white background. All scores in a
given experiment setting without white background
are significantly worse than the best score in that ex-
periment setting, unless there is no border separating
them from the best score.
In all experiment settings, the TAM Models per-
form better than the baseline-HMM. Our experi-
ments showed that the baseline-HMM benefits from
Dirichlet priors to a larger extent than the TAM mod-
els. Dirichlet priors help reduce the overfitting in
the case of rare words. The size of the word vo-
cabulary is larger than the size of the morpheme
vocabulary. Therefore the number of rare words is
larger for words than it is for morphemes. Conse-
quently, baseline-HMM, using only the word vocab-
38
                                                
BLEU
EN to TR
BLEU
TR to EN
AER
Der+Inf POS Der+Inf POS Der+Inf POS
NO
 
DIR
TAM
Morph only 22.57 22.54 29.30 29.45 0.293 0.276
Word & Morph 21.95 22.37 28.81 29.01 0.286 0.282
WORD
IBM 4 21.82 21.82 27.91 27.91 0.357 0.370
Base-HMM 21.78 21.38 28.22 28.02 0.381 0.375
IBM 4 Morph 17.15 17.94 25.70 26.33 N/A N/A
DIR
TAM
Morph only 22.18 22.52 29.32 29.98 0.304 0.256
Word & Morph 22.43 21.62 29.21 29.11 0.338 0.317
WORD
IBM 4 21.82 21.82 27.91 27.91 0.357 0.370
Base-HMM 21.69 21.95 28.76 29.13 0.381 0.377
IBM 4 Morph 17.15 17.94 25.70 26.33 N/A N/A
Table 2: AER and BLEU Scores
ulary, benefits from the use of Dirichlet priors more
than the TAM models.
In four out of eight experiment settings, the
morpheme-only model performs better than the
word-and-morpheme version of TAM. However,
please note that our extensive experimentation
with TAM models revealed that the superiority
of the morpheme-only model over the word-and-
morpheme model is highly dependent on segmenta-
tion accuracy, degree of segmentation, and morpho-
logical richness of languages.
Finally, we treated morphemes as words and
trained IBM Model 4 on the morpheme segmented
versions of the data. To obtain BLEU scores, we
had to unsegment the translation output: we con-
catenated the prefixes to the morpheme to the right,
and suffixes to the morpheme to the left. Since this
process creates malformed words, the BLEU scores
obtained are much lower than the scores obtained by
IBM Model 4, the baseline and the TAM Models.
5 Conclusion
We presented two versions of a two-level alignment
model for morphologically rich languages. We ob-
served that information provided by word transla-
tions and morpheme translations interact in a way
that enables the model to be receptive to the par-
tial information in rarely occurring words through
their frequently occurring morphemes. We obtained
significant improvement of BLEU scores over IBM
Model 4. In conclusion, morphologically aware
word alignment models prove to be superior to their
word-only counterparts.
Acknowledgments Funded by NSF award IIS-
0910611. Kemal Oflazer acknowledges the gen-
erous support of the Qatar Foundation through
Carnegie Mellon University?s Seed Research pro-
gram. The statements made herein are solely the
responsibility of this author(s), and not necessarily
that of Qatar Foundation.
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David
Yarowsky. 1999. Statistical machine translation.
39
Technical report, Final Report, JHU Summer Work-
shop.
R.H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The CELEX Lexical Database (Release 2) [CD-ROM].
Linguistic Data Consortium, University of Pennsylva-
nia [Distributor], Philadelphia, PA.
Matthew J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Univer-
sity College London.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Tagyoung Chung and Daniel Gildea. 2009. Unsu-
pervised tokenization for machine translation. In
EMNLP, pages 718?726.
Martin C?mejrek, Jan Cur???n, and Jir??? Havelka. 2003.
Czech-English dependency-based machine transla-
tion. In EACL, pages 83?90, Morristown, NJ, USA.
Association for Computational Linguistics.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
HLT-EMNLP.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In EMNLP-CoNLL, pages 296?305,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388?395.
Young-suk Lee. 2004. Morphological analysis for statis-
tical machine translation. In HLT-NAACL, pages 57?
60.
Robert C. Moore. 2004. Improving IBM word alignment
model 1. In ACL, pages 518?525, Barcelona, Spain,
July.
Jason Naradowsky and Kristina Toutanova. 2011. Unsu-
pervised bilingual morpheme segmentation and align-
ment with context-rich Hidden Semi-Markov Models.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 895?904, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Sonja Niessen and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In Computa-
tional Linguistics, pages 1081?1085, Morristown, NJ,
USA. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kemal Oflazer. 1994. Two-level description of Turkish
morphology. Literary and Linguistic Computing, 9(2).
Has?im Sak, Tunga Gu?ngo?r, and Murat Sarac?lar. 2007.
Morphological disambiguation of Turkish text with
perceptron algorithm. In CICLing, pages 107?118,
Berlin, Heidelberg. Springer-Verlag.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In COLING, pages 836?841.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based statis-
tical machine translation from English to Turkish. In
ACL, pages 454?464, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
40
Proceedings of NAACL-HLT 2013, pages 201?210,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Text Alignment for Real-Time Crowd Captioning
Iftekhar Naim, Daniel Gildea, Walter Lasecki and Jeffrey P. Bigham
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
The primary way of providing real-time cap-
tioning for deaf and hard of hearing people
is to employ expensive professional stenogra-
phers who can type as fast as natural speak-
ing rates. Recent work has shown that a
feasible alternative is to combine the partial
captions of ordinary typists, each of whom
types part of what they hear. In this paper,
we describe an improved method for combin-
ing partial captions into a final output based
on weighted A? search and multiple sequence
alignment (MSA). In contrast to prior work,
our method allows the tradeoff between accu-
racy and speed to be tuned, and provides for-
mal error bounds. Our method outperforms
the current state-of-the-art on Word Error Rate
(WER) (29.6%), BLEU Score (41.4%), and
F-measure (36.9%). The end goal is for
these captions to be used by people, and so
we also compare how these metrics correlate
with the judgments of 50 study participants,
which may assist others looking to make fur-
ther progress on this problem.
1 Introduction
Real-time captioning provides deaf or hard of hear-
ing people access to speech in mainstream class-
rooms, at public events, and on live television. To
maintain consistency between the captions being
read and other visual cues, the latency between when
a word was said and when it is displayed must be
under five seconds. The most common approach to
real-time captioning is to recruit a trained stenogra-
pher with a special purpose phonetic keyboard, who
transcribes the speech to text within approximately 5
seconds. Unfortunately, professional captionists are
quite expensive ($150 per hour), must be recruited in
blocks of an hour or more, and are difficult to sched-
ule on short notice. Automatic speech recognition
(ASR) (Saraclar et al, 2002) attempts to solve this
TXLFNIR[OD]\GRJ
&RPELQHUWKHEURZQIR[MXPSHG
IR[MXPSHGRYHUWKHOD]\
WKHTXLFNEURZQIR[MXPSHGRYHUWKHOD]\GRJ
)LQDO&DSWLRQ
0HUJLQJ,QFRPSOHWH&DSWLRQV
&
&
&
Figure 1: General layout of crowd captioning systems.
Captionists (C1, C2, C3) submit partial captions that are
automatically combined into a high-quality output.
problem by converting speech to text completely au-
tomatically. However, the accuracy of ASR quickly
plummets to below 30% when used on an untrained
speaker?s voice, in a new environment, or in the ab-
sence of a high quality microphone (Wald, 2006b).
An alternative approach is to combine the efforts
of multiple non-expert captionists (anyone who can
type) (Lasecki et al, 2012; Lasecki and Bigham,
2012; Lasecki et al, 2013). In this approach, mul-
tiple non-expert human workers transcribe an audio
stream containing speech in real-time, and their par-
tial input is combined to produce a final transcript
(see Figure 1). This approach has been shown to
dramatically outperform ASR in terms of both accu-
racy and Word Error Rate (WER), even when us-
ing captionists drawn from Amazon?s Mechanical
Turk. Furthermore, recall approached and even ex-
ceeded that of a trained expert stenographer with
seven workers contributing, suggesting that the in-
formation is present to meet the performance of a
stenographer. However, combining these captions
involves real-time alignment of partial captions that
may be incomplete and that often have spelling er-
rors and inconsistent timestamps. In this paper,
we present a more accurate combiner that leverages
201
Multiple Sequence Alignment (MSA) and Natural
Language Processing to improve performance.
Gauging the quality of captions is not easy. Al-
though word error rate (WER) is commonly used in
speech recognition, it considers accuracy and com-
pleteness, not readability. As a result, a lower WER
does not always result in better understanding (Wang
et al, 2003). We compare WER with two other com-
monly used metrics: BLEU (Papineni et al, 2002)
and F-measure (Melamed et al, 2003), and report
their correlation with that of 50 human evaluators.
The key contributions of this paper are as follows:
? We have implemented an A?-search based Mul-
tiple Sequence Alignment algorithm (Lermen
and Reinert, 2000) that can trade-off speed and
accuracy by varying the heuristic weight and
chunk-size parameters. We show that it outper-
forms previous approaches in terms of WER,
BLEU score, and F-measure.
? We propose a beam-search based technique us-
ing the timing information of the captions that
helps to restrict the search space and scales ef-
fectively to align longer sequences efficiently.
? We evaluate the correlation of WER, BLEU,
and F-measure with 50 human ratings of cap-
tion readability, and found that WER was more
highly correlated than BLEU score (Papineni
et al, 2002), implying it may be a more useful
metric overall when evaluating captions.
2 Related Work
Most of the previous research on real-time caption-
ing has focused on Automated Speech Recognition
(ASR) (Saraclar et al, 2002; Cooke et al, 2001;
Praz?a?k et al, 2012). However, experiments show
that ASR systems are not robust enough to be ap-
plied for arbitrary speakers and in noisy environ-
ments (Wald, 2006b; Wald, 2006a; Bain et al, 2005;
Bain et al, 2012; Cooke et al, 2001).
2.1 Crowd Captioning
To address these limitations of ASR-based tech-
niques, the Scribe system collects partial captions
from the crowd and then uses a graph-based in-
cremental algorithm to combine them on the fly
(Lasecki et al, 2012). The system incrementally
builds a chain graph, where each node represents a
set of equivalent words entered by the workers and
the link between nodes are adjusted according to the
order of the input words. A greedy search is per-
formed to identify the path with the highest confi-
dence, based on worker input and an n-gram lan-
guage model. The algorithm is designed to be used
online, and hence has high speed and low latency.
However, due to the incremental nature of the algo-
rithm and due to the lack of a principled objective
function, it is not guaranteed to find the globally op-
timal alignment for the captions.
2.2 Multiple Sequence Alignment
The problem of aligning and combining multiple
transcripts can be mapped to the well-studied Mul-
tiple Sequence Alignment (MSA) problem (Edgar
and Batzoglou, 2006). MSA is an important prob-
lem in computational biology (Durbin et al, 1998).
The goal is to find an optimal alignment from a
given set of biological sequences. The pairwise
alignment problem can be solved efficiently using
dynamic programming in O(N2) time and space,
where N is the sequence length. The complexity of
the MSA problem grows exponentially as the num-
ber of sequences grows, and has been shown to be
NP-complete (Wang and Jiang, 1994). Therefore,
it is important to apply some heuristic to perform
MSA in a reasonable amount of time.
Most MSA algorithms for biological sequences
follow a progressive alignment strategy that first per-
forms pairwise alignment among the sequences, and
then builds a guide tree based on the pairwise simi-
larity between these sequences (Edgar, 2004; Do et
al., 2005; Thompson et al, 1994). Finally, the input
sequences are aligned according to the order spec-
ified by the guide tree. While not commonly used
for biological sequences, MSA with A?-style search
has been applied to these problems by Horton (1997)
and Lermen and Reinert (2000).
Lasecki et al explored MSA in the context of
merging partial captions by using the off-the-shelf
MSA tool MUSCLE (Edgar, 2004), replacing the nu-
cleotide characters by English characters (Lasecki
et al, 2012). The substitution cost for nucleotides
was replaced by the ?keyboard distance? between
English characters, learned from the physical lay-
out of a keyboard and based on common spelling
202
errors. However, MUSCLE relies on a progressive
alignment strategy and may result in suboptimal so-
lutions. Moreover, it uses characters as atomic sym-
bols instead of words. Our approach operates on a
per-word basis and is able to arrive at a solution that
is within a selectable error-bound of optimal.
3 Multiple Sequence Alignment
We start with an overview of the MSA problem us-
ing standard notations as described by Lermen and
Reinert (2000). Let S1, . . . , SK ,K ? 2, be the K
sequences over an alphabet ?, and having length
N1, . . . , NK . The special gap symbol is denoted by
??? and does not belong to ?. Let A = (aij) be a
K ? Nf matrix, where aij ? ? ? {?}, and the ith
row has exactly (Nf ? Ni) gaps and is identical to
Si if we ignore the gaps. Every column of A must
have at least one non-gap symbol. Therefore, the jth
column of A indicates an alignment state for the jth
position, where the state can have one of the 2K ? 1
possible combinations. Our goal is to find the op-
timum alignment matrix AOPT that minimizes the
sum of pairs (SOP) cost function:
c(A) =
?
1?i?j?K
c(Aij) (1)
where c(Aij) is the cost of the pairwise alignment
between Si and Sj according to A. Formally,
c(Aij) =
?Nf
l=1 sub(ail, ajl), where sub(ail, ajl)
denotes the cost of substituting ajl for ail. If ail
and ajl are identical, the substitution cost is usu-
ally zero. For the caption alignment task, we treat
each individual word as a symbol in our alphabet
?. The substitution cost for two words is estimated
based on the edit distance between two words. The
exact solution to the SOP optimization problem is
NP-Complete, but many methods solve it approxi-
mately. In this paper, we adapt weighted A? search
for approximately solving the MSA problem.
3.1 A? Search for MSA
The problem of minimizing the SOP cost func-
tion for K sequences is equivalent to estimating the
shortest path between a single source and single sink
node in a K-dimensional lattice. The total num-
ber of nodes in the lattice is (N1 + 1) ? (N2 +
Algorithm 1 MSA-A? Algorithm
Require: K input sequences S = {S1, . . . , SK} having
length N1, . . . , NK , heuristic weight w, beam size b
1: start? 0K , goal? [N1, . . . , NK ]
2: g(start)? 0, f(start)? w ? h(start).
3: Q? {start}
4: while Q 6= ? do
5: n? EXTRACT-MIN(Q)
6: for all s ? {0, 1}K ? {0K} do
7: ni ? n + s
8: if ni = goal then
9: Return the alignment matrix for the reconstructed
path from start to ni
10: else if ni 6? Beam(b) then
11: continue;
12: else
13: g(ni)? g(n) + c(n, ni)
14: f(ni)? g(ni) + w ? h(ni)
15: INSERT-ITEM(Q, ni, f(ni))
16: end if
17: end for
18: end while
1) ? ? ? ? ? (NK + 1), each corresponding to a dis-
tinct position in K sequences. The source node is
[0, . . . , 0] and the sink node is [N1, . . . , NK ]. The
dynamic programming algorithm for estimating the
shortest path from source to sink treats each node
position [n1, . . . , nK ] as a state and calculates a ma-
trix that has one entry for each node. Assuming the
sequences have roughly same length N , the size of
the dynamic programming matrix is O(NK). At
each vertex, we need to minimize the cost over all
its 2K ? 1 predecessor nodes, and, for each such
transition, we need to estimate the SOP objective
function that requires O(K2) operations. Therefore,
the dynamic programming algorithm has time com-
plexity of O(K22KNK) and space complexity of
O(NK), which is infeasible for most practical prob-
lem instances. However, we can efficiently solve it
via heuristic A? search (Lermen and Reinert, 2000).
We use A? search based MSA (shown in Algo-
rithm 1, illustrated in Figure 2) that uses a prior-
ity queue Q to store dynamic programming states
corresponding to node positions in the K dimen-
sional lattice. Let n = [n1, . . . , nK ] be any node
in the lattice, s be the source, and t be the sink. The
A? search can find the shortest path using a greedy
Best First Search according to an evaluation func-
tion f(n), which is the summation of the cost func-
203

Q Q 7
&
&
&
WKH TXLFN EURZQ IR[ MXPSHG RYHU WKH OD]\ GRJ
WKH EURZQ IR[ MXPSHG
TXLFN IR[ OD]\ GRJ
IR[ MXPSHG RYHU WKH OD]\ BBBBBBBB BBBB
BBBBBBBBBBBBBBBB
BBBBBBBBBBBBBB
WKH EURZQ IR[ MXPSHG
TXLFN IR[ OD]\ GRJ
IR[ MXPSHG RYHU WKH OD]\

WKH EURZQ IR[ MXPSHG
TXLFN IR[ OD]\ GRJ
IR[ MXPSHG RYHU WKH OD]\BBBBBB BBBB
BBBBBB
BBBB WKH EURZQ IR[ MXPSHG
TXLFN IR[ OD]\ GRJ
IR[ MXPSHG RYHU WKH OD]\BBBBBB BBBB
BBBBBBBBBBB
BBBB WKH EURZQ IR[ MXPSHG
TXLFN IR[ OD]\ GRJ
IR[ MXPSHG RYHU WKH OD]\ BBBBBBBB BBBB
BBBBBBBBBBBBBBBB
BBBBBBBBBBBBBB
L
BMXPSHG MXPSHG GRJ
BB
BB
IR[IR[
IR[6

 N
BBBB
RYHU
OD]\
BB
RYHU

BBBB
BB
&DSWLRQ
&DSWLRQ
&DSWLRQ
OD]\BB BB
Figure 2: A? MSA search algorithm. Each branch is one of 2K ? 1 possible alignments for the current input. The
branch with minimum sum of the current alignment cost and the expected heuristic value hpair (precomputed).
tions g(n) and the heuristic function h(n) for node
n. The cost function g(n) denotes the cost of the
shortest path from the source s to the current node
n. The heuristic function h(n) is the approximate
estimated cost of the shortest path from n to the des-
tination t. At each step of the A? search algorithm,
we extract the node with the smallest f(n) value
from the priority queue Q and expand it by one edge.
The heuristic function h(n) is admissible if it never
overestimates the cost of the cheapest solution from
n to the destination. An admissible heuristic func-
tion guarantees that A? will explore the minimum
number of nodes and will always find the optimal
solution. One commonly used admissible heuristic
function is hpair(n):
hpair(n) = L(n ? t) =
?
1?i<j?K
c(A?p(?ni , ?nj ))
(2)
where L(n ? t) denotes the lower bound on the
cost of the shortest path from n to destination t, A?p
is the optimal pairwise alignment, and ?ni is the suf-
fix of node n in the i-th sequence. A? search using
the pairwise heuristic function hpair significantly re-
duces the search space and also guarantees finding
the optimal solution. We must be able to estimate
hpair(n) efficiently. It may appear that we need to
estimate the optimal pairwise alignment for all the
pairs of suffix sequences at every node. However,
we can precompute the dynamic programming ma-
trix over all the pair of sequences (Si, Sj) once from
the backward direction, and then reuse these values
at each node. This simple trick significantly speeds
up the computation of hpair(n).
Despite the significant reduction in the search
space, the A? search may still need to explore a
large number of nodes, and may become too slow
for real-time captioning. However, we can further
improve the speed by following the idea of weighted
A? search (Pohl, 1970). We modify the evaluation
function f(n) = g(n)+hpair(n) to a weighted eval-
uation function f ?(n) = g(n) + whpair(n), where
w ? 1 is a weight parameter. By setting the value
of w to be greater than 1, we increase the relative
weight of the estimated cost to reach the destina-
tion. Therefore, the search prefers the nodes that are
closer to the destination, and thus reaches the goal
faster. Weighted A? search can significantly reduce
the number of nodes to be examined, but it also loses
the optimality guarantee of the admissible heuristic
function. We can trade-off between accuracy and
speed by tuning the weight parameter w.
3.2 Beam Search using Time-stamps
The computational cost of the A? search algorithm
grows exponentially with increase in the number of
sequences. However, in order to keep the crowd-
sourced captioning system cost-effective, only a
small number of workers are generally recruited at
a time (typically K ? 10). We, therefore, are more
concerned about the growth in computational cost as
the sequence length increases.
In practice, we break down the sequences into
smaller chunks by maintaining a window of a given
time interval, and we apply MSA only to the smaller
chunks of captions entered by the workers during
that time window. As the window size increases,
the accuracy of our MSA based combining system
increases, but so does the computational cost and la-
tency. Therefore, it is important to apply MSA with
a relatively small window size for real-time caption-
ing applications. Another interesting application can
be the offline captioning, for example, captioning an
entire lecture and uploading the captions later.
For the offline captioning problem, we can fo-
cus less on latency and more on accuracy by align-
ing longer sequences. To restrict the search space
from exploding with sequence length (N ), we apply
a beam constraint on our search space using the time
stamps of each captioned words. For example, if we
204
1. so now what i want to do is introduce some of the
2. what i wanna do is introduce some of the aspects of the class
3. so now what i want to do is is introduce some of the aspects of the class
4. so now what i want to do is introduce
5. so now what i want to do is introduce some of the operational of the class
6. so i want to introduce some of the operational aspects of the clas
C. so now what i want to do is introduce some of the operational aspects of the class
Figure 3: An example of applying MSA-A? (threshold tv = 2) to combine 6 partial captions (first 6 lines) by human
workers to obtain the final output caption (C).
set the beam size to be 20 seconds, then we ignore
any state in our search space that aligns two words
having more than 20 seconds time lag. Given a fixed
beam size b, we can restrict the number of priority
queue removals by the A? algorithm to O(NbK).
The maximum size of the priority queue is O(NbK).
For each node in the priority queue, for each of the
O(2K) successor states, the objective function and
heuristic estimation requires O(K2) operations and
each priority queue insertion requires O(log(NbK))
i.e. O(logN + K log b) operations. Therefore,
the overall worst case computational complexity is
O
(
NbK2K(K2 + logN + K log b)
)
. Note that for
fixed beam size b and number of sequences K, the
computational cost grows as O(N logN) with the
increase in N . However, in practice, weighted A?
search explores much smaller number of states com-
pared to this beam-restricted space.
3.3 Majority Voting after Alignment
After aligning the captions by multiple workers in a
given chunk, we need to combine them to obtain the
final caption. We do that via majority voting at each
position of the alignment matrix containing a non-
gap symbol. In case of tie, we apply the language
model to choose the most likely word.
Often workers type in nonstandard symbols, ab-
breviations, or misspelled words that do not match
with any other workers? input and end up as a sin-
gle word aligned to gaps in all the other sequences.
To filter out such spurious words, we apply a vot-
ing threshold (tv) during majority voting and filter
out words having less than tv votes. Typically we
set tv = 2 (see the example in Figure 3). While ap-
plying the voting threshold improves the word error
rate and readability, it runs the risk of loosing correct
words if they are covered by only a single worker.
3.4 Incorporating an N-gram Language Model
We also experimented with a version of our system
designed to incorporate the score from an n-gram
language model into the search. For this purpose,
we modified the alignment algorithm to produce a
hypothesized output string as it moves through the
input strings, as opposed to using voting to produce
the final string as a post-processing step. The states
for our dynamic programming are extended to in-
clude not only the current position in each input
string, but also the last two words of the hypothesis
string (i.e. [n1, . . . , nK , wi?1, wi?2]) for use in com-
puting the next trigram language model probability.
We replace our sum-of-all-pairs objective function
with the sum of the alignment cost of each input with
the hypothesis string, to which we add the log of the
language model probability and a feature for the to-
tal number of words in the hypothesis. Mathemati-
cally, we consider the hypothesis string to be the 0th
row of the alignment matrix, making our objective
function:
c(A) =
?
1?i?K
c(A0,i) + wlen
Nf
?
l=1
I(a0,l 6= ?)
+ wlm
Nf
?
l=1
logP (a0,l|a0,l?2, a0,l?1)
where wlm and wlen are negative constants indicat-
ing the relative weights of the language model prob-
ability and the length penalty.
Extending states with two previous words results
in a larger computational complexity. Given K se-
quences of length N each, we can have O(NK) dis-
tinct words. Therefore, the number distinct states
is O(NbK(NK)2) i.e. O(N3K2bK). Each state
can have O(K2K) successors, giving an overall
computational complexity of O(N3K3bK2K(K2 +
logN + logK + K log b)). Alternatively, if the vo-
205
cabulary size |V | is smaller than NK, the number of
distinct states is bounded by O(NbK |V |2).
3.5 Evaluation Metric for Speech to Text
Captioning
Automated evaluation of speech to text captioning is
known to be a challenging task (Wang et al, 2003).
Word Error Rate (WER) is the most commonly used
metric that finds the best pairwise alignment be-
tween the candidate caption and the ground truth
reference sentence. WER is estimated as S+I+DN ,
where S, I , and D is the number of incorrect word
substitutions, insertions, and deletions required to
match the candidate sentence with reference, and N
is the total number of words in the reference. WER
has several nice properties such as: 1) it is easy
to estimate, and 2) it tries to preserve word order-
ing. However, WER does not account for the overall
?readability? of text and thus does not always corre-
late well with human evaluation (Wang et al, 2003;
He et al, 2011).
The widely-used BLEU metric has been shown
to agree well with human judgment for evaluating
translation quality (Papineni et al, 2002). However,
unlike WER, BLEU imposes no explicit constraints
on the word ordering. BLEU has been criticized as
an ?under-constrained? measure (Callison-Burch et
al., 2006) for allowing too much variation in word
ordering. Moreover, BLEU does not directly esti-
mate recall, and instead relies on the brevity penalty.
Melamed et al (2003) suggest that a better approach
is to explicitly measure both precision and recall and
combine them via F-measure.
Our application is similar to automatic speech
recognition in that there is a single correct output,
as opposed to machine translation where many out-
puts can be equally correct. On the other hand, un-
like with ASR, out-of-order output is frequently pro-
duced by our alignment system when there is not
enough overlap between the partial captions to de-
rive the correct ordering for all words. It may be
the case that even such out-of-order output can be
of value to the user, and should receive some sort of
partial credit that is not possible using WER. For
this reason, we wished to systematically compare
BLEU, F-measure, and WER as metrics for our task.
We performed a study to evaluate the agreement
of the three metrics with human judgment. We ran-
Metric Spearman Corr. Pearson Corr.
1-WER 0.5258 0.6282
BLEU 0.3137 0.6181
F-measure 0.4389 0.6240
Table 1: The correlation of average human judgment with
three automated metrics: 1-WER, BLEU, and F-measure.
domly extracted one-minute long audio clips from
four MIT OpenCourseWare lectures. Each clip was
transcribed by 7 human workers, and then aligned
and combined using four different systems: the
graph-based system, and three different versions of
our weighted A? algorithm with different values of
tuning parameters. Fifty people participated in the
study and were split in two equal sized groups. Each
group was assigned two of the four audio clips,
and each person evaluated all four captions for both
clips. Each participant assigned a score between 1
to 10 to these captions, based on two criteria: 1) the
overall estimated agreement of the captions with the
ground truth text, and 2) the readability and under-
standability of the captions.
Finally, we estimated the correlation coefficients
(both Spearman and Pearson) for the three metrics
discussed above with respect to the average score
assigned by the human participants. The results
are presented in Table 1. Among the three metrics,
WER had the highest agreement with the human par-
ticipants. This indicates that reconstructing the cor-
rect word order is in fact important to the users, and
that, in this aspect, our task has more of the flavor of
speech recognition than of machine translation.
4 Experimental Results
We experiment with the MSA-A? algorithm for cap-
tioning different audio clips, and compare the results
with two existing techniques. Our experimental set
up is similar to the experiments by Lasecki et al
(2012). Our dataset consists of four 5-minute long
audio clips extracted from lectures available on MIT
OpenCourseWare. The audio clips contain speech
from electrical engineering and chemistry lectures.
Each audio clip is transcribed by ten non-expert hu-
man workers in real-time. We then combine these
inputs using our MSA-A? algorithm, and also com-
pare with the existing graph-based system and mul-
206
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.58 0.60
0.36
0.47
0.54
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.60
0.63
0.40
0.49
0.41
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.53 0.55
0.35
0.45 0.42
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.49 0.51
0.26
0.36
0.30
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.53 0.55
0.44
0.39 0.37
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.56 0.56
0.45
0.39
0.19
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.43 0.44 0.41
0.35
0.23
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.43
0.46
0.36
0.29
0.09
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.62 0.64
0.53
0.47
0.55
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7 0.63 0.63
0.53
0.45 0.44
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.52 0.54 0.49
0.43 0.39
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.53 0.56
0.46
0.38
0.35
(1.0-WER) BLEU Score F-Measure
D
at
a 
Se
t 1
D
at
a 
Se
t 2
D
at
a 
Se
t 3
D
at
a 
Se
t 4
A*-10-t 
(c=10 sec, threshold=2)
A*-15-t 
(c=15 sec, threshold=2)
A*-15 
(c=15 sec, no threshold)
Graph-
based
MUSCLE
Figure 4: Evaluation of different systems on using three
different automated metrics for measuring transcription
quality: 1- Word Error Rate (WER), BLEU, and F-
measure on the four audio clips.
tiple sequence alignment using MUSCLE.
As explained earlier, we vary the four key pa-
rameters of the algorithm: the chunk size (c), the
heuristic weight (w), the voting threshold (tv), and
the beam size (b). The heuristic weight and chunk
size parameters help us to trade-off between speed
versus accuracy; the voting threshold tv helps im-
prove precision by pruning words having less than
tv votes, and beam size reduces the search space by
restricting states to be inside a time window/beam.
We use affine gap penalty (Edgar, 2004) with dif-
ferent gap opening and gap extension penalty. We
set gap opening penalty to 0.125 and gap extension
penalty to 0.05. We evaluate the performance using
the three standard metrics: Word Error Rate (WER),
BLEU, and F-measure. The performance in terms of
these metrics using different systems is presented in
Figure 4.
Out of the five systems in Figure 4, the first three
are different versions of our A? search based MSA
algorithm with different parameter settings: 1) A?-
10-t system (c = 10 seconds, tv = 2), 2) A?-15-t (c =
15 seconds, tv = 2), and 3) A?-15 (c = 15 seconds, tv
= 1 i.e. no pruning while voting). For all three sys-
tems, the heuristic weight parameter w is set to 2.5
and beam size b = 20 seconds. The other two sys-
tems are the existing graph-based system and mul-
tiple sequence alignment using MUSCLE. Among
the three A? based algorithms, both A?-15-t and A?-
10-t produce better quality transcripts and outper-
form the existing algorithms. Both systems apply
the voting threshold that improves precision. The
system A?-15 applies no threshold and ends up pro-
ducing many spurious words having poor agreement
among the workers, and hence it scores worse in all
the three metrics. The A?-15-t achieves 57.4% aver-
age accuracy in terms of (1-WER), providing 29.6%
improvement with respect to the graph-based sys-
tem (average accuracy 42.6%), and 35.4% improve-
ment with respect to the MUSCLE-based MSA sys-
tem (average accuracy 41.9%). On the same set of
audio clips, Lasecki et al (2012) reported 36.6% ac-
curacy using ASR (Dragon Naturally Speaking, ver-
sion 11.5 for Windows), which is worse than all the
crowd-based based systems used in this experiment.
To measure the statistical significance of this im-
provement, we performed a t-test at both the dataset
level (n = 4 clips) and the word level (n = 2862
words). The improvement over the graph-based
model was statistically significant with dataset level
p-value 0.001 and word level p-value smaller than
0.0001. The average time to align each 15 second
chunk with 10 input captions is ?400 milliseconds.
We have also experimented with a trigram lan-
guage model, trained on the British National Cor-
pus (Burnard, 1995) having ?122 million words.
The language-model-integrated A? search provided
a negligible 0.21% improvement in WER over the
A?-15-t system on average. The task of combin-
ing captions does not require recognizing words; it
only requires aligning them in the correct order. This
could explain why language model did not improve
accuracy, as it does for speech recognition. Since
the standard MSA-A? algorithm (without language
model) produced comparable accuracy and faster
running time, we used that version in the rest of the
207
2 3 4 5 6 7 8
0.42
0.44
0.46
0.48
0.5
0.52
0.54
0.56
0.58
1?
W
ER
Avg Running Time (in Seconds)
 
 
c = 5
c = 10
c = 15
c = 20
c = 40
c = 60
(a) Varying heuristic weights for fixed chunk sizes (c)
2 3 4 5 6 7 8
0.42
0.44
0.46
0.48
0.5
0.52
0.54
0.56
0.58
1?
W
ER
Avg Running Time (in Seconds)
 
 
w = 1.8
w = 2
w = 2.5
w = 3
w = 4
w = 6
w = 8
(b) Varying chunk size for fixed heuristic weight (w)
Figure 5: The trade-off between speed and accuracy for different heuristic weights and chunk size parameters.
experiments.
Next, we look at the critical speed versus accuracy
trade-off for different values of the heuristic weight
(w) and the chunk size (c) parameters. Since WER
has been shown to correlate most with human judg-
ment, we show the next results only with respect to
WER. First, we fix the chunk size at different val-
ues, and then vary the heuristic weight parameter:
w = 1.8, 2, 2.5, 3, 4, 6, and 8. The results are
shown in Figure 5(a), where each curve represents
how time and accuracy changed over the range of
values of w and a fixed value of c. We observe that
for smaller values of w, the algorithm is more accu-
rate, but comparatively slower. As w increases, the
search reaches the goal faster, but the quality of the
solution degrades as well. Next, we fix w and vary
chunk size c = 5, 10, 15, 20, 40, 60 second. We re-
peat this experiment for a range of values of w and
the results are shown in Figure 5(b). We can see that
the accuracy improves steeply up to c = 20 seconds,
and does not improve much beyond c = 40 seconds.
For all these benchmarks, we set the beam size (b)
to 20 seconds and voting threshold (tv) to 2.
In our tests, the beam size parameter (b) did not
play a significant role in performance, and setting it
to any reasonably large value (usually ? 15 seconds)
resulted in similar accuracy and running time. This
is because the A? search with hpair heuristic already
reduces the the search space significantly, and usu-
ally reaches the goal in a number of steps smaller
than the state space size after the beam restriction.
Finally, we investigate how the accuracy of our
algorithm varies with the number of inputs/workers.
We start with a pool of 10 input captions for one of
the audio clips. We vary the number of input cap-
tions (K) to the MSA-A? algorithm from 2 up to 10.
The quality of input captions differs greatly among
the workers. Therefore, for each value of K, we re-
peat the experiment min
(
20,
(10
K
))
times; each time
we randomly select K input captions out of the total
pool of 10. Figure 6 shows that accuracy steeply
increases as the number of inputs increases to 7,
and after that adding more workers does not pro-
vide much improvement in accuracy, but increases
running time.
5 Discussion and Future Work
In this paper, we show that the A? search based
MSA algorithm performs better than existing algo-
rithms for combining multiple captions. The exist-
ing graph-based model has low latency, but it usually
can not find a near optimal alignment because of its
incremental alignment. Weighted A? search on the
other hand performs joint multiple sequence align-
ment, and is guaranteed to produce a solution hav-
ing cost no more than (1 + ?) times the cost of the
optimal solution, given a heuristic weight of (1+ ?).
Moreover, A? search allows for straightforward in-
tegration of an n-gram language model during the
search.
Another key advantage of the proposed algorithm
is the ease with which we can trade-off between
208
0 2 4 6 8 10
0
0.1
0.2
0.3
0.4
0.5
0.6
Av
er
ag
e 
(1?
W
ER
)
Average Running Time (in sec)
Figure 6: Experiments showing how the accuracy of the
final caption by MSA-A? algorithm varies with the num-
ber of inputs from 2 to 10.
speed and accuracy. The algorithm can be tailored
to real-time by using a larger heuristic weight. On
the other hand, we can produce better transcripts for
offline tasks by choosing a smaller weight.
It is interesting to compare our results with those
achieved using the MUSCLE MSA tool of Edgar
(2004). One difference is that our system takes a hi-
erarchical approach in that it aligns at the word level,
but also uses string edit distance at the letter level
as a substitution cost for words. Thus, it is able to
take advantage of the fact that individual transcrip-
tions do not generally contain arbitrary fragments of
words. More fundamentally, it is interesting to note
that MUSCLE and most other commonly used MSA
tools for biological sequences make use of a guide
tree formed by a hierarchical clustering of the in-
put sequences. The guide tree produced by the algo-
rithms may or may not match the evolutionary tree
of the organisms whose genomes are being aligned,
but, nevertheless, in the biological application, such
an underlying evolutionary tree generally exists. In
aligning transcriptions, there is no particular reason
to expect individual pairs of transcriptions to be es-
pecially similar to one another, which may make the
guide tree approach less appropriate.
In order to get competitive results, the A? search
based algorithm aligns sequences that are at least 7-
10 seconds long. The delay for collecting the cap-
tions within a chunk can introduce latency, however,
each alignment usually takes less than 300 millisec-
onds, allowing us to repeatedly align the stream of
words, even before the window is filled. This pro-
vides less accurate but immediate response to users.
Finally, when we have all the words entered in a
chunk, we perform the final alignment and show the
caption to users for the entire chunk.
After aligning the input sequences, we obtain the
final transcript by majority voting at each alignment
position, which treats each worker equally and does
not take individual quality into account. Recently,
some work has been done for automatically estimat-
ing individual worker?s quality for crowd-based data
labeling tasks (Karger et al, 2011; Liu et al, 2012).
Extending these methods for crowd-based text cap-
tioning could be an interesting future direction.
6 Conclusion
In this paper, we have introduced a new A? search
based MSA algorithm for aligning partial captions
into a final output stream in real-time. This method
has advantages over prior approaches both in for-
mal guarantees of optimality and the ability to trade
off speed and accuracy. Our experiments on real
captioning data show that it outperforms prior ap-
proaches based on a dependency graph model and a
standard MSA implementation (MUSCLE). An ex-
periment with 50 participants explored whether ex-
iting automatic metrics of quality matched human
evaluations of readability, showing WER did best.
Acknowledgments Funded by NSF awards IIS-
1218209 and IIS-0910611.
References
Keith Bain, Sara Basson, A Faisman, and D Kanevsky.
2005. Accessibility, transcription, and access every-
where. IBM Systems Journal, 44(3):589?603.
Keith Bain, Eunice Lund-Lucas, and Janice Stevens.
2012. 22. transcribe your class: Using speech recogni-
tion to improve access for at-risk students. Collected
Essays on Learning and Teaching, 5.
Lou Burnard. 1995. Users Reference Guide British Na-
tional Corpus Version 1.0.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of bleu in ma-
chine translation research. In Proceedings of EACL,
volume 2006, pages 249?256.
209
Martin Cooke, Phil Green, Ljubomir Josifovski, and As-
cension Vizinho. 2001. Robust automatic speech
recognition with missing and unreliable acoustic data.
Speech Communication, 34(3):267?285.
Chuong B Do, Mahathi SP Mahabhashyam, Michael
Brudno, and Serafim Batzoglou. 2005. Prob-
cons: Probabilistic consistency-based multiple se-
quence alignment. Genome Research, 15(2):330?340.
Richard Durbin, Sean R Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological sequence analy-
sis: probabilistic models of proteins and nucleic acids.
Cambridge university press.
Robert C Edgar and Serafim Batzoglou. 2006. Multi-
ple sequence alignment. Current opinion in structural
biology, 16(3):368?373.
Robert C Edgar. 2004. MUSCLE: multiple sequence
alignment with high accuracy and high throughput.
Nucleic Acids Research, 32(5):1792?1797.
Xiaodong He, Li Deng, and Alex Acero. 2011. Why
word error rate is not a good metric for speech rec-
ognizer training for the speech translation task? In
IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), 2011, pages 5632?
5635. IEEE.
Phillip B Horton. 1997. Strings, algorithms, and ma-
chine learning applications for computational biology.
Ph.D. thesis, University of California, Berkeley.
David R Karger, Sewoong Oh, and Devavrat Shah. 2011.
Iterative learning for reliable crowdsourcing systems.
In Proceedings of Advances in Neural Information
Processing Systems (NIPS), volume 24, pages 1953?
1961.
Walter Lasecki and Jeffrey Bigham. 2012. Online qual-
ity control for real-time crowd captioning. In Pro-
ceedings of the 14th international ACM SIGACCESS
conference on Computers and accessibility (ASSETS
2012), pages 143?150. ACM.
Walter Lasecki, Christopher Miller, Adam Sadilek, An-
drew Abumoussa, Donato Borrello, Raja Kushalnagar,
and Jeffrey Bigham. 2012. Real-time captioning by
groups of non-experts. In Proceedings of the 25rd an-
nual ACM symposium on User interface software and
technology, UIST ?12.
Walter Lasecki, Christopher Miller, and Jeffrey Bigham.
2013. Warping time for more effective real-time
crowdsourcing. In Proceedings of the ACM confer-
ence on Human Factors in Computing Systems, CHI
?13, page To Appear, New York, NY, USA. ACM.
Martin Lermen and Knut Reinert. 2000. The prac-
tical use of the A* algorithm for exact multiple se-
quence alignment. Journal of Computational Biology,
7(5):655?671.
Qiang Liu, Jian Peng, and Alex Ihler. 2012. Varia-
tional inference for crowdsourcing. In Proceedings of
Advances in Neural Information Processing Systems
(NIPS), volume 25, pages 701?709.
Dan Melamed, Ryan Green, and Joseph P Turian. 2003.
Precision and recall of machine translation. In Pro-
ceedings HLT-NAACL 2003, volume 2, pages 61?63.
Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual meeting of Association for Computational
Linguistics, pages 311?318. Association for Computa-
tional Linguistics.
Ira Pohl. 1970. Heuristic search viewed as path finding
in a graph. Artificial Intelligence, 1(3):193?204.
Ales? Praz?a?k, Zdene?k Loose, Jan Trmal, Josef V Psutka,
and Josef Psutka. 2012. Captioning of Live
TV Programs through Speech Recognition and Re-
speaking. In Text, Speech and Dialogue, pages 513?
519. Springer.
Murat Saraclar, Michael Riley, Enrico Bocchieri, and
Vincent Goffin. 2002. Towards automatic closed cap-
tioning: Low latency real time broadcast news tran-
scription. In Proceedings of the International Confer-
ence on Spoken Language Processing (ICSLP), pages
1741?1744.
Julie D Thompson, Desmond G Higgins, and Toby J
Gibson. 1994. Clustal w: improving the sensitivity
of progressive multiple sequence alignment through
sequence weighting, position-specific gap penalties
and weight matrix choice. Nucleic Acids Research,
22(22):4673?4680.
Mike Wald. 2006a. Captioning for deaf and hard of
hearing people by editing automatic speech recogni-
tion in real time. Computers Helping People with Spe-
cial Needs, pages 683?690.
Mike Wald. 2006b. Creating accessible educational mul-
timedia through editing automatic speech recognition
captioning in real time. Interactive Technology and
Smart Education, 3(2):131?141.
Lusheng Wang and Tao Jiang. 1994. On the complexity
of multiple sequence alignment. Journal of Computa-
tional Biology, 1(4):337?348.
Ye-Yi Wang, Alex Acero, and Ciprian Chelba. 2003. Is
word error rate a good indicator for spoken language
understanding accuracy. In IEEE Workshop on Auto-
matic Speech Recognition and Understanding, 2003.
ASRU?03. 2003, pages 577?582. IEEE.
210
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 450?459,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Optimal Head-Driven Parsing Complexity
for Linear Context-Free Rewriting Systems
Pierluigi Crescenzi
Dip. di Sistemi e Informatica
Universita` di Firenze
Daniel Gildea
Computer Science Dept.
University of Rochester
Andrea Marino
Dip. di Sistemi e Informatica
Universita` di Firenze
Gianluca Rossi
Dip. di Matematica
Universita` di Roma Tor Vergata
Giorgio Satta
Dip. di Ingegneria dell?Informazione
Universita` di Padova
Abstract
We study the problem of finding the best head-
driven parsing strategy for Linear Context-
Free Rewriting System productions. A head-
driven strategy must begin with a specified
righthand-side nonterminal (the head) and add
the remaining nonterminals one at a time in
any order. We show that it is NP-hard to find
the best head-driven strategy in terms of either
the time or space complexity of parsing.
1 Introduction
Linear Context-Free Rewriting Systems (LCFRSs)
(Vijay-Shankar et al, 1987) constitute a very general
grammatical formalism which subsumes context-
free grammars (CFGs) and tree adjoining grammars
(TAGs), as well as the synchronous context-free
grammars (SCFGs) and synchronous tree adjoin-
ing grammars (STAGs) used as models in machine
translation.1 LCFRSs retain the fundamental prop-
erty of CFGs that grammar nonterminals rewrite
independently, but allow nonterminals to generate
discontinuous phrases, that is, to generate more
than one span in the string being produced. This
important feature has been recently exploited by
Maier and S?gaard (2008) and Kallmeyer and Maier
(2010) for modeling phrase structure treebanks with
discontinuous constituents, and by Kuhlmann and
Satta (2009) for modeling non-projective depen-
dency treebanks.
The rules of a LCFRS can be analyzed in terms
of the properties of rank and fan-out. Rank is the
1To be more precise, SCFGs and STAGs generate languages
composed by pair of strings, while LCFRSs generate string lan-
guages. We can abstract away from this difference by assuming
concatenation of components in a string pair.
number of nonterminals on the right-hand side (rhs)
of a rule, while fan-out is the number of spans of
the string generated by the nonterminal in the left-
hand side (lhs) of the rule. CFGs are equivalent to
LCFRSs with fan-out one, while TAGs are one type
of LCFRSs with fan-out two. Rambow and Satta
(1999) show that rank and fan-out induce an infi-
nite, two-dimensional hierarchy in terms of gener-
ative power; while CFGs can always be reduced to
rank two (Chomsky Normal Form), this is not the
case for LCFRSs with any fan-out greater than one.
General algorithms for parsing LCFRSs build a
dynamic programming chart of recognized nonter-
minals bottom-up, in a manner analogous to the
CKY algorithm for CFGs (Hopcroft and Ullman,
1979), but with time and space complexity that are
dependent on the rank and fan-out of the gram-
mar rules. Whenever it is possible, binarization of
LCFRS rules, or reduction of rank to two, is there-
fore important for parsing, as it reduces the time
complexity needed for dynamic programming. This
has lead to a number of binarization algorithms for
LCFRSs, as well as factorization algorithms that
factor rules into new rules with smaller rank, with-
out necessarily reducing rank all the way to two.
Kuhlmann and Satta (2009) present an algorithm
for binarizing certain LCFRS rules without increas-
ing their fan-out, and Sagot and Satta (2010) show
how to reduce rank to the lowest value possible for
LCFRS rules of fan-out two, again without increas-
ing fan-out. Go?mez-Rodr??guez et al (2010) show
how to factorize well-nested LCFRS rules of arbi-
trary fan-out for efficient parsing.
In general there may be a trade-off required
between rank and fan-out, and a few recent pa-
pers have investigated this trade-off taking gen-
450
eral LCFRS rules as input. Go?mez-Rodr??guez et
al. (2009) present an algorithm for binarization of
LCFRSs while keeping fan-out as small as possi-
ble. The algorithm is exponential in the resulting
fan-out, and Go?mez-Rodr??guez et al (2009) mention
as an important open question whether polynomial-
time algorithms to minimize fan-out are possible.
Gildea (2010) presents a related method for bina-
rizing rules while keeping the time complexity of
parsing as small as possible. Binarization turns out
to be possible with no penalty in time complexity,
but, again, the factorization algorithm is exponen-
tial in the resulting time complexity. Gildea (2011)
shows that a polynomial time algorithm for factor-
izing LCFRSs in order to minimize time complexity
would imply an improved approximation algorithm
for the well-studied graph-theoretic property known
as treewidth. However, whether the problem of fac-
torizing LCFRSs in order to minimize time com-
plexity is NP-hard is still an open question in the
above works.
Similar questions have arisen in the context of
machine translation, as the SCFGs used to model
translation are also instances of LCFRSs, as already
mentioned. For SCFG, Satta and Peserico (2005)
showed that the exponent in the time complexity
of parsing algorithms must grow at least as fast as
the square root of the rule rank, and Gildea and
?Stefankovic? (2007) tightened this bound to be lin-
ear in the rank. However, neither paper provides an
algorithm for finding the best parsing strategy, and
Huang et al (2009) mention that whether finding the
optimal parsing strategy for an SCFG rule is NP-
hard is an important problem for future work.
In this paper, we investigate the problem of rule
binarization for LCFRSs in the context of head-
driven parsing strategies. Head-driven strategies be-
gin with one rhs symbol, and add one nontermi-
nal at a time. This rules out any factorization in
which two subsets of nonterminals of size greater
than one are combined in a single step. Head-driven
strategies allow for the techniques of lexicalization
and Markovization that are widely used in (projec-
tive) statistical parsing (Collins, 1997). The statis-
tical LCFRS parser of Kallmeyer and Maier (2010)
binarizes rules head-outward, and therefore adopts
what we refer to as a head-driven strategy. How-
ever, the binarization used by Kallmeyer and Maier
(2010) simply proceeds left to right through the rule,
without considering the impact of the parsing strat-
egy on either time or space complexity. We examine
the question of whether we can efficiently find the
strategy that minimizes either the time complexity
or the space complexity of parsing. While a naive
algorithm can evaluate all r! head-driven strategies
in time O(n ? r!), where r is the rule?s rank and n
is the total length of the rule?s description, we wish
to determine whether a polynomial-time algorithm
is possible.
Since parsing problems can be cast in terms of
logic programming (Shieber et al, 1995), we note
that our problem can be thought of as a type of
query optimization for logic programming. Query
optimization for logic programming is NP-complete
since query optimization for even simple conjunc-
tive database queries is NP-complete (Chandra and
Merlin, 1977). However, the fact that variables in
queries arising from LCFRS rules correspond to the
endpoints of spans in the string to be parsed means
that these queries have certain structural properties
(Gildea, 2011). We wish to determine whether the
structure of LCFRS rules makes efficient factoriza-
tion algorithms possible.
In the following, we show both the the time- and
space-complexity problems to be NP-hard for head-
driven strategies. We provide what is to our knowl-
edge the first NP-hardness result for a grammar fac-
torization problem, which we hope will aid in under-
standing parsing algorithms in general.
2 LCFRSs and parsing complexity
In this section we briefly introduce LCFRSs and de-
fine the problem of optimizing head-driven parsing
complexity for these formalisms. For a positive in-
teger n, we write [n] to denote the set {1, . . . , n}.
As already mentioned in the introduction,
LCFRSs generate tuples of strings over some finite
alphabet. This is done by associating each produc-
tion p of a grammar with a function g that takes as
input the tuples generated by the nonterminals in p?s
rhs, and rearranges their string components into a
new tuple, possibly adding some alphabet symbols.
Let V be some finite alphabet. We write V ? for
the set of all (finite) strings over V . For natural num-
bers r ? 0 and f, f1, . . . , fr ? 1, consider a func-
451
tion g : (V ?)f1 ? ? ? ? ? (V ?)fr ? (V ?)f defined by
an equation of the form
g(?x1,1, . . . , x1,f1?, . . . , ?xr,1, . . . , xr,fr?) = ~? .
Here the xi,j?s denote variables over strings in V ?,
and ~? = ??1, . . . , ?f ? is an f -tuple of strings over
g?s argument variables and symbols in V . We say
that g is linear, non-erasing if ~? contains exactly
one occurrence of each argument variable. We call r
and f the rank and the fan-out of g, respectively,
and write r(g) and f(g) to denote these quantities.
Example 1 g1(?x1,1, x1,2?) = ?x1,1x1,2? takes as
input a tuple with two strings and returns a tuple
with a single string, obtained by concatenating the
components in the input tuple. g2(?x1,1, x1,2?) =
?ax1,1b, cx1,2d? takes as input a tuple with two
strings and wraps around these strings with sym-
bols a, b, c, d ? V . Both functions are linear, non-
erasing, and we have r(g1) = r(g2) = 1, f(g1) = 1
and f(g2) = 2. 2
A linear context-free rewriting system is a tuple
G = (VN , VT , P, S), where VN and VT are finite,
disjoint alphabets of nonterminal and terminal sym-
bols, respectively. Each A ? VN is associated with
a value f(A), called its fan-out. The nonterminal S
is the start symbol, with f(S) = 1. Finally, P is a
set of productions of the form
p : A ? g(A1, A2, . . . , Ar(g)) , (1)
where A,A1, . . . , Ar(g) ? VN , and g : (V ?T )f(A1)
? ? ? ?? (V ?T )f(Ar(g)) ? (V ?T )f(A) is a linear, non-
erasing function.
Production (1) can be used to transform the
r(g) string tuples generated by the nonterminals
A1, . . . , Ar(g) into a tuple of f(A) strings gener-
ated by A. The values r(g) and f(g) are called the
rank and fan-out of p, respectively, written r(p) and
f(p). Given that f(S) = 1, S generates a set of
strings, defining the language L(G).
Example 2 Let g1 and g2 be as in Example 1, and
let g3() = ??, ??. Consider the LCFRS G defined by
the productions p1 : S ? g1(A), p2 : A ? g2(A)
and p3 : A ? g3(). We have f(S) = 1, f(A) =
f(G) = 2, r(p3) = 0 and r(p1) = r(p2) = r(G) =
1. We have L(G) = {anbncndn |n ? 1}. For in-
stance, the string a3b3c3d3 is generated by means
fan-out strategy
4 ((A1 ?A4) ?A3)? ?A2
3 (A1 ?A4)? ? (A2 ?A3)
3 ((A1 ?A2)? ?A4) ?A3
2 ((A?2 ?A3) ?A4) ?A1
Figure 1: Some parsing strategies for production p in Ex-
ample 3, and the associated maximum value for fan-out.
Symbol ? denotes the merging operation, and superscript
? marks the first step in the strategy in which the highest
fan-out is realized.
of the following bottom-up process. First, the tuple
??, ?? is generated by A through p3. We then iterate
three times the application of p2 to ??, ??, resulting
in the tuple ?a3b3, c3d3?. Finally, the tuple (string)
?a3b3c3d3? is generated by S through application of
p1. 2
Existing parsing algorithms for LCFRSs exploit
dynamic programming. These algorithms compute
partial parses of the input string w, represented by
means of specialized data structures called items.
Each item indexes the boundaries of the segments
of w that are spanned by the partial parse. In the
special case of parsing based on CFGs, an item con-
sists of two indices, while for TAGs four indices are
required.
In the general case of LCFRSs, parsing of a pro-
duction p as in (1) can be carried out in r(g) ? 1
steps, collecting already available parses for nonter-
minals A1, . . . , Ar(g) one at a time, and ?merging?
these into intermediate partial parses. We refer to the
order in which nonterminals are merged as a pars-
ing strategy, or, equivalently, a factorization of the
original grammar rule. Any parsing strategy results
in a complete parse of p, spanning f(p) = f(A)
segments of w and represented by some item with
2f(A) indices. However, intermediate items ob-
tained in the process might span more than f(A)
segments. We illustrate this through an example.
Example 3 Consider a linear non-erasing function
g(?x1,1, x1,2?, ?x2,1, x2,2?, ?x3,1, x3,2?, ?x4,1, x4,2?)
= ?x1,1x2,1x3,1x4,1, x3,2x2,2x4,2x1,2?, and a pro-
duction p : A ? g(A1, A2, A3, A4), where all the
nonterminals involved have fan-out 2. We could
parse p starting from A1, and then merging with A4,
452
v1
v2
v3 v4e1
e3
e2
e4
Figure 2: Example input graph for our construction of an
LCFRS production.
A3, and A2. In this case, after we have collected the
first three nonterminals, we have obtained a partial
parse having fan-out 4, that is, an item spanning 4
segments of the input string. Alternatively, we could
first merge A1 and A4, then merge A2 and A3, and
finally merge the two obtained partial parses. This
strategy is slightly better, resulting in a maximum
fan-out of 3. Other possible strategies can be ex-
plored, displayed in Figure 1. It turns out that the
best parsing strategy leads to fan-out 2. 2
The maximum fan-out f realized by a parsing
strategy determines the space complexity of the
parsing algorithm. For an input string w, items will
require (in the worst-case) 2f indices, each taking
O(|w|) possible values. This results in space com-
plexity of O(|w|2f ). In the special cases of parsing
based on CFGs and TAGs, this provides the well-
known space complexity of O(|w|2) and O(|w|4),
respectively.
It can also be shown that, if a partial parse hav-
ing fan-out f is obtained by means of the combi-
nation of two partial parses with fan-out f1 and f2,
respectively, the resulting time complexity will be
O(|w|f+f1+f2) (Seki et al, 1991; Gildea, 2010). As
an example, in the case of parsing based on CFGs,
nonterminals as well as partial parses all have fan-
out one, resulting in the standard time complexity of
O(|w|3) of dynamic programming methods. When
parsing with TAGs, we have to manipulate objects
with fan-out two (in the worst case), resulting in time
complexity of O(|w|6).
We investigate here the case of general LCFRS
productions, whose internal structure is consider-
ably more complex than the context-free or the tree
adjoining case. Optimizing the parsing complexity
for a production means finding a parsing strategy
that results in minimum space or time complexity.
We now turn the above optimization problems
into decision problems. In the MIN SPACE STRAT-
EGY problem one takes as input an LCFRS produc-
tion p and an integer k, and must decide whether
there exists a parsing strategy for p with maximum
fan-out not larger than k. In the MIN TIME STRAT-
EGY problem one is given p and k as above and must
decide whether there exists a parsing strategy for
p such that, in any of its steps merging two partial
parses with fan-out f1 and f2 and resulting in a par-
tial parse with fan-out f , the relation f+f1+f2 ? k
holds.
In this paper we investigate the above problems in
the context of a specific family of linguistically mo-
tivated parsing strategies for LCFRSs, called head-
driven. In a head-driven strategy, one always starts
parsing a production p from a fixed nonterminal in
its rhs, called the head of p, and merges the remain-
ing nonterminals one at a time with the partial parse
containing the head. Thus, under these strategies,
the construction of partial parses that do not include
the head is forbidden, and each parsing step involves
at most one partial parse. In Figure 1, all of the dis-
played strategies but the one in the second line are
head-driven (for different choices of the head).
3 NP-completeness results
For an LCFRS production p, let H be its head non-
terminal, and let A1, . . . , An be all the non-head
nonterminals in p?s rhs, with n + 1 = r(p). A head-
driven parsing strategy can be represented as a per-
mutation pi over the set [n], prescribing that the non-
head nonterminals in p?s rhs should be merged with
H in the order Api(1), Api(2), . . . , Api(n). Note that
there are n! possible head-driven parsing strategies.
To show that MIN SPACE STRATEGY is NP-
hard under head-driven parsing strategies, we reduce
from the MIN CUT LINEAR ARRANGEMENT prob-
lem, which is a decision problem over (undirected)
graphs. Given a graph M = (V,E) with set of ver-
tices V and set of edges E, a linear arrangement
of M is a bijective function h from V to [n], where
|V | = n. The cutwidth of M at gap i ? [n? 1] and
with respect to a linear arrangement h is the number
of edges crossing the gap between the i-th vertex and
its successor:
cw(M,h, i) = |{(u, v) ? E |h(u) ? i < h(v)}| .
453
p : A ? g(H,A1, A2, A3, A4)
g(?xH,e1 , xH,e2 , xH,e3 , xH,e4?, ?xA1,e1,l, xA1,e1,r, xA1,e3,l, xA1,e3,r?, ?xA2,e1,l, xA2,e1,r, xA2,e2,l, xA2,e2,r?,
?xA3,e2,l, xA3,e2,r, xA3,e3,l, xA3,e3,r, xA3,e4,l, xA3,e4,r?, ?xA4,e4,l, xA4,e4,r?) =
? xA1,e1,lxA2,e1,lxH,e1xA1,e1,rxA2,e1,r, xA2,e2,lxA3,e2,lxH,e2xA2,e2,rxA3,e2,r,
xA1,e3,lxA3,e3,lxH,e3xA1,e3,rxA3,e3,r, xA3,e4,lxA4,e4,lxH,e4xA3,e4,rxA4,e4,r ?
Figure 3: The construction used to prove Theorem 1 builds the LCFRS production p shown, when given as input the
graph of Figure 2.
The cutwidth of M is then defined as
cw(M) = min
h
max
i?[n?1]
cw(M,h, i) .
In the MIN CUT LINEAR ARRANGEMENT problem,
one is given as input a graph M and an integer k, and
must decide whether cw(M) ? k. This problem has
been shown to be NP-complete (Gavril, 1977).
Theorem 1 The MIN SPACE STRATEGY problem
restricted to head-driven parsing strategies is NP-
complete.
PROOF We start with the NP-hardness part. Let
M = (V,E) and k be an input instance for
MIN CUT LINEAR ARRANGEMENT, and let V =
{v1, . . . , vn} and E = {e1, . . . , eq}. We assume
there are no self loops in M , since these loops do not
affect the value of the cutwidth and can therefore be
removed. We construct an LCFRS production p and
an integer k? as follows.
Production p has a head nonterminal H and a non-
head nonterminal Ai for each vertex vi ? V . We let
H generate tuples with a string component for each
edge ei ? E. Thus, we have f(H) = q. Accord-
ingly, we use variables xH,ei , for each ei ? E, to
denote the string components in tuples generated by
H .
For each vi ? V , let E(vi) ? E be the set of
edges impinging on vi; thus |E(vi)| is the degree
of vi. We let Ai generate a tuple with two string
components for each ej ? E(vi). Thus, we have
f(Ai) = 2 ? |E(vi)|. Accordingly, we use variables
xAi,ej ,l and xAi,ej ,r , for each ej ? E(vi), to de-
note the string components in tuples generated by
Ai (here subscripts l and r indicate left and right
positions, respectively; see below).
We set r(p) = n + 1 and f(p) = q, and
define p by A ? g(H,A1, A2, . . . , An), with
g(tH , tA1 , . . . , tAn) = ??1, . . . , ?q?. Here tH is the
tuple of variables for H and each tAi , i ? [n], is the
tuple of variables for Ai. Each string ?i, i ? [q], is
specified as follows. Let vs and vt be the endpoints
of ei, with vs, vt ? V and s < t. We define
?i = xAs,ei,lxAt,ei,lxH,eixAs,ei,rxAt,ei,r .
Observe that whenever edge ei impinges on vertex
vj , then the left and right strings generated by Aj
and associated with ei wrap around the string gen-
erated by H and associated with the same edge. Fi-
nally, we set k? = q + k.
Example 4 Given the input graph of Figure 2, our
reduction constructs the LCFRS production shown
in Figure 3. Figure 4 gives a visualization of how the
spans in this production fit together. For each edge
in the graph of Figure 2, we have a group of five
spans in the production: one for the head nontermi-
nal, and two spans for each of the two nonterminals
corresponding to the edge?s endpoints. 2
Assume now some head-driven parsing strategy
pi for p. For each i ? [n], we define Dpii to be the
partial parse obtained after step i in pi, consisting
of the merge of nonterminals H,Api(1), . . . , Api(i).
Consider some edge ej = (vs, vt). We observe that
for any Dpii that includes or excludes both nontermi-
nals As and At, the ?j component in the definition
of p is associated with a single string, and therefore
contributes with a single unit to the fan-out of the
partial parse. On the other hand, if Dpii includes only
one nonterminal between As and At, the ?j compo-
nent is associated with two strings and contributes
with two units to the fan-out of the partial parse.
We can associate with pi a linear arrangement hpi
of M by letting hpi(vpi(i)) = i, for each vi ? V .
From the above observation on the fan-out of Dpii ,
454
xA1,e1,lxA2,e1,l xH,e1 xA1,e1,rxA2,e1,r xA2,e2,lxA3,e2,l xH,e2 xA2,e2,rxA3,e2,r xA1,e3,lxA3,e3,l xH,e3 xA1,e3,rxA3,e3,r xA3,e4,lxA4,e4,l xH,e4 xA3,e4,rxA4,e4,r
H
A1
A2
A3
A4
Figure 4: A visualization of how the spans for each nonterminal fit together in the left-to-right order defined by the
production of Figure 3.
we have the following relation, for every i ? [n?1]:
f(Dpii ) = q + cw(M,hpi, i) .
We can then conclude that M,k is a positive instance
of MIN CUT LINEAR ARRANGEMENT if and only
if p, k? is a positive instance of MIN SPACE STRAT-
EGY. This proves that MIN SPACE STRATEGY is
NP-hard.
To show that MIN SPACE STRATEGY is in NP,
consider a nondeterministic algorithm that, given an
LCFRS production p and an integer k, guesses a
parsing strategy pi for p, and tests whether f(Dpii ) ?
k for each i ? [n]. The algorithm accepts or rejects
accordingly. Such an algorithm can clearly be im-
plemented to run in polynomial time. 
We now turn to the MIN TIME STRATEGY prob-
lem, restricted to head-driven parsing strategies. Re-
call that we are now concerned with the quantity
f1 + f2 + f , where f1 is the fan-out of some partial
parse D, f2 is the fan-out of a nonterminal A, and f
is the fan out of the partial parse resulting from the
merge of the two previous analyses.
We need to introduce the MODIFIED CUTWIDTH
problem, which is a variant of the MIN CUT LIN-
EAR ARRANGEMENT problem. Let M = (V,E) be
some graph with |V | = n, and let h be a linear ar-
rangement for M . The modified cutwidth of M at
position i ? [n] and with respect to h is the number
of edges crossing over the i-th vertex:
mcw(M,h, i) = |{(u, v) ? E |h(u) < i < h(v)}| .
The modified cutwidth of M is defined as
mcw(M) = min
h
max
i?[n]
mcw(M,h, i) .
In the MODIFIED CUTWIDTH problem one is given
as input a graph M and an integer k, and must
decide whether mcw(M) ? k. The MODIFIED
CUTWIDTH problem has been shown to be NP-
complete by Lengauer (1981). We strengthen this
result below; recall that a cubic graph is a graph
without self loops where each vertex has degree
three.
Lemma 1 The MODIFIED CUTWIDTH problem re-
stricted to cubic graphs is NP-complete.
PROOF The MODIFIED CUTWIDTH problem has
been shown to be NP-complete when restricted to
graphs of maximum degree three by Makedon et al
(1985), reducing from a graph problem known as
bisection width (see also Monien and Sudborough
(1988)). Specifically, the authors construct a graph
G? of maximum degree three and an integer k? from
an input graph G = (V,E) with an even number n
of vertices and an integer k, such that mcw(G?) ? k?
if and only if the bisection width bw(G) of G is not
greater than k, where
bw(G) = min
A,B?V
|{(u, v) ? E |u ? A ? v ? B}|
with A ?B = ?, A ?B = V , and |A| = |B|.
The graph G? has vertices of degree two and three
only, and it is based on a grid-like gadget R(r, c); see
Figure 5. For each vertex of G, G? includes a com-
ponent R(2n4, 8n4+8). Moreover, G? has a compo-
nent called an H-shaped graph, containing left and
right columns R(3n4, 12n4 + 12) connected by a
middle bar R(2n4, 12n4 + 9); see Figure 6. From
each of the n vertex components there is a sheaf of
2n2 edges connecting distinct degree 2 vertices in
the component to 2n2 distinct degree 2 vertices in
455
x
x x1
x2
x3
x4
x5
x x1 x2
x5
x3
x4
Figure 5: The R(5, 10) component (left), the modification of its degree 2 vertex x (middle), and the corresponding
arrangement (right).
the middle bar of the H-shaped graph. Finally, for
each edge (vi, vj) of G there is an edge in G? con-
necting a degree 2 vertex in the component corre-
sponding to the vertex vi with a degree 2 vertex in
the component corresponding to the vertex vj . The
integer k? is set to 3n4 + n3 + k ? 1.
Makedon et al (1985) show that the modified
cutwidth of R(r, c) is r ? 1 whenever r ? 3 and
c ? 4r + 8. They also show that an optimal lin-
ear arrangement for G? has the form depicted in Fig-
ure 6, where half of the vertex components are to
the left of the H-shaped graph and all the other ver-
tex components are to the right. In this arrangement,
the modified cutwidth is attested by the number of
edges crossing over the vertices in the left and right
columns of the H-shaped graph, which is equal to
3n4 ? 1 + n
2
2n2 + ? = 3n4 + n3 + ? ? 1 (2)
where ? denotes the number of edges connecting
vertices to the left with vertices to the right of the
H-shaped graph. Thus, bw(G) ? k if and only if
mcw(G?) ? k?.
All we need to show now is how to modify the
components of G? in order to make it cubic.
Modifying the vertex components All vertices
x of degree 2 of the components corresponding to
a vertex in G can be transformed into a vertex of
degree 3 by adding five vertices x1, . . . , x5 con-
nected as shown in the middle bar of Figure 5. Ob-
serve that these five vertices can be positioned in
the arrangement immediately after x in the order
x1, x2, x5, x3, x4 (see the right part of the figure).
The resulting maximum modified cutwidth can in-
crease by 2 in correspondence of vertex x5. Since
the vertices of these components, in the optimal
arrangement, have modified cutwidth smaller than
2n4 + n3 + n2, an increase by 2 is still smaller than
the maximum modified cutwidth of the entire graph,
which is 3n4 + O(n3).
Modifying the middle bar of the H-shaped graph
The vertices of degree 2 of this part of the graph can
be modified as in the previous paragraph. Indeed, in
the optimal arrangement, these vertices have mod-
ified cutwidth smaller than 2n4 + 2n3 + n2, and
an increase by 2 is still smaller than the maximum
cutwidth of the entire graph.
Modifying the left/right columns of the H-shaped
graph We replace the two copies of component
R(3n4, 12n4 + 12) with two copies of the new
component D(3n4, 24n4 + 16) shown in Figure 7,
which is a cubic graph. In order to prove that rela-
tion (2) still holds, it suffices to show that the modi-
fied cutwidth of the component D(r, c) is still r ? 1
whenever r ? 3 and c = 8r + 16.
We first observe that the linear arrangement ob-
tained by visiting the vertices of D(r, c) from top to
bottom and from left to right has modified cutwidth
r? 1. Let us now prove that, for any partition of the
vertices into two subsets V1 and V2 with |V1|, |V2| ?
4r2, there exist at least r disjoint paths between ver-
tices of V1 and vertices of V2. To this aim, we dis-
tinguish the following three cases.
? Any row has (at least) one vertex in V1 and one
vertex in V2: in this case, it is easy to see there
exist at least r disjoint paths between vertices
of V1 and vertices of V2.
? There exist at least 3r ?mixed? columns, that is,
columns with (at least) one vertex in V1 and one
vertex in V2. Again, it is easy to see that there
exist at least r disjoint paths between vertices
456
Figure 6: The optimal arrangement of G?.
of V1 and vertices of V2 (at least one path every
three columns).
? The previous two cases do not apply. Hence,
there exists a row entirely formed by vertices
of V1 (or, equivalently, of V2). The worst case
is when this row is the smallest one, that is, the
one with (c?3?1)2 + 1 = 4r + 7 vertices. Since
at most 3r ? 1 columns are mixed, we have
that at most (3r ? 1)(r ? 2) = 3r2 ? 7r +
2 vertices of V2 are on these mixed columns.
Since |V2| ? 4r2, this implies that at least r
columns are fully contained in V2. On the other
hand, at least 4r+7?(3r?1) = r+8 columns
are fully contained in V1. If the V1-columns
interleave with the V2-columns, then there exist
at least 2(r?1) disjoint paths between vertices
of V1 and vertices of V2. Otherwise, all the V1-
columns precede or follow all the V2-columns
(this corresponds to the optimal arrangement):
in this case, there are r disjoint paths between
vertices of V1 and vertices of V2.
Observe now that any linear arrangement partitions
the set of vertices in D(r, c) into the sets V1, consist-
ing of the first 4r2 vertices in the arrangement, and
V2, consisting of all the remaining vertices. Since
there are r disjoint paths connecting V1 and V2, there
must be at least r?1 edges passing over every vertex
in the arrangement which is assigned to a position
between the (4r2 + 1)-th and the position 4r2 + 1
from the right end of the arrangement: thus, the
modified cutwidth of any linear arrangement of the
vertices of D(r, c) is at least r ? 1.
We can then conclude that the original proof
of Makedon et al (1985) still applies, according to
relation (2). 
Figure 7: The D(5, 10) component.
We can now reduce from the MODIFIED
CUTWIDTH problem for cubic graphs to the MIN
TIME STRATEGY problem restricted to head-driven
parsing strategies.
Theorem 2 The MIN TIME STRATEGY problem re-
stricted to head-driven parsing strategies is NP-
complete.
PROOF We consider hardness first. Let M and k
be an input instance of the MODIFIED CUTWIDTH
problem restricted to cubic graphs, where M =
(V,E) and V = {v1, . . . , vn}. We construct an
LCFRS production p exactly as in the proof of The-
orem 1, with rhs nonterminals H,A1, . . . , An. We
also set k? = 2 ? k + 2 ? |E| + 9.
Assume now some head-driven parsing strategy pi
for p. After parsing step i ? [n], we have a partial
parse Dpii consisting of the merge of nonterminals
H,Api(1), . . . , Api(i). We write tc(p, pi, i) to denote
the exponent of the time complexity due to step i.
As already mentioned, this quantity is defined as the
sum of the fan-out of the two antecedents involved
in the parsing step and the fan-out of its result:
tc(p, pi, i) = f(Dpii?1) + f(Api(i)) + f(Dpii ) .
Again, we associate with pi a linear arrangement
hpi of M by letting hpi(vpi(i)) = i, for each vi ? V .
As in the proof of Theorem 1, the fan-out of Dpii
is then related to the cutwidth of the linear arrange-
457
ment hpi of M at position i by
f(Dpii ) = |E| + cw(M,hpi, i) .
From the proof of Theorem 1, the fan-out of nonter-
minal Api(i) is twice the degree of vertex vpi(i), de-
noted by |E(vpi(i))|. We can then rewrite the above
equation in terms of our graph M :
tc(p, pi, i) = 2 ? |E| + cw(M,hpi, i? 1) +
+ 2 ? |E(vpi(i))| + cw(M,hpi, i) .
The following general relation between cutwidth
and modified cutwidth is rather intuitive:
mcw(M,hpi, i) =
1
2
? [cw(M,hpi, i? 1) +
? |E(vpi(i))| + cw(M,hpi, i)] .
Combining the two equations above we obtain:
tc(p, pi, i) = 2 ? |E| + 3 ? |E(vpi(i))| +
+ 2 ?mcw(M,hpi, i) .
Because we are restricting M to the class of cubic
graphs, we can write:
tc(p, pi, i) = 2 ? |E| + 9 + 2 ?mcw(M,hpi, i) .
We can thus conclude that there exists a head-driven
parsing strategy for p with time complexity not
greater than 2 ? |E| + 9 + 2 ? k = k? if and only
if mcw(M) ? k.
The membership of MODIFIED CUTWIDTH in NP
follows from an argument similar to the one in the
proof of Theorem 1. 
We have established the NP-completeness of both
the MIN SPACE STRATEGY and the MIN TIME
STRATEGY decision problems. It is now easy to see
that the problem of finding a space- or time-optimal
parsing strategy for a LCFRS production is NP-hard
as well, and thus cannot be solved in polynomial (de-
terministic) time unless P = NP.
4 Concluding remarks
Head-driven strategies are important in parsing
based on LCFRSs, both in order to allow statistical
modeling of head-modifier dependencies and in or-
der to generalize the Markovization of CFG parsers
to parsers with discontinuous spans. However, there
are n! possible head-driven strategies for an LCFRS
production with a head and n modifiers. Choosing
among these possible strategies affects both the time
and the space complexity of parsing. In this paper
we have shown that optimizing the choice according
to either metric is NP-hard. To our knowledge, our
results are the first NP-hardness results for a gram-
mar factorization problem.
SCFGs and STAGs are specific instances of
LCFRSs. Grammar factorization for synchronous
models is an important component of current ma-
chine translation systems (Zhang et al, 2006), and
algorithms for factorization have been studied by
Gildea et al (2006) for SCFGs and by Nesson et al
(2008) for STAGs. These algorithms do not result
in what we refer as head-driven strategies, although,
as machine translation systems improve, lexicalized
rules may become important in this setting as well.
However, the results we have presented in this pa-
per do not carry over to the above mentioned syn-
chronous models, since the fan-out of these models
is bounded by two, while in our reductions in Sec-
tion 3 we freely use unbounded values for this pa-
rameter. Thus the computational complexity of opti-
mizing the choice of the parsing strategy for SCFGs
is still an open problem.
Finally, our results for LCFRSs only apply when
we restrict ourselves to head-driven strategies. This
is in contrast to the findings of Gildea (2011), which
show that, for unrestricted parsing strategies, a poly-
nomial time algorithm for minimizing parsing com-
plexity would imply an improved approximation al-
gorithm for finding the treewidth of general graphs.
Our result is stronger, in that it shows strict NP-
hardness, but also weaker, in that it applies only to
head-driven strategies. Whether NP-hardness can be
shown for unrestricted parsing strategies is an im-
portant question for future work.
Acknowledgments
The first and third authors are partially supported
from the Italian PRIN project DISCO. The sec-
ond author is partially supported by NSF grants IIS-
0546554 and IIS-0910611.
458
References
Ashok K. Chandra and Philip M. Merlin. 1977. Op-
timal implementation of conjunctive queries in rela-
tional data bases. In Proc. ninth annual ACM sympo-
sium on Theory of computing, STOC ?77, pages 77?90.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proc. 35th Annual
Conference of the Association for Computational Lin-
guistics (ACL-97), pages 16?23.
F. Gavril. 1977. Some NP-complete problems on graphs.
In Proc. 11th Conf. on Information Sciences and Sys-
tems, pages 91?95.
Daniel Gildea and Daniel ?Stefankovic?. 2007. Worst-case
synchronous grammar rules. In Proc. 2007 Meeting
of the North American chapter of the Association for
Computational Linguistics (NAACL-07), pages 147?
154, Rochester, NY.
Daniel Gildea, Giorgio Satta, and Hao Zhang. 2006.
Factoring synchronous grammars by sorting. In
Proc. International Conference on Computational
Linguistics/Association for Computational Linguistics
(COLING/ACL-06) Poster Session, pages 279?286.
Daniel Gildea. 2010. Optimal parsing strategies for Lin-
ear Context-Free Rewriting Systems. In Proc. 2010
Meeting of the North American chapter of the Associa-
tion for Computational Linguistics (NAACL-10), pages
769?776.
Daniel Gildea. 2011. Grammar factorization by tree de-
composition. Computational Linguistics, 37(1):231?
248.
Carlos Go?mez-Rodr??guez, Marco Kuhlmann, Giorgio
Satta, and David Weir. 2009. Optimal reduction of
rule length in Linear Context-Free Rewriting Systems.
In Proc. 2009 Meeting of the North American chap-
ter of the Association for Computational Linguistics
(NAACL-09), pages 539?547.
Carlos Go?mez-Rodr??guez, Marco Kuhlmann, and Gior-
gio Satta. 2010. Efficient parsing of well-nested linear
context-free rewriting systems. In Proc. 2010 Meeting
of the North American chapter of the Association for
Computational Linguistics (NAACL-10), pages 276?
284, Los Angeles, California.
John E. Hopcroft and Jeffrey D. Ullman. 1979. Intro-
duction to Automata Theory, Languages, and Compu-
tation. Addison-Wesley, Reading, MA.
Liang Huang, Hao Zhang, Daniel Gildea, and Kevin
Knight. 2009. Binarization of synchronous
context-free grammars. Computational Linguistics,
35(4):559?595.
Laura Kallmeyer and Wolfgang Maier. 2010. Data-
driven parsing with probabilistic linear context-free
rewriting systems. In Proc. 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 537?545.
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Proc. 12th Conference of the European
Chapter of the ACL (EACL-09), pages 478?486.
Thomas Lengauer. 1981. Black-white pebbles and graph
separation. Acta Informatica, 16:465?475.
Wolfgang Maier and Anders S?gaard. 2008. Treebanks
and mild context-sensitivity. In Philippe de Groote,
editor, Proc. 13th Conference on Formal Grammar
(FG-2008), pages 61?76, Hamburg, Germany. CSLI
Publications.
F. S. Makedon, C. H. Papadimitriou, and I. H. Sudbor-
ough. 1985. Topological bandwidth. SIAM J. Alg.
Disc. Meth., 6(3):418?444.
B. Monien and I.H. Sudborough. 1988. Min cut is NP-
complete for edge weighted trees. Theor. Comput.
Sci., 58:209?229.
Rebecca Nesson, Giorgio Satta, and Stuart M. Shieber.
2008. Optimal k-arization of synchronous tree adjoin-
ing grammar. In Proc. 46th Annual Meeting of the
Association for Computational Linguistics (ACL-08),
pages 604?612.
Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting sys-
tems. Theor. Comput. Sci., 223(1-2):87?120.
Beno??t Sagot and Giorgio Satta. 2010. Optimal rank re-
duction for linear context-free rewriting systems with
fan-out two. In Proc. 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 525?533,
Uppsala, Sweden.
Giorgio Satta and Enoch Peserico. 2005. Some com-
putational complexity results for synchronous context-
free grammars. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 803?810, Vancouver, Canada.
H. Seki, T. Matsumura, M. Fujii, and T. Kasami. 1991.
On multiple context-free grammars. Theoretical Com-
puter Science, 88:191?229.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. The Journal of Logic Programming,
24(1-2):3?36.
K. Vijay-Shankar, D. L. Weir, and A. K. Joshi. 1987.
Characterizing structural descriptions produced by
various grammatical formalisms. In Proc. 25th An-
nual Conference of the Association for Computational
Linguistics (ACL-87), pages 104?111.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proc. 2006 Meeting of the North Ameri-
can chapter of the Association for Computational Lin-
guistics (NAACL-06), pages 256?263.
459
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 401?406,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Terminal-Aware Synchronous Binarization
Licheng Fang, Tagyoung Chung and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We present an SCFG binarization algorithm
that combines the strengths of early termi-
nal matching on the source language side and
early language model integration on the tar-
get language side. We also examine how dif-
ferent strategies of target-side terminal attach-
ment during binarization can significantly af-
fect translation quality.
1 Introduction
Synchronous context-free grammars (SCFG) are be-
hind most syntax-based machine translation mod-
els. Efficient machine translation decoding with an
SCFG requires converting the grammar into a bina-
rized form, either explicitly, as in synchronous bina-
rization (Zhang et al, 2006), where virtual nontermi-
nals are generated for binarization, or implicitly, as
in Earley parsing (Earley, 1970), where dotted items
are used.
Given a source-side binarized SCFG with termi-
nal set T and nonterminal set N , the time complex-
ity of decoding a sentence of length n with a m-gram
language model is (Venugopal et al, 2007):
O(n3(|N | ? |T |2(m?1))K)
where K is the maximum number of right-hand-side
nonterminals. SCFG binarization serves two impor-
tant goals:
? Parsing complexity for unbinarized SCFG
grows exponentially with the number of non-
terminals on the right-hand side of grammar
rules. Binarization ensures cubic time decod-
ing in terms of input sentence length.
? In machine translation, integrating language
model states as early as possible is essential to
reducing search errors. Synchronous binariza-
tion (Zhang et al, 2006) enables the decoder to
incorporate language model scores as soon as a
binarized rule is applied.
In this paper, we examine a CYK-like syn-
chronous binarization algorithm that integrates a
novel criterion in a unified semiring parsing frame-
work. The criterion we present has explicit consider-
ation of source-side terminals. In general, terminals
in a rule have a lower probability of being matched
given a sentence, and therefore have the effect of
?anchoring? a rule and limiting its possible applica-
tion points. Hopkins and Langmead (2010) formal-
ized this concept as the scope of a rule. A rule of
scope of k can be parsed in O(nk). The scope of a
rule can be calculated by counting the number of ad-
jacent nonterminal pairs and boundary nonterminals.
For example,
A? w1BCw2D
has scope two. Building on the concept of scope,
we define a cost function that estimates the expected
number of hyperedges to be built when a particular
binarization tree is applied to unseen data. This ef-
fectively puts hard-to-match derivations at the bot-
tom of the binarization tree, which enables the de-
coder to decide early on whether an unbinarized rule
can be built or not.
We also investigate a better way to handle target-
side terminals during binarization. In theory, differ-
ent strategies should produce equivalent translation
results. However, because decoding always involves
401
 0
 2000
 4000
 6000
 8000
 10000
 12000
 14000
 16000
 18000
1 2 3 4 5 6 7
N
um
be
r o
f r
ul
es
Number of right-hand-side nonterminals
Total
Binarizable
Monotonic
Figure 1: Rule Statistics
pruning, we show that different strategies do have a
significant effect in translation quality.
Other works investigating alternative binarization
methods mostly focus on the effect of nonterminal
sharing. Xiao et al (2009) also proposed a CYK-
like algorithm for synchronous binarization. Appar-
ently the lack of virtual nonterminal sharing in their
decoder caused heavy competition between virtual
nonterminals, and they created a cost function to
?diversify? binarization trees, which is equivalent to
minimizing nonterminal sharing.
DeNero et al (2009b) used a greedy method to
maximize virtual nonterminal sharing on the source
side during the -LM parsing phase. They show that
effective source-side binarization can improve the ef-
ficiency of parsing SCFG. However, their method
works only on the source side, and synchronous bina-
rization is put off to the +LM decoding phase (DeN-
ero et al, 2009a).
Although these ideas all lead to faster decoding
and reduced search errors, there can be conflicts in
the constraints each of them has on the form of rules
and accommodating all of them can be a challenge.
In this paper, we present a cubic time algorithm to
find the best binarization tree, given the conflicting
constraints.
2 The Binarization Algorithm
An SCFG rule is synchronously binarizable if when
simultaneously binarizing source and target sides,
virtual nonterminals created by binarizations always
have contiguous spans on both sides (Huang, 2007).
Algorithm 1 The CYK binarization algorithm.
CYK-BINARIZE(X ? ??, ??)
for i = 0 . . . |?| ? 1 do
T [i, i + 1]? cinit(i)
for s = 2 . . . |?| do
for i = 0 . . . |?|-1 do
j ? i + s
for k = i + 1 . . . j ? 1 do
t? T [i, k] + T [k, j] + c(?i, k, j?)
T [i, j]? min(T [i, j], t)
Even with the synchronous binarization constraint,
many possible binarizations exist. Analysis of our
Chinese-English parallel corpus has shown that the
majority of synchronously binarizable rules with ar-
ity smaller than 4 are monotonic, i.e., the target-side
nonterminal permutation is either strictly increasing
or decreasing (See Figure 1). For monotonic rules,
any source-side binarization is also a permissible
synchronous binarization.
The binarization problem can be formulated as a
semiring parsing (Goodman, 1999) problem. We
define a cost function that considers different bina-
rization criteria. A CYK-like algorithm can be used
to find the best binarization tree according to the
cost function. Consider an SCFG rule X ? ??, ??,
where ? and ? stand for the source side and the tar-
get side. Let B(?) be the set of all possible bina-
rization trees for ?. With the cost function c defined
over hyperedges in a binarization tree t, the optimal
binarization tree t? is
t? = argmin
t?B(?)
?
h?t
c(h)
where c(h) is the cost of a hyperedge h in t.
The optimization problem can be solved by Al-
gorithm 1. ?i, k, j? denotes a hyperedge h that con-
nects the spans (i, k) and (k, j) to the span (i, j).
cinit is the initialization for the cost function c. We
can recover the optimal source-side binarization tree
by augmenting the algorithm with back pointers.
Binarized rules are generated by iterating over the
nodes in the optimal binarization tree, while attach-
ing unaligned target-side terminals. At each tree
node, we generate a virtual nonterminal symbol by
concatenating the source span it dominates.
We define the cost function c(h) to be a
tuple of component cost functions: c(h) =
402
(c1(h), c2(h), ...). When two costs a and b are com-
pared, the components are compared piecewise, i.e.
c < c? ? c1 < c?1 ? (c1 = c?1 ? c2 < c?2) ? . . .
If the (min,+) operators on each component cost
satisfy the semiring properties, the cost tuple is also
a semiring. Next, we describe our cost functions and
how we handle target-side terminals.
2.1 Synchronous Binarization as a Cost
We use a binary cost b to indicate whether a binariza-
tion tree is a permissible synchronous binarization.
Given a hyperedge ?i, k, j?, we say k is a permissible
split of the span (i, j) if and only if the spans (i, k)
and (k, j) are both synchronously binarizable and
the span (i, j) covers a consecutive sequence of non-
terminals on the target side. A span is synchronously
binarizable if and only if the span is of length one,
or a permissible split of the span exists. The cost b
is defined as:
b(?i, k, j?) =
{
T if k is a permissible split of (i, j)
F otherwise
binit(i) = T
Under this configuration, the semiring operators
(min,+) defined for the cost b are (?,?). Using b as
the first cost function in the cost function tuple guar-
antees that we will find a tree that is a synchronously
binarized if one exists.
2.2 Early Source-Side Terminal Matching
When a rule is being applied while parsing a sen-
tence, terminals in the rule have less chance of be-
ing matched. We can exploit this fact by taking ter-
minals into account during binarization and placing
terminals lower in the binarization tree. Consider the
following SCFG rule:
VP ? PP?? JJ NN,propose a JJ NN PP
The synchronous binarization algorithm of Zhang et
al. (2006) binarizes the rule1 by finding the right-
most binarizable points on the source side:
1We follow Wu (1997) and use square brackets for straight
rules and pointed brackets for inverted rules. We also mark
brackets with indices to represent virtual nonterminals.
VP ? PP [?? [JJ NN]1]2,[[propose a JJ NN]1]2 PP
The source side of the first binarized rule ?[]1 ? JJ
NN, propose a JJ NN? contains a very frequent non-
terminal sequence ?JJ NN?. If one were to parse
with the binarized rule, and if the virtual nontermi-
nal []1 has been built, the parser needs to continue
following the binarization tree in order to determine
whether the original rule would be matched. Further-
more, having two consecutive nonterminals adds to
complexity since the parser needs to test each split
point.
The following binarization is equally valid but in-
tegrates terminals early:
VP ? PP [[?? JJ]1 NN]2,[[propose a JJ]1 NN]2 PP
Here, the first binarized rule ?[]1 ? ?? JJ, pro-
pose a JJ? anchors on a terminal and enables earlier
pruning of the original rule.
We formulate this intuition by asking the ques-
tion: given a source-side string ?, what binarization
tree, on average, builds the smallest number of hy-
peredges when the rule is applied? This is realized
by defining a cost function e which estimates the
probability of a hyperedge ?i, k, j? being built. We
use a simple model: assume each terminal or non-
terminal in ? is matched independently with a fixed
probability, then a hyperedge ?i, k, j? is derived if
and only if all symbols in the source span (i, j) are
matched. The cost e is thus defined as2
e(?i, k, j?) =
?
i?`<j
p(?`)
einit(i) = 0
For terminals, p(?`) can be estimated by counting
the source side of the training corpus. For nontermi-
nals, we simply assume p(?`) = 1.
With the hyperedge cost e, the cost of a binariza-
tion tree t is
?
h?t e(h), i.e., the expected number of
hyperedges to be built when a particular binarization
of a rule is applied to unseen data.3 The operators
2In this definition, k does not appear on the right-hand side
of the equation because all edges leading to the same span share
the same cost value.
3Although this cost function is defined as an expectation, it
does not form an expectation semiring (Eisner, 2001) because
403
for the cost e are the usual (min,+) operators on
real numbers.
2.3 Maximizing Nonterminal Sharing
During binarization, newly created virtual nontermi-
nals are named according to the symbols (terminals
and nonterminals) that they generate. For example, a
new virtual nonterminal covering two nonterminals
NP and VP is named NP+VP. To achieve maximum
virtual nonterminal sharing, we also define a cost
function n to count the number new nonterminals
generated by a binarization tree. We keep track of
all the nonterminals that have been generated when
binarizing a rule set. When the i?th rule is being
binarized, a nonterminal is considered new if it is
previously unseen in binarizing rules 1 to i?1. This
greedy approach is similar to that of DeNero et al
(2009b). The cost function is thus defined as:
n(?i, k, j?) =
{
1 if the VT for span (i, j) is new
0 otherwise
ninit(i) = 0
The semiring operators for this cost are also
(min,+) on real numbers.
2.4 Late Target-Side Terminal Attachment
Once the optimal source-side binarization tree is
found, we have a good deal of freedom to attach
target-side terminals to adjacent nonterminals, as
long as the bracketing of nonterminals is not vio-
lated. The following example is taken from Zhang
et al (2006):
ADJP ? RB?? PP? NN,RB responsible for the NN PP
With the source-side binarization fixed, we can pro-
duce distinct binarized rules by choosing different
ways of attaching target-side terminals:
ADJP ? [RB??]1 ? [PP?]3 NN ?2,[RB]1 ? resp. for the NN [PP]3 ?2
ADJP ? [RB??]1 ? [PP?]3 NN ?2,[RB]1 resp. for the ? NN [PP]3 ?2
The first binarization is generated by attaching the
target-side terminals as low as possible in a post-
it is defined as an expectation over input strings, instead of an
expectation over trees.
order traversal of the binarization tree. The conven-
tional wisdom is that early consideration of target-
side terminals promotes early language model score
integration (Huang et al, 2009). The second bina-
rization, on the contrary, attaches the target-side ter-
minals as high as possible in the binarization tree.
We argue that this late target-side terminal attach-
ment is in fact better for two reasons.
First, as in the example above, compare the fol-
lowing two rules resulting from early attachment of
target terminals and late attachment of target termi-
nals:
??2 ? []3 NN, resp. for the NN []3
??2 ? []3 NN, NN []3
The former has a much smaller chance of sharing
the same target side with other binarized rules be-
cause on the target side, many nonterminals will be
attached without any lexical evidence. We are more
likely to have a smaller set of rules with the latter
binarization.
Second, with the presence of pruning, dynamic
programming states that are generated by rules with
many target-side terminals are disadvantaged when
competing with others in the same bin because of
the language model score. As a result, these would
be discarded earlier, even if the original unbinarized
rule has a high probability. Consequently, we lose
the benefit of using larger rules, which have more
contextual information. We show in our experiment
that late target side terminal attachment significantly
outperforms early target side terminal attachment.
Although the problem can be alleviated by pre-
computing a language model score for the original
unbinarized rule and applying the heuristic to its bi-
narized rules, this still grants no benefit over late ter-
minal attachment. We show in our experiment that
late target-side terminal attachment significantly out-
performs early target side terminal attachment.
3 Experiments
3.1 Setup
We test our binarization algorithm on an Chinese-
English translation task. We extract a GHKM gram-
mar (Galley et al, 2004) from a parallel corpus with
the parsed English side with some modification so
404
-395
-390
-385
-380
-375
-370
-365
-360
-355
 10  100
M
od
el
 S
co
re
 (lo
g-p
rob
ab
ilit
y)
Seconds / Sentence (log scale)
(b,n)-early
(b,n)-late
(b,e,n)-early
(b,e,n)-late
Figure 2: Model Scores vs. Decoding Time
 17.5
 18
 18.5
 19
 19.5
 20
 20.5
 10  100
BL
EU
Seconds / Sentence (log scale)
(b,n)-early
(b,n)-late
(b,e,n)-early
(b,e,n)-late
Figure 3: BLEU Scores vs Decoding Time
as not to extract unary rules (Chung et al, 2011).
The corpus consists of 250K sentence pairs, which
is 6.3M words on the English side. A 392-sentence
test set was to evaluate different binarizations.
Decoding is performed by a general CYK SCFG
decoder developed in-house and a trigram language
model is used. The decoder runs the CYK algorithm
with cube-pruning (Chiang, 2007). In all our exper-
iments, we discard unbinarizable rules, which have
been shown by Zhang et al (2006) to have no signif-
icant effect on translation accuracy.
3.2 Results
We first discuss effects of maximizing nonterminal
sharing. Having nonterminal sharing maximization
as a part of the cost function for binarization did
yield slightly smaller grammars. However, we could
not discern any noticeable difference or trend in
terms of BLEU score, decoding speed, or model
score when comparing translation results that used
grammars that employed nonterminal sharing max-
imization and ones that did not. In the rest of this
section, all the results we discuss use nonterminal
sharing maximization as a part of the cost function.
We then compare the effects of early target-side
terminal attachment and late attachment. Figure 2
shows model scores of each decoder run with vary-
ing bin sizes, and Figure 3 shows BLEU scores
for corresponding runs of the experiments. (b,n)-
early is conventional synchronous binarization with
early target-side terminal attachment and nontermi-
nal sharing maximization, (b,n)-late is the same set-
ting with late target-side terminal attachment. The
tuples represent cost functions that are discussed in
Section 2. The figures clearly show that late attach-
ment of target-side terminals is better. Although
Figure 3 does not show perfect correlation with Fig-
ure 2, it exhibits the same trend. The same goes for
(b,e,n)-early and (b,e,n)-late.
Finally, we examine the effect of including the
source-side terminal-aware cost function, denoted
?e? in our cost tuples. Comparing (b,e,n)-late with
(b,n)-late, we see that terminal-aware binarization
gives better model scores and BLEU scores. The
trend is the same when one compares (b,e,n)-early
and (b,n)-early.
4 Conclusion
We examined binarizing synchronous context-free
grammars within a semiring parsing framework. We
proposed binarization methods that explicitly take
terminals into consideration. We have found that al-
though binarized rules are already scope 3, we can
still do better by putting infrequent derivations as
low as possible in a binarization tree to promote
early pruning. We have also found that attaching
target side terminals as late as possible promotes
smarter pruning of rules thereby improving model
score and translation quality at decoding time. Im-
provements we discuss in this paper result in better
search, and hence better translation.
Acknowledgments We thank Hao Zhang for use-
ful discussions and the anonymous reviewers for
their helpful comments. This work was supported
by NSF grants IIS-0546554 and IIS-0910611.
405
References
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Tagyoung Chung, Licheng Fang, and Daniel Gildea.
2011. Issues concerning decoding with synchronous
context-free grammar. In Proceedings of the ACL
2011 Conference Short Papers, Portland, Oregon, June.
Association for Computational Linguistics.
J. DeNero, A. Pauls, and D. Klein. 2009a. Asynchronous
binarization for synchronous grammars. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 141?144. Association for Computational
Linguistics.
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.
2009b. Efficient parsing for transducer grammars. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 227?235, Boulder, Colorado, June. Association
for Computational Linguistics.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 6(8):451?455.
J. Eisner. 2001. Expectation semirings: Flexible EM
for learning finite-state transducers. In Proceedings of
the ESSLLI workshop on finite-state methods in NLP.
Citeseer.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of the 2004 Meeting of the North American
chapter of the Association for Computational Linguis-
tics (NAACL-04), pages 273?280.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Mark Hopkins and Greg Langmead. 2010. SCFG decod-
ing without binarization. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 646?655, Cambridge, MA,
October. Association for Computational Linguistics.
Liang Huang, Hao Zhang, Daniel Gildea, and Kevin
Knight. 2009. Binarization of synchronous
context-free grammars. Computational Linguistics,
35(4):559?595.
Liang Huang. 2007. Binarization, synchronous bina-
rization, and target-side binarization. In Proceedings
of the NAACL/AMTA Workshop on Syntax and Struc-
ture in Statistical Translation (SSST), pages 33?40,
Rochester, NY.
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In NAACL07,
Rochester, NY, April.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Compu-
tational Linguistics, 23(3):377?403.
T. Xiao, M. Li, D. Zhang, J. Zhu, and M. Zhou. 2009.
Better synchronous binarization for machine transla-
tion. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing: Vol-
ume 1-Volume 1, pages 362?370. Association for Com-
putational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the 2006 Meeting of the
North American chapter of the Association for Compu-
tational Linguistics (NAACL-06), pages 256?263, New
York, NY.
406
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 413?417,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Issues Concerning Decoding with Synchronous Context-free Grammar
Tagyoung Chung, Licheng Fang and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We discuss some of the practical issues that
arise from decoding with general synchronous
context-free grammars. We examine problems
caused by unary rules and we also examine
how virtual nonterminals resulting from bina-
rization can best be handled. We also inves-
tigate adding more flexibility to synchronous
context-free grammars by adding glue rules
and phrases.
1 Introduction
Synchronous context-free grammar (SCFG) is
widely used for machine translation. There are many
different ways to extract SCFGs from data. Hiero
(Chiang, 2005) represents a more restricted form of
SCFG, while GHKM (Galley et al, 2004) uses a gen-
eral form of SCFG.
In this paper, we discuss some of the practical is-
sues that arise from decoding general SCFGs that
are seldom discussed in the literature. We focus on
parsing grammars extracted using the method put
forth by Galley et al (2004), but the solutions to
these issues are applicable to other general forms of
SCFG with many nonterminals.
The GHKM grammar extraction method produces
a large number of unary rules. Unary rules are the
rules that have exactly one nonterminal and no ter-
minals on the source side. They may be problematic
for decoders since they may create cycles, which are
unary production chains that contain duplicated dy-
namic programming states. In later sections, we dis-
cuss why unary rules are problematic and investigate
two possible solutions.
GHKM grammars often have rules with many
right-hand-side nonterminals and require binariza-
tion to ensure O(n3) time parsing. However, bina-
rization creates a large number of virtual nontermi-
nals. We discuss the challenges of, and possible so-
lutions to, issues arising from having a large num-
ber of virtual nonterminals. We also compare bina-
rizing the grammar with filtering rules according to
scope, a concept introduced by Hopkins and Lang-
mead (2010). By explicitly considering the effect
of anchoring terminals on input sentences, scope-
3 rules encompass a much larger set of rules than
Chomsky normal form but they can still be parsed in
O(n3) time.
Unlike phrase-based machine translation, GHKM
grammars are less flexible in how they can seg-
ment sentence pairs into phrases because they are
restricted not only by alignments between words in
sentence pairs, but also by target-side parse trees. In
general, GHKM grammars suffer more from data
sparsity than phrasal rules. To alleviate this issue,
we discuss adding glue rules and phrases extracted
using methods commonly used in phrase-based ma-
chine translation.
2 Handling unary rules
Unary rules are common in GHKM grammars. We
observed that as many as 10% of the rules extracted
from a Chinese-English parallel corpus are unary.
Some unary rules are the result of alignment er-
rors, but other ones might be useful. For example,
Chinese lacks determiners, and English determiners
usually remain unaligned to any Chinese words. Ex-
tracted grammars include rules that reflect this fact:
NP ? NP, the NP
NP ? NP, a NP
413
However, unary rules can be problematic:
? Unary production cycles corrupt the translation
hypergraph generated by the decoder. A hyper-
graph containing a unary cycle cannot be topo-
logically sorted. Many algorithms for parame-
ter tuning and coarse-to-fine decoding, such as
the inside-outside algorithm and cube-pruning,
cannot be run in the presence of unary cycles.
? The existence of many unary rules of the form
?NP ? NP, the NP? quickly fills a pruning bin
with guesses of English words to insert without
any source-side lexical evidence.
The most obvious way of eliminating problem-
atic unary rules would be converting grammars into
Chomsky normal form. However, this may result
in bloated grammars. In this section, we present
two different ways to handle unary rules. The first
involves modifying the grammar extraction method,
and the second involves modifying the decoder.
2.1 Modifying grammar extraction
We can modify the grammar extraction method such
that it does not extract any unary rules. Galley et al
(2004) extracts rules by segmenting the target-side
parse parse tree based on frontier nodes. We modify
the definition of a frontier node in the following way.
We label frontier nodes in the English parse tree, and
examine the Chinese span each frontier node cov-
ers. If a frontier node covers the same span as the
frontier node that immediately dominates it, then the
dominated node is no longer considered a frontier.
This modification prevents unary rules from being
extracted.
Figure 1 shows an example of an English-Chinese
sentence pair with the English side automatically
parsed. Frontier nodes in the tree in the original
GHKM rule extraction method are marked with a
box. With the modification, only the top bold-
faced NP would be considered a frontier node. The
GHKM rule extraction results in the following rules:
NPB ????, the snowy egret
NP ? NPB, NPB
PP ? NP, with NP
NP ? PP, romance PP
With the change, only the following rule is extracted:
NP
NPB
NNP
romance
PP
IN
with
NP
NPB
DT
the
JJ
snowy
NN
egret
?? ? ? ?
Figure 1: A sentence fragment pair with erroneous align-
ment and tokenization
NP ????, romance with the snowy egret
We examine the effect of this modification has on
translation performance in Section 5.
2.2 Modifying the decoder
Modifying how grammars are extracted has an ob-
vious down side, i.e., the loss of generality. In the
previous example, the modification results in a bad
rule, which is the result of bad alignments. Before
the modification, the rule set includes a good rule:
NPB ????, the snowy egret
which can be applied at test time. Because of this,
one may still want to decode with all available unary
rules. We handle unary rules inside the decoder in
the following ways:
? Unary cycle detection
The na?ve way to detect unary cycles is back-
tracking on a unary chain to see if a newly gen-
erated item has been generated before. The run-
ning time of this is constrained only by the num-
ber of possible items in a chart span. In prac-
tice, however, this is often not a problem: if all
unary derivations have positive costs and a pri-
ority queue is used to expand unary derivations,
414
only the best K unary items will be generated,
where K is the pruning constant.
? Ban negative cost unary rules
When tuning feature weights, an optimizer may
try feature weights that may give negative costs
to unary productions. This causes unary deriva-
tions to go on forever. The solution is to set
a maximum length for unary chains, or to ban
negative unary productions outright.
3 Issues with binarization
3.1 Filtering and binarization
Synchronous binarization (Zhang et al, 2006) is
an effective method to reduce SCFG parsing com-
plexity and allow early language model integration.
However, it creates virtual nonterminals which re-
quire special attention at parsing time. Alternatively,
we can filter rules that have more than scope-3 to
parse in O(n3) time with unbinarized rules. This
requires Earley (Earley, 1970) style parsing, which
does implicit binarization at decoding time. Scope-
filtering may filter out unnecessarily long rules that
may never be applied, but it may also throw out
rules with useful contextual information. In addi-
tion, scope-filtering does not accommodate early lan-
guage model state integration. We compare the two
with an experiment. For the rest of the section, we
discuss issues created by virtual nonterminals.
3.2 Handling virtual nonterminals
One aspect of grammar binarization that is rarely
mentioned is how to assign probabilities to binarized
grammar rules. The na?ve solution is to assign prob-
ability one to any rule whose left-hand side is a vir-
tual nonterminal. This maintains the original model.
However, it is generally not fair to put chart items of
virtual nonterminals and those of regular nontermi-
nals in the same bin, because virtual items have arti-
ficially low costs. One possible solution is adding a
heuristic to push up the cost of virtual items for fair
comparison.
For our experiments, we use an outside estimate
as a heuristic for a virtual item. Consider the follow-
ing rule binarization (only the source side shown):
A ? BCD : ? log(p) ? V ? BC : 0A ? VD : ? log(p)
A ? BCD is the orginal rule and ? log(p) is the cost
of the rule. In decoding time, when a chart item is
generated from the binarized rule V ? BC, we add
? log(p) to its total cost as an optimistic estimate of
the cost to build the original unbinarized rule. The
heuristic is used only for pruning purposes, and it
does not change the real cost. The idea is similar
to A* parsing (Klein and Manning, 2003). One com-
plication is that a binarized rule can arise from multi-
ple different unbinarized rules. In this case, we pick
the lowest cost among the unbinarized rules as the
heuristic.
Another approach for handling virtual nontermi-
nals would be giving virtual items separate bins and
avoiding pruning them at all. This is usually not
practical for GHKM grammars, because of the large
number of nonterminals.
4 Adding flexibility
4.1 Glue rules
Because of data sparsity, an SCFG extracted from
data may fail to parse sentences at test time. For
example, consider the following rules:
NP ? JJ NN, JJ NN
JJ ? c1, e1
JJ ? c2, e2
NN ? c3, e3
This set of rules is able to parse the word sequence
c1 c3 and c2 c3 but not c1 c2 c3, if we have not seen
?NP ? JJ JJ NN? at training time. Because SCFGs
neither model adjunction, nor are they markovized,
with a small amount of data, such problems can oc-
cur. Therefore, we may opt to add glue rules as used
in Hiero (Chiang, 2005):
S ? C, C
S ? S C, S C
where S is the goal state and C is the glue nonter-
minal that can produce any nonterminals. We re-
fer to these glue rules as the monotonic glue rules.
We rely on GHKM rules for reordering when we use
the monotonic glue rules. However, we can also al-
low glue rules to reorder constituents. Wu (1997)
presents a better-constrained grammar designed to
only produce tail-recursive parses. See Table 1 for
the complete set of rules. We refer to these rules as
ABC glue rules. These rules always generate left-
415
S ? A A ? [A B] B ? ? B A ?
S ? B A ? [B B] B ? ? A A ?
S ? C A ? [C B] B ? ? C A ?
A ? [A C] B ? ? B C ?
A ? [B C] B ? ? A C ?
A ? [C C] B ? ? C C ?
Table 1: The ABC Grammar. We follow the convention
of Wu (1997) that square brackets stand for straight rules
and angle brackets stand for inverted rules.
heavy derivations, weeding out ambiguity and mak-
ing search more efficient. We learn probabilities of
ABC glue rules by using expectation maximization
(Dempster et al, 1977) to train a word-level Inver-
sion Transduction Grammar from data.
In our experiments, depending on the configura-
tion, the decoder failed to parse about 5% of sen-
tences without glue rules, which illustrates their ne-
cessity. Although it is reasonable to believe that re-
ordering should always have evidence in data, as
with GHKM rules, we may wish to reorder based
on evidence from the language model. In our ex-
periments, we compare the ABC glue rules with the
monotonic glue rules.
4.2 Adding phrases
GHKM grammars are more restricted than the
phrase extraction methods used in phrase-based
models, since, in GHKM grammar extraction,
phrase segmentation is constrained by parse trees.
This may be a good thing, but it suffers from loss
of flexibility, and it also cannot use non-constituent
phrases. We use the method of Koehn et al (2003)
to extract phrases, and, for each phrase, we add a
rule with the glue nonterminal as the left-hand side
and the phrase pair as the right-hand side. We exper-
iment to see whether adding phrases is beneficial.
There have been other efforts to extend GHKM
grammar to allow more flexible rule extraction. Gal-
ley et al (2006) introduce composed rules where
minimal GHKM rules are fused to form larger rules.
Zollmann and Venugopal (2006) introduce a model
that allows more generalized rules to be extracted.
BLEU
Baseline + monotonic glue rules 20.99
No-unary + monotonic glue rules 23.83
No-unary + ABC glue rules 23.94
No-unary (scope-filtered) + monotonic 23.99
No-unary (scope-filtered) + ABC glue rules 24.09
No-unary + ABC glue rules + phrases 23.43
Table 2: BLEU score results for Chinese-English with
different settings
5 Experiments
5.1 Setup
We extracted a GHKM grammar from a Chinese-
English parallel corpus with the English side parsed.
The corpus consists of 250K sentence pairs, which
is 6.3M words on the English side. Terminal-aware
synchronous binarization (Fang et al, 2011) was ap-
plied to all GHKM grammars that are not scope-
filtered. MERT (Och, 2003) was used to tune pa-
rameters. We used a 392-sentence development set
with four references for parameter tuning, and a 428-
sentence test set with four references for testing. Our
in-house decoder was used for experiments with a
trigram language model. The decoder is capable
of both CNF parsing and Earley-style parsing with
cube-pruning (Chiang, 2007).
For the experiment that incorporated phrases, the
phrase pairs were extracted from the same corpus
with the same set of alignments. We have limited
the maximum size of phrases to be four.
5.2 Results
Our result is summarized in Table 2. The baseline
GHKM grammar with monotonic glue rules yielded
a worse result than the no-unary grammar with the
same glue rules. The difference is statistically signif-
icant at p < 0.05 based on 1000 iterations of paired
bootstrap resampling (Koehn, 2004).
Compared to using monotonic glue rules, using
ABC glue rules brought slight improvements for
both the no-unary setting and the scope-filtered set-
ting, but the differences are not statistically signifi-
cant. In terms of decoding speed and memory usage,
using ABC glues and monotonic glue rules were vir-
tually identical. The fact that glue rules are seldom
used at decoding time may account for why there is
416
little difference in using monotonic glue rules and us-
ing ABC glue rules. Out of all the rules that were ap-
plied to decoding our test set, less than one percent
were glue rules, and among the glue rules, straight
glue rules outnumbered inverted ones by three to
one.
Compared with binarized no-unary rules, scope-
3 filtered no-unary rules retained 87% of the rules
but still managed to have slightly better BLEU score.
However, the score difference is not statistically sig-
nificant. Because the size of the grammar is smaller,
compared to using no-unary grammar, it used less
memory at decoding time. However, decoding speed
was somewhat slower. This is because the decoder
employs Early-style dotted rules to handle unbina-
rized rules, and in order to decode with scope-3
rules, the decoder needs to build dotted items, which
are not pruned until a rule is completely matched,
thus leading to slower decoding.
Adding phrases made the translation result
slightly worse. The difference is not statistically
significant. There are two possible explanations for
this. Since there were more features to tune, MERT
may have not done a good job. We believe the
more important reason is that once a phrase is used,
only glue rules can be used to continue the deriva-
tion, thereby losing the richer information offered
by GHKM grammar.
6 Conclusion
In this paper, we discussed several issues concerning
decoding with synchronous context-free grammars,
focusing on grammars resulting from the GHKM
extraction method. We discussed different ways to
handle cycles. We presented a modified grammar
extraction scheme that eliminates unary rules. We
also presented a way to decode with unary rules in
the grammar, and examined several different issues
resulting from binarizing SCFGs. We finally dis-
cussed adding flexibility to SCFGs by adding glue
rules and phrases.
Acknowledgments We would like to thank the
anonymous reviewers for their helpful comments.
This work was supported by NSF grants IIS-
0546554 and IIS-0910611.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL-05, pages 263?270, Ann Arbor, MI.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(1):1?21.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 6(8):451?455.
Licheng Fang, Tagyoung Chung, and Daniel Gildea.
2011. Terminal-aware synchronous binarization. In
Proceedings of the ACL 2011 Conference Short Pa-
pers, Portland, Oregon, June. Association for Compu-
tational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of NAACL-04, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-
rich syntactic translation models. In Proceedings of
COLING/ACL-06, pages 961?968, July.
Mark Hopkins and Greg Langmead. 2010. SCFG decod-
ing without binarization. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 646?655, Cambridge, MA,
October. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: Fast exact Viterbi parse selection. In Proceedings
of NAACL-03.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL-03, Edmonton, Alberta.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388?395, Barcelona, Spain, July.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of ACL-
03.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Compu-
tational Linguistics, 23(3):377?403.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of NAACL-06, pages 256?
263, New York, NY.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proc. Workshop on Statistical Machine Translation,
pages 138?141.
417
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 306?310,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Improving the IBM Alignment Models Using Variational Bayes
Darcey Riley and Daniel Gildea
Computer Science Dept.
University of Rochester
Rochester, NY 14627
Abstract
Bayesian approaches have been shown to re-
duce the amount of overfitting that occurs
when running the EM algorithm, by placing
prior probabilities on the model parameters.
We apply one such Bayesian technique, vari-
ational Bayes, to the IBM models of word
alignment for statistical machine translation.
We show that using variational Bayes im-
proves the performance of the widely used
GIZA++ software, as well as improving the
overall performance of the Moses machine
translation system in terms of BLEU score.
1 Introduction
The IBM Models of word alignment (Brown et
al., 1993), along with the Hidden Markov Model
(HMM) (Vogel et al, 1996), serve as the starting
point for most current state-of-the-art machine trans-
lation systems, both phrase-based and syntax-based
(Koehn et al, 2007; Chiang, 2005; Galley et al,
2004).
Both the IBM Models and the HMM are
trained using the EM algorithm (Dempster et al,
1977). Recently, Bayesian techniques have become
widespread in applications of EM to natural lan-
guage processing tasks, as a very general method of
controlling overfitting. For instance, Johnson (2007)
showed the benefits of such techniques when ap-
plied to HMMs for unsupervised part of speech tag-
ging. In machine translation, Blunsom et al (2008)
and DeNero et al (2008) use Bayesian techniques to
learn bilingual phrase pairs. In this setting, which in-
volves finding a segmentation of the input sentences
into phrasal units, it is particularly important to con-
trol the tendency of EM to choose longer phrases,
which explain the training data well but are unlikely
to generalize.
However, most state-of-the-art machine transla-
tion systems today are built on the basis of word-
level alignments of the type generated by GIZA++
from the IBM Models and the HMM. Overfitting is
also a problem in this context, and improving these
word alignment systems could be of broad utility in
machine translation research.
Moore (2004) discusses details of how EM over-
fits the data when training IBM Model 1. He dis-
covers that the EM algorithm is particularly suscep-
tible to overfitting in the case of rare words, due to
the ?garbage collection? phenomenon. Suppose a
sentence contains an English word e1 that occurs
nowhere else in the data, and its French transla-
tion f1. Suppose that same sentence also contains a
word e2 which occurs frequently in the overall data
but whose translation in this sentence, f2, co-occurs
with it infrequently. If the translation t(f2|e2) oc-
curs with probability 0.1, then the sentence will have
a higher probability if EM assigns the rare word and
its actual translation a probability of t(f1|e1) = 0.5,
and assigns the rare word?s translation to f2 a prob-
ability of t(f2|e1) = 0.5, than if it assigns a proba-
bility of 1 to the correct translation t(f1|e1). Moore
suggests a number of solutions to this issue, includ-
ing add-n smoothing and initializing the probabili-
ties based on a heuristic rather than choosing uni-
form probabilities. When combined, his solutions
cause a significant decrease in alignment error rate
(AER). More recently, Mermer and Saraclar (2011)
have added a Bayesian prior to IBM Model 1 us-
ing Gibbs sampling for inference, showing improve-
ments in BLEU scores.
In this paper, we describe the results of incorpo-
306
rating variational Bayes (VB) into the widely used
GIZA++ software for word alignment. We use VB
both because it converges more quickly than Gibbs
sampling, and because it can be applied in a fairly
straightforward manner to all of the models imple-
mented by GIZA++. In Section 2, we describe VB
in more detail. In Section 3, we present results for
VB for the various models, in terms of perplexity of
held-out test data, alignment error rate (AER), and
the BLEU scores which result from using our ver-
sion of GIZA++ in the end-to-end phrase-based ma-
chine translation system Moses.
2 Variational Bayes and GIZA++
Beal (2003) gives a detailed derivation of a varia-
tional Bayesian algorithm for HMMs. The result is
a very slight change to the M step of the original
EM algorithm. During the M step of the original al-
gorithm, the expected counts collected in the E step
are normalized to give the new values of the param-
eters:
?xi|y =
E[c(xi|y)]
?
j E[c(xj |y)]
(1)
The variational Bayesian M step performs an inexact
normalization, where the resulting parameters will
add up to less than one. It does this by passing
the expected counts collected in the E step through
the function f(v) = exp(?(v)), where ? is the
digamma function, and ? is the hyperparameter of
the Dirichlet prior (Johnson, 2007):
?xi|y =
f(E[c(xi|y)] + ?)
f(
?
j(E[c(xj |y)] + ?))
(2)
This modified M step can be applied to any model
which uses a multinomial distribution; for this rea-
son, it works for the IBM Models as well as HMMs,
and is thus what we use for GIZA++.
In practice, the digamma function has the effect
of subtracting 0.5 from its argument. When ? is
set to a low value, this results in ?anti-smoothing?.
For the translation probabilities, because about 0.5
is subtracted from the expected counts, small counts
corresponding to rare co-occurrences of words will
be penalized heavily, while larger counts will not be
affected very much. Thus, low values of ? cause
the algorithm to favor words which co-occur fre-
quently and to distrust words that co-occur rarely.
Sentence pair count
e2 9
f3
e2 2
f2
e1 e2 1
f1 f2
Table 1: An example of data with rare words.
In this way, VB controls the overfitting that would
otherwise occur with rare words. On the other hand,
higher values of ? can be chosen if smoothing is de-
sired, for instance in the case of the alignment prob-
abilities, which state how likely a word in position i
of the English sentence is to align to a word in po-
sition j of the French sentence. For these probabili-
ties, smoothing is important because we do not want
to rule out any alignment altogether, no matter how
infrequently it occurs in the data.
We implemented VB for the translation probabil-
ities as well as for the position alignment probabili-
ties of IBM Model 2. We discovered that adding VB
for the translation probabilities improved the perfor-
mance of the system. However, including VB for
the alignment probabilities had relatively little ef-
fect, because the alignment table in its original form
does some smoothing during normalization by inter-
polating the counts with a uniform distribution. Be-
cause VB can itself be a form of smoothing, the two
versions of the code behave similarly. We did not
experiment with VB for the distortion probabilities
of the HMM or Models 3 and 4, as these distribu-
tions have fewer parameters and are likely to have
reliable counts during EM. Thus, in Section 3, we
present the results of using VB for the translation
probabilities only.
3 Results
First, we ran our modified version of GIZA++ on a
simple test case designed to be similar to the exam-
ple from Moore (2004) discussed in Section 1. Our
test case, shown in Table 1, had three different sen-
tence pairs; we included nine instances of the first,
two instances of the second, and one of the third.
Human intuition tells us that f2 should translate to
e2 and f1 should translate to e1. However, the EM
algorithm without VB prefers e1 as the translation
307
0.1
0
0.1
5
0.2
0
0.2
5
0.3
0
0.3
5
0.4
0
0.4
5
0.5
0
 
1e-
10
 
1e-
08
 
1e-
06
 
0.0
001
 
0.0
1
 
1
AER
Alp
ha
AE
R a
fter
 En
tire
 Tr
ain
ing
Fre
nch
 (ba
selin
e)
Fre
nch
 (va
riati
ona
l Ba
yes)
Ch
ine
se 
(bas
eline
)
Ch
ine
se 
(var
iatio
nal 
Bay
es)
Ge
rma
n (b
ase
line)
Ge
rma
n (v
aria
tion
al B
aye
s)
Figure 1: Determining the best value of ? for the transla-
tion probabilities. Training data is 10,000 sentence pairs
from each language pair. VB is used for Model 1 only.
This table shows the AER for different values of ? af-
ter training is complete (five iterations each of Models 1,
HMM, 3, and 4).
of f2, due to the ?garbage collection? phenomenon
described above. The EM algorithm with VB does
not overfit this data and prefers e2 as f2?s translation.
For our experiments with bilingual data, we used
three language pairs: French and English, Chi-
nese and English, and German and English. We
used Canadian Hansard data for French-English,
Europarl data for German-English, and newswire
data for Chinese-English. For measuring align-
ment error rate, we used 447 French-English sen-
tences provided by Hermann Ney and Franz Och
containing both sure and possible alignments, while
for German-English we used 220 sentences pro-
vided by Chris Callison-Burch with sure alignments
only, and for Chinese-English we used the first 400
sentences of the data provided by Yang Liu, also
with sure alignments only. For computing BLEU
scores, we used single reference datasets for French-
English and German-English, and four references
for Chinese-English. For minimum error rate train-
ing, we used 1000 sentences for French-English,
2000 sentences for German-English, and 1274 sen-
tences for Chinese-English. Our test sets con-
tained 1000 sentences each for French-English and
German-English, and 686 sentences for Chinese-
English. For scoring the Viterbi alignments of each
system against gold-standard annotated alignments,
 
400
 
600
 
800
 
100
0
 
120
0
 
140
0
 
160
0  5
 
10
 
15
 
20
 
25
Test Perplexity
Iter
atio
ns 
of M
ode
l 1
Mo
del
 1 S
usc
ept
ibili
ty t
o O
ver
fittin
g
Fre
nch
 (ba
selin
e)
Fre
nch
 (va
riati
ona
l Ba
yes)
Figure 2: Effect of variational Bayes on overfitting for
Model 1. Training data is 10,000 sentence pairs. This
table contrasts the test perplexities of Model 1 with vari-
ational Bayes and Model 1 without variational Bayes af-
ter different numbers of training iterations. Variational
Bayes successfully controls overfitting.
we use the alignment error rate (AER) of Och and
Ney (2000), which measures agreement at the level
of pairs of words.
We ran our code on ten thousand sentence pairs
to determine the best value of ? for the transla-
tion probabilities t(f |e). For our training, we ran
GIZA++ for five iterations each of Model 1, the
HMM, Model 3, and Model 4. Variational Bayes
was only used for Model 1. Figure 1 shows how VB,
and different values of ? in particular, affect the per-
formance of GIZA++ in terms of AER. We discover
that, after all training is complete, VB improves the
performance of the overall system, lowering AER
(Figure 1) for all three language pairs. We find that
low values of ? cause the most consistent improve-
ments, and so we use ? = 0 for the translation prob-
abilities in the remaining experiments. Note that,
while a value of ? = 0 does not define a proba-
bilistically valid Dirichlet prior, it does not cause any
practical problems in the update equation for VB.
Figure 2 shows the test perplexity after GIZA++
has been run for twenty-five iterations of Model 1:
without VB, the test perplexity increases as training
continues, but it remains stable when VB is used.
Thus, VB eliminates the need for the early stopping
that is often employed with GIZA++.
After choosing 0 as the best value of ? for the
308
0.0
5
0.1
0
0.1
5
0.2
0
0.2
5
0.3
0
0.3
5
0.4
0
0.4
5
0.5
0
 
100
00
 
300
00
 
500
00
 
700
00
 
900
00
AER
Co
rpu
s S
ize
AE
R f
or D
iffe
ren
t C
orp
us 
Siz
es
Fre
nch
 (ba
selin
e)
Fre
nch
 (va
riati
ona
l Ba
yes)
Ch
ine
se 
(bas
eline
)
Ch
ine
se 
(var
iatio
nal 
Bay
es)
Ge
rma
n (b
ase
line)
Ge
rma
n (v
aria
tion
al B
aye
s)
Figure 3: Performance of GIZA++ on different amounts
of test data. Variational Bayes is used for Model 1 only.
Table shows AER after all the training has completed
(five iterations each of Models 1, HMM, 3, and 4).
AER
French Chinese German
Baseline 0.14 0.42 0.43
M1 Only 0.12 0.39 0.41
HMM Only 0.14 0.42 0.42
M3 Only 0.14 0.42 0.43
M4 Only 0.14 0.42 0.43
All Models 0.19 0.44 0.45
Table 2: Effect of Adding Variational Bayes to Specific
Models
translation probabilities, we reran the test above
(five iterations each of Models 1, HMM, 3, and
4, with VB turned on for Model 1) on different
amounts of data. We found that the results for larger
data sizes were comparable to the results for ten
thousand sentence pairs, both with and without VB
(Figure 3).
We then tested whether VB should be used for the
later models. In all of these experiments, we ran
Models 1, HMM, 3, and 4 for five iterations each,
training on the same ten thousand sentence pairs that
we used in the previous experiments. In Table 2, we
show the performance of the system when no VB is
used, when it is used for each of the four models in-
dividually, and when it is used for all four models
simultaneously. We saw the most overall improve-
ment when VB was used only for Model 1; using VB
for all four models simultaneously caused the most
improvement to the test perplexity, but at the cost of
BLEU Score
French Chinese German
Baseline 26.34 21.03 21.14
M1 Only 26.54 21.58 21.73
All Models 26.46 22.08 21.96
Table 3: BLEU Scores
the AER.
For the MT experiments, we ran GIZA++ through
Moses, training Model 1, the HMM, and Model 4 on
100,0 0 sentence pairs from each language pair. We
ran three experiments, one with VB turned on for all
models, one with VB turned on for Model 1 only,
and one (the baseline) with VB turned off for all
models. When VB was turned on, we ran GIZA++
for five iterations per model as in our earlier tests,
but when VB was turned off, we ran GIZA++ for
only four iterations per model, having determined
that this was the optimal number of iterations for
baseline system. VB was used for the translation
probabilities only, with ? set to 0.
As can be seen in Table 3, using VB increases
the BLEU score for all three language pairs. For
French, the best results were achieved when VB was
used for Model 1 only; for Chinese and German, on
the other hand, using VB for all models caused the
most improvements. For French, the BLEU score
increased by 0.20; for German, it increased by 0.82;
for Chinese, it increased by 1.05. Overall, VB seems
to have the greatest impact on the language pairs that
are most difficult to align and translate to begin with.
4 Conclusion
We find that applying variational Bayes with a
Dirichlet prior to the translation models imple-
mented in GIZA++ improves alignments, both in
terms of AER and the BLEU score of an end-to-end
translation system. Variational Bayes is especially
beneficial for IBM Model 1, because its lack of fer-
tility and position information makes it particularly
susceptible to the garbage collection phenomenon.
Applying VB to Model 1 alone tends to improve
the performance of later models in the training se-
quence. Model 1 is an essential stepping stone in
avoiding local minima when training the following
models, and improvements to Model 1 lead to im-
provements in the end-to-end system.
309
References
Matthew J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Univer-
sity College London.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian synchronous grammar induction. In Neural
Information Processing Systems (NIPS).
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL-05, pages 263?270, Ann Arbor, MI.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(1):1?21.
John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 314?323, Honolulu, Hawaii, October.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of NAACL-04, pages 273?280, Boston.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 296?305,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL, Demonstration Session, pages 177?180.
Coskun Mermer and Murat Saraclar. 2011. Bayesian
word alignment for statistical machine translation. In
Proceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-11), pages
182?187.
Robert C. Moore. 2004. Improving IBM word alignment
Model 1. In Proceedings of the 42nd Meeting of the
Association for Computational Linguistics (ACL?04),
Main Volume, pages 518?525, Barcelona, Spain, July.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL-
00, pages 440?447, Hong Kong, October.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In COLING-96, pages 836?841.
310
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 64?72,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Plurality, Negation, and Quantification:
Towards Comprehensive Quantifier Scope Disambiguation
Mehdi Manshadi, Daniel Gildea, and James Allen
University of Rochester
734 Computer Studies Building
Rochester, NY 14627
mehdih,gildea,james@cs.rochester.edu
Abstract
Recent work on statistical quantifier scope
disambiguation (QSD) has improved upon
earlier work by scoping an arbitrary num-
ber and type of noun phrases. No corpus-
based method, however, has yet addressed
QSD when incorporating the implicit uni-
versal of plurals and/or operators such as
negation. In this paper we report early,
though promising, results for automatic
QSD when handling both phenomena. We
also present a general model for learning
to build partial orders from a set of pair-
wise preferences. We give an n log n algo-
rithm for finding a guaranteed approxima-
tion of the optimal solution, which works
very well in practice. Finally, we signifi-
cantly improve the performance of the pre-
vious model using a rich set of automati-
cally generated features.
1 Introduction
The sentence there is one faculty member in ev-
ery graduate committee is ambiguous with respect
to quantifier scoping, since there are at least two
possible readings: If one has wide scope, there is
a unique faculty member on every committee. If
every has wide scope, there can be different fac-
ulty members on each committee. Over the past
decade there has been some work on statistical
quantifier scope disambiguation (QSD) (Higgins
and Sadock, 2003; Galen and MacCartney, 2004;
Manshadi and Allen, 2011a). However, the extent
of the work has been quite limited for several rea-
sons. First, in the past two decades, the main focus
of the NLP community has been on shallow text
processing. As a deep processing task, QSD is not
essential for many NLP applications that do not re-
quire deep understanding. Second, there has been
a lack of comprehensive scope-disambiguated cor-
pora, resulting in the lack of work on extensive
statistical QSD. Third, QSD has often been con-
sidered only in the context of explicit quantifica-
tion such as each and every versus some and a/an.
These co-occurrences do not happen very often in
real-life data. For example, Higgins and Sadock
(2003) find fewer than 1000 sentences with two or
more explicit quantifiers in the Wall Street journal
section of Penn Treebank. Furthermore, for more
than 60% of those sentences, the order of the quan-
tifiers does not matter, either as a result of the logi-
cal equivalence (as in two existentials), or because
they do not have any scope interaction.
Having said that, with deep language processing
receiving more attention in recent years, QSD is
becoming a real-life issue.1 At the same time, new
scope-disambiguated corpora have become avail-
able (Manshadi et al, 2011b). In this paper, we
aim at tackling the third issue mentioned above.
We push statistical QSD beyond explicit quantifi-
cation, and address an interesting, yet practically
important, problem in QSD: plurality and quan-
tification. In spite of an extensive literature in
theoretical semantics (Hamm and Hinrichs, 2010;
Landmann, 2000), this topic has not been well in-
vestigated in computational linguistics. To illus-
trate the phenomenon, consider (1):
1. Three words start with a capital letter.
A deep understanding of this sentence, requires
deciding whether each word in the set, referred
to by Three words, starts with a potentially dis-
tinct capital letter (as in Apple, Orange, Banana)
or there is a unique capital letter which each word
starts with (as in Apple, Adam, Athens). By treat-
ing the NP Three words as a single atomic entity,
earlier work on automatic QSD has overlooked
this problem. In general, every plural NP poten-
tially introduces an implicit universal, ranging
1For example, Liang et al (2011) in their state-of-the-art
statistical semantic parser within the domain of natural lan-
guage queries to databases, explicitly devise quantifier scop-
ing in the semantic model.
64
over the collection of entities introduced by the
plural.2 Scoping this implicit universal is just as
important. While explicit universals may not oc-
cur very often in natural language, the usage of
plurals is very common. Plurals form 18% of the
NPs in our corpus and 20% of the nouns in Penn
Treebank. Explicit universals, on the other hand,
form less than 1% of the determiners in Penn Tree-
bank. Quantifiers are also affected by negation.
Previous work (e.g., Morante and Blanco, 2012)
has investigated automatically detecting the scope
and focus of negation. However, the scope of
negation with respect to quantifiers is a different
phenomenon. Consider the following sentence.
2. The word does not start with a capital letter.
Transforming this sentence into a meaning repre-
sentation language, for almost any practical pur-
poses, requires deciding whether the NP a capital
letter lies in the scope of the negation or outside
of it. The former describes the preferred reading
where The word starts with a lowercase letter as
in apple, orange, banana, but the latter gives the
unlikely reading, according to which there exists a
particular capital letter, say A, that The word starts
with, as in apple, Orange, Banana. By not in-
volving negation in quantifier scoping, a semantic
parser may produce an unintended interpretation.
Previous work on statistical QSD has been quite
restricted. Higgins and Sadock (2003), which
we refer to as HS03, developed the first statisti-
cal QSD system for English. Their system dis-
ambiguates the scope of exactly two explicitly
quantified NPs in a sentence, ignoring indefinite
a/an, definites and bare NPs. Manshadi and Allen
(2011a), hence MA11, go beyond those limita-
tions and scope an arbitrary number of NPs in a
sentence with no restriction on the type of quantifi-
cation. However, although their corpus annotates
the scope of negations and the implicit universal of
plurals, their QSD system does not handle those.
As a step towards comprehensive automatic
QSD, in this paper we present our work on auto-
matic scoping of the implicit universal of plurals
and negations. For data, we use a new revision
of MA11?s corpus, first introduced in Manshadi et
al. (2011b). The new revision, called QuanText,
carries a more detailed, fine-grained scope annota-
tion (Manshadi et al, 2012). The performance of
2Although plurals carry different types of quantification
(Herbelot and Copestake, 2010), almost always there exists
an implicit universal. The importance of scoping this univer-
sal, however, may vary based on the type of quantification.
our model defines a baseline for future efforts on
(comprehensive) QSD over QuanText. In addition
to addressing plurality and negation, this work im-
proves upon MA11?s in two directions.
? We theoretically justify MA11?s ternary-
classification approach, formulating it as a
general framework for learning to build par-
tial orders. An n log n algorithm is then given
to find a guaranteed approximation within a
fixed ratio of the optimal solution from a set
of pairwise preferences (Sect. 3.1).
? We replace MA11?s hand-annotated features
with a set of automatically generated linguis-
tic features. Our rich set of features signifi-
cantly improves the performance of the QSD
model, even though we give up the gold-
standard dependency features (Sect. 3.3).
2 Task definition
In QuanText, scope-bearing elements (or, as we
call them, scopal terms) of each sentence have
been identified using labeled chunks, as in (3).
3. Replace [1/ every line] in [2/ the file] ending
in [3/ punctuation] with [4/ a blank line] .
NP chunks follow the definition of baseNP
(Ramshaw and Marcus, 1995) and hence are flat.
Outscoping relations are used to specify the rel-
ative scope of scopal terms. The relation i > j
means that chunk i outscopes (or has wide scope
over) chunk j. Equivalently, chunk j is said to
have narrow scope with respect to i. Each sen-
tence is annotated with its most preferred scoping
(according to the annotators? judgement), repre-
sented as a partial order:
4. SI : (2 > 1 > 4; 1 > 3)
If neither i > j nor j > i is entailed from the
scoping, i and j are incomparable. This happens
if both orders are equivalent (as in two existentials)
or when the two chunks have no scope interaction.
Since a partial order can be represented by a Di-
rected Acyclic Graph (DAG), we use DAGs to
represent scopings. For example, G1 in Figure 1
represents the scoping in (4).
2.1 Evaluation metrics
Given the gold standard DAG Gg = (V,Eg) and
the predicted DAG Gp = (V,Ep), a similarity
measure may be defined based on the ratio of the
number of pairs (of nodes) labeled correctly to the
65
21
3 4
(a) G1
2
1
3 4
(b) G+1
2
1 4
3
(c) G2
2 1 3 4
(d) G3
Figure 1: Scoping as DAG
total number of pairs. In order to take the transi-
tivity of outscoping relations into account, we use
the transitive closure (TC) of DAGs. Let G+ =
(V,E+) represent the TC of a DAG G = (V,E).3
G1 and G+1 in Figure 1 illustrate this concept. We
now define the similiarty metric S+ as follows:
?+ =
|E+p ? E+g | ? |E?+p ? E?+g |
|V |(|V | ? 1)/2 (1)
in which G? = (V, E?) is the complement of the
underlying undirected version of G.
HS03 and others have used such a similarity
measure for evaluation purposes. A disadvantage
of this metric is that it gives the same weight to
outscoping and incomparability relations. In prac-
tice, if two scopal terms with equivalent ordering
(and hence, no outscoping relation) are incorrectly
labeled with an outscoping, the logical form still
remains valid. But if an outscoping relation is mis-
labeled, it will change the interpretation of the sen-
tence. Therefore, in MA11, we suggest defining a
precision/recall based on the number of outscop-
ing relations recovered correctly: 4
P+ =
|E+p ? E+g |
|E+p |
, R+ =
|E+p ? E+g |
|E+g |
(2)
3 (u, v) ? G+ ?? ((u, v)?G ?
?w1 . . . wn?V, (u,w1) . . . (wn, v) ? E )
4MA11 argues that TC-based metrics tend to produce
higher numbers. For example if G3 in Figure 1 is a gold-
standard DAG andG1 is a candidate DAG, TC-based metrics
count 2>3 as another match, even though it is entailed from
2 > 1 and 1 > 3. They give an alternative metric based on
transitive reduction (TR), obtained by removing all the re-
dundant edges of a DAG. TR-based metrics, however, have
their own disadvantage. For example, if G2 is another candi-
date forG3, TR-based metrics produce the same numbers for
both G1 and G2, even though G1 is clearly closer to G3 than
G2. Therefore, in this paper we stick to TC-based metrics.
3 Our framework
3.1 Learning to do QSD
Since we defined QSD as a partial ordering, auto-
matic QSD would become the problem of learn-
ing to build partial orders. The machine learning
community has studied the problem of learning to-
tal orders (ranking) in depth (Cohen et al, 1999;
Furnkranz and Hullermeier, 2003; Hullermeier et
al., 2008). Many ranking systems create partial
orders as output when the confidence level for the
relative order of two objects is below some thresh-
old. However, the target being a partial order is
a fundamentally different problem. While the lack
of order between two elements is interpreted as the
lack of confidence in the former, it should be inter-
preted as incomparability in the latter. Learning
to build partial orders has not attracted much atten-
tion in the learning community, although as seen
shortly, the techniques developed for ranking can
be adopted for learning to build partial orders.
As mentioned before, a partial order P can be
represented by a DAG G, with a preceding b in P
if and only if a reaches b in G by a directed path.
Although there could be many DAGs representing
a partial order P , only one of those is a transitive
DAG.5 Therefore, in order to have a one-to-one re-
lationship between QSDs and DAGs, we only con-
sider the class of transitive DAGs, or TDAG. Ev-
ery non-transitive DAG will be converted into its
transitive counterpart by taking its transitive clo-
sure (as shown in Figure 1).
Consider V , a set of nodes and a TDAG G =
(V,E). It would help to think of disconnected
nodes u, v of G, as connected with a null edge .
We define the labeling function ?G : V ? V ??
{+,?, } assigning one of the three labels to each
pair of nodes in G:
?G(u, v) =
?
?
?
+ (u, v) ? G
? (v, u) ? G
 otherwise
(3)
Given the true TDAG G? = (V, E?), and a candidate
TDAG G, we define the Loss function to be the
total number of incorrect edges:
L(G, G?) =
?
u?v?V
I(?G(u, v) 6= ?G?(u, v)) (4)
in which ? is an arbitrary total order over the
nodes in V 6, and I(?) is the indicator function. We
5G is transitive iff (u, v), (v, w) ? G =? (u,w) ? G.
6E.g., the left-to-right order of the corresponding chunks
in the sentence.
66
adopt a minimum Bayes risk (MBR) approach,
with the goal of finding the graph with the lowest
expected loss against the (unknown) target graph:
G? = argmin
G?TDAG
EG?
[
L(G, G?)
]
(5)
Substituting in the definition of the loss function
and exchanging the order of the expectation and
summation, we get:
G? = argmin
G?TDAG
?
u?v?V
EG?
[
I(?G(u, v) 6= ?G?(u, v)
]
= argmin
G?TDAG
?
u?v?V
P (?G(u, v) 6= ?G?(u, v)) (6)
This means that in order to solve Eq. (5), we need
only the probabilities of each of the three labels for
each of the C(n, 2) = n(n? 1)/2 pairs of nodes7
in the graph, rather than a probability for each
of the superexponentially many possible graphs.
We train a classifier to estimate these probabili-
ties directly for a given pair. Therefore, we have
reduced the problem of predicting a partial order
to pairwise comparison, analogous to ranking by
pairwise comparison or RPC (Hullermeier et al,
2008; Furnkranz and Hullermeier, 2003), a popu-
lar technique in learning total orders. The differ-
ence though is that in RPC, the comparison is a
(soft) binary classification, while for partial orders
we have the case of incomparability (the label ),
hence a (soft) ternary classification.
A soft ternary classifier generates three proba-
bilities, pu,v(+), pu,v(?), and pu,v() for each pair
(u, v),8 corresponding to the three labels. Hence,
equation Eq. (6) can be rearranged as follows:
G? = argmax
G?TDAG
?
u?v?V
pu,v(?G(u, v)) (7)
Let ?p be a graph like the one in Figure 2, contain-
ing exactly three edges between every two nodes,
weighted by the probabilities from the n(n? 1)/2
classifiers. We call ?p the preference graph. In-
tuitively speaking, the solution to Eq. (7) is the
transitive directed acyclic subgraph of ?p that has
the maximum sum of weights. Unfortunately find-
ing this subgraph is an NP-hard problem.9
7Throughout this subsection, unless otherwise specified,
by a pair of nodes we mean a pair (u, v) with u?v.
8pv,u for u?v is defined in the obvious way: pv,u(+) =
pu,v(?), pv,u(?) = pu,v(+), and pv,u() = pu,v().
9 The proof is beyond the scope of this paper, but the idea
is similar to that of Cohen et al (1999), on finding total or-
ders. Although they don?t use an RPC technique, Cohen et
3
2
0.5
10.10.8
0.2
0.3
0.1
0.3
0.1
0.6
Figure 2: A preference graph over three nodes.
1. Let ?p be the preference graph and
set G to ?.
2. ?u ? V , let pi(u) =?v pu,v(+)?
?
v pu,v(?).
3. Let u? = argmaxu pi(u),
S? =
?
v?G pv,u?(?) & S =
?
v?G pv,u?().
4. Remove u? and all its incident edges
from ?p.
5. Add u? to G; also if S? > S, for
every v ? G? u?, add (v, u?) to G.
6. If ?p is empty, output G, otherwise
repeat steps 2-5.
Figure 3: An approximation algorithm for Eq. (7)
Since it is very unlikely to find an efficient al-
gorithm to solve Eq. (7), instead, we propose the
algorithm in Figure 3 which finds an approximate
solution. The idea of the algorithm is simple. By
finding u? with the highest pi(u) in step 3, we form
a topological order for the nodes in G in a greedy
way (see Footnote 9). We then add u? to G. A
directed edge is added either from every node in
G?u? to u? or from no node, depending on which
case makes the sum of the weights in G higher.
Theorem 1 The algorithm in Figure 3 is a 1/3-
OPT approximation algorithm for Eq. (7).
Proof idea. First of all, note that G is a TDAG,
because edges are only added to the most recently
created node in step 5. Let OPT be the optimum
value of the right hand side of Eq. (7). The sum of
all the weights in ?p is an upper bound for OPT :
?
u?v?V
?
??{+,?,}
pu,v(?) ? OPT
Step 5 of the algorithm guarantees that the labels
?G(u, v) satisfy:
?
u?v?V
pu,v(?G(u, v)) ?
?
u?v?V
pu,v(?) (8)
al. (1999) encounter a similar optimization problem. They
propose an approximation algorithm which finds the solution
(a total order) in a greedy way. Here we use the same greedy
technique to find a total order, but take it only as the topolog-
ical order of the solution (Figure 3).
67
for any ? ? {+,?, }. Hence:
?
u?v?V
pu,v(?G(u, v))=
1
3
(
3
?
u?v?V
pu,v(?G(u, v))
)
? 13
?
u?v?V
?
??{+,?,}
pu,v(?)
? 13OPT
In practice, we improve the algorithm in Figure 3,
while maintaining the approximation guarantee, as
follows. When adding a node u? to graph G, we
do not make a binary decision as to whether con-
nect every node in G to u? or none, but we use
some heuristics to choose a subset of nodes (pos-
sibly empty) in G that if connected to u? results
in a TDAG whose sum of weights is at least as
big as the binary none-vs-all case. As described in
Sec. 4, the algorithm works very well in our QSD
system, finding the optimum solution in virtually
all cases we examined.
3.2 Dealing with plurality and negation
Consider the following sentence with the plural
NP chunk the lines.
5. Merge [1p/ the lines], ending in [2/ a punctu-
ation], with [3/ the next non-blank line].
6. SI : (1c > 1d > 2; 1d > 3) 10
In QuanText, plural chunks are indexed with a
number followed by the lowercase letter ?p?. As
seen in (6), the scoping looks different from before
in that the terms 1d and 1c are not the label of any
chunk. These two terms refer to the two quantified
terms introduced by the plural chunk 1p: 1c (for
collection) represents the set (or in better words
collection) of entities, defined by the plural, and 1d
(for distribution) refers to the implicit universal,
introduced by the plural. In other words, for a plu-
ral chunk ip, id represents the universally quanti-
fied entity over the collection ic. The outscoping
relation 1d > 2 in (6) states that every line in the
collection, denoted by 1c, starts with its own punc-
tuation character. Similarly, 1d > 3 indicates that
every line has its own next non-blank line. Fig-
ure 4(a) shows a DAG for the scoping in (6).
In (7) we have a sentence containing a negation.
In QuanText, negation chunks are labeled with an
uppercase ?N? followed by a number.
10This scoping corresponds to the logical formula:
Dx1c, Collection(x1c) ? ?x1d, In(x1d, x1c)?
(Line(x1d)?(?x2, Punctuation(x2)?EndIn(x1d, x2))?
(Dx3,?blank(x3) ? next(x1d, x3) ?merge(x1d, x3)))
It is straightforward to write a formula for, say, 1c > 2 > 1d.
(a)
1c 1d
2
3
(b)
2 1
3
N1 4
Figure 4: DAGs for scopings in (6) and (8)
7. Extract [1/ every word] in [2/ file ?1.txt?],
which starts with [3/ a capital letter], but
does [N1/ not] end with [4/ a capital letter].
8. SI : (2 > 1 > 3; 1 > N1 > 4)
As seen here, a negation simply introduces a
chunk, which participates in outscoping relations
like an NP chunk. Figure 4(b) represents the scop-
ing in (8) as a DAG.
From these examples, as long as we create two
nodes in the DAG corresponding to each plu-
ral chunk, and one node corresponding to each
negation, there is no need to modify the under-
lying model (defined in the previous section).
However, when u (or v) is a negation (Ni) or
an implicit universal (id) node, the probabilities
p?u,v (? ? {+,?, }) may come from a different
source, e.g. a different classification model or the
same model with a different set of features, as de-
scribed in the following section.
3.3 Feature selection
Previous work has shown that the lexical item
of quantifiers and syntactic clues (often extracted
from phrase structure trees) are good at predicting
quantifier scoping. Srinivasan and Yates (2009)
use the semantics of the head noun in a quantified
NP to predict the scoping. MA11 also find the lex-
ical item of the head noun to be a good predictor.
In this paper, we introduce a new set of syntac-
tic features which we found very informative: the
?type? dependency features of de Marneffe et al
(2006). Adopting this new set of features, we out-
perform MA11?s system by a large margin. An-
other point to mention here is that the features that
are predictive of the relative scope of quantifiers
are not necessarily as helpful when determining
the scope of negation and vice versa. Therefore we
do not use exactly the same set of features when
68
one of the scopal terms in the pair11 is a negation,
although most of the features are quite similar.
3.3.1 NP chunks
We first describe the set of features we have
adopted when both scopal terms in a pair are NP-
chunks. We have organized the features into dif-
ferent categories listed below.
Individual NP-chunk features
Following features are extracted for both NP
chunks in a pair.
? The part-of-speech (POS) tag of the head of chunk
? The lexical item of the head noun
? The lexical item of the determiner/quantifier
? The lexical item of the pre-determiner
? Does the chunk contain a constant (e.g. ?do?, ?x?)?
? Is the NP-chunk a plural?
Implicit universal of a plural
Remember that every plural chunk i introduces
two nodes in the DAG, ic and id. Both nodes
are introduced by the same chunk i, therefore they
use the same set of features. The only exception
is a single additional binary feature for plural NP
chunks, which determines whether the given node
refers to the implicit universal of the plural (i.e. id)
or to the collection itself (i.e. ic).
? Does this node refer to an implicit universal?
Syntactic features ? phrase structure tree
As mentioned above, we have used two sets
of syntactic features. The first is motivated by
HS03?s work and is based on the constituency (i.e.
phrase structure) tree T of the sentence. Since
our model is based on pairwise comparison, the
following features are defined for each pair of
chunks. In the following, by chunk we mean the
deepest phrase-level node in T dominating all the
words in the chunk. If the constituency tree is cor-
rect, this node is usually an NP node. Also, P
refers to the undirected path in T connecting the
two chunks.
? Syntactic category of the deepest common ancestor
? Does 1st/2nd chunk C-command 2nd/1st one?
? Length of the path P
? Syntactic categories of nodes on P
? Is there a conjoined node on P ?
? List of punctuation marks dominated by nodes on P
Syntactic features ? dependency tree
Although regular ?untyped? dependency relations
do not seem to help our QSD system in the pres-
ence of phrase-structure trees, we found the col-
11Since our model is based on pairwise comparison, every
sample is in fact a pair of nodes (u, v) of the DAG.
lapsed typed dependencies (de Marneffe and Man-
ning, 2008) very helpful, even when used on top of
the phrase-structure features. Below is the list of
features we extract from the collapsed typed de-
pendency tree Td of each sentence. In the follow-
ing, by noun we mean the node in Td which corre-
sponds to the head of the chunk. The choice of the
word noun, however, may be sloppy, as the head
of an NP chunk may not be a noun.
? Does 1st/2nd noun dominate 2nd/1st noun?
? Does 1st/2nd noun immediately dominate 2nd/1st?
? Type of incoming dependency relation of each noun
? Syntactic category of the deepest common ancestor
? Lexical item of the deepest common ancestor
? Length of the undirected path between the two
3.3.2 Negations
There are no sentences in our corpus with more
than one negation. Therefore, for every pair of
nodes with one negation, the other node must re-
fer to an NP chunk. We use the following word-
level, phrase-structure, and dependency features
for these pairs.
? Lexical item of the determiner for the NP chunk
? Does the NP chunk contain a constant?
? Is the NP chunk a plural?
? If so, does this node refer to its implicit universal?
? Does the negation C-command the NP chunk in T ?
? Does the NP chunk C-command the negation in T ?
? What is the POS of the parent p of negation in Td?
? Does p dominate the noun in Td?
? Does the noun dominate p in Td?
? Does p immediately dominate the noun in Td?
? If so, what is the type of the dependency?
? Does the noun immediately dominate p in Td?
? If so, what is the type of the dependency?
? Length of the undirected path between the two in Td
4 Experiments
QuanText contains 500 sentences with a total of
1750 chunks, that is 3.5 chunks/sentence on av-
erage. Of those, 1700 chunks are NP chunks.
The rest are scopal operators, mainly negation. Of
all the NP chunks, 320 (more than 18%) are plu-
ral, each introducing an implicit universal, that is,
an additional node in the DAG. Since we feed
each pair of elements to the classifiers indepen-
dently, each (unordered) pair introduces one sam-
ple. Therefore, a sentence with n scopal elements
creates C(n, 2) = n(n ? 1)/2 samples for classi-
fication. When all the elements are taken into ac-
count,12 the total number of samples in the corpus
will be:
12Here by all elements we mean explicit chunks and the
implicit universals. QuanText labels some other (implicit) el-
ements, which we have not been handled in this work. In
particular, some nouns introduce two entities: a type and a
69
?i
C(ni, 2) ? 4500 (9)
Where ni is the number of scopal terms introduced
by sentence i. Out of the 4500 samples, around
1800 involve at least one implicit universal (i.e.,
id), but only 120 samples contain a negation. We
evaluate the performance of the system for implicit
universals and negation both separately and in the
context of full scope disambiguation. We split the
corpus at random into three sets of 50, 100, and
350 sentences, as development, test, and train sets
respectively.13
To extract part-of-speech tags, phrase structure
trees, and typed dependencies, we use the Stan-
ford parser (Klein and Manning, 2003; de Marn-
effe et al, 2006) on both train and test sets. Since
we are using SVM, we have passed the confidence
levels through a softmax function to convert them
into probabilities P ?u,v before applying the algo-
rithm of Section 3. We take MA11?s system as the
baseline. However, in order to have a fair com-
parison, we have used the output of the Stanford
parser to automatically generate the same features
that MA11 have hand-annotated.14 In order to run
the baseline system on implicit universals, we take
the feature vector of a plural NP and add a fea-
ture to indicate that this feature vector represents
the implicit universal of the corresponding chunk.
Similarly, for negation we add a feature to show
that the chunk represents a negation. As shown in
Section 3.3.2, we have used a more compact set
of features for negations. Once again, in order to
have a fair comparison, we apply a similar modifi-
cation to the baseline system. We also use the ex-
act same classifier as used in MA11.15 Figure 5(a)
compares the performance of our model, which we
refer to as RPC-SVM-13, with the baseline sys-
tem, but only on explicit NP chunks.16 The goal
for running this experiment has been to compare
the performance of our model to the baseline sys-
token, as described by Manshadi et al (2012). In this work,
we have only considered the token entity introduced by those
nouns and have ignored the type entity.
13Since the percentage of sentences with negation is small,
we made sure that those sentences are distributed uniformly
between three sets.
14MA11?s features are similar to part-of-speech tags and
untyped dependency relations.
15SVMMulticlass from SVM-light (Joachims, 1999).
16In all experiments, we ignore NP conjunctions. Previous
work treats a conjunction of NPs as separate NPs. However,
similar to plurals, NP conjunctions (disjunctions) introduce
an extra scopal element: a universal (existential). We are
working on an annotation scheme for NP conjunctions, so
we have left this for after the annotations become available.
NP-Chunks only (no id or negation) ?+ P+ R+ F+ AR ABaseline (MA11) 0.762 0.638 0.484 0.550 0.59 0.47Our model (RPC-SVM-13) 0.827 0.743 0.677 0.709 0.68 0.55
(a) Scoping explicit NP chunksOverall system (including negation and implicit universals) ?+ P+ R+ F+ AR ABaseline (MA11) 0.787 0.688 0.469 0.557 0.59 0.47Our model (RPC-SVM-13) 0.863 0.784 0.720 0.751 0.69 0.55
(b) Scoping all elements (including id and Ni)
Figure 5: Performance on QuanText data
tem on the task that it was actually defined to per-
form (that is scoping only explicit NP chunks).
As seen in this table, by incorporating a richer
set of features and a better learning algorithm, our
model outperforms the baseline by almost 15%.
The measure A in these figures shows sentence-
based accuracy. A sentence counts as correct iff
every pair of scopal elements has been labeled
correctly. Therefore A is a tough measure. Fur-
thermore, it is sensitive to the length of the sen-
tence. Following MA11, we have computed an-
other sentence-based accuracy measure, AR. In
computing AR, a sentence counts as correct iff all
the outscoping relations have been recovered cor-
rectly ? in other words, iff R = 100%, regardless
of the value of P. AR may be more practically
meaningful, because if in the correct scoping of
the sentence there is no outscoping between two
elements, inserting one does not affect the inter-
pretation of the sentence. In other words, precision
is less important for QSD in practice.
Figure 5(b) gives the performance of the over-
all model when all the elements including the im-
plicit universals and the negations are taken into
account. That the F-score of our model for the
second experiment is 0.042 higher than F-score for
the first indicates that scoping implicit universals
and/or negations must be easier than scoping ex-
plicit NP chunks. In order to find how much one or
both of the two elements contribute to this gain, we
have run two more experiments, scoping only the
pairs with at least one implicit universal and pairs
with one negation, respectively. Figure 6 reports
the results. As seen, the contribution in boosting
the overall performance comes from the implicit
universals while negations, in fact, lower the per-
formance. The performance for pairs with implicit
universal is higher because universals, in general,
70
Implicit universals only (pairs with at least one id) P+ R+ F+Baseline (MA11) 0.776 0.458 0.576Our model (RPC-SVM-13) 0.836 0.734 0.782
(a) Pairs with at least one implicit universalNegation only (pairs with one negation) P+ R+ F+Baseline (MA11) 0.502 0.571 0.534Our model (RPC-SVM-13) 0.733 0.55 0.629
(b) Pairs with at least one negation
Figure 6: Implicit universals and negations
are easier to scope, even for the human annota-
tors.17 There are several reasons for poor perfor-
mance with negations as well. First, the number
of negations in the corpus is small, therefore the
data is very sparse. Second, the RPC model does
not work well for negations. Scoping a negation
relative to an NP chunk, with which it has a long
distance dependency, often depends on the scope
of the elements in between. Third, scoping nega-
tion usually requires a deep semantic analysis.
In order to see how well our approximation al-
gorithm is working, similar to the approach of
Chambers and Jurafsky (2008), we tried an ILP
solver18 for DAGs with at most 8 nodes to find the
optimum solution, but we found the difference in-
significant. In fact, the approximation algorithm
finds the optimum solution in all but one case.19
5 Related work
Since automatic QSD is in general challenging,
traditionally quantifier scoping is left underspec-
ified in deep linguistic processing systems (Al-
shawi and Crouch, 1992; Bos, 1996; Copestake et
al., 2001). Some efforts have been made to move
underspecification frameworks towards weighted
constraint-based graphs in order to produce the
most preferred reading (Koller et al, 2008), but
the source of these types of constraint are often
discourse, pragmatics, world knowledge, etc., and
hence, they are hard to obtain automatically. In or-
17Trivially, we have taken the relation outscoping ic > id
for granted and not counted it towards higher performance.
18lpsolve: http://sourceforge.net/projects/lpsolve
19To find the gain that can be obtained with gold-standard
parses, we used MA11?s system with their hand-annotated
and the equivalent automatically generated features. The
former boost the performance by 0.04. Incidentally, HS03
lose almost 0.04 when switching to automatically generated
parses.
der to evade scope disambiguation, yet be able to
perform entailment, Koller and Thater (2010) pro-
pose an algorithm to calculate the weakest read-
ings20 from a scope-underspecified representation.
Early efforts on automatic QSD (Moran, 1988;
Hurum, 1988) were based on heuristics, manually
formed into rules with manually assigned weights
for resolving conflicts. To the best of our knowl-
edge, there have been four major efforts on statisti-
cal QSD for English: Higgins and Sadock (2003),
Galen and MacCartney (2004), Srinivasan and
Yates (2009), and Manshadi and Allen (2011a).
The first three only scope two scopal terms in a
sentence, where the scopal term is an NP with an
explicit quantification. MA11 is the first to scope
any number of NPs in a sentence with no restric-
tion on the type of quantification. Besides ignor-
ing negation and implicit universals, their system
has some other limitations too. First, the learning
model is not theoretically justified. Second, hand-
annotated features (e.g. dependency relations) are
used on both the train and the test data.
6 Summary and future work
We develop the first statistical QSD model ad-
dressing the interaction of quantifiers with nega-
tion and the implicit universal of plurals, defining
a baseline for this task on QuanText data (Man-
shadi et al, 2012). In addition, our work improves
upon Manshadi and Allen (2011a)?s work by (ap-
proximately) optimizing a well justified criterion,
by using automatically generated features instead
of hand-annotated dependencies, and by boosting
the performance by a large margin with the help of
a rich feature vector.
This work can be improved in many directions,
among which are scoping more elements such as
other scopal operators and implicit entities, de-
ploying more complex learning models, and de-
veloping models which require less supervision.
Acknowledgement
We need to thank William de Beaumont and
Jonathan Gordon for their comments on the pa-
per and Omid Bakhshandeh for his assistance.
This work was supported in part by NSF grant
1012205, and ONR grant N000141110417.
20Those which can be entailed from other readings but do
not entail any other reading
71
References
Hiyan Alshawi and Richard Crouch. 1992. Monotonic
semantic interpretation. In Proceedings of Associa-
tion for Computational Linguistics, pages 32?39.
Johan Bos. 1996. Predicate logic unplugged. In Pro-
ceedings of the 10th Amsterdam Colloquium, pages
133?143.
Nathanael Chambers and Dan Jurafsky. 2008. Jointly
combining implicit constraints improves temporal
ordering. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?08, pages 698?706, Stroudsburg, PA.
William W. Cohen, Robert E. Schapire, and Yoram
Singer. 1999. Learning to order things. Journal
of Artificial Intelligence Research, 10:243?270.
Ann Copestake, Alex Lascarides, and Dan Flickinger.
2001. An algebra for semantic construction in
constraint-based grammars. In Proceedings of As-
sociation for Computational Linguistics ?01, pages
140?147.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, CrossParser ?08, pages 1?8.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure trees. In
Proceedings of International Conference on Lan-
guage Resources and Evaluation ?12.
Johannes Furnkranz and Eyke Hullermeier. 2003.
Pairwise preference learning and ranking. In Pro-
ceedings of the 14th European Conference on Ma-
chine Learning, volume 2837, pages 145?156.
Andrew Galen and Bill MacCartney. 2004. Statistical
resolution of scope ambiguity in natural language.
http://nlp.stanford.edu/nlkr/scoper.pdf.
Fritz Hamm and Edward W. Hinrichs. 2010. Plurality
and Quantification. Studies in Linguistics and Phi-
losophy. Springer.
Aurelie Herbelot and Ann Copestake. 2010. Anno-
tating underquantification. In Proceedings of the
Fourth Linguistic Annotation Workshop, LAW IV
?10, pages 73?81.
Derrick Higgins and Jerrold M. Sadock. 2003. A ma-
chine learning approach to modeling scope prefer-
ences. Computational Linguistics, 29(1):73?96.
Eyke Hullermeier, Johannes Furnkranz, Weiwei
Cheng, and Klaus Brinker. 2008. Label ranking
by learning pairwise preferences. Artificial Intelli-
gence, 172(1617):1897 ? 1916.
Sven Hurum. 1988. Handling scope ambiguities in
English. In Proceedings of the second conference
on Applied natural language processing, ANLC ?88,
pages 58?65.
Thorsten Joachims. 1999. Making large-scale sup-
port vector machine learning practical. In Bernhard
Scho?lkopf, Christopher J. C. Burges, and Alexan-
der J. Smola, editors, Advances in kernel methods,
pages 169?184. MIT Press, Cambridge, MA, USA.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430.
Alexander Koller and Stefan Thater. 2010. Comput-
ing weakest readings. In Proceedings of the 48th
Annual Meeting on Association for Computational
Linguistics, Uppsala, Sweden.
Alexander Koller, Michaela Regneri, and Stefan
Thater. 2008. Regular tree grammars as a formal-
ism for scope underspecification. In Proceedings of
Annual Meeting on Association for Computational
Linguistics and Human Language Technologies ?08.
Fred Landmann. 2000. Events and plurality. Kluwer
Academic Publishers, Dordrecht.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of Association for Computa-
tional Linguistics (ACL).
Mehdi Manshadi and James Allen. 2011a. Unre-
stricted quantifier scope disambiguation. In Pro-
ceedings of Association for Computational Linguis-
tics ?11, Workshop on Graph-based Methods for
NLP (TextGraph-6).
Mehdi Manshadi, James Allen, and Mary Swift.
2011b. A corpus of scope-disambiguated English
text. In Proceedings of Association for Computa-
tional Linguistics and Human Language Technolo-
gies ?11: short papers, pages 141?146.
Mehdi Manshadi, James Allen, and Mary Swift. 2012.
An annotation scheme for quantifier scope disam-
biguation. In Proceedings of International Confer-
ence on Language Resources and Evaluation ?12.
Douglas Moran. 1988. Quantifier scoping in the SRI
core language engine. In Proceedings of the 26th
Annual Meeting on Association for Computational
Linguistics.
Lance Ramshaw and Mitch Marcus. 1995. Text
Chunking Using Transformation-Based Learning.
In Proceedings of the Third Workshop on Very Large
Corpora, pages 82?94, Somerset, New Jersey.
Prakash Srinivasan and Alexander Yates. 2009. Quan-
tifier scope disambiguation using extracted prag-
matic knowledge: preliminary results. In Proceed-
ings of EMNLP ?09.
72
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 419?423,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Semantic Roles for String to Tree Machine Translation
Marzieh Bazrafshan and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We experiment with adding semantic role
information to a string-to-tree machine
translation system based on the rule ex-
traction procedure of Galley et al (2004).
We compare methods based on augment-
ing the set of nonterminals by adding se-
mantic role labels, and altering the rule
extraction process to produce a separate
set of rules for each predicate that encom-
pass its entire predicate-argument struc-
ture. Our results demonstrate that the sec-
ond approach is effective in increasing the
quality of translations.
1 Introduction
Statistical machine translation (SMT) has made
considerable advances in using syntactic proper-
ties of languages in both the training and the de-
coding of translation systems. Over the past few
years, many researchers have started to realize that
incorporating semantic features of languages can
also be effective in increasing the quality of trans-
lations, as they can model relationships that often
are not derivable from syntactic structures.
Wu and Fung (2009) demonstrated the promise
of using features based on semantic predicate-
argument structure in machine translation, using
these feature to re-rank machine translation out-
put. In general, re-ranking approaches are lim-
ited by the set of translation hypotheses, leading
to a desire to incorporate semantic features into
the translation model used during MT decoding.
Liu and Gildea (2010) introduced two types of
semantic features for tree-to-string machine trans-
lation. These features model the reorderings and
deletions of the semantic roles in the source sen-
tence during decoding. They showed that addition
of these semantic features helps improve the qual-
ity of translations. Since tree-to-string systems are
trained on parse trees, they are constrained by the
tree structures and are generally outperformed by
string-to-tree systems.
Xiong et al (2012) integrated two discrimi-
native feature-based models into a phrase-based
SMT system, which used the semantic predicate-
argument structure of the source language. Their
first model defined features based on the context of
a verbal predicate, to predict the target translation
for that verb. Their second model predicted the re-
ordering direction between a predicate and its ar-
guments from the source to the target sentence.
Wu et al (2010) use a head-driven phrase struc-
ture grammar (HPSG) parser to add semantic rep-
resentations to their translation rules.
In this paper, we use semantic role labels to en-
rich a string-to-tree translation system, and show
that this approach can increase the BLEU (Pap-
ineni et al, 2002) score of the translations. We
extract GHKM-style (Galley et al, 2004) transla-
tion rules from training data where the target side
has been parsed and labeled with semantic roles.
Our general method of adding information to the
syntactic tree is similar to the ?tree grafting? ap-
proach of Baker et al (2010), although we fo-
cus on predicate-argument structure, rather than
named entity tags and modality. We modify the
rule extraction procedure of Galley et al (2004) to
produce rules representing the overall predicate-
argument structure of each verb, allowing us to
model alternations in the mapping from syntax to
semantics of the type described by Levin (1993).
2 Semantic Roles for String-to-Tree
Translation
2.1 Semantic Role Labeling
Semantic Role Labeling (SRL) is the task of iden-
tifying the arguments of the predicates in a sen-
tence, and classifying them into different argu-
ment labels. Semantic roles can provide a level
419
of understanding that cannot be derived from syn-
tactic analysis of a sentence. For example, in
sentences ?Ali opened the door.? and ?The door
opened?, the word door has two different syntac-
tic roles but only one semantic role in the two sen-
tences.
Semantic arguments can be classified into core
and non-core arguments (Palmer et al, 2010).
Core arguments are necessary for understanding
the sentence. Non-core arguments add more infor-
mation about the predicate but are not essential.
Automatic semantic role labelers have been de-
veloped by training classifiers on hand annotated
data (Gildea and Jurafsky, 2000; Srikumar and
Roth, 2011; Toutanova et al, 2005; Fu?rstenau and
Lapata, 2012). State-of-the-art semantic role la-
belers can predict the labels with accuracies of
around 90%.
2.2 String-to-Tree Translation
We adopt the GHKM framework of Galley et al
(2004) using the parses produced by the split-
merge parser of Petrov et al (2006) as the English
trees. As shown by Wang et al (2010), the refined
nonterminals produced by the split-merge method
can aid machine translation. Furthermore, in all
of our experiments, we exclude unary rules during
extraction by ensuring that no rules will have the
same span in the source side (Chung et al, 2011).
2.3 Using Semantic Role Labels in SMT
To incorporate semantic information into a string-
to-tree SMT system, we tried two approaches:
? Using semantically enriched GHKM rules,
and
? Extracting semantic rules separately from the
regular GHKM rules, and adding a new fea-
ture for distinguishing the semantic rules.
The next two sections will explain these two
methods in detail.
2.4 Semantically Enriched Rules (Method 1)
In this method, we tag the target trees in the train-
ing corpus with semantic role labels, and extract
the translation rules from the tagged corpus. Since
the SCFG rule extraction methods do not assume
any specific set of non-terminals for the target
parse trees, we can attach the semantic roles of
each constituent to its label in the tree, and use
S
NP?ARG0
NPB
NN
everybody
VP
VBG?PRED
lending
NP?ARG1
NPB
DT
a
NN
hand
Figure 1: A target tree after inserting semantic
roles. ?Lending? is the predicate, ?everybody? is
argument 0, and ?a hand? is argument 1 for the
predicate.
S-8
NP-7-ARG1 1 victimized by NP-7-ARG0 2
NP-7-ARG1 1 ? NP-7-ARG0 2
Figure 2: A complete semantic rule.
these new labels for rule extraction. We only la-
bel the core arguments of each predicate, to make
sure that the rules are not too specific to the train-
ing data. We attach each semantic label to the root
of the subtree that it is labeling. Figure 1 shows
an example target tree after attaching the semantic
roles. We then run a GHKM rule extractor on the
labeled training corpus and use the semantically
enriched rules with a syntax-based decoder.
2.5 Complete Semantic Rules with Added
Feature (Method 2)
This approach uses the semantic role labels to
extract a set of special translation rules, that on
the target side form the smallest tree fragments in
which one predicate and all of its core arguments
are present. These rules model the complete se-
mantic structure of each predicate, and are used
by the decoder in addition to the normal GHKM
rules, which are extracted separately.
Starting by semantic role labeling the target
parse trees, we modify the GHKM component of
the system to extract a semantic rule for each pred-
icate. We define labels p as the set of semantic
role labels related to predicate p. That includes all
420
Number of rules
dev test
Baseline 1292175 1300589
Method 1 1340314 1349070
Method 2 1416491 1426159
Table 1: The number of the translation rules used
by the three experimented methods
of the labels of the arguments of p, and the label
of p itself. Then we add the following condition
to the definition of the ?frontier node? defined in
Galley et al (2004):
A frontier node must have either all or none of
the semantic role labels from labels p in its de-
scendants in the tree.
Adding this new condition, we extract one se-
mantic rule for each predicate, and for that rule we
discard the labels related to the other predicates.
This semantic rule will then have on its target side,
the smallest tree fragment that contains all of the
arguments of predicate p and the predicate itself.
Figure 2 depicts an example of a complete se-
mantic rule. Numbers following grammatical cat-
egories (for example, S-8 at the root) are the re-
fined nonterminals produced by the split-merge
parser. In general, the tree side of the rule may
extend below the nodes with semantic role labels
because of the general constraint on frontier nodes
that they must have a continuous span in the source
(Chinese) side. Also, the internal nodes of the
rules (such as a node with PRED label in Figure
2) are removed because they are not used in de-
coding.
We also extract the regular GHKM rules using
the original definition of the frontier nodes, and
add the semantic rules to them. To differentiate
the semantic rules from the non-semantic ones, we
add a new binary feature that is set to 1 for the
semantic rules and to 0 for the rest of the rules.
3 Experiments
Semantic role labeling was done using the Prop-
Bank standard (Palmer et al, 2005). Our labeler
uses a maximum entropy classifier and for iden-
tification and classification of semantic roles, and
has a percision of 90% and a recall of 88%. The
features used for training the labeler are a subset of
the features used by Gildea and Jurafsky (2000),
Xue and Palmer (2004), and Pradhan et al (2004).
The string-to-tree training data that we used is
a Chinese to English parallel corpus that contains
more than 250K sentence pairs, which consist of
6.3M English words. The corpus was drawn from
the newswire texts available from LDC.1 We used
a 392-sentence development set with four refer-
ences for parameter tuning, and a 428-sentence
test set with four references for testing. They are
drawn from the newswire portion of NIST evalua-
tion (2004, 2005, 2006). The development set and
the test set only had sentences with less than 30
words for decoding speed. A set of nine standard
features, which include globally normalized count
of rules, lexical weighting (Koehn et al, 2003),
length penalty, and number of rules used, was used
for the experiments. In all of our experiments, we
used the split-merge parsing method of Petrov et
al. on the training corpus, and mapped the seman-
tic roles from the original trees to the result of the
split-merge parser. We used a syntax-based de-
coder with Earley parsing and cube pruning (Chi-
ang, 2007). We used the Minimum Error Rate
Training (Och, 2003) to tune the decoding param-
eters for the development set and tested the best
weights that were found on the test set.
We ran three sets of experiments: Baseline
experiments, where we did not do any seman-
tic role labeling prior to rule extraction and only
extracted regular GHKM rules, experiments with
our method of Section 2.4 (Method 1), and a set
of experiments with our method of Section 2.5
(Method 2).
Table 1 contains the numbers of the GHKM
translation rules used by our three method. The
rules were filtered by the development and the test
to increase the decoding speed. The increases in
the number of rules were expected, but they were
not big enough to significantly change the perfor-
mance of the decoder.
3.1 Results
For every set of experiments, we ran MERT on the
development set with 8 different starting weight
vectors picked randomly. For Method 2 we added
a new random weight for the new feature. We then
tested the system on the test set, using for each
experiment the weight vector from the iteration of
MERT with the maximum BLEU score on the de-
velopment set. Table 3 shows the BLEU scores
that we found on the test set, and their correspond-
ing scores on the development set.
1We randomly sampled our data from various different
sources. The language model is trained on the English side
of entire data (1.65M sentences, which is 39.3M words.)
421
Source ?? 13????? ,????? ,????? .
Reference to solve the problem of 1.3 billion people , we can only rely on ourselves and nobody else .
Baseline cannot rely on others , can only resolve the problem of 13 billion people , on their own .
Method 2 to resolve the issue of 1.3 billion people , they can?t rely on others , and it can only rely on themselves .
Source ???????? ,???????????? .
Reference in the new situation of the millennium , the development of asia is facing new opportunities .
Baseline facing new opportunities in the new situation in the new century , the development of asia .
Method 2 under the new situation in the new century , the development of asia are facing a new opportunity .
Source ?? ,???????????? ??????????? .
Reference he said the arab league is the best partner to discuss with the united states about carrying out democratic reforms in the middle east .
Baseline arab league is the best with democratic reform in the middle east region in the discussion of the united states , he said .
Method 2 arab league is the best partner to discuss the middle east region democratic reform with the united states , he said .
Table 2: Comparison of example translations from the baseline method and our Method 2.
The best BLEU score on the test set is 25.92,
which is from the experiments of Method 2.
Method 1 system seems to behave slightly worse
than the baseline and Method 2. The reason for
this behavior is that the rules that were extracted
from the semantic role labeled corpus could have
isolated semantic roles in them which would not
necessarily get connected to the right predicate
or argument during decoding. In other words,
it is possible for a rule to only contain one or
some of the semantic arguments of a predicate,
and not even include the predicate itself, and there-
fore there is no guarantee that the predicate will be
translated with the right arguments and in the right
order. The difference between the BLEU scores
of the best Method 2 results and the baseline is
0.92. This improvement is statistically significant
(p = 0.032) and it shows that incorporating se-
mantic roles in machine translation is an effective
approach.
Table 2 compares some translations from the
baseline decoder and our Method 2. The first line
of each example is the Chinese source sentence,
and the second line is one of the reference trans-
lations. The last two lines compare the baseline
and Method 2. These examples show how our
Method 2 can outperform the baseline method, by
translating complete semantic structures, and gen-
erating the semantic roles in the correct order in
the target language. In the first example, the pred-
icate rely on for the argument themselves was not
translated by the baseline decoder, but it was cor-
rectly translated by Method 2. The second ex-
ample is a case where the baseline method gener-
ated the arguments in the wrong order (in the case
of facing and development), but the translation by
Method 2 has the correct order. In the last example
we see that the arguments of the predicate discuss
have the wrong order in the baseline translation,
BLEU Score
dev test
Baseline 26.01 25.00
Method 1 26.12 24.84
Method 2 26.5 25.92
Table 3: BLEU scores on the test and development
sets, of 8 experiments with random initial feature
weights.
but Method 2 generated the correct oder.
4 Conclusion
We proposed two methods for incorporating se-
mantic role labels in a string-to-tree machine
translation system, by learning translation rules
that are semantically enriched. In one approach,
the system learned the translation rules by us-
ing a semantic role labeled corpus and augment-
ing the set of nonterminals used in the rules, and
in the second approach, in addition to the regu-
lar SCFG rules, the system learned semantic roles
which contained the complete semantic structure
of a predicate, and added a feature to distinguish
those rules.
The first approach did not perform any better
than the baseline, which we explained as being due
to having rules with only partial semantic struc-
tures and not having a way to guarantee that those
rules will be used with each other in the right way.
The second approach significantly outperformed
the baseline of our experiments, which shows that
complete predicate-argument structures can im-
prove the quality of machine translation.
Acknowledgments Partially funded by NSF
grant IIS-0910611.
422
References
Kathryn Baker, Michael Bloodgood, Chris Callison-
Burch, Bonnie J. Dorr, Nathaniel W. Filardo, Lori
Levin, Scott Miller, and Christine Piatko. 2010.
Semantically-informed machine translation: A tree-
grafting approach. In Proceedings of The Ninth Bi-
ennial Conference of the Association for Machine
Translation in the Americas, Denver, Colorado.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Tagyoung Chung, Licheng Fang, and Daniel Gildea.
2011. Issues concerning decoding with synchronous
context-free grammar. In Proceedings of the ACL
2011 Conference Short Papers, Portland, Oregon.
Association for Computational Linguistics.
Hagen Fu?rstenau and Mirella Lapata. 2012. Semi-
supervised semantic role labeling via structural
alignment. Computational Linguistics, 38(1):135?
171.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation
rule? In Proceedings of NAACL-04, pages 273?280,
Boston.
Daniel Gildea and Daniel Jurafsky. 2000. Automatic
labeling of semantic roles. In Proceedings of ACL-
00, pages 512?520, Hong Kong, October.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL-03, pages 48?54, Edmonton,
Alberta.
Beth Levin. 1993. English Verb Classes And Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago.
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In COLING-10, Bei-
jing.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL-03, pages 160?167.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Martha Palmer, Daniel Gildea, and Nianwen Xue.
2010. Semantic Role Labeling. Synthesis Lec-
tures on Human Language Technology Series. Mor-
gan and Claypool.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of ACL-02, pages 311?318.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In Proceed-
ings of NAACL-04.
V. Srikumar and D. Roth. 2011. A joint model for
extended semantic role labeling. In EMNLP, Edin-
burgh, Scotland.
Kristina Toutanova, Aria Haghighi, and Christopher
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of ACL-05, pages 589?
596.
Wei Wang, Jonathan May, Kevin Knight, and Daniel
Marcu. 2010. Re-structuring, re-labeling, and
re-aligning for syntax-based machine translation.
Computational Linguistics, 36:247?277, June.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
smt: A hybrid two-pass model. In Proceedings of
the HLT-NAACL 2009: Short Papers, Boulder, Col-
orado.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.
2010. Fine-grained tree-to-string translation rule ex-
traction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL ?10, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Mod-
eling the translation of predicate-argument structure
for smt. In ACL (1), pages 902?911.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of EMNLP.
423
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 236?240,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Sliding Alignment Windows for Real-Time Crowd Captioning
Mohammad Kazemi, Rahman Lavaee, Iftekhar Naim and Daniel Gildea
Dept. of Electrical and Computer Engineering and
Dept. of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
The primary way of providing real-time
speech to text captioning for hard of hear-
ing people is to employ expensive profes-
sional stenographers who can type as fast
as natural speaking rates. Recent work has
shown that a feasible alternative is to com-
bine the partial captions of ordinary typ-
ists, each of whom is able to type only
part of what they hear. In this paper, we
extend the state of the art fixed-window
alignment algorithm (Naim et al, 2013)
for combining the individual captions into
a final output sequence. Our method per-
forms alignment on a sliding window of
the input sequences, drastically reducing
both the number of errors and the latency
of the system to the end user over the pre-
viously published approaches.
1 Introduction
Real-time captioning provides deaf or hard of
hearing people access to speech in mainstream
classrooms, at public events, and on live televi-
sion. Studies performed in the classroom set-
ting show that the latency between when a word
was said and when it is displayed must be under
five seconds to maintain consistency between the
captions being read and other visual cues (Wald,
2005; Kushalnagar et al, 2014). The most com-
mon approach to real-time captioning is to recruit
a trained stenographer with a special purpose pho-
netic keyboard, who transcribes the speech to text
with less than five seconds of latency. Unfortu-
nately, professional captionists are quite expensive
($150 per hour), must be recruited in blocks of an
hour or more, and are difficult to schedule on short
TXLFNIR[OD]\GRJ
&RPELQHUWKHEURZQIR[MXPSHG
IR[MXPSHGRYHUWKHOD]\
WKHTXLFNEURZQIR[MXPSHGRYHUWKHOD]\GRJ
)LQDO&DSWLRQ
0HUJLQJ,QFRPSOHWH&DSWLRQV
&
&
&
Figure 1: General layout of crowd captioning sys-
tems. Captionists (C1, C2, C3) submit partial cap-
tions that are automatically combined into a high-
quality output.
notice. Automatic speech recognition (ASR) sys-
tems (Saraclar et al, 2002), on the other hand, at-
tempts to provide a cheap and fully automated so-
lution to this problem. However, the accuracy of
ASR quickly plummets to below 30% when used
on an untrained speaker?s voice, in a new environ-
ment, or in the absence of a high quality micro-
phone (Wald, 2006). The accuracy of the ASR
systems can be improved using the ?re-speaking?
technique, which requires a person that the ASR
has been trained on to repeat the words said by a
speaker as he hears them. Simultaneously hearing
and speaking, however, is not straightforward, and
requires some training.
An alternative approach is to combine the ef-
forts of multiple non-expert captionists (anyone
who can type), instead of relying on trained work-
ers (Lasecki et al, 2012; Naim et al, 2013). In
this approach, multiple non-expert human work-
ers transcribe an audio stream containing speech
in real-time. Workers type as much as they can of
236
the input, and, while no one worker?s transcript is
complete, the portions captured by various work-
ers tend to overlap. For each input word, a time-
stamp is recorded, indicating when the word is
typed by a worker. The partial inputs are com-
bined to produce a final transcript (see Figure 1).
This approach has been shown to dramatically out-
perform ASR in terms of both accuracy and Word
Error Rate (WER) (Lasecki et al, 2012; Naim et
al., 2013). Furthermore, recall of individual words
irrespective of their order approached and even ex-
ceeded that of a trained expert stenographer with
seven workers contributing, suggesting that the in-
formation is present to meet the performance of
a stenographer (Lasecki et al, 2012). However,
aligning these individual words in the correct se-
quential order remains a challenging problem.
Lasecki et al (2012) addressed this alignment
problem using off-the-shelf multiple sequence
alignment tools, as well as an algorithm based on
incrementally building a precedence graph over
output words. Improved results for the alignment
problem were shown using weighted A
?
search
by Naim et al (2013). To speed the search for
the best alignment, Naim et al (2013) divided se-
quences into chunks of a fixed time duration, and
applied the A
?
alignment algorithm to each chunk
independently. Although this method speeds the
search for the best alignment, it introduces a sig-
nificant number of errors to the output of the sys-
tem due to inconsistency at the boundaries of the
chunks. In this paper, we introduce a novel slid-
ing window technique which avoids the errors pro-
duced by previous systems at the boundaries of
the chunks used for alignment. This technique
produces dramatically fewer errors for the same
amount of computation time.
2 Problem Overview and Background
The problem of aligning and combining multiple
transcripts can be mapped to the well-studiedMul-
tiple Sequence Alignment (MSA) problem (Edgar
and Batzoglou, 2006). Let S
1
, . . . , S
K
,K ? 2,
be the K sequences over an alphabet ?, and hav-
ing length N
1
, . . . , N
K
. For the caption align-
ment task, we treat each individual word as a sym-
bol in our alphabet ?. The special gap symbol
??? represents a missing word and does not be-
long to ?. Let A = (a
ij
) be a K ? N
f
matrix,
where a
ij
? ? ? {?}, and the i
th
row has exactly
(N
f
?N
i
) gaps and is identical to S
i
if we ignore
Algorithm 1 MSA-A
?
Algorithm
Require: K input sequences S = {S
1
, . . . , S
K
} having
length N
1
, . . . , N
K
, heuristic weight w, beam size b
input start ? N
K
, goal ? N
k
output an N ?K matrix of integers indicating the index into
each input sequence of each position in the output se-
quence
1: g(start)? 0, f(start)? w ? h(start).
2: Q? {start}
3: while Q 6= ? do
4: n? EXTRACT-MIN(Q)
5: for all s ? {0, 1}
K
? {0
K
} do
6: n
i
? n + s
7: if n
i
= goal then
8: Return the alignment matrix for the recon-
structed path from start to n
i
9: else if n
i
6? Beam(b) then
10: continue;
11: else
12: g(n
i
)? g(n) + c(n, n
i
)
13: f(n
i
)? g(n
i
) + w ? h(n
i
)
14: INSERT-ITEM(Q,n
i
, f(n
i
))
15: end if
16: end for
17: end while
the gaps. Every column of A must have at least
one non-gap symbol. Therefore, the j
th
column
of A indicates an alignment state for the j
th
posi-
tion, where the state can have one of the 2
K
? 1
possible combinations. Our goal is to find the op-
timum alignment matrix A
OPT
that minimizes the
sum of pairs (SOP) cost function:
c(A) =
?
1?i?j?K
c(A
ij
) (1)
where c(A
ij
) is the cost of the pairwise align-
ment between S
i
and S
j
according toA. Formally,
c(A
ij
) =
?
N
f
l=1
sub(a
il
, a
jl
), where sub(a
il
, a
jl
)
denotes the cost of substituting a
jl
for a
il
. If a
il
and a
jl
are identical, the substitution cost is zero.
The substitution cost for two words is estimated
based on the edit distance between two words. The
exact solution to the SOP optimization problem is
NP-Complete (Wang and Jiang, 1994), but many
methods solve it approximately. Our approach is
based on weighted A
?
search for approximately
solving the MSA problem (Lermen and Reinert,
2000; Naim et al, 2013).
2.1 Weighted A
?
Search for MSA
The problem of minimizing the SOP cost function
for K sequences is equivalent to estimating the
shortest path between a single source node and a
single sink node in a K-dimensional mesh graph,
where each node corresponds to a distinct position
in the K sequences. The source node is [0, . . . , 0]
237
Algorithm 2 Fixed Window Algorithm
Require: K input sequences S = {S
1
, . . . , S
K
} having
lengthN
1
, . . . , N
K
, window parameter chunk length.
1: start time? 0
2: while goal ? [N
1
, . . . , N
K
] do
3: for all i do
4: start[i]? closest word(i, start time)
5: end for
6: end time? start time + chunk length
7: for all i do
8: goal[i]? closest word(i, end time)? 1
9: end for
10: alignmatrix?MSA-A
?
(start, goal)
11: concatenate alignmatrix onto end of finalmatrix
12: start time? end time
13: end while
14: Return finalmatrix
and the sink node is [N
1
, . . . , N
K
]. The total num-
ber of nodes in the lattice is (N
1
+1)?(N
2
+1)?
? ? ??(N
K
+1), and each node has 2
K
?1 possible
successors and predecessors. The A
?
search algo-
rithm treats each node position n = [n
1
, . . . , n
K
]
as a search state, and estimates the cost function
g(n) and the heuristic function h(n) for each state.
The cost function g(n) represents the exact min-
imum SOP cost to align the K sequences from
the beginning to the current position. The heuris-
tic function represents the approximate minimum
cost of aligning the suffixes of the K sequences,
starting after the current position n. The com-
monly used heuristic function is h
pair
(n):
h
pair
(n) = L(n ? t) =
?
1?i<j?K
c(A
?
p
(?
n
i
, ?
n
j
))
(2)
where L(n ? t) denotes the lower bound on the
cost of the shortest path from n to destination t,
A
?
p
is the optimal pairwise alignment, and ?
n
i
is
the suffix of node n in the i-th sequence. The
weighted A
?
search uses a priority queue Q to
store the search states n. At each step of the A
?
search algorithm, the node with the smallest eval-
uation function, f(n) = g(n)+wh
pair
(n) (where
w ? 1), is extracted from the priority queue Q and
expanded by one edge. The search continues un-
til the goal node is extracted from Q. To further
speed up the search, a beam constraint is applied
on the search space using the timestamps of each
individual input words. If the beam size is set to b
seconds, then any state that aligns two words hav-
ing more than b seconds time lag is ignored. The
detailed procedure is shown in Algorithm 1. Af-
ter the alignment, the captions are combined via
majority voting at each position of the alignment
matrix. We ignore the alignment columns where
the majority vote is below a certain threshold t
v
(typically t
v
= 2), and thus filter out spurious er-
rors and spelling mistakes.
Although weighted A
?
significantly speeds the
search for the best alignment, it is still too slow
for very long sequences. For this reason, Naim
et al (2013) divided the sequences into chunks of
a fixed time duration, and applied the A
?
align-
ment algorithm to each chunk independently. The
chunks were concatenated to produce the final out-
put sequence, as shown in Algorithm 2.
2.2 Limitations of Fixed Window Algorithm
The fixed window based alignment has two key
limitations. First, aligning disjoint chunks inde-
pendently tends to introduce a large number of
errors at the boundary of each chunk. This is
because the chunk boundaries are defined with
respect to the timestamps associated with each
word in the captions, but the timestamps can
vary greatly between words that should in fact be
aligned. After all, if the timestamps corresponded
precisely to the original time at which each word
was spoken, the entire alignment problem would
be trivial. The fact that the various instances of
a single word in each transcription may fall on ei-
ther side of a chunk boundary leads to errors where
a word is either duplicated in the final output for
more than one chunk, or omitted entirely. This
problem also causes errors in ordering among the
words remaining within one chunk, because there
is less information available to constrain the order-
ing relations between transcriptions. Second, the
fixed window alignment algorithm requires longer
chunks (? 10 seconds) to obtain reasonable accu-
racy, and thus introduces unsatisfactory latency.
3 Sliding Alignment Windows
In order to address the problems described above,
we explore a technique based on a sliding align-
ment window, shown in Algorithm 3. We start
with alignment with a fixed chunk size. After
aligning the first chunk, we use the information
derived from the alignment to determine where
the next chunk should begin within each transcrip-
tion. We use a single point in the aligned output
as the starting point for the next chunk, and de-
termine the corresponding starting position within
each original transcription. This single point is
determined by a tunable parameter keep length
238
Algorithm 3 Sliding Window Algorithm
Require: K input sequences S = {S
1
, . . . , S
K
}
having length N
1
, . . . , N
K
, window parameters
chunk length and keep length.
1: start? 0
K
, goal? 0
K
2: while goal ? [N
1
, . . . , N
K
] do
3: endtime? chunk length+max
i
time(start[i])
4: for all i do
5: goal[i]? closest word(i, endtime)
6: end for
7: alignmatrix?MSA-A
?
(start, goal)
8: concatenate first keep length columns of
alignmatrix onto end of finalmatrix
9: for all i do
10: start[i]? alignmatrix[keep length][i]
11: end for
12: end while
13: Return finalmatrix
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
 
A
cc
ur
ac
y 
(1 
? W
ER
)
 
 
 Sliding?window, k = 0.50
 Sliding?window, k = 0.67
 Sliding?window, k = 0.85
 Graph?based
 MUSCLE
 Fixed?window
Figure 2: Evaluation of different systems on using
WER metric for measuring transcription quality.
(line 10 of Algorithm 3). The materials in the
output alignment that follow this point is thrown
away, and replaced with the output produced by
aligning the next chunk starting from this point
(line 8). The process continues iteratively, allow-
ing us to avoid using the erroneous output align-
ments in the neighborhood of the arbitrary end-
points for each chunk.
4 Experimental Results
We evaluate our system on a dataset of four 5-
minute long audio clips of lectures in electrical
engineering and chemistry lectures taken from
MIT OpenCourseWare. The same dataset used
by (Lasecki et al, 2012) and (Naim et al, 2013).
Each audio clip is transcribed by 10 non-expert
human workers in real time. We measure the ac-
curacy in terms of Word Error Rate (WER) with
respect to a reference transcription.
We are interested in investigating how the three
5 10 15 20 25 30
0.46
0.48
0.5
0.52
0.54
0.56
0.58
0.6
0.62
 Latency (millisecond)
 
A
cc
ur
ac
y 
(1?
W
ER
)
 
 
 sliding window, k = 50 %
 sliding window, k = 67 %
 sliding window, k = 85 %
 fixed window
(a) varying keep-lengths for fixed heuristic weight
4000 6000 8000 10000 12000 140000.48
0.5
0.52
0.54
0.56
0.58
0.6
0.62
 Average Running Time (millisecond)
 
A
cc
ur
ac
y 
(1 
? W
ER
)
 
 
 w = 3
 w = 4
 w = 6
 w = 8
(b) varying heuristic weights for fixed keep-length
Figure 3: Tradeoff between speed and accuracy
for different heuristic wights and keep-lengths
key parameters of the algorithm, i.e., the chunk
size (c), the heuristic weight (w) and the keep-
length (k), affect the system latency, the search
speed, and the alignment accuracy. The chunk size
directly determines the latency of the system to the
end user, as alignment cannot begin until an entire
chunk is captured. Furthermore, the chunk size,
the heuristic weight, and the keep-length help us
to trade-off speed versus accuracy. We also com-
pare the performance of our algorithm with that
of the most accurate fixed alignment window al-
gorithm (Naim et al, 2013). The performance
in terms of WER for sliding and fixed alignment
windows is presented in Figure 2. Out of the sys-
tems in Figure 2, the first three systems consist of
sliding alignment window algorithm with different
values of keep-length parameter: (1) keep-length
= 0.5; (2) keep-length = 0.67; and (3) keep-length
= 0.85. The other systems are the graph-based al-
gorithm of (Lasecki et al, 2012), the MUSCLE
algorithm of (Edgar, 2004), and the most accu-
239
rate fixed alignment window algorithm of (Naim
et al, 2013). We set the heuristic weight param-
eter (w) to 3 and the chunk size parameter (c) to
5 seconds for all the three sliding window sys-
tems and the fixed window system. Sliding align-
ment window produces better results and outper-
forms the other algorithms even for large values of
the keep-length parameter. The sliding alignment
window with keep-length 0.5 achieves 0.5679 av-
erage accuracy in terms of (1-WER), providing a
18.09% improvement with respect to the most ac-
curate fixed alignment window (average accuracy
0.4857). On the same dataset, Lasecki et al (2012)
reported 36.6% accuracy using the Dragon Natu-
rally Speaking ASR system (version 11.5 for Win-
dows).
To show the trade-off between latency and ac-
curacy, we fix the heuristic weight (w = 3) and
plot the accuracy as a function of chunk size in
Figure 3. We repeat this experiment for different
values of keep-length. We observe that the slid-
ing window approach dominates the fixed window
approach across a wide range of chunk sizes. Fur-
thermore, we can see that for smaller values of the
chunk size parameter, increasing the keep-length
makes the system less accurate. As the chunk
size parameter increases, the performance of slid-
ing window systems with different values of keep-
length parameter converges. Therefore, at larger
chunk sizes, for which there are smaller number of
boundaries, the keep-length parameter has lower
impact.
Next, we show the trade-off between computa-
tion speed and accuracy in Figure 3, as we fix the
heuristic weight and vary the chunk size over the
range [5, 10, 15, 20, 30] seconds. Larger chunks
are more accurately aligned but require computa-
tion time that grows as N
K
in the chunk size N in
the worst case. Furthermore, smaller weights al-
low faster alignment, but provide lower accuracy.
5 Conclusion
In this paper, we present a novel sliding win-
dow based text alignment algorithm for real-time
crowd captioning. By effectively addressing the
problem of alignment errors at chunk boundaries,
our sliding window approach outperforms the ex-
isting fixed window based system (Naim et al,
2013) in terms of word error rate, particularly
when the chunk size is small, and thus achieves
higher accuracy at lower latency.
Acknowledgments Funded by NSF awards IIS-
1218209 and IIS-0910611.
References
Robert C Edgar and Serafim Batzoglou. 2006. Mul-
tiple sequence alignment. Current opinion in struc-
tural biology, 16(3):368?373.
Robert C Edgar. 2004. MUSCLE: multiple sequence
alignment with high accuracy and high throughput.
Nucleic Acids Research, 32(5):1792?1797.
Raja S Kushalnagar, Walter S Lasecki, and Jeffrey P
Bigham. 2014. Accessibility evaluation of class-
room captions. ACM Transactions on Accessible
Computing (TACCESS), 5(3):7.
Walter Lasecki, Christopher Miller, Adam Sadilek, An-
drew Abumoussa, Donato Borrello, Raja Kushalna-
gar, and Jeffrey Bigham. 2012. Real-time caption-
ing by groups of non-experts. In Proceedings of the
25rd annual ACM symposium on User interface soft-
ware and technology, UIST ?12.
Martin Lermen and Knut Reinert. 2000. The practi-
cal use of the A* algorithm for exact multiple se-
quence alignment. Journal of Computational Biol-
ogy, 7(5):655?671.
Iftekhar Naim, Daniel Gildea, Walter Lasecki, and Jef-
frey Bigham. 2013. Text alignment for real-time
crowd captioning. In Proceedings of the 2013 Meet-
ing of the North American chapter of the Association
for Computational Linguistics (NAACL-13).
Murat Saraclar, Michael Riley, Enrico Bocchieri, and
Vincent Goffin. 2002. Towards automatic closed
captioning: Low latency real time broadcast news
transcription. In Proceedings of the International
Conference on Spoken Language Processing (IC-
SLP), pages 1741?1744.
Mike Wald. 2005. Using automatic speech recognition
to enhance education for all students: Turning a vi-
sion into reality. In Proceedings 35th Annual Con-
ference on Frontiers in Education, 2005. FIE ?05.,
pages S3G?S3G, Oct.
Mike Wald. 2006. Creating accessible educational
multimedia through editing automatic speech recog-
nition captioning in real time. Interactive Technol-
ogy and Smart Education, 3(2):131?141.
Lusheng Wang and Tao Jiang. 1994. On the complex-
ity of multiple sequence alignment. Journal of Com-
putational Biology, 1(4):337?348.
240
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 49?57,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Factors Affecting the Accuracy of Korean Parsing
Tagyoung Chung, Matt Post and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We investigate parsing accuracy on the Ko-
rean Treebank 2.0 with a number of different
grammars. Comparisons among these gram-
mars and to their English counterparts suggest
different aspects of Korean that contribute to
parsing difficulty. Our results indicate that the
coarseness of the Treebank?s nonterminal set
is a even greater problem than in the English
Treebank. We also find that Korean?s rela-
tively free word order does not impact parsing
results as much as one might expect, but in
fact the prevalence of zero pronouns accounts
for a large portion of the difference between
Korean and English parsing scores.
1 Introduction
Korean is a head-final, agglutinative, and mor-
phologically productive language. The language
presents multiple challenges for syntactic pars-
ing. Like some other head-final languages such
as German, Japanese, and Hindi, Korean exhibits
long-distance scrambling (Rambow and Lee, 1994;
Kallmeyer and Yoon, 2004). Compound nouns are
formed freely (Park et al, 2004), and verbs have
well over 400 paradigmatic endings (Martin, 1992).
Korean Treebank 2.0 (LDC2006T09) (Han and
Ryu, 2005) is a subset of a Korean newswire corpus
(LDC2000T45) annotated with morphological and
syntactic information. The corpus contains roughly
5K sentences, 132K words, and 14K unique mor-
phemes. The syntactic bracketing rules are mostly
the same as the previous version of the treebank
(Han et al, 2001) and the phrase structure annota-
tion schemes used are very similar to the ones used
in Penn English treebank. The Korean Treebank is
constructed over text that has been morphologically
analyzed; not only is the text tokenized into mor-
phemes, but all allomorphs are neutralized.
To our knowledge, there have been only a few pa-
pers focusing on syntactic parsing of Korean. Herm-
jakob (2000) implemented a shift-reduce parser for
Korean trained on very limited (1K sentences) data,
and Sarkar and Han (2002) used an earlier version
of the Treebank to train a lexicalized tree adjoining
grammar. In this paper, we conduct a range of ex-
periments using the Korean Treebank 2.0 (hereafter,
KTB) as our training data and provide analyses that
reveal insights into parsing morphologically rich lan-
guages like Korean. We try to provide comparisons
with English parsing using parsers trained on a simi-
lar amount of data wherever applicable.
2 Difficulties parsing Korean
There are several challenges in parsing Korean com-
pared to languages like English. At the root of many
of these challenges is the fact that it is highly in-
flected and morphologically productive. Effective
morphological segmentation is essential to learning
grammar rules that can generalize beyond the train-
ing data by limiting the number of out-of-vocabulary
words. Fortunately, there are good techniques for do-
ing so. The sentences in KTB have been segmented
into basic morphological units.
Second, Korean is a pro-drop language: subjects
and objects are dropped wherever they are pragmati-
cally inferable, which is often possible given its rich
morphology. Zero pronouns are a remarkably fre-
quent phenomenon in general (Han, 2006), occuring
49
an average of 1.8 times per sentence in the KTB.
The standard approach in parsing English is to ig-
nore NULL elements entirely by removing them (and
recursively removing unary parents of empty nodes
in a bottom-up fashion). This is less of a problem in
English because these empty nodes are mostly trace
elements that denote constituent movement. In the
KTB, these elements are removed altogether and a
crucial cue to grammatical inference is often lost.
Later we will show the profound effect this has on
parsing accuracy.
Third, word order in Korean is relatively free.
This is also partly due to the richer morphology,
since morphemes (rather than word order) are used
to denote semantic roles of phrases. Consider the
following example:
?? ???? ?? ??? .
John-NOM Mary-DAT book-ACC give-PAST .
In the example, any permutation of the first three
words produces a perfectly acceptable sentence.
This freedom of word order could potentially result
in a large number of rules, which could complicate
analysis with new ambiguities. However, formal
written Korean generally conforms to a canonical
word order (SOV).
3 Initial experiments
There has been some work on Korean morphologi-
cal analysis showing that common statistical meth-
ods such as maximum entropy modeling and condi-
tional random fields perform quite well (Lee et al,
2000; Sarkar and Han, 2002; Han and Palmer, 2004;
Lee and Rim, 2005). Most claim accuracy rate over
95%. In light of this, we focus on the parsing part of
the problem utilizing morphology analysis already
present in the data.
3.1 Setup
For our experiments we used all 5,010 sentences in
the Korean Treebank (KTB), which are already seg-
mented. Due to the small size of the corpus, we used
ten-fold cross validation for all of our experiments,
unless otherwise noted. Sentences were assigned to
folds in blocks of one (i.e., fold 1 contained sen-
tences 1, 11, 21, and so on.). Within each fold, 80%
of the data was assigned to training, 10% to devel-
opment, and 10% to testing. Each fold?s vocabulary
model F1 F1?40 types tokens
Korean 52.78 56.55 6.6K 194K
English (?02?03) 71.06 72.26 5.5K 96K
English (?02?04) 72.20 73.29 7.5K 147K
English (?02?21) 71.61 72.74 23K 950K
Table 1: Parser scores for Treebank PCFGs in Korean
and English. For English, we vary the size of the training
data to provide a better point of comparison against Ko-
rean. Types and tokens denote vocabulary sizes (which
for Korean is the mean over the folds).
was set to all words occurring more than once in its
training data, with a handful of count one tokens re-
placing unknown words based on properties of the
word?s surface form (all Korean words were placed
in a single bin, and English words were binned fol-
lowing the rules of Petrov et al (2006)). We report
scores on the development set.
We report parser accuracy scores using the stan-
dard F1 metric, which balances precision and recall
of the labeled constituents recovered by the parser:
2PR/(P + R). Throughout the paper, all evalua-
tion occurs against gold standard trees that contain
no NULL elements or nonterminal function tags or
annotations, which in some cases requires the re-
moval of those elements from parse trees output by
the parser.
3.2 Treebank grammars
We begin by presenting in Table 1 scores for the
standard Treebank grammar, obtained by reading a
standard context-free grammar from the trees in the
training data and setting rule probabilities to rela-
tive frequency (Charniak, 1996). For these initial
experiments, we follow standard practice in English
parsing and remove all (a) nonterminal function tags
and (b) NULL elements from the parse trees before
learning the grammar. For comparison purposes, we
present scores from parsing the Wall Street Journal
portion of the English Penn Treebank (PTB), using
both the standard training set and subsets of it cho-
sen to be similar in size to the KTB. All English
scores are tested on section 22.
There are two interesting results in this table.
First, Korean parsing accuracy is much lower than
English parsing accuracy, and second, the accuracy
difference does not appear to be due to a difference
in the size of the training data, since reducing the
50
size of the English training data did not affect accu-
racy scores very much.
Before attempting to explain this empirically, we
note that Rehbein and van Genabith (2007) demon-
strate that the F1 metric is biased towards parse trees
with a high ratio of nonterminals to terminals, be-
cause mistakes made by the parser have a smaller
effect on the overall evaluation score.1 They rec-
ommend that F1 not be used for comparing parsing
accuracy across different annotation schemes. The
nonterminal to terminal ratio in the KTB and PTB
are 0.40 and 0.45, respectively. It is a good idea to
keep this bias in mind, but we believe that this small
ratio difference is unlikely to account for the huge
gap in scores displayed in Table 1.
The gap in parsing accuracy is unsurprising in
light of the basic known difficulties parsing Korean,
summarized earlier in the paper. Here we observe a
number of features of the KTB that contribute to this
difficulty.
Sentence length On average, KTB sentences are
much longer than PTB sentences (23 words versus
48 words, respectively). Sentence-level F1 is in-
versely correlated with sentence length, and the rel-
atively larger drop in F1 score going from column 3
to 2 in Table 1 is partially accounted for by the fact
that column 3 represents 33% of the KTB sentences,
but 92% of the English sentences.
Flat annotation scheme The KTB makes rela-
tively frequent use of very flat and ambiguous rules.
For example, consider the extreme cases of rule am-
biguity in which the lefthand side nonterminal is
present three or more times on its righthand side.
There are only three instances of such ?triple+-
recursive? NPs among the?40K trees in the training
portion of the PTB, each occurring only once.
NP? NP NP NP , CC NP
NP? NP NP NP CC NP
NP? NP NP NP NP .
The KTB is an eighth of the size of this, but has
fifteen instances of such NPs (listed here with their
frequencies):
1We thank one of our anonymous reviewers for bringing this
to our attention.
NP? NP NP NP NP (6)
NP? NP NP NP NP NP (3)
NP? NP NP NP NP NP NP (2)
NP? NP NP NP NP NP NP NP (2)
NP? SLQ NP NP NP SRQ PAD (1)
NP? SLQ NP NP NP NP SRQ PAN (1)
Similar rules are common for other nonterminals as
well. Generally, flatter rules are easier to parse with
because they contribute to parse trees with fewer
nodes (and thus fewer independent decision points).
However, the presence of a single nonterminal on
both the left and righthand side of a rule means that
the annotation scheme is failing to capture distribu-
tional differences which must be present.
Nonterminal granularity This brings us to a final
point about the granularity of the nonterminals in the
KTB. After removing function tags, there are only
43 nonterminal symbols in the KTB (33 of them
preterminals), versus 72 English nonterminals (44
of them preterminals). Nonterminal granularity is
a well-studied problem in English parsing, and there
is a long, successful history of automatically refin-
ing English nonterminals to discover distributional
differences. In light of this success, we speculate
that the disparity in parsing performance might be
explained by this disparity in the number of nonter-
minals. In the next section, we provide evidence that
this is indeed the case.
4 Nonterminal granularity
There are many ways to refine the set of nontermi-
nals in a Treebank. A simple approach suggested
by Johnson (1998) is to simply annotate each node
with its parent?s label. The effect of this is to re-
fine the distribution of each nonterminal over se-
quences of children according to its position in the
sentence; for example, a VP beneath an SBAR node
will have a different distribution over children than a
VP beneath an S node. This simple technique alone
produces a large improvement in English Treebank
parsing. Klein and Manning (2003) expanded this
idea with a series of experiments wherein they manu-
ally refined nonterminals to different degrees, which
resulted in parsing accuracy rivaling that of bilexi-
calized parsing models of the time. More recently,
Petrov et al (2006) refined techniques originally
proposed by Matsuzaki et al (2005) and Prescher
51
SBJ subject with nominative case marker
OBJ complement with accusative case marker
COMP complement with adverbial postposition
ADV NP that function as adverbial phrase
VOC noun with vocative case maker
LV NP coupled with ?light? verb construction
Table 2: Function tags in the Korean treebank
model F1 F1?40
Korean
coarse 52.78 56.55
w/ function tags 56.18 60.21
English (small)
coarse 72.20 73.29
w/ function tags 70.50 71.78
English (standard)
coarse 71.61 72.74
w/ function tags 72.82 74.05
Table 3: Parser scores for Treebank PCFGs in Korean
and English with and without function tags. The small
English results were produced by training on ?02?04.
(2005) for automatically learning latent annotations,
resulting in state of the art parsing performance with
cubic-time parsing algorithms.
We begin this section by conducting some sim-
ple experiments with the existing function tags, and
then apply the latent annotation learning procedures
of Petrov et al (2006) to the KTB.
4.1 Function tags
The KTB has function tags that mark grammatical
functions of NP and S nodes (Han et al, 2001),
which we list all of them in Table 2. These function
tags are principally grammatical markers. As men-
tioned above, the parsing scores for both English
and Korean presented in Table 1 were produced with
grammars stripped of their function tags. This is
standard practice in English, where the existing tags
are known not to help very much. Table 3 presents
results of parsing with grammars with nonterminals
that retain these function tags (we include results
from Section 3 for comparison). Note that evalua-
tion is done against the unannotated gold standard
parse trees by removing the function tags after pars-
ing with them.
The results for Korean are quite pronounced:
we see a nearly seven-point improvement when re-
taining the existing tags. This very strongly sug-
gests that the KTB nonterminals are too coarse
when stripped of their function tags, and raises the
question of whether further improvement might be
gained from latent annotations.
The English scores allow us to make another point.
Retaining the provided function tags results in a
solid performance increase with the standard train-
ing corpus, but actually hurts performance when
training on the small dataset. Note clearly that this
does not suggest that parsing performance with the
grammar from the small English data could not be
improved with latent annotations (indeed, we will
show that they can), but only that the given annota-
tions do not help improve parsing accuracy. Taking
the Korean and English accuracy results from this ta-
ble together provides another piece of evidence that
the Korean nonterminal set is too coarse.
4.2 Latent annotations
We applied the latent annotation learning procedures
of Petrov et al2 to refine the nonterminals in the
KTB. The trainer learns refinements over the coarse
version of the KTB (with function tags removed). In
this experiment, rather than doing 10-fold cross vali-
dation, we split the KTB into training, development,
and test sets that roughly match the 80/10/10 splits
of the folds:
section file IDs
training 302000 to 316999
development 317000 to 317999
testing 320000 to 320999
This procedure results in grammars which can then
be used to parse new sentences. Table 4 displays the
parsing accuracy results for parsing with the gram-
mar (after smoothing) at the end of each split-merge-
smooth cycle.3 The scores in this table show that,
just as with the PTB, nonterminal refinement makes
a huge difference in parser performance.
Again with the caveat that direct comparison of
parsing scores across annotation schemes must be
taken loosely, we note that the KTB parsing accu-
racy is still about 10 points lower than the best ac-
2http://code.google.com/p/berkeleyparser/
3As described in Petrov et al (2006), to score a parse tree
produced with a refined grammar, we can either take the Viterbi
derivation or approximate a sum over derivations before project-
ing back to the coarse tree for scoring.
52
Viterbi max-sum
cycle F1 F1?40 F1 F1?40
1 56.93 61.11 61.04 64.23
2 63.82 67.94 66.31 68.90
3 69.86 72.83 72.85 75.63
4 74.36 77.15 77.18 78.18
5 78.07 80.09 79.93 82.04
6 78.91 81.55 80.85 82.75
Table 4: Parsing accuracy on Korean test data from the
grammars output by the Berkeley state-splitting grammar
trainer. For comparison, parsing all sentences of ?22 in
the PTB with the same trainer scored 89.58 (max-sum
parsing with five cycles) with the standard training corpus
and 85.21 when trained on ?2?4.
curacy scores produced in parsing the PTB which,
in our experiments, were 89.58 (using max-sum to
parse all sentences with the grammar obtained after
five cycles of training).
An obvious suspect for the difference in parsing
accuracy with latent grammars between English and
Korean is the difference in training set sizes. This
turns out not to be the case. We learned latent anno-
tations on sections 2?4 of the PTB and again tested
on section 22. The accuracy scores on the test set
peak at 85.21 (max-sum, all sentences, five cycles of
training). This is about five points lower than the En-
glish grammar trained on sections 2?21, but is still
over four points higher than the KTB results.
In the next section, we turn to one of the theoret-
ical difficulties with Korean parsing with which we
began the paper.
5 NULL elements
Both the PTB and KTB include many NULL ele-
ments. For English, these elements are traces de-
noting constituent movement. In the KTB, there
are many more kinds of NULL elements, in includ-
ing trace markers, zero pronouns, relative clause re-
duction, verb deletions, verb ellipsis, and other un-
known categories. Standard practice in English pars-
ing is to remove NULL elements in order to avoid
the complexity of parsing with ?-productions. How-
ever, another approach to parsing that avoids such
productions is to retain the NULL elements when
reading the grammar; at test time, the parser is given
sentences that contain markers denoting the empty
elements. To evaluate, we remove these elements
model F1 F1?40 tokens
English (standard training corpus)
coarse 71.61 72.74 950K
w/ function tags 72.82 74.05 950K
w/ NULLs 73.29 74.38 1,014K
Korean
w/ verb ellipses 52.85 56.52 3,200
w/ traces 55.88 59.42 3,868
w/ r.c. markers 56.74 59.87 3,794
w/ zero pronouns 57.56 61.17 4,101
latent (5) w/ NULLs 89.56 91.03 22,437
Table 5: Parser scores for Treebank PCFGs in English
and Korean with NULL elements. Tokens denotes the
number of words in the test data. The latent grammar
was trained for five iterations.
from the resulting parse trees output by the parser
and compare against the stripped-down gold stan-
dard used in previous sections, in order to provide
a fair point of comparison.
Parsing in this manner helps us to answer the ques-
tion of how much easier or more difficult parsing
would be if the NULL elements were present. In
this section, we present results from a variety of ex-
periments parsing will NULL tokens in this manner.
These results can be seen in Table 5. The first ob-
servation from this table is that in English, retaining
NULL elements makes a few points difference.
The first four rows of the KTB portion of Table 5
contains results with retaining different classes of
NULL elements, one at a time, according to the man-
ner described above. Restoring deleted pronouns
and relative clause markers has the largest effect,
suggesting that the absence of these optional ele-
ments removes key cues needed for parsing.
In order to provide a more complete picture of
the effect of empty elements, we train the Berkeley
latent annotation system on a version of the KTB
in which all empty elements are retained. The fi-
nal row of Table 5 contains the score obtained when
evaluating parse trees produced from parsing with
the grammar after the fifth iteration (after which per-
formance began to fall). With the empty elements,
we have achieved accuracy scores that are on par
with the best accuracy scores obtained parsing the
English Treebank.
53
6 Tree substitution grammars
We have shown that coarse labels and the prevalence
of NULL elements in Korean both contribute to pars-
ing difficulty. We now turn to grammar formalisms
that allow us to work with larger fragments of parse
trees than the height-one rules of standard context-
free grammars. Tree substitution grammars (TSGs)
have been shown to improve upon the standard En-
glish Treebank grammar (Bod, 2001) in parser ac-
curacy, and more recently, techniques for inferring
TSG subtrees in a Bayesian framework have enabled
learning more efficiently representable grammars,
permitting some interesting analysis (O?Donnell et
al., 2009; Cohn et al, 2009; Post and Gildea, 2009).
In this section, we try parsing the KTB with TSGs.
We experiment with different methods of learning
TSGs to see whether they can reveal any insights
into the difficulties parsing Korean.
6.1 Head rules
TSGs present some difficulties in learning and rep-
resentation, but a simple extraction heuristic called
a spinal grammar has been shown to be very use-
ful (Chiang, 2000; Sangati and Zuidema, 2009; Post
and Gildea, 2009). Spinal subtrees are extracted
from a parse tree by using a set of head rules to
maximally project each lexical item (a word or mor-
pheme). Each node in the parse tree having a differ-
ent head from its parent becomes the root of a new
subtree, which induces a spinal TSG derivation in
the parse tree (see Figure 1). A probabilistic gram-
mar is derived by taking counts from these trees,
smoothing them with counts of all depth-one rules
from the same training set, and setting rule probabil-
ities to relative frequency.
This heuristic requires a set of head rules, which
we present in Table 6. As an evaluation of our rules,
we list in Table 7 the accuracy results for parsing
with spinal grammars extracted using the head rules
we developed as well as with two head rule heuris-
tics (head-left and head-right). As a point of compar-
ison, we provide the same results for English, using
the standard Magerman/Collins head rules for En-
glish (Magerman, 1995; Collins, 1997). Function
tags were retained for Korean but not for English.
We observe a number of things from Table 7.
First, the relative performance of the head-left and
NT RC rule
S SFN second rightmost child
VV EFN rightmost XSV
VX EFN rightmost VJ or CO
ADJP EFN rightmost VJ
CV EFN rightmost VV
LV EFN rightmost VV
NP EFN rightmost CO
VJ EFN rightmost XSV or XSJ
VP EFN rightmost VX, XSV, or VV
? ? rightmost child
Table 6: Head rules for the Korean Treebank. NT is the
nonterminal whose head is being determined, RC identi-
fies the label of its rightmost child. The default is to take
the rightmost child as the head.
head-right spinal grammars between English and
Korean capture the linguistic fact that English is pre-
dominantly head-first and Korean is predominantly
head-final. In fact, head-finalness in Korean was so
strong that our head rules consist of only a handful
of exceptions to it. The default rule makes heads
of postpositions (case and information clitics) such
as dative case marker and topic marker. It is these
words that often have dependencies with words in
the rest of the sentence. The exceptions concern
predicates that occur in the sentence-final position.
As an example, predicates in Korean are composed
of several morphemes, the final one of which indi-
cates the mood of the sentence. However, this mor-
pheme often does not require any inflection to re-
flect long-distance agreement with the rest of the
sentence. Therefore, we choose the morpheme that
would be considered the root of the phrase, which
in Korean is the verbalization/adjectivization suf-
fix, verb, adjective, auxiliary predicate, and copula
(XSV, XSJ, VV, VJ, VX, CO). These items often in-
clude the information about valency of the predicate.
Second, in both languages, finer-grained specifi-
cation of head rules results in performance above
that of the heuristics (and in particular, the head-
left heuristic for English and head-right heuristic for
Korean). The relative improvements in the two lan-
guages are in line with each other: significant, but
not nearly as large as the difference between the
head-left and head-right heuristics.
Finally, we note that the test results together sug-
gest that parsing with spinal grammars may be a
54
(a) TOP
S
NP-SBJ
NPR
???
NNC
??
PAU
?
VP
NP-ADV
DAN
?
NNC
?
VP
VV
NNC
??
XSV
??
EPF
?
EFN
?
SFN
.
(b) S
NP-SBJ
NPR
???
NNC PAU
VP SFN
(c) S
NP-SBJ VP SFN
.
Figure 1: (a) A KTB parse tree; the bold nodes denote the top-level spinal subtree using our head selection rules. (b)
The top-level spinal subtree using the head-left and (c) head-right extraction heuristics. A gloss of the sentence is
Doctor Schwartz was fired afterward.
model F1 F1?40 size
Korean
spinal (head left) 59.49 63.33 49K
spinal (head right) 66.05 69.96 29K
spinal (head rules) 66.28 70.61 29K
English
spinal (head left) 77.92 78.94 158K
spinal (head right) 72.73 74.09 172K
spinal (head rules) 78.82 79.79 189K
Table 7: Spinal grammar scores on the KTB and on PTB
section 22.
good evaluation of a set of head selection rules.
6.2 Induced tree substitution grammars
Recent work in applying nonparametric machine
learning techniques to TSG induction has shown that
the resulting grammars improve upon standard En-
glish treebank grammars (O?Donnell et al, 2009;
Cohn et al, 2009; Post and Gildea, 2009). These
techniques use a Dirichlet Process prior over the sub-
tree rewrites of each nonterminal (Ferguson, 1973);
this defines a model of subtree generation that pro-
duces new subtrees in proportion to the number of
times they have previously been generated. Infer-
ence under this model takes a treebank and uses
Gibbs sampling to determine how to deconstruct a
parse tree into a single TSG derivation. In this sec-
tion, we apply these techniques to Korean.
This TSG induction requires one to specify a base
measure, which assigns probabilities to subtrees be-
ing generated for the first time in the model. One
base measure employed in previous work scored a
subtree by multiplying together the probabilities of
the height-one rules inside the subtree with a ge-
ometric distribution on the number of such rules.
Since Korean is considered to be a free word-order
language, we modified this base measure to treat the
children of a height-one rule as a multiset (instead of
a sequence). This has the effect of producing equiva-
lence classes among the sets of children of each non-
terminal, concentrating the mass on these classes in-
stead of spreading it across their different instantia-
tions.
To build the sampled grammars, we initialized the
samplers from the best spinal grammar derivations
and ran them for 100 iterations (once again, func-
tion tags were retained). We then took the state of
the training data at every tenth iteration, smoothed
together with the height-one rules from the standard
Treebank. The best score on the development data
for a sampled grammar was 68.93 (all sentences)
and 73.29 (sentences with forty or fewer words):
well above the standard Treebank scores from ear-
lier sections and above the spinal heuristics, but well
below the scores produced by the latent annotation
learning procedures (a result that is consistent with
English).
This performance increase reflects the results for
English demonstrated in the above works. We see a
large performance increase above the baseline Tree-
bank grammar, and a few points above the best
spinal grammar. One nice feature of these induced
TSGs is that the rules learned lend themselves to
analysis, which we turn to next.
6.3 Word order
In addition to the base measure mentioned above,
we also experimented with the standard base mea-
55
NP
NPR NNC
??
NNU NNX
?
Figure 2: Example of a long distance dependency learned
by TSG induction.
sure proposed by Cohn et al and Post & Gildea, that
treats the children of a nonterminal as a sequence.
The grammars produced sampling under a model
with this base measure were not substantively differ-
ent from those of the unordered base measure. A par-
tial explanation for this is that although Korean does
permit a significant amount of reordering relative to
English, the sentences in the KTB come from writ-
ten newswire text, where word order is more stan-
dardized. Korean sentences are characterized as hav-
ing a subject-object-verb (SOV) word order. There
is some flexibility; OSV, in particular, is common
in spoken Korean. In formal writing, though, SOV
word order is overwhelmingly preferred. We see this
reflected in the KTB, where SOV sentences are 63.5
times more numerous that OSV among sentences
that have explicitly marked both the subject and the
object. However, word order is not completely fixed
even in the formal writing. NP-ADV is most likely
to occur right before the VP it modifies, but can be
moved earlier. For example,
S? NP-SBJ NP-ADV VP
is 2.4 times more frequent than the alternative with
the order of the NPs reversed.
Furthermore, the notion of free(er) word order
does not apply to all constituents. An example is
nonterminals directly above preterminals. A Korean
verb may have up to seven affixes; however, they al-
ways agglutinate in a fixed order.
6.4 Long distance dependencies
The TSG inference procedure can be thought of
as discovering structural collocations in parse trees.
The model prefers subtrees that are common in the
data set and that comprise highly probable height-
one rules. The parsing accuracy of these grammars
is well below state of the art, but the grammars are
smaller, and the subtrees learned can help us analyze
the parse structure of the Treebank. One particular
class of subtree is one that includes multiple lexical
items with intervening nonterminals, which repre-
sent long distance dependencies that commonly co-
occur. In Korean, a certain class of nouns must ac-
company a particular class of measure word (a mor-
pheme) when counting the noun. In the example
shown in Figure 2, (NNC ??) (members of as-
sembly) is followed by NNU, which expands to in-
dicate ordinal, cardinal, and numeral nouns; NNU is
in turn followed by (NNX?), the politeness neutral
measure word for counting people.
7 Summary & future work
In this paper, we addressed several difficult aspects
of parsing Korean and showed that good parsing ac-
curacy for Korean can be achieved despite the small
size of the corpus.
Analysis of different parsing results from differ-
ent grammatical formalisms yielded a number of
useful observations. We found, for example, that the
set of nonterminals in the KTB is not differentiated
enough for accurate parsing; however, parsing accu-
racy improves substantially from latent annotations
and state-splitting techniques that have been devel-
oped with English as a testbed. We found that freer
word order may not be as important as might have
been thought from basic a priori linguistic knowl-
edge of Korean.
The prevalence of NULL elements in Korean is
perhaps the most interesting difficulty in develop-
ing good parsing approaches for Korean; this is
a key difference from English parsing that to our
knowledge is not addressed by any available tech-
niques. One potential approach is a special an-
notation of parents with deleted nodes in order to
avoid conflating rewrite distributions. For example,
S ? VP is the most common rule in the Korean
treebank after stripping away empty elements; how-
ever, this is a result of condensing the rule S? (NP-
SBJ *pro*) VP and S?VP, which presumably have
different distributions. Another approach would be
to attempt automatic recovery of empty elements as
a pre-processing step.
Acknowledgments We thank the anonymous re-
viewers for their helpful comments. This work
was supported by NSF grants IIS-0546554 and ITR-
0428020.
56
References
Rens Bod. 2001. What is the minimal set of fragments
that achieves maximal parse accuracy? In Proc. ACL,
Toulouse, France, July.
Eugene Charniak. 1996. Tree-bank grammars. In Proc.
of the National Conference on Artificial Intelligence,
pages 1031?1036.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proc. ACL, Hong Kong.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In Proc. NAACL.
Michael Collins. 1997. Three penerative, lexicalised
models for statistical parsing. In Proc. ACL/EACL.
Thomas S. Ferguson. 1973. A Bayesian analysis of
some nonparametric problems. Annals of Mathemat-
ical Statistics, 1(2):209?230.
Chung-Hye Han and Martha Palmer. 2004. A mor-
phological tagger for Korean: Statistical tagging com-
bined with corpus-based morphological rule applica-
tion. Machine Translation, 18(4):275?297.
Na-Rae Han and Shijong Ryu. 2005. Guidelines for
Penn Korean Treebank version 2.0. Technical report,
IRCS, University of Pennsylvania.
Chung-hye Han, Na-Rae Han, and Eon-Suk Ko. 2001.
Bracketing guidelines for Penn Korean Treebank.
Technical report, IRCS, University of Pennsylvania.
Na-Rae Han. 2006. Korean zero pronouns: analysis and
resolution. Ph.D. thesis, University of Pennsylvania,
Philadelphia, PA, USA.
Ulf Hermjakob. 2000. Rapid parser development: a ma-
chine learning approach for Korean. In Proc. NAACL,
pages 118?123, May.
Mark Johnson. 1998. PCFGmodels of linguistic tree rep-
resentations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and SinWon Yoon. 2004. Tree-local
MCTAG with shared nodes: Word order variation in
German and Korean. In Proc. TAG+7, Vancouver,
May.
Dan Klein and Chris Manning. 2003. Accurate unlexi-
calized parsing. In Proc. ACL.
Do-Gil Lee and Hae-Chang Rim. 2005. Probabilistic
models for Korean morphological analysis. In Com-
panion to the Proceedings of the International Joint
Conference on Natural Language Processing, pages
197?202.
Sang-zoo Lee, Jun-ichi Tsujii, and Hae-Chang Rim.
2000. Hidden markov model-based Korean part-of-
speech tagging considering high agglutinativity, word-
spacing, and lexical correlativity. In Proc. ACL.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proc. ACL.
Samuel E. Martin. 1992. Reference Grammar of Korean:
A Complete Guide to the Grammar and History of the
Korean Language. Tuttle Publishing.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc. ACL, Ann Arbor, Michigan.
Timothy J. O?Donnell, Noah D. Goodman, and Joshua B.
Tenenbaum. 2009. Fragment grammar: Exploring
reuse in hierarchical generative processes. Technical
report, MIT.
Seong-Bae Park, Jeong-Ho Chang, and Byoung-Tak
Zhang. 2004. Korean compound noun decomposition
using syllabic information only. In Computational
Linguistics and Intelligent Text Processing (CICLing),
pages 146?157.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. COLING/ACL, Syd-
ney, Australia, July.
Matt Post and Daniel Gildea. 2009. Bayesian learning of
a tree substitution grammar. In Proc. ACL, Singapore,
Singapore, August.
Detlef Prescher. 2005. Inducing head-driven PCFGs
with latent heads: Refining a tree-bank grammar for
parsing. Machine Learning: ECML 2005, pages 292?
304.
Owen Rambow and Young-Suk Lee. 1994. Word order
variation and tree-adjoining grammar. Computational
Intelligence, 10:386?400.
Ines Rehbein and Josef van Genabith. 2007. Eval-
uating evaluation measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
(NODALIDA).
Federico Sangati and Willem Zuidema. 2009. Unsuper-
vised methods for head assignments. In Proc. EACL.
Anoop Sarkar and Chung-hye Han. 2002. Statistical
morphological tagging and parsing of Korean with an
LTAG grammar. In Proc. TAG+6.
57
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 494?502,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Multi-rate HMMs for Word Alignment
Elif Eyigo?z
Computer Science
University of Rochester
Rochester, NY 14627
Daniel Gildea
Computer Science
University of Rochester
Rochester, NY 14627
Kemal Oflazer
Computer Science
Carnegie Mellon University
PO Box 24866, Doha, Qatar
Abstract
We apply multi-rate HMMs, a tree struc-
tured HMM model, to the word-alignment
problem. Multi-rate HMMs allow us to
model reordering at both the morpheme
level and the word level in a hierarchical
fashion. This approach leads to better ma-
chine translation results than a morpheme-
aware model that does not explicitly model
morpheme reordering.
1 Introduction
We present an HMM-based word-alignment
model that addresses transitions between mor-
pheme positions and word positions simultane-
ously. Our model is an instance of a multi-scale
HMM, a widely used method for modeling dif-
ferent levels of a hierarchical stochastic process.
In multi-scale modeling of language, the deepest
level of the hierarchy may consist of the phoneme
sequence, and going up in the hierarchy, the next
level may consist of the syllable sequence, and
then the word sequence, the phrase sequence, and
so on. By the same token, in the hierarchical word-
alignment model we present here, the lower level
consists of the morpheme sequence and the higher
level the word sequence.
Multi-scale HMMs have a natural application in
language processing due to the hierarchical nature
of linguistic structures. They have been used for
modeling text and handwriting (Fine et al, 1998),
in signal processing (Willsky, 2002), knowledge
extraction (Skounakis et al, 2003), as well as in
other fields of AI such as vision (Li et al, 2006;
Luettgen et al, 1993) and robotics (Theocharous
et al, 2001). The model we propose here is most
similar to multi-rate HMMs (C?etin et al, 2007),
which were applied to a classification problem in
industrial machine tool wear.
The vast majority of languages exhibit morphol-
ogy to some extent, leading to various efforts in
machine translation research to include morphol-
ogy in translation models (Al-Onaizan et al, 1999;
Niessen and Ney, 2000; C?mejrek et al, 2003;
Lee, 2004; Chung and Gildea, 2009; Yeniterzi and
Oflazer, 2010). For the word-alignment problem,
Goldwater and McClosky (2005) and Eyigo?z et al
(2013) suggested word alignment models that ad-
dress morphology directly.
Eyigo?z et al (2013) introduced two-level align-
ment models (TAM), which adopt a hierarchi-
cal representation of alignment: the first level in-
volves word alignment, the second level involves
morpheme alignment. TAMs jointly induce word
and morpheme alignments using an EM algorithm.
TAMs can align rarely occurring words through
their frequently occurring morphemes. In other
words, they use morpheme probabilities to smooth
rare word probabilities.
Eyigo?z et al (2013) introduced TAM 1, which is
analogous to IBM Model 1, in that the first level is
a bag of words in a pair of sentences, and the sec-
ond level is a bag of morphemes. By introducing
distortion probabilities at the word level, Eyigo?z
et al (2013) defined the HMM extension of TAM
1, the TAM-HMM. TAM-HMM was shown to
be superior to its single-level counterpart, i.e., the
HMM-based word alignment model of Vogel et al
(1996).
The alignment example in Figure 1 shows a
Turkish word aligned to an English phrase. The
morphemes of the Turkish word are aligned to
the English words. As the example shows, mor-
phologically rich languages exhibit complex re-
ordering phenomena at the morpheme level, which
is left unutilized in TAM-HMMs. In this paper,
we add morpheme sequence modeling to TAMs
to capture morpheme level distortions. The ex-
ample also shows that the Turkish morpheme or-
494
Figure 1: Turkish word aligned to an English
phrase.
der is the reverse of the English word order. Be-
cause this pattern spans several English words, it
can only be captured by modeling morpheme re-
ordering across word boundaries. We chose multi-
rate HMMs over other hierarchical HMM mod-
els because multi-rate HMMs allow morpheme se-
quence modeling across words over the entire sen-
tence.
It is possible to model the morpheme sequence
by treating morphemes as words: segmenting the
words into morphemes, and using word-based
word alignment models on the segmented data.
Eyigo?z et al (2013) showed that TAM-HMM per-
forms better than treating morphemes as words.
Since the multi-rate HMM allows both word
and morpheme sequence modeling, it is a gener-
alization of TAM-HMM, which allows only word
sequence modeling. TAM-HMM in turn is a gen-
eralization of the model suggested by Goldwater
and McClosky (2005) and TAM 1. Our results
show that multi-rate HMMs are superior to TAM-
HMMs. Therefore, multi-rate HMMs are the best
two-level alignment models proposed so far.
2 Two-level Alignment Model (TAM)
The two-level alignment model (TAM) takes the
approach of assigning probabilities to both word-
to-word translations and morpheme-to-morpheme
translations simultaneously, allowing morpheme-
level probabilities to guide alignment for rare word
pairs. TAM is based on a concept of alignment
defined at both the word and morpheme levels.
2.1 Morpheme Alignment
A word alignment aw is a function mapping a set
of word positions in a target language sentence e
to a set of word positions in a source language sen-
tence f , as exemplified in Figure 2. A morpheme
alignment am is a function mapping a set of mor-
pheme positions in a target language sentence to
a set of morpheme positions in a source language
sentence. A morpheme position is a pair of inte-
gers (j, k), which defines a word position j and a
relative morpheme position k in the word at posi-
tion j, as shown in Figure 3. The word and mor-
pheme alignments below are depicted in Figures 2
and 3.
aw(1) = 1 am(2, 1) = (1, 1) aw(2) = 1
A morpheme alignment am and a word alignment
aw are compatible if and only if they satisfy the
following conditions: If the morpheme alignment
am maps a morpheme of e to a morpheme of f ,
then the word alignment aw maps e to f . If the
word alignment aw maps e to f , then the mor-
pheme alignment am maps at least one morpheme
of e to a morpheme of f . If the word align-
ment aw maps e to null, then all of its morphemes
are mapped to null. Figure 3 shows a morpheme
alignment that is compatible with, i.e., restricted
by, the word alignment in Figure 2. The smaller
boxes embedded inside the main box in Figure 3
depict the embedding of the morpheme level in-
side the word level in two-level alignment models
(TAM).
2.2 TAM 1
We call TAM without sequence modeling TAM 1,
because it defines an embedding of IBM Model 1
(Brown et al, 1993) for morphemes inside IBM
Model 1 for words. In TAM 1, p(e|f), the prob-
ability of translating the sentence f into e is com-
puted by summing over all possible word align-
ments and all possible morpheme alignments that
are compatible with a given word alignment aw:
Word Morpheme
Rw
|e|?
j=1
|f |?
i=0
?
?t(ej |fi) Rm
|ej |?
k=1
|fi|?
n=0
t(ekj |fni )
?
?
(1)
where fni is the nth morpheme of the word at po-
sition i. The probability of translating the word fi
into the word ej is computed by summing over all
possible morpheme alignments between the mor-
phemes of ej and fi. Rw substitutes P (le|lf )(lf+1)le for
easy readability.1 Rm is equivalent to Rw except
1le = |e| is the number of words in sentence e and lf =
|f |.
495
Figure 2: Word alignment Figure 3: Morpheme alignment
for the fact that its domain is not the set of sen-
tences but the set of words. The length of a word is
the number of morphemes in the word. The length
of words ej and fi in R(ej , fi) are the number of
morphemes of ej and fi. We assume that all un-
aligned morphemes in a sentence map to a special
null morpheme.
TAM 1 with the contribution of both word and
morpheme translation probabilities, as in Eqn. 1, is
called ?word-and-morpheme? version of TAM 1.
The model is technically deficient probabilisti-
cally, as it models word and morpheme transla-
tion independently, and assigns mass to invalid
word/morpheme combinations. We can also de-
fine the ?morpheme-only? version of TAM 1 by
canceling out the contribution of word translation
probabilities and assigning 1 to t(ej |fi) in Eqn. 1.
Please note that, although this version of the two-
level alignment model does not use word transla-
tion probabilities, it is also a word-aware model, as
morpheme alignments are restricted to correspond
to a valid word alignment. As such, it also allows
for word level sequence modeling by HMMs. Fi-
nally, canceling out the contribution of morpheme
translation probabilities reduces TAM 1 to IBM
Model 1. Just as IBM Model 1 is used for initial-
ization before HMM-based word-alignment mod-
els (Vogel et al, 1996; Och and Ney, 2003), TAM
Model 1 is used to initialize its HMM extensions,
which are described in the next section.
3 Multi-rate HMM
Like other multi-scale HMM models such as hi-
erarchical HMM?s (Fine et al, 1998) and hidden
Markov trees (Crouse et al, 1998), the multi-rate
HMM characterizes the inter-scale dependencies
by a tree structure. As shown in Figure 5, scales
are organized in a hierarchical manner from coarse
to fine, which allows for efficient representation of
both short- and long-distance context simultane-
ously.
We found that 51% of the dependency relations
in the Turkish Treebank (Oflazer et al, 2003) are
between the last morpheme of a dependent word
and the first morpheme (the root) of the head word
that is immediately to its right, which is exempli-
fied below. The following examples show English
sentences in Turkish word/morpheme order. The
pseudo Turkish words are formed by concatena-
tion of English morphemes, which are indicated
by the ?+? between the morphemes.
? ? I will come from X.
? X+ABL come+will+I
? ? I will look at X.
? X+DAT look+will+I
In English, the verb ?come? subcategorizes for
a PP headed by ?from? in the example above.
In the pseudo Turkish version of this sentence,
?come? subcategorizes for a NP marked with abla-
tive case (ABL), which corresponds to the prepo-
sition ?from?. Similarly, ?look? subcategorizes for
a PP headed by ?at? in English, and a NP marked
with dative case (DAT) in Turkish. Just as the verb
and the preposition that it subcategorizes for are
frequently found adjacent to each other in English,
the verb and the case that it subcategorizes for are
frequently found adjacent to each other in Turk-
ish. Thus, we have a pattern of three correspond-
ing morphemes appearing in reverse order in En-
glish and Turkish, spanning two words in Turkish
and three words in English. In order to capture
such regularities, we chose multi-rate HMMs over
other hierarchically structured HMM models be-
cause, unlike other models, multi-rate HMMs al-
low morpheme sequence modeling across words
over the entire sentence. This allows us to capture
morpheme-mediated syntactic relations between
words (Eryig?it et al, 2008), as exemplified above.
Morpheme sequence modeling across words is
shown in Figure 4 by the arrows after the nodes
496
Figure 4: Multi-rate HMM graph.
representing fam(0,2) and fam(1,2). The circles
represent the words and morphemes of the source
language, the squares represent the words and
morphemes of the target language. e0,2 is the last
morpheme of word e0, and e1,0 is the first mor-
pheme of the next word e1. fam(1,0) is conditioned
on fam(0,2), which is in the previous word.
In order to model the morpheme sequence
across words, we define the function prev(j, k),
which maps the morpheme position (j, k) to the
previous morpheme position:
prev(j, k) =
{
(j, k ? 1) if k > 1
(j ? 1, |ej?1|) if k = 1
If a morpheme is the first morpheme of a word,
then the previous morpheme is the last morpheme
of the previous word.
3.1 Transitions
3.1.1 Morpheme transitions
Before introducing the morpheme level transition
probabilities, we first restrict morpheme level tran-
sitions according to the assumptions of our model.
We consider only the morpheme alignment func-
tions that are compatible with a word alignment
function. If we allow unrestricted transitions be-
tween morphemes, then this would result in some
morpheme alignments that do not allow a valid
word alignment function.
To avoid this problem, we restrict the transi-
tion function as follows: at each time step, we
allow transitions between morphemes in sentence
f if the morphemes belong to the same word.
This restriction reduces the transition matrix to a
block diagonal matrix. The block diagonal matrix
Ab below is a square matrix which has blocks of
square matrices A1 ? ? ?An on the main diagonal,
and the off-diagonal values are zero.
Ab =
?
????
A0 0 ? ? ? 0
0 A1 ? ? ? 0... ... . . . ...
0 0 ? ? ? An
?
????
The square blocks A0, . . . ,An have the dimen-
sions |f0|, . . . , |fn|, the length of the words in sen-
tence f . In each step of the forward-backward al-
gorithm, multiplying the forward (or backward)
probability vectors with the block diagonal ma-
trix restricts morpheme transitions to occur only
within the words of sentence f .
In order to model the morpheme sequence
across words, we also allow transitions between
morphemes across the words in sentence f . How-
ever, we allow cross-word transitions only at cer-
tain time steps: between the last morpheme of a
word in sentence e and the first morpheme of the
next word in sentence e. This does not result in
morpheme alignments that do not allow a valid
word alignment function. Instead of the block di-
agonal matrix Ab, we use a transition matrix A
which is not necessarily block diagonal, to model
morpheme transitions across words.
In sum, we multiply the forward (or backward)
probability vectors with either the transition ma-
trix Ab or the transition matrix A, depending on
whether the transition is occurring at the last mor-
pheme of a word in e. We introduce the function
?(p, q, r, s) to indicate whether a transition is al-
lowed from source position (p, q) to source posi-
497
tion (r, s) when advancing one target position:
?(p, q, r, s) =
{
1 if p = r or s = 1
0 otherwise
Morpheme transition probabilities have four
components. First, the ? function as described
above. Second, the jump width:
J (p, q, r, s) = abs(r, s)? abs(p, q)
where abs(j, k) maps a word-relative morpheme
position to an absolute morpheme position, i.e., to
the simple left-to-right ordering of a morpheme in
a sentence. Third, the morpheme class of the pre-
vious morpheme:2
M(p, q) = Class(f qp )
Fourth, as the arrow from faw(0) to fam(0,0) in Fig-
ure 4 shows, there is a conditional dependence on
the word class that the morpheme is in:
W(r) = Class(fr)
Putting together these components, the morpheme
transitions are formulated as follows:
p(am(j, k) = (r, s) | am(prev(j, k)) = (p, q)) ?
p
(
J (p, q, r, s)|M(p, q),W(r)
)
?(p, q, r, s)
(2)
The block diagonal matrix Ab consists of mor-
pheme transition probabilities.
3.1.2 Word transitions
In the multi-rate HMM, word transition probabili-
ties have two components. First, the jump width:
J (p, r) = r ? p
Second, the word class of the previous word:
W(p) = Class(fp)
The jump width is conditioned on the word class
of the previous word:
p(aw(j) = r | aw(j ? 1) = p) ?
p(J (p, r) | W(p)) (3)
The transition matrix A, which is not necessarily
block diagonal, consists of values which are the
product of a morpheme transition probability, as
defined in Eqn. 2, and a word transition probabil-
ity, as defined in Eqn. 3.
2We used the mkcls tool in GIZA (Och and Ney, 2003)
to learn the word and the morpheme classes.
3.2 Probability of translating a sentence
Finally, putting together Eqn. 1, Eqn. 2 and Eqn. 3,
we formulate the probability of translating a sen-
tence p(e|f) as follows:
Rw
?
aw
|e|?
j=1
(
t(ej |faw(j))p(aw(j)|aw(j?1))
Rm
?
am
|ej |?
k=1
t(ej,k|fam(j,k))
p(am(j,k)|am(prev(j,k)))
)
Rw is the same as it is in Eqn. 1, whereas
Rm = P (le|lf ). If we cancel out morpheme tran-
sitions by setting p(am(j, k)|am(prev(j, k))) =
1/|fam(j,k)|, i.e., with a uniform distribution, then
we get TAM with only word-level sequence mod-
eling, which we call TAM-HMM.
The complexity of the multi-rate HMM is
O(m3n3), where n is the number of words, and
m is the number of morphemes per word. TAM-
HMM differs from multi-rate HMM only by the
lack of morpheme-level sequence modeling, and
has complexity O(m2n3).
For the HMM to work correctly, we must han-
dle jumping to and jumping from null positions.
We learn the probabilities of jumping to a null po-
sition from the data. To compute the transition
probability from a null position, we keep track of
the nearest previous source word (or morpheme)
that does not align to null, and use the position of
the previous non-null word to calculate the jump
width. In order to keep track of the previous non-
null word, we insert a null word between words
(Och and Ney, 2003). Similarly, we insert a null
morpheme after every non-null morpheme.
3.3 Counts
We use Expectation Maximization (EM) to learn
the word and morpheme translation probabili-
ties, as well as the transition probabilities of the
reordering model. This is done with forward-
backward training at the morpheme level, collect-
ing translation and transition counts for both the
word and the morphemes from the morpheme-
level trellis.
In Figure 5, the grid on the right depicts the
morpheme-level trellis. The grid on the left is
the abstraction of the word-level trellis over the
498
Figure 5: Multi-rate HMM trellis
morpheme-level trellis. For each target word e and
for each source word f , there is a small HMM trel-
lis with dimensions |e|?|f | inside the morpheme-
level trellis, as shown by the shaded area inside the
grid on the right. We collect counts for words by
summing over the values in the small HMM trellis
associated with the words.
3.3.1 Translation counts
Morpheme translation counts We compute ex-
pected counts over the morpheme-level trellis.
The morpheme translation count function below
collects expected counts for a morpheme pair
(h, g) in a sentence pair (e, f):
cm(h|g; e, f) =
?
(j,k)
s.t.
h=ekj
?
(p,q)
s.t.
g=fqp
?j,k(p, q)
where ?j,k(p, q) stands for the posterior mor-
pheme translation probabilities for source position
(p, q) and target position (i, j) that are computed
with the forward-backward algorithm.
Word translation counts For each target word
e and source word f , we collect word transla-
tion counts by summing over posterior morpheme
translation probabilities that are in the small trellis
associated with e and f .
Since ? allows only within-word transitions to
occur inside the small trellis, the posterior proba-
bility of observing the word e given the word f
is preserved across time points within the small
trellis associated with e and f . In other words,
the sum of the posterior probabilities in each col-
umn of the small trellis is the same. Therefore, we
collect word translation counts only from the last
morphemes of the words in e.
The word translation count function below col-
lects expected counts from a sentence pair (e, f)
for a particular source word f and target word e:
cw(e|f ; e, f) =
?
j
s.t.
e=ej
?
p
s.t.
f=fp
?
1?q?|f |
?j,|e|(p, q)
3.3.2 Transition counts
Morpheme transition counts For all target po-
sitions (j, k) and all pairs of source positions (p, q)
and (r, s), we compute morpheme transition pos-
teriors:
?j,k((p, q), (r, s))
using the forward-backward algorithm. These
expected counts are accumulated to esti-
mate the morpheme jump width probabilities
p
(
J (p, q, r, s)|M(p, q),W(r)
) used in Eqn. 2.
Word transition counts We compute posterior
probabilities for word transitions by summing over
morpheme transition posteriors between the mor-
phemes of the words fl and fn:
?j(p, r) =
?
1?q?|fp|
?
1?s?|fr|
?j,|ej |((p, q), (r, s))
Like the translation counts, the transition counts
are collected from the last morphemes of words
in e. These expected counts are accumulated
to estimate the word jump width probabilities
p(J (p, r) | W(p)) used in Eqn. 3.
Finally, Rm = P (le|lf ) does not cancel out in
the counts of the multi-rate HMM. To compute the
conditional probability P (le|lf ), we assume that
the length of word e varies according to a Poisson
distribution with a mean that is linear with length
of the word f (Brown et al, 1993).
3.4 Variational Bayes
In order to prevent overfitting, we use the Varia-
tional Bayes extension of the EM algorithm (Beal,
2003). This amounts to a small change to the
M step of the original EM algorithm. We in-
troduce Dirichlet priors ? to perform an inexact
normalization by applying the function f(v) =
exp(?(v)) to the expected counts collected in the
E step, where ? is the digamma function (John-
son, 2007). The M-step update for a multinomial
parameter ?x|y becomes:
?x|y =
f(E[c(x|y)] + ?)
f(
?
j E[c(xj |y)] + ?)
499
Multi-rate
HMM
TAM-HMM WORD
Word-
Morph 
Morph 
only IBM 4 Baseline
BLEU TR to EN 30.82 29.48 29.98 29.13 27.91EN to TR 23.09 22.55  22.54 21.95 21.82
AER 0.254 0.255 0.256 0.375 0.370
Table 1: AER and BLEU Scores
We set ? to 10?20, a very low value, to have the
effect of anti-smoothing, as low values of ? cause
the algorithm to favor words which co-occur fre-
quently and to penalize words that co-occur rarely.
We used Dirichlet priors on morpheme translation
probabilities.
4 Experiments and Results
4.1 Data
We trained our model on a Turkish-English paral-
lel corpus of approximately 50K sentences which
have a maximum of 80 morphemes. Our parallel
data consists mainly of documents in international
relations and legal documents from sources such
as the Turkish Ministry of Foreign Affairs, EU,
etc. The Turkish data was first morphologically
parsed (Oflazer, 1994), then disambiguated (Sak
et al, 2007) to select the contextually salient inter-
pretation of words. In addition, we removed mor-
phological features that are not explicitly marked
by an overt morpheme. For English, we use part-
of-speech tagged data. The number of English
words is 1,033,726 and the size of the English vo-
cabulary is 28,647. The number of Turkish words
is 812,374, the size of the Turkish vocabulary is
57,249. The number of Turkish morphemes is
1,484,673 and the size of the morpheme vocab-
ulary is 16,713.
4.2 Experiments
We initialized our implementation of the single
level ?word-only? model, which we call ?baseline?
in Table 1, with 5 iterations of IBM Model 1, and
further trained the HMM extension (Vogel et al,
1996) for 5 iterations. Similarly, we initialized
TAM-HMM and multi-rate HMM with 5 iterations
of TAM 1 as explained in Section 2.2. Then we
trained TAM-HMM and the multi-rate HMM for 5
iterations. We also ran GIZA++ (IBM Model 1?4)
on the data. We translated 1000 sentence test sets.
We used Dirichlet priors in both IBM Model 1
and TAM 1 training. We experimented with using
Dirichlet priors on the HMM extensions of both
IBM-HMM and TAM-HMM. We report the best
results obtained for each model and translation di-
rection.
We evaluated the performance of our model in
two different ways. First, we evaluated against
gold word alignments for 75 Turkish-English sen-
tences. Table 1 shows the AER (Och and Ney,
2003) of the word alignments; we report the grow-
diag-final (Koehn et al, 2003) of the Viterbi align-
ments. Second, we used the Moses toolkit (Koehn
et al, 2007) to train machine translation systems
from the Viterbi alignments of our various models,
and evaluated the results with BLEU (Papineni et
al., 2002).
In order to reduce the effect of nondetermin-
ism, we run Moses three times per experiment set-
ting, and report the highest BLEU scores obtained.
Since the BLEU scores we obtained are close,
we did a significance test on the scores (Koehn,
2004). In Table 1, the colors partition the table
into equivalence classes: If two scores within the
same row have different background colors, then
the difference between their scores is statistically
significant. The best scores in the leftmost column
were obtained from multi-rate HMMs with Dirich-
let priors only during the TAM 1 training. On the
contrary, the best scores for TAM-HMM and the
baseline-HMM were obtained with Dirichlet pri-
ors both during the TAM 1 and the TAM-HMM
500
training. In Table 1, as the scores improve grad-
ually towards the left, the background color gets
gradually lighter, depicting the statistical signifi-
cance of the improvements. The multi-rate HMM
performs better than the TAM-HMM, which in
turn performs better than the word-only models.
5 Conclusion
We presented a multi-rate HMM word alignment
model, which models the word and the morpheme
sequence simultaneously. We have tested our
model on the Turkish-English pair and showed
that our model is superior to the two-level word
alignment model which has sequence modeling
only at the word level.
Acknowledgments Partially funded by NSF
award IIS-0910611. Kemal Oflazer acknowledges
the generous support of the Qatar Foundation
through Carnegie Mellon University?s Seed Re-
search program. The statements made herein are
solely the responsibility of this author(s), and not
necessarily that of Qatar Foundation.
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David
Yarowsky. 1999. Statistical machine translation.
Technical report, Final Report, JHU Summer Work-
shop.
Matthew J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Univer-
sity College London.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
O?zgu?r C?etin, Mari Ostendorf, and Gary D. Bernard.
2007. Multirate coupled Hidden Markov Models
and their application to machining tool-wear clas-
sification. IEEE Transactions on Signal Processing,
55(6):2885?2896, June.
Tagyoung Chung and Daniel Gildea. 2009. Unsu-
pervised tokenization for machine translation. In
EMNLP, pages 718?726.
Martin C?mejrek, Jan Cur???n, and Jir??? Havelka. 2003.
Czech-English dependency-based machine transla-
tion. In EACL, pages 83?90.
Matthew Crouse, Robert Nowak, and Richard Bara-
niuk. 1998. Wavelet-based statistical signal pro-
cessing using Hidden Markov Models. IEEE Trans-
actions on Signal Processing, 46(4):886?902.
Gu?ls?en Eryig?it, Joakim Nivre, and Kemal Oflazer.
2008. Dependency parsing of Turkish. Computa-
tional Linguistics, 34(3):357?389.
Elif Eyigo?z, Daniel Gildea, and Kemal Oflazer. 2013.
Simultaneous word-morpheme alignment for statis-
tical machine translation. In NAACL.
Shai Fine, Yoram Singer, and Naftali Tishby. 1998.
The hierarchical Hidden Markov model: Analysis
and applications. Machine Learning, 32(1):41?62,
July.
Sharon Goldwater and David McClosky. 2005. Im-
proving statistical MT through morphological anal-
ysis. In HLT-EMNLP.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In EMNLP-CoNLL, pages
296?305, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388?395.
Young-suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In HLT-NAACL, pages
57?60.
Jia Li, Robert Gray, and Richard Olshen. 2006.
Multiresolution image classification by hierarchi-
cal modeling with two-dimensional Hidden Markov
Models. IEEE Transactions on Information Theory,
46(5):1826?1841, September.
Mark R. Luettgen, William C. Karl, Alan S. Willsky,
and Robert R. Tenney. 1993. Multiscale representa-
tions of Markov Random Fields. IEEE Transactions
on Signal Processing, 41(12):3377?3396.
Sonja Niessen and Hermann Ney. 2000. Improving
SMT quality with morpho-syntactic analysis. In
COLING, pages 1081?1085.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
Models. Computational Linguistics, 29(1):19?51.
501
Kemal Oflazer, Bilge Say, Dilek Z. Hakkani-Tu?r, and
Go?khan Tu?r. 2003. Building a Turkish treebank. In
A. Abeille?, editor, Treebanks: Building and Using
Parsed Corpora, pages 261?277. Kluwer, London.
Kemal Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Comput-
ing, 9(2).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Conference of the Association for
Computational Linguistics (ACL-02), pages 311?
318.
Has?im Sak, Tunga Gu?ngo?r, and Murat Sarac?lar. 2007.
Morphological disambiguation of Turkish text with
perceptron algorithm. In CICLing, pages 107?118.
Marios Skounakis, Mark Craven, and Soumya Ray.
2003. Hierarchical Hidden Markov Models for in-
formation extraction. In International Joint Con-
ference on Artificial Intelligence, volume 18, pages
427?433.
Georgios Theocharous, Khashayar Rohanimanesh, and
Sridhar Maharlevan. 2001. Learning hierarchi-
cal observable Markov decision process Models for
robot navigation. In ICRA 2001, volume 1, pages
511?516.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In COLING, pages 836?841.
Alan S. Willsky. 2002. Multiresolution Markov Mod-
els for signal and image processing. In Proceedings
of the IEEE, pages 1396?1458.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based sta-
tistical machine translation from English to Turkish.
In ACL 2010, pages 454?464.
502
