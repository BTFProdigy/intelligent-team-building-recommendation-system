Proceedings of the ACL-HLT 2011 Student Session, pages 46?51,
Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational Linguistics
Disambiguating Temporal?Contrastive Discourse Connectives for Machine
Translation
Thomas Meyer
Idiap Research Institute / Martigny, Switzerland
EPFL - EDEE doctoral school / Lausanne, Switzerland
Thomas.Meyer@idiap.ch
Abstract
Temporal?contrastive discourse connectives
(although, while, since, etc.) signal various
types of relations between clauses such as tem-
poral, contrast, concession and cause. They
are often ambiguous and therefore difficult to
translate from one language to another. We
discuss several new and translation-oriented
experiments for the disambiguation of a spe-
cific subset of discourse connectives in order
to correct some of the translation errors made
by current statistical machine translation sys-
tems.
1 Introduction
The probabilistic phrase-based models used in sta-
tistical machine translation (SMT) have been im-
proved by integrating linguistic information during
training stages. Recent attempts include, for exam-
ple, the reordering of the source language syntax in
order to align it closer to the target language word
order (Collins et al, 2010) or the tagging of pro-
nouns for grammatical gender agreement (Le Na-
gard and Koehn, 2010). On the other hand, inte-
grating discourse information, such as discourse re-
lations holding between two spans of text or between
sentences, has not yet been applied to SMT.
This paper describes several disambiguation and
translation experiments for a specific subset of dis-
course connectives. Based on examinations in mul-
tilingual corpora, we identified the connectives al-
though, but, however, meanwhile, since, though,
when and while as being particularly problematic for
machine translation. These discourse connectives
signal various types of relations between clauses,
such as temporal, contrast, concession, expansion,
cause and condition, which are, as we also show,
hard to annotate even by humans. Disambiguating
these senses and tagging them in large corpora is
hypothesized to help in improving SMT systems to
avoid translation errors.
The paper is organized as follows. Section 2
exemplifies translation and human annotation dif-
ficulties. Resources and the state of the art for
discourse connective disambiguation and parsing
are described in Section 3. Section 4 summarizes
our experiments for disambiguating the senses of
temporal?contrastive connectives. The impact of
connective disambiguation on SMT is briefly pre-
sented in Section 5. Section 6 concludes the paper
with an outline of future work.
2 Translating Connectives
Discourse connectives can signal multiple
senses (Miltsakaki et al, 2005). For instance,
the connective since can have a temporal and causal
meaning. The disambiguation of these senses is
crucial to the correct translation of texts from one
language to another. Translation can be difficult
because there may be no direct lexical correspon-
dence for the explicit source language connective
in the target language, as shown by the reference
translation of the first example in Table 1, taken
from the Europarl corpus (Koehn, 2005).
More often, the incorrect rendering of the sense of
a connective can lead to wrong translations, as in the
second, third and fourth example in Table 1, which
were translated by the Moses SMT decoder (Koehn
46
EN So what we want the European Patent Office to do
is something on behalf of the European Commission
[while] temporal the Office itself is not a Community insti-
tution.
FR Aussi, ce que nous souhaitons, c?est que l?Office europe?en
des brevets agisse au nom de la Commission europe?enne
[tout en n?e?tant] temporal pas une institution communau-
taire.
EN Finally, and in conclusion, Mr President, with the expiry of
the ECSC Treaty, the regulations will have to be reviewed
[since] causal I think that the aid system will have to con-
tinue beyond 2002. . .
FR *Enfin, et en conclusion, Monsieur le pre?sident, a`
l?expiration du traite? ceca, la re?glementation devra e?tre revu
[depuis que] temporal je pense que le syste`me d?aides de-
vront continuer au-dela` de 2002. . .
EN Between 1998 and 1999, loyalists assaulted and shot 123
people, [while] contrast republicans assaulted and shot 93
people.
FR Entre 1998 et 1999, les loyalistes ont attaque? et abattu 123
personnes, [ ] 93 pour les re?publicains.
EN He said Akzo is considering alliances with American drug
companies, [although] contrast he wouldn?t elaborate.
DE *Er sagte Akzo erwa?gt Allianzen mit amerikanischen Phar-
makonzerne, [obwohl] concession er mo?chte nicht na?her
eingehen.
Table 1: Translation examples from Europarl and the
PDTB. The discourse connectives, their translations, and
their senses are indicated in bold. The first example is a
reference translation from EN into FR, while the second,
third and fourth example are wrong translations gener-
ated by MT (EN?FR and EN?DE), hence marked with
an asterisk.
et al, 2007) trained on the Europarl EN?FR and re-
spectively EN?DE subcorpora. The reference trans-
lation for the second example uses the French con-
nective car with a correct causal sense, instead of
the wrong depuis que generated by SMT, which ex-
presses a temporal relation. In the third example,
the SMT system failed to translate the English con-
nective while to French. The French translation is
therefore not coherent, the contrastive discourse in-
formation cannot be established without an explicit
connective. The last example in Table 1 is a sen-
tence from the Penn Discourse Treebank (Prasad et
al., 2008), see Section 3. In its German translation,
it would be correct to use the connective auch wenn
(for contrast) instead of obwohl (for concession).
These examples illustrate the difficulties in trans-
lating discourse connectives, even when they are
lexically explicit. Our hypothesis is, that the auto-
matic annotation of the senses prior to translation
can help finding more often the correct lexical cor-
respondences of a connective (see Section 5 for one
while (489) Translation EN-FR
56% T tout en V-gerund (22%), tant que (22%),
tandis que (11%)
30% CT tandis que (56%), alors que (40%)
14% CO me?me si (100%)
although (347) Translation EN-DE
76.7% CO obwohl (74%), zwar (9%), auch wenn (9%)
23.3% CT obgleich (43%), obwohl (29%)
Table 2: The English connectives while and although in
the Europarl corpus (sections numbered 199x, EN-FR
and EN-DE) with token frequency, sense distribution and
most frequent translations ordered by the corresponding
senses (T = temporal, CO = concession, CT = contrast).
of the methods to achieve this).
When examining the frequency and sense distri-
bution of these connectives and their translations in
the Europarl corpus, the results confirm that at least
such a fine-grained disambiguation as the one be-
tween contrast and concession is necessary for a cor-
rect translation. Table 2 shows cases where the dif-
ferent senses of the connectives while and although
lead to different translations. Disambiguation of the
senses here can help finding the correct lexical cor-
respondence of the connective.
To confirm that the automatic translation of dis-
course connectives is not straightforward, we anno-
tated 80 sentences from the Europarl corpus con-
taining the connective while with the correspond-
ing sense (T, CO or CT) and another 60 sentences
containing the French connective alors que (T or
CT). We then translated these sentences with the al-
ready mentioned EN?FR and FR?EN Moses SMT
system and compared the output manually to the ref-
erence translations from the corpus. The overall sys-
tem performance was 61% of correct translations for
sentences with while and 55% of correct translations
with alors que. As mistakes we either counted miss-
ing target connective words (only when the output
sentence became incoherent) or wrong connective
words because of failure in correct sense rendering.
Also, the manual sense annotation task is not triv-
ial. In a manual annotation experiment, the senses of
the connective while (T, CO and CT) were indicated
in 30 sentences by 4 annotators. The overall agree-
ment on the senses was not higher than a kappa value
of 0.6, which is acceptable but would need improve-
ment in order to produce a reliable resource.
47
3 Data and Related Work
One of the few available discourse annotated cor-
pora in English is the Penn Discourse Treebank
(PDTB) (Prasad et al, 2008). For this resource, one
hundred types of explicit connectives were manually
annotated, as well as implicit relations not signaled
by a connective.
For French, the ANNODIS project for anno-
tation of discourse (Pery-Woodley et al, 2009)
will provide an original, discourse-annotated cor-
pus. Resources for Czech are also becoming avail-
able (Zikanova et al, 2010). For German, a lexi-
con of discourse connectives exists since the 1990s,
namely DiMLex for lexicon of discourse markers
(Stede and Umbach, 1998). An equivalent, more re-
cent database for French is LexConn for lexicon of
connectives (Roze et al, 2010) ? containing a list
of 328 explicit connectives. For each of them, Lex-
Conn indicates and exemplifies the possible senses,
chosen from a list of 30 labels inspired from Rhetor-
ical Structure Theory (Mann and Thompson, 1988).
For the first classification experiments in Sec-
tion 4, we concentrated on English and the explicit
connectives in the PDTB data. The sense hierarchy
used in the PDTB consists of three levels, reach-
ing from four top level senses (Temporal, Contin-
gency, Comparison and Expansion) via 16 subsenses
on the second level to 23 further subsenses on the
third level. As the annotators were allowed to as-
sign one or two senses for each connective there
are 129 possible simple or complex senses for more
than 18,000 explicit connectives. The PDTB fur-
ther sees connectives as discourse-level predicates
that have two propositional arguments. Argument 2
is the one containing the explicit connective. The
sentence from the first example in Table 1 can be
represented as while(So what we...[argument 1], the
Office itself...[argument 2]), which is very helpful to
examine the context of a connective (see Section 4.1
on features).
The release of the PDTB had quite an impact on
disambiguation experiments. The state of the art for
recognizing explicit connectives in English is there-
fore already high, at a level of 94% for disambiguat-
ing the four main senses on the first level of the
PDTB sense hierarchy (Pitler and Nenkova, 2009).
However, when using all 100 types of connectives
and the whole PDTB training set, it is not so dif-
ficult to achieve such a high score, because of the
large amount of instances and the rather broad dis-
tinction of the four main classes only. As we show
in the next section, when building separate classi-
fiers for specific connectives with senses from the
more detailed second hierarchy level of the PDTB, it
is more difficult to reach high accuracies. Recently,
Lin et al (2010) built the first end-to-end PDTB dis-
course parser, which is able to parse unrestricted text
with an F1 score of 38.18% on PDTB test data and
for senses on the second hierarchy level.
4 Disambiguation Experiments
For the experiments described here we used the
WEKA machine learning toolkit (Hall et al, 2009)
and its implementation of a RandomForest classi-
fier (Breiman, 2001). This method outperformed, in
our task, the C4.5 decision tree and NaiveBayes al-
gorithms often used in recent research on discourse
connective classification.
Our first experiment was aimed at sense disam-
biguation down to the third level of the PDTB hi-
erarchy. The training set here consisted of all 100
types of explicit connectives annotated in the PDTB
training set (15,366 instances). To make the figures
and results of this paper comparable to related work,
we use the subdivision of the PDTB recommended
in the annotation manual: sections 02?21 as train-
ing set and section 23 as test set. The only two
features were the (capitalized) connective word to-
kens from the PDTB and their Part of Speech (POS)
tags. For all 129 possible sense combinations, in-
cluding complex senses, results reach 66.51% ac-
curacy with 10-fold cross validation on the train-
ing set and 74.53% accuracy on the PDTB test set1.
This can be seen as a baseline experiment. For in-
stance, Pitler and Nenkova (2009) report an accu-
racy of 85.86% for correctly classified connectives
(with the 4 main senses), when using the connective
token as the only feature.
Based on the analysis of translations and frequen-
cies from Section 2, we then reduced the list of
senses to the following six: temporal (T), cause (C),
1As far as we know, Versley (2010) is the only reference
reporting results down to the third level, reaching an accuracy of
79%, using more features, but not stating whether the complex
sense annotations were included.
48
Connective Senses with number of occurrences Best feature subset Accuracy Baseline kappa
although 134 CO, 133 CT 8, 9, 10 58.4% 48.7% 0.17
but 2090 CT, 485 CO, 77 E 5, 8, 9, 10 76.4% 78.8% 0.02
however 261 CT, 119 CO 1?10 68.4% 68.7% 0.05
meanwhile 77 T, 57 E, 22 CT 1?10 51.9% 49.4% 0.09
since 83 C, 67 T 1, 4, 6, 8, 9, 10 75.3% 55.3% 0.49
though 136 CO, 125 CT 1, 2, 3, 9, 10 65.1% 52.1% 0.30
when 640 T, 135 COND, 17 C, 8 CO, 2 CT 1, 2, 10 79.9% 79.8% 0.05
while 342 CT, 159 T, 77 CO, 53 E 3, 5, 7, 8, 9, 10 59.6% 54.1% 0.23
all 2975 CT, 959 CO, 943 T, 187 E, 135 COND, 100 C 1?10 72.6% 56.1% 0.50
Table 3: Disambiguation of temporal?contrastive connectives.
condition (COND), contrast (CT), concession (CO)
and expansion (E). All subsenses from the third
PDTB hierarchy level were merged under second
level ones (C, COND, CT, CO). Exceptions were
the top level senses T and E, which, so far, need
no further disambiguation for translation. In addi-
tion, we extracted separate training sets for each of
the 8 temporal?contrastive connectives in question
and one training set for all them. The number of oc-
currences and senses in the sets for the single con-
nectives is listed in Table 3. The total number of
instances in the training set for all 8 connectives
is 5,299 occurrences, with a sense distribution of
56.1% CT, 18% CO, 17.8% T, 3.5% E, 2.5% COND,
1.9% C.
Before summarizing the results, we describe the
features implemented and used so far.
4.1 Features
The following basic surface features were consid-
ered when disambiguating the senses signaled by
connectives. Their values were extracted from the
PDTB manual gold annotation. Future automated
disambiguation will be applied to unrestricted text,
identifying the discourse arguments and syntactical
elements in automatically parsed and POS?tagged
sentences.
1. the (capitalized) connective word form
2. its POS tag
3. first word of argument 1
4. last word of argument 1
5. first word of argument 2
6. last word of argument 2
7. POS tag of the first word of argument 2
8. type of first word of argument 2
9. parent syntactical categories of the connective
10. punctuation pattern
The cased word forms (feature 1) were left as is,
therefore also indicating whether the connective is
located at the beginning of a sentence or not. The
variations from the PDTB (e.g. when ? back when
etc.) were also included, supplemented by their POS
tags (feature 2). As shown by Lin et al (2010)
and duVerle and Prendinger (2009), the context of
a connective is very important. The arguments may
include other (reinforcing or opposite) connectives,
numbers and antonyms (to express contrastive rela-
tions). We extracted the words at the beginning and
at the end of argument 1 (features 3, 4) and argu-
ment 2 (features 5, 6) which are, as observed, other
connectives, gerunds, adverbs or determiners (fur-
ther generalized by features 7 and 8). The paths to
syntactical ancestors (feature 9) in which the con-
nective word form appears are quite numerous and
were therefore truncated to a maximum of four an-
cestors (e.g. |SBAR?VP?S|, |ADVP?ADJP?VP?S|,
etc). Punctuation patterns (feature 10) are of the
form C,A ? A,CA etc. where C is the explicit con-
nective and A a placeholder for all the other words.
Punctuation is important for locating connectives as
many of them are subordinating and coordinating
conjunctions, separated by commas (Haddow, 2005,
p. 23).
4.2 Results
In the disambiguation experiments described
here, results were generated separately for every
temporal?contrastive connective (supposing one
may try to improve the translation of only certain
connectives), in addition to one result for the whole
subset. The results in Table 3 above are based
on 10-fold cross validation on the training sets.
They were measured using accuracy (percentage
of correctly classified instances) and the kappa
49
value. The baseline is the majority class, i.e. the
prediction for the most frequent sense annotated for
the corresponding connective. Feature selection was
performed in order to find the best feature subset,
which also improved the accuracy in a range of
1% to 2%. Marked in bold are the accuracy values
significantly above the baseline ones2. The last
result for all 8 temporal?contrastive connectives
reports a six-way classification of senses very close
to one another: the accuracy and kappa values are
well above random agreement and prediction of the
majority class.
Note that experiments for specific subsets of con-
nectives have very rarely been tried in research.
Miltsakaki et al (2005) describe results for since,
while and when, reporting accuracies of 89.5%,
71.8% and 61.6%. The results for the single connec-
tives are comparable with ours in the case of since
and while, where similar senses were used. For when
they only distinguished three senses, whereas we re-
port a higher accuracy for 5 different senses, see Ta-
ble 3.
5 SMT Experiments
We have started to explore how to constrain an SMT
system to use labeled connectives resulting from the
experiments above. There are at least two meth-
ods to integrate labeled discourse connectives in the
SMT process. A first method modifies the phrase ta-
ble of the Moses SMT decoder (Koehn et al, 2007)
in order to encourage it to translate a specific sense
of a connective with an acceptable equivalent. A
second, more natural method for an SMT system
would be to apply the discourse information ob-
tained from the disambiguation module, adding the
sense tags to the discourse connectives in a large par-
allel corpus. This corpus could then be used to train
a new SMT system learning and weighting these
tags during the training.
So far, we experimented with method one. Infor-
mation about the possible senses of the connective
while, labeled as temporal(1), contrast(2) or con-
cession(3)) was directly introduced to the English
source language phrases when there was an appro-
2Paired t-tests were performed at 95% confidence level. The
other accuracy values are either near to the baseline ones or not
significantly below them.
priate translation of the connective in the French
equivalent phrase. We also increased the lexical
probability scores for such modified phrases. The
following example gives an idea of the changes in
the phrase table of the above-mentioned EN?FR
Moses SMT system:
< original:
and the commission , while preserving ||| et la commission tout en
de?fendant ||| 1 3.8131e-06 1 5.56907e-06 2.718 ||| ||| 1 1
and while many ||| et bien que de nombreuses ||| 1 0.00140575 0.5
0.000103573 2.718 ||| ||| 1 1
> modified:
and the commission , while-1 preserving ||| et la commission tout
en de?fendant ||| 1 1 1 1 2.718 ||| ||| 1 1
and while-3 many ||| et bien que de nombreuses ||| 1 1 0.5 1 2.718
||| ||| 1 2
Experiments with such modifications have al-
ready demonstrated a slight increase of BLEU
scores (by 0.8% absolute) on a small test corpus
(20 hand-labeled sentences). The analysis of results
has shown that the system behaves as expected, i.e.
labeled connectives are correctly translated. This
tends to confirm the hypothesis of this paper, that
information regarding discourse connectives indeed
can lead to better translations.
6 Conclusion and Future Work
The paper described new translation-oriented ap-
proaches to the disambiguation of a subset of ex-
plicit discourse connectives with highly ambiguous
temporal?contrastive senses. Although lexically ex-
plicit, their translation by current SMT systems is
often wrong. Disambiguation results in reasonably
high accuracies but also shows that one should find
more accurate and additional features. We will try
to better model the context of a connective, for in-
stance by integrating word similarity distances from
WordNet as features.
In addition, the paper showed a first method to
force an existing and trained SMT system to trans-
late discourse connectives correctly. This led to
noticeable improvements on the translations of the
tested sentences. We will continue to train SMT sys-
tems on automatically labeled discourse connectives
in large corpora.
Acknowledgments
This work is funded by the Swiss National Sci-
ence Foundation (SNSF) under the Project Sinergia
50
COMTIS, contract number CRSI22 127510, www.
idiap.ch/comtis/. Many thanks go to Dr. An-
drei Popescu-Belis, Dr. Bruno Cartoni and Dr. San-
drine Zufferey, for insightful comments and collab-
oration.
References
Leo Breiman. 2001. Random Forests. Machine Learn-
ing, 45(1):5?32.
Michael Collins, Phillipp Koehn, Ivona Kucerova. 2005.
Clause Restructuring for Statistical Machine Transla-
tion. Proceedings of the 43rd Annual Meeting of the
ACL, 531?540
David duVerle, Helmut Prendinger. 2009. A Novel Dis-
course Parser Based on Support Vector Machine Clas-
sification. Proceedings of the 47th Annual Meeting of
the ACL and the 4th IJCNLP of the AFNLP, 665?673.
Barry Haddow. 2005. Acquiring a Disambiguation
Model For Discourse Connectives. Master Thesis.
University of Edinburgh, School of Informatics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bern-
hard Pfahringer, Peter Reutemann, Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1).
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. Proceedings of MT
Summit X, 79?86.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen,
Christine Moran, Richard Zens, Chris Dyer, On-
drej Bojar, Alexandra Constantin, Evan Herbs. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. Proceedings of the 45th Annual Meeting
of the ACL, Demonstration session, 177?180.
Ronan Le Nagard, Philipp Koehn. 2010. Aiding Pronoun
Translation with Co-Reference Resolution. Proceed-
ings of the Joint 5th Workshop on Statistical Machine
Translation and Metrics MATR, 258?267.
Ziheng Lin, Hwee Tou Ng, Min-Yen Kan. 2010. A
PDTB-Styled End-to-End Discourse Parser. Techni-
cal Report TRB8/10. School of Computing, National
University of Singapore, 1?15.
William C. Mann, Sandra A. Thompson. 1988. Rhetori-
cal structure theory: towards a functional theory of text
organization. Text 8(3):243?281.
Eleni Miltsakaki, Nikhil Dinesh, Rashmi Prasad, Ar-
avind Joshi, Bonnie Webber. 2005. Experiments on
Sense Annotations and Sense Disambiguation of Dis-
course Connectives. Proceedings of the Fourth Work-
shop on Treebanks and Linguistic Theories (TLT).
Marie-Paule Pe?ry-Woodley, Nicholas Asher, Patrice En-
jalbert, Farah Benamara, Myriam Bras, Ce?cile Fabre,
Ste?phane Ferrari, Lydia-Mai Ho-Dac, Anne Le
Draoulec, Yann Mathet, Philippe Muller, Lau-
rent Pre?vot, Josette Rebeyrolle, Ludovic Tan-
guy, Marianne Vergez-Couret, Laure Vieu, An-
toine Widlo?cher. 2009. ANNODIS: une approche out-
ille de l?annotation de structures discursives. Proceed-
ings of TALN.
Emily Pitler, Ani Nenkova. 2009. Using Syntax to
Disambiguate Explicit Discourse Connectives in Text.
Proceedings of the ACL-IJCNLP 2009 Conference,
Short Papers. 13?16.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, Bonnie Webber.
2008. The Penn Discourse Treebank 2.0. Proceed-
ings of the 6th International Conference on Language
Resources and Evaluation (LREC), 29641-2968.
Charlotte Roze, Laurence Danlos, Philippe Muller. 2010.
LEXCONN: a French Lexicon of Discourse Connec-
tives. Proceedings of Multidisciplinary Approaches to
Discourse (MAD).
Manfred Stede, Carla Umbach. 1998. DiMLex: a lex-
icon of discourse markers for text generation and un-
derstanding. Proceedings of the 36th Annual Meeting
of the ACL, 1238?1242.
Yannick Versley. 2010. Discovery of Ambiguous and
Unambiguous Discourse Connectives via Annotation
Projection. Proceedings of Workshop on Annotation
and Exploitation of Parallel Corpora (AEPC), 83?82
Sa?rka Zika?nova?, Lucie Mladova?, Jir??? M??rovsky?,
Pavlina J??nova?. 2010. Typical Cases of Annotators?
Disagreement in Discourse Annotations in Prague
Dependency Treebank. Proceedings of the Seventh
International Conference on Language Resources and
Evaluation (LREC), 2002?2006.
51
How Comparable are Parallel Corpora?  
Measuring the Distribution of General Vocabulary and Connectives  
 
Bruno Cartoni Sandrine Zufferey Thomas Meyer Andrei Popescu-Belis 
Linguistics Department 
University of Geneva 
2, rue de Candolle 
CH ? 1211 Geneva 4 
Linguistics Department 
University of Geneva 
2, rue de Candolle 
CH ? 1211 Geneva 4 
Idiap Research Institute 
Rue Marconi 19 
CH ? 1920 Martigny  
Idiap Research Institute 
Rue Marconi 19 
CH ? 1920 Martigny  
{bruno.cartoni|sandrine.zufferey}@unige.ch {thomas.meyer|andrei.popescu-
belis}@idiap.ch 
 
Abstract 
In this paper, we question the 
homogeneity of a large parallel corpus 
by measuring the similarity between 
various sub-parts. We compare results 
obtained using a general measure of 
lexical similarity based on ?2 and by 
counting the number of discourse 
connectives. We argue that discourse 
connectives provide a more sensitive 
measure, revealing differences that are 
not visible with the general measure. We 
also provide evidence for the existence 
of specific characteristics defining 
translated texts as opposed to non-
translated ones, due to a universal 
tendency for explicitation. 
1 Introduction 
Comparable corpora are often considered as a 
solution to compensate for the lack of parallel 
corpora. Indeed, parallel corpora are still 
perceived as the gold standard resource for many 
multilingual natural language processing 
applications, such as statistical machine 
translation.  
The aim of this paper is to assess the 
homogeneity of the widely used Europarl 
parallel corpus (Koehn 2005) by comparing a 
distributional measure of lexical similarity with 
results focused on a more specific measure, the 
frequency of use of discourse connectives. 
Various perspectives can be taken to assess the 
homogeneity of this corpus. First, we evaluate 
the (dis)similarities between translated and 
original language (Experiment 1) and then the 
(dis)similarities between texts translated from 
different source languages (Experiment 2). 
Analyzing the use of discourse connectives 
such as because and since in English highlights 
important differences between translated and 
original texts. The analysis also reveals 
important differences when comparing, for a 
given language, texts that have been translated 
from various source languages. The different 
distribution of connectives in original vs. 
translated French, as well as across varieties of 
French translated from various source languages 
(English, German, Italian and Spanish), are all 
the more intriguing that they are not matched by 
a distributional difference of the general 
vocabulary in these corpora. We will indeed 
show that a well-known method (Kilgarriff 
2001) designed to compare corpora finds that the 
original French and the various translated 
portions of Europarl are rather similar, 
regardless of their source language. 
The paper is structured as follows: we first 
present related work on the characterization of 
translated text (Section 2). In Section 3, we 
argue that analyzing discourse connectives sheds 
new light on text (dis)similarity. Section 4 
presents the Europarl parallel corpus and its sub-
parts that have been used in our studies, as well 
as the methodology and measures that have been 
applied to assess text similarities. Section 5 
presents our main findings and Section 6 
discusses our results, drawing methodological 
conclusions about the use of parallel corpora. 
78
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 78?86,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
2 Previous Work 
Existing studies on translated corpora are mainly 
designed to automatically identify the presence 
of so-called ?translationese? or ?third code?, in 
other words, a text style deemed to be specific to 
translated texts, as in (Baroni and Bernardini 
2005) or in (Ilisei et al 2010). In the literature, 
many possible characteristics of translationese 
have been identified, such as those listed in 
(Baker 1996): translations are simpler than 
original texts (Laviosa-Braithwaite 1996); 
translations are more explicit than original texts 
due to an increase of cohesion markers (Blum-
Kulka 1986); and the items that are unique in the 
target system (i.e. that do not have exact 
equivalents in the source language) are under-
represented in translations (Tirkkonen-Condit 
2000).  
In the field of natural language processing, 
several studies on parallel corpora have shown 
that when building a statistical machine 
translation system, knowing which texts have 
been originally written in a given language and 
which ones are translations has an impact on the 
quality of the system (Ozdowska 2009). A 
recent study using machine learning has 
confirmed the universal of simplification as a 
feature of translated texts (Ilisei et al 
2010).Corpora can be compared using similarity 
measures. Most of these measures are based on 
lexical frequency. Kilgariff (2001) provides a 
comprehensive review of the different methods 
for computing similarity. 
In this study, we chose to use the CBDF 
measure (Chi-by-degrees-of-freedom), as 
proposed in (Kilgariff 1997), to assess the 
similarity of our sub-corpora, as explained in 
Section 4.3. We compare this measure with 
another marker of text diversity (connectives), as 
explained in the following section. 
3 Discourse Connectives as Markers of 
Text Diversity 
Discourse connectives like but, because or while 
form a functional category of lexical items that 
are very frequently used to mark coherence 
relations such as explanation or contrast 
between units of text or discourse (e.g. Halliday 
& Hassan 1976; Mann & Thomson 1992; Knott 
& Dale 1994; Sanders 1997). One of the unique 
properties of discourse connectives is that the 
relation they convey can in many cases be 
inferred even when they are removed, as 
illustrated in (1) and (2):  
1 Max fell because Jack pushed him. 
2 Max fell. Jack pushed him.  
The causal relation conveyed by because in 
(1) is also inferable when the connective is 
absent by using world knowledge about the 
possible relation between the fact of pushing 
someone and this person?s fall in (2). In other 
words, contrary to most other lexical items, 
connectives can be used or left out without 
producing ungrammatical results or losing 
important aspects of meaning. At a macro-
textual level, it is however clear that a text 
containing no connective at all would become 
rather difficult to understand. Several psycho-
linguistic studies have indeed stressed the role of 
connectives for processing (Millis & Just 1994; 
Noordman & Blijzer 2000). But the point we 
want to make here is that in most texts or 
discourses, some coherence relations are 
conveyed by the use of connectives while others 
are not, depending on what the author/speaker 
feels necessary to mark explicitly. 
Another consequence of the fact that 
connectives are optional is that their use in 
translation can vary tremendously between the 
source and the target texts. Studies that have 
examined at the use of connectives in translation 
have indeed found that connectives were often 
removed or added in the target texts, and that the 
type of coherence relation conveyed was 
sometimes even modified due to the actual 
choice of connectives in the target system 
(Altenberg 1986; Baker 1993; Lamiroy 1994; 
Halverson 2004). For all these reasons, 
discourse connectives appear to be particularly 
interesting to investigate in relation to corpus 
homogeneity. 
In this study, we focus more particularly on 
the category of causal connectives, that is to say 
connectives such as because and since in 
English. This particular category seemed 
especially appropriate for our purposes for a 
number of reasons. First, causal connectives 
form a well-defined cluster in many languages 
and can be studied comprehensively. Second, 
causal relations are amongst the most basic ones 
79
for human cognition and in consequence causal 
connectives are widely used in almost all text 
types (Sanders & Sweetser 2009). Lastly, causal 
connectives have been found to be more volatile 
in translation than other categories, such as for 
example concessive connectives like but, 
however, etc. (Halverson 2004; Altenberg 1986). 
From a quantitative perspective, function 
words are usually very frequent whereas most 
content words tend to be in the tail of the 
distribution. This provides another reason to 
treat connectives as a key feature for assessing 
text similarities. 
4 Corpora and Methodology 
4.1 Corpora 
Our analysis is based on the Europarl corpus 
(Koehn 2005), a resource initially designed to 
train statistical machine translation systems. 
Europarl is a multilingual corpus that contains 
the minutes of the European Parliament. At the 
parliament, every deputy usually speaks in 
his/her own language, and all statements are 
transcribed, and then translated into the other 
official languages of the European Union (a total 
of 11 languages for this version of the corpus ? 
version 5). Based on this data, several parallel 
bilingual corpora can be extracted, but caution is 
necessary because the exact status of every text, 
original or translated, is not always clearly 
stated. However, for a number of statements, a 
specific tag provides this information.  
From this multilingual corpus, we extracted 
for our first experiment two parallel and 
?directional? corpora (En-Fr and Fr-En). By 
?directional? we mean that the original and 
translated texts are clearly identified in these 
corpora. Namely, in the English-French subset, 
the original speeches were made in English 
(presumably mostly by native speakers), and 
then translated into French, while the reverse is 
true for French-English. Still, for many 
applications, these would appear as two 
undifferentiated subsets of an English-French 
parallel corpus.  
Since language tags are scarcely present, we 
automatically gathered all the tag information in 
all the language-specific files, correcting all the 
tags and discarding texts with contradictory 
information. Therefore, these extracted 
directional corpora are made of discontinuous 
sentences, because of the very nature of this 
multilingual corpus. In one single debate, each 
speaker speaks in his/her own language, and 
when extracting statements of one particular 
language, discourse cohesion across speakers is 
lost. However, this has no incidence at the 
global level on the quantitative distribution of 
connectives.  
We have focused our investigation on the 
years 1996 to 1999 of the Europarl corpus. 
Indeed, statistical investigations and information 
gathered at the European Parliament revealed 
that the translation policy had changed over the 
years. The 1996-1999 period appeared to contain 
the most reliable translated data of the whole 
corpus. 
For Experiment 1, we extracted two parallel 
directional corpora made of two languages ? 
French and English ? in order to compare 
translated and original texts in both languages, 
as shown in Figure 1. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 1 gives the number of tokens in the 
English-French and in the French-English 
parallel directional corpora. 
 
Parallel corpus Token in ST Token in TT 
English-French (EF) 1,412,316 1,583,775 
French-English (FE) 1,257,879 1,188,923 
Table 1: Number of tokens in Source Texts (ST) 
and Translated Texts (TT) of the parallel 
directional corpora. 
 
Following the same methodology, we extracted 
for Experiment 2 other parallel directional 
Figure 1: Parallel and comparable corpora 
extracted from Europarl 
Parallel directional corpora 
Comparable corpora 
Original 
English 
Original 
French 
Translated 
French 
Translated 
English 
80
corpora, again with French as a target language 
(also from the 1996-1999 period), as shown in 
Figure 2. Table 2 presents the sizes of these four 
additional comparable corpora.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Parallel corpus Token in ST Token in TT 
German-French (DF) 1,254,531 1,516,634 
Italian-French (IF) 552,242 624,534 
Spanish-French (SF) 597,607 633,918 
Table 2: Number of tokens in Source Texts (ST) 
and Translated Texts (TT) of the three additional 
parallel directional corpora of translated French. 
 
These parallel directional corpora have been 
used as comparable corpora in our study because 
they are written in the same language and are of 
the same genre, but do not have the same 
?status?, since some are original texts while 
others are translations, as shown in . Moreover, 
for comparison purposes, we have also used a 
sub-part of Europarl which was originally 
produced in French (noted OF), corresponding 
to the French part of the French-English corpus 
described in Table 1 
All the experiments described below are 
based on these comparable corpora, i.e. on the 
translated vs. original corpus (for French and 
English) and on the different corpora of 
translated French (with Italian, English, Spanish 
and German as source languages).  
4.2 First Measure: CBDF Measure 
Following a proposal by Kilgarriff (2001), who 
criticizes a number of simpler techniques, we 
have measured corpus similarity by computing 
the ?2 statistic over the 500 most frequent words 
from the two corpora to be compared, which 
were limited to 200,000 words each, so that 
comparison with the values given by Kilgarriff 
was possible. The value was normalized by the 
number of degrees of freedom, which is (500?
1) ? (2?1) = 499, hence its name. As shown by 
Kilgarriff with artificially designed corpora, for 
which the similarity level was known in 
advance, the ?2 statistic is a reliable indicator of 
similarity. Moreover, Kilgarriff (2001: Table 10, 
page 260) provides a table with the ?2 values for 
all 66 pairs of 200,000-word corpora selected 
from 12 English corpora, which we will use for 
comparison below. The table also lists internal 
homogeneity values for each corpus, obtained by 
averaging the ?2 statistic over each 200,000-
word corpus split several times in half. In fact, 
as the same method is used for computing both 
similarity and homogeneity, only 100,000-word 
fragments are used for similarity, as stated by 
Kilgarriff. 
The CBDF similarity values between 
100,000-word subsets of Original French (OF), 
French translated from English (EF), from 
Italian (IF), from German (DF), and from 
Spanish (SF) are shown in Table 4 below. 
Taking OF vs. EF as an example, these values 
are computed by summing up, for all of the most 
frequent 500 words in OF+EF, the difference 
between the observed and the expected number 
of occurrences in each of OF and EF, more 
precisely (o ? e)2 / e, and then dividing the sum 
by 499. The expected number is simply the 
average of OF and EF occurrences, which is the 
best guess given the observations. The lower the 
result, the closer the two corpora are considered 
to be, in terms of lexical distribution, as shown 
by Kilgarriff (2001). 
For measuring homogeneity, we sliced each 
corpus in 10 equal parts, and computed the score 
by randomly building 10 different corpus 
configurations and calculating the average of the 
values.  
4.3 Second Measure: Counting Connectives 
As explained above, we focused our experiments 
on comparing frequencies of causal connectives. 
For French, our list of items included parce que, 
puisque, car, and ?tant donn? que. For English, 
Figure 2: Parallel and comparable corpora 
for Translated French 
Parallel directional corpora 
Comparable corpora 
Original 
English 
Original 
Italian 
Trans-
lated 
French 
Original 
German 
Original 
Spanish 
Trans-
lated 
French 
Transl-
lated 
French 
Trans-
lated 
French 
81
we included because, since, and given that1. In 
the case of since, we manually annotated its two 
meanings in order to distinguish its causal uses 
from its temporal ones, and retained only its 
causal uses in our counts. 
To count the number of occurrences for each 
causal connective in each sub-part of the corpus, 
we first pre-processed the corpora to transform 
each connective as one word-form (e.g. ?tant 
donn? que became ?tantdonn?que, and puisqu? 
became puisque.). Then, we counted each 
connective, and normalized the figures to obtain 
a ratio of connectives per 100,000 tokens. 
Moreover, when comparing French sub-
corpora translated from different source 
languages, we also computed the rank of each 
connective in the frequency list extracted from 
each corpus. Comparing these ranks provided 
important information about their respective 
frequencies.  
We have found that the frequency of each 
connective does not vary significantly 
throughout the corpus (years 1996-1999), which 
tends to prove that the use of connectives does 
not depend crucially on the style of a particular 
speaker or translator.  
5 Results  
This section presents the results of the CBDF 
measure for each corpus (Section 5.1), and 
shows how the frequencies of connectives reveal 
differences between translated and original texts 
(Section 5.2) and between texts translated from 
various source languages (Section 5.3).   
5.1 Text Similarity according to CBDF 
For Experiment 1, we have compared the 
differences between original and translated texts, 
for English and French. The values of CBDF 
similarity resulting from this comparison are 
shown in Table 3. Compared to the different 
scores computed by Kilgarriff, these scores 
indicate that the two pairs of corpora are both 
quite similar.  
                                                          
1
  The English causal connective for is more 
difficult to address because of its ambiguity with the 
homographic preposition. However, on a sample of 500 
tokens of for randomly extracted from Europarl, we found 
only two occurrences of the connective for, leading us to 
exclude this connective from our investigation. 
 CBDF 
Original English ? Translated English 13.28 
Original French ? Translated French 12.28 
Table 3: CBDF between original and translated 
texts 
 
The similarities between sub-corpora of 
French translated from different source 
languages (Experiment 2) are shown in Table 4. 
The values comparing the same portion (e.g. 
OF/OF) indicate the homogeneity score of the 
respective sub-corpus. 
 
 OF EF DF IF SF 
OF 2.64     
EF 6.00 3.34    
DF 5.11 4.83 2.74   
IF 4.88 6.30 4.99 2.86  
SF 5.34 5.43 5.36 4.43 2.22 
Table 4: Values of CBDF (?2 statistic 
normalized by degrees of freedom) for all pairs 
of source-specific 200,000-word subsets from 
Europarl. The lower the value, the more similar 
the subsets.   
 
Looking at the values in Table 4, we can see 
that the similarity score between OF and EF is 
6.00, which, compared to Kilgarriff?s values for 
British corpora, is lower than all but two of the 
66 pairs of corpora he compared. Most of the 
values observed by Kilgarriff are in fact between 
20 and 40, and the similarity we found for OF 
vs. EF is, for instance, in the same range as the 
one for the journal The Face vs. The Daily 
Mirror, a tabloid, and higher than the similarity 
of two broadsheet newspapers (i.e., they get a 
lower CBDF value). Therefore, we can conclude 
that OF and EF are very similar from a word 
distribution point of view. 
As for the other pairs, they are all in the same 
range of similarity, again much more similar 
than the corpora cited in Kilgarriff?s Table 10. 
Regarding internal comparisons, OF/EF appears 
as the second most dissimilar pair, preceded 
only by IF/EF (French translated from Italian vs. 
from English). The most similar pair is Original 
French vs. French translated from Italian, which 
is not surprising given that the two languages are 
closely related. Also similar to OF/IF are the 
IF/SF and EF/DF pairs, reflecting the similarity 
of translations from related languages. 
82
Homogeneity values are higher than similarity 
values (the ?2 scores are lower). These values 
are again comparable, albeit clearly lower, than 
those found by Kilgarriff, and presumably 
account for the lower variety of parliamentary 
discourse. Still, these values are similar to those 
of the most homogeneous subset used by 
Kilgarriff, the Dictionary of National Biography 
(1.86) or the Computergram (2.20). 
Figures on the distribution of connectives, 
presented in the next section, tend to show that 
these sub-corpora are however not as similar as 
they may seem at a first view.  
5.2 Text Similarities Measured with the 
Use of Causal Connectives: 
Experiment 1 
In Experiment 1, we highlight the differences in 
the use of causal connectives between original 
English and translated English. Figure 3 shows 
the discrepancy between the use of the same 
connectives in original and translated texts. 
Among these connectives, since is the only truly 
ambiguous word. We have therefore also 
evaluated the proportion of causal uses of since 
among all the uses of the word since. In original 
English, this proportion is 31.8% and doubles in 
translated English to reach 67.7%. 
 
 
Figure 3: Ratio connectives/100,000 tokens in 
original and translated English. 
 
These figures show that original and 
translated texts differ, at least in terms of the 
number of causal connectives they contain. 
While because seems equally used in original 
and translated English, since and given that are 
used three times more frequently in translated 
than in original texts. This variability is also 
noticeable when comparing original and 
translated uses of French connectives, as shown 
in Figure 4.  
 
Figure 4: Ratio connectives/100?000 tokens in 
original and translated French. 
 
For French, while car seems to be equally 
used in both sub-parts of the corpus, parce que 
is used twice less frequently in translated than in 
original texts. This discrepancy is even bigger in 
the case of puisque, which is used five times less 
frequently in translated than in original texts. 
The reverse phenomenon is observed for ?tant 
donn? que, which is used four times more 
frequently in translated than in original texts.  
By looking at the translation of every 
connective, we were able to count the number of 
connectives inserted in the target language, that 
is to say when there was a connective in the 
target system but no connective in the original 
text. Conversely, we have also counted the 
number of connectives removed in the target 
text, when a connective in the source language 
was not translated at all. Overall, we found that 
connectives were inserted much more often than 
removed during the process of translation. In the 
case of English as a target language, 65 
connectives were inserted while 35 were 
83
removed. In the case of French, 46 connectives 
were inserted while 11 were removed.  
5.3 Text similarities measured by the use of 
causal connectives: Experiment 2 
When comparing the number of occurrences of 
French causal connectives across texts translated 
from different languages, the differences are 
striking. Indeed, every source language seems to 
increase the use of one specific connective in the 
French translations.  
Figure 5 presents the ratio of connectives per 
100?000 token. The data compares the use of 
connectives in French translated from English, 
Italian, Spanish and German.  
 
 
Figure 5: Connectives per 100,000 tokens in 
French texts translated from various source 
languages (for each connective, from left to right 
OF, EF, IF, DF, SF) 
 
Table 5 provides the rank of every connective 
in the word frequency list (sorted by decreasing 
frequency) computed for each sub-corpus. Grey 
cells indicate the most frequent connective in 
each sub-corpus. 
 
 
 
 
 OF EF IF DF SF 
parce que 115 292 99 159 87 
car 136 172 201 82 85 
puisque 235 1070 601 886 790 
?tant donn? que 3882 1368 2104 1450 459 
Table 5: Rank of the connectives in word 
frequency list for each corpus. Note that the 
order varies with the source language. 
 
These figures show that the distribution of 
every connective differs radically according to 
the source language. Every source language 
seems to increase the use of one specific 
connective. When German is the source 
language, car is used twice more often than 
when English or Italian are the source 
languages. When Italian is the source language, 
parce que is used twice as often and when 
English is the source language, ?tant donn? que 
is again used twice as often. Overall, puisque is 
the only connective that does not seem to be 
enhanced by any of the source languages, which 
confirms some prior linguistic analyses of this 
item, showing that puisque does not have exact 
equivalents in other close languages (Degand 
2004; Zufferey to appear).  
6 Discussion 
We have compared the use of discourse 
connectives in different sub-parts of the 
Europarl parallel corpus with the use of general 
vocabulary, as computed by a measure of lexical 
homogeneity. Our main finding is that even 
though the lexical measure showed the similarity 
of these sub-parts, the use of discourse 
connectives varied tremendously between the 
various sub-parts of our corpus. 
One of the reasons why connectives show 
more variability than many other lexical items is 
that they are almost always optional. In other 
words, as argued in Section 3, for every 
individual use of a connective, the translator has 
the option to use another connective in the target 
language or to leave the coherence relation it 
conveys implicit. Coherence marking is 
therefore a global rather than a local textual 
strategy. 
Given that connectives can be used or left out 
without producing ungrammatical results, 
studying their variability between comparable 
corpora provides interesting indications about 
84
their global homogeneity. The significant 
variability that we report between comparable 
(monolingual) sub-parts of the Europarl corpus 
indicates that they are not as homogeneous as 
global lexical measures like the CBDF tend to 
indicate. In other words, the various sub-parts of 
the corpus are not equivalents of one another for 
all purposes, and should not be used as such 
without caution. These differences were 
noticeable both by the different number of every 
connective used in every sub-part of the corpus, 
but also by the rather different frequency rank 
that was measured for every one of them in these 
same sub-parts. 
From a translation perspective, our study also 
provides some further confirmation for the 
existence of specific characteristics that define 
translated texts (i.e. ?translationese? or ?third 
code?). More specifically, our study 
corroborates the explicitation hypothesis (Blum-
Kulka 1986), positing that translated texts are 
more explicit than original ones due to an 
increase of cohesion markers. Connectives are 
part of the lexical markers that contribute to 
textual coherence, and we found that they are 
indeed more numerous in translated than in 
original texts. For English as a target language, 
translators have inserted twice as many 
connectives as they have removed. For French, 
this proportion raises to four times more 
insertions than omissions.  
However, our data also indicates that the 
source language has an important influence on 
the nature of its translation. Indeed, for the use 
of connectives, we report important variations 
between texts translated into French from 
various source languages. More interestingly 
still, every source language triggered the use of 
one specific connective over the others. This 
connective was always specific to one particular 
source language. 
It is also noteworthy that the similarity 
between texts translated into French, as 
measured with the CBDF, is greater when the 
source languages are typologically related. In 
our corpora of translated French, we found that 
texts were more similar when comparing the 
portion translated from Spanish and Italian 
(Romance languages) and when comparing texts 
translated from English and German (Germanic 
languages). This result makes intuitive sense and 
provides further confirmation of the reliability of 
this measure to assess global similarity between 
portions of texts. 
7 Conclusion 
The Europarl corpus is mostly used in NLP 
research without taking into account the 
direction of translation, in other words, without 
knowing which texts were originally produced 
in one language and which ones are translations. 
The experiments reported in this paper show that 
this status has a crucial influence of the nature of 
texts and should therefore be considered. 
Moreover, we have shown that translated texts 
from different source languages are not 
homogeneous either, therefore there is no unique 
translationese, and we identified some 
characteristics that vary according to the source 
language. 
Our study also indicates that global measures 
of corpus similarity are not always sensitive 
enough to detect all forms of lexical variation, 
notably in the use of discourse connectives. 
However, the variability observed in the use of 
these items should not be discarded, both 
because of their rather frequent use and because 
they form an important aspect of textual 
strategies involving cohesion. 
Acknowledgments 
This study was partially funded by the Swiss 
National Science Foundation through the 
COMTIS Sinergia project 
(www.idiap.ch/comtis). The authors would 
particularly like to thank Adam Kilgarriff for his 
explanations regarding the CBDF measure.  
References  
Altenberg Bengt. 1986. Contrastive linking in spoken 
and written English. In Tottie G. & B?cklund U. 
(Eds.), English in Speech and writing: a 
symposium. Uppsala, 13-40. 
Baker Mona. 1993. In Other Words. A coursebook on 
translation. Routledge, London/New York. 
Baker Mona. 1996. Corpus-based translation studies: 
The challenges that lie ahead. In Somers H. (Ed.) 
Terminology, LSP and Translation. Studies in 
language engineering in honour of Juan C. Sager. 
John Benjamins, Amsterdam, 175-186. 
85
Baroni Marco and Bernardini Silvia. 2006. A new 
approach to the study of translationese: Machine-
learning the difference between original and 
translated text. Literary and Linguistic Computing 
21(3). 259-274 
Degand Liesbeth. 2004. Contrastive analyses, 
translation and speaker involvement: the case of 
puisque and aangezien. In Achard, M. & Kemmer, 
S. (Eds.), Language, Culture and Mind. The 
University of Chicago Press, Chicago, 251-270. 
Halliday Michael and Hasan Ruqaiya. 1976. Cohesion 
in English. Longman, London 
Halverson Sandra. 2004. Connectives as a translation 
problem. In Kittel, H. et al (Eds.) An International 
Encyclopedia of Translation Studies. Walter de 
Gruyter, Berlin/New York, 562-572. 
Ilisei Iustina, Inkpen Diana, Corpas Pastor Gloria and 
Mitkov Russlan. 2010 Identification of 
Translationese: A Machine Learning Approach. In 
Gelbukh, A. (Ed), Computational Linguistics and 
Intelligent Text Processing Lecture Notes in 
Computer Science. Springer, Berlin / Heidelberg, 
503-511 
Kilgarriff Adam. 2001. Comparing Corpora. Intl. 
Journal of Corpus Linguistics 6(1): 1-37. 
Kilgariff Adam. 1997. Using word frequency lists to 
measure corpus homogeneity and similarity 
between corpora. In Fifth ACL Workshop on Very 
Large Corpora, Beijing. 
Knott Alistair and Dale Robert. 1994. Using 
linguistic phenomena to motivate a set of 
coherence relations. Discourse processes 18(1), 
35-62. 
Koehn Philipp. 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation, MT Summit 2005. 
Lamiroy Beatrice. 1994. Pragmatic connectives and 
L2 acquisition. The case of French and Dutch. 
Pragmatics 4(2), 183-201. 
Laviosa-Braithwaite Sara. 1996. The English 
Comparable Corpus (ECC): A Resource and a 
Methodology for the Empirical Study of 
Translation. PhD Thesis, Manchester, UMIST. 
 
Mann William and Thomson Sandra. 1992. 
Relational Discourse Structure: A Comparison of 
Approaches to Structuring Text by 'Contrast'. In 
Hwang S. & Merrifield W. (Eds.), Language in 
Context: Essays for Robert E. Longacre. SIL, 
Dallas, 19-45. 
Millis Keith & Just Marcel. 1994. The influence of 
connectives on sentence comprehension. Journal 
of Memory and Language 33 (1): 128-147. 
New Boris, Pallier Christophe, Brysbaert Marc, Ferr 
Ludovic and Holloway Royal. 2004. Lexique~2: A 
New French Lexical Database. Behavior Research 
Methods, Instruments, & Computers, 36 (3): 516-
524.  
Noordman Leo and de Blijzer Femke. 2000. On the 
processing of causal relations. In E. Couper-
Kuhlen & B. Kortmann (Eds.) Cause, Condition, 
Concession, Contrast. Mouton de Gruyter, Berlin. 
35-56. 
Ozdowska Sylvia. 2009. Donn?es bilingues pour la 
TAS fran?ais-anglais : impact de la langue source 
et direction de traduction originales sur la qualit? 
de la traduction. Proceedings of Traitement 
Automatique des Langues Naturelles, TALN'09, 
Senlis, France. 
Sanders Ted. 1997. Semantic and pragmatic sources 
of coherence: On the categorization of coherence 
relations in context. Discourse Processes 24: 119?
147. 
Sanders Ted and Sweetser Eve (Eds) 2009. Causal 
Categories in Discourse and Cognition. Mouton de 
Gruyter, Berlin. 
Tirkkonen-Condit Sonja. 2000. In search of 
translation universals: non-equivalence or 
? unique ? items in a corpus test. Paper presented 
at the UMIST/UCL Research Models in 
Translation Studies Conference, Manchester, UK, 
April 2000. 
Zufferey Sandrine to appear. ?Car, parce que, 
puisque? Revisited. Three empirical studies on 
French causal connectives. Journal of Pragmatics. 
 
 
 
86
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 194?203,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Multilingual Annotation and Disambiguation of Discourse Connectives for
Machine Translation
Thomas Meyer and Andrei Popescu-Belis
Idiap Research Institute
Rue Marconi 19, 1920 Martigny, Switzerland
Thomas.Meyer@idiap.ch, Andrei.Popescu-Belis@idiap.ch
Sandrine Zufferey and Bruno Cartoni
Department of Linguistics, University of Geneva
Rue de Candolle 2, 1211 Geneva 4, Switzerland
Sandrine.Zufferey@unige.ch, Bruno.Cartoni@unige.ch
Abstract
Many discourse connectives can signal several
types of relations between sentences. Their
automatic disambiguation, i.e. the labeling of
the correct sense of each occurrence, is impor-
tant for discourse parsing, but could also be
helpful to machine translation. We describe
new approaches for improving the accuracy
of manual annotation of three discourse con-
nectives (two English, one French) by using
parallel corpora. An appropriate set of labels
for each connective can be found using infor-
mation from their translations. Our results for
automatic disambiguation are state-of-the-art,
at up to 85% accuracy using surface features.
Using feature analysis, contextual features are
shown to be useful across languages and con-
nectives.
1 Introduction
Discourse connectives are generally considered as
indicators of discourse structure, relating two sen-
tences of a written or spoken text, and making ex-
plicit the rhetorical or coherence relation between
them. Leaving aside the cases when connectives are
only implicit, the presence of a connective does not
unambiguously signal a specific discourse relation.
In fact, many connectives can indicate several types
of relations between sentences, i.e. they have several
possible ?senses? in context.
This paper studies the manual and automated dis-
ambiguation of three ambiguous connectives in two
languages: alors que in French, since and while in
English. We will show how the multilingual per-
spective helps to improve the accuracy of annota-
tion, and how it helps to find appropriate labels for
automated processing and MT. Results from auto-
matic annotation experiments, which are close to the
state of the art, as well as feature analysis, help to as-
sess the usefulness of the proposed labels.
The paper is organized as follows. Section 2 ex-
plains the motivation of our experiments, and of-
fers a wider perspective on our research goals, illus-
trating them with examples of translation problems
which arise from ambiguous discourse connectives.
Current resources and methods for discourse anno-
tation are discussed in Section 3. Section 4 analyzes
our experiments in manual annotation and in partic-
ular the influence of the set of labels on the reliability
of annotation. The automatic disambiguation exper-
iments, the features used, the results and the analysis
of features are described in Section 5. Section 6 con-
cludes the paper and outlines future work.
2 Explicit Connectives and their
Translation
2.1 Three Multi-functional Connectives
Discourse connectives form a functional category of
lexical items that are used to mark coherence rela-
tions such as Cause or Contrast between units of
discourse. Along with other function words, many
connectives appear among the most frequent words,
as shown for instance by counts (Cartoni et al,
2011) over the Europarl corpus (Koehn, 2005). The
Penn Discourse Treebank (Prasad et al, 2008) (see
Section 3.1 below) includes around 100 connective
types, but the exact number varies across studies,
194
depending on the discourse theory used to classify
them. Among these types, Pitler et al(2008) have
shown that most of them are unambiguous and easy
to identify, but others, especially temporal ones, of-
ten signal multiple senses depending on their con-
text.
Following the terminology of Petukhova and
Bunt (2009, Section 2), we are interested here in
?sequential? multi-functionality, i.e. the fact that the
same connective can signal different relations in dif-
ferent contexts. We do not deal with ?simultane-
ous? multi-functionality, i.e. the possibility for a
single occurrence to signal several relations, which
has been less frequently studied for connectives (see
Petukhova and Bunt (2009) for the discourse usage
of and).
We identified the two English connectives while
and since, along with the French connective alors
que, as being particularly problematic because they
are highly multi-functional, i.e. they can signal mul-
tiple senses. For alors que, a French database of
connectives (LexConn (Roze et al, 2010), see Sec-
tion 3 below) contains examples of sentences where
alors que expresses either a Background or a Con-
trast relation. For the English connective since,
Miltsakaki et al (2005) identified three possible
meanings: Temporal, Causal, and simultaneously
Temporal/Causal. For while, even more senses are
observed: Comparison, Contrast, Concession, and
Opposition. In fact, in the Penn Discourse Tree-
bank, the connective while is annotated with more
than twenty different senses.
2.2 Wider Research Objectives
Our long-term goal is to identify automatically the
senses of connectives for an application to machine
translation (MT). Going beyond the labels provided
by discourse theories, the goal is thus to find the
most appropriate labels in a new multilingual, em-
pirical approach that makes use of parallel corpora to
annotate and then learn the various senses of connec-
tives. The disambiguation of such connectives in a
source text is crucial for its translation, because each
sense may be translated by a different connective
and/or syntactical construct in the target language.
More specifically, we hypothesize that correctly
labeled connectives are easier to learn and to trans-
late by statistical MT systems than unlabeled ones.
To support this hypothesis, we set up an experiment
(Meyer, 2011) in which we constrained the transla-
tion of the three senses of the discourse connective
while that were previously annotated as Temporal,
Contrast and Concession. The system was forced to
use predefined French translations known to be cor-
rect, by directly modifying the phrase table of the
trained MT system. This modification noticeably
helped to improve translation quality and rose the
BLEU score by 0.8 for a preliminary test set of 20
sentences.
2.3 Illustration of Mistranslations
Among the connectives that we plan to process in or-
der to improve MT, the three connectives we focus
on in this paper are frequent, ambiguous and there-
fore difficult to translate correctly by MT systems,
as illustrated in the following examples.
A first reason why machine translation of connec-
tives can be difficult is that there may be no direct
lexical correspondence for the explicit source lan-
guage connective in the target language, as shown
in the reference translation of the first example in
Table 1, taken from the Europarl corpus (Koehn,
2005).
EN It is also important that we should not leave these indica-
tors floating in the air while congratulating ourselves on
the fact that we have produced them.
FR Il est e?galement important de ne pas laisser ces indicateurs
flotter, en nous fe?licitant de les avoir instaure?s.
EN Finally, and in conclusion, Mr President, with the expiry of
the ECSC Treaty, the regulations will have to be reviewed
since [causal] I think that the aid system will have to con-
tinue beyond 2002 . . .
FR *Enfin, et en conclusion, Monsieur le pre?sident, a`
l?expiration du traite? ceca, la re?glementation devra e?tre
revu depuis que [temporal] je pense que le syste`me d?aides
devront continuer au-dela` de 2002 . . .
FR Oui, bien entendu, sauf que le de?veloppement ne se ne?gocie
pas, alors que [contrast] le commerce, lui, se ne?gocie.
EN *Yes, of course, but development cannot be negotiated, so
[causal] that trade can.
EN Between 1998 and 1999, loyalists assaulted and shot 123
people, while [contrast] republicans assaulted and shot 93
people.
FR *Entre 1998 et 1999, les loyalistes ont attaque? et abattu
123 personnes, ? 93 pour les re?publicains.
Table 1: Translation examples from Europarl. Discourse
connectives, their translations, and their senses are indi-
cated in bold. The first example is a reference transla-
tion from EN into FR, while the others are wrong transla-
tions generated by MT (EN/FR and respectively FR/EN),
hence marked with an asterisk.
195
When an ambiguous connective is explicitly
translated by another connective, the incorrect ren-
dering of its sense can lead to erroneous translations,
as in the second and third examples in Table 1, which
are translated by the Moses SMT decoder (Koehn et
al., 2007) trained on the Europarl corpus. The ref-
erence translation for the second example uses the
French connective car with a correct causal sense,
instead of the wrong depuis que generated by SMT,
which expresses a temporal relation. In the third ex-
ample, the French connective alors que, in its con-
trastive usage, is wrongly translated into the English
connective so, which has a causal meaning (the ref-
erence translation uses whereas to express contrast).
It may even occur that the system fails to translate a
connective at all, as in the fourth example where the
discourse information provided by while, namely a
Contrast relation, is lost in the French translation,
which is hardly coherent any longer.
3 Related Work
3.1 Annotated Resources
One of the very few available discourse annotated
corpora is the Penn Discourse Treebank (PDTB) in
English (Prasad et al, 2008). For this resource, one
hundred types of explicit discourse connectives were
manually annotated, as well as implicit relations not
signaled by a connective. The sense hierarchy used
for annotation consists of three levels, from four top-
level senses (Temporal, Contingency, Comparison,
and Expansion), to 16 subsenses on the second level,
and 23 further ones on the third level. The annota-
tors were allowed to assign more than one sense to
each occurrence, so 129 simple or complex labels
are observed, over more than 18,000 explicit con-
nectives. For French, the ANNODIS project (Pe?ry-
Woodley et al, 2009) will provide annotation of dis-
course on an original corpus. Resources for Czech
are also becoming available (Zika?nova? et al, 2010).
For German, a lexicon of discourse markers
named DiMLex exists since the 1990s (Stede and
Umbach, 1998). An equivalent, more recent
database for French is the LexConn lexicon of con-
nectives (Roze et al, 2010) containing a list of 328
explicit connectives. For each of them, LexConn
indicates and exemplifies the possible senses, cho-
sen from a list of 30 labels inspired from Rhetorical
Structure Theory (Mann and Thompson, 1988).
3.2 Automatic Disambiguation of Connectives
The release of the PDTB had quite an impact on
automatic disambiguation experiments. The state-
of-the-art for recognizing all types of explicit con-
nectives in English is therefore already high, at
97% accuracy for disambiguating discourse vs. non-
discourse uses (Lin et al, 2010) and 94% for disam-
biguating the four main senses from the PDTB hier-
archy (Pitler and Nenkova, 2009). Lin et al (2010)
recently built the first end-to-end PDTB discourse
parser, which is able to parse unrestricted text with
an F1 score of 38.18% for senses on the second level
of the PDTB hierarchy. Other important contribu-
tions to automatic discourse connective classifica-
tion and feature analysis has been provided by Well-
ner et al (2006) and Elwell and Baldrige (2008).
Fewer studies focus on the detailed analysis of
specific discourse connectives. In Section 5.3, we
will compare our results to Miltsakaki et al (2005)
who report classification results for the connectives
since, while and when. In their study, as in the
present one, the goal is to disambiguate senses from
the second level of the PDTB hierarchy, a level
which, as we will show, is appropriate for the trans-
lation of these connectives as well.
4 Connective Annotation in Parallel
Corpora
The resources mentioned above are either monolin-
gual only (PDTB, LexConn) and/or not yet publicly
available (ANNODIS, DiMLex). Moreover, our
overall goal is related to multilingualism and trans-
lation, as explained in Section 2.2 above. There-
fore, we performed manual annotation of connec-
tives in a multilingual, aligned resource: the Eu-
roparl corpus (Koehn, 2005). We extracted from Eu-
roparl two subcorpora for each translation direction,
EN/FR and FR/EN, to take into account the varying
distribution of connectives in translated vs. original
language, as explained in Cartoni et al (2011).
As the full PDTB hierarchy seemed too fine-
grained given current capabilities for automatic la-
beling and the needs for translating connectives,
we defined a simplified set of labels for the senses
of connectives, by considering their usefulness and
196
granularity with respect to translation, focusing on
those that may lead to different connectives or syn-
tactical constructs in the target language.
4.1 Method
There are two major ways to annotate explicit dis-
course connectives. The first approach is to label
each occurrence of a connective with a label for
its sense, similar to the PDTB or LexConn hierar-
chies of senses. However, as shown among others
by Zikanova et al (2010), this is a difficult and time-
consuming task even when the annotators are trained
over a long period of time. This is confirmed by the
rather low kappa scores resulting from the manual
sense annotations as can be seen for each connective
in detail below.
The second approach to annotation, which is the
one put forward in this paper, is based on translation
spotting. In a first step, human annotators work on
bilingual sentence pairs, and annotate the translation
of each connective in the target language. The trans-
lations are either a target language connective (sig-
naling in principle the same sense(s) as the source
one), or a reformulation, or a construct with no con-
nective at all. In a second step of the annotation,
all translations of a connective are manually clus-
tered by the experimenters to derive sense labels, by
grouping together similar translations.
As demonstrated in the following subsections, for
the three connectives under study, the second ap-
proach to connective annotation not only facilitates
the annotation task, but also helps to derive the ap-
propriate level of granularity for the sense labels.
4.2 Annotation of alors que
This first manual annotation involved two experi-
enced annotators who annotated alors que in 423
original French sentences. The two main senses
identified for alors que are Background (labeled B)
Contrast (labeled C), as in the LexConn database.
Annotators were also allowed to use the J label if
they did not know which label to assign, and a
D label for discarded sentences ? due to a non-
connective use of the two words which could not be
filtered out automatically (e.g. Alors, que fera-t-on?
). The annotators found 20 sentences labeled with
D, which were removed from the data. 15 sentences
were labeled with J by one annotator (but none by
both), and it was decided to assign to them the label
(either B or C) provided by the other annotator.
The inter-annotator agreement on the B vs. C la-
bels was quite low, showing the difficulty of the task:
kappa reached 0.43, quite below the 0.7 mark often
considered as indicating reliability. The following
example from Europarl illustrates the difficulty of
choosing between B and C. In particular, the refer-
ence translation into English also uses an ambiguous
connective, namely while.
FR La monnaie unique va entrer en vigueur au milieu
de la tourmente financie`re, alors que de nombreux
comple?ments, logiques, mais que les E?tats ne sem-
blaient pas avoir pre?vus, n?ont pas encore e?te? ap-
porte?s.
EN The single currency is going to come into force in the
midst of financial turmoil, while a great many ad-
ditional factors which were only to be expected, but
which the states do not seem to have anticipated, have
not been taken into consideration.
Two methods were applied to deal with diverg-
ing manual annotations. To prepare the datasets for
the automated disambiguation experiments, one so-
lution (named A1, see Table 2) is to use the double-
sense label B/C for sentences labeled differently by
annotators (B vs. C). This label reflects the diffi-
culty of manual annotation and preserves the am-
biguity which might be genuinely present in each
occurrence. The relevance of the B/C label is also
supported by results from automatic labeling in Sec-
tion 5.3 below.
For comparison purposes, a second dataset named
A2 was derived from translation spotting on the
same French sentences aligned to English ones, as
explained in Section 4.1. Alors que appeared to be
mainly translated by the following English equiv-
alents and constructs: although, whereas, while,
whilst, when, at a time when. Through this opera-
tion, inter-annotator disagreement can sometimes be
solved: when the translation is a clearly contrastive
English connective (whereas or although), then the
C label was assigned instead of B/C. Conversely,
when the English translation was still ambiguous
(while, whilst, or when), the experimenters made a
decision in favor of either B or C by re-examining
source and target sentences.
4.3 Annotation of since
For since, 30 sentences were annotated by four ex-
perimenters in a preliminary round, with a kappa
197
ID Connective Sent. Labels (nb. of occ.)
A1 alors que 403 B (92), C (191), B/C (120)
A2 alors que 403 B (126), C (277)
B1 since 727 T (375), C (341), T/C (11)
B2 since 727 T (375), C (352)
C1 while 299 T/C (92), CONC (134), C (43)
T/CAUSAL (19), T/DUR (7)
T/PUNCT (4)
C2 while 299 T (30), C (135), CONC (134)
Table 2: The six datasets resulting from the manual anno-
tation of the three connectives, with total number of sen-
tences, possible labels and their number of occurrences.
The explanations of the labels are given in Sections 4.2
through 4.4.
score of 0.77, indicating good agreement. Then,
each half of the entire dataset (727 sentences) was
annotated by another person with three possible
sense labels: T for Temporal, C for Causal and
T/C for a simultaneously Temporal/Causal meaning.
Two datasets were again derived from this manual
annotation. To study the effects of a supplementary
label, we kept the label T/C for dataset B1, but con-
densed it under label C in dataset B2, as shown in
Table 2.
4.4 Annotation of while
The English connective while is highly ambiguous.
In the PDTB, occurrences of while are annotated
with no less than 21 possible senses, ranging from
Conjunction to Contrast, Concession, or Synchrony.
We performed a pilot annotation of 30 sentences
containing while with five different experimenters,
resulting in a quite low inter-annotator agreement,
? = 0.56. We therefore decided to perform a
translation spotting task only, with two experienced
annotators fluent in English and French. The ob-
served translations into French confirm the ambigu-
ity of while, as they include several connectives and
constructs, quite evenly distributed in terms of fre-
quency: alors que, gerundive reformulations, other
reformulations, si, tandis que, me?me si, bien que,
etc.
The translations were manually clustered to de-
rive senses for while, in an empirical manner.
For example, alors que signals Temporal/Contrast,
which is also true for tandis que. Similarly, me?me si
and bien que are clustered under the label Conces-
sion, and so forth. The translation spotting shows
that at least Contrast, Concession, and several tem-
poral senses are necessary to account for a correct
translation. These distinctions are comparable to the
semantic granularity of the second PDTB hierarchy
level.
To generate training sets for automated classifica-
tion out of a total of 500 sentences, we discarded 201
sentences labeled by annotators with G (gerundive
constructions), P (reformulations) or Z (no transla-
tion at all) ? these cases could be reconsidered in fur-
ther work, as they represent valid translation prob-
lems. For the remaining 299 sentences, we created
the following six labels by clustering the spotted
translations: T/C (Temporal/Contrast), T/PUNCT
(Temporal/Punctual), T/DUR (Temporal/Duration),
T/CAUSAL (Temporal/Causal), CONC (Conces-
sion) and C (Contrast). These were used to tag the
remaining 299 sentences, forming dataset C1. A
second dataset (C2) with fewer senses was obtained
from C1 by merging T/C to C (Contrast only) and
all T/x to T (Temporal only).
5 Disambiguation Experiments
The features for connective classification, the re-
sults obtained and a detailed feature analysis are dis-
cussed in this section. We show that an automated
disambiguation system can be used to determine the
most appropriate set of labels, and thus to corrob-
orate the selection we made using translation spot-
ting.
5.1 Features
For feature extraction, all the datasets described in
Section 4 were processed as follows. The English
texts were parsed and POS-tagged by Charniak and
Johnson?s (2005) reranking parser. The French texts
were POS-tagged with the MElt tagger (Denis and
Sagot, 2009) and parsed with MaltParser (Nivre,
2003). As the English parser provides constituency
trees, and the parser for French generates depen-
dency trees, the features are slightly different in the
two languages. The other features below were ex-
tracted using elementary pre-processing of the sen-
tences.
For English sentences, we used the following fea-
tures: the sentence-initial character of the connec-
198
tive (yes/no); the POS tag of the first verb in the
sentence; the type of first auxiliary verb in the sen-
tence (if any); the word preceding the connective;
the word following the connective; the POS tag of
the first verb following the connective; the type of
the first auxiliary verb after the connective (if any).
For French sentences, the features were the fol-
lowing: the sentence-initial character of the connec-
tive (yes/no); the dependency tag of the connective;
the first verb in the sentence; its dependency tag; the
word preceding the connective; its POS tag; its de-
pendency tag; the word following the connective; its
POS tag; its dependency tag; the first verb after the
connective; its dependency tag.
The cased connective word forms from the cor-
pus were not lower-cased, thus keeping the implicit
indication of the sentence-initial character of the oc-
currence, i.e. whether it starts a sentence or not. The
output of the POS taggers was used for neighboring
words, but not for the connectives, which almost al-
ways received the same tag. Charniak?s parser for
English provides POS tags which differentiate the
verb tenses, such as VBD (past), VBG (gerund), and
so on. These were considered for the verb directly
preceding and the one directly following the connec-
tive. Tense was believed to be potentially relevant
because since and while can have temporal mean-
ings.
The occurrence of auxiliary verbs (be, have, do,
or need) may give additional indications about tem-
poral relations in the sentence. We therefore used
the types of auxiliary verbs as features, including
the elementary conjugations, represented for to be
as: be present, be past, be part, be inf, be gerund
? and similarly for the other auxiliary verbs, as in
(Miltsakaki et al, 2005).
As shown by Lin et al (2010), duVerle and
Prendinger (2009) or Wellner et al (2006), the con-
text of a connective is very important. We there-
fore extracted the words preceding and following
each connective, the verbs and the first and the last
word of the sentences. These may include numbers,
sometimes indicating a numerical comparison, time
expressions, or antonyms, which could indicate con-
trastive relations, such as rise vs. fall (e.g. It is inter-
esting to see the fundamental stock pickers scream
?foul? on program trading when the markets de-
cline, while hailing the great values still abounding
as the markets rise.).
For French, we likewise extracted the words im-
mediately preceding and following each connective,
supplemented by their POS tags. In contrast to con-
stituents, dependency structures contain information
about the grammatical function of each word (heads)
and link the dependents belonging to the same head.
However, as the dependency parser provides no dif-
ferentiated verb tags, we extracted the verb word
forms themselves and added their dependency tags.
The same applies to the connective itself, and pre-
ceding and following words and their dependency
tags.
The dependency tag of the non-connectives varies
between subj (subject), det (determiner), mod (mod-
ifier) and obj (object). The first verb in the sentence
often belongs to the root dependency while the verb
following the connective most often belongs to the
obj dependency. For alors que, the most frequent
dependency tags were mod mod and mod obj, indi-
cating the connective?s main function as a modifier
of its argument.
5.2 Experimental Setting
Our classification experiments made use of the
WEKA machine learning toolkit (Hall et al, 2009)
to run and compare several classification algorithms:
Random Forest (sets of decision trees), Naive Bayes,
and Support Vector Machine. The results are re-
ported with 10-fold cross validation on the entire
data for each connective, using all features.
Table 3 lists for each method ? including the ma-
jority classifier as a baseline ? the percentage of cor-
rectly classified instances (or accuracy, noted Acc.),
and the kappa values. Significance above the base-
line is computed using paired t-tests at 95% confi-
dence. When a score is significantly above the base-
line, it is shown in italics in Table 3. The best scores
for each dataset, across classifiers, are indicated in
boldface. When these scores were not significantly
above the baseline, at least they were never signifi-
cantly below either.
5.3 Results and Discussion
Overall, the SVM classifier performed best, which
may be due to the large number of textual features
(3 for EN data and 5 for FR data), as SVMs are
known to handle them well (Joachims, 1998; du-
199
ID Connective # Labels Baseline R. Forest N. Bayes SVM
Acc. Acc. ? Acc. ? Acc. ?
A1 alors que 403 B, C, B/C 46.9 53.1 0.2 55.7 0.3 54.2 0.3
A2 alors que B, C 68.7 69.2 0.1 68.3 0.2 64.7 0.1
B1 since 727 T, C, T/C 51.6 79.8 0.6 82.3 0.7 85.4 0.7
B2 since T, C 51.6 80.7 0.6 84.0 0.7 85.7 0.7
C1 while 299 T/C, T/PUNCT, T/DUR,
T/CAUSAL, CONC, C
44.8 43.2 0.1 49.9 0.2 52.2 0.2
C2 while T, C, CONC 43.5 60.5 0.3 59.9 0.3 60.9 0.3
Table 3: Disambiguation scores for three connectives (number of occurrences in the training sets), with two sets of
labels each, for various classification algorithms. Accuracy (Acc.) is in percentage (%), and kappa is zero for the
baseline method (majority class). The best scores for each data set are in boldface, and scores significantly above the
baseline (95% t-test) are in italics.
Verle and Prendinger, 2009). The maximum accu-
racy for alors que is 55.7%, for since it is 85.7%, and
for while it is 60.9%. While close to other reported
values, there is still potential for improvement in the
future.
The analysis of results for each data sets leads
to observations that are specific to each connective.
The high improvement of over the baseline for A1,
as opposed to no improvement for A2, confirms the
usefulness of the double-sense B/C label for alors
que, showing that in this case the three-way classi-
fication is probably better adapted to the linguistic
properties of alors que than a two-way classifica-
tion. Indeed, alors que, just as its frequently spot-
ted translation while, is linguistically ambiguous in
some contexts (see for instance the example in Sec-
tion 4.2), in which the temporal and the contrastive
meaning are likely to co-exist. In the case of A2,
where the labels were forced to B or C only, auto-
matic classifiers do not significantly outperform the
baseline. While more elaborate features might help,
these low scores can be related to the difficulties of
human annotators (Section 4.2), and make a strong
case against using a two-label schema for alors que.
The features used so far lead to high scores for
since in datasets B1 and B2. The results are com-
parable to those from Miltsakaki et al (2005), who
used similar features and labels, though with a Max-
imum Entropy classifier. Moreover, they provide re-
sults for individual connectives, and not, as most of
the related work for the PDTB, on the whole set
of ca. 100 discourse connective types. However,
Miltsakaki et al (2005) used their own datasets for
each connective, which are different from the PDTB,
because the PDTB was not available at that time.
Our SVM classifier outperforms considerably the
Maximum Entropy classifier on the three-way clas-
sification task (with T, C, T/C), with an accuracy
of 85.4% vs. 75.5%, obtained however on differ-
ent datasets. For the two-way classification (T, C),
again on different datasets, our accuracy of 85.7% is
slightly lower than the 89.5% given in Miltsakaki et
al. (2005).1
For while, when comparing C1 to C2, it appears
that reducing the number of labels from six to three
increases accuracy by 8-10%. This is probably
due to the small number of training instances for
the labels T/PUNCT and T/DUR in C1 for exam-
ple. However, even for the larger set of labels, the
scores are significantly above baseline (52.2% vs.
44.8%), which indicates that such a classifier might
still be useful as input to an MT system, possibly
improved thanks to a larger training set. The perfor-
mance obtained by Miltsakaki et al (2005) on while
is markedly better than ours, with an accuracy of
71.8% compared to ours of 60.9% with three labels.
5.4 Feature Analysis
The relevance of features can be measured using
WEKA by computing the information gain (IG)
brought by each feature to the classification task,
1In another experiment (Meyer, 2011), we also applied our
classifiers to the PDTB data, with less features however. The
results were in the same range as those from Miltsakaki et
al. (2005), i.e. 75.3% accuracy for since and 59.6% for while.
200
R Feature IG
A1 A2
1 preceding word 1.12 0.64
2 following verb 0.81 0.51
3 first verb 0.74 0.42
4 following word 0.68 0.23
5 preceding word?s POS tag 0.15 0.05
5 first verb?s dep. tag 0.14 0.06
5 following word?s POS tag 0.19 0.03
8 preceding word?s dep. tag 0.10 0.03
8 connective?s dep. tag 0.09 0.04
10 following word?s dep. tag 0.13 0.013
10 following verb?s dep. tag 0.04 0.03
12 sentence initial 0.05 0.001
Table 4: Information gain (IG) of features for French con-
nective alors que, ordered by decreasing average ranking
(R) in experiments A1 and A2. Features 1?4 are consid-
erably more relevant than the following ones.
R Feature IG
B1 B2
1 preceding word 0.83 0.75
2 following word 0.56 0.52
3 following verb?s POS tag 0.24 0.21
4 type of following aux. verb 0.13 0.12
5 type of first aux. verb 0.11 0.11
6 first verb?s POS tag 0.02 0.01
7 sentence initial 0.00 0.00
Table 5: Information gain (IG) of features for EN con-
nective since, ordered by decreasing average ranking (R)
in experiments B1 and B2.
i.e. the reduction in entropy with respect to desired
classes (Hall et al, 2009) ? the higher the IG, the
more relevant the feature. Features can be ranked
by decreasing IG, as shown in Tables 4, 5 and 6, in
which ranks were averaged over the first and the sec-
ond data set in each series.
The tables show that across all three connectives
and the two languages, the contextual features are
always in the first positions, thus confirming the im-
portance of the context of a connective. Following
these are verbal features, which are, for these con-
nectives, of importance because the temporal mean-
ings are additionally established by verbal tenses.
POS and dependency features seem the least help-
R Feature IG
C1 C2
1 preceding word 1.02 0.65
2 following word 0.83 0.55
3 type of first aux. verb 0.12 0.07
4 following verb?s POS tag 0.16 0.04
5 first verb?s POS tag 0.07 0.09
5 type of following aux. verb 0.12 0.05
7 sentence initial 0.08 0.07
Table 6: Information gain (IG) of features for EN con-
nective while, ordered by decreasing average ranking (R)
in experiments C1 and C2. The first two features are con-
siderably more relevant than the remaining ones.
ful for disambiguation.
6 Conclusion and Future Work
We have described a translation-oriented approach
to the manual and automatic annotation of discourse
connectives, with the goal of identifying their senses
automatically, prior to machine translation. The
manual annotation of the senses of connectives has
been enhanced through parallel corpora and transla-
tion spotting. This has lead to tag sets that improved
both inter-annotator agreement and automatic label-
ing, which reached state-of-the-art scores. The ana-
lysis of relevant features has shown the utility of
contextual information.
To improve over these initial results, we will use
more semantic information, such as relations found
in WordNet between words in the neighborhood of
connectives ? e.g. word similarity measures and se-
mantic relations such as antonymy. To generate
more training instances of the labels found, man-
ual annotation will continue in order to see whether
the senses found through translation spotting can im-
prove automatic disambiguation of many more con-
nectives. The annotation of a large parallel corpus
will then help to train disambiguation tools along
with statistical MT systems that use their output.
Acknowledgments
We are grateful for the funding of this work by the
Swiss National Science Foundation (SNSF) under
the COMTIS Sinergia Project, n. CRSI22 127510
(see www.idiap.ch/comtis/).
201
References
Bruno Cartoni, Sandrine Zufferey, Thomas Meyer, and
Andrei Popescu-Belis. 2011. How comparable are
parallel corpora? Measuring the distribution of gen-
eral vocabulary and connectives. In Proceedings of 4th
Workshop on Building and Using Comparable Cor-
pora, Portland, OR.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL 2005 (43rd Annual Meet-
ing of the ACL), pages 173?180, Ann Arbor, MI.
Pascal Denis and Beno??t Sagot. 2009. Coupling an anno-
tated corpus and a morphosyntactic lexicon for state-
of-the-art POS tagging with less human effort. In
Proceedings of PACLIC 2009 (23rd Pacific Asia Con-
ference on Language, Information and Computation),
pages 110?119, Hong Kong, China.
David duVerle and Helmut Prendinger. 2009. A novel
discourse parser based on support vector machine clas-
sification. In Proceedings of ACL-IJCNLP 2009 (47th
Annual Meeting of the ACL and 4th International Joint
Conference on NLP of the AFNLP), pages 665?673,
Singapore.
Robert Elwell and Jason Baldridge. 2008. Discourse
connective argument identification with connective
specific rankers. In Proceedings of ICSC 2008 (2nd
IEEE International Conference on Semantic Comput-
ing), pages 198?205, Santa Clara, CA.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
ACM SIGKDD Explorations Newsletter, 11:10?18.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. In Proceedings of ECML 1998 (10th Euro-
pean Conference on Machine Learning), pages 137?
142, Chemnitz, Germany.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbs. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of ACL 2007 (45th Annual Meeting of the
ACL), Demonstration Session, pages 177?180, Prague,
Czech Republic.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit X, pages 79?86, Phuket, Thailand.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A
PDTB-styled end-to-end discourse parser. Technical
Report TRB8/10, School of Computing, National Uni-
versity of Singapore, Singapore.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: towards a functional the-
ory of text organization. Text, 8(3):243?281.
Thomas Meyer. 2011. Disambiguating temporal-
contrastive discourse connectives for machine transla-
tion. In Proceedings of ACL-HLT 2011 (49th Annual
Meeting of the ACL: Human Language Technologies),
Student Session, Portland, OR.
Eleni Miltsakaki, Nikhil Dinesh, Rashmi Prasad, Aravind
Joshi, and Bonnie Webber. 2005. Experiments on
sense annotations and sense disambiguation of dis-
course connectives. In Proceedings of the TLT 2005
(4th Workshop on Treebanks and Linguistic Theories),
Barcelona, Spain.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of IWPT
2008 (8th International Workshop on Parsing Tech-
nologies), pages 149?160, Tokyo, Japan.
Marie-Paule Pe?ry-Woodley, Nicholas Asher, Patrice
Enjalbert, Farah Benamara, Myriam Bras, Ce?cile
Fabre, Ste?phane Ferrari, Lydia-Mai Ho-Dac, Anne
Le Draoulec, Yann Mathet, Philippe Muller, Laurent
Pre?vot, Josette Rebeyrolle, Ludovic Tanguy, Marianne
Vergez-Couret, Laure Vieu, and Antoine Widlo?cher.
2009. Annodis: une approche outille?e de l?annotation
de structures discursives. In Proceedings of TALN
2009 (16e`me Confe?rence sur le Traitement Automa-
tique des Langues Naturelles), Paris, France.
Volha Petukhova and Harry Bunt. 2009. Towards a
multidimensional semantics of discourse markers in
spoken dialogue. In Proceedings of IWCS-8 (8th In-
ternational Conference on Computational Semantics),
pages 157?168, Tilburg, The Netherlands.
Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text. In
Proceedings of ACL-IJCNLP 2009 (47th Annual Meet-
ing of the ACL and 4th International Joint Conference
on NLP of the AFNLP), Short Papers, pages 13?16,
Singapore.
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind Joshi. 2008. Eas-
ily identifiable discourse relations. In Proceedings of
Coling 2008 (22nd International Conference on Com-
putational Linguistics), Companion Volume: Posters,
pages 87?90, Manchester, UK.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0. In
Proceedings of LREC 2008 (6th International Confer-
ence on Language Resources and Evaluation), pages
2961?2968, Marrakech, Morocco.
Charlotte Roze, Laurence Danlos, and Phillippe Muller.
2010. LEXCONN: a French lexicon of discourse con-
nectives. In Proceedings of MAD 2010 (Multidis-
202
ciplinary Approaches to Discourse), pages 114?125,
Moissac, France.
Manfred Stede and Carla Umbach. 1998. DiMLex: a
lexicon of discourse markers for text generation and
understanding. In Proceedings of ACL 1998 (36th An-
nual Meeting of the ACL), pages 1238?1242, Mon-
treal, Canada.
Ben Wellner, James Pustejovsky, Catherine Havasi,
Roser Sauri, and Anna Rumshisky. 2006. Classifica-
tion of discourse coherence relations: An exploratory
study using multiple knowledge sources. In Proceed-
ings of 7th SIGDIAL Workshop on Discourse and Di-
alogue, pages 117?125, Sydney, Australia.
Sa?rka Zika?nova?, Lucie Mladova?, Jir??? M??rovsky?, and
Pavlina J??nova?. 2010. Typical cases of annotators?
disagreement in discourse annotations in Prague De-
pendency Treebank. In Proceedings of LREC 2010
(7th International Conference on Language Resources
and Evaluation), pages 2002?2006, Valletta, Malta.
203
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 129?138,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Using Sense-labeled Discourse Connectives
for Statistical Machine Translation
Thomas Meyer and Andrei Popescu-Belis
Idiap Research Institute
Rue Marconi 19, 1920 Martigny, Switzerland
{thomas.meyer, andrei.popescu-belis}@idiap.ch
Abstract
This article shows how the automatic dis-
ambiguation of discourse connectives can
improve Statistical Machine Translation
(SMT) from English to French. Connec-
tives are firstly disambiguated in terms of
the discourse relation they signal between
segments. Several classifiers trained using
syntactic and semantic features reach state-
of-the-art performance, with F1 scores of
0.6 to 0.8 over thirteen ambiguous English
connectives. Labeled connectives are then
used into SMT systems either by mod-
ifying their phrase table, or by training
them on labeled corpora. The best modi-
fied SMT systems improve the translation
of connectives without degrading BLEU
scores. A threshold-based SMT system us-
ing only high-confidence labels improves
BLEU scores by 0.2?0.4 points.
1 Introduction
Current approaches to Statistical Machine Trans-
lation (SMT) have difficulties in modeling long-
range dependencies between words, including
those that are due to discourse-level phenomena.
Among these, discourse connectives are words
that signal rhetorical relations between clauses or
sentences. Their translation often depends on the
exact relation signaled in context, a feature that
current SMT systems were not designed to cap-
ture, hence their frequent mistranslations of con-
nectives (see Section 2 below).
In this paper, we present a series of experiments
that aim to use, in SMT systems, data with au-
tomatically labeled discourse connectives. Sec-
tion 3 first presents the data sets used in our ex-
periments. We designed classifiers that attempt to
assign sense labels to ambiguous discourse con-
nectives, and their scores compare favorably with
the state-of-the-art for this task, as shown in Sec-
tion 4. In particular, we consider WordNet rela-
tions and temporal expressions as well as candi-
date translations of connectives as additional fea-
tures (Section 4.2).
However, our main goal is not the disambigua-
tion of connectives per se, but the use of the labels
assigned to connectives as additional input to an
SMT system. To the best of our knowledge, our
experiments are the first attempts to combine con-
nective disambiguation and SMT. Three solutions
to this combination are compared in Section 5:
modifying phrase tables, and training on data la-
beled manually, or automatically, with senses of
connectives. We further show that a modified
SMT system is best used when the confidence for
a given label is high (Section 6). The paper con-
cludes with a comparison to related work (Sec-
tion 7) and an outline of future work (Section 8).
2 Discourse Connectives in Translation
Discourse connectives such as although, however,
since or while form a functional category of
lexical items that are frequently used to mark
coherence or discourse relations such as expla-
nation, synchrony or contrast between units of
text or discourse. For example, in the Europarl
corpus from years 199x (Koehn, 2005), the
following nine lexical items, which are often
(though not always) discourse connectives, are
among the 400 most frequent tokens over a
total of 12,846,003 (in parentheses, rank and
number of occurrences): after (244th/6485),
although (375th/4062), however (110th/12,857),
indeed (334th/4486), rather (316th/4688),
129
since (190th/8263), still (168th/9195), while
(390th/3938), yet (331st/4532) ? see also (Car-
toni et al, 2011). Discourse connectives can
be difficult to translate, because many of them
can signal different relations between clauses in
different contexts. Moreover, if a wrong connec-
tive is used in translation, then a text becomes
incoherent, as in the two examples below, taken
from Europarl and translated (EN/FR) with
Moses (Koehn et al, 2007) trained on the entire
corpus:
1. EN: This tax, though [contrast], does not come
without its problems.
FR-SMT: *Cette taxe, me?me si [concession],
ne se pre?sente pas sans ses proble`mes.
2. EN: Finally, and in conclusion, Mr President,
with the expiry of the ECSC Treaty, the
regulations will have to be reviewed since
[causal] I think that the aid system will have
to continue beyond 2002 . . .
FR-SMT: *Enfin, et en conclusion, Monsieur
le pre?sident, a` l?expiration du traite? CECA,
la re?glementation devra e?tre revu depuis que
[temporal] je pense que le syste`me d?aides
devront continuer au-dela` de 2002 . . .
In the first example, the connective generated
by SMT (me?me si, literally ?even if?) signals a
concession and not a contrast, for which the con-
nective mais should have been used (as in the ref-
erence). In the second example, the connective
depuis que (literally ?from the time?) generated
by SMT expresses a temporal relation and not a
causal one, which should have been conveyed e.g.
by the French car.
Such examples suggest that the disambiguation
of connectives prior to translation could help SMT
systems to generate a correct connective in the tar-
get language. Of course, depending on the lan-
guage pair, some ambiguities can be carried over
from the source to the target language, so they
need not be solved. Still, improving the over-
all translation of discourse connectives should in-
crease the overall coherence of MT output, with a
potential large impact on perceived quality.
3 Data Used in Our Experiments
For both tasks, the disambiguation of connectives
and SMT, different training and testing data sets
are available. This section shows how we made
use of these resources and how we augmented
them by manual and automated annotation of the
senses of discourse connectives.
3.1 Data for the Disambiguation of
Discourse Connectives
One of the most important resources for discourse
connectives in English is the Penn Discourse
Treebank (Prasad et al, 2008). The PDTB pro-
vides a discourse-layer annotation over the Wall
Street Journal Corpus (WSJ) and the Penn Tree-
bank syntactic annotation. The discourse anno-
tation consists of manually annotated senses for
about 100 types of explicit connectives, for im-
plicit ones, and their clause spans. For the en-
tire WSJ corpus of about 1,000,000 tokens there
are 18,459 instances of annotated explicit connec-
tives. The senses that discourse connectives can
signal are organized in a hierarchy with 4 toplevel
senses, followed by 16 subtypes on the second
level and 23 detailed subsenses on the third level.
Studies making use of the PDTB to build classi-
fiers usually split the WSJ corpus into Sections
02?21 for training and Section 23 for testing (as
we did for our disambiguation experiments, see
Section 4).
From the PDTB, we extracted the 13 most fre-
quent and most ambiguous connectives: after, al-
though, however, indeed, meanwhile, neverthe-
less, nonetheless, rather, since, still, then, while,
and yet. This set shows in particular that connec-
tives signaling contrastive or temporal senses are
the most ambiguous ones, hence they are also po-
tentially difficult to translate, as this ambiguity is
often not preserved across languages (Danlos and
Roze, 2011). We used the senses from the sec-
ond PDTB hierarchy level (as the third level is too
fine-grained for EN/FR translation) and generated
the training and testing sets listed with statistics in
Table 1 (Section 4).
In principle, classifiers trained on PDTB data
can be applied directly to label connectives over
the English side of the Europarl corpus (Koehn,
2005) used for training and testing SMT. How-
ever, to control the difference in register from
newswire texts to formal political speech, and to
allow for future studies of other languages, we
also performed manual annotation (Cartoni et al,
2011) of five connectives over the Europarl corpus
(although, even though, since, though and while).
130
The manual annotation was performed on sub-
sets of Europarl v5 (years 199x) for the first few
hundred occurrences of each connective. Instead
of a potentially difficult and costly annotation of
senses, as in the PDTB, we performed translation
spotting, asking annotators to highlight the trans-
lation of each of the five connectives in the French
side of the corpus. From the list of all observed
translations one can then cluster the necessary
sense labels, as some target language connectives
clearly signal only one sense or, in cases where
ambiguity is preserved, one can group the equally
ambiguous connectives under one composite la-
bel. For example, while is sometimes translated
to the French discourse connectives tandis que or
alors que which both preserve the ambiguity of
while signaling a temporal or contrastive sense.
With this method we built the data sets listed with
statistics in Table 2 below (Section 4).
3.2 Data for Statistical Machine Translation
The translation data for our SMT experiments has
been often used in other MT research work and is
freely distributed for the shared tasks of the Work-
shop on Machine Translation (WMT)1.
For training our SMT systems, the EN/FR Eu-
roparl corpus v5 was used in three ways to inte-
grate data with labeled discourse connectives into
SMT: no changes (for MT phrase table modifica-
tions), integration of manually annotated data and
integration of automatically labeled data. These
methods are described below in Section 5 ? here,
we gather descriptions of the corresponding data.
a: Modification of the phrase table: Europarl
(346,803 sentences), labeling the translation
model after training.
b: Integration of manual annotation: Europarl
(346,803 sentences), minus all 8,901 sen-
tences containing one of the above 5 connec-
tive types, plus 1,147 sentences with manu-
ally sense-labeled connectives.
c: Integration of automated annotation: Europarl
? years 199x (58,673 sentences), all occur-
rences of the 13 PDTB subset connective
types have been labeled by classifiers (in
6,961 sentences).
For Minimum Error Rate tuning (MERT) (Och,
2003) of the SMT systems, we used the 2009
1statmt.org/wmt10/translation-task.html
News Commentary (NC) EN/FR development set
with the following modifications:
d: Phrase table: NC 2009 (2,051 sentences), no
modifications.
e: Manual annotation: NC 2009 (2,051 sen-
tences), minus all 123 sentences containing
one of the above 5 connective types, plus 102
sentences with manually sense-labeled con-
nectives.
f: Automated annotation: NC 2009 (2,051 sen-
tences), all occurrences of the 13 PDTB sub-
set connective types have been labeled by
classifiers (in 340 sentences).
For testing our modified SMT systems, three
test sets were extracted in the following way:
g: 35 sentences from NC 2007, with 7 occur-
rences for each of the 5 connective types
above, manually labeled.
h: 62 sentences from NC 2007 and 2006 with oc-
currences for the 13 PDTB connective types,
automatically labeled with classifiers.
i: 10,311 sentences from the EN/FR UN corpus,
all occurrences of the five Europarl connec-
tive types, automatically labeled with classi-
fiers.
These test sets might appear small compared to
the amount of data normally used for SMT system
testing. In our system evaluation however, apart
from automated scoring, we also had to perform
manual counts of improved translations, which is
why we could not evaluate more than a hundred
sentences (Section 5). When counting manually
for test set (i), it was downsampled to the same
amount of 35 and 62 sentences as for sets (g)
and (h), by extracting the first occurrences of each
connective.
In all experiments, we use the Moses Phrase-
based SMT decoder (Koehn et al, 2007) and a 5-
gram language model built over the entire French
part of the Europarl corpus v5.
4 Automatically Disambiguating
Discourse Connectives
4.1 Classifier PT: Trained on PDTB Data
A first classifier (?PT?) for ambiguous discourse
connectives and their senses was built by using
the PDTB subset of 13 ambiguous connectives as
training material. For each connective we built a
131
Connective Number of occurrences and senses F1 Scores
Training set: total and per sense Test set: total and per sense PT PT+
after 507 456 As, 51 As/Ca 25 22 As, 3 As/Ca 0.66 1.00
although 267 135 Cs, 118 Ct, 14 Cp 16 9 Ct, 7 Cs 0.60 0.66
however 176 121 Ct, 32 Cs, 23 Cp 14 13 Ct, 1 Cs 0.33 1.00
indeed 69 37 Cd, 24 R, 3 Ca, 3 E, 2 I *2 2 R *0.50 *0.50
meanwhile 117 66 Cj/S, 16 Cd, 16 S, 14
Ct/S, 5 Ct
10 5 S, 5 Ct/S 0.32 0.53
nevertheless 26 15 Ct, 11 Cs 6 4 Cs, 2 Ct 0.44 0.66
nonetheless 12 7 Cs, 3 Ct, 2 Cp *1 1 Cs *1.00 *1.00
rather 10 6 R, 2 Al, 1 Ca, 1 Ct *1 1 Al *0.00 *0.00
since 166 75 As, 83 Ca, 8 As/Ca 9 4 As, 3 Ca, 2
As/Ca
0.78 0.78
still 114 56 Cs, 51 Ct, 7 Cp 13 9 Ct, 4 Cs 0.60 0.66
then 145 136 As, 6 Cd, 3 As/Ca 6 5 As, 1 Cd 0.83 1.00
while 631 317 Ct, 140 S, 79 Cs, 41
Ct/S, 36 Cd, 18 Cp
37 19 Ct, 10 S, 4 Cs,
4 Ct/S
0.93 0.96
yet 80 46 Ct, 25 Cs, 9 Cp *2 2 Ct *0.5 *1.00
Total 2,320 ? 142 ? 0.57 0.75
Table 1: Performance of MaxEnt connective sense classifiers: Classifier PT (initial feature set) and Classifier
PT+ (with candidate translation features) for 13 temporal and contrastive connectives in the PDTB. The sense
labels are coded as follows. Al: alternative, As: asynchronous, Ca: cause, Cd: condition, Cj: conjunction, Cp:
comparison, Cs: concession, Ct: contrast, E: expansion, I: instantiation, R: restatement, S: synchrony. In some
cases marked with ?*?, the test sets are too small to provide meaningful scores.
specialized classifier, by using the Stanford Max-
imum Entropy classifier package (Manning and
Klein, 2003). Maximum Entropy is known to han-
dle discrete features well and has been applied
successfully to connective disambiguation before
(see Section 7).
An initial set of features can directly be ob-
tained from the PDTB (and must hence be con-
sidered as oracle features): the (capitalized) con-
nective token, its POS tag, first word of clause 1,
last word of clause 1, first word of clause 2 (the
one containing the explicit connective), last word
of clause 2, POS tag of the first word of clause 2,
type of first word of clause 2, parent syntactical
categories of the connective, punctuation pattern
of the sentences. Apart from these standard fea-
tures in discourse connective disambiguation we
used WordNet (Miller, 1995) to compute lexical
similarity scores with the lesk metric (Baner-
jee and Pedersen, 2002) for all the possible com-
binations of nouns, verbs and adjectives in the
two clauses, as well as antonyms found for these
word groups. In addition, we used features that
are likely to help detecting temporal relations and
were obtained from the Tarsqi Toolkit (Verhagen
and Pustejovsky, 2008), which annotates English
sentences automatically with the TimeML anno-
tation language for temporal expressions. For ex-
ample, in the sentence The crimes may appear
small, but the prices can be huge (PDTB Sec-
tion 2, WSJ file 0290), for example, our features
would indicate the antonyms small vs. huge that
signal the contrast, along with a temporal order-
ing of the event appear before the event can.
We report the classifier performances as F1
scores for each connective (weighting precision
and recall equally) in Table 1, testing on Section
23 of the PDTB. This sense classifier will be re-
ferred to as Classifier PT in the rest of the paper,
in particular when used for the SMT experiments.
4.2 Classifier PT+: With Candidate
Translations as Features
In an attempt to improve Classifier PT, we added
a new type of feature, resulting in Classifier PT+.
Namely, we used candidate translations of dis-
course connectives from a baseline SMT system
(not adapted to connectives). To find these values,
a Moses baseline decoder was used to translate the
PDTB data, which was then word-aligned (En-
132
Connective Number of occurrences and senses F1
Size of training set: total and per sense Test set: total and per sense Score
although 173 155 Cs, 18 Ct 10 5 Cs, 5 Ct 0.67
even though 179 165 Cs, 14 Ct 10 5 Cs, 5 Ct 1.00
since 413 274 S, 131 Ca, 8 S/Ca 10 5 Ca, 3 S, 2 S/Ca 0.80
though 150 80 Cs, 70 Ct 10 5 Cs, 5 Ct 1.00
while 280 130 Cs, 41 Ct, 89 S/Ct, 13 S/Ca, 7 S 14 4 Cs, 2 Ct, 2 S/Ct, 2
S/Ca, 4 S
0.64
Total 1,195 ? 54 ? 0.82
Table 2: Performance of a MaxEnt connective sense classifier (Classifier EU) for 5 connectives in the Europarl
corpus. The sense labels are coded as follows. Cs: Concession, Ct: Contrast, S: Synchrony, Ca: Cause.
glish source with target French) by using GIZA++
(Och and Ney, 2003). In this alignment, we
searched for the translation equivalents of the 13
PDTB connectives by using a hand-crafted dic-
tionary of possible French translations. When the
translation candidate is not ambiguous ? e.g. bien
que as a translation for while clearly signals a con-
cession ? its specific sense label was added as the
value of an additional feature. In some cases,
however, the values of the features are not de-
termined (and are set to NONE): either when the
SMT system or GIZA++ failed in translating or
aligning a connective, or when the target connec-
tive was just as ambiguous as the source one (e.g.
while translated as tandis que, which can be la-
beled both temporal or contrast). Overall, this
procedure led to an accuracy gain of Classifier
PT+ with respect to Classifier PT of about 0.1 to
0.6 F1 score for some of the connectives, as can
be seen in the last column of Table 1.
4.3 Classifier EU: Trained on Europarl Data
As explained in Section 3.1, we performed man-
ual annotation of connective senses in Europarl
as well, to provide labeled instances directly in
the data used for SMT training and to account for
the register change. For the Europarl data sets,
we built a new MaxEnt classifier (called Classi-
fier EU) using the same feature set as Classifier
PT. However, all features were this time extracted
automatically (no oracle). In particular, we used
Charniak and Johnson?s (2005) parser to then ex-
tract the syntactic features. In Table 2, we re-
port the results of Classifier EU, again in terms
of F1 scores. For all three classifiers, PT, PT+
and EU, the F1 scores are in a range of 0.6 and
0.8, thus comparing favorably to the state-of-the-
art for discourse connective disambiguation with
detailed senses (Section 7). Classifier EU also
compares favorably to PT and PT+, as seen for in-
stance for since (0.80 vs. 0.78) or although (0.67
vs. 0.60?0.66).
5 Use of Labeled Connectives for SMT
In this section, we report on experiments that
study the effect of discourse connective labeling
on SMT. The experiments differ with respect to
the method used for taking advantage of the la-
bels, but also with respect to the data sets and the
sense classifiers that are used.
5.1 Evaluation Metrics for MT
The variation in MT quality can be estimated in
several ways. On the one hand, we use the BLEU
metric (Papineni et al, 2002) with one reference
translation as is most often done in current SMT
research2. To improve confidence in the BLEU
scores, especially when test sets are small, we
also compute BLEU scores using bootstrapping
of data sets (Zhang and Vogel, 2010); the test
sets are re-sampled a thousand times and the av-
erage BLEU score is computed from individual
sample scores. The BLEU approach is not likely,
however, to be sensitive enough to the small dif-
ferences due to the correction of discourse con-
nectives (less than one word per sentence). We
therefore additionally resort to a manual evalua-
tion metric, referred to as ?Connectives, which
counts the occurrences of connectives that are bet-
ter translated by our modified systems compared
to the baseline ones.
2The scores are generated by the NIST MTeval script
version 11b, available from www.itl.nist.gov/iad/
mig/tools/.
133
MT system N. Connectives in MT test data ?Conn. (%) BLEU scores
Occ. Types Labeling + = ? Standard Bootstrap
Modified phrase table 1 35 5 manual 29 51 20 39.92 40.54
2 10,311 5 Cl. EU 34 46 20 22.13 23.63
Trained on manual 3 35 5 manual 32 57 11 41.58 42.38
annotations 4 10,311 5 Cl. EU 26 66 8 22.43 24.00
Trained on automatic 5 62 13 Cl. PT 16 60 24 14.88 15.96
annotations (Cl. PT) 6 10,311 5 Cl. EU 16 66 18 19.78 21.17
Trained on automatic 7 62 13 Cl. PT+ 11 70 19 15.67 16.73
annotations (Cl. PT+) 8 10,311 5 Cl. EU 18 68 14 20.14 21.55
Table 3: MT systems dealing with manually and automatically (PT, PT+, EU) sense-labeled connectives: BLEU
scores (including bootstrapped ones) and variation in the translation of individual connectives (?Connectives,
as a percentage). The description of each condition and the baseline BLEU scores are in the text of the article.
5.2 Phrase Table Modification
A first way of using labeled connectives is to
modify the phrase table of an SMT system previ-
ously trained/tuned on data sets (a)/(d) from Sec-
tion 3.2, in order to force it to translate each spe-
cific sense of a discourse connective (as indicated
by its label) with an acceptable equivalent se-
lected among those learned from the training data.
Of course, this only handles cases when connec-
tives are translated by explicit lexical items (typ-
ically, target connectives) and not by more com-
plex grammatical constructs.
The phrase table modification is done as fol-
lows. Based on a small dictionary of the five con-
nective types of Table 2, their acceptable French
equivalents and the possible senses, the initial
phrase table is searched for phrases containing a
connective and each occurrence is inspected to
find out which sense is reflected in the transla-
tion. If the sense is non-ambiguous, then the ta-
ble entry is modified to include the label, and the
probability score is set to 1 in order to maximize
the chance that the respective translation is found
during decoding. For instance, for every phrase
table entry where while is translated as alors que,
this corresponds to a contrastive use and while is
changed into while CONTRAST. Or, for the en-
tries where while is translated as bien que, the
lexical entry is changed into while CONCESSION.
However, when the source entry is as ambiguous
as the target one, no modification is made. This
means that during decoding (testing) with labeled
sentences, these entries will never be used.
The results of the SMT system are shown in
experiments 1 and 2 in Table 3, respectively test-
ing over data set (g) (7 manually annotated sen-
tences for each of the 5 connectives) and over
set (i), in which the 5 connectives were automat-
ically labeled with Classifier EU. In the first test,
the translations of 29% of the connectives are im-
proved by the modified system, while 20% are
degraded and 51% remain unchanged ? thus re-
flecting an overall 10% improvement in the trans-
lations of connectives (?Connectives). How-
ever, for this test set, the BLEU score is about 3
points below the baseline SMT system that used
the same phrase table without modification of la-
bels and scores (not shown in Table 3). In exper-
iment 2, however, the BLEU score of the modi-
fied system is in the same range as the baseline
one (22.13 vs. 22.76). As for ?Connectives,
as it was not possible to score manually all the
10,311 connectives, we sampled 35 sentences and
found that 34% of the connectives are improved,
20% are degraded and 46% remain unchanged,
again reflecting an improvement in the translation
of connectives. This shows that piping automatic
labeling and SMT with a modified phrase table
does not degrade the overall BLEU score, while
increasing ?Connectives.
5.3 Training on Tagged Corpora
We explored a more principled way to integrate
external labels into SMT, by using labeled data
(manually or automatically) for training, so that
the system directly learns a modified phrase table
which allows the translation of labeled data (auto-
matically) when testing.
134
5.3.1 Manual Gold Annotation
We report first two experiments using the man-
ual gold annotation for the five connective types
over Europarl excerpts, used for training. When
used also for testing (experiment 3 in Table 3),
this can be seen as an oracle experiment, measur-
ing the translation improvement when connective
sense labeling is perfect. However, in experiment
4, the SMT system uses the output of an auto-
matic labeler. For training/tuning we used data
sets (b)/(e), Section 3.2.
In experiment 3, for test set (g), 32% of the
connectives were translated better by the modi-
fied system, 57% remained the same, and 11%
were degraded. In experiment 4, over a 35 sen-
tence sample of the bigger test set (i), 26% were
improved, 66% remained the same, and only
8% were degraded. The baseline SMT system
(not shown in Table 3) was built with the same
amounts of unlabeled training and tuning data.
Overall, the BLEU scores of our modified systems
are similar to the baseline ones, though still lower
? 41.58 vs. 42.77 for experiment 3, and 22.43
vs. 22.76 for experiment 4, also confirmed by the
bootstrapped scores.
Another comparison shows that the system
trained on manual annotations (exp. 4) outper-
forms the system using a modified phrase ta-
ble (exp. 2) in terms of BLEU scores (22.43 vs.
22.13) and bootstrapped ones (24.00 vs. 23.63).
5.3.2 Automated Annotation
We evaluated an SMT system trained on data
that was automatically labeled using the classi-
fiers in Section 4. This method provides a large
amount of imperfect training data, and uses no
manual annotations at all, except for the initial
training of the classifiers. For these experiments
(5 and 6 in Table 3), the BLEU scores as well
as the manual counts of improved connectives are
lower than in the preceding experiments because,
overall, less training/tuning data was used ? about
15% of Europarl, data sets (c) and (f) in Sec-
tion 3.2. The baseline system was built over the
same amount of data, with no labels.
Testing here was performed over the slightly
bigger test set (h) with 62 sentences (13 connec-
tive types). The occurrences were tagged with
Classifier PT prior to translation (exp. 5). Com-
pared to the baseline system, the translations of
16% of the connectives were improved, while
60% remained the same and 24% were degraded.
In experiment 6, the 10,311 UN occurrences for 5
connective types were first tagged with Classifier
EU. Evaluated on a sample of 62 sentences, 16%
of the connectives were improved, while 66% re-
mained the same and 18% were degraded. De-
spite less training data, in terms of BLEU, the dif-
ference to the respective baseline system (scores
not shown in Table 3) is similar in both experi-
mental settings: 19.78 vs. 20.11 for experiment
6 (automated annotation), compared to 22.43 vs.
22.76 for experiment 4 (manual annotation).
Finally, we carried out two experiments (7
and 8) with Classifier PT+, which uses as addi-
tional features the translation candidates and has a
higher accuracy than PT (Section 4.2). As a result,
the translation of connectives (?Connectives) is
indeed improved compared (respectively) to ex-
periments 5 and 6, as it appears from lines 7?8
of Table 3. Also, the BLEU scores of the corre-
sponding SMT systems are increased in experi-
ments 7 vs. 5 and in 8 vs. 6, and are now equal
to the baseline ones (for experiment 8: 20.14 vs.
20.11, or, bootstrapped, 21.55 vs. 21.55).
The results of experiments 7/8 vs. 5/6 in-
dicate that improved classifiers for connec-
tives also improve SMT output as measured by
?Connectives, with BLEU remaining fairly
constant, and therefore are worth investigating
in more depth in the future. When compar-
ing manual (experiments 3/4) vs. automated an-
notation (experiments 5/6/7/8) and their use in
SMT, the differences in the scores (BLEU and
?Connectives) highlight a trade-off: manually
annotated data used for training leads to better
scores, but noisier and larger training data that is
annotated automatically is an acceptable solution
when manual annotations are not available.
6 Classifier Confidence Scores
As shown with the above experiments, the accu-
racy of the connective classifiers influences SMT
quality. We therefore hypothesize that an SMT
system dealing with labeled connectives would
best be used when the confidence of the classi-
fier is high, while a generic SMT system could be
used for lower confidence values.
We experimented with the confidence scores of
Classifier EU, which assigns a score between 0
and 1 to each of its decisions on the connectives?
labels. (All processing is automatic in these ex-
135
(a) although (b) since
Figure 1: Use of a combined system (COMB) that directs the input sentences either to a system trained on a sense-
labeled corpus (TTC) or to a baseline one (BASE), depending on the confidence of the connective classifier. The
x-axis shows the threshold above which TTC is used ? BASE being used below it ? and the y-axis shows the
BLEU scores of COMB with respect to TTC and BASE. Figure (a) is for although and (b) for since.
periments, and the evaluation is done solely in
terms of BLEU). We defined a threshold-based
procedure to combine SMT systems: if the con-
fidence for a sense label is above a certain thresh-
old, then the sentence is translated by an SMT
system trained on labeled data from experiment
4 (or ?tagged corpus?, hence noted TTC), and if it
is below the threshold, it is sent to a baseline sys-
tem (noted BASE). The resulting BLEU scores of
the combined system (COMB) obtained for vari-
ous threshold values are shown in Figure 1 for two
connectives.
Firstly, we considered all the 1,572 sentences
from the UN corpus which contained the connec-
tive although, labeled either as contrast or con-
cession. We show BLEU scores of the COMB
system for several thresholds in the interval of ob-
served confidence scores, along with the scores of
BASE and TTC, in Figure 1(a). The results show
that the scores of COMB increase with the value
of the threshold, and that for at least one value
of the threshold (0.95) COMB outperforms both
TTC and BASE by 0.20 BLEU points.
To confirm this finding with another connec-
tive, we took the first 1,572 sentences containing
the connective since from the UN corpus. The
BLEU scores for COMB are shown for the range
of observed confidence values (0.4?1.0) in Fig-
ure 1(b). For several values of the threshold,
COMB outperforms both BASE and TTC, in par-
ticular for 0.85, with a difference of 0.39 BLEU
points.
The significance of the observed improvement
was tested as follows. For each of the two con-
nectives, we split the test sets of 1,572 sentences
each in five folds, and compared for each fold the
scores of COMB for the best performing thresh-
old (0.95 or 0.85) with the highest of BASE or
TTC (i.e. BASE for although and TTC for since).
We performed a paired t-test to compute the sig-
nificance of the difference, and found p = 0.12 for
although. This value, although slightly above the
conventional boundary of 0.1, shows that the five
pairs of scores reflect a significant difference in
quality. Similarly, when performing a t-test for
since, the difference in scores is found significant
at the 0.01 level (p = 0.005). Of course, COMB
is always significantly better than the lower of
BASE or TTC (p < 0.05). In the future, the sys-
tem combination will be tested for all connectives,
and the respective values of the thresholds will be
set on tuning, not on test data.
7 Related Work
Discourse parsing (Marcu, 2000) has proven to
be a difficult task, even when complex models
(CRFs, SVMs) are used (Wellner, 2009; Her-
nault et al, 2010). The performance of discourse
parsers is in a range of 0.4 to 0.6 F1 score.
136
With the release of the PDTB, recent research
focused on the disambiguation of discourse con-
nectives as a task in its own right. For the disam-
biguation of explicit connectives, the state-of-the-
art performance for labeling all types of connec-
tives in English is quite high. In the PDTB data,
the disambiguation of discourse vs. non-discourse
uses of connectives reaches 97% accuracy (Lin et
al., 2010). The labeling of the four main senses
from the PDTB sense hierarchy (temporal, contin-
gency, comparison, expansion) reaches 94% ac-
curacy (Pitler and Nenkova, 2009) ? however, the
baseline accuracy is already around 85% when us-
ing only the connective token as a feature. Vari-
ous methods for classification and feature analy-
sis have been proposed (Wellner et al, 2006; El-
well and Baldridge, 2008). Other studies have
focused on the analysis of highly ambiguous dis-
course connectives only. Miltsakaki et al (2005)
report classification results for the connectives
since, while and when. Using a Maximum En-
tropy classifier, they reach 75.5% accuracy for
since, 71.8% for while and 61.6% for when. As
the PDTB was not completed at that time, the data
sets and labels are not exactly identical to the ones
that we used above (see Section 4).
The disambiguation of senses signaled by dis-
course connectives can be seen as a word sense
disambiguation (WSD) problem for functional
words (as opposed to WSD for content words,
which is more frequently studied). The integra-
tion of WSD into SMT has especially been stud-
ied by Carpuat and Wu (2007), who used the
translation candidates output by a baseline SMT
system as word sense labels. This is similar to
our use of translation candidates as an additional
feature for classification in Section 4.2. Then,
the output of several classifiers based on linguis-
tic features was weighed against the translation
candidates output by the baseline SMT system.
With this procedure, their WSD+SMT system im-
proved the BLEU scores by 0.4?0.5 for the En-
glish/Chinese pair.
Chang et al (2009) use a LogLinear classi-
fier with linguistic features in order to disam-
biguate the Chinese particle ?DE? that has five dif-
ferent context-dependent uses (modifier, preposi-
tion, relative clause etc.). When the classifier is
used to annotate the particle prior to SMT, the
output of the translation system improves by up
to 1.49 BLEU score for phrase-based Chinese to
English translation. Ma et al (2011) use a Maxi-
mum Entropy model to POS tag English colloca-
tional particles (e.g. come down/by, turn against,
inform of ) more specifically than a usual POS tag-
ger does (where only one label is given to all par-
ticles). The authors claim the usefulness of such
a particle tagger for English/Chinese translation,
but do not show its actual integration into an MT
system.
These approaches, as well as ours, show that
integrating discourse information into SMT is
promising and deserves future examination. The
disambiguation of word senses, including func-
tion words, can improve SMT output when the
senses are annotated in a pre-processing step that
uses classifiers based on linguistic features at
the semantic and discourse levels, which are not
available to a state-of-the-art SMT systems.
8 Conclusion and Future Work
This paper has presented methods and results for
the disambiguation of temporal and contrastive
discourse connectives using MaxEnt classifiers
with syntactic and semantic features, in English
texts, in terms of senses intended to help SMT.
These classifiers have been used to perform exper-
iments with connective-annotated data applied to
EN/FR SMT systems. The results have shown an
improvement in the translation of connectives for
fully automatic systems trained on either hand-
labeled or automatically-labeled data. Moreover,
BLEU scores were significantly improved by 0.2?
0.4 when such systems were only used for con-
nectives that had been disambiguated with high
confidence.
In future work we plan to improve the sense
classifiers using additional features, to improve
their integration with SMT, and to unify our data
sets through additional manual annotations over
Europarl. The applicability of the method to other
languages will also be demonstrated experimen-
tally.
Acknowledgments
We are grateful for the funding of this
work to the Swiss National Science Foun-
dation (SNSF), under the COMTIS Sin-
ergia Project n. CRSI22 127510 (see
www.idiap.ch/comtis/).
137
References
Satanjeev Banerjee and Ted Pedersen. 2002. An
Adapted Lesk Algorithm for Word Sense Disam-
biguation Using WordNet. In Alexander Gel-
bukh, editor, Computational Linguistics and Intelli-
gent Text Processing, LNCS 2276, pages 117?171.
Springer, Berlin/Heidelberg.
Marine Carpuat and Dekai Wu. 2007. Improving Sta-
tistical Machine Translation using Word Sense Dis-
ambiguation. Proc. of EMNLP-CoNLL, pages 61?
72, Prague.
Bruno Cartoni, Sandrine Zufferey, Thomas Meyer, and
Andrei Popescu-Belis. 2011. How Comparable
are Parallel Corpora? Measuring the Distribution of
General Vocabulary and Connectives. Proc. of the
4th Workshop on Building and Using Comparable
Corpora (BUCC), pages 78?86, Portland, OR.
Pi-Chuan Chang, Dan Jurafsky, and Christopher D.
Manning. 2009. Disambiguating ?DE? for Chinese-
English Machine Translation. Proc. of the Fourth
Workshop on Statistical Machine Translation at
EACL-2009, Athens.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best Parsing and MaxEnt Discriminative
Reranking. Proc. of the 43rd Annual Meeting of the
ACL, pages 173?180, Ann Arbor, MI.
Laurence Danlos and Charlotte Roze. 2011. Traduc-
tion (Automatique) des Connecteurs de Discours.
Actes 18e Confe?rence sur le Traitement Automa-
tique des Langues Naturelles (TALN), Montpellier.
Robert Elwell and Jason Baldridge. 2008. Discourse
Connective Argument Identification with Connec-
tive Specific Rankers. Proc. of the 2nd IEEE
International Conference on Semantic Computing
(ICSC), pages 198?205, Santa Clara, CA.
Hugo Hernault, Helmut Prendinger, David A. duVerle,
and Mitsuru Ishizuka. 2010. HILDA: A Discourse
Parser using Support Vector Machine classification.
Dialogue and Discourse, 3(1):1?33.
Philipp Koehn, et al 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. Proc.
of 45th Annual Meeting of the ACL, Demonstration
Session, pages 177?180, Prague.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. Proc. of MT Sum-
mit X, pages 79?86, Phuket.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
A PDTB-styled End-to-end Discourse Parser. Tech-
nical Report TRB8/10, School of Computing, Na-
tional University of Singapore.
Jianjun Ma, Degen Huang, Haixia Liu, and Wenfeng
Sheng. 2011. POS Tagging of English Particles
for Machine Translation. Proc. of MT Summit XIII,
pages 57?63, Xiamen.
Christopher Manning and Dan Klein. 2003. Opti-
mization, MaxEnt Models, and Conditional Estima-
tion without Magic. Tutorial at HLT-NAACL and
41st ACL conferences, Edmonton, AB and Sapporo.
Daniel Marcu. 2000. The Theory and Practice of
Discourse Parsing and Summarization. A Bradford
Book. The MIT Press, Cambridge, MA.
George A. Miller. 1995. WordNet: A Lexical
Database for English. Communications of the ACM,
38(11):39?41.
Eleni Miltsakaki, Nikhil Dinesh, Rashmi Prasad, Ar-
avind Joshi, and Bonnie Webber. 2005. Exper-
iments on Sense Annotations and Sense Disam-
biguation of Discourse Connectives. Proc. of the
4th Workshop on Treebanks and Linguistic Theories
(TLT), Barcelona.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. Proc. of the
41st Annual Meeting of the ACL, pages 160?167,
Sapporo.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: A method for Au-
tomatic Evaluation of Machine Translation. Proc.
of 40th Annual Meeting of the ACL, pages 311?318,
Philadelphia, PA.
Emily Pitler and Ani Nenkova. 2009. Using Syntax
to Disambiguate Explicit Discourse Connectives in
Text. Proc. of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference of the
AFNLP (ACL-IJCNLP), Short Papers, pages 13?16,
Singapore.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bon-
nie Webber. 2008. The Penn Discourse Treebank
2.0. Proc. of the 6th International Conference on
Language Resources and Evaluation (LREC), pages
2961?2968, Marrakech.
Marc Verhagen and James Pustejovsky. 2008. Tem-
poral Processing with the TARSQI Toolkit. Proc.
of the 22nd International Conference on Com-
putational Linguistics (COLING), Demonstrations,
pages 189?192, Manchester, UK.
Ben Wellner, James Pustejovsky, Catherine Havasi,
Roser Sauri, and Anna Rumshisky. 2006. Classi-
fication of Discourse Coherence Relations: An Ex-
ploratory Study using Multiple Knowledge Sources.
Proc. of the 7th SIGdial Meeting on Discourse and
Dialog, pages 117?125, Sydney.
Ben Wellner. 2009. Sequence Models and Ranking
Methods for Discourse Parsing. PhD thesis, Bran-
deis University, Waltham, MA.
Ying Zhang and Stefan Vogel. 2010. Significance
Tests of Automatic Machine Translation Evaluation
Metrics. Machine Translation, 24(1):51?65.
138
Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 19?26,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Implicitation of Discourse Connectives in (Machine) Translation
Thomas Meyer
Idiap Research Institute and EPFL
Martigny and Lausanne, Switzerland
thomas.meyer@idiap.ch
Bonnie Webber
University of Edinburgh
Edinburgh, UK
bonnie@inf.ed.ac.uk
Abstract
Explicit discourse connectives in a source
language text are not always translated to
comparable words or phrases in the tar-
get language. The paper provides a corpus
analysis and a method for semi-automatic
detection of such cases. Results show
that discourse connectives are not trans-
lated into comparable forms (or even any
form at all), in up to 18% of human refer-
ence translations from English to French
or German. In machine translation, this
happens much less frequently (up to 8%
only). Work in progress aims to cap-
ture this natural implicitation of discourse
connectives in current statistical machine
translation models.
1 Introduction
Discourse connectives (DCs), a class of frequent
cohesive markers, such as although, however, for
example, in addition, since, while, yet, etc., are es-
pecially prone to ?translationese?, i.e. the use of
constructions in the target language (TL) that dif-
fer in frequency or position from how they would
be found in texts born in the language. That is,
?translationese? makes DCs prone to being trans-
lated in ways that can differ markedly from their
use in the source language. (Blum-Kulka, 1986;
Cartoni et al, 2011; Ilisei et al, 2010; Halverson,
2004; Hansen-Schirra et al, 2007; Zufferey et al,
2012). For cohesive markers and DCs, Koppel and
Ordan (2011) and Cartoni et al (2011) have shown
that they may be more explicit (increased use) or
less explicit (decreased use) in translationese. The
paper focuses on the latter case, but the same de-
tection method can be applied in reverse, in order
to find increased use (explicitation) as well.
In English about 100 types of explicit DCs have
been annotated in the Penn Discourse TreeBank,
or PDTB (Prasad et al, 2008) (We say more about
this in Section 3.1). The actual set of markers or
connectives is however rather open-ended (Prasad
et al, 2010). DCs signal discourse relations that
connect two spans of text and can be ambiguous
with respect to the discourse relation they convey.
Moreover, the same DC can simultaneously con-
vey more than one discourse relation. For exam-
ple, while can convey contrast or temporality, or
both at the same time. On the other hand, dis-
course relations can also be conveyed implicitly,
without an explicit DC.
Human translators can chose to not translate a
SL DC with a TL DC, where the latter would be re-
dundant or where the SL discourse relation would
more naturally be conveyed in the TL by other
means (cf. Section 2). We will use the term ?zero-
translation? or ?implicitation? for a valid transla-
tion that conveys the same sense as a lexically ex-
plicit SL connective, but not with the same form.
As we will show, current SMT models either learn
the explicit lexicalization of a SL connective to
a TL connective, or treat the former as a ran-
dom variation, realizing it or not. Learning other
valid ways of conveying the same discourse rela-
tion might not only result in more fluent TL text,
but also help raise its BLEU score by more closely
resembling its more implicit human reference text.
The paper presents work in progress on a cor-
pus study where zero-translations of DCs have
been semi-automatically detected in human refer-
ence and machine translations from English (EN)
to French (FR) and German (DE) (Section 3).
Two types of discourse relations that are very fre-
quently omitted in FR and DE translations are
studied in detail and we outline features on how
these omissions could be modeled into current
SMT systems (Section 4).
19
2 Implicitation of connectives in
translation
Figure 1 is an extract from a news article in the
newstest2010 data set (see Section 3.2). It con-
tains two EN connectives ? as and otherwise ?
that were annotated in the PDTB1. Using the set of
discourse relations of the PDTB, as can be said to
signal the discourse relation CAUSE (subtype Rea-
son), and otherwise the discourse relation ALTER-
NATIVE. This is discussed further in Section 3.1.
EN: The man with the striking bald head was
still needing a chauffeur, 1. as the town was still
unknown to him. 2. Otherwise he could have
driven himself ? 3. after all, no alcohol was
involved and the 55-year-old was not drunk.
FR-REF: L?homme, dont le cra?ne chauve
attirait l?attention, se laissa conduire 1. 0
dans la ville qui lui e?tait encore e?trange`re. 2.
Autrement notre quinquage?naire aurait pu
prendre lui-me?me le volant ? 3. 0 il n?avait
pas bu d?alcool et il n?e?tait pas non plus ivre de
bonheur.
DE-REF: Der Mann mit der markanten
Glatze liess sich 1. wegen/Prep der ihm noch
fremden Stadt chauffieren. 2. Ansonsten ha?tte
er auch selbst fahren ko?nnen ? Alkohol war 3.
schliesslich/Adv nicht im Spiel, und besoffen
vor Glu?ck war der 55-ja?hrige genauso wenig.
Figure 1: Examples of EN source connectives
translated as zero or by other means in human ref-
erence translations.
The human reference translations do not trans-
late the first connective as explicitly. In FR there
is no direct equivalent, and the reason why the
man needed a driver is given with a relative clause:
...dans la ville qui... (lit.: in the town that was still
foreign to him). In DE as is realized by means of
a preposition, wegen (lit.: because of). The sec-
ond EN connective otherwise, maintains its form
in translation to the target connective autrement in
FR and ansonsten in DE.
On the other hand, baseline SMT systems for
1The excerpt contains a third possible connective after all
that was not annotated in the PDTB, and our data as a whole
contains other possible connectives not yet annotated there,
including given that and at the same time. We did not analyse
such possible connectives in the work described here.
EN/FR and EN/DE (Section 3.2) both translated
the two connectives as and otherwise explicitly by
the usual target connectives, in FR: comme, sinon
and in DE wie, sonst.
3 Semi-automatic detection of
zero-translations
3.1 Method
The semi-automatic method that identifies zero- or
non-connective translations in human references
and machine translation output is based on a list
of 48 EN DCs with a frequency above 20 in the
Penn Discourse TreeBank Version 2.0 (Prasad et
al., 2008). In order to identify which discourse re-
lations are most frequently translated as zero, we
have assigned each of the EN DCs the level-2 dis-
course relation that it is most frequently associated
with in the PDTB corpus. The total list of EN con-
nectives is given in Table 1.
For every source connective, we queried its
most frequent target connective translations from
the online dictionary Linguee2 and added them to
dictionaries of possible FR and DE equivalents.
With these dictionaries and Giza++ word align-
ment (Och and Ney, 2003), the SL connectives
can be located and the sentences of its transla-
tion (reference and/or automatic) can be scanned
for an aligned occurrence of the TL dictionary
entries. If more than one DC appears in the
source sentence and/or a DC is not aligned with a
connective or connective-equivalent found in the
dictionaries, the word position (word index) of
the SL connective is compared to the word in-
dexes of the translation in order to detect whether
a TL connective (or connective-equivalent from
the dictionaries) appears in a 5-word window to
its left and right.3. This also helps filtering out
cases of non-connective uses of e.g. separately
or once as adverbs. Finally, if no aligned entry
is present and the alignment information remains
empty, the method counts a zero-translation and
collects statistics on these occurrences.
After a first run where we only allowed for ac-
tual connectives as translation dictionary entries,
we manually looked through 400 cases for each,
FR and DE reference translations, that were output
2http://www.linguee.com
3The method extends on the ACT metric (Hajlaoui and
Popescu-Belis, 2013) that measures MT quality in terms of
connectives in order to detect more types of DCs and their
equivalents.
20
Figure 2: Percentage of zero-translations in newstest2010+2012 for EN/FR per discourse relation and
translation type: human reference (Ref) or MT output (MT).
as zero-translations (in the newtest2012 data, see
Section 3.2). We found up to 100 additional cases
that actually were not implicitations, but conveyed
the SL connective?s meaning by means of a para-
phrase, e.g. EN: if ? FR: dans le cas ou` (lit.: in
case where) ? DE: im Falle von (lit.: in case of).
For example, the EN connective otherwise ended
up with the dictionary entries in Figure 3.
EN: otherwise ALTERNATIVE :
FR: autrement|sinon|car|dans un autre
cas|d?une autre manie`re
DE: ansonsten|andernfalls|anderenfalls
|anderweitig|widrigenfalls|andrerseits|
andererseits|anders|sonst
Figure 3: Dictionary entries of FR and DE connec-
tives and equivalents for the EN connective other-
wise.
3.2 Data
For the experiments described here, we con-
catenated two data sets, the newstest2010 and
newstest2012 parallel texts as publicly available
by the Workshop on Machine Translation4. The
texts consist of complete articles from various
daily news papers that have been translated from
EN to FR, DE and other languages by translation
agencies.
In total, there are 5,492 sentences and 117,799
words in the SL texts, of which 2,906 are tokens
4http://www.statmt.org/wmt12/
of the 48 EN connectives. See Table 1 for the con-
nectives and their majority class, which aggregate
to the detailed statistics given in Table 2.
Rel. TC Rel. TC
Alternative 30 Conjunction 329
Asynchrony 588 Contrast 614
Cause 308 Instantiation 43
Concession 140 Restatement 14
Condition 159 Synchrony 681
Table 2: Total counts (TC) of English dis-
course connectives (2,906 tokens) from the
newstest2010+2012 corpora, whose majority
sense conveys one of the 10 PDTB level-2 dis-
course relations (Rel.) listed here.
To produce machine translations of the same
data sets we built EN/FR and EN/DE base-
line phrase-based SMT systems, by using the
Moses decoder (Koehn et al, 2007), with the Eu-
roparl corpus v7 (Koehn, 2005) as training and
newtest2011 as tuning data. The 3-gram language
model was built with IRSTLM (Federico et al,
2008) over Europarl and the rest of WMT?s news
data for FR and DE.
3.3 Results
In order to group the individual counts of zero-
translations per DC according to the discourse re-
lation they signal, we calculated the relative fre-
quency of zero-translations per relation as percent-
ages, see Figures 2 for EN/FR, and 4 for EN/DE.
21
Figure 4: Percentage of zero-translations in newstest2010+2012 for EN/DE per discourse relation and
translation type: human reference (Ref) or MT output (MT).
The total percentage of zero-translations in the ref-
erences and the baseline MT output is given in
Table 3.
A first observation is that an MT system seems
to produce zero-translations for DCs significantly
less often than human translators do. Human FR
translations seem to have a higher tendency to-
ward omitting connectives than the ones in DE.
Figures 2 and 4 also show that the discourse re-
lations that are most often rendered as zero are de-
pendent on the TL. In the FR reference transla-
tions, SYNCHRONY, ALTERNATIVE and CONCES-
SION account for most implicitations, while in the
DE reference translations, CONDITION, ALTER-
NATIVE and CONCESSION are most often left im-
plicit.
Translation Type C %
EN/FR Ref 508 17.5
MT 217 7.5
EN/DE Ref 392 13.5
MT 129 4.4
Table 3: Counts (C) and relative frequency (%)
of zero-translations for EN/FR and EN/DE in hu-
man references (Ref) and MT output (MT) over
newstest2010+2012.
The results are to some extent counterintuitive
as one would expect that semantically dense dis-
course relations like CONCESSION would need to
be explicit in translation in order to convey the
same meaning. Section 4 presents some non-
connective means available in the two TLs, by
which the discourse relations are still established.
We furthermore looked at the largest implicita-
tion differences per discourse relation in the hu-
man reference translations and the MT output. For
EN/FR for example, 13.8% of all CONDITION re-
lations are implicitated in the references, by mak-
ing use of paraphrases such as dans le moment
ou` (lit.: in the moment where) or dans votre cas
(lit.: in your case) in place of the EN connective
if. The MT system translates if in 99.4% of all
cases to the explicit FR connective si. Similarly,
for INSTANTIATION relations and the EN connec-
tive for instance in the references, the translators
made constrained use of verbal paraphrases such
as on y trouve (lit.: among which we find). MT on
the other hand outputs the explicit FR connective
par exemple in all cases of for instance.
For EN/DE, there is the extreme case, where
ALTERNATIVE relations are, in human reference
translations, quite often implicitated (in 23.3% of
all cases), whereas the MT system translates all
the instances explicitly to DE connectives: wenn
(unless), sonst (otherwise) and statt, stattdessen,
anstatt (instead). The translators however make
use of constructions with a sentence-initial verb in
conditional mood (cf. Section 4.2) for otherwise
and unless, but not for instead, which is, as with
MT, always explicitly translated by humans, most
often to the DE connective statt. The very op-
posite takes place for the RESTATEMENT relation
22
and the EN connective in fact. Here, MT leaves
implicit just as many instances as human transla-
tors do, i.e. 14.3% of all cases. Translators use
paraphrases such as in Wahrheit (lit.: in truth) or
u?brigens (lit.: by the way), while the translation
model tends to use im Gegenteil (lit.: opposite),
which is not a literal translation of in fact (usually
in der Tat or tatsa?chlich in DE), but reflects the
contrastive function this marker frequently had in
the Europarl training data of the baseline MT sys-
tem.
4 Case studies
4.1 Temporal connectives from EN to FR
The most frequent implicitated discourse relation
for EN/FR translation is SYNCHRONY, i.e. con-
nectives conveying that their arguments describe
events that take place at the same time. However,
since the situations in which SYNCHRONY rela-
tions are implicitated are similar to those in which
CONTRAST relations are implicitated, we discuss
the two together.
We exemplify here cases where EN DCs that
signal SYNCHRONY and/or CONTRAST are trans-
lated to FR with a ?en/Preposition + Verb in
Gerund? construction without a TL connective.
The EN source instances giving rise to such im-
plicitations in FR are usually of the form ?DC +
Verb in Present Continuous? or ?DC + Verb in Sim-
ple Past?, see sentences 1 and 2 in Figure 5.
Out of 13 cases of implicitations for while in the
data, 8 (61.5%) have been translated to the men-
tioned construction in FR, as illustrated in the first
example in Figure 5, with a reference and machine
translation from newstest2010. The DC while here
ambiguously signals SYNCHRONY and/or CON-
TRAST, but there is a second temporal marker
(at the same time, a connective-equivalent not yet
considered in this paper or in the PDTB), that dis-
ambiguates while to its CONTRAST sense only or
to the composite sense SYNCHRONY/CONTRAST.
The latter is conveyed in FR by en me?prisant, with
CONTRAST being reinforced by tout (lit.: all).
In Example 2, from newstest2012, the sentence-
initial connective when, again signaling SYN-
CHRONY, is translated to the very same construc-
tion of ?en/Preposition + Verb in Gerund? in the
FR reference.
In the baseline MT output for Example 1, nei-
ther of the two EN DCs is deleted, while is literally
translated to alors que and at the same time to dans
1. EN: In her view, the filmmaker ?is asking
a favour from the court, while at the same time
showing disregard for its authority?.
FR-REF: Pour elle, le cine?aste ?demande une
faveur a` la cour, tout en/Prep me?prisant/V/Ger
son autorite??.
FR-MT*: Dans son avis, le re?alisateur de
?demande une faveur de la cour, alors que dans
le me?me temps une marque de me?pris pour son
autorite??.
2. EN: When Meder looked through the
weather-beaten windows of the red, white and
yellow Art Nouveau building, she could see
weeds growing up through the tiles.
FR-REF: En/Prep jetant/V/Ger un coup
d??il par la fene?tre de l?immeuble-art nou-
veau en rouge-blanc-jaune, elle a observe?
l?e?panouissement des mauvaises herbes entre les
carreaux.
FR-MT*: Lorsque Meder semblait weather-
beaten a` travers les fene?tres du rouge, jaune et
blanc de l?art nouveau ba?timent, elle pourrait
voir les mauvaises herbes qui grandissent par les
tuiles.
Figure 5: Translation examples for the EN tempo-
ral connectives while and when, rendered in the FR
reference as a ?preposition + Verb in Gerund? con-
struction. MT generates the direct lexical equiva-
lents alors que and lorsque.
le me?me temps. While the MT output is not totally
wrong, it sounds disfluent, as dans le me?me temps
after alors que is neither necessary nor appropri-
ate.
In the baseline MT output for Example 2, the di-
rect lexical equivalent for when ? lorsque is gen-
erated, which is correct, although the translation
has other mistakes such as the wrong verb sem-
blait and the untranslated weather-beaten.
To model such cases for SMT one could use
POS tags to detect the ?DC + Present Continu-
ous/Simple Past? in EN and apply a rule to trans-
late it to ?Preposition + Gerund? in FR. Further-
more, when two DCs follow each other in EN,
and both can signal the same discourse relations,
a word-deletion feature (as it is available in the
Moses decoder via sparse features), could be used
to trigger the deletion of one of the EN connec-
tives, so that only one is translated to the TL. We
23
will examine in future work whether there are sys-
tematic patterns in the translation of such ?dou-
ble? connectives in SL and TL. Another possibility
would be to treat cases like while at the same time
as a multi-word phrase that is then translated to the
corresponding prepositional construction in FR.
4.2 Conditional connectives from EN to DE
Out of the 41 cases involving a CONDITION re-
lation (10.5% of all DE implicitations), 40 or
97.6% were due to the EN connective if not be-
ing translated to its DE equivalents wenn, falls,
ob. Instead, in 21 cases (52.5%), the human
reference translations made use of a verbal con-
struction which obviates the need for a connec-
tive in DE when the verb in the if -clause is
moved to sentence-initial position and its mood
is made conditional, as in Figure 6, a refer-
ence translation from newstest2012, with the DE
verb wa?re (lit.: were) (VMFIN=modal finite verb,
Konj=conditional). This construction is also avail-
able in EN (Were you here, I would...), but seems
to be much more formal and less frequent than in
DE where it is ordinarily used across registers. In
the baseline MT output for this sentence, if was
translated explicitly to the DE connective wenn,
which is in principle correct, but the syntax of the
translation is wrong, mainly due to the position of
the verb tun, which should be at the end of the sen-
tence.
The remaining 19 cases of EN if were either
translated to DE prepositions (e.g. bei, wo, lit.: at,
where) or the CONDITION relation is not expressed
at all and verbs in indicative mood make the use of
a conditional DE connective superfluous.
Of the 21 tokens of if whose reference transla-
tions used a verbal construction in DE, 14 (66.7%)
were tokens of if whose argument clause explic-
itly referred to the preceding context ? e.g., if they
were, if so, if this is true etc. These occurrences
could therefore be identified in EN and could be
modeled for SMT as re-ordering rules on the ver-
bal phrase in the DE syntax tree after constituent
parsing in syntax-based translation models.
5 Conclusion
This study showed that human translators do not
translate explicit EN discourse connectives as FR
or DE discourse connectives in up to 18% of all
cases. In MT output this happens about 3 times
less often. We thus plan to examine how to pro-
EN: If not for computer science, they would be
doing amazing things in other fields.
DE-REF: 0 Wa?re/VMFIN/Konj es
nicht die Computerbranche gewesen, wu?rden
sie in anderen Bereichen fantastische Dinge
schaffen.
DE-MT*: Wenn nicht fu?r die Informatik,
wu?rden sie tun, erstaunlich, Dinge auf anderen
Gebieten.
Figure 6: Translation example for the EN connec-
tive if, rendered in the DE reference as a construc-
tion with a sentence-initial verb in conditional
mood. MT generates the direct lexical equivalent
wenn.
duce higher-scoring translations without a target
language connective but with some other syntactic
pattern that conveys the same source language dis-
course relation. Depending on the features identi-
fied, movements of syntactical constituents or re-
ordering of POS tags at the phrase and/or sub-tree
level will be implemented for hierarchical syntac-
tic or phrase-based SMT models.
Acknowledgments
We are grateful to the Swiss National Science
Foundation (SNSF) for partially funding this work
and the research visit to Edinburgh with the
COMTIS Sinergia project, n. CRSI22 127510
(see www.idiap.ch/comtis/) and to the
three anonymous reviewers for their helpful com-
ments.
References
Shoshana Blum-Kulka. 1986. Shifts of Cohesion and
Coherence in Translation. In Juliane House and
Shoshana Blum-Kulka, editors, Interlingual and In-
tercultural Communication. Discourse and cogni-
tion in translation and second language acquisition,
pages 17?35. Narr Verlag, Tu?bingen, Germany.
Bruno Cartoni, Sandrine Zufferey, Thomas Meyer, and
Andrei Popescu-Belis. 2011. How Comparable
are Parallel Corpora? Measuring the Distribution of
General Vocabulary and Connectives. In Proceed-
ings of 4th Workshop on Building and Using Compa-
rable Corpora (BUCC), pages 78?86, Portland, OR.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an Open Source Toolkit for
24
Handling Large Scale Language Models. In Pro-
ceedings of Interspeech, Brisbane, Australia.
Najeh Hajlaoui and Andrei Popescu-Belis. 2013.
Assessing the Accuracy of Discourse Connective
Translations: Validation of an Automatic Met-
ric. In 14th International Conference on Intelligent
Text Processing and Computational Linguistics (CI-
CLING), Samos, Greece.
Sandra Halverson. 2004. Connectives as a Translation
Problem. In H. et al (Eds.) Kittel, editor, Encyclo-
pedia of Translation Studies, pages 562?572. Walter
de Gruyter, Berlin/New York.
Silvia Hansen-Schirra, Stella Neumann, and Erich
Steiner. 2007. Cohesive Explicitness and Explicita-
tion in an English-German Translation Corpus. Lan-
guages in Contrast, 7:241?265.
Iustina Ilisei, Diana Inkpen, Gloria Pastor Corpas, and
Ruslan Mitkov. 2010. Identifcation of Transla-
tionese: A Machine Learning Approach. In A. Gel-
bukh, editor, Computational Linguistics and Intelli-
gent Text Processing Lecture Notes in Computer Sci-
ence. Springer-Verlag, Berlin, Heidelberg, Germany.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbs. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of 45th Annual Meeting of the
Association for Computational Linguistics (ACL),
Demonstration Session, pages 177?180, Prague,
Czech Republic.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit X, pages 79?86, Phuket, Thailand.
Moshe Koppel and Noam Ordan. 2011. Translationese
and its Dialects. In Proceedings of ACL-HLT 2011
(49th Annual Meeting of the ACL: Human Language
Technologies, pages 1318?1326, Portland, OR.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0.
In Proceedings of 6th International Conference on
Language Resources and Evaluation (LREC), pages
2961?2968, Marrakech, Morocco.
Rashmi Prasad, Aravind Joshi, and Bonnie Webber.
2010. Realization of Discourse Relations by Other
Means: Alternative Lexicalizations. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics (COLING), pages 1023?1031,
Beijing, China.
Sandrine Zufferey, Liesbeth Degand, Andrei Popescu-
Belis, and Ted Sanders. 2012. Empirical Valida-
tions of Multilingual Annotation Schemes for Dis-
course Relations. In Proceedings of ISA-8 (8th
Workshop on Interoperable Semantic Annotation),
pages 77?84, Pisa, Italy.
25
EN conn. Majority rel. Tokens EN conn. Majority rel. Tokens
after Asynchrony 575/577 just as Synchrony 13/14
also Conjunction 1735/1746 later Asynchrony 90/91
although Contrast *157/328 meanwhile Synchrony 148/193
as Synchrony 543/743 moreover Conjunction 100/101
as a result Cause 78/78 nevertheless Concession *19/44
as if Concession *4/16 nonetheless Concession 17/27
as long as Condition 20/24 now that Cause 20/22
as soon as Asynchrony 11/20 once Asynchrony 78/84
because Cause 854/858 on the other hand Contrast 35/37
before Asynchrony 326/326 otherwise Alternative 22/24
but Contrast 2427/3308 previously Asynchrony 49/49
by contrast Contrast 27/27 separately Conjunction 73/74
even if Concession *41/83 since Cause 104/184
even though Concession 72/95 so that Cause 31/31
finally Asynchrony *14/32 still Concession 83/190
for example Instantiation 194/196 then Asynchrony 312/340
for instance Instantiation 98/98 therefore Cause 26/26
however Contrast 355/485 though Concession *156/320
if Condition 1127/1223 thus Cause 112/112
in addition Conjunction 165/165 unless Alternative 94/95
indeed Conjunction 54/104 until Asynchrony 140/162
in fact Restatement *39/82 when Synchrony 594/989
instead Alternative 109/112 while Contrast 455/781
in turn Asynchrony 20/30 yet Contrast 53/101
Table 1: English connectives with a frequency above 20 in the PDTB. Also listed are the level-2 majority
relations with the number of tokens out of the total tokens of the connective in the PDTB (counts includ-
ing the majority relation being part of a composite sense tag). *For some connectives there is no level-2
majority because some instances have only been annotated with level-1 senses. We did not consider the
connectives and and or (too many non-connective occurrences for automatic detection).
26
Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 33?42,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Detecting Narrativity to Improve English to French
Translation of Simple Past Verbs
Thomas Meyer
Idiap Research Institute and EPFL
Martigny and Lausanne, Switzerland
thomas.meyer@idiap.ch
Cristina Grisot
University of Geneva
Switzerland
cristina.grisot@unige.ch
Andrei Popescu-Belis
Idiap Research Institute
Martigny, Switzerland
andrei.popescu-belis@idiap.ch
Abstract
The correct translation of verb tenses en-
sures that the temporal ordering of events
in the source text is maintained in the tar-
get text. This paper assesses the utility
of automatically labeling English Simple
Past verbs with a binary discursive fea-
ture, narrative vs. non-narrative, for sta-
tistical machine translation (SMT) into
French. The narrativity feature, which
helps deciding which of the French past
tenses is a correct translation of the En-
glish Simple Past, can be assigned with
about 70% accuracy (F1). The narrativity
feature improves SMT by about 0.2 BLEU
points when a factored SMT system is
trained and tested on automatically labeled
English-French data. More importantly,
manual evaluation shows that verb tense
translation and verb choice are improved
by respectively 9.7% and 3.4% (absolute),
leading to an overall improvement of verb
translation of 17% (relative).
1 Introduction
The correct rendering of verbal tenses is an im-
portant aspect of translation. Translating to a
wrong verbal tense in the target language does not
convey the same meaning as the source text, for
instance by distorting the temporal order of the
events described in a text. Current statistical ma-
chine translation (SMT) systems may have diffi-
culties in choosing the correct verb tense transla-
tions, in some language pairs, because these de-
pend on a wider-range context than SMT systems
consider. Indeed, decoding for SMT is still at
the phrase or sentence level only, thus missing
information from previously translated sentences
(which is also detrimental to lexical cohesion and
co-reference).
In this paper, we explore the merits of a dis-
course feature called narrativity in helping SMT
systems to improve their translation choices for
English verbs in the Simple Past tense (hence-
forth, SP) into one of the three possible French
past tenses. The narrativity feature characterizes
each occurrence of an SP verb, either as narrative
(for ordered events that happened in the past) or
non-narrative (for past states of affairs). Narra-
tivity is potentially relevant to EN/FR translation
because three French past tenses can potentially
translate an English Simple Past (SP), namely the
Passe? Compose? (PC), Passe? Simple (PS) or Impar-
fait (IMP). All of them can be correct translations
of an EN SP verb, depending on its narrative or
non-narrative role.
The narrativity feature can be of use to SMT
only if it can be assigned with sufficient preci-
sion over a source text by entirely automatic meth-
ods. Moreover, a narrativity-aware SMT model is
likely to make a difference with respect to base-
line SMT only if it is based on additional features
that are not captured by, e.g., a phrase-based SMT
model. In this study, we use a small amount of
manually labeled instances to train a narrativity
classifier for English texts. The (imperfect) out-
put of this classifier over the English side of a
large parallel corpus will then be used to train a
narrativity-aware SMT system. In testing mode,
the narrativity classifier provides input to the SMT
system, resulting (as we will show below) in im-
proved tense and lexical choices for verbs, and
a modest but statistically significant increase in
BLEU and TER scores. Overall, the method is
similar in substance to our previous work on the
33
combination of a classifier for discourse connec-
tives with an SMT system (Meyer and Popescu-
Belis, 2012; Meyer et al, 2012).
The paper is organized as follows. Section 2 ex-
emplifies the hypothesized relation between nar-
rativity and the translations of the English Sim-
ple Past into French, along with related work on
modeling tense for MT. The automatic labeling
experiments are presented in Section 3. Exper-
iments with SMT systems are presented in Sec-
tion 4, with results from both automatic (4.3) and
manual translation scoring (4.4), followed by a
discussion of results and suggestions on improv-
ing them (Section 5).
2 English Simple Past in Translation
2.1 Role of Narrativity: an Example
The text in Figure 1 is an example taken from the
?newstest 2010? data described in Section 4 below.
In this four-sentence discourse, the English verbs,
all in Simple Past, express a series of events hav-
ing occurred in the past, which no longer affect
the present. As shown in the French translation by
a baseline SMT system (not aware of narrativity),
the English SP verbs are translated into the most
frequent tense in French, as learned from the par-
allel data the SMT was trained on.
When looking more closely, however, it ap-
pears that the Simple Past actually conveys dif-
ferent temporal and aspectual information. The
verbs offered and found describe actual events that
were ordered in time and took place subsequently,
whereas were and was describe states of general
nature, not indicating any temporal ordering.
The difference between narrative and non-
narrative uses of the English Simple Past is not al-
ways captured correctly by the baseline SMT out-
put in this example. The verbs in the first and third
sentences are correctly translated into the French
PC (one of the two tenses for past narratives in
French along with the PS). The verb in the sec-
ond sentence is also correctly rendered as IMP,
in a non-narrative use. However, the verb was in
the fourth sentence should also have been trans-
lated as an IMP, but from lack of sufficient infor-
mation, it was incorrectly translated as a PC. A
non-narrative label could have helped to find the
correct verb tense, if it would have been annotated
prior to translation.
EN: (1) After a party, I offered [Narrative] to
throw out a few glass and plastic bottles. (2) But,
on Kounicova Ulice, there were [Non-narrative]
no colored bins to be seen. (3) Luckily, on the
way to the tram, I found [Narrative] the right
place. (4) But it was [Non-narrative] overflow-
ing with garbage.
FR from BASELINE MT system: (1) Apre`s
un parti, j?ai propose? pour rejeter un peu de
verre et les bouteilles en plastique. (2) Mais, sur
Kounicova Ulice, il n?y avait pas de colored bins
a` voir. (3) Heureusement, sur la manie`re de le
tramway, j?ai trouve? la bonne place. (4) Mais il
*a e?te? de?borde?s avec des ramasseurs.
Figure 1: Example English text from ?newstest
2010? data with narrativity labels and a translation
into French from a baseline SMT. The tenses gen-
erated in French are, respectively: (1) PC, (2) IMP,
(3) PC, (4) PC. The mistake on the fourth one is
explained in the text.
2.2 Modeling Past Tenses
The classical view on verb tenses that express past
tense in French (PC, PS and IMP) is that both the
PC and PS are perfective, indicating that the event
they refer to is completed and finished (Martin,
1971). Such events are thus single points in time
without internal structure. However, on the one
hand, the PC signals an accomplished event (from
the aspectual point of view) and thus conveys as
its meaning the possible consequence of the event.
The PS on the other hand is considered as aspectu-
ally unaccomplished and is used in contexts where
time progresses and events are temporally ordered,
such as narratives.
The IMP is imperfective (as its name suggests),
i.e. it indicates that the event is in its preparatory
phrase and is thus incomplete. In terms of aspect,
the IMP is unaccomplished and provides back-
ground information, for instance ongoing state of
affairs, or situations that are repeated in time, with
an internal structure.
Conversely, in English, the SP is described as
having as its main meaning the reference to past
tense, and as specific meanings the reference to
present or future tenses identified under certain
contextual conditions (Quirk et al, 1986). Cor-
blin and de Swart (2004) argue that the SP is as-
pectually ?transparent?, meaning that it applies to
34
all types of events and it preserves their aspectual
class.
The difficulty for the MT systems is thus
to choose correctly among the three above-
mentioned tenses in French, which are all valid
possibilities of translating the English SP. When
MT systems fail to generate the correct tense in
French, several levels of incorrectness may oc-
cur, exemplified in Figure 2 with sentences taken
from the data used in this paper (see Section 3 and
Grisot and Cartoni (2012)).
1. In certain contexts, tenses may be quite inter-
changeable, which is the unproblematic case
for machine translation, depending also on
the evaluation measure. In Example 1 from
Figure 2, the verb e?taient conside?re?es (were
seen) in IMP has a focus on temporal length
which is preserved even if the translated tense
is a PC (ont e?te? conside?re?es, i.e. have been
seen) thanks to the adverb toujours (always)).
2. In other contexts, the tense proposed by the
MT system can sound strange but remains ac-
ceptable. For instance, in Example 2, there
is a focus on temporal length with the IMP
translation (voyait, viewed) but this meaning
is not preserved if a PC is used (a vu, has
viewed) though it can be recovered by the
reader.
3. The tense output by an MT system may be
grammatically wrong. In Example 3, the PC
a renouvele? (has renewed) cannot replace the
IMP renouvelaient (renewed) because of the
conflict with the imperfective meaning con-
veyed by the adverbial sans cesse (again and
again).
4. Finally, a wrong tense in the MT output can
be misleading, if it does not convey the mean-
ing of the source text but remains unnoticed
by the reader. In Example 4, using the PC
a e?te? leads to the interpretation that the per-
son was no longer involved when he died,
whereas using IMP e?tait implies that he was
still involved, which may trigger very differ-
ent expectations in the mind of the reader
(e.g. on the possible cause of the death, or its
importance to the peace process).
1. EN: Although the US viewed Musharraf as an agent
of change, he has never achieved domestic political legiti-
macy, and his policies were seen as rife with contradictions.
FR: Si les Etats-Unis voient Moucharraf comme un agent
de changement, ce dernier n?est jamais parvenu a` avoir
une le?gitimite? dans son propre pays, ou` ses politiques ont
toujours e?te? conside?re?es (PC) / e?taient conside?re?es (IMP)
comme un tissu de contradictions.
2. EN: Indeed, she even persuaded other important
political leaders to participate in the planned January 8
election, which she viewed as an opportunity to challenge
religious extremist forces in the public square.
FR: Benazir Bhutto a me?me convaincu d?autres dirigeants
de participer aux e?lections pre?vues le 8 janvier, qu?elle voy-
ait (IMP) / ?a vu (PC) comme une occasion de s?opposer
aux extre?mistes religieux sur la place publique.
3. EN: The agony of grief which overpowered them
at first, was voluntarily renewed, was sought for, was
created again and again...
FR: Elles s?encourage`rent l?une l?autre dans leur affliction,
la renouvelaient (IMP) / l?*a renouvele? (PC) volontaire-
ment, et sans cesse...
4. EN: Last week a person who was at the heart of
the peace process passed away.
FR: La semaine passe?e une personne qui e?tait (IMP) / a
e?te? (PC) au c?ur du processus de paix est de?ce?de?e.
Figure 2: Examples of translations of the English
SP by an MT system, differing from the refer-
ence translation: (1) unproblematic, (2) strange
but acceptable, (3) grammatically wrong (*), and
(4) misleading.
2.3 Verb Tenses in SMT
Modeling verb tenses for SMT has only recently
been addressed. For Chinese/English translation,
Gong et al (2012) built an n-gram-like sequence
model that passes information from previously
translated main verbs onto the next verb so that
its tense can be more correctly rendered. Tense is
morphologically not marked in Chinese, unlike in
English, where the verbs forms are modified ac-
cording to tense (among other factors). With such
a model, the authors improved translation by up to
0.8 BLEU points.
Conversely, in view of English/Chinese trans-
lation but without implementing an actual trans-
lation system, Ye et al (2007) used a classifier
to generate and insert appropriate Chinese aspect
markers that in certain contexts have to follow the
Chinese verbs but are not present in the English
source texts.
For translation from English to German, Gojun
and Fraser (2012) reordered verbs in the English
source to positions where they normally occur in
35
German, which usually amounts to a long-distance
movement towards the end of clauses. Reordering
was implemented as rules on syntax trees and im-
proved the translation by up to 0.61 BLEU points.
In this paper, as SMT training needs a large
amount of data, we use an automatic classifier to
tag instances of English SP verbs with narrativity
labels. The labels output by this classifier are then
modeled when training the SMT system.
3 Automatic Labeling of Narrativity
3.1 Data
A training set of 458 and a test set of 118 English
SP verbs that were manually annotated with narra-
tivity labels (narrative or non-narrative) was pro-
vided by Grisot and Cartoni (2012) (see their ar-
ticle for more details about the data). The train-
ing set consists of 230 narrative and 228 non-
narrative instances, the test set has 75 narrative in-
stances and 43 non-narrative ones. The sentences
come from parallel EN/FR corpora of four dif-
ferent genres: literature, news, parliamentary de-
bates and legislation. For each instance, the En-
glish sentence with the SP verb that must be clas-
sified, as well as the previous and following sen-
tences, had been given to two human annotators,
who assigned a narrative or non-narrative label. To
avoid interference with the translation into French,
which could have provided clues about the label,
the translations were not shown to annotators1.
Annotators agreed over only 71% of the in-
stances, corresponding to a kappa value of only
0.44. As this is at the lower end of the accept-
able spectrum for discourse annotation (Carletta,
1996), one of the important questions we ask in
this paper is: what can be achieved with this qual-
ity of human annotation, in terms of an automatic
narrativity classifier (intrinsic performance) and of
its use for improving verb translation by SMT (ex-
trinsic evaluation)? It must be noted that instances
on which the two annotators had disagreed were
resolved (to either narrative or non-narrative) by
looking at the French human translation (an ac-
ceptable method given that our purpose here is
translation into French), thus increasing the qual-
ity of the annotation.
1The goal was to focus on the narrativity property, regard-
less of its translation. However, annotations were adjudicated
also by looking at the FR translation. For a different ap-
proach, considering exclusively the tense in translation, see
the discussion in Section 5.
Model Recall Prec. F1 ?
MaxEnt 0.71 0.72 0.71 +0.43
CRF 0.30 0.44 0.36 ?0.44
Table 1: Performance of MaxEnt and CRF clas-
sifiers on narrativity. We report recall, precision,
their mean (F1), and the kappa value for class
agreement.
3.2 Features for Narrativity
The manually annotated instances were used for
training and testing a Maximum Entropy classi-
fier using the Stanford Classifier package (Man-
ning and Klein, 2003). We extracted the following
features from the sentence containing the verb to
classify and the preceding sentence as well, thus
modeling a wider context than the one modeled by
phrase-based SMT systems. For each verb form,
we considered its POS tag and syntactical cate-
gory, including parents up to the first verbal phrase
(VP) parent node, as generated by Charniak and
Johnson?s constituent parser (2005). This parser
also assigns special tags to auxiliary (AUX) and
modal verbs (MD), which we include in the fea-
tures.
We further used a TimeML parser, the Tarsqi
Toolkit (Verhagen et al, 2005; Verhagen and
Pustejovsky, 2008), which automatically outputs
an XML-like structure of the sentence, with a hy-
pothesis on the temporal ordering of the events
mentioned. From this structure we extract event
markers such as PAST-OCCURRENCE and aspec-
tual information such as STATE.
Temporal ordering is often also signaled by
other markers such as adverbials (e.g., three weeks
before). We manually gathered a list of 66 such
temporal markers and assigned them, as an addi-
tional feature, a label indicating whether they sig-
nal synchrony (e.g., meanwhile, at the same time)
or asynchrony (e.g., before, after).
3.3 Results of Narrativity Labeling
With the above features, we obtained the classi-
fication performance indicated in Table 1. The
MaxEnt classifier reached 0.71 F1 score, which is
similar to the human annotator?s agreement level.
Moreover, the kappa value for inter-class agree-
ment was 0.43 between the classifier and the hu-
man annotation, a value which is also close to the
kappa value for the two human annotators. In a
sense, the classifier thus reaches the highest scores
36
that are still meaningful, i.e. those of inter-coder
agreement. As a baseline for comparison, the ma-
jority class in the test set (the ?narrative? label)
would account for 63.56% of correctly classified
instances, whereas the classifier correctly labeled
72.88% of all test instances.
For further comparison we built a CRF
model (Lafferty et al, 2001) in order to label nar-
rativity in sequence of other tags, such as POS.
The CRF uses as features the two preceding POS
tags to label the next POS tag in a sequence of
words. The same training set of 458 sentences
as used above was POS-tagged using the Stan-
ford POS tagger (Toutanova et al, 2003), with the
left3words-distsim model. We replaced
the instances of ?VBD? (the POS tag for SP verbs)
with the narrativity labels from the manual annota-
tion. The same procedure was then applied to the
118 sentences of the test set on which CRF was
evaluated.
Overall, the CRF model only labeled narrativity
correctly at an F1 score of 0.36, while kappa had
a negative value signaling a weak inverse correla-
tion. Therefore, it appears that the temporal and
semantic features used for the MaxEnt classifier
are useful and account for the much higher per-
formance of MaxEnt, which is used in the SMT
experiments described below.
We further evaluate the MaxEnt classifier by
providing in Table 2 the confusion matrix of the
automatically obtained narrativity labels over the
test set. Labeling non-narrative uses is slightly
more prone to errors (32.6% error rate) than nar-
rative ones (24% errors), likely due to the larger
number of narratives vs. non-narratives in the
training and the test data.
System
Reference Narr. Non-narr. Total
Narrative 57 18 75
Non-narr. 14 29 43
Total 71 47 118
Table 2: Confusion matrix for the labels output
by the MaxEnt classifier (System) versus the gold
standard labels (Reference).
4 SMT with Narrativity Labels
4.1 Method
Two methods to use labels conveying to SMT in-
formation about narrativity were explored (though
more exist). First, as in our initial studies ap-
plied to discourse connectives, the narrativity la-
bels were simply concatenated with the SP verb
form in EN (Meyer and Popescu-Belis, 2012) ?
see Example 2 in Figure 3. Second, we used
factored translation models (Koehn and Hoang,
2007), which allow for any linguistic annotation
to be considered as additional weighted feature
vectors, as in our later studies with connectives
(Meyer et al, 2012). These factors are log-linearly
combined with the basic features of phrase-based
SMT models (phrase translation, lexical and lan-
guage model probabilities).
To assess the performance gain of narrativity-
augmented systems, we built three different SMT
systems, with the following names and configura-
tions:
? BASELINE: plain text, no verbal labels.
? TAGGED: plain text, all SP verb forms con-
catenated with a narrativity label.
? FACTORED: all SP verbs have narrativity
labels as source-side translation factors (all
other words labeled ?null?).
1. BASELINE SMT: on wednesday the c?ssd de-
clared the approval of next year?s budget to be a
success. the people?s party was also satisfied.
2. TAGGED SMT: on wednesday the c?ssd
declared-Narrative the approval of next year?s
budget to be a success. the people?s party was-
Non-narrative also satisfied.
3. FACTORED SMT: on wednesday the c?ssd
declared|Narrative the approval of next year?s
budget to be a success. the people?s party
was|Non-narrative also satisfied.
Figure 3: Example input sentence from ?newstest
2010? data for three translation models: (1) plain
text; (2) concatenated narrativity labels; (3) narra-
tivity as translation factors (the ?|null? factors on
other words were omitted for readability).
Figure 3 shows an example input sentence for
these configurations. For the FACTORED SMT
model, both the EN source word and the factor
37
information are used to generate the FR surface
target word forms. The tagged or factored annota-
tions are respectively used for the training, tuning
and test data as well.
For labeling the SMT data, no manual annota-
tion is used. In a first step, the actual EN SP verbs
to be labeled are identified using the Stanford POS
tagger, which assigns a ?VBD? tag to each SP verb.
These tags are replaced, after feature extraction
and execution of the MaxEnt classifier, by the nar-
rativity labels output by the latter. Of course, the
POS tagger and (especially) our narrativity clas-
sifier may generate erroneous labels which in the
end lead to translation errors. The challenge is
thus to test the improvement of SMT with respect
to the baseline, in spite of the noisy training and
test data.
4.2 Data
In all experiments, we made use of parallel En-
glish/French training, tuning and testing data from
the translation task of the Workshop on Machine
Translation (www.statmt.org/wmt12/).
? For training, we used Europarl v6 (Koehn,
2005), original EN2 to translated FR
(321,577 sentences), with 66,143 instances
of SP verbs labeled automatically: 30,452
are narrative and 35,691 are non-narrative.
? For tuning, we used the ?newstest 2011? tun-
ing set (3,003 sentences), with 1,401 auto-
matically labeled SP verbs, of which 807 are
narrative and 594 non-narrative.
? For testing, we used the ?newstest 2010? data
(2,489 sentences), with 1,156 automatically
labeled SP verbs (621 narrative and 535 non-
narrative).
We built a 5-gram language model with SRILM
(Stolcke et al, 2011) over the entire FR part of Eu-
roparl. Tuning was performed by Minimum Error
Rate Training (MERT) (Och, 2003). All transla-
tion models were phrase-based using either plain
text (possibly with concatenated labels) or fac-
tored training as implemented in the Moses SMT
toolkit (Koehn et al, 2007).
2We only considered texts that were originally authored
in English, not translated into it from French or a third-party
language, to ensure only proper tenses uses are observed. The
relevance of this constraint is discussed for connectives by
Cartoni et al (2011).
4.3 Results: Automatic Evaluation
In order to obtain reliable automatic evaluation
scores, we executed three runs of MERT tuning for
each type of translation model. With MERT being
a randomized, non-deterministic optimization pro-
cess, each run leads to different feature weights
and, as a consequence, to different BLEU scores
when translating unseen data.
Table 3 shows the average BLEU and TER
scores on the ?newstest 2010? data for the three
systems. The scores are averages over the three
tuning runs, with resampling of the test set,
both provided in the evaluation tool by Clark
et al (2011) (www.github.com/jhclark/
multeval). BLEU is computed using jBLEU
V0.1.1 (an exact reimplementation of NIST?s
?mteval-v13.pl? script without tokenization). The
Translation Error Rate (TER) is computed with
version 0.8.0 of the software (Snover et al, 2006).
A t-test was used to compute p values that indicate
the significance of differences in scores.
Translation model BLEU TER
BASELINE 21.4 61.9
TAGGED 21.3 61.8
FACTORED 21.6* 61.7*
Table 3: Average values of BLEU (the higher the
better) and TER (the lower the better) over three
tuning runs for each model on ?newstest 2010?.
The starred values are significantly better (p <
0.05) than the baseline.
In terms of overall BLEU and TER scores, the
FACTORED model improves performance over the
BASELINE by +0.2 BLEU and -0.2 TER (as lower
is better), and these differences are statistically
significant at the 95% level. On the contrary,
the concatenated-label model (noted TAGGED)
slightly decreases the global translation perfor-
mance compared to the BASELINE. A similar
behavior was observed when using labeled con-
nectives in combination with SMT (Meyer et al,
2012).
The lower scores of the TAGGED model may
be due to the scarcity of data (by a factor of 0.5)
when verb word-forms are altered by concatenat-
ing them with the narrativity labels. The small
improvement by the FACTORED model of over-
all scores (such as BLEU) is also related to the
scarcity of SP verbs: although their translation is
38
improved, as we will now show, the translation of
all other words is not changed by our method, so
only a small fraction of the words in the test data
are changed.
4.4 Results: Human Evaluation
To assess the improvement specifically due to the
narrativity labels, we manually evaluated the FR
translations by the FACTORED model for the 207
first SP verbs in the test set against the transla-
tions from the BASELINE model. As the TAGGED
model did not result in good scores, we did not fur-
ther consider it for evaluation. Manual scoring was
performed along the following criteria for each oc-
currence of an SP verb, by bilingual judges look-
ing both at the source sentence and its reference
translation.
? Is the narrativity label correct? (?correct? or
?incorrect?) ? this is a direct evaluation of the
narrativity classifier from Section 3
? Is the verb tense of the FACTORED model
more accurate than the BASELINE one?
(noted ?+? if improved, ?=? if similar, ??? if
degraded)
? Is the lexical choice of the FACTORED model
more accurate than the BASELINE one, re-
gardless of the tense? (again noted ?+? or ?=?
or ???)
? Is the BASELINE translation of the verb
phrase globally correct? (?correct? or ?incor-
rect?)
? Is the FACTORED translation of the verb
phrase globally correct? (?correct? or ?incor-
rect?)
Tables 4 and 5 summarize the counts and per-
centages of improvements and/or degradations of
translation quality with the systems FACTORED
and BASELINE. The correctness of the labels, as
evaluated by the human judges on SMT test data,
is similar to the values given in Section 3 when
evaluated against the test sentences of the narra-
tivity classifier. As shown in Table 4, the narrativ-
ity information clearly helps the FACTORED sys-
tem to generate more accurate French verb tenses
in almost 10% of the cases, and also helps to find
more accurate vocabulary for verbs in 3.4% of the
cases. Overall, as shown in Table 5, the FAC-
TORED model yields more correct translations of
the verb phrases than the BASELINE in 9% of the
cases ? a small but non-negligible improvement.
Criterion Rating N. % ?
Labeling correct 147 71.0
incorrect 60 29.0
Verb + 35 17.0
tense = 157 75.8 +9.7
? 15 7.2
Lexical + 19 9.2
choice = 176 85.0 +3.4
? 12 5.8
Table 4: Human evaluation of verb translations
into French, comparing the FACTORED model
against the BASELINE. The ? values show the
clear improvement of the narrativity-aware fac-
tored translation model.
System Rating Number %
BASELINE correct 94 45.5
incorrect 113 54.5
FACTORED correct 113 54.5
incorrect 94 45.5
Table 5: Human evaluation of the global cor-
rectness of 207 translations of EN SP verbs into
French. The FACTORED model yields 9% more
correct translations than the BASELINE one.
An example from the test data shown in Fig-
ure 4 illustrates the improved verb translation. The
BASELINE system translates the SP verb looked
incorrectly into the verb conside?rer (consider), in
wrong number and its past participle only (con-
side?re?s, plural). The FACTORED model generates
the correct tense and number (IMP, semblait, sin-
gular) and the better verb sembler (look, appear).
This example is scored as follows: the labeling is
correct (?yes?), the tense was improved (?+?), the
lexical choice was improved too (?+?), the BASE-
LINE was incorrect while the FACTORED model
was correct.
5 Discussion and Future Work
When looking in detail through the translations
that were degraded by the FACTORED model,
some were due to the POS tagging used to find
the EN SP verbs to label. For verb phrases made
of an auxiliary verb in SP and a past participle
(e.g. was born), the POS tagger outputs was/VBD
born/VBN. As a consequence, our classifier only
considers was, as non-narrative, although was
39
EN: tawa hallae looked|Non-narrative like
many other carnivorous dinosaurs.
FR BASELINE: tawa hallae *conside?re?s
comme de nombreuses autres carnivores di-
nosaures.
FR FACTORED: tawa hallae semblait comme
de nombreux autres carnivores dinosaures.
Figure 4: Example comparison of a baseline and
improved factored translation. The ?|null? factors
in EN were omitted for readability. See the text
for a discussion.
born as a whole is a narrative event. This can
then result in wrong FR tense translations. For
instance, the fragment nelson mandela was|Non-
narrative born on . . . is translated as: nelson man-
dela *e?tait ne? en . . . , which in FR is pluperfect
tense instead of the correct Passe? Compose? est ne?
as in the reference translation. A method to con-
catenate such verb phrases to avoid such errors is
under work.
A further reason for the small improvements in
translation quality might be that factored transla-
tion models still operate on rather local context,
even when the narrativity information is present.
To widen the context captured by the translation
model, labeling entire verbal phrase nodes in hi-
erarchical or tree-based syntactical models will be
considered in the future. Moreover, it has been
shown that it is difficult to choose the optimal pa-
rameters for a factored translation model (Tam-
chyna and Bojar, 2013).
In an alternative approach currently under work,
a more direct way to label verb tense is imple-
mented, where a classifier can make use of the
same features as those extracted here (in Sec-
tion 3.2), but its classes are those that directly
indicate which target verb tense should be out-
put by the SMT. Thus, not only SP verbs can
be considered and no intermediate category such
as narrativity (that is more difficult to learn) is
needed. The classifier will predict which FR tense
should be used depending on the context of the
EN verbs, for which the FR tense label can be
annotated as above, within a factored translation
model. Through word alignment and POS tag-
ging, this method has the additional advantage of
providing much more training data, extracted from
word alignment of the verb phrases, and can be
applied to all tenses, not only SP. Moreover, the
approach is likely to learn which verbs are prefer-
ably translated with which tense: for instance, the
verb started is much more likely to become a com-
mence? (PC) in FR than to commenc?ait (IMP), due
to its meaning of a punctual event in time, rather
than a continuous or repetitive one.
6 Conclusion
The paper presented a method to automatically la-
bel English verbs in Simple Past tense with a bi-
nary pragmatic feature, narrativity, which helps
to distinguish temporally ordered events that hap-
pened in the past (?narrative?) from past states of
affairs (?non-narrative?). A small amount of man-
ually annotated data, combined with the extraction
of temporal semantic features, allowed us to train a
classifier that reached 70% correctly classified in-
stances. The classifier was used to automatically
label the English SP verbs in a large parallel train-
ing corpus for SMT systems. When implement-
ing the labels in a factored SMT model, translation
into French of the English SP verbs was improved
by about 10%, accompanied by a statistically sig-
nificant gain of +0.2 BLEU points for the overall
quality score. In the future, we will improve the
processing of verb phrases, and study a classifier
with labels that are directly based on the target lan-
guage tenses.
Acknowledgments
We are grateful for the funding of this work to the
Swiss National Science Foundation (SNSF) under
the COMTIS Sinergia Project, n. CRSI22 127510
(see www.idiap.ch/comtis/). We would
also like to thank the anonymous reviewers for
their helpful suggestions.
References
Jean Carletta. 1996. Assessing Agreement on Classi-
fication Tasks: The Kappa Statistic. Computational
Linguistics, 22:249?254.
Bruno Cartoni, Sandrine Zufferey, Thomas Meyer, and
Andrei Popescu-Belis. 2011. How Comparable
are Parallel Corpora? Measuring the Distribution of
General Vocabulary and Connectives. In Proceed-
ings of 4th Workshop on Building and Using Compa-
rable Corpora (BUCC), pages 78?86, Portland, OR.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best Parsing and MaxEnt Discriminative
40
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 173?180, Ann Arbor, MI.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah
Smith. 2011. Better Hypothesis Testing for Statisti-
cal Machine Translation: Controlling for Optimizer
Instability. In Proceedings of ACL-HLT 2011 (46th
Annual Meeting of the ACL: Human Language Tech-
nologies), Portland, OR.
Francis Corblin and Henrie?tte de Swart. 2004. Hand-
book of French Semantics. CSLI Publications, Stan-
ford, CA.
Anita Gojun and Alexander Fraser. 2012. Determin-
ing the Placement of German Verbs in English-to-
German SMT. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL), pages 726?735,
Avignon, France.
Zhengxian Gong, Min Zhang, Chew Lim Tan, and
Guodong Zhou. 2012. N-Gram-Based Tense
Models for Statistical Machine Translation. In
Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing (EMNLP)
and Computational Natural Language Learning
(CoNLL), pages 276?285, Jeju Island, Korea.
Cristina Grisot and Bruno Cartoni. 2012. Une de-
scription bilingue des temps verbaux: e?tude con-
trastive en corpus. Nouveaux cahiers de linguistique
franc?aise, 30:101?117.
Philipp Koehn and Hieu Hoang. 2007. Factored
Translation Models. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) and Computational
Natural Language Learning (CoNLL), pages 868?
876, Prague, Czech Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbs. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of 45th Annual Meeting of the
Association for Computational Linguistics (ACL),
Demonstration Session, pages 177?180, Prague,
Czech Republic.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit X, pages 79?86, Phuket, Thailand.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. The Journal of Machine Learning Re-
search, 8:693?723.
Christopher Manning and Dan Klein. 2003. Optimiza-
tion, MaxEnt Models, and Conditional Estimation
without Magic. In Tutorial at HLT-NAACL and 41st
ACL conferences, Edmonton, Canada and Sapporo,
Japan.
Robert Martin. 1971. Temps et aspect: essai sur
l?emploi des temps narratifs en moyen franc?ais.
Klincksieck, Paris, France.
Thomas Meyer and Andrei Popescu-Belis. 2012. Us-
ing Sense-labeled Discourse Connectives for Statis-
tical Machine Translation. In Proceedings of the
EACL 2012 Joint Workshop on Exploiting Synergies
between IR and MT, and Hybrid Approaches to MT
(ESIRMT-HyTra), pages 129?138, Avignon, FR.
Thomas Meyer, Andrei Popescu-Belis, Najeh Hajlaoui,
and Andrea Gesmundo. 2012. Machine Translation
of Labeled Discourse Connectives. In Proceedings
of the Tenth Biennial Conference of the Association
for Machine Translation in the Americas (AMTA),
San Diego, CA.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics (ACL), pages 160?167,
Sapporo, Japan.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1986. A Comprehensive Gram-
mar of the English Language. Pearson Longman,
Harlow, UK.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of the Tenth Biennial Con-
ference of the Association for Machine Translation
in the Americas (AMTA), Cambridge, MA.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at Sixteen: Update and
Outlook. In Proceedings of the IEEE Automatic
Speech Recognition and Understanding Workshop,
Waikoloa, Hawaii.
Ales? Tamchyna and Ondr?ej Bojar. 2013. No Free
Lunch in Factored Phrase-Based Machine Transla-
tion. In Proceedings of the 14th International Con-
ference on Computational Linguistics and Intelli-
gent Text Processing (CICLING), Samos, Greece.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich Part-of-
Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of Human Language Tech-
nology Conference and the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL), pages 252?259, Edmonton, CA.
Marc Verhagen and James Pustejovsky. 2008. Tempo-
ral Processing with the TARSQI Toolkit. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics (COLING), Companion
volume: Demonstrations, pages 189?192, Manch-
ester, UK.
41
Marc Verhagen, Inderjeet Mani, Roser Sauri, Jes-
sica Littman, Robert Knippen, Seok Bae Jang,
Anna Rumshisky, John Phillips, and James Puste-
jovsky. 2005. Automating Temporal Annotation
with TARSQI. In Proceedings of the 43th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Demo Session, pages 81?84, Ann
Arbor, USA.
Yang Ye, Karl-Michael Schneider, and Steven Abney.
2007. Aspect Marker Generation for English-to-
Chinese Machine Translation. In Proceedings of MT
Summit XI, pages 521?527, Copenhagen, Danmark.
42
Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 43?50,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Machine Translation with Many Manually Labeled Discourse Connectives
Thomas Meyer
Idiap Research Institute and EPFL
Martigny and Lausanne, Switzerland
thomas.meyer@idiap.ch
Lucie Pola?kova?
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics
Charles University in Prague
Prague, Czech Republic
polakova@ufal.mff.cuni.cz
Abstract
The paper presents machine translation ex-
periments from English to Czech with a
large amount of manually annotated dis-
course connectives. The gold-standard
discourse relation annotation leads to bet-
ter translation performance in ranges of
4?60% for some ambiguous English con-
nectives and helps to find correct syntacti-
cal constructs in Czech for less ambiguous
connectives. Automatic scoring confirms
the stability of the newly built discourse-
aware translation systems. Error analysis
and human translation evaluation point to
the cases where the annotation was most
and where less helpful.
1 Introduction
Recently, research in statistical machine transla-
tion (SMT) has renewed interest in the fact that
for a variety of linguistic phenomena one needs
information from a longer-range context. Cur-
rent statistical translation models and decoding
algorithms operate at the sentence and/or phrase
level only, not considering already translated con-
text from previous sentences. This local dis-
tance is in many cases too restrictive to correctly
model lexical cohesion, referential expressions
(noun phrases, pronouns), and discourse markers,
all of which relate to the sentence(s) before the one
to be translated.
Discourse relations between sentences are often
conveyed by explicit discourse connectives (DC),
such as although, because, but, since, while. DCs
play a significant role in coherence and readabil-
ity of a text. Likewise, if a wrong connective is
used in translation, the target text can be fully in-
comprehensible or not conveying the same mean-
ing as was established by the discourse relations
in the source text. In English, about 100 types
of such explicit connectives have been annotated
in the Penn Discourse TreeBank (PDTB, see Sec-
tion 4), signaling discourse relations such as tem-
porality or contrast between two spans of text. De-
pending on the set of relations used, there can be
up to 130 such relations and combinations thereof.
Discourse relations can also be present implicitly
(inferred from the context), without any explicit
marker being present. Although annotation for im-
plicit DCs exists as well, we only deal with explicit
DCs in this paper. DCs are difficult to translate
mainly because a same English connective can sig-
nal different discourse relations in different con-
texts and when the target language has either dif-
ferent connectives according to the source rela-
tions signaled or uses different lexical or syntac-
tical constructs in place of the English connective.
In this paper, we present MT experiments from
English (EN) to Czech (CZ) with a large amount
of manually annotated DCs. The corpus, the par-
allel Prague Czech-English Dependency Treebank
(PCEDT) (Section 4), is directly usable for MT
experiments: the entire discourse annotation in
EN is paralleled with a human CZ translation.
This means that we can build and evaluate, against
the CZ reference, a translation system, that learns
from the EN gold standard discourse relations.
These then have no distortion from wrongly la-
beled connectives as it is given in related work
(Section 3) where automatic classifiers have been
used to label the connectives with a certain er-
ror rate. Furthermore, we can use the sense la-
bels for 100 types of EN connectives, whereas re-
lated work only focused on a few highly ambigu-
ous connectives that are especially problematic for
translation.
The paper starts by illustrating difficult trans-
lations involving connectives (Section 2) and dis-
cusses related work in Section 3. The resources
and data used are introduced in Section 4. The
MT experiments are explained in Section 5 and
43
automatic evaluation is given in Section 6. We fur-
ther provide a detailed manual evaluation and error
analysis for the CZ translations generated by our
SMT systems (Section 7). Future work described
in Section 8 concludes the paper.
2 Motivation
The following example shows a CZ translation
of the English DC meanwhile. The previous
sentences to the example were about other com-
puter producers expected to report disappointing
financial results. The interpretation of meanwhile
and the discourse relation (or sense) signaled is
therefore CONTRASTIVE and not TEMPORAL:
SOURCE: Apple Computer Inc., mean-
while<COMPARISONCONTRAST>, is expected to
show improved earnings for the period ended September.
BASELINE: Spolec?nost Apple Computer Inc., mezit??m by
me?la uka?zat leps??? pr???jmy za obdob?? konc???c?? v za?r???.
SYSTEM2: Spolec?nost Apple Computer Inc., naopak by
me?la uka?zat leps??? pr???jmy za obdob?? konc???c?? v za?r???.
A baseline SMT system for EN/CZ generated the
incorrect CZ connective mezit??m which signals a
temporal relation only. The translation marked
SYSTEM2 in the example was output by one of
the systems we trained on manual DC annotations
(cf. Section 5). The system correctly generated
the CZ connective naopak signaling a contrastive
sense. The example sentence is taken from the
Wall Street Journal corpus, section 2365. The
sense tag for meanwhile was manually annotated
in the Penn Discourse TreeBank, see Section 4.
3 Related Work
The disambiguation of DCs can be seen as a spe-
cial form of Word Sense Disambiguation (WSD),
that has been applied to SMT for content words
with slight improvements to translation qual-
ity (Chan et al, 2007; Carpuat and Wu, 2007).
DCs however form a class of procedural function
words that relate text spans from an arbitrarily
long context and their disambiguation needs fea-
tures from that longer-range context. Only few
studies address function word disambiguation for
SMT: Chang et al (2009) disambiguate a mul-
tifunctional Chinese particle for Chinese/English
translation and Ma et al (2011) use tagging of
English collocational particles for translation into
Chinese. Lexical cohesion at the document level
has recently also come into play, with studies
on lexical consistency in SMT (Carpuat, 2009;
Carpuat and Simard, 2012), topic modeling ap-
plied to SMT (Eidelman et al, 2012) or decod-
ing with document-wide features (Hardmeier et
al., 2012). A recently published article summa-
rizes most of the work on SMT with the broader
perspective of discourse, lexical cohesion and co-
reference (Hardmeier, 2013).
For discourse relations and DCs especially,
more and more annotated resources have be-
come available in several languages, such as En-
glish (Prasad et al, 2008), French (Pe?ry-Woodley
et al, 2009; Danlos et al, 2012), German (Stede,
2004), Arabic (AlSaif, 2012), Chinese (Zhou and
Xue, 2012) and Czech (Mladova? et al, 2009).
These resources however remain mostly monolin-
gual, i.e. translations or parallel texts in other lan-
guages do normally not exist. This makes these
resources not directly usable for MT experiments.
Recent work has shown that more adequate
and coherent translations can be generated for
English/French when ambiguous connectives in
the source language are annotated with the dis-
course relation they signal (Popescu-Belis et al,
2012). SMT systems for European language
pairs are most often trained on Europarl corpus
data (Koehn, 2005), where only a small amount of
discourse-annotated instances is available (8 con-
nectives with about 300-500 manual annotations
each). Meyer and Popescu-Belis (2012) there-
fore used these few examples to train automatic
classifiers that introduce the sense labels for the
connectives in the entire English text of the Eu-
roparl corpus. Although these classifiers are state-
of-the-art, they can have an error rate of up to
30% when labeling unseen instances of connec-
tives. The discourse-aware SMT systems never-
theless improved about 8-10% of the connective
translations. When integrating into SMT directly
the small manually-labeled data, without train-
ing classifiers, hardly any translation improvement
was measurable, cf. (Meyer and Popescu-Belis,
2012).
4 The Parallel Prague Czech-English
Dependency Treebank
With the English-Czech parallel text provided in
the Prague Czech-English Dependency Treebank
2.0 (PCEDT) (Hajic? et al, 2011)1, comes a hu-
man CZ translation of the entire Wall Street Jour-
nal Corpus in EN (WSJ, sections 00-24, approxi-
1http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2012T08
44
mately 50k sentences).
The syntactical annotation of WSJ, the Penn
TreeBank (Marcus et al, 1993), has been followed
by a discourse annotation project, the Penn Dis-
course TreeBank (PDTB) (Prasad et al, 2008),
over the same sections of the corpus. In the
PDTB version 2.0, 18,459 instances of explicit
DCs, among other discourse-related phenomena
(implicit relations, alternative lexicalizations), are
labeled along with the text spans they connect (dis-
course arguments) and the discourse relation they
signal (sense tags).
The sense tags are organized in a three-level
sense hierarchy with four top semantic classes,
16 sub-senses on the second and further 23 sub-
senses on the third hierarchy level. The annotators
were not forced to make the finest distinction (on
the sub-sense level). A token can also be annotated
with two senses, forming a composite sense with
a label combination from wherever in the hierar-
chy, resulting in 129 theoretically possible distinct
sense tags (see Section 5 for the sense levels we
use). For the latter reason, some of the sense labels
are very scarcely used and although they make for
important and fine-grained distinctions in English,
this granularity level might not be useful for trans-
lation, where only certain ambiguities have to be
resolved to obtain a correct target language con-
nective, see Section 7.
The PCEDT is a 1:1 sentence-aligned paral-
lel resource with a manual multilayer dependency
analysis of both original Penn TreeBank-WSJ
texts and their translations to Czech. Despite
the manually annotated parallel dependency trees
which are very valuable in other linguistic stud-
ies, for translation we only used the plain CZ texts
provided with the treebank.
5 Experimental Setup
In the following, we describe a series of SMT ex-
periments that made direct use of the EN/CZ text
as provided with the PCEDT. The SMT models
were all phrase-based and trained with the Moses
decoder (Koehn et al, 2007), either on plain text
for the BASELINE or on text where the EN con-
nective word-forms have been concatenated with
the PDTB sense labels. All texts have been tok-
enized and lowercased with the Moses tools before
training SMT. In future work, we will build fac-
tored translation models (Koehn and Hoang, 2007)
as well, as this would reduce the label scarcity
that was likely a problem when just concatenating
word-forms and labels (see Sections 7 and 8).
For SYSTEM1 in the following, we inserted, into
the English side of the PCEDT data, the full sense
labels from the PDTB, which can be, as already
mentioned, as detailed as containing 3 sense levels
and allowing for composite tags (where annotators
chose that two senses hold at the same time). SYS-
TEM1 therefore operates on a total of 63 distinct
and observed sense tags for all DCs.
For SYSTEM2, we reduced the sense labels to
contain only senses from PDTB sense hierarchy
level 2 and 1, not allowing for composite senses,
i.e. for those instances that were annotated with
two senses we discarded the secondary (but not
less important) sense. This reduced the set of
senses for SYSTEM2 to 22.
The procedure is exemplified in the example
below with an EN sentence 1 (WSJ section
2300) containing a complex PDTB sense tag
that has been kept for SYSTEM1. For SYS-
TEM2 we have reduced the sense of when to:
<CONTINGENCYCONDITIONGENERAL>. Sen-
tence 2 (WSJ section 2341) contains two already
simplified sense tags. The original PDTB sense
tags for meanwhile and as were respectively
<COMPARISONCONTRASTJUXTAPOSITION>
and <CONTINGENCYPRAGMATICCAUSE-
JUSTIFICATION>, where JUXTAPOSITION and
JUSTIFICATION were dropped because they stem
from the third level of the PDTB sense hierarchy:
1. Selling snowballed because of waves of au-
tomatic ?stop-loss? orders, which are triggered by
computer when<CONTINGENCYCONDITIONGENERAL-
TEMPORALASYNCHRONOUSSUCCESSION> prices fall to
certain levels.
2. Meanwhile<COMPARISONCONTRAST>, analysts said
Pfizer?s recent string of lackluster quarterly performances
continued, as<CONTINGENCYPRAGMATICCAUSE> earn-
ings in the quarter were expected to decline by about 5%.
In order to build SMT systems of reasonable
quality, we still need to combine the PCEDT texts
(50k sentences) with other resources such as the
EN/CZ parts of the Europarl corpus. This results
in a mixture of labeled and unlabeled DCs in the
data and estimates might be noisy. We however
also checked system performance on the PDTB
test set (section 23) with labeled DCs only (see
Section 6) for which the unlabeled ones in the
model do not pose a problem, as they are not con-
sidered as valid target phrases by the SMT de-
coder. The following list gives an overview of the
data used to build three SMT systems. No modi-
45
fications have been done to the texts of the BASE-
LINE system, that uses exactly the same amount of
sentences, but no sense labels.
? BASELINE: no tags for connectives
? SYSTEM1: complex PDTB sense tags
? SYSTEM2: simplified PDTB sense tags
? training: Europarlv7 (645,155 sentences)
+ PDTB sections 02-21 (41,532 sentences;
15,402 connectives)
? tuning: newstest2011 (3,003 sentences) +
PDTB sections 00,01,22,24 (5,260 sentences;
2,134 connectives)
? testing: newstest2012 (3,001 sentences) +
PDTB section 23 (2,416 sentences; 923 con-
nectives)2
The language model, the same for BASE-
LINE, SYSTEM1 and SYSTEM2, was built using
SRILM (Stolcke et al, 2011) with 5-grams over
Europarl and the news data sets 2007-2011 in CZ,
as distributed by the Workshop on Machine Trans-
lation3. All systems were tuned by MERT (Och,
2003) as implemented in Moses.
6 Automatic Evaluation
Most automatic MT scoring relies on n-gram
matching of a system?s candidate translation
against (usually) only one human reference trans-
lation. For DCs therefore, automatic scores do not
reveal much of a system?s performance, as often
only one or two words, i.e. the DC is changed.
When a candidate translation however contains a
more accurate and correct connective, the trans-
lation output is often more coherent and readable
than the baseline?s output, see Section 7.
Automatic evaluation has been done using the
MultEval tool, version 0.5.1 (Clark et al, 2011).
The BLEU scores are computed by jBLEU V0.1.1
(an exact reimplementation of NIST?s mteval-
v13.pl without tokenization). Table 1 provides an
overview of the BLEU scores for the BASELINE
and systems 1 and 2 on the full test set (new-
stest2012 + PDTB section 23), and on PDTB sec-
tion 23 only, the latter containing 2,416 sentences
and 923 labeled DCs.
In order to gain reliable automatic evaluation
scores, we executed 5 runs of MERT for each
2Note that this PDTB section division for training, devel-
opment and testing is the same as is used for automatic clas-
sification experiments, as recommended in the PDTB anno-
tation manual.
3http://www.statmt.org/wmt12/
translation model configuration. MERT is imple-
mented as a randomized, non-deterministic opti-
mization process, so that each run leads to differ-
ent feature weights and as a consequence, to dif-
ferent BLEU scores when translating unseen text.
The scores from the 5 runs were then averaged and
with a t-test we calculated the confidence p-values
for the score differences. When these are below
0.05, they confirm that it is statistically likely,
that such scores would occur again in other tun-
ing runs. In terms of BLEU, neither SYSTEM1 nor
SYSTEM2 therefore performs significantly better
or worse than the BASELINE.
In order to show how little the DC labeling ac-
tually affects the BLEU score, we randomized all
connective sense tags in PDTB test section 23 and
translated again 5 times (with the weights from
each tuning run) with both, SYSTEM1 and SYS-
TEM2. With randomized labels, both systems per-
form statistically significantly worse (p = 0.01,
marked with a star in Table 1) than the BASELINE,
but only with an average performance loss of ?0.6
BLEU points. Note that some sense tags might
still have been correct due to randomization.
Test set System BLEU
nt2012 + PDTB 23
BASELINE 17.6
SYSTEM1 17.6
SYSTEM2 17.6
PDTB 23
BASELINE 21.4
SYSTEM1 21.4
SYSTEM2 21.4
PDTB 23 random
SYSTEM1 20.8*
SYSTEM2 20.8*
Table 1: BLEU scores when testing on the com-
bined test set (newstest2012 + PDTB 23); on
PDTB section 23 only (2416 sentences, 923 con-
nectives); and when randomizing the sense tags
(PDTB 23 random), for the BASELINE system and
the two systems using PDTB connective labels:
SYSTEM1: complex labels, SYSTEM2: simplified
labels. When testing on randomized sense labels
(PDTB 23 random), the BLEU scores are statisti-
cally significantly lower than the ones on the cor-
rectly labeled test set (PDTB 23), which is indi-
cated by starred values.
Automatic MT scoring does therefore not reveal
actual changes in translation quality due to DC
usage. In the next section, we manually analyze
46
samples of the translation output by SYSTEM2 that
reached the highest scores observed in some of the
single tuning runs before averaging.
7 Manual Evaluation and Error Analysis
Two human judges went both through two random
samples of SYSTEM2 translations from WSJ sec-
tion 23, namely sentences 1-300 and 1000-2416.
In these sentences, there were 630 observed con-
nectives. The judges counted the translations that
were better, equal and worse in terms of the DCs as
output by SYSTEM2 versus the BASELINE system.
We then summarized the counts over the two sam-
ples and give the scores as ?(%) in Table 2. To
further test if we just had bad samples, the judges
went through another set of translations (1024?
1138), containing 50 DCs, for which the counts
are summarized in Table 2 as well. A translation
was counted as being correct when it generated a
valid CZ connective for the corresponding context,
without grading the rest of the sentences.
Overall, it was found that the number of better
translations is only slightly higher for SYSTEM2
than the ones from the BASELINE system. The
vast majority of DCs was translated correctly by
both the BASELINE and SYSTEM2, and in very few
cases, both systems translated the DCs incorrectly.
SYSTEM2 appeared to systematically repeat one
mistake, namely translating the very frequent con-
nective but preferably with jenz?e, which is correct
but rare in CZ (the primary and default equivalent
for but in CZ is ale). This ?mis-learning? likely
happened to a frequent correspondence of but?
jenz?e in the SMT training data, which then does
not necessarily scale to and be of appropriate style
in the testing data. If one disregards these occur-
rences, SYSTEM2 translates between about 8 and
20% of all connectives better than the BASELINE
(discounted percentages for jenz?e in Table 2). The
results seem therefore to be dependent on the parts
of the test set evaluated and the DCs occurring in
them.
The only slight quantitative improvements and
cases were SYSTEM2 performed worse are most
likely due to the overall scarcity of the PDTB
sense tags (cf. Section 4). Especially for SYS-
TEM1 but to some extent also for SYSTEM2, rare
sense tags such as CONTINGENCYPRAGMATIC-
CAUSE might not be seen often or even not at all in
the SMT training data and therefore not be learned
appropriately to provide good translations for the
test data. In relation to that, simply concatenat-
ing the sense tags onto the connective word-forms
leads to scarcity of the latter, whereas other ways
to include linguistic labels in SMT, such as fac-
tored translation models, would account for the la-
bels as additional translation features, which will
be investigated in future work (Section 8).
In the following, we analyze cases where SYS-
TEM2 translates the connectives better and more
appropriately than the BASELINE. These cases
include highly ambiguous connectives, temporal
DCs with verbal ing-forms and conditionals.
In general, for the very ambiguous EN connec-
tives (e.g. as, when, while), disambiguated for
SYSTEM2 with the PDTB sense tags, we indeed
obtained more accurate translations than those
generated by the BASELINE. One of the human
judges had a close look at 25 randomly sampled
instances of as, taken from the manually evalu-
ated sets mentioned above. In these test cases,
68% of all occurrences of as were better translated
by SYSTEM2 and only 4% of the translations were
degraded when compared to the BASELINE. For
details, see Table 34.
In the following translation example (WSJ
section 2365), and often elsewhere, the BASELINE
system treats the connective as as a preposition
jako with the meaning She worked as a teacher.
This frequent interpretation seems to be learned
quite reasonably from the SMT training data, it is
however incorrect where as actually functions as
a DC. SYSTEM2, in agreement with the tagging,
then correctly generates the causal connective
protoz?e:
SOURCE: In the occupied lands, underground leaders of
the Arab uprising rejected a U.S. plan to arrange Israeli-
Palestinian talks as<CONTINGENCYCAUSE> Shamir op-
posed holding such discussions in Cairo.
BASELINE: *Na okupovany?ch u?zem??ch, podzemn?? vu?dcu?
arabsky?ch povsta?n?? odm??tl americky? pla?n uspor?a?dat
izraelsko-palestinske? rozhovory jako S?amira proti por?a?da?n??
takovy?ch diskus?? v Ka?hir?e.
SYSTEM2: Na okupovany?ch u?zem??ch, podzemn?? vu?dcu?
arabske?ho povsta?n?? odm??tl americky? pla?n uspor?a?dat
izraelsko-palestinske? rozhovory, protoz?e S?amira proti
por?a?da?n?? takovy?ch diskus?? v Ka?hir?e.
DCs can also be translated to other syntactical
constructs available in the target language that
convey the same discourse relation without any
4We included simple occurrences only, i.e. not compound
connectives like as if, as soon as or translations were the con-
nective was dropped. In the PDTB, as can have up to 17
distinct senses, ranging from temporal, causal to concessive
relations.
47
Configuration ?(%) vs. BASELINE Total (%)
Improved Equal Degraded
sentences 1?300 / 1000?2416
630 labeled DCs
SYSTEM2 7.9 75.2 9.4 92.5
not counting 25 x but?jenz?e 8.2 80.3 4.0 92.5
both systems wrong 7.5
100
sentences 1024?1138
50 labeled DCs
SYSTEM2 16 76 6 98
not counting 2 x but?jenz?e 19 77 2 98
both systems wrong 2
100
Table 2: Performance of SYSTEM2 (simplified PDTB tags) when manually counting for improved, equal
and degraded translations compared to the BASELINE, in samples from the PDTB section 23 test set.
explicit DC. For EN/CZ this occurs for DCs such
as before/after/since + Verb in Present Continu-
ous. In CZ, these either should be rendered as a
verbal clause or a nominalization. We accounted
for translations as being well-formed, if the
SMT systems generated one of these possibilities
correctly, i.e. not only the connective/preposition
but also the verb/noun. In CZ, it must be decided
between using a preposition (e.g. pr?ed) or a
connective (e.g. nez?). A good translation would
for example be: before climbing = PREP+NP or
DC+V, and a bad translation: before climbing
= PREP+V/ADJ or DC+NP. The following
example (WSJ section 2381) is a SYSTEM2 output
where the sense tag in English helped to translate
the connective before more correctly by DC+V,
whereas the BASELINE renders this wrongly by
using PREP+ADJ:
SOURCE: Mr. Weisman predicts stocks will
appear to stabilize in the next few days be-
fore<TEMPORALASYNCHRONOUS> declining again,
trapping more investors.
BASELINE: *Pan Weisman pr?edpov??da?, z?e akcie budou
stabilizovat v pr???s?t??ch ne?kolika dnech pr?ed/PREP kle-
saj??c??m/ADJ ope?t odchytu v??ce investoru?.
SYSTEM2: Pan Weisman pr?edpov??da?, z?e akcie bude
stabilizovat, jak se zda?, v pr???s?t??ch ne?kolika dn??, nez?/DC
ope?t klesat/V, zablokova?n?? v??ce investoru?.
A further difficult case in CZ is the binding of
conditionals with personal pronouns, e.g. if I =
kdybych, if you = kdybys, if he/she = kdyby etc.
In the following example (WSJ section 2386), the
BASELINE system completely missed to render
the personal pronoun (but still generated the
correct conditional connective if?pokud), whereas
SYSTEM2 outputs the much better if I?kdybych.
However, apart from the better connective, SYS-
TEM2?s translation is worse than the BASELINE?s,
because the first verb form is misconjugated and
the second verb (will take) is missing:
SOURCE: If<CONTINGENCYCONDITION> I sell now, I?ll
take a big loss.
BASELINE: *Pokud chte?l prodat, ted? budu bra?t s velkou
ztra?tou.
LIT.: If he-wanted to-sell, now I-will take with big-
Instrumental loss-Instrumental.
SYSTEM2: Kdybych se nyn?? proda?vaj??, se z tohohle velkou
ztra?tu.
LIT.: If-I themselves-ReflexPron now they-are selling, Re-
flexPron out-of this big-Accusative loss-Accusative.
From the automatic and manual translation
evaluation, we conclude that using the sense tags
for all 100 connectives in EN is not the most ap-
propriate method, and that only certain connec-
tives such as as, when, while, yet and a few oth-
ers are very problematic in translation due to the
many discourse relations they can signal. In fu-
ture work, we will therefore analyze in more detail
which connectives and which sense labels from the
PDTB should actually be included in the data to
train SMT.
48
BASELINE SYSTEM2 occ. PDTB
jak kdyz? 1 SY
jak kdyz? 1 SY
jelikoz? jelikoz? 1 CA
nebot? nebot? 1 CA
protoz?e protoz?e 2 SY/CO; CA
a protoz?e 1 SY/CO
aby kdyz? 1 SY
jak kdyz? 1 SY
jak protoz?e 1 CA
jako protoz?e 4 SY/CO; CA
jako kdyz? 5 SY; ASY; CA
jako kdy 2 SY
protoz?e kdyz? 1 SY
z?e kdyz? 1 SY
jako jak 1 SY
jako pote?, co 1 SY
Total 25
SYS2 + 68%
SYS2 = 20%
SYS2 ? 4%
both ? 8%
Table 3: Translation outputs for the EN con-
nective as, which was translated more correctly
by SYSTEM2 thanks to the disambiguating sense
tags compared to the BASELINE that often just
produces the prepositional as ? jako. The erro-
neous translations are marked in bold. The PDTB
sense tags indicate the meaning of the CZ trans-
lations and are encoded as follows: Synchrony
(Sy), Asynchrony (Asy), Contingency (Co), Cause
(Ca).
8 Conclusion
We presented experiments for EN/CZ SMT with
a large amount of hand-labeled discourse connec-
tives that are disambiguated in the source language
and training material for MT systems by their
sense tags or discourse relations they signal. This
leads to improved translations in cases where the
source DC is highly ambiguous or where the tar-
get language uses other syntactical constructs than
a connective to convey the discourse relation.
Using all 100 types of EN DCs in the corpus
and/or all the detailed sense tags from the man-
ual annotation most probably lead to the only very
slight improvements for the discourse-aware sys-
tems when measured quantitatively over the whole
test sets. In future work we plan to more thor-
oughly analyze which connectives need to be dis-
ambiguated at which sense granularity level before
implementing them into an SMT system.
For label implementation there also are other
ways worth examining, such as factored transla-
tion models that handle the supplementary linguis-
tic information as separate features and alternative
decoding paths.
Acknowledgments
We are grateful for the funding of this work to the
Swiss National Science Foundation (SNSF) under
the COMTIS Sinergia project, n. CRSI22 127510
(see www.idiap.ch/comtis/), to the Grant
Agency of the Czech Republic (project n.
P406/12/0658) and to the SVV of the Charles
University (project n. 267 314). We would like
to thank Lenka Sa?ndor for her help with manual
translation evaluation.
References
Amal AlSaif. 2012. Human and Automatic Annota-
tion of Discourse Relations for Arabic. Ph.D. thesis,
University of Leeds.
Marine Carpuat and Michel Simard. 2012. The Trou-
ble with SMT Consistency. In Proceedings of the
7th Workshop on Statistical Machine Translation
(WMT), pages 442?449, Montreal, Canada.
Marine Carpuat and Dekai Wu. 2007. Improving Sta-
tistical Machine Translation using Word Sense Dis-
ambiguation. In Proceedings of Joint Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP) and Computational Natural Language
Learning (CoNLL), pages 61?72, Prague, Czech Re-
public.
Marine Carpuat. 2009. One Translation per Discourse.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW), pages 19?27, Singapore.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word Sense Disambiguation Improves Sta-
tistical Machine Translation. In Proceedings of the
45th Annual Meeting of the Association of Com-
putational Linguistics (ACL), pages 33?40, Prague,
Czech Republic.
Pi-Chuan Chang, Dan Jurafsky, and Christopher D.
Manning. 2009. Disambiguating ?DE? for Chinese-
English Machine Translation. In Proceedings of the
Fourth Workshop on Statistical Machine Translation
at the 12th Meeting of the European Chapter of the
Association for Computational Linguistics (EACL),
Athens, Greece.
49
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah
Smith. 2011. Better Hypothesis Testing for Statisti-
cal Machine Translation: Controlling for Optimizer
Instability. In Proceedings of ACL-HLT 2011 (46th
Annual Meeting of the ACL: Human Language Tech-
nologies), Portland, OR.
Laurence Danlos, Die?go Antolinos-Basso, Chloe?
Braud, and Charlotte Roze. 2012. Vers le
FDTB : French Discourse Tree Bank. In Actes de
la confe?rence conjointe JEP-TALN-RECITAL 2012,
volume 2: TALN, pages 471?478, Grenoble, France.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic Models for Dynamic Transla-
tion Model Adaptation. In Proceedings of ACL 2012
(50th Annual Meeting of the Association for Compu-
tational Linguistics, pages 115?119, Jeju, Republic
of Korea.
Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr Sgall,
Silvie Cinkova?, Eva Fuc???kova?, Marie Mikulova?, Petr
Pajas, Jan Popelka, Jir??? Semecky?, Jana S?indlerova?,
Jan S?te?pa?nek, Josef Toman, Zden?ka Ures?ova?, and
Zdene?k Z?abokrtsky?. 2011. Prague Czech-English
Dependency Treebank 2.0. Institute of Formal
and Applied Linguistics, Charles University, Prague,
Czech Republic.
Christian Hardmeier, Joakim Nivre, and Jo?rg Tiede-
mann. 2012. Document-Wide Decoding for Phrase-
Based Statistical Machine Translation. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing and Natural Language
Learning (EMNLP-CoNLL), Jeju, Korea.
Christian Hardmeier. 2013. Discourse in Statistical
Machine Translation. DISCOURS, 11:1?29.
Philipp Koehn and Hieu Hoang. 2007. Factored
Translation Models. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) and Computational
Natural Language Learning (CONLL), pages 868?
876, Prague, Czech Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbs. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of 45th Annual Meeting of the
Association for Computational Linguistics (ACL),
Demonstration Session, pages 177?180, Prague,
Czech Republic.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit X, pages 79?86, Phuket, Thailand.
Jianjun Ma, Degen Huang, Haixia Liu, and Wenfeng
Sheng. 2011. POS Tagging of English Particles for
Machine Translation. In Proceedings of the Thir-
teenth Machine Translation Summit, pages 57?63,
Xiamen, China.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Thomas Meyer and Andrei Popescu-Belis. 2012. Us-
ing Sense-labeled Discourse Connectives for Statis-
tical Machine Translation. In Proceedings of the
EACL 2012 Joint Workshop on Exploiting Synergies
between IR and MT, and Hybrid Approaches to MT
(ESIRMT-HyTra), pages 129?138, Avignon, FR.
Lucie Mladova?, S?a?rka Zika?nova?, Zuzanna Bedr?ichova?,
and Eva Hajic?ova?. 2009. Towards a discourse cor-
pus of Czech. In Proceedings of the Corpus Linguis-
tics Conference, Liverpool, UK.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics (ACL), pages 160?167,
Sapporo, Japan.
Marie-Paule Pe?ry-Woodley, Nicholas Asher, Patrice
Enjalbert, Farah Benamara, Myriam Bras, Ce?cile
Fabre, Ste?phane Ferrari, Lydia-Mai Ho-Dac, Anne
Le Draoulec, Yann Mathet, Philippe Muller, Lau-
rent Pre?vot, Josette Rebeyrolle, Ludovic Tanguy,
Marianne Vergez-Couret, Laure Vieu, and Antoine
Widlo?cher. 2009. ANNODIS: une approche out-
ille?e de l?annotation de structures discursives. In
Actes de la 16e`me Confe?rence sur le Traitement
Automatique des Langues Naturelles (TALN), Paris,
France.
Andrei Popescu-Belis, Thomas Meyer, Jeevanthi
Liyanapathirana, Bruno Cartoni, and Sandrine Zuf-
ferey. 2012. Discourse-level Annotation over Eu-
roparl for Machine Translation: Connectives and
Pronouns. In Proceedings of the eighth interna-
tional conference on Language Resources and Eval-
uation (LREC), Istanbul, Turkey.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0.
In Proceedings of 6th International Conference on
Language Resources and Evaluation (LREC), pages
2961?2968, Marrakech, Morocco.
Manfred Stede. 2004. The Potsdam Commentary Cor-
pus. In Proceedings of the ACL Workshop on Dis-
course Annotation, pages 96?102, Barcelona, Spain.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at Sixteen: Update and
Outlook. In Proceedings of the IEEE Automatic
Speech Recognition and Understanding Workshop,
Waikoloa, Hawaii.
Yuping Zhou and Nianwen Xue. 2012. PDTB-style
Discourse Annotation of Chinese Text. In Proceed-
ings of the 50th Annual Meeting on Association for
Computational Linguistics (ACL), Jeju Island, Ko-
rea.
50
