Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2052?2063, Dublin, Ireland, August 23-29 2014.
Quality Estimation of English-French Machine Translation:
A Detailed Study of the Role of Syntax
Rasoul Kaljahi
??
, Jennifer Foster
?
, Raphael Rubino
??
, Johann Roturier
?
?
NCLT, School of Computing, Dublin City University, Ireland
{rkaljahi, jfoster, rrubino}@computing.dcu.ie
?
Symantec Research Labs, Dublin, Ireland
johann roturier@symantec.com
Abstract
We investigate the usefulness of syntactic knowledge in estimating the quality of English-French
translations. We find that dependency and constituency tree kernels perform well but the error
rate can be further reduced when these are combined with hand-crafted syntactic features. Both
types of syntactic features provide information which is complementary to tried-and-tested non-
syntactic features. We then compare source and target syntax and find that the use of parse trees
of machine translated sentences does not affect the performance of quality estimation nor does
the intrinsic accuracy of the parser itself. However, the relatively flat structure of the French
Treebank does appear to have an adverse effect, and this is significantly improved by simple
transformations of the French trees. Finally, we provide further evidence of the usefulness of
these transformations by applying them in a separate task ? parser accuracy prediction.
1 Introduction
Quality Estimation (QE) for Machine Translation (MT) involves judging the correctness of the output
of an MT system given an input and no reference translation (Blatz et al., 2003; Ueffing et al., 2003;
Specia et al., 2009). An accurate QE-for-MT system would mean that reliable decisions could be made
regarding whether to publish a machine translation as is or to re-direct it to a translator, either for post-
editing or to be translated from scratch. The scores produced by a QE system can also be used to choose
between translations, in a system combination framework or in n-best list reranking. The work presented
here takes place in the context of a wider study, the aim of which is to develop an English-French QE
system so that technical support material that is produced on a daily basis by a company?s English-
speaking customers can be translated automatically into French and made available with confidence to
the company?s French-speaking customer base.
It is reasonable to assume that syntactic features are useful in QE for MT as a way of capturing
the syntactic complexity of the source sentence, the grammaticality of the target translation and the
syntactic symmetry between the source sentence and its translation. This assumption has been borne out
by previous research which has demonstrated the usefulness of syntactic features for English-Spanish
QE (Hardmeier et al., 2012; Rubino et al., 2012). We focus more closely on understanding the role
of syntax by comparing the use of hand-crafted features and tree kernels (Collins and Duffy, 2002;
Moschitti, 2006), and by teasing apart the contribution of target and source syntax.
We find that both tree kernels and manually engineered features produce statistically significantly
better results than a strong set of non-syntactic features provided as a baseline by the organisers of the
2012 WMT shared task on QE for MT (Callison-Burch et al., 2012), and that both types of syntactic
features can be combined fruitfully with this baseline. Furthermore, we show that it is worthwhile to
combine tree kernels with hand-crafted features. Our tree kernel features are the complete set of tree
fragments of both the constituency and dependency trees of the source and target sentences. Our hand-
crafted feature set consists of an initial set of 489 constituency and dependency features which are then
reduced to a set of 144 with no significant loss in performance.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
2052
We then show that source (English) constituency trees significantly outperform target (French) transla-
tion constituency trees in this task. We hypothesise that this is happening because a) the French parser has
a lower accuracy compared to the English, or b) the target trees sentences are harder to parse, represent-
ing, as they do, potentially ill-formed machine translations which may result in noisier parse trees which
are harder to learn from. If the first hypothesis were true, we would expect to see a drop in the accuracy
of our QE system when we use lower-accuracy parses. We do not observe this. If the second hypothesis
were true, we would expect to observe that the target trees were also less useful than the source trees in
the opposite translation direction (French-English). Instead, we find that the target (English) constituency
trees significantly outperform the source (French) constituency trees, suggesting that the difference be-
tween source and target that we observe in the original English-French experiment is related neither to
intrinsic parser accuracy nor to translation direction but rather to the languages/treebanks.
We explore the extent to which the difference between French and English constituency trees is due
to the relatively flatter structure of the French treebank. We use simple transformation heuristics to
introduce more nodes into the French trees and significantly improve the performance. We also apply
these heuristics in a second task, parser accuracy prediction. This task is similar to QE for MT except
we are predicting the quality of a parse tree in the absence of a reference parse tree. We also find here
that the modified trees also outperform the original trees, suggesting that one must proceed with caution
when using French Treebank tree fragments in a machine-learning task.
The paper?s novel contributions are as follows:
1. Evidence that syntactic information is useful in English-French QE for MT and further evidence
that it is useful in QE for MT in general
2. A comparison of two methods of representing syntactic information in QE
3. A more comprehensive set of syntactic features than has been previously been used in QE for MT
4. A comparison of the role of source and target syntax in English-French QE for MT
5. A set of heuristics that can be applied to French Treebank trees resulting in performance improve-
ments in the tasks of both QE for MT and parser accuracy prediction
The rest of this paper is organised as follows: we discuss related work in using syntax in QE in
Section 2, we describe the data in Section 3, and we then go on to describe the QE framework and the
systems built in Section 4. We follow this with an investigation of the role of source and target syntax in
Section 5 before presenting our heuristics to modify the French constituency trees in Section 6.
2 Related Work
Features extracted from parser output have been used before in training QE for MT systems. Quirk
(2004) uses a single syntax-based feature which indicates whether a full parse for the source sentence
could be found. Hardmeier et al. (2012) employ tree kernels to predict the 1-to-5 post-editing cost of a
machine-translated sentence. They use tree kernels derived from syntactic constituency and dependency
trees of the source side (English) and only dependency trees of the translation side (Spanish). The tree
kernels are used both alone and combined with non-syntactic features. The combined setting ranked
second in the 2012 shared task on QE for MT (Callison-Burch et al., 2012). Rubino et al. (2012) explore
a variety of syntactic features extracted from the output of both a hand-crafted broad-coverage gram-
mar/parser and a statistical constituency parser on the WMT 2012 data set. They find that the syntactic
features make an important contribution to the overall system. In a framework for combining QE and
automatic metrics to evaluate MT output, Specia and Gim?enez (2010) use part-of-speech (POS) tag lan-
guage model probabilities of the MT output 3-grams as features for QE and features built upon syntactic
chunks, dependencies and constituent structure to build automatic MT evaluation metrics. Avramidis
(2012) builds a series of models for estimating post-editing effort using syntactic features such as parse
probabilities and syntactic label frequency. In a similar vein, Gamon et al. (2005) use POS tag trigrams,
CFG rules and features derived from a semantic analysis of the MT output to classify it as fluent or
disfluent.
2053
In this work, we compare the use of tree kernels and hand-crafted features extracted from the con-
stituency and dependency trees of the source and target sides of a translation pair, as well as comparing
the role of source and target syntax. In addition, we conduct a more in-depth analysis of these approaches
and compare the utility of syntactic information extracted from the source side and target sides of the
translation.
3 Data
While there is evidence to suggest that predicting human evaluation scores is superior to predicting
automatic metrics in QE for ME (Quirk, 2004), it has also been shown that human judgements are not
necessarily consistent (Snover et al., 2006). A more practical consideration is that human evaluation
exists for just a few language pairs and domains. To the best of our knowledge, the only available
English-to-French data set which contains human judgements of translation quality are as follows:
? CESTA (Hamon et al., 2007), which is selected from the Official Journal of the European Commis-
sion and also from the health domain. In addition to the domain (and style) difference to newswire
(the domain on which our parsers are trained), a major stumbling block which prevents us from
using this data set is its small size: only 1135 segments have been evaluated manually.
? WMT 2007 (Callison-Burch et al., 2007), which contains only 302 distinct source segments (each
with approx. 5 translations) only half of which is in the news domain.
? FAUST
1
, which is out-of-domain and difficult to apply to our setting as the evaluations and post-
edits are user feedbacks, often in the form of phrases/fragments.
Thus, we instead attempt to predict automatic metric scores as there is a sufficient amount of parallel
text for our language pair and domain. We use BLEU
2
(Papineni et al., 2002), TER
3
(Snover et al., 2006)
and METEOR
4
(Denkowski and Lavie, 2011), which are the most-widely used MT evaluation metrics.
All metrics are applied at the segment level.
5
We randomly select 4500 parallel segments from the News development data sets released for the
WMT13 translation task (Bojar et al., 2013). In order to be independent of any one translation system,
we translate the data set with the following three systems and randomly choose 1500 distinct segments
from each:
? ACCEPT
6
: a phrase-based Moses system trained on training sets of WMT12 releases of Europarl
and News Commentary plus data from Translators Without Borders (TWB)
? SYSTRAN: a proprietary rule-based system
? Bing
7
: an online translation system
The data set is randomly split into 3000 training, 500 development and 1000 test segments. We use the
development set for tuning model parameters and building hand-crafted feature sets, and the test set for
testing model performance and analyses purposes.
4 Syntax-based QE
One way to employ syntactic information in a machine-learning task is to manually compile a set of
features that can be extracted automatically from a parse tree. An example of one such feature is the
label of the root of the tree. Another method is to directly use these trees in a tree kernel (Collins and
Duffy, 2002; Moschitti, 2006). This approach allows exponentially-sized feature spaces (e.g. all subtrees
1
http://www.faust-fp7.eu/faust/Main/DataReleases
2
Version 13a of MTEval script was used at the segment level.
3
TER COMpute 0.7.25: http://www.cs.umd.edu/
?
snover/tercom/
4
METEOR 1.4: http://www.cs.cmu.edu/
?
alavie/METEOR/
5
We present 1-TER to be more easily comparable to BLEU and METEOR. There is no upper bound for TER scores unlike
the other two metrics. Scores higher than 1 occur when the number of errors is higher than the segment length. To avoid this,
scores higher than 1 are cut-off to 1 before being converted to 1-TER.
6
http://www.accept.unige.ch/Products/D_4_1_Baseline_MT_systems.pdf
7
http://www.bing.com/translator
2054
of a tree) to be efficiently modelled using dynamic programming and has shown to be effective in many
natural language processing tasks including parsing and named entity recognition (Collins and Duffy,
2002), semantic role labelling (Moschitti, 2006), sentiment analysis (Wiegand and Klakow, 2010) and
QE for MT (Hardmeier et al., 2012). Although there can be overlap between the information captured by
the two approaches, each can capture information that the other one cannot. In addition, while tree ker-
nels involve minimal feature engineering, hand-crafted features offer more flexibility. Moschitti (2006)
shows that combining the two is beneficial. We use both hand-crafted features and tree kernels, applied
separately and combined together.
For parsing the English and French data into their constituency structures, a PCFG-LA parser
8
is
used. We train the English parser on the training section of the Wall Street Journal (WSJ) section of the
Penn Treebank (PTB) (Marcus et al., 1993). The French parser is trained on the training section of the
French Treebank (FTB) (Abeill?e et al., 2003). We obtain dependency parses by converting the English
constituency parses using the Stanford converter (de Marneffe and Manning, 2008) and the French
parses using Const2Dep (Candito et al., 2010). We evaluate the performance of the QE models using
Root Mean Square Error (RMSE) and Pearson correlation coefficient (r). To compute the statistical
significance of the performance differences between QE models, we use paired bootstrap resampling
following Koehn (2004). We randomly resample (with replacement) a set of N instances from the
predictions of each of the two given systems, where N is the size of the test set. We repeat this sampling
N times and count the number of times each of the two settings is better in terms of each measure (RMSE
and Pearson r). If a setting is better more than 95% of the time, we consider it statistically significant
at p < 0.05.
In the following sections, we first describe our baseline systems and then the quality estimation sys-
tems build using tree kernels, hand-crafted features and a combination of both.
4.1 Baseline QE Systems
In order to verify the usefulness of syntax-based QE, we build two baselines. The first baseline (BM) uses
the mean of the segment-level evaluation scores in the training set for all instances. In the second baseline
(BW), the 17 baseline features of the WMT12 QE Shared Task are used. BW is considered a strong baseline
as the system that used only these features was ranked higher than many of the participating systems.
We use support vector regression implemented in the SVMLight toolkit
9
to build BW. The Radial Basis
Function (RBF) kernel is used. The results for both baselines are presented in the first two rows of
Table 1. Since BW is a stronger baseline than BM, we will compare all syntax-based systems to BW only.
4.2 Syntax-based QE with Tree Kernels
Tree kernels are kernel functions that compute the similarity between two instances of data represented
as trees based on the number of common fragments between them. Therefore, the need for explicitly en-
coding an instance in terms of manually-designed and extracted features is eliminated, while benefitting
from a very high-dimensional feature space. Moschitti (2006) introduces an efficient implementation
of tree kernels within a support vector machine framework. Instead of extracting all possible tree frag-
ments, the algorithm compares only tree fragments rooted in two similar nodes. This algorithm is made
available through SVMLight-TK software
10
, which is used in this work.
In order to extract tree kernels from dependency trees, the labels on the arcs must be removed. Fol-
lowing Tu et al. (2012), the nodes in the resulting tree representation are word forms and dependency
relations, omitting POS tag information. An example is shown in Figure 1. A word is a child of its
dependency relation to its head. The dependency relation in turn is the child of the head word. This
continues until the root of the tree.
Based on preliminary experiments on our development set, we use subset tree kernels, where the tree
fragments are subtrees rooted at any node in the tree so that no production rule expanding a node in the
8
https://github.com/CNGLdlab/LORG-Release. The Lorg parser is very similar to the Berkeley parser (Petrov
et al., 2006), the main difference being its unknown word handling mechanism (Attia et al., 2010).
9
http://svmlight.joachims.org/
10
http://disi.unitn.it/moschitti/Tree-Kernel.htm
2055
BLEU 1-TER METEOR
RMSE r RMSE r RMSE r
BM 0.1626 0 0.1965 0 0.1657 0
BW 0.1601 0.1766 0.1949 0.1565 0.1625 0.2047
TK 0.1581 0.2437 0.1888 0.2774 0.1595 0.2715
BW+TK 0.1570 0.2696 0.1879 0.2939 0.1576 0.3111
HC 0.1603 0.1998 0.1913 0.2365 0.1610 0.2516
BW+HC 0.1587 0.2418 0.1899 0.2611 0.1585 0.2964
SyQE 0.1577 0.2535 0.1887 0.2797 0.1594 0.2743
BW+SyQE 0.1568 0.2802 0.1879 0.2937 0.1576 0.3127
Table 1: QE performances measured by RMSE and Pearson r; BM: Mean baseline, BW: WMT 17 base-
line features, TK: tree kernels, HC: hand-crafted features, SyQE: full syntax-based systems (TK+HC).
Statistically significantly better scores compared to their counterpart (upper row in the row block) are in
bold.
rootcamecc      advmod      nsubj      punctAnd       then           era            .                      det      amod                        the    American 
Figure 1: Tree Kernel Representation of Dependency Structure for And then the American era came.
subtree is split. Unlike subtree kernels, subset tree kernels allow tree fragments with non-terminals as
leaves. We tune the C parameter for Pearson r on the development set, with all other parameters left as
default.
We build a system with all four parse trees for every training instance, which includes the constituency
and dependency trees of the source and target side of the translation. The third row of Table 1 shows
the performance of this system which is named TK. The results achieved using this system represent a
statistically significant improvement over the BW baseline results. In order to examine their complemen-
tarity, we combine these tree kernels and the baseline features (BW+TK) in the fourth row of Table 1.
This combined system performs better than the two individual systems.
While BLEU prediction is the most accurate (lowest RMSE), METEOR prediction appears to be the
easiest to learn (highest Pearson r). TER prediction seems to be more difficult than BLEU and METEOR
prediction, especially in terms of prediction error. This is probably related to the distribution of each of
these metric scores in our data set. The standard deviations (?) of BLEU, TER and METEOR scores are
0.1620, 0.1943 and 0.1652 respectively. The substantially higher ? of TER scores makes them harder to
predict accurately leading to higher prediction error.
4.3 Syntax-based QE with Hand-crafted Features
We design a set of constituency and dependency feature types, some of which have previously been used
by the works described in Section 2 and some introduced here. Each feature type contains at least two
features, one extracted from the source and the other from the translation. Numerical feature types can
be further instantiated by extracting the ratio and differences between the source and target side feature
values. Some feature types are parametric meaning that they can be varied by changing the value of a pa-
rameter. For example, the non-terminal label is a parameter for the non-terminal-label-count
2056
Constituency
?1 Label of the root node of the constituency tree
2 Height of the constituency tree which is the number of edges from root node to the farthest terminal (leaf) node
?3 Number of nodes in the constituency tree
4 Log probability of the constituency parse assigned by the parser
?5 Parseval F
1
score of the tree with respect to a tree produced by the Stanford parser (Klein and Manning, 2003)
?6 Right hand side of the CFG production rule expanding the root node
7 All non-lexical and lexical CFG production rules expanding the tree nodes
?8 Average arity of the non-lexical CFG production rules expanding the constituency tree nodes
9 Counts of each non-terminal label in the tree
?10 POS unigrams, 3-grams and 5-grams
11 POS n-gram scores against language models trained on the POS tags of the respective treebanks using the SRILM
toolkit (http://www.speech.sri.com/projects/srilm/) with Witten-Bell smoothing
?12 Counts of each 12 universal POS tags (Petrov et al., 2012)
?13 Location of the first verb in the sentence in terms of the token distance from the beginning
?14 Average number of POS n-grams in each n-gram frequency quartile of the POS corpora of the respective treebanks
Dependency
?1 POS tag of the top node (dependent of the dummy root node) of the dependency tree
?2 Number of dependents of the top node
?3 Sequence of all dependency relations which modify the top node
?4 Sequence of the POS tags of the dependents of the top node
?5 Average number of dependents per node
?6 Height of the tree computed in the same way for the constituency tree
?7 3- and 5-gram sequences of dependency relations of the tokens to their head
?8 Number of most frequent dependency relations in our News training set
?9 Dependency relation n-gram scores against language models trained on the respective treebanks for each language
?10 Average number of dependency relation n-grams in each n-gram frequency quartile of the respective treebanks
?11 Pairs of tokens and their dependency relations to their head
Table 2: Constituency and dependency feature types
feature type. Therefore, it instantiates as several features, one for each non-terminal-label.
As in BW, we use support vector machines (SVM) to build the QE systems using these hand-crafted
features. We keep only those features which fire for more than a threshold which is set empirically on
the development set. Table 2 lists our syntax-based feature types and their descriptions. Those that have,
to the best of our knowledge, not been used in QE for MT before are marked with an asterisk.
The total number of feature-value pairs in the full feature set is 489. Since this feature set is large
and contains many sparse features, we attempt to reduce it through ablation experiments in which we
directly compare the effect of leaving out features that we suspect may be redundant. For example, we
investigate whether either the ratio or difference of the source and target numerical features or both of
them are redundant by building three systems, one without ratio features, one without difference features
and one with neither. This process is also carried out for log probability and perplexity features, original
and universal POS-tag-based features, n-gram and language model score features, lexical and non-lexical
CFG rules, and n-gram orders (i.e. 3-gram vs. 5-gram features). This process proved useful: we found,
for example, that either 3- or 5-grams worked better than both together and features based on universal
POS tags better than those based on original POS tags.
The final reduced feature set contains 144 features-value pairs. We build one QE system with all 489
features HC-all and one with the reduced set of 144 features HC . Table 3 compares the performance on
the development and test set. The system with the reduced feature set performs consistently better than
the HC-all system on the development set, mostly with statistically significant differences. However,
on the test set, the performance degrades albeit not statistically significantly. Considering a more than
70% reduction in feature set size, this relatively small degradation is tolerable. We use the reduced
feature set as our hand-crafted feature set for the rest of the work.
Compared to TK in Table 1 (third and fourth versus fifth and sixth rows), the performances are lower
for all MT metrics, though not statistically significantly. It is worth noting that we observed an opposite
2057
BLEU 1-TER METEOR
RMSE r RMSE r RMSE r
Development Set
HC-all 0.1567 0.3026 0.1851 0.2746 0.1575 0.2996
HC 0.1540 0.3398 0.1819 0.3263 0.1547 0.3452
Test Set
HC-all 0.1603 0.2108 0.1902 0.2510 0.1607 0.2493
HC 0.1603 0.1998 0.1913 0.2365 0.1610 0.2516
Table 3: QE performance with all hand-crafted syntactic features HC-all and the reduced feature set
HC. Statistically significantly better scores compared to their counterpart (upper row) are in bold.
RMSE r
TK-CD-ST 0.1581 0.2437
TK-CD-S 0.1584 0.2294
TK-CD-T 0.1597 0.2101
TK-C-S 0.1583 0.2312
TK-C-T 0.1608 0.1479
TK-D-S 0.1598 0.1869
TK-D-T 0.1598 0.2102
Table 4: BLEU prediction performances with tree kernels of only source S or translation T side trees.
The scores in bold are statistically better than their counterparts in the same row block. The original
result with source and target combined is provided for reference in the first row.
behaviour on the development set, where hand-crafted features largely outperform tree kernels. This
suggests that the tree kernels are more generalisable. We also combine these features with the WMT
17 baseline features (BW+HC). This combination also improves over both syntax-based and baseline
systems, confirming again the usefulness of syntactic information in addition to surface features.
We combine tree kernels and hand-crafted features to build a full syntax-based QE system (SyQE),
which improves over both TK and HC (Table 1) . The improvements for TER and METEOR prediction
are slight but statistically significant for BLEU prediction. This system is also combined with BW in
BW+SyQE (the last row of Table 1), resulting in statistically significant gains for all metrics.
5 Source and Target Syntax in Syntax-based QE
We now turn our attention to the parts played by source and target syntax in QE for MT. To save space,
we present only the BLEU scores for the tree kernel systems. Table 4 shows the results achieved by
systems built using either the source or target side of the translations.
At a glance, it can be seen that the source side constituency tree kernels outperform the target side
ones, while the opposite is the case for dependency tree kernels. The differences for constituency trees
are however substantially bigger. When both constituency and dependency trees are combined, the source
side trees perform better (TK-CD-S vs. TK-CD-T).
The following three hypotheses could explain this difference between TK-C-S and TK-C-T:
1. The Role of Parser Accuracy: The fact that French parsing models do not reach the high Parseval
F1s achieved by English parsing models could explain the difference in usefulness between the
French and English consistuency trees. On the standard parsing test sets, the English parsing model
achieves an F1 of 89.6 and the French an F
1
of 83.4.
2. Parsing Machine Translation Output: The difference between the source and target could be
happening because the target side is machine translation output and (presumably) represents a lower
2058
Sombre  Matter  Affects  de  vol  Probes   spatiale
NP
NP
NP
PP
SENT
NC
ET ET P
ET ET ET
Figure 2: Parse tree of the machine translation of Dark Matter Affects Flight of Space Probes to French
quality set of sentences than the source (see Figure 2 for an example of a parse tree for a poor
translation).
3. Differences in Annotation Strategies: The difference between the source and target could be due
to the idiosyncrasies of the underlying treebanks which is not carried over via the conversion tools
to the dependency structure.
Hypotheses 1 and 2 relate the usefulness of parse trees in QE to the intrinsic quality of the parse trees.
French constituency trees are less accurate than English ones, either because the French parsing model
is not as accurate as the English one (Hypothesis 1) or because the possibly ungrammatical nature of the
French parsing input adversely affects the quality of the parse tree (Hypothesis 2). Although this low
quality would be expected to affect the dependency trees in the same way since they are directly derived
from the consistency trees, this is not the case and it appears that the problematic aspects of the French
parses are abstracted away from the dependency trees.
To test the first hypothesis, we investigate the role of parser accuracy in QE. For both languages, we
substitute the standard parsing models used in all our prior experiments with ?lower-accuracy? mod-
els trained using only a fraction of the training data (following Quirk and Corston-Oliver (2006)). The
English parsing model achieves an F
1
of 72.5 and the French an F
1
of 66.5, representing drops of ap-
proximately 17 points from the original models. The RMSE and Pearson r of the new QE model are
0.1583 and 0.2350 compared to 0.1581 and 0.2437 of the one trained with original trees (see also the
third row of Table 1). These results show that the use of these lower-accuracy models has only a minimal
and statistically insignificant effect on QE performance, suggesting that intrinsic parser accuracy is not
the reason why the target constituency trees are less useful than the source constituency trees.
11
To investigate the second hypothesis, we switch the translation direction to French-to-English. There-
fore, we now parse the well-formed French input sentences and the machine-translated English segments.
If the second hypothesis were true, the target side parse trees in this direction would still underperform
the source side ones. The results are shown in Table 5. All the systems using target trees outperform
those using source trees. The difference between source and target in the models that use constituency
trees is especially substantial and statistically significant. Thus, it is apparent that the suspected lower
quality of constituency parse trees of MT output is not the reason for the lower QE performance.
We now seek the answer in our third hypothesis, i.e. in the difference between the annotation schemes
of the PTB and the FTB. One major difference, noted by, for example, Schluter and van Genabith (2007),
is that the FTB has a relatively flatter structure. It lacks a verb phrase (VP) node and phrases modifying
the verb are the sibling of the verb nucleus. We investigate this further in the next section.
6 Modifying French Parse Trees
In order to test whether the annotation strategy is a reason for the lower performance of French con-
stituency tree kernels, we apply a set of three heuristics which introduce more structure to the French
parse trees (1&2) or simply make them more PTB-like (3):
? Heuristic 1 automatically adds a VP node above the verb node (VN) and at most 3 of its immediate
adjacent nodes if they are noun or prepositional phrases (NP or PP).
11
See (Kaljahi et al., 2013) for a more detailed exploration of the role of parser accuracy in QE for MT.
2059
RMSE r
TK-FE/CD-ST 0.1561 0.2334
TK-FE/CD-S 0.1574 0.1830
TK-FE/CD-T 0.1559 0.2423
TK-FE/C-S 0.1581 0.1578
TK-FE/C-T 0.1556 0.2336
TK-FE/D-S 0.1577 0.1655
TK-FE/D-T 0.1579 0.1886
Table 5: BLEU prediction performances with tree kernels for Fr-En direction (FE) (C: constituency, D:
dependency, S: source, T: translation)
RMSE r
TK-C-T 0.1608 0.1479
TK-C-T
m
0.1591 0.2143
TK-CD-ST 0.1581 0.2437
TK-CD-ST
m
0.1574 0.2609
Table 6: QE with tree kernels using original and modified French trees (
m
)
? Heuristic 2 stratifies some of the production rules in the tree by grouping together every two equal
adjacent POS tags under a new node with a tag made of the POS tag suffixed with St.
? Heuristic 3 moves coordinated nodes (the immediate left sibling of the COORD node) under COORD.
Figure 3 shows examples of the application of each of these methods. We apply these heuristics to
the parsed MT output in the English-French translation direction and rebuild the tree kernel system with
translation side constituency trees (TK-C-T) and the full tree kernel system (TK-CD-ST) with the mod-
ified trees. The results are presented in Table 6. Despite the possibility of introducing linguistic errors,
these heuristics yield a statistically significant improvement in QE performance. Unsurprisingly, the
changes are bigger for the system with only translation side constituency trees as in the full system there
are three other tree types involved. These results suggest that the structure of the French constituency
trees is a factor in the lower performance of its tree kernels in QE.
12
The gain achieved by applying these heuristics is related to the fact that there are more similar frag-
ments extracted from the modified structure which are useful for the tree kernel system. For example, in
the original top left tree in Figure 3, there is no chance that a fragment consisting only of VN and NP ?
a very common structure and thus useful in calculating tree similarity ? will be extracted by the subset
tree kernel. The reason is that this kernel type does not allow the production rule to be split (in this case
the rule expanding the S node). However, after applying Heuristic 1, the fragment equivalent to VP ->
VN NP production rule can be easily extracted. Among the three heuristics, the first one contributes the
largest part of the improvement; the other two have a very slight effect according to the results of their
individual application, though they contribute to the overall performance when all three are combined.
The success of using modified French trees in improving tree kernel performance may of course de-
pend on the data set and even the task in hand, and may not be generalisable. We next explore this
question by applying the modification to a different task and a different data set.
6.1 Parser Accuracy Prediction
The task we choose is parser accuracy prediction, the aim of which is to predict the accuracy of a parse
tree without a reference (QE for parsing). The task was previously explored for English by Ravi et al.
12
We also see a slightly smaller improvement for the hand-crafted features using the modified French trees. The combina-
tion of tree kernels and hand-crafted features with the modified trees leads to a statistically significant improvement over the
combination with the original trees.
2060
Figure 3: Application of tree modification heuristics on example French translation parse trees
RMSE r
PAP 0.1239 0.4035
PAP
m
0.1233 0.4197
Table 7: Parser Accuracy Prediction (PAP) performance with tree kernels using original and modified
French trees (
m
)
(2008). We build a tree kernel model to predict the accuracy of French parses. To train the system, we
parse the training section of FTB with our French parser and score them using F
1
. We use the FTB
development set to tune the SVM C parameter and test the model on the FTB test set. Two parser
accuracy prediction models are then built using this setting, one with the original parse trees and the
second with the modified parse trees produced using the three heuristics listed above. The results are
presented in Table 7.
Both RMSE and Pearson r improve with the modified trees, where the r improvement is statistically
significant. Although the improvement we observe is not as large as the one we observed for the QE for
MT task, the results add weight to our claim that the structure of the FTB trees should be optimised for
use in tree kernel learning.
7 Conclusion
We analysed the utility of syntactic information in QE of English-French MT and found it useful both
individually and combined with standard QE features. We found that tree kernels are a convenient and
effective way of encoding syntactic knowledge but that our hand-crafted feature set also brings additional,
useful information. As a result of comparing the role of source and target syntax, we also found that the
constituent structure in the FTB could be amended to be more useful in QE for MT and parser accuracy
prediction. Now that we have explored the role of syntax in this project, our next step is try to further
improve our QE system by adding semantic information. However, there are many other ways in which
the research in this paper could be further extended. Our focus is on the language pair English-French
and the QE task but it would certainly be interesting to perform a similar analysis on the role of syntax
in QE for other language pairs, or to investigate the impact of French tree modification on other tasks.
2061
Acknowledgments
This research has been supported by the Irish Research Council Enterprise Partnership Scheme (EP-
SPG/2011/102 and EPSPD/2011/135) and the computing infrastructure of the Centre for Next Gener-
ation Localisation at Dublin City University. We are grateful to Djam?e Seddah for useful discussions
about the French Treebank. We also thank the reviewers for their helpful comments.
References
Anne Abeill?e, Lionel Cl?ement, and Franc?ois Toussenel. 2003. Building a treebank for French. In Treebanks:
Building and Using Syntactically Annotated Corpora. Kluwer Academic Publishers.
Mohammed Attia, Jennifer Foster, Deirdre Hogan, Joseph Le Roux, Lamia Tounsi, and Josef van Genabith. 2010.
Handling unknown words in statistical latent-variable parsing models for Arabic, English and French. In Pro-
ceedings of the 1st Workshop on Statistical Parsing of Morphologically Rich Languages.
Eleftherios Avramidis. 2012. Quality estimation for machine translation output using linguistic analysis and
decoding features. In Proceedings of WMT.
John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. 2003. Confidence estimation for machine translation. In JHU/CLSP Summer Workshop Final
Report.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 workshop on statistical machine
translation. In Proceedings of the 8th WMT.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. (meta-)
evaluation of machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation,
pages 136?158.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings
of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh WMT.
Marie Candito, Benot Crabb, and Pascal Denis. 2010. Statistical French dependency parsing: treebank conversion
and first results. In Proceedings of LREC.
Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete
structures, and the voted perceptron. In Proceedings of the ACL.
Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representa-
tion. In Proceedings of the COLING Workshop on Cross-Framework and Cross-Domain Parser Evaluation.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation
of machine translation systems. In Proceedings of WMT.
Michael Gamon, Anthony Aue, and Martine Smets. 2005. Sentence-level MT evaluation without reference trans-
lations: beyond language modeling. In EAMT.
Olivier Hamon, Antony Hartley, Andr?ei Popescu-Belis, and Khalid Choukri. 2007. Assessing human and auto-
mated quality judgments in the french MT evaluation campaign CESTA. In Proceedings of the MT Summit.
Christian Hardmeier, Joakim Nivre, and J?org Tiedemann. 2012. Tree kernels for machine translation quality
estimation. In Proceedings of the WMT.
Rasoul Samed Zadeh Kaljahi, Jennifer Foster, Raphael Rubino, Johann Roturier, and Fred Hollowood. 2013.
Parser accuracy in quality estimation of machine translation: A tree kernel approach. In Proceedings of IJCNLP.
Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of the ACL.
Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of
English: the Penn Treebank. Computational Linguistics, 19(2):313?330.
Alessandro Moschitti. 2006. Making tree kernels practical for natural language learning. In Proceedings of EACL.
2062
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation
of machine translation. In Proceedings of the ACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact and interpretable
tree annotation. In Proceedings of the 21st COLING-ACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of
LREC.
Chris Quirk and Simon Corston-Oliver. 2006. The impact of parse quality on syntactically-informed statistical
machine translation. In Proceedings of EMNLP.
Chris Quirk. 2004. Training a sentence-level machine translation confidence measure. In Proceedings of LREC.
Sujith Ravi, Kevin Knight, and Radu Soricut. 2008. Automatic prediction of parser accuracy. In Proceedings of
EMNLP.
Raphael Rubino, Jennifer Foster, Joachim Wagner, Johann Roturier, Rasoul Kaljahi, and Fred Hollowood. 2012.
DCU-Symantec submission for the WMT 2012 quality estimation task. In Proceedings of WMT.
Natalie Schluter and Josef van Genabith. 2007. Preparing, restructuring, and augmenting a french treebank:
Lexicalised parsers or coherent treebanks? In Proceedings of the 10th Conference of the Pacific Association for
Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of transla-
tion edit rate with targeted human annotation. In Proceedings of AMTA.
Lucia Specia and Jes?us Gim?enez. 2010. Combining confidence estimation and reference-based metrics for seg-
ment level mt evaluation. In Proceedings of AMTA.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc Dymetman, and Nello Cristianini. 2009. Estimating the
sentence-level quality of machine translation systems. In EAMT, pages 28?35.
Zhaopeng Tu, Yifan He, Jennifer Foster, Josef van Genabith, Qun Liu, and Shouxun Lin. 2012. Identifying high-
impact sub-structures for convolution kernels in document-level sentiment classification. In Proceedings of the
ACL.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. 2003. Confidence measures for statistical machine transla-
tion. In Machine Translation Summit IX.
Michael Wiegand and Dietrich Klakow. 2010. Convolution kernels for opinion holder extraction. In Proceedings
of NAACL-HLT.
2063
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 87?92,
Dublin, Ireland, August 23-24 2014.
Semantic Role Labelling with minimal resources:
Experiments with French
Rasoul Kaljahi
??
, Jennifer Foster
?
, Johann Roturier
?
?
NCLT, School of Computing, Dublin City University, Ireland
{rkaljahi, jfoster}@computing.dcu.ie
?
Symantec Research Labs, Dublin, Ireland
johann roturier@symantec.com
Abstract
This paper describes a series of French se-
mantic role labelling experiments which
show that a small set of manually anno-
tated training data is superior to a much
larger set containing semantic role labels
which have been projected from a source
language via word alignment. Using uni-
versal part-of-speech tags and dependen-
cies makes little difference over the orig-
inal fine-grained tagset and dependency
scheme. Moreover, there seems to be no
improvement gained from projecting se-
mantic roles between direct translations
than between indirect translations.
1 Introduction
Semantic role labelling (SRL) (Gildea and Juraf-
sky, 2002) is the task of identifying the predicates
in a sentence, their semantic arguments and the
roles these arguments take. The last decade has
seen considerable attention paid to statistical SRL,
thanks to the existence of two major hand-crafted
resources for English, namely, FrameNet (Baker
et al., 1998) and PropBank (Palmer et al., 2005).
Apart from English, only a few languages have
SRL resources and these resources tend to be of
limited size compared to the English datasets.
French is one of those languages which suffer
from a scarcity of hand-crafted SRL resources.
The only available gold-standard resource is a
small set of 1000 sentences taken from Europarl
(Koehn, 2005) and manually annotated with Prop-
bank verb predicates (van der Plas et al., 2010b).
This dataset is then used by van der Plas et al.
(2011) to evaluate their approach to projecting the
SRLs of English sentences to their translations
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
in French. They additionally build a large, ?ar-
tificial? or automatically labelled dataset of ap-
proximately 1M Europarl sentences by projecting
the SRLs from English sentences to their French
translations and use it for training an SRL system.
We build on the work of van der Plas et al.
(2010b) by answering the following questions: 1)
How much artificial data is needed to train an
SRL system? 2) Is it better to use direct trans-
lations than indirect translations, i.e. is it better
to use for projection a source-target pair where
the source represents the original sentence and the
target represents its direct translation as opposed
to a source-target pair where the source and tar-
get are both translations of an original sentence
in a third language? 3) Is it better to use coarse-
grained syntactic information (in the form of uni-
versal part-of-speech tags and universal syntactic
dependencies) than to use fine-grained syntactic
information? We find that SRL performance lev-
els off after only 5K training sentences obtained
via projection and that direct translations are no
more useful than indirect translations. We also
find that it makes very little difference to French
SRL performance whether we use universal part-
of-speech tags and syntactic dependencies or more
fine-grained tags and dependencies.
The surprising result that SRL performance lev-
els off after just 5K training sentences leads us
to directly compare the small hand-crafted set of
1K sentences to the larger artificial training set.
We use 5-fold cross-validation on the small dataset
and find that the SRL performance is substantially
higher (>10 F
1
in identification and classification)
when the hand-crafted annotations are used.
2 Related Work
There has been relatively few works in French
SRL. Lorenzo and Cerisara (2012) propose a clus-
tering approach for verb predicate and argument
labelling (but not identification). They choose
87
VerbNet style roles (Schuler, 2006) and manu-
ally annotate sentences with them for evaluation,
achieving an F
1
of 78.5.
Gardent and Cerisara (2010) propose a method
for semi-automatically annotating the French de-
pendency treebank (Candito et al., 2010) with
Propbank core roles (no adjuncts). They first
manually augment TreeLex (Kup?s?c and Abeill?e,
2008), a syntactic lexicon of French, with seman-
tic roles of syntactic arguments of verbs (i.e. verb
subcategorization). They then project this anno-
tation to verb instances in the dependency trees.
They evaluate their approach by performing error
analysis on a small sample and suggest directions
for improvement. The annotation work is however
at its preliminary stage and no data is published.
As mentioned earlier, van der Plas et al. (2011)
use word alignments to project the SRLs of the
English side of EuroParl to its French side result-
ing in a large artificial dataset. This idea is based
on the Direct Semantic Transfer hypothesis which
assumes that a semantic relationship between two
words in a sentence can be transferred to any
two words in the translation which are aligned to
these source-side words. Evaluation on their 1K
manually-annotated dataset shows that a syntactic-
semantic dependency parser trained on this artifi-
cial data set performs significantly better than di-
rectly projecting the labelling from its English side
? a promising result because, in a real-world sce-
nario, the English translations of the French data
to be annotated do not necessarily exist.
Pad?o and Lapata (2009) also make use of word
alignments to project SRLs from English to Ger-
man. The word alignments are used to compute
the semantic similarity between syntactic con-
stituents. In order to determine the extent of se-
mantic correspondence between English and Ger-
man, they manually annotate a set of parallel sen-
tences and find that about 72% of the frames and
92% of the argument roles exist in both sides, ig-
noring their lexical correspondence.
3 Datasets, SRL System and Evaluation
We use the two datasets described in (van der Plas
et al., 2011) and the delivery report of the Clas-
sic project (van der Plas et al., 2010a). These
are the gold standard set of 1K sentences which
was annotated by manually identifying each verb
predicate, finding its equivalent English frameset
in PropBank and identifying and labelling its ar-
guments based on the description of the frame-
set (henceforth known as Classic1K), and the syn-
thetic dataset consisting of more than 980K sen-
tences (henceforth known as Classic980K), which
was created by word aligning an English-French
parallel corpus (Europarl) using GIZA++ (Och
and Ney, 2003) and projecting the French SRLs
from the English SRLs via the word alignments.
The joint syntactic-semantic parser described in
(Titov et al., 2009) was used to produce the En-
glish SRLs and the dependency parses of the
French side were produced using the ISBN parser
described in (Titov and Henderson, 2007).
We use LTH (Bj?orkelund et al., 2009), a
dependency-based SRL system, in all of our ex-
periments. This system was among the best-
performing systems in the CoNLL 2009 shared
task (Haji?c et al., 2009) and is straightforward to
use. It comes with a set of features tuned for each
shared task language (English, German, Japanese,
Spanish, Catalan, Czech, Chinese). We compared
the performance of the English and Spanish fea-
ture sets on French and chose the former due to its
higher performance (by 1 F
1
point).
To evaluate SRL performance, we use the
CoNLL 2009 shared task scoring script
1
, which
assumes a semantic dependency between the argu-
ment and predicate and the predicate and a dummy
root node and then calculates the precision (P), re-
call (R) and F
1
of identification of these dependen-
cies and classification (labelling) of them.
4 Experiments
4.1 Learning Curve
The ultimate goal of SRL projection is to build a
training set which partially compensates for the
lack of hand-crafted resources. van der Plas et
al. (2011) report encouraging results showing that
training on their projected data is beneficial over
directly obtaining the annotation via projection
which is not always possible. Although the quality
of such automatically-generated training data may
not be comparable to the manual one, the possi-
bility of building much bigger data sets may pro-
vide some advantages. Our first experiment inves-
tigates the extent to which the size of the synthetic
training set can improve performance.
We randomly select 100K sentences from Clas-
sic980K, shuffle them and split them into 20 sub-
1
https://ufal.mff.cuni.cz/
conll2009-st/eval09.pl
88
0
1000
0
2000
0
3000
0
4000
0
5000
0
6000
0
7000
0
8000
0
9000
0
1000
0020
30
40
50
60
70
80
PrecisionRecallF1
Figure 1: Learning curve with 100K training data
of projected annotations
0
1000
0
2000
0
3000
0
4000
0
5000
0
6000
0
7000
0
8000
0
9000
0
1000
0020
30
40
50
60
70
80
PrecisionRecallF1
Figure 2: Learning curve with 100K training data
of projected annotations on only direct translations
sets of 5K sentences. We then split the first 5K into
10 sets of 500 sentences. We train SRL models
on the resulting 29 subsets using LTH. The per-
formance of the models evaluated on Classic1K
is presented in Fig. 1. Surprisingly, the best F
1
(58.7) is achieved by only 4K sentences, and af-
ter that the recall (and consequently F
1
) tends to
drop though precision shows a positive trend, sug-
gesting that the additional sentences bring little in-
formation. The large gap between precision and
recall is also interesting, showing that the projec-
tions do not have wide semantic role coverage.
2
4.2 Direct Translations
Each sentence in Europarl was written in one of
the official languages of the European Parliament
and translated to all of the other languages. There-
fore both sides of a parallel sentence pair can be in-
direct translations of each other. van der Plas et al.
(2011) suggest that translation divergence may af-
2
Note that our results are not directly comparable with
(van der Plas et al., 2011) because they split Classic1K into
development and test sets, while we use the whole set for
testing. We do not have access to their split.
fect automatic projection of semantic roles. They
therefore select for their experiments only those
276K sentences from the 980K which are direct
translations between English and French. Moti-
vated by this idea, we replicate the learning curve
in Fig. 1 with another set of 100K sentences ran-
domly selected from only the direct translations.
The curve is shown in Fig. 2. There is no no-
ticeable difference between this and the graph in
Fig. 1, suggesting that the projections obtained via
direct translations are not of higher quality.
4.3 Impact of Syntactic Annotation
Being a dependency-based semantic role labeller,
LTH employs a large set of features based on syn-
tactic dependency structure. This inspires us to
compare the impact of different types of syntactic
annotations on the performance of this system.
Based on the observations from the previous
sections, we choose two different sizes of training
sets. The first set contains the first 5K sentences
from the original 100K, as we saw that more than
this amount tends to diminish performance. The
second set contains the first 50K from the original
100K, the purpose of which is to check if changing
the parses affects the usefulness of adding more
data. We will call these data sets Classic5K and
Classic50K respectively.
Petrov et al. (2012) create a set of 12 univer-
sal part-of-speech (POS) tags which should in the-
ory be applicable to any natural language. It is
interesting to know whether these POS tags are
more useful for SRL than the original set of the 29
more fine-grained POS tags used in French Tree-
bank which we have used so far. To this end, we
convert the original POS tags of the data to uni-
versal POS tags and retrain and evaluate the SRL
models. The results are given in the second row of
Table 1 (OrgDep+UniPOS). The first row of the
table (Original) shows the performance using
the original annotation. Even though the scores
increase in most cases ? due mostly to a rise in
recall ? the changes are small. It is worth noting
that identification seems to benefit more from the
universal POS tags.
Similar to universal POS tags, McDonald et al.
(2013) introduce a set of 40 universal dependency
types which generalize over the dependency struc-
ture specific to several languages. For French, they
provide a new treebank, called uni-dep-tb,
manually annotating 16,422 sentences from vari-
89
5K 50K
Identification Classification Identification Classification
P R F
1
P R F
1
P R F
1
P R F
1
Original 85.95 59.64 70.42 71.34 49.50 58.45 86.67 58.07 69.54 72.44 48.54 58.13
OrgDep+UniPOS 86.71 60.46 71.24 71.11 49.58 58.43 86.82 58.71 70.05 72.30 48.90 58.34
StdUniDep+UniPOS 86.14 59.76 70.57 70.60 48.98 57.84 86.38 58.90 70.04 71.61 48.83 58.07
CHUniDep+UniPOS 85.98 59.21 70.13 70.66 48.66 57.63 86.47 58.26 69.61 71.74 48.34 57.76
Table 1: SRL performance using different syntactic parses with Classic 5K and 50K training sets
ous domains. We now explore the utility of this
new dependency scheme in SRL.
The French universal dependency treebank
comes in two versions, the first using the stan-
dard dependency structure based on basic Stanford
dependencies (de Marneffe and Manning, 2008)
where content words are the heads except in cop-
ula and adposition constructions, and the second
which treats content words as the heads for all
constructions without exemption. We use both
schemes in order to verify their effect on SRL.
In order to obtain universal dependencies for
our data, we train parsing models with Malt-
Parser (Nivre et al., 2006) using the entire
uni-dep-tb.
3
We then parse our data us-
ing these MaltParser models. The input POS
tags to the parser are the universal POS tags
used in OrgDep+UniPOS. We train and evalu-
ate new SRL models on these data. The results
are shown in the third and fourth rows of Table
1. StdUniDept+UniPOS is the setting using
standard dependencies and CHUDep+UPOS using
content-head dependencies.
According to the third and fourth rows in Table
1, content-head dependencies are slightly less use-
ful than standard dependencies. The general ef-
fect of universal dependencies can be compared to
those of original ones by comparing these results
to OrgDep+UniPOS - the use of universal de-
pendencies appears to have only a modest (nega-
tive) effect. However, we must be careful of draw-
ing too many conclusions because in addition to
the difference in dependency schemes, the training
data used to train the parsers as well as the parsers
themselves are different.
Overall, we observe that the universal annota-
tions can be reliably used when the fine-grained
annotation is not available. This can be especially
3
Based on our preliminary experiments on the pars-
ing performance, we use LIBSVM as learning algorithm,
nivreeager as parsing algorithm for the standard depen-
dency models and stackproj for the content-head ones.
Identification Classification
P R F
1
P R F
1
1K 83.76 83.00 83.37 68.40 67.78 68.09
5K 85.94 59.62 70.39 71.30 49.47 58.40
1K+5K 85.74 66.53 74.92 71.48 55.46 62.46
SelfT 83.82 83.66 83.73 67.91 67.79 67.85
Table 2: Average scores of 5-fold cross-validation
with Classic 1K (1K), 5K (5K), 1K plus 5K
(1K+5K) and self-training with 1K seed and 5K
unlabeled data (SelfT)
useful for languages which lack such resources
and require techniques such as cross-lingual trans-
fer to replace them.
4.4 Quality vs. Quantity
In Section 4.1, we saw that adding more data an-
notated through projection did not elevate SRL
performance. In other words, the same perfor-
mance was achieved using only a small amount
of data. This is contrary to the motivation for cre-
ating synthetic training data, especially when the
hand-annotated data already exist, albeit in a small
size. In this section, we compare the performance
of SRL models trained using manually-annotated
data with SRL models trained using 5K of artifi-
cial or synthetic training data. We use the original
syntactic annotations for both datasets.
To this end, we carry out a 5-fold cross-
validation on Classic1K. We then evaluate the
Classic5K model, on each of the 5 test sets gen-
erated in the cross-validation. The average scores
of the two evaluation setups are compared. The
results are shown in Table 2.
While the 5K model achieves higher precision,
its recall is far lower resulting in dramatically
lower F
1
. This high precision and low recall is due
to the low confidence of the model trained on pro-
jected data suggesting that a considerable amount
of information is not transferred during the projec-
tion. This issue can be attributed to the fact that the
90
Classic projection uses intersection of alignments
in the two translation directions, which is the most
restrictive setting and leaves many source predi-
cates and arguments unaligned.
We next add the Classic5K projected data to
the manually annotated training data in each fold
of another cross-validation setting and evaluate
the resulting models on the same test sets. The
results are reported in the third row of the Ta-
ble 2 (1K+5K). As can be seen, the low qual-
ity of the projected data significantly degrades the
performance compared to when only manually-
annotated data are used for training.
Finally, based on the observation that the qual-
ity of labelling using manually annotated data is
higher than using the automatically projected data,
we replicate 1K+5K with the 5K data labelled us-
ing the model trained on the training subset of 1K
at each cross-validation fold. In other words, we
perform a one-round self-training with this model.
The performance of the resulting model evaluated
in the same cross-validation setting is given in the
last row of Table 2 (SelfT).
As expected, the labelling obtained by mod-
els trained on manual annotation are more useful
than the projected ones when used for training new
models. It is worth noting that, unlike with the
1K+5K setting, the balance between precision and
recall follows that of the 1K model. In addition,
some of the scores are the highest among all re-
sults, although the differences are not significant.
4.5 How little is too little?
In the previous section we saw that using a manu-
ally annotated dataset with as few as 800 sentences
resulted in significantly better SRL performance
than using projected annotation with as many as
5K sentences. This unfortunately indicates the
need for human labour in creating such resources.
It is interesting however to know the lower bound
of this requirement. To this end, we reverse our
cross-validation setting and train on 200 and test
on 800 sentences. We then compare to the 5K
models evaluated on the same 800 sentence sets
at each fold. The results are presented in Table 3.
Even with only 200 manually annotated sentences,
the performance is considerably higher than with
5K sentences of projected annotations. However,
as one might expect, compared to when 800 sen-
tences are used for training, this small model per-
forms significantly worse.
Identification Classification
P R F
1
P R F
1
1K 82.34 79.61 80.95 64.14 62.01 63.06
5K 85.95 59.64 70.42 71.34 49.50 58.45
Table 3: Average scores of 5-fold cross-validation
with Classic 1K (1K) and 5K (5K) using 200 sen-
tences for training and 800 for testing at each fold
5 Conclusion
We have explored the projection-based approach
to SRL by carrying out experiments with a large
set of French semantic role labels which have been
automatically transferred from English. We have
found that increasing the number of these artificial
projections that are used in training an SRL sys-
tem does not improve performance as might have
been expected when creating such a resource. In-
stead it is better to train directly on what little gold
standard data is available, even if this dataset con-
tains only 200 sentences. We suspect that the dis-
appointing performance of the projected dataset
originates in the restrictive way the word align-
ments have been extracted. Only those alignments
that are in the intersection of the English-French
and French-English word alignment sets are re-
tained resulting in low SRL recall. Recent prelim-
inary experiments show that less restrictive align-
ment extraction strategies including extracting the
union of the two sets or source-to-target align-
ments lead to a better recall and consequently F
1
both when used for direct projection to the test
data or for creating the training data and then ap-
plying the resulting model to the test data.
We have compared the use of universal POS
tags and dependency labels to the original, more
fine-grained sets and shown that there is only a
little difference. However, it remains to be seen
whether this finding holds for other languages or
whether it will still hold for French when SRL per-
formance can be improved. It might also be in-
teresting to explore the combination of universal
dependencies with fine-grained POS tags.
Acknowledgments
This research has been supported by the Irish
Research Council Enterprise Partnership Scheme
(EPSPG/2011/102) and the computing infrastruc-
ture of the CNGL at DCU. We thank Lonneke van
der Plas for providing us the Classic data. We also
thank the reviewers for their helpful comments.
91
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th ACL, pages 86?90.
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 43?48.
Marie Candito, Benot Crabb?e, and Pascal Denis. 2010.
Statistical french dependency parsing: treebank
conversion and first results. In Proceedings of
LREC?2010.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies
representation. In Proceedings of the COLING
Workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
Claire Gardent and Christophe Cerisara. 2010. Semi-
Automatic Propbanking for French. In TLT9 -
The Ninth International Workshop on Treebanks and
Linguistic Theories.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?18.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Sum-
mit, pages 79?86.
Anna Kup?s?c and Anne Abeill?e. 2008. Growing
treelex. In Proceedings of the 9th International Con-
ference on Computational Linguistics and Intelli-
gent Text Processing, CICLing?08, pages 28?39.
Alejandra Lorenzo and Christophe Cerisara. 2012.
Unsupervised frame based semantic role induction:
application to french and english. In Proceedings of
the ACL 2012 Joint Workshop on Statistical Parsing
and Semantic Processing of Morphologically Rich
Languages, pages 30?35.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria
Bertomeu Castell?o, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 92?97.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In In Proceedings of LREC.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Sebastian Pad?o and Mirella Lapata. 2009. Cross-
lingual annotation projection of semantic roles. J.
Artif. Int. Res., 36(1):307?340.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania.
Ivan Titov and James Henderson. 2007. A latent vari-
able model for generative dependency parsing. In
Proceedings of the 10th International Conference on
Parsing Technologies, pages 144?155.
Ivan Titov, James Henderson, Paola Merlo, and
Gabriele Musillo. 2009. Online projectivisation for
synchronous parsing of semantic and syntactic de-
pendencies. In In Proceedings of the Internation
Joint Conference on Artificial Intelligence (IJCAI),
pages 1562?1567.
Lonneke van der Plas, James Henderson, and Paola
Merlo. 2010a. D6. 2: Semantic role annotation of a
french-english corpus.
Lonneke van der Plas, Tanja Samard?zi?c, and Paola
Merlo. 2010b. Cross-lingual validity of propbank
in the manual annotation of french. In Proceedings
of the Fourth Linguistic Annotation Workshop, LAW
IV ?10, pages 113?117.
Lonneke van der Plas, Paola Merlo, and James Hen-
derson. 2011. Scaling up automatic cross-lingual
semantic role annotation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 299?304.
92
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 138?144,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
DCU-Symantec Submission for the WMT 2012 Quality Estimation Task
Raphael Rubino??, Jennifer Foster?, Joachim Wagner?,
Johann Roturier?, Rasul Samad Zadeh Kaljahi??, Fred Hollowood?
?Dublin City University, ?Symantec, Ireland
?firstname.lastname@computing.dcu.ie
?firstname lastname@symantec.com
Abstract
This paper describes the features and the ma-
chine learning methods used by Dublin City
University (DCU) and SYMANTEC for the
WMT 2012 quality estimation task. Two sets
of features are proposed: one constrained, i.e.
respecting the data limitation suggested by the
workshop organisers, and one unconstrained,
i.e. using data or tools trained on data that was
not provided by the workshop organisers. In
total, more than 300 features were extracted
and used to train classifiers in order to predict
the translation quality of unseen data. In this
paper, we focus on a subset of our feature set
that we consider to be relatively novel: fea-
tures based on a topic model built using the
Latent Dirichlet Allocation approach, and fea-
tures based on source and target language syn-
tax extracted using part-of-speech (POS) tag-
gers and parsers. We evaluate nine feature
combinations using four classification-based
and four regression-based machine learning
techniques.
1 Introduction
For the first time, the WMT organisers this year pro-
pose a Quality Estimation (QE) shared task, which
is divided into two sub-tasks: scoring and ranking
automatic translations. The aim of this workshop is
to define useful sets of features and machine learn-
ing techniques in order to predict the quality of a
machine translation (MT) output T (Spanish) given
a source segment S (English). Quality is measured
using a 5-point likert scale which is based on post-
editing effort, following the scoring scheme:
1. The MT output is incomprehensible
2. About 50-70% of the MT output needs to be
edited
3. About 25-50% of the MT output needs to be
edited
4. About 10-25% of the MT output needs to be
edited
5. The MT output is perfectly clear and intelligi-
ble
The final score is a combination of the scores as-
signed by three evaluators. The use of a 5-point scale
makes the scoring task more difficult than a binary
classification task where a translation is considered
to be either good or bad. However, if the task is
successfully carried out, the score produced is more
useful.
Dublin City University and Symantec jointly ad-
dress the scoring task. For each pair (S, T ) of source
segment S and machine translation T , we train three
classifiers and one classifier combination using the
training data provided by the organisers to predict
5-point Likert scores. In this paper, we present the
classification results on the test set alng with addi-
tional results obtained using regression techniques.
We evaluate the usefulness of two new sets of fea-
tures:
1. topic-based features using Latent Dirichlet Al-
location (LDA (Blei et al, 2003)),
2. syntax-based features using POS taggers and
parsers (Wagner et al, 2009)
The remainder of this paper is organised as fol-
lows. In Section 2, we give an overview of all the
138
features employed in our QE system. Then, in Sec-
tion 3, we describe the topic and syntax-based fea-
tures in more detail. Section 4 presents the vari-
ous classification and regression techniques we ex-
plored. Our results are presented and discussed in
Section 5. Finally, we summarise and outline our
plans in Section 6.
2 Features Overview
In this section, we describe the features used in our
QE system. In the first subsection, the features in-
cluded in our constrained system are presented. In
the second subsection, we detail the features in-
cluded in our unconstrained system. Both of these
systems include the 17 baseline features provided
for the shared task.
2.1 Constrained System
The constrained system is based only on the data
provided by the organisers. We extracted 70 fea-
tures in total (including the baseline features) and
we present them here according to the type of infor-
mation they capture.
Word and Phrase-Level Features
? Ratio of source and target segment length:
the number of source words divided by the
number of target words
? Ratio of source and target number of punc-
tuation marks: the number of source punctua-
tion marks divided by the number of target ones
? Number of phrases comprising the MT out-
put: given a phrase-table, we assume that a
sentence composed of several phrases indicates
uncertainty on the part of the MT system.
? Average length of source and target phrases:
concatenating short phrases may result in lower
fluency compared to the use of longer ones.
? Ratio of source and target averaged phrase
length
? Number of source prepositions and conjunc-
tions word: our assumption here is that seg-
ments containing a relatively high number of
prepositions and conjunctions may be more
complex and difficult to translate.
? Number of source out-of-vocabulary words
Language Model Features
All the language models (LMs) used in our work
are n-gram LMs with Kneser-Ney smoothing built
with the SRI Toolkit (Stolcke, 2002).
? Backward 2-gram and 3-gram source and
target log probabilities: as proposed by
Duchateau et al (2002)
? Log probability of target segments on
5-gram MT-output-based LM: using
MOSES (Koehn et al, 2007) trained on the
provided parallel corpus, we translated the En-
glish side of this corpus into Spanish, assuming
that the MT output contains mistakes. This
MT output is used to build a LM that models
the behavior of the MT system. We assume
that for a given MT output, a high n-gram
probability (or a low perplexity) of the LM
indicates that the MT output contains mistakes.
MT-system Features
? 15 scores provided by Moses: phrase-table,
language model, reordering model and word
penalty (weighted and unweighted)
? Number of n-bests for each source segment
? MT output back-translation: from Spanish to
English using MOSES trained on the provided
parallel corpus, scored with TER (Snover et
al., 2006), BLEU (Papineni et al, 2002) and
the Levenshtein distance (Levenshtein, 1966),
based on the source segments as a translation
reference
Topic Model Features
? Probability distribution over topics: Source
and target segment probability distribution over
topics for a 10-dimension topic model
? Cosine distance between source and target
topic vectors
More details about these two features are provided
in Section 3.1.
2.2 Unconstrained System
In addition to the features used for the constrained
system, a further 238 unconstrained features were
included in our unconstrained system.
139
MT System Features
As for our constrained system, we use MT output
back-translation from Spanish to English, but this
time using Bing Translator1 in addition to Moses.
Each back-translated segment is scored with TER,
BLEU and the Levenshtein distance, based on the
source segments as a translation reference.
Source Syntax Features
Wagner et al (2007; 2009) propose a series of
features to measure sentence grammaticality. These
features rely on a part-of-speech tagger, a probabilis-
tic parser and a precision grammar/parser. We have
at our disposal these tools for English and so we ap-
ply them to the source data. The features themselves
are described in more detail in Section 3.2.
Target Syntax Features
We use a part-of-speech tagger trained on Spanish
to extract from the target data the subset of grammat-
icality features proposed by Wagner et al (2007;
2009) that are based on POS n-grams. In addition
we extract features which reflect the prevalence of
particular POS tags in each target segment. These
are explained in more detail in Section 3.2 below.
Grammar Checker Features
LANGUAGETOOL (based on (Naber, 2003)) is an
open-source grammar and style proofreading tool
that finds errors based on pre-defined, language-
specific rules. The latest version of the tool can
be run in server mode, so individual sentences can
be checked and assigned a total number of errors
(which may or may not be true positives).2 This
number is used as a feature for each source segment
and its corresponding MT output.
3 Topic and Syntax-based Features
In this section, we focus on the set of features
that aim to capture adequacy using topic modelling
and grammaticality using POS tagging and syntactic
parsing.
1http://www.microsofttranslator.com/
2The list of English and Spanish rules is available at:
http://languagetool.org/languages.
3.1 Topic-based Features
We extract source and target features based on a
topic model built using LDA. The main idea in topic
modelling is to produce a set of thematic word clus-
ters from a collection of documents. Using the par-
allel corpus provided for the task, a bilingual corpus
is built where each line is composed of a source seg-
ment and its translation separated by a space. Each
pair of segments is considered as a bilingual docu-
ment. This corpus is used to train a bilingual topic
model after stopwords removal. The resulting model
is one set of bilingual topics z containing words w
with a probability p(wn|zn, ?) (with n equal to the
vocabulary size in the whole parallel corpus). This
model can be used to infer the probability distri-
bution of unseen source and target segments over
bilingual topics. During the test step, each source
segment and its translation are considered individu-
ally, as two monolingual documents. This method
allows us to compare the source and target topic dis-
tributions. We assume that a source segment and its
translation share topic similarities.
We propose two ways of using topic-based fea-
tures for quality estimation: keeping source and tar-
get topic vectors as two sets of k features, or com-
puting a vector distance between these two vectors
and using one feature only. To measure the prox-
imity of two vectors, we decided to used the Co-
sine distance, as it leads to the best results in terms
of classification accuracy. However, we plan to
study different metrics in further experiments, like
the Manhattan or the Euclidean distances. Some
parameters related to LDA have to be studied more
carefully too, such as the number of topics (dimen-
sions in the topic space), the number of words per
topic, the Dirichlet hyperparameter ?, etc. In our
experiments, we built a topic model composed of 10
dimensions using Gibbs sampling with 1000 itera-
tions. We assume that a higher dimensionality can
lead to a better repartitioning of the vocabulary over
the topics.
Multilingual LDA has been used before in nat-
ural language processing, e.g. polylingual topic
models (Mimno et al, 2009) or multilingual topic
models for unaligned text (Boyd-Graber and Blei,
2009). In the field of machine translation, Tam et
al. (2007) propose to adapt a translation and a lan-
140
guage model to a specific topic using Latent Se-
mantic Analysis (LSA, or Latent Semantic Index-
ing, LSI (Deerwester et al, 1990)). More recently,
some studies were conducted on the use of LDA to
adapt SMT systems to specific domains (Gong et al,
2010; Gong et al, 2011) or to extract bilingual lexi-
con from comparable corpora (Rubino and Linare`s,
2011). Extracting features from a topic model is, to
the best of our knowledge, the first attempt in ma-
chine translation quality estimation.
3.2 Syntax-based Features
Syntactic features have previously been used in MT
for confidence estimation and for building automatic
evaluation measures. Corston-Oliver et al (2001)
build a classifier using 46 parse tree features to pre-
dict whether a sentence is a human translation or MT
output. Quirk (2004) uses a single parse tree feature
in the quality estimation task with a 4-point scale,
namely whether a spanning parse can be found, in
addition to LM perplexity and sentence length. Liu
and Gildea (2005) measure the syntactic similarity
between MT output and reference translation. Al-
brecht and Hwa (2007) measure the syntactic simi-
larity between MT output and reference translation
and between MT output and a large monolingual
corpus. Gimenez and Marquez (2007) explore lexi-
cal, syntactic and shallow semantic features and fo-
cus on measuring the similarity of MT output to ref-
erence translation. Owczarzak et al (2007) use la-
belled dependencies together with WordNet to avoid
penalising valid syntactic and lexical variations in
MT evaluation. In what follows, we describe how
we make use of syntactic information in the QE task,
i.e. evaluating MT output without a reference trans-
lation.
Wagner et al (2007; 2009) use three sources
of linguistic information in order to extract features
which they use to judge the grammaticality of En-
glish sentences:
1. For each POS n-gram (with n ranging from 2 to
7), a feature is extracted which represents the
frequency of the least frequent n-gram in the
sentence according to some reference corpus.
TreeTagger (Schmidt, 1994) is used to produce
POS tags.
2. Features provided by a hand-crafted, broad-
coverage precision grammar of English (Butt
et al, 2002) and a Lexical Functional Grammar
parser (Maxwell and Kaplan, 1996). These in-
clude whether or not a sentence could be parsed
without resorting to robustness measures, the
number of analyses found and the parsing time.
3. Features extracted from the output of three
probabilistic parsers of English (Charniak and
Johnson, 2005), one trained on Wall Street
Journal trees (Marcus et al, 1993), one trained
on a distorted version of the treebank obtained
by automatically creating grammatical error
and adjusting the parse trees, and the third
trained on the union of the original and dis-
torted versions.
These features were originally designed to distin-
guish grammatical sentences from ungrammatical
ones and were tested on sentences from learner cor-
pora by Wagner et al (2009) and Wagner (2012).
In this work we extract all three sets of features
from the source side of our data and the POS-based
subset from the target side.3 We use the publicly
available pre-trained TreeTagger models for English
and Spanish4. The reference corpus used to obtain
POS n-gram frequences is the MT translation model
training data.5
In addition to the POS-based features described in
Wagner et al (2007; 2009), we also extract the fol-
lowing features from the Spanish POS-tagged data:
for each POS tag P and target segment T , we ex-
tract a feature which is the proportion of words in
T that are tagged as P . Two additional features are
extracted to represent the proportion of words in T
that are assigned more than one tag by the tagger,
3Unfortunately, due to time constraints, we were unable to
source a suitable probabilistic phrase-structure parser and a pre-
cision grammar for Spanish and were thus unable to extract
parser-based features for Spanish. We expect that these features
would be more useful on the target side than the source side.
4http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/
5To aid machine learning methods that linearly combine fea-
ture values, we add binarised features derived from the raw XLE
and POS n-gram features described above, for example we add
a feature indicating whether the frequency of the least frequent
POS 5-gram is below 10. We base the choice of binary fea-
tures on (a) decision rules observed in decision trees trained for
a binary scoring task and (b) decision rules of simple classifiers
(decision trees with just one decision node and 2 leaf nodes)
that form a convex hull of optimal classifiers in ROC space.
141
and the proportion of words in T that are unknown
to the tagger.
4 Machine Learning
In this section, we describe the machine learning
methods that we experimented with. Our final sys-
tems submitted for the shared task are based on clas-
sification methods. However, we also performed
some experiments with regression methods.
We evaluate the systems on the test set using the
official evaluation script and the reference scores.
We report the evaluation results as Mean Aver-
age Error (MAE) and Root Mean Squared Error
(RMSE).
4.1 Classification
In order to apply classification algorithms to the
set of features associated with each source and tar-
get segment, we rounded the training data scores
to the closest integer. We tested several classifiers
and empirically chose three algorithms: Support
Vector Machine using sequential minimal optimiza-
tion and RBF kernel (parameters optimized by grid-
search) (Platt, 1999), Naive Bayes (John and Lang-
ley, 1995) and Random Forest (Breiman, 2001) (the
latter two techniques were applied with default pa-
rameters). We use the Weka toolkit (Hall et al,
2009) to train the classifiers and predict the scores
on the test set. Each method is evaluated individu-
ally and then combined by averaging the predicted
scores.
4.2 Regression
We applied three different regression techniques:
SVM epsilon-SVR with RBF kernel, Linear Regres-
sion and M5P (Quinlan, 1992; Wang and Witten,
1997). The two latter algorithms were used with
default parameters, whereas SVM parameters (?, c
and ) were optimized by grid-search. We also per-
formed a combination of the three algorithms by av-
eraging the predicted scores. We apply a linear func-
tion on the predicted scores S in order to keep them
in the correct range (from 1 to 5) as detailed in (1),
where S? is the rescaled sentence score, Smin is the
lowest predicted score and Smax is the highest pre-
dicted score.
S? = 1 + 4?
S ? Smin
Smax ? Smin
(1)
5 Evaluation
Table 1 shows the results obtained by our classifi-
cation approach on various feature subsets. Note
that the two submitted systems used the combined
classifier approach with the constrained and uncon-
strained feature sets. Table 2 shows the results for
the same feature combinations, this time using re-
gression rather than classification.
The results of quality estimation using classifica-
tion methods show that the baseline and the syntax-
based features with the classifier combination leads
to the best results with an MAE of 0.71 and an
RMSE of 0.87. However, these scores are substan-
tially lower than the ones obtained using regression,
where the unconstrained set of features with SVM
leads to an MAE of 0.62 and an RMSE of 0.78.
It seems that the classification methods are not
suitable for this task according to the different sets
of features studied. Furthermore, the topic-distance
feature is not correlated with the quality scores, ac-
cording to the regression results. On the other hand,
the syntax-based features appear to be the most in-
formative and lead to an MAE of 0.70.
6 Conclusion
We presented in this paper our submission for the
WMT12 Quality Estimation shared task. We also
presented further experiments using different ma-
chine learning techniques and we evaluated the im-
pact of two sets of features - one set which is based
on linguistic features extracted using POS tagging
and parsing, and a second set which is based on topic
modelling. The best results are obtained by our un-
constrained system containing all features and us-
ing an -SVR regression method with a Radial Basis
Function kernel. This setup leads to a Mean Aver-
age Error of 0.62 and a Root Mean Squared Error
of 0.78. Unfortunately, we did not submit our best
configuration for the shared task.
We plan to continue working on the task of ma-
chine translation quality estimation. Our immediate
next steps are to continue to investigate the contribu-
tion of individual features, to explore feature selec-
tion in a more detailed fashion and to apply our best
system to other types of data including sentences
taken from an online discussion forum.
142
SMO NAIVE BAYES RANDOM FOREST Combination
Features MAE RMSE MAE RMSE MAE RMSE MAE RMSE
baseline 0.74 0.89 0.85 1.10 0.84 1.06 0.71 0.88
topic distribution 0.84 1.02 1.09 1.38 0.91 1.15 0.78 0.98
topic distance 0.88 1.11 0.93 1.17 1.04 1.23 0.84 1.04
syntax 0.78 0.97 1.01 1.27 0.83 1.05 0.72 0.90
baseline + topic 0.82 1.01 1.00 1.31 0.84 1.05 0.75 0.95
baseline + syntax 0.76 0.94 1.01 1.25 0.79 0.98 0.71 0.87
baseline + topic + syntax 0.82 1.04 1.03 1.29 0.79 0.98 0.74 0.93
all constrained 0.99 1.26 1.12 1.46 0.71 0.88 0.86 ? 1.12 ?
all unconstrained 0.97 1.25 0.80 1.02 0.79 0.99 0.75 ? 0.97 ?
Table 1: MAE and RMSE results for different sets of features using three classification methods. The results with ?
and ? correspond to the DCU-SYMC constrained and the DCU-SYMC unconstrained systems respectively, submitted
for the shared task.
SVM LINEAR REG. M5P Combination
Features MAE RMSE MAE RMSE MAE RMSE MAE RMSE
baseline 0.78 0.93 0.80 0.99 0.73 0.91 0.72 0.88
topic distribution 0.78 0.95 0.79 0.96 0.80 0.96 0.79 0.95
topic distance 1.38 1.67 1.31 1.62 1.85 2.09 1.00 1.24
syntax 0.70 0.88 0.97 1.22 1.41 1.65 0.76 0.92
baseline + topic 0.78 0.96 1.06 1.31 1.16 1.42 0.88 1.10
baseline + syntax 0.67 0.82 0.90 1.12 2.17 2.38 0.98 1.22
baseline + topic + syntax 0.68 0.84 0.93 1.16 2.12 2.33 0.97 1.21
all constrained 0.83 1.02 0.94 1.18 0.78 0.99 0.71 0.88
all unconstrained 0.62 0.78 1.33 1.60 0.71 0.89 0.73 0.91
Table 2: MAE and RMSE results for different sets of features using three regression methods.
References
J. Albrecht and R. Hwa. 2007. A re-examination of ma-
chine learning approaches for sentence-level MT eval-
uation. In Proceedings of the 45th Annual Meeting of
the ACL, pages 880?887.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet Allocation. The Journal of Machine Learn-
ing Research, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2009. Multilin-
gual topic models for unaligned text. In Proceedings
of the 25th Conference on Uncertainty in Artificial In-
telligence, pages 75?82.
L. Breiman. 2001. Random forests. Machine learning,
45(1):5?32.
M. Butt, H. Dyvik, T. Holloway King, H. Masuichi, and
C. Rohrer. 2002. The parallel grammar project. In
Proceedings of the Coling Workshop on Grammar En-
gineering and Evaluation.
E. Charniak and M. Johnson. 2005. Course-to-fine n-
best-parsing and maxent discriminative reranking. In
Proceedings of the 43rd Annual Meeting of the ACL,
pages 173?180, Ann Arbor.
S. Corston-Oliver, M. Gamon, and C. Brockett. 2001.
A machine learning approach to the automatic evalu-
ation of machine translation. In Proceedings of 39th
Annual Meeting of the Association for Computational
Linguistics, pages 148?155, Toulouse, France, July.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by Latent Semantic
Analysis. Journal of the American Society for Infor-
mation Science, 41(6):391?407.
J. Duchateau, K. Demuynck, and P. Wambacq. 2002.
Confidence scoring based on backward language mod-
els. In Proceedings IEEE international confer-
ence on acoustics, speech, and signal processing,
ICASSP?2002, volume 1, pages 221?224.
J. Gime?nez and L. Ma`rquez. 2007. Linguistic features
for automatic evaluation of heterogenous MT systems.
In Proceedings of the Second Workshop on Statistical
Machine Translation, pages 256?264, Prague, Czech
Republic, June.
143
Z. Gong, Y. Zhang, and G. Zhou. 2010. Statistical ma-
chine translation based on lda. In Universal Commu-
nication Symposium (IUCS), 2010 4th International,
pages 286?290.
Z. Gong, G. Zhou, and L. Li. 2011. Improve smt with
source-side ?topic-document? distributions. In MT
Summit, pages 496?501.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
G.H. John and P. Langley. 1995. Estimating continuous
distributions in bayesian classifiers. In Eleventh con-
ference on uncertainty in artificial intelligence, pages
338?345. Morgan Kaufmann Publishers Inc.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
pages 177?180.
V.I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. In Soviet
Physics Doklady, volume 10-8, pages 707?710.
D. Liu and D. Gildea. 2005. Syntactic features for eval-
uation of machine translation. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion, pages 25?32, Ann Arbor, Michigan.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: the penn treebank. Computational Linguistics,
19(2):313?330.
John Maxwell and Ron Kaplan. 1996. An Efficient
Parser for LFG. In Proceedings of LFG-96, Grenoble.
D. Mimno, H.M. Wallach, J. Naradowsky, D.A. Smith,
and A. McCallum. 2009. Polylingual topic models.
In Proceedings of EMNLP: Volume 2-Volume 2, pages
880?889. Association for Computational Linguistics.
D. Naber. 2003. A rule-based style and grammar
checker. Technical report, Bielefeld University Biele-
feld, Germany.
K. Owczarzak, J. van Genabith, and A. Way. 2007. La-
belled dependencies in machine translation evaluation.
In Proceedings of the Second Workshop on Statistical
Machine Translation, pages 104?111, Prague, Czech
Republic, June.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In ACL, pages 311?318.
J.C. Platt. 1999. Fast training of support vector machines
using sequential minimal optimization. In Advances in
kernel methods, pages 185?208. MIT Press.
R. J. Quinlan. 1992. Learning with continuous classes.
In 5th Australian Joint Conference on Artificial Intel-
ligence, pages 343?348, Singapore. World Scientific.
C. Quirk. 2004. Training a sentence-level machine trans-
lation confidence measure. In Proceedings of LREC,
Lisbon, June.
R. Rubino and G. Linare`s. 2011. A multi-view approach
for term translation spotting. Computational Linguis-
tics and Intelligent Text Processing, 6609:29?40.
H. Schmidt. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of the Interna-
tional Conference on New Methods in Natural Lan-
guage Processing.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proceedings of Associ-
ation for Machine Translation in the Americas, pages
223?231.
A. Stolcke. 2002. SRILM-an extensible language mod-
eling toolkit. In InterSpeech, volume 2, pages 901?
904.
Y.C. Tam, I. Lane, and T. Schultz. 2007. Bilingual
lsa-based adaptation for statistical machine translation.
Machine Translation, 21(4):187?207.
J. Wagner, J. Foster, and J. van Genabith. 2007. A com-
parative evaluation of deep and shallow approaches to
the automatic detection of common grammatical er-
rors. In Proceedings of EMNLP-CoNLL, pages 112?
121, Prague, Czech Republic, June.
J. Wagner, J. Foster, and J. van Genabith. 2009. Judg-
ing grammaticality: Experiments in sentence classifi-
cation. CALICO Journal, 26(3):474?490.
J. Wagner. 2012. Detecting grammatical errors with
treebank-induced probabilistic parsers. Ph.D. thesis,
Dublin City University.
Y. Wang and I. H. Witten. 1997. Induction of model trees
for predicting continuous classes. In Poster papers of
the 9th European Conference on Machine Learning.
Springer.
144
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 392?397,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
DCU-Symantec at the WMT 2013 Quality Estimation Shared Task
Raphael Rubino??, Joachim Wagner??, Jennifer Foster?,
Johann Roturier? Rasoul Samad Zadeh Kaljahi?? and Fred Hollowood?
?NCLT, School of Computing, Dublin City University, Ireland
?Center for Next Generation Localisation, Dublin, Ireland
?Symantec Research Labs, Dublin, Ireland
?{rrubino, jwagner, jfoster}@computing.dcu.ie
?{johann roturier, fhollowood}@symantec.com
Abstract
We describe the two systems submit-
ted by the DCU-Symantec team to Task
1.1. of the WMT 2013 Shared Task on
Quality Estimation for Machine Transla-
tion. Task 1.1 involve estimating post-
editing effort for English-Spanish trans-
lation pairs in the news domain. The
two systems use a wide variety of fea-
tures, of which the most effective are the
word-alignment, n-gram frequency, lan-
guage model, POS-tag-based and pseudo-
references ones. Both systems perform at
a similarly high level in the two tasks of
scoring and ranking translations, although
there is some evidence that the systems are
over-fitting to the training data.
1 Introduction
The WMT 2013 Quality Estimation Shared Task
involve both sentence-level and word-level qual-
ity estimation (QE). The sentence-level task con-
sist of three subtasks: scoring and ranking transla-
tions with regard to post-editing effort (Task 1.1),
selecting among several translations produced by
multiple MT systems for the same source sentence
(Task 1.2), and predicting post-editing time (Task
1.3). The DCU-Symantec team enter two systems
to Task 1.1. Given a set of source English news
sentences and their Spanish translations, the goals
are to predict the HTER score of each translation
and to produce a ranking based on HTER for the
set of translations. A set of 2,254 sentence pairs
are provided for training.
On the ranking task, our system DCU-SYMC
alltypes is second placed out of thirteen sys-
tems and our system DCU-SYMC combine is
ranked fifth, according to the Delta Average met-
ric. According to the Spearman rank correlation,
our systems are the joint-highest systems. In the
scoring task, the DCU-SYMC alltypes system
is placed sixth out of seventeen systems accord-
ing to Mean Absolute Error (MAE) and third ac-
cording to Root Mean Squared Error (RMSE). The
DCU-SYMC combine system is placed fifth ac-
cording to MAE and second according to RMSE.
In this system description paper, we describe the
features, the learning methods used, the results for
the two submitted systems and some other systems
we experiment with.
2 Features
Our starting point for the WMT13 QE shared task
was the feature set used in the system we submit-
ted to the WMT12 QE task (Rubino et al, 2012).
This feature set, comprising 308 features in to-
tal, extended the 17 baseline features provided by
the task organisers to include 6 additional sur-
face features, 6 additional language model fea-
tures, 17 additional features derived from the
MT system components and the n-best lists, 138
features obtained by part-of-speech tagging and
parsing the source sentences and 95 obtained by
part-of-speech tagging the target sentences, 21
topic model features, 2 features produced by a
grammar checker1 and 6 pseudo-source (or back-
translation) features.
We made the following modifications to this
2012 feature set:
? The pseudo-source (or back-translation) fea-
tures were removed, as they did not con-
tribute useful information to our system last
year.
? The language model and n-gram frequency
feature sets were extended in order to cover
1 to 5 gram sequences, as well as source and
target ratios for these feature values.
? The word-alignment feature set was also
extended by considering several thresholds
1http://www.languagetool.org/
392
when counting the number of target words
aligned with source words.
? We extracted 8 additional features from the
decoder log file, including the number of dis-
carded hypotheses, the total number of trans-
lation options and the number of nodes in the
decoding graph.
? The set of topic model features was reduced
in order to keep only those that were shown
to be effective on three quality estimation
datasets (the details can be found in (Rubino
et al (to appear), 2013)). These features en-
code the difference between source and target
topic distributions according to several dis-
tance/divergence metrics.
? Following Soricut et al (2012), we employed
pseudo-reference features. The source sen-
tences were translated with three different
MT systems: an in-house phrase-based SMT
system built using Moses (Koehn et al,
2007) and trained on the parallel data pro-
vided by the organisers, the rule-based sys-
tem Systran2 and the online, publicly avail-
able, Bing Translator3. The obtained trans-
lations are compared to the target sentences
using sentence-level BLEU (Papineni et al,
2002), TER (Snover et al, 2006) and the Lev-
enshtein distance (Levenshtein, 1966).
? Also following Soricut et al (2012), one-
to-one word-alignments, with and without
Part-Of-Speech (POS) agreement, were in-
cluded as features. Using the alignment in-
formation provided by the decoder, we POS
tagged the source and target sentences with
TreeTagger (Schmidt, 1994) and the publicly
available pre-trained models for English and
Spanish. We mapped the tagsets of both lan-
guages by simplifying the initial tags and ob-
tain a reduced set of 8 tags. We applied that
simplification on the tagged sentences before
checking for POS agreement.
3 Machine Learning
In this section, we describe the learning algo-
rithms and feature selection used in our experi-
ments, leading to the two submitted systems for
the shared task.
2Systran Enterprise Server version 6
3http://www.bing.com/translator
3.1 Primary Learning Method
To estimate the post-editing effort of translated
sentences, we rely on regression models built us-
ing the Support Vector Machine (SVM) algorithm
for regression -SVR, implemented in the LIB-
SVM toolkit (Chang and Lin, 2011). To build
our final regression models, we optimise SVM
hyper-parameters (C, ? and ) using a grid-search
method with 5-fold cross-validation for each pa-
rameter triplet. The parameters leading to the best
MAE, RMSE and Pearson?s correlation coefficient
(r) are kept to build the model.
3.2 Feature Selection on Feature Types
In order to reduce the feature and obtain more
compact models, we apply feature selection on
each of our 15 feature types. Examples of feature
types are language model features or topic model
features. For each feature type, we apply a feature
subset evaluation method based on the wrapper
paradigm and using the best-first search algorithm
to explore the feature space. The M5P (Wang
and Witten, 1997) regression tree algorithm im-
plemented in the Weka toolkit (Hall et al, 2009)
is used with default parameters to train and eval-
uate a regression model for each feature subset
obtained with best-first search. A 10-fold cross-
validation is performed for each subset and we
keep the features leading to the best RMSE. We
use M5P regression trees instead of -SVR be-
cause grid-search with the latter is too computa-
tionally expensive to be applied so many times.
Using feature selection in this way, we obtain 15
reduced feature sets that we combine to form the
DCU-SYMC alltypes system, containing 102
features detailed in Table 1.
3.3 Feature Binarisation
In order to aid the SVM learner, we also experi-
ment with binarising our feature set, i.e. convert-
ing our features with various feature value ranges
into features whose values are either 1 or 0. Again,
we employ regression tree learning. We train
regression trees with M5P and M5P-R4 (imple-
mented in the Weka toolkit) and create a binary
feature for each regression rule found in the trees
(ignoring the leaf nodes). For example, a binary
feature indicating whether the Bing TER score is
less than or equal to 55.685 is derived from the
4We experiment with J48 decision trees as well, but this
method did not outperform regression tree methods.
393
Backward LM
Source 1-gram perplexity.
Source & target 1-grams perplexity ratio.
Source & target 3-grams and 4-gram perplexity ratio.
Target Syntax
Frequency of tags: ADV, FS, DM, VLinf, VMinf, semicolon, VLger, NC, PDEL, VEfin, CC, CCNEG, PPx, ART, SYM,
CODE, PREP, SE and number of ambiguous tags
Frequency of least frequent POS 3-gram observed in a corpus.
Frequency of least frequent POS 4-gram and 6-gram with sentence padding (start and end of sentence tags) observed in a
corpus.
Source Syntax
Features from three probabilistic parsers. (Rubino et al, 2012).
Frequency of least frequent POS 2-gram, 4-gram and 9-gram with sentence padding observed in a corpus.
Number of analyses found and number of words, using a Lexical Functional Grammar of English as described in Rubino
et al (2012).
LM
Source unigram perplexity.
Target 3-gram and 4-gram perplexity with sentence padding.
Source & target 1-gram and 5-gram perplexity ratio.
Source & target unigram log-probability.
Decoder
Component scores during decoding.
Number of phrases in the best translation.
Number of translation options.
N -gram Frequency
Target 2-gram in second and third frequency quartiles.
Target 3-gram and 5-gram in low frequency quartiles.
Number of target 1-gram seen in a corpus.
Source & target 1-grams in highest and second highest frequency quartile.
One-to-One Word-Alignment
Count of O2O word alignment, weighted by target sentence length.
Count of O2O word alignment with POS agreement, weighted by count of O2O, by source length, by target length.
Pseudo-Reference
Moses translation TER score.
Bing translation number of words and TER score.
Systran sBLEU, number of substitutions and TER score.
Surface
Source number of punctuation marks and average words occurrence in source sentence.
Target number of punctuation marks, uppercased letters and binary value if the last character of the sentence is a punctuation
mark.
Ratio of source and target sentence lengths, average word length and number of punctuation marks over sentence lengths.
Topic Model
Cosine distance between source and target topic distributions.
Jensen-Shannon divergence between source and target topic distributions.
Word Alignment
Averaged number of source words aligned per target words with p(s|t) thresholds: 1.0, 0.75, 0.5, 0.25, 0.01
Averaged number of source words aligned per target words with p(s|t) = 0.01 weighted by target words frequency
Averaged number of target words aligned per source word with p(t|s) = 0.01 weighted by source words frequency
Ratio of source and target averaged aligned words with thresholds: 1.0 and 0.1, and with threshold: 0.75, 0.5, 0.25 weighted
by words frequency
Table 1: Features selected with the wrapper approach using best-first search and M5P. These features are
included in the submitted system alltypes.
394
Feature to which threshold t is applied t (?)
Target 1-gram backward LM log-prob. ?35.973
Target 3-gram backward LM perplexity 7144.99
Probabilistic parsing feature 3.756
Probabilistic parsing feature 57.5
Frequency of least frequent POS 6-gram 0.5
Source 3-gram LM log-prob. 65.286
Source 4-gram LM perplexity with padding 306.362
Target 2-gram LM perplexity 176.431
Target 4-gram LM perplexity 426.023
Target 4-gram LM perplexity with padding 341.801
Target 5-gram LM perplexity 112.908
Ratio src&trg 5-gram LM log-prob. 1.186
MT system component score ?50
MT system component score ?0.801
Source 2-gram frequency in low quartile 0.146
Ratio src&trg 2-gram in high freq. quartile 0.818
Ratio src&trg 3-gram in high freq. quartile 0.482
O2O word alignment 15.5
Pseudo-ref. Moses Levenshtein 19
Pseudo-ref. Moses TER 21.286
Pseudo-ref. Bing TER 16.905
Pseudo-ref. Bing TER 23.431
Pseudo-ref. Bing TER 37.394
Pseudo-ref. Bing TER 55.685
Pseudo-ref. Systran sBLEU 0.334
Pseudo-ref. Systran TER 36.399
Source average word length 4.298
Target uppercased/lowercased letters ratio 0.011
Ratio src&trg average word length 1.051
Source word align., p(s|t) > 0.75 11.374
Source word align., p(s|t) > 0.1 485.062
Source word align., p(s|t) > 0.75 weighted 0.002
Target word align., p(t|s) > 0.01 weighted 0.019
Word align. ratio p > 0.25 weighted 1.32
Table 2: Features selected with the M5P-R M50
binarisation approach. For each feature, the cor-
responding rule indicates the binary feature value.
These features are included in the submitted sys-
tem combine in addition to the features presented
in Table 1.
regression rule Bing TER score ? 55.685.
The primary motivation for using regression
tree learning in this way was to provide a quick
and convenient method for binarising our feature
set. However, we can also perform feature selec-
tion using this method by experimenting with vari-
ous minimum leaf sizes (Weka parameter M ). We
plot the performance of the M5P and M5P-R (opti-
mising towards correlation) over the parameter M
and select the best three values of M . To experi-
ment with the effect of smaller and larger feature
sets, we further include parameters of M that (a)
lead to an approximately 50% bigger feature set
and (b) to an approximately 50% smaller feature
set.
Our DCU-SYMC combine system was built
by combining the DCU-SYMC alltypes fea-
ture set, reduced using the best-first M5P wrap-
per approach as described in subsection 3.2, with
a binarised set produced using an M5P regres-
sion tree with a minimum of 50 nodes per leaf.
This latter configuration, containing 34 features
detailed in Table 2, was selected according to the
evaluation scores obtained during cross-validation
on the training set using -SVR, as described in
the next section. Finally, we run a greedy back-
ward feature selection algorithm wrapping -SVR
on both DCU-SYMC alltypes and DCU-SYMC
combine in order to optimise our feature sets for
the SVR learning algorithm, removing 6 and 2 fea-
tures respectively.
4 System Evaluation and Results
In this section, we present the results obtained with
-SVR during 5-fold cross-validation on the train-
ing set and the final results obtained on the test
set. We selected two systems to submit amongst
the different configurations based on MAE, RMSE
and r. As several systems reach the same perfor-
mance according to these metrics, we use the num-
ber of support vectors (noted SV) as an indicator
of training data over-fitting. We report the results
obtained with some of our systems in Table 3.
The results show that the submitted sys-
tems DCU-SYMC alltypes and DCU-SYMC
combine lead to the best scores on cross-
validation, but they do not outperform the system
combining the 15 feature types without feature se-
lection (15 types). This system reaches the best
scores on the test set compared to all our systems
built on reduced feature sets. This indicates that
we over-fit and fail to generalise from the training
data.
Amongst the systems built using reduced fea-
ture sets, the M5P-R M80 system, based on the
tree binarisation approach using M5P-R, yields
the best results on the test set on 3 out of 4 offi-
cial metrics. These results indicate that this sys-
tem, trained on 16 features only, tends to estimate
HTER scores more accurately on the unseen test
data. The results of the two systems based on
the M5P-R binarisation method are the best com-
pared to all the other systems presented in this
Section. This feature binarisation and selection
method leads to robust systems with few features:
31 and 16 for M5P-R M50 and M5P-R M80 re-
spectively. Even though these systems do not lead
to the best results, they outperform the two sub-
mitted systems on one metric used to evaluate the
395
Cross-Validation Test
System nb feat MAE RMSE r SV MAE RMSE DeltaAvg Spearman
15 types 442 0.106 0.138 0.604 1194.6 0.126 0.156 0.108 0.625
M5P M50 34 0.106 0.138 0.600 1417.8 0.135 0.167 0.102 0.586
M5P M130 4 0.114 0.145 0.544 750.6 0.142 0.173 0.079 0.517
M5P-R M50 31 0.106 0.137 0.610 655.4 0.135 0.166 0.100 0.591
M5P-R M80 16 0.107 0.139 0.597 570.6 0.134 0.165 0.106 0.597
alltypes? 96 0.104 0.135 0.624 1130.6 0.135 0.171 0.101 0.589
combine? 134 0.104 0.134 0.629 689.8 0.134 0.166 0.098 0.588
Table 3: Results obtained with different regression models, during cross-validation on the training set
and on the test set, depending on the feature selection method. Systems marked with ? were submitted
for the shared task.
scoring task and two metrics to evaluate the rank-
ing task.
On the systems built using reduced feature sets,
we observe a difference of approximately 0.03pt
absolute between the MAE and RMSE scores ob-
tained during cross-validation and those on the test
set. Such a difference can be related to train-
ing data over-fitting, even though the feature sets
obtained with the tree binarisation methods are
small. For instance, the system M5P M130 is
trained on 4 features only, but the difference be-
tween cross-validation and test MAE scores is
similar to the other systems. We see on the fi-
nal results that our feature selection methods is an
over-fitting factor: by selecting the features which
explain well the training set, the final model tends
to generalise less. The selected features are suited
for the specificities of the training data, but are less
accurate at predicting values on the unseen test set.
5 Discussion
Training data over-fitting is clearly shown by the
results presented in Table 3, indicated by the per-
formance drop between results obtained during
cross-validation and the ones obtained on the test
set. While this drop may be related to data over-
fitting, it may also be related to the use of differ-
ent machine learning methods for feature selec-
tion (M5P and M5P-R) and for building the fi-
nal regression models (-SVR). In order to ver-
ify this aspect, we build two regression models
using M5P, based on the feature sets alltypes
and combine. Results are presented in Table 4
and show that, for the alltypes feature set, the
RMSE, DeltaAvg and Spearman scores are im-
proved using M5P compared to SVM. For the
combine feature set, the scoring results (MAE
and RMSE) are better using SVM, while the rank-
ing results are similar for both machine learning
methods.
The performance drop between the results on
the training data (or a development set) and the
test data was also observed by the highest ranked
participants in the WMT12 QE shared task. To
compare our system without feature selection to
the winner of the previous shared task, we eval-
uate the 15 types system in Table 3 using the
WMT12 QE dataset. The results are presented in
Table 5. We can see that similar MAEs are ob-
tained with our feature set and the WMT12 QE
winner, whereas our system gets a higher RMSE
(+0.01). For the ranking scores, our system is
worse using the DeltaAvg metric while it is bet-
ter on Spearman coefficient.
6 Conclusion
We presented in this paper our experiments for the
WMT13 Quality Estimation shared task. Our ap-
proach is based on the extraction of a large ini-
tial feature set, followed by two feature selection
methods. The first one is a wrapper approach us-
ing M5P and a best-first search algorithm, while
the second one is a feature binarisation approach
using M5P and M5P-R. The final regression mod-
els were built using -SVR and we selected two
systems to submit based on cross-validation re-
sults.
We observed that our system reaching the best
scores on the test set was not a system trained on
a reduced feature set and it did not yield the best
cross-validation results. This system was trained
using 442 features, which are the combination of
15 different feature types. Amongst the systems
built on reduced sets, the best results are obtained
396
System nb feat MAE RMSE DeltaAvg Spearman
alltypes 96 0.135 0.165 0.104 0.604
combine 134 0.139 0.169 0.098 0.587
Table 4: Results obtained with the two feature sets contained in our submitted systems using M5P to
build the regression models instead of -SVR.
System nb feat MAE RMSE DeltaAvg Spearman
WMT12 winner 15 0.61 0.75 0.63 0.64
15 types 442 0.61 0.76 0.60 0.65
Table 5: Results obtained on WMT12 QE dataset with our best system (15 types) compared to WMT12
QE highest ranked team, in the Likert score prediction task.
using the feature binarisation approach M5P-R
80, which contains 16 features selected from our
initial set of features. The tree-based feature bina-
risation is a fast and flexible method which allows
us to vary the number of features by optimising the
leaf size and leads to acceptable results with a few
selected features.
Future work involves a deeper analysis of the
over-fitting effect and an investigation of other
methods in order to outperform the non-reduced
feature set. We are also interested in finding a ro-
bust way to optimise the leaf size parameter for
our tree-based feature binarisation method, with-
out using cross-validation on the training set with
an SVM algorithm.
Acknowledgements
The research reported in this paper has been
supported by the Research Ireland Enterprise
Partnership Scheme (EPSPG/2011/102 and EP-
SPD/2011/135) and Science Foundation Ireland
(Grant 12/CE/I2267) as part of the Centre for
Next Generation Localisation (www.cngl.ie)
at Dublin City University.
References
Chih-Chung Chang and Chih-Jen Lin. 2011.
LIBSVM: A Library for Support Vector Ma-
chines. ACM Transactions on Intelligent Sys-
tems and Technology, 2:27:1?27:27. Soft-
ware available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The WEKA Data Mining Software: an
Update. ACM SIGKDD Explorations Newsletter,
11(1):10?18.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
pages 177?180.
Vladimir I Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet physics doklady, volume 10, page 707.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311?
318.
Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-
hann Roturier, Rasul Samad Zadeh Kaljahi, and Fred
Hollowood. 2012. DCU-Symantec Submission for
the WMT 2012 Quality Estimation Task. In Pro-
ceedings of the Seventh WMT, pages 138?144.
Raphael Rubino et al (to appear). 2013. Topic Models
for Translation Quality Estimation for Gisting Pur-
poses. In Proceeding of MT Summit XIV.
Helmut Schmidt. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Natu-
ral Language Processing.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA, pages 223?231.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceedings of
the Seventh WMT, pages 145?151.
Yong Wang and Ian H Witten. 1997. Inducing Model
Trees for Continuous Classes. In Proceedings of
ECML, pages 128?137. Prague, Czech Republic.
397
Workshop on Humans and Computer-assisted Translation, pages 66?71,
Gothenburg, Sweden, 26 April 2014.
c?2014 Association for Computational Linguistics
The ACCEPT Portal: An Online Framework for the Pre-editing and
Post-editing of User-Generated Content
Violeta Seretan
FTI/TIM
University of Geneva
Switzerland
Violeta.Seretan@unige.ch
Johann Roturier
Symantec Ltd.
Dublin, Ireland
johann roturier@symantec.com
David Silva
Symantec Ltd.
Dublin, Ireland
David Silva@symantec.com
Pierrette Bouillon
FTI/TIM
University of Geneva
Switzerland
Pierrette.Bouillon@unige.ch
Abstract
With the development of Web 2.0, a
lot of content is nowadays generated on-
line by users. Due to its characteristics
(e.g., use of jargon and abbreviations, ty-
pos, grammatical and style errors), the
user-generated content poses specific chal-
lenges to machine translation. This pa-
per presents an online platform devoted to
the pre-editing of user-generated content
and its post-editing, two main types of hu-
man assistance strategies which are com-
bined with domain adaptation and other
techniques in order to improve the trans-
lation of this type of content. The plat-
form has recently been released publicly
and is being tested by two main types of
user communities, namely, technical fo-
rum users and volunteer translators.
1 Introduction
User-generated content ? i.e., information posted
by Internet users in social communication chan-
nels like blogs, forum posts, social networks ? is
one of the main sources of information available
today. Huge volumes of such content are created
each day, reach a very broad audience instantly.
1
The democratisation of content creation due
to the emergence of the Web 2.0 paradigm also
means a diversification of the languages used on
the Internet.
2
Despite its availability, the new con-
tent is only accessible to the speakers of the lan-
guage in which it was created. The automatic
translation of user-generated content is therefore
one of the key issues to be addressed in the field of
human language technologies. However, as stated
1
For instance, 58 million tweets are sent on aver-
age per day (http://www.statisticbrain.com/
twitter-statistics/).
2
See http://en.wikipedia.org/wiki/
Languages_used_on_the_Internet for statistics.
by Jiang et al. (2012), despite the obvious bene-
fits, there are relatively little attempts at translating
user-generated content.
The reason may lie in the fact that user-ge-
nerated content is very challenging for machine
translation. As shown, among others, by Nagara-
jan and Gamon (2011), there are several charac-
teristics of this content that pose new process-
ing challenges with respect to traditional content:
informal style, slang, abbreviations, specific ter-
minology, irregular grammar and spelling. In-
deed, Internet users are rarely professional writ-
ers.
3
They often write in a language which is not
their own, and sacrifice quality for speed, not pay-
ing attention to spelling, punctuation, or grammar
rules.
The ACCEPT project
4
addresses these chal-
lenges by developing a technology integrating
modules for automatic and manual content pre-
editing, statistical machine translation, as well
as output evaluation and post-editing. Thus, the
project aims to improve the translation of user-ge-
nerated content by proposing a full workflow, in
which the participation of humans is essential.
The application scenario considered in the
project are user communities sharing specific in-
formation on a given topic. The project focuses,
more specifically, on the following use cases:
1. the commercial use case, in which the tar-
get community is the user community built
around a software company in order for
members to help each other with issues re-
lated to products;
2. the NGO use case, in which non-go-
vernmental organisations such as Doctors
Without Borders produce health-care content
for distributions in areas of need.
3
Even when they are, as in the case of government agen-
cies, the type of content produced (e.g., tweets) still poses
?multiple challenges? to translation (Gotti et al., 2013).
4
http://www.accept-project.eu/
66
The language pairs considered in the project are
English to French, German and Japanese, as well
as French into English for the first use case (in-
volving technical forum information), and French
to and from English for the second use case (in-
volving healthcare information).
Past halfway into its research program, the
project has accomplished significant progress in
the main areas mentioned above (pre-editing, sta-
tistical machine translation, post-editing, and eval-
uation). The ACCEPT technology has recently
been released to the broad public as an on-
line framework, which demonstrates the different
modules of the workflow and provides access to
associated software components (plug-ins, APIs),
as well as to documentation. The pre-editing tech-
nology has been deployed on the targeted user fo-
rum
5
, allowing users to check their messages be-
fore posting them. The post-editing technology is
being used by a community of translators, which
provide pro-bono translation services to the NGOs
considered in our second use case.
In this paper, we describe the framework by pre-
senting its architecture and main modules (Sec-
tion 2). We discuss related work in Section 3 and
conclude in Section 4.
2 The Framework
The ACCEPT technology has been made acces-
sible to a broad audience in the form of an on-
line framework, i.e., an integrated environment
where registered users can perform pre-editing,
post-editing and evaluation work. The framework
? henceforth, the ACCEPT Portal ? is hosted on a
cloud computing infrastructure and is available at
www.accept-portal.eu.
2.1 Architecture of the Framework
As explained in Section 1, the ACCEPT techno-
logy consists of the following main modules:
1. Pre-editing module;
2. Machine translation module,
3. Post-editing module,
4. Evaluation module.
The typical workflow is incremental, but the
modules are independent. They can be used both
within and outside the portal, as they are built on a
REST API facilitating integration.
5
https://community.norton.com/
In the remaining of this section, we introduce
each of the framework modules.
6
2.2 Pre-editing Module
The pre-editing module leverages existing ling-
ware which provides authoring support rules
aimed at language professionals, by relying on
shallow language processing (Bredenkamp et al.,
2000). The existing English checker and the lin-
guistic resources on which it relies have been ex-
tended and adapted to suit the type of data gener-
ated by community users. In particular, the soft-
ware extension consisted of designing a number
of pre-editing rules aimed at source normalisation,
for the purpose of making the input text easier
to handle by the SMT systems. In the case of
French, the pre-editing rules have been designed
from scratch. The pre-editing rules pertain to the
levels of spelling, grammar, style and terminology.
They are defined using the original lingware?s rule
formalism and are incorporated into a server dedi-
cated to the project.
The rule development was corpus-driven and
was performed on data collected for this purpose.
A stable set of pre-edition rules is available in
the portal for each of the domains and source
languages considered (i.e., technical forum and
heathcare data in English and French). The rules
are described in detail in the project deliverable
D 2.2 (2013).
The rules proposed have been evaluated individ-
ually and in combination (Roturier et al., 2012;
Gerlach et al., 2013; Seretan et al., 2014). As
a general observation, it is important to notice
that, for SMT, the improvement of the input text
does not go hand in hand with the improvement of
translation. For example, in French the rule for
correcting verbal forms to the subjunctive tense
had a negative impact since the subjunctive is not
frequent in the training data. Conversely, it was
possible to define lexical reformulations which de-
graded the quality of the input text, but had a po-
sitive impact on translation quality.
The combined impact of the rule applica-
tion was measured in a variety of settings in a
large-scale evaluation campaign involving transla-
tion students (Seretan et al., 2014). As the rules
are divided into two major groups, those automati-
cally applicable and those requiring human inter-
6
The MT module will be omitted, as it is not part of the
portal. The interested reader is referred to D 4.2 (2013).
67
Figure 1: The ACCEPT Pre-edit plug-in in action (screen capture)
vention, the evaluation was carried out for the full
set of rules, as well as for the automatic rules only.
In addition, the evaluation was performed in both
a monolingual and a bilingual setting, i.e., with
the evaluators having or not access to the source
text, and it involved evaluation scales of different
granularities. The evaluation results showed a sys-
tematic statistically significant improvement over
the baseline when pre-editing is performed on the
source content. More details about the evalua-
tion methodology and results can be found in the
project deliverable D 9.2.2 (2013).
A data excerpt illustrating the impact of pre-
editing on translation quality is presented in Ex-
ample 1 below. The simple correction of an ac-
cented letter, du? d?u, leads to the change of seve-
ral target words, and to a much better translation of
the input sentence.
1. a) Source (original):
J?ai du m?absenter hier apr`es midi.
b) Source (pre-edited):
J?ai d?u m?absenter hier apr
`es midi.
c) Target (original):
I have the leave me yesterday afternoon.
d) Target (pre-edited):
I had to leave yesterday afternoon.
The pre-editing component of the ACCEPT
technology is available as a JQuery plug-in, which
can be downloaded and installed by Web applica-
tion owners, so that it can be used with text areas
and other text-bearing elements. APIs and ac-
companying documentation have also been made
available, so that the pre-editing rules can be
leveraged in automatic steps, without the plug-in,
across devices and platforms. A demo site illus-
trating the use of the plug-in in a TinyMCE envi-
ronment is available on the portal (see Figure 1).
The latest developments of the pre-editing mo-
dule include the possibility for users to customise
the application of rule sets, in particular, to ignore
specific rules and to manage their own dictionary,
in order to prevent the activation of checking flags.
2.3 Post-editing Module
The post-editing module of the framework (see
also Roturier et al., (2013)) is designed to fulfil
the project?s objective of collecting post-editing
data in order to learn correction rules and, through
feedback loops, to integrate them into the SMT
engines (with the goal of automating corrections
whenever possible). The project relies on the par-
ticipation of volunteer community members, who
are subject matter experts, native speakers of the
68
Figure 2: The ACCEPT Portal showing the post-editing demo (screen capture)
target language and, possibly, of the source lan-
guage. Accordingly, the post-editing environment
(see Figure 2) provides functionalities for both
monolingual and bilingual post-editing.
The post-editing text is organised in tasks be-
longing to post-editing projects. The latter are
created and managed by project administrators,
by defining the project settings (e.g., source and
target languages, monolingual or bilingual mode,
collaborative or non-collaborative type
7
), upload-
ing the text for each task
8
, inviting participants by
e-mail, and monitoring revision progress.
The post-editors edit the target text in a
sentence-by-sentence fashion. They have access
to the task guidelines and to help documentation.
The interface of the post-editing window displays
the whole text, through which they can navigate
with next-previous buttons or by clicking on a
specific sentence. Users can check the text they
are editing by accessing, with a button, the con-
tent checking technology described in Section 2.2.
Their actions ? in terms of keystrokes and usage
7
In a collaborative editing scenario, users may see edits
from other users and do not have to repeat them when work-
ing on the same project task. Conflicts are avoided by pre-
venting concurrent access.
8
Currently, the JSON format is used for the input data.
of translation options ? and time spent editing are
recorded in the portal.
9
When they are done edi-
ting, they can click on a button marking the com-
pletion of the task. At any time, they can interrupt
their work and save their results for later.
Users can enter a comment on the post-editing
task they have performed. The feedback elicited
from users include the difficulty of the task and
their sentiment (Was it easy to post-edit? Did you
enjoy the post-editing task?). For systematically
collecting user feedback, the project administra-
tors can specify on the project configuration page
a link to a post-task survey, which will be sent to
users after completing their tasks.
The post-editing module includes a JQuery
plug-in for deployment in any Web-based envi-
ronment; a dedicated section of the portal; APIs
enabling the use of the post-editing functionality
outside the portal; and sample evaluation projects
for several language pairs.
The post-editing technology has been exten-
sively used in specific post-editing campaigns in-
volving translator volunteers and Amazon Me-
chanical Turk
10
workers. The campaigns, includ-
9
The post-editing data is exported in XLIFF format.
10
The integration was done via the ACCEPT API.
69
ing reports on post-task surveys, are documented
inter alia in deliverable D 8.1.2 (2013). A notable
finding was that professional translators, who were
reticent towards MT before the task, had a more
positive sentiment after post-editing and their mo-
tivation to post-edit in the future increased.
2.4 Evaluation Module
The role of the evaluation module is to support the
collection of user ratings for assessing the quality
of source, machine-translated and post-edited con-
tent, and, ultimately, to support the development
of the technology created in the project.
This module groups several software compo-
nents: an evaluation environment available as a
section of the portal; APIs enabling the collection
of user evaluations in-context; and a third com-
ponent which is a customisation of the Appraise
toolkit for the collaborative collection of human
judgements (Federmann, 2012).
As in the case of post-editing module, this mod-
ule provides functionality for creating and man-
aging projects. Using the evaluation environ-
ment/APIs, project creators can define question
categories, add questions and possible answers,
and upload evaluation data (in JSON format). For
traditional evaluation projects, the Appraise sys-
tem is used instead.
3 Related Work
Transforming the source text in order to better
fit the needs of machine translation is a well-
investigated area of research. Strategies like
source control, source re-ordering, or source sim-
plification at the lexical or structural level have
been largely explored; for reviews, see, for in-
stance, Huhn (2013), Kazemi (2013), and Feng
(2008), respectively.
User-generated content has been investigated
in the context of machine translation in recent
work dealing specifically with spelling correc-
tion (Bertoldi et al., 2010; Formiga and Fonol-
losa, 2012); lexical normalisation by substituting
ill-formed words with their correct counterpart,
e.g., makn ? making (Han and Baldwin, 2011);
missing word ? e.g., zero-pronoun ? recovery and
punctuation correction (Wang and Ng, 2013).
Rather than focusing on specific phenomena or
Web genres (i.e., tweets), we adopt a more gen-
eral approach in which we address the problem of
source normalisation at multiple levels ? punctua-
tion, spelling, grammar, and style ? for any type of
linguistically imperfect text.
Another peculiarity of our approach is that it
is rule-based and does not require parallel data
for learning corrections. In exchange, a limi-
tation of our pre-editing approach is that it is
language-dependent, as the underlying technology
is based on shallow analysis and is therefore time-
expensive to extend to a new language.
The post-editing technology differs from exist-
ing (standalone or Web-based) dedicated tools ?
e.g., iOmegaT
11
or MateCat
12
? in that it is tai-
lored to community users, and, consequently, it
is lighter, it generates more concise reports, and
a simpler interface replaces the grid-like format
for presenting data. Another specificity is that it
is sufficiently flexible to be used in other environ-
ments (e.g., Amazon Mechanical Turk, cf. ?2.3).
4 Conclusion
The technology outlined in this paper demon-
strates a specific case of human-computer interac-
tion, in which, for the first time, several modules
are integrated in a full process in which human
pre-editors, post-editors and evaluators play a key
role for improving the translation of community
content. The technology is freely accessible in the
online portal, has been deployed on a major user
forum, and can be downloaded for integration in
other Web-based environments. Since it is built on
top of a REST API, it is portable across devices
and platforms. The technology would be useful to
anyone who needs information instantly and relia-
bly translated, despite linguistic imperfections.
One of the main future developments concerns
the further improvement of SMT, by exploring,
in particular, the use of text analytics and senti-
ment detection. In addition, by incorporating post-
editing rules and developing techniques to change
the phrase table and system parameters dynam-
ically, it will be possible to reduce the amount
of error corrections that human post-editors have
to perform repeatedly. Another major develop-
ment (joint work with the CASMACAT European
project) will focus on novel types of assistance for
translators, aimed specifically at helping transla-
tors by identifying problematic parts of the ma-
chine translation output and signalling the para-
phrases that are more likely to be useful.
11
http://try-and-see-mt.org/
12
http://www.matecat.com/
70
Acknowledgments
The research leading to these results has received
funding from the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement n
o
288769.
References
Nicola Bertoldi, Mauro Cettolo, and Marcello Fe-
derico. 2010. Statistical machine translation of texts
with misspelled words. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 412?419, Los Angeles,
California.
Andrew Bredenkamp, Berthold Crysmann, and Mirela
Petrea. 2000. Looking for errors: A declarative for-
malism for resource-adaptive language checking. In
Proceedings of the Second International Conference
on Language Resources and Evaluation, Athens,
Greece.
2013. ACCEPT deliverable D 2.2 Definition of
pre-editing rules for English and French (final
version). http://www.accept.unige.
ch/Products/D2_2_Definition_of_
Pre-Editing_Rules_for_English_and_
French_with_appendixes.pdf.
2013. ACCEPT deliverable D 9.2.2: Survey of
evaluation results. http://www.accept.
unige.ch/Products/D_9_2_Survey_of_
evaluation_results.pdf.
2013. ACCEPT deliverable D 4.2 Report on
robust machine translation: domain adap-
tation and linguistic back-off. http:
//www.accept.unige.ch/Products/
D_4_2_Report_on_robust_machine_
translation_domain_adaptation_and_
linguistic_back-off.pdf.
2013. ACCEPT deliverable D 8.1.2 Data and
report from user studies - Year 2. http:
//www.accept.unige.ch/Products/D_
8_1_2_Data_and_report_from_user_
studies_-_Year_2.pdf.
Christian Federmann. 2012. Appraise: An open-
source toolkit for manual evaluation of machine
translation output. The Prague Bulletin of Mathe-
matical Linguistics (PBML), 98:25?35.
Lijun Feng. 2008. Text simplification: A survey.
Technical report, CUNY.
Llu??s Formiga and Jos?e A. R. Fonollosa. 2012. Dea-
ling with input noise in statistical machine transla-
tion. In Proceedings of COLING 2012: Posters,
pages 319?328, Mumbai, India.
Johanna Gerlach, Victoria Porro, Pierrette Bouillon,
and Sabine Lehmann. 2013. La pr?e?edition avec
des r`egles peu co?uteuses, utile pour la TA statistique
des forums ? In Actes de la 20e conf?erence sur
le Traitement Automatique des Langues Naturelles
(TALN?2013), pages 539?546, Les Sables d?Olonne,
France.
Fabrizio Gotti, Philippe Langlais, and Atefeh Farzin-
dar. 2013. Translating government agencies? tweet
feeds: Specificities, problems and (a few) solutions.
In Proceedings of the Workshop on Language Anal-
ysis in Social Media, pages 80?89, Atlanta, Georgia.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 368?378, Port-
land, Oregon.
Jie Jiang, Andy Way, and Rejwanul Haque. 2012.
Translating user-generated content in the social net-
working space. In Proceedings of the Tenth Biennial
Conference of the Association for Machine Transla-
tion in the Americas (AMTA-2012), San Diego, Cali-
fornia.
Arefeh Kazemi, Amirhassan Monadjemi, and Moham-
madali Nematbakhsh. 2013. A quick review on re-
ordering approaches in statistical machine transla-
tion systems. IJCER, 2(4).
Tobias Kuhn. 2013. A survey and classification of con-
trolled natural languages. Computational Linguis-
tics.
Meenakshi Nagarajan and Michael Gamon, editors.
2011. Proceedings of the Workshop on Language
in Social Media (LSM 2011). Portland, Oregon.
Johann Roturier, Linda Mitchell, Robert Grabowski,
and Melanie Siegel. 2012. Using automatic ma-
chine translation metrics to analyze the impact of
source reformulations. In Proceedings of the Con-
ference of the Association for Machine Translation
in the Americas (AMTA), San Diego, California.
Johann Roturier, Linda Mitchell, and David Silva.
2013. The ACCEPT post-editing environment: a
flexible and customisable online tool to perform and
analyse machine translation post-editing. In Pro-
ceedings of MT Summit XIV Workshop on Post-edi-
ting Technology and Practice, Nice, France.
Violeta Seretan, Pierrette Bouillon, and Johanna Ger-
lach. 2014. A large-scale evaluation of pre-edi-
ting strategies for improving user-generated content
translation. In Proceedings of the 9th Edition of the
Language Resources and Evaluation Conference,
Reykjavik, Iceland.
Pidong Wang and Hwee Tou Ng. 2013. A beam-
search decoder for normalization of social media
text with application to machine translation. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
471?481, Atlanta, Georgia.
71
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67?77,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Syntax and Semantics in Quality Estimation of Machine Translation
Rasoul Kaljahi
??
, Jennifer Foster
?
, Johann Roturier
?
?
NCLT, School of Computing, Dublin City University, Ireland
{rkaljahi, jfoster}@computing.dcu.ie
?
Symantec Research Labs, Dublin, Ireland
{johann roturier}@symantec.com
Abstract
We employ syntactic and semantic infor-
mation in estimating the quality of ma-
chine translation from a new data set
which contains source text from English
customer support forums and target text
consisting of its machine translation into
French. These translations have been both
post-edited and evaluated by professional
translators. We find that quality estima-
tion using syntactic and semantic informa-
tion on this data set can hardly improve
over a baseline which uses only surface
features. However, the performance can
be improved when they are combined with
such surface features. We also introduce
a novel metric to measure translation ade-
quacy based on predicate-argument struc-
ture match using word alignments. While
word alignments can be reliably used,
the two main factors affecting the per-
formance of all semantic-based methods
seems to be the low quality of seman-
tic role labelling (especially on ill-formed
text) and the lack of nominal predicate an-
notation.
1 Introduction
The problem of evaluating machine translation
output without reference translations is called
quality estimation (QE) and has recently been the
centre of attention (Bojar et al., 2014) following
the seminal work of Blatz et al. (2003). Most
QE studies have focused on surface and language-
model-based features of the source and target. The
quality of translation is however closely related to
the syntax and semantics of the languages, the for-
mer concerning fluency and the latter adequacy.
While there have been some attempts to utilize
syntax in this task, semantics has been paid less
attention. In this work, we aim to exploit both
syntax and semantics in QE, with a particular fo-
cus on the latter. We use shallow semantic analy-
sis obtained via semantic role labelling (SRL) and
employ this information in QE in various ways in-
cluding statistical learning using both tree kernels
and hand-crafted features. We also design a QE
metric which is based on the Predicate-Argument
structure Match (PAM ) between the source and its
translation. The semantic-based system is then
combined with the syntax-based system to evalu-
ate the full power of structural linguistic informa-
tion. We also combine this system with a baseline
system consisting of effective surface features.
A second contribution of the paper is the release
of a new data set for QE.
1
This data set comprises
a set of 4.5K sentences chosen from customer sup-
port forum text. The machine translation of the
sentences are not only evaluated in terms of ade-
quacy and fluency, but also manually post-edited
allowing various metrics of interest to be applied
to measure different aspects of quality. All exper-
iments are carried out on this data set.
The rest of the paper is organized as follows:
after reviewing the related work, the data is de-
scribed and the semantic role labelling approach
is explained. The baseline is then introduced, fol-
lowed by the experiments with tree kernels, hand-
crafted features, the PAM metric and finally the
combination of all methods. The paper ends with
a summary and suggestions for future work.
2 Related Work
Syntax has been exploited in QE in various ways
including tree kernels (Hardmeier et al., 2012;
Kaljahi et al., 2013; Kaljahi et al., 2014b),
parse probabilities and syntactic label frequency
(Avramidis, 2012), parseability (Quirk, 2004) and
POS n-gram scores (Specia and Gim?enez, 2010).
1
The data will be made publicly available - see http://
www.computing.dcu.ie/mt/confidentmt.html
67
Turning to the role of semantic knowledge in
QE and MT evaluation in general, Pighin and
M`arquez (2011) propose a method for ranking two
translation hypotheses that exploits the projection
of SRL from a sentence to its translation using
word alignments. They first project the SRL of a
source corpus to its parallel corpus and then build
two translation models: 1) translations of proposi-
tion labelling sequences in the source to its projec-
tion in the target and 2) translations of argument
role fillers in the source to their counterparts in
the target. The source SRL is then projected to
its machine translation and the above models are
forced to translate source proposition labelling se-
quences to the projected ones. Finally the confi-
dence scores of these translations and their reach-
ability are used to train a classifier which selects
the better of the two translation hypotheses with
an accuracy of 64%. Factors hindering their clas-
sifier are word alignment limitations and low SRL
recall due to the lack of a verb or the loss of a
predicate during translation.
In MT evaluation, where reference translations
are available, Gim?enez and M`arquez (2007) use
semantic roles in building several MT evaluation
metrics which measure the full or partial lexical
match between the fillers of same semantic roles in
the hypothesis and translation, or simply the role
label matches between them. They conclude that
these features can only be useful in combination
with other features and metrics reflecting different
aspects of the quality.
Lo and Wu (2011) introduce HMEANT, a man-
ual MT evaluation metric based on predicate-
argument structure matching which involves two
steps of human engagement: 1) semantic role an-
notation of the reference and machine translation,
2) evaluating the translation of predicates and ar-
guments. The metric calculates the F
1
score of
the semantic frame match between the reference
and machine translation based on this evaluation.
To keep the costs reasonable, the first step is car-
ried out by amateur annotators who were mini-
mally trained with a simplified list of 10 thematic
roles. On a set of 40 examples, the metric is
meta-evaluated in terms of correlation with human
judgements of translation adequacy ranking, and a
correlation as high as that of HTER is reported.
Lo et al. (2012) propose MEANT, a variant of
HMEANT, which automatizes its manual steps
using 1) automatic SRL systems for (only) verb
predicates, 2) automatic alignment of predicates
and their arguments in the reference and ma-
chine translation based on their lexical similarity.
Once the predicates and arguments are aligned,
their similarities are measured using a variety of
methods such as cosine distance and even Me-
teor and BLEU. In computation of the final score,
the similarity scores replace the counts of correct
and partial translations used in HMEANT. This
metric outperforms several automatic metrics in-
cluding BLEU, Meteor and TER, but it signifi-
cantly under-performs HMEANT and HTER. Fur-
ther analysis shows that automatizing the second
step does not affect the performance of MEANT.
Therefore, it seems to be the lower accuracy of the
semantic role labelling that is responsible.
Bojar and Wu (2012) identify a set of flaws
with HMEANT and propose solutions for them.
The most important problems stem from the su-
perficial SRL annotation guidelines. These prob-
lems are exacerbated in MEANT due to the auto-
matic nature of the two steps. More recently, Lo
et al. (2014) extend MEANT to ranking transla-
tions without a reference by using phrase transla-
tion probabilities for aligning semantic role fillers
of the source and its translation.
3 Data
We randomly select 4500 segments from a large
collection of Symantec English Norton forum
text.
2
In order to be independent of any one MT
system, we translate these segments into French
with the following three systems and randomly
choose 1500 distinct segments from each.
? ACCEPT
3
: a phrase-based Moses system
trained on training sets of WMT12 releases
of Europarl and News Commentary plus
Symantec translation memories
? SYSTRAN: a proprietary rule-based system
augmented with domain-specific dictionaries
? Bing
4
: an online translation system
These translations are evaluated in two ways.
The first method involves light post-editing by
a professional human translator who is a native
2
http://community.norton.com
3
http://www.accept.unige.ch/Products/
D_4_1_Baseline_MT_systems.pdf
4
http://www.bing.com/translator(on24-
Feb-2014)
68
Adequacy Fluency
5 All meaning Flawless Language
4 Most of meaning Good Language
3 Much of meaning Non-native Language
2 Little meaning Disfluent Language
1 None of meaning Incomprehensible
Table 2: Adequacy/fluency score interpretation
French speaker.
5
Each sentence translation is then
scored against its post-edit using BLEU
6
(Papineni
et al., 2002), TER (Snover et al., 2006) and
METEOR (Denkowski and Lavie, 2011), which are
the most widely used MT evaluation metrics. Fol-
lowing Snover et al. (2006), we consider this way
of scoring MT output to be a variation of human-
targeted scoring, where no reference translation
is provided to the post-editor, so we call them
HBLEU, HTER and HMETEOR. The average scores
for the entire data set together with their standard
deviations are presented in Table 1.
7
In the second method, we asked three profes-
sional translators, who are again native French
speakers, to assess the quality of MT output in
terms of adequacy and fluency in a 5-grade scale
(LDC, 2002). The interpretation of the scores is
given in Table 2. Each evaluator was given the
entire data set for evaluation. We therefore col-
lected three sets of scores and averaged them to
obtain the final scores. The averages of these
scores for the entire data set together with their
standard deviations are presented in Table 1. To
be easily comparable to human-targeted scores,
we scale these scores to the [0,1] range, i.e. ad-
equacy/fluency scores of 1 and 5 are mapped to 0
and 1 respectively and all the scores in between
are accordingly scaled.
The average Kappa inter-annotator agreement
for adequacy scores is 0.25 and for fluency scores
0.19. However, this measurement does not dif-
ferentiate between small and large differences in
agreement. In other words, the difference between
5
The post-editing guidelines are based on the
TAUS/CNGL guidelines for achieving ?good enough?
quality downloaded from https://evaluation.
taus.net/images/stories/guidelines/taus-
cngl-machine-translation-postediting-
guidelines.pdf.
6
Version 13a of MTEval script was used at the segment
level which performs smoothing.
7
Note that HTER scores have no upper limit and can be
higher than 1 when the number of errors is higher than the
segment length. In addition, the higher HTER indicates lower
translation quality. To be comparable to the other scores, we
cut-off them at 1 and convert to 1-HTER.
1-HTER HBLEU HMeteor Adq Flu
1-HTER - - - - -
HBLEU 0.9111 - - - -
HMeteor 0.9207 0.9314 - - -
Adq 0.6632 0.7049 0.6843 - -
Flu 0.6447 0.7213 0.6652 0.8824 -
Table 3: Pearson r between pairs of metrics on the
entire 4.5K data set
scores of 5 and 4 is the same as the difference
between 5 and 2. To account for this, we use
weighted Kappa instead. Specifically, we consider
two scores of difference 1 to represent 75% agree-
ment instead of 100%. All the other differences
are considered to be a disagreement. The aver-
age weighted Kappa computed in this way is 0.65
for adequacy and 0.63 for fluency. Though the
weighting used is quite strict, the weighted Kappa
values are in the substantial agreement range.
Once we have both human-targeted and manual
evaluation scores together, it is interesting to know
how they are correlated. We calculate the Pearson
correlation coefficient r between each pair of the
five scores and present them in Table 3. HBLEU
has the highest correlation with both adequacy and
fluency scores among the human-targeted metrics.
HTER on the other hand has the lowest correla-
tion. Moreover, HBLEU is more correlated with
fluency than with adequacy which is the opposite
to HMeteor. This is expected according to the
definition of BLEU and Meteor. There is also
a high correlation between adequacy and fluency
scores. Although this could be related to the fact
that both scores are from the same evaluators, it
indicates that if either the fluency and adequacy of
the MT output is low or high, the other tends to be
the same.
The data is split into train, development and test
sets of 3000, 500 and 1000 sentences respectively.
4 Semantic Role Labelling
The type of semantic information we use in this
work is the predicate-argument structure or se-
mantic role labelling of the sentence. This infor-
mation needs to be extracted from both sides of the
translation, i.e. English and French. Though the
SRL of English has been well-studied (M`arquez
et al., 2008) thanks to the existence of two major
hand-crafted resources, namely FrameNet (Baker
et al., 1998) and PropBank (Palmer et al., 2005),
French is one of the under-studied languages in
69
1-HTER HBLEU HMeteor Adequacy Fluency
Average 0.6976 0.5517 0.7221 0.6230 0.4096
Standard Deviation 0.2446 0.2927 0.2129 0.2488 0.2780
Table 1: Average and standard deviation of the evaluation scores for the entire data set
this respect mainly due to a lack of such resources.
The only available gold standard resource is a
small set of 1000 sentences taken from Europarl
(Koehn, 2005) and manually annotated with Prop-
bank verb predicates (van der Plas et al., 2010).
van der Plas et al. (2011) attempt to tackle this
scarcity by automatically projecting SRL from the
English side of a large parallel corpus to its French
side. Our preliminary experiments (Kaljahi et al.,
2014a), however, show that SRL models trained
on the small manually annotated corpus have a
higher quality than ones trained on the much larger
projected corpus. We therefore use the 1K gold
standard set to train a French SRL model. For En-
glish, we use all the data provided in the CoNLL
2009 shared task (Haji?c et al., 2009).
We use LTH (Bj?orkelund et al., 2009), a
dependency-based SRL system, for both the En-
glish and French data. This system was among
the best performing systems in the CoNLL 2009
shared task and is straightforward to use. It comes
with a set of features tuned for each shared task
language (English, German, Japanese, Spanish,
Catalan, Czech, Chinese). We compared the per-
formance of the English and Spanish feature sets
on French and chose the former due to its higher
performance (by 1 F
1
point).
It should be noted that the English SRL data
come with gold standard syntactic annotation. On
the other hand, for our QE data set, such anno-
tation is not available. Our preliminary experi-
ments show that, since the SRL system heavily
relies on syntactic features, the performance con-
siderably drops when the syntactic annotation of
the test data is obtained using a different parser
than that of the training data. We therefore re-
place the parses of the training data with those ob-
tained automatically by first parsing the data us-
ing the Lorg PCFG-LA parser
8
(Attia et al., 2010)
and then converting them to dependencies using
Stanford converter (de Marneffe and Manning,
2008). The POS tags are also replaced with those
output by the parser. For the same reason, we re-
8
https://github.com/CNGLdlab/LORG-
Release.
place the original POS tagging of the French 1K
data with those obtained by the MElt tagger (De-
nis and Sagot, 2012).
The English SRL achieves 77.77 and 67.02 la-
belled F
1
points when trained only on the training
section of PropBank and tested on the WSJ and
Brown test sets respectively.
9
The French SRL is
evaluated using 5-fold cross-validation on the 1K
data set and obtains an F
1
average of 67.66. When
applied to the QE data set, these models identify
9133, 8875 and 8795 propositions on its source
side, post-edits and MT output respectively.
5 Baseline
We compare the results of our experiments to a
baseline built using the 17 baseline features of the
WMT QE shared task (Bojar et al., 2014). These
features provide a strong baseline and have been
used in all three years of the shared task. We
use support vector regression implemented in the
SVMLight toolkit
10
with Radial Basis Function
(RBF) kernel to build this baseline. To extract
these features, a parallel English-French corpus
is required to build a lexical translation table us-
ing GIZA++ (Och and Ney, 2003). We use the
Europarl English-French parallel corpus (Koehn,
2005) plus around 1M segments of Symantec
translation memory.
Table 4 shows the performance of this system
(WMT17) on the test set measured by Root Mean
Square Error (RMSE) and Pearson correlation co-
efficient (r). We only report the results on predict-
ing four of the metrics introduced above, omitting
HMeteor due to space constraints. C and ? pa-
rameters are tuned on the development set with re-
spect to r. The results show a significant differ-
ence between manual and human-targeted metric
prediction. The higher r for the former suggests
that the patterns of these scores are easier to learn.
The RMSE seems to follow the standard deviation
9
Although the English SRL data are annotated for noun
predicates as well as verb predicates, since the French data
has only verb predicate annotations, we only consider verb
predicates for English.
10
http://svmlight.joachims.org/
70
of the scores as the same ranking is seen in both.
6 Tree Kernels
Tree kernels (Moschitti, 2006) have been success-
fully used in QE by Hardmeier et al. (2012) and
in our previous work (Kaljahi et al., 2013; Kal-
jahi et al., 2014b), where syntactic trees are em-
ployed. Tree kernels eliminate the burden of man-
ual feature engineering by efficiently utilizing all
subtrees of a tree. We employ both syntactic and
semantic information in learning quality scores,
using the SVMLight-TK
11
, a support vector ma-
chine (SVM) implementation of tree kernels.
We implement a syntactic tree kernel QE sys-
tem with constituency and dependency trees of
the source and target side, following our previous
work (Kaljahi et al., 2013; Kaljahi et al., 2014b).
The performance of this system (TKSyQE) is
shown in Table 4. Unlike our previous results,
where the syntax-based system significantly out-
performed the WMT17 baseline, TKSyQE can only
beat the baseline in HTER and fluency prediction,
with neither difference being statistically signifi-
cant and it is below the baseline for HBLEU and
adequacy prediction.
12
It should be noted that in
our previous work, a WMT News data set was
used as the QE data set which, unlike our new data
set, is well-formed and in the same domain as the
parsers? training data. The discrepancy between
our new and old results suggests that the perfor-
mance is strongly dependent on the data set.
Unlike syntactic parsing, semantic role la-
belling does not produce a tree to be directly used
in the tree kernel framework. There can be var-
ious ways to accomplish this goal. We first try
a method inspired by the PAS format introduced
by Moschitti et al. (2006). In this format, a fixed
number of nodes are gathered under a dummy root
node as slots of one predicate and 6 arguments of
a proposition (one tree per predicate). Each node
dominates an argument label or a dummy label for
the predicate, which in turn dominates the POS
tag of the argument or the predicate lemma. If a
proposition has more than 6 arguments they are
ignored, if it has fewer than 6 arguments, the extra
slots are attached to a dummy null label. Note that
these trees are derived from the dependency-based
SRL of both the source and target side (Figure
11
http://disi.unitn.it/moschitti/Tree-
Kernel.htm
12
We use paired bootstrap resampling Koehn (2004) for
statistical significance testing.
1-HTER HBLEU Adq Flu
RMSE
WMT17 0.2310 0.2696 0.2219 0.2469
TKSyQE 0.2267 0.2721 0.2258 0.2431
D-PAS 0.2489 0.2856 0.2423 0.2652
D-PST 0.2409 0.2815 0.2383 0.2606
C-PST 0.2400 0.2809 0.2410 0.2615
CD-PST 0.2394 0.2795 0.2373 0.2578
TKSSQE 0.2269 0.2722 0.2253 0.2425
Pearson r
WMT17 0.3661 0.3806 0.4710 0.4769
TKSyQE 0.3693 0.3559 0.4306 0.5013
D-PAS 0.1774 0.1843 0.2770 0.3252
D-PST 0.2136 0.2450 0.3169 0.3670
C-PST 0.2319 0.2541 0.2966 0.3616
CD-PST 0.2311 0.2714 0.3303 0.3923
TKSSQE 0.3682 0.3537 0.4351 0.5046
Table 4: RMSE and Pearson r of the 17 base-
line features (WMT17) and tree kernel systems;
TKSyQE: syntax-based tree kernels, D-PAS:
dependency-based PAS tree kernels of Moschitti
et al. (2006), D-PST, C-PST and CD-PST:
dependency-based, constituency-based proposi-
tion subtree kernels and their combination,
TKSSQE: syntactic-semantic tree kernels
1(a)). The results are shown in Table 4 (D-PAS).
The performance is statistically significantly lower
than the baseline.
13
In order to encode more information in the trees,
we propose another format in which proposition
subtrees (PST) of the sentence are gathered un-
der a dummy root node. A dependency PST (Fig-
ure 1(b)) is formed by the predicate label under
the root dominating its lemma and all its argu-
ments roles. Each of these nodes in turn dominates
three nodes: the argument word form (the predi-
cate word form for the case of a predicate lemma),
its syntactic dependency relation to its head and its
POS tag. We preserve the order of arguments and
predicate in the sentence.
14
This system is named
D-PST in Table 4. Tree kernels in this format sig-
nificantly outperform D-PAS. However, the per-
formance is still far lower than the baseline.
The above formats are based on dependency
trees. We try another PST format derived from
constituency trees. These PSTs (Figure 1(c)) are
the lowest common subtrees spanning the predi-
cate node and its argument nodes and are gath-
ered under a dummy root node. The argument role
13
Note that the only lexical information in this format is
the predicate lemma. We tried replacing the POS tags with
argument word forms, which led to a slight degradation.
14
This format is chosen among several other variations due
to its higher performance.
71
(a) D-PAS (b) D-PST (c) C-PST (d) D-TKSSQE (e) C-TKSSQE
Figure 1: Semantic tree kernel formats for the sentence: Can anyone help?
labels are concatenated with the syntactic non-
terminal category of the argument node. Predi-
cates are not marked. However, our dependency-
based SRL is required to be converted into a
constituency-based format. While constituency-
to-dependency conversion is straightforward us-
ing head-finding rules (Surdeanu et al., 2008),
the other way around is not. We therefore ap-
proximate the conversion using a heuristic we call
(D2C).
15
As shown in Table 4, the system built us-
ing these PSTs C-PST improves over D-PST for
human-targeted metric prediction, but not man-
ual metric prediction. However, when they are
combined in CD-PST, we can see improvement
over the highest scores of both systems, except
for HTER prediction for Pearson r. The fluency
prediction improvement is statistically significant.
The other changes are not statistically significant.
An alternative approach to formulating seman-
tic tree kernels is to augment syntactic trees with
semantic information. We augment the trees in
TKSyQE with semantic role labels. We attach se-
mantic roles to dependency labels of the argument
nodes in the dependency trees as in Figure 1(d).
For constituency trees, we use the D2C heuristic
to elevate roles up the terminal nodes and attach
the labels to the syntactic non-terminal category
of the node as in Figure 1(e). The performance
of the resulting system, TKSSQE, is shown in Ta-
ble 4. It substantially outperforms its counterpart,
CD-PST, all differences being statistically signif-
icant. However, compared to the plain syntactic
tree kernels (TKSyQE), the changes are slight and
inconsistent, rendering the augmentation not use-
ful. We consider this system to be our syntactic-
15
This heuristic (D2C) recursively elevates the argument
role already assigned to a terminal node (based on the
dependency-based argument position) to the parent node as
long as 1) the argument node is not a root node or is not
tagged as a POS (possessive), 2) the role is not an AM-NEG,
AM-MOD or AM-DIS adjunct, and 3) the argument does not
dominate its predicate?s node or another argument node of the
same proposition.
1-HTER HBLEU Adq Flu
RMSE
WMT17 0.2310 0.2696 0.2219 0.2469
HCSyQE 0.2435 0.2797 0.2334 0.2479
HCSeQE 0.2482 0.2868 0.2416 0.2612
Pearson r
WMT17 0.3661 0.3806 0.4710 0.4769
HCSyQE 0.2572 0.3080 0.3961 0.4696
HCSeQE 0.1794 0.1636 0.2972 0.3577
Table 5: RMSE and Pearson r of the 17 baseline
features (WMT17) and hand-crafted features
semantic tree kernel system.
7 Hand-crafted Features
In our previous work (Kaljahi et al., 2014b), we
experiment with a set of hand-crafted syntactic
features extracted from both constituency and de-
pendency trees on a different data set. We apply
the same feature set on the new data set here. The
results are reported in Table 5. The performance of
this system (HCSyQE) is significantly lower than
the baseline. This is opposite to what we ob-
serve with the same feature set on a different data
set, again showing that the role of data is funda-
mental in understanding system performance. The
main difference between these two data sets is that
the former is extracted from a well-formed text in
the news domain, the same domain on which our
parsers and SRL system have been trained, while
the new data set does not necessarily contain well-
formed text nor is it from the same domain.
We design another set of feature types aiming
at capturing the semantics of the source and trans-
lation via predicate-argument structure. The fea-
ture types are listed in Table 6. Feature types
1 to 8 each contain two features, one extracted
from the source and the other from the transla-
tion. To compute argument span sizes (feature
types 4 and 5), we use the constituency conver-
sion of SRL obtained using the D2C heuristic in-
troduced in Section 6. The proposition label se-
72
1 Number of propositions
2 Number of arguments
3 Average number of arguments per proposition
4 Sum of span sizes of arguments
5 Ratio of sum of span sizes of arguments to sentence
length
6 Proposition label sequences
7 Constituency label sequences of proposition elements
8 Dependency label sequences of proposition elements
9 Percentage of predicate/argument word alignment
mapping types
Table 6: Semantic feature types
quence (feature type 6) is the concatenation of ar-
gument roles and predicate labels of the propo-
sition with their preserved order (e.g. A0-go.01-
A4). Similarly, constituency and dependency la-
bel sequences (feature types 4 and 5) are extracted
by replacing argument and predicate labels with
their constituency and dependency labels respec-
tively. Feature type 9 consists of three features
based on word alignment of source and target
sentences: number of non-aligned, one-to-many-
aligned and many-to-one-aligned predicates and
arguments. The word alignments are obtained us-
ing the grow-diag-final-and heuristic as
they performed slightly better than other types.
16
As in the baseline system, we use SVMs to build
the QE systems using these hand-crafted features.
The nominal features are binarized to be usable by
SVM. However, the set of possible feature values
can be large, leading to a large number of binary
features. For example, there are more than 5000
unique proposition label sequences in our data.
Not only does this high dimensionality reduce the
efficiency of the system, it can also affect its per-
formance as these features are sparse. To tackle
this issue, we impose a frequency cutoff on these
features: we keep only frequent features using a
threshold set empirically on the development set.
Table 5 shows the performance of the system
(HCSeQE) built with these features. The semantic
features perform substantially lower than the syn-
tactic features and thus the baseline, especially in
predicting human-targeted scores. Since these fea-
tures are chosen from a comprehensive set of se-
mantic features, and as they should ideally capture
adequacy better than general features, a probable
reason for their low performance is the quality of
16
It should be noted that a number of features in addition
to those presented here have been tried, e.g. the ratio and dif-
ference of the source and target values of numerical features.
However, through manual feature selection, we have removed
features which do not appear to contribute much.
the underlying syntactic and semantic analysis.
8 Predicate-Argument Match (PAM)
Translation adequacy measures how much of the
source meaning is preserved in the translated text.
Predicate-argument structure or semantic role la-
belling expresses a substantial part of the meaning.
Therefore, the matching between the predicate-
argument structure of the source and its transla-
tion could be an important clue to the translation
adequacy, independent of the language pair used.
We attempt to exploit predicate-argument match
(PAM) to create a metric that measures the trans-
lation adequacy.
The algorithm to compute PAM score starts
by aligning the predicates and arguments of the
source side to its target side using word align-
ments.
17
It then treats the problem as one of SRL
scoring, similar to the scoring scheme used in the
CoNLL 2009 shared task (Haji?c et al., 2009). As-
suming the source side SRL as a reference, it com-
putes unlabelled precision and recall of the target
side SRL with respect to it:
UPrec =
# aligned preds and their args
# target side preds and args
URec =
# aligned preds and their args
# source side preds and args
Labelled precision and recall are calculated in
the same way except that they also require argu-
ment label agreement. UF
1
and LF
1
are the har-
monic means of unlabelled and labelled scores re-
spectively. Inspired by the observation that most
source sentences with no identified proposition are
short and can be assumed to be easier to translate,
and based on experiments on the dev set, we assign
a score of 1 to such sentences. When no proposi-
tion is identified in the target side while there is a
proposition in the source, we assign a score of 0.5.
We obtain word alignments using the Moses
toolkit (Hoang et al., 2009), which can gener-
ate alignments in both directions and combine
them using a number of heuristics. We try in-
tersection, union, source-to-target only, as well
as the grow-diag-final-and heuristic, but
only the source-to-target results are reported here
as they slightly outperform the others.
Table 7 shows the RMSE and Pearson r for
each of the unlabelled and labelled F
1
against ade-
17
We also tried lexical and phrase translation tables for this
purpose in addition to word alignments but they do not out-
perform word alignments.
73
1-HTER HBLEU Adq Flu
RMSE
1 UF
1
0.3175 0.3607 0.3108 0.4033
LF
1
0.4247 0.3903 0.3839 0.3586
Pearson r
UF
1
0.2328 0.2179 0.2698 0.2865
LF
1
0.1784 0.1835 0.2225 0.2688
Table 7: RMSE and Pearson r of PAM unlabelled
and labelled F
1
scores as estimation of the MT
evaluation metrics
1-HTER HBLEU Adq Flu
RMSE
PAM 0.2414 0.2833 0.2414 0.2661
HCSeQE 0.2482 0.2868 0.2416 0.2612
HCSeQE
pam
0.2445 0.2822 0.2370 0.2575
Pearson r
PAM 0.2292 0.2195 0.2787 0.3210
HCSeQE 0.1794 0.1636 0.2972 0.3577
HCSeQE
pam
0.2387 0.2368 0.3571 0.3908
Table 8: RMSE and Pearson r of PAM scores as
features, alone and combined (PAM)
quacy and also fluency scores on the test data set.
18
According to the results, the unlabelled F
1
(UF
1
)
is a closer estimation than the labelled one. Its
Pearson correlation scores are overall competitive
to the hand-crafted semantic features (HCSeQE in
Table 5): they are better for the automatic metric
cases but lower for manual ones. However, the
RMSE scores are considerably larger. Overall, the
performance is not comparable to the baseline and
other well performing systems. We investigate the
reasons behind this result in the next section.
Another way to employ the PAM scores in QE
is to use them in a statistical framework. We build
a SVM model using all 6 PAM scores The per-
formance of this system (PAM) on the test set is
shown in Table 8. The performance is consider-
ably higher than when the PAM scores are used
directly as estimations. Interestingly, compared to
the 47 semantic hand-crafted features (HCSeQE),
this small feature set performs better in predicting
human-targeted metrics.
We add these features to our set of hand-
crafted features in Section 7 to yield a new sys-
tem (HCSeQE
pam
in Table 8). All scores improve
compared to the stronger of the two components.
However, only the manual metric prediction im-
provements are statistically significant. The per-
formance is still not close to the baseline.
18
Precision and recall scores were also tried. Precision
proved to be the weakest estimator, whereas recall scores
were highest for some settings.
8.1 Analyzing PAM
Ideally, PAM scores should capture the adequacy
of translation with a high accuracy. The results
are however far from ideal. There are two fac-
tors involved in the PAM scoring procedure, the
quality of which can affect its performance: 1)
predicate-argument structure of the source and
target side of the translation, 2) alignment of
predicate-argument structures of source and target.
The SRL systems for both English and French
are trained on edited newswire. On the other
hand, our data is neither from the same domain nor
edited. The problem is exacerbated on the trans-
lation target side, where our French SRL system
is trained on only a small data set and applied to
machine translation output. To discover the con-
tribution of each of these factors in the accuracy
of PAM, we carry out a manual analysis. We ran-
domly select 10% of the development set (50 sen-
tences) and count the number of problems of each
of these two categories.
We find only 8 cases in which a wrong word
alignment misleads PAM scoring. On the other
hand, there are 219 cases of SRL problems, in-
cluding predicate and argument identification and
labelling: 82 cases (37%) in the source and 138
cases (63%) in the target.
We additionally look for the cases where a
translation divergence causes predicate-argument
mismatch in the source and translation. For ex-
ample, without sacrificing is translated into sans
impact sur (without impact on), a case of transpo-
sition, where the source side verb predicate is left
unaligned thus affecting the PAM score. We find
only 9 such cases in the sample, which is similar
to the proportion of word alignment problems.
As mentioned in the previous section, PAM
scoring has to assign default values for cases in
which there is no predicate in the source or tar-
get. This can be another source of estimation error.
In order to verify its effect, we find such cases in
the development set and manually categorize them
based on the reason causing the sentence to be left
without predicates. There are 79 (16%) source and
96 (19%) target sentences for which the SRL sys-
tems do not identify any predicate, out of which
64 cases have both sides without any predicate.
Among such source sentences, 20 (25%) have no
predicate due to a predicate identification error of
the SRL system, 57 (72%) because of the sentence
structure (e.g. copula verbs which are not labelled
74
1-HTER HBLEU Adq Flu
RMSE
WMT17 0.2310 0.2696 0.2219 0.2469
SyQE 0.2255 0.2711 0.2248 0.2419
SeQE 0.2249 0.2710 0.2242 0.2404
SSQE 0.2246 0.2696 0.2230 0.2402
SSQE+WMT17 0.2225 0.2673 0.2202 0.2379
Pearson r
WMT17 0.3661 0.3806 0.4710 0.4769
SyQE 0.3824 0.3650 0.4393 0.5087
SeQE 0.3884 0.3648 0.4447 0.5182
SSQE 0.3920 0.3768 0.4538 0.5196
SSQE+WMT17 0.4144 0.3953 0.4771 0.5331
Table 9: RMSE and Pearson r of the 17 baseline
features (WMT17) and system combinations
as predicates in the SRL training data, titles, etc.),
and the remaining 2 due to spelling errors mislead-
ing the SRL system. Among the target side sen-
tences, most of the cases are due to the sentence
structure (65 or 68%) and only 14 (15%) cases are
caused by an SRL error. In 13 cases, no verb pred-
icate in the source is translated correctly. Among
the remaining cases, two are due to untranslated
spelling errors in the source and the other two due
to tokenization errors misleading the SRL system.
These numbers show that the main reason lead-
ing to the sentences without verbal predicates is
the sentence structure. This problem can be al-
leviated by employing nominal predicates in both
sides. While this is possible for the English side,
there is currently no French resource where nomi-
nal predicates have been annotated.
9 Combining Systems
We now combine the systems we have built so
far (Table 9). We first combine syntax-based
and semantic-based systems individually. SyQE
is the combination of the syntactic tree kernel
system (TKSyQE) and the hand-crafted features
(HCSyQE). Likewise, SeQE is the combination
of the semantic tree kernel system (TKSSQE) and
the semantic hand-crafted features including PAM
features (HCSeQE
pam
). These two systems are
combined in SSQE but without syntactic tree ker-
nels (TKSyQE) to avoid redundancy with TKSSQE
as these are the augmented syntactic tree kernels.
We finally combine SSQE with the baseline.
SyQE significantly improves over its tree ker-
nel and hand-crafted components. It also outper-
forms the baseline in HTER and fluency predic-
tion, but is beaten by it in HBLEU and adequacy
prediction. None of these differences are statis-
tically significant however. SeQE also performs
better than the stronger of its components. Except
for adequacy prediction, the other improvements
are statistically significant. This system performs
slightly better than SyQE. Its comparison to the
baseline is the same as that of SyQE, except that
its superiority to the baseline in fluency prediction
is statistically significant.
The full syntactic-semantic system (SSQE) also
improves over its syntactic and semantic compo-
nents. However, the improvements are not statisti-
cally significant. Compared to the baseline, HTER
and fluency prediction perform better, the latter
being statistically significant. HBLEU prediction
is around the same as the baseline, but adequacy
prediction performance is lower, though not statis-
tically significantly.
Finally, when we combine the syntactic-
semantic system with the baseline system, the
combination continues to improve further. Com-
pared to the stronger component however, only the
HTER and fluency prediction improvements are
statistically significant.
10 Conclusion
We introduced a new QE data set drawn from cus-
tomer support forum text, machine translated and
both post-edited and manually evaluated for ad-
equacy and fluency. We used syntactic and se-
mantic QE systems via both tree kernels and hand-
crafted features. We found it hard to improve over
a baseline, albeit strong, using such information
which is extracted by applying parsers and seman-
tic role labellers on out-of-domain and unedited
text. We also defined a metric for estimating the
translation adequacy based on predicate-argument
structure match between source and target. This
metric relies on automatic word alignments and
semantic role labelling. We find that word align-
ment and translation divergence only have minor
effects on the performance of this metric, whereas
the quality of semantic role labelling is the main
hindering factor. Another major issue affecting the
performance of PAM is the unavailability of nom-
inal predicate annotation.
Our PAM scoring method is based on only word
matches as there are no constituent SRL resources
available for French ? perhaps constituent-based
arguments can make a more accurate comparison
between the source and target predicate-argument
structure possible.
75
Acknowledgments
This research has been supported by the Irish
Research Council Enterprise Partnership Scheme
(EPSPG/2011/102) and the computing infrastruc-
ture of the CNGL at DCU. We thank the reviewers
for their helpful comments.
References
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English
and French. In Proceedings of the 1st Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages.
Eleftherios Avramidis. 2012. Quality estimation for
Machine Translation output using linguistic analysis
and decoding features. In Proceedings of the 7th
WMT.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley Framenet project. In Proceed-
ings of the 36th ACL.
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning: Shared Task.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence
estimation for Machine Translation. In JHU/CLSP
Summer Workshop Final Report.
Ond?rej Bojar and Dekai Wu. 2012. Towards a
predicate-argument evaluation for MT. In Proceed-
ings of the Sixth Workshop on Syntax, Semantics and
Structure in Statistical Translation.
Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Ale?s
Tamchyna. 2014. Findings of the 2014 workshop
on Statistical Machine Translation. In Proceedings
of the 9th WMT.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies
representation. In Proceedings of the COLING
Workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
Pascal Denis and Beno??t Sagot. 2012. Coupling an
annotated corpus and a lexicon for state-of-the-art
pos tagging. Lang. Resour. Eval., 46(4):721?736.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the 6th WMT.
Jes?us Gim?enez and Llu??s M`arquez. 2007. Linguistic
features for automatic evaluation of heterogenous mt
systems. In Proceedings of the Second Workshop on
Statistical Machine Translation.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?18.
Christian Hardmeier, Joakim Nivre, and J?org Tiede-
mann. 2012. Tree kernels for machine translation
quality estimation. In Proceedings of the Seventh
WMT.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A unified framework for phrase-based, hierarchical,
and syntax-based statistical machine translation. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT).
Rasoul Kaljahi, Jennifer Foster, Raphael Rubino, Jo-
hann Roturier, and Fred Hollowood. 2013. Parser
accuracy in quality estimation of machine transla-
tion: a tree kernel approach. In International Joint
Conference on Natural Language Processing (IJC-
NLP).
Rasoul Kaljahi, Jennifer Foster, and Johann Roturier.
2014a. Semantic role labelling with minimal re-
sources: Experiments with french. In Third Joint
Conference on Lexical and Computational Seman-
tics (*SEM).
Rasoul Kaljahi, Jennifer Foster, Raphael Rubino, and
Johann Roturier. 2014b. Quality estimation of
english-french machine translation: A detailed study
of the role of syntax. In International Conference on
Computational Linguistics (COLING).
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Conference Pro-
ceedings: the tenth Machine Translation Summit.
LDC. 2002. Linguistic data annotation specification:
Assessment of fluency and adequacy in chinese-
english translations. Technical report.
Chi-kiu Lo and Dekai Wu. 2011. Meant: An inex-
pensive, high-accuracy, semi-automatic metric for
evaluating translation utility via semantic frames. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
2012. Fully automatic semantic mt evaluation. In
Proceedings of the Seventh WMT.
76
Chi-kiu Lo, Meriem Beloucif, Markus Saers, and
Dekai Wu. 2014. Xmeant: Better semantic mt eval-
uation without reference translations. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), June.
Llu??s M`arquez, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Se-
mantic role labeling: An introduction to the special
issue. Comput. Linguist., 34(2):145?159, June.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Tree kernel engineering for propo-
sition re-ranking. In Proceedings of Mining and
Learning with Graphs (MLG).
Alessandro Moschitti. 2006. Making tree kernels prac-
tical for natural language learning. In Proceedings
of EACL.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
ACL, pages 311?318.
Daniele Pighin and Llu??s M`arquez. 2011. Automatic
projection of semantic structures: An application to
pairwise translation ranking. In Proceedings of the
Fifth Workshop on Syntax, Semantics and Structure
in Statistical Translation, pages 1?9.
Chris Quirk. 2004. Training a sentence-level machine
translation confidence measure. In Proceedings of
LREC.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA.
Lucia Specia and Jes?us Gim?enez. 2010. Combining
confidence estimation and reference-based metrics
for segment level MT evaluation. In Proceedings of
AMTA.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
conll-2008 shared task on joint parsing of syntactic
and semantic dependencies. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning.
Lonneke van der Plas, Tanja Samard?zi?c, and Paola
Merlo. 2010. Cross-lingual validity of propbank
in the manual annotation of french. In Proceedings
of the Fourth Linguistic Annotation Workshop, LAW
IV ?10.
Lonneke van der Plas, Paola Merlo, and James Hen-
derson. 2011. Scaling up automatic cross-lingual
semantic role annotation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies.
77
