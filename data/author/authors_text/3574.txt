Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 314?321,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Ranking Help Message Candidates Based on Robust Grammar
Verification Results and Utterance History in Spoken Dialogue Systems
Kazunori Komatani Satoshi Ikeda Yuichiro Fukubayashi
Tetsuya Ogata Hiroshi G. Okuno
Graduate School of Informatics
Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan
{komatani,sikeda,fukubaya,ogata,okuno}@kuis.kyoto-u.ac.jp
Abstract
We address an issue of out-of-grammar
(OOG) utterances in spoken dialogue sys-
tems by generating help messages for
novice users. Help generation for OOG
utterances is a challenging problem be-
cause language understanding (LU) re-
sults based on automatic speech recogni-
tion (ASR) results for such utterances are
always erroneous as important words are
often misrecognized or missed from such
utterances. We first develop grammar ver-
ification for OOG utterances on the ba-
sis of a Weighted Finite-State Transducer
(WFST). It robustly identifies a grammar
rule that a user intends to utter, even when
some important words are missed from the
ASR result. We then adopt a ranking algo-
rithm, RankBoost, whose features include
the grammar verification results and the
utterance history representing the user?s
experience.
1 Introduction
Studies on spoken dialogue systems have recently
proceeded from in-laboratory systems to ones de-
ployed to the open public (Raux et al, 2006; Ko-
matani et al, 2007; Nisimura et al, 2005). Ac-
cordingly, opportunities are increasing as general
citizens use the systems. This situation means
that novice users directly access the systems with
no instruction, which is quite different from in-
laboratory experiments where some instructions
can be given. In such cases, users often experi-
ence situations where their utterances are not cor-
rectly recognized. This is because of a gap be-
tween the actual system and a user?s mental model,
that is, a user?s expectation of the system. Ac-
tually, a user?s utterance often cannot be inter-
preted by the system because of the system?s lim-
ited grammar for language understanding (LU).
We call such an unacceptable utterance an ?out-
of-grammar (OOG) utterance.? When users? ut-
terances are OOG, they cannot change their ut-
terances into acceptable ones unless they are in-
formed what expressions are acceptable by the
system.
We aim to manage the problem of OOG utter-
ances by providing help messages showing an ex-
ample of acceptable language expressions when a
user utterance is not acceptable. We prepare help
messages corresponding to each grammar rule the
system has. We therefore assume that appropri-
ate help messages can be provided if a user?s in-
tention, i.e., a grammar rule the user originally
intends to use by his utterance, is correctly esti-
mated.
Issues for generating such help messages in-
clude:
1. Estimating a grammar rule corresponding to
user intention even from OOG utterances,
and
2. Complementing missing information in a sin-
gle utterance.
The first issue focuses on the fact that automatic
speech recognition (ASR) results, used as main in-
put data, are erroneous for OOG utterances. Es-
timating a grammar rule that the user intends to
use becomes accordingly difficult especially when
content words, which correspond to database en-
tries such as place names and their attributes, are
not correctly recognized. That is, any type of ASR
error in any position should be taken into consid-
eration in ASR results of OOG utterances. On the
314
other hand, the second issue focuses on the fact
that an ASR result for an OOG utterance does not
necessarily contain sufficient information to esti-
mate the user intention. This is because of ASR
errors or that users may omit some elements from
their utterances because they are in context.
We develop a grammar verification method
based on Weighted Finite-State Transducer
(WFST) as a solution to the first issue. The
grammar verification method robustly estimates
which a grammar rule is intended to use by a
user?s utterance. The WFST is automatically
generated to represent an ASR result in which any
possibility of error is taken into consideration. We
furthermore adopt a boosting algorithm, Rank-
Boost (Freund et al, 2003), to put help messages
in order of probability to address the second issue.
Because it is difficult even for human annotators
to uniquely determine which help message should
be provided for each case, we adopt an algorithm
that can be used for training on several data
examples that have a certain order of priority.
We also incorporate features representing the
user?s utterance history for preventing message
repetition.
2 Related Work
Various studies have been done on generating help
messages in spoken dialogue systems. Gorrell et
al. (2002) trained a decision tree to classify causes
of errors for OOG utterances. Hockey et al (2003)
also classified OOG utterances into the three cate-
gories of endpointing errors, unknown vocabulary,
and subcategorization mistakes, by comparing two
kinds of ASR results. This was called Targeted
Help and provided a user with immediate feedback
tailored to what the user said. Lee et al (2007) also
addressed error recovery by generating help mes-
sages in an example-based dialog modeling frame-
work. These studies, however, determined what
help messages should be provided mainly on the
basis of literal ASR results. Therefore, help mes-
sages would be degraded by ASR results in which
a lot of information was missing, especially for
OOG utterances. The same help messages would
be repeated when the same ASR results were ob-
tained.
An example dialogue enabled by our method,
especially the part of the method described in Sec-
tion 4, is shown in Figure 1. Here, user utter-
ances are transcriptions, and utterance numbers
U1: Tell me your recommending sites.
Underlined parts are not in-vocabulary and no
valid LU result is obtained. The estimated gram-
mar is [Obtaining info on a site] although the most
appropriate help message is that corresponding to
[Searching tourist sites].
S1: I did not understand. You can say ?Tell me
the address of Kiyomizu Temple? for example,
if getting information on a site.
The help message corresponding to [Obtaining info
on a site] is provided.
U2: Tell me your recommending sites.
The user repeats the same utterance probably be-
cause the help message (S1) was not helpful. The
estimated grammar is [Obtaining info on a site]
again.
S2: I did not understand. You can say ?Search
shrines or museums? for example, if searching
tourist sites.
Another help message corresponding to [Searching
tourist sites] is provided after ranking candidates
by also using the user?s utterance history.
[] denotes grammar rules.
Figure 1: Example dialogue enabled by our
method
start with ?S? and ?U? denote system and user
utterances, respectively. In this example, ASR
results for the user utterances (U1 and U2) do
not contain sufficient information because the ut-
terances are short and contain out-of-vocabulary
words. These two results are similar, and ac-
cordingly, the help message after U2 provided by
methods like Targeted Help (Gorrell et al, 2002;
Hockey et al, 2003) is the same as Utterance S1
because they are only based on ASR results. Our
method can provide different help messages as Ut-
terance S2 after ranking candidates by consider-
ing the utterance history and grammar verification
results. Because the candidates are arranged in
the order of probability, the most appropriate help
message can be provided in fewer attempts.
This ranking method for help message candi-
dates is also useful in multimodal interfaces with
speech input. Help messages are necessary when
ASR is used as its input modality, and such mes-
sages were actually implemented in City Browser
(Gruenstein and Seneff, 2007), for example. This
system lists template-based help messages on the
screen by using ASR results and internal states of
the system. The order of help messages is impor-
tant, especially in portable devices with a small
screen, on which the number of help messages dis-
315
played at one time is limited, as Hartmann and
Schreiber (2008) pointed out. Even in cases where
sufficiently large screens are available, too many
help messages without any order will distract the
user?s attention and thus spoil its usability.
3 Grammar Verification based on WFST
We estimate a user?s intention even from OOG ut-
terances as a grammar rule that the user intends
to use by his utterance. We call this estimation
grammar verification. This process is applied to
ASR outputs based on a statistical language model
(LM) in this paper. We use two transducers: a
finite-state transducer (FST) representing the task
grammar, and weighted FST (WFST) representing
an ASR result and its confidence score. Hereafter,
we denote these two as ?grammar FST? and ?input
WFST? and depict examples in Figure 2.
A strong point of our method is that it takes
all three types of ASR error into consideration.
The input WFST is designed to represent all cases
where any word in an ASR result is an inserted or
substituted error, or any word is deleted. Its weight
is designed to reflect confidence scores of ASR re-
sults. By composing this WFST and the gram-
mar FST, we can obtain all possible sequences
and their accumulated weights when arbitrary se-
quences represented by the input WFST are input
into the grammar FST. The optimal results having
the maximum accumulated weight consist of the
LU result and the grammar rule that is the nearest
to the ASR result. The result can be obtained even
when any element in it is misrecognized or absent
from the ASR result.
An LU result is a set of concepts that consist
of slots and their values corresponding to database
entries the system handles. For example, an LU
result ?month=2, day=22? consists of two con-
cepts, such as the value of slotmonth is 2, and the
value of slot day is 22.
3.1 Design of input WFST and grammar FST
In input WFSTs and grammar FSTs, each arc rep-
resenting state transitions has a label in the form of
?a:b/c? denoting its input symbol, output symbol,
and weight, in this order. Input symbol ? means a
state transition without any input symbol, that is,
an epsilon transition. Output symbol ? means no
output in the state transition. For example, a state
transition ?please:?/1.0? is executed when an in-
put symbol is ?please,? no output symbol is gen-
erated, and 1.0 is added to the accumulated weight.
Weights are omitted in the grammar FST because
no weight is given in it.
An input WFST is automatically constructed
from an ASR result. Sequential state transitions
are assigned to each word in the ASR result, and
each of them is paralleled by filler transitions, as
shown in Figure 2 where the ASR result was ?Ev-
ery Monday please? for example. Filler transitions
such as INS, DEL, and SUB are assigned to each
state for representing every kind of error such as
insertion, deletion, and substitution errors. All in-
put symbols in the input WFST are ?, by which the
WFST represents all possible sequences contain-
ing arbitrary errors. For example, the input WFST
in Figure 2 represents all possible sequences such
as ?Every Monday please,? ?Every Monday F,? ?F
Monday F,? and so on. Here, every word can be
replaced by the symbol F that represents an inser-
tion or substitution error. Moreover, the error sym-
bol DEL can be inserted into its output symbol se-
quence at any position, which corresponds to dele-
tion errors in ASR results. Each weight per state
transition is summed up and then the optimal re-
sult is determined. The weights will be explained
in Section 3.2.
A grammar FST is generated from a task gram-
mar, which is written by a system developer for
each task. It determines whether an input se-
quence conforms to the task grammar. We also
assign filler transitions to each state for handling
each type of error of ASR results considered in
the input WFST. A filler transition, either of INS,
DEL, or SUB, is added to each state in the FST
except for states within keyphrases, which are ex-
plicitly indicated by a system developer. In the
example shown in Figure 2, ?SUB $ Monday
date-repeat=Mon please? is output for an input
sequence ?SUB Monday please?. Here, date-
repeat=Mon denotes an LU result, and $ is a sym-
bol for marking words corresponding to a concept.
3.2 Weights assigned to input WFST
We defined two kinds of weights:
1. Rewards for accepted words (wacc), and
2. Penalties for each kind of error (wsub, wdel,
wins).
An accumulated weight for a single utterance is
defined as the sum of these weights as shown be-
316
Input WFST
Every:
Every
Monday:
Monday
please:
please
Grammar FST
input:output/weight
ASR result: ?Every Monday please?
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !!
  !"#$%& :
 !!
  !"#$"%& :
 !!
  !"#$%&' :
 !"# !"
 !"# !"
 !"# !"
$:?
repeat-date:?
Mon=
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"# !"  !"# !"
 !"# !"
 !"# !"
 !"# !"
Figure 2: Example of input WFST and grammar FST
low.
w =
?
Eaccepted
wacc +
?
Eerror
(wsub + wdel + wins)
Here, Eaccepted denotes a set of accepted words
corresponding to elements of each grammar rule,
and Eerror denotes a set of words that are not ac-
cepted and that have either error symbol. Note that
the weights are not given beforehand but are cal-
culated and given to the input WFST in runtime
according to each ASR result.
A weight for an accepted word easr is defined
by using its confidence score CM(easr) (Lee et
al., 2004) and its word length. A word length in
mora is denoted as l(?), which is normalized by
that of the longest word in the vocabulary.
wacc = CM(easr)l(easr)
This weight wacc gives preference to sequences
containing longer words with higher confidence
scores.
Weights for each type of error have negative val-
ues because they are penalties:
wsub = ?{CM(easr)l(easr) + l(egram)}/2
wdel = ?{l(e) + l(egram)}/2
wins = ?{CM(easr)l(easr) + l(e)}/2
where l(e) is the average word length in the vocab-
ulary and egram is a grammar element i.e., either a
word or a class. A deletion error is a case when a
grammar element does not correspond to any word
in the ASR result. A substitution error is a case
when an element is replaced by another word in
the ASR result. An insertion error is a case when
no grammar element corresponds to the ASR re-
sult. Every weight is defined as an average of a
word length of a grammar element and the corre-
sponding one in the ASR result multiplied by its
confidence score. When correspondences cannot
be defined in insertion and deletion errors, l(e) is
used instead. In the case when egram is a class in
the grammar, the average word length in that class
is used as l(egram).
3.3 Example of calculating the weights
We show how a weight is calculated by using the
example in Figure 3. In this example, the user ut-
terance was ?Tell me a liaison of Koetsu-ji (a tem-
ple name).? The word ?liaison? was not in the sys-
tem vocabulary. The ASR result accordingly con-
tained errors for that part; the result was ?Tell me
all Sakyo-ward Koetsu-ji.?
Weights are calculated for each grammar rule
the system has. This example shows calcula-
tions for two grammar rules: [get info] accept-
ing ?Tell me ?item name? of ?temple name?,? and
[search ward] accepting ?Tell me ?facility name?
of ?ward name?.? Here, [] and ?? denote a gram-
mar rule and a class in grammars. Two alignment
results are also shown for grammar [get info] in
this example. Weights are calculated for any align-
ment as shown here, and the alignment result with
the largest weight is selected. In this example,
weight +0.16 for the grammar [get info] was the
largest.
We consequently obtained the result that gram-
mar rule [get info] had the highest score for this
OOG utterance and its accumulated weight was
317
User utterance: ?Tell me a liaison of Koetsu-ji?. (Underlined parts denote OOG.)
ASR result tell me all Sakyo-ward Koetsu-ji
(ward) (temple)
grammar [get info] tell me ?item name? of ?temple name?
WFST output tell me INS SUB DEL Koetsu-ji
weights +0.09 +0.06 ?0.04 ?0.11 ?0.02 +0.18 +0.16
grammar [get info] tell me ?item name ? of ?temple name?
WFST output tell me SUB SUB Koetsu-ji
weights +0.09 +0.06 ?0.21 ?0.10 +0.18 +0.02
grammar [search ward] tell me ?facility type? in ?ward name?
WFST output tell me INS SUB DEL SUB
weights +0.09 +0.06 ?0.04 ?0.12 ?0.02 ?0.21 ?0.24
Figure 3: Example of calculating weights in our grammar verification
+0.16. The result also indicated each type of er-
ror as a result of the alignment: ?item name? was
substituted by ?Sakyo-ward?, ?of? in the grammar
[get info] was deleted, and ?all? in the ASR result
was inserted.
4 Ranking Help Message Candidates by
Integrating Dialogue Context
We furthermore develop a method to rank help
message candidates per grammar rule by integrat-
ing the grammar verification result and the user?s
utterance history. This complements information
that is often absent from utterances or misrecog-
nized in ASR and prevents that the same help mes-
sages are repeated. An outline of the method is
depicted in Figure 4.
4.1 Features used in Ranking
Features used in our methods are listed in Table
1. These features are calculated for each help
message candidate corresponding to each gram-
mar rule. Features H1 to H5 represent how reli-
able a grammar verification result is. Feature H1 is
a grammar verification score, that is, the resulting
accumulated weight described in Section 3. Fea-
ture H2 is calculated by normalizing H1 by the
total score of all grammar rules. This represents
how reliable the grammar verification result is rel-
atively compared to others. Features H3 to H5
represent how partially the user utterance matches
with the grammar rule.
Features H6 and H7 correspond to a dialogue
context. Feature H6 reflects the case in which
users tend to repeat similar utterances when their
utterances were not understood by the system.
Feature H7 represents whether and how the user
knows about the language expression of the gram-
mar rule. This feature corresponds to the known
degree we previously proposed (Fukubayashi et
Table 1: Features of each instance (help message
candidate)
H1: accumulated weight of GV (GV score)
H2: GV score normalized by the total GV score of other
instances
H3: ratio of # of accepted words in GV result to # of all
words
H4: maximum number of successively accepted words
in GV result
H5: number of accepted slots in GV result
H6: how before the grammar rule was selected as GV
result (in # of utterances)
H7: maximum GV score for the grammar rule until then
H8: whether it belongs to the ?command? class
H9: whether it belongs to the ?query? class
H10: whether it belongs to the ?request-info? class
H11-H17: products of H8 and each of H1 to H7
H18-H24: products of H9 and each of H1 to H7
H25-H31: products of H10 and each of H1 to H7
GV: grammar verification
al., 2006), and prevents a help message the user
already knows from being provided repeatedly.
Features H8 to H10 represent properties of
utterances corresponding to the grammar rules,
which are categorized into three classes such as
?command,? ?query,? and ?request-info.? In the
sightseeing task, the numbers of grammar rules for
the three classes were 8, 4, and 11, respectively.
More specifically, utterances in either ?query? or
?request-info? class tend to appear successively
because they are used when users try and com-
pare several query conditions; on the other hand,
utterances in ?command? class tend to appear in-
dependently of the context. Features H11 to H31
are the products of features H8, H9, and H10 and
each feature from H1 to H7. These were defined to
consider combinations of properties of utterances
represented by H8, H9, and H10 and their reliabil-
ity represented by H1 to H7, because RankBoost
318
Help candidate 
Help candidate
Ranking
(RankBoost)
?
=
T
t
tt
xhxH )()( ?
1
x
LL )()(
111
xfxf
i
LL )()(
1 nin
xfxf
n
x
User
utterance
Context
deft
qi,,,??
Parameters
Training 
data
p
x
q
x
Grammar
verification
Calculating features
Sorted by H(x)
Statistical LM-based
ASR outputs
Figure 4: Outline of our ranking method for help message candidates
does not consider them.
4.2 Ranking Algorithm
We adopt RankBoost (Freund et al, 2003), a
boosting algorithm based on machine learning, to
rank help message candidates. This algorithm can
be used for training on several data examples hav-
ing a certain order of priority. This attribute fits
for the problem in this paper; it is difficult even
for human annotators to determine the unique ap-
propriate help message to be provided. Target in-
stances x of the algorithm are help message can-
didates corresponding to grammar rules in this pa-
per.
RankBoost trains a score function H(x) and ar-
ranges instances x in the order. Here, H(x?) <
H(x??) means x?? is ranked higher than x?. This
score function is defined as a linear combination
of weak rankers giving partial information regard-
ing the order:
H(x) =
T
?
t
?tht(x)
where T , ht(), and ?t denote the number of boost-
ing iterations, a weak ranker, and its associated
weight, respectively. The weak ranker ht is de-
fined by comparing the value of a feature fi of an
instance x with a threshold ?. That is,
ht(x) =
?
?
?
?
?
1 if fi(x) > ?
0 if fi(x) ? ?
qdef if fi(x) = ?
(1)
where qdef ? {0, 1}. Here, fi(x) denotes the
value of the i-th feature of instance x, and ? de-
notes that no value is given in fi(x).
5 Experimental Evaluation
5.1 Target Data
Data were collected by 30 subjects in total by us-
ing a multi-domain spoken dialogue system that
handles five domains such as restaurant, hotel,
sightseeing, bus, and weather (Komatani et al,
2008). The data consisted of 180 dialogues and
11,733 utterances. Data from five subjects were
used to determine the number of boosting iter-
ations and to improve LMs for ASR. We used
utterances in the restaurant, hotel, and sightsee-
ing domains because the remaining two, bus and
weather, did not have many grammar rules. We
then extracted OOG utterances on the basis of the
grammar verification results to evaluate the per-
formance of our method for such utterances. We
regarded an utterance whose accumulated weight
was negative as OOG. As a result, 1,349 OOG ut-
terances by 25 subjects were used for evaluation,
hereafter. These consisted of 363 utterances in the
restaurant domain, 563 in the hotel domain, and
423 in the sightseeing domain. These data were
collected under the following conditions: subjects
were given no instructions on concrete language
expressions the system accepts. System responses
were made only by speech, and no screen for dis-
playing outputs was used. Subjects were given six
scenarios describing tasks to be completed.
We used Julius1 that is a statistical-LM-based
ASR engine. We constructed class 3-gram LMs
for ASR by using 10,000 sentences generated
from the task grammars and the 600 utterances
collected by the five subjects. The vocabulary
sizes for the restaurant, hotel, and sightseeing do-
mains were 3,456, 2,625, and 3,593, and ASR ac-
curacies for them were 45.8%, 57.1%, and 43.5%,
respectively. These ASR accuracies were not very
high because the target utterances were all OOG.
A set of possible thresholds in the weak rankers
described in Section 4.2 consisted of all feature
values that appeared in the training data. The num-
bers of boosting iterations were determined on the
basis of accuracies for the data by the five sub-
1http://julius.sourceforge.jp/
319
!"#
$"#
%"#
&"#
'"#
("#
)"#
*""#
*+,-./ 0+,-./ !+,-./ $+,-./ %+,-./
1+,-./
!
"
"
#
$
%
"
&
!"#$%&'$ ()*+,$-.(/
Figure 5: Accuracy when N candidates were pro-
vided in sightseeing domain (1 ? N ? 5)
jects. The numbers were 400, 100, and 500 for the
restaurant, hotel, and sightseeing domains.
5.2 Evaluation Criterion
We manually gave five help messages correspond-
ing to grammar rules as reference labels per ut-
terance in the order of having a strong relation to
the utterance. The numbers of candidate help mes-
sages were 28, 27, and 23 for the restaurant, hotel
and sightseeing domains, respectively.
We evaluated our ranking method as the accu-
racy where at least one of the reference labels was
contained in its top N candidates. This corre-
sponds to a probability where at least one appro-
priate help message was contained in a list of N
candidates. The accuracy was calculated by 5-fold
cross validation. In the baseline method we set,
help messages were provided only by using the
grammar verification scores.
5.3 Results
Results in the sightseeing domain are plotted in
Figure 5. We can see that our method outper-
formed the baseline in the accuracies for all N
values. All these differences were statistically sig-
nificant (p < 0.05) by the McNemar test. The ac-
curacies were also better in the other two domains
for all N values, and the average differences for
the three domains were 11.7 points for N=1, 9.7
points for N=2, and 6.7 points for N=3. The dif-
ferences were large especially for small N values.
This result indicates that we can successfully re-
duce the number of help messages when providing
several ones for users. The improvements were
derived from the features we incorporated such as
the estimated user knowledge in addition to gram-
mar verification results. The baseline method was
only based on grammar verification results for sin-
gle utterances, which contained insufficient infor-
mation because OOG utterances were often mis-
recognized or misunderstood.
Table 2: Sum of absolute values of weight ? for
each feature
H7 H17 H19 H2 H6
(H7*H8) (H2*H9)
9.58 6.91 6.61 6.02 6.01
We also investigated dominant features by cal-
culating the sum of absolute values of final weight
? for each feature in RankBoost. Five dominant
features based on the sums are shown in Table
2. These five features include a feature obtained
from grammar verification result (H2), a feature
about the user?s utterance history (H6), a feature
representing estimated user knowledge (H7), and
features representing properties of the utterances.
The most dominant feature was H7, which ap-
peared twice in this table. This was because user
utterances were not likely to be OOG utterances
again after the user had already known an expres-
sion corresponding to the grammar rule, which can
be detected when user utterances for it were cor-
rectly accepted, that is, its grammar verification
score was high. The second dominant feature was
H2, which showed that grammar verification re-
sults worked effectively.
6 Conclusion
We addressed an issue of OOG utterances in spo-
ken dialogue systems by generating help mes-
sages. To manage situations when a user utter-
ance could not be accepted, we robustly estimated
a user?s intention as a grammar rule that the user
intends to use. We furthermore integrated various
information as well as the grammar verification
results for complementing missing information in
single utterances, and then ranked help message
candidates corresponding to the grammar rules for
efficiently providing them.
Our future work includes the following. The
evaluation in this paper was taken place only on
the basis of utterances collected beforehand. Pro-
viding help messages itself should be evaluated by
another experiment through dialogues. Further-
more, we assumed that language expressions of
help messages to show an example language ex-
pression were fixed. We also need to investigate
what kind of expression is more helpful to novice
users.
320
References
Yoav Freund, Raj D. Iyer, Robert E. Schapire, and
Yoram Singer. 2003. An efficient boosting algo-
rithm for combining preferences. Journal of Ma-
chine Learning Research, 4:933?969.
Yuichiro Fukubayashi, Kazunori Komatani, Tetsuya
Ogata, and Hiroshi G. Okuno. 2006. Dynamic
help generation by estimating user?s mental model in
spoken dialogue systems. In Proc. Int?l Conf. Spo-
ken Language Processing (INTERSPEECH), pages
1946?1949.
Genevieve Gorrell, Ian Lewin, and Manny Rayner.
2002. Adding intelligent help to mixed-initiative
spoken dialogue systems. In Proc. Int?l Conf. Spo-
ken Language Processing (ICSLP), pages 2065?
2068.
Alexander Gruenstein and Stephanie Seneff. 2007.
Releasing a multimodal dialogue system into the
wild: User support mechanisms. In Proc. 8th SIG-
dial Workshop on Discourse and Dialogue, pages
111?119.
Melanie Hartmann and Daniel Schreiber. 2008. Proac-
tively adapting interfaces to individual users for mo-
bile devices. In Adaptive Hypermedia and Adap-
tive Web-Based Systems, 5th International Confer-
ence (AH 2008), volume 5149 of Lecture Notes in
Computer Science, pages 300?303. Springer.
Beth A. Hockey, Oliver Lemon, Ellen Campana, Laura
Hiatt, Gregory Aist, James Hieronymus, Alexander
Gruenstein, and John Dowding. 2003. Targeted
help for spoken dialogue systems: intelligent feed-
back improves naive users? performance. In Proc.
10th Conf. of the European Chapter of the ACL
(EACL2003), pages 147?154.
Kazunori Komatani, Tatsuya Kawahara, and Hiroshi G.
Okuno. 2007. Analyzing temporal transition of real
user?s behaviors in a spoken dialogue system. In
Proc. INTERSPEECH, pages 142?145.
Kazunori Komatani, Satoshi Ikeda, Tetsuya Ogata,
and Hiroshi G. Okuno. 2008. Managing out-of-
grammar utterances by topic estimation with domain
extensibility in multi-domain spoken dialogue sys-
tems. Speech Communication, 50(10):863?870.
Akinobu Lee, Kiyohiro Shikano, and Tatsuya Kawa-
hara. 2004. Real-time word confidence scoring us-
ing local posterior probabilities on tree trellis search.
In IEEE Int?l Conf. Acoust., Speech & Signal Pro-
cessing (ICASSP), volume 1, pages 793?796.
Cheongjae Lee, Sangkeun Jung, Donghyeon Lee, and
Gary Guenbae Lee. 2007. Example-based error re-
covery strategy for spoken dialog system. In Proc.
of IEEE Automatic Speech Recognition and Under-
standing Workshop (ASRU), pages 538?543.
Ryuichi Nisimura, Akinobu Lee, Masashi Yamada, and
Kiyohiro Shikano. 2005. Operating a public spo-
ken guidance system in real environment. In Proc.
European Conf. Speech Commun. & Tech. (EU-
ROSPEECH), pages 845?848.
Antoine Raux, Dan Bohus, Brian Langner, Alan W.
Black, and Maxine Eskenazi. 2006. Doing research
on a deployed spoken dialogue system: One year of
Let?s Go! experience. In Proc. INTERSPEECH.
321
Transforming a Sentence End into News Headline Style
Satoshi Ikeda and Kazuhide Yamamoto
Dept. of Electrical Engineering, Nagaoka University of Technology
ikeda@nlp.nagaokaut.ac.jp, yamamoto@fw.ipsj.or.jp
Abstract
News on electrical bulletin boards con-
sist of high density expressions. Many
sentences end with unique expressions
that consist of nouns and case parti-
cles. This paper focuses on expressions
used at the end of sentences and at-
tempts to summarize them by forming
noun or case particle endings. We sum-
marize the news sentence through pat-
tern matching approach. Our evalua-
tion illustrates that the summarizer re-
duces 2.50 characters per sentence on
average; the reduction ratio is 6%. We
also show that people perceive the cor-
rect meanings of the summarized sen-
tences with 95% accuracy.
1 Introduction
Electrical bulletin board displays the latest
news headlines which each newspaper office an-
nounced. News headlines are shorter than news-
papers? news with laconicism because they are
summarized to transfer in limited space.
One of a characteristics of Japanese news head-
lines can be seen at sentence ends (Exp.1).
Exp.1)????????????????
(Countermeasure for alleged abduction is judged
after the movement of administration party.)
Although the end of sentence in Exp.1 is omit-
ted, we have no difficulty to understand the mean-
ing. We unconsciously complete the sentence by
guessing what is omitted without a mistake. With-
out unnecessary ends, these type of sentences are
short and nonredundant.
The final purpose of this work is to transform
news sentences into a news headline style. In
Japanese, sentences end with nouns or case par-
ticles are grammatically incorrect, however, this
kind of expressions are shorter than grammati-
cally perfect sentences, and hence often used to
meet the limited length. We believe many sen-
tences have semantically redundant expressions
in the end, which needs to be focused in summa-
rization.
In this paper we present a list of deletable ex-
pressions at Japanese sentence ends. We have
to carefully investigate which sentence ends are
deletable, and how to change into the headline
style. We present the concrete expressions of
deletion with examples and illustrate effects of
deletion.
2 Related Works
As the most similar work to ours, Sato et al[6]
tries to extract paraphrasing patterns of sentence
end by preparing a lot of alignment pairs between
news sentences and their headline versions. They
compare the sentences from the ends and obtain
many correspondences between the two. How-
ever, they have no proposals on how to use these
one-to-N correspondences, i.e., the way to select
one from many candidates. Our approach is to ob-
tain many transformation patterns as well, but we
do not use aligned corpus; we use a large collec-
tion of news headlines instead and find patterns
by our thorough observation.
Wakao et al[7] compares newscast and corre-
sponding subtitle expressions to investigate the
differences of them. One of the observation tar-
gets is sentence end, and they have shown us
some typical patterns of conversion into a short
news. This enumerates phraseologies which are
able to be cut down and investigates the frequency
of use. In news subtitles nouns or case particles
are used at the sentence end. This work is drew
upon literature [7] while we investigated in our
own right. We shaded light on the phraseologies
which do not exist literature [7] such as?????????????? (There seems to sur-
render.)?. We examined the phraseologies which
41
are disposed by machine. Fukushima et al [1] cut
off the unnecessary part from literature [7].
There are investigations to summarize text
by confining the number of characters [2,3,5].
Ishizako et al[2] cut off areas of overlap. Ohmori
et al[5] and Mikami et al[3] summarized text al-
together, but these investigations do not focus on
sentence ends.
3 News Headlines and Their Sentence
Ends
There is an email service that delivers Japanese
news headlines three times a day on week-
days. That is Nikkei news mail(1) provided by
NIKKEI-goo. We have been collecting them
since December 1999. Table1 shows the statistics
we have obtained.
Table 1: Statistical datum which are collected
number of mails 3365
number of stories 21127
number of sentences 40374
News headlines are more distinctive than news
stories in sentence end. Therefore, we investi-
gated part of speech on both news headlines and
newspaper(Nihon Keizai Shimbun(2)). Table2
shows the comparison.
Table 2: Occurrence ratio of POS in sentence end
occurrence ratio[%]
POS newspaper headlines
noun 23.70 55.92
(verbal noun) (5.00) (39.90)
verb 28.66 15.91
adjective 1.80 0.19
adverb 0.20 0.22
particle 1.56 8.83
(case particles) (0.34) (6.41)
auxiliary verb 38.59 18.52
symbol 5.42 0.40
In the newspaper, declinable words are respon-
sible for the majority of sentence ends. In news
headlines, there are in fact many verbal nouns in
sentence ends.
Japanese words are classified broadly into two
types; one derived from China and another origi-
nated in Japan. News headlines contain the for-
mer more than the latter because words from
China carry more information in fewer characters.
We investigated news headlines and news on a pa-
per which contained words of both Chinese and
Japanese origins. The result is shown in Table3.
In fact, news headlines preferably use the words
of Chinese origin about three times as much as
that of Japanese origin.
Table 3: Ratios of Chinese and Japanese origin
words in (a) newspaper and (b) headlines.
ratios [%]
Japanese Chinese (a) (b) a/b???? ?? (to be found) 1.059 2.658 0.398??? ?? (to decide) 0.622 2.184 0.285?? ?? (to elect) 0.210 2.643 0.079??? ?? (to find out) 0.181 2.875 0.063??? ?? (to order) 1.132 3.841 0.295??? ?? (to say) 0.456 0.181 2.493??? ?? (to investigate) 6.284 53.333 0.118
total 2.712 7.271 0.373
We can imagine that a short phraseology
is preferably used when the phraseology has
the same information. We estimate that the
news headlines are high density phraseology than
newspaper.
4 Method of Summarization
In order to transform a sentence end into a
shorter one, we have conducted three kinds of
procedures:
(1) Deletion of target words at sentence end
(2) Deletion with minor transformation after the
target words
(3) Transformation of sentence end
More precisely, we have proposed conduct-
ing the following 10 procedures for transforming
Japanese sentence ends into a news headline style:
1. Cut off dictum and honorific phraseology (1)
2. Cut off???? (wo shimesu:show)? (1)
3. Change verbal noun(2)
4. Cut off??? (naru)?(2)
5. Cut off the part which follows????? (akirakani)?
(2)
6. Change words of Japanese origin(2)
7. Cut off????? (teshimau)?(1)
8. Cut off??? (tatu)?(2)
9. Transform phraseology indicated the action in the fu-
ture (2)
42
10. Change to compound noun (3)
We summarized in this order, and process 3.?
9. can be switched.
4.1 Cut Off Dictum and Honorific Phraseol-
ogy
Phraseologies shown below are dictum or hon-
orific phraseology. These phraseology in sentence
end is cut off because these are not necessary to
understand the meaning.
? dictum phraseology: ???? (datta)? ????
(dearu)??? (da)?
? honorific phraseology:??? (masu)???? (desu)?
4.2 Cut Off???? (wo simesu:show)?
When a sentence end is ???? (wo
shimesu)?or????? (wo shimeshita)?, this
phraseology is cut off because??? (shimesu)?
has little meaning in that sentence. The main verb
of the sentence is the verbal noun before???? (wo shimesu)?.
4.3 Change Verbal Nouns
The expression after the verbal noun closest
to the main verb of the sentence is deleted. In
Japanese, we put a word??? (suru)? after a
verbal noun to make a verb, but in the summary
it can be deleted since we can still understand the
usage.
When a self-sufficient word exists following a
verbal noun, we do not dispose this.
Step 1 The part following??? (suru)? is cut.
Nominalized verbal noun to cut??? (suru)? is
the verbal noun in this arrangement.
Step 2 When the cut part contains an estimation
phraseology????? (mirareru)? or ???? (daou)?, tack on?? (ka))? and finish.
Exp.2)??????????????????
?????????????
(He seemed to surrender in trouble with escape fund.)
Step 3 When the cut part contains a contradic-
tion phraseology??? (nai)? or ?? (nu)?,
tack on??? (sezu)?at the sentence end and fin-
ish. When this part concurrently contains a pas-
sive phraseology??? (reru)?, tack on???? (sarezu)? and finish.
Step 4 When a sentence end is?noun?? (wo)? verbal noun?,?? (wo)?is cut to become a
compound noun?noun? verbal noun?.
Exp.3)?????????????????????????????????????????????????
(Starting this month, Japanese chess problems are seen in
ads of each station and in trains.)
Step 5 When a sentence end is ?particle1? noun ????? (surukoto) ? particle2 ?
noun?, ????? (surukoto)? is cut. If the
particle1 is?? (wo)?or?? (ka)?, this parti-
cle changes to?? (no)?.
If the cut part contains ???? (ha-
jimete:first)?, procedures from Step 2 is different
as follows.
Step 2 When the cut part contains?????
(surunoha)? or????? (shitanoha)?, ???? (hajimete:first)? is tacked on before verbal
noun. When the part of cut contains?????
(mirareru)?, tack on?? (ka)? in sentence end
and finish.
Step 3 When the cut part contains ???
(shite)?, ??? (go hatsu)?is tacked on in the
sentence end. When the term just before noun
is particle ?? (ka)?, this particle ?? (ka)?
changes into particle?? (no)?.
Exp.4)????????????????????
????????
?????????????????????
?????
(He first acceded the interview since Karmapa Seventeenth
left China.)
Step 4 .1 When a verbal noun is ??? (hat-
sugen:delivery)?or??? (genkyuu:citation)?,???? (hajimete:first)? is tacked on before the
verbal noun.
Exp.5)????????????????????
?????????????????
(Russian troop?s cadre first adverted to retreat.)
Step 4.2 When a verbal noun is not??? (hat-
sugen:delivery)?or??? (genkyuu:citation)?,
the term before the verbal noun is checked. The
sentence end is processed the following.
? particle?? (no)?,?? (ga)?? verbal noun? particle?? (no)?? verbal noun ???? (ha
hatsu)?
? particle?? (wo)?,?? (mo)?? verbal noun? particle?? (wo)?,?? (mo)?? verbal noun
? otherwise?verbal noun ???? verbal noun ???? (ha
hatsu)?
43
Step 5 When the cut part contains?????
(mirareru)?, ?? (ka)?is tacked on in the sen-
tence end.
4.4 Cut Off??? (naru)?
When ?particle ??? (naru)? exists in a
sentence, this part and the following are cut off.
When a self-sufficient word exists in the cut part,
the meaning changes or we do not understand the
meaning.
Therefore, when a self-sufficient word exists?particle??? (naru)?following, the sentence
is not disposed this arrangement.
The?particle??? (naru)? and the follow-
ing are cut off. When the particle is?? (ni)? or?? (to)?,?? (ni)? is tacked on in the sentence
end.
Exp.6)????????????????????
???????
?????????????????????
????
(The accord became the bare adoption because this arranged
after three and half months of general election ballot )
When the cut part contains a contradiction
phraseology??? (nai)? or?? (nu)?,???? (narazu)? is tacked on in the sentence end.
Exp.7)???????????????????
????????
????????????????????
?????
(Almost all detonating agents did not work because they
seemed to wet)
4.5 Cut Off the Part After??????
When????? (akirakani:out of doubt)?ex-
ists in a sentence, the part which follows????? (akirakani)? is cut off. When the cut part con-
tains a self-sufficient word, the meaning changes
or we do not understand the meaning.
Then, when a self-sufficient word exists in the
sentence, the sentence is not disposed this ar-
rangement.
Step 1 The part which follows????? (aki-
rakani)? is cut off.
Step 2 Research the part of cut and dispose the
cut part.
? Contradiction phraseology??? (nai)?or?? (nu)?
and passive phraseology??? (reru)? exist.????? (sarezu)?is tacked on in the sentence end.
? The contradiction phraseology??? (nai)? or??
(nu)?exists.???? (sezu)? is tacked on in the sentence end.
Exp.8)????????????????
??????????? ???(The amount of
loss is not announced.)
Step 3 When?????? (surukoto wo)? ex-
ists before????? (akirakani)?,?????
(surukoto wo)?is cut off. When the part before
the cut is?particle?? (ni)?? verbal noun?,?? (ni)? is changed to?? (e)?. When the part
before the cut part is?particle?? (wo)?? ver-
bal noun?,?? (wo)? is changed to?? (no)?.
4.6 Change Words of Japanese Origin
When a Japanese origin word by Table3 exists
in a sentence, the part before it is cut off. Then
the Japanese origin word is replaced by Chinese
one.
When a self-sufficient word exists following
Japanese origin word, the sentence is not disposed
of this arrangement. We changed the word which
shows Table3.
Step 1 Japanese origin word and following are
cut off.
Step 2 When sentence end is ?????? (surukoto wo)?, cut off ????? (su-
rukoto:doing)?, tack on the correspondent Chi-
nese origin word, and finish the arrangement.
Exp.9)?????????????????????
???????
????????????????????
????
(They have decided to start making an ?instruction book on
extensive assistance for disaster?.)
Step 3 When a sentence condition is followed,
the sentence is disposed.
? A sentence end is a particle?? (ga)? and Japanese
origin word is???? (wakaru:understand)?? tack on??? (hanmei:understand)? and finish.
? A sentence end is a particle?? (ga)? and Japanese
origin word is not???? (siraberu:census)?? The particle?? (ga)?is changed to a particle??
(wo)?.
? A sentence end is?? (ga)? noun?? (de)????
(no)? noun?? (wo)?
? A sentence end is a particle?? (ha)? and Japanese
origin word is???? (wakaru:understand)?? Get the former sentence back again and finish.
? Japanese origin word is ???? (siraberu:census)?
and the cut part contains????? (siteiru)?
44
? tack on???? (tyousa tyu:under survey)?at the
sentence end and finish.
Step 4 Chinese origin word which corresponds
Japanese origin word tacked on the sentence end.
Exp.10)????????????????
??????????????
(The total of 359 counterfeit coins were found.)
4.7 Cut Off????? (teshimau)?
When a sentence contains????? (teshi-
mau)?, we feel that the sentence is negative and????? (teshimau)? is not necessary to un-
derstand the meaning of the sentence. Thus we
cut off????? (teshimau)? in the headline.
This arrangement is used not only the sentence
ends but middle of the sentence. When the term
after the cut part is ?? (ba)?, we do not dis-
pose it. When the sentence end is?????
(teshimau)?, change the term before?????
(teshimau)?to primitive form and finish.
When????? (teshimau)? exists without
the sentence end, ????? (teshimau)? and
the character before this phraseology is cut off.
4.8 Cut off??? (tatsu)?
When a sentence contains??? (tatsu)?,??? (tatsu)?, the part following it is cut off. When
the following part contains the self-sufficient
word, the meaning changes or we do not under-
stand the meaning.
Therefore, when a self-sufficient word exists in
the following part, the sentence is not disposed
this arrangement. When??? (tatsu)? is a part
of idiom, the sentence is not disposed of this ar-
rangement.
Step 1 ??? (tatsu)? and the following part
are cut off.
Exp.11)?????????????????????
??????
??????????????????????
????
(?Top boy? is acme in TV game retail business)
Step 2 When a contradiction phraseology???
(nai)? or ?? (nu)?exists in the cut part,???? (tatazu)? is tacked on at the sentence end.
4.9 Phraseology of Words Implying Future
When a phraseology which indicate the action
in the future such as ??? (keikaku:attempt)?
or ??? (yotei:plan)? exists in the sentence,
the phraseology can changed to ?? (he)? in
Japanese. Therefore, the terms listed below are
the phraseology of indicated the action in the fu-
ture. When ??? (suru) ? this phraseology?
exists in the sentence, this part and following are
changed to?? (he)?.
??? (yotei:plan)???? (keikaku:attempt)????
(houshin:policy)???? (houkou:future direction)?
When??? (suru)? this phraseology?exists
in a sentence and the following contains a contra-
diction phraseology??? (nai)?or?? (nu)?,
the sentence is not disposed of this arrangement.
When the following contains the???? (toiu)?
or ???, the sentence is not disposed of this ar-
rangement.
??? (suru)? this phraseology?and follow-
ing are cut off. when the sentence end is particle,
the particle is cut off.?? (he)? is tacked on the
sentence end.
4.10 Change to a Compound Noun
When a sentence end is?noun? particle?
verbal noun? after the above arrangements, the
particle cut off to become a compound noun.
When the noun is neither pronoun, person name,
unique noun nor postfix for Chasen(3), this ar-
rangement is not disposed. When the particle is??? (kara)?,?? (de)? or?? (mo)?, this
arrangement is not disposed.
We make a compound noun dictionary for The
Mainichi Newspapers(4) to check the adequacy of
compound nouns. When?noun particle? (ni)
verbal noun? and the dictionary contains?noun? verbal noun? which is cut of???,?noun? particle? (ni)? verbal noun?is changed to?noun? verbal noun?. When the particle is not?? (ni)?,?noun? particle? verbal noun?is
changed to?noun? verbal noun?.
Exp.12)????????????????????
??????????????????
?????????????????
(A man?s body was found on the third floor of burned-out
site.)
5 Experiments
We implemented the proposed technique with
Perl programming language to measure the ade-
45
quacy of proposed technique. We summary with
this program. Then input sentence are all sen-
tences seen in the newspaper corpus. The number
of input sentences is 232,038, and 73,512 outputs
are somehow summarized in our method.
5.1 Summarization Ratio
We calculated a sentence ratio and number of
reduced characters in a sentence. This result of
experiment is shown in Table4. The method of
Table4 shows the section number. This Table4
shows the result which used the only one method.
The summarization ratio is 94%. In fact, this
method is reduced the 6% about one sentence.
Table 4: Summarization ratio
process 4.1 4.2 4.3 4.4 4.5
# sentence 16825 1313 37995 7510 199
summ. ratio 0.94 0.94 0.94 0.93 0.90
# reduced char. 1.60 4.00 2.56 3.12 5.41
process 4.6 4.7 4.8 4.9 total
# sentence 7194 600 197 848 72681
summ. ratio 0.96 0.89 0.92 0.87 0.94
# reduced char. 2.20 3.93 3.28 6.57 2.45
5.2 Subjective Evaluation
We also evaluated the proposed technique by
human judgment. We picked up 1,000 sentences
at random from summary sentences, and three ex-
aminees individually accounted them. The sen-
tences are measured by majority decision. As-
sessment criterion is: (1) same meaning without
context, and (2) low unnaturalness. The result is
shown in Table5. The numbers in the table de-
note the section numbers explaining the process
of transformation.
Table 5: Correctness of each process
method 4.1 4.2 4.3 4.4 4.5
# sentence 231 19 492 107 9
# correct 205 18 481 106 8
ratio 0.89 0.95 0.98 0.99 0.89
method 4.6 4.7 4.8 4.9 total
# sentence 116 21 3 13 1000
# correct 113 17 3 12 952
ratio 0.97 0.81 1 0.92 0.95
We have also computed the influence of per-
sonal difference. In this kind of subjective evalua-
tion different person may answer difference judg-
ment. We have evaluated our results in three cri-
teria: (1) at least one said correct, (2) at least two
said correct, and (3) all three said correct. This re-
sult is shown in Table6. The Table illustrates that
correctness is more than 90% in all cases.
Table 6: Correctness changes by personal differ-
ences.
? 1 ? 2 = 3
correctness 0.98 0.95 0.91
5.3 Comparison to the Human Summaries
We compare summaries of the proposed
method and by the human. We picked up 100 sen-
tences in summary sentences at random. One ex-
aminee summarized the original sentences which
corresponded the pick up the summary sentences.
We computed the summarization ratio about these
sentences. The result is shown in Table7.
Table 7: Comparison of summaries by proposed
technique and manual summary
machine human
# sentence 72727 100
summ. ratio 0.94 0.92
# reduced characters 2.45 3.87
Although the sentence ratio of machine sum-
mary is close to the manual summary?s one, num-
ber of reduced characters are approximately one
character different. This indicates that human try
to change many parts of sentence according to
the change of the sentence end, while the ma-
chine does not consider such influence. Change
of sentence end often requires transforming the
whole syntax structure, such as change of aspect
or form. We need more investigations on this is-
sue.
6 Discussions
6.1 Discussion of Erroneous Summaries
In this section we describe some erroneous
summaries by our method and discuss the rea-
sons.
Exp.13)????????????????????
???? 15.5mm???????????
46
?*1????????????????????
????????????
(The face show the character like an annual ring.)
Exp.13 is error example in arrangement ?cut
off the ???? (wo shimesu:show)?? When
the term before ???? (wo shimesu)?is the
noun, the sentence does not have main verb. The
main verb which does not exist in the sentence
is not right in Japanese. when the term before???? (wo shimesu)? is noun, this arrange-
ment does not disposed. This kind of error is
covered. But when the noun is ??? (kan-
gae:concept)?,??? (ikou:disposition)?or???? (mitooshi:forecast)?, this arrangement is
correct.
Exp.14)?????????????????????
?*????????????????
(It is decided to caution the overuse to user.)
Exp.14 is the error example ?change the word
of Japanese origin. When the cut off????? (surukoto:doing)?, the modification relation
is changed. Therefore, the modification relation
is a wrong one. When the particle?? (wo)? is
changed to particle?? (no)?, this kind of error
is covered(Exp.15).
Exp.15)??????????????????????????????????????
Exp.16)?????????????????
?*?????????????
(He thinks that his mother killed.)
Exp.16 is an error example in ?cut off????? (teshimau)??. When????? (tesimau)?is
cut off, it is not congruent inflected forms of????? (teshimau)? and the verb. When????? (teshimau)? is cut off, the inflected forms
must be congruent.
6.2 Verbalness/Nominalness of Verbal Noun
The sentence end is???? (ha hatsu:first)?
in Section4.3. There are a big differences by hu-
mans in degree of accepting this expression. We
thus change expression???? (ha hatsu)?into????? (hajimete:first)??. The example be-
fore changed is shown in Exp.17.
Exp.17)??????????????????
??????????
????????????????????
?????
1symbol ?*? indicates that the sentence is wrong.
(It is the first time that President Putin has a talk to the cap-
tain of Arab Crown)
Some people feel unnatural or wrong in this
example. But when the original sentences do
not have ???? (hajimete:first)?, the sum-
mary sentences are correct. The example is shown
Exp.18 without???? (hajimete:first)?.
Exp.18)??????????????????
?????
??????????????????????
This example gives us no unnaturalness. We
think the verbal noun affect this. The verbal noun
represents that indicates the kind. The verbal op-
eration of verbal noun is varied by humans.
We think concretely about ???
(kaidan:meeting)? of Exp.17and Exp.18.
First, we think that ??? (kaidan:meeting)?
is complemented the verbal noun ?????
(kaidan suru:have a talk)?. The predicate is
generally at sentence end in Japanese. When
the predicate does not exist in a sentence, it is
inclinable in human thought that sentence end
term is predicate. The other hand, we think
that ??? (kaidan:colloquy)? is nominal
or verbal operation in Exp.18 because ???
(kaidan:meeing)? is not sentence end. then
when ??? (kaidan:meeting)? is nominal,
human have unnaturalness. And when ???
(kaidan:meeting)? is verbal, human do not feel
unnatural.
We cite the error summary which sentence end
is noun other than verbal noun in this paper but
the verbal operation of noun is pertained in these
sentences. And the noun of operation verbal is??? (kangae:concept)?other than verbal noun.
6.3 Comparison of Machine and Manual
Summaries
We examine the machine and manual sum-
maries. Although many sentences are not much
different, some sentences have big differences for
summarization. One example is shown as fol-
lows, original sentence, its machine summary and
its manual summary respectively.
Exp.19)?????????????????????
????????????????????
??????????????????
(There is graph used the color picture.)
Exp.19 is cut off the honorific phraseology but
the manual summary is cut off??? (aru)? too.
47
This is shown that??? (aru)?is dictum phrase-
ology. And the sentence end is?? (mo)?. This
is often seen in the news headline. But the pro-
posed technique do not deal with them.
6.4 Summarization Failure
We examine the sentences which are not sum-
marized by the method. We picked up the 200
sentences at random and examine whether or not
it should be summarized. This results is that 9
sentences are missing. The example is shown be-
low with the supposed summary.
Exp.20)???????????????
????
?????????????????
(Mr. Ikemoto?s blob was found from the burned-out site.)
Exp.20 is not summarized. The reason of this
error is caused by an error of the morphological
analysis.
7 Conclusion
In order to generate short and smart style seen
in news headlines, this paper presents a method of
transforming Japanese sentence end expressions
into short style. Our observation reveals that the
end of sentence in the headlines are either nouns
or case particles in many sentences, we thus at-
tempt to summarize them as short as possible. We
have implemented the approach and evaluated in
summarization ratio and their correctness. The
results illustrates that the reduction ratio is 6%
against overall sentence length, and the sentence
is expected to be cut off 2.50 characters per sen-
tence. The length of automatic shortening is ap-
proximately the same as manual summarization.
We also confirmed that 95% of the summaries
were judged to be correct.
Acknowledgment
This work was supported in part by MEXT Grants-in-Aid
for Young Scientists (B) 16700134, and for Scientific Re-
search (A) 16200009, Japan.
Tools and language resources
(1) Nikkei news mail, NIKKEI-goo,
http://nikkeimail.goo.ne.jp/
(2) Nihon Keizai Shimbun Newspaper Corpus,
year 2000, Nihon Keizai Shimbun, Inc.
(3) Chasen, Ver.2.3.3, Matsumoto Lab, Nara In-
stitute of Science and Technology.
http://chasen.naist.jp/hiki/ChaSen/
(4) The Mainichi Newspapers Corpus, year 2000,
Mainichi Newspaper Co., Ltd.
References
[1] Takahiro Fukushima, Terumasa Ehara and Kat-
suhiko Shirai. 1999. Regulation for Reducing Num-
ber of Characters for Sentence Simplification, Pro-
ceedings of The Fifth Annual Meeting of The As-
sociation for Natural Language Processing, pp.221?
224. (in Japanese)
[2] Yuko Ishizako, Akira Kataoka, Shigeru Masuyama
and Seiichi Nakagawa. 1999. Summarization by
Reducing Overlaps and Its Application to TV News
Texts. IPSJ SIG Technical Reports 99-NL-133(7),
pages 45?52. Information Processing Society of
Japan. (in Japanese)
[3] Makoto Mikami, Shigeru Masuyama and Seiichi
Nakagawa. 1999. A Summarization Method by Re-
ducing Redundancy of Each Sentence for Mak-
ing Captions of Newscasting. Journal of Natural
Language Processing Vol.6, No.6, pp.65?81. (in
Japanese)
[4] Kiyonori Ohtake and Kazuhide Yamamoto. 2001.
Paraphrasing Honorifics. Proc. of NLPRS2001
Post-Conference Workshop on Automatic Para-
phrasing: Theories and Applications, pp. 13?20.
[5] Takefumi Oomori, Hidetaka Masuda and Hiroshi
Nakagawa. 2003. Web News Articles Summariza-
tion and its Evaluation using Articles for Mobile
Terminals, IPSJ SIG Technical Reports 2003-NL-
153(1). pages 1?8. Information Processing Society
of Japan. (in Japanese)
[6] Dai Sato, Moritaka Iwakoshi, Hidetaka Masuda
and Hiroshi Nakagawa. 2004. Extraction of Para-
phrasing Patterns from Aligned Corpora of Web
and Mobile Terminal News Articles. IPSJ SIG
Technical Reports 2004-NL-159(27). pages 193?
200. Information Processing Society of Japan. (in
Japanese)
[7] Takahiro Wakao, Terumasa Ehara and Katsuhiko
Shirai. 1997. Summarization Methods Used for
Caption in TV News Programs, IPSJ SIG Technical
Reports 97-NL-122(13). pages 83?89. Information
Processing Society of Japan. (in Japanese)
48
