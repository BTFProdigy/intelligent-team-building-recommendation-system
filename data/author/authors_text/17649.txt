Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1304?1311,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Entity Linking for Tweets
Xiaohua Liu?, Yitong Li?, Haocheng Wu?, Ming Zhou?, Furu Wei?, Yi Lu?
?Microsoft Research Asia, Beijing, 100190, China
?School of Electronic and Information Engineering
Beihang University, Beijing, 100191, China
?University of Science and Technology of China
No. 96, Jinzhai Road, Hefei, Anhui, China
?School of Computer Science and Technology
Harbin Institute of Technology, Harbin, 150001, China
?{xiaoliu, mingzhou, fuwei}@microsoft.com
?tong91222@126.com ?v-haowu@microsoft.com ?v-y@microsoft.com
Abstract
We study the task of entity linking for
tweets, which tries to associate each
mention in a tweet with a knowledge base
entry. Two main challenges of this task are
the dearth of information in a single tweet
and the rich entity mention variations.
To address these challenges, we propose
a collective inference method that
simultaneously resolves a set of mentions.
Particularly, our model integrates three
kinds of similarities, i.e., mention-entry
similarity, entry-entry similarity, and
mention-mention similarity, to enrich
the context for entity linking, and to
address irregular mentions that are not
covered by the entity-variation dictionary.
We evaluate our method on a publicly
available data set and demonstrate the
effectiveness of our method.
1 Introduction
Twitter is a widely used social networking service.
With millions of active users and hundreds of
millions of new published tweets every day1,
it has become a popular platform to capture
and transmit the human experiences of the
moment. Many tweet related researches are
inspired, from named entity recognition (Liu et al,
2012), topic detection (Mathioudakis and Koudas,
2010), clustering (Rosa et al, 2010), to event
extraction (Grinev et al, 2009).
In this work, we study the entity linking task
for tweets, which maps each entity mention in
a tweet to a unique entity, i.e., an entry ID
of a knowledge base like Wikipedia. Entity
1http://siteanalytics.compete.com/twitter.com/
linking task is generally considered as a bridge
between unstructured text and structured machine-
readable knowledge base, and represents a critical
role in machine reading program (Singh et al,
2011). Entity linking for tweets is particularly
meaningful, considering that tweets are often hard
to read owing to its informal written style and
length limitation of 140 characters.
Current entity linking methods are built on top
of a large scale knowledge base such asWikipedia.
A knowledge base consists of a set of entities,
and each entity can have a variation list2. To
decide which entity should be mapped, they may
compute: 1) the similarity between the context of
a mention, e.g., a text window around the mention,
and the content of an entity, e.g., the entity page of
Wikipedia (Mihalcea and Csomai, 2007; Han and
Zhao, 2009); 2) the coherence among the mapped
entities for a set of related mentions, e.g, multiple
mentions in a document (Milne and Witten, 2008;
Kulkarni et al, 2009; Han and Zhao, 2010; Han et
al., 2011).
Tweets pose special challenges to entity linking.
First, a tweet is often too concise and too
noisy to provide enough information for similarity
computing, owing to its short and grass root
nature. Second, tweets have rich variations of
named entities3, and many of them fall out of
the scope of the existing dictionaries mined from
Wikipedia (called OOV mentions hereafter). On
2Entity variation lists can be extracted from the
entity resolution pages of Wikipedia. For example, the
link ?http://en.wikipedia.org/wiki/Svm? will lead us to a
resolution page, where ?Svm? are linked to entities like
?Space vector modulation? and ?Support vector machine?.
As a result, ?Svm? will be added into the variation lists of
?Space vector modulation? and ?Support vector machine? ,
respectively.
3According to Liu et al (2012), on average a named entity
has 3.3 different surface forms in tweets.
1304
the other hand, the huge redundancy in tweets
offers opportunities. That means, an entity
mention often occurs in many tweets, which
allows us to aggregate all related tweets to
compute mention-mention similarity and mention-
entity similarity.
We propose a collective inference method
that leverages tweet redundancy to address those
two challenges. Given a set of mentions, our
model tries to ensure that similar mentions are
linked to similar entities while pursuing the
high total similarity between matched mention-
entity pairs. More specifically, we define
local features, including context similarity and
edit distance, to model the similarity between
a mention and an entity. We adopt in-link
based similarity (Milne and Witten, 2008), to
measure the similarity between entities. Finally,
we introduce a set of features to compute
the similarity between mentions, including how
similar the tweets containing the mentions are,
whether they come from the tweets of the same
account, and their edit distance. Notably, our
model can resolve OOV mentions with the help
of their similar mentions. For example, for the
OOVmention ?LukeBryanOnline?, our model can
find similar mentions like ?TheLukeBryan? and
?LukeBryan?. Considering that most of its similar
mentions are mapped to the American country
singer ?Luke Bryan?, our model tends to link
?LukeBryanOnline? to the same entity.
We evaluate our method on the public available
data set shared by Meij et al (2012)4.
Experimental results show that our method
outperforms two baselines, i.e., Wikify! (Mihalcea
and Csomai, 2007) and system proposed by Meij
et al (2012). We also study the effectiveness
of features related to each kind of similarity, and
demonstrate the advantage of our method for OOV
mention linkage.
We summarize our contributions as follows.
1. We introduce a novel collective inference
method that integrates three kinds of
similarities, i.e., mention-entity similarity,
entity-entity similarity, and mention-mention
similarity, to simultaneously map a set of
tweet mentions to their proper entities.
2. We propose modeling the mention-mention
similarity and demonstrate its effectiveness
4http://ilps.science.uva.nl/resources/wsdm2012-adding-
semantics-to-microblog-posts/
in entity linking for tweets, particularly for
OOV mentions.
3. We evaluate our method on a public data
set, and show our method compares favorably
with the baselines.
Our paper is organized as follows. In the next
section, we introduce related work. In Section
3, we give the formal definition of the task. In
Section 4, we present our solution, including
the framework, features related to different kinds
of similarities, and the training and decoding
procedures. We evaluate our method in Section 5.
Finally in Section 6, we conclude with suggestions
of future work.
2 Related Work
Existing entity linking work can roughly be
divided into two categories. Methods of the
first category resolve one mention at each time,
and mainly consider the similarity between
a mention-entity pair. In contrast, methods
of the second category take a set of related
mentions (e.g., mentions in the same document)
as input, and figure out their corresponding entities
simultaneously.
Examples of the first category include the first
Web-scale entity linking system SemTag (Dill
et al, 2003), Wikify! (Mihalcea and Csomai,
2007), and the recent work of Milne and Witten
(2008). SemTag uses the TAP knowledge
base5, and employs the cosine similarity with
TF-IDF weighting scheme to compute the
match degree between a mention and an entity,
achieving an accuracy of around 82%. Wikify!
identifies the important concepts in the text
and automatically links these concepts to the
corresponding Wikipedia pages. It introduces two
approaches to define mention-entity similarity,
i.e., the contextual overlap between the paragraph
where the mention occurs and the corresponding
Wikipedia pages, and a Naive Bayes classifier
that predicts whether a mention should be linked
to an entity. It achieves 80.69% F1 when two
approaches are combined. Milne and Witten
work on the same task of Wikify!, and also
train a classifier. However, they cleverly use the
5TAB (http://www.w3.org/2002/05/tap/) is a shallow
knowledge base that contains a broad range of lexical and
taxonomic information about popular objects like music,
movies, authors, sports, autos, health, etc.
1305
links found within Wikipedia articles for training,
exploiting the fact that for every link, aWikipedian
has manually selected the correct destination to
represent the intended sense of the anchor. Their
method achieves an F1 score of 75.0%.
Representative studies of the second category
include the work of Kulkarni et al (2009),
Han et al (2011), and Shen et al (2012).
One common feature of these studies is that
they leverage the global coherence between
entities. Kulkarni et al (2009) propose
a graphical model that explicitly models the
combination of evidence from local mention-
entity compatibility and global document-level
topical coherence of the entities, and show that
considering global coherence between entities
significantly improves the performance. Han et
al. (2011) introduce a graph-based representation,
called Referent Graph, to model the global
interdependence between different entity linking
decisions, and jointly infer the referent entities of
all name mentions in a document by exploiting
the interdependence captured in Referent Graph.
Shen et al (2012) propose LIEGE, a framework
to link the entities in web lists with the knowledge
base, with the assumption that entities mentioned
in a Web list tend to be a collection of entities of
the same conceptual type.
Most work of entity linking focuses on web
pages. Recently, Meij et al (2012) study
this task for tweets. They propose a machine
learning based approach using n-gram features,
concept features, and tweet features, to identify
concepts semantically related to a tweet, and
for every entity mention to generate links to its
corresponding Wikipedia article. Their method
belongs to the first category, in the sense that
they only consider the similarity between mention
(tweet) and entity (Wikipedia article).
Our method belongs to the second category.
However, in contrast with existing collective
approaches, our method works on tweets which
are short and often noisy. Furthermore, our
method is based on the ?similar mention with
similar entity? assumption, and explicitly models
and integrates the mention similarity into the
optimization framework. Compared with Meij et
al. (2012), our method is collective, and integrates
more features.
3 Task Definition
Given a sequence of mentions, denoted by
M? = (m1,m2, ? ? ? ,mn), our task is to
output a sequence of entities, denoted by
E? = (e1, e2, ? ? ? , en), where ei is the entity
corresponding to mi. Here, an entity refers
to an item of a knowledge base. Following
most existing work, we use Wikipedia as the
knowledge base, and an entity is a definition page
in Wikipedia; a mention denotes a sequence of
tokens in a tweet that can be potentially linked to
an entity.
Several notes should be made. First, we
assume that mentions are given, e.g., identified by
some named entity recognition system. Second,
mentions may come from multiple tweets. Third,
mentions with the same token sequence may
refer to different entities, depending on mention
context. Finally, we assume each entity e has
a variation list6, and a unique ID through which
all related information about that entity can be
accessed.
Here is an example to illustrate the task. Given
mentions ?nbcbightlynews?, ?Santiago?, ?WH?
and ?Libya? from the following tweet ?Chuck
Todd: Prepping for @nbcnightlynews here in
Santiago, reporting on WH handling of Libya
situation.?, the expected output is ?NBC Nightly
News(194735)?, ?Santiago Chile(51572)?,
?White House(33057)? and ?Libya(17633)?,
where the numbers in the parentheses are the IDs
of the corresponding entities.
4 Our Method
In this section, we first present the framework of
our entity linking method. Then we introduce
features related to different kinds of similarities,
followed by a detailed discussion of the training
and decoding procedures.
4.1 Framework
Given the input mention sequence M? =
(m1,m2, ? ? ? ,mn), our method outputs the entity
sequence E?? = (e?1, e?2, ? ? ? , e?n) according to
Formula 1:
6For example, the variation list of the entity ?Obama? may
contain ?Barack Obama?, ?Barack Hussein Obama II?, etc.
1306
E?? = argmax?E??C(M?)?
n?
i=1
w? ? f?(ei,mi)
+(1 ? ?)
?
i ?=j
r(ei, ej)s(mi,mj)
(1)
Where:
? C(M?) is the set of all possible entity
sequences for the mention sequence M? ;
? E? denotes an entity sequence instance,
consisting of e1, e2, ? ? ? , en;
? f?(ei,mi) is the feature vector that models the
similarity between mention mi and its linked
entity ei;
? w? is the feature weight vector related to f? ,
which is trained on the training data set; w? ?
f?(ei,mi) is the similarity between mention
mi and entity ei;
? r(ei, ej) is the function that returns the
similarity between two entities ei and ej ;
? s(mi,mj) is the function that returns the
similarity between two mentions mi and mj ;
? ? ? (0, 1) is a systematic parameter, which
is determined on the development data set; it
is used to adjust the tradeoff between local
compatibility and global consistence. It is
experimentally set to 0.8 in our work.
From Formula 1, we can see that: 1) our
method considers the mention-entity similarly,
entity-entity similarity and mention-mention
similarity. Mention-entity similarly is used to
model local compatibility, while entity-entity
similarity and mention-mention similarity
combined are to model global consistence; and 2)
our method prefers configurations where similar
mentions have similar entities and with high local
compatibility.
C(M?) is worth of more discussion here.
It represents the search space, which can be
generated using the entity variation list. To
achieve this, we first build an inverted index
of all entity variation lists, with each unique
variation as an entry pointing to a list of entities.
Then for any mention m, we look up the index,
and get al possible entities, denoted by C(m).
In this way, given a mention sequence M? =
(m1,m2, ? ? ? ,mn), we can enumerate all possible
entity sequence E? = (e1, e2, ? ? ? , en), where ei ?
C(m). This means |C(M?)| = ?m?M |C(m)| ,
which is often large. There is one special case:
if m is an OOV mention, i.e., |C(m)| = 0, then
|C(M?)| = 0, and we get no solution. To address
this problem, we can generate a list of candidates
for an OOV mention using its similar mentions.
Let S(m) denote OOV mention m?s similar
mentions, we define C(m) = ?m??S(m) C(m
?).
If still C(m) = 0, we remove m from M? , and
report we cannot map it to any entity.
Here is an example to illustrate our framework.
Suppose we have the following tweets:
? UserA: Yeaaahhgg #habemusfut..
I love monday night futbol =)
#EnglishPremierLeague ManU vs
Liverpool1
? UserA: Manchester United 3 - Liverpool2
2 #EnglishPremierLeague GLORY, GLORY,
MAN.UNITED!
? ? ? ?
Figure 1: An illustrative example to show our
framework. Ovals in orange and in blue represent
mentions and entities, respectively. Each mention
pair, entity pair, and mention entity pair have
a similarity score represented by s, r and f ,
respectively.
We need find out the best entity sequence
E?? for mentions M? = { ?Liverpool1?,
?Manchester United?, ?ManU?, ?Liverpool2?},
from the entity sequences C(M?) = { (Liverpool
(film), Manchester United F.C., Manchester
United F.C., Liverpool (film)), ? ? ? , (Liverpool,
F.C.,Manchester United, F.C., Manchester United
F.C., Liverpool (film) }. Figure 1 illustrate
our solution, where ?Liverpool1? (on the left)
and ?Liverpool2? (on the right) are linked
1307
to ?Liverpool F.C.? (the football club), and
?Manchester United? and ?ManU? are linked to
?Manchester United F.C.?. Notably, ?ManU?
is an OOV mention, but has a similar mention
?Manchester United?, with which ?ManU? is
successfully mapped.
4.2 Features
We group features into three categories: local
features related to mention-entity similarity
(f?(e,m)), features related to entity-entity
similarity (r(ei, ej)) , and features related to
mention-mention similarity (s(mi,mj)).
4.2.1 Local Features
? Prior Probability:
f1(mi, ei) =
count(ei)?
?ek?C(mi) count(ek)
(2)
where count(e) denotes the frequency of
entity e in Wikipedia?s anchor texts.
? Context Similarity:
f2(mi, ei) =
coocurence number
tweet length (3)
where: coccurence number is the the
number of the words that occur in both the
tweet containing mi and the Wikipedia page
of ei; tweet length denotes the number of
tokens of the tweet containing mention mi.
? Edit Distance Similarity:
IfLength(mi)+ED(mi, ei) = Length(ei),
f3(mi, ei) = 1, otherwise 0. ED(?, ?)
computes the character level edit distance.
This feature helps to detect whether
a mention is an abbreviation of its
corresponding entity7.
? Mention Contains Title: If the mention
contains the entity title, namely the title of
the Wikipedia page introducing the entity ei,
f4(mi, ei) = 1, else 0.
? Title Contains Mention: If the entry title
contains the mention, f5(mi, ei) = 1,
otherwise 0.
7Take ?ms? and ?Microsoft? for example. The length of
?ms? is 2, and the edit distance between them is 7. 2 plus 7
equals to 9, which is the length of ?Microsoft?.
4.2.2 Features Related to Entity Similarity
There are two representative definitions of entity
similarity: in-link based similarity (Milne and
Witten, 2008) and category based similarity (Shen
et al, 2012). Considering that the Wikipedia
categories are often noisy (Milne and Witten,
2008), we adopt in-link based similarity, as
defined in Formula 4:
r(ei, ej) =
log|g(ei) ? g(ej)| ? log max(|g(ei)|, |g(ej)|)
log(Total)? log min(|g(ei)|, |g(ej)|)
(4)
Where:
? Total is the total number of knowledge base
entities;
? g(e) is the number of Wikipedia definition
pages that have a link to entity e.
4.2.3 Features Related to Mention Similarity
We define 5 features to model the similarity
between two mentions mi and mj , as listed
below, where t(m) denotes the tweet that contains
mention m:
? s1(mi,mj): The cosine similarity of t(mi)
and t(mj); and tweets are represented as TF-
IDF vectors;
? s2(mi,mj): The cosine similarity of t(mi)
and t(mj); and tweets are represented as
topic distribution vectors;
? s3(mi,mj): Whether t(mi) and t(mj) are
published by the same account;
? s4(mi,mj): Whether t(mi) and t(mj)
contain any common hash tag;
? s5(mi,mj): Edit distance related similarity
between mi and mj , as defined in Formula 5.
s5(mi,mj) = 1, if min{Length(mi), Length(mj)}
+ED(mi,mj) = max{Length(mi), Length(mj)},
else s5(mi,mj) = 1 ? ED(mi,mj)max{Length(mi), Length(mj)}
(5)
Note that: 1) before computing TF-IDF vectors,
stop words are removed; 2) we use the Stanford
Topic Modeling Toolbox8 to compute the topic
model, and experimentally set the number of
topics to 50.
8http://nlp.stanford.edu/software/tmt/tmt-0.4/
1308
Finally, Formula 6 is used to integrate all the
features. a? = (a1, a2, a3, a4, a5) is the feature
weight vector for mention similarity, where ak ?
(0, 1), k = 1, 2, 3, 4, 5, and?5k=1 ak = 1.
s(mi,mj) =
5?
k=1
aksk(mi,mj) (6)
4.3 Training and Decoding
Given n mentions m1,m2, ? ? ? ,mn and their
corresponding entities e1, e2, ? ? ? , en, the goal of
training is to determine: w??, the weights of local
features, and a??, the weights of the features related
to mention similarity, according to Formula 7 9.
(w??, a??) = arg minw?,?a{
1
n
n?
i=1
L1(ei,mi)
+?1||w?||2 +
?2
2
n?
i,j=1
s(mi,mj)L2(?a, ei, ej)}
(7)
Where:
? L1 is the loss function related to local
compatibility, which is defined as
1
w??f?(ei,mi)+1
;
? L2(?a, ei, ej) is the loss function related
to global coherence, which is defined as
1
r(ei,ej)
?5
k=1 aksk(mi,mj)+1
;
? ?1 is the weight of regularization, which is
experimentally set to 1.0;
? ?2 is the weight of L2 loss, which is
experimentally set to 0.2.
Since the decoding problem defined by
Formula 1 is NP hard (Kulkarni et al, 2009), we
develop a greedy hill-climbing approach to tackle
this challenge, as demonstrated in Algorithm 1.
In Algorithm 1, it is the number of iterations;
Score(E?, M?) = ??ni=1 w? ? f?(ei,mi) + (1 ?
?)
?
i?=j r(ei, ej)s(mi,mj); E?ij is the vector after
replacing ei with ej ? C(mi) for current E?;
scij is the score of E?ij , i.e., Score(E?ij , M?). In
each iteration, this rounding solution iteratively
substitute entry ei in E? to increase the total score
cur. If the score cannot be further improved, it
stops and returns current E?.
9This optimization problem is non-convex. We use
coordinate descent to get a local optimal solution.
Algorithm 1 Decoding Algorithm.
Input: Mention Set M? = (m1,m2, ? ? ? ,mn)
Output: Entity Set E? = (e1, e2, ? ? ? , en)
1: for i = 1 to n do
2: Initialize e(0)i as the entity with the largest prior
probability given mention mi.
3: end for
4: cur = Score(E?(0), M?)
5: it = 1
6: while true do
7: for i = 1 to n do
8: for ej ? C(mi) do
9: if ej ?= e(it?1)i then
10: E?(it)ij = E?(it?1) ? {e(it?1)i } + {ej}.
11: end if
12: scij = Score(E?(it)ij , M?).
13: end for
14: end for
15: (l,m) = argmax(i,j)scij .
16: sc? = sclm
17: if sc? > cur then
18: cur = sc?.
19: E?(it) = E?(it?1) ? {e(it?1)l } + {em}.
20: it = it + 1.
21: else
22: break
23: end if
24: end while
25: return E?(it).
5 Experiments
In this section, we introduce the data set and
experimental settings, and present results.
5.1 Data Preparation
Following most existing studies, we choose
Wikipedia as our knowledge base10. We index
the Wikipedia definition pages, and prepare all
required prior knowledge, such as count(e), g(e),
and entity variation lists. We also build an inverted
index with about 60 million entries for the entity
variation lists.
For tweets, we use the data set shared by Meij et
al. (2012)11. This data set is annotated manually
by two volunteers. We get 502 annotated tweets
from this data set. We keep 55 of them for
10We download the December 2012 version of Wikipedia,
which contains about four million articles.
11http://ilps.science.uva.nl/resources/wsdm2012-adding-
semantics-to-microblog-posts/.
1309
development, and the remaining for 5 fold cross-
validation.
5.2 Settings
We consider following settings to evaluate our
method.
? Comparing our method with two baselines,
i.e., Wikify! (Mihalcea and Csomai, 2007)
and the system proposed byMeij et al (2012)
12;
? Using only local features;
? Using various mention similarity features;
? Experiments on OOV mentions.
5.3 Results
Table 1 reports the comparison results. Our
method outperforms both systems in terms of
all metrics. Since the main difference between
our method and the baselines is that our method
considers not only local features, but also global
features related to entity similarity and mention
similarity, these results indicate the effectiveness
of collective inference and global features. For
example, we find two baselines incorrectly link
?Nickelodeon? in the tweet ?BOH will make a
special appearance on Nickelodeon?s ?Yo Gabba
Gabba? tomorrow? to the theater instead of a TV
channel. In contrast, our method notices that ?Yo
Gabba Gabba? in the same tweet can be linked
to ?Yo Gabba Gabba (TV show)?, and thus it
correctly maps ?Nickelodeon? to ?Nickelodeon
(TV channel)?.
System Pre. Rec. F1
Wikify! 0.375 0.421 0.396
Meij?s Method 0.734 0.632 0.679
Our Method 0.752 0.675 0.711
Table 1: Comparison with Baselines.
Table 2 shows the results when local features
are incrementally added. It can be seen that:
1) using only Prior Probability feature already
yields a reasonable F1; and 2) Context Similarity
and Edit Distance Similarity feature have little
contribution to the F1, while Mention and Entity
Title Similarity feature greatly boosts the F1.
12We re-implement Wikify! since we use a new evaluation
data set.
Local Feature Pre. Rec. F1
P.P. 0.700 0.599 0.646
+C.S. 0.694 0.597 0.642
+E.D.S. 0.696 0.598 0.643
+M.E.T.S. 0.735 0.632 0.680
Table 2: Local Feature Analysis. P.P.,C.S., E.D.S.,
and M.E.T.S. denote Prior Probability, Context
Similarity, Edit Distance Similarity, and Mention
and Entity Title Similarity, respectively.
The performance of our method with various
mention similarity features is reported in Table 3.
First, we can see that with this kind of features,
the F1 can be significantly improved from 0.680
to 0.704. Second, we notice that TF-IDF (s1) and
Topic Model (s2) features perform equally well,
and combining all mention similarity features
yields the best performance.
Global Feature Pre. Rec. F1
s3+s4+s5 0.744 0.653 0.700
s3+s4+s5 +s1 0.759 0.652 0.702
s3+s4+s5+s2 0.760 0.653 0.703
s3+s4+s5+s1+s2 0.764 0.653 0.704
Table 3: Mention Similarity Feature Analysis.
For any OOV mention, we use the strategy
of guessing its possible entity candidates using
similar mentions, as discussed in Section 4.1.
Table 4 shows the performance of our system for
OOV mentions. It can be seen that with our
OOV strategy, the recall is improved from 0.653
to 0.675 (with p < 0.05) while the Precision is
slightly dropped and the overall F1 still gets better.
A further study reveals that among all the 125
OOV mentions, there are 48 for which our method
cannot find any entity; and nearly half of these
48 OOV mentions do have corresponding entities
13. This suggests that we may need enlarge the
size of variation lists or develop some mention
normalization techniques.
OOV Method Precision Recall F1
Ignore OOV Mention 0.764 0.653 0.704
+ OOV Method 0.752 0.675 0.711
Table 4: Performance for OOV Mentions.
13?NATO-ukraine cooperations? is such an example. It
is mapped to NULL but actually has a corresponding entity
?Ukraine-NATO relations?
1310
6 Conclusions and Future work
We have presented a collective inference method
that jointly links a set of tweet mentions to
their corresponding entities. One distinguished
characteristic of our method is that it integrates
mention-entity similarity, entity-entity similarity,
and mention-mention similarity, to address the
information lack in a tweet and rich OOV
mentions. We evaluate our method on a
public data set. Experimental results show our
method outperforms two baselines, and suggests
the effectiveness of modeling mention-mention
similarity, particularly for OOV mention linking.
In the future, we plan to explore two directions.
First, we are going to enlarge the size of entity
variation lists. Second, we want to integrate
the entity mention normalization techniques as
introduced by Liu et al (2012).
Acknowledgments
We thank the anonymous reviewers for their
valuable comments. We also thank all the
QuickView team members for the helpful
discussions.
References
S. Dill, N. Eiron, D. Gibson, D. Gruhl, and R. Guha.
2003. Semtag and seeker: bootstrapping the
semantic web via automated semantic annotation. In
Proceedings of the 12th international conference on
World Wide Web, WWW ?03, pages 178?186, New
York, NY, USA. ACM.
Maxim Grinev, Maria Grineva, Alexander Boldakov,
Leonid Novak, Andrey Syssoev, and Dmitry
Lizorkin. 2009. Sifting micro-blogging stream for
events of user interest. In Proceedings of the 32nd
international ACM SIGIR conference on Research
and development in information retrieval, SIGIR
?09, pages 837?837, New York, NY, USA. ACM.
Xianpei Han and Jun Zhao. 2009. Nlpr-kbp in tac 2009
kbp track: A two-stage method to entity linking. In
Proceedings of Test Analysis Conference.
Xianpei Han and Jun Zhao. 2010. Structural
semantic relatedness: a knowledge-based method
to named entity disambiguation. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics.
Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective
entity linking in web text: A graph-based method.
In SIGIR?11.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective
annotation of wikipedia entities in web text.
In Proceedings of the 15th ACM SIGKDD
international conference on Knowledge discovery
and data mining, pages 457?465.
Xiaohua Liu, Ming Zhou, Xiangyang Zhou,
Zhongyang Fu, and Furu Wei. 2012. Joint inference
of named entity recognition and normalization for
tweets. In ACL (1), pages 526?535.
Michael Mathioudakis and Nick Koudas. 2010.
Twittermonitor: trend detection over the twitter
stream. In Proceedings of the 2010 ACM SIGMOD
International Conference on Management of data,
SIGMOD ?10, pages 1155?1158, New York, NY,
USA. ACM.
Edgar Meij, Wouter Weerkamp, and Maarten de Rijke.
2012. Adding semantics to microblog posts.
In Proceedings of the fifth ACM international
conference on Web search and data mining.
Rada Mihalcea and Andras Csomai. 2007. Wikify!:
linking documents to encyclopedic knowledge.
In Proceedings of the sixteenth ACM conference
on Conference on information and knowledge
management, CIKM ?07, pages 233?242, NewYork,
NY, USA. ACM.
David Milne and Ian H. Witten. 2008. Learning
to link with wikipedia. In Proceeding of the 17th
ACM conference on Information and knowledge
management.
Kevin Dela Rosa, Rushin Shah, Bo Lin, Anatole
Gershman, and Robert Frederking. 2010. Topical
clustering of tweets. In SWSM?10.
Wei Shen, Jianyong Wang, Ping Luo, and Min Wang.
2012. Liege: Link entities in web lists with
knowledge base. In KDD?12.
Sameer Singh, Amarnag Subramanya, Fernando
Pereira, and Andrew McCallum. 2011. Large-
scale cross-document coreference using distributed
inference and hierarchical models. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ?11, pages 793?
803, Stroudsburg, PA, USA. Association for
Computational Linguistics.
1311
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 365?372,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Quality Estimation for Machine Translation Using the Joint Method 
of Evaluation Criteria and Statistical Modeling 
 
 
Aaron Li-Feng Han 
hanlifengaaron@gmail.com 
Yi Lu 
mb25435@umac.mo 
Derek F. Wong 
derekfw@umac.mo 
   
Lidia S. Chao 
lidiasc@umac.mo 
Liangye He 
wutianshui0515@gmail.com 
Junwen Xing 
mb15470@umac.mo 
    
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory 
Department of Computer and Information Science 
University of Macau, Macau S.A.R. China 
 
  
 
Abstract 
This paper is to introduce our participation in 
the WMT13 shared tasks on Quality Estima-
tion for machine translation without using ref-
erence translations. We submitted the results 
for Task 1.1 (sentence-level quality estima-
tion), Task 1.2 (system selection) and Task 2 
(word-level quality estimation). In Task 1.1, 
we used an enhanced version of BLEU metric 
without using reference translations to evalu-
ate the translation quality. In Task 1.2, we uti-
lized a probability model Na?ve Bayes (NB) as 
a classification algorithm with the features 
borrowed from the traditional evaluation met-
rics. In Task 2, to take the contextual infor-
mation into account, we employed a discrimi-
native undirected probabilistic graphical mod-
el Conditional random field (CRF), in addition 
to the NB algorithm. The training experiments 
on the past WMT corpora showed that the de-
signed methods of this paper yielded promis-
ing results especially the statistical models of 
CRF and NB. The official results show that 
our CRF model achieved the highest F-score 
0.8297 in binary classification of Task 2. 
 
1 Introduction 
Due to the fast development of Machine transla-
tion, different automatic evaluation methods for 
the translation quality have been proposed in re-
cent years. One of the categories is the lexical 
similarity based metric. This kind of metrics in-
cludes the edit distance based method, such as 
WER (Su et al, 1992), Multi-reference WER 
(Nie?en et al, 2000), PER (Tillmann et al, 
1997), the works of (Akiba, et al, 2001), 
(Leusch et al, 2006) and (Wang and Manning, 
2012); the precision based method, such as 
BLEU (Papineni et al, 2002), NIST (Doddington, 
2002), and SIA (Liu and Gildea, 2006); recall 
based method, such as ROUGE (Lin and Hovy 
2003); and the combination of precision and re-
call, such as GTM (Turian et al, 2003), METE-
OR (Lavie and Agarwal, 2007), BLANC (Lita et 
al., 2005), AMBER (Chen and Kuhn, 2011), 
PORT (Chen et al, 2012b), and LEPOR (Han et 
al., 2012). 
Another category is the using of linguistic fea-
tures. This kind of metrics includes the syntactic 
similarity, such as the POS information used by 
TESLA (Dahlmeier et al, 2011), (Liu et al, 
2010) and (Han et al, 2013), phrase information 
used by (Povlsen, et al, 1998) and (Echizen-ya 
and Araki, 2010), sentence structure used by 
(Owczarzak et al, 2007); the semantic similarity, 
such as textual entailment used by (Mirkin et al, 
2009) and (Castillo and Estrella, 2012), Syno-
nyms used by METEOR (Lavie and Agarwal, 
2007), (Wong and Kit, 2012), (Chan and Ng, 
2008); paraphrase used by (Snover et al, 2009). 
The traditional evaluation metrics tend to 
evaluate the hypothesis translation as compared 
to the reference translations that are usually of-
fered by human efforts. However, in the practice, 
there is usually no golden reference for the trans-
lated documents, especially on the internet works. 
How to evaluate the quality of automatically 
translated documents or sentences without using 
the reference translations becomes a new chal-
lenge in front of the NLP researchers. 
365
ADJ ADP ADV CONJ DET NOUN NUM PRON PRT VERB X . 
ADJ PREP, 
PREP/DEL 
ADV, 
NEG 
CC, 
CCAD, 
CCNEG, 
CQUE, 
CSUBF, 
CSUBI, 
CSUBX 
ART NC, 
NMEA, 
NMON, 
NP, 
PERCT,  
UMMX 
CARD, 
CODE, 
QU 
DM, 
INT, 
PPC, 
PPO, 
PPX, 
REL 
SE VCLIger, 
VCLIinf, 
VCLIfin, 
VEadj, 
VEfin, 
VEger, 
VEinf, 
VHadj, 
VHfin, 
VHger, 
VHinf, 
VLadj, 
VLfin, 
VLger, 
VLinf, 
VMadj, 
VMfin, 
VMger, 
VMinf, 
VSadj, 
VSfin, 
VSger, 
VSinf 
ACRNM, 
ALFP, 
ALFS, 
FO, ITJN, 
ORD, 
PAL, 
PDEL, 
PE, PNC, 
SYM 
BACKSLASH, 
CM, COLON, 
DASH, DOTS, 
FS, LP, QT, 
RP, SEMICO-
LON, SLASH 
Table 1: Developed POS mapping for Spanish and universal tagset 
 
2 Related Works 
Gamon et al (2005) perform a research about 
reference-free SMT evaluation method on sen-
tence level. This work uses both linear and non-
linear combinations of language model and SVM 
classifier to find the badly translated sentences. 
Albrecht and Hwa (2007) conduct the sentence-
level MT evaluation utilizing the regression 
learning and based on a set of weaker indicators 
of fluency and adequacy as pseudo references. 
Specia and Gimenez (2010) use the Confidence 
Estimation features and a learning mechanism 
trained on human annotations. They show that 
the developed models are highly biased by diffi-
culty level of the input segment, therefore they 
are not appropriate for comparing multiple sys-
tems that translate the same input segments. Spe-
cia et al (2010) discussed the issues between the 
traditional machine translation evaluation and the 
quality estimation tasks recently proposed. The 
traditional MT evaluation metrics require refer-
ence translations in order to measure a score re-
flecting some aspects of its quality, e.g. the 
BLEU and NIST. The quality estimation ad-
dresses this problem by evaluating the quality of 
translations as a prediction task and the features 
are usually extracted from the source sentences 
and target (translated) sentences. They also show 
that the developed methods correlate better with 
human judgments at segment level as compared 
to traditional metrics. Popovi? et al (2011) per-
form the MT evaluation using the IBM model 
one with the information of morphemes, 4-gram 
POS and lexicon probabilities. Mehdad et al 
(2012) use the cross-lingual textual entailment to 
push semantics into the MT evaluation without 
using reference translations. This evaluation 
work mainly focuses on the adequacy estimation. 
Avramidis (2012) performs an automatic sen-
tence-level ranking of multiple machine transla-
tions using the features of verbs, nouns, sentenc-
es, subordinate clauses and punctuation occur-
rences to derive the adequacy information. Other 
descriptions of the MT Quality Estimation tasks 
can be gained in the works of (Callison-Burch et 
al., 2012) and (Felice and Specia, 2012). 
3 Tasks Information  
This section introduces the different sub-tasks we 
participated in the Quality Estimation task of 
WMT 13 and the methods we used.  
3.1 Task 1-1 Sentence-level QE 
Task 1.1 is to score and rank the post-editing 
effort of the automatically translated English-
Spanish sentences without offering the reference 
translation. 
Firstly, we develop the English and Spanish 
POS tagset mapping as shown in Table 1. The 75 
Spanish POS tags yielded by the Treetagger 
(Schmid, 1994) are mapped to the 12 universal 
tags developed in (Petrov et al, 2012). The Eng-
lish POS tags are extracted from the parsed sen-
tences using the Berkeley parser (Petrov et al, 
2006). 
Secondly, the enhanced version of BLEU 
(EBLEU) formula is designed with the factors of 
modified length penalty (   ), precision, and 
recall, the   and   representing the lengths of 
hypothesis (target) sentence and source sentence 
respectively. We use the harmonic mean of pre-
cision and recall, i.e.  (       ). We assign 
the weight values     and    , i.e. higher 
weight value is assigned to precision, which is 
different with METEOR (the inverse values). 
 
       
          (?      ( (       ))) (1) 
 
     {
   
 
            
   
 
            
 (2) 
 
    
                    
                                
 (3) 
 
    
                    
                                
 (4) 
366
Lastly, the scoring for the post-editing effort 
of the automatically translated sentences is per-
formed on the extracted POS sequences of the 
source and target languages. The evaluated per-
formance of EBLEU on WMT 12 corpus is 
shown in Table 2 using the Mean-Average-Error 
(MAE), Root-Mean-Squared-Error (RMSE).  
 
 Precision Recall MLP EBLEU 
MAE 0.17 0.19 0.25 0.16 
RMSE 0.22 0.24 0.30 0.21 
Table 2: Performance on the WMT12 corpus 
The official evaluation scores of the testing re-
sults on WMT 13 corpus are shown in Table 3. 
The testing results show similar scores as com-
pared to the training scores (the MAE score is 
around 0.16 and the RMSE score is around 0.22), 
which shows a stable performance of the devel-
oped model EBLEU. However, the performance 
of EBLEU is not satisfactory currently as shown 
in the Table 2 and Table 3. This is due to the fact 
that we only used the POS information as lin-
guistic feature. This could be further improved 
by the combination of lexical information and 
other linguistic features such as the sentence 
structure, phrase similarity, and text entailment. 
 
 MAE RMSE DeltaAvg 
Spearman 
Corr 
EBLEU 16.97 21.94 2.74 0.11 
Baseline 
SVM 
14.81 18.22 8.52 0.46 
Table 3: Performance on the WMT13 corpus 
3.2 Task 1-2 System Selection 
Task 1.2 is the system selection task on EN-ES 
and DE-EN language pairs. Participants are re-
quired to rank up to five alternative translations 
for the same source sentence produced by multi-
ple translation systems.  
Firstly, we describe the two variants of 
EBLEU method for this task. We score the five 
alternative translation sentences as compared to 
the source sentence according to the closeness of 
their POS sequences. The German POS is also 
extracted using Berkeley parser (Petrov et al, 
2006). The mapping of German POS to universal 
POS tagset is using the developed one in the 
work of (Petrov et al, 2012). When we convert 
the absolute scores into the corresponding rank 
values, the variant EBLEU-I means that we use 
five fixed intervals (with the span from 0 to 1) to 
achieve the alignment as shown in Table 4. 
[1,0.4) [0.4, 0.3) [0.3, 0.25) [0.25, 0.2) [0.2, 0] 
5 4 3 2 1 
Table 4: Convert absolute scores into ranks 
 
The alignment work from absolute scores to 
rank values shown in Table 4 is empirically de-
termined. We have made a statistical work on the 
absolute scores yielded by our metrics, and each 
of the intervals shown in Table 4 covers the simi-
lar number of sentence scores. 
On the other hand, in the metric EBLEU-A, 
?A? means average. The absolute sentence edit 
scores are converted into the five rank values 
with the same number (average number). For 
instance, if there are 1000 sentence scores in to-
tal then each rank level (from 1 to 5) will gain 
200 scores from the best to the worst. 
Secondly, we introduce the NB-LPR model 
used in this task. NB-LPR means the Na?ve 
Bayes classification algorithm using the features 
of Length penalty (introduced in previous sec-
tion), Precision, Recall and Rank values. NB-
LPR considers each of its features independently. 
Let?s see the conditional probability that is also 
known as Bayes? rule. If the  ( | )  is given, 
then the  ( | ) can be calculated as follows: 
 
  ( | )  
 ( | ) ( )
 ( )
 (5) 
 
Given a data point identified as 
 (          ) and the classifications 
 (          ), Bayes? rule can be applied to 
this statement: 
 
  (  |          )  
 (         |  ) (  )
 (         )
 (6) 
 
As in many practical applications, parameter 
estimation for NB-LPR model uses the method 
of maximum likelihood. For details of Na?ve 
Bayes algorithm, see the works of (Zhang, 2004) 
and (Harrington, 2012). 
Thirdly, the SVM-LPR model means the sup-
port vector machine classification algorithm us-
ing the features of Length penalty, Precision, 
Recall and Rank values, i.e. the same features as 
in NB-LPR. SVM solves the nonlinear classifica-
tion problem by mapping the data from a low 
dimensional space to a high dimensional space 
using the Kernel methods. In the projected high 
dimensional space, the problem usually becomes 
a linear one, which is easier to solve. SVM is 
also called maximum interval classifier because 
it tries to find the optimized hyper plane that 
367
separates different classes with the largest mar-
gin, which is usually a quadratic optimization 
problem. Let?s see the formula below, we should 
find the points with the smallest margin to the 
hyper plane and then maximize this margin. 
 
          {    (      ( 
    ))  
 
? ?
}
 (7) 
 
where   is normal to the hyper plane, || || is 
the Euclidean norm of  , and | | || ||  is the 
perpendicular distance from the hyper plane to 
the origin. For details of SVM, see the works of 
(Cortes and Vapnik, 1995) and (Burges, 1998). 
 
EN-ES 
NB-LPR SVM-LPR 
MAE RMSE Time MAE RMSE Time 
.315 .399 .40s .304 .551 60.67s 
DE-EN 
NB-LPR SVM-LPR 
MAE RMSE Time MAE RMSE Time 
.318 .401 .79s .312 .559 111.7s 
Table 5: NB-LPR and SVM-LPR training 
In the training stage, we used all the officially 
released data of WMT 09, 10, 11 and 12 for the 
EN-ES and DE-EN language pairs. We used the 
WEKA (Hall et al, 2009) data mining software 
to implement the NB and SVM algorithms. The 
training scores are shown in Table 5. The NB-
LPR performs lower scores than the SVM-LPR 
but faster than SVM-LPR. 
 
 DE-EN EN-ES 
Methods 
Tau(ties 
penalized) 
|Tau|(ties 
ignored) 
Tau(ties 
penalized) 
|Tau|(ties 
ignored) 
EBLEU-I -0.38 -0.03 -0.35 0.02 
EBLEU-A N/A N/A -0.27 N/A 
NB-LPR -0.49 0.01 N/A 0.07 
Baseline  -0.12 0.08 -0.23 0.03 
Table 6: QE Task 1.2 testing scores 
The official testing scores are shown in Table 
6. Each task is allowed to submit up to two sys-
tems and we submitted the results using the 
methods of EBLEU and NB-LPR. The perfor-
mance of NB-LPR on EN-ES language pair 
shows higher Tau score (0.07) than the baseline 
system score (0.03) when the ties are ignored. 
Because of the number limitation of submitted 
systems for each task, we did not submit the 
SVM-LPR results. However, the training exper-
iments prove that the SVM-LPR model performs 
better than the NB-LPR model though SVM-
LPR takes more time to run. 
3.3 Task 2 Word-level QE 
Task 2 is the word-level quality estimation of 
automatically translated news sentences from 
English to Spanish without given reference trans-
lations. Participants are required to judge each 
translated word by assigning a two- or multi-
class labels. In the binary classification, a good 
or a bad label should be judged, where ?bad? 
indicates the need for editing the token. In the 
multi-class classification, the labels include 
?keep?, ?delete? and ?substitute?. In addition to 
the NB method, in this task, we utilized a dis-
criminative undirected probabilistic graphical 
model, i.e. Conditional Random Field (CRF). 
CRF is early employed by Lefferty (Lefferty 
et al, 2001) to deal with the labeling problems of 
sequence data, and is widely used later by other 
researchers. As the preparation for CRF defini-
tion, we assume that   is a variable representing 
the input sequence, and   is another variable rep-
resenting the corresponding labels to be attached 
to  . The two variables interact as conditional 
probability  ( | )  mathematically. Then the 
definition of CRF: Let a graph model   (   ) 
comprise a set   of vertices or nodes together 
with a set   of edges or lines and      |  
  , such that   is indexed by the vertices of  , 
then (   ) shapes a CRF model. This set meets 
the following form:  
 
   ( | )      
(?     (   |   )       ?     (   |   )     )
 (8) 
 
where   and   represent the data sequence and 
label sequence respectively;    and    are the 
features to be defined;    and    are the parame-
ters trained from the datasets. We used the tool 
CRF++1 to implement the CRF algorithm. The 
features we used for the NB and CRF are shown 
in Table 7. We firstly trained the CRF and NB 
models on the officially released training corpus 
(produced by Moses and annotated by computing 
TER with some tweaks). Then we removed the 
truth labels in the training corpus (we call it 
pseudo test corpus) and labeled each word using 
the derived training models. The test results on 
the pseudo test corpus are shown in Table 8, 
                                                 
1 https://code.google.com/p/crfpp/ 
368
which specifies CRF performs better than NB 
algorithm. 
 
     (    ) 
Unigram, from antecedent 4th 
to subsequent 3rd token 
       
 (    ) 
Bigram, from antecedent 2nd 
to subsequent 2nd token 
      
Jump bigram, antecedent and 
subsequent token 
          
 (    ) 
Trigram, from antecedent 2nd 
to subsequent 2nd token 
Table 7: Developed features 
 
Binary 
CRF NB 
Training Accuracy Training Accuracy 
Itera=108 
Time=2.48s 
0.944 Time=0.59s 0.941 
Multi-classes 
CRF NB 
Training Accuracy Training Accuracy 
Itera=106 
Time=3.67s 
0.933 Time=0.55s 0.929 
Table 8: Performance on pseudo test corpus 
The official testing scores of Task 2 are shown 
in Table 9. We include also the results of other 
participants (CNGL and LIG) and their ap-
proaches. 
 
 Binary Multiclass 
Methods Pre Recall F1 Acc 
CNGL-
dMEMM 
0.7392 0.9261 0.8222 0.7162 
CNGL-
MEMM 
0.7554 0.8581 0.8035 0.7116 
LIG-All N/A N/A N/A 0.7192 
LIG-FS 0.7885 0.8644 0.8247 0.7207 
LIG-
BOOSTING 
0.7779 0.8843 0.8276 N/A 
NB 0.8181 0.4937 0.6158 0.5174 
CRF 0.7169 0.9846 0.8297 0.7114 
Table 9: QE Task 2 official testing scores 
The results show that our method CRF yields 
a higher recall score than other systems in binary 
judgments task, and this leads to the highest F1 
score (harmonic mean of precision and recall). 
The recall value reflects the loyalty to the truth 
data. The augmented feature set designed in this 
paper allows the CRF to take the contextual in-
formation into account, and this contributes 
much to the recall score. On the other hand, the 
accuracy score of CRF in multiclass evaluation is 
lower than LIG-FS method. 
4 Conclusions 
This paper describes the algorithms and features 
we used in the WMT 13 Quality Estimation tasks. 
In the sentence-level QE task (Task 1.1), we de-
velop an enhanced version of BLEU metric, and 
this shows a potential usage for the traditional 
evaluation criteria. In the newly proposed system 
selection task (Task 1.2) and word-level QE task 
(Task 2), we explore the performances of several 
statistical models including NB, SVM, and CRF, 
of which the CRF performs best, the NB per-
forms lower than SVM but much faster than 
SVM. The official results show that the CRF 
model yields the highest F-score 0.8297 in binary 
classification judgment of word-level QE task. 
Acknowledgments 
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for our research, 
under the reference No. 017/2009/A and 
RG060/09-10S/CS/FST. The authors also wish to 
thank the anonymous reviewers for many helpful 
comments. 
References  
Akiba, Yasuhiro, Kenji Imamura, and Eiichiro Sumita. 
2001. Using Multiple Edit Distances to Automati-
cally Rank Machine Translation Output. In Pro-
ceedings of the MT Summit VIII, Santiago de 
Compostela, Spain. 
Albrecht, Joshua, and Rebecca Hwa. 2007. Regres-
sion for sentence-level MT evaluation with pseudo 
references. ACL. Vol. 45. No. 1. 
Avramidis, Eleftherios. 2012. Comparative quality 
estimation: Automatic sentence-level ranking of 
multiple machine translation outputs. In Proceed-
ings of 24th International Conference on 
Computational Linguistics (COLING), pages 
115?132, Mumbai, India. 
Burges, Christopher J. C. 1998. A Tutorial on Support 
Vector Machines for Pattern Recognition. J. Data 
Min. Knowl. Discov. Volume 2 Issue 2, June 
1998, 121-167. Kluwer Academic Publishers 
Hingham, MA, USA. 
Callison-Burch, Chris, Philipp Koehn, Christof Monz, 
Matt Post, Radu Soricut, and Lucia Specia. 2012. 
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh 
369
Workshop on Statistical Machine Translation, 
pages 10?51, Montr?al, Canada, June. 
Castillo, Julio and Paula Estrella. 2012. Semantic 
Textual Similarity for MT evaluation, Proceed-
ings of the 7th Workshop on Statistical Ma-
chine Translation (WMT2012), pages 52?58, 
Montre a?l, Canada, June 7-8. Association for 
Computational Linguistics. 
Chan, Yee Seng and Hwee Tou Ng. 2008. MAXSIM: 
A maximum similarity metric for machine transla-
tion evaluation. In Proceedings of ACL 2008: 
HLT, pages 55?62. Association for Computational 
Linguistics. 
Chen, Boxing and Roland Kuhn. 2011. Amber: A 
modified bleu, enhanced ranking metric. In Pro-
ceedings of the Sixth Workshop on Statistical 
Machine translation of the Association for 
Computational Linguistics(ACL-WMT), pages 
71-77, Edinburgh, Scotland, UK. 
Chen, Boxing, Roland Kuhn and Samuel Larkin. 2012. 
PORT: a Precision-Order-Recall MT Evaluation 
Metric for Tuning, Proceedings of the 50th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 930?939, Jeju, Republic 
of Korea, 8-14 July. 
Cortes, Corinna and Vladimir Vapnik. 1995. Support-
Vector Networks, J. Machine Learning, Volume 
20, issue 3, pp 273-297. Kluwer Academic Pub-
lishers, Boston. Manufactured in The Netherlands. 
Dahlmeier, Daniel, Chang Liu, and Hwee Tou Ng. 
2011. TESLA at WMT2011: Translation evalua-
tion and tunable metric. In Proceedings of the 
Sixth Workshop on Statistical Machine Trans-
lation, Association for Computational Linguis-
tics (ACL-WMT), pages 78-84, Edinburgh, Scot-
land, UK. 
Doddington, George. 2002. Automatic evaluation of 
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Lan-
guage Technology Research (HLT '02). Morgan 
Kaufmann Publishers Inc., San Francisco, CA, 
USA, 138-145. 
Echizen-ya, Hiroshi and Kenji Araki. 2010. Automat-
ic evaluation method for machine translation using 
noun-phrase chunking. In Proceedings of ACL 
2010, pages 108?117. Association for Computa-
tional Linguistics. 
Gamon, Michael, Anthony Aue, and Martine Smets. 
2005. Sentence-level MT evaluation without refer-
ence translations: Beyond language modeling. 
Proceedings of EAMT. 
Hall, Mark, Eibe Frank, Geoffrey Holmes, Bernhard 
Pfahringer, Peter Reutemann, Ian H. Witten. 2009. 
The WEKA data mining software: An update. 
SIGKDD Explorations, 11. 
Han, Aaron Li-Feng, Derek F. Wong and Lidia S. 
Chao. 2012. LEPOR: A Robust Evaluation Metric 
for Machine Translation with Augmented Factors. 
Proceedings of the 24th International Confer-
ence on Computational Linguistics (COLING 
2012: Posters), Mumbai, India. 
Han, Aaron Li-Feng, Derek F. Wong, Lidia S. Chao, 
Liangye He, Yi Lu, Junwen Xing and Xiaodong 
Zeng. 2013. Language-independent Model for Ma-
chine Translation Evaluation with Reinforced Fac-
tors. Proceedings of the 14th International 
Conference of Machine Translation Summit 
(MT Summit 2013), Nice, France. 
Harrington, Peter. 2012. Classifying with probability 
theory: na?ve bayes. Machine Learning in Ac-
tion, Part 1 Classification. Page 61-82. Publisher: 
Manning Publications. April. 
Lafferty, John, McCallum Andrew, and Pereira C.N. 
Ferando. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling se-
quence data. In Proceeding of 18th Internation-
al Conference on Machine Learning. 282-289. 
Lavie, Alon and Abhaya Agarwal. 2007. METEOR: 
An Automatic Metric for MT Evaluation with High 
Levels of Correlation with Human Judgments, 
Proceedings of the ACL Second Workshop on 
Statistical Machine Translation, pages 228-231, 
Prague, June. 
Leusch, Gregor, Nicola Ueffing, and Hermann Ney. 
2006. CDer: Efficient MT Evaluation Using Block 
Movements. In Proceedings of the 13th Confer-
ence of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-06), 
241-248. 
Lin, Chin-Yew and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
occurrence Statistics. In Proceedings of 2003 
Language Technology Conference (HLT-
NAACL 2003), Edmonton, Canada, May 27 - June 
1. 
Lita, Lucian Vlad, Monica Rogati and Alon Lavie. 
2005. BLANC: Learning Evaluation Metrics for 
MT, Proceedings of Human Language Tech-
nology Conference and Conference on Empir-
ical Methods in Natural Language Processing 
(HLT/EMNLP), pages 740?747, Vancouver, Oc-
tober. Association for Computational Linguistics. 
Liu, Chang, Daniel Dahlmeier and Hwee Tou Ng. 
2010. TESLA: Translation evaluation of sentences 
370
with linear-programming-based analysis. In Pro-
ceedings of the Joint Fifth Workshop on Statis-
tical Machine Translation and MetricsMATR. 
Liu, Ding and Daniel Gildea. 2006. Stochastic itera-
tive alignment for machine translation evaluation. 
Sydney. ACL06. 
Mariano, Felice and Lucia Specia. 2012. Linguistic 
Features for Quality Estimation. Proceedings of 
the 7th Workshop on Statistical Machine 
Translation, pages 96?103. 
Mehdad, Yashar, Matteo Negri, and Marcello Federi-
co. 2012. Match without a referee: evaluating MT 
adequacy without reference translations. Proceed-
ings of the Seventh Workshop on Statistical 
Machine Translation. Association for Compu-
tational Linguistics. 
Mirkin, Shachar, Lucia Specia, Nicola Cancedda, Ido 
Dagan, Marc Dymetman, and Idan Szpektor. 2009. 
Source-Language Entailment Modeling for Trans-
lating Unknown Terms, Proceedings of the Joint 
Conference of the 47th Annual Meeting of the 
ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the 
AFNLP, pages 791?799, Suntec, Singapore, 2-7. 
ACL and AFNLP. 
Nie?en, Sonja, Franz Josef Och, Gregor Leusch, and 
Hermann Ney. 2000. A Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Re-
search. In Proceedings of the 2nd International 
Conference on Language Resources and Eval-
uation (LREC-2000). 
Owczarzak, Karolina, Josef van Genabith and Andy 
Way. 2007. Labelled Dependencies in Machine 
Translation Evaluation, Proceedings of the ACL 
Second Workshop on Statistical Machine 
Translation, pages 104-111, Prague. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. 
In Proceedings of the 40th Annual Meeting on 
Association for Computational Linguis-
tics (ACL '02). Association for Computational 
Linguistics, Stroudsburg, PA, USA, 311-318. 
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan 
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the 
21st International Conference on Computa-
tional Linguistics and the 44th annual meeting 
of the Association for Computational Linguis-
tics (ACL-44). Association for Computational 
Linguistics, Stroudsburg, PA, USA, 433-440. 
Popovic, Maja, David Vilar, Eleftherios Avramidis, 
Aljoscha Burchardt. 2011. Evaluation without ref-
erences: IBM1 scores as evaluation metrics. In 
Proceedings of the Sixth Workshop on Statisti-
cal Machine Translation, Association for 
Computational Linguistics (ACL-WMT), pages 
99-103, Edinburgh, Scotland, UK. 
Povlsen, Claus, Nancy Underwood, Bradley Music, 
and Anne Neville. 1998. Evaluating Text-Type 
Suitability for Machine Translation a Case Study 
on an English-Danish System. Proceedings of the 
First Language Resources and Evaluation 
Conference, LREC-98, Volume I. 27-31. Grana-
da, Spain. 
Schmid, Helmut. 1994. Probabilistic Part-of-Speech 
Tagging Using Decision Trees. In Proceedings of 
International Conference on New Methods in 
Language Processing, Manchester, UK. 
Snover, Matthew G., Nitin Madnani, Bonnie Dorr, 
and Richard Schwartz. 2009. TER-Plus: paraphrase, 
semantic, and alignment enhancements to Transla-
tion Edit Rate. J. Machine Tranlslation, 23: 117-
127. 
Specia, Lucia and Gimenez, J. 2010. Combining Con-
fidence Estimation and Reference-based Metrics 
for Segment-level MT Evaluation. The Ninth 
Conference of the Association for Machine 
Translation in the Americas (AMTA). 
Specia, Lucia, Dhwaj Raj, and Marco Turchi. 2010. 
Machine Translation Evaluation Versus Quality 
Estimation. Machine Translation, 24:39?50. 
Su, Keh-Yih, Wu Ming-Wen and Chang Jing-Shin. 
1992. A New Quantitative Quality Measure for 
Machine Translation Systems. In Proceedings of 
the 14th International Conference on Compu-
tational Linguistics, pages 433?439, Nantes, 
France, July. 
Tillmann, Christoph, Stephan Vogel, Hermann Ney, 
Arkaitz Zubiaga, and Hassan Sawaf. 1997. Accel-
erated DP Based Search For Statistical Translation. 
In Proceedings of the 5th European Confer-
ence on Speech Communication and Technol-
ogy (EUROSPEECH-97). 
Turian, Joseph P., Luke Shen, and I. Dan Melamed. 
2003. Evaluation of Machine Translation and its 
Evaluation. In Machine Translation Summit IX, 
pages 386?393. International Association for Ma-
chine Translation. 
Wang, Mengqiu and Christopher D. Manning. 2012. 
SPEDE: Probabilistic Edit Distance Metrics for 
MT Evaluation, WMT2012, 76-83. 
Wong, Billy T. M. and Chunyu Kit. 2012. Extending 
Machine Translation Evaluation Metrics with Lex-
ical Cohesion to Document Level. Proceedings of 
the 2012 Joint Conference on Empirical 
371
Methods in Natural Language Processing and 
Computational Natural Language Learning, 
pages 1060?1068, Jeju Island, Korea, 12?14 July. 
Association for Computational Linguistics. 
Zhang, Harry. 2004. The Optimality of Naive Bayes. 
Proceedings of the Seventeenth International 
Florida Artificial Intelligence Research Socie-
ty Conference, Miami Beach, Florida, USA. 
AAAI Press. 
372
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 414?421,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
A Description of Tunable Machine Translation Evaluation Systems in 
WMT13 Metrics Task 
 
 
Aaron L.-F. Han 
hanlifengaaron@gmail.com 
Derek F. Wong 
derekfw@umac.mo 
Lidia S. Chao 
lidiasc@umac.mo 
   
Yi Lu 
mb25435@umac.mo 
Liangye He 
wutianshui0515@gmail.com 
Yiming Wang 
mb25433@umac.mo 
Jiaji Zhou 
mb25473@uamc.mo 
    
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory 
Department of Computer and Information Science 
University of Macau, Macau S.A.R. China 
 
  
 
Abstract 
This paper is to describe our machine transla-
tion evaluation systems used for participation 
in the WMT13 shared Metrics Task. In the 
Metrics task, we submitted two automatic MT 
evaluation systems nLEPOR_baseline and 
LEPOR_v3.1. nLEPOR_baseline is an n-gram 
based language independent MT evaluation 
metric employing the factors of modified sen-
tence length penalty, position difference penal-
ty, n-gram precision and n-gram recall. 
nLEPOR_baseline measures the similarity of 
the system output translations and the refer-
ence translations only on word sequences. 
LEPOR_v3.1 is a new version of LEPOR met-
ric using the mathematical harmonic mean to 
group the factors and employing some linguis-
tic features, such as the part-of-speech infor-
mation. The evaluation results of WMT13 
show LEPOR_v3.1 yields the highest average-
score 0.86 with human judgments at system-
level using Pearson correlation criterion on 
English-to-other (FR, DE, ES, CS, RU) lan-
guage pairs. 
1 Introduction 
Machine translation has a long history since the 
1950s (Weaver, 1955) and gains a fast develop-
ment in the recent years because of the higher 
level of computer technology. For instances, Och 
(2003) presents Minimum Error Rate Training 
(MERT) method for log-linear statistical ma-
chine translation models to achieve better trans-
lation quality; Menezes et al (2006) introduce a 
syntactically informed phrasal SMT system for 
English-to-Spanish translation using a phrase 
translation model, which is based on global reor-
dering and the dependency tree; Su et al (2009) 
use the Thematic Role Templates model to im-
prove the translation; Costa-juss? et al (2012) 
develop the phrase-based SMT system for Chi-
nese-Spanish translation using a pivot language. 
With the rapid development of Machine Transla-
tion (MT), the evaluation of MT has become a 
challenge in front of researchers. However, the 
MT evaluation is not an easy task due to the fact 
of the diversity of the languages, especially for 
the evaluation between distant languages (Eng-
lish, Russia, Japanese, etc.). 
2 Related works 
The earliest human assessment methods for ma-
chine translation include the intelligibility and 
fidelity used around 1960s (Carroll, 1966), and 
the adequacy (similar as fidelity), fluency and 
comprehension (improved intelligibility) (White 
et al, 1994). Because of the expensive cost of 
manual evaluations, the automatic evaluation 
metrics and systems appear recently. 
The early automatic evaluation metrics in-
clude the word error rate WER (Su et al, 1992) 
and position independent word error rate PER 
(Tillmann et al, 1997) that are based on the Le-
venshtein distance. Several promotions for the 
MT and MT evaluation literatures include the 
ACL?s annual workshop on statistical machine 
translation WMT (Koehn and Monz, 2006; Calli-
son-Burch et al, 2012), NIST open machine 
translation (OpenMT) Evaluation series (Li, 
2005) and the international workshop of spoken 
language translation IWSLT, which is also orga-
nized annually from 2004 (Eck and Hori, 2005; 
414
Paul, 2008, 2009; Paul, et al, 2010; Federico et 
al., 2011). 
BLEU (Papineni et al, 2002) is one of the 
commonly used evaluation metrics that is de-
signed to calculate the document level precisions. 
NIST (Doddington, 2002) metric is proposed 
based on BLEU but with the information weights 
added to the n-gram approaches. TER (Snover et 
al., 2006) is another well-known MT evaluation 
metric that is designed to calculate the amount of 
work needed to correct the hypothesis translation 
according to the reference translations. TER in-
cludes the edit categories such as insertion, dele-
tion, substitution of single words and the shifts of 
word chunks. Other related works include the 
METEOR (Banerjee and Lavie, 2005) that uses 
semantic matching (word stem, synonym, and 
paraphrase), and (Wong and Kit, 2008), (Popovic, 
2012), and (Chen et al, 2012) that introduces the 
word order factors, etc. The traditional evalua-
tion metrics tend to perform well on the language 
pairs with English as the target language. This 
paper will introduce the evaluation models that 
can also perform well on the language pairs that 
with English as source language. 
3 Description of Systems 
3.1 Sub Factors 
Firstly, we introduce the sub factor of modified 
length penalty inspired by BLEU metric. 
 
    {
   
 
            
                  
   
 
            
 (1) 
 
In the formula,    means sentence length 
penalty that is designed for both the shorter or 
longer translated sentence (hypothesis translation) 
as compared to the reference sentence. Parame-
ters   and   represent the length of candidate 
sentence and reference sentence respectively. 
Secondly, let?s see the factors of n-gram pre-
cision and n-gram recall. 
 
    
              
                           
 (2) 
 
    
              
                          
  (3) 
 
The variable                represents the 
number of matched n-gram chunks between hy-
pothesis sentence and reference sentence. The n-
gram precision and n-gram recall are firstly cal-
culated on sentence-level instead of corpus-level 
that is used in BLEU (  ). Then we define the 
weighted n-gram harmonic mean of precision 
and recall (WNHPR). 
 
          (?       (        
 
    (4) 
 
Thirdly, it is the n-gram based position differ-
ence penalty (NPosPenal). This factor is de-
signed to achieve the penalty for the different 
order of successfully matched words in reference 
sentence and hypothesis sentence. The alignment 
direction is from the hypothesis sentence to the 
reference sentence. It employs the  -gram meth-
od into the matching period, which means that 
the potential matched word will be assigned 
higher priority if it also has nearby matching. 
The nearest matching will be accepted as a back-
up choice if there are both nearby matching or 
there is no other matched word around the poten-
tial pairs. 
 
                 (5) 
 
     
 
          
?      
         
    (6) 
 
                             (7) 
 
The variable           means the length of 
the hypothesis sentence; the variables 
          and           represent the posi-
tion number of matched words in hypothesis sen-
tence and reference sentence respectively.  
3.2 Linguistic Features 
The linguistic features could be easily employed 
into our evaluation models. In the submitted ex-
periment results of WMT Metrics Task, we used 
the part of speech information of the words in 
question. In grammar, a part of speech, which is 
also called a word class, a lexical class, or a lexi-
cal category, is a linguistic category of lexical 
items. It is generally defined by the syntactic or 
morphological behavior of the lexical item in 
question. The POS information utilized in our 
metric LEPOR_v3.1, an enhanced version of 
LEPOR (Han et al, 2012), is extracted using the 
Berkeley parser (Petrov et al, 2006) for English, 
German, and French languages, using COM-
POST Czech morphology tagger (Collins, 2002) 
for Czech language, and using TreeTagger 
(Schmid, 1994) for Spanish and Russian lan-
guages respectively. 
415
Ratio 
other-to-English English-to-other 
CZ-EN DE-EN ES-EN FR-EN EN-CZ EN-DE EN-ES EN-FR 
HPR:LP:NPP(word) 7:2:1 3:2:1 7:2:1 3:2:1 7:2:1 1:3:7 3:2:1 3:2:1 
HPR:LP:NPP(POS) NA 3:2:1 NA 3:2:1 7:2:1 7:2:1 NA 3:2:1 
    (      1:9 9:1 1:9 9:1 9:1 9:1 9:1 9:1 
    (     NA 9:1 NA 9:1 9:1 9:1 NA 9:1 
        NA 1:9 NA 9:1 1:9 1:9 NA 9:1 
Table 1. The tuned weight values in LEPOR_v3.1 system 
 
System 
Correlation Score with Human Judgment 
other-to-English English-to-other Mean 
score CZ-EN DE-EN ES-EN FR-EN EN-CZ EN-DE EN-ES EN-FR 
LEPOR_v3.1 0.93 0.86 0.88 0.92 0.83 0.82 0.85 0.83 0.87 
nLEPOR_baseline 0.95 0.61 0.96 0.88 0.68 0.35 0.89 0.83 0.77 
METEOR 0.91 0.71 0.88 0.93 0.65 0.30 0.74 0.85 0.75 
BLEU 0.88 0.48 0.90 0.85 0.65 0.44 0.87 0.86 0.74 
TER 0.83 0.33 0.89 0.77 0.50 0.12 0.81 0.84 0.64 
Table 2. The performances of nLEPOR_baseline and LEPOR_v3.1 systems on WMT11 corpora 
 
3.3 The nLEPOR_baseline System 
The nLEPOR_baseline system utilizes the simple 
product value of the factors: modified length 
penalty, n-gram position difference penalty, and 
weighted n-gram harmonic mean of precision 
and recall. 
 
                           (8) 
 
The system level score is the arithmetical 
mean of the sentence level evaluation scores. In 
the experiments of Metrics Task using the 
nLEPOR_baseline system, we assign N=1 in the 
factor WNHPR, i.e. weighted unigram harmonic 
mean of precision and recall. 
3.4 The LEPOR_v3.1 System 
The system of LEPOR_v3.1 (also called as 
hLEPOR) combines the sub factors using 
weighted mathematical harmonic mean instead 
of the simple product value. 
 
        
                   
   
  
 
          
         
 
    
   
 (9) 
 
Furthermore, this system takes into account 
the linguistic features, such as the POS of the 
words. Firstly, we calculate the hLEPOR score 
on surface words            (the closeness of 
the hypothesis translation and the reference 
translation). Then, we calculate the hLEPOR 
score on the extracted POS sequences 
          (the closeness of the corresponding 
POS tags between hypothesis sentence and refer-
ence sentence). The final score             is 
the combination of the two sub-scores 
           and          . 
 
             
 
       
(              
               (10) 
 
4 Evaluation Method 
In the MT evaluation task, the Spearman rank 
correlation coefficient method is usually used by 
the authoritative ACL WMT to evaluate the cor-
relation of different MT evaluation metrics. So 
we use the Spearman rank correlation coefficient 
  to evaluate the performances of 
nLEPOR_baseline and LEPOR_v3.1 in system 
level correlation with human judgments. When 
there are no ties,   is calculated using: 
 
     
 ?  
 
 (     
  (11) 
 
The variable    is the difference value be-
tween the ranks for         and   is the number 
of systems. We also offer the Pearson correlation 
coefficient information as below. Given a sample 
of paired data (X, Y) as (      ,         , the 
Pearson correlation coefficient is: 
 
     
? (      (      
 
   
?? (       
 
   
?? (     )
 
     
 (12) 
416
where    and    specify the mean of discrete 
random variable X and Y respectively. 
 
Directions 
EN-
FR 
EN-
DE 
EN-
ES 
EN-
CS 
EN-
RU 
Av 
LEPOR_v3.1 .91 .94 .91 .76 .77 .86 
nLEPOR_baseline .92 .92 .90 .82 .68 .85 
SIMP-
BLEU_RECALL 
.95 .93 .90 .82 .63 .84 
SIMP-
BLEU_PREC 
.94 .90 .89 .82 .65 .84 
NIST-mteval-
inter 
.91 .83 .84 .79 .68 .81 
Meteor .91 .88 .88 .82 .55 .81 
BLEU-mteval-
inter 
.89 .84 .88 .81 .61 .80 
BLEU-moses .90 .82 .88 .80 .62 .80 
BLEU-mteval .90 .82 .87 .80 .62 .80 
CDER-moses .91 .82 .88 .74 .63 .80 
NIST-mteval .91 .79 .83 .78 .68 .79 
PER-moses .88 .65 .88 .76 .62 .76 
TER-moses .91 .73 .78 .70 .61 .75 
WER-moses .92 .69 .77 .70 .61 .74 
TerrorCat .94 .96 .95 na na .95 
SEMPOS na na na .72 na .72 
ACTa .81 -.47 na na na .17 
ACTa5+6 .81 -.47 na na na .17 
Table 3. System-level Pearson correlation scores 
on WMT13 English-to-other language pairs 
5 Experiments 
5.1 Training 
In the training stage, we used the officially re-
leased data of past WMT series. There is no Rus-
sian language in the past WMT shared tasks. So 
we trained our systems on the other eight lan-
guage pairs including English to other (French, 
German, Spanish, Czech) and the inverse transla-
tion direction. In order to avoid the overfitting 
problem, we used the WMT11 corpora as train-
ing data to train the parameter weights in order to 
achieve a higher correlation with human judg-
ments at system-level evaluations. For the 
nLEPOR_baseline system, the tuned values of   
and   are 9 and 1 respectively for all language 
pairs except for (   ,    ) for CS-EN lan-
guage pair. For the LEPOR_v3.1 system, the 
tuned values of weights are shown in Table 1. 
The evaluation scores of the training results on 
WMT11 corpora are shown in Table 2. The de-
signed methods have shown promising correla-
tion scores with human judgments at system lev-
el, 0.87 and 0.77 respectively for 
nLEPOR_baseline and LEPOR_v3.1 of the mean 
score on eight language pairs. As compared to 
METEOR, BLEU and TER, we have achieved 
higher correlation scores with human judgments.  
5.2 Testing 
In the WMT13 shared Metrics Task, we also 
submitted our system performances on English-
to-Russian and Russian-to-English language 
pairs. However, since the Russian language did 
not appear in the past WMT shared tasks, we 
assigned the default parameter weights to Rus-
sian language for the submitted two systems. The 
officially released results on WMT13 corpora are 
shown in Table 3, Table 4 and Table 5 respec-
tively for system-level and segment-level per-
formance on English-to-other language pairs. 
 
Directions 
EN-
FR 
EN-
DE 
EN-
ES 
EN-
CS 
EN-
RU 
Av 
SIMP-
BLEU_RECALL 
.92 .93 .83 .87 .71 .85 
LEPOR_v3.1 .90 .9 .84 .75 .85 .85 
NIST-mteval-
inter 
.93 .85 .80 .90 .77 .85 
CDER-moses .92 .87 .86 .89 .70 .85 
nLEPOR_baseline .92 .90 .85 .82 .73 .84 
NIST-mteval .91 .83 .78 .92 .72 .83 
SIMP-
BLEU_PREC 
.91 .88 .78 .88 .70 .83 
Meteor .92 .88 .78 .94 .57 .82 
BLEU-mteval-
inter 
.92 .83 .76 .90 .66 .81 
BLEU-mteval .89 .79 .76 .90 .63 .79 
TER-moses .91 .85 .75 .86 .54 .78 
BLEU-moses .90 .79 .76 .90 .57 .78 
WER-moses .91 .83 .71 .86 .55 .77 
PER-moses .87 .69 .77 .80 .59 .74 
TerrorCat .93 .95 .91 na na .93 
SEMPOS na na na .70 na .70 
ACTa5+6 .81 -.53 na na na .14 
ACTa .81 -.53 na na na .14 
Table 4. System-level Spearman rank correlation 
scores on WMT13 English-to-other language 
pairs 
 
Table 3 shows LEPOR_v3.1 and 
nLEPOR_baseline yield the highest and the sec-
ond highest average Pearson correlation score 
0.86 and 0.85 respectively with human judg-
ments at system-level on five English-to-other 
language pairs. LEPOR_v3.1 and 
417
nLEPOR_baseline also yield the highest Pearson 
correlation score on English-to-Russian (0.77) 
and English-to-Czech (0.82) language pairs re-
spectively. The testing results of LEPOR_v3.1 
and nLEPOR_baseline show better correlation 
scores as compared to METEOR (0.81), BLEU 
(0.80) and TER-moses (0.75) on English-to-other 
language pairs, which is similar with the training 
results.  
On the other hand, using the Spearman rank 
correlation coefficient, SIMPBLEU_RECALL 
yields the highest correlation score 0.85 with 
human judgments. Our metric LEPOR_v3.1 also 
yields the highest Spearman correlation score on 
English-to-Russian (0.85) language pair, which 
is similar with the result using Pearson correla-
tion and shows its robust performance on this 
language pair.  
 
Directions 
EN-
FR 
EN-
DE 
EN-
ES 
EN-
CS 
EN-
RU Av 
SIMP-
BLEU_RECALL 
.16 .09 .23 .06 .12 .13 
Meteor .15 .05 .18 .06 .11 .11 
SIMP-
BLEU_PREC 
.14 .07 .19 .06 .09 .11 
sentBLEU-moses .13 .05 .17 .05 .09 .10 
LEPOR_v3.1 .13 .06 .18 .02 .11 .10 
nLEPOR_baseline .12 .05 .16 .05 .10 .10 
dfki_logregNorm-
411 
na na .14 na na .14 
TerrorCat .12 .07 .19 na na .13 
dfki_logregNormS
oft-431 
na na .03 na na .03 
Table 5. Segment-level Kendall?s tau correlation 
scores on WMT13 English-to-other language 
pairs 
 
However, we find a problem in the Spearman 
rank correlation method. For instance, let two 
evaluation metrics MA and MB with their evalu-
ation scores   ??????                   and  
  ???? ??                   respectively reflecting 
three MT systems  
 ??            . Before the calculation of cor-
relation with human judgments, they will be 
converted into   ??????  ?          and   ???? ??  ?          
with the same rank sequence using Spearman 
rank method; thus, the two evaluation systems 
will get the same correlation with human judg-
ments whatever are the values of human judg-
ments. But the two metrics reflect different re-
sults indeed: MA gives the outstanding score 
(0.95) to M1 system and puts very low scores 
(0.50 and 0.45) on the other two systems M2 and 
M3 while MB thinks the three MT systems have 
similar performances (scores from 0.74 to 0.77). 
This information is lost using the Spearman rank 
correlation methodology. 
The segment-level performance of 
LEPOR_v3.1 is moderate with the average Ken-
dall?s tau correlation score 0.10 on five English-
to-other language pairs, which is due to the fact 
that we trained our metrics at system-level in this 
shared metrics task. Lastly, the officially released 
results on WMT13 other-to-English language 
pairs are shown in Table 6, Table 7 and Table 8 
respectively for system-level and segment-level 
performance.  
 
Directions 
FR-
EN 
DE-
EN 
ES-
EN 
CS-
EN 
RU-
EN Av 
Meteor .98 .96 .97 .99 .84 .95 
SEMPOS .95 .95 .96 .99 .82 .93 
Depref-align .97 .97 .97 .98 .74 .93 
Depref-exact .97 .97 .96 .98 .73 .92 
SIMP-
BLEU_RECALL 
.97 .97 .96 .94 .78 .92 
UMEANT .96 .97 .99 .97 .66 .91 
MEANT .96 .96 .99 .96 .63 .90 
CDER-moses .96 .91 .95 .90 .66 .88 
SIMP-
BLEU_PREC 
.95 .92 .95 .91 .61 .87 
LEPOR_v3.1 .96 .96 .90 .81 .71 .87 
nLEPOR_baseline .96 .94 .94 .80 .69 .87 
BLEU-mteval-
inter 
.95 .92 .94 .90 .61 .86 
NIST-mteval-inter .94 .91 .93 .84 .66 .86 
BLEU-moses .94 .91 .94 .89 .60 .86 
BLEU-mteval .95 .90 .94 .88 .60 .85 
NIST-mteval .94 .90 .93 .84 .65 .85 
TER-moses .93 .87 .91 .77 .52 .80 
WER-moses .93 .84 .89 .76 .50 .78 
PER-moses .84 .88 .87 .74 .45 .76 
TerrorCat .98 .98 .97 na na .98 
Table 6. System-level Pearson correlation scores 
on WMT13 other-to-English language pairs 
 
METEOR yields the highest average correla-
tion scores 0.95 and 0.94 respectively using 
Pearson and Spearman rank correlation methods 
on other-to-English language pairs. The average 
performance of nLEPOR_baseline is a little bet-
ter than LEPOR_v3.1 on the five language pairs 
of other-to-English even though it is also moder-
ate as compared to other metrics. However, using 
418
the Pearson correlation method, 
nLEPOR_baseline yields the average correlation 
score 0.87 which already wins the BLEU (0.86) 
and TER (0.80) as shown in Table 6. 
 
Directions 
FR-
EN 
DE-
EN 
ES-
EN 
CS-
EN 
RU-
EN Av 
Meteor .98 .96 .98 .96 .81 .94 
Depref-align .99 .97 .97 .96 .79 .94 
UMEANT .99 .95 .96 .97 .79 .93 
MEANT .97 .93 .94 .97 .78 .92 
Depref-exact .98 .96 .94 .94 .76 .92 
SEMPOS .94 .92 .93 .95 .83 .91 
SIMP-
BLEU_RECALL 
.98 .94 .92 .91 .81 .91 
BLEU-mteval-
inter 
.99 .90 .90 .94 .72 .89 
BLEU-mteval .99 .89 .89 .94 .69 .88 
BLEU-moses .99 .90 .88 .94 .67 .88 
CDER-moses .99 .88 .89 .93 .69 .87 
SIMP-
BLEU_PREC 
.99 .85 .83 .92 .72 .86 
nLEPOR_baseline .95 .95 .83 .85 .72 .86 
LEPOR_v3.1 .95 .93 .75 0.8 .79 .84 
NIST-mteval .95 .88 .77 .89 .66 .83 
NIST-mteval-inter .95 .88 .76 .88 .68 .83 
TER-moses .95 .83 .83 0.8 0.6 
0.8
0 
WER-moses .95 .67 .80 .75 .61 .76 
PER-moses .85 .86 .36 .70 .67 .69 
TerrorCat .98 .96 .97 na na .97 
Table 7. System-level Spearman rank correlation 
scores on WMT13 other-to-English language 
pairs 
 
Once again, our metrics perform moderate at 
segment-level on other-to-English language pairs 
due to the fact that they are trained at system-
level. We notice that some of the evaluation met-
rics do not submit the results on all the language 
pairs; however, their performance on submitted 
language pair is sometimes very good, such as 
the dfki_logregFSS-33 metric with a segment-
level correlation score 0.27 on German-to-
English language pair. 
6 Conclusion 
This paper describes our participation in the 
WMT13 Metrics Task. We submitted two sys-
tems nLEPOR_baseline and LEPOR_v3.1. Both 
of the two systems are trained and tested using 
the officially released data. LEPOR_v3.1 yields 
the highest Pearson correlation average-score 
0.86 with human judgments on five English-to-
other language pairs, and nLEPOR_baseline 
yields better performance than LEPOR_v3.1 on 
other-to-English language pairs. Furthermore, 
LEPOR_v3.1 shows robust system-level perfor-
mance on English-to-Russian language pair, and 
nLEPOR_baseline shows best system-level per-
formance on English-to-Czech language pair us-
ing Pearson correlation criterion. As compared to 
nLEPOR_baseline, the experiment results of 
LEPOR_v3.1 also show that the proper use of 
linguistic information can increase the perfor-
mance of the evaluation systems. 
 
Directions 
FR-
EN 
DE-
EN 
ES-
EN 
CS-
EN 
RU-
EN Av 
SIMP-
BLEU_RECALL 
.19 .32 .28 .26 .23 .26 
Meteor .18 .29 .24 .27 .24 .24 
Depref-align .16 .27 .23 .23 .20 .22 
Depref-exact .17 .26 .23 .23 .19 .22 
SIMP-
BLEU_PREC 
.15 .24 .21 .21 .17 .20 
nLEPOR_baseline .15 .24 .20 .18 .17 .19 
sentBLEU-moses .15 .22 .20 .20 .17 .19 
LEPOR_v3.1 .15 .22 .16 .19 .18 .18 
UMEANT .10 .17 .14 .16 .11 .14 
MEANT .10 .16 .14 .16 .11 .14 
dfki_logregFSS-
33 
na .27 na na na .27 
dfki_logregFSS-
24 
na .27 na na na .27 
TerrorCat .16 .30 .23 na na .23 
Table 8. Segment-level Kendall?s tau correlation 
scores on WMT13 other-to-English language 
pairs 
Acknowledgments 
The authors wish to thank the anonymous re-
viewers for many helpful comments. The authors 
are grateful to the Science and Technology De-
velopment Fund of Macau and the Research 
Committee of the University of Macau for the 
funding support for our research, under the refer-
ence No. 017/2009/A and RG060/09-
10S/CS/FST.  
References  
Banerjee, Satanjeev and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In 
Proceedings of the 43th Annual Meeting of the 
419
Association of Computational Linguistics 
(ACL- 05), pages 65?72, Ann Arbor, US, June. 
Association of Computational Linguistics. 
Callison-Burch, Chris, Philipp Koehn, Christof Monz, 
and Omar F. Zaidan. 2011. Findings of the 2011 
Workshop on Statistical Machine Translation. 
In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation (WMT '11). Asso-
ciation for Computational Linguistics, Stroudsburg, 
PA, USA, 22-64. 
Callison-Burch, Chris, Philipp Koehn, Christof Monz, 
Matt Post, Radu Soricut, and Lucia Specia. 2012. 
Findings of the 2012Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh 
Workshop on Statistical Machine Translation, 
pages 10?51, Montreal, Canada. Association for 
Computational Linguistics. 
Carroll, John B. 1966. An Experiment in Evaluating 
the Quality of Translations, Mechanical Transla-
tion and Computational Linguistics, vol.9, 
nos.3 and 4, September and December 1966, page 
55-66, Graduate School of Education, Harvard 
University. 
Chen, Boxing, Roland Kuhn and Samuel Larkin. 2012. 
PORT: a Precision-Order-Recall MT Evaluation 
Metric for Tuning, Proceedings of the 50th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 930?939, Jeju, Republic 
of Korea, 8-14 July 2012. 
Collins, Michael. 2002. Discriminative Training 
Methods for Hidden Markov Models: Theory and 
Experiments with Perceptron Algorithms. In Pro-
ceedings of the ACL-02 conference on Empiri-
cal methods in natural language processing, 
Volume 10 (EMNLP 02), pages 1-8. Association 
for Computational Linguistics, Stroudsburg, PA, 
USA. 
Costa-juss?, Marta R., Carlos A. Henr?quez and Ra-
fael E. Banchs. 2012. Evaluating Indirect Strategies 
for Chinese-Spanish Statistical Machine Transla-
tion. Journal of artificial intelligence research, 
Volume 45, pages 761-780. 
Doddington, George. 2002. Automatic evaluation of 
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Lan-
guage Technology Research (HLT '02). Morgan 
Kaufmann Publishers Inc., San Francisco, CA, 
USA, 138-145. 
Eck, Matthias and Chiori Hori. 2005. Overview of the 
IWSLT 2005 Evaluation Campaign. Proceedings 
of IWSLT 2005. 
Federico, Marcello, Luisa Bentivogli, Michael Paul, 
and Sebastian Stiiker. 2011. Overview of the 
IWSLT 2011 Evaluation Campaign. In Proceed-
ings of IWSLT 2011, 11-27. 
Han, Aaron Li-Feng, Derek F. Wong and Lidia S. 
Chao. 2012. LEPOR: A Robust Evaluation Metric 
for Machine Translation with Augmented Factors. 
Proceedings of the 24th International Confer-
ence on Computational Linguistics (COLING 
2012: Posters), Mumbai, India. 
Koehn, Philipp and Christof Monz. 2006. Manual and 
Automatic Evaluation of Machine Translation be-
tween European Languages. Proceedings of the 
ACLWorkshop on Statistical Machine Trans-
lation, pages 102?121, New York City. 
Li, A. (2005). Results of the 2005 NIST machine 
translation evaluation. In Machine Translation 
Workshop. 
Menezes, Arul, Kristina Toutanova and Chris Quirk. 
2006. Microsoft Research Treelet Translation Sys-
tem: NAACL 2006 Europarl Evaluation, Proceed-
ings of the ACLWorkshop on Statistical Ma-
chine Translation, pages 158-161, New York 
City. 
Och, Franz Josef. 2003. Minimum Error Rate Train-
ing for Statistical Machine Translation.  In Pro-
ceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics (ACL-
2003). pp. 160-167. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. 
In Proceedings of the 40th Annual Meeting on 
Association for Computational Linguis-
tics (ACL '02). Association for Computational 
Linguistics, Stroudsburg, PA, USA, 311-318. 
Paul, Michael. 2008. Overview of the IWSLT 2008 
Evaluation Campaign. Proceeding of IWLST 
2008, Hawaii, USA. 
Paul, Michael. 2009. Overview of the IWSLT 2009 
Evaluation Campaign. In Proc. of IWSLT 2009, 
Tokyo, Japan, pp. 1?18. 
Paul, Michael, Marcello Federico and Sebastian 
Stiiker. 2010. Overview of the IWSLT 2010 Eval-
uation Campaign, Proceedings of the 7th Inter-
national Workshop on Spoken Language 
Translation, Paris, December 2nd and 3rd, 2010, 
page 1-25. 
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan 
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the 
21st International Conference on Computa-
tional Linguistics and the 44th annual meeting 
of the Association for Computational Linguis-
420
tics (ACL-44). Association for Computational 
Linguistics, Stroudsburg, PA, USA, 433-440. 
Popovic, Maja. 2012. Class error rates for evaluation 
of machine translation output. Proceedings of the 
7th Workshop on Statistical Machine Transla-
tion, pages 71?75, Canada. 
Schmid, Helmut. 1994. Probabilistic Part-of-Speech 
Tagging Using Decision Trees. In Proceedings of 
International Conference on New Methods in 
Language Processing, Manchester, UK. 
Snover, Matthew, Bonnie J. Dorr, Richard Schwartz, 
Linnea Micciulla, and John Makhoul. 2006. A 
Study of Translation Edit Rate with Targeted Hu-
man Annotation. In Proceedings of the 7th Con-
ference of the Association for Machine Trans-
lation in the Americas (AMTA-06), pages 223?
231, USA. Association for Machine Translation in 
the Americas. 
Su, Hung-Yu and Chung-Hsien Wu. 2009. Improving 
Structural Statistical Machine Translation for Sign 
Language With Small Corpus Using Thematic 
Role Templates as Translation Memory, IEEE 
TRANSACTIONS ON AUDIO, SPEECH, AND 
LANGUAGE PROCESSING, VOL. 17, NO. 7. 
Su, Keh-Yih, Wu Ming-Wen and Chang Jing-Shin. 
1992. A New Quantitative Quality Measure for 
Machine Translation Systems. In Proceedings of 
the 14th International Conference on Compu-
tational Linguistics, pages 433?439, Nantes, 
France. 
Tillmann, Christoph, Stephan Vogel, Hermann Ney, 
Arkaitz Zubiaga, and Hassan Sawaf. 1997. Accel-
erated DP Based Search For Statistical Translation. 
In Proceedings of the 5th European Confer-
ence on Speech Communication and Technol-
ogy (EUROSPEECH-97). 
Weaver, Warren. 1955. Translation. In William Locke 
and A. Donald Booth, editors, Machine Transla-
tion of Languages: Fourteen Essays. John 
Wiley & Sons, New York, pages 15?23. 
White, John S., Theresa O?Connell, and Francis 
O?Mara. 1994. The ARPA MT evaluation method-
ologies: Evolution, lessons, and future approaches. 
In Proceedings of the Conference of the Asso-
ciation for Machine Translation in the Ameri-
cas (AMTA 1994). pp193-205. 
Wong, Billy and Chunyu Kit. 2008. Word choice and 
word position for automatic MT evaluation. In 
Workshop: MetricsMATR of the Association for 
Machine Translation in the Americas (AMTA), 
short paper, Waikiki, Hawai?I, USA. Association 
for Machine Translation in the Americas. 
 
421
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 83?90,
Baltimore, Maryland, 26-27 July 2014. c?2014 Association for Computational Linguistics
Factored Statistical Machine Translation for Grammatical Error 
Correction 
 
 
Yiming Wang, Longyue Wang, Derek F. Wong, Lidia S. Chao, Xiaodong Zeng, Yi Lu 
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory,  
Department of Computer and Information Science, 
University of Macau, Macau S.A.R., China 
{wang2008499,vincentwang0229}@gmail.com,derekfw@umac.mo, 
lidiasc@umac.mo,nlp2ct.samuel@gmail.com,mb25435@umac.mo 
 
  
 
Abstract 
This paper describes our ongoing work on 
grammatical error correction (GEC). Focusing 
on all possible error types in a real-life 
environment, we propose a factored statistical 
machine translation (SMT) model for this task. 
We consider error correction as a series of 
language translation problems guided by 
various linguistic information, as factors that 
influence translation results. Factors included 
in our study are morphological information, i.e. 
word stem, prefix, suffix, and Part-of-Speech 
(PoS) information. In addition, we also 
experimented with different combinations of 
translation models (TM), phrase-based and 
factor-based, trained on various datasets to 
boost the overall performance. Empirical 
results show that the proposed model yields an 
improvement of 32.54% over a baseline 
phrase-based SMT model. The system 
participated in the CoNLL 2014 shared task 
and achieved the 7
th
 and 5
th
 F0.5 scores
1 on the 
official test set among the thirteen 
participating teams. 
 
1 Introduction 
The task of grammatical error detection and 
correction (GEC) is to make use of 
computational methods to fix the mistakes in a 
written text. It is useful in two aspects. For a 
non-native English learner it may help to 
improve the grammatical quality of the written 
text. For a native speaker the tool may help to 
remedy mistakes automatically. Automatic 
                                                          
1  These two rankings are based on gold-standard edits 
without and with alternative answers, respectively. 
correction of grammatical errors is an active 
research topic, aiming at improving the writing 
process with the help of artificial intelligent 
techniques. Second language learning is a user 
group of particular interest. 
Recently, Helping Our Own (HOO) and 
CoNLL held a number of shared tasks on this 
topic (Dale et al., 2012, Ng et al., 2013, Ng et al., 
2014). Previous studies based on rules (Sidorov 
et al., 2013), data-driven methods (Berend et al., 
2013, Yi et al., 2013) and hybrid methods (Putra 
and Szab?, 2013, Xing et al., 2013) have shown 
substantial gains for some frequent error types 
over baseline methods. Most proposed methods 
share the commonality that a sub-model is built 
for a specific type of error, on top of which a 
strategy is applied to combine a number of these 
individual models. Also, detection and correction 
are often split into two steps. For example, Xing 
et al. (2013) presented the UM-Checker for five 
error types in the CoNLL 2013 shared task. The 
system implements a cascade of five individual 
detection-and-correction models for different 
types of error. Given an input sentence, errors are 
detected and corrected one-by-one by each sub-
model at the level of its corresponding error type.  
The specifics of an error type are fully 
considered in each sub-model, which is easier to 
realize for a single error type than for multiple 
types in a single model. In addition, dividing the 
error detection and correction into two steps 
alleviates the application of machine learning 
classifiers. However, an approach that considers 
error types individually may have negative 
effects: 
? This approach assumes independence 
between each error type. It ignores the 
interaction of neighboring errors. Results 
(Xing et al., 2013) have shown that 
83
consecutive errors of multiple types tend to 
hinder solving these errors individually. 
? As the number of error types increases, the 
complexities of analyzing, designing, and 
implementing the model increase, in 
particular when combinatorial errors are 
taken into account. 
? Looking for an optimal model combination 
becomes complex. A simple pipeline 
approach would result in interference and the 
generation of new errors, and hence to 
propagating those errors to the subsequent 
processes. 
? Separating the detection and correction tasks 
may result in more errors. For instance, once 
a candidate is misidentified as an error, it 
would be further revised and turned into an 
error by the correction model. In this 
scenario the model risks losing precision. 
In the shared task of this year (Ng et la., 
2014), two novelties are introduced: 1) all types 
of errors present in an essay are to be detected 
and corrected (i.e., there is no restriction on the 
five error types of the 2013 shared task); 2) the 
official evaluation metric of this year adopts F0.5, 
weighting precision twice as much as recall. This 
requires us to explore an alternative universal 
joint model that can tackle various kinds of 
grammatical errors as well as join the detection 
and correction processes together. Regarding 
grammatical error correction as a process of 
translation has been shown to be effective (Ehsan 
and Faili, 2013, Mizumoto et al., 2011, 
Yoshimoto et al., 2013, Yuan and Felice, 2013). 
We treat the problematic sentences and golden 
sentences as pairs of source and target sentences. 
In SMT, a translation model is trained on a 
parallel corpus that consists of the source 
sentences (i.e. sentences that may contain 
grammatical errors) and the targeted translations 
(i.e. the grammatically well-formed sentences). 
The challenge is that we need a large amount of 
these parallel sentences for constructing such a 
data-driven SMT system. Some researches 
(Brockett et al., 2006, Yuan and Felice, 2013) 
explore generating artificial errors to resolve this 
sparsity problem. Other studies (Ehsan and Faili, 
2013, Yoshimoto et al., 2013, Yuan and Felice, 
2013) focus on using syntactic information (such 
as PoS or tree structure) to enhance the SMT 
models.  
In this paper, we propose a factored SMT 
model by taking into account not only the surface 
information contained in the sentence, but also 
morphological and syntactic clues (i.e., word 
stem, prefix, suffix and finer PoS information). 
To counter the sparsity problem we do not use 
artificial or manual approaches to enrich the 
training data. Instead we apply factored and 
transductive learning techniques to enhance the 
model on a small dataset. In addition, we also 
experimented with different combinations of 
translation models (TM), phrase- and factor-
based, that are trained on different datasets to 
boost the overall performance. Empirical results 
show that the proposed model yields an 
improvement of 32.54% over a baseline phrase-
based SMT model. 
The remainder of this paper is organized as 
follows: Section 2 describes our proposed 
methods. Section 3 reports on the design of our 
experiments. We discuss the result, including the 
official shared task results, in Section 4,. We 
summarize our conclusions in Section 5. 
2 Methodology 
In contrast with phrase-based translation models, 
factored models make use of additional linguistic 
clues to guide the system such that it generates 
translated sentences in which morphological and 
syntactic constraints are met (Koehn and Hoang, 
2007). The linguistic clues are taken as factors in 
a factored model; words are represented as 
vectors of factors rather than as a single token. 
This requires us to pre-process the training data 
to factorize all words. In this study, we explore 
the use of various types of morphological 
information and PoS as factors. For each possible 
factor we build an individual translation model. 
The effectiveness of all factors is analyzed by 
comparing the performance of the corresponding 
models on the grammatical error correction task. 
Furthermore, two approaches are proposed to 
combine those models. One adopts the model 
cascading method based on transductive learning. 
The second approach relies on learning and 
decoding multiple factors learning. The details of 
each approach are discussed in the following 
sub-sections. 
2.1 Data Preparation 
In order to construct a SMT model, we convert 
the training data into a parallel corpus where the 
problematic sentences that ought to be corrected 
are regarded as source sentences, while the 
reference sentences are treated as the 
corresponding target translations. We discovered 
that a number of sentences is absent at the target 
side due to incorrect annotations in the golden 
84
data. We removed these unparalleled sentences 
from the data. Secondly, the initial 
capitalizations of sentences are converted to their 
most probable casing using the Moses truecaser2. 
URLs are quite common in the corpus, but they 
are not useful for learning and even may cause 
the model to apply unnecessary correction on it. 
Thus, we mark all of the ULRs with XML 
markups, signaling the SMT decoder not to 
analyze an URL and output it as is.  
2.2 Model Construction 
In this study we explore four different factors: 
prefix, suffix, stem, and PoS. This linguistic 
information not only helps to capture the local 
constraints of word morphologies and the 
interaction of adjacent words, but also helps to 
prevent data sparsity caused by inflected word 
variants and insufficient training data.  
Word stem: Instead of lemmas, we prefer  
word stemming as one of the factors, considering 
that stemming does not requires deep 
morphological analysis and is easier to obtain. 
Second, during the whole error detection and 
correction process, stemming information is used 
as auxiliary information in addition to the 
original word form. Third, for grammatical error 
correction using word lemmas or word stems in 
factored translation model shows no significant 
difference. This is because we are translating text 
of the same language, and the translation of this 
factor, stem or lemma, is straightforwardly 
captured by the model. Hence, we do not rely on 
the word lemma. In this work, we use the 
English Porter stemmer (Porter, 1980) for 
generating word stems.  
Prefix: The second type of morphological 
information we explored is the word prefix. 
Although a prefix does not present strong 
evidence to be useful to the grammatical error 
correction, we include it in our study in order to 
fully investigate all types of morphological 
information. We believe the prefix can be an 
important factor in the correction of initial 
capitalization, e.g. ?In this era, engineering 
designs?? should be changed to ?In this era, 
engineering designs?? In model construction, 
we take the first three letters of a word as its 
prefix. If the length of a word is less than three, 
we use the word as the prefix factor. 
Suffix: Suffix, one of the important factors, 
helps to capture the grammatical agreements 
between predicates and arguments within a 
                                                          
2 After decoding, we will de-truecase all these words. 
sentence. Particularly the endings of plural nouns 
and inflected verb variants are useful for the 
detection of agreement violations that shown up 
in word morphologies. Similar to how we 
represent the prefix, we are interested in the last 
three characters of a word.  
 Examples 
Sentence 
this card contains biometric data to 
add security and reduce the risk of 
falsification 
Original 
POS 
DT NN BVZ JJ NNS TO VB NN 
CC VB DT NN IN NN 
Specific 
POS 
DT NN VBZ JJ NNS TO_to VB 
NN CC VB DT_the NN IN_of 
NN 
Table 1: Example of modified PoS. 
According to the description of factors, Figure 
1 illustrates the forms of various factors 
extracted from a given example sentence.  
Surface 
constantly combining ideas will 
result in better solutions being 
formulated 
Prefix con com ide wil res in bet sol bei for 
Suffix tly ing eas ill ult in ter ons ing ted 
Stem 
constantli combin idea will result in 
better solut be formul 
Specific 
POS 
RB VBG NNS MD VB IN JJR NNS 
VBG VBN 
Figure 1: The factorized sentence. 
PoS: Part-of-Speech tags denote the morpho-
syntactic category of a word. The use of PoS 
sequences enables us to some extent to recover 
missing determiners, articles, prepositions, as 
well as the modal verb in a sentence. Empirical 
studies (Yuan and Felice, 2013) have 
demonstrated that the use of this information can 
greatly improve the accuracy of the grammatical 
error correction. To obtain the PoS, we adopt the 
Penn Treebank tag set (Marcus et al., 1993), 
which contains 45 PoS tags. The Stanford parser 
(Klein and Manning, 2002) is used to extract the 
PoS information. Inspired by Yuan and Felice 
(2013), who used preposition-specific tags to fix 
the problem of being unable to distinguish 
between prepositions and obtained good 
performance, we create specific tags both for 
determiners (i.e., a, an, the) and prepositions. 
Table 1 provides an example of this modification, 
where prepositions, TO and IN, and determiner, 
85
DT, are revised to TO_to, IN_of and DT_the, 
respectively. 
2.3 Model Combination 
In addition to the design of different factored 
translation models, two model combination 
strategies are designed to treat grammatical error 
correction problem as a series of translation 
processes, where an incorrect sentence is 
translated into the correct one. In both 
approaches we pipeline two translation models, 
    and    . In the first approach, we derive 
four combinations of different models that 
trained on different sources.  
? In case I,    
  and    
  are both factored 
models but trained on different factors, e.g. 
for     
 training on ?surface + factori? and 
    
  on ?surface + factori?j?. Both models 
use the same training sentences, but different 
factors.  
? In case II,     
  is trained on sentences that 
paired with the output from the previous 
model,     
 , and the golden correct sentences. 
We want to create a second model that can 
also tackle the new errors introduced by the 
first model. 
? In case III, similar to case II, the second 
translation model,    
  is replaced by a 
phrase-based translation model.  
? In case IV, the quality of training data is 
considered vital to the construction of a good 
translation model. The present training dataset 
is not large enough. To complement this, the 
second model,     
 , is trained on an enlarged 
data set, by combining the training data of 
both models, i.e. the original parallel data 
(official incorrect and correct sentence pairs) 
and the supplementary parallel data 
(sentences output from the first model,     
 , 
and the correct sentences). Note that we do 
not de-duplicate sentences.  
In all cases, the testing process is carried out 
as follows. The test set is translated by the first 
translation model,     
 . The output from the first 
model is then fed into the second translation 
model,     
 . The output of the second model is 
used as the final corrections. 
The second combination approach is to make 
use of multiple factors for model construction. 
The question is whether multiple factors when 
used together may improve the correction results. 
In this setting we combine two factors together 
with the word surface form to build a multi-
factored translation model. All pairs of factors 
are used, e.g. stem and PoS. The decoding 
sequence is as follows: translate the input stems 
into target stems; translate the PoS; and generate 
the surface form given the factors of stem and 
PoS. 
3 Experiment Setup  
3.1 Dataset 
We pre-process the NUCLE corpus (Dahlmeier 
et al., 2013) as described in Section 2 for training 
different translation models. We use both the 
official golden sentences and additional 
WMT2014 English monolingual data3 to train an 
in-domain and a general-domain language model 
(LM), respectively. These language models are 
linearly interpolated in the decoding phase. We 
also randomly select a number of sentence pairs 
from the parallel corpus as a development set and 
a test set, disjoint from the training data. Table 2 
summarizes the statistics of all the datasets.  
Corpus Sentences Tokens 
Parallel 
Corpus 
55,503 
1,124,521 / 
1,114,040 
Additional 
Monolingual 
85,254,788 2,033,096,800 
Dev. Set 500 10,532 / 10,438 
Test Set 900 18,032 / 17,906 
Table 2: Statistics of used corpora. 
The experiments were carried out with 
MOSES 1.04 (Philipp Koehn et al., 2007). The 
translation and the re-ordering model utilizes the 
?grow-diag-final? symmetrized word-to-word 
alignments created with GIZA++5 (Och and Ney, 
2003) and the training scripts of MOSES. A 5-
gram LM was trained using the SRILM toolkit6 
(Stolcke et al., 2002), exploiting the improved 
modified Kneser-Ney smoothing (Kneser and 
Ney, 1995), and quantizing both probabilities 
and back-off weights. For the log-linear model 
training, we take minimum-error-rate training 
(MERT) method as described in (Och, 2003). 
The result is evaluated by M2 Scorer (Dahlmeier 
and Ng, 2012) computing precision, recall and 
F0.5.  
                                                          
3 http://www.statmt.org/wmt14/translation-task.html. 
4 http://www.statmt.org/moses/. 
5 http://code.google.com/p/giza-pp/. 
6 http://www.speech.sri.com/projects/srilm/. 
86
In total, one baseline system, five individual 
systems, and four combination systems are 
evaluated in this study. The baseline system 
(Baseline) is trained on the words-only corpus 
using a phrase-based translation model. For the 
individual systems we adopt the factored 
translation model that are trained respectively on 
1) surface and stem factors (Sys+stem), 2) surface 
and suffix factors (Sys+suf), 3) surface and prefix 
factors (Sys+pref), 4) surface and PoS factors 
(Sys+PoS), and 5) surface and modified-PoS 
factors (Sys+MPoS). The combination systems 
include: 1) the combination of ?factored + 
phrase-based? and ?factored + factored? for 
models cascading; and 2) the factors of surface, 
stem and modified-PoS (Sys+stem+MPoS) are 
combined for constructing a correction system 
based on a multi-factor model. 
4 Results and Discussions 
We report our results in terms of the precision, 
recall and F0.5 obtained by each of the individual 
models and combined models.  
4.1 Individual Model 
Table 3 shows the absolute measures for the 
baseline system, while the other individual 
models are listed with values relative to the 
baseline.  
Model Precision  Recall  F0.5 
Baseline 25.58 3.53 11.37 
Sys+stem -14.84 +13.00 +0.18 
Sys+suf -14.57 +14.77 +0.60 
Sys+pref -15.74 +12.20 -0.77 
Sys+PoS -11.63 +9.79 +2.45 
Sys+MPoS -10.25 +10.60 +3.70 
Table 3: Performance of various models. 
The baseline system has the highest precision 
score but the lowest recall. Nearly all individual 
models except Sys+pref show improvements in the 
correction result (F0.5) over the baseline. Overall, 
Sys+MPoS achieves the best result for the 
grammatical error correction task. It shows a 
significant improvement over the other models 
and outperforms the baseline model by 3.7 F0.5 
score. The Sys+stem and Sys+suf models obtain an 
improvement of 0.18 and 0.60 in F0.5 scores, 
respectively, compared to the baseline. Although 
the differences are not significant, it confirms our 
hypothesis that morphological clues do help to 
improve error correction. The F0.5 score of 
Sys+pref is the lowest among the models including 
the baseline, showing a drop of 0.77 in F0.5 score 
against the baseline. One possible reason is that 
few errors (in the training corpus) involve word 
prefixes. Thus, the prefix does not seem to be a 
suitable factor for tackling the GEC problem. 
Type 
Sys+stem 
(%) 
Sys+suf 
(%) 
Sys+MPoS 
(%) 
Error 
Num. 
Vt 17.07 12.20 12.20 41 
ArtOrDet 37.65 36.47 29.41 85 
Nn 33.33 19.61 23.53 51 
Prep 10.26 10.26 12.82 39 
Wci 9.10 10.61 6.10 66 
Rloc- 15.20 13.92 10.13 79 
Table 4: The capacity of different models in 
handling six frequent error types. 
We analyze the capacities of the models on 
different types of errors. Sys+PoS and Sys+MPoS are 
built by using the PoS and modified PoS. Both of 
them yield an improvement in F0.5 score. Overall, 
Sys+MPoS produces more accurate results than 
Sys+pref. Therefore, we specifically compare and 
evaluate the best three models, Sys+stem, Sys+suf 
and Sys+MPoS. Table 4 presents evaluation scores 
of these models for the six most frequent error 
types, which take up a large part of the training 
and test data. Among them, Sys+stem displays a 
powerful capacity to handle determiner and 
noun/number agreement errors, up to 37.65% 
and 33.33%. Sys+suf shows the ability to correct 
determiner errors at 36.47%; Sys+MPoS yields a 
similar performance to Sys+suf. All three 
individual models exhibit a relatively high 
capacity to handle determiner errors. The likely 
reason is that this mistake constitutes the largest 
portion in training data and test set, giving the 
learning models many examples to capture this 
problem well. In the case of preposition errors, 
Sys+MPoS demonstrates a better performance. This, 
once again, confirms the result (Yuan and Felice, 
2013) that the modified PoS factor is effective 
for every preposition word. For these six error 
types, the individual models show a weak 
capacity to handle the word collocation or idiom 
error category (Wci). Although Sys+MPoS 
achieves the highest F0.5 score in the overall 
evaluation, it only achieves 6.10% in handling 
this error type. The likely reason is that idioms 
are not frequent in the training data, and also that 
in most of the cases they contain out-of-
vocabulary words never seen in training data. 
4.2 Model Combination 
We intend to further boost the overall 
performance of the correction system by 
87
combining the strengths of individual models 
through model combination, and compare against 
the baseline. The systems compared here cover 
three pipelined models and a multi-factored 
model, as described earlier in Section 3. The 
combined systems include: 1) CSyssuf+phrase: the 
combination of Sys+suf and the baseline phrase-
based translation model; 2) CSyssuf+suf: we 
combine two similar factored models with suffix 
factors, Sys+suf, which is trained on the same 
corpus; and 3) TSyssuf+phrase: similar to 
CSyssuf+phrase, but the training data for the second 
phrase-based model is augmented by adding the 
output sentences from the previous model (paired 
with the correct sentences). Our intention is to 
enlarge the size of the training data. The 
evaluation results are presented in Table 5. 
Model Precision Recall F0.5 
Baseline 25.58 3.53 11.37 
CSyssuf+phrase -14.70 +14.61 +0.45 
CSyssuf+suf -15.04 +14.13 +0.09 
TSyssuf+phrase -14.76 +14.61 +0.40 
Sys+stem+MPoS -15.87 +11.72 -0.90 
Table 5: Evaluation results of combined models. 
In Table 5 we observe that Sys+stem+MPoS hurts 
performance and shows a drop of 0.9% in F0.5 
score. Both the CSyssuf+phrase and CSyssuf+suf 
show minor improvements over the baseline 
system. Even when we enrich the training data 
for the second model in TSyssuf+phrase, it cannot 
help in boosting the overall performance of the 
system. One of the problems we observe is that, 
with this combination structure, new incorrect 
sentences are introduced by the model at each 
step. The errors are propagated and accumulated 
to the final result. Although CSyssuf+phrase and 
CSyssuf+suf produce a better F0.5 score over the 
baseline, they are not as good as the individual 
models, Sys+PoS and Sys+MPoS, which are trained 
on PoS and modified-PoS, respectively. 
4.3 The Official Result 
After fully evaluating the designed individual 
models as well as the integrated ones, we adopt 
Sys+MPoS as our designated system for this 
grammatical error correction task. The official 
test set consists of 50 essays, and 2,203 errors. 
Table 6 shows the final result obtained by our 
submitted system.  
Table 7 details the correction rate of the five 
most frequent error types obtained by our system. 
The result suggests that the proposed system has 
a better ability in handling the verb, article and 
determiner error than other error types. 
Criteria Result Alt. Result 
P 0.3127 0.4317 
R 0.1446 0.1972 
F0.5 0.2537 0.3488 
Table 6: The official correction results of our 
submitted system. 
Type Error Correct % 
Vt 203/201 21/22 10.34/10.94 
V0 57/54 9/9 15.79/16.67 
Vform 156/169 11/18 7.05/10.65 
ArtOrDet 569/656 84/131 14.76/19.97 
Nn 319/285 31/42 9.72/10.91 
Table 7: Detailed error information of evaluation 
system (with alternative result). 
5 Conclusion 
This paper describes our proposed grammatical 
error detection and correction system based on a 
factored statistical machine translation approach. 
We have investigated the effectiveness of models 
trained with different linguistic information 
sources, namely morphological clues and 
syntactic PoS information. In addition, we also 
explore some ways to combine different models 
in the system to tackle the correction problem. 
The constructed models are compared against the 
baseline model, a phrase-based translation model. 
Results show that PoS information is a very 
effective factor, and the model trained with this 
factor outperforms the others. One difficulty of 
this year?s shared task is that participants have to 
tackle all 28 types of errors, which is five times 
more than last year. From the results, it is 
obvious there are still many rooms for improving 
the current system. 
Acknowledgements 
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for their research, 
under the Reference nos. MYRG076 (Y1-L2)-
FST13-WF and MYRG070 (Y1-L2)-FST12-CS. 
The authors also wish to thank the anonymous 
reviewers for many helpful comments with 
special thanks to Antal van den Bosch for his 
generous help on this manuscript. 
  
88
References  
 G?bor Berend, Veronika Vincze, Sina Zarriess, 
and Rich?rd Farkas. 2013. LFG-based 
Features for Noun Number and Article 
Grammatical Errors. CoNLL-2013. 
Chris Brockett, William B. Dolan, and Michael 
Gamon. 2006. Correcting ESL errors using 
phrasal SMT techniques. Proceedings of the 
21st International Conference on 
Computational Linguistics and the 44th 
annual meeting of the Association for 
Computational Linguistics pages 249?256. 
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei 
Wu. 2013. Building a Large Annotated 
Corpus of Learner English: The NUS Corpus 
of Learner English. Proceedings of the Eighth 
Workshop on Innovative Use of NLP for 
Building Educational Applications. pages 22-
31. 
Robert Dale, Ilya Anisimoff, and George 
Narroway. 2012. HOO 2012: A report on the 
preposition and determiner error correction 
shared task. Proceedings of the Seventh 
Workshop on Building Educational 
Applications Using NLP pages 54?62. 
Nava Ehsan, and Heshaam Faili. 2013. 
Grammatical and context-sensitive error 
correction using a statistical machine 
translation framework. Software: Practice and 
Experience. Wiley Online Library. 
D. Klein, and C. D. Manning. 2002. Fast exact 
inference with a factored model for natural 
language parsing. Advances in neural 
information processing systems. 
Reinhard Kneser, and Hermann Ney. 1995. 
Improved backing-off for m-gram language 
modeling. Acoustics, Speech, and Signal 
Processing, 1995. ICASSP-95., 1995 
International Conference on Vol. 1, pages 
181?184. 
P. Koehn, and H. Hoang. 2007. Factored 
translation models. Proceedings of the Joint 
Conference on Empirical Methods in Natural 
Language Processing and Computational 
Natural Language Learning (EMNLP-CoNLL) 
Vol. 868, pages 876?876. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, 
Chris Callison-Burch, Marcello Federico, 
Nicola Bertoldi, Brooke Cowan, et al. 2007. 
Moses: Open source toolkit for statistical 
machine translation. Proceedings of the 45th 
Annual Meeting of the ACL on Interactive 
Poster and Demonstration Sessions pages 
177?180. 
M. P. Marcus, M. A. Marcinkiewicz, and B. 
Santorini. 1993. Building a large annotated 
corpus of English: The Penn Treebank. 
Computational linguistics. MIT Press. 
Tomoya Mizumoto, Mamoru Komachi, Masaaki 
Nagata, and Yuji Matsumoto. 2011. Mining 
Revision Log of Language Learning SNS for 
Automated Japanese Error Correction of 
Second Language Learners. IJCNLP pages 
147?155. 
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, 
Christian Hadiwinoto, Raymond Hendy 
Susanto, and Bryant Christopher. 2014. The 
conll-2014 shared task on grammatical error 
correction. Proceedings of CoNLL. Baltimore, 
Maryland, USA. 
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, 
Christian Hadiwinoto, and Joel Tetreault. 
2013. The conll-2013 shared task on 
grammatical error correction. Proceedings of 
CoNLL. 
Franz Josef Och. 2003. Minimum Error Rate 
Training in Statistical Machine Translation, 
160?167. 
Franz Josef Och, and Hermann Ney. 2003. A 
systematic comparison of various statistical 
alignment models. Computational linguistics. 
MIT Press. 
Martin F. Porter. 1980. An algorithm for suffix 
stripping. Program: electronic library and 
information systems. MCB UP Ltd. 
Desmond Darma Putra, and Lili Szab?. 2013. 
UdS at the CoNLL 2013 Shared Task. 
CoNLL-2013. 
Grigori Sidorov, Anubhav Gupta, Martin Tozer, 
Dolors Catala, Angels Catena, and Sandrine 
Fuentes. 2013. Rule-based System for 
Automatic Grammar Correction Using 
Syntactic N-grams for English Language 
Learning (L2). CoNLL-2013. 
Andreas Stolcke, and others. 2002. SRILM-an 
extensible language modeling toolkit. 
INTERSPEECH. 
Junwen Xing, Longyue Wang, Derek F. Wong, 
Lidia S. Chao, and Xiaodong Zeng. 2013. 
UM-Checker: A Hybrid System for English 
Grammatical Error Correction. Proceedings of 
the Seventeenth Conference on Computational 
Natural Language Learning: Shared Task, 
34?42. Sofia, Bulgaria: Association for 
Computational Linguistics. Retrieved from 
http://www.aclweb.org/anthology/W13-3605 
Bong-Jun Yi, Ho-Chang Lee, and Hae-Chang 
Rim. 2013. KUNLP Grammatical Error 
Correction System For CoNLL-2013 Shared 
89
Task. CoNLL-2013. 
Ippei Yoshimoto, Tomoya Kose, Kensuke 
Mitsuzawa, Keisuke Sakaguchi, Tomoya 
Mizumoto, Yuta Hayashibe, Mamoru 
Komachi, et al. 2013. NAIST at 2013 CoNLL 
grammatical error correction shared task. 
CoNLL-2013. 
Zheng Yuan, and Mariano Felice. 2013. 
Constrained grammatical error correction 
using Statistical Machine Translation. 
CoNLL-2013. 
  
90
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 233?238,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
Domain Adaptation for Medical Text Translation Using Web Re-
sources
Yi Lu, Longyue Wang, Derek F. Wong, Lidia S. Chao, Yiming Wang, Francisco Oliveira
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory,
Department of Computer and Information Science,
University of Macau, Macau, China
takamachi660@gmail.com, vincentwang0229@hotmail.com,
derekfw@umac.mo, lidiasc@umac.mo, wang2008499@gmail.com, 
olifran@umac.mo
Abstract
This paper describes adapting statistical 
machine translation (SMT) systems to 
medical domain using in-domain and 
general-domain data as well as web-
crawled in-domain resources. In order to 
complement the limited in-domain corpo-
ra, we apply domain focused web-
crawling approaches to acquire in-
domain monolingual data and bilingual 
lexicon from the Internet. The collected 
data is used for adapting the language 
model and translation model to boost the 
overall translation quality. Besides, we 
propose an alternative filtering approach
to clean the crawled data and to further 
optimize the domain-specific SMT sys-
tem. We attend the medical summary
sentence unconstrained translation task of 
the Ninth Workshop on Statistical Ma-
chine Translation (WMT2014). Our sys-
tems achieve the second best BLEU 
scores for Czech-English, fourth for 
French-English, English-French language 
pairs and the third best results for re-
minding pairs.
1 Introduction
In this paper, we report the experiments carried 
out by the NLP2CT Laboratory at University of 
Macau for WMT2014 medical sentence transla-
tion task on six language pairs: Czech-English 
(cs-en), French-English (fr-en), German-English 
(de-en) and the reverse direction pairs (i.e., en-cs, 
en-fr and en-de). 
As data in specific domain are usually rela-
tively scarce, the use of web resources to com-
plement the training resources provides an effec-
tive way to enhance the SMT systems (Resnik 
and smith, 2003; Espl?-Gomis and Forcada, 2010; 
Pecina et al., 2011; Pecina et al., 2012; Pecina et 
al., 2014). In our experiments, we not only use 
all available training data provided by the
WMT2014 standard translation task 1 (general-
domain data) and medical translation task2 (in-
domain data), but also acquire addition in-
domain bilingual translations (i.e. dictionary) and 
monolingual data from online sources.
First of all, we collect the medical terminolo-
gies from the web. This tiny but significant par-
allel data are helpful to reduce the out-of-
vocabulary words (OOVs) in translation models. 
In addition, the use of larger language models 
during decoding is aided by more efficient stor-
age and inference (Heafield, 2011). Thus, we 
crawl more in-domain monolingual data from the 
Internet based on domain focused web-crawling
approach. In order to detect and remove out-
domain data from the crawled data, we not only 
explore text-to-topic classifier, but also propose 
an alternative filtering approach combined the 
existing one (text-to-topic classifier) with per-
plexity. After carefully pre-processing all the 
available training data, we apply language model 
adaptation and translation model adaptation us-
ing various kinds of training corpora. Experi-
mental results show that the presented approach-
es are helpful to further boost the baseline system.
The reminder of this paper is organized as fol-
lows. In Section 2, we detail the workflow of 
web resources acquisition. Section 3 describes 
the pre-processing steps for the corpora. Section 
5 presents the baseline system. Section 6 reports 
the experimental results and discussions. Finally, 
                                                
1 http://www.statmt.org/wmt14/translation-task.html.
2 http://www.statmt.org/wmt14/medical-task/.
233
the submitted systems and the official results are 
reported in Section 7.
2 Domain Focused Web-Crawling
In this section, we introduce our domain focused 
web-crawling approaches on acquisition of in-
domain translation terminologies and monolin-
gual sentences. 
2.1 Bilingual Dictionary
Terminology is a system of words used to name 
things in a particular discipline. The in-domain 
vocabulary size directly affects the performance 
of domain-specific SMT systems. Small size of 
in-domain vocabulary may result in serious 
OOVs problem in a translation system. Therefore, 
we crawl medical terminologies from some 
online sources such as dict.cc3, where the vocab-
ularies are divided into different subjects. We 
obtain the related bilingual entries in medicine 
subject by using Scala build-in XML parser and 
XPath. After cleaning, we collected 28,600, 
37,407, and 37,600 entries in total for cs-en, de-
en, and fr-en respectively.
2.2 Monolingual Data
The workflow for acquiring in-domain resources 
consists of a number of steps such as domain 
identification, text normalization, language iden-
tification, noise filtering, and post-processing as 
well as parallel sentence identification.
Firstly we use an open-source crawler, Com-
bine4, to crawl webpages from the Internet. In 
order to classify these webpages as relevant to 
the medical domain, we use a list of triplets 
<term, relevance weight, topic class> as the 
basic entries to define the topic. Term is a word 
or phrase. We select terms for each language 
from the following sources: 
? The Wikipedia title corpus, a WMT2014 of-
ficial data set consisting of titles of medical 
articles. 
? The dict.cc dictionary, as is described in Sec-
tion 2.1.
? The DrugBank corpus, which is a WMT2014 
official data set on bioinformatics and 
cheminformatics.
For the parallel data, i.e. Wikipedia and dict.cc 
dictionary, we separate the source and target text 
into individual text and use either side of them
for constructing the term list for different lan-
                                                
3 http://www.dict.cc/.
4 http://combine.it.lth.se/.
guages. Regarding the DrugBank corpus, we di-
rectly extract the terms from the ?name? field. 
The vocabulary size of collected text for each 
language is shown in Table 1.
EN CS DE FR
Wikipedia Titles 12,684 3,404 10,396 8,436
dict.cc 29,294 16,564 29,963 22,513
DrugBank 2,788
Total 44,766 19,968 40,359 30,949
Table 1: Size of terms used for topic definition.
Relevance weight is the score for each occur-
rence of the term, which is assigned by its length, 
i.e., number of tokens. The topic class indicates 
the topics. In this study, we are interested in 
medical domain, the topic class is always marked 
with ?MED? in our topic definition. 
The topic relevance of each document is cal-
culated5 as follows:
  ? ?      
   
  
   
 
   (1)
where is the amount of terms in the topic defi-
nition;   
 is the weight of term  ;   
 is the 
weight of term at location  .    is the number of 
occurrences of term  at  position. In implemen-
tation, we use the default values for setting and
parameters. Another input required by the crawl-
er is a list of seed URLs, which are web sites that 
related to medical topic. We limit the crawler 
from getting the pages within the http domain 
guided by the seed links. We acquired the list 
from the Open Directory Project6, which is a re-
pository maintained by volunteer editors. Totally, 
we collected 12,849 URLs from the medicine
category.
Text normalization is to convert the text of 
each HTML page into UTF-8 encoding accord-
ing to the content_charset of the header. In addi-
tion, HTML pages often consist of a number of 
irrelevant contents such as the navigation links, 
advertisements disclaimers, etc., which may neg-
atively affect the performance of SMT system. 
Therefore, we use the Boilerpipe tool 
(Kohlsch?tter et al., 2010) to filter these noisy
data and preserve the useful content that is 
marked by the tag, <canonicalDocument>. The 
resulting text is saved in an XML file, which will 
be further processed by the subsequent tasks. For 
language identification, we use the language-
detection7 toolkit to determine the possible lan-
                                                
5
http://combine.it.lth.se/documentation/DocMain/node6.html.
6 http://www.dmoz.org/Health/Medicine/.
7 https://code.google.com/p/language-detection/.
234
guage of the text, and discard the articles which 
are in the right language we are interested.
2.3 Data Filtering
The web-crawled documents (described in Sec-
tion 2.2) may consist a number of out-domain 
data, which would harm the domain-specific lan-
guage and translation models. We explore and 
propose two filtering approaches for this task. 
The first one is to filter the documents based on 
their relative score, Eq. (1). We rank all the doc-
uments according to their relative scores and se-
lect top K percentage of entire collection for fur-
ther processing. 
Second, we use a combination method, which 
takes both the perplexity and relative score into 
account for the selection. Perplexity-based data 
selection has shown to be a powerful mean on 
SMT domain adaptation (Wang et al., 2013; 
Wang et al., 2014; Toral, 2013; Rubino et al., 
2013; Duh et al., 2013). The combination method 
is carried out as follows: we first retrieve the 
documents based on their relative scores. The 
documents are then split into sentences, and
ranked according to their perplexity using Eq. (2)
(Stolcke et al., 2002). The used language model 
is trained on the official in-domain data. Finally, 
top N percentage of ranked sentences are consid-
ered as additional relevant in-domain data. 
    ( )        
 ( )
    (2)
where  is a input sentence or document,  ( ) is 
the probability of  -gram segments estimated 
from the training set.     is the number of 
tokens of an input string.
3 Pre-processing
Both official training data and web-crawled re-
sources are processed using the Moses scripts8, 
this includes the text tokenization, truecasing and 
length cleaning. For trusecasing, we use both the 
target side of parallel corpora and monolingual 
data to train the trucase models. We consider the 
target system is intended for summary translation, 
the sentences tend to be short in length. We re-
move sentence pairs which are more than 80 
words at length in either sides of the parallel text.
In addition to these general data filtering steps,
we introduce some extra steps to pre-process the 
training data. The first step is to remove the du-
plicate sentences. In data-driven methods, the 
more frequent a term occurs, the higher probabil-
                                                
8 http://www.statmt.org/moses/?n=Moses.Baseline.
ity it biases. Duplicate data may lead to unpre-
dicted behavior during the decoding. Therefore, 
we keep only the distinct sentences in monolin-
gual corpus. By taking into account multiple 
translations in parallel corpus, we remove the 
duplicate sentence pairs. We also use a biomedi-
cal sentence splitter9 (Rune et al., 2007) to split 
sentences in monolingual corpora. The statistics 
of the data are provided in Table 2.
4 Baseline System
We built our baseline system on an optimized 
level. It is trained on all official in-domain train-
ing corpora and a portion of general-domain data. 
We apply the Moore-Lewis method (Moore and 
Lewis, 2010) and modified Moore-Lewis method 
(Axelrod et al., 2011) for selecting in-domain 
data from the general-domain monolingual and 
parallel corpora, respectively. The top M per-
centages of ranked sentences are selected as a 
pseudo in-domain data to train an additional LM
and TM. For LM, we linearly interpolate the ad-
ditional LM with in-domain LM. For TM, the 
additional model is log-linearly interpolated with 
the in-domain model using the multi-decoding 
method described in (Koehn and Schroeder, 
2007). Finally, LM adaptation and TM adapta-
tion are combined to further improve the transla-
tion quality of baseline system.
5 Experiments and Results
The official medical summary development sets 
(dev) are used for tuning and evaluating the 
comparative systems. The official medical sum-
mary test sets (test) are only used in our final 
submitted systems.
The experiments were carried out with the 
Moses 1.010 (Koehn et al., 2007). The translation 
and the re-ordering model utilizes the ?grow-
diag-final? symmetrized word-to-word align-
ments created with MGIZA++11 (Och and Ney, 
2003; Gao and Vogel, 2008) and the training 
scripts from Moses. A 5-gram LM was trained 
using the SRILM toolkit12 (Stolcke et al., 2002), 
exploiting improved modified Kneser-Ney 
smoothing, and quantizing both probabilities and 
back-off weights. For the log-linear model train-
ing, we take the minimum-error-rate training 
(MERT) method as described in (Och, 2003).
                                                
9 http://www.nactem.ac.uk/y-matsu/geniass/.
10 http://www.statmt.org/moses/.
11 http://www.kyloo.net/software/doku.php/mgiza:overview.
12 http://www.speech.sri.com/projects/srilm/.
235
In the following sub-sections, we describe the
results of baseline systems, which are trained on 
the official corpora. We also present the en-
hanced systems that make use of the web-
crawled bilingual dictionary and monolingual 
data as the additional training resources. Two
variants of enhanced system are constructed 
based on different filtering criteria.
5.1 Baseline System
The baseline systems is constructed based on the 
combination of TM adaptation and LM adapta-
tion, where the corresponding selection thresh-
olds ( ) are manually tuned. Table 3 shows the 
BLEU scores of baseline systems as well as the
threshold values of for general-domain mono-
lingual corpora and parallel corpora selection, 
respectively.
By looking into the results, we find that en-cs 
system performs poorly, because of the limited 
in-domain parallel and monolingual corpora 
(shown in Table 2). While the fr-en and en-fr 
systems achieve the best scores, due the availa-
bility of the high volume training data. We ex-
periment with different values of ={0, 25, 50, 
75, 100} that indicates the percentages of sen-
tences out of the general corpus used for con-
structing the LM adaptation and TM adaptation. 
After tuning the parameter  , we find that
BLEU scores of different systems peak at differ-
ent values of . LM adaptation can achieve the 
best translation results for cs-en, en-fr and de-en 
pairs when  =25, en-cs and en-de pairs when 
 =50, and fr-en pair when  =75. While TM 
adaptation yields the best scores for en-fr and en-
de pairs at  =25 and cs-en and fr-en pairs at 
 =50, de-en pair when =75 and en-cs pair at 
 =100.
Lang. Pair BLEU
Mono.
(M%)
Parallel
(M%)
en-cs 17.57 50% 100%
cs-en 31.29 25% 50%
en-fr 38.36 25% 25%
fr-en 44.36 75% 50%
en-de 18.01 50% 25%
de-en 32.50 25% 75%
Table 3: BLEU scores of baseline systems for 
different language pairs.
5.2 Based on Relevance Score Filtering
As described in Section 2.3, we use the relevance
score to filter out the non-in-domain documents. 
Once again, we evaluate different values of 
Data Set Lang. Sent. Words Vocab. Ave. Len. Sites Docs
In-domain 
Parallel Data
cs/en 1,770,421
9,373,482/
10,605,222
134,998/
156,402
5.29/
5.99
de/en 3,894,099
52,211,730/
58,544,608
1,146,262/
487,850
13.41/
15.03
fr/en 4,579,533
77,866,237/
68,429,649
495,856/
556,587
17.00/
14.94
General-
domain 
Parallel Data
cs/en 12,426,374
180,349,215/
183,841,805
1,614,023/
1,661,830
14.51/
14.79
de/en 4,421,961
106,001,775/
112,294,414
1,912,953/
919,046
23.97/
25.39
fr/en 36,342,530
1,131,027,766/
953,644,980
3,149,336/
3,324,481
31.12/
26.24
In-domain 
Mono. Data
cs 106,548 1,779,677 150,672 16.70
fr 1,424,539 53,839,928 644,484 37.79
de 2,222,502 53,840,304 1,415,202 24.23
en 7,802,610 199430649 1,709,594 25.56
General-
domain 
Mono. Data
cs 33,408,340 567,174,266 3,431,946 16.98
fr 30,850,165 780,965,861 2,142,470 25.31
de 84,633,641 1,548,187,668 10,726,992 18.29
en 85,254,788 2,033,096,800 4,488,816 23.85
Web-crawled 
In-domain 
Mono. Data
en 8,448,566 280,211,580 3,047,758 33.16 26 1,601
cs 44,198 1,280,326 137,179 28.96 4 388
de 473,171 14,087,687 728,652 29.77 17 968
fr 852,036 35,339,445 718,141 41.47 10 683
Table 2: Statistics summary of corpora after pre-processing.
236
 ={0, 25, 50, 75, 100} that represents the per-
centages of crawled documents we used for 
training the LMs. In Table 4, we show the abso-
lute BLEU scores of the evaluated systems, listed 
with the optimized thresholds, and the relative 
improvements (?%) in compared to the baseline 
system. The size of additional training data (for 
LM) is displayed at the last column.
Lang. 
Pair
Docs
( %)
BLEU
? 
(%)
Sent.
en-cs 50 17.59 0.11 31,065 
en-de 75 18.52 2.83 435,547 
en-fr 50 39.08 1.88 743,735 
cs-en 75 32.22 2.97 7,943,931
de-en 25 33.50 3.08 4,951,189
fr-en 100 45.45 2.46 8,448,566
Table 4: Evaluation results for systems that 
trained on relevance-score-filtered documents.
The relevance score filtering approach yields 
an improvement of 3.08% of BLEU score for de-
en pair that is the best result among the language 
pairs. On the other hand, en-cs pair obtains a 
marginal gain. The reason is very obvious that 
the training data is very insufficient. Empirical 
results of all language pairs expect fr-en indicate
that data filtering is the necessity to improve the 
system performance.
5.3 Based on Moore-Lewis Filtering
In this approach, we need to determine the values 
of two parameters, top  documents and top  
sentences, where  ={100, 75, 50} and  ={75, 
50, 25},    . When  =100, it is a conven-
tional perplexity-based data selection method, i.e. 
no document will be filtered. Table 5 shows the 
combination of different  and  that gives the 
best translation score for each language pair. We 
provide the absolute BLEU for each system, to-
gether with relative improvements (?%) that 
compared to the baseline system.
Lang.  
Pair
Docs
( %)
Target 
Size ( %)
BLEU ? (%)
en-cs 50 25 17.69 0.68
en-de 100 50 18.03 0.11
en-fr 100 50 38.73 0.96
cs-en 100 25 32.20 2.91
de-en 100 25 33.10 1.85
fr-en 100 25 45.22 1.94
Table 5: Evaluation results for systems that 
trained on combination filtering approach.
In this shared task, we have a quality and 
quantity in-domain monolingual training data for 
English. All the systems that take English as the 
target translation always outperform the other
reverse pairs. Besides, we found the systems 
based on the perplexity data selection method
tend to achieve a better scores in BLEU.
6 Official Results and Conclusions
We described our study on developing uncon-
strained systems in the medical translation task 
of 2014 Workshop on Statistical Machine Trans-
lation. In this work, we adopt the web crawling 
strategy for acquiring the in-domain monolingual 
data.  In detection the domain data, we exploited 
Moore-Lewis data selection method to filter the 
collected data in addition to the build-in scoring 
model provided by the crawler toolkit. However, 
after investigation, we found that the two meth-
ods are very competitive to each other.
The systems we submitted to the shared task 
were built using the language models and trans-
lation models that yield the best results in the 
individual testing. The official test set is convert-
ed into the recased and detokenized SGML for-
mat. Table 9 presents the official results of our 
submissions for every language pair.
Lang. 
Pair
BLEU of Combined 
systems
Official 
BLEU
en-cs 23.16 (+5.59) 22.10
cs-en 36.8 (+5.51) 37.40
en-fr 40.34 (+1.98) 40.80
fr-en 45.79 (+1.43) 43.80
en-de 19.36 (+1.35) 18.80
de-en 34.17 (+1.67) 32.70
Table 6: BLEU scores of the submitted systems 
for the medical translation task in six language 
pairs.
Acknowledgments
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for their research, 
under the Reference nos. MYRG076 (Y1-L2)-
FST13-WF and MYRG070 (Y1-L2)-FST12-CS.
References 
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 
2011. Domain adaptation via pseudo in-domain da-
ta selection. In Proceedings of EMNLP, pages 355-
362.
237
K. Duh, G. Neubig, K. Sudoh, H. Tsukada. 2013. Ad-
aptation data selection using neural language mod-
els: Experiments in machine translation. In Pro-
ceedings of the Annual Meeting of the Association 
for Computational Linguistics, pages, 678?683.
M. Espl?-Gomis and M. L. Forcada. 2010. Combining 
Content-Based and URL-Based Heuristics toHar-
vest Aligned Bitexts from Multilingual Sites with 
Bitextor. The Prague Bulletin of Mathemathical 
Lingustics, 93:77?86.
Qin Gao and Stephan Vogel. 2008. Parallel Imple-
mentations of Word Alignment Tool. Software En-
gineering, Testing, and Quality Assurance for Nat-
ural Language Processing, pp. 49-57.
Kenneth Heafield. 2011. KenLM: Faster and smaller 
language model queries. In Proceedings of the 
Sixth Workshop on Statistical Machine Translation, 
pages 187-197.
Papineni, Kishore, Salim Roukos, ToddWard, and-
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. In 40th Annu-
al Meeting on Association for Computational Lin-
guistics, ACL ?02, pages 311?318, Philadelphia, 
USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran et al. 
2007. Moses: Open source toolkit for statistical 
machine translation. In Proceedings of ACL, pages
177-180.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine 
translation. In Proceedings of the 2nd ACL Work-
shop on Statistical Machine Translation, pages 
224-227.
Christian Kohlsch?tter, Peter Fankhauser, and Wolf-
gang Nejdl. 2010. Boilerplate detection using shal-
low text features. In Proceedings of the 3rd ACM
International Conference on Web Search and Data
Mining, pages 441-450.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In 
Proceedings of ACL: Short Papers, pages 220-224.
Franz Josef Och. 2003. Minimum error rate training 
in statistical machine translation. Proceedings of 
ACL, pp. 160-167.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment 
models. Computational Linguistics, 29:19-51.
P. Pecina, A. Toral, A. Way, V. Papavassiliou, P. 
Prokopidis, and M. Giagkou. 2011. Towards Using 
WebCrawled Data for Domain Adaptation in Sta-
tistical Machine Translation. In Proceedings of the 
15th Annual Conference of the European Associta-
tion for Machine Translation, pages 297-304.
P. Pecina, A. Toral, V. Papavassiliou, P. Prokopidis, J. 
van Genabith,  and R. I. C. Athena. 2012. Domain 
adaptation of statistical machine translation using 
web-crawled resources: a case study. In Proceed-
ings of the 16th Annual Conference of the Europe-
an Association for Machine Translation, pp. 145-
152.
P. Pecina, O. Du?ek, L. Goeuriot, J. Haji?, J. Hla-
v??ov?, G. J. Jones, and Z. Ure?ov?. 2014. Adapta-
tion of machine translation for multilingual infor-
mation retrieval in the medical domain. Artificial 
intelligence in medicine, pages 1-25.
Philip Resnik and Noah A. Smith. 2003. The Web as 
a parallel corpus. Computational Linguistics, 
29:349?380
Raphael Rubino, Antonio Toral, Santiago Cort?s 
Va?llo, Jun Xie, Xiaofeng Wu, Stephen Doherty, 
and Qun Liu. 2013. The CNGL-DCU-Prompsit 
translation systems for WMT13. In Proceedings of 
the Eighth Workshop on Statistical Machine Trans-
lation, pages 213-218.
S?tre Rune, Kazuhiro Yoshida, Akane Yakushiji, 
Yusuke Miyao, Yuichiro Matsubayashi and Tomo-
ko Ohta. 2007. AKANE System: Protein-Protein 
Interaction Pairs in BioCreAtIvE2 Challenge, PPI-
IPS subtask. In Proceedings of the Second BioCre-
ative Challenge Evaluation Workshop, pp. 209-212.
Andreas Stolcke. 2002. SRILM-an extensible lan-
guage modeling toolkit. Proceedings of the Inter-
national Conference on Spoken Language Pro-
cessing, pp. 901-904.
Antonio Toral. 2013. Hybrid selection of language 
model training data using linguistic information 
and perplexity. In ACL Workshop on Hybrid Ma-
chine Approaches to Translation.
Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi Lu, 
and Junwen Xing. 2014. A Systematic Com-
parison of Data Selection Criteria for SMT Domain 
Adaptation. The Scientific World Journal, vol. 
2014, Article ID 745485, 10 pages.
Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi Lu, 
Junwen Xing. 2013. iCPE: A Hybrid Data Selec-
tion Model for SMT Domain Adaptation. Chinese 
Computational Linguistics and Natural Language 
Processing Based on Naturally Annotated Big Da-
ta. Springer Berlin Heidelberg. pages, 280-290
238
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 254?259,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
Combining Domain Adaptation Approaches for Medical Text Transla-
tion 
 
Longyue Wang, Yi Lu, Derek F. Wong, Lidia S. Chao, Yiming Wang, Francisco Oliveira 
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory, 
Department of Computer and Information Science, 
University of Macau, Macau, China 
vincentwang0229@hotmail.com,  
{mb25435, derekfw, lidiasc, mb25433, olifran}@umac.mo 
 
 
Abstract 
This paper explores a number of simple 
and effective techniques to adapt statisti-
cal machine translation (SMT) systems in 
the medical domain. Comparative exper-
iments are conducted on large corpora for 
six language pairs. We not only compare 
each adapted system with the baseline, 
but also combine them to further improve 
the domain-specific systems. Finally, we 
attend the WMT2014 medical summary 
sentence translation constrained task and 
our systems achieve the best BLEU 
scores for Czech-English, English-
German, French-English language pairs 
and the second best BLEU scores for re-
minding pairs. 
 
1. Introduction 
This paper presents the experiments conducted 
by the NLP2CT Laboratory at the University of 
Macau for WMT2014 medical sentence transla-
tion task on six language pairs: Czech-English 
(cs-en), French-English (fr-en), German-English 
(de-en) and the reverse direction pairs, i.e., en-cs, 
en-fr and en-de.  
By comparing the medical text with common 
text, we discovered some interesting phenomena 
in medical genre. We apply domain-specific 
techniques in data pre-processing, language 
model adaptation, translation model adaptation, 
numeric and hyphenated words translation.  
Compared to the baseline systems (detailed in 
Section 2 & 3), the results of each method show 
reasonable gains. We combine individual ap-
proach to further improve the performance of our 
systems. To validate the robustness and lan-
guage-independency of individual and combined 
systems, we conduct experiments on the official 
training data (detailed in Section 3) in all six lan-
guage pairs. We anticipate the numeric compari-
son (BLEU scores) on these individual and com-
bined domain adaptation approaches that could 
be valuable for others on building a real-life do-
main-specific system. 
The reminder of this paper is organized as fol-
lows. In Section 2, we detail the configurations 
of our experiments as well as the baseline sys-
tems. Section 3 presents the specific pre-
processing for medical data. In Section 4 and 5, 
we describe the language model (LM) and trans-
lation model (TM) adaptation, respectively. Be-
sides, the techniques for numeric and hyphenated 
words translation are reported in Section 6 and 7. 
Finally, the performance of design systems and 
the official results are reported in Section 8. 
2. Experimental Setup 
All available training data from both WMT2014 
standard translation task1 (general-domain data) 
and medical translation task 2  (in-domain data) 
are used in this study. The official medical sum-
mary development sets (dev) are used for tuning 
and evaluating all the comparative systems. The 
official medical summary test sets (test) are only 
used in our final submitted systems.  
The experiments were carried out with the 
Moses 1.03 (Koehn et al., 2007). The translation 
and the re-ordering model utilizes the ?grow-
diag-final? symmetrized word-to-word align-
ments created with MGIZA++4 (Och and Ney, 
                                                 
1 http://www.statmt.org/wmt14/translation-task.html. 
2 http://www.statmt.org/wmt14/medical-task/. 
3 http://www.statmt.org/moses/. 
4 http://www.kyloo.net/software/doku.php/mgiza:overview. 
254
2003; Gao and Vogel, 2008) and the training 
scripts from Moses. A 5-gram LM was trained 
using the SRILM toolkit5 (Stolcke et al., 2002), 
exploiting improved modified Kneser-Ney 
smoothing, and quantizing both probabilities and 
back-off weights. For the log-linear model train-
ing, we take the minimum-error-rate training 
(MERT) method as described in (Och, 2003). 
3. Task Oriented Pre-processing 
A careful pre-processing on training data is sig-
nificant for building a real-life SMT system. In 
addition to the general data preparing steps used 
for constructing the baseline system, we intro-
duce some extra steps to pre-process the training 
data. 
The first step is to remove the duplicate sen-
tences. In data-driven methods, the more fre-
quent a term occurs, the higher probability it bi-
ases. Duplicate data may lead to unpredicted be-
havior during the decoding. Therefore, we keep 
only the distinct sentences in monolingual cor-
pus. By taking into account multiple translations 
in parallel corpus, we remove the duplicate sen-
tence pairs. The second concern in pre-
processing is symbol normalization. Due to the 
nature of medical genre, symbols such as num-
bers and punctuations are commonly-used to pre-
sent chemical formula, measuring unit, terminol-
ogy and expression. Fig. 1 shows the examples 
of this case. These symbols are more frequent in 
medical article than that in the common texts. 
Besides, the punctuations of apostrophe and sin-
gle quotation are interchangeably used in French 
text, e.g. ?l?effet de l'inhibition?. We unify it by 
replacing with the apostrophe. In addition, we 
observe that some monolingual training subsets 
(e.g., Gene Regulation Event Corpus) contain 
sentences of more than 3,000 words in length. To 
avoid the long sentences from harming the true-
case model, we split them into sentences with a 
sentence splitter6 (Rune et al., 2007) that is opti-
mized for biomedical texts. On the other hand, 
we consider the target system is intended for 
summary translation, the sentences tend to be 
short in length. For instance, the average sen-
tence lengths in development sets of cs, fr, de 
and en are around 15, 21, 17 and 18, respective-
ly. We remove sentence pairs which are more 
than 80 words at length. In order to that our ex-
periments are reproducible, we give the detailed 
                                                 
5 http://www.speech.sri.com/projects/srilm/. 
6 http://www.nactem.ac.uk/y-matsu/geniass/. 
statistics of task oriented pre-processed training 
data in Table 2. 
1,25-OH 
47 to 80% 
10-20 ml/kg 
A&E department 
Infective endocarditis (IE) 
Figure 1. Examples of the segments with sym-
bols in medical texts. 
To validate the effectiveness of the pre-
processing, we compare the SMT systems 
trained on original data 7 (Baseline1) and task-
oriented-processed data (Baseline2), respective-
ly. Table 1 shows the results of the baseline sys-
tems. We found all the Baseline2 systems outper-
form the Baseline1 models, showing that the sys-
tems can benefit from using the processed data. 
For cs-en and en-cs pairs, the BLEU scores im-
prove quite a lot. For other language pairs, the 
translation quality improves slightly.  
By analyzing the Baseline2 results (in Table 1) 
and the statistics of training corpora (in Table 2), 
we can further elaborate and explain the results. 
The en-cs system performs poorly, because of 
the short average length of training sentences, as 
well as the limited size of in-domain parallel and 
monolingual corpora. On the other hand, the fr-
en system achieves the best translation score, as 
we have sufficient training data. The translation 
quality of cs-en, en-fr, fr-en and de-en pairs is 
much higher than those in the other pairs. Hence, 
Baseline2 will be used in the subsequent compar-
isons with the proposed systems described in 
Section 4, 5, 6 and 7. 
Lang. Pair Baseline1 Baseline2 Diff. 
en-cs 12.92 17.57 +4.65 
cs-en 20.85 31.29 +10.44 
en-fr 38.31 38.36 +0.05 
fr-en 44.27 44.36 +0.09 
en-de 17.81 18.01 +0.20 
de-en 32.34 32.50 +0.16 
Table 1: BLEU scores of two baseline systems 
trained on original and processed corpora for 
different language pairs. 
4. Language Model Adaptation 
The use of LMs (trained on large data) during 
decoding is aided by more efficient storage and 
inference (Heafield, 2011). Therefore, we not 
                                                 
7 Data are processed according to Moses baseline tutorial: 
http://www.statmt.org/moses/?n=Moses.Baseline. 
255
Data Set Lang. Sent. Words Vocab. Ave. Len. 
In-domain  
Parallel Data 
cs/en 1,770,421 
9,373,482/ 
10,605,222 
134,998/ 
156,402 
5.29/ 
5.99 
de/en 3,894,099 
52,211,730/ 
58,544,608 
1,146,262/ 
487,850 
13.41/ 
15.03 
fr/en 4,579,533 
77,866,237/ 
68,429,649 
495,856/ 
556,587 
17.00/ 
14.94 
General-domain  
Parallel Data 
cs/en 12,426,374 
180,349,215/ 
183,841,805 
1,614,023/ 
1,661,830 
14.51/ 
14.79 
de/en 4,421,961 
106,001,775/ 
112,294,414 
1,912,953/ 
919,046 
23.97/ 
25.39 
fr/en 36,342,530 
1,131,027,766/ 
953,644,980 
3,149,336/ 
3,324,481 
31.12/ 
26.24 
In-domain  
Mono. Data 
cs 106,548 1,779,677 150,672 16.70 
fr 1,424,539 53,839,928 644,484 37.79 
de 2,222,502 53,840,304 1,415,202 24.23 
en 7,802,610 199430649 1,709,594 25.56 
General-domain  
Mono. Data 
cs 33,408,340 567,174,266 3,431,946 16.98 
fr 30,850,165 780,965,861 2,142,470 25.31 
de 84,633,641 1,548,187,668 10,726,992 18.29 
en 85,254,788 2,033,096,800 4,488,816 23.85 
Table 2: Statistics summary of corpora after pre-processing. 
only use the in-domain training data, but also the 
selected pseudo in-domain data 8  from general-
domain corpus to enhance the LMs (Toral, 2013; 
Rubino et al., 2013; Duh et al., 2013). Firstly, 
each sentence s in general-domain monolingual 
corpus is scored using the cross-entropy differ-
ence method in (Moore and Lewis, 2010), which 
is calculated as follows: 
 ( ) ( ) ( )I Gscore s H s H s? ? (1) 
where H(s) is the length-normalized cross-
entropy. I and G are the in-domain and general-
domain corpora, respectively. G is a random sub-
set (same size as the I) of the general-domain 
corpus. Then top N percentages of ranked data 
sentences are selected as a pseudo in-domain 
subset to train an additional LM. Finally, we lin-
early interpolate the additional LM with in-
domain LM.  
We use the top N% of ranked results, where 
N={0, 25, 50, 75, 100} percentages of sentences 
out of the general corpus. Table 3 shows the ab-
solute BLEU points for Baseline2 (N=0), while 
the LM adapted systems are listed with values 
relative to the Baseline2. The results indicate that 
LM adaptation can gain a reasonable improve-
ment if the LMs are trained on more relevant 
data for each pair, instead of using the whole 
training data. For different systems, their BLEU 
                                                 
8 Axelrod et al. (2011) names the selected data as pseudo in-
domain data. We adopt both terminologies in this paper. 
scores peak at different values of N. It gives the 
best results for cs-en, en-fr and de-en pairs when 
N=25, en-cs and en-de pairs when N=50, and fr-
en pair when N=75. Among them, en-cs and en-
fr achieve the highest BLEU scores. The reason 
is that their original monolingual (in-domain) 
data for training the LMs are not sufficient. 
When introducing the extra pseudo in-domain 
data, the systems improve the translation quality 
by around 2 BLEU points. While for cs-en, fr-en 
and de-en pairs, the gains are small. However, it 
can still achieve a significant improvement of 
0.60 up to 1.12 BLEU points. 
Lang. N=0 N=25 N=50 N=75 N=100 
en-cs 17.57 +1.66 +2.08 +1.72 +2.04 
cs-en 31.29 +0.94 +0.60 +0.66 +0.47 
en-fr 38.36 +1.82 +1.66 +1.60 +0.08 
fr-en 44.36 +0.91 +1.09 +1.12 +0.92 
en-de 18.01 +0.57 +1.02 -4.48 -4.54 
de-en 32.50 +0.60 +0.50 +0.56 +0.38 
Table 3: BLEU scores of LM adapted systems. 
5. Translation Model Adaptation 
As shown in Table 2, general-domain parallel 
corpora are around 1 to 7 times larger than the 
in-domain ones. We suspect if general-domain 
corpus is broad enough to cover some in-domain 
sentences. To observe the domain-specificity of 
general-domain corpus, we firstly evaluate sys-
tems trained on general-domain corpora. In Ta-
256
ble 4, we show the BLEU scores of general-
domain systems9 on translating the medical sen-
tences. The BLEU scores of the compared sys-
tems are relative to the Baseline2 and the size of 
the used general-domain corpus is relative to the 
corresponding in-domain one. For en-cs, cs-en, 
en-fr and fr-en pairs, the general-domain parallel 
corpora we used are 6 times larger than the orig-
inal ones and we obtain the improved BLEU 
scores by 1.72 up to 3.96 points. While for en-de 
and de-en pairs, the performance drops sharply 
due to the limited training corpus we used. 
Hence we can draw a conclusion: the general-
domain corpus is able to aid the domain-specific 
translation task if the general-domain data is 
large and broad enough in content.  
Lang. Pair BLEU Diff. Corpus 
en-cs 21.53 +3.96 
+601.89% 
cs-en 33.01 +1.72 
en-fr 41.57 +3.21 
+693.59% 
fr-en 47.33 +2.97 
en-de 16.54 -1.47 
+13.63% 
de-en 27.35 -5.15 
Table 4: The BLEU scores of systems trained on 
general-domain corpora. 
Taking into account the performance of gen-
eral-domain system, we explore various data se-
lection methods to derive the pseudo in-domain 
sentence pairs from general-domain parallel cor-
pus for enhancing the TMs (Wang et al., 2013; 
Wang et al., 2014). Firstly, sentence pair in cor-
responding general-domain corpora is scored by 
the modified Moore-Lewis (Axelrod et al., 
2011), which is calculated as follows: 
 ? ?
g g
( ) ( ) ( )
( ) ( )
I src G src
I t t G t t
score s H s H s
H s H s
? ?
? ?
? ?
? ?? ?? ?
 (2) 
which is similar to Eq. (1) and the only differ-
ence is that it considers the both the source (src) 
and target (tgt) sides of parallel corpora. Then 
top N percentage of ranked sentence pairs are 
selected as a pseudo in-domain subset to train an 
individual translation model. The additional 
model is log-linearly interpolated with the in-
domain model (Baseline2) using the multi-
decoding method described in (Koehn and 
Schroeder, 2007). 
Similar to LM adaptation, we use the top N% 
of ranked results, where N={0, 25, 50, 75, 100} 
percentages of sentences out of the general cor-
                                                 
9  General-domain systems are trained only on genera-
domain training corpora (i.e., parallel, monolingual). 
pus. Table 5 shows the absolute BLEU points for 
Baseline2 (N=0), while for the TM adapted sys-
tems we show the values relative to the Base-
line2. For different systems, their BLEU peak at 
different N. For en-fr and en-de pairs, it gives the 
best translation results at N=25. Regarding cs-en 
and fr-en pairs, the optimal performance is 
peaked at N=50. While the best results for de-en 
and en-cs pairs are N=75 and N=100 respective-
ly. Besides, performance of TM adapted system 
heavily depends on the size and (domain) broad-
ness of the general-domain data. For example, 
the improvements of en-de and de-en systems are 
slight due to the small general-domain corpora. 
While the quality of other systems improve about 
3 BLEU points, because of their large and broad 
general-domain corpora.  
Lang. N=0 N=25 N=50 N=75 N=100 
en-cs 17.57 +0.84 +1.53 +1.74 +2.55 
cs-en 31.29 +2.03 +3.12 +3.12 +2.24 
en-fr 38.36 +3.87 +3.66 +3.53 +2.88 
fr-en 44.36 +1.29 +3.36 +1.84 +1.65 
en-de 18.01 +0.02 -0.13 -0.07 0 
de-en 32.50 -0.12 +0.06 +0.31 +0.24 
Table 5: BLEU scores of TM adapted systems. 
6. Numeric Adaptation 
As stated in Section 3, numeric occurs frequently 
in medical texts. However, numeric expression in 
dates, time, measuring unit, chemical formula are 
often sparse, which may lead to OOV problems 
in phrasal translation and reordering. Replacing 
the sparse numbers with placeholders may pro-
duce more reliable statistics for the MT models.  
Moses has support using placeholders in train-
ing and decoding. Firstly, we replace all the 
numbers in monolingual and parallel training 
corpus with a common symbol (a sample phrase 
is illustrated in Fig. 2). Models are then trained 
on these processed data. We use the XML 
markup translation method for decoding.  
Original: Vitamin D 1,25-OH  
Replaced: Vitamin D @num@, @num@-OH 
Figure 2. Examples of placeholders. 
Table 6 shows the results on this number ad-
aptation approach as well as the improvements 
compared to the Baseline2. The method im-
proves the Baseline2 systems by 0.23 to 0.40 
BLEU scores. Although the scores increase 
slightly, we still believe this adaptation method is 
significant for medical domain. The WMT2014 
medical task only focuses on the summary of 
257
medical text, which may contain fewer chemical 
expression in compared with the full article. As 
the used of numerical instances increases, place-
holder may play a more important role in domain 
adaptation.  
Lang. Pair BLEU (Dev) Diff. 
en-cs 17.80 +0.23 
cs-en 31.52 +0.23 
en-fr 38.72 +0.36 
fr-en 44.69 +0.33 
en-de 18.41 +0.40 
de-en 32.88 +0.38 
Table 6: BLEU scores of numeric adapted sys-
tems. 
7. Hyphenated Word Adaptation 
Medical texts prefer a kind of compound words, 
hyphenated words, which is composed of more 
than one word. For instance, ?slow-growing? and 
?easy-to-use? are composed of words and linked 
with hyphens. These hyphenated words occur 
quite frequently in medical texts. We analyze the 
development sets of cs, fr, en and de respective-
ly, and observe that there are approximately 
3.2%, 11.6%, 12.4% and 19.2% of sentences that 
contain one or more hyphenated words. The high 
ratio of such compound words results in Out-Of-
Vocabulary words (OOV) 10 , and harms the 
phrasal translation and reordering. However, a 
number of those hyphenated words still have 
chance to be translated, although it is not precise-
ly, when they are tokenized into individual 
words.  
Algorithm: Alternative-translation Method 
Input: 
1. A sentence, s, with M hyphenated words 
2. Translation lexicon 
Run: 
1. For i = 1, 2, ?, M 
2.   Split the ith hyphenated word (Ci) into 
Pi 
3.   Translate  Pi into Ti 
4.   If (Ti are not OOVs): 
5.      Put alternative translation Ti in XML 
6.    Else: keep Ci unchanged 
Output: 
Sentence, s?, embedded with alternative 
translations for all Ti. 
End 
Table 7: Alternative-translation algorithm. 
                                                 
10 Default tokenizer does not handle the hyphenated words. 
To resolve this problem, we present an alter-
native-translation method in decoding. Table 7 
shows the proposed algorithm. 
In the implementation, we apply XML markup 
to record the translation (terminology) for each 
compound word. During the decoding, a hyphen-
ated word delimited with markup will be re-
placed with its corresponding translation. Table 8 
shows the BLEU scores of adapted systems ap-
plied to hyphenated translation. This method is 
effective for most language pairs. While the 
translation systems for en-cs and cs-en do not 
benefit from this adaptation, because the hy-
phenated words ratio in the en and cs dev are 
asymmetric. Thus, we only apply this method for 
en-fr, fr-en, de-en and en-de pairs. 
Lang. Pair BLEU (Dev) Diff. 
en-cs 16.84 -0.73 
cs-en 31.23 -0.06 
en-fr 39.12 +0.76 
fr-en 45.02 +0.66 
en-de 18.64 +0.63 
de-en 33.01 +0.51 
Table 8: BLEU scores of hyphenated word 
adapted systems. 
3. Final Results and Conclusions 
According to the performance of each individual 
domain adaptation approach, we combined the 
corresponding models for each language pair. In 
Table 10, we show the BLEU scores and its in-
crements (compared to the Baseline2) of com-
bined systems in the second column. The official 
test set is converted into the recased and deto-
kenized SGML format. The official results of our 
submissions are given in the last column of Table 
9. 
Lang. 
Pair 
BLEU of Com-
bined systems 
Official 
BLEU 
en-cs 23.66 (+6.09) 22.60 
cs-en 38.05 (+6.76) 37.60 
en-fr 42.30 (+3.94) 41.20 
fr-en 48.25 (+3.89) 47.10 
en-de 21.14 (+3.13) 20.90 
de-en 36.03 (+3.53) 35.70 
Table 9: BLEU scores of the submitted systems 
for the medical translation task. 
This paper presents a set of experiments con-
ducted on all available training data for six lan-
guage pairs. We explored various domain adap-
tation approaches for adapting medical transla-
258
tion systems. Compared with other methods, lan-
guage model adaptation and translation model 
adaptation are more effective. Other adapted 
techniques are still necessary and important for 
building a real-life system. Although all individ-
ual methods are not fully additive, combining 
them together can further boost the performance 
of the overall domain-specific system. We be-
lieve these empirical approaches could be valua-
ble for SMT development. 
Acknowledgments 
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for their research, 
under the Reference nos. MYRG076 (Y1-L2)-
FST13-WF and MYRG070 (Y1-L2)-FST12-CS. 
The authors also wish to thank the colleagues in 
CNGL, Dublin City University (DCU) for their 
helpful suggestion and guidance on related work. 
Reference 
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 
2011. Domain adaptation via pseudo in-domain da-
ta selection. In Proceedings of EMNLP, pages 355-
362. 
K. Duh, G. Neubig, K. Sudoh, H. Tsukada. 2013. Ad-
aptation data selection using neural language mod-
els: Experiments in machine translation. In Pro-
ceedings of the Annual Meeting of the Association 
for Computational Linguistics, pages, 678?683. 
Qin Gao and Stephan Vogel. 2008. Parallel imple-
mentations of word alignment tool. Software Engi-
neering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49-57. 
Kenneth Heafield. 2011. KenLM: Faster and smaller 
language model queries. In Proceedings of the 
Sixth Workshop on Statistical Machine Transla-
tion, pages 187-197. 
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine 
translation. In Proceedings of the 2nd ACL Work-
shop on Statistical Machine Translation, pages 
224-227. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Ber-
toldi, Brooke Cowan, Wade Shen, Christine Moran 
et al. 2007. Moses: open source toolkit for statisti-
cal machine translation. In Proceedings of ACL, 
pages 177-180. 
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In 
Proceedings of ACL: Short Papers, pages 220-224. 
S?tre Rune, Kazuhiro Yoshida, Akane Yakushiji, 
Yusuke Miyao, Yuichiro Matsubayashi and Tomo-
ko Ohta. 2007. AKANE system: protein-protein in-
teraction pairs in BioCreAtIvE2 challenge, PPI-IPS 
subtask. In Proceedings of the Second BioCreative 
Challenge Evaluation Workshop, pages 209-212.  
Raphael Rubino, Antonio Toral, Santiago Cort?s 
Va?llo, Jun Xie, Xiaofeng Wu, Stephen Doherty, 
and Qun Liu. 2013. The CNGL-DCU-Prompsit 
translation systems for WMT13. In Proceedings of 
the Eighth Workshop on Statistical Machine Trans-
lation, pages 213-218. 
Andreas Stolcke and others. 2002. SRILM-An exten-
sible language modeling toolkit. In Proceedings of 
the International Conference on Spoken Language 
Processing, pages 901-904. 
Antonio Toral. 2013. Hybrid selection of language 
model training data using linguistic information 
and perplexity. In ACL Workshop on Hybrid Ma-
chine Approaches to Translation. 
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment 
models. Computational Linguistics, 29:19-51. 
Franz Josef Och. 2003. Minimum error rate training 
in statistical machine translation. In Proceedings of 
the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 160-167. 
Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi 
Lu, and Junwen Xing. 2014 ?A Systematic Com-
parison of Data Selection Criteria for SMT Domain 
Adaptation,? The Scientific World Journal, vol. 
2014, Article ID 745485, 10 pages. 
Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi 
Lu, Junwen Xing. 2013. iCPE: A Hybrid Data Se-
lection Model for SMT Domain Adaptation. Chi-
nese Computational Linguistics and Natural Lan-
guage Processing Based on Naturally Annotated 
Big Data. Springer Berlin Heidelberg. pages, 280-
290. 
 
259
