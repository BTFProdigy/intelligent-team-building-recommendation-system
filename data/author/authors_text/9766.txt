Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 185?192
Manchester, August 2008
Looking for Trouble
Stijn De Saeger Kentaro Torisawa
Language Infrastructure Group
National Institute of Information
and Communications Technology
{stijn,torisawa}@nict.go.jp
Jun?ichi Kazama
School of Information Science
Japan Advanced Institute
of Science and Technology
kazama@jaist.ac.jp
Abstract
This paper presents a method for mining
potential troubles or obstacles related to
the use of a given object. Some exam-
ple instances of this relation are ?medicine,
side effect? and ?amusement park, height
restriction?. Our acquisition method con-
sists of three steps. First, we use an un-
supervised method to collect training sam-
ples from Web documents. Second, a set
of expressions generally referring to trou-
bles is acquired by a supervised learning
method. Finally, the acquired troubles
are associated with objects so that each
of the resulting pairs consists of an ob-
ject and a trouble or obstacle in using that
object. To show the effectiveness of our
method we conducted experiments using
a large collection of Japanese Web doc-
uments for acquisition. Experimental re-
sults show an 85.5% precision for the top
10,000 acquired troubles, and a 74% pre-
cision for the top 10% of over 60,000 ac-
quired object-trouble pairs.
1 Introduction
The Stanford Encyclopedia of Philosophy defines
an artifact as ?. . . an object that has been inten-
tionally made or produced for a certain purpose?.
Because of this purpose-orientedness, most human
actions relating to an object or artifact fall into
two broad categories ? actions relating to its in-
tended use (e.g. reading a book), and the prepa-
rations necessary therefore (like buying the book).
Information concerning potential obstacles, harm-
ful effects or troubles that interfere with this in-
tended use is therefore highly relevant to the user.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
While some such troubles are self-evident, others
represent a genuine obstacle whose existence was
thusfar unknown to the user. For example, in early
2008 a food poisoning case caused a big media stir
in Japan when dozens of people fell ill after eating
Chinese-imported frozen food products containing
residual traces of toxic pesticides. While suppos-
edly the presence of toxic chemicals in imported
frozen foods had already been established on sev-
eral occasions before, until the recent incidents
public awareness of these facts remained low. In
retrospect, a publicly available system suggesting
?residual agrichemicals? as a potential danger with
the consumption of ?frozen foods? based on in-
formation mined from a large collection of Web
documents might have led to earlier detection of
this crisis. From the viewpoint of manufacturers
as well, regularly monitoring the Internet for prod-
uct names and associated troubles may allow them
to find out about perceived flaws in their products
sooner and avoid large scale recalls and damage to
their brand.
For a less dramatic example, searching for
?Tokyo Disneyland? on the Internet typically
yields many commercial sites offering travel deals,
but little or no information about potential ob-
stacles such as ?height restrictions? (constraints
on who can enjoy a given attraction
1
) and ?traf-
fic jams? (a necessary preparation for enjoying a
theme park is actually getting there in time). Ofter
users have no way of finding out about this until
they actually go there.
These examples demonstrate the importance of
a highly accurate automatic method for acquir-
ing what we will call ?object-trouble? relations ?
pairs ?e
o
, e
t
? in which the thing referred to by e
t
constitutes an (actual or potential) trouble, obsta-
cle or risk in the context of use of an object e
o
.
1
For example, one has to be over 3 ft. tall to get on the
Splash Mountain.
185
Large scale acquisition of this type of contextual
knowledge has not been thoroughly studied so far.
In this paper, we propose a method for automati-
cally acquiring Japanese noun phrases referring to
troubles, (henceforth referred to as trouble expres-
sions), and associating them with expressions de-
noting artifacts, objects or facilities.
Our acquisition method consists of three steps.
As a first step, we use an unsupervised method for
efficiently collecting training data from a Web cor-
pus. Then, a set of expressions denoting troubles is
acquired by a supervised learning method ? Sup-
port Vector Machines (Vapnik, 1998) ? trained on
this data. Finally, the acquired trouble expressions
are paired with noun phrases referring to objects,
using a combination of pairwise mutual informa-
tion and a verb-noun dependency filter based on
statistics in a Web corpus.
A broad focus on noun-verb dependencies ?
and in particular the distinction between depen-
dency relations with negated versus non-negated
verbs ? is the main characteristic of our method.
While this distinction did not prove useful for im-
proving the supervised classifier?s performance in
step 2, it forms the basis underlying the unsuper-
vised method for training sample selection in the
first step, and the final filtering mechanism in the
third step.
The rest of this paper is organized as follows.
Section 2 points out related work. Section 3 ex-
amines the notion of trouble expressions and their
evidences. Section 4 describes our method, whose
experimental results are discussed in Section 5.
2 Related Work
Our goal of automatically acquiring object-trouble
pairs from Web documents is perhaps best viewed
as a problem of semantic relation extraction. Re-
cently the Automatic Content Extraction (ACE)
program (Doddington et al, 2004) is a well-
known benchmark task concerned with the au-
tomatic recognition of semantic relations from
unstructured text. Typical target relations in-
clude ?Reaction? and ?Production? (Pantel and
Pennacchiootti, 2006), ?person-affiliation? and
?organization-location? (Zelenko et al, 2002),
?part-whole? (Berland and Charniak, 1999; Girju
et al, 2006) and temporal precedence relations be-
tween events (Chklovski and Pantel, 2004; Tori-
sawa, 2006). Our current task of acquiring ?object-
trouble? relations is new and object-trouble rela-
tions are inherently more abstract and indirect than
relations like ?person-affiliation? ? they crucially
depend on additional knowledge about whether
and how a given object?s use might be hampered
by a specific trouble.
Another line of research closely related to our
work is the recognition of semantic orientation and
sentiment analysis (Turney, 2002; Takamura et al,
2006; Kaji and Kitsuregawa, 2006). Clearly trou-
bles should be associated with a negative orien-
tation of an expression, but studies on the acqui-
sition of semantic orientation traditionally do not
bother with the context of evaluation. While re-
cent work on sentiment analysis has started to as-
sociate sentiment-related attribute-evaluation pairs
to objects (Kobayashi et al, 2007), these attributes
usually concern intrinsic properties of the objects,
such as a digital camera?s colors ? they do not
extend to sentiment-related factors external to the
object like ?traffic jams? for theme parks. The ac-
quisition method proposed in this work addresses
both these matters.
Finally, our task of acquiring trouble expres-
sions can be regarded as hyponymy acquisition,
where target expressions are hyponyms of the
word ?trouble?. Although we used the classical
lexico-syntactic patterns for hyponymy acquisition
(Hearst, 1992; Imasumi, 2001; Ando et al, 2003)
to reflect this intuition, our experiments show we
were unable to attain satisfactory performance us-
ing lexico-syntactic patterns alone. Thus, we also
use verb-noun dependencies as evidence in learn-
ing (Pantel and Ravichandran, 2004; Shinzato and
Torisawa, 2004). We treat the evidences uniformly
as elements in a feature vector given to a super-
vised learning method, which allowed us to ex-
tract a considerably larger number of trouble ex-
pressions than could be acquired by sparse lexico-
syntactic patterns alone, while still keeping decent
precision. What kind of hyponymy relations can
be acquired by noun-verb dependencies is still an
open question in NLP. In this work we show that
at least trouble expressions can successfully be ac-
quired based on noun-verb dependency informa-
tion alone.
3 Trouble Expressions and Features for
Their Acquisition
In section 1 we have characterized trouble expres-
sions as a kind of ?trouble? that occurs in the spe-
cific context of using some object, in other words:
186
1. hyponym ni nita hypernym
(hyponym similar to hypernym)
2. hyponym to yobareru hypernym
(hypernym called hyponym)
3. hyponym igai no hypernym
(hypernym other than hyponym)
4. hyponym no youna hypernym
(hypernym like hyponym)
5. hyponym to iu hypernym
(hypernym called hyponym)
6. hyponym nado(no|,) hypernym
(hypernym such as hyponym)
Table 1: Japanese lexico-syntactic patterns for hy-
ponymy relations
as hyponyms of ?trouble?. Hence one source of ev-
idence for acquisition are hyponymy relations with
?trouble? or its synonyms. Another characteriza-
tion of trouble expressions is to think of them as
obstacles in a broad sense: things that prevent cer-
tain actions from being undertaken properly. In
this sense traffic jams and sickness are troubles
since they prevent people from going places and
doing things. This assumption underlies a second
important class of evidences for learning.
More precisely, the evidence used for learning is
classified into three categories: (i) lexico-syntactic
patterns for hyponymy relations, (ii) dependency
relations between expressions and negated verbs,
and (iii) dependency relations between expres-
sions and non-negated verbs. The first two cat-
egories are assumed to contain positive evidence
of trouble expressions, while we assumed the third
to function mostly as negative evidence. Our ex-
periments show that (i) turns out to be less useful
than expected, while the combination of (ii) and
(iii) alone already gave quite reasonable precision
in acquiring trouble expressions. Each category of
evidence is described further below.
3.1 Lexico-syntactic patterns for hyponymy
Since trouble expressions are hyponyms of ?trou-
ble?, one obvious way of acquiring trouble expres-
sions is to use classical lexico-syntactic patterns
for hyponymy acquisition (Hearst, 1992). Table
1 lists some of the patterns proposed in studies
on hyponymy acquisition for Japanese (Ando et
al., 2003; Imasumi, 2001) that are utilized in this
work.
In actual acquisition, we instantiated the hy-
pernym positions in the patterns by Japanese
translations of ?trouble? and its synonyms,
namely toraburu (troubles), sainan (acci-
dents), saigai (disasters) and shougai (obsta-
cles or handicaps), and used the instantiated pat-
terns as evidence. Hereafter, we call these pat-
terns LSPHs (Lexico-Syntactic Patterns for Hy-
ponymy).
3.2 Dependency relations with Verbs
We expect expressions that frequently refer to
troubles to have a distinct dependency profile, by
which we mean a specific set of dependency rela-
tions with verbs (i.e. occurrences in specific argu-
ment positions). If T is a trouble expression, then
given a sufficiently large corpus one would expect
to find a reasonable number of instantiations of
patterns like the following:
? T kept X from doing Y .
? X didn?t enjoy Y because of T .
Similarly, ?X enjoyed T ? would present neg-
ative evidence for T being a trouble expression.
Rather than single out a set of particular depen-
dency relations suspected to be indicative of trou-
ble expressions, we let a supervised classifier learn
an appropriate weight for each feature in a large
vector of dependency relations. Two classes of de-
pendency relations proved to be especially benefi-
cial in determining trouble candidates in an unsu-
pervised manner, so we discuss them in more detail
below.
Dependency relations with negated verbs Fol-
lowing our characterization of troubles as things
that prevent specific actions from taking place, we
expect a good deal of trouble expressions to appear
in patterns like the following.
? X cannot go to Y because of T .
? X did not enjoy Y because of T .
The important points in the above are (i) the
negated verbs and (ii) the mention of T as the rea-
son for not verb-ing. The following are Japanese
translations of the above patterns. Here P denotes
postpositions (Japanese case markers), V stands
for verbs and the phrase ?because of? is translated
as the postposition de.
?
T de Y ni ikenai.
P P V (cannot go)
?
T de X ga tanoshikunakatta.
P P V (did not enjoy)
187
We refer to the following dependency relations
between expressions marked with the postposition
de and negated verbs in these patterns as DNVs
(Dependencies to Negated Verbs).
T de ? negated verb (1)
We allow any verb to be the negated verb, expect-
ing that inappropriate verbs will be less weighted
by machine learning techniques. For instance,
the dependency relations to negated verbs with an
originally negative orientation such as ?suffer? and
?die? will not work as positive examples for trou-
ble expressions.
Unfortunately, these patterns still present only
weak evidence for trouble expressions. The pre-
cision of the trouble expressions collected using
DNV patterns is extremely low ? around 6.5%.
This is due to the postposition de?s ambiguity ?
besides ?because of? relations it also functions as a
marker for location, time and instrument relations,
among others. As a result, non-trouble expressions
such as ?by car? (instrument) and ?in Tokyo? (lo-
cation) are marked by the postposition de as well.
We consider a second class of dependency rela-
tions, acting mostly as a counter to the noisy ex-
pressions introduced by the ambiguity of the post-
position de.
Dependency relations with non-negated verbs
The final type of evidence is formulated as the fol-
lowing dependency relation.
T de ? non-negated verb
We call this type of relation DAVs (Dependen-
cies to Affirmative Verbs). The use of these pat-
terns is motivated by the intuition that noisy ex-
pressions found with DNVs, such as expressions
about locations or instruments, will also frequently
appear with non-negated verbs. That is, if you ob-
serve ?cannot go to Y (by / because of) X? and X
is not a trouble expression, then you can expect to
find ?can go to Y (by / because of) X? as well.
Our initial expectation was that the DNV and
DAV evidences observed with the postposition de
alone would contain sufficient information to ob-
tain an accurate classifier, but this expectation was
not borne out by our early experiments. As it
turns out, using dependency relations to verbs in
all argument positions as features to the SVM re-
sulted roughly in a 10?15% increase in precision.
Therefore in our final experiments we let the DNV
and DAV evidence consist of dependencies with
four additional postpositions (ha, ga, wo and ni),
which are used to indicate topicalization, subject,
object and indirect object. We found that the SVM
was quite successful in learning a dependency pro-
file for trouble expressions based on this informa-
tion.
Nonetheless, the DNV/DAV patterns proved to
be useful besides as evidence for supervised learn-
ing, for instance in gathering sufficient trouble can-
didates and sample selection when preparing train-
ing data
2
.
4 Method
As mentioned, our method for finding troubles
in using some objects consists of three steps, de-
scribed in more detail below.
Step 1 Gather training data with a sufficient
amount of positive samples using an unsuper-
vised method to reduce the workload of man-
ual annotation.
Step 2 Collect expressions commonly perceived
as troubles by using the evidences described
in the previous section.
Step 3 Identify pairs of trouble expressions and
objects such that the trouble expressions rep-
resent an obstacle in using the objects.
4.1 Step 1: Gathering Training Data
We considered noun phrases observed with the
LSPH and DNV evidences as candidate trouble ex-
pressions. However, we still found only 7% of
the samples observed with these evidences to be
real troubles. Because of the diversity of our ev-
idences (dependencies with verbs) we need a rea-
sonable amount of positive samples in order to ob-
tain an accurate classifier. Without some sample
selection scheme, we would have to manually an-
notate about 8000 samples in order to obtain only
560 positive samples in the training data. For this
reason we used the following scoring function as
an unsupervised method for sample selection.
Score(e) =
f
LSPH
(e) + f
DNV
(e)
f
LSPH
(e) + f
DNV
(e) + f
DAV
(e)
(2)
Here, f
LSPH
(e), f
DNV
(e) and f
DAV
(e) are the fre-
quencies that expression e appears with the re-
spective evidences. Intuitively, this function gives
2
We will discuss yet another use of the DNV evidence in
step 2 of our acquisition method.
188
a large score to expressions that occur frequently
with the positive evidences for trouble expressions
(LSPHs and DNVs), or those that appear rarely
with the negative evidences (DAVs). In prepar-
ing training data we ranked all candidates accord-
ing to the above score, and annotated N elements
from the top and bottom of the ranking as train-
ing data. In our experiments, the top elements in-
cluded a reasonable number of positive samples
(25.8%) while there were almost none in the worst
elements.
4.2 Step 2: Finding Trouble Expressions
In this step our aim is to acquire expressions of-
ten associated with troubles. We use a super-
vised classifier, namely Support Vector Machines
(SVMs) (Vapnik, 1998) for distinguishing trou-
bles from non-troubles, based on the evidences de-
scribed above. Each dimension of the feature vec-
tor presented to the SVM corresponds to the obser-
vation of a particular evidence (i.e., these are bi-
nary features). We tried using frequencies instead
of binary feature values but could not find any sig-
nificant improvement in performance. After learn-
ing we sort the candidate trouble expressions ac-
cording to their distance to the hyperplane learned
by the SVM, and consider the top N expressions
in the sorted list as true trouble expressions.
4.3 Step 2: Identifying Object-Trouble Pairs
In this third stage we rank possible combinations
of objects and trouble expressions acquired in the
previous step according to their degree of associ-
ation and apply a filter using negated verbs to the
top pairs in the ranking. The final output of our
method is the top N pairs that survived the filter-
ing. We describe each step below.
Generating Object-Trouble Pairs To generate
and rank object-trouble pairs we use a variant of
pairwise mutual information that scores an object-
trouble pair ?e
o
, e
t
? based on the observed fre-
quency of the following pattern.
e
o
no e
t
P
(3)
The postposition no is a genitive case marker, and
the whole pattern can be translated as ?e
t
of / in
e
o
?. We assume that appearance of expression e
t
in this pattern refers to a trouble in using the object
e
o
.
More precisely, we generate all possible combi-
nations of trouble expression and objects and rank
them according to the following score.
I(e
o
, e
t
) =
f(?e
o
no e
t
?)
f(?e
o
?)f(?e
t
?)
(4)
where f(e) denotes an expression e?s frequency.
This score is large when the pattern ?e
o
no e
t
?
is observed more frequently than can be expected
from e
o
and e
t
?s individual frequencies. Frequency
data for all noun phrases was precomputed for the
whole Web corpus.
Filtering Object-Trouble Pairs The filtering in
the second step is based on the following assump-
tion.
Assumption If a trouble expression e
t
refers to a
trouble in using an object e
o
, there is a verb v
such that v frequently co-occurs with e
o
and
v has the following dependency relation with
e
t
.
e
t
de ? negated v
The intuition behind this assumption can be ex-
plained as follows. First, if e
o
denotes an object or
artifact then its frequently co-occurring verbs are
likely to be related to a use of e
o
. Second, if e
t
is a
trouble in using e
o
, there is some action associated
with e
o
that e
t
prevents or hinders, implying that e
t
should be observed with its negation. For instance,
if ?traffic jam? is a trouble in using an amusement
park, then we can expect the following pattern to
appear also in a corpus.
?
juutai de yuuenchi ni ikenai.
traffic jam P theme park P V (cannot go)
cannot go to a theme park because of a traffic jam
The verb ?to go? co-occurs often with the noun
?theme park? and the above pattern contains the
dependency relation ?traffic jam de? cannot go?.
Substituting v in the hypothesis for ?to go?, the as-
sumption becomes valid. Because of data sparse-
ness the above pattern may not actually appear in
the corpus, but even so the dependency relation
?traffic jam de ? cannot go? may be observed
with other facilities, and thus making the assump-
tion hold anyway.
As a final filtering procedure, we gathered K
verbs most frequently co-occurring with each ob-
ject and checked if the trouble expression in the
pair has dependency relations with the K verbs in
189
negated form and the postposition de. If none of
the K verbs has such a dependency with the trou-
ble expression, the pair is discarded. Otherwise, it
is produced as the final output of our method.
5 Experimental Results
5.1 Finding Trouble Expressions
We extracted noun phrases observed in LSPH,
DNV and DAV patterns from 6? 10
9
sentences in
about 10
8
crawled Japanese Web documents, and
used the LSPH and DNV data
3
as candidate trou-
ble expressions. After restricting the noun phrases
to those observed more than 10 times in the evi-
dences, we had 136,212 noun phrases. We denote
this set asD. Extracting 200 random samples from
D we found the ratio of troubles to non-troubles
was around 7% and thus expected to find about
10, 000 real trouble expressions in D.
4
Using the sample selection method described in
Section 4.2 we prepared 6,500 annotated samples
taken from D as training data. The top 3,500 sam-
ples included 912 positive samples and the worst
3,000 had just 9 positives, thereby confirming the
effectiveness of the scoring function for selecting
a reasonable amount of positive samples. Our final
training data thus contained 14% positives.
For the feature vectors we included dependen-
cies with all verbs occurring more than 30 times
in our Web corpus. Besides the LSPH, DNV and
DAV evidences discussed previously, we also in-
cluded 10 additional binary features indicating for
each of the five postpositions whether the expres-
sion was observed with DNV or DAV evidence at
all, and found that including this information im-
proved performance.
We trained a classifier with a polynomial kernel
of degree 1 on these evidences using the software
TinySVM
5
, and evaluated the results obtained by
the supervised acquisition method by asking three
human raters whether a randomly selected sample
expression denotes a kind of ?trouble? in general
situations. More specifically, we asked whether
the expression is a kind of toraburu (trou-
ble), sainan (accident), saigai (disaster) or
shougai (obstacle or handicap).
6
For various
3
We restricted noun phrases from the DNV data to those
found with the postposition de, as these are most likely to
refer to troubles.
4
Thus, in the experiments we evaluated the top 10,000
samples output by our method.
5
http://chasen.org/?taku/software/TinySVM/
6
Actually one of the raters is a co-author of this paper, but
 0
 20
 40
 60
 80
 100
 0  20  40  60  80  100
Pr
ec
isio
n (%
)
Number of Samples (%)
random
LSPH
Score
full
w/o LSPH
w/o DAV
w/o DNV
w/o sum DAV/DNV
Figure 1: Performance of trouble expression ac-
quisition (all 3 raters)
combinations of evidences (described below), we
presented 200 randomly sampled expressions from
the top 10,000 expressions ranked according to the
distance to the hyperplane learned by the SVM.
Samples of all the compared methods are merged
and shuffled before evaluation. The kappa statistic
for assessing the inter-rater agreement was 0.78,
indicating substantial agreement according to Lan-
dis and Koch, 1977.
7
We made no effort to remove
samples used in training from the experiment, and
found that the samples scored by the raters (1281
in total, after removal of duplicates) contained 67
training samples. The 200 samples from the ?full?
classifier contained 12 of these.
Fig. 1 shows the precision of the acquired trou-
ble expressions compared to the samples labeled
as troubles by all three raters. We sorted the sam-
ples according to their distance to the SVM hyper-
plane and plotted the precision of the top N sam-
ples. The best overall precision (85.5%) was ob-
tained by a classifier trained on the full combina-
tion of evidences (labeled ?full? in Fig. 1), main-
taining over 90% precision for the top 70% of the
200 samples.
The remaining results show the relative con-
tributions of the evidences. They were obtained
by retraining the ?full? classifier with a particu-
lar set of evidences removed, respectively LSPH
evidences (labeled ?w/o LSPH?), DNV evidences
(?w/o DNV?), DAV evidences (?w/o DAV?) and
the 10 features indicating the observation of
he had no knowledge of the experimental setting nor had seen
the acquired data prior to the experiment.
7
This kappa value was calculated over the sum total of
samples presented to the raters for scoring (duplicates re-
moved).
190
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100
Pr
ec
isio
n (%
)
Number of Samples (%)
random
top 10% MI
top 50% MI
top 10% proposed
top 50% proposed
Figure 2: Performance of object-trouble pair ac-
quisition (3 raters)
DAV/DNV evidence per postposition (?w/o sum
DAV/DNV?).
As Fig. 1 shows, leaving out DNV and even
LSPH evidences did not affect performance as
much as we expected, while leaving out the DAV
dependencies gave more than 20% worse results.
Of further interest is the importance of the binary
features for DAV/DNV presence per postposition
(?w/o sum DAV/DNV?). The absence of these 10
binary features accounts for a 10% precision loss
compared to the full feature set (75%).
We also compared it with a baseline method us-
ing only lexico-syntactic patterns. We extracted
100 random noun phrases from the LSPH evidence
in D for evaluation (?LSPH? in Fig. 1). The pre-
cision for this method was 31%, confirming that
lexico-syntactic patterns for hyponymy constitute
fairly weak evidence for predicting trouble expres-
sions when used alone. ?Score? shows the preci-
sion of the top 100 samples output by our Score
function from section 4. Finally, ?random? (drawn
as a straight line) denotes 100 random samples
from D and roughly corresponds to our estimate
of 7% true positives.
5.2 Identifying Object-Trouble Pairs
For the second step, we assumed the top 10,000 ex-
pressions obtained by our best-scoring supervised
learning method (?full? in the previous experi-
ments) to be trouble expressions, and proceeded
to combine them with terms denoting artifacts or
facilities.
We randomly picked 2,500 words that ap-
peared as direct objects of the verbs kau (?to
buy?), tsukau (?to use?), tsukuru (?to make?),
taberu (?to eat?) and tanoshimu (?to enjoy?)
rank
/raters object trouble expressions
1/3 kousoku douro sakeyoi unten
(highway) (drunk driving)
7/3 kouseibushitsu ranyou
(antibiotics) (abuse)
8/3 suidousui suiatsu teika
(tap water) (drop in water pressure)
21/3 nouyaku zanryuubushitsu
(agrichemicals) (residue)
98/2 kikai gohandan
(machine) (judgement error)
136/3 zaisan souzoku funsou
(estate) (succession dispute)
Figure 3: Examples of acquired object-trouble
pairs
more than 500 times in our Web corpus, assuming
that this would yield a representative set of noun
phrases denoting objects or artifacts.
8
Combining
this set of objects with the acquired trouble expres-
sions gave a list of 61,873 object-trouble pairs (all
pairs ?e
o
, e
t
? with at least one occurrence of the
pattern ?e
o
no e
t
?). Of this list, 58,570 pairs sur-
vived the DNV filtering step and form the final out-
put of our method. For the DNV filtering, we used
the top 30 verbs most frequently co-occurring with
the object.
We again evaluated the resulting object-trouble
pairs by asking three human raters whether the pre-
sented pairs consist of an object and an expression
referring to an actual or potential trouble in using
the object. The kappa statistic was 0.60, indicating
moderate inter-rater agreement.
Fig. 2 shows the precision of the acquired pairs
when comparing with what are considered true
object-trouble relations by all three raters. Some
examples of the pairs obtained by our method are
listed in table 3 along with their ranking and the
number of raters who judged the pair to be correct.
The precision for our proposed method when
considering the top 10% of pairs ranked by the I
score and filtered by the method described in sec-
tion 4.3 is 71.5% (?top 10% proposed? in Fig. 2),
which is actually worse than the results obtained
without the final DNV filtering (?top 10% MI?,
74%). For the first half of all samples however, we
do observe some performance increase by the fil-
tering, though both methods appear to converge in
the second half of the graph. This tendency is mir-
rored closely when considering the results for the
top 50% of all pairs (respectively ?top 50% pro-
posed? and ?top 50% MI? in Fig. 2). The 15%
decrease in precision compared to top 10% results
8
We manually removed pronouns from this set.
191
indicates that performance drops gradually when
moving to the lower ranked pairs.
6 Concluding Remarks and Future Work
We have presented an automatic method for find-
ing potential troubles in using objects, mainly ar-
tifacts and facilities. Our method acquired 10,000
trouble expressions with 85.5% precision, and over
6000 pairs of objects and trouble expressions with
74% precision.
Currently, we are developing an Internet search
engine frontend that issues warnings about poten-
tial troubles related to search keywords. Although
we were able to acquire object-trouble pairs with
reasonable precision, we plan to make a large-scale
highly precise list of troubles by manually check-
ing the output of our method. We expect such a list
to lead to even more acurate object-trouble pair ac-
quisition.
References
Ando, M., S. Sekine, and S. Ishizaki. 2003. Automatic
extraction of hyponyms from newspaper using lexi-
cosyntactic patterns. In IPSJ SIG Technical Report
2003-NL-157, pages 77?82. in Japanese.
Berland, M. and E. Charniak. 1999. Finding parts in
very large corpora. In Proc. of ACL-1999, pages 57?
64.
Chklovski, T. and P. Pantel. 2004. Verbocean: Mining
the web for fine-grained semantic verb relations. In
Proc. of EMNLP-04.
Doddington, G., A. Mitchell, M. Przybocki,
L. Ramshaw, S. Strassel, and R. Weischedel.
2004. The Automatic Content Extraction (ACE)
Program?Tasks, Data, and Evaluation. Proceedings
of LREC 2004, pages 837?840.
Girju, R., A. Badulescu, and D. Moldvan. 2006. Au-
tomatic discovery of part-whole relations. Computa-
tional Linguistics, 32(1):83?135.
Hearst, M. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of COLING?92,
pages 539?545.
Imasumi, K. 2001. Automatic acqusition of hyponymy
relations from coordinated noun phrases and apposi-
tions. Master?s thesis, Kyushu Institute of Technol-
ogy.
Kaji, N. and M. Kitsuregawa. 2006. Automatic con-
struction of polarity-tagged corpus from html docu-
ments. In Proc. of COLING/ACL 2006, pages 452?
459. (poster session).
Kobayashi, N., K. Inui, and Y. Matsumoto. 2007. Ex-
tracting aspect-evaluation and aspect-of relations in
opinion mining. In Proc. of EMNLP-CoNLL 2007,
pages 1065?1074.
Pantel, P. and M. Pennacchiootti. 2006. Espresso:
Leveranging generic patterns for automatically
harvesting semantic relations. In Proc. of
COLING/ACL-06, pages 113?120.
Pantel, P. and D. Ravichandran. 2004. Automatically
labelling semantic classes. In Proc. of HLT/NAACL-
04, pages 321?328.
Shinzato, K. and K. Torisawa. 2004. Acquir-
ing hyponymy relations from web documents. In
HLT/NAACL-04, pages 73?80.
Takamura, H., T. Inui, and M. Okumura. 2006. Latent
variable models for semantic orientation of phrases.
In Proc. of EACL 2006, pages 201?208.
Torisawa, K. 2006. Acquiring inference rules with
temporal constraints by using japanese coordinated
sentences and noun-verb co-occurrences. In Moore,
R.C., J.A. Bilmes, J. Chu-Carroll, and M. Sanderson,
editors, HLT-NAACL. The Association for Computa-
tional Linguistics.
Turney, P. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification
of reviews. In Proc. of ACL?02, pages 417?424.
Vapnik, Vladimir N. 1998. Statistical Learning The-
ory. Wiley-Interscience.
Zelenko, D., C. Aone, and A. Richardella. 2002. Ker-
nel methods for relation extraction. In EMNLP ?02:
Proceedings of the ACL-02 conference on Empirical
methods in natural language processing, pages 71?
78, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
192
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 929?937,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Hypernym Discovery Based on Distributional Similarity                     
and Hierarchical Structures 
Ichiro Yamada?, Kentaro Torisawa?, Jun?ichi Kazama?, Kow Kuroda?,  
Masaki Murata?, Stijn De Saeger?, Francis Bond? and Asuka Sumida? 
 
?National Institute of Information and Communications Technology 
3-5 Hikaridai, Keihannna Science City 619-0289, JAPAN 
{iyamada,torisawa,kazama,kuroda,murata,stijn,bond}@nict.go.jp
?Japan Advanced Institute of Science and Technology 
1-1 Asahidai, Nomi-shi, Ishikawa-ken 923-1211, JAPAN 
a-sumida@jaist.ac.jp 
 
Abstract 
This paper presents a new method of devel-
oping a large-scale hyponymy relation data-
base by combining Wikipedia and other Web 
documents. We attach new words to the hy-
ponymy database extracted from Wikipedia 
by using distributional similarity calculated 
from documents on the Web. For a given tar-
get word, our algorithm first finds k similar 
words from the Wikipedia database. Then, 
the hypernyms of these k similar words are 
assigned scores by considering the distribu-
tional similarities and hierarchical distances 
in the Wikipedia database. Finally, new hy-
ponymy relations are output according to the 
scores. In this paper, we tested two distribu-
tional similarities. One is based on raw verb-
noun dependencies (which we call ?RVD?), 
and the other is based on a large-scale clus-
tering of verb-noun dependencies (called 
?CVD?). Our method achieved an attachment 
accuracy of 91.0% for the top 10,000 rela-
tions, and an attachment accuracy of 74.5% 
for the top 100,000 relations when using 
CVD. This was a far better outcome com-
pared to the other baseline approaches. Ex-
cluding the region that had very high scores, 
CVD was found to be more effective than 
RVD. We also confirmed that most relations 
extracted by our method cannot be extracted 
merely by applying the well-known lexico-
syntactic patterns to Web documents. 
1 Introduction 
Large-scale taxonomies such as WordNet (Fell-
baum 1998) play an important role in informa-
tion extraction and question answering. However, 
extremely high costs are borne to manually en-
large and maintain such taxonomies. Thus, appli-
cations using these taxonomies tend to face the 
drawback of data sparseness. This paper presents 
a new method for discovering a large set of hy-
ponymy relations. Here, a word1 X is regarded as 
a hypernym of a word Y if Y is a kind of X or Y 
is an instance of X. We are able to generate 
large-scale hyponymy relations by attaching new 
words to the hyponymy database extracted from 
Wikipedia (referred to as ?Wikipedia relation 
database?) by using distributional similarity cal-
culated from Web documents. Relations ex-
tracted from Wikipedia are relatively clean. On 
the other hand, reliable distributional similarity 
can be calculated using a large number of docu-
ments on the Web. In this paper, we combine the 
advantages of these two resources.  
Using distributional similarity, our algorithm 
first computes k similar words for a target word. 
Then, each k similar word assigns a score to its 
ancestors in the hierarchical structures of the 
Wikipedia relation database. The hypernym that 
has the highest score for the target word is se-
lected as the hypernym of the target word. Figure 
1 is an overview of the proposed approach. 
In the experiment, we extracted hypernyms for 
approximately 670,000 target words that are not 
included in the Wikipedia relation database but 
are found on the Web. We tested two distribu-
tional similarities: one based on raw verb-noun 
dependencies (RVD) and the other based on a 
large-scale clustering of verb-noun dependencies 
(CVD). The experimental results showed that the 
proposed methods were more effective than the 
other baseline approaches. In addition, we con-
firmed that most of the relations extracted by our 
method could not be extracted using the lexico-
syntactic pattern-based method.  
In the remainder of this paper, we first intro-
                                                 
1 In this paper, we use the term ?word? for both ?a 
single-word word? and ?a multi-word word.? 
929
duce some related works in Section 2. Section 3 
describes the Wikipedia relation database. Sec-
tion 4 describes the distributional similarity cal-
culated by the two methods. In Section 5, we 
describe a method to discover an appropriate 
hypernym for each target word. The experimen-
tal results are presented in Section 6 before con-
cluding the paper in Section 7. 
2 Related Works 
Most previous researchers have relied on lex-
ico-syntactic patterns for hyponymy acquisition. 
Lexico-syntactic patterns were first used by 
Hearst (1992). The patterns used by her included 
?NP0 such as NP1,? in which NP0 is a hypernym 
of NP1. Using these patterns as seeds, Hearst dis-
covered new patterns by which to semi-
automatically extract hyponymy relations. Pantel 
et al (2004a) proposed a method to automatical-
ly discover the patterns using a minimal edit dis-
tance. Ando et al (2003) applied predefined lex-
ico-syntactic patterns to Japanese news articles. 
Snow et al (2005) generalized these lexico-
syntactic pattern-based methods by using depen-
dency path features for machine learning. Then, 
they extended the framework such that this me-
thod was capable of making use of heterogenous 
evidence (Snow et al 2006). These pattern-based 
methods require the co-occurrences of a target 
word and the hypernym in a document. It should 
be noted that the requirement of such co-
occurrences actually poses a problem when we 
extract a large set of hyponymy relations since 
they are not frequently observed (Shinzato et al 
2004, Pantel et al 2004b). 
Clustering-based methods have been proposed 
as another approach. Caraballo (1999), Pantel et 
al. (2004b), and Shinzato et al (2004) proposed a 
method to find a common hypernym for word 
classes, which are automatically constructed us-
ing some measures of word similarities or hierar-
chical structures in HTML documents. Etzioni et 
al. (2005) used both a pattern-based approach 
and a clustering-based approach. The required 
amount of co-occurrences is significantly re-
duced due to class-based generalization 
processes. Note that these clustering-based me-
thods obtain the same hypernym for all the words 
in a particular class. This causes a problem for 
selecting an appropriate hypernym for each word 
in the case when the granularity or the construc-
tion of the classes is incorrect. Figure 2 shows 
the drawbacks of the existing approaches. 
Ponzetto et al (2007) and Sumida et al (2008) 
proposed a method for acquiring hyponymy rela-
tions from Wikipedia. This Wikipedia-based ap-
proach can extract a large volume of hyponymy 
relations with high accuracy. However, it is also 
true that this approach does not account for many 
words that usually appear in Web documents; 
this could be because of the unbalanced topics in 
Wikipedia or merely because of the incomplete 
coverage of articles on Wikipedia. Our method 
can target words that frequently appear on the 
Web but are not included in the Wikipedia rela-
tion database, thus making the results of the Wi-
kipedia-based approach richer and more ba-
lanced. Our approach uses distributional similari-
Figure 1: Overview of the proposed approach. 
hypernym : 
Target word:  Selected from the Web 
: word
k similar words
No direct co-occurrences of 
hypernym and hyponym in 
corpora are needed.
Selected from hypernyms in the 
Wikipedia relation database.
A hypernym is selected for 
each word independently.
Wikipedia relation database
Wikipedia-based approach
(Ponzetto et al 2007 and 
Sumida et al 2008)
Hyponymy relations are 
extracted using the layout 
information of Wikipedia.
Wikipedia
Figure 2: Drawbacks in existing approaches for hypo-
nymy acquisition. 
Pattern-based method
(Hearst 1992, Pantel et al 
2004a, Ando et al 2003, 
Snow et al 2005, Snow et al 
2006, and Etzioni et al 2005)
Clustering-based method
(Caraballo 1999, Pantel et al 
2004b, Shinzato et al 2004, 
and Etzioni et al 2005)
DocumentsCorpus/documents
Co-occurrences 
in a pattern are 
needed 
hypernym such as word hypernym ..?   word
word
word
wordword
Word Class
word
The same hypernym 
is selected for all 
words in a class.
930
ty, which is computed based on the noun-verb 
dependency profiles on the Web. The use of dis-
tributional similarity resembles the clustering-
based approach; however, our method can select 
a hypernym for each word independently, and it 
does not suffer from class granularity mismatch 
or the low quality of classes. In addition, our ap-
proach exploits the hierarchical structures of the 
Wikipedia hypernym relations.  
3 Wikipedia Relation Database 
Our Wikipedia relation database is based on the 
extraction method of Sumida et al (2008). They 
proposed a method of automatically acquiring 
hyponymy relations by focusing on the hierar-
chical layout of articles on Wikipedia. By way of 
an example, Figure 3 shows part of the source 
code clipped from the article titled ?Penguin.? 
An article has hierarchical structures composed 
of titles, sections, itemizations, etc. The entire 
article is divided into sections titled ?Anatomy,? 
?Mating habits,? ?Systematics and evolution,? 
?Penguins in popular culture,? and so on. The 
section ?Systematics and evolution? has a sub-
section ?Systematics,? which is further divided 
into ?Aptenodytes,? ?Eudyptes,? and so on. 
Some of these section-subsection relations can be 
regarded as valid hyponymy relations. In this 
article, relations such as the one between ?Apte-
nodytes? and ?Emperor Penguin? and that be-
tween ?Book? and ?Penguins of the World? are 
valid hyponymy relations.  
First, Sumida et al (2008) extracted hypony-
my relation candidates from hierarchical struc-
tures on Wikipedia. Then, they selected proper 
hyponymy relations using a support vector ma-
chine classifier. They used several kinds of fea-
tures for the hyponymy relation candidate, such 
as a POS tag for each word, the appearance of 
morphemes of each word, the distance between 
two words in the hierarchical structures of Wiki-
pedia, and the last character of each word. As a 
result of their experiments, approximately 2.4 
million hyponymy relations in Japanese were 
extracted, with a precision rate of 90.1%.  
Compared to the traditional taxonomies, these 
extracted hyponymy relations have the following 
characteristics (Fellbaum 1998, Bond et al 2008). 
(a) The database includes a more extensive 
vocabulary. 
(b)  The database includes a large number of 
named entities. 
Popular Japanese taxonomies GoiTaikei (Ike-
hara et al 1997) and Bunrui-Goi-Hyo (1996) 
contain approximately 300,000 words and 
96,000 words, respectively. In contrast, the ex-
tracted hyponymy relations contain approximate-
ly 1.2 million hyponyms and are undoubtedly 
much larger than the existing taxonomies. 
Another difference is that since Wikipedia covers 
a large number of named entities, the extracted 
hyponymy relations also contain a large number 
of named entities.  
Note that the extracted relations have a hierar-
chical structure because one hypernym of a cer-
tain word may also be the hyponym of another 
hypernym. However, we observed that the depth 
of the hierarchy, on an average, is extremely 
shallow. To make the hierarchy appropriate for 
our method, we extended these into a deeper hie-
rarchical structure. The extracted relations in-
clude many compound nouns as hypernyms, and 
we decomposed a compound noun into a se-
quence of nouns using a morphological analyzer. 
Since Japanese is a head-final language, the suf-
fix of a noun sequence becomes the hypernym of 
the original compound noun if the suffix forms 
another valid compound noun. We extracted suf-
fixes of compound nouns and manually checked 
whether they were valid compound nouns; then, 
we constructed a hierarchy of compound nouns. 
The hierarchy can be extended such that it in-
cludes the hyponyms of the original hypernym 
and the resulting hierarchy constitutes a hierar-
chical taxonomy. We use this hierarchical tax-
onomy as a target for expansion.2  
                                                 
2  Note that this modification was performed as part of 
another project of ours aimed at constructing a large-scale 
and clean hypernym knowledge base by human annotation. 
We do not think this cost is directly relevant to the method 
proposed here. 
Figure 3: A part of source code clipped from the 
article ?Penguin? in Wikipedia. 
'''Penguins''' are a group of 
[[Aquatic animal|aquatic]], 
[[flightless bird]]s. 
== Anatomy == 
== Mating habits == 
==Systematics and evolution== 
===Systematics=== 
* Aptenodytes 
**[[Emperor Penguin]] 
** [[King Penguin]] 
* Eudyptes 
== Penguins in popular culture == 
== Book == 
* Penguins 
* Penguins of the World 
== Notes == 
* Penguinone 
* the [[Penguin missile]] 
[[Category:Penguins]] 
[[Category:Birds]]
931
4 Distributional Similarity 
The distributional hypothesis states that words 
that occur in similar contexts tend to be semanti-
cally similar (Harris 1985). In this section, we 
first introduce distributional similarity based on 
raw verb-noun dependencies (RVD). To avoid 
the sparseness problem of the co-occurrence of 
verb-noun dependencies, we also use distribu-
tional similarity based on a large-scale clustering 
of verb-noun dependencies (CVD). 
In the experiment mentioned in the following 
section, we used the TSUBAKI corpus (Shinzato 
et al 2008) to calculate distributional similarity. 
This corpus provides a collection of 100 million 
Japanese Web pages containing 6 ? 109
 
sentences. 
4.1 Distributional Similarity Based on RVD 
When calculating the distributional similarity 
based on RVD, we use the triple <v, rel, n>, 
where v is a verb, n is a noun phrase, and rel 
stands for the relation between v and n. In Japa-
nese, a relation rel is represented by postposi-
tions attached to n and the phrase composed of n 
and rel modifies v. Each triple is divided into two 
parts. The first is <v, rel> and the second is n. 
Then, we consider the conditional probability of 
occurrence of the pair <v, rel>: P(<v, rel>|n).  
P(<v, rel>|n) can be regarded as the distribution 
of the grammatical contexts of the noun phrase n. 
The distributional similarity can be defined as 
the distance between these distributions. There 
are several kinds of functions for evaluating the 
distance between two distributions (Lee 1999). 
Our method uses the Jensen-Shannon divergence. 
The Jensen-Shannon divergence between two 
probability distributions, )|( 1nP ?  and )|( 2nP ? , 
can be calculated as follows: 
 
)),
2
)|()|(
||)|((
)
2
)|()|(
||)|(((
2
1
))|(||)|((
21
2
21
1
21
nPnP
nPD
nPnP
nPD
nPnPD
KL
KL
JS
?+??+
?+??=
??
 
 
where DKL indicates the Kullback-Leibler diver-
gence and is defined as follows: 
 
.
)|(
)|(
log)|())|(||)|((
2
1
121 ? ???=?? nP nPnPnPnPDKL  
 
Finally, the distributional similarity between 
two words, n1 and n2, is defined as follows: 
 
)).|(||)|((1),( 2121 nPnPDnnsim JS ???=  
 
This similarity assumes a value from 0 to 1. If 
two words are similar, the value will be close to 
1; if two words have entirely different meanings, 
the value will be 0.
 
In the experiment, we used 1,000,000 noun 
phrases and 100,000 pairs of verbs and postposi-
tions to calculate the probability P(<v, rel>|n) 
from the dependency relations extracted from the 
above-mentioned Web corpus (Shinzato et al 
2008). The probabilities are computed using the 
following equation by modifying for the fre-
quency using the log function: 
 
?
>?<
+><
+><=><
Drelv
nrelvf
nrelvf
nrelvP
,
1),,(log(
1)),,(log(
)|,(
,0),,(if >>< nrelvf
  
where f(<v, rel, n>) is the frequency of a triple 
<v, rel, n> and D is the set defined as { <v, rel > | 
f(<v, rel, n>) > 0 }. In the case of f(<v, rel, n>) = 
0, P(<v, rel>|n) is set to 0.  
Instead of using the observed frequency di-
rectly as in the usual maximum likelihood esti-
mation, we modified it as above. Although this 
might seems strange, this kind of modification is 
common in information retrieval as a term 
weighing method (Manning et al 1999) and  it is 
also applied in some studies to yield better word 
similarities (Terada et al 2006, Kazama et al 
2009). We also adopted this idea in this study. 
4.2 Distributional Similarity Based on CVD 
Rooth et al (1999) and Torisawa (2001) showed 
that EM-based clustering using verb-noun de-
pendencies can produce semantically clean noun 
clusters. We exploit these EM-based clustering 
results as the smoothed contexts for noun n. In 
Torisawa?s model (2001), the probability of oc-
currence of the triple <v, rel, n> is defined as 
follows: 
 
,)()|()|,(
),,(
? ? ><=
><
Aadef aPanParelvP
nrelvP
 
 
where a denotes a hidden class of <v,rel> and n. 
In this equation, the probabilities P(<v,rel>|a), 
P(n|a), and P(a) cannot be calculated directly 
because class a is not observed in a given corpus. 
The EM-based clustering method estimates these 
probabilities using a given corpus. In the E-step, 
932
the probability P(a|<v,rel>) is calculated. In the 
M-step, the probabilities P(<v,rel>|a), P(n|a), 
and P(a) are updated to arrive at the maximum 
likelihood using the results of the E-step. From 
the results of estimation of this EM-based clus-
tering method, we can obtain the probabilities 
P(<v,rel>|a), P(n|a), and P(a) for each <v, rel>, n, 
and a. Then, P(a|n) is calculated by the following 
equation: 
 
.
)()|(
)()|(
)|( ? ?= Aa aPanP
aPanP
naP  
 
P(a|n) can be used to find the class of n. For 
example, the class that has the maximum P(a|n) 
can be regarded as the class to which n belongs. 
Noun phrases that occur with similar pairs 
<v,rel> tend to be classified in the same class. 
Kazama et al (2008) proposed the paralleliza-
tion of this EM-based clustering with the aim of 
enabling large-scale clustering and using the re-
sulting clusters in named entity recognition. Ka-
zama et al (2009) reported the calculation of 
distributional similarity using the clustering re-
sults. The distributional similarity was calculated 
by the Jensen-Shannon divergence, which was 
used in this paper. Similar to the case in Kazama 
et al, we performed word clustering using 
1,000,000 noun phrases and 2,000 classes. Note 
that the frequencies of dependencies were mod-
ified with the log function, as in RVD, described 
in the previous section. 
5 Discovering an Appropriate Hyper-
nym for a Target word 
In the Wikipedia relation database, there are 
about 95,000 hypernyms and about 1.2 million 
hyponyms. In both RVD and CVD, the words 
used were selected according to the number (the 
number of kinds, not the frequency) of <v, rel >s 
that n has dependencies in the data. As a result, 1 
million words were selected. The number of 
common words that are also included in the Wi-
kipedia relation database are as follows: 
 
Hypernyms     28,015 (common hypernyms) 
Hyponyms   175,022 (common hyponyms) 
 
These common hypernyms become candidates 
for hypernyms for a target word. On the other 
hand, the common hyponyms are used as clues 
for identifying appropriate hypernyms. 
In our task, the potential target words are 
about 810,000 in number and are not included in 
the Wikipedia relation database. These include 
some strange words or word phrases that are ex-
tracted due to the failure of morphological analy-
sis. We exclude these words using simple rules. 
Consequently, the number of target words for our 
process is reduced to about 670,000.  
In the following section, we outline the scor-
ing method that uses k similar words to discover 
an appropriate hypernym for a target word. We 
also explain several baseline approaches that use 
distributional similarity. 
5.1 Scoring with k similar Words 
In this approach, we first calculate the similari-
ties between the common hyponyms and a target 
word and select the k most similar common hy-
ponyms. Here, we use a similarity threshold val-
ue Smin to avoid the effect of words having lower 
similarities. If the similarity is less than the thre-
shold value, the word is excluded from the set of 
k similar words. Next, each k similar word votes 
a score to its ancestors in the hierarchical struc-
tures of the Wikipedia relation database. The 
score used to vote for a hypernym nhyper is as fol-
lows: 
 
,),(
)(
)()(
1),(?
??
? ?=
trghyperhypo
hypohyper
nksimilarnDescn
hypotrg
nnr
hyper
nnsimd
nscore
 
 
where ntrg is the target word, Desc(nhyper) is the 
descendant of the hypernym nhyper, ksimilar(ntrg) 
is the k similar word of ntrg, 
1),( ?hypohyper nnrd is a 
penalty that depends on the differences in the 
depth of hierarchy, d is a parameter for the penal-
ty value and has a value between 0 and 1, and 
r(ntrg, nhypo) is the difference in the depth of hie-
rarchy between ntrg and nhypo. sim(ntrg,nhypo) is a 
distributional similarity between ntrg and nhypo.  
As a result of scoring, each hypernym has a 
score for the target word. The hypernym that has 
the highest score for the target word is selected 
as its hypernym. The hyponymy relations thus 
produced are ranked according to the scores. 
Figure 4 shows an example of the scoring 
process. In this example, we use CitroenAX as the 
target word whose hypernym will be identified. 
First, the k similar words are extracted from the 
common hyponyms in the Wikipedia relation: 
Opel Astra, TVR Tuscan, Mitsubishi Minica, and 
Renault Lutecia are extracted. Next, each k simi-
lar word votes a score to its ancestors. The words 
Opel Astra, TVR Tuscan, and Renault Lutecia 
vote to their parent car and the word Mitsubishi 
933
Minica votes to its parent mini-vehicle and its 
grandparent car with a small penalty. Finally, the 
hypernym car, which has the highest score, is 
selected as the hypernym of the target word Ci-
troenAX. 
5.2 Baseline Approaches 
Using distributional similarity, we can also de-
velop the following baseline approaches to dis-
cover hyponymy relations. 
 
Selecting the hypernym of the most similar hy-
ponym (baseline approach 1) 
We use the heuristics that similar words tend to 
have the same hypernym. In this approach, we 
first calculate the similarities between the com-
mon hyponyms and the target word. The com-
mon hyponym most similar to the target word is 
extracted. Then, the parent of the extracted 
common hyponym is regarded as the hypernym 
of the target word. This approach outputs several 
hypernyms when the most similar hyponym has 
several hypernyms. This approach can be consi-
dered to be the same as the scoring method using 
k similar words when k = 1. We use the distribu-
tional similarity between the target word and the 
most similar hyponym in the Wikipedia relation 
database as the score for the appropriateness of 
the resulting hyponymy. 
 
Selecting the most similar hypernym (baseline 
approach 2) 
The distributional similarity between the com-
mon hypernym and the target word is calculated. 
Then, the hypernym that has the highest distribu-
tional similarity is regarded as the hypernym of 
the target word. The similarity is used as the 
score of the appropriateness of the produced hy-
ponymy. 
 
Scoring based on the average similarity of the 
hypernym?s children (baseline approach 3) 
This approach uses the probabilistic distributions 
of the hypernym?s children. We define the prob-
ability )|( hyperchild nP ? characterized by the children 
of the hypernym nhyper, as follows: 
 
,
)(
)()|(
)|(
)(
)(
?
?
?
?
?
=?
hyperhypo
hyperhypo
nChn
hypo
nChn
hypohypo
hyperchild nP
nPnP
nP  
 
where Ch(nhyper) is a set of all children of nhyper. 
Then, distributional similarities between a com-
mon hypernym nhyper and the target word nhypo are 
calculated. The hypernym that has the highest 
distributional similarity is selected as the hyper-
nym of the word. This distributional similarity is 
used as the score of the appropriateness of the 
produced hyponymy. 
If a hypernym has only a few children, the re-
liability of the probabilistic distribution of 
hypernym defined here will be low because the 
Wikipedia relation database includes some incor-
rect relations. For this reason, we use the hyper-
nym only if the number of children it has is more 
than a threshold value.  
6 Experiments 
We evaluated our proposed methods by using it 
in experiments to discover hypernyms from the 
Wikipedia relation database for the target words 
extracted from about 670,000 noun phrases.  
6.1 Parameter Estimation by Preliminary 
Experiments 
In the proposed methods, there are several para-
meters. We performed parameter optimization by 
randomly selecting 694 words as development 
data in our preliminary experiments. The hyper-
nyms of these words were determined manually. 
We adjusted the parameters so that each method 
achieved the best performance for this develop-
ment data. 
The parameters in the scoring method with k 
similar words were adjusted as follows3:  
 (RVD) 
Number of similar words:         k = 100. 
Similarity threshold:           Smin = 0.05. 
Penalty value for ancestors:    d = 0.6. 
                                                 
3 We tested the parameter values k = {100, 200, 300, 400, 
500, 600, 700, 800, 900, 1000}, Smin={0, 0.05, 0.1, 0.15, 0.2, 
0.25, 0.3, 0.35, 0.4} and d={0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 
0.8, 0.85, 0.9, 0.95, 1.0}. 
Figure 4: Overview of the scoring process.
car
CitroenAX
mini
vehicle
hybrid 
vehicle
Opel 
Astra
Renault 
Lutecia
Mitsubishi
Minica
k similar words
Each k-similar word
votes the score to its 
ancestors in the Wikipedia 
relation database.
Target word selected 
from the Web text (ntrg).
TVR 
Tuscan
: common hypernym(nhyper)
: k similar word &  
common hyponym(nhypo)
x d1
x d0
x d0
934
(CVD) 
Number of similar words:         k = 200. 
Similarity threshold:                Smin = 0.3. 
Penalty value for ancestors:    d = 0.6. 
 
The parameter in baseline approach 3 was ad-
justed as follows: 
Threshold for the number of children: 20. 
6.2 Evaluation of the Experimental Results 
on the Basis of Score Ranking 
Using the adjusted parameters, we conducted 
experiments to extract the hypernym of each tar-
get word with the help of the scoring method 
based on k similar words. In these experiments, 
two kinds of distributional similarity mentioned 
in Section 4 were exploited individually. The 
words that were used in the development data 
were excluded.  
We also conducted a comparative experiment 
in which the parameter value for the penalty of 
the hierarchal difference, d, was set to 0 to clari-
fy the ability of using hierarchal structures in the 
k similar words method. This means each k simi-
lar word votes only to their parent. 
We then judged the quality of each acquired 
hypernym. The evaluation data sets were sam-
pled from the top 1,000, 10,000, 100,000, and 
670,000 results that were ranked according to the 
score of each method. Then, against 200 samples 
that were randomly sampled from each set, one 
of the authors judged whether the hypernym ex-
tracted by each method for the target word was 
correct or not. In this evaluation, if the sentence 
?The target word is a kind of the hypernym? or 
?The target word is an instance of the hypernym? 
was consistent, the extracted hyponymy was 
judged as correct. It should be noted that the out-
puts of the compared methods are combined and 
shuffled to enable fair comparison. In addition, 
baseline approach 1 extracted several hypernyms 
for the target word. In this case, we judged the 
hypernym as correct when the case where one of 
the hypernyms was correct.  
The precision of each result is shown in Table 
1. The results of the k similar words method are 
far better than those of the other baseline me-
thods. In particular, the k similar words method 
with CVD outperformed the methods of the k 
similar words where the parameter value d was 
set to 0 and the method using RVD except for the 
top 1,000 results. This means that the use of hie-
rarchal structures and the clustering process for 
calculating distributional similarity are effective 
for this task. We confirmed the significant differ-
ences of the proposed method (CVD) as com-
pared with all the baseline approaches at the 1% 
significant level by the Fisher?s exact test (Hays 
1988). 
The precision of baseline approach 2 that se-
lected the most similar hypernym was the worst 
among all the methods. There were words that 
were similar to the target word among the hyper-
nyms extracted incorrectly. For example, the 
word semento-kojo (cement factory) was ex-
tracted for the hypernym of the word kuriningu-
kojo (dry cleaning plant). It is difficult to judge 
whether the word is a hypernym or just a similar 
word by using only the similarity measure. 
As for the results of baseline approach 1 using 
the most similar hyponym and baseline approach 
3 using the similarity of the set of hypernym?s 
children, the noise on the Wikipedia relation da-
tabase decreased the precision. Moreover, over-
specified hypernyms were extracted incorrectly 
by these methods. In contrast, the method of 
scoring based on the use of k similar words was 
robust against noise because it uses the voting 
approach for the similarities. Further, this me-
thod can extract hypernyms that are not over-
specific because it uses all descendants for scor-
ing.  
Table 2 shows some examples of relations ex-
tracted by the k similar words method using 
CVD. 
 
Table 1:  Precision of each approach based on the score ranking. CVD represents the method that uses the dis-
tributional similarity based on large-scale of clustering of verb-noun dependencies. RVD represents the 
one based on raw verb-noun dependencies. 
 k-similar words
(CVD) 
k-similar words
(RVD) 
k-similar words
(CVD, d = 0)
Baseline  
approach 1 
(CVD) 
Baseline  
approach 2 
(CVD) 
Baseline  
approach 3 
(CVD) 
1,000 0.940 1.000 0.850 0.730 0.290 0.630 
10,000 0.910 0.875 0.875 0.555 0.300 0.445 
100,000 0.745 0.710 0.730 0.500 0.280 0.435 
670,000 0.520 0.500 0.470 0.345 0.115 0.170 
935
6.3 Investigation of the Extracted Relation 
Overlap with a Conventional Method 
We randomly sampled 300 hyponymy rela-
tions that were extracted correctly using the k 
similar words method exploiting CVD and inves-
tigated whether or not these relations can be ex-
tracted by the conventional method based on the 
lexico-syntactic pattern. The possible hyponymy 
relations were extracted using the pattern-based 
method (Ando et al 2003) from the TSUBAKI 
corpus (Shinzato et al 2008). From a comparison 
of these relations, we found only 57 common 
hyponymy relations. That is, the remaining 243 
hyponymy relations were not included in the 
possible hyponymy relations. This result indi-
cates that our method can acquire the hyponymy 
relations that cannot be extracted by the conven-
tional pattern-based method. 
6.4 Discussions 
We investigated the reason for the errors gener-
ated by the method of scoring using k similar 
words exploiting CVD. We conducted experi-
ments on hypernym extraction targeting 694 
words in the development data mentioned in Sec-
tion 6.1. Among these, 286 relations were ex-
tracted incorrectly. In these relations, there were 
some frequent hypernyms. For example, the 
word sakuhin (work) appeared 28 times and hon 
(book) appeared 20 times. As shown in Table 2, 
hon (book) was also extracted for the target word 
meru-seminah (mail seminar). It is really diffi-
cult even for a human to identify whether the 
title is that of the book or the event. If we can 
identify these difficult hypernyms in advance, we 
can improve precision by excluding them from 
the target hypernyms. This will be one of the top-
ics for future study. 
7 Conclusion 
In this paper, we proposed a method for disco-
vering hyponymy relations between nouns by 
fusing the Wikipedia relation database and words 
from the Web. We demonstrated that the method 
using k similar words has high accuracy. The 
experimental results showed the effectiveness of 
using hierarchal structures and the clustering 
process for calculating distributional similarity 
for this task. The experimental results showed 
that our method could achieve 91.0% attachment 
accuracy for the top 10,000 hyponymy relations 
and 74.5% attachment accuracy for the top 
100,000 relations when using the clustering-
based similarity. We confirmed that most rela-
tions extracted by the proposed method could not 
be handled by the lexico-syntactic pattern-based 
method. Future work will be to filter out difficult 
hypernyms for hyponymy extraction process to 
achieve higher precision. 
References 
M. Ando, S. Sekine and S. Ishizaki. 2003. Automatic 
Extraction of Hyponyms from Newspaper Using 
Lexicosyntactic Patterns. IPSJ SIG Notes, 2003-
NL-157, pp. 77?82 (in Japanese). 
F. Bond, H. Isahara, K. Kanzaki and K. Uchimoto. 
2008. Boot-strapping a WordNet Using Multiple 
Existing WordNets. In the 6th International Confe-
rence on Language Resources and Evaluation 
(LREC), Marrakech.  
Bunruigoihyo. 1996. The National Language Re-
search Institute (in Japanese). 
S. A. Caraballo. 1999. Automatic Construction of a 
Hypernym-labeled Noun Hierarchy from Text. In 
Proceedings of the Conference of the Association 
for Computational Linguistics (ACL). 
O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. 
Shaked, S. Soderland, D. Weld and A. Yates. 2005. 
Unsupervised Named-Entity Extraction from the 
Web: An Experimental Study. Artificial Intelli-
gence, 165(1):91?134. 
C. Fellbaum. 1998. WordNet: An Electronic Lexical 
Table2:  Hypernym discovery results by the k-similar 
words based approach (CVD). The underline indi-
cates the hypernyms which are extracted incorrectly.
Score Target word Extracted hypernym 
58.6 INDIVI burando 
(fashion label)
54.3 kureome (Cleome) hana (flower)
34.4 UOKR  gemu (game)
21.7 Okido (Okido) machi (town)
20.5 Sumatofotsu 
(Smart fortwo) 
kuruma  
(car) 
15.6 Fukagawameshi 
(Fukagawa rice)
ryori (dish) 
8.9 John Barry sakkyokuka 
 (composer)
8.5 JVM sofuto-wea 
(software) 
6.6 metangasu 
(methane gas) 
genso 
(chemical element)
5.4 me-ru semina 
(mail seminar) 
Hon (book) 
3.9 gurometto 
(grommet) 
shohin 
(merchandise)
3.1 supuringubakku  
(spring back) 
gensho 
(phenomenon)
936
Database. Cambridge, MA: MIT Press. 
Z. Harris. 1985. Distributional Structure. In Katz, J. J. 
(ed.) The Philosophy of Linguistics, Oxford Uni-
versity Press, pp. 26?47. 
W. L. Hays. 1988. Statistics: Analyzing Qualitative 
Data, Rinehart and Winston, Inc., Ch. 18, pp. 769?
783. 
M. Hearst. 1992. Automatic Acquisition of Hypo-
nyms from Large Text Corpora. In Proceedings of 
the 14th Conference on Computational Linguistics 
(COLING), pp. 539?545.  
S. Ikehara, M. Miyazaki, S. Shirai, A. Yokoo, H. Na-
kaiwa, K. Ogura, Y. Ooyama and Y. Hayashi. 1997. 
Goi-Taikei A Japanese Lexicon, Iwanami Shoten. 
J. Kazama and K. Torisawa. 2008. Inducing Gazet-
teers for Named Entity Recognition by Large-scale 
Clustering of Dependency Relations. In Proceed-
ings of ACL-08: HLT, pp. 407?415. 
J. Kazama, Stijn De Saeger, K. Torisawa and M. Mu-
rata. 2009. Generating a Large-scale Analogy List 
Using a Probabilistic Clustering Based on Noun-
Verb Dependency Profiles. In 15th Annual Meeting 
of the Association for Natural Language 
Processing, C1?3 (in Japanese). 
L. Lee. 1999. Measures of Distributional Similarity. 
In Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics, pp. 25?
32. 
C. D. Manning and H. Schutze. 1999. Foundations of 
Statistical Natural Language Processing. The MIT 
Press. 
P. Pantel, D. Ravichandran and E. Hovy. 2004a. To-
wards Terascale Knowledge Acquisition. In Pro-
ceedings of the 20th International Conference on 
Computational Linguistics. 
P. Pantel and D. Ravichandran. 2004b. Automatically 
Labeling Semantic Classes. In Proceedings of the 
Human Language Technology and North American 
Chapter of the Association for Computational Lin-
guistics Conference. 
S. P. Ponzetto, and M. Strube. 2007. Deriving a Large 
Scale Taxonomy from Wikipedia. In Proceedings 
of the 22nd National Conference on Artificial Intel-
ligence, pp. 1440?1445. 
M. Rooth, S. Riezler, D. Presher, G. Carroll and F. 
Beil. 1999. Inducing a Semantically Annotated 
Lexicon via EM-based Clustering. In Proceedings 
of the 37th annual meeting of the Association for 
Computational Linguistics, pp. 104?111. 
K. Shinzato and K. Torisawa. 2004. Acquiring Hypo-
nymy Relations from Web Documents. In Proceed-
ings of HLT-NAACL, pp. 73?80. 
K. Shinzato, D. Kawahara, C. Hashimoto and S. Ku-
rohashi. 2008. A Large-Scale Web Data Collection 
as A Natural Language Processing Infrastructure. 
In the 6th International Conference on Language 
Resources and Evaluation (LREC). 
R. Snow, D. Jurafsky and A. Y. Ng. 2005. Learning 
Syntactic Patterns for Automatic Hypernym Dis-
covery. NIPS 2005. 
R. Snow, D. Jurafsky, A. Y. Ng. 2006. Semantic Tax-
onomy Induction from Heterogenous Evidence. In 
Proceedings of the 21st International Conference 
on Computational Linguistics and the 44th annual 
meeting of the Association for Computational Lin-
guistics, pp. 801?808. 
A. Sumida, N. Yoshinaga and K. Torisawa. 2008. 
Boosting Precision and Recall of Hyponymy Rela-
tion Acquisition from Hierarchical Layouts in Wi-
kipedia. In the 6th International Conference on 
Language Resources and Evaluation (LREC). 
A. Terada, M. Yoshida, H. Nakagawa. 2006. A Tool 
for Constructing a Synonym Dictionary using con-
text Information. In proceedings of IPSJ SIG Tech-
nical Reports, vol.2006 No.124, pp. 87-94. (In Jap-
anese). 
K. Torisawa. 2001. An Unsupervised Method for Ca-
nonicalization of Japanese Postpositions. In Pro-
ceedings of the 6th Natural Language Processing 
Pacific Rim Symposium (NLPRS), pp. 211?218. 
K. Torisawa, Stijn De Saeger, Y. Kakizawa, J. Kaza-
ma, M. Murata, D. Noguchi and A. Sumida. 2008. 
TORISHIKI-KAI, An Autogenerated Web Search 
Directory. In Proceedings of the second interna-
tional symposium on universal communication, pp. 
179?186, 2008. 
937
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1172?1181,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Large-Scale Verb Entailment Acquisition from the Web
Chikara Hashimoto? Kentaro Torisawa? Kow Kuroda?
Stijn De Saeger? Masaki Murata? Jun?ichi Kazama?
National Institute of Information and Communications Technology
Sorakugun, Kyoto, 619-0289, JAPAN
{
? ch,? torisawa,? kuroda,? stijn,?murata,? kazama}@nict.go.jp
Abstract
Textual entailment recognition plays a
fundamental role in tasks that require in-
depth natural language understanding. In
order to use entailment recognition tech-
nologies for real-world applications, a
large-scale entailment knowledge base is
indispensable. This paper proposes a con-
ditional probability based directional sim-
ilarity measure to acquire verb entailment
pairs on a large scale. We targeted 52,562
verb types that were derived from 108
Japanese Web documents, without regard
for whether they were used in daily life
or only in specific fields. In an evaluation
of the top 20,000 verb entailment pairs ac-
quired by previous methods and ours, we
found that our similarity measure outper-
formed the previous ones. Our method
also worked well for the top 100,000 re-
sults.
1 Introduction
We all know that if you snored, you must have
been sleeping, that if you are divorced, you must
have been married, and that if you won a lawsuit,
you must have sued somebody. These relation-
ships between events where one is the logical con-
sequence of the other are called entailment. Such
knowledge plays a fundamental role in tasks that
require in-depth natural language understanding,
e.g., answering questions and using natural lan-
guage interfaces.
This paper proposes a novel method for verb
entailment acquisition. Using a Japanese Web
corpus (Kawahara and Kurohashi, 2006a) derived
from 108 Japanese Web documents, we automat-
ically acquired such verb pairs as snore ? sleep
and divorce ? marry, where entailment holds be-
tween the verbs in the pair.1 Our definition of ?en-
tailment? is the same as that in WordNet3.0; v
1
entails v
2
if v
1
cannot be done unless v
2
is, or has
been, done.2
Our method follows the distributional similar-
ity hypothesis, i.e., words that occur in the same
context tend to have similar meanings. Just as in
the methods of Lin and Pantel (2001) and Szpek-
tor and Dagan (2008), we regard the arguments
of verbs as the context in the hypothesis. How-
ever, unlike the previous methods, ours is based
on conditional probability and is augmented with
a simple trick that improves the accuracy of verb
entailment acquisition. In an evaluation of the top
20,000 verb entailment pairs acquired by the pre-
vious methods and ours, we found that our similar-
ity measure outperformed the previous ones. Our
method also worked well for the top 100,000 re-
sults,
Since the scope of Natural Language Process-
ing (NLP) has advanced from a formal writing
style to a colloquial style and from restricted to
open domains, it is necessary for the language re-
sources for NLP, including verb entailment knowl-
edge bases, to cover a broad range of expressions,
regardless of whether they are used in daily life
or only in specific fields that are highly techni-
cal. As we will discuss later, our method can ac-
quire, with reasonable accuracy, verb entailment
pairs that deal not only with common and familiar
verbs but also with technical and unfamiliar ones
like podcast ? download and jibe ? sail.
Note that previous researches on entailment ac-
quisition focused on templates with variables or
word-lattices (Lin and Pantel, 2001; Szpektor and
Dagan, 2008; Barzilay and Lee, 2003; Shinyama
1Verb entailment pairs are described as v
1
? v
2
(v
1
is
the entailing verb and v
2
is the entailed one) henceforth.
2WordNet3.0 provides entailment relationships between
synsets like divorce, split up?marry, get married, wed, con-
join, hook up with, get hitched with, espouse.
1172
et al, 2002). Certainly these templates or word
lattices are more useful in such NLP applications
as Q&A than simple entailment relations between
verbs. However, our contention is that entailment
certainly holds for some verb pairs (like snore ?
sleep) by themselves, and that such pairs consti-
tute the core of a future entailment rule database.
Although we focused on verb entailment, our
method can also acquire template-level entailment
pairs with a reasonable accuracy.
The rest of this paper is organized as follows.
In ?2, related works are described. ?3 presents our
proposed method. After this, an evaluation of our
method and the existing methods is presented in
Section 4. Finally, we conclude the paper in ?5.
2 Related Work
Previous studies on entailment, inference rules,
and paraphrase acquisition are roughly classi-
fied into those that require comparable corpora
(Shinyama et al, 2002; Barzilay and Lee, 2003;
Ibrahim et al, 2003) and those that do not (Lin
and Pantel, 2001; Weeds and Weir, 2003; Geffet
and Dagan, 2005; Pekar, 2006; Bhagat et al, 2007;
Szpektor and Dagan, 2008).
Shinyama et al (2002) regarded newspaper arti-
cles that describe the same event as a pool of para-
phrases, and acquired them by exploiting named
entity recognition. They assumed that named en-
tities are preserved across paraphrases, and that
text fragments in the articles that share several
comparable named entities should be paraphrases.
Barzilay and Lee (2003) also used newspaper ar-
ticles on the same event as comparable corpora
to acquire paraphrases. They induced paraphras-
ing patterns by sentence clustering. Ibrahim et al
(2003) relied on multiple English translations of
foreign novels and sentence alignment to acquire
paraphrases. We decided not to take this approach
since using comparable corpora limits the scale
of the acquired paraphrases or entailment knowl-
edge bases. Although obtaining comparable cor-
pora has been simplified by the recent explosion
of the Web, the availability of plain texts is incom-
parably better.
Entailment acquisition methods that do not re-
quire comparable corpora are mostly based on the
distributional similarity hypothesis and use plain
texts with a syntactic parser. Basically, they parse
texts to obtain pairs of predicate phrases and their
arguments, which are regarded as features of the
predicates with appropriately assigned weights.
Lin and Pantel (2001) proposed a paraphrase ac-
quisition method (non-directional similarity mea-
sure) called DIRT which acquires pairs of binary-
templates (predicate phrases with two argument
slots) that are paraphrases of each other. DIRT em-
ploys the following similarity measure proposed
by Lin (1998):
Lin(l, r) =
?
f?F
l
?F
r
[w
l
(f) + w
r
(f)]
?
f?F
l
w
l
(f) +
?
f?F
r
w
r
(f)
where l and r are the corresponding slots of two
binary templates, F
s
is s?s feature vector (argu-
ment nouns), and w
s
(f) is the weight of f ? F
s
(PMI between s and f ). The intuition behind this
is that the more nouns two templates share, the
more semantically similar they are. Since we ac-
quire verb entailment pairs based on unary tem-
plates (Szpektor and Dagan, 2008) we used the
Lin formula to acquire unary templates directly
rather than using the DIRT formula, which is the
arithmetic-geometric mean of Lin?s similarities for
two slots in a binary template.
Bhagat et al (2007) developed an algorithm
called LEDIR for learning the directionality of
non-directional inference rules like those pro-
duced by DIRT. LEDIR implements a Direction-
ality Hypothesis: when two binary semantic re-
lations tend to occur in similar contexts and the
first one occurs in significantly more contexts than
the second, then the second most likely implies the
first and not vice versa.
Weeds and Weir (2003) proposed a general
framework for distributional similarity that mainly
consists of the notions of what they call Precision
(defined below) and Recall:
Precision(l, r) =
?
f?F
l
?F
r
w
l
(f)
?
f?F
l
w
l
(f)
where l and r are the targets of a similarity mea-
surement, F
s
is s?s feature vector, and w
s
(f) is the
weight of f ? F
s
. The best performing weight is
PMI. Precision is a directional similarity measure
that examines the coverage of l?s features by those
of r?s, with more coverage indicating more simi-
larity.
Szpektor and Dagan (2008) proposed a direc-
tional similarity measure called BInc (Balanced-
Inclusion) that consists of Lin and Precision, as
BInc(l, r) =
?
Lin(l, r) ? Precision(l, r)
1173
where l and r are the target templates. For weight-
ing features, they used PMI. Szpektor and Dagan
(2008) also proposed a unary template, which is
defined as a template consisting of one argument
slot and one predicate phrase. For example, X take
a nap ? X sleep is an entailment pair consisting
of two unary templates. Note that the slot X must
be shared between templates. Though most of the
previous entailment acquisition studies focused on
binary templates, unary templates have an obvi-
ous advantage over binary ones; they can handle
intransitive predicate phrases and those that have
omitted arguments. The Japanese language, which
we deal with here, often omits arguments, and thus
the advantage of unary templates is obvious.
As shown in ?4, our method outperforms Lin,
Precision, and BInc in accuracy.
Szpector et al (2004) addressed broad coverage
entailment acquisition. But their method requires
an existing lexicon to start, while ours does not.
Apart from the dichotomy of the compara-
ble corpora and the distributional similarity ap-
proaches, Torisawa (2006) exploited the structure
of Japanese coordinated sentences to acquire verb
entailment pairs. Pekar (2006) used the local
structure of coherent text by identifying related
clauses within a local discourse. Zanzotto et al
(2006) exploited agentive nouns. For example,
they acquired win ? play from ?the player wins.?
Geffet and Dagan (2005) proposed the Distribu-
tional Inclusion Hypotheses, which claimed that if
a word v entails another word w, then all the char-
acteristic features of v are expected to appear with
w, and vice versa. They applied this to noun en-
tailment pair acquisition, rather than verb pairs.
3 Proposed Method
This section presents our method of verb entail-
ment acquisition. First, the basics of Japanese are
described. Then, we present the directional sim-
ilarity measure that we developed in ?3.2. ?3.3
describes the structure and acquisition of the web-
based data from which entailment pairs are de-
rived. Finally, we show how we acquire verb en-
tailment pairs using our proposed similarity mea-
sure and the web-based data in ?3.4.
3.1 Basics of Japanese
Japanese explicitly marks arguments including the
subject and object by postpositions, and is a head-
final language. Thus, a verb phrase consisting of
an object hon (book) and a verb yomu (read), for
example, is expressed as hon-wo yomu (book-ACC
read) ?read a book? with the accusative postpo-
sition wo marking the object.3 Accordingly, we
refer to a unary template as ?p, v? hereafter, with
p and v referring to the postposition and a verb.
Also, we abbreviate a template-level entailment
?p
l
, v
l
? ? ?p
r
, v
r
? as l ? r for simplicity. We
define a unary template as a template consisting
of one argument slot and one predicate, following
Szpektor and Dagan (2008).
3.2 Directional Similarity Measure based on
Conditional Probability
The directional similarity measure that we devel-
oped and called Score is defined as follows:
Score(l, r) = Score
base
(l, r) ? Score
trick
(l, r)
where l and r are unary templates, and Score in-
dicates the probability of l ? r. Score
base
, which
is the base of Score, is defined as follows:
Score
base
(l, r) =
?
f?F
l
?F
r
P (r|f)P (f |l)
where F
s
is s?s feature vector (nouns including
compounds). The intention behind the definition
of Score
base
is to emulate the conditional proba-
bility P (v
r
|v
l
)
4 in a distributional similarity style
function. Note that P (v
r
|v
l
) should be 1 when en-
tailment v
l
? v
r
holds (i.e., v
r
is observed when-
ever v
l
is observed) and we have reliable proba-
bility values. Then, if we can directly estimate
P (v
r
|v
l
), it is reasonable to assume v
l
? v
r
if
P (v
r
|v
l
) is large enough. However, we cannot es-
timate P (v
r
|v
l
) directly since it is unlikely that we
will observe the verbs v
r
and v
l
at the same time.
(People do not usually repeat v
r
and v
l
in the same
document to avoid redundancy.) Thus, instead of
a direct estimation, we substitute Score
base
(l, r)
as defined above. In other words, we assume
P (v
r
|v
l
) ? P (r|l) ? ?
f?F
l
?F
r
P (f |l)P (r|f).
Actually, Score
base
originally had another mo-
tivation, inspired by Torisawa (2005), for which no
postposition but the instrumental postposition de
was relevant. In this discussion, all of the nouns
(fs) that are marked by the instrumental postposi-
tion are seen as ?tools,? and P (f |l) is interpreted
3ACC represents an accusative postposition in Japanese.
Likewise, NOM, DAT, INS, and TOP are the symbols for the
nominative, dative, instrumental, and topic postpositions.
4Remember that v
l
and v
r
are the verbs of unary tem-
plates l and r.
1174
as a measure of how typically the tool f is used
to perform the action denoted by (the v
l
of) l; if
P (f |l) is large enough, f is a typical tool used in
l. On the other hand, P (r|f) indicates the proba-
bility of (the v
r
of) r being the purpose for using
the tool f . See (1) for an example.
(1) konro-de chouri-suru
cooking.stove-INS cook
?cook (something) using a cooking stove.?
The purpose of using a cooking stove is to cook.
Torisawa (2005) has pointed out that when r ex-
presses the purpose of using a tool f , P (r|f) tends
to be large. This predicts that P (r|cooking stove)
is large, where r is ?de, cook?.
According to this observation, if f is a single
purpose tool and P (f |l), the probability of f be-
ing the tool by which l is performed, and P (r|f),
the probability of r being the purpose of using the
tool f , are large enough, then the typical perfor-
mance of the action v
l
should contain some ac-
tions that can be described by v
r
, i.e., the pur-
pose of using f . Moreover, if all the typical tools
(fs) used in v
l
are also used for v
r
, most perfor-
mances of the action v
l
should contain a part de-
scribed by the action v
r
. In summary, this means
that when ?
f?F
l
?F
r
P (r|f)P (f |l), Score
base
, has
a large value, we can expect v
l
? v
r
.
For example, let v
l
be deep-fry and v
r
be cook.
Note that v
l
? v
r
holds for this example. There
are many tools that are used for deep-frying,
such as cooking stove, pot, or pan. This means
that P (cooking stove|l), P (pot|l), or P (pan|l) are
large. On the other hand, the purpose of using all
of these tools is cooking, based on common sense.
Thus, probabilities such as P (r|cooking stove)
and P (r|pan) should have large values. Accord-
ingly, ?
f?F
l
?F
r
P (f |l)P (r|f), Score
base
, should
be relatively large for deep-fry ? cook,
Actually, we defined Score
base
based on the
above assumption However, through a series of
preliminary experiments, we found that the same
score could be applied without losing the preci-
sion to the other postpositions. Thus, we gener-
alized the framework so that it could deal with
most postpositions, namely ga (NOM), wo (ACC),
ni (DAT), de (INS), and wa (TOP). Note that this
is a variation of the distributional inclusion hy-
pothesis (Geffet and Dagan, 2005), but that we do
not use mutual information as in previous works,
based on the hypothesis discussed above. Actu-
ally, as shown in ?4, our conditional probability
based method outperformed the mutual informa-
tion based metrics in our experiments.
On the other hand, Score
trick
implements an-
other assumption that if only one feature con-
tributes to Score
base
and the contribution of the
other nouns is negligible, if any, the similarity is
unreliable. Accordingly, for Score
trick
, we uni-
formly ignore the contribution of the most domi-
nant feature from the similarity measurement.
Score
trick
(l, r)
= Score
base
(l, r) ? max
f?F
l
?F
r
P (r|f)P (f |l)
As shown in ?4, this trick actually improved the
entailment acquisition accuracy.
We used maximum likelihood estimation to ob-
tain P (r|f) and P (f |l) in the above discussion.
Bannard and Callison-Burch (2005) and Fujita
and Sato (2008) also proposed directional simi-
larity measures based on conditional probability,
which are very similar to Score
base
, although ei-
ther their method?s prerequisites or the targets of
the similarity measurements were different from
ours. The method of Bannard and Callison-Burch
(2005) requires bilingual parallel corpora, and
uses the translations of expressions as its feature.
Fujita and Sato (2008) dealt with productive pred-
icate phrases, while our target is non-productive
lexical units, i.e., verbs. Thus, this is the first
attempt to apply a conditional probability based
similarity measure to verb entailment acquisition.
In addition, the trick implemented in Score
trick
is
novel.
3.3 Preparing Template-Feature Tuples
Our method starts from a dataset called template-
feature tuples, which was derived from the Web
in the following way: 1) Parse the Japanese Web
corpus (Kawahara and Kurohashi, 2006a) derived
from 108 Japanese Web documents with Japanese
dependency parser KNP (Kawahara and Kuro-
hashi, 2006b). 2) Extract triples ?n, p, v? consist-
ing of nouns (n), postpositions (p), and verbs (v),
where an n marked by a p depends on a v from
the parsed Web text. 3) From the triple database,
construct template-feature tuples ?n, ?p, v?? by re-
garding ?p, v? as a unary template and n as one of
its features. 4) Convert the verbs into their canon-
ical forms as defined by KNP. 5) Filter out tuples
that fall into one of the following categories: 5-
1) Freq(?p, v?) < 20. 5-2) Its verb is passivized,
1175
causativized, or negated. 5-3) Its verb is semanti-
cally vague like be, do, or become. 5-4) Its post-
position is something other than ga (NOM), wo
(ACC), ni (DAT), de (INS), or wa (TOP).
The resulting unary template-feature tuples in-
cluded 127,808 kinds of templates that consisted
of 52,562 verb types and five kinds of postpo-
sitions. The verbs included compound words
like bosi-kansen-suru (mother.to.child-infection-
do) ?infect from mothers to infants.?
3.4 Acquiring Entailment Pairs
We acquired verb entailment pairs using the fol-
lowing procedure: i) From the template-feature
tuples mentioned in ?3.3, acquire unary template
pairs that exhibit an entailment relation between
them using the directional similarity measure in
?3.2. ii) Convert the acquired unary templates
?p, v? into naked verbs v by stripping the postpo-
sitions p. iii) Remove the duplicated verb pairs
resulting from stripping ps. To be precise, when
we removed the duplicated pairs, we left the high-
est ranked one. iv) Retrieve N-best verb pairs as
the final output from the result of iii). That is, we
first acquired unary template pairs and then trans-
formed them into verb pairs.
Although this paper focuses on verb entailment
acquisition, we also evaluated the accuracy of
template-level entailment acquisition, in order to
show that our similarity measure works well, not
only for verb entailment acquisition, but also for
template entailment acquisition (See ?4.4). we
created two kinds of unary templates: the ?Scoring
Slots? template and the ?Nom(inative) Slots? tem-
plate. The first is simply the result of the procedure
i); all of the templates have slots that are used for
similarity scoring. The second one was obtained
in the following way: 1) Only templates whose p
is not a nominative are sampled from the result of
the procedure i). 2) Their ps are all changed to a
nominative. Templates of the second kind are used
to show that the corresponding slots between tem-
plates (nominative, in this case) that are not used
for similarity scoring can be incorporated to re-
sulting template-level entailment pairs if the scor-
ing function really captures the semantic similarity
between templates.
Note that, for unary template entailment pairs
like (2) to be well-formed, the two unary slots (X-
wo) between templates must share the same noun
as the index i indicates. This is relevant in ?4.4.
(2) X
i
-wo musaborikuu ? X
i
-wo taberu
X
i
-ACC gobble X
i
-ACC eat
4 Evaluation
We compare the accuracy of our method with that
of the alternative methods in ?4.1. ?4.2 shows
the effectiveness of the trick. We examine the en-
tailment acquisition accuracy for frequent verbs in
?4.3, and evaluate the performance of our method
when applied to template-level entailment acquisi-
tion in ?4.4. Finally, by showing the accuracy for
verb pairs obtained from the top 100,000 results,
we claim that our method provides a good start-
ing point from which a large-scale verb entailment
resource can be constructed in ?4.5.
For the evaluation, three human annotators (not
the authors) checked whether each acquired entail-
ment pair was correct. The average of the three
Kappa values for each annotator pair was 0.579
for verb entailment pairs and 0.568 for template
entailment pairs, both of which indicate the mid-
dling stability of this evaluation annotation.
4.1 Experiment 1: Verb Pairs
We applied Score, BInc, Lin, and Precision to the
template-feature tuples (?3.3), obtained template
entailment pairs, and finally obtained verb entail-
ment pairs by removing the postpositions from the
templates as described in ?3. As a baseline, we
created pairs from randomly chosen verbs.
Since we targeted all of the verbs that ap-
peared on the Web (under the condition of
Freq(?p, v?) ? 20), the annotators were con-
fronted with technical terms and slang that they
did not know. In such cases, they consulted dic-
tionaries (either printed or machine readable ones)
and the Web. If they still could not find the mean-
ing of a verb, they labeled the pair containing the
unknown verb as incorrect.
We used the accuracy = # of correct pairs# of acquired pairs as
an evaluation measure. We regarded a pair as cor-
rect if it was judged correct by one (Accuracy-1),
two (Accuracy-2), or three (Accuracy-3) annota-
tors.
We evaluated 200 entailment pairs sampled
from the top 20,000 for each method (# of ac-
quired pairs = 200). For fairness, the evaluation
samples for each method were shuffled and placed
in one file from which the annotators worked. In
this way, they were unable to know which entail-
ment pair came from which method.
1176
Note that the verb entailment pairs produced
by Lin do not provide the directionality of en-
tailment. Thus, the annotators decided the direc-
tionality of these entailment pairs as follows: i)
Copy 200 original samples and reverse the order
of v
1
and v
2
. ii) Shuffle the 400 Lin samples
(the original and reversed samples) with the other
ones. iii) Evaluate all of the shuffled pairs. Each
Lin pair was regarded as correct if either direction
was judged correct. In other words, we evaluated
the upper bound performance of the LEDIR algo-
rithm.
Table 1 shows the accuracy of the acquired
verb entailment pairs for each method. Figure 1
Method Acc-1 Acc-2 Acc-3
Score 0.770 0.660 0.460
BInc 0.450 0.255 0.125
Precision 0.725 0.545 0.385
Lin 0.590 0.370 0.160
Random 0.050 0.010 0.005
Table 1: Accuracy of verb entailment pairs.
shows the accuracy figures for the N-best entail-
ment pairs for each method, with N being 1,000,
2,000, . . ., or 20,000. We observed the following
points from the results. First, Score outperformed
all the other methods. Second, Score and Pre-
cision, which are directional similarity measures,
worked well, while Lin, which is a symmetric one,
performed poorly even though the directionality of
its output was determined manually.
Looking at the evaluated samples, Score suc-
cessfully acquired pairs in which the entailed
verbs generalized entailing verbs that were techni-
cal terms. (3) shows examples of Score?s outputs.
(3) a. RSS-haisin-suru ? todokeru
RSS-feed-do deliver
?feed the RSS data?
b. middosippu-maunto-suru ? tumu
midship-mounting-do mount
?have (engine) midship-mounted?
The errors made by DIRT (4) and BInc (5) in-
cluded pairs consisting of technical terms.
(4) kurakkingu-suru
software.cracking-do
?crack a (security) system?
? koutiku-hosyu-suru
building-maintenance-do
?build and maintain a system?
Accuracy-1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
BInc
Precision
Lin
Accuracy-2
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
BInc
Precision
Lin
Accuracy-3
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
BInc
Precision
Lin
Figure 1: Accuracy of verb entailment pairs.
(5) suisou-siiku-suru
tank-raising-do
?raise (fish) in a tank?
? siken-houryuu-suru
test-discharge-do
?stock (with fish) experimentally?
These terms are related in some sense, but they
are not entailment pairs.
4.2 Experiment 2: Effectiveness of the Trick
Next, we investigated the effectiveness of the trick
described in ?3. We evaluated Score, Score
trick
,
and Score
base
. Table 2 shows the accuracy figures
for each method. Figure 2 shows the accuracy fig-
ures for the N-best outputs for each method. The
1177
Method Acc-1 Acc-2 Acc-3
Score 0.770 0.660 0.460
Score
trick
0.725 0.610 0.395
Score
base
0.590 0.465 0.315
Table 2: Effectiveness of the trick.
results illustrate that introducing the trick signif-
icantly improved the performance of Score
base
,
and so did multiplying Score
trick
and Score
base
,
which is our proposal Score.
(6) shows an example of Score
base
?s errors.
(6) gazou-sakusei-suru ? henkou-suru
image-making-do change-do
?make an image? ?change?
This pair has only two shared nouns (f ? F
l
?F
r
),
and more than 99.99% of the pair?s similarity re-
flects only one of the two. Clearly, the trick would
have prevented the pair from being highly ranked.
4.3 Experiment 3: Pairs of Frequent Verbs
We found that the errors made by Lin and BInc
in Experiment 1 were mostly pairs of infrequent
verbs such as technical terms. Thus, we con-
ducted the acquisition of entailment pairs targeting
more frequent verbs to see how their performance
changed. The experimental conditions were the
same as in Experiment 1, except that the templates
(?p, v?) used were all Freq(?p, v?) ? 200.
Table 3 shows the accuracy figures for each
method with the changes in accuracy from those
of the original methods in parentheses. The re-
Method Acc-1 Acc-2 Acc-3
Score
0.690 0.520 0.335
(?0.080) (?0.140) (?0.125)
BInc 0.455 0.295 0.160(+0.005) (+0.040) (+0.035)
Precision 0.450 0.355 0.205(?0.275) (?0.190) (?0.180)
Lin 0.635 0.385 0.205(+0.045) (+0.015) (+0.045)
Table 3: Accuracy of frequent verb pairs.
sults show that the accuracies of Score and Pre-
cision (the two best methods in Experiment 1) de-
graded, while the other two improved a little. We
suspect that the performance difference between
these methods would get smaller if we further re-
stricted the target verbs to more frequent ones.
Accuracy-1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
ScoretrickScorebase
Accuracy-2
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
ScoretrickScorebase
Accuracy-3
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
ScoretrickScorebase
Figure 2: Accuracy of verb entailment pairs ac-
quired by Score, Score
trick
, and Score
base
.
However, we believe that dealing with verbs com-
prehensively, including infrequent ones, is impor-
tant, since, in the era of information explosion, the
impact on applications is determined not only by
frequent verbs but also infrequent ones that consti-
tute the long tail of a verb-frequency graph. Thus,
this tendency does not matter for our purpose.
4.4 Experiment 4: Template Pairs
This section presents the entailment acquisition
accuracy for template pairs to show that our
method can also perform the entailment acqui-
sition of unary templates. We presented pairs
of unary templates, obtained by the procedure in
1178
?3.4, to the annotators. In doing so, we restricted
the correct entailment pairs to those for which en-
tailment always held regardless of what argument
filled the two unary slots, and the two slots had to
be filled with the same argument, as exemplified
in (2). We evaluated Score and Precision.
Table 4 shows the accuracy of the acquired pairs
of unary templates. Compared to verb entailment
Method Acc-1 Acc-2 Acc-3
Score
0.655 0.510 0.300
Scoring (?0.115) (?0.150) (?0.160)
Slots Precision 0.565 0.430 0.265(?0.160) (?0.115) (?0.120)
Score
0.665 0.515 0.315
Nom (?0.105) (?0.145) (?0.145)
Slots Precision 0.490 0.325 0.215(?0.235) (?0.220) (?0.170)
Table 4: Accuracy of entailment pairs of templates
whose slots were used for scoring.
acquisition, the accuracy of both methods dropped
by about 10%. This was mainly due to the evalua-
tion restriction exemplified in (2) which was not
introduced in the previous experiments; the an-
notators ignored the argument correspondence be-
tween the verb pairs in Experiment 1. Also note
that Score outperformed Precision in this experi-
ment, too.
(7) and (8) are examples of the Scoring Slots
template entailment pairs and (9) is that of the
Nom Slots acquired by our method.
(7) X-wo tatigui-suru ? X-wo taberu
X-ACC standing.up.eating-do X-ACC eat
?eat X standing up? ?eat X?
(8) X-de marineedo-suru ? X-wo ireru
X-INS marinade-do X-ACC pour
?marinate with X? ?pour X?
(9) X-ga NBA-iri-suru ? ? ? (was X-de (INS))
X-NOM NBA-entering-do
?X joins an NBA team?
? X-ga nyuudan-suru ? ? ? (was X-de)
X-NOM enrollment-do
?X joins a team?
4.5 Experiment 5: Verb Pairs form the Top
100,000
Finally, we examined the accuracy of the top
100,000 verb pairs acquired by Score and Preci-
sion. As Table 5 shows, Score outperformed Pre-
Method Acc-1 Acc-2 Acc-3
Score 0.610 0.480 0.300
Precision 0.470 0.295 0.190
Table 5: Accuracy of the top 100,000 verb pairs.
cision. Note also that Score kept a reasonable ac-
curacy for the top 100,000 results (Acc-2: 48%).
The accuracy is encouraging enough to consider
human annotation for the top 100,000 results to
produce a language resource for verb entailment,
which we actually plan to do.
Below are correct verb entailment examples
from the top 100,000 results of our method.
(10) The 121th pair
kaado-kessai-suru ? siharau
card-payment-do pay
?pay by card? ?pay?
(11) The 6,081th pair
saitei-suru ? sadameru
adjudicate-do settle
?adjudicate? ?settle?
(12) The 15,464th pair
eraa-syuuryou-suru ? jikkou-suru
error-termination-do perform-do
?abend? ?execute?
(13) The 30,044th pair
ribuuto-suru ? kidou-suru
reboot-do start-do
?reboot? ?boot?
(14) The 57,653th pair
rinin-suru ? syuunin-suru
resignation-do accession-do
?resign? ?accede?
(15) The 70,103th pair
sijou-tounyuu-suru ? happyou-suru
market-input-do publication-do
?bring to the market? ?publicize?
Below are examples of erroneous pairs from our
results. (16) is a causal relation but not an entail-
ment. (17) is a contradictory pair.
(16) The 5,475th pair
juken-suru ? goukaku-suru
take.an.exam-do acceptance-do
?take an exam? ?gain admission?
1179
(17) The 40,504th pair
ketujou-suru ? syutujou-suru
not.take.part-do take.part-do
?not take part? ?take part?
5 Conclusion
This paper addressed verb entailment acquisition
from the Web, and proposed a novel directional
similarity measure Score. Through a series of ex-
periments, we showed i) that Score outperforms
the previously proposed measures, Lin, Precision,
and BInc in large scale verb entailment acquisi-
tion, ii) that our proposed trick implemented in
Score
trick
significantly improves the accuracy of
verb entailment acquisition despite its simplicity,
iii) that Score worked better than the others even
when we restricted the target verbs to more fre-
quent ones, iv) that our method is also moder-
ately successful at producing template-level en-
tailment pairs, and v) that our method maintained
reasonable accuracy (in terms of human annota-
tion) for the top 100,000 results. As examples of
the acquired verb entailment pairs illustrated, our
method can acquire from an ocean of information,
namely the Web, a variety of verb entailment pairs
ranging from those that are used in daily life to
those that are used in very specific fields.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL2005),
pages 597?604.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceedings of
HLT-NAACL 2003, pages 16?23.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP2007), pages 161?170.
Atsushi Fujita and Satoshi Sato. 2008. A probabilis-
tic model for measuring grammaticality and similar-
ity of automatically generated paraphrases of pred-
icate phrases. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING2008), pages 225?232.
Maayan Geffet and Ido Dagan. 2005. The dis-
tributional inclusion hypotheses and lexical entail-
ment. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL2005), pages 107?114.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Ex-
tracting structural paraphrases from aligned mono-
lingual corpora. In Proceedings of the 2nd Interna-
tional Workshop on Paraphrasing (IWP2003), pages
57?64.
Daisuke Kawahara and Sadao Kurohashi. 2006a.
Case Frame Compilation from the Web using High-
Performance Computing. In Proceedings of The 5th
International Conference on Language Resources
and Evaluation (LREC-06), pages 1344?1347.
Daisuke Kawahara and Sadao Kurohashi. 2006b. A
Fully-Lexicalized Probabilistic Model for Japanese
Syntactic and Case Structure Analysis. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the
Association for Computational Linguistics (HLT-
NAACL2006), pages 176?183.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics (COLING-ACL1998), pages
768?774.
Viktor Pekar. 2006. Acquisition of verb entailment
from text. In Proceedings of the main confer-
ence on Human Language Technology Conference
of the North American Chapter of the Association
of Computational Linguistics (HLT-NAACL2006),
pages 49?56.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news
articles. In Proceedings of the 2nd international
Conference on Human Language Technology Re-
search (HLT2002), pages 313?318.
Idan Szpector, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP2004), pages 41?48.
Idan Szpektor and Ido Dagan. 2008. Learning en-
tailment rules for unary template. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics (COLING2008), pages 849?856.
Kentaro Torisawa. 2005. Automatic acquisition of ex-
pressions representing preparation and utilization of
an object. In Proceedings of the Recent Advances
in Natural Language Processing (RANLP05), pages
556?560.
1180
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using japanese cood-
inated sentences and noun-verb co-occurences. In
Proceedings of the Human Language Technology
Conference of the Norh American Chapter of the
ACL (HLT-NAACL2006), pages 57?64.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP2003), pages 81?88.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using se-
lectional preferences. In Proceedings of the 44th
Annual Meeting of the Association for Computa-
tional Linguistics and 21th InternationalConference
on Computational Linguistics (COLING-ACL2006),
pages 849?856.
1181
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 842?850,
Beijing, August 2010
Co-STAR: A Co-training Style Algorithm for Hyponymy Relation
Acquisition from Structured and Unstructured Text
Jong-Hoon Oh, Ichiro Yamada, Kentaro Torisawa, and Stijn De Saeger
Language Infrastructure Group, MASTAR Project,
National Institute of Information and Communications Technology (NICT)
{rovellia,iyamada,torisawa,stijn}@nict.go.jp
Abstract
This paper proposes a co-training style
algorithm called Co-STAR that acquires
hyponymy relations simultaneously from
structured and unstructured text. In Co-
STAR, two independent processes for hy-
ponymy relation acquisition ? one han-
dling structured text and the other han-
dling unstructured text ? collaborate by re-
peatedly exchanging the knowledge they
acquired about hyponymy relations. Un-
like conventional co-training, the two pro-
cesses in Co-STAR are applied to dif-
ferent source texts and training data.
We show the effectiveness of this al-
gorithm through experiments on large-
scale hyponymy-relation acquisition from
Japanese Wikipedia and Web texts. We
also show that Co-STAR is robust against
noisy training data.
1 Introduction
Acquiring semantic knowledge, especially se-
mantic relations between lexical terms, is re-
garded as a crucial step in developing high-level
natural language applications. This paper pro-
poses Co-STAR (a Co-training STyle Algorithm
for hyponymy Relation acquisition from struc-
tured and unstructured text). Similar to co-
training (Blum and Mitchell, 1998), two hy-
ponymy relation extractors in Co-STAR, one for
structured and the other for unstructured text, it-
eratively collaborate to boost each other?s perfor-
mance.
Many algorithms have been developed to auto-
matically acquire semantic relations from struc-
tured and unstructured text. Because term pairs
are encoded in structured and unstructured text in
different styles, different kinds of evidence have
been used for semantic relation acquisition:
Evidence from unstructured text: lexico-
syntactic patterns and distributional similar-
ity (Ando et al, 2004; Hearst, 1992; Pantel
et al, 2009; Snow et al, 2006; De Saeger et
al., 2009; Van Durme and Pasca, 2008);
Evidence from structured text: topic hierarchy,
layout structure of documents, and HTML
tags (Oh et al, 2009; Ravi and Pasca, 2008;
Sumida and Torisawa, 2008; Shinzato and
Torisawa, 2004).
Recently, researchers have used both structured
and unstructured text for semantic-relation acqui-
sition, with the aim of exploiting such different
kinds of evidence at the same time. They ei-
ther tried to improve semantic relation acquisition
by putting the different evidence together into a
single classifier (Pennacchiotti and Pantel, 2009)
or to improve the coverage of semantic relations
by combining and ranking the semantic relations
obtained from two source texts (Talukdar et al,
2008).
In this paper we propose an algorithm called
Co-STAR. The main contributions of this work
can be summarized as follows.
? Co-STAR is a semi-supervised learning
method composed of two parallel and iter-
ative processes over structured and unstruc-
tured text. It was inspired by bilingual co-
training, which is a framework for hyponymy
relation acquisition from source texts in two
languages (Oh et al, 2009). Like bilingual
co-training, two processes in Co-STAR op-
erate independently on structured text and
unstructured text. These two processes are
trained in a supervised manner with their
initial training data and then each of them
tries to enlarge the existing training data of
the other by iteratively exchanging what they
842
have learned (more precisely, by transfer-
ring reliable classification results on com-
mon instances to one another) (see Section
4 for comparison Co-STAR and bilingual
co-training). Unlike the ensemble semantic
framework (Pennacchiotti and Pantel, 2009),
Co-STAR does not have a single ?master?
classifier or ranker to integrate the differ-
ent evidence found in structured and unstruc-
tured text. We experimentally show that, at
least in our setting, Co-STAR works better
than a single ?master? classifier.
? Common relation instances found in both
structured and unstructured text act as a
communication channel between the two ac-
quisition processes. Each process in Co-
STAR classifies common relation instances
and then transfers its high-confidence classi-
fication results to training data of the other
process (as shown in Fig. 1), in order to im-
prove classification results of the other pro-
cess. Moreover, the efficiency of this ex-
change can be boosted by increasing the
?bandwidth? of this channel. For this pur-
pose each separate acquisition process auto-
matically generates a set of relation instances
that are likely to be negative. In our experi-
ments, we show that the above idea proved
highly effective.
? Finally, the acquisition algorithm we propose
is robust against noisy training data. We
show this by training one classifier in Co-
STAR with manually labeled data and train-
ing the other with automatically generated
but noisy training data. We found that Co-
STAR performs well in this setting. This is-
sue is discussed in Section 6.
This paper is organized as follows. Sections 2
and 3 precisely describe our algorithm. Section 4
describes related work. Sections 5 and 6 describe
our experiments and present their results. Conclu-
sions are drawn in Section 7.
2 Co-STAR
Co-STAR consists of two processes that simul-
taneously but independently extract and classify
Structured	 ?Texts	 Unstructured	 ?Texts	
Itera?on	
Training	 ?Data	 ?for	 ?Structured	 ?Texts	
Classifier	 Classifier	Training	 Training	
Enlarged	 ?	 ?Training	 ?Data	 ?for	 ?Structured	 ?Text	
Enlarged	 ?	 ?Training	 ?Data	 ?for	 ?Unstructured	 ?Texts	
Training	 ?Data	 ?For	 ?Unstructured	 ?Texts	
Classifier	Classifier	
Further	 ?Enlarged	 ?Training	 ?Data	 ?for	 ?Structured	 ?Texts	
Further	 ?Enlarged	 ?Training	 ?Data	 ?for	 ?Unstructured	 ?Texts	
Training	
Training	 Training	
Training	
?..	 ?..	Common	 ?instances	Transferring	 ?reliable	 ?classifica?on	 ?results	 ?of	 ?classifiers	
Transferring	 ?reliable	 ?classifica?on	 ?results	 ?of	 ?classifiers	
Figure 1: Concept of Co-STAR.
hyponymy relation instances from structured and
unstructured text. The core of Co-STAR is the
collaboration between the two processes, which
continually exchange and compare their acquired
knowledge on hyponymy relations. This collabo-
ration is made possible through common instances
shared by both processes. These common in-
stances are classified separately by each process,
but high-confidence classification results by one
process can be transferred as new training data to
the other.
2.1 Common Instances
Let S and U represent a source (i.e. corpus)
of structured and unstructured text, respectively.
In this paper, we use the hierarchical layout of
Wikipedia articles and the Wikipedia category
system as structured text S (see Section 3.1), and
a corpus of ordinary Web pages as unstructured
text U . Let XS and XU denote a set of hyponymy
relation candidates extracted from S and U , re-
spectively. XS is extracted from the hierarchi-
cal layout of Wikipedia articles (Oh et al, 2009)
and XU is extracted by lexico-syntactic patterns
for hyponymy relations (i.e., hyponym such as hy-
ponymy) (Ando et al, 2004) (see Section 3 for a
detailed explanation)
We define two types of common instances,
called ?genuine? common instances (G) and ?vir-
tual? common instances (V ). The set of common
instances is denoted by Y = G ? V . Genuine
common instances are hyponymy relation candi-
dates found in both S and U (G = XS ?XU ). On
843
the other hand, term pairs are obtained as virtual
common instances when:
? 1) they are extracted as hyponymy relation
candidates in either S or U and;
? 2) they do not seem to be a hyponymy rela-
tion in the other text
The first condition corresponds to XS ? XU .
Term pairs satisfying the second condition are de-
fined as RS and RU , where RS ? XS = ? and
RU ?XU = ?.
RS contains term pairs that are found in the
Wikipedia category system but neither term ap-
pears as ancestor of the other1. For example, (nu-
trition,protein) and (viruses,viral disease), respec-
tively, hold a category-article relation, where nu-
trition is not ancestor of viruses and vice versa in
the Wikipedia category system. Here, term pairs,
such as (nutrition, viruses) and (viral disease, nu-
trition), can be ones in RS .
RU is a set of term pairs extracted from U
when:
? they are not hyponymy relation candidates in
XU and;
? they regularly co-occur in the same sentence
as arguments of the same verb (e.g., A cause
B or A is made by B);
As a result, term pairs in RU are thought as hold-
ing some other semantic relations (e.g., A and B
in ?A cause B? may hold a cause/effect relation)
than hyponymy relation. Finally, virtual common
instances are defined as:
? V = (XS ?XU ) ? (RS ?RU )
The virtual common instances, from the view-
point of either S or U , are unlikely to hold a hy-
ponymy relation even if they are extracted as hy-
ponymy relation candidates in the other text. Thus
many virtual common instances would be a nega-
tive example for hyponymy relation acquisition.
On the other hand, genuine common instances
(hyponymy relation candidates found in both S
1A term pair often holds a hyponymy relation if one term
in the term pair is a parent of the other in the Wikipedia cat-
egory system (Suchanek et al, 2007).
and U ) are more likely to hold a hyponymy re-
lation than virtual common instances.
In summary, genuine and virtual common in-
stances can be used as different ground for collab-
oration as well as broader collaboration channel
between the two processes than genuine common
instances used alone.
2.2 Algorithm
We assume that classifier c assigns class label
cl ? {yes, no} (?yes? (hyponymy relation) or
?no? (not a hyponymy relation)) to instances in
x ? X with confidence value r ? R+, a non-
negative real number. We denote the classifica-
tion result by classifier c as c(x) = (x, cl, r). We
used support vector machines (SVMs) in our ex-
periments and the absolute value of the distance
between a sample and the hyperplane determined
by the SVMs as confidence value r.
1: Input: Common instances (Y = G ? V ) and
the initial training data (L0S and L0U )
2: Output: Two classifiers (cnS and cnU )
3: i = 0
4: repeat
5: ciS := LEARN(LiS)
6: ciU := LEARN(LiU )
7: CRiS := {ciS(y)|y ? Y , y /? LiS ? LiU}
8: CRiU := {ciU (y)|y ? Y , y /? LiS ? LiU}
9: for each (y, clS , rS) ? TopN(CRiS) and
(y, clU , rU ) ? CRiU do
10: if (rS > ? and rU < ?)
or (rS > ? and clS = clU ) then
11: L(i+1)U := L
(i+1)
U ? {(y, clS)}
12: end if
13: end for
14: for each (y, clU , rU ) ? TopN(CRiU ) and
(y, clS , rS) ? CRiS do
15: if (rU > ? and rS < ?)
or (rU > ? and clS = clU ) then
16: L(i+1)S := L
(i+1)
S ? {(y, clU )}
17: end if
18: end for
19: i = i+ 1
20: until stop condition is met
Figure 2: Co-STAR algorithm
844
The Co-STAR algorithm is given in Fig. 2. The
algorithm is interpreted as an iterative procedure
1) to train classifiers (ciU , ciS) with the existing
training data (LiS and LiU ) and 2) to select new
training instances from the common instances to
be added to existing training data. These are re-
peated until stop condition is met.
In the initial stage, two classifiers c0S and c0U
are trained with manually prepared labeled in-
stances (or training data) L0S and L0U , respec-
tively. The learning procedure is denoted by
c = LEARN(L) in lines 5?6, where c is a re-
sulting classifier. Then ciS and ciU are applied
to classify common instances in Y (lines 7?8).
We denote CRiS as a set of the classification re-
sults of ciS for common instances, which are not
included in the current training data LiS ? LiU .
Lines 9?13 describe a way of selecting instances
in CRiS to be added to the existing training data
in U . During the selection, ciS acts as a teacher
and ciU as a student. TopN(CRiS) is a set of
ciS(y) = (y, clS , rS), whose rS is the top-N high-
est in CRiS . (In our experiments, N = 900.) The
teacher instructs his student the class label of y if
the teacher can decide the class label of y with a
certain level of confidence (rS > ?) and the stu-
dent satisfies one of the following two conditions:
? the student agrees with the teacher on class
label of y (clS = clU ) or
? the student?s confidence in classifying y is
low (rU < ?)
rU < ? enables the teacher to instruct his student
in spite of their disagreement over a class label.
If one of the two conditions is satisfied, (y, clS)
is added to existing labeled instances L(i+1)U . The
roles are reversed in lines 14?18, so that ciU be-
comes the teacher and ciS the student.
The iteration stops if the change in the differ-
ence between the two classifiers is stable enough.
The stability is estimated by d(ciS , ciU ) in Eq. (1),
where ?i represents the change in the average
difference between the confidence values of the
two classifiers in classifying common instances.
We terminate the iteration if d(ciS , ciU ) is smaller
than 0.001 in three consecutive rounds (Wang and
Zhou, 2007).
d(ciS , ciU ) = |?i ? ?(i?1)|/|?(i?1)| (1)
3 Hyponymy Relation Acquisition
In this section we explain how each process ex-
tracts hyponymy relations from its respective text
source either Wikipedia or Web pages. Each pro-
cess extracts hyponymy relation candidates (de-
noted by (hyper,hypo) in this section). Because
there are many non-hyponymy relations in these
candidates2, we classify hyponymy relation can-
didates into correct hyponymy relation or not. We
used SVMs (Vapnik, 1995) for the classification
in this paper.
3.1 Acquisition from Wikipedia
(a) Layout structure
Range
Siberian tiger
Bengal tiger
Subspecies
Taxonomy
Tiger
Malayan tiger
(b) Tree structure
Figure 3: Example borrowed from Oh et al
(2009): Layout and tree structures of Wikipedia
article TIGER
We follow the method in Oh et al (2009) for
acquiring hyponymy relations from the Japanese
Wikipedia. Every article is transformed into a tree
structure as shown in Fig. 3, based on the items in
its hierarchical layout including title, (sub)section
headings, and list items. Candidate relations are
extracted from this tree structure by regarding a
node as a hypernym candidate and all of its subor-
dinate nodes as potential hyponyms of the hyper-
nym candidate (e.g., (TIGER, TAXONOMY) and
(TIGER, SIBERIAN TIGER) from Fig. 3). We ob-
tained 1.9?107 Japanese hyponymy relation can-
didates from Wikipedia.
2Only 25?30% of candidates was true hyponymy relation
in our experiments.
845
Type Description
Feature from Wikipedia Lexical Morphemes and POS of hyper and hypo; hyper and hypo themselves
(?WikiFeature?) Structure Distance between hyper and hypo in a tree structure;
Lexical patterns for article or section names, where listed items often appear;
Frequently used section headings in Wikipedia (e.g., ?Reference?);
Layout item type (e.g., section or list); Tree node type (e.g., root or leaf);
Parent and children nodes of hyper and hypo
Infobox Attribute type and its value obtained from Wikipedia infoboxes
Feature from Web texts Lexical Morphemes and POS of hyper and hypo; hyper and hypo themselves
(?WebFeature?) Pattern Lexico-syntactic patterns applied to hyper and hypo;
PMI score between pattern and hyponymy relation candidate (hyper,hypo)
Collocation PMI score between hyper and hypo
Noun Class Noun classes relevant to hyper and hypo
Table 1: Feature sets (WikiFeature and WebFeature): hyper and hypo represent hypernym and hyponym
parts of hyponymy relation candidates, respectively.
As features for classification we used lex-
ical, structure, and infobox information from
Wikipedia (WikiFeature), as shown in Table 1.
Because they are the same feature sets as those
used in Oh et al (2009), here we just give a brief
overview of the feature sets. Lexical features3
are used to recognize the lexical evidence for
hyponymy relations encoded in hyper and hypo.
For example, the common head morpheme tiger
in (TIGER, BENGAL TIGER) can be used as the
lexical evidence. Such information is provided
along with the words/morphemes and the parts of
speech of hyper and hypo, which can be multi-
word/morpheme nouns.
Structure features provide evidence found in
layout or tree structures for hyponymy relations.
For example, hyponymy relations (TIGER, BEN-
GAL TIGER) and (TIGER,MALAYAN TIGER) can
be obtained from tree structure ?(root node, chil-
dren nodes of Subspecies)? in Fig 3.
3.2 Acquisition from Web Texts
As the target for hyponymy relation acquisition
from the Web, we used 5 ? 107 pages from
the TSUBAKI corpus (Shinzato et al, 2008),
a 108 page Japanese Web corpus that was de-
pendency parsed with KNP (Kurohashi-Nagao
Parser) (Kurohashi and Kawahara, 2005). Hy-
ponymy relation candidates are extracted from the
corpus based on the lexico-syntactic patterns such
as ?hypo nado hyper (hyper such as hypo)? and
?hypo to iu hyper (hyper called hypo)? (Ando
3MeCab (http://mecab.sourceforge.net/)
was used to provide the lexical features.
et al, 2004). We extracted 6 ? 106 Japanese
hyponymy relation candidates from the Japanese
Web texts. Features (WebFeature) used for classi-
fication are summarized in Table 1. Similar to the
hyponymy relation acquisition from Wikipedia,
lexical features are used to recognize the lexical
evidence for hyponymy relations.
Lexico-syntactic patterns for hyponymy rela-
tion show different coverage and accuracy in hy-
ponymy relation acquisition (Ando et al, 2004).
Further if multiple lexico-syntactic patterns sup-
port acquisition of hyponymy relation candidates,
these candidates are more likely to be actual hy-
ponymy relations. The pattern feature of hy-
ponymy relation candidates is used for these ev-
idence.
We use PMI (point-wise mutual information)
of hyponymy relation candidate (hyper, hypo) as
a collocation feature (Pantel and Ravichandran,
2004), where we assume that hyper and hypo in
candidates would frequently co-occur in the same
sentence if they hold a hyponymy relation.
Semantic noun classes have been regarded as
useful information in semantic relation acquisi-
tion (De Saeger et al, 2009). EM-based clus-
tering (Kazama and Torisawa, 2008) is used for
obtaining 500 semantic noun classes4 from 5 ?
105 nouns (including single-word and multi-word
ones) and their 4? 108 dependency relations with
5 ? 105 verbs and other nouns in our target Web
4Because EM clustering provides a probability distri-
bution over noun class nc, we obtain discrete classes of
each noun n with a probability threshold p(nc|n) ?
0.2 (De Saeger et al, 2009).
846
Co-training Bilingual co-training Co-STAR
(Blum and Mitchell, 1998) (Oh et al, 2009) (Proposed method)
Instance space Same Different Almost different
Feature space Split by human decision Split by languages Split by source texts
Common instances Genuine-common Genuine-common Genuine-common and
(or All unlabeled) instances instances (Translatable) virtual-common instances
Table 2: Differences among co-training, bilingual co-training, and Co-STAR
corpus. For example, noun class C311 includes
biological or chemical substances such as tatou
(polysaccharide) and yuukikagoubutsu (organic
compounds). Noun classes (i.e., C311) relevant to
hyper and hypo, respectively, are used as a noun
class feature.
4 Related Work
There are two frameworks, which are most rele-
vant to our work ? bilingual co-training and en-
semble semantics.
The main difference between bilingual co-
training and Co-STAR lies in an instance space.
In bilingual co-training, instances are in different
spaces divided by languages while, in Co-STAR,
many instances are in different spaces divided by
their source texts. Table 2 shows differences be-
tween co-training, bilingual co-training and Co-
STAR.
Ensemble semantics is a relation acquisition
framework, where semantic relation candidates
are extracted from multiple sources and a single
ranker ranks or classifies the candidates in the fi-
nal step (Pennacchiotti and Pantel, 2009). In en-
semble semantics, one ranker is in charge of rank-
ing all candidates extracted from multiple sources;
while one classifier classifies candidates extracted
from one source in Co-STAR.
5 Experiments
We used the July version of Japanese Wikipedia
(jawiki-20090701) as structured text. We ran-
domly selected 24,000 hyponymy relation candi-
dates from those identified in Wikipedia and man-
ually checked them. 20,000 of these samples were
used as training data for our initial classifier, the
rest was equally divided into development and test
data for Wikipedia. They are called ?WikiSet.?
As unstructured text, we used 5 ? 107 Japanese
Web pages in the TSUBAKI corpus (Shinzato et
al., 2008). Here, we manually checked 9,500
hyponymy relation candidates selected randomly
from Web texts. 7,500 of these were used as train-
ing data. The rest was split into development and
test data. We named this data ?WebSet?.
In both classifiers, the development data was
used to select the optimal parameters, and the test
data was used to evaluate our system. We used
TinySVM (TinySVM, 2002) with a polynomial
kernel of degree 2 as a classifier. ? (the threshold
value indicating high confidence), ? (the thresh-
old value indicating low confidence), and TopN
(the maximum number of training instances to be
added to the existing training data in each iter-
ation) were selected through experiments on the
development set. The combination of ? = 1,
? = 0.3, and TopN=900 showed the best perfor-
mance and was used in the following experiments.
Evaluation was done by precision (P ), recall (R),
and F-measure (F ).
5.1 Results
We compare six systems. Three of these, B1?B3,
show the effect of different feature sets (?Wik-
iFeature? and ?WebFeature? in Table 1) and dif-
ferent training data. We trained two separate clas-
sifiers in B1 and B2, while we integrated feature
sets and training data for training a single classi-
fier in B3. The classifiers in these three systems
are trained with manually prepared training data
(?WikiSet? and ?WebSet?). For the purpose of our
experiment, we consider B3 as the closest possible
approximation of the ensemble semantics frame-
work (Pennacchiotti and Pantel, 2009).
? B1 consists of two completely independent
classifiers. Both S and U classifiers are
trained and tested on their own feature and
data sets (respectively ?WikiSet + WikiFea-
ture? and ?WebSet + WebFeature?).
847
? B2 is the same as B1, except that both clas-
sifiers are trained with all available training
data ? WikiSet and WebSet are combined
(27,500 training instances in total). However,
each classifier only uses its own feature set
(WikiFeature or WebFeature)5.
? B3 adds a master classifier to B1. This third
classifier is trained on the complete 27,500
training instances (same as B2) using all
available features from Table 1, including
each instance?s SVM scores obtained from
the two B1 classifiers6. The verdict of the
master classifier is considered to be the final
classification result.
The other three systems, BICO, Co-B, and Co-
STAR (our proposed method), are for compari-
son between bilingual co-training (Oh et al, 2009)
(BICO) and variants of Co-STAR (Co-B and Co-
STAR). Especially, we prepared Co-B and Co-
STAR to show the effect of different configura-
tions of common instances on the Co-STAR al-
gorithm. We use both B1 and B2 as the initial
classifiers of Co-B and Co-STAR. We notate Co-
B and Co-STAR without ??? when B1 is used as
their initial classifier and those with ??? when B2
is used.
? BICO implements the bilingual co-training
algorithm of (Oh et al, 2009), in which
two processes collaboratively acquire hy-
ponymy relations in two different languages.
For BICO, we prepared 20,000 English and
20,000 Japanese training samples (Japanese
ones are the same as training data in the
WikiSet) by hand.
? Co-B is a variant of Co-STAR that uses only
the genuine-common instances as common
instances (67,000 instances)7, to demonstrate
5Note that training instances from WebSet (or WikiSet)
can have WikiFeature (or WebFeature) if they also appear
in Wikipedia (or Web corpus). But they can always have
lexical feature, the common feature set between WikiFeature
and WebFeature.
6SVM scores are assigned to the instances in training data
in a 10-fold cross validation manner.
7Co-B can be considered as conventional co-
training (Blum and Mitchell, 1998) in the sense that
two classifiers collaborate through actual common instances.
the effectiveness of the virtual common in-
stances.
? Co-STAR is our proposed method, which
uses both genuine-common and virtual-
common instances (643,000 instances in to-
tal).
WebSet WikiSet
P R F P R F
B1 84.3 65.2 73.5 87.8 74.7 80.7
B2 83.4 69.6 75.9 87.4 79.5 83.2
B3 82.2 72.0 76.8 86.1 77.7 81.7
BICO N/A N/A N/A 84.5 81.8 83.1
Co-B 86.2 63.5 73.2 89.7 74.1 81.2
Co-B? 85.5 69.9 77.0 89.6 76.5 82.5
Co-STAR 85.9 76.0 80.6 88.0 81.8 84.8
Co-STAR? 83.3 80.7 82.0 87.6 81.8 84.6
Table 3: Comparison of different systems
Table 3 summarizes the result. Features for
common instances in Co-B and Co-STAR are pre-
pared in the same way as training data in B2, so
that both classifiers can classify the common in-
stances with their trained feature sets.
Comparison between B1?B3 shows that B2 and
B3 outperform B1 in F-measure. More train-
ing data used in B2?B3 (27,500 instances for
both WebSet and WikiSet) results in higher per-
formance than that of B1 (7,500 and 20,000 in-
stances used separately). We think that the lexical
features, assigned regardless of source text to in-
stances in B2?B3, are mainly responsible for the
performance gain over B1, as they are the least
domain-dependent type of features. B2?B3 are
composed of different number of classifiers, each
of which is trained with different feature sets and
training instances. Despite this difference, B2 and
B3 showed similar performance in F-measure.
Co-STAR outperformed the algorithm similar
to the ensemble semantics framework (B3), al-
though we admit that a more extensive com-
parison is desirable. Further Co-STAR outper-
formed BICO. While the manual cost for build-
ing the initial training data used in Co-STAR
and BICO is hard to quantify, Co-STAR achieves
better performance with fewer training data in
total (27,500 instances) than BICO (40,000 in-
stances). The difference in performance between
Co-B and Co-STAR shows the effectiveness of
848
the automatically generated virtual-common in-
stances. From these comparison, we can see that
virtual-common instances coupled with genuine-
common instances can be leveraged to enable
more effective collaboration between the two clas-
sifiers in Co-STAR.
As a result, our proposed method outperforms
the others in F-measure by 1.4?8.5%. We ob-
tained 4.3 ? 105 hyponymy relations from Web
texts and 4.6? 106 ones from Wikipedia8.
6 Co-STAR with Automatically
Generated Training Data
For Co-STAR, we need two sets of manually pre-
pared training data, one for structured text and the
other for unstructured text. As in any other su-
pervised system, the cost of preparing the training
data is an important issue. We therefore investi-
gated whether Co-STAR can be trained for a lower
cost by generating more of its training data auto-
matically.
We automatically built training data for Web
texts by using definition sentences9 and category
names in the Wikipedia articles, while we stuck to
manually prepared training data for Wikipedia. To
obtain hypernyms from Wikipedia article names,
we used definition-specific lexico-syntactic pat-
terns such as ?hyponym is hypernym? and ?hy-
ponym is a type of hypernym? (Kazama and Tori-
sawa, 2007; Sumida and Torisawa, 2008). Then,
we extracted hyponymy relations consisting of
pairs of Wikipedia category names and their mem-
ber articles when the Wikipedia category name
and the hypernym obtained from the definition
of the Wikipedia article shared the same head
word. Next, we selected a subset of the extracted
hyponymy relations that are also hyponymy re-
lation candidates in Web texts, as positive in-
stances for hyponymy relation acquisition from
Web text. We obtained around 15,000 positive in-
stances in this way. Negative instances were cho-
sen from virtual-common instances, which also
originated from the Wikipedia category system
and hyponymy relation candidates in Web texts
8We obtained them with 90% precision by setting the
SVM score threshold to 0.23 for Web texts and 0.1 for
Wikipedia.
9The first sentences of Wikipedia articles.
(around 293,000 instances).
The automatically built training data was noisy
and its size was much bigger than manually pre-
pared training data in WebSet. Thus 7,500 in-
stances as training data (the same number of man-
ually built training data in WebSet) were ran-
domly chosen from the positive and negative in-
stances with a positive:negative ratio of 1:410.
WebSet WikiSet
P R F P R F
B1 81.0 47.6 60.0 87.8 74.7 80.7
B2 80.0 55.4 65.5 87.1 79.5 83.1
B3 82.0 33.7 47.8 87.1 75.6 81.0
Co-STAR 82.2 60.8 69.9 87.3 80.7 83.8
Co-STAR? 79.2 69.6 74.1 87.0 81.8 84.4
Table 4: Results with automatically generated
training data
With the automatically built training data for
Web texts and manually prepared training data for
Wikipedia, we evaluated B1?B3 and Co-STAR,
which are the same systems in Table 3. The results
in Table 4 are encouraging. Co-STAR was robust
even when faced with noisy training data. Further
Co-STAR showed better performance than B1?
B3, although its performance in Table 4 dropped a
bit compared to Table 3. This result shows that we
can reduce the cost of manually preparing training
data for Co-STAR with only small loss of the per-
formance.
7 Conclusion
This paper proposed Co-STAR, an algorithm for
hyponymy relation acquisition from structured
and unstructured text. In Co-STAR, two indepen-
dent processes of hyponymy relation acquisition
from structured texts and unstructured texts, col-
laborate in an iterative manner through common
instances. To improve this collaboration, we in-
troduced virtual-common instances.
Through a series of experiments, we showed
that Co-STAR outperforms baseline systems and
virtual-common instances can be leveraged to
achieve better performance. We also showed that
Co-STAR is robust against noisy training data,
which requires less human effort to prepare it.
10We select the ratio by testing different ratio from 1:2 to
1:5 with our development data in WebSet and B1.
849
References
Ando, Maya, Satoshi Sekine, and Shun Ishiza. 2004.
Automatic extraction of hyponyms from Japanese
newspaper using lexico-syntactic patterns. In Proc.
of LREC ?04.
Blum, Avrim and Tom Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training. In
COLT? 98: Proceedings of the eleventh annual con-
ference on Computational learning theory, pages
92?100.
De Saeger, Stijn, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large
scale relation acquisition using class dependent pat-
terns. In Proc. of ICDM 2009, pages 764?769.
Hearst, Marti A. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics,
pages 539?545.
Kazama, Jun?ichi and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proc. of Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing, pages 698?707.
Kazama, Jun?ichi and Kentaro Torisawa. 2008. In-
ducing gazetteers for named entity recognition by
large-scale clustering of dependency relations. In
Proceedings of ACL-08: HLT, pages 407?415.
Kurohashi, Sadao and Daisuke Kawahara. 2005. KNP
(Kurohashi-Nagao Parser) 2.0 users manual.
Oh, Jong-Hoon, Kiyotaka Uchimoto, and Kentaro
Torisawa. 2009. Bilingual co-training for mono-
lingual hyponymy-relation acquisition. In Proc. of
ACL-09: IJCNLP, pages 432?440.
Pantel, Patrick and Deepak Ravichandran. 2004. Au-
tomatically labeling semantic classes. In Proc. of
HLT-NAACL ?04, pages 321?328.
Pantel, Patrick, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP ?09, pages 938?947.
Pennacchiotti, Marco and Patrick Pantel. 2009. En-
tity extraction via ensemble semantics. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 238?247.
Ravi, Sujith and Marius Pasca. 2008. Using structured
text for large-scale attribute extraction. In CIKM-
08, pages 1183?1192.
Shinzato, Keiji and Kentaro Torisawa. 2004. Ex-
tracting hyponyms of prespecified hypernyms from
itemizations and headings in web documents. In
Proceedings of COLING ?04, pages 938?944.
Shinzato, Keiji, Tomohide Shibata, Daisuke Kawa-
hara, Chikara Hashimoto, and Sadao Kurohashi.
2008. Tsubaki: An open search engine infrastruc-
ture for developing new information access. In Pro-
ceedings of IJCNLP ?08, pages 189?196.
Snow, Rion, Daniel Jurafsky, and Andrew Y. Ng.
2006. Semantic taxonomy induction from heteroge-
nous evidence. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 801?808.
Suchanek, Fabian M., Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In Proc. of WWW ?07, pages 697?706.
Sumida, Asuka and Kentaro Torisawa. 2008. Hack-
ing Wikipedia for hyponymy relation acquisition.
In Proc. of the Third International Joint Conference
on Natural Language Processing (IJCNLP), pages
883?888, January.
Talukdar, Partha Pratim, Joseph Reisinger, Marius
Pasca, Deepak Ravichandran, Rahul Bhagat, and
Fernando Pereira. 2008. Weakly-supervised acqui-
sition of labeled class instances using graph random
walks. In Proc. of EMNLP08, pages 582?590.
TinySVM. 2002. http://chasen.org/?taku/
software/TinySVM.
Van Durme, Benjamin and Marius Pasca. 2008. Find-
ing cars, goddesses and enzymes: Parametrizable
acquisition of labeled instances for open-domain in-
formation extraction. In Proc. of AAAI08, pages
1243?1248.
Vapnik, Vladimir N. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Wang, Wei and Zhi-Hua Zhou. 2007. Analyzing co-
training style algorithms. In ECML ?07: Proceed-
ings of the 18th European conference on Machine
Learning, pages 454?465.
850
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 825?835,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Relation Acquisition using Word Classes and Partial Patterns
Stijn De Saeger?? Kentaro Torisawa? Masaaki Tsuchida? Jun?ichi Kazama?
Chikara Hashimoto? Ichiro Yamada? Jong Hoon Oh? Istva?n Varga? Yulan Yan?
? Information Analysis Laboratory, National Institute of
Information and Communications Technology, 619-0289 Kyoto, Japan
{stijn,torisawa,kazama,ch,rovellia,istvan,yulan}@nict.go.jp
? Information and Media Processing Laboratories, NEC Corporation, 630-0101 Nara, Japan
m-tsuchida@cq.jp.nec.com
? Human & Information Science Research Division,
NHK Science & Technology Research Laboratories, 157-8510 Tokyo, Japan
yamada.i-hy@nhk.or.jp
Abstract
This paper proposes a semi-supervised rela-
tion acquisition method that does not rely on
extraction patterns (e.g. ?X causes Y? for
causal relations) but instead learns a combi-
nation of indirect evidence for the target re-
lation ? semantic word classes and partial
patterns. This method can extract long tail
instances of semantic relations like causality
from rare and complex expressions in a large
JapaneseWeb corpus? in extreme cases, pat-
terns that occur only once in the entire cor-
pus. Such patterns are beyond the reach of cur-
rent pattern based methods. We show that our
method performs on par with state-of-the-art
pattern based methods, and maintains a rea-
sonable level of accuracy even for instances
acquired from infrequent patterns. This abil-
ity to acquire long tail instances is crucial for
risk management and innovation, where an ex-
haustive database of high-level semantic rela-
tions like causation is of vital importance.
1 Introduction
Pattern based relation acquisition methods rely on
lexico-syntactic patterns (Hearst, 1992) for extract-
ing relation instances. These are templates of natu-
ral language expressions such as ?X causes Y ? that
signal an instance of some semantic relation (i.e.,
causality). Pattern based methods (Agichtein and
Gravano, 2000; Pantel and Pennacchiotti, 2006b;
Pas?ca et al, 2006; De Saeger et al, 2009) learn many
? This work was done when all authors were at the National
Institute of Information and Communications Technology.
such patterns to extract new instances (word pairs)
from the corpus.
However, since extraction patterns are learned us-
ing statistical methods that require a certain fre-
quency of observations, pattern based methods fail
to capture relations from complex expressions in
which the pattern connecting the two words is rarely
observed. Consider the following sentence:
?Curing hypertension alleviates the deteriora-
tion speed of the renal function, thereby lower-
ing the risk of causing intracranial bleeding?
Humans can infer that this sentence expresses a
causal relation between the underlined noun phrases.
But the actual pattern connecting them, i.e., ?Cur-
ing X alleviates the deterioration speed of the re-
nal function, thereby lowering the risk of causing
Y ?, is rarely observed more than once even in a 108
page Web corpus. In the sense that the term pat-
tern implies a recurring event, this expression con-
tains no pattern for detecting the causal relation be-
tween hypertension and intracranial bleeding. This
is what we mean by ?long tail instances? ? words
that co-occur infrequently, and only in sparse extrac-
tion contexts.
Yet an important application of relation extraction
is mining the Web for so-called unknown unknowns
? in the words of D. Rumsfeld, ?things we don?t
know we don?t know? (Torisawa et al, 2010). In
knowledge discovery applications like risk manage-
ment and innovation, the usefulness of relation ex-
traction lies in its ability to find many unexpected
remedies for diseases, causes of social problems,
and so on. To give an example, our relation extrac-
825
tion system found a blog post mentioning Japanese
automaker Toyota as a hidden cause of Japan?s de-
flation. Several months later the same connection
was made in an article published in an authoritative
economic magazine.
We propose a semi-supervised relation extraction
method that does not rely on direct pattern evidence
connecting the two words in a sentence. We argue
that the role of binary patterns can be replaced by a
combination of two types of indirect evidence: se-
mantic class information about the target relation
and partial patterns, which are fragments or sub-
patterns of binary patterns. The intuition is this: if
a sentence like the example sentence above contains
some wordX belonging to the class of medical con-
ditions and another word Y from the class of trau-
mas, and X matches the partial pattern ?. . . causing
X?, there is a decent chance that this sentence ex-
presses a causal relation between X and Y . We
show that just using this combination of indirect
evidence we can pick up semantic relations with
roughly 50% precision, regardless of the complexity
or frequency of the expression in which the words
co-occur. Furthermore, by combining this idea with
a straightforward machine learning approach, the
overall performance of our method is on par with
state-of-the-art pattern based methods. However,
our method manages to extract a large number of
instances from sentences that contain no pattern that
can be learned by pattern induction methods.
Our method is a two-stage system. Figure 1
presents an overview. In Stage 1 we apply a state-
of-the-art pattern based relation extractor to a Web
corpus to obtain an initial batch of relation instances.
In Stage 2 a supervised classifier is built from vari-
ous components obtained from the output of Stage
1. Given the output of Stage 1 and access to a
Web corpus, the Stage 2 extractor is completely
self-sufficient, and the whole method requires no
supervision other than a handful of seed patterns
to start the first stage extractor. The whole proce-
dure is therefore minimally supervised. Semantic
word classes and partial patterns play a crucial role
throughout all steps of the process.
We evaluate our method on three relation acqui-
sition tasks (causation, prevention and material re-
lations) using a 600 million Japanese Web page cor-
Figure 1: Proposed method: data flow.
pus (Shinzato et al, 2008) and show that our sys-
tem can successfully acquire relations from both
frequent and infrequent patterns. Our system ex-
tracted 100,000 causal relations with 84.6% preci-
sion, 50,000 prevention relations with 58.4% preci-
sion and 25,000 material relations with 76.1% preci-
sion. In the extreme case, we acquired several thou-
sand word pairs co-occurring only in patterns that
appear once in the entire corpus. We call such pat-
terns single occurrence (SO) patterns. Word pairs
that co-occur only with SO patterns represent the
theoretical limiting case of relations that cannot be
acquired using existing pattern based methods. In
this sense our method can be seen as complemen-
tary with pattern based approaches, and merging our
method?s output with that of a pattern based method
may be beneficial.
2 Stage 1 Extractor
This section introduces our Stage 1 extractor: the
pattern based method from (De Saeger et al, 2009),
which we call CDP for ?class dependent patterns?.
We give a brief overview below, and refer the reader
to the original paper for a more comprehensive ex-
planation.
CDP takes a set of seed patterns as input, and au-
tomatically learns new class dependent patterns as
paraphrases of the seed patterns. Class dependent
patterns are semantic class restricted versions of or-
dinary lexico-syntactic patterns. Existing methods
use class independent patterns such as ?X causes
Y ? to learn causal relations betweenX and Y . Class
dependent patterns however place semantic class re-
826
strictions on the noun pairs they may extract, like
?Yaccidents causes Xincidents?. The accidents and
incidents subscripts specify the semantic class of the
X and Y slot fillers.
These class restrictions make it possible to distin-
guish between multiple senses of highly ambiguous
patterns (so-called ?generic? patterns). For instance,
given the generic pattern ?Y by X?, if we restrict
Y and X in to the semantic classes of injuries and
accidents (as in ?death by drowning?), the class de-
pendent pattern ?Yinjuries by Xaccidents? becomes a
valid paraphrase of ?X causes Y ? and can safely be
used to extract causal relations, whereas other class
dependent versions of the same generic pattern (e.g.,
?Yproducts byXcompanies?, as in ?iPhone by Apple?)
may not.
CDP ranks each noun pair in the corpus accord-
ing to a score that reflects its likelihood of being
a proper instance of the target relation, by calcu-
lating the semantic similarity of a set of seed pat-
terns to the class dependent patterns this noun pair
co-occurs with. The output of CDP is a list of noun
pairs ranked by score, together with the highest scor-
ing class dependent pattern each noun pair co-occurs
with. This list becomes the input to Stage 2 of our
method, as shown in Figure 1. We adopted CDP as
Stage 1 extractor because, besides having generally
good performance, the class dependent patterns pro-
vide the two fundamental ingredients for Stage 2 of
our method ? the target semantic word classes for a
given relation (in the form of the semantic class re-
strictions attached to patterns), and partial patterns.
To obtain fine-grained semantic word classes we
used the large scale word clustering algorithm from
(Kazama and Torisawa, 2008), which uses the EM
algorithm to compute the probability that a word w
belongs to class c, i.e., P (c|w). Probabilistic cluster-
ing defines no discrete boundary between members
and non-members of a semantic class, so we simply
assume w belongs to c whenever P (c|w) ? 0.2. For
this work we clustered 106 nouns into 500 classes.
Finally, we adopt the structural representation of
patterns introduced in (Lin and Pantel, 2001). All
sentences in our corpus are dependency parsed, and
patterns consist of words on the path of dependency
relations connecting two nouns.
3 Stage 2 Extractor
We use CDP as our Stage 1 extractor, and the top
N noun pairs along with the class dependent pat-
terns that extract them are given as input to Stage 2,
which represents the main contribution of this work.
As shown in Figure 1, Stage 2 consists of three mod-
ules: a candidate generator, a training data gener-
ator and a supervised classifier. The training data
generator builds training data for the classifier from
the top N output of CDP and sentences retrieved
from the Web corpus. This classifier then scores and
ranks the candidate relations generated by the can-
didate relation generator. We introduce each module
below.
Candidate Generator This module generates
sentences containing candidate word pairs for the
target relation from the corpus. It does so using the
semantic class restrictions and partial patterns ob-
tained from the output of CDP. The set of all seman-
tic class pairs obtained from the class dependent pat-
terns that extracted the topN results become the tar-
get semantic class pairs from which new candidate
instances are generated. We extract all sentences
containing a word pair belonging to one of the target
class pairs from the corpus.
From these sentences we keep only those that con-
tain a trace of evidence for the target semantic re-
lation. For this we decompose the class dependent
patterns from the Stage 1 extractor into partial pat-
terns. As mentioned previously, patterns consist of
words on the path of dependency relations connect-
ing the two target words in a syntactic tree. To obtain
partial patterns we split this dependency path into its
two constituent branches, each one leading from the
leaf word (i.e. variable) to the syntactic head of the
pattern. For example, ?X subj?? causes obj?? Y ? is
split into two partial patterns ?X subj?? causes? and
?causes obj?? Y ?. These partial patterns capture the
predicate structures in binary patterns.1 We discard
partial patterns with syntactic heads other than verbs
or adjectives.
The candidate genarator retrieves all sentences
from the corpus in which two nouns belonging to
one of the target semantic classes co-occur and
1 In Japanese, case information is encoded in post-positions
attached to the noun.
827
where at least one of the nouns matches a partial pat-
tern. As shown in Figure 1, these sentences and the
candidate noun pairs they contain (called (noun pair,
sentence) triples hereafter) are submitted to the clas-
sifier for scoring. Restricting candidate noun pairs
by this combination of semantic word classes and
partial pattern matching proved to be quite powerful.
For instance, in the case of causal relations we found
that close to 60% of the (noun pair, sentence) triples
produced by the candidate generator were correct
(Figure 6).
Training Data Generator As shown in Figure 1,
the (noun pair, sentence) triples used as training data
for the SVM classifier were generated from the top
results of the Stage 1 extractor and the corpus. We
consider the noun pairs in the top N output of the
Stage 1 extractor as true instances of the target re-
lation (even though they may contain erroneous ex-
tractions), and retrieve from the corpus all sentences
in which these noun pairs co-occur and that match
one of the partial patterns mentioned above. In our
experiments we set N to 25, 000. We randomly se-
lect positive training samples from this set of (noun
pair, sentence) triples.
Negative training samples are also selected ran-
domly, as follows. If one member of the target noun
pair in the positive samples above matches a partial
pattern but the other does not, we randomly replace
the latter by another noun found in the same sen-
tence, and generate this new (noun pair, sentence)
triple as a negative training sample. In the causal
relation experiments this approach had about 5%
chance of generating false negatives ? noun pairs
contained in the top N results of the Stage 1 extrac-
tor. Such samples were discarded. Our experimen-
tal results show that this scheme works quite well in
practice. We randomly sample M positive and neg-
ative samples from the autogenerated training data
to train the SVM. M was empirically set to 50,000
in our experiments.
SVM Classifier We used a straightforward fea-
ture set for training the SVM classifier. Because
our classifier will be faced with sentences contain-
ing long and infrequent patterns where the distance
between the two target nouns may be quite large,
we did not try to represent lexico-syntactic patterns
as features but deliberately restricted the feature set
to local context features of the candidate noun pair
in the target sentence. Concretely, we looked at bi-
grams and unigrams surrounding both nouns of the
candidate relation, as the local context around the
target words may contain many telling expressions
like ?increase in X? or ?X deficiency? which are use-
ful clues for causal relations. Also, in Japanese case
information is encoded in post-positions attached to
the noun, which is captured by the unigram features.
In addition to these base features, we include the
semantic classes to which the candidate noun pair
belongs, the partial patterns they match in this sen-
tence, and the infix words inbetween the target noun
pair. Note that this feature set is not intended to
be optimal beyond the actual claims of this paper,
and we have deliberately avoided exhaustive fea-
ture engineering so as not to obscure the contribu-
tion of semantic classes and partial pattern to our
approach. Clearly an optimal classifier will incorpo-
rate many more advanced features (see GuoDong et
al. (2005) for a comprehensive overview), but even
without sophisticated feature engineering our clas-
sifier achieved sufficient performance levels to sup-
port our claims. An overview of the feature set is
given in Table 1. The relative contribution of each
type of features is discussed in section 4. In prelim-
inary experiments we found a polynomial kernel of
degree 3 gave the best results, which suggests the ef-
fectiveness of combining different types of indirect
evidence.
The SVM classifier outputs (noun pair, sentence)
triples, ranked by SVM score. To obtain the final
output of our method we assign each unique noun
pair the maximum score from all (noun pair, sen-
tence) triples it occurs in, and discard all other sen-
tences for this noun pair. In section 4 below we eval-
uate the acquired noun pairs in the context of the
sentence that maximizes their score.
4 Evaluation
We demonstrate the effectiveness of semantic word
classes and partial pattern matching for relation ex-
traction by showing that the method proposed in this
paper performs at the level of other state-of-the-art
relation acquisition methods. In addition we demon-
strate that our method can successfully extract re-
lation instances from infrequent patterns, and we
828
Feature type Description Number of features
Morpheme features Unigram and bigram morphemes surrounding both target words. 554,395
POS features Coarse- and fine-grained POS tags of the noun pair and morpheme features. 2,411
Semantic features Semantic word classes of the target noun pair. 1000 (500 classes ?2)
Infix word features Morphemes found inbetween the target noun pair. 94,448
Partial patterns Partial patterns matching the target noun pair. 86
Table 1: Feature set used in the Stage 2 classifier, and their number for the causal relation experiments.
explore several criteria for what constitutes an in-
frequent pattern ? including the theoretical limit-
ing case of patterns observed only once in the en-
tire corpus. These instances are impossible to ac-
quire by pattern based methods. The ability to ac-
quire relations from extremely infrequent expres-
sions with decent accuracy demonstrates the utility
of combining semantic word classes with partial pat-
tern matching.
4.1 Experimental Setting
We evaluate our method on three semantic relation
acquisition tasks: causality, prevention and mate-
rial. Two concepts stand in a causal relation when
the source concept (the ?cause?) is directly or indi-
rectly responsible for the subsequent occurrence of
the target concept (its ?effect?). In a prevention rela-
tion the source concept directly or indirectly acts to
avoid the occurrence of the target concept, and in a
material relation the source concept is a material or
ingredient of the target concept.
For our experiments we used the latest version
of the TSUBAKI corpus (Shinzato et al, 2008),
a collection of 600 million Japanese Web pages
dependency parsed by the Japanese dependency
parser KNP2. In our implementation of CDP, lexico-
syntactic patterns consist of words on the path con-
necting two nouns in a dependency parse tree. We
discard patterns from dependency paths longer than
8 constituent nodes. Furthermore, we estimated pat-
tern frequencies in a subset of the corpus (50 million
pages, or 1/12th of the entire corpus) and discarded
patterns that co-occur with less than 10 unique noun
pairs in this smaller corpus. These restrictions do
not apply to the proposed method, which can extract
noun pairs connected by patterns of arbitrary length,
even if found only once in the corpus. For our pur-
2 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html
pose we treat dependency paths whose observed fre-
quency is below this threshold as insufficiently fre-
quent to be considered as ?patterns?. This threshold
is of course arbitrary, but in section 4 we show that
our results are not affected by these implementation
details.
We asked three human judges to evaluate ran-
dom (noun pair, sentence) triples, i.e. candidate
noun pairs in the context of some corpus sentence
in which they co-occur. If the judges find the sen-
tence contains sufficient evidence that the target re-
lation holds between the candidate nouns, they mark
the noun pair correct. To evaluate the performance
of each method we use two evaluation criteria: strict
(all judges must agree the candidate relation is cor-
rect) and lenient (decided by the judges? majority
vote). Over all experiments the interrater agreement
(Kappa) ranged between 0.57 and 0.82 with an aver-
age of 0.72, indicating substantial agreement (Lan-
dis and Koch, 1977).
4.1.1 Methods Compared
We compare our results to two pattern based
methods: CDP (the Stage 1 extractor) and Espresso
(Pantel and Pennacchiotti, 2006a).
Espresso is a popular bootstrapping based method
that uses a set of seed instances to induce extraction
patterns for the target relation and then acquire new
instances in an iterative bootstrapping process. In
each iteration Espresso performs pattern induction,
pattern ranking and selection using previously ac-
quired instances, and uses the newly acquired pat-
terns to extraction new instances. Espresso com-
putes a reliability score for both instances and pat-
terns based on their pointwise mutual information
(PMI) with the top-scoring patterns and instances
from the previous iteration.3 We refer to (Pantel and
3 In our implementation of Espresso we found that, despite
the many parameters for controlling the bootstrapping process,
829
30%
40%
50%
60%
70%
80%
90%
100%
0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K
Proposed (L)
Proposed (S)
Prop. w/o CDP (L)
Prop. w/o CDP (S)
Prop. w/o pattern (L)
Prop. w/o pattern (S)
Espresso (L)
Espresso (S)CDP (L)CDP (S)
Figure 2: Precision of acquired relations (causality). L
and S denote lenient and strict evaluation.
Pennacchiotti, 2006a) for a more detailed descrip-
tion.
For all methods compared we rank the acquired
noun pairs by their score and evaluated 500 random
samples from the top 100,000 results. For noun pairs
acquired by CDP and Espresso we select the pattern
that extracted this noun pair (in the case of Espresso,
the pattern with the highest PMI for this noun pair),
and randomly select a sentence in which the noun
pair co-occurs with that pattern from our corpus. For
the proposed method we evaluate noun pairs in the
context of the (noun pair, sentence) triple with the
highest SVM score.
4.2 Results and Discussion
The performance of each method on the causality,
prevention and material relations are shown in Fig-
ures 2, 3 and 4 respectively. In the causality exper-
iments (Figure 2) the proposed method performs on
par with CDP for the top 25,000 results, both achiev-
ing close to 90% precision. But whereas CDP?s per-
it remains very difficult to prevent semantic drift (Komachi et
al., 2008) from occurring. One small adjustment to the al-
gorithm stabilized the bootstrapping process considerably and
gave overall better results. In the pattern induction step (sec-
tion 3.2 in (Pantel and Pennacchiotti, 2006a)), Espresso com-
putes a reliability score for each candidate pattern based on the
weighted PMI of the pattern with all instances extracted so far.
As the number of extracted instances increases this dispropor-
tionally favours high frequency (i.e. generic) patterns, so in-
stead of using all instances for computing pattern reliability we
only use the m most reliable instances from the previous iter-
ation, which were used to extract the candidate patterns of the
current iteration (m = 200, like the original).
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K
Proposed (L)
Proposed (S)
Prop. w/o CDP (L)
Prop. w/o CDP (S)
Prop. w/o pattern (L)
Prop. w/o pattern (S)
Espresso (L)
Espresso (S)CDP (L)CDP (S)
Figure 3: Precision of acquired relations (prevention). L
and S denote lenient and strict evaluation.
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K
Proposed (L)
Proposed (S)
Prop. w/o CDP (L)
Prop. w/o CDP (S)
Prop. w/o pattern (L)
Prop. w/o pattern (S)
Espresso (L)
Espresso (S)CDP (L)CDP (S)
Figure 4: Precision of acquired relations (material). L
and S denote lenient and strict evaluation.
formance drops from there our method maintains
the same high precision throughout (84.6%, lenient).
Both our method and CDP outperform Espresso by
a large margin.
For the prevention relation (Figure 3), precision
is considerably lower for all methods except the top
10,000 of CDP (82% precision, lenient). The pro-
posed method peaks at around 20,000 results (67%
precision, lenient) and performance remains more or
less constant from there on. The proposed method
overtakes CDP?s performance around the top 45,000
mark, which suggests that combining the results of
both methods may be beneficial.
In the material relations the proposed method
slightly outperforms both pattern based methods
in the top 10,000 results (92% precision, lenient).
830
However for this relation our method produced only
35,409 instances. The reason is that the top 25,000
results of CDP were all extracted by just 12 patterns,
and these contained many patterns whose syntactic
head is not a verb or adjective (like ?Y rich in X? or
?Y containing X?). Only 12 partial patterns were ob-
tained, which greatly reduced the output of the pro-
posed method. Taking into account the high perfor-
mance of CDP for material relations, this suggests
that for some relations our method?s N and M pa-
rameters could use some tuning. In conclusion, in
all three relations our method performs at a level
comparable to state-of-the-art pattern based meth-
ods, which is remarkable given that it only uses in-
direct evidence.
Dealing with Difficult Extractions How does our
method handle noun pairs that are difficult to ac-
quire by pattern based methods? The graphs marked
?Prop. w/o CDP? (Proposed without CDP) in Fig-
ures 2 , 3 and 4 show the number and precision of
evaluated samples from the proposed method that do
not co-occur in our corpus with any of the patterns
that extracted the top N results of the first stage ex-
tractor. These graphs show that our method is not
simply regenerating CDP?s top results but actually
extracts many noun pairs that do not co-occur in pat-
terns that are easily learned. Figure 2 shows that
roughly two thirds of the evaluated samples are in
this category, and that their performance is not sig-
nificantly worse than the overall result. The same
conclusion holds for the prevention results (Figure
3), where over 80% of the proposed method?s sam-
ples are noun pairs that do not co-occur with eas-
ily learnable patterns. Their precision is about 5%
worse than all samples from the proposed method.
For material relations (Figure 4) about half of all
evaluated samples are in this category, but their pre-
cision is markedly worse compared to all results.
For genuinely infrequent patterns, the graphs
marked ?Prop. w/o pattern? (Proposed without pat-
tern) in Figures 2 , 3 and 4 show the number and
precision of noun pairs evaluated for the proposed
method that were acquired from sentences without
any discernible pattern. As explained in section 4
above, these constitute noun pairs co-occurring in a
sentence in which the path of dependency relations
connecting them is either longer than 8 nodes or can
 0
 5
 10
 15
 20
1 2 32 1024 32768 1.05x106 3.36x107
% o
f al
l sa
mp
les
# of noun pairs co-occurring with patterns
Pattern frequency, CDPPattern frequency, ProposedPattern frequency, Espresso
Figure 5: Frequencies of patterns in the evaluation data
(causation).
extract fewer than 10 noun pairs in 50 million Web
pages. Note that in theory it is possible that these
noun pairs could not be acquired by pattern based
methods due to this threshold ? patterns must be
able to extract more than 10 different noun pairs in
a subset of our corpus, while the proposed method
does not have this constraint. So at least in the-
ory, pattern based methods might be able to acquire
all noun pairs obtained by our method by lowering
this threshold. To see that this is unlikely to be the
case, consider Figure 5, which shows the pattern fre-
quency of the patterns induced by CDP and Espresso
for the causality experiment. The x-axis represents
pattern frequency in terms of the number of unique
noun pairs a pattern co-occurs with in our corpus (on
a log scale), and the y-axis shows the percentage of
samples that was extracted by patterns of a given fre-
quency.4 Figure 5 shows that for the pattern based
methods, the large majority of noun pairs was ex-
tracted by patterns that co-occur with several thou-
sand different noun pairs. Extrapolating the original
frequency threshold of 10 nounpairs to the size of
our entire corpus roughly corresponds to about 120
distinct noun pairs (10 times in 1/12th of the entire
corpus). In Figure 5, the histograms for the pattern
based methods CDP and Espresso start around 1000
noun pairs, which is far above this new lowerbound.
4 In the case of CDP we ignore semantic class restrictions
on the patterns when comparing frequencies. For Espresso, the
most frequent pattern (?Y by X? at the 24,889,329 data point
on the x-axis) extracted up to 53.8% of the results, but the graph
was cut at 20% for readability.
831
Cau
sali
ty
??????? ??????????????????????????????????????????[????]??????
Because ?catecholamine? causes a rapid increase of heart rate, the change of circulation inside the blood vessels leads to blood vessel
disorders and promotes [thrombus generation].
????? ??????????????????????????????????? [????]?????????????
When we injected Xylocaine during a ?tachycardia seizure?, the patient suddenly lost consciousness and fell into a fit of [convulsions].
???????? ????????? ????????? [???]???????????????
(. . . ) The reason is that by taking a lot of ?animal proteins? the causative agents of [tragomaschalia] increase.
*???????????? ?????? ?????????????????????? [???]?
* [Radon] heightens the (body?s) antioxidative function and is effective for eliminating activated oxygen, which is a cause of aging and
?lifestyle-related? diseases.
Pre
ven
tion
???????????????????? ???? ??????????????[???]?????????????
Because the fatty meat of tuna contains DHA and ?EPA? in abundance, it is effective for preventing [neuralgia].
??????? ????? ??????? [????]?????????
If you use ?nitrogen gas? instead of air you may prevent [dust explosions].
??????????? ??????? ???????????????????????? [???]???????????
In ancient Europe ?orthosiphon aristatus? tea was called a ?diet tea?, and supposedly it helps preventing triglycerides and [adult diseases].
* ?? ???????????????????????? [????]????????
* (It) is something that prevents [scratches] on the screen if the ?calash? gets stuck between the screens during storage.
Table 2: Causality and Prevention relations acquired from Single Occurrence (SO) patterns. ?X? and [Y] indicate the
relation instance?s source and target words, and ?*? indicates erroneous extractions.
Thus, pattern based methods naturally tend to induce
patterns that are much more frequent than the range
of patterns our method can capture, and it is unlikely
that this is a result of implementation details like pat-
tern frequency threshold.
The precision of noun pairs in the category ?Prop.
w/o pattern? is clearly lower than the overall re-
sults, but the graphs demonstrate that our method
still handles these difficult cases reasonably well.
The 500 samples evaluated contained 155 such in-
stances for causality, 403 for prevention and 276 for
material. For prevention, the high ratio of these noun
pairs helps explain why the overall performance was
lower than for the other relations.
Finally, the theoretical limiting case for pattern
based algorithms consists of patterns that only co-
occur with a single noun pair in the entire corpus
(single occurrence or SO patterns). Pattern based
methods learn new patterns that share many noun
pairs with a set of reliable patterns in order to extract
new relation instances. If a noun pair that co-occurs
with a SO pattern also co-occurs with more reliable
patterns there is no need to learn the SO pattern. If
that same noun pair does not co-occur with any other
reliable pattern, the SO pattern is beyond the reach
of any pattern induction method. Thus, SO patterns
are effectively useless for pattern based methods.
For the 500 samples evaluated from the causality
and prevention relations acquired by our method we
found 7 causal noun pairs that co-occur only in SO
patterns and 29 such noun pairs for prevention. The
precision of these instances was 42.9% and 51.7%
respectively. In total we found 8,716 causal noun
pairs and 7,369 prevention noun pairs that co-occur
only with SO patterns. Table 2 shows some example
relations from our causality and prevention experi-
ments that were extracted from SO patterns. To con-
clude, our method is able to acquire correct relations
even from the most extreme infrequent expressions.
Semantic Classes, Partial Patterns or Both? In
the remainder of this section we look at how the
combination of semantic word classes and partial
patterns benefits our method. For each relation we
evaluated 1000 random (noun pair, sentence) triples
satisfying the two conditions from section 3 ?
matching semantic class pairs and partial patterns.
Surprisingly, the precision of these samples was
59% for causality, 40% for prevention and 50.4%
for material, showing just how compelling these two
types of indirect evidence are in combination.
To estimate the relative contribution of each
heuristic we compared our candidate generation
method against two baselines. The first baseline
evaluates the precision of random noun pairs from
832
 50
 60
 70
 80
 90
 100
 0  200  400  600  800  1000
pre
cis
ion
 (%
)
(noun pair, sentence) triples ranked by score
Base features onlyAll minus semantic classesAll minus infix wordsAll minus partial patternsAll features
Figure 6: Contribution of feature sets (causality).
 30
 40
 50
 60
 70
 80
 90
 100
 0  200  400  600  800  1000
pre
cis
ion
 (%
)
(noun pair, sentence) triples ranked by score
Base features onlyAll minus semantic classesAll minus infix wordsAll minus partial patternsAll features
Figure 7: Contribution of feature sets (prevention).
the target semantic classes that co-occur in a sen-
tence. The second baseline does the same for the
second heuristic, selecting random sentences con-
taining a noun pair that matches some partial pat-
tern. Evaluating 100 samples for causality and pre-
vention, we found the precision of the semantic class
baseline was 16% for causality and 5% for preven-
tion. The pattern fragment baseline gave 9% for
causality and 22% for prevention. This is consid-
erably lower than the precision of random samples
that satisfy both the semantic class and partial pat-
tern conditions, showing that the combination of se-
mantic classes and partial patterns is more effective
than either one individually.
Finally, we investigated the effect of the various
feature sets used in the classifier. Figures 6, 7 and
8 show the results for the respective semantic re-
lations. The ?Base features? graph shows the per-
 30
 40
 50
 60
 70
 80
 90
 100
 0  200  400  600  800  1000
pre
cis
ion
 (%
)
(noun pair, sentence) triples ranked by score
Base features onlyAll minus semantic classesAll minus infix wordsAll minus partial patternsAll features
Figure 8: Contribution of feature sets (material).
formance the unigram, bigram and part-of-speech
features. ?All features? uses all features in Table
1. The other graphs show the effect of removing
one type of features. These graphs suggest that the
contribution of the individual feature types (seman-
tic class information, partial patterns or infix words)
to the classification performance is relatively minor,
but in combination they do give a marked improve-
ment over the base features, at least for some rela-
tions like causation and material. In other words,
the main contribution of semantic word classes and
partial patterns to our method?s performance lies not
in the final classification step but seems to occur at
earlier stages of the process, in the candidate and
training data generation steps.
5 Related Work
Using lexico-syntactic patterns to extract semantic
relations was first explored by Hearst (Hearst, 1992),
and has inspired a large body of work on semi-
supervised relation acquisition methods (Berland
and Charniak, 1999; Agichtein and Gravano, 2000;
Etzioni et al, 2004; Pantel and Pennacchiotti,
2006b; Pas?ca et al, 2006; De Saeger et al, 2009),
two of which were used in this work.
Some researchers have addressed the sparse-
ness problems inherent in pattern based methods.
Downey et al (2007) starts from the output of
the unsupervised information extraction system Tex-
tRunner (Banko and Etzioni, 2008), and uses lan-
guage modeling techniques to estimate the reliabil-
ity of sparse extractions. Pas?ca et al (2006) alle-
833
viates pattern sparseness by using infix patterns that
are generalized using classes of distributionally sim-
ilar words. In addition, their method employs clus-
tering based semantic similarities to filter newly ex-
tracted instances in each iteration of the bootstrap-
ping process. A comparison with our method would
have been instructive, but we were unable to imple-
ment their method because the original paper con-
tains insufficient detail to allow replication.
There is a large body of research in the super-
vised tradition that does not use explicit pattern rep-
resentations ? kernel based methods (Zelenko et
al., 2003; Culotta, 2004; Bunescu and Mooney,
2005) and CRF based methods (Culotta et al, 2006).
These approaches are all fully supervised, whereas
in our work the automatic generation of candi-
dates and training data is an integral part of the
method. An interesting alternative is distant super-
vision (Mintz et al, 2009), which trains a classi-
fier using an existing database (Freebase) containing
thousands of semantic relations, with millions of in-
stances. We believe our method is more general, as
depending on external resources like a database of
semantic relations limits both the range of seman-
tic relations (i.e., Freebase contains only relations
between named entities, and none of the relations
in this work) and languages (i.e., no resource com-
parable to Freebase exists for Japanese) to which
the technology can be applied. Furthermore, it is
unclear whether distant supervision can deal with
noisy input such as automatically acquired relation
instances.
Finally, inference based methods (Carlson et al,
2010; Schoenmackers et al, 2010; Tsuchida et al,
2010) are another attempt at relation acquisition that
goes beyond pattern matching. Carlson et al (2010)
proposed a method based on inductive logic pro-
gramming (Quinlan, 1990). Schoenmackers et al
(2010) takes relation instances produced by Tex-
tRunner (Banko and Etzioni, 2008) as input and in-
duces first-order Horn clauses, and new instances are
infered using a Markov Logic Network (Richardson
and Domingo, 2006; Huynh and Mooney, 2008).
Tsuchida et al (2010) generated new relation hy-
potheses by substituting words in seed instances
with distributionally similar words. The difference
between these works and ours lies in the treatment
of evidence. While the above methods learn infer-
ence rules to acquire new relation instances from in-
dependent information sources scattered across dif-
ferent Web pages, our method takes the other option
of working with all the clues and indirect evidence a
single sentence can provide. In the future, a combi-
nation of both approaches may prove beneficial.
6 Conclusion
We have proposed a relation acquisition method that
is able to acquire semantic relations from infrequent
expressions by focusing on the evidence provided by
semantic word classes and partial pattern matching
instead of direct extraction patterns. We experimen-
tally demonstrated the effectiveness of this approach
on three relation acquisition tasks, causality, preven-
tion and material relations. In addition we showed
our method could acquire a significant number of
relation instances that are found in extremely infre-
quent expressions, the most extreme case of which
are single occurrence patterns, which are beyond
the reach of existing pattern based methods. We be-
lieve this ability is of crucial importance for acquir-
ing valuable long tail instances. In future work we
will investigate whether the current framework can
be extended to acquire inter-sentential relations.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proc. of the fifth ACM conference on Digital li-
braries, pages 85?94.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proc. of the 46th ACL-08:HLT, pages 28?36.
Matthew Berland and Eugene Charniak. 1999. Find-
ing parts in very large corpora. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics, pages 57?64, College Park, Mary-
land, USA, June.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing (HLT ?05), pages 724?731.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for neverend-
ing language learning. In Proc of the 24th AAAI, pages
1306?1313.
834
Aron Culotta, Andrew McCallum, and Jonathan Betz.
2006. Integrating probabilistic extraction models and
data mining to discover relations and patterns in text.
In Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics (HLT/NAACL), pages 296?303.
Aron Culotta. 2004. Dependency tree kernels for rela-
tion extraction. In In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL-04, pages 423?429.
Stijn De Saeger, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large Scale
Relation Acquisition Using Class Dependent Patterns.
In Proc. of the 9th International Conference on Data
Mining (ICDM), pages 764?769.
Doug Downey, Stefan Schoenmackers, and Oren Etzioni.
2007. Sparse information extraction: Unsupervised
language models to the rescue. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL2007).
Oren Etzioni, Michael Cafarella, Doug Downey, Stanley
Kok, Ana-Maria Popescu, Tal Shaked, Stephen Soder-
land, Daniel Weld, and Alexander Yates. 2004. Web-
scale information extraction in KnowItAll. In Proc. of
the 13th international conference on World Wide Web
(WWW04), pages 100?110.
Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation extrac-
tion. In Proc. of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, ACL ?05, pages
419?444.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th In-
ternational Conference on Computational Linguistics
(COLING?92), pages 539?545.
Tuyen N. Huynh and Raymond J. Mooney. 2008.
Discriminative structure and parameter learning for
markov logic networks. In Proc. of the 25th ICML,
pages 416?423.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-scale
clustering of dependency relations. In Proc. of the
46th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL-08: HLT), pages 407?415.
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of se-
mantic drift in espresso-like bootstrapping algorithms.
In Proc. of EMNLP?08. Honolulu, USA, pages 1011?
1020.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery
of inference rules from text. In Proc. of the ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining, pages 323?328.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proc. of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 1003?1011.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Names and Similarities on
the Web: Fact Extraction in the Fast Lane. In Proc. of
the COLING-ACL06, pages 809?816.
Patrick Pantel and Marco Pennacchiotti. 2006a.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proc. of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics (COLING-ACL-06,
pages 113?120.
Patrick Pantel and Pennacchiotti Pennacchiotti, Marco.
2006b. Espresso: Leveraging generic patterns for au-
tomatically harvesting semantic relations. In Proc. of
the COLING-ACL06, pages 113?120.
J. R. Quinlan. 1990. Learning logical definitions from
relations. Machine Learning, 5(3):239?266.
Matthew Richardson and Pedro Domingo. 2006.
Markov logic networks. Machine Learning, 26:107?
136.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In Proc. of EMNLP2010, pages
1088?1098.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
Chikara Hashimoto, and Sadao Kurohashi. 2008.
TSUBAKI: An open search engine infrastructure for
developing new information access. In Proc. of IJC-
NLP, pages 189?196.
Kentaro Torisawa, Stijn De Saeger, Jun?ichi Kazama,
Asuka Sumida, Daisuke Noguchi, Yasunari Kakizawa,
Masaaki Murata, Kow Kuroda, and Ichiro Yamada.
2010. Organizing the web?s information explosion to
discover unknown unknowns. New Generation Com-
puting, 28(3):217?236.
Masaaki Tsuchida, Stijn De Saeger, Kentaro Torisawa,
Masaki Murata, Jun?ichi Kazama, Kow Kuroda, and
Hayato Ohwada. 2010. Large scale similarity-based
relation expansion. In Proc of the 4th IUCS, pages
140?147.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. Journal of Machine Learning Research,
pages 1083?1106.
835
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 368?378, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Why Question Answering using Sentiment Analysis and Word Classes
Jong-Hoon Oh? Kentaro Torisawa? Chikara Hashimoto ?
Takuya Kawada? Stijn De Saeger? Jun?ichi Kazama? Yiou Wang??
Information Analysis Laboratory
Universal Communication Research Institute
National Institute of Information and Communications Technology (NICT)
{? rovellia,? torisawa,? ch,? tkawada,?stijn,? kazama,??wangyiou}@nict.go.jp
Abstract
In this paper we explore the utility of sen-
timent analysis and semantic word classes
for improving why-question answering on a
large-scale web corpus. Our work is moti-
vated by the observation that a why-question
and its answer often follow the pattern that if
something undesirable happens, the reason is
also often something undesirable, and if some-
thing desirable happens, the reason is also of-
ten something desirable. To the best of our
knowledge, this is the first work that intro-
duces sentiment analysis to non-factoid ques-
tion answering. We combine this simple idea
with semantic word classes for ranking an-
swers to why-questions and show that on a set
of 850 why-questions our method gains 15.2%
improvement in precision at the top-1 answer
over a baseline state-of-the-art QA system that
achieved the best performance in a shared task
of Japanese non-factoid QA in NTCIR-6.
1 Introduction
Question Answering (QA) research for factoid ques-
tions has recently achieved great success as demon-
strated by IBM?s Watson at Jeopardy: its accuracy
has been reported to be around 85% on factoid ques-
tions (Ferrucci et al2010). Although recent shared
QA tasks (Voorhees, 2004; Pe?as et al2011; Fuku-
moto et al2007) have stimulated the research com-
munity to move beyond factoid QA, comparatively
little attention has been paid to QA for non-factoid
questions such as why questions and how to ques-
tions, and the performance of the state-of-art non-
factoid QA systems reported in the literature (Mu-
rata et al2007; Surdeanu et al2011; Verberne et
al., 2010) remains considerably lower than that of
factoid QA (i.e., 34% in MRR at top-150 results on
why-questions (Verberne et al2010)).
In this paper we explore the utility of sentiment
analysis (Pang et al2002; Turney, 2002; Nakagawa
et al2010) and semantic word classes for improv-
ing why-question answering (why-QA) on a large-
scale web corpus. The inspiration behind this work
is the observation that why-questions and their an-
swers often have the following tendency:
? if something undesirable happens, the reason is
often also something undesirable, and
? if something desirable happens, its reason is of-
ten also desirable.
Consider the following question Q1, and its an-
swer candidates A1-1 and A1-2.
? Q1: Why does cancer occur?
? A1-1: Carcinogens such as nitrosamine and
benzopyrene may increase the risk of cancer by
altering DNA in cells.
? A1-2: Maintaining a healthy weight may lower
the risk of various types of cancer.
Here A1-1 describes an undesirable event related to
cancer, while A1-2 suggests a desirable action for
its prevention. Our hypothesis suggests that A1-1
is more appropriate for answering Q1. If this hy-
pothesis holds, we can obtain a significant improve-
ment in performance on why-QA tasks by exploiting
the sentiment orientation1 of expressions obtainable
1 In this paper we denote the desirable/undesirable polar-
ity of an expression by the term ?sentiment orientation? instead
of ?semantic orientation? to avoid confusion with our different
notion of ?semantic word classes.?
368
by automatic sentiment analysis of questions and an-
swers.
A second observation motivating this work is that
there are often significant associations between the
lexico-semantic classes of words in a question and
those in its answer sentence. For instance, questions
concerning diseases like Q1 often have answers that
include references to specific semantic word classes
such as chemicals (like A1-1), viruses, body parts,
and so on. Capturing such statistical correlations be-
tween diseases and harmful substances may lead to
higher why-QA performance. For this purpose we
use classes of semantically similar words that were
automatically acquired from a large web corpus us-
ing an EM-based clustering method (Kazama and
Torisawa, 2008).
Another issue is that simply introducing the sen-
timent orientation of words or phrases in question
and answer sentences in a naive way is insufficient,
since answer candidate sentences may contain mul-
tiple sentiment expressions with different polarities
in answer candidates (i.e., about 33% of correct an-
swers had such multiple sentiment expressions with
different polarities in our test set). For example, if
A1-2 contained a second sentiment expression with
negative polarity like the example below,
?Trusting a specific food is not effective
for preventing cancer, but maintaining a
healthy weight may help lower the risk of
various types of cancer.?
both A1-1 and A1-2 would contain sentiment ex-
pressions with the same polarity as that of Q1. Thus,
it is difficult to expect that the sentiment orientation
alone will work well for recognizing A1-1 as a cor-
rect answer to Q1. To address this problem, we con-
sider the combination of sentiment polarity and the
contents of sentiment expressions associated with
the polarity in questions and their answer candidates
as well. To deal with the data sparseness problem
arising in using the content of sentiment expressions,
we developed a feature set that combines the polar-
ity and the semantic word classes effectively.
We exploit these two main ideas (concerned with
the sentiment orientation and the semantic classes
described so far) for training a supervised classi-
fier to rank answer candidates to why-questions.
Through a series of experiments on 850 Japanese
why-questions, we showed that the proposed seman-
tic features were effective in identifying correct an-
swers, and our proposed method obtained more than
15% improvement in precision of its top answer
(P@1) over our baseline, which achieved the best
performance in the non-factoid QA task in NTCIR-
6 (Murata et al2007). We also show that our
method can potentially perform with high precision
(64.8% in P@1) when answer candidates containing
at least one correct answer are given to our re-ranker.
2 Approach
Our proposed method is composed of answer re-
trieval and answer re-ranking. The first step, an-
swer retrieval, extracts a set of answer candidates to
a why-question from 600 million Japanese Web cor-
pus. The answer retrieval is our implementation of
the state-of-art method that has shown the best per-
formance in the shared task of Japanese non-factoid
QA in NTCIR-6 (Murata et al2007; Fukumoto et
al., 2007). The second step, answer re-ranking, is
the focus of this work.
2.1 Answer Retrieval
We use Solr2 to retrieve documents from a 600 mil-
lion Japanese Web page corpus3for a given why-
question. Let a set of content words in a why-
question be T = {t1, ? ? ? , tn}. Two boolean queries
for a why-question, ?t1 AND ? ? ? AND tn? and ?t1
OR ? ? ? OR tn,? are given to Solr and top-300 doc-
uments for each query are retrieved. Note that re-
trieved documents by each query have different cov-
erage and relevance to a given why-question. To
keep balance between the coverage and relevance of
retrieved documents, we use a set of retrieved doc-
uments by these two queries for obtaining answer
candidates. Each document in the result of docu-
ment retrieval is split into a set of answer candi-
dates consisting of five subsequent sentences4. Sub-
sequent answer candidates can share up to two sen-
tences to avoid errors due to wrong document seg-
mentation.
2 http://lucene.apache.org/solr
3 To the best of our knowledge, few Japanese non-factoid
QA systems in the literature have used such a large-scale cor-
pus.
4 The length of acceptable answer candidates for why-
QA in the literature ranges from one sentence to two para-
graphs (Fukumoto et al2007; Murata et al2007; Higashinaka
and Isozaki, 2008; Verberne et al2007; Verberne et al2010).
369
Answer candidate ac for question q is ranked
according to scoring function S(q, ac) given in
Eq. (1) (Murata et al2007). Murata et al2007)?s
method uses text search to look for answer candi-
dates containing terms from the question with ad-
ditional clue terms referring to ?reason? or ?cause.?
Following the original method we used riyuu (rea-
son), genin (cause) and youin (cause) as clue terms.
The top-20 answer candidates for each question are
passed on to the next step, which is answer re-
ranking. S(q, ac) assigns a score to answer candi-
dates like tf -idf , where 1/dist(t1, t2) functions like
tf and 1/df(t2) is idf for given terms t1 and t2 that
are shared by q and ac.
S(q, ac) = maxt1?T
?
t2?T
?? log(ts(t1, t2)) (1)
ts(t1, t2) =
N
2? dist(t1, t2)? df(t2)
Here T is a set of terms including nouns, verbs, and
adjectives in question q that appear in answer can-
didate ac. Note that the clue terms are added to T
if they exist in ac. N is the total number of docu-
ments (600 million), dist(t1, t2) represents the dis-
tance (the number of characters) between t1 and t2
in answer candidate ac, df(t) is the document fre-
quency of term t, and ? ? {0, 1} is an indicator,
where ? = 1 if ts(t1, t2) > 1, ? = 0 otherwise.
2.2 Answer Re-ranking
Our re-ranker is a supervised classifier (SVMs)
(Vapnik, 1995) that uses three types of feature
sets: features expressing morphological and syn-
tactic analysis of questions and answer candidates,
features representing semantic word classes appear-
ing in questions and answer candidates, and features
from sentiment analysis. All answer candidates of a
question are ranked in a descending order of their
score given by SVMs. We trained and tested the
re-ranker using 10-fold cross validation on a cor-
pus composed of 850 why-questions and their top-
20 answer candidates provided by the answer re-
trieval procedure in Section 2.1. The answer candi-
dates were manually annotated by three human an-
notators (not by the authors). Our corpus construc-
tion method is described in more detail in Section 4.
3 Features for Answer Re-ranking
This section describes our feature sets for answer
re-ranking: features expressing morphological and
syntactic analysis (MSA), features representing se-
mantic word class (SWC), and features indicat-
ing sentiment analysis (SA). MSA, which has been
widely used for re-ranking answers in the literature,
is used to identify associations between questions
and answers at the morpheme, word phrase, and syn-
tactic dependency levels. The other two feature sets
are proposed in this paper. SWC is devised for iden-
tifying semantic word class associations between
questions and answers. SA is used for identify-
ing sentiment orientation associations between ques-
tions and answers as well as expressing the combi-
nation of each sentiment expression and its polarity.
Table 1 summarizes the respective feature sets, each
of which is described in detail below.
3.1 Morphological and Syntactic Analysis
MSA including n-grams of morphemes, words, and
syntactic dependencies has been widely used for re-
ranking answers in non-factoid QA (Higashinaka
and Isozaki, 2008; Surdeanu et al2011; Verberne
et al2007; Verberne et al2010). We use MSA as
a baseline feature set in this work.
We represent all sentences in a question and
its answer candidate in three ways: morphemes,
word phrases (bunsetsu5) and syntactic dependency
chains. These are obtained using a morphological
analyzer6 and a dependency parser7. From each
question and answer candidate we extract n-grams
of morphemes, word phrases, and syntactic depen-
dencies, where n ranges from 1 to 3. Syntactic de-
pendency n-grams are defined as a syntactic depen-
dency chain containing n word phrases. Syntactic
dependency 1-grams coincide with word phrase 1-
grams, so they are ignored.
Table 1 defines four types of MSA (MSA1 to
MSA4). MSA1 is n-gram features from all sen-
tences in a question and its answer candidates and
distinguishes an n-gram feature found in a ques-
tion from that same feature found in answer candi-
dates. MSA2 contains n-grams found in the answer
5 A bunsetsu is a syntactic constituent composed of a content
word and several function words such as post-positions and case
markers. It is the smallest unit of syntactic analysis in Japanese.
6 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
7 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP
370
MSA1 Morpheme n-grams, word phrase n-grams, and syntactic dependency n-grams in a question and its answer candidate, where n ranges
from 1 to 3. n-grams in a question and those in an answer candidate are distinguished.
MSA2 MSA1?s n-grams in an answer candidate that contain a question term.
MSA3 MSA1?s n-grams that contain a clue term including riyuu (reason), genin (cause) and youin (cause). These n-grams in a question and
those in an answer candidate are distinguished.
MSA4 The ratio of the number of question terms in an answer candidate to the total number of question terms.
SWC1 Word class n-grams in a question and its answer candidate. These n-grams in a question and those in an answer candidate are distin-
guished.
SWC2 SWC1?s n-grams in an answer candidate whose original MSA1?s n-grams contain any question term.
SA@W1 Word polarity n-grams in a question and its answer candidate. These n-grams in a question and those in an answer candidate are
distinguished.
SA@W2 SA@W1?s n-grams in an answer candidate whose original MSA1 n-grams contain any question term.
SA@W3 Joint class-polarity n-grams in a question and its answer candidate. These n-grams in a question and those in an answer candidate are
distinguished.
SA@W4 SA@W3?s n-grams in an answer candidates whose original MSA1 n-grams contain any question term.
SA@P1 The indicator for polarity agreement between sentiment phrases, one in a question and the other in an answer candidate: 1 if any pair of
such sentiment phrases has polarity in agreement, 0 otherwise.
SA@P2 The phrase-polarity, positive or negative, of a pair of sentiment phrases for which the indicator in SA@P1 is 1.
SA@P3 Morpheme n-grams, word phrase n-grams, and syntactic dependency n-grams in sentiment phrases are coupled with their phrase-polarity,
where n ranges from 1 to 3. These n-grams in a question and those in an answer candidate are distinguished.
SA@P4 SA@P3?s n-grams in an answer candidates that contain a question term.
SA@P5 The ratio of the number of question terms in sentences that have sentiment phrases in answer candidates to the total number of question
terms.
SA@P6 Word class n-grams in sentiment phrases are coupled with phrase-polarity. These n-grams in a question and those in an answer candidate
are distinguished.
SA@P7 SA@P6?s n-grams in an answer candidates, whose original MSA1?s n-grams include any question term.
SA@P8 Joint class-polarity n-grams in sentiment phrases of a question and its answer candidate are coupled with phrase-polarity of the sentiment
phrases. These n-grams in a question and those in an answer candidate are distinguished.
SA@P9 SA@P8?s n-grams in an answer candidates, whose original MSA1?s n-grams include any question term.
SA@P10 A pair of SA@P6?s n-grams, one from sentiment phrases in a question and the other from those in an answer candidate, where the two
sentiment phrases should have the same sentiment orientation.
Table 1: Features used in training our re-ranker
candidates that themselves contain a term from the
question (e.g., ?types of cancer? in example A1-2).
MSA3 is the n-gram feature that contains one of the
clue terms used for answer retrieval (riyuu (reason),
genin (cause) or youin (cause)). Here too, n-grams
obtained from the questions and answer candidates
are distinguished. Finally, MSA4 is the percentage
of the question terms found in an answer candidate.
3.2 Semantic Word Class
Semantic word classes are sets of semantically simi-
lar words. We construct these semantic word classes
by using the noun clustering algorithm proposed in
Kazama and Torisawa (2008). The algorithm fol-
lows the distributional hypothesis, which states that
semantically similar words tend to appear in simi-
lar contexts (Harris, 1954). By treating syntactic de-
pendency relations between words as ?contexts,? the
clustering method defines a probabilistic model of
noun-verb dependencies with hidden classes as:
p(n, v, r) =
?
c
p(n|c)p(?v, r?|c)p(c) (2)
Here, n is a noun, v is a verb or noun on which n de-
pends via a grammatical relation r (post-positions in
Japanese), and c is a hidden class. Dependency rela-
tion frequencies were obtained from our 600-million
page web corpus, and model parameters p(n|c),
p(?v, r?|c) and p(c) were estimated using the EM
algorithm (Hofmann, 1999). We successfully clus-
tered 5.5 million nouns into 500 classes. For each
noun n, EM clustering estimates a probability dis-
tribution over hidden variables representing seman-
tic classes. From this distribution we obtained dis-
crete semantic word classes by assigning each noun
n to semantic class c = argmaxc? p(c?|n). The
resulting classes actually form clean semantic cat-
egories such as chemicals, nutrients, diseases and
conditions, in our examples of Q1 and Q2. The fol-
lowing are the top-10 words (English translation) ac-
cording to p(c|n) for these classes.
chemicals: acetylene, hydrogenation product,
phosphoric monoester, methacrylate, levoglu-
cosan, ammonium salt, halogenated organic
compound, benzonitrile, alkyne, nitrosamine
371
nutrients: glucide, carbonhydrate, mineral, salt,
sugar, water, fat, vitamin, nutrients, protein
diseases: pneumonia, neuritis, cancer, oral leuko-
plakia, pachymeningitis, acidosis, encephalitis,
abdominal injury, valvulitis, gingivitis
conditions: proficiency, decrepitude, deficiency,
impurity, abnormalities, floated, crisis, dis-
placement, condition, shortage
Semantic word class (SWC) features are used to
capture associations between semantic classes of
words in the question and those in the answer candi-
dates. For example:
? Q2: Why does rickets (Wdisease) occur in chil-
dren?
? A2: Deficiency (Wcondition) of vitamin D
(Wnutrients) can cause rickets (Wdisease).
Wcondition, Wdisease and Wnutrients represent se-
mantic word classes of conditions, diseases and nu-
trients, respectively. If this question-answer pair is
given to the classifier as a positive training sample,
we expect it to learn that if a disease name appears
in a question then, everything else being equal, an-
swers including nutrient names are more likely to be
correct. Note that in principle the same association
could be learned between word pairs, i.e., rickets and
vitamin D. However, we found that word level asso-
ciations are often too specific, and because of data
sparseness this knowledge cannot easily be general-
ized to unseen questions. This is our main motiva-
tion for introducing broad coverage semantic word
classes into the feature set.
We call the feature set with the word classes SWC
and use two types of SWC, as shown in Table 1. To
obtain the first type (SWC1), we convert all nouns
in the MSA1 n-grams into their respective word
classes, and keep all n-grams that contain at least
one word class. We call these features word class
n-grams. Again, word class n-grams obtained from
questions are distinguished from the ones in answer
candidates. For example, we extract ?Wdisease oc-
cur? as a word class 2-gram from Q2.
The second type of SWC, SWC2, represents word
class n-grams in an answer candidate, in which
question terms are replaced by their respective se-
mantic word classes. For example, Wdisease in word
class 2-gram ?cause Wdisease? from A2 is the se-
mantic word class of rickets, one of the question
terms. These features capture the correspondence
between semantic word classes in the question and
answer candidates.
3.3 Sentiment Analysis
Sentiment analysis (SA) features are classified into
word-polarity and phrase-polarity features. We use
opinion extraction tool8 and sentiment orientation
lexicon in the tool for these features.
3.3.1 Opinion Extraction Tool
Opinion extraction tool is a software, the im-
plementation of Nakagawa et al2010). It ex-
tracts linguistic expressions representing opinions
(henceforth, we call them sentiment phrases) from
a Japanese sentence and then identifies the polarity
of these sentiment phrases using machine learning
techniques. For example, rickets occur in Q2 and
Deficiency of vitamin D can cause rickets in A2 can
be identified as sentiment phrases with a negative
polarity. The tool identifies sentiment phrases and
their polarity by using polarities of words and de-
pendency subtrees as evidence, where these polari-
ties are given in a word polarity dictionary.
In this paper, we use a trained model and a word
polarity dictionary (containing about 35,000 entries)
distributed via the ALAGIN forum9 for our sen-
timent analysis. Table 2 shows the performance
of opinion extraction tool, precision (P), recall (R)
and F-value (F), in this setting (reported in the
Japanese homepage of this tool). In the evaluation of
sentiment-phrase extraction, an extracted sentiment
phrase is determined as correct if its head word is
the same as one in the gold standard. Polarity clas-
sification is evaluated under the condition that all of
the sentiment phrases are correctly extracted.
P R F
Sentiment-phrase extraction 0.602 0.408 0.486
Polarity classification (pos.) 0.873 0.893 0.883
Polarity classification (neg.) 0.866 0.842 0.854
Table 2: The performance of opinion extraction tool
3.3.2 Word Polarity (SA@W)
Polarities of words are identified by simply look-
ing up the word polarity dictionary of opinion ex-
8 Available at http://alaginrc.nict.go.jp/opinion/index_e.html
9 http://www.alagin.jp/index-e.html. Only the members of
the ALAGIN forum can access these resources.
372
traction tool. Word polarity features are used
for identifying associations between the polarity of
words in a question and that in a correct answer. For
example:
? Q2: Why does rickets (W?) occur in children?
? A2: Deficiency (W?) of vitamin D can cause
rickets (W?).
Here, W? represents negative word polarities. We
expect our classifier to learn from this question and
answer pair that if a word with negative polarity ap-
pears in a question then its correct answer is likely
to contain a negative polarity word as well.
SA@W1 and SA@W2 in Table 1 are sentiment
analysis features from word polarity n-grams, which
contain at least one word that has word polarities.
We obtain these n-grams by converting all nouns in
MSA1 n-grams into their word polarities through
dictionary lookup. For example, from Q2 in the
above example we extract ?W? occur? as a word
polarity 2-gram. SA@W1 is concerned with all
word polarity n-grams in questions and answer can-
didates. For SA@W2, we restrict word polarity
n-grams from SA@W1 to those whose original n-
gram include a question term.
Furthermore, word polarities are coupled with se-
mantic word classes so that our classifier can iden-
tify meaningful combinations of both. For example,
deficiency in A2 can be represented asW?condition by
its respective semantic word class and word polar-
ity, which allows for the representation of undesir-
able conditions. This in turn lets our system learn
meaningful correlations between words expressing
these kind of negative conditions and their connec-
tion to questions asking about diseases. SA@W3
and SA@W4 are features from this combination.
They are defined in the same way as SA@W1 and
SA@W2 except that word polarities are replaced
with the combination of semantic word classes and
word polarities. We call n-grams in SA@W3 and
SA@W4 joint (word) class-polarity n-grams.
3.3.3 Phrase Polarity (SA@P)
Opinion extraction tool is applied to question and
its answer candidate to identify sentiment phrases
and their phrase-polarities. In preliminary tests we
found that sentiment phrases do not help to iden-
tify correct answers if answer sentences including
the sentiment phrases do not have any term from the
question. So we restrict the target sentiment phrases
to those acquired from sentences containing at least
one question term. From these sentiment phrases we
extract three categories of features.
First, SA@P1 and SA@P2 are features concerned
with phrase-polarity agreement between sentiment
phrases in a question and its answer candidate. We
consider all possible pairs of sentiment phrases from
the question and answer. If any such pair agrees
in phrase-polarity, an indicator for the agreement
and its polarity in the agreement become features
SA@P1 and SA@P2, respectively.
Secondly, following the original hypothesis un-
derlying this paper, we assume that sentiment
phrases often represent the core part of the cor-
rect answer (e.g., A2 above) and it is important
to express the content of the sentiment phrases in
features. SA@P3 and SA@P4 were devised for
this purpose. SA@P3 represents this sentiment
phrase contents as n-grams of morphemes, words,
and syntactic dependencies of sentiment phrases,
together with their phrase-polarity. Furthermore,
SA@P4 is the subset of SA@P3 n-grams restricted
to those that include terms found in the question,
and SA@P5 indicates the percentage of sentiment
n-grams from the question that are found in a given
answer candidate.
Finally, features SA@P6 through SA@P9 use se-
mantic word classes to generalize the content fea-
tures mentioned above. These features consist of
word class n-grams and joint class-polarity n-grams
taken from sentiment phrases, together with their
phrase polarity. Similar to the definition of SA@P4,
for SA@P7 and SA@P9 we restrict ourselves to n-
grams containing a question term. SA@P10 repre-
sents the semantic content of two sentiment phrases
with the same sentiment orientation (one from a
question and the other from an answer candidate)
using word class n-grams, together with the phrase-
polarity in agreement.
4 Test Set
We prepared three sets of why-questions (QS1, QS2
and QS3) and used these questions to build two test
sets for our experiments.
Why-questions in QS1 are taken from the
Japanese version of Yahoo! Answers (called Ya-
hoo! Chiebukuro)10. We automatically extracted
10 We used ?Yahoo! Chiebukuro Data (2nd edition)? which is
373
questions consisting of a single sentence and con-
taining the interrogative naze (why), and our anno-
tators verified that these questions are meaningful
without further context. For example, they discarded
questions like ?Why doesn?t the WBC (world box-
ing council) make an objection to the WBC (World
baseball classic)?? (the object of the objection is
unclear) and ?Why do minors trade at the auction
even though it is disallowed by the rules? (informa-
tion about which auction is not provided).
Because questions in Yahoo! Answers are aimed
at human readers, users often ?set the stage? by giv-
ing lots of background information about their ques-
tion. This often leads to large stylistic differences
between the questions in Yahoo! Answers and those
typically posed to a QA system. We therefore cre-
ated a second set of why-questions, QS2, whose
style should be more appropriate for a QA system
(examples showing these differences are given in the
supplementary materials of this paper). Six human
annotators (not the authors) were asked to create
why-questions in their own words, keeping in mind
that the questions they create are for a QA system. In
addition, the annotators were asked to verify on the
Web that the questions they created ask about some
real event or phenomena. For example, a question
like ?Why does Mars appear blue?? is disallowed in
QS2 because ?Mars appears blue? is false. Note that
the correct answer to these questions does not have
to be either in our target corpus or in real-world Web
texts. These two sets of why-questions, QS1 and
QS2, are used to build a test set for evaluating our
proposed method.
Finally, QS3 contains why-questions that have at
least one answer in our target corpus (600 million
Japanese Web page corpus). For creating such why-
questions, four human annotators (not the authors)
were given a text passage composed of three contin-
uous sentences and asked to locate the reasons for
some event as described in this passage. Then they
created a why-question for which the description is a
correct answer. Because randomly selected passages
from our target corpus have little chance of generat-
ing good why-questions we extracted passages from
our target corpus that include at least one of the clue
terms used in our answer retrieval step (i.e. riyuu
(reason), genin (cause), or youin (cause)). This set-
provided by Yahoo Japan Corporation and contains 16 million
questions asked from April, 2004 to April 2009.
ting may not necessarily reflect a ?real world? dis-
tribution of why-questions, in which ideally a wide
range of people ask questions that may or may not
have an answer in our corpus. However, QS3 al-
lows us to evaluate our method under the idealized
conditions where we have a perfect answer retrieval
module whose answer candidates always contain at
least one correct answer (the source passage used
for creating the why-question). This setting allows
us to estimate the ideal-case performance of our
method. Under these circumstances we found that
our method achieves almost 65% precision in P@1,
which suggests that it can potentially perform with
high precision if the answer candidates given by the
answer retrieval module contain at least one correct
answer. This is the main purpose of QS3. Addition-
ally, we use QS3 for building training data, to check
whether questions that do not reflect the real-world
distribution of why-questions are useful for improv-
ing the system?s performance on ?real-world? ques-
tions (see Section 5.1).
In addition, we checked QS1, QS2 and QS3 for
questions having the same topic, to avoid the pos-
sibility that the distribution of questions is biased
towards certain topics. We manually extracted the
questions? topic words and randomly selected a sin-
gle representative question from all questions with
the same topic. For example, ?Why does Twitter
only allow 140 characters?? and ?Why is Twitter
so popular?? both have as topic Twitter. In the end
we obtained 250 questions in QS1, 250 questions in
QS2 and 350 questions in QS3.
For evaluation we prepared two test sets, Set1 and
Set2. Set1 contains question-answer pairs whose
questions are taken from QS1 and QS2. In our ex-
periment, we evaluate systems with 10-fold cross
validation on Set1. Set2 has question-answer pairs
whose questions are from QS3. Set2 is mainly used
for estimating estimate the ideal-case performance
of our method with a perfect answer retrieval mod-
ule. Furthermore Set2 is used as additional training
data in evaluating systems with 10-fold cross vali-
dation on Set1. We used our answer retrieval sys-
tem to obtain the top-20 answer candidates for each
question, and all question-answer (candidate) pairs
were checked by three annotators, where their inter-
rater agreement (Fleiss? kappa) was 0.634, indicat-
ing substantial agreement. Finally, correct answers
to each question were determined by majority vote.
374
Q1:???????????????????????????????????????????
(Why does the increase of greenhouse gases such as carbon dioxide in the atmosphere lead to a rise of ocean level?)
A1: .. ????????????????????????????????????????????????????????
??????????????????????????????????????? ... ???????????????????
???????????????????????
(The burning of fossil fuels contributes to the increase of atmospheric concentrations of greenhouse gases and this makes the atmosphere absorb more
thermal radiation. As a result, Earth?s average surface temperature increases. This is global warming. ... There are warnings that the increase of sea
water and melting of polar ice due to the global warming may cause sea-surface height to rise by 9?88 cm on average.
Q2:?????????????????????????????
(Why does hemoglobin deficiency cause lack of oxygen in the human body?)
A2:... ????????????????????????????????????????????????????????
?????????????????????????????????????????????????????????..
(... Hemoglobin has an important role in the human body of carrying oxygen to the organs and transferring carbon dioxide back to the lungs, to be
dispensed from the organism. If the amount of hemoglobin produced by the body is insufficient due to iron deficiency, the amount of oxygen delivered
throughout the body decreases, causing oxygen deficiency. ... )
Table 3: Correct question-answer pairs in our test set
Table 3 shows a sample of correct question-answer
pairs in our test set. Please see the supplementary
materials of this paper for more examples.
Note that word and phrase polarities are not con-
sidered by the annotators in building our test sets
and these polarities are automatically identified us-
ing a word polarity dictionary and opinion extraction
tool. We confirmed that about 35% of questions and
40% of answer candidates had at least one sentiment
phrase by opinion extraction tool, and about 45% of
questions and 85% of answer candidates contained
at least one word having polarity by a word polarity
dictionary.
5 Experiments
We use TinySVM11 with a linear kernel for training
our re-ranker. Evaluation was done by P@1 (Pre-
cision of the top answer) and MAP (Mean Average
Precision). P@1 measures how many questions have
a correct top answer candidate. MAP, widely used in
evaluation of IR systems, measures the overall qual-
ity of the top-n answer candidates (n=20 in this ex-
periment) using the formula:
MAP =
1
|Q|
?
q?Q
?n
k=1(Prec(k)? rel(k))
|Aq|
(3)
Here Q is a set of why-questions, Aq is a set of cor-
rect answers to why-question q ? Q, Prec(k) is the
precision at cut-off k in the top-n answer candidates,
rel(k) is an indicator, 1 if the item at rank k is a cor-
rect answer in Aq, 0 otherwise.
We evaluated all systems using 10-fold cross val-
idation in two ways. In the first setting we per-
formed 10-fold cross validation on Set1. Set1 con-
11 http://chasen.org/?taku/software/TinySVM/
sists of 10,000 question-answer pairs (500 questions
with their 20 answer candidates), and was parti-
tioned into 10 subsamples such that the questions
in one subsample do not overlap with those of the
other subsamples. 9 subsamples (9,000 question-
answer pairs) were used as training data and the
remaining subsample (1,000 question-answer pairs)
was retained as test data. This experiment is called
CV(Set1). It shows the effect of answer re-ranking
when evaluating our proposed method with train-
ing data built with real world why-questions alone.
In the second setting, we used the same 10 sub-
samples of Set1 as in CV(Set1) and exploited Set2
(composed of 7,000 question-answer pairs) as ad-
ditional training data for 10-fold cross validation.
As a result, in each fold 16,000 question-answer
pairs (9,000 from Set1 and 7,000 from Set2) were
used as training data for re-rankers, and all systems
were evaluated on the remaining 1,000 question-
answer pair subsample from Set1. We call this set-
ting CV(Set1+Set2). It verifies whether training
data that does not necessarily reflect a real-world
distribution of why-questions can improve why-QA
performance on real-world questions.
5.1 Results
Table 4 shows the evaluation results of six different
systems. For each system, we represent the perfor-
mance in P@1 and MAP. B-QA is a system of our
answer retrieval and the other five re-rank top-20 an-
swer candidates using their own re-ranker.
B-QA: our answer retrieval system, our implemen-
tation of Murata et al2007).
B-Ranker: a system that has a re-ranker trained
with morphological and syntactic analysis
(MSA) features alone.
375
System CV(Set1) CV(Set1+Set2)P@1 MAP P@1 MAP
B-QA 0.222 (0.368) 0.270 (0.447) 0.222 (0.368) 0.270 (0.447)
B-Ranker 0.256 (0.424) 0.319 (0.528) 0.274 (0.454) 0.323 (0.535)
B-Ranker+CR 0.262 (0.434) 0.319 (0.528) 0.278 (0.460) 0.325 (0.538)
B-Ranker+WN 0.257 (0.425) 0.320 (0.530) 0.275 (0.455) 0.325 (0.538)
Proposed 0.336 (0.56) 0.377 (0.624) 0.374 (0.619) 0.391 (0.647)
UpperBound 0.604 (1) 0.604 (1) 0.604 (1) 0.604 (1)
Table 4: Comparison of systems
B-Ranker+CR: a system has a re-ranker trained
with our MSA features and the causal relation
(CR) features used in Higashinaka and Isozaki
(2008). The CR features include binary fea-
tures indicating whether an answer candidate
contains a causal relation pattern, which causal
relation pattern the answer candidate has, and
whether the question-answer pair contains a
causal relation instance ? cause in the answer,
effect in the question). We acquired causal
relation instances from our target corpus us-
ing the method from (De Saeger et al2009),
and exploited the top-100,000 causal relation
instances and the patterns that extracted them
for CR features. Note that these CR features
are introduced only for comparing our semantic
features with ones in Higashinaka and Isozaki
(2008) and they are not a part of our method.
B-Ranker+WN: its re-ranker is trained with our
MSA features and the WordNet features in Ver-
berne et al2010). The WordNet features in-
clude the percentage of the question terms and
their synonyms in WordNet synsets found in
an answer candidate and the semantic related-
ness score between a question and its answer
candidate, the average of the concept similar-
ity between each question term and all of the
answer terms by WordNet::Similarity (Peder-
sen et al2004). We used the Japanese Word-
Net 1.1 (Bond et al2009) for these WordNet
features. Note that the Japanese WordNet 1.1
has 93,834 Japanese words linked to 57,238
WordNet synsets, while the English WordNet
3.0 covers 155,287 words linked to 117,659
synsets. Due to this lower coverage, the Word-
Net features in Japanese may have a less power
for finding a correct answer than those in En-
glish used in Verberne et al2010).
Proposed: our proposed method. All of the MSA,
SWC and SA features are used for training our
re-ranker.
UpperBound: a system that ranks all n correct an-
swers as the top n results of the 20 answer can-
didates if there are any. This indicates the per-
formance upperbound in this experiment. The
relative performance of each system compared
to UpperBound is shown in parentheses.
The proposed method achieved the best perfor-
mance both in CV(Set1) and CV(Set1+Set2). Our
method shows a significant improvement (11.4?
15.2% in P@1 and 10.7?12.1% in MAP) over our
answer retrieval method, B-QA. Its improvement
over B-Ranker, B-Ranker+CR and B-Ranker+WN
(7.6?10% in P@1 and 5.7?6.6% in MAP) shows
the effectiveness of our proposed feature set over
the features used in previous works. Both B-
Ranker+CR and B-Ranker+WN did not show signif-
icant performance improvement over B-Ranker. At
least in our setting, the causal relation and WordNet
features did not prove effective. The performance
gap between B-Ranker and B-QA (3.4?5.2% in P@1
and 4.9?5.3% in MAP) suggests the effectiveness
of re-ranking. All systems consistently show better
performance in CV(Set1+Set2) than CV(Set1). This
suggests that training data built with why-questions
that does not reflect real-world distribution of why-
questions is useful in training re-rankers.
We investigate the contribution of each type of
features to the performance by removing one fea-
ture set from the all feature sets in training our re-
ranker. In this experiment, we split SA into SA@W
(features expressing words and their polarity) and
SA@P (features expressing phrases and their po-
larity) to investigate their contribution either. The
results are summarized in Table 5.
In Table 5, MSA+SWC+SA represents our pro-
posed method using all feature sets. The perfor-
mance gap between MSA+SWC+SA and the others
confirms that all the features contributed to a higher
376
System CV(Set1) CV(Set1+Set2)P@1 MAP P@1 MAP
SWC+SA 0.302 0.324 0.314 0.332
MSA+SWC 0.308 0.349 0.318 0.358
MSA+SA 0.300 0.352 0.314 0.364
MSA+SWC+SA@W 0.312 0.358 0.325 0.365
MSA+SWC+SA@P 0.323 0.369 0.358 0.384
MSA+SWC+SA 0.336 0.377 0.374 0.391
UpperBound 0.604 0.604 0.604 0.604
Table 5: Evaluation with different combination of feature
sets used in training our re-ranker
performance. The significant performance improve-
ment by SA (features from sentiment analysis) and
SWC (features from semantic word classes) (The
gap between MSA+SWC+SA and MSA+SWC was
2.8?6% and that between MSA+SWC+SA and
MSA+SA was 3.6%?6% in P@1) supports the hy-
pothesis for sentiment analysis and semantic word
classes in this paper.
Though the performance gap between
MSA+SWC+SA and MSA+SWC+SA@P
(1.3%?1.6% in P@1) shows that SA@W is
useful in training our re-ranker, we found that
MSA+SWC+SA@W made only 0.4?0.7% im-
provement over MSA+SWC. We believe that this
is mainly because SA@W and SWC are based on
semantic and sentiment information at the word
level, and these often capture a similar type of
information. For instance, disease names that are
grouped together into one class in SWC are typi-
cally classified as negative in SA@W. Therefore the
similarity in the information provided by SA@W
and SWC causes a classifier trained with both of
these features to obtain only a minor improvement
over a classifier using only one of the features.
To estimate the ideal-case performance of our
proposed method, we made another experiment by
using Set1 as training data for our re-ranker and
Set2 as test data for evaluating our proposed method.
Here, we assume a perfect answer retrieval module
that adds the source passage that was used for gener-
ating the original why-question in Set2 as a correct
answer to the set of existing answer candidates, giv-
ing 21 answer candidates. The performance of our
method in this setting was 64.8% in P@1 and 66.6%
in MAP. This evaluation result suggests that our re-
ranker can potentially perform with high precision
when at least one correct answer in answer candi-
dates is given by the answer retrieval module.
6 Related Work
In the QA literature, Higashinaka and Isozaki
(2008), Verberne et al2010), and Surdeanu et al
(2011) are closest to our work. The first two deal
with why-questions, the last with how-questions.
Similar to our method, they use machine learn-
ing techniques to re-rank answer candidates to non-
factoid questions based on various combinations of
syntactic, semantic and other statistical features such
as the density and frequency of question terms in the
answer candidates and patterns for causal relations
in the answer candidates. Especially for why-QA,
Higashinaka and Isozaki (2008) used causal relation
features and Verberne et al2010) exploited Word-
Net features as a kind of semantic features for train-
ing their re-ranker, where we used these features, re-
spectively, for B-Ranker+CR and B-Ranker+WN in
our experiment.
Our work differs from the above approaches in
that we propose semantic word classes and senti-
ment analysis as a new type of semantic features,
and show their usefulness in why-QA. Sentiment
analysis has been used before on the slightly un-
usual task of opinion question answering, where the
system is asked to answer subjective opinion ques-
tions (Stoyanov et al2005; Dang, 2008; Li et al
2009). To the best of our knowledge though, no pre-
vious work has systematically explored the use of
sentiment analysis in a general QA setting beyond
opinion questions.
7 Conclusion
In this paper, we have explored the utility of senti-
ment analysis and semantic word classes for ranking
answer candidates to why-questions. We proposed a
set of semantic features that exploit sentiment anal-
ysis and semantic word classes obtained from large-
scale noun clustering, and used them to train an an-
swer candidate re-ranker. Through a series of exper-
iments on 850 why-questions, we showed that the
proposed semantic features were effective in identi-
fying correct answers, and our proposed method ob-
tained more than 15% improvement in precision of
its top answer (P@1) over our baseline, a state-of-
the-art IR based QA system. We plan to use new se-
mantic knowledge such as semantic orientation, ex-
citatory or inhibitory, proposed in Hashimoto et al
(2012) for improving why-QA.
377
References
Francis Bond, Hitoshi Isahara, Sanae Fujita, Kiyotaka
Uchimoto, Takayuki Kuribayashi, and Kyoko Kan-
zaki. 2009. Enhancing the japanese wordnet. In Pro-
ceedings of the 7th Workshop on Asian Language Re-
sources, pages 1?8.
Hoa Tran Dang. 2008. Overview of the TAC 2008 opin-
ion question answering and summarization tasks. In
Proc. TAC 2008.
Stijn De Saeger, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large scale
relation acquisition using class dependent patterns. In
Proc. of ICDM 2009, pages 764?769.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John M.
Prager, Nico Schlaefer, and Christopher A. Welty.
2010. Building Watson: An overview of the DeepQA
project. AI Magazine, 31(3):59?79.
Junichi Fukumoto, Tsuneaki Kato, Fumito Masui, and
Tsunenori Mori. 2007. An overview of the 4th ques-
tion answering challenge (QAC-4) at NTCIR work-
shop 6. In Proc. of NTCIR-6.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger,
Jong-Hoon Oh, and Jun?ichi Kazama. 2012. Excita-
tory or inhibitory: A new semantic orientation extracts
contradiction and causality from the web. In Proceed-
ings of EMNLP-CoNLL 2012.
Ryuichiro Higashinaka and Hideki Isozaki. 2008.
Corpus-based question answering for why-questions.
In Proc. of IJCNLP, pages 418?425.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proc. of the 22nd annual international
ACM SIGIR conference on Research and development
in information retrieval, SIGIR ?99, pages 50?57.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-scale
clustering of dependency relations. In Proc. of ACL-
08: HLT, pages 407?415.
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan
Zhu. 2009. Answering opinion questions with ran-
dom walks on graphs. In Proc. of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2, pages
737?745.
Masaki Murata, Sachiyo Tsukawaki, Toshiyuki Kana-
maru, Qing Ma, and Hitoshi Isahara. 2007. A system
for answering non-factoid Japanese questions by using
passage retrieval weighted based on type of answer. In
Proc. of NTCIR-6.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classification
using CRFs with hidden variables. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 786?794, Los An-
geles, California, June. Association for Computational
Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proc. of the 2002 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 79?86.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity: measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004, HLT-NAACL?Demonstrations
?04, pages 38?41.
Anselmo Pe?as, Eduard H. Hovy, Pamela Forner, ?lvaro
Rodrigo, Richard F. E. Sutcliffe, Corina Forascu, and
Caroline Sporleder. 2011. Overview of QA4MRE at
CLEF 2011: Question answering for machine reading
evaluation. In CLEF.
Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-perspective question answering using the
opqa corpus. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods in
Natural Language Processing, HLT ?05, pages 923?
930.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computa-
tional Linguistics, 37(2):351?383.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proc. of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 417?424.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc., New
York, NY, USA.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-
Arno Coppen. 2007. Evaluating discourse-based an-
swer extraction for why-question answering. In SIGIR,
pages 735?736.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-
Arno Coppen. 2010. What is not in the bag of words
for why-QA? Computational Linguistics, 36:229?
245.
Ellen M. Voorhees. 2004. Overview of the TREC 2004
question answering track. In TREC.
378
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 619?630, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Excitatory or Inhibitory: A New Semantic Orientation Extracts
Contradiction and Causality from the Web
Chikara Hashimoto? Kentaro Torisawa? Stijn De Saeger?
Jong-Hoon Oh? Jun?ichi Kazama?
National Institute of Information and Communications Technology
Kyoto, 619-0289, JAPAN
{? ch, ? torisawa, ? stijn, ? rovellia, ?kazama}@nict.go.jp
Abstract
We propose a new semantic orientation, Ex-
citation, and its automatic acquisition method.
Excitation is a semantic property of predicates
that classifies them into excitatory, inhibitory
and neutral. We show that Excitation is useful
for extracting contradiction pairs (e.g., destroy
cancer ? develop cancer) and causality pairs
(e.g., increase in crime ? heighten anxiety).
Our experiments show that with automatically
acquired Excitation knowledge we can extract
one million contradiction pairs and 500,000
causality pairs with about 70% precision from
a 600 million page Web corpus. Furthermore,
by combining these extracted causality and
contradiction pairs, we can generate one mil-
lion plausible causality hypotheses that are not
written in any single sentence in our corpus
with reasonable precision.
1 Introduction
Recognizing semantic relations between events in
texts is crucial for such NLP tasks as question an-
swering (QA). For example, to answer the question
?What ruined the crops in Japan?? a QA system
must recognize that the sentence ?the Fukushima
nuclear power plant caused radioactive pollution
and contaminated the crops in Japan? contains a
causal relation and that contaminate crops entails
ruin crops but contradicts preserve crops.
To facilitate the acquisition of causality, contra-
diction, paraphrase and entailment relations between
events we propose a new semantic orientation, Ex-
citation, that classifies unary predicates (templates,
hereafter) into excitatory, inhibitory and neutral. An
excitatory template entails that the main function or
effect of the referent of its argument is activated or
enhanced (e.g., cause X, preserve X), while an in-
hibitory template entails that it is deactivated or sup-
pressed (e.g., ruin X, contaminate X, prevent X).
Excitation is useful for extracting contradiction;
if two templates with similar distributional profiles
have opposite Excitation polarities, they tend to be
contradictions (e.g., contaminate crops and preserve
crops). With extracted contradictions we can distin-
guish paraphrases from contradictions among distri-
butionally similar phrases. Furthermore, contradic-
tion in itself is important knowledge for Recogniz-
ing Textual Entailment (RTE) (Voorhees, 2008).
Excitation is also a powerful indicator of causal-
ity. In the physical world, the activation or de-
activation of one thing often causes the activation
or deactivation of another. Two excitatory or in-
hibitory templates that co-occur in some temporal
or logical order in the same narrative often describe
a causal chain of events, like ?the Fukushima nu-
clear power plant caused radioactive pollution and
contaminated crops in Japan?.
In this paper we propose both the concept of Ex-
citation and an automatic method for its acquisition.
Our method acquires Excitation templates based on
certain natural, language independent constraints on
narrative structures found in text. We also propose
acquisition methods for contradiction and causal-
ity relations based on Excitation. Our methods ex-
tract one million contradiction pairs with over 70%
precision, and 500,000 causality pairs with about
70% precision from a 600 million page Web corpus.
Moreover, by combining these extracted causality
pairs and contradiction pairs, we generated one mil-
lion plausible causality hypotheses that were not
619
written in any single sentence in our corpus with rea-
sonable precision. For example, a causality hypoth-
esis prevent radioactive pollution ? preserve crops
can be generated from an extracted causality cause
radioactive pollution ? contaminate crops.
We target the Japanese language in this paper.
2 What is Excitation?
Excitation classifies templates into excitatory, in-
hibitory, and neutral, as explained below.
excitatory templates entail that the function, ef-
fect, purpose or role of their argument?s refer-
ent is activated or enhanced. (e.g., cause X, buy
X, produce X, import X, increase X, enable X)
inhibitory templates entail that the function, ef-
fect, purpose or role of their argument?s refer-
ent is deactivated or suppressed. (e.g., prevent
X, discard X, remedy X, decrease X, disable X)
neutral templates are neither excitatory nor in-
hibitory. (e.g., consider X, proportional to X,
related to X, evaluate X, close to X)
For example, when fire fills the X slot of cause X,
it suggests that the effect of fire is activated. If pre-
vent X?s slot is filled with flu, the effect of flu is sup-
pressed. In this study, we aim to acquire excitatory
and inhibitory templates that are useful for extract-
ing contradiction and causality, though neutral tem-
plates are the most frequent in our data (See Section
5.1). Collectively we call excitatory and inhibitory
templates Excitation templates, and excitatory and
inhibitory two opposite polarities.
Excitation is independent of the good/bad seman-
tic orientation. (Hatzivassiloglou and McKeown,
1997; Turney, 2002; Rao and Ravichandran, 2009).
For example, sophisticate X and complicate X are
both excitatory, but only the former has a positive
connotation. Similarly, remedy X and degrade X are
both inhibitory but only the latter is negative.
General Inquirer (Stone et al1966) deals with
semantic factors some of which were proposed by
Osgood et al1957). Their ?activity? factor involves
binary opposition between ?active? and ?passive.?
Notice that activity and Excitation are independent.
In General Inquirer, both accelerate X and abolish
X are active, but only the former is excitatory. Both
accept X and abate X are passive, but only the lat-
ter is inhibitory. Pustejovsky (1995) proposed telic
and agentive roles, which inspired our excitatory no-
tion, but they have no corresponding notion of in-
hibitory. Andreevskaia and Bergler (2006) acquired
the increase/decrease semantic orientation, which is
a subclass of Excitation.
Excitation is inverted if a template?s predicate is
negated. For example, preserve X is excitatory,
while don?t preserve X is inhibitory. We acknowl-
edge that this may seem somewhat counter-intuitive
and will address this issue in future work.
3 Excitation Template Acquisition
This section presents our acquisition method of Ex-
citation templates. We introduce constraints in the
co-occurrence of templates in text that seem both ro-
bust and language independent in Section 3.1. Our
method exploits these constraints for the acquisition
of Excitation templates. First we construct a tem-
plate network where nodes are templates and links
represent that two connected templates have either
SAME or OPPOSITE polarities. Given 46 manually
prepared seed templates we calculate the Excitation
value of each template, a value in range [?1, 1] that
is positive if the template is excitatory and negative
if it is inhibitory. Technically, our method treats all
templates as excitatory or inhibitory, and, upon com-
pletion, regards templates with small absolute Exci-
tation values as neutral.
The whole method is a bootstrapping process.
Each iteration expands the network and the Excita-
tion value of each template is (re-)calculated.
3.1 Characteristics of Excitation Templates
Our method exploits natural discourse constraints on
the possible combinations of (a) the polarity of co-
occurring templates, (b) the nouns that fill their ar-
gument slots and (c) the connectives that link the
templates in a given sentence. Table 1 shows the
constraints and Figure 1 shows examples that will
be explained shortly. Though our target is Japanese
we believe these constraints are universal discourse
principles, and as such not language dependent. Ex-
amples are given in English for ease of explanation.
We first identify two categories of connectives
in our target sentences: AND/THUS-type (e.g., and,
thus and since) and BUT-type (e.g., but and though).
Both types suggest a sort of consistency or inconsis-
tency between predicates. We manually classified
169 frequently used connectives into AND/THUS-
620
(1) He smoked cigarettes, AND/THUS he suffered lung
cancer. (Both smoke X and suffer X are excitatory.)
(2) He quit cigarettes, AND/THUS was immune from lung
cancer. (quit X and immune from X are inhibitory.)
(3) He smoked cigarettes, BUT didn?t suffer lung cancer.
(smoke X is excitatory, not suffer X is inhibitory.)
(4) He quit cigarettes, BUT he suffered lung cancer. (quit
X is inhibitory, but suffer X is excitatory.)
(5) He underwent cancer treatment, AND/THUS he could
cure the cancer. (undergo X is excitatory, cure X is
inhibitory.)
(6) He underwent cancer treatment, BUT still had cancer.
(Both undergo X and have X are excitatory.)
(7) Unnatural: He smoked cigarettes, BUT he suffered
lung cancer. (smoke X and suffer X are excitatory.)
Figure 1: Examples of constraints: (cigarettes, lung can-
cer) is PNP and (cancer treatment, cancer) is NNP.
PNPs NNPs others
AND/THUS SAME OPPOSITE N/A
BUT OPPOSITE SAME N/A
Table 1: Constraint matrix.
and BUT-type (See supplementary materials).
Next we extract sentences from the Web in which
two templates co-occur and are joined by one of
these connectives, and then classify the noun pairs
filling the templates? argument slots into ?positively-
associated? and ?negatively-associated? noun pairs
(PNPs and NNPs). Mirroring our definition of Excita-
tion, PNPs are noun pairs in which the referent of the
first noun facilitates the emergence of the referent
of the second noun. PNPs can range from causally
related noun pairs like (cigarettes, lung cancer) to
?material-product? relation pairs like (semiconduc-
tor, electronic circuit). We found that PNPs only
fill the argument slots of (a) same Excitation polar-
ity templates connected by AND/THUS-type connec-
tives (examples 1 and 2 in Figure 1), or (b) opposite
Excitation polarity templates connected by a BUT-
type connectives (examples 3 and 4). Violating such
constraints (example 7) seems unnatural. Similarly,
NNPs are noun pairs in which the referent of one
noun suppresses the emergence of the referent of the
other noun. Examples include such ?inverse causal-
ity? pairs as (cancer treatment, cancer). NNPs only
fill the argument slots of (a) opposite Excitation po-
larity templates connected by AND/THUS-type con-
nectives (example 5), or (b) same polarity templates
connected by a BUT-type connective (example 6).
All these constraints are summarized in Table 1,
which we will call the constraint matrix. Accord-
ing to the constraint matrix, we can know whether
two templates? polarities are the same or opposite if
we know whether a noun pair filling the two tem-
plates? slots is PNP or NNP. Conversely, we can
know whether a noun pair is PNP or NNP if we know
whether two templates whose slots are filled with
the noun pair have the same or opposite polarities.
We believe these constraints capture certain univer-
sal principles of discourse, since it is difficult in any
language to produce natural sounding sentences that
violate these constraints. We empirically confirm
their validity for Japanese in Section 5.1.
3.2 Bootstrapping Approach to Excitation
Template Acquisition
To calculate the Excitation values for the templates,
we construct a template network where templates
are connected by links indicating polarity agreement
between two connected templates (either SAME or
OPPOSITE polarity), as determined by the constraint
matrix. Excitation values are determined by spread-
ing activation applied to the network, given a small
number of manually prepared seed templates.
However, we cannot construct the network unless
we know whether each noun pair is PNP or NNP, due
to the configuration of the constraint matrix, and cur-
rently we have no feasible method to classify all of
them into PNPs and NNPs in advance. We therefore
adopt a bootstrapping method (Figure 2) that starts
from manually prepared excitatory and inhibitory
seed templates (Step 1 in Figure 2). Our method
begins by extracting noun pairs from the Web that
co-occur with two seed templates connected by a
AND/THUS- or BUT-type connective, and classifies
these noun pairs into PNPs and NNPs based on the
constraint matrix (Steps 2 and 3). Next, we automat-
ically extract additional (non-seed) template pairs
from the Web that co-occur with these PNPs and
NNPs. Links (either SAME or OPPOSITE) between
all template pairs are determined by the constraint
matrix (Step 4), and we construct a template network
from both seed and non-seed template pairs (Step 5).
Our method calculates the Excitation values for
all the templates in the network by first assign-
ing Excitation values +1 and ?1 to the excitatory
and inhibitory seed templates, and applies a spread-
ing activation method proposed by Takamura et al
(2005) (Step 6) to the network. This method calcu-
621
1. Prepare initial seed templates with fixed excitation values (either
+1 or ?1).
2. Make seed template pairs that are combinations of two seed tem-
plates and a connective (either AND/THUS-type or BUT-type).
3. Extract noun pairs that co-occur with one of the seed template
pairs from the Web. Classify the noun pairs into PNPs and NNPs
based on the constraints matrix. Filter out those noun pairs that
appear as both PNP and NNP on the Web or those whose occur-
rence frequency is less than or equal to F, which is set to 5.
4. Extract additional (non-seed) template pairs that are filled by one
of the PNPs or NNPs from the Web. Determine the link type
(SAME or OPPOSITE) for each template pair based on the con-
straint matrix. If a template pair appears on the Web as having
both link types, we determine its link type by majority vote.
5. Construct the template network from all the template pairs. Re-
move from the network those templates whose number of linked
templates is less than D, which is set to 5.
6. Apply Takamura et al method to the network and fix the Exci-
tation value of each template.
7. Extract the top- and bottom-ranked N ? i templates from the
result of Takamura et al method. N is a constant, which is
set to 30. i is the iteration number. They are used as additional
seed templates for the next iteration. The top-ranked templates
are given Excitation value +1 and the bottom-ranked templates
are assigned ?1. Go to Step 2.
Figure 2: Bootstrapping for template acquisition.
lates all templates? excitation values by solving the
network constraints imposed by the SAME and OP-
POSITE links, and the Excitation values of the seed
templates (This method is detailed in Section 3.3).
In each iteration i, our method selects the N ? i top-
ranked and bottom-ranked templates as additional
seed templates for the next iteration (N is set to 30)
(Step 7). Our method then constructs a new tem-
plate network using the augmented seed templates
and restarts the calculation process. Figure 2 sum-
marizes our bootstrapping process.
Bootstrapping stops after M iterations, with M
set to 7 based on our preliminary experiments.
To prepare the initial seed templates we con-
structed a maximal template network that could in
theory be created by our bootstrapping method. This
maximal network consists of any two templates that
co-occur in a sentence with any connective, regard-
less of their arguments. We manually selected 36
excitatory and 10 inhibitory seed templates from
among 114 templates with the most links in the net-
work (See supplementary materials).
3.3 Determining Excitation in the Network
This section details Step 6 of our bootstrapping
method, i.e., how Takamura et al method calcu-
lates the Excitation value of each template. Their
method is based on the spin model in physics, where
each electron has a spin of either up or down. We
chose this method due to the straightforward parallel
between the spin model and our Excitation template
model. Both models capture the spreading of acti-
vation (either spin direction or excitation polarity)
between neighboring objects in a network. Deter-
mining the optimal algorithm for this task is beyond
our current scope, but for the purpose of our experi-
ments we found that Takamura et al method gave
satisfactory results.
The spin model defines an energy function on a
spin network, and each electron?s spin can be esti-
mated by minimizing this function:
E(x,W ) = ?1/2? ?ijwijxixj
Here, xi, xj ? x are spins of electrons i and j, and
matrix W = {wij} assigns weights to links between
electrons. We regard templates as electrons and Ex-
citation polarities as their spins (up and down corre-
spond to excitatory and inhibitory). We define the
weight wij of the link between templates i and j as:
wij =
{
1/
?
d(i)d(j) if SAME(i, j)
?1/
?
d(i)d(j) if OPPOSITE(i, j)
Here, d(i) denotes the number of templates linked
to i. SAME(i, j) (OPPOSITE(i, j)) indicates a SAME
(OPPOSITE) link exists between i and j. We obtain
excitation values by minimizing the above energy
function. Note that after minimizing E, xi and xj
tend to get the same polarity when wij is positive.
When wij is negative, xi and xj tend to have op-
posite polarities. Initially seed templates are given
values +1 or ?1 depending on whether they are ex-
citatory or inhibitory, and others are given 0.
We used SUPPIN (http://www.lr.pi.titech.
ac.jp/?takamura/pubs/SUPPIN-0.01.tar.gz),
an implementation of Takamura et al method. Its
parameter ? is set to the default value (0.75).
4 Knowledge Acquisition by Excitation
This section shows how the concept of Excitation
can be used for automatic knowledge acquisition.
4.1 Contradiction Extraction
Our first knowledge acquisition method extracts
contradiction pairs like destroy cancer ? develop
cancer, based on our assumption that they often con-
sist of distributionally similar templates that have a
sharp contrast in Excitation value. Concretely, we
622
extract two phrases as a contradiction pair if (a)
their templates have opposite Excitation polarities,
(b) they share the same argument noun, and (c) the
part-of-speech of their predicates is the same. Then
the contradiction pairs are ranked by Ct:
Ct(p1, p2) = |s1| ? |s2| ? sim(t1, t2)
Here p1 and p2 are two phrases that satisfy condi-
tions (a), (b) and (c) above, t1 and t2 are their re-
spective templates, and |s1| and |s2| are the absolute
values of t1 and t2?s excitation values. sim(t1, t2) is
the distributional similarity proposed by Lin (1998).
Note that ?contradiction? here includes what we
call ?quasi-contradiction.? This consists of two
phrases such that, if the tendencies of the events they
describe get stronger, they eventually become con-
tradictions. For example, the pair emit smells ? re-
duce smells is not logically contradictory since the
two events can happen at the same time. However,
they become almost contradictory when their ten-
dencies get stronger (i.e., emit smells more strongly
? thoroughly reduce smells). We believe quasi-
contradictions are useful for NLP tasks.
4.2 Causality Extraction
Our second knowledge acquisition method extracts
causality pairs like increase in crime ? heighten
anxiety that co-occur with AND/THUS-type connec-
tives in a sentence. The assumption is that if two
templates (t1 and t2) with a strong Excitation ten-
dency are connected by an AND/THUS-type connec-
tive in a sentence, the event described by t1 and its
argument n1 tends to be a cause of the event de-
scribed by t2 and its argument n2. Here, Excitation
strength is expressed by absolute Excitation values.
The intuition is that, if the referent of n1 is strongly
activated or suppressed, it tends to have some causal
effect on the referent of n2 in the same sentence.
We focus on extracting causality pairs that co-
occur with only ?non-causal connectives? like and,
which are AND/THUS-type connectives that do NOT
explicitly signal causality, since causal connectives
like thus can mask the effectiveness of Excitation.
We prepared 139 non-causal connectives (See sup-
plementary materials). We extract two templates
such as increase in X and heighten Y co-occurring
with only non-causal connectives, as well as the
noun pair that fills the two templates? slots (e.g.,
(crime, anxiety)) to obtain causal phrase pairs. In
Japanese, the temporal order between events is usu-
ally determined by precedence in the sentence. Cs
ranks the obtained causality pairs:
Cs(p1, p2) = |s1| ? |s2|
Here p1 and p2 are the phrases of causality pair, and
|s1| and |s2| are absolute Excitation values of p1?s
and p2?s templates. As is common in the literature,
this notion of causality should be interpreted prob-
abilistically rather than logically, i.e., we interpret
causality A ? B as ?if A happens, the probability of
B increases?. This interpretation is often more use-
ful for NLP tasks than a strict logical interpretation.
4.3 Causality Hypothesis Generation
Our third knowledge acquisition method generates
plausible causality hypotheses that are not written in
any single sentence using the previously extracted
contradiction and causality pairs. We assume that if
a causal relation (e.g., increase in crime ? heighten
anxiety ) is valid, its inverse (e.g., decrease in crime
? diminish anxiety ) is often valid as well. From
a logical definition of causation, taking the inverse
of an implication obviously does not preserve valid-
ity. However, at least under our probabilistic inter-
pretation, taking the inverse of a given causality pair
using the extracted contradiction pairs proves to be
a viable strategy for generating non-trivial causality
hypotheses, as our experiments in Section 5.4 show.
For an extracted causality pair, we generate its
inverse as a causality hypothesis by replacing both
phrases in the original pair with their contradiction
counterparts. For instance, a causality hypothesis
decrease in crime ? diminish anxiety is generated
from a causality increase in crime ? heighten anxi-
ety by two contradictions, decrease in crime ? in-
crease in crime and diminish anxiety ? heighten
anxiety. Since we are interested in finding new
causal hypotheses, we filter out hypotheses whose
phrase pair co-occurs in a sentence in our corpus.
Remaining causality hypotheses are ranked by Hp.
Hp(q1, q2) = Ct(p1, q1)? Ct(p2, q2)? Cs?(p1, p2)
Here, q1 and q2 are two phrases of a causality hy-
pothesis. p1 and p2 are two phrases of a hypothesis?s
original causality. That is, p1 ? q1 and p2 ? q2 are
contradiction pairs, and Ct(p1, q1) and Ct(p2, q2)
are their contradiction scores. Cs?(p1, p2) is the
original causality?s causality score. Cs? can be Cs
623
from Section 4.2, but based on preliminary experi-
ments we found the following score works better:
Cs?(p1, p2) = |s1| ? |s2| ? npfreq(n1, n2)
|s1| and |s2| are absolute Excitation values of p1?s
and p2?s templates, whose slots are filled with n1 and
n2. npfreq(n1, n2) is the co-occurrence frequency
of (n1, n2) with polarity-identical template pairs (if
(n1, n2) is PNP) or with polarity-opposite template
pairs (if (n1, n2) is NNP). Thus, npfreq indicates a
sort of association strength between two nouns.
5 Experiments
This section shows that our template acquisition
method acquired many Excitation templates. More-
over, using only the acquired templates we extracted
one million contradiction pairs with more than 70%
precision, and 500,000 causality pairs with about
70% precision. Further, using only these extracted
contradiction and causality pairs we generated one
million causality hypotheses with 57% precision.
In our experiments we removed evaluation sam-
ples containing the initial seed templates and exam-
ples used for annotation instruction from the evalua-
tion data. Three annotators (not the authors) marked
all evaluation samples, which were randomly shuf-
fled so that they could not identify which sample was
produced by which method. Information about the
predicted labels or ranks was also removed from the
evaluation data. Final judgments were made by ma-
jority vote between the annotators. They were non-
experts without formal training in linguistics or se-
mantics. See supplementary materials for our anno-
tation manuals (translated into English).
We used 600 million Japanese Web pages
(Akamine et al2010) parsed by KNP (Kawahara
and Kurohashi, 2006) as a corpus. We restricted
the argument positions of templates to ha (topic),
ga (nominative), wo (accusative), ni (dative), and de
(instrumental). We discarded templates appearing
fewer than 20 times in compound sentences (regard-
less of connectives) in our corpus.
5.1 Excitation Template Acquisition
We show that our proposed method for template ex-
traction (PROPtmp) successfully acquired many Ex-
citation templates from which we obtained a huge
number of contradiction and causality pairs, and that
Excitation is a reasonably comprehensible notion
even for non-experts. We also show that PROPtmp
outperformed two baselines by a large margin.
The template network constructed by PROPtmp
contained 10,825 templates. Among these, the boot-
strapping process classified 8,685 templates as exci-
tatory and 2,140 as inhibitory. Note that these can-
didates in fact also contain neutral templates, as ex-
plained at the beginning of Section 3.
Baselines The baseline methods are ALLEXC and
SIM. ALLEXC regards all templates that are ran-
domly extracted from the Web as excitatory, since in
our data excitatory templates outnumber inhibitory
ones. Actually, in our data neutral templates rep-
resent the most frequent class, but since our objec-
tive is to acquire excitatory and inhibitory templates,
a baseline marking all templates as neutral would
make little sense. SIM is a distributional similarity
baseline that takes as input the same 10,825 tem-
plates of PROPtmp above, constructs a network by
connecting two templates whose distributional simi-
larity is greater than zero, and regards two connected
templates as having the same polarity. The weight
of the links between templates is set to their distri-
butional similarity based on Lin (1998). Then SIM
is given the same initial seed templates as PROPtmp,
by which it calculates the Excitation values of tem-
plates using Takamura et al method. As a result,
SIM assigned positive Excitation values to all tem-
plates, and except for the 10 inhibitory initial seed
templates no templates were regarded inhibitory.
Evaluation scheme We randomly sampled 100
templates each from PROPtmp?s 8,685 excitatory
candidates, PROPtmp?s 2,140 inhibitory candidates,
all the ALLEXC?s templates, and all the SIM?s tem-
plates, i.e., 400 templates in total. To make the an-
notators? judgements easier, we randomly filled the
argument slot of each template with a noun filling its
argument slot in our Web corpus. Three annotators
labeled each sample (a combination of a template
and a noun) as ?excitatory,? ?inhibitory,? ?neutral,? or
?undecided? if they were not sure about its label.
Results for excitatory In the top graph in Fig-
ure 3, ?Proposed? shows PROPtmp?s precision curve.
The curve is drawn from its 100 samples whose X-
axis positions represent their ranks. We plot a dot for
every 5 samples. Among the 100 samples, 37 were
judged as excitatory, 6 as inhibitory, 45 as neutral,
and 6 as ?undecided?. For the remaining 6 samples,
624
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Pre
cisio
n
?Proposed?
?Sim?
?Allexc?
 0.4
 0.6
 0.8
 1
 0  500  1000  1500  2000  2500
Pre
cisio
n
Top-N
?Proposed?
Figure 3: Precision of template acquisition: excitatory
(top) and inhibitory (bottom).
the three annotators gave three different labels and
the label was not fixed (?split-votes? hereafter). For
calculating precision, only the 37 samples labeled
excitatory were regarded as correct. PROPtmp out-
performed all baselines by a large margin, with an
estimated 70% precision for the top 2,000 templates.
?Allexc? and ?Sim? in Figure 3 denote ALLEXC and
SIM. Among ALLEXC?s 100 samples, 19 were
judged as excitatory, 5 as inhibitory, 74 as neutral,
and 2 as ?undecided?. SIM?s low performance re-
flects the fact that templates with opposite polarities
are sometimes distributionally similar, and as a re-
sult get connected by SAME links.
Results for inhibitory ?Proposed? in the bottom
graph in Figure 3 shows the precision curve drawn
from the 100 samples of PROPtmp?s inhibitory can-
didates. Among the 100 samples, 41 were judged as
inhibitory, 15 as excitatory, 32 as neutral, 4 as ?unde-
cided?, and 8 as ?split-votes?. Only the 41 inhibitory
samples were regarded as correct. From the curve
we estimate that PROPtmp achieved about 70% pre-
cision for the top 500. Note that SIM could not ac-
quire any inhibitory templates, yet we can think of
no other reasonable baseline for this task.
Inter-annotator agreement The Fleiss? kappa
(Fleiss, 1971) of annotator judgements was 0.48
(moderate agreement (Landis and Koch, 1977)). For
training, the annotators were given a one-page anno-
tation manual (see supplementary materials), which
basically described the same contents in Section 2,
in addition to 14 examples of excitatory, 14 exam-
ples of inhibitory, and 6 examples of neutral tem-
plates that were manually prepared by the authors.
Using the manual and the examples, we instructed
all the annotators face-to-face for a few hours. We
also made sure the evaluation data did not contain
any examples used during instruction.
Observations about argument positions Among
the 200 evaluation samples of PROPtmp (for both ex-
citatory and inhibitory evaluations), 52 were judged
as excitatory, 47 as inhibitory, and 77 as neutral. For
the excitatory templates, the numbers of nominative,
topic, accusative, dative, and instrumental argument
positions are 15, 11, 10, 8, and 8, respectively. For
the inhibitory templates, the numbers are 17, 11, 16,
3, and 0. For the neutral templates, the numbers are
8, 23, 17, 21, and 8. Accordingly, we found no no-
ticeable bias with regard to their numbers. Likewise,
we found no noticeable bias regarding their useful-
ness for contradiction and causality acquisition re-
ported shortly, too.
Summary PROPtmp works well, as it outperforms
the baselines. Its performance demonstrates the va-
lidity of our constraint matrix in Table 1. Besides,
since our annotators were non-experts but showed
moderate agreement, we conclude that Excitation is
a reasonably comprehensible notion.
5.2 Contradiction Extraction
This section shows that our proposed method for
contradiction extraction (PROPcont) extracted one
million contradiction pairs with more than 70% pre-
cision, and that Excitation values are useful for con-
tradiction ranking. As input for PROPcont we took
the top 2,000 excitatory and the top 500 inhibitory
templates from the previous experiment (i.e., the
other templates were regarded as neutral).
Baselines Our baseline methods are RANDcont
and PROPcont-NE. RANDcont randomly combines
two phrases, each consisting of a template and a
noun that they share. It does not rank its output.
PROPcont-NE is the same as PROPcont except that it
does not use Excitation values; ranking is based only
on sim(t1, t2). PROPcont-NE does combine phrases
with opposite template polarities, just like PROPcont.
Evaluation scheme We randomly sampled 200
phrase pairs from the top one million results of each
625
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  200000  400000  600000  800000  1e+06
Pre
cisio
n
Top-N
?Proposed?
?Proposed-ne?
?Random?
Figure 4: Precision of contradiction extraction.
PROPcont and PROPcont-NE, and 100 samples from
the output of RANDcont?s output, giving 500 sam-
ples. Three annotators labeled whether the samples
are contradictions. Fleiss? kappa was 0.78 (substan-
tial agreement).
Results ?Proposed? in Figure 4 shows the preci-
sion curve of PROPcont. PROPcont achieved an esti-
mated 70% precision for its top one million results.
Readers might wonder whether PROPcont?s output
consists of a small number of template pairs that are
filled with many different nouns. If this were the
case, PROPcont?s performance would be somewhat
misleading. However, we found that PROPcont?s 200
samples contained 194 different template pairs, sug-
gesting that our method can acquire a large variety
of contradiction phrases. ?Proposed-ne? is the pre-
cision curve for PROPcont-NE. Its precision is more
than 10% lower than PROPcont at the top one million
results. ?Random? shows that RANDcont?s precision
is only 4%. Table 2 shows examples of PROPcont?s
outputs and their English translation. The labels
?Cont,? ?Quasi? and ?6? denote whether a pair is con-
tradictory, quasi-contradictory, or not contradictory.
Among PROPcont?s 145 samples judged by the an-
notators as contradiction, 46 were judged as quasi-
contradictory by one of the authors. The first 6
case in Table 2 was caused by the template, X??
??? (improve X). It is tricky since it is excitatory
when taking arguments like function, while it is in-
hibitory when taking arguments like disorder. How-
ever, PROPtmp currently cannot distinguish these us-
ages and judged it as inhibitory in our experiments
in Section 5.1, though it must be interpreted as ex-
citatory for the 6 case. The second 6 case was due
to PROPtmp?s error; it incorrectly judged the neutral
template, X????? (related to X), as inhibitory.
Rank Contradiction Pairs Label
8,767 ??????????? ????????????? Cont
repair imbalance ? become imbalanced
103,581 ?????? ??????? Cont
assist the driver ? disturb the driver
151,338 ????????? ?????? Quasi
calm tension ? feel tension
184,014 ??????? ??????? 6
improve function ? boost function
316,881 ?????? ???????? Cont
yen depreciation stops ? yen depreciation develops
317,028 ???????? ???????? Cont
noise gets worse ? noise abates
334,642 ????? ??????? Cont
a sour taste is augmented ? a sour taste is lost
487,496 ??????? ??????? Quasi
feel pain ? reduce pain
529,173 ???????? ?????????? Cont
access occurs ? curb access
555,049 ?????? ??????? Cont
lose nuclear plants ? augment nuclear plants
608,895 ????????? ??????? Quasi
radioactivity is released ? radioactivity is reduced
638,092 ???????? ????????? Cont
Euro falls ? Euro gets strong
757,423 ??????? ????????? Quasi
have share (in market) ? share decreases
833,941 ?????????? ?????????? 6
generate active oxygen ? related to active oxygen
848,331 ??????? ????????? Cont
destroy cancer ? develop cancer
982,980 ????????? ??????????? Cont
virus becomes extinct ? virus is activated
Table 2: Examples of PROPcont?s outputs.
Summary PROPcont is a low cost but high perfor-
mance method, since it acquired one million con-
tradiction pairs with over 70% precision from only
the 46 initial seed templates. Besides, Excitation
contributes to contradiction ranking since PROPcont
outperformed PROPcont-NE by a 10% margin for the
top one million results. Thus we conclude that our
assumption on contradiction extraction is valid.
5.3 Causality Extraction
We show that our method for causality extraction
(PROPcaus) extracted 500,000 causality pairs with
about 70% precision, and that Excitation values con-
tribute to the ranking of causal pairs. PROPcaus took
as input all 10,825 templates classified by PROPtmp.
Baselines RANDcaus randomly extracts two
phrases that co-occur in a sentence with one of the
AND/THUS-type connectives, i.e., it uses not only
non-causal connectives but also causal ones like
thus. FREQ is the same as PROPcaus except that it
ranks its output by the phrase pair co-occurrence
frequency rather than Excitation values.
626
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  200000  400000  600000  800000  1e+06
Pre
cisio
n
Top-N
?Proposed?
?Freq?
?Random?
Figure 5: Precision of causality extraction.
Evaluation scheme We randomly sampled 100
pairs each from the top one million results of
PROPcaus and FREQ, and all RANDcaus?s output.
The annotators were shown the original sentences
from which the samples were extracted. Fleiss?
kappa was 0.68 (substantial agreement).
Results ?Proposed? in Figure 5 is the precision
curve for PROPcaus. From this curve the estimated
precision of PROPcaus is about 70% around the top
500,000. Note that PROPcaus outperformed FREQ
by a large margin, and extracted a large variety of
causal pairs since its 100 samples contained 91 dif-
ferent template pairs. Table 3 shows examples of
PROPcaus?s output along with English translations.
The labels ?4? and ?6? denote whether a pair is
causality or not. The 6 cases in Table 3 were
exceptions to our assumption described in Section
4.2; even if two Excitation templates co-occur in a
sentence with an AND/THUS-type connective, they
sometimes do not constitute causality. Actually, the
first 6 case consists of two phrases that co-occurred
in a sentence with a (non-causal) AND/THUS-type
connective but described two events that happen as
the effects of introducing the RAID storage system;
both are caused by the third event. In the second 6
case, the two phrases co-occurred in a sentence with
a (non-causal) AND/THUS-type connective but just
described two opposing events.
Summary PROPcaus performs well since it ex-
tracted 500,000 causality pairs with about 70%
precision. Moreover, Excitation values contribute
to causality ranking since PROPcaus outperformed
FREQ by a large margin. Then we conclude that our
assumption on causality extraction is confirmed.
Rank Causality Pairs Label
1,036 ?????????????????? 4
increase basal metabolism ? enhance fat-burning ability
2,128 ?????????????????? 4
increase desire to learn ? facilitate self-learning
6,471 ?????????????? 6
improve reliability ? increase capacity
29,638 ???????????????????????? 4
circulating thyroid hormone level increases ? improves metabolism
56,868 ??????????????? 4
exports increase ? GDP grows
267,364 ???????????????? 4
promote blood circulation ? improve metabolism
268,670 ???????????????? 4
BSE outbreak occurs ? import ban (on beef) is issued
290,846 ????????????????? 4
improve the view ? improve the efficiency of work
322,121 ??????????????????? 4
giant earthquake occurs ? meltdown is triggered
532,106 ??????????????? 4
good at thermal efficiency ? enhance heating efficiency
563,462 ???????????????? 4
promote inflation (in Japan) ? yen depreciation develops
591,175 ???????????????? 6
bring profit ? bring detriment
657,676 ?????????????? 4
physical strength declines ? immune system weakens
676,902 ?????????????????? 4
sharp fall in government bond futures occurs ? interest rates increase
914,101 ?????????????? 4
have a margin of error ? cause trouble
Table 3: Examples of PROPcaus?s outputs.
5.4 Causality Hypothesis Generation
Here we show that our causality hypothesis genera-
tion method in Section 4.3 (PROPhyp) extracted one
million hypotheses with about 57% precision.
This experiment took the top 100,000 results of
PROPcaus as input, generated hypotheses from them,
and randomly selected 100 samples from the top one
million hypotheses. We evaluated only PROPcaus,
since we could not think of any reasonable baseline
for this task. Randomly coupling two phrases might
be a baseline, but it would perform so poorly that it
could not be a reasonable baseline.
The annotators judged each sample in the same
way as Section 5.3, except that we presented them
with source causality pairs from which hypotheses
were generated, as well as the original sentences of
these source pairs. Fleiss? kappa was 0.51 (moderate
agreement).
As a result, PROPhyp generated one million hy-
potheses with 57% precision. It generated various
kinds of hypotheses, since these 100 samples con-
tained 99 different template pairs. Table 4 shows
some causal hypotheses generated by PROPhyp. The
source causal pair is shown in parentheses. The la-
627
bels ?4? and ?6? denote whether a pair is causality
or not. The first 6 case was due to an error made by
Rank Causality Hypotheses (and their Origin) Label
18,886 ?????????????????? 4
(???????????????) 4
alleviate stress ? remedy insomnia
(increase stress ? continue to have insomnia)
93,781 ???????????????? 4
(????????????) 4
halt deflation ? tax revenue increases
(deflation is promoted ? tax revenes declines)
121,163 ?????????????????? 4
(???????????????) 4
enjoyment increases ? stress decreases
(enjoyment decreases ? stress grows)
205,486 ?????????????? 4
(??????????????) 4
decrease in crime ? diminish anxiety
(increase in crime ? heighten anxiety)
253,531 ????????????????? 4
(????????????????????) 4
reduce chlorine ? bacteria grow
(generate chlorine ? bacteria extinct)
450,353 ???????????????? 4
(????????????) 4
expand demand ? decrease unemployment rate
(decrease demand ? increase unemployment rate)
464,546 ??????????????????? 6
(??????????????????) 6
(ability of) digestion deteriorates ? cholesterol increases
(aid digestion ? decrease cholesterol)
538,310 ??????????????? 4
(?????????????) 4
relieve fatigue ? improve immunity
(feel fatigued ? immunity is weakened)
789,481 ??????????????? 4
(????????????????) 4
conditions improve ? prevent troubles
(conditions become bad ? cause troubles)
837,850 ????????????????? 6
(????????????????) 4
control economic conditions ? accompany problems
(economic conditions improve ? problems are solved)
Table 4: Examples of causality hypotheses.
our causality extraction method PROPcaus; the case
was erroneous since its original causality was erro-
neous. The second 6 case was due to the fact that
one of the contradiction phrase pairs used to gener-
ate the hypothesis was in fact not contradictory (?
?????????? 6? ??????? ?con-
trol economic conditions 6? economic conditions im-
prove?).
From these results, we conclude that our assump-
tion on causality hypothesis generation is valid.
6 Related Work
While the semantic orientation involving good/bad
(or desirable/undesirable) has been extensively stud-
ied (Hatzivassiloglou and McKeown, 1997; Turney,
2002; Rao and Ravichandran, 2009; Velikovich et
al., 2010), we believe Excitation represents a gen-
uinely new semantic orientation.
Most previous methods of contradiction extrac-
tion require either thesauri like Roget?s or WordNet
(Harabagiu et al2006; Mohammad et al2008; de
Marneffe et al2008) or large training data for su-
pervision (Turney, 2008). In contrast, our method
requires only a few seed templates. Lin et al2003)
used a few ?incompatibility? patterns to acquire
antonyms, but they did not report their method?s per-
formance on the incompatibility identification task.
Many methods for extracting causality or script-
like knowledge between events exist (Girju, 2003;
Torisawa, 2005; Torisawa, 2006; Abe et al2008;
Chambers and Jurafsky, 2009; Do et al2011; Shi-
bata and Kurohashi, 2011), but none uses a notion
similar to Excitation. As we have shown, we expect
that Excitation will improve their performance.
Regarding the acquisition of semantic knowledge
that is not explicitly written in corpora, Tsuchida et
al. (2011) proposed a novel method to generate se-
mantic relation instances as hypotheses using auto-
matically discovered inference rules. We think that
automatically generating plausible semantic knowl-
edge that is not written (explicitly) in corpora as hy-
potheses and augmenting semantic knowledge base
is important for the discovery of so-called ?unknown
unknowns? (Torisawa et al2010), among others.
7 Conclusion
We proposed a new semantic orientation, Excitation,
and its acquisition method. Our experiments showed
that Excitation allows to acquire one million con-
tradiction pairs with over 70% precision, as well as
causality pairs and causality hypotheses of the same
volume with reasonable precision from the Web. We
plan to make all our acquired knowledge resources
available to the research community soon (Visit
http://www.alagin.jp/index-e.html).
We will investigate additional applications of Ex-
citation in future work. For instance, we expect that
Excitation and its related semantic knowledge ac-
quired in this study will improve the performance
of Why-QA system like the one proposed by Oh et
al. (2012).
628
References
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.
Two-phrased event relation acquisition: Coupling the
relation-oriented and argument-oriented approaches.
In Proceedings of the 22nd International Conference
on Computational Linguistics (COLING 2008), pages
1?8.
Susumu Akamine, Daisuke Kawahara, Yoshikiyo Kato,
Tetsuji Nakagawa, Yutaka I. Leon-Suematsu, Takuya
Kawada, Kentaro Inui, Sadao Kurohashi, and Yutaka
Kidawara. 2010. Organizing information on the web
to support user judgments on information credibil-
ity. In Proceedings of 2010 4th International Uni-
versal Communication Symposium Proceedings (IUCS
2010), pages 122?129.
Alina Andreevskaia and Sabine Bergler. 2006. Semantic
tag extraction from wordnet glosses. In Proceedings
of the 5th International Conference on Language Re-
sources and Evaluation (LREC 2006).
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proceedings of the 47th Annual Meeting
of the ACL and the 4th IJCNLP of the AFNLP (ACL-
IJCNLP 2009), pages 602?610.
Marie-Catherine de Marneffe, Anna Rafferty, and
Christopher D. Manning. 2008. Finding contradiction
in text. In Proceedings of the 48th Annual Meeting of
the Association of Computational Linguistics: Human
Language Technologies (ACL-08: HLT), pages 1039?
1047.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011.
Minimally supervised event causality identification.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2011), pages 294?303.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
Roxana Girju. 2003. Automatic detection of causal re-
lations for question answering. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2003), Workshop on Multi-
lingual Summarization and Question Answering - Ma-
chine Learning and Beyond, pages 76?83.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text pro-
cessing. In Proceedings of the 21st National Confer-
ence on Artificial Intelligence (AAAI-06), pages 755?
762.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35 Annual Meeting of
the Association for Computational Linguistics and the
8the Conference of the European Chapter of the Asso-
ciation of Computational Linguistics, pages 174?181.
Daisuke Kawahara and Sadao Kurohashi. 2006. A fully-
lexicalized probabilistic model for Japanese syntactic
and case structure analysis. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL (HLT-NAACL2006),
pages 176?183.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159?174.
Dekang Lin, Shaojun Zhao Lijuan Qin, and Ming Zhou.
2003. Identifying synonyms among distributionally
similar words. In Proceedings of the 18th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-03), pages 1492?1493.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics (COLING-ACL1998), pages 768?
774.
Saif Mohammad, Bonnie Dorr, and Greame Hirst. 2008.
Computing word-pair antonymy. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 982?991.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Takuya Kawada, Stijn De Saeger, Junichi Kazama, and
Yiou Wang. 2012. Why question answering using
sentiment analysis and word classes. In Proceedings
of EMNLP-CoNLL 2012: Conference on Empirical
Methods in Natural Language Processing and Natural
Language Learning.
Charles E. Osgood, George J. Suci, and Percy H. Tannen-
baum. 1957. The measurement of meaning. Univer-
sity of Illinois Press.
James Pustejovsky. 1995. The Generative Lexicon. MIT
Press.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of the 12th Conference of the European Chapter of the
ACL, pages 675?682.
Tomohide Shibata and Sadao Kurohashi. 2011. Acquir-
ing strongly-related events using predicate-argument
co-occurring statistics and case frames. In Proceed-
ings of the 5th International Joint Conference on Natu-
ral Language Processing (IJCNLP 2011), pages 1028?
1036.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientation of words using
629
spin model. In Proceedings of the 43rd Annual Meet-
ing of the ACL, pages 133?140.
Kentaro Torisawa, Stijn De Saeger, Jun?ichi Kazama,
Asuka Sumida, Daisuke Noguchi, Yasunari Kakizawa,
Masaki Murata, Kow Kuroda, and Ichiro Yamada.
2010. Organizing the web?s information explosion
to discover unknown unknowns. New Generation
Computing (Special Issue on Information Explosion),
28(3):217?236.
Kentaro Torisawa. 2005. Automatic acquisition of ex-
pressions representing preparation and utilization of
an object. In Proceedings of the Recent Advances
in Natural Language Processing (RANLP05), pages
556?560.
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using japanese coordi-
nated sentences and noun-verb co-occurrences. In
Proceedings of the Human Language Technology Con-
ference of the North American Chapter of the ACL
(HLT-NAACL2006), pages 57?64.
Masaaki Tsuchida, Kentaro Torisawa, Stijn De Saeger,
Jong-Hoon Oh, Jun?ichi Kazama, Chikara Hashimoto,
and Hayato Ohwada. 2011. Toward finding semantic
relations not written in a single sentence: An inference
method using auto-discovered rules. In Proceedings
of the 5th International Joint Conference on Natural
Language Processing (IJCNLP 2011), pages 902?910.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2002), pages 417?424.
Peter Turney. 2008. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics (COLING 2008), pages 905?912.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and RyanMcDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the ACL, pages 777?785.
Ellen M. Voorhees. 2008. Contradictions and justifica-
tions: Extensions to the textual entailment task. In
Proceedings of the 48th Annual Meeting of the Associ-
ation of Computational Linguistics: Human Language
Technologies (ACL-08: HLT), pages 63?71.
630
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 693?703,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Two-stage Method for Large-scale Acquisition of
Contradiction Pattern Pairs using Entailment
Julien Kloetzer? Stijn De Saeger? Kentaro Torisawa? Chikara Hashimoto?
Jong-Hoon Oh? Motoki Sano? Kiyonori Ohtake??
Information Analysis Laboratory,
National Institute of Information and Communications Technology (NICT), Kyoto, Japan
{?julien, ? stijn, ? torisawa, ? ch, ?rovellia, ?msano, ??kiyonori.ohtake}@nict.go.jp
Abstract
In this paper we propose a two-stage method
to acquire contradiction relations between
typed lexico-syntactic patterns such as Xdrug
prevents Ydisease and Ydisease caused by
Xdrug . In the first stage, we train an SVM
classifier to detect contradiction pattern pairs
in a large web archive by exploiting the exci-
tation polarity (Hashimoto et al, 2012) of the
patterns. In the second stage, we enlarge the
first stage classifier?s training data with new
contradiction pairs obtained by combining the
output of the first stage?s classifier and that of
an entailment classifier. We acquired this way
750,000 typed Japanese contradiction pattern
pairs with an estimated precision of 80%. We
plan to release this resource to the NLP com-
munity.
1 Introduction
The ability to detect contradictory information in
text has many practical applications. Among those,
Murakami et al (2009) pointed out that a contra-
diction recognition system can detect conflicts and
anomalies in large bodies of texts and flag them to
help users identify unreliable information. For ex-
ample, many Japanese web pages claim that agari-
cus prevents cancer, where agaricus is a species of
mushroom found in a variety of commercial prod-
ucts. Although this has been accepted by many
Japanese people, by Googling keywords ?agaricus?,
?promotes? and ?cancer?, we can find pages claim-
ing that ?agaricus promotes cancer?, some of which
point to a study authorized by the Japanese Min-
istry of Health, Labour and Welfare1 reporting that
1 http://www.mhlw.go.jp/topics/bukyoku/iyaku/syoku-
anzen/qa/060213-1.html
a commercial product containing agaricus promoted
cancer. Obviously, the existence of these pages casts
serious doubt on the ability of agaricus to prevent
cancer and encourages readers to dig more about this
subject.
The above example suggests that recognizing
contradictory information can guide users to a true
fact. Likewise, we believe that contradiction recog-
nition is also useful when dealing with non-factual
information that occupy most of our daily lives. For
instance, there is a big controversy recently whether
Japan should join an economic partnership agree-
ment called the Trans Pacific Partnership (TPP), and
quite serious but contradictory claims are plentiful in
the mass media and on the web, e.g., TPP will wipe
out Japan?s agricultural businesses and TPP will
strengthen Japan?s agricultural businesses. Neither
of these are facts; they are predictions that can only
be realized or disputed after the underlying decision-
making is done: joining or refusing the TPP.
Furthermore, after reading documents including
contradictory predictions, one should notice that
each of them is supported by a convincing the-
ory that has no obvious defect, e.g., ?Exports of
Japan?s agricultural products will increase thanks to
the TPP? or ?A large amount of low-price agricul-
tural products will be imported to Japan due to the
TPP?. Even if one of these predictions may just hap-
pen to be true because of unexpected reasons such as
minor fluctuations in the Japanese yen, we must sur-
vey such theories that support contradictory predic-
tions, conduct balanced decision-making, and pre-
pare counter measures for the expected problems af-
ter examining multiple viewpoints. Contradiction
recognition should be useful to select documents to
be surveyed.
693
Figure 1: Method workflow
We have developed a method for recog-
nizing pairs of contradictory binary patterns
such as ??X promotes Y?, ?X prevents Y?? and
??X will wipe out Y?, ?X will strengthen Y??. To
solve the problem described above, we can easily
develop a system that can find contradictory text
fragments from the web like ?agaricus promotes
cancer? and ?agaricus prevents cancer? from the
discovered contradictory pattern pairs.
Our method is a two-stage procedure with three
supervised classifiers (Fig. 1). In the first stage,
we build a classifier BASE to recognize contradic-
tions between binary patterns, and a classifier ENT
to recognize entailment. In the second stage, we
combine the contradiction pairs recognized by BASE
and the entailment pairs recognized by ENT to ex-
pand BASE?s training data and train a new contra-
diction classifier, EXP. This expansion using en-
tailment is one key idea of this work: we acquired
750,000 contradiction pairs with 80% precision us-
ing the expanded training data, more than doubling
the 285,000 pairs acquired at the same precision
level without expansion. We also demonstrate that
this result is not trivial by showing that our method
outperforms an alternative one based on Integer Lin-
ear Programming inspired by the successful entail-
ment recognition method of Berant et al (2011).
As another technical contribution of this work, we
exploit the recently proposed semantic polarity of
excitation (Hashimoto et al, 2012) to recognize con-
tradictions between binary patterns. Hashimoto et
al. (2012) previously showed that excitation polari-
ties are useful to recognize contradictions between
phrases that consist of a noun and a predicate, such
as ?promote cancer? and ?prevent cancer?. While
it is trivial to extend this framework to contradic-
tions between unary patterns such as ?promote X?
and ?prevent X? by replacing the common nouns
in each pair with a variable, the information rep-
resented in unary patterns is often vague, and it is
unlikely that a contradiction between unary patterns
directly leads to the discovery of unreliable infor-
mation to be flagged or to a meaningful survey of
complex problems. As exemplified by the agaricus
and TPP examples, contradictions between binary
patterns that include two variables such as ?X pro-
motes Y? or ?X will wipe out Y? are more useful
than those between unary patterns. We also show
that it is not trivial to recognize contradictions be-
tween binary patterns using contradictions between
unary patterns.
Most works dealing with contradiction recogni-
tion up till now (Harabagiu et al, 2006; Bobrow
et al, 2007; Kawahara et al, 2008; Kawahara et
al., 2010; Ohki et al, 2011) focus on recognizing
contradictions between full sentences or documents,
not text fragments that match our relatively short
patterns (survey in Section 5). We expect that the
contradictory pattern pairs we acquired can be used
as building blocks in such full-fledged contradiction
recognition for full sentences or documents, simi-
larly to antonym pairs in Harabagiu et al (2006).
Also, we should emphasize that our method
focuses on the most challenging part of contra-
diction recognition according to the classification
of De Marneffe et al (2008). Since we discard
patterns with negations, an evident source of contra-
dictions like ??X causes Y?, ?X does not cause Y??,
most of our output are non-trivial contradic-
tions related to high-level semantic phenomena,
e.g., contradiction pairs related to antonyms
like ??X? Y?????, ?X? Y??????
(??X increases Y?, ?X decreases Y??), lexical contra-
dictions like ??X? Y????, ?Y? X?????
(??X wins against Y?, ?Y wins against X??), or
contradictions due to common-sense knowledge
like ??X? Y???????, ?X? Y??????
(??X reassures Y?, ?X betrays Y??). We believe
acquiring such contradictions in a large scale is a
valuable contribution.
The following is the outline of this paper. Sec-
tion 2 details our target and our proposed method.
Evaluation results are discussed in Section 3. Sec-
694
Figure 2: Detailed data flow
tion 4 details our features set, and Section 5 related
work. Section 6 provides a conclusion.
2 Proposed method
As showed in Figure 1, our method consists of
three supervised classifiers. Classifiers BASE and
EXP recognize contradiction relations between bi-
nary patterns, and ENT recognizes entailment rela-
tions between binary patterns. The contradiction
pairs recognized by BASE and the entailment pairs
recognized by ENT are combined to generate new
contradiction pairs, part of which are then added to
BASE training data to train the EXP classifier. Our
final output is the set of all binary pattern pairs re-
garded as contradictions by EXP. Since the depen-
dencies between these three classifiers, their distinct
sets of training data, and the two data sets to be clas-
sified (we describe those in the two sections below)
is a bit complex, we show a complete description of
the whole process in Figure 2.
The key idea is in the scheme that expands the
training data. Logically speaking, patterns p and r
are contradictory if there exists a pattern q such that
p entails q and q contradicts r. For example, since
?X causes Y? entails ?X promotes Y? and ?X pro-
motes Y? contradicts ?X prevents Y?, then ?X causes
Y? contradicts ?X prevents Y?. Hence, by combin-
ing entailment and contradiction pairs, we can ob-
tain more contradiction pairs.
Following this property of contradiction relations,
we collect a set of pattern pairs {?p, r?} for which
there exists a pattern q such that ENT recognizes that
p entails q and BASE recognizes that q contradicts r.
Then we rank these pairs based on a novel scoring
function called Contradiction Derivation Precision
(CDP) and expand BASE training data by adding to
it the top-ranked pairs according to CDP in order to
train EXP. This ranking scheme selects highly accu-
rate contradiction pairs and prevents errors caused
by BASE and ENT from being propagated to EXP.
In the following, after defining the patterns for
which we acquire contradiction relations, we de-
scribe BASE, EXP, ENT, and our expansion scheme.
2.1 Patterns
In this work, a binary pattern is a word sequence
on the path of dependency relations connecting two
nouns in a syntactic dependency tree, like ?X causes
Y?, and we say a noun pair co-occurs with a pattern
if the two nouns are connected by this pattern in the
dependency tree of a sentence in the corpus.
We focus on typed binary patterns, which place
semantic class restrictions on the noun pairs they
co-occur with, e.g., ?Yorganization is in Xlocation?.
Subscripts organization and location indicate the se-
mantic classes of the X and Y slots. Since typed
patterns can distinguish between multiple senses
of ambiguous patterns, they greatly reduce errors
due to pattern ambiguity (De Saeger et al, 2009;
Schoenmackers et al, 2010; Berant et al, 2011).
We automatically induced semantic classes from our
corpus using the EM-based noun clustering algo-
695
rithm presented in Kazama and Torisawa (2008),
and clustered one million nouns into 500 rela-
tively clean semantic classes, including for example
classes of diseases and of chemical substances.
The binary patterns and their co-occurring noun
pairs were extracted from our corpus of 600 mil-
lion Japanese web pages dependency parsed with
KNP (Kurohashi and Nagao, 1994). We restricted
our patterns to the most frequent 3.9 million pat-
terns of the form ?X-[case particle] Y-[case parti-
cle] predicate? such as ?X-ga Y-ni aru? (?X is in Y?)
which do not contain any negation, number, symbol
or punctuation character. Based on our observation
that patterns in meaningful contradiction and entail-
ment pairs tend to share many co-occurring noun
pairs, we used as input to our classifiers the set Pall
of 792 million pattern pairs for which both patterns
share three co-occurring noun pairs.
2.2 BASE: First stage Classifier for
Contradiction
Below, we detail BASE: its training data and input
data to be classified, and some experimental results.
Our first stage classifier for contradictions, BASE,
is an SVM that uses commonsensical surface and
lexical resources based features, such as n-grams ex-
tracted from patterns, which will be detailed in Sec-
tion 4. An important point to be stressed here is
that we restricted the pattern pairs to be classified
by BASE by exploiting their excitation polarity, a
semantic orientation proposed by Hashimoto et al
(2012). Excitation characterizes unary patterns as
excitatory, inhibitory, or neutral. Excitatory unary
patterns, such as ?cause X? or ?increase X?, entail
that the function, effect, purpose, or role of their ar-
gument?s referent is activated or enhanced, and in-
hibitory unary patterns, such as ?prevent X? or ?X
disappears?, entail that the function, effect, purpose,
or role of their argument?s referent is deactivated or
suppressed. Neutral unary patterns like ?close to X?
are neither excitatory nor inhibitory.
We exploited excitation to restrict the input of
BASE. Based on the result of Hashimoto et al
(2012) showing that two unary patterns with op-
posite polarity have a higher chance to be a con-
tradiction, we extracted from set Pall the set Popp
of binary pattern pairs that contain unary patterns
with opposite excitation polarities as sub-patterns.
??Y cause X?, ?Y prevent X?? is an example of such
a pair since the unary sub-patterns ?cause X? and
?prevent X? are respectively excitatory and in-
hibitory. We used here 6,470 excitation unary pat-
terns hand-labeled as either excitatory (4,882 pat-
terns) or inhibitory (1,588 patterns). Set Popp con-
tains 8 million pattern pairs with roughly 38% true
contradiction pairs, and is the input to BASE. We
will show in experiments at the end of this section
that this restriction is necessary to obtain good per-
formance for BASE. We also tried to add the excita-
tion polarities in BASE?s feature set and classify Pall,
but the performance was worse.
Training Data Another key feature of BASE is
that it is distantly supervised. We did not use
training samples that are directly manually anno-
tated. Instead we automatically generated training
data from a smaller set of (non-)contradiction unary
pattern pairs. We first prepared a set of roughly
800 unary pattern pairs hand-labeled by three human
annotators as contradictions (238 pairs) and non-
contradictions (558 pairs) using majority vote. The
inter-annotator agreement was 0.78 (Fleiss?kappa).
Inspired by Hashimoto et al (2012), we selected
these unary pattern pairs among pairs with high dis-
tributional similarity, with and without restricting
them to having opposite excitation polarity, such as
to get a fair distribution of contradictions and non-
contradictions.
We then extracted from set Pall all 256,000 pat-
tern pairs containing a contradictory unary pattern
pair, and all 5.2 million pattern pairs containing a
non-contradictory unary pattern pair, which we re-
spectively used as positive and negative training data
(estimated 79% and 73% accuracy from 200 hand-
labeled samples). Table 1 shows some examples.
The optimal composition of training data for
BASE was determined according to preliminary ex-
periments using our development set (1,000 manu-
ally labelled samples. See Section 3.1). We trained
20 different classifiers using from 6,250 to 50,000
positive samples (4 sets) and from 12,500 to 200,000
negative samples (5 sets), doubling the amounts in
each step, for a total of 20 configurations. We could
not try a larger training data due to long training time
but we do not expect it to be a problem because the
worst performance was observed with large train-
696
Table 1: Examples of training samples for BASE obtained from unary pattern pairs
Binary pattern pair (the unary pattern pair that extracted it is underlined) Unary pattern pair label
Y ? X ??? (X is bad in Y too) - Y ?? X ??? (X is good even in Y) contradiction
Y ? X ???? (Y too heads toward X) - Y ? X ??? (Y too comes out of X) contradiction
X ?Y ? ??? (add Y to X) - X ?Y ? ??? (insert X into Y) non-contradiction
Y ? X ??? (Y too comes to X) - Y ?? X ??? (go to X with Y) non-contradiction
Figure 3: Effect of the restriction using excitation
ing data (25,000 positives and 200,000 negatives;
the difference from the optimal setting was 2.3% in
average precision). The optimal training data set,
Trainbase, consists of 12,500 positives and 100,000
negatives samples as described above and is the one
we use in our experiments below and in Section 3.
Since BASE input for classification data is Popp
we also tried sampling Trainbase from Popp. We
obtained 56.27% average precision for our classi-
fier BASE, and 52.99% when restricting the source
of training data to pairs in Popp. We believe that the
difference lies in the size of the sets from which we
sampled our training data: while there are 5.46 mil-
lion binary pattern pairs in Pall with a hand-labeled
unary pattern pair in Pall, there are only 237,000
pairs in Popp. We believe this much smaller sam-
ple source lead to a lower performance because it
included much less variations of the patterns.
To train BASE and other classifiers mentioned in
this paper, we used the SVM tool TinySVM2 with
a polynomial kernel of degree 2, the setting which
showed the best performance during our preliminary
experiments.
Effect of Excitation Polarities We also empiri-
cally examined the effect of the restriction on the
patterns using excitation polarities. We used our test
set (2,000 manually annotated samples described in
2 http://chasen.org/?taku/software/TinySVM/
Section 3.1) and 250 manually annotated samples
(majority vote from 3 annotators) from top ranked
pairs of Pall to draw precision curves for BASE over
the top 2 million binary pairs from both Popp and
Pall. In each case we assumed that pairs were dis-
tributed uniformly (i.e., with a constant interval) in
the ranked list of pairs of Popp and Pall, and com-
puted precision accordingly. Since the pairs sets
are reasonably large and were sampled randomly we
thought this was a reasonable hypothesis. The pre-
cision over Popp is higher than that over Pall with
a large margin, suggesting that the restriction using
excitation polarities is beneficial.
2.3 ENT: First stage Classifier for Entailment
ENT is an SVM classifier for entailment trained us-
ing 27,500 hand-annotated binary pattern pairs (set
Trainent, 45% of positive entailment pairs) created
for some previous work (Kloetzer et al, 2013). It es-
sentially uses the same feature set as that for BASE
with the addition of several distributional similar-
ity measures (see Section 4 below for more details).
This classifier is given all pairs of Pall as input and
scores each of them. For this study, we considered
the 44.5 million pattern pairs with a positive SVM
score as entailment pairs. Manual annotation of 200
random samples revealed that the precision of these
pairs was 63% and that the top 7.1 million pairs had
80% precision (result interpolated from the top 16%
of the annotated samples).
2.4 Second stage: Training Data Expansion
and Classifier EXP
Below, we show how we combine BASE?s top output
(hereafter C) and ENT?s top output (hereafter E) in
the second stage of our method to expand Trainbase
and train a new classifier, EXP.
The training data expansion process is based on
the following logical constraint: if a pattern p entails
a pattern q and pattern q contradicts a third pattern r,
then p must contradict r. For example, because ?X
697
Table 2: Examples of triplets ?p, q,r? where p entails q, q contradicts r, and hence p contradicts r
Pattern p Pattern q Pattern r X/Y examples SV M Score(p, r) CDP (p, r)
Y ?? X ???? Y ?? X ????? Y ? X ???? ??/? 0.3 0.98X disappears from Y X vanishes from Y Y is full of X anger/eye
Y ? X ????? Y ? X ???? Y ?? X ???? ??/?? -0.3 0.61stop X in Y finish X in Y start X in Y April/activity
X ? Y ??? X ? Y ??? X ? Y ??? ???/?? 0.07 0.45X shows Y X have Y X loses Y team/confidence
Algorithm 1 Training data expansion: C is the top 5%
output of BASE, E is the top output of ENT (score > 0)
1: procedure EXPAND(C, E)
2: Compute the set of expanded pairs C? = {?p, r? |
?q : ?p, q?? E,?q, r?? C}.
3: Rank the pairs in C? using CDP.
4: Add the N top-ranked pairs in C? \ C as new positive
samples to Trainbase.
5: Remove incoherent negative training samples using
negative cleaning.
6: end procedure
causes Y? (pattern p) entails ?X promotes Y? (pattern
q) and the latter contradicts ?X prevents Y? (pattern
r), we conclude that ?X causes Y? (p) contradicts
?X prevents Y? (r). We call the former contradic-
tion ?q, r? a source contradiction pair, and the later
pair ?p, r? an expanded contradiction pair. Based on
this idea, we combine C and E to aggressively ex-
pand Trainbase. This process is described in Al-
gorithm 1, and Table 2 shows examples of triples
?p, q,r? obtained in our experiments.
Expanding pairs fromC andE compounds the er-
rors made by BASE and ENT, hence it is crucial to
select a highly precise subset of the expanded pairs.
Taking the top pairs according to their SVM score
would achieve this, but since BASE already handles
correctly such pairs, they should not help much as
new training data. We therefore propose a new scor-
ing function for selecting highly precise expanded
pairs: Contradiction Derivation Precision (CDP ).
CDP was designed according to the following
assumption: a source contradiction pair that derives
correct expanded pairs with a high precision should
be reliable. Probably, all the expanded pairs derived
from such a reliable source pair will be correct and
should be included in the new training data .
In our formulation of CDP , correctness of an ex-
panded pair is judged according to the pair?s SVM
score using BASE. In other words, we regard an
expanded pair that has an SVM score above some
threshold ? as a true contradiction. A source contra-
diction pair that derives true contradiction pairs with
a high precision is regarded as a reliable source con-
tradiction pair. CDP , which is defined over a ex-
panded pairs, is the maximum precision among that
of the source contradiction pairs that derive a given
expanded pair.
We first define CDPsub(q, r) over a source con-
tradiction pair ?q, r? as the ratio of expanded pairs
obtained from ?q, r? whose SVM score is above
threshold ?. This ratio corresponds to the precision
of the expanded pairs derived from the source con-
tradiction pair ?q, r?.
CDPsub(q, r) = |{?p, r? ? Ex(q, r) | Sc(p, r) > ?}|
|Ex(q, r)|
HereEx(q, r) is the set of expanded pairs derived
from a source pair ?q, r?, and Sc is the SVM score
given by BASE. In our experiments, we set ? = 0.46
such that pattern pairs for which BASE gives a score
over ? corresponds to the top 5% of BASE?s output.
CDP (p, r) over an expanded pair is defined as fol-
lows, where Source(p, r) is the set of source con-
tradiction pairs that were derived into the expanded
pair ?p, r?.
CDP (p, r) = max?q,r??Source(p,r)CDPsub(q, r)
We then expand the top 5% contradictions of
BASE?s output (set C) and pattern pairs scored pos-
itively by ENT (set E), rank all expanded pairs not
already in C according to CDP, and add the top N
pairs with the highest CDP values as positives to
Trainbase to train EXP. The value of N shall be
determined empirically in later experiments using
a development set. Note that, since CDP (p, r) is
independent of ?p, r??s SVM score, even pairs that
were assigned a negative score by BASE can become
highly ranked by CDP (second triplet in Table 2)
698
and be added to train EXP, hence we expect EXP to
learn something new from these pairs.
Finally, after the addition of expanded pairs, we
remove incoherent training samples. We propose to
remove from the negative training samples of EXP
any pattern pair that may conflict with the newly
added positives; we call this step negative cleaning.
Intuitively, since the content word pairs in a pattern
pair should present some of the strongest evidence
for determining the patterns (non-)contradiction sta-
tus, we remove any negative sample that shares a
content word pair with one of the added expanded
pairs. The final training data for EXP, set Trainexp,
consists of the following: (1) positive samples from
Trainbase, (2) (positive) expanded pairs, and (3)
negative training samples from Trainbase, cleaned
using negative cleaning. We confirmed in our exper-
iments that negative cleaning was necessary to train
a strong EXP classifier (details omitted for reason of
space).
After training EXP with Trainexp, we classify
Popp with EXP to produce the final output of the
whole method. Note that while this expansion pro-
cess can be re-iterated with EXP?s output, our exper-
iments failed to show any improvement with subse-
quent iterations.
3 Evaluation
This section presents our experimental results. We
describe first how we constructed test and develop-
ment data, and then report comparison results be-
tween our method and others including BASE and an
Integer Linear Programming-based (ILP) method.
3.1 Development and Test Data
We asked three human annotators to label 3,000 bi-
nary pattern pairs randomly sampled from Popp as
contradiction or non-contradiction to be used as de-
velopment (1,000 pairs) and test (2,000 pairs) sets.
We considered a pattern pair as a true contradic-
tion relation if at least two out of the three annota-
tors marked it as positive. The inter-rater agreement
score (Fleiss Kappa) was 0.523, indicating moderate
agreement (Landis and Koch, 1977). As a definition
of contradiction, we used the notion of incompati-
bility (i.e., two statements are extremely unlikely to
be simultaneously true) proposed by De Marneffe et
Figure 4: Precision of all the compared methods
al. (2008). We then say binary patterns such as ?X
causes Y? and ?X prevents Y? are contradictory if
the above definition holds for any noun pair that can
instantiate the patterns? variables in the provided se-
mantic class pair.
Because our semantic classes are obtained by au-
tomatic clustering and have no meaningful labels,
we followed Szpektor et al (2007) and provided the
annotators with three random noun pairs that co-
occur with the patterns as a proxy for the class pair.
The annotators marked a given pattern pair as posi-
tive if the contradiction relation between the patterns
held for all three noun pairs presented.
3.2 Experimental Results
Here we show how our proposed method outper-
forms baseline methods. We compare the following
four methods:
? PROPOSED: our proposed method. N , the
number of newly added positive training sam-
ples during the training data expansion pro-
cess, was set to 6,000 according to preliminary
experiments using the development set. We
tried 50 different values of N from 1,000 up to
50,000, adding 1,000 each time, and chose the
N value giving the highest average precision
against our development set (1,000 samples).
? BASE: our first stage classifier.
? PROP-SCORE: same as PROPOSED except for
the use of BASE?s SVM score instead of CDP .
N was set to 30,000 in the same way we set N
for PROPOSED.
? HAS: an adaptation of the contradiction ex-
traction method presented in Hashimoto et al
699
(2012). For a binary pattern pair we first
extracted its unary pattern pair with opposite
polarity (or one at random in case there are
two) and scored it based on our implementa-
tion of Hashimoto et al (2012); the score is
based on the distributional similarity between
unary patterns and an excitation score obtained
using a minimally supervised method based on
the spin model. We then scored the binary pat-
tern pair by the score of this unary pattern pair.
We ranked the pattern pairs of our test set (2,000
random pairs from set Popp) based on the score pro-
duced by each method. For each tested method we
assumed that pairs in the test set were distributed
uniformly like explained in Section 2.2. The pre-
cision curves we obtained are shown in Figure 4.
PROPOSED clearly outperformed BASE and ac-
quired around 750,000 contradiction pattern pairs
with an estimated precision of 80%, out of which
some examples are shown in Table 3. These pairs
cover 26,941 content word pairs and reduce to
272,164 untyped pairs, showing that PROPOSED
does not just acquire a handful of contradictions in
many different class pairs. Also, when matching
these pairs against an antonyms database (extracted
from the dictionary of the morphical analyzer JU-
MAN) we found that only 100,886 of these pattern
pairs contain an antonym pair, which means that
most of the extracted pairs? contradictions are due
to more complex phenomena than simple antonymy.
With the same precision, BASE and PROP-SCORE
acquired only 285,000 pairs (covering 11,794 con-
tent word pairs) and 636,000 pairs respectively. This
implies that our two-stage method can more than
double the number of highly precise contradiction
pairs we acquire as well as increasing their vari-
ety, and that ranking expanded pairs using our scor-
ing function CDP is better than with SVM score,
though even PROP-SCORE performs better than
BASE in our setting. Finally, the poor performance
of HAS suggests that extending the Hashimoto et
al.?s framework to recognition of binary patterns is
not a trivial task.
As to why adding only 6,000 top pairs ranked
by CDP performs better than adding 30,000 pairs
ranked by SVM score, the pattern pairs added in
PROP-SCORE had high SVM scores given by BASE
and as such are already handled nicely by BASE.
Table 3: Examples of pairs acquired by PROPOSED: con-
tradiction (label +) and non-contradiction (label -)
Lab. Pattern pairs (with rank) X/Y example
Y ? X ???? - Y ?? X ????? ??/??
+ X finished Y - X started from Y sale/yesterday
Rank 228,039
X ? Y ??? - Y ? X ??? ??/????
+ X wins against Y - Y wins against X Japan/Vietnam
Rank: 258,068
X ? Y ??? - X ?? Y ??? ?/??
- X lose Y - Have Y in X people/interest
Rank 474,143
Y ? X ???? - Y ?? X ??? ??/??
+ Lose X in Y - Have X in Y too confidence/
Rank 522,534 oneself
Y ? X ????? - X ? Y ???? 9 ?/??
- Y falls down to X - raise Y to X 9th/ranking
Rank 538,901
X ? Y ????? - X ?? Y ??? ?/????
+ Y exists in X - Keep Y out X inside/virus
Rank 620,430
X ?? Y ??? - X ? Y ???? ?/?
- Remove Y off X - X answer with Y I (or me)/eyes
Rank 652,530
Y ? X ?????? - X ? Y ??? ?/??
+ Kick out Y from X - Y remains in X body/fatigue
Rank 697,177
Y ? X ?????? - Y ? X ????? ?/??
+ X reassures Y - X betrays Y I/her
Rank: 749,916
Hence, we think the effect of adding a new sam-
ple from PROP-SCORE is smaller than that in PRO-
POSED, because in PROPOSED we add to the train-
ing data pattern pairs with both high and low (possi-
bly negative) SVM scores.
Finally, while the quality of the entailment pairs
plays a very important role in the assumption that
was the base of CDP , these results show that even
a simple rule such as ?Use entailment pairs with
SVM score over 0 to expand contradictions before
ranking them with CDP ? is sufficient to make the
method work. Though it may be possible to design
a more complex CDP formula which takes entail-
ment score into account, we did not explore this di-
rection in this work.
Comparison with an ILP-based method Finally,
we would like to compare our method with an ILP-
based method. The interaction between contradic-
tion and entailment that forms the basis for our ex-
pansion method has a natural interpretation as an op-
timization problem. We thus compared our method
to the following ILP formulation of this interaction
inspired by Berant et al (2011), using our test set:
700
Figure 5: Comparison between PROPOSED, BASE and
BASE+ILP on a restricted test set (1,306 samples)
(1) G = argmax
?
p6=q
(e(p, q)??)?Epq +(c(p, q)??)?Cpq
(2) s.t. ?p,q,r Epq + Cqr ? Cpr ? 1
(3) ?p,q Epq + Cpq ? 1
(4) ?p,q Epq ? {0, 1} (5) ?p,q Cpq ? {0, 1}
The objective in Equation (1) is a sum over the
weights of every pair of patterns ?p, q?, where Epq
indicates whether a pair ?p, q? is an entailment pair
(Equation (4)), andCpq indicates whether it is a con-
tradiction pair (Equation (5)). e(p, q) and c(p, q) are
the score given respectively by ENT and BASE, and
? is a prior defining the weight of a pair as neither
entailment nor contradiction that shall be set before
any experimentation. Equation (2) states the tran-
sitivity relation which is the basis of our expansion
method. Finally, Equation (3) states that a given pat-
tern pair cannot be a contradiction pair and an entail-
ment pair at the same time. Since our patterns are
class-dependent, we solved separate ILP instances
for each semantic class pair.
We drew a precision curve for each of BASE,
PROPOSED and BASE+ILP. To draw the curve for
BASE+ILP, we incrementally raised the sample?s
non-contradiction non-entailment prior ? (more de-
tails in Berant et al (2011)). Because of the com-
putational difficulty of ILP (NP-complete) and the
size of our data, the computation for the ILP-based
method ran out of memory on a 72GB machine for
116 class pairs out of the 1,031 that our test set cov-
ers. For this reason, we only used the 1,306 samples
of the test set covered by the remaining 915 class
pairs. We also measured the performance of BASE
and PROPOSED on the same restricted test set.
Figure 5 shows that under these conditions the
ILP-based method performance resembles BASE
and is worse than PROPOSED on all data points.
PROPOSED performs slightly worse in this setting
compared to when classifying the whole of Popp,
but this only means that its performance is good for
the 116 class pairs we ignored in this experiment.
While this comparison is only made in a restricted
setting, our expansion method still outperforms ILP
and is clearly more scalable. The ILP results could
be improved by adding more constraints (contradic-
tion is symmetric, entailment is transitive), but this
would also make the problem even more intractable
in terms of computational costs.
4 Features
In this section we present the features used in our
classifiers, which are mainly categorized into three:
surface features (i.e., those reflecting the patterns?
content itself), features based on external lexical re-
sources, and distributional similarity based features;
all features are listed in Table 4. ENT uses all the
features while BASE and EXP use all except for the
distributional similarity based ones. The optimality
of the feature sets was confirmed through ablation
tests using the development set (results omitted for
the sake of space).
Since patterns with a contradiction or entailment
relation are often superficially similar, for instance,
in case structure or inflection, we use a number of
surface features based on string similarity measures,
extending the feature sets used by Malakasiotis and
Androutsopoulos (2007) for entailment recognition.
They include bag-of-words features such as n-grams
and similarity scores concerning the bag-of-words
such as their Euclidian distance.
To complement the surface features with knowl-
edge about the content words, we used lexi-
cal databases including such as antonymy, syn-
onymy, entailment, or allography. The presence
of such word pairs is usually a good indicator of
(non-)contradiction or (non-)entailment at the pat-
tern level. More specifically, for any word pair
?wp,wq? taken from a pattern pair ?p, q? we mark
the presence of ?wp,wq? in each of the lexical re-
sources as a binary feature. We used the Japanese
lexical resources distributed by the ALAGIN Fo-
rum3: the verb entailment database (117,000 verb
3 http://www.alagin.jp/
701
Table 4: Features summary, computed over a pair of patterns ?p, q?
su
rfa
ce Similarity measures: common elements ratios, Dice coefficient, Jaccard and discounted Jaccard scores, Cosine, Euclidian, Manhattan, Levenshteinand Jaro distances; computed over: the patterns? 1-, 2- and 3-grams sets of: characters, morphemes, their stems & POS; content words and stems
binary feature for each of the patterns? subtrees, 1- and 2-grams ; patterns? lengths and length ratios
le
x.
r. entries in databases of verb entailments and non-entailments, synonyms, antonyms, allographs ; checked over: pairs of content words,
pairs of content word stems, same for the reverse pattern pair ?q, p?
di
s.s
. Distributional similarity measures: Common elements ratios, Jaccard and discounted Jaccard scores, sets and sets intersection cardinality,
DIRT (Lin and Pantel, 2001), Weeds (Weeds and Weir, 2003) and Hashimoto (Hashimoto et al, 2009) scores; computed over: patterns?
co-occurring noun pairs, POS tags of those, nouns co-occurring in each variable slot, nouns co-occurring with each unary sub-patterns
ot
he
r binary feature for each semantic class pair and individual semantic classes
patterns frequency rank in the given semantic class pair
pairs; Alagin ID A-2), the databases of synonyms,
antonyms and meronyms (respectively 111,000,
5000 and 2500 pairs; Alagin ID A-9), and the al-
lographic word database (2.7 million pairs; Alagin
ID A-7). We also used the information concerning
allographic words in the dictionary of the morpho-
logical analyzer JUMAN4.
Distributional similarity values between patterns
are based on the idea that patterns that appear in
similar contexts tend to have similar meanings and
as such are useful to recognize entailment (Lin and
Pantel, 2001). We computed as features several dis-
tributional similarity measures on the sets of each
pattern?s co-occurring noun pairs and their POS
tags, of nouns co-occurring in each variable slot, and
with each of the pattern?s unary sub-patterns.
We also added a few more uncategorizable fea-
tures. See Table 4 for more details.
5 Related Work
A number of previous work dealt with the recogni-
tion of contradictions between sentences. Harabagiu
et al (2006) proposed a contradiction detection
method that focuses on negation, antonymy and
some discourse information. Kawahara et al (2010)
also used negations and antonyms to extract con-
trastive/contradictory statements from the web to
present users with a bird ?s-eye view of statements
about a given topic. Bobrow et al (2007) showed
a method using logical forms with relatively precise
results. Ohki et al (2011) proposed a method to rec-
ognize confinment, a novel semantic relation related
to both entailment and contradiction. While we do
not deal ourselves directly with sentences, we expect
that the binary pattern pairs we acquire can play a
role similar to that of basic linguistic resources such
4 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
as antonyms and negations in these works. Closer
to our work, Ritter et al (2008) presented a method
for detecting contradictions between functional re-
lations like ?X was born in Y?, but these constitute
only a part of the semantic relations expressed by the
binary patterns we deal with in this paper.
Other works analyzed contradictions from lin-
guistic/semantic viewpoints. Voorhees (2008) ana-
lyzed the contradiction recognition-task of the RTE3
contest. Magnini and Cabrio (2010) examined rela-
tions between contradictions and textual entailment
samples. De Marneffe et al (2008) presented a
typology of contradictions, and showed that con-
tradictions can arise from a multitude of phenom-
ena. They showed contradictions based on lexical or
world knowledge are challenging and require a high-
level understanding of language and/or the world.
As stated in the introduction, these are the types of
contradictions our method focuses on.
6 Conclusion
This paper showed how to acquire a large number of
contradiction pairs between lexico-syntactic binary
patterns by exploiting (1) the interaction between
contradiction and entailment, and (2) excitation po-
larities. In the end, we could acquire 750,000 typed
contradiction pattern pairs with an estimated 80%
precision. The resulting contradiction pairs cov-
ered ones deeply related to world knowledge such
as the pair ??X reassures Y?, ?X betrays Y??. We ex-
pect our work to lead to a high level analysis of
textual information, such as flagging unreliable in-
formation or identifying important documents to be
surveyed for understanding complex social prob-
lems. We plan to release the data we acquired to
the NLP community through the ALAGIN Forum5.
5 http://www.alagin.jp/
702
References
J. Berant, I. Dagan, and J. Goldberger. 2011. Global
learning of typed entailment rules. In Proceedings of
ACL 2011, pages 610?619.
D. G. Bobrow, C. Condoravdi, R. Crouch, V. De Paiva,
L. Karttunen, T. H. King, R. Nairn, L. Price, and
A. Zaenen. 2007. Precision-focused textual inference.
In Proceedings of the ACL-PASCAL Workshop on Tex-
tual Entailment and Paraphrasing, page 16?21.
M.-C. De Marneffe, A. N. Rafferty, and C. D. Manning.
2008. Finding contradictions in text. Proceedings of
ACL 2008, page 1039?1047.
S. De Saeger, K. Torisawa, J. Kazama, K. Kuroda, and
M. Murata. 2009. Large scale relation acquisition us-
ing class dependent patterns. In Proceedings of ICDM
2009, page 764?769.
S.M. Harabagiu, A. Hickl, and V.F. Lacatusu. 2006.
Negation, contrast and contradiction in text process-
ing. In Proceedings of AAAI 2006, pages 755?762.
C. Hashimoto, K. Torisawa, K. Kuroda, S. De Saeger,
M. Murata, and J. Kazama. 2009. Large-scale verb
entailment acquisition from the web. In Proceedings
of EMNLP 2009, volume 3, page 1172?1181.
C. Hashimoto, K. Torisawa, S. De Saeger, J.-H. Oh, and
J. Kazama. 2012. Excitatory or inhibitory: A new se-
mantic orientation extracts contradiction and causality
from the web. In Proceedings of EMNLP 2012.
D. Kawahara, S. Kurohashi, and K. Inui. 2008. Grasp-
ing major statements and their contradictions toward
information credibility analysis of web contents. In
Proceedings of WI-IAT 2008, volume 1, page 393?
397.
D. Kawahara, K. Inui, and S. Kurohashi. 2010. Iden-
tifying contradictory and contrastive relations between
statements to outline web information on a given topic.
In Proceedings of COLING 2010, page 534?542.
J. Kazama and K. Torisawa. 2008. Inducing gazetteers
for named entity recognition by large-scale clustering
of dependency relations. Proceedings of ACL 2008,
page 407?415.
J. Kloetzer, S. De Saeger, K. Torisawa, M. Sano,
C. Hashimoto, and J. Gotoh. 2013. Large-scale acqui-
sition of entailment pattern pairs. In Information Pro-
cessing Society of Japan (IPSJ) Kansai-Branch Con-
vention.
S. Kurohashi and M. Nagao. 1994. KN parser: Japanese
dependency/case structure analyzer. In Proceedings
of the Workshop on Sharable Natural Language Re-
sources, page 48?55.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
page 159?174.
D. Lin and P. Pantel. 2001. Dirt - discovery of inference
rules from text. In Proceedings of the ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing, pages 323?328.
B. Magnini and E. Cabrio. 2010. Contradiction-focused
qualitative evaluation of textual entailment. In Pro-
ceedings of the Workshop on Negation and Speculation
in Natural Language Processing, page 86?94.
P. Malakasiotis and I. Androutsopoulos. 2007. Learning
textual entailment using SVMs and string similarity
measures. In Proceedings of the ACL- PASCAL Work-
shop on Textual Entailment and Paraphrasing, page 42
?47.
K. Murakami, E. Nichols, S. Matsuyoshi, A. Sumida,
S. Masuda, K. Inui, and Y. Matumoto. 2009. State-
ment map: assisting information crediblity analysis
by visualizing arguments. In Proceedings of the 3rd
workshop on Information credibility on the web, page
43?50. ACM.
M. Ohki, S. Matsuyoshi, J. Mizuno, K. Inui, E. Nichols,
K. Murakami, S. Masuda, and Y. Matsumoto. 2011.
Recognizing confinement in web texts. In the Pro-
ceedings of the Ninth International Conference on
Computational Semantics, page 215?224.
A. Ritter, D. Downey, S. Soderland, and O. Etzioni.
2008. It?s a contradiction?no, it?s not: a case study
using functional relations. In Proceedings of EMNLP
2008, pages 11?20.
S. Schoenmackers, O. Etzioni, D. S Weld, and J. Davis.
2010. Learning first-order horn clauses from web text.
In Proceedings of EMNLP 2010, page 1088?1098.
I. Szpektor, E. Shnarch, and I. Dagan. 2007. Instance-
based evaluation of entailment rule acquisition. In
Proceedings of ACL 2007, volume 45, page 456?463.
E. M. Voorhees. 2008. Contradictions and justifications:
Extensions to the textual entailment task. In Proceed-
ings of ACL 2008, page 63?71.
J. Weeds and D. Weir. 2003. A general framework for
distributional similarity. In Proceedings of EMNLP
2003, page 81?88. Association for Computational
Linguistics.
703
Proceedings of NAACL-HLT 2013, pages 63?73,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Minimally Supervised Method for Multilingual Paraphrase Extraction
from Definition Sentences on the Web
Yulan Yan? Chikara Hashimoto? Kentaro Torisawa?
Takao Kawai? Jun?ichi Kazama? Stijn De Saeger??
? ? ? ?? ?? Information Analysis Laboratory
Universal Communication Research Institute
National Institute of Information and Communications Technology (NICT)
{? yulan, ? ch, ? torisawa, ??stijn}@nict.go.jp
Abstract
We propose a minimally supervised method
for multilingual paraphrase extraction from
definition sentences on the Web. Hashimoto
et al (2011) extracted paraphrases from
Japanese definition sentences on the Web, as-
suming that definition sentences defining the
same concept tend to contain paraphrases.
However, their method requires manually an-
notated data and is language dependent. We
extend their framework and develop a mini-
mally supervised method applicable to multi-
ple languages. Our experiments show that our
method is comparable to Hashimoto et al?s
for Japanese and outperforms previous unsu-
pervised methods for English, Japanese, and
Chinese, and that our method extracts 10,000
paraphrases with 92% precision for English,
82.5% precision for Japanese, and 82% preci-
sion for Chinese.
1 Introduction
Automatic paraphrasing has been recognized as an
important component for NLP systems, and many
methods have been proposed to acquire paraphrase
knowledge (Lin and Pantel, 2001; Barzilay and
McKeown, 2001; Shinyama et al, 2002; Barzilay
and Lee, 2003; Dolan et al, 2004; Callison-Burch,
2008; Hashimoto et al, 2011; Fujita et al, 2012).
We propose a minimally supervised method for
multilingual paraphrase extraction. Hashimoto et al
(2011) developed a method to extract paraphrases
from definition sentences on the Web, based on
their observation that definition sentences defining
the same concept tend to contain many paraphrases.
Their method consists of two steps; they extract def-
inition sentences from the Web, and extract phrasal
(1) a. Paraphrasing is the use of your own words to express the au-
thor?s ideas without changing the meaning.
b. Paraphrasing is defined as a process of transforming an expres-
sion into another while keeping its meaning intact.
(2) a. ?????????????????????????
???????????????? (Paraphrasing refers to
the replacement of an expression into another without changing
the semantic content.)
b. ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
???????????????????????
????????? (Paraphrasing is a process of trans-
forming an expression into another of the same language while
preserving the meaning and content as much as possible.)
(3) a. ????????????????????????
??????? (Paraphrasing refers to the transformation
of sentence structure by the translator without changing the
meaning of original text.)
b. ?????????????????????????
(Paraphrasing is a translation method of keeping the content of
original text but not keeping the expression.)
Figure 1: Multilingual definition pairs on ?paraphrasing.?
paraphrases from the definition sentences. Both
steps require supervised classifiers trained by manu-
ally annotated data, and heavily depend on their tar-
get language. However, the basic idea is actually
language-independent. Figure 1 gives examples of
definition sentences on the Web that define the same
concept in English, Japanese, and Chinese (with En-
glish translation). As indicated by underlines, each
definition pair has a phrasal paraphrase.
We aim at extending Hashimoto et al?s method
to a minimally supervised method, thereby enabling
acquisition of phrasal paraphrases within one lan-
guage, but in different languages without manually
annotated data. The first contribution of our work
is to develop a minimally supervised method for
multilingual definition extraction that uses a clas-
sifier distinguishing definition from non-definition.
The classifier is learnt from the first sentences in
63
Defini?n	 ?sentences	 Defini?n	 ?pairs	 ? Paraphrase	 ?candidates	 ?
Ranked	 ?paraphrase	 ?candidates	 ?Classifier	Web	
Defini?on	 ?Extrac?on	 ?(Sec?on	 ?2.1)	 Paraphrase	 ?Extrac?on	 ?(Sec?on	 ?2.2)	
Ranking	 ?by	 ?Score	Automa?cally	 ?constructed	 ?training	 ?data	
Web	 Wikipedia	
Figure 2: Overall picture of our method.
Wikipedia articles, which can be regarded as the def-
inition of the title of Wikipedia article (Kazama and
Torisawa, 2007) and hence can be used as positive
examples. Our method relies on a POS tagger, a de-
pendency parser, a NER tool, noun phrase chunking
rules, and frequency thresholds for each language,
in addition to Wikipedia articles, which can be seen
as a manually annotated knowledge base. How-
ever, our method needs no additional manual anno-
tation particularly for this task and thus we catego-
rize our method as a minimally supervised method.
On the other hand, Hashimoto et al?s method heav-
ily depends on the properties of Japanese like the
assumption that characteristic expressions of defini-
tion sentences tend to appear at the end of sentence
in Japanese. We show that our method is applica-
ble to English, Japanese, and Chinese, and that its
performance is comparable to state-of-the-art super-
vised methods (Navigli and Velardi, 2010). Since
the three languages are very different we believe that
our definition extraction method is applicable to any
language as long as Wikipedia articles of the lan-
guage exist.
The second contribution of our work is to de-
velop a minimally supervised method for multi-
lingual paraphrase extraction from definition sen-
tences. Again, Hashimoto et al?s method utilizes
a supervised classifier trained with annotated data
particularly prepared for this task. We eliminate the
need for annotation and instead introduce a method
that uses a novel similarity measure considering
the occurrence of phrase fragments in global con-
texts. Our paraphrase extraction method is mostly
language-independent and, through experiments for
the three languages, we show that it outperforms
unsupervised methods (Pas?ca and Dienes, 2005;
Koehn et al, 2007) and is comparable to Hashimoto
et al?s supervised method for Japanese.
Previous methods for paraphrase (and entailment)
extraction can be classified into a distributional sim-
ilarity based approach (Lin and Pantel, 2001; Gef-
fet and Dagan, 2005; Bhagat et al, 2007; Szpek-
tor and Dagan, 2008; Hashimoto et al, 2009) and a
parallel corpus based approach (Barzilay and McK-
eown, 2001; Shinyama et al, 2002; Barzilay and
Lee, 2003; Dolan et al, 2004; Callison-Burch,
2008). The former can exploit large scale monolin-
gual corpora, but is known to be unable to distin-
guish paraphrase pairs from antonymous pairs (Lin
et al, 2003). The latter rarely mistakes antonymous
pairs for paraphrases, but preparing parallel corpora
is expensive. As with Hashimoto et al (2011), our
method is a kind of parallel corpus approach in that it
uses definition pairs as a parallel corpus. However,
our method does not suffer from a high labor cost
of preparing parallel corpora, since it can automati-
cally collect definition pairs from the Web on a large
scale. The difference between ours and Hashimoto
et al?s is that our method requires no manual label-
ing of data and is mostly language-independent.
2 Proposed Method
Our method first extracts definition sentences from
the Web, and then extracts paraphrases from the def-
inition sentences, as illustrated in Figure 2.
2.1 Definition Extraction
2.1.1 Automatic Construction of Training Data
Our method learns a classifier that classifies sen-
tences into definition and non-definition using auto-
matically constructed training data, TrDat. TrDat?s
positive examples, Pos, are the first sentences of
Wikipedia articles and the negative examples, Neg,
are randomly sampled Web sentences. The former
can be seen as definition, while the chance that the
sentences in the latter are definition is quite small.
Our definition extraction not only distinguishes
definition from non-definition but also identities the
defined term of a definition sentence, and in the
paraphrase extraction step our method couples two
definition sentences if their defined terms are identi-
cal. For example, the defined terms of (1a) and (1b)
in Figure 1 are both ?Paraphrasing? and thus the two
definition sentences are coupled. For Pos, we mark
up the title of Wikipedia article as the defined term.
For Neg, we randomly select a noun phrase in a sen-
64
(A)
N-gram definition pattern N-gram non-definition pattern
?[term] is the [term] may be
[term] is a type of [term] is not
(B)
Subsequence definition pattern Subsequence non-definition pattern
[term] is * which is located you may * [term]
[term] is a * in the was [term] * , who is
(C)
Subtree definition pattern Subtree non-definition pattern
[term] is defined as the NP [term] will not be
Table 1: Examples of English patterns.
tence and mark it up as a (false) defined term. Any
marked term is uniformly replaced with [term].
2.1.2 Feature Extraction and Learning
As features, we use patterns that are characteristic
of definition (definition patterns) and those that are
unlikely to be a part of definition (non-definition pat-
terns). Patterns are either N-grams, subsequences, or
dependency subtrees, and are mined automatically
from TrDat. Table 1 shows examples of patterns
mined by our method. In (A) of Table 1, ??? is
a symbol representing the beginning of a sentence.
In (B), ?*? represents a wildcard that matches any
number of arbitrary words. Patterns are represented
by either their words? surface form, base form, or
POS. (Chinese words do not inflect and thus we do
not use the base form for Chinese.)
We assume that definition patterns are fre-
quent in Pos but are infrequent in Neg, and
non-definition patterns are frequent in Neg but
are infrequent in Pos. To see if a given pat-
tern ? is likely to be a definition pattern, we
measure ??s probability rate Rate(?). If the
probability rate of ? is large, ? tends to be a
definition pattern. The probability rate of ? is:
Rate(?) =
freq(?,Pos)/|Pos|
freq(?,Neg)/|Neg|
, iffreq(?,Neg) 6= 0.
Here, freq(?,Pos) = |{s ? Pos : ? ? s}| and
freq(?,Neg) = |{s ? Neg : ? ? s}|. We write ? ? s
if sentence s contains ?. If freq(?,Neg) = 0,
Rate(?) is set to the largest value of all the patterns?
Rate values. Only patterns whose Rate is more
than or equal to a Rate threshold ?pos and whose
freq(?,Pos) is more than or equal to a frequency
threshold are regarded as definition patterns. Simi-
larly, we check if ? is likely to be a non-definition
pattern. Only patterns whose Rate is less or equal
English Japanese Chinese
Type Representation Pos Neg Pos Neg Pos Neg
N-gram
Surface 120 400 30 100 20 100
Base 120 400 30 100 ? ?
POS 2,000 4,000 500 500 100 400
Subsequence
Surface 120 400 30 100 20 40
Base 120 400 30 100 ? ?
POS 2,000 2,000 500 500 200 400
Subtree
Surface 5 10 5 10 5 5
Base 5 10 5 10 ? ?
POS 25 50 25 50 25 50
Table 2: Values of frequency threshold.
to a Rate threshold ?neg and whose freq(?,Neg)
is more than or equal to a frequency threshold are
regarded as non-definition patterns. The probability
rate is based on the growth rate (Dong and Li,
1999).
?pos and ?neg are set to 2 and 0.5, while the fre-
quency threshold is set differently according to lan-
guages, pattern types (N-gram, subsequence, and
subtree), representation (surface, base, and POS),
and data (Pos and Neg), as in Table 2. The thresholds
in Table 2 were determined manually, but not really
arbitrarily. Basically they were determined accord-
ing to the frequency of each pattern in our data (e.g.
how frequently the surface N-gram of English ap-
pears in English positive training samples (Pos)).
Below, we detail how patterns are acquired. First,
we acquire N-gram patterns. Then, subsequence
patterns are acquired using the N-gram patterns as
input. Finally, subtree patterns are acquired using
the subsequence patterns as input.
N-gram patterns We collect N-gram patterns
from TrDat with N ranging from 2 to 6. We filter
out N-grams using thresholds on the Rate and fre-
quency, and regard those that are kept as definition
or non-definition N-grams.
Subsequence patterns We generate subsequence
patterns as ordered combinations of N-grams with
the wild card ?*? inserted between them (we use
two or three N-grams for a subsequence). Then, we
check each of the generated subsequences and keep
it if there exists a sentence in TrDat that contains the
subsequence and whose root node is contained in the
subsequence. For example, subsequence ?[term]
is a * in the? is kept if a term-marked sentence like
?[term] is a baseball player in the Dominican Re-
public.? exists in TrDat. Then, patterns are filtered
65
out using thresholds on the Rate and frequency as
we did for N-grams.
Subtree patterns For each definition and non-
definition subsequence, we retrieve all the term-
marked sentences that contain the subsequence from
TrDat, and extract a minimal dependency subtree
that covers all the words of the subsequence from
each retrieved sentence. For example, assume that
we retrieve a term-marked sentence ?[term] is
usually defined as the way of life of a group of peo-
ple.? for subsequence ?[term] is * defined as the?.
Then we extract from the sentence the minimal de-
pendency subtree in the left side of (C) of Table 1.
Note that all the words of the subsequence are con-
tained in the subtree, and that in the subtree a node
(?way?) that is not a part of the subsequence is re-
placed with its dependency label (?NP?) assigned by
the dependency parser. The patterns are filtered out
using thresholds on the Rate and frequency.
We train a SVM classifier1 with a linear kernel,
using binary features that indicate the occurrence of
the patterns described above in a target sentence.
In theory, we could feed all the features to the
SVM classifier and let the classifier pick informa-
tive features. But we restricted the feature set for
practical reasons: the number of features would be-
come tremendously large. There are two reasons for
this. First, the number of sentences in our automati-
cally acquired training data is huge (2,439,257 posi-
tive sentences plus 5,000,000 negative sentences for
English, 703,208 positive sentences plus 1,400,000
negative sentences for Japanese and 310,072 posi-
tive sentences plus 600,000 negative sentences for
Chinese). Second, since each subsequence pattern
is generated as a combination of two or three N-
gram patterns and one subsequence pattern can gen-
erate one or more subtree patterns, using all possi-
ble features leads to a combinatorial explosion of
features. Moreover, since the feature vector will be
highly sparse with a huge number of infrequent fea-
tures, SVM learning becomes very time consuming.
In preliminary experiments we observed that when
using all possible features the learning process took
more than one week for each language. We there-
fore introduced the current feature selection method,
in which the learning process finished in one day but
1http://svmlight.joachims.org.
Original Web sentence: Albert Pujols is a baseball player.
Term-marked sentence 1: [term] is a baseball player.
Term-marked sentence 2: Albert Pujols is a [term].
Figure 3: Term-marked sentences from a Web sentence.
still obtains good results.
2.1.3 Definition Extraction from the Web
We extract a large amount of definition sen-
tences by applying this classifier to sentences in our
Web archive. Because our classifier requires term-
marked sentences (sentences in which the term be-
ing defined is marked) as input, we first have to iden-
tify all such defined term candidates for each sen-
tence. For example, Figure 3 shows a case where a
Web sentence has two NPs (two candidates of de-
fined term). Basically we pick up NPs in a sen-
tence by simple heuristic rules. For English, NPs are
identified using TreeTagger (Schmid, 1995) and two
NPs are merged into one when they are connected by
?for? or ?of?. After applying this procedure recur-
sively, the longest NPs are regarded as candidates of
defined terms and term-marked sentences are gener-
ated. For Japanese, we first identify nouns that are
optionally modified by adjectives as NPs, and allow
two NPs connected by ??? (of ), if any, to form
a larger NP. For Chinese, nouns that are optionally
modified by adjectives are considered as NPs.
Then, each term-marked sentence is given a fea-
ture vector and classified by the classifier. The term-
marked sentence whose SVM score (the distance
from the hyperplane) is the largest among those from
the same original Web sentence is chosen as the final
classification result for the original Web sentence.
2.2 Paraphrase Extraction
We use all the Web sentences classified as defini-
tion and all the sentences in Pos for paraphrase ex-
traction. First, we couple two definition sentences
whose defined term is the same. We filter out defini-
tion sentence pairs whose cosine similarity of con-
tent word vectors is less than or equal to threshold
C, which is set to 0.1. Then, we extract phrases
from each definition sentence, and generate all pos-
sible phrase pairs from the coupled sentences. In
this study, phrases are restricted to predicate phrases
that consist of at least one dependency relation and
in which all the constituents are consecutive in a
66
f1
The ratio of the number of words shared between two can-
didate phrases to the number of all of the words in the two
phrases. Words are represented by either their surface form
(f1,1), base form (f1,2) or POS (f1,3).
f2
The identity of the leftmost word (surface form (f2,1), base
form (f2,2) or POS (f2,3)) between two candidate phrases.
f3
The same as f2 except that we use the rightmost word.
There are three corresponding subfunctions (f3,1 to f3,3).
f4
The ratio of the number of words that appear in a candidate
phrase segment of a definition sentence s1 and in a segment
that is NOT a part of the candidate phrase of another def-
inition sentence s2 to the number of all the words of s1?s
candidate phrase. Words are in their base form (f4,1).
f5 The reversed (s1 ? s2) version of f4,1 (f5,1).
f6
The ratio of the number of words (the surface form) of a
shorter candidate phrase to that of a longer one (f6,1).
f7
Cosine similarity between two definition sentences from
which two candidate phrases are extracted. Only content
words in the base form are used (f7,1).
f8
The ratio of the number of parent dependency subtrees that
are shared by two candidate phrases to the number of all the
parent dependency subtrees. The parent dependency sub-
trees are adjacent to the candidate phrases and represented
by their surface form (f8,1), base form (f8,2), or POS (f8,3).
f9
The same as f8 except that we use child dependency sub-
trees. There are 3 subfunctions (f9,1 to f9,3) of f9 type.
f10
The ratio of the number of context N-grams that are shared
by two candidate phrases to the number of all the context N-
grams of both candidate phrases. The context N-grams are
adjacent to the candidate phrases and represented by either
the surface form, the base form, or POS. The N ranges from
1 to 3, and the context is either left-side or right-side. Thus,
there are 18 subfunctions (3? 3? 2).
Table 3: Local similarity subfunctions, f1,1 to f10,18.
sentence. Accordingly, if two definition sentences
that are coupled have three such predicate phrases
respectively, we get nine phrase pairs, for instance.
A phrase pair extracted from a definition pair is a
paraphrase candidate and is given a score that indi-
cates the likelihood of being a paraphrase, Score. It
consists of two similarity measures, local similarity
and global similarity, which are detailed below.
Local similarity Following Hashimoto et al, we
assume that two candidate phrases (p1, p2) tend to
be a paraphrase if they are similar enough and/or
their surrounding contexts are sufficiently similar.
Then, we calculate the local similarity (localSim) of
(p1, p2) as the weighted sum of 37 similarity sub-
functions that are grouped into 10 types (Table 3.)
For example, the f1 type consists of three subfunc-
tions, f1,1, f1,2, and f1,3. The 37 subfunctions are
inspired by Hashimoto et al?s features. Then, local-
Sim is defined as:
localSim(p1, p2) = max
(dl,dm)?DP (p1,p2)
ls(p1, p2, dl, dm).
Here, ls(p1, p2, dl, dm) =
?10
i=1
?ki
j=1
wi,j?fi,j(p1,p2,dl,dm)
ki .
DP (p1, p2) is the set of all definition sentence pairs
that contain (p1, p2). (dl, dm) is a definition sen-
tence pair containing (p1, p2). ki is the number
of subfunctions of fi type. wi,j is the weight for
fi,j . wi,j is uniformly set to 1 except for f4,1
and f5,1, whose weight is set to ?1 since they
indicate the unlikelihood of (p1, p2)?s being a
paraphrase. As the formula indicates, if there is
more than one definition sentence pair that contains
(p1, p2), localSim is calculated from the definition
sentence pair that gives the maximum value of
ls(p1, p2, dl, dm). localSim is local in the sense that
it is calculated based on only one definition pair
from which (p1, p2) are extracted.
Global similarity The global similarity (global-
Sim) is our novel similarity function. We decompose
a candidate phrase pair (p1, p2) into Comm, the com-
mon part between p1 and p2, and Diff , the difference
between the two. For example, Comm and Diff of
(?keep the meaning intact?, ?preserve the meaning?)
is (?the meaning?) and (?keep, intact?, ?preserve?).
globalSim measures the semantic similarity of
the Diff of a phrase pair. It is proposed based on
the following intuition: phrase pair (p1, p2) tend
to be a paraphrase if their surface difference (i.e.
Diff ) have the same meaning. For example, if
?keep, intact? and ?preserve? mean the same, then
(?keep the meaning intact?, ?preserve the meaning?)
is a paraphrase.
globalSim considers the occurrence of Diff in
global contexts (i.e., all the paraphrase candidates
from all the definition pairs). The globalSim of a
given phrase pair (p1, p2) is measured by basically
counting how many times the Diff of (p1, p2) ap-
pears in all the candidate phrase pairs from all the
definition pairs. The assumption is that Diff tends to
share the same meaning if it appears repeatedly in
paraphrase candidates from all definition sentence
pairs, i.e., our parallel corpus. Each occurrence of
Diff is weighted by the localSim of the phrase pair
in which Diff occurs. Precisely, globalSim is defined
as:
67
Threshold The frequency threshold of Table 2 (Section 2.1.2).
NP rule Rules for identifying NPs in sentences (Section 2.1.3).
POS list The list of content words? POS (Section 2.2).
Tagger/parser POS taggers, dependency parsers and NER tools.
Table 4: Language-dependent components.
globalSim(p1, p2) =
?
(pi,pj)?PP (p1,p2)
localSim(pi, pj)
M
.
PP (p1, p2) is the set of candidate phrase pairs
whose Diff is the same as (p1, p2).2 M is the num-
ber of similarity subfunction types whose weight is
1, i.e. M = 8 (all the subfunction types except f4
and f5). It is used to normalize the value of each
occurrence of Diff to [0, 1].3 globalSim is global
in the sense that it considers all the definition pairs
that have a phrase pair with the same Diff as a target
candidate phrase pair (p1, p2).
The final score for a candidate phrase pair is:
Score(p1, p2) = localSim(p1, p2) + ln globalSim(p1, p2).
The way of combining the two similarity functions
has been determined empirically after testing several
other ways of combining them. This ranks all the
candidate phrase pairs.
Finally, we summarize language-dependent com-
ponents that we fix manually in Table 4.
3 Experiments
3.1 Experiments of Definition Extraction
We show that our unsupervised definition extrac-
tion method is competitive with state-of-the-art su-
pervised methods for English (Navigli and Velardi,
2010), and that it extracts a large number of defini-
tions reasonably accurately for English (3,216,121
definitions with 70% precision), Japanese (651,293
definitions with 62.5% precision), and Chinese
(682,661 definitions with 67% precision).
2If there are more than one (pi, pj) in a definition pair, we
use only one of them that has the largest localSim value.
3Although we claim that our idea of using globalSim is ef-
fective, we do not claim that the above formula for calculating
is the optimal way to implement the idea. Currently we are in-
vestigating a more mathematically well-motivated model.
3.1.1 Preparing Corpora
First we describe Pos, Neg, and the Web corpus
from which definition sentences are extracted. As
the source of Pos, we used the English Wikipedia
of April 2011 (3,620,149 articles), the Japanese
Wikipedia of October 2011 (830,417 articles), and
the Chinese Wikipedia of August 2011 (365,545 ar-
ticles). We removed category articles, template ar-
ticles, list articles and so on from them. Then the
number of sentences of Pos was 2,439,257 for En-
glish, 703,208 for Japanese, and 310,072 for Chi-
nese. We verified our assumption that Wikipedia
first sentences can mostly be seen as definition by
manually checking 200 random samples from Pos.
96.5% of English Pos, 100% of Japanese Pos, and
99.5% of Chinese Pos were definitions.
As the source of Neg, we used 600 million
Japanese Web pages (Akamine et al, 2010) and
the ClueWeb09 corpus for English (about 504 mil-
lion pages) and Chinese (about 177 million pages).4
From each Web corpus, we collected the sentences
satisfying following conditions: 1) they contain 5
to 50 words and at least one verb, 2) less than half
of their words are numbers, and 3) they end with a
period. Then we randomly sampled sentences from
the collected sentences as Neg so that |Neg| was
about twice as large as |Pos|: 5,000,000 for English,
1,400,000 for Japanese, and 600,000 for Chinese.
In Section 3.1.3, we use 10% of the Web corpus as
the input to the definition classifier. The number of
sentences are 294,844,141 for English, 245,537,860
for Japanese, and 68,653,130 for Chinese.
All the sentences were POS-tagged and parsed.
We used TreeTagger and MSTParser (McDonald
et al, 2006) for English, JUMAN (Kurohashi and
Kawahara, 2009a) and KNP (Kurohashi and Kawa-
hara, 2009b) for Japanese, MMA (Kruengkrai et al,
2009) and CNP (Chen et al, 2009) for Chinese.
3.1.2 Comparison with Previous Methods
We compared our method with the state-of-the-
art supervised methods proposed by Navigli and Ve-
lardi (2010), using their WCL datasets v1.0 (http:
//lcl.uniroma1.it/wcl/), definition and non-
definition datasets for English (Navigli et al, 2010).
Specifically, we used its training data (TrDatwcl,
hereafter), which consisted of 1,908 definition and
4http://lemurproject.org/clueweb09.php/
68
Method Precision Recall F1 Accuracy
Proposeddef 86.79 86.97 86.88 89.18
WCL-1 99.88 42.09 59.22 76.06
WCL-3 98.81 60.74 75.23 83.48
Table 5: Definition classification results on TrDatwcl.
2,711 non-definition sentences, and compared the
following three methods. WCL-1 and WCL-3 are
methods proposed by Navigli and Velardi (2010).
They were trained and tested with 10 fold cross vali-
dation using TrDatwcl. Proposeddef is our method,
which used TrDat for acquiring patterns (Section
2.1.2) and training. We tested Proposeddef on each
of TrDatwcl?s 10 folds and averaged the results.
Note that, for Proposeddef , we removed sentences
in TrDatwcl from TrDat in advance for fairness.
Table 5 shows the results. The numbers for WCL-
1 and WCL-3 are taken from Navigli and Velardi
(2010). Proposeddef outperformed both methods in
terms of recall, F1, and accuracy. Thus, we conclude
that Proposeddef is comparable to WCL-1/WCL-3.
We conducted ablation tests of our method to in-
vestigate the effectiveness of each type of pattern.
When using only N-grams, F1 was 85.41. When
using N-grams and subsequences, F1 was 86.61.
When using N-grams and subtrees, F1 was 86.85.
When using all the features, F1 was 86.88. The re-
sults show that each type of patterns contribute to the
performance, but the contributions of subsequence
patterns and subtree patterns do not seem very sig-
nificant.
3.1.3 Experiments of Definition Extraction
We extracted definitions from 10% of the Web
corpus. We applied Proposeddef to the cor-
pus of each language, and the state-of-the-art su-
pervised method for Japanese (Hashimoto et al,
2011) (Hashidef , hereafter) to the Japanese corpus.
Hashidef was trained on their training data that con-
sisted of 2,911 sentences, 61.1% of which were def-
initions. Note that we removed sentences in TrDat
from 10% of the Web corpus in advance, while we
did not remove Hashimoto et al?s training data from
the corpus. This means that, for Hashidef , the train-
ing data is included in the test data.
For each method, we filtered out its positive out-
puts whose defined term appeared more than 1,000
times in 10% of the Web corpus, since those terms
tend to be too vague to be a defined term or re-
fer to an entity outside the definition sentence. For
example, if ?the college? appears more than 1,000
times in 10% of the corpus, we filter out sen-
tences like ?The college is one of three colleges
in the Coast Community College District and was
founded in 1947.? For Proposeddef , the number of
remaining positive outputs is 3,216,121 for English,
651,293 for Japanese, and 682,661 for Chinese. For
Hashidef , the number of positive outputs is 523,882.
For Proposeddef of each language, we randomly
sampled 200 sentences from the remaining positive
outputs. For Hashidef , we first sorted its output by
the SVM score in descending order and then ran-
domly sampled 200 from the top 651,293, i.e., the
same number as the remaining positive outputs of
Proposeddef of Japanese, out of all the remaining
sentences of Hashidef .
For each language, after shuffling all the samples,
two human annotators evaluated each sample. The
annotators for English and Japanese were not the au-
thors, while one of the Chinese annotators was one
of the authors. We regarded a sample as a defini-
tion if it was regarded as a definition by both an-
notators. Cohen?s kappa (Cohen, 1960) was 0.55
for English (moderate agreement (Landis and Koch,
1977)), 0.73 for Japanese (substantial agreement),
and 0.69 for Chinese (substantial agreement).
For English, Proposeddef achieved 70% precision
for the 200 samples. For Japanese, Proposeddef
achieved 62.5% precision for the 200 samples, while
Hashidef achieved 70% precision for the 200 sam-
ples. For Chinese, Proposeddef achieved 67% pre-
cision for the 200 samples. From these results, we
conclude that Proposeddef can extract a large num-
ber of definition sentences from the Web moderately
well for the three languages.
Although the precision is not very high, our ex-
periments in the next section show that we can still
extract a large number of paraphrases with high pre-
cision from these definition sentences, due mainly to
our similarity measures, localSim and globalSim.
3.2 Experiments of Paraphrase Extraction
We show (1) that our paraphrase extraction method
outperforms unsupervised methods for the three lan-
guages, (2) that globalSim is effective, and (3) that
our method is comparable to the state-of-the-art su-
69
ProposedScore: Our method. Outputs are ranked by Score.
Proposedlocal: This is the same as ProposedScore except that it ranks
outputs by localSim. The performance drop from ProposedScore
shows globalSim?s effectiveness.
Hashisup: Hashimoto et al?s supervised method. Training data is the
same as Hashimoto et al Outputs are ranked by the SVM score
(the distance from the hyperplane). This is for Japanese only.
Hashiuns: The unsupervised version of Hashisup. Outputs are
ranked by the sum of feature values. Japanese only.
SMT: The phrase table construction method of Moses (Koehn et al,
2007). We assume that Moses should extract a set of two phrases
that are paraphrases of each other, if we input monolingual par-
allel sentence pairs like our definition pairs. We used default
values for all the parameters. Outputs are ranked by the product
of two phrase translation probabilities of both directions.
P&D: The distributional similarity based method by Pas?ca and Di-
enes (2005) (their ?N-gram-Only? method). Outputs are ranked
by the number of contexts two phrases share. Following Pas?ca
and Dienes (2005), we used the parameters LC = 3 and
MaxP = 4, while MinP , which was 1 in Pas?ca and Dienes
(2005), was set to 2 since our target was phrasal paraphrases.
Table 6: Evaluated paraphrase extraction methods.
pervised method for Japanese.
3.2.1 Experimental Setting
We extracted paraphrases from definition sen-
tences in Pos and those extracted by Proposeddef in
Section 3.1.3. First we coupled two definition sen-
tences whose defined term was the same. The num-
ber of definition pairs was 3,208,086 for English,
742,306 for Japanese, and 457,233 for Chinese.
Then we evaluated six methods in Table 6.5 All
the methods except P&D took the same definition
pairs as input, while P&D?s input was 10% of the
Web corpus. The input can be seen as the same for
all the methods, since the definition pairs were de-
rived from that 10% of the Web corpus. In our ex-
periments Exp1 and Exp2 below, all evaluation sam-
ples were shuffled so that human annotators could
not know which sample was from which method.
Annotators were the same as those who conducted
the evaluation in Section 3.1.3. Cohen?s kappa (Co-
hen, 1960) was 0.83 for English, 0.88 for Japanese,
5We filtered out phrase pairs in which one phrase contained a
named entity but the other did not contain the named entity from
the output of ProposedScore, Proposedlocal, SMT , and P&D,
since most of them were not paraphrases. We used Stanford
NER (Finkel et al, 2005) for English named entity recognition
(NER), KNP for Japanese NER, and BaseNER (Zhao and Kit,
2008) for Chinese NER. Hashisup and Hashiuns did the named
entity filtering of the same kind (footnote 3 of Hashimoto et al
(2011)), and thus we did not apply the filter to them any further.
and 0.85 for Chinese, all of which indicated reason-
ably good (Landis and Koch, 1977). We regarded a
candidate phrase pair as a paraphrase if both annota-
tors regarded it as a paraphrase.
Exp1 We compared the methods that take def-
inition pairs as input, i.e. ProposedScore, Pro-
posedlocal, Hashisup, Hashiuns, and SMT . We ran-
domly sampled 200 phrase pairs from the top 10,000
for each method for evaluation. The evaluation of
each candidate phrase pair (p1, p2) was based on
bidirectional checking of entailment relation, p1 ?
p2 and p2 ? p1, with p1 and p2 embedded in con-
texts, as Hashimoto et al (2011) did. Entailment
relation of both directions hold if (p1, p2) is a para-
phrase. We used definition pairs from which candi-
date phrase pairs were extracted as contexts.
Exp2 We compared ProposedScore and P&D.
Since P&D restricted its output to phrase pairs in
which each phrase consists of two to four words,
we restricted the output of ProposedScore to 2-to-4-
words phrase pairs, too. We randomly sampled 200
from the top 3,000 phrase pairs from each method
for evaluation, and the annotators checked entail-
ment relation of both directions between two phrases
using Web sentence pairs that contained the two
phrases as contexts.
3.2.2 Results
From Exp1, we obtained precision curves in the
upper half of Figure 4. The curves were drawn from
the 200 samples that were sorted in descending order
by their score, and we plotted a dot for every 5 sam-
ples. ProposedScore outperformed Proposedlocal for
the three languages, and thus globalSim was effec-
tive. ProposedScore outperformed Hashisup. How-
ever, we observed that ProposedScore acquired many
candidate phrase pairs (p1, p2) for which p1 and p2
consisted of the same content words like ?send a
postcard to the author? and ?send the author a post-
card,? while the other methods tended to acquire
more content word variations like ?have a chance?
and ?have an opportunity.? Then we evaluated all
the methods in terms of how many paraphrases with
content word variations were extracted. We ex-
tracted from the evaluation samples only candidate
phrase pairs whose Diff contained a content word
(content word variation pairs), to see how many
70
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200
Prec
ision
(A) Top N (#Samples)  0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200(B) Top N (#Samples)  0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200(C) Top N (#Samples)
?Proposed_score?
?Proposed_local?
?SMT?
?Hashi_sup?
?Hashi_uns?
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200
Prec
ision
(a) Top N (#Samples)  0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200(b) Top N (#Samples)  0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200(c) Top N (#Samples)
?Proposed_score_cwv?
?Proposed_local_cwv?
?SMT_cwv?
?Hashi_sup_cwv?
?Hashi_uns_cwv?
Figure 4: Precision curves of Exp1: English (A)(a), Chinese (B)(b), and Japanese (C)(c).
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200
Prec
ision
(A) Top N (#Samples)  0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200(B) Top N (#Samples)  0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200(C) Top N (#Samples)
?Proposed_score?
?Proposed_score_cwv?
?Pasca?
?Pasca_cwv?
Figure 5: Precision curves of Exp2: English (A), Chinese (B), and Japanese (C).
of them were paraphrases. The lower half of Fig-
ure 4 shows the results (curves labeled with cwv).
The number of samples for ProposedScore reduced
drastically compared to the others for English and
Japanese, though precision was kept at a high level.
It is due mainly to the globalSim; the Diff of the
non-content word variation pairs appears frequently
in paraphrase candidates, and thus their globalSim
scores are high.
From Exp2, precision curves in Figure 5 were
obtained. P&D acquired more content word varia-
tion pairs as the curves labeled by cwv indicates.
However, ProposedScore?s precision outperformed
P&D?s by a large margin for the three languages.
From all of these results, we conclude (1) that our
paraphrase extraction method outperforms unsuper-
vised methods for the three languages, (2) that glob-
alSim is effective, and (3) that our method is com-
parable to the state-of-the-art supervised method for
Japanese, though our method tends to extract fewer
content word variation pairs than the others.
Table 7 shows examples of English paraphrases
extracted by ProposedScore.
is based in Halifax = is headquartered in Halifax
used for treating HIV = used to treat HIV
is a rare form = is an uncommon type
is a set = is an unordered collection
has an important role = plays a key role
Table 7: Examples of extracted English paraphrases.
4 Conclusion
We proposed a minimally supervised method for
multilingual paraphrase extraction. Our experiments
showed that our paraphrase extraction method out-
performs unsupervised methods (Pas?ca and Dienes,
2005; Koehn et al, 2007; Hashimoto et al, 2011)
for English, Japanese, and Chinese, and is compara-
ble to the state-of-the-art language dependent super-
vised method for Japanese (Hashimoto et al, 2011).
71
References
Susumu Akamine, Daisuke Kawahara, Yoshikiyo Kato,
Tetsuji Nakagawa, Yutaka I. Leon-Suematsu, Takuya
Kawada, Kentaro Inui, Sadao Kurohashi, and Yutaka
Kidawara. 2010. Organizing information on the web
to support user judgments on information credibil-
ity. In Proceedings of 2010 4th International Uni-
versal Communication Symposium Proceedings (IUCS
2010), pages 122?129.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL
2003, pages 16?23.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the ACL joint
with the 10th Meeting of the European Chapter of the
ACL (ACL/EACL 2001), pages 50?57.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning direc-
tionality of inference rules. In Proceedings of Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP2007), pages 161?170.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 196?205.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving dependency
parsing with subtrees from auto-parsed data. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?09,
pages 570?579, Singapore. Association for Computa-
tional Linguistics.
Jacob Cohen. 1960. Coefficient of agreement for nom-
inal scales. In Educational and Psychological Mea-
surement, pages 37?46.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of the 20th international conference on Com-
putational Linguistics (COLING 2004), pages 350?
356, Geneva, Switzerland, Aug 23?Aug 27.
Guozhu Dong and Jinyan Li. 1999. Efficient mining of
emerging patterns: discovering trends and differences.
In Proceedings of the fifth ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ?99, pages 43?52, San Diego, California, United
States.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2005),
pages 363?370.
Atsushi Fujita, Pierre Isabelle, and Roland Kuhn. 2012.
Enlarging paraphrase collections through generaliza-
tion and instantiation. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL 2012), pages 631?
642.
Maayan Geffet and Ido Dagan. 2005. The distributional
inclusion hypotheses and lexical entailment. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2005), pages
107?114.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,
Stijn De Saeger, Masaki Murata, and Jun?ichi Kazama.
2009. Large-scale verb entailment acquisition from
the web. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2009), pages 1172?1181.
Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger,
Jun?ichi Kazama, and Sadao Kurohashi. 2011. Ex-
tracting paraphrases from definition sentences on the
web. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1087?1097, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Exploit-
ing Wikipedia as external knowledge for named entity
recognition. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 698?707, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint chinese word segmentation and pos
tagging. In Proceedings of the Joint Conference of the
72
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 513?521, Suntec, Singapore,
August. Association for Computational Linguistics.
Sadao Kurohashi and Daisuke Kawahara. 2009a.
Japanese morphological analyzer system ju-
man version 6.0 (in japanese). Kyoto University,
http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN.
Sadao Kurohashi and Daisuke Kawahara. 2009b.
Japanese syntax and case analyzer knp version 3.0
(in japanese). Kyoto University, http://nlp.ist.i.kyoto-
u.ac.jp/EN/index.php?KNP.
J. Richard Landis and Gary G. Koch. 1977. Measure-
ment of observer agreement for categorical data. Bio-
metrics, 33(1):159?174.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Dekang Lin, Shaojun Zhao Lijuan Qin, and Ming Zhou.
2003. Identifying synonyms among distributionally
similar words. In Proceedings of the 18th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-03), pages 1492?1493.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning, CoNLL-X ?06, pages 216?220, New
York City, New York.
Roberto Navigli and Paola Velardi. 2010. Learning
word-class lattices for definition and hypernym extrac-
tion. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1318?1327, Uppsala, Sweden, July. Association for
Computational Linguistics.
Roberto Navigli, Paola Velardi, and Juana Mar??a Ruiz-
Mart??nez. 2010. An annotated dataset for extracting
definitions and hypernyms from the web. In Proceed-
ings of LREC 2010, pages 3716?3722.
Marius Pas?ca and Pe?ter Dienes. 2005. Aligning needles
in a haystack: paraphrase acquisition across the web.
In Proceedings of the Second international joint con-
ference on Natural Language Processing, IJCNLP?05,
pages 119?130, Jeju Island, Korea.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to german. In Proceedings
of the ACL SIGDAT-Workshop, pages 47?50.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proceedings of the 2nd international Con-
ference on Human Language Technology Research
(HLT2002), pages 313?318.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary template. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING2008), pages 849?856.
Hai Zhao and Chunyu Kit. 2008. Unsupervised seg-
mentation helps supervised learning of character tag-
ging for word segmentation and named entity recog-
nition. In Proceedings of the Sixth SIGHAN Workshop
on Chinese Language Processing, pages 106?111, Hy-
derabad, India.
73
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247?256,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Bayesian Method for Robust Estimation of Distributional Similarities
Jun?ichi Kazama Stijn De Saeger Kow Kuroda
Masaki Murata? Kentaro Torisawa
Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology (NICT)
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289 Japan
{kazama, stijn, kuroda, torisawa}@nict.go.jp
?Department of Information and Knowledge Engineering
Faculty/Graduate School of Engineering, Tottori University
4-101 Koyama-Minami, Tottori, 680-8550 Japan?
murata@ike.tottori-u.ac.jp
Abstract
Existing word similarity measures are not
robust to data sparseness since they rely
only on the point estimation of words?
context profiles obtained from a limited
amount of data. This paper proposes a
Bayesian method for robust distributional
word similarities. The method uses a dis-
tribution of context profiles obtained by
Bayesian estimation and takes the expec-
tation of a base similarity measure under
that distribution. When the context pro-
files are multinomial distributions, the pri-
ors are Dirichlet, and the base measure is
the Bhattacharyya coefficient, we can de-
rive an analytical form that allows efficient
calculation. For the task of word similar-
ity estimation using a large amount of Web
data in Japanese, we show that the pro-
posed measure gives better accuracies than
other well-known similarity measures.
1 Introduction
The semantic similarity of words is a long-
standing topic in computational linguistics be-
cause it is theoretically intriguing and has many
applications in the field. Many researchers have
conducted studies based on the distributional hy-
pothesis (Harris, 1954), which states that words
that occur in the same contexts tend to have similar
meanings. A number of semantic similarity mea-
sures have been proposed based on this hypothesis
(Hindle, 1990; Grefenstette, 1994; Dagan et al,
1994; Dagan et al, 1995; Lin, 1998; Dagan et al,
1999).
?The work was done while the author was at NICT.
In general, most semantic similarity measures
have the following form:
sim(w1, w2) = g(v(w1), v(w2)), (1)
where v(wi) is a vector that represents the con-
texts in which wi appears, which we call a context
profile of wi. The function g is a function on these
context profiles that is expected to produce good
similarities. Each dimension of the vector corre-
sponds to a context, fk, which is typically a neigh-
boring word or a word having dependency rela-
tions with wi in a corpus. Its value, vk(wi), is typ-
ically a co-occurrence frequency c(wi, fk), a con-
ditional probability p(fk|wi), or point-wise mu-
tual information (PMI) between wi and fk, which
are all calculated from a corpus. For g, various
works have used the cosine, the Jaccard coeffi-
cient, or the Jensen-Shannon divergence is uti-
lized, to name only a few measures.
Previous studies have focused on how to de-
vise good contexts and a good function g for se-
mantic similarities. On the other hand, our ap-
proach in this paper is to estimate context profiles
(v(wi)) robustly and thus to estimate the similarity
robustly. The problem here is that v(wi) is com-
puted from a corpus of limited size, and thus in-
evitably contains uncertainty and sparseness. The
guiding intuition behind our method is as follows.
All other things being equal, the similarity with
a more frequent word should be larger, since it
would be more reliable. For example, if p(fk|w1)
and p(fk|w2) for two given words w1 and w2 are
equal, but w1 is more frequent, we would expect
that sim(w0, w1) > sim(w0, w2).
In the NLP field, data sparseness has been rec-
ognized as a serious problem and tackled in the
context of language modeling and supervised ma-
chine learning. However, to our knowledge, there
247
has been no study that seriously dealt with data
sparseness in the context of semantic similarity
calculation. The data sparseness problem is usu-
ally solved by smoothing, regularization, margin
maximization and so on (Chen and Goodman,
1998; Chen and Rosenfeld, 2000; Cortes and Vap-
nik, 1995). Recently, the Bayesian approach has
emerged and achieved promising results with a
clearer formulation (Teh, 2006; Mochihashi et al,
2009).
In this paper, we apply the Bayesian framework
to the calculation of distributional similarity. The
method is straightforward: Instead of using the
point estimation of v(wi), we first estimate the
distribution of the context profile, p(v(wi)), by
Bayesian estimation and then take the expectation
of the original similarity under this distribution as
follows:
simb(w1, w2) (2)
= E[sim(w1, w2)]{p(v(w1)),p(v(w2))}
= E[g(v(w1), v(w2))]{p(v(w1)),p(v(w2))}.
The uncertainty due to data sparseness is repre-
sented by p(v(wi)), and taking the expectation en-
ables us to take this into account. The Bayesian
estimation usually gives diverging distributions for
infrequent observations and thus decreases the ex-
pectation value as expected.
The Bayesian estimation and the expectation
calculation in Eq. 2 are generally difficult and
usually require computationally expensive proce-
dures. Since our motivation for this research is to
calculate good semantic similarities for a large set
of words (e.g., one million nouns) and apply them
to a wide range of NLP tasks, such costs must be
minimized.
Our technical contribution in this paper is to
show that in the case where the context profiles are
multinomial distributions, the priors are Dirich-
let, and the base similarity measure is the Bhat-
tacharyya coefficient (Bhattacharyya, 1943), we
can derive an analytical form for Eq. 2, that en-
ables efficient calculation (with some implemen-
tation tricks).
In experiments, we estimate semantic similari-
ties using a large amount of Web data in Japanese
and show that the proposed measure gives bet-
ter word similarities than a non-Bayesian Bhat-
tacharyya coefficient or other well-known similar-
ity measures such as Jensen-Shannon divergence
and the cosine with PMI weights.
The rest of the paper is organized as follows. In
Section 2, we briefly introduce the Bayesian esti-
mation and the Bhattacharyya coefficient. Section
3 proposes our new Bayesian Bhattacharyya coef-
ficient for robust similarity calculation. Section 4
mentions some implementation issues and the so-
lutions. Then, Section 5 reports the experimental
results.
2 Background
2.1 Bayesian estimation with Dirichlet prior
Assume that we estimate a probabilistic model for
the observed data D, p(D|?), which is parame-
terized with parameters ?. In the maximum like-
lihood estimation (MLE), we find the point esti-
mation ?? = argmax?p(D|?). For example, we
estimate p(fk|wi) as follows with MLE:
p(fk|wi) = c(wi, fk)/
X
k
c(wi, fk). (3)
On the other hand, the objective of the Bayesian
estimation is to find the distribution of ? given
the observed data D, i.e., p(?|D), and use it in
later processes. Using Bayes? rule, this can also
be viewed as:
p(?|D) = p(D|?)pprior(?)p(D) . (4)
pprior(?) is a prior distribution that represents the
plausibility of each ? based on the prior knowl-
edge. In this paper, we consider the case where
? is a multinomial distribution, i.e.,
?
k ?k = 1,
that models the process of choosing one out of K
choices. Estimating a conditional probability dis-
tribution ?k = p(fk|wi) as a context profile for
each wi falls into this case. In this paper, we also
assume that the prior is the Dirichlet distribution,
Dir(?). The Dirichlet distribution is defined as
follows.
Dir(?|?) =
?(
PK
k=1 ?k)
QK
k=1 ?(?k)
K
Y
k=1
??k?1k . (5)
?(.) is the Gamma function. The Dirichlet distri-
bution is parametrized by hyperparameters ?k(>
0).
It is known that p(?|D) is also a Dirichlet dis-
tribution for this simplest case, and it can be ana-
lytically calculated as follows.
p(?|D) = Dir(?|{?k + c(k)}), (6)
where c(k) is the frequency of choice k in data D.
For example, c(k) = c(wi, fk) in the estimation
of p(fk|wi). This is very simple: we just need to
add the observed counts to the hyperparameters.
248
2.2 Bhattacharyya coefficient
When the context profiles are probability distribu-
tions, we usually utilize the measures on probabil-
ity distributions such as the Jensen-Shannon (JS)
divergence to calculate similarities (Dagan et al,
1994; Dagan et al, 1997). The JS divergence is
defined as follows.
JS(p1||p2) =
1
2(KL(p1||pavg) + KL(p2||pavg)),
where pavg = p1+p22 is a point-wise average of p1
and p2 and KL(.) is the Kullback-Leibler diver-
gence. Although we found that the JS divergence
is a good measure, it is difficult to derive an ef-
ficient calculation of Eq. 2, even in the Dirichlet
prior case.1
In this study, we employ the Bhattacharyya co-
efficient (Bhattacharyya, 1943) (BC for short),
which is defined as follows:
BC(p1, p2) =
K
X
k=1
?
p1k ? p2k.
The BC is also a similarity measure on probabil-
ity distributions and is suitable for our purposes as
we describe in the next section. Although BC has
not been explored well in the literature on distribu-
tional word similarities, it is also a good similarity
measure as the experiments show.
3 Method
In this section, we show that if our base similarity
measure is BC and the distributions under which
we take the expectation are Dirichlet distributions,
then Eq. 2 also has an analytical form, allowing
efficient calculation.
Here, we calculate the following value given
two Dirichlet distributions:
BCb(p1, p2) = E[BC(p1, p2)]{Dir(p1|?? ),Dir(p2|?? )}
=
ZZ
???
Dir(p1|?
?
)Dir(p2|?
?
)BC(p1, p2)dp1dp2.
After several derivation steps (see Appendix A),
we obtain the following analytical solution for the
above:
1A naive but general way might be to draw samples of
v(wi) from p(v(wi)) and approximate the expectation using
these samples. However, such a method will be slow.
= ?(?
?
0)?(?
?
0)
?(??0 + 12 )?(?
?
0 + 12 )
K
X
k=1
?(?
?
k + 12 )?(?
?
k + 12 )
?(??k)?(?
?
k)
, (7)
where ??0 =
?
k ?
?
k and ?
?
0 =
?
k ?
?
k. Note that
with the Dirichlet prior, ??k = ?k + c(w1, fk) and
??k = ?k + c(w2, fk), where ?k and ?k are the
hyperparameters of the priors of w1 and w2, re-
spectively.
To put it all together, we can obtain a new
Bayesian similarity measure on words, which can
be calculated only from the hyperparameters for
the Dirichlet prior, ? and ?, and the observed
counts c(wi, fk). It is written as follows.
BCb(w1, w2) = (8)
?(?0 + a0)?(?0 + b0)
?(?0 + a0 + 12 )?(?0 + b0 +
1
2 )
?
K
X
k=1
?(?k + c(w1, fk) + 12 )?(?k + c(w2, fk) +
1
2 )
?(?k + c(w1, fk))?(?k + c(w2, fk))
,
where a0 =
?
k c(w1, fk) and b0 =
?
k c(w2, fk). We call this new measure the
Bayesian Bhattacharyya coefficient (BCb for
short). For simplicity, we assume ?k = ?k = ? in
this paper.
We can see that BCb actually encodes our guid-
ing intuition. Consider four words, w0, w1, w2,
and w4, for which we have c(w0, f1) = 10,
c(w1, f1) = 2, c(w2, f1) = 10, and c(w3, f1) =
20. They have counts only for the first dimen-
sion, i.e., they have the same context profile:
p(f1|wi) = 1.0, when we employ MLE. When
K = 10, 000 and ?k = 1.0, the Bayesian similar-
ity between these words is calculated as
BCb(w0, w1) = 0.785368
BCb(w0, w2) = 0.785421
BCb(w0, w3) = 0.785463
We can see that similarities are different ac-
cording to the number of observations, as ex-
pected. Note that the non-Bayesian BC will re-
turn the same value, 1.0, for all cases. Note
also that BCb(w0, w0) = 0.78542 if we use Eq.
8, meaning that the self-similarity might not be
the maximum. This is conceptually strange, al-
though not a serious problem since we hardly use
sim(wi, wi) in practice. If we want to fix this,
we can use the special definition: BCb(wi, wi) ?
1. This is equivalent to using simb(wi, wi) =
E[sim(wi, wi)]{p(v(wi))} = 1 only for this case.
249
4 Implementation Issues
Although we have derived the analytical form
(Eq. 8), there are several problems in implement-
ing robust and efficient calculations.
First, the Gamma function in Eq. 8 overflows
when the argument is larger than 170. In such
cases, a commonly used way is to work in the log-
arithmic space. In this study, we utilize the ?log
Gamma? function: ln?(x), which returns the log-
arithm of the Gamma function directly without the
overflow problem.2
Second, the calculation of the log Gamma func-
tion is heavier than operations such as simple mul-
tiplication, which is used in existing measures.
In fact, the log Gamma function is implemented
using an iterative algorithm such as the Lanczos
method. In addition, according to Eq. 8, it seems
that we have to sum up the values for all k, be-
cause even if c(wi, fk) is zero the value inside the
summation will not be zero. In the existing mea-
sures, it is often the case that we only need to sum
up for k where c(wi, fk) > 0. Because c(wi, fk)
is usually sparse, that technique speeds up the cal-
culation of the existing measures drastically and
makes it practical.
In this study, the above problem is solved by
pre-computing the required log Gamma values, as-
suming that we calculate similarities for a large
set of words, and pre-computing default values for
cases where c(wi, fk) = 0. The following values
are pre-computed once at the start-up time.
For each word:
(A) ln?(?0 + a0) ? ln?(?0 + a0 + 12)
(B) ln?(?k+c(wi, fk))?ln?(?k+c(wi, fk)+ 12)
for all k where c(wi, fk) > 0
(C) ? exp(2(ln?(?k + 12) ? ln?(?k)))) +
exp(ln?(?k + c(wi, fk)) ? ln?(?k +
c(wi, fk) + 12) + ln?(?k +
1
2) ? ln?(?k))
for all k where c(wi, fk) > 0;
For each k:
(D): exp(2(ln?(?k + 12)).
In the calculation of BCb(w1, w2), we first as-
sume that all c(wi, fk) = 0 and set the output
variable to the default value. Then, we iterate
over the sparse vectors c(w1, fk) and c(w2, fk). If
2We used the GNU Scientific Library (GSL)
(www.gnu.org/software/gsl/), which implements this
function.
c(w1, fk) > 0 and c(w2, fk) = 0 (and vice versa),
we update the output variable just by adding (C).
If c(w1, fk) > 0 and c(w2, fk) > 0, we update
the output value using (B), (D) and one additional
exp(.) operation. With this implementation, we
can make the computation of BCb practically as
fast as using other measures.
5 Experiments
5.1 Evaluation setting
We evaluated our method in the calculation of sim-
ilarities between nouns in Japanese.
Because human evaluation of word similari-
ties is very difficult and costly, we conducted au-
tomatic evaluation in the set expansion setting,
following previous studies such as Pantel et al
(2009).
Given a word set, which is expected to con-
tain similar words, we assume that a good simi-
larity measure should output, for each word in the
set, the other words in the set as similar words.
For given word sets, we can construct input-and-
answers pairs, where the answers for each word
are the other words in the set the word appears in.
We output a ranked list of 500 similar words
for each word using a given similarity measure
and checked whether they are included in the an-
swers. This setting could be seen as document re-
trieval, and we can use an evaluation measure such
as the mean of the precision at top T (MP@T ) or
the mean average precision (MAP). For each input
word, P@T (precision at top T ) and AP (average
precision) are defined as follows.
P@T = 1T
T
X
i=1
?(wi ? ans),
AP = 1R
N
X
i=1
?(wi ? ans)P@i.
?(wi ? ans) returns 1 if the output word wi is
in the answers, and 0 otherwise. N is the number
of outputs and R is the number of the answers.
MP@T and MAP are the averages of these values
over all input words.
5.2 Collecting context profiles
Dependency relations are used as context profiles
as in Kazama and Torisawa (2008) and Kazama et
al. (2009). From a large corpus of Japanese Web
documents (Shinzato et al, 2008) (100 million
250
documents), where each sentence has a depen-
dency parse, we extracted noun-verb and noun-
noun dependencies with relation types and then
calculated their frequencies in the corpus. If a
noun, n, depends on a word, w, with a relation,
r, we collect a dependency pair, (n, ?w, r?). That
is, a context fk, is ?w, r? here.
For noun-verb dependencies, postpositions
in Japanese represent relation types. For
example, we extract a dependency relation
(???, ? ??,? ?) from the sentence below,
where a postposition ?? (wo)? is used to mark
the verb object.
??? (wine)? (wo)?? (buy) (? buy a wine)
Note that we leave various auxiliary verb suf-
fixes, such as ??? (reru),? which is for passiviza-
tion, as a part of w, since these greatly change the
type of n in the dependent position.
As for noun-noun dependencies, we considered
expressions of type ?n1 ? n2? (? ?n2 of n1?) as
dependencies (n1, ?n2,? ?).
We extracted about 470 million unique depen-
dencies from the corpus, containing 31 million
unique nouns (including compound nouns as de-
termined by our filters) and 22 million unique con-
texts, fk. We sorted the nouns according to the
number of unique co-occurring contexts and the
contexts according to the number of unique co-
occurring nouns, and then we selected the top one
million nouns and 100,000 contexts. We used only
260 million dependency pairs that contained both
the selected nouns and the selected contexts.
5.3 Test sets
We prepared three test sets as follows.
Set ?A? and ?B?: Thesaurus siblings We
considered that words having a common
hypernym (i.e., siblings) in a manually
constructed thesaurus could constitute a
similar word set. We extracted such sets
from a Japanese dictionary, EDR (V3.0)
(CRL, 2002), which contains concept hier-
archies and the mapping between words and
concepts. The dictionary contains 304,884
nouns. In all, 6,703 noun sibling sets were
extracted with the average set size of 45.96.
We randomly chose 200 sets each for sets
?A? and ?B.? Set ?A? is a development set to
tune the value of the hyperparameters and
?B? is for the validation of the parameter
tuning.
Set ?C?: Closed sets Murata et al (2004) con-
structed a dataset that contains several closed
word sets such as the names of countries,
rivers, sumo wrestlers, etc. We used all of
the 45 sets that are marked as ?complete? in
the data, containing 12,827 unique words in
total.
Note that we do not deal with ambiguities in the
construction of these sets as well as in the calcu-
lation of similarities. That is, a word can be con-
tained in several sets, and the answers for such a
word is the union of the words in the sets it belongs
to (excluding the word itself).
In addition, note that the words in these test sets
are different from those of our one-million-word
vocabulary. We filtered out the words that are not
included in our vocabulary and removed the sets
with size less than 2 after the filtering.
Set ?A? contained 3,740 words that are actually
evaluated, with about 115 answers on average, and
?B? contained 3,657 words with about 65 answers
on average. Set ?C? contained 8,853 words with
about 1,700 answers on average.
5.4 Compared similarity measures
We compared our Bayesian Bhattacharyya simi-
larity measure, BCb, with the following similarity
measures.
JS Jensen-Shannon divergence between p(fk|w1)
and p(fk|w2) (Dagan et al, 1994; Dagan et
al., 1999).
PMI-cos The cosine of the context profile vec-
tors, where the k-th dimension is the point-
wise mutual information (PMI) between
wi and fk defined as: PMI(wi, fk) =
log p(wi,fk)p(wi)p(fk) (Pantel and Lin, 2002; Pantel
et al, 2009).3
Cls-JS Kazama et al (2009) proposed using
the Jensen-Shannon divergence between hid-
den class distributions, p(c|w1) and p(c|w2),
which are obtained by using an EM-based
clustering of dependency relations with a
model p(wi, fk) =
?
c p(wi|c)p(fk|c)p(c)
(Kazama and Torisawa, 2008). In order to
3We did not use the discounting of the PMI values de-
scribed in Pantel and Lin (2002).
251
alleviate the effect of local minima of the EM
clustering, they proposed averaging the simi-
larities by several different clustering results,
which can be obtained by using different ini-
tial parameters. In this study, we combined
two clustering results (denoted as ?s1+s2? in
the results), each of which (?s1? and ?s2?)
has 2,000 hidden classes.4 We included this
method since clustering can be regarded as
another way of treating data sparseness.
BC The Bhattacharyya coefficient (Bhat-
tacharyya, 1943) between p(fk|w1) and
p(fk|w2). This is the baseline for BCb.
BCa The Bhattacharyya coefficient with absolute
discounting. In calculating p(fk|wi), we sub-
tract the discounting value, ?, from c(wi, fk)
and equally distribute the residual probabil-
ity mass to the contexts whose frequency is
zero. This is included as an example of naive
smoothing methods.
Since it is very costly to calculate the sim-
ilarities with all of the other words (one mil-
lion in our case), we used the following approx-
imation method that exploits the sparseness of
c(wi, fk). Similar methods were used in Pantel
and Lin (2002), Kazama et al (2009), and Pan-
tel et al (2009) as well. For a given word, wi,
we sort the contexts in descending order accord-
ing to c(wi, fk) and retrieve the top-L contexts.5
For each selected context, we sort the words in de-
scending order according to c(wi, fk) and retrieve
the top-M words (L = M = 1600).6 We merge
all of the words above as candidate words and cal-
culate the similarity only for the candidate words.
Finally, the top 500 similar words are output.
Note also that we used modified counts,
log(c(wi, fk)) + 1, instead of raw counts,
c(wi, fk), with the intention of alleviating the ef-
fect of strangely frequent dependencies, which can
be found in the Web data. In preliminary ex-
periments, we observed that this modification im-
proves the quality of the top 500 similar words as
reported in Terada et al (2004) and Kazama et al
(2009).
4In the case of EM clustering, the number of unique con-
texts, fk, was also set to one million instead of 100,000, fol-
lowing Kazama et al (2009).
5It is possible that the number of contexts with non-zero
counts is less than L. In that case, all of the contexts with
non-zero counts are used.
6Sorting is performed only once in the initialization step.
Table 1: Performance on siblings (Set A).
Measure MAP MP
@1 @5 @10 @20
JS 0.0299 0.197 0.122 0.0990 0.0792
PMI-cos 0.0332 0.195 0.124 0.0993 0.0798
Cls-JS (s1) 0.0319 0.195 0.122 0.0988 0.0796
Cls-JS (s2) 0.0295 0.198 0.122 0.0981 0.0786
Cls-JS (s1+s2) 0.0333 0.206 0.129 0.103 0.0841
BC 0.0334 0.211 0.131 0.106 0.0854
BCb (0.0002) 0.0345 0.223 0.138 0.109 0.0873
BCb (0.0016) 0.0356 0.242 0.148 0.119 0.0955
BCb (0.0032) 0.0325 0.223 0.137 0.111 0.0895
BCa (0.0016) 0.0337 0.212 0.133 0.107 0.0863
BCa (0.0362) 0.0345 0.221 0.136 0.110 0.0890
BCa (0.1) 0.0324 0.214 0.128 0.101 0.0825
without log(c(wi, fk)) + 1 modification
JS 0.0294 0.197 0.116 0.0912 0.0712
PMI-cos 0.0342 0.197 0.125 0.0987 0.0793
BC 0.0296 0.201 0.118 0.0915 0.0721
As for BCb, we assumed that all of the hyper-
parameters had the same value, i.e., ?k = ?. It
is apparent that an excessively large ? is not ap-
propriate because it means ignoring observations.
Therefore, ?must be tuned. The discounting value
of BCa is also tuned.
5.5 Results
Table 1 shows the results for Set A. The MAP and
the MPs at the top 1, 5, 10, and 20 are shown for
each similarity measure. As for BCb and BCa, the
results for the tuned and several other values for ?
are shown. Figure 1 shows the parameter tuning
for BCb with MAP as the y-axis (results for BCa
are shown as well). Figure 2 shows the same re-
sults with MPs as the y-axis. The MAP and MPs
showed a correlation here. From these results, we
can see that BCb surely improves upon BC, with
6.6% improvement in MAP and 14.7% improve-
ment in MP@1 when ? = 0.0016. BCb achieved
the best performance among the compared mea-
sures with this setting. The absolute discounting,
BCa, improved upon BC as well, but the improve-
ment was smaller than with BCb. Table 1 also
shows the results for the case where we did not
use the log-modified counts. We can see that this
modification gives improvements (though slight or
unclear for PMI-cos).
Because tuning hyperparameters involves the
possibility of overfitting, its robustness should be
assessed. We checked whether the tuned ? with
Set A works well for Set B. The results are shown
in Table 2. We can see that the best ? (= 0.0016)
found for Set A works well for Set B as well. That
is, the tuning of ? as above is not unrealistic in
252
 0.02
 0.022
 0.024
 0.026
 0.028
 0.03
 0.032
 0.034
 0.036
 1e-06  1e-05  0.0001  0.001  0.01  0.1  1
MA
P
? (log-scale)
BayesAbsolute Discounting
Figure 1: Tuning of ? (MAP). The dashed hori-
zontal line indicates the score of BC.
 0.04 0.06
 0.08 0.1
 0.12 0.14
 0.16 0.18
 0.2 0.22
 0.24 0.26
 1e-06  1e-05  0.0001  0.001  0.01
MP
? (log-scale)
 MP@1
 MP@5
 MP@10
 MP@20
 MP@30
 MP@40
Figure 2: Tuning of ? (MP).
practice because it seems that we can tune it ro-
bustly using a small subset of the vocabulary as
shown by this experiment.
Next, we evaluated the measures on Set C, i.e.,
the closed set data. The results are shown in Ta-
ble 3. For this set, we observed a tendency that
is different from Sets A and B. Cls-JS showed a
particularly good performance. BCb surely im-
proves upon BC. For example, the improvement
was 7.5% for MP@1. However, the improvement
in MAP was slight, and MAP did not correlate
well with MPs, unlike in the case of Sets A and
B.
We thought one possible reason is that the num-
ber of outputs, 500, for each word was not large
enough to assess MAP values correctly because
the average number of answers is 1,700 for this
dataset. In fact, we could output more than 500
words if we ignored the cost of storage. Therefore,
we also included the results for the case where
L = M = 3600 and N = 2, 000. Even with
this setting, however, MAP did not correlate well
with MPs.
Although Cls-JS showed very good perfor-
mance for Set C, note that the EM clustering
is very time-consuming (Kazama and Torisawa,
2008), and it took about one week with 24 CPU
cores to get one clustering result in our computing
environment. On the other hand, the preparation
Table 2: Performance on siblings (Set B).
Measure MAP MP
@1 @5 @10 @20
JS 0.0265 0.208 0.116 0.0855 0.0627
PMI-cos 0.0283 0.203 0.116 0.0871 0.0660
Cls-JS (s1+s2) 0.0274 0.194 0.115 0.0859 0.0643
BC 0.0295 0.223 0.124 0.0922 0.0693
BCb (0.0002) 0.0301 0.225 0.128 0.0958 0.0718
BCb (0.0016) 0.0313 0.246 0.135 0.103 0.0758
BCb (0.0032) 0.0279 0.228 0.127 0.0938 0.0698
BCa (0.0016) 0.0297 0.223 0.125 0.0934 0.0700
BCa (0.0362) 0.0298 0.223 0.125 0.0934 0.0705
BCa (0.01) 0.0300 0.224 0.126 0.0949 0.0710
Table 3: Performance on closed-sets (Set C).
Measure MAP MP
@1 @5 @10 @20
JS 0.127 0.607 0.582 0.566 0.544
PMI-cos 0.124 0.531 0.519 0.508 0.493
Cls-JS (s1) 0.125 0.589 0.566 0.548 0.525
Cls-JS (s2) 0.137 0.608 0.592 0.576 0.554
Cls-JS (s1+s2) 0.152 0.638 0.617 0.603 0.583
BC 0.131 0.602 0.579 0.565 0.545
BCb (0.0004) 0.133 0.636 0.605 0.587 0.563
BCb (0.0008) 0.131 0.647 0.615 0.594 0.568
BCb (0.0016) 0.126 0.644 0.615 0.593 0.564
BCb (0.0032) 0.107 0.573 0.556 0.529 0.496
L = M = 3200 and N = 2000
JS 0.165 0.605 0.580 0.564 0.543
PMI-cos 0.165 0.530 0.517 0.507 0.492
Cls-JS (s1+s2) 0.209 0.639 0.618 0.603 0.584
BC 0.168 0.600 0.577 0.562 0.542
BCb (0.0004) 0.170 0.635 0.604 0.586 0.562
BCb (0.0008) 0.168 0.647 0.615 0.594 0.568
BCb (0.0016) 0.161 0.644 0.615 0.593 0.564
BCb (0.0032) 0.140 0.573 0.556 0.529 0.496
for our method requires just an hour with a single
core.
6 Discussion
We should note that the improvement by using our
method is just ?on average,? as in many other NLP
tasks, and observing clear qualitative change is rel-
atively difficult, for example, by just showing ex-
amples of similar word lists here. Comparing the
results of BCb and BC, Table 4 lists the numbers
of improved, unchanged, and degraded words in
terms of MP@20 for each evaluation set. As can
be seen, there are a number of degraded words, al-
though they are fewer than the improved words.
Next, Figure 3 shows the averaged differences of
MP@20 in each 40,000 word-ID range.7 We can
observe that the advantage of BCb is lessened es-
7Word IDs are assigned in ascending order when we chose
the top one million words as described in Section 5.2, and
they roughly correlate with frequencies. So, frequent words
tend to have low-IDs.
253
Table 4: The numbers of improved, unchanged,
and degraded words in terms of MP@20 for each
evaluation set.
# improved # unchanged # degraded
Set A 755 2,585 400
Set B 643 2,610 404
Set C 3,153 3,962 1,738
-0.01 0
 0.01 0.02
 0.03 0.04
 0.05 0.06
 0  500000  1e+06Avg
. Diff. 
of MP
@20
ID range
-0.01 0
 0.01 0.02
 0.03 0.04
 0.05 0.06
 0  500000  1e+06Avg
. Diff. 
of MP
@20
ID range
-0.01 0 0.01
 0.02 0.03 0.04
 0.05 0.06 0.07
 0.08
 0  500000  1e+06Avg
. Diff. 
of MP
@20
ID range
Figure 3: Averaged Differences of MP@20 be-
tween BCb (0.0016) and BC within each 40,000
ID range (Left: Set A. Right: Set B. Bottom: Set
C).
pecially for low-ID words (as expected) with on-
average degradation.8 The improvement is ?on av-
erage? in this sense as well.
One might suspect that the answer words tended
to be low-ID words, and the proposed method is
simply biased towards low-ID words because of
its nature. Then, the observed improvement is a
trivial consequence. Table 5 lists some interest-
ing statistics about the IDs. We can see that BCb
surely outputs more low-ID words than BC, and
BC more than Cls-JS and JS.9 However, the av-
erage ID of the outputs of BC is already lower
than the average ID of the answer words. There-
fore, even if BCb preferred lower-ID words than
BC, it should not have the effect of improving
the accuracy. That is, the improvement by BCb
is not superficial. From BC/BCb, we can also see
that the IDs of the correct outputs did not become
smaller compared to the IDs of the system outputs.
Clearly, we need more analysis on what caused
the improvement by the proposed method and how
that affects the efficacy in real applications of sim-
ilarity measures.
The proposed Bayesian similarity measure out-
performed the baseline Bhattacharyya coefficient
8This suggests the use of different ?s depending on ID
ranges (e.g., smaller ? for low-ID words) in practice.
9The outputs of Cls-JS are well-balanced in the ID space.
Table 5: Statistics on IDs. (A): Avg. ID of an-
swers. (B): Avg. ID of system outputs. (C): Avg.
ID of correct system outputs.
Set A Set C
(A) 238,483 255,248
(B) (C) (B) (C)
Cls-JS (s1+s2) 282,098 176,706 273,768 232,796
JS 183,054 11,3442 211,671 201,214
BC 162,758 98,433 193,508 189,345
BCb(0.0016) 55,915 54,786 90,472 127,877
BC/BCb 2.91 1.80 2.14 1.48
and other well-known similarity measures. As
a smoothing method, it also outperformed a
naive absolute discounting. Of course, we can-
not say that the proposed method is better than
any other sophisticated smoothing method at this
point. However, as noted above, there has
been no serious attempt to assess the effect of
smoothing in the context of word similarity cal-
culation. Recent studies have pointed out that
the Bayesian framework derives state-of-the-art
smoothing methods such as Kneser-Ney smooth-
ing as a special case (Teh, 2006; Mochihashi et
al., 2009). Consequently, it is reasonable to re-
sort to the Bayesian framework. Conceptually,
our method is equivalent to modifying p(fk|wi)
as p(fk|wi) =
{
?(?0+a0)?(?k+c(wi,fk)+ 12 )
?(?0+a0+ 12 )?(?k+c(wi,fk))
}2
and
taking the Bhattacharyya coefficient. However,
the implication of this form has not yet been in-
vestigated, and so we leave it for future research.
Our method is the simplest one as a Bayesian
method. We did not employ any numerical opti-
mization or sampling iterations, as in a more com-
plete use of the Bayesian framework (Teh, 2006;
Mochihashi et al, 2009). Instead, we used the ob-
tained analytical form directly with the assump-
tion that ?k = ? and ? can be tuned directly by
using a simple grid search with a small subset of
the vocabulary as the development set. If substan-
tial additional costs are allowed, we can fine-tune
each ?k using more complete Bayesian methods.
We also leave this for future research.
In terms of calculation procedure, BCb has the
same form as other similarity measures, which is
basically the same as the inner product of sparse
vectors. Thus, it can be as fast as other similar-
ity measures with some effort as we described in
Section 4 when our aim is to calculate similarities
between words in a fixed large vocabulary. For ex-
ample, BCb took about 100 hours to calculate the
254
top 500 similar nouns for all of the one million
nouns (using 16 CPU cores), while JS took about
57 hours. We think this is an acceptable additional
cost.
The limitation of our method is that it can-
not be used efficiently with similarity measures
other than the Bhattacharyya coefficient, although
that choice seems good as shown in the experi-
ments. For example, it seems difficult to use the
Jensen-Shannon divergence as the base similar-
ity because the analytical form cannot be derived.
One way we are considering to give more flexi-
bility to our method is to adjust ?k depending on
external knowledge such as the importance of a
context (e.g., PMIs). In another direction, we will
be able to use a ?weighted? Bhattacharyya coeffi-
cient:
?
k ?(w1, fk)?(w2, fk)
?
p1k ? p2k, where
the weights, ?(wi, fk), do not depend on pik, as
the base similarity measure. The analytical form
for it will be a weighted version of BCb.
BCb can also be generalized to the case where
the base similarity is BCd(p1, p2) =
?K
k=1 pd1k ?
pd2k, where d > 0. The Bayesian analytical form
becomes as follows.
BCdb (w1, w2) =
?(?0 + a0)?(?0 + b0)
?(?0 + a0 + d)?(?0 + b0 + d)
?
K
X
k=1
?(?k + c(w1, fk) + d)?(?k + c(w2, fk) + d)
?(?k + c(w1, fk))?(?k + c(w2, fk))
.
See Appendix A for the derivation. However, we
restricted ourselves to the case of d = 12 in this
study.
Finally, note that our BCb is different from
the Bhattacharyya distance measure on Dirichlet
distributions of the following form described in
Rauber et al (2008) in its motivation and analyti-
cal form:
p
?(??0)?(?
?
0)
q
Q
k ?(?
?
k)
q
Q
k ?(?
?
k)
?
Q
k ?((?
?
k + ?
?
k)/2)
?( 12
PK
k (?
?
k + ?
?
k))
. (9)
Empirical and theoretical comparisons with this
measure also form one of the future directions.10
7 Conclusion
We proposed a Bayesian method for robust distri-
butional word similarities. Our method uses a dis-
tribution of context profiles obtained by Bayesian
10Our preliminary experiments show that calculating sim-
ilarity using Eq. 9 for the Dirichlet distributions obtained by
Eq. 6 does not produce meaningful similarity (i.e., the accu-
racy is very low).
estimation and takes the expectation of a base sim-
ilarity measure under that distribution. We showed
that, in the case where the context profiles are
multinomial distributions, the priors are Dirichlet,
and the base measure is the Bhattacharyya coeffi-
cient, we can derive an analytical form, permitting
efficient calculation. Experimental results show
that the proposed measure gives better word simi-
larities than a non-Bayesian Bhattacharyya coeffi-
cient, other well-known similarity measures such
as Jensen-Shannon divergence and the cosine with
PMI weights, and the Bhattacharyya coefficient
with absolute discounting.
Appendix A
Here, we give the analytical form for the general-
ized case (BCdb ) in Section 6. Recall the following
relation, which is used to derive the normalization
factor of the Dirichlet distribution:
Z
?
Y
k
??
?
k?1
k d? =
Q
k ?(?
?
k)
?(??0)
= Z(?
?
)?1. (10)
Then, BCdb (w1, w2)
=
ZZ
???
Dir(?1|?
?
)Dir(?2|?
?
)
X
k
?d1k?d2k d?1 d?2
= Z(?
?
)Z(?
?
)?
ZZ
???
Y
l
??
?
l?1
1l
Y
m
??
?
m?1
2m
X
k
?d1k?d2k d?1 d?2
| {z }
A
.
Using Eq. 10, A in the above can be calculated as
follows:
=
Z
?
Y
m
??
?
m?1
2m
2
4
X
k
?d2k
Z
?
??
?
k+d?1
1k
Y
l?=k
??
?
l?1
1l d?1
3
5 d?2
=
Z
?
Y
m
??
?
m?1
2m
"
X
k
?d2k
?(?
?
k + d)
Q
l?=k ?(?
?
l)
?(??0 + d)
#
d?2
=
X
k
?(?
?
k + d)
Q
l?=k ?(?
?
l)
?(??0 + d)
Z
?
??
?
k+d?1
2k
Y
m ?=k
??
?
m?1
2m d?2
=
X
k
?(?
?
k + d)
Q
l?=k ?(?
?
l)
?(??0 + d)
?(?
?
k + d)
Q
m?=k ?(?
?
m)
?(??0 + d)
=
Q
?(?
?
l)
Q
?(?
?
m)
?(??0 + d)?(?
?
0 + d)
X
k
?(?
?
k + d)
?(??k)
?(?
?
k + d)
?(??k)
.
This will give:
BCdb (w1, w2) =
?(?
?
0)?(?
?
0)
?(??0 + d)?(?
?
0 + d)
K
X
k=1
?(?
?
k + d)?(?
?
k + d)
?(??k)?(?
?
k)
.
255
References
A. Bhattacharyya. 1943. On a measure of divergence
between two statistical populations defined by their
probability distributions. Bull. Calcutta Math. Soc.,
49:214?224.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. TR-10-98, Computer Science Group,
Harvard University.
Stanley F. Chen and Ronald Rosenfeld. 2000. A
survey of smoothing techniques for ME models.
IEEE Transactions on Speech and Audio Process-
ing, 8(1):37?50.
Corinna Cortes and Vladimir Vapnik. 1995. Support
vector networks. Machine Learning, 20:273?297.
CRL. 2002. EDR electronic dictionary version 2.0
technical guide. Communications Research Labo-
ratory (CRL).
Ido Dagan, Fernando Pereira, and Lillian Lee. 1994.
Similarity-based estimation of word cooccurrence
probabilities. In Proceedings of ACL 94.
Ido Dagan, Shaul Marcus, and Shaul Markovitch.
1995. Contextual word similarity and estimation
from sparse data. Computer, Speech and Language,
9:123?152.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1997.
Similarity-based methods for word sense disam-
biguation. In Proceedings of ACL 97.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based models of word cooccurrence
probabilities. Machine Learning, 34(1-3):43?69.
Gregory Grefenstette. 1994. Explorations In Auto-
matic Thesaurus Discovery. Kluwer Academic Pub-
lishers.
Zellig Harris. 1954. Distributional structure. Word,
pages 146?142.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In Proceedings of
ACL-90, pages 268?275.
Jun?ichi Kazama and Kentaro Torisawa. 2008. In-
ducing gazetteers for named entity recognition by
large-scale clustering of dependency relations. In
Proceedings of ACL-08: HLT.
Jun?ichi Kazama, Stijn De Saeger, Kentaro Torisawa,
and Masaki Murata. 2009. Generating a large-scale
analogy list using a probabilistic clustering based on
noun-verb dependency profiles. In Proceedings of
15th Annual Meeting of The Association for Natural
Language Processing (in Japanese).
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING/ACL-
98, pages 768?774.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested Pitman-Yor language modeling.
In Proceedings of ACL-IJCNLP 2009, pages 100?
108.
Masaki Murata, Qing Ma, Tamotsu Shirado, and Hi-
toshi Isahara. 2004. Database for evaluating ex-
tracted terms and tool for visualizing the terms. In
Proceedings of LREC 2004 Workshop: Computa-
tional and Computer-Assisted Terminology, pages
6?9.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 613?619.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP 2009, pages 938?947.
T. W. Rauber, T. Braun, and K. Berns. 2008. Proba-
bilistic distance measures of the Dirichlet and Beta
distributions. Pattern Recognition, 41:637?645.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
Chikara Hashimoto, and Sadao Kurohashi. 2008.
Tsubaki: An open search engine infrastructure for
developing new information access. In Proceedings
of IJCNLP 2008.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of COLING-ACL 2006, pages 985?992.
Akira Terada, Minoru Yoshida, and Hiroshi Nakagawa.
2004. A tool for constructing a synonym dictionary
using context information. In IPSJ SIG Technical
Report (in Japanese), pages 87?94.
256
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1087?1097,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Extracting Paraphrases from Definition Sentences on the Web
Chikara Hashimoto? Kentaro Torisawa? Stijn De Saeger?
Jun?ichi Kazama? Sadao Kurohashi?
? ? ? ? National Institute of Information and Communications Technology
Kyoto, 619-0237, JAPAN
? ?Graduate School of Informatics, Kyoto University
Kyoto, 606-8501, JAPAN
{? ch,? torisawa, ? stijn,? kazama}@nict.go.jp
?kuro@i.kyoto-u.ac.jp
Abstract
We propose an automatic method of extracting
paraphrases from definition sentences, which
are also automatically acquired from the Web.
We observe that a huge number of concepts
are defined in Web documents, and that the
sentences that define the same concept tend
to convey mostly the same information using
different expressions and thus contain many
paraphrases. We show that a large number
of paraphrases can be automatically extracted
with high precision by regarding the sentences
that define the same concept as parallel cor-
pora. Experimental results indicated that with
our method it was possible to extract about
300,000 paraphrases from 6? 108 Web docu-
ments with a precision rate of about 94%.
1 Introduction
Natural language allows us to express the same in-
formation in many ways, which makes natural lan-
guage processing (NLP) a challenging area. Ac-
cordingly, many researchers have recognized that
automatic paraphrasing is an indispensable compo-
nent of intelligent NLP systems (Iordanskaja et al,
1991; McKeown et al, 2002; Lin and Pantel, 2001;
Ravichandran and Hovy, 2002; Kauchak and Barzi-
lay, 2006; Callison-Burch et al, 2006) and have tried
to acquire a large amount of paraphrase knowledge,
which is a key to achieving robust automatic para-
phrasing, from corpora (Lin and Pantel, 2001; Barzi-
lay and McKeown, 2001; Shinyama et al, 2002;
Barzilay and Lee, 2003).
We propose a method to extract phrasal para-
phrases from pairs of sentences that define the same
concept. The method is based on our observation
that two sentences defining the same concept can
be regarded as a parallel corpus since they largely
convey the same information using different expres-
sions. Such definition sentences abound on the Web.
This suggests that we may be able to extract a large
amount of phrasal paraphrase knowledge from the
definition sentences on the Web.
For instance, the following two sentences, both of
which define the same concept ?osteoporosis?, in-
clude two pairs of phrasal paraphrases, which are
indicated by underlines 1? and 2?, respectively.
(1) a. Osteoporosis is a disease that 1? decreases the
quantity of bone and 2? makes bones fragile.
b. Osteoporosis is a disease that 1? reduces bone
mass and 2? increases the risk of bone fracture.
We define paraphrase as a pair of expressions be-
tween which entailment relations of both directions
hold. (Androutsopoulos and Malakasiotis, 2010).
Our objective is to extract phrasal paraphrases
from pairs of sentences that define the same con-
cept. We propose a supervised method that exploits
various kinds of lexical similarity features and con-
textual features. Sentences defining certain concepts
are acquired automatically on a large scale from the
Web by applying a quite simple supervised method.
Previous methods most relevant to our work
used parallel corpora such as multiple translations
of the same source text (Barzilay and McKeown,
2001) or automatically acquired parallel news texts
(Shinyama et al, 2002; Barzilay and Lee, 2003;
Dolan et al, 2004). The former requires a large
amount of manual labor to translate the same texts
1087
in several ways. The latter would suffer from the
fact that it is not easy to automatically retrieve large
bodies of parallel news text with high accuracy. On
the contrary, recognizing definition sentences for
the same concept is quite an easy task at least for
Japanese, as we will show, and we were able to find
a huge amount of definition sentence pairs from nor-
mal Web texts. In our experiments, about 30 million
definition sentence pairs were extracted from 6?108
Web documents, and the estimated number of para-
phrases recognized in the definition sentences using
our method was about 300,000, for a precision rate
of about 94%. Also, our experimental results show
that our method is superior to well-known compet-
ing methods (Barzilay and McKeown, 2001; Koehn
et al, 2007) for extracting paraphrases from defini-
tion sentence pairs.
Our evaluation is based on bidirectional check-
ing of entailment relations between paraphrases that
considers the context dependence of a paraphrase.
Note that using definition sentences is only the
beginning of our research on paraphrase extraction.
We have a more general hypothesis that sentences
fulfilling the same pragmatic function (e.g. defini-
tion) for the same topic (e.g. osteoporosis) convey
mostly the same information using different expres-
sions. Such functions other than definition may in-
clude the usage of the same Linux command, the
recipe for the same cuisine, or the description of re-
lated work on the same research issue.
Section 2 describes related works. Section 3
presents our proposed method. Section 4 reports on
evaluation results. Section 5 concludes the paper.
2 Related Work
The existing work for paraphrase extraction is cat-
egorized into two groups. The first involves a dis-
tributional similarity approach pioneered by Lin and
Pantel (2001). Basically, this approach assumes that
two expressions that have a large distributional simi-
larity are paraphrases. There are also variants of this
approach that address entailment acquisition (Geffet
and Dagan, 2005; Bhagat et al, 2007; Szpektor and
Dagan, 2008; Hashimoto et al, 2009). These meth-
ods can be applied to a normal monolingual corpus,
and it has been shown that a large number of para-
phrases or entailment rules could be extracted. How-
ever, the precision of these methods has been rela-
tively low. This is due to the fact that the evidence,
i.e., distributional similarity, is just indirect evidence
of paraphrase/entailment. Accordingly, these meth-
ods occasionally mistake antonymous pairs for para-
phrases/entailment pairs, since an expression and its
antonymous counterpart are also likely to have a
large distributional similarity. Another limitation of
these methods is that they can find only paraphrases
consisting of frequently observed expressions since
they must have reliable distributional similarity val-
ues for expressions that constitute paraphrases.
The second category is a parallel corpus approach
(Barzilay and McKeown, 2001; Shinyama et al,
2002; Barzilay and Lee, 2003; Dolan et al, 2004).
Our method belongs to this category. This approach
aligns expressions between two sentences in par-
allel corpora, based on, for example, the overlap
of words/contexts. The aligned expressions are as-
sumed to be paraphrases. In this approach, the ex-
pressions do not need to appear frequently in the
corpora. Furthermore, the approach rarely mistakes
antonymous pairs for paraphrases/entailment pairs.
However, its limitation is the difficulty in preparing
a large amount of parallel corpora, as noted before.
We avoid this by using definition sentences, which
can be easily acquired on a large scale from theWeb,
as parallel corpora.
Murata et al (2004) used definition sentences in
two manually compiled dictionaries, which are con-
siderably fewer in the number of definition sen-
tences than those on the Web. Thus, the coverage of
their method should be quite limited. Furthermore,
the precision of their method is much poorer than
ours as we report in Section 4.
For a more extensive survey on paraphrasing
methods, see Androutsopoulos and Malakasiotis
(2010) and Madnani and Dorr (2010).
3 Proposed method
Our method, targeting the Japanese language, con-
sists of two steps: definition sentence acquisition
and paraphrase extraction. We describe them below.
3.1 Definition sentence acquisition
We acquire sentences that define a concept (defini-
tion sentences) as in Example (2), which defines ??
1088
???? (osteoporosis), from the 6?108 Web pages
(Akamine et al, 2010) and the Japanese Wikipedia.
(2) ??????????????????????
(Osteoporosis is a disease that makes bones fragile.)
Fujii and Ishikawa (2002) developed an unsuper-
vised method to find definition sentences from the
Web using 18 sentential templates and a language
model constructed from an encyclopedia. On the
other hand, we developed a supervised method to
achieve a higher precision.
We use one sentential template and an SVM clas-
sifier. Specifically, we first collect definition sen-
tence candidates by a template ??NP??.*?, where
? is the beginning of sentence and NP is the noun
phrase expressing the concept to be defined followed
by a particle sequence, ??? (comitative) and ???
(topic) (and optionally followed by comma), as ex-
emplified in (2). As a result, we collected 3,027,101
sentences. Although the particle sequence tends
to mark the topic of the definition sentence, it can
also appear in interrogative sentences and normal as-
sertive sentences in which a topic is strongly empha-
sized. To remove such non-definition sentences, we
classify the candidate sentences using an SVM clas-
sifier with a polynominal kernel (d = 2).1 Since
Japanese is a head-final language and we can judge
whether a sentence is interrogative or not from the
last words in the sentence, we included morpheme
N -grams and bag-of-words (with the window size
of N ) at the end of sentences in the feature set. The
features are also useful for confirming that the head
verb is in the present tense, which definition sen-
tences should be. Also, we added the morpheme
N -grams and bag-of-words right after the particle
sequence in the feature set since we observe that
non-definition sentences tend to have interrogative
related words like ??? (what) or ???? ((what) on
earth) right after the particle sequence. We chose 5
as N from our preliminary experiments.
Our training data was constructed from 2,911 sen-
tences randomly sampled from all of the collected
sentences. 61.1% of them were labeled as positive.
In the 10-fold cross validation, the classifier?s ac-
curacy, precision, recall, and F1 were 89.4, 90.7,
1We use SVMlight available at http://svmlight.
joachims.org/.
92.2, and 91.4, respectively. Using the classifier,
we acquired 1,925,052 positive sentences from all
of the collected sentences. After adding definition
sentences from Wikipedia articles, which are typi-
cally the first sentence of the body of each article
(Kazama and Torisawa, 2007), we obtained a total
of 2,141,878 definition sentence candidates, which
covered 867,321 concepts ranging from weapons to
rules of baseball. Then, we coupled two definition
sentences whose defined concepts were the same
and obtained 29,661,812 definition sentence pairs.
Obviously, our method is tailored to Japanese. For
a language-independent method of definition acqui-
sition, see Navigli and Velardi (2010) as an example.
3.2 Paraphrase extraction
Paraphrase extraction proceeds as follows. First,
each sentence in a pair is parsed by the depen-
dency parser KNP2 and dependency tree frag-
ments that constitute linguistically well-formed con-
stituents are extracted. The extracted dependency
tree fragments are called candidate phrases here-
after. We restricted candidate phrases to predicate
phrases that consist of at least one dependency re-
lation, do not contain demonstratives, and in which
all the leaf nodes are nominal and all of the con-
stituents are consecutive in the sentence. KNP indi-
cates whether each candidate phrase is a predicate
based on the POS of the head morpheme. Then,
we check all the pairs of candidate phrases between
two definition sentences to find paraphrase pairs.3
In (1), repeated in (3), candidate phrase pairs to be
checked include ( 1? decreases the quantity of bone,
1? reduces bone mass), ( 1? decreases the quantity
of bone, 2? increases the risk of bone fracture), ( 2?
makes bones fragile, 1? reduces bone mass), and ( 2?
makes bones fragile, 2? increases the risk of bone
fracture).
(3) a. Osteoporosis is a disease that 1? decreases the
quantity of bone and 2? makes bones fragile.
b. Osteoporosis is a disease that 1? reduces bone
mass and 2? increases the risk of bone fracture.
2http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp.html.
3Our method discards candidate phrase pairs in which one
subsumes the other in terms of their character string, or the dif-
ference is only one proper noun like ?toner cartridges that Ap-
ple Inc. made? and ?toner cartridges that Xerox made.? Proper
nouns are recognized by KNP.
1089
f1 The ratio of the number of morphemes shared between two candidate phrases to the number of all of the morphemes in the two phrases.
f2 The ratio of the number of a candidate phrase?s morphemes, for which there is a morpheme with small edit distance (1 in our experiment) in
another candidate phrase, to the number of all of the morphemes in the two phrases. Note that Japanese has many orthographical variations
and edit distance is useful for identifying them.
f3 The ratio of the number of a candidate phrase?s morphemes, for which there is a morpheme with the same pronunciation in another candidate
phrase, to the number of all of the morphemes in the two phrases. Pronunciation is also useful for identifying orthographic variations.
Pronunciation is given by KNP.
f4 The ratio of the number of morphemes of a shorter candidate phrase to that of a longer one.
f5 The identity of the inflected form of the head morpheme between two candidate phrases: 1 if they are identical, 0 otherwise.
f6 The identity of the POS of the head morpheme between two candidate phrases: 1 or 0.
f7 The identity of the inflection (conjugation) of the head morpheme between two candidate phrases: 1 or 0.
f8 The ratio of the number of morphemes that appear in a candidate phrase segment of a definition sentence s1 and in a segment that is NOT a
part of the candidate phrase of another definition sentence s2 to the number of all of the morphemes of s1?s candidate phrase, i.e. how many
extra morphemes are incorporated into s1?s candidate phrase.
f9 The reversed (s1 ? s2) version of f8.
f10 The ratio of the number of parent dependency tree fragments that are shared by two candidate phrases to the number of all of the parent de-
pendency tree fragments of the two phrases. Dependency tree fragments are represented by the pronunciation of their component morphemes.
f11 A variation of f10; tree fragments are represented by the base form of their component morphemes.
f12 A variation of f10; tree fragments are represented by the POS of their component morphemes.
f13 The ratio of the number of unigrams (morphemes) that appear in the child context of both candidate phrases to the number of all of the child
context morphemes of both candidate phrases. Unigrams are represented by the pronunciation of the morpheme.
f14 A variation of f13; unigrams are represented by the base form of the morpheme.
f15 A variation of f14; the numerator is the number of child context unigrams that are adjacent to both candidate phrases.
f16 The ratio of the number of trigrams that appear in the child context of both candidate phrases to the number of all of the child context
morphemes of both candidate phrases. Trigrams are represented by the pronunciation of the morpheme.
f17 Cosine similarity between two definition sentences from which a candidate phrase pair is extracted.
Table 1: Features used by paraphrase classifier.
The paraphrase checking of candidate phrase
pairs is performed by an SVM classifier with a linear
kernel that classifies each pair of candidate phrases
into a paraphrase or a non-paraphrase.4 Candidate
phrase pairs are ranked by their distance from the
SVM?s hyperplane. Features for the classifier are
based on our observation that two candidate phrases
tend to be paraphrases if the candidate phrases them-
selves are sufficiently similar and/or their surround-
ing contexts are sufficiently similar. Table 1 lists the
features used by the classifier.5 Basically, they rep-
resent either the similarity of candidate phrases (f1-
9) or that of their contexts (f10-17). We think that
they have various degrees of discriminative power,
and thus we use the SVM to adjust their weights.
Figure 1 illustrates features f8-12, for which you
may need supplemental remarks. English is used for
ease of explanation. In the figure, f8 has a positive
value since the candidate phrase of s1 contains mor-
phemes ?of bone?, which do not appear in the can-
4We use SVMperf available at http://svmlight.
joachims.org/svm perf.html.
5In the table, the parent context of a candidate phrase con-
sists of expressions that appear in ancestor nodes of the candi-
date phrase in terms of the dependency structure of the sentence.
Child contexts are defined similarly.
Figure 1: Illustration of features f8-12.
didate phrase of s2 but do appear in the other part
of s2, i.e. they are extra morphemes for s1?s candi-
date phrase. On the other hand, f9 is zero since there
is no such extra morpheme in s2?s candidate phrase.
Also, features f10-12 have positive values since the
two candidate phrases share two parent dependency
tree fragments, (that increases) and (of fracture).
We have also tried the following features, which
we do not detail due to space limitation: the sim-
ilarity of candidate phrases based on semantically
similar nouns (Kazama and Torisawa, 2008), entail-
ing/entailed verbs (Hashimoto et al, 2009), and the
identity of the pronunciation and base form of the
head morpheme; N -grams (N=1,2,3) of child and
parent contexts represented by either the inflected
form, base form, pronunciation, or POS of mor-
1090
Original definition sentence pair (s1, s2) Paraphrased definition sentence pair (s?1, s?2)
s1: Osteoporosis is a disease that reduces bonemass and makes bones
fragile.
s?1: Osteoporosis is a disease that decreases the quantity of bone and
makes bones fragile.
s2: Osteoporosis is a disease that decreases the quantity of bone and
increases the risk of bone fracture.
s?2: Osteoporosis is a disease that reduces bone mass and increases
the risk of bone fracture.
Figure 2: Bidirectional checking of entailment relation (?) of p1 ? p2 and p2 ? p1. p1 is ?reduces bone mass?
in s1 and p2 is ?decreases the quantity of bone? in s2. p1 and p2 are exchanged between s1 and s2 to generate
corresponding paraphrased sentences s?1 and s?2. p1 ? p2 (p2 ? p1) is verified if s1 ? s?1 (s2 ? s?2) holds. In this
case, both of them hold. English is used for ease of explanation.
pheme; parent/child dependency tree fragments rep-
resented by either the inflected form, base form, pro-
nunciation, or POS; adjacent versions (cf. f15) of
N -gram features and parent/child dependency tree
features. These amount to 78 features, but we even-
tually settled on the 17 features in Table 1 through
ablation tests to evaluate the discriminative power
of each feature.
The ablation tests were conducted using training
data that we prepared. In preparing the training data,
we faced the problem that the completely random
sampling of candidate paraphrase pairs provided us
with only a small number of positive examples.
Thus, we automatically collected candidate para-
phrase pairs that were expected to have a high like-
lihood of being positive as examples to be labeled.
The likelihood was calculated by simply summing
all of the 78 feature values that we have tried, since
they indicate the likelihood of a given candidate
paraphrase pair?s being a paraphrase. Note that val-
ues of the features f8 and f9 are weighted with ?1,
since they indicate the unlikelihood. Specifically,
we first randomly sampled 30,000 definition sen-
tence pairs from the 29,661,812 pairs, and collected
3,000 candidate phrase pairs that had the highest
likelihood from them. The manual labeling of each
candidate phrase pair (p1, p2) was based on bidirec-
tional checking of entailment relation, p1 ? p2 and
p2 ? p1, with p1 and p2 embedded in contexts.
This scheme is similar to the one proposed by
Szpektor et al (2007). We adopt this scheme since
paraphrase judgment might be unstable between an-
notators unless they are given a particular context
based on which they make a judgment. As de-
scribed below, we use definition sentences as con-
texts. We admit that annotators might be biased by
this in some unexpected way, but we believe that
this is a more stable method than that without con-
texts. The labeling process is as follows. First, from
each candidate phrase pair (p1, p2) and its source
definition sentence pair (s1, s2), we create two para-
phrase sentence pairs (s?1, s?2) by exchanging p1 and
p2 between s1 and s2. Then, annotators check if s1
entails s?1 and s2 entails s?2 so that entailment rela-
tions of both directions p1 ? p2 and p2 ? p1 are
checked. Figure 2 shows an example of bidirectional
checking. In this example, both entailment relations,
s1 ? s?1 and s2 ? s?2, hold, and thus the candidate
phrase pair (p1, p2) is judged as positive. We used
(p1, p2), for which entailment relations of both di-
rections held, as positive examples (1,092 pairs) and
the others as negative ones (1,872 pairs).6
We built the paraphrase classifier from the train-
ing data. As mentioned, candidate phrase pairs were
ranked by the distance from the SVM?s hyperplane.
4 Experiment
In this paper, our claims are twofold.
I. Definition sentences on the Web are a treasure
trove of paraphrase knowledge (Section 4.2).
II. Our method of paraphrase acquisition from
definition sentences is more accurate than well-
known competing methods (Section 4.1).
We first verify claim II by comparing our method
with that of Barzilay and McKeown (2001) (BM
method), Moses7 (Koehn et al, 2007) (SMT
method), and that of Murata et al (2004) (Mrt
method). The first two methods are well known for
accurately extracting semantically equivalent phrase
pairs from parallel corpora.8 Then, we verify claim
6The remaining 36 pairs were discarded as they contained
garbled characters of Japanese.
7http://www.statmt.org/moses/
8As anonymous reviewers pointed out, they are unsuper-
vised methods and thus unable to be adapted to definition sen-
1091
I by comparing definition sentence pairs with sen-
tence pairs that are acquired from the Web using Ya-
hoo!JAPANAPI9 as a paraphrase knowledge source.
In the latter data set, two sentences of each pair
are expected to be semantically similar regardless of
whether they are definition sentences. Both sets con-
tain 100,000 pairs.
Three annotators (not the authors) checked evalu-
ation samples. Fleiss? kappa (Fleiss, 1971) was 0.69
(substantial agreement (Landis and Koch, 1977)).
4.1 Our method vs. competing methods
In this experiment, paraphrase pairs are extracted
from 100,000 definition sentence pairs that are ran-
domly sampled from the 29,661,812 pairs. Before
reporting the experimental results, we briefly de-
scribe the BM, SMT, and Mrt methods.
BM method Given parallel sentences like multi-
ple translations of the same source text, the BM
method works iteratively as follows. First, it collects
from the parallel sentences identical word pairs and
their contexts (POS N -grams with indices indicat-
ing corresponding words between paired contexts)
as positive examples and those of different word
pairs as negative ones. Then, each context is ranked
based on the frequency with which it appears in pos-
itive (negative) examples. The most likely K posi-
tive (negative) contexts are used to extract positive
(negative) paraphrases from the parallel sentences.
Extracted positive (negative) paraphrases and their
morpho-syntactic patterns are used to collect addi-
tional positive (negative) contexts. All the positive
(negative) contexts are ranked, and additional para-
phrases and their morpho-syntactic patterns are ex-
tracted again. This iterative process finishes if no
further paraphrase is extracted or the number of iter-
ations reaches a predefined threshold T . In this ex-
periment, following Barzilay and McKeown (2001),
K is 10 and N is 1 to 3. The value of T is not given
in their paper. We chose 3 as its value based on our
preliminary experiments. Note that paraphrases ex-
tracted by this method are not ranked.
tences. Nevertheless, we believe that comparing these methods
with ours is very informative, since they are known to be accu-
rate and have been influential.
9http://developer.yahoo.co.jp/webapi/
SMT method Our SMT method uses Moses
(Koehn et al, 2007) and extracts a phrase table, a
set of two phrases that are translations of each other,
given a set of two sentences that are translations of
each other. If you give Moses monolingual parallel
sentence pairs, it should extract a set of two phrases
that are paraphrases of each other. In this experi-
ment, default values were used for all parameters.
To rank extracted phrase pairs, we assigned each of
them the product of two phrase translation probabil-
ities of both directions that were given by Moses.
For other SMT methods, see Quirk et al (2004) and
Bannard and Callison-Burch (2005) among others.
Mrt method Murata et al (2004) proposed a
method to extract paraphrases from two manually
compiled dictionaries. It simply regards a difference
between two definition sentences of the same word
as a paraphrase candidate. Paraphrase candidates are
ranked according to an unsupervised scoring scheme
that implements their assumption. They assume that
a paraphrase candidate tends to be a valid paraphrase
if it is surrounded by infrequent strings and/or if it
appears multiple times in the data.
In this experiment, we evaluated the unsupervised
version of our method in addition to the supervised
one described in Section 3.2, in order to compare
it fairly with the other methods. The unsupervised
method works in the same way as the supervised
one, except that it ranks candidate phrase pairs by
the sum of all 17 feature values, instead of the dis-
tance from the SVM?s hyperplane. In other words,
no supervised learning is used. All the feature val-
ues are weighted with 1, except for f8 and f9, which
are weighted with ?1 since they indicate the unlike-
lihood of a candidate phrase pair being paraphrases.
BM, SMT, Mrt, and the two versions of our method
were used to extract paraphrase pairs from the same
100,000 definition sentence pairs.
Evaluation scheme Evaluation of each para-
phrase pair (p1, p2) was based on bidirectional
checking of entailment relations p1 ? p2 and p2 ?
p1 in a way similar to the labeling of the training
data. The difference is that contexts for evaluation
are two sentences that are retrieved from the Web
and contain p1 and p2, instead of definition sen-
tences from which p1 and p2 are extracted. This
1092
is intended to check whether extracted paraphrases
are also valid for contexts other than those from
which they are extracted. The evaluation proceeds
as follows. For the top m paraphrase pairs of each
method (in the case of the BM method, randomly
sampled m pairs were used, since the method does
not rank paraphrase pairs), we retrieved a sentence
pair (s1, s2) for each paraphrase pair (p1, p2) from
the Web, such that s1 contains p1 and s2 contains p2.
In doing so, we make sure that neither s1 nor s2 are
the definition sentences from which p1 and p2 are
extracted. For each method, we randomly sample
n samples from all of the paraphrase pairs (p1, p2)
for which both s1 and s2 are retrieved. Then, from
each (p1, p2) and (s1, s2), we create two paraphrase
sentence pairs (s?1, s?2) by exchanging p1 and p2 be-
tween s1 and s2. All samples, each consisting of
(p1, p2), (s1, s2), and (s?1, s?2), are checked by three
human annotators to determine whether s1 entails
s?1 and s2 entails s?2 so that entailment relations of
both directions are verified. In advance of evaluation
annotation, all the evaluation samples are shuffled
so that the annotators cannot find out which sample
is given by which method for fairness. We regard
each paraphrase pair as correct if at least two annota-
tors judge that entailment relations of both directions
hold for it. You may wonder whether only one pair
of sentences (s1, s2) is enough for evaluation since a
correct (wrong) paraphrase pair might be judged as
wrong (correct) accidentally. Nevertheless, we sup-
pose that the final evaluation results are reliable if
the number of evaluation samples is sufficient. In
this experiment, m is 5,000 and n is 200. We use
Yahoo!JAPAN API to retrieve sentences.
Graph (a) in Figure 3 shows a precision curve
for each method. Sup and Uns respectively indi-
cate the supervised and unsupervised versions of our
method. The figure indicates that Sup outperforms
all the others and shows a high precision rate of
about 94% at the top 1,000. Remember that this
is the result of using 100,000 definition sentence
pairs. Thus, we estimate that Sup can extract about
300,000 paraphrase pairs with a precision rate of
about 94%, if we use all 29,661,812 definition sen-
tence pairs that we acquired.
Furthermore, we measured precision after trivial
paraphrase pairs were discarded from the evaluation
samples of each method. A candidate phrase pair
Definition sentence pairs Sup Uns BM SMT Mrt
with trivial 1,381,424 24,049 9,562 18,184
without trivial 1,377,573 23,490 7,256 18,139
Web sentence pairs Sup Uns BM SMT Mrt
with trivial 277,172 5,101 4,586 4,978
without trivial 274,720 4,399 2,342 4,958
Table 2: Number of extracted paraphrases.
(p1, p2) is regarded as trivial if the pronunciation is
the same between p1 and p2,10 or all of the con-
tent words contained in p1 are the same as those
of p2. Graph (b) gives a precision curve for each
method. Again, Sup outperforms the others too, and
maintains a precision rate of about 90% until the top
1,000. These results support our claim II.
The upper half of Table 2 shows the number of
extracted paraphrases with/without trivial pairs for
each method.11 Sup and Uns extracted many more
paraphrases. It is noteworthy that Sup performed the
best in terms of both precision rate and the number
of extracted paraphrases.
Table 3 shows examples of correct and incorrect
outputs of Sup. As the examples indicate, many of
the extracted paraphrases are not specific to defini-
tion sentences and seem very reusable. However,
there are few paraphrases involving metaphors or id-
ioms in the outputs due to the nature of definition
sentences. In this regard, we do not claim that our
method is almighty. We agree with Sekine (2005)
who claims that several different methods are re-
quired to discover a wider variety of paraphrases.
In graphs (a) and (b), the precision of the SMT
method goes up as rank goes down. This strange be-
havior is due to the scoring by Moses that worked
poorly for the data; it gave 1.0 to 82.5% of all the
samples, 38.8% of which were incorrect. We suspect
SMTmethods are poor at monolingual alignment for
paraphrasing or entailment tasks since, in the tasks,
data is much noisier than that used for SMT. See
MacCartney et al (2008) for similar discussion.
4.2 Definition pairs vs. Web sentence pairs
To collect Web sentence pairs, first, we randomly
sampled 1.8 million sentences from the Web corpus.
10There are many kinds of orthographic variants in Japanese,
which can be identified by their pronunciation.
11We set no threshold for candidate phrase pairs of each
method, and counted all the candidate phrase pairs in Table 2.
1093
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000
Pre
cisi
on
Top-N
?Sup_def?
?Uns_def?
?SMT_def?
?BM_def?
?Mrt_def?
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000
Pre
cisi
on
Top-N
?Sup_def_n?
?Uns_def_n?
?SMT_def_n?
?BM_def_n?
?Mrt_def_n?
(a) Definition sentence pairs with trivial paraphrases (b) Definition sentence pairs without trivial paraphrases
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000
Pre
cisi
on
Top-N
?Sup_www?
?Uns_www?
?SMT_www?
?BM_www?
?Mrt_www?
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000
Pre
cisi
on
Top-N
?Sup_www_n?
?Uns_www_n?
?SMT_www_n?
?BM_www_n?
?Mrt_www_n?
(c) Web sentence pairs with trivial paraphrases (d) Web sentence pairs without trivial paraphrases
Figure 3: Precision curves of paraphrase extraction.
Rank Paraphrase pair
Correct
13 ?????????????? (send a message to the e-mail address)????????????????? (send
an e-mail message to the e-mail address)
19 ????????? (requested by a customer)?????????? (commissioned by a customer)
70 ?????????? (describe the fiscal condition of company) ??????????? (indicate the fiscal state
of company)
112 ???????????? (get information)???????? (get news)
656 ???????? (it is a convention)????????? (it is a rule)
841 ??????????????? (represent the energy scale of earthquake)????????? (represent the scale
of earthquake)
929 ???????? (cause the oxidation of cells)????????? (cause cellular aging)
1,553 ??????? (remove dead skin cells)??????? (peel off dead skin cells)
2,243 ????????? (required for the development of fetus)??????????????? (indispensable for the
growth and development of fetus)
2,855 ??????? (correct eyesight)???????? (perform eyesight correction)
2,931 ????????? (call it even)?????????? (call it quits)
3,667 ?????????????? (accumulated on a hard disk)?????????????????? (stored on a
hard disk drive)
4,870 ????????? (excrete harmful substance)?????????? (discharge harmful toxin)
5,501 ????????????????????????? (mount two processor cores on one CPU)????????
????????????????? (build two processor cores into one package)
10,675 ??????? (trade foreign currencies)???????? (exchange one currency for another)
112,819 ??????????? (become a regular staff member of the company where (s)he has worked as a temp) ???
????????? (employed by the company where (s)he has worked as a temp)
193,553 ????????????? (access Web sites)??????????? (visit WWW sites)
Incorrect
903 ?????????? (send to a Web browser)??????????? (send to a PC)
2,530 ?????? (intend to balance)?????????? (intend to refresh)
3,008 ???????????? (unable to digest with digestive enzymes)???????????? (hard to digest with
digestive enzymes)
Table 3: Examples of correct and incorrect paraphrases extracted by our supervised method with their rank.
1094
We call them sampled sentences. Then, using Ya-
hoo!JAPANAPI, we retrieved up to 20 snippets rele-
vant to each sampled sentence using all of the nouns
in each sentence as a query. After that, each snippet
was split into sentences, which we call snippet sen-
tences. We paired a sampled sentence and a snippet
sentence that was the most similar to the sampled
sentence. Similarity is the number of nouns shared
by the two sentences. Finally, we randomly sampled
100,000 pairs from all the pairs.
Paraphrase pairs were extracted from the Web
sentence pairs by using BM, SMT, Mrt and the su-
pervised and unsupervised versions of our method.
The features used with our methods were selected
from all of the 78 features mentioned in Section 3.2
so that they performed well for Web sentence pairs.
Specifically, the features were selected by ablation
tests using training data that was tailored to Web
sentence pairs. The training data consisted of 2,741
sentence pairs that were collected in the same way as
the Web sentence pairs and was labeled in the same
way as described in Section 3.2.
Graph (c) of Figure 3 shows precision curves. We
also measured precision without trivial pairs in the
same way as the previous experiment. Graph (d)
shows the results. The lower half of Table 2 shows
the number of extracted paraphrases with/without
trivial pairs for each method.
Note that precision figures of our methods in
graphs (c) and (d) are lower than those of our meth-
ods in graphs (a) and (b). Additionally, none of the
methods achieved a precision rate of 90% using Web
sentence pairs.12 We think that a precision rate of
at least 90% would be necessary if you apply auto-
matically extracted paraphrases to NLP tasks with-
out manual annotation. Only the combination of Sup
and definition sentence pairs achieved that precision.
Also note that, for all of the methods, the numbers
of extracted paraphrases from Web sentence pairs
are fewer than those from definition sentence pairs.
From all of these results, we conclude that our
claim I is verified.
12Precision of SMT is unexpectedly good. We found some
Web sentence pairs consisting of two mostly identical sentences
on rare occasions. The method worked relatively well for them.
5 Conclusion
We proposed a method of extracting paraphrases
from definition sentences on the Web. From the ex-
perimental results, we conclude that the following
two claims of this paper are verified.
1. Definition sentences on the Web are a treasure
trove of paraphrase knowledge.
2. Our method extracts many paraphrases from
the definition sentences on the Web accurately;
it can extract about 300,000 paraphrases from
6 ? 108 Web documents with a precision rate
of about 94%.
Our future work is threefold. First, we will release
extracted paraphrases from all of the 29,661,812
definition sentence pairs that we acquired, after hu-
man annotators check their validity. The result will
be available through the ALAGIN forum.13
Second, we plan to induce paraphrase rules
from paraphrase instances. Though our method
can extract a variety of paraphrase instances on
a large scale, their coverage might be insufficient
for real NLP applications since some paraphrase
phenomena are highly productive. Therefore, we
need paraphrase rules in addition to paraphrase in-
stances. Barzilay and McKeown (2001) induced
simple POS-based paraphrase rules from paraphrase
instances, which can be a good starting point.
Finally, as mentioned in Section 1, the work in
this paper is only the beginning of our research on
paraphrase extraction. We are trying to extract far
more paraphrases from a set of sentences fulfilling
the same pragmatic function (e.g. definition) for the
same topic (e.g. osteoporosis) on the Web. Such
functions other than definition may include the us-
age of the same Linux command, the recipe for the
same cuisine, or the description of related work on
the same research issue.
Acknowledgments
We would like to thank Atsushi Fujita, Francis
Bond, and all of the members of the Information
Analysis Laboratory, Universal Communication Re-
search Institute at NICT.
13http://alagin.jp/
1095
References
Susumu Akamine, Daisuke Kawahara, Yoshikiyo Kato,
Tetsuji Nakagawa, Yutaka I. Leon-Suematsu, Takuya
Kawada, Kentaro Inui, Sadao Kurohashi, and Yutaka
Kidawara. 2010. Organizing information on the web
to support user judgments on information credibil-
ity. In Proceedings of 2010 4th International Uni-
versal Communication Symposium Proceedings (IUCS
2010), pages 122?129.
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entailment
methods. Journal of Artificial Intelligence Research,
38:135?187.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL-2005), pages 597?
604.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL
2003, pages 16?23.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the ACL joint
with the 10th Meeting of the European Chapter of the
ACL (ACL/EACL 2001), pages 50?57.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning direc-
tionality of inference rules. In Proceedings of Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP2007), pages 161?170.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of the 2006 Human
Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics (HLT-NAACL 2006), pages 17?24.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of the 20th international conference on Com-
putational Linguistics (COLING 2004), pages 350?
356.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
Atsushi Fujii and Tetsuya Ishikawa. 2002. Extraction
and organization of encyclopedic knowledge informa-
tion using the World Wide Web (written in Japanese).
Institute of Electronics, Information, and Communica-
tion Engineers, J85-D-II(2):300?307.
Maayan Geffet and Ido Dagan. 2005. The distributional
inclusion hypotheses and lexical entailment. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2005), pages
107?114.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,
Stijn De Saeger, Masaki Murata, and Jun?ichi Kazama.
2009. Large-scale verb entailment acquisition from
the web. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2009), pages 1172?1181.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical selection and paraphrase in
a meaning-text generation model. In Ce?cile L. Paris,
William R. Swartout, and William C. Mann, editors,
Natural language generation in artificial intelligence
and computational linguistics, pages 293?312. Kluwer
Academic Press.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings of
the 2006 Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL 2006), pages
455?462.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Exploit-
ing Wikipedia as external knowledge for named entity
recognition. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL 2007), pages 698?707, June.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-scale
clustering of dependency relations. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (ACL-08: HLT), pages 407?415.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2007), pages
177?180.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159?174.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model for
natural language inference. In Proceedings of the 2008
1096
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2008), pages 802?811.
Nitin Madnani and Bonnie Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Computational Linguistics, 36(3).
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news
on a daily basis with columbia?s newsblaster. In Pro-
ceedings of the 2nd international conference on Hu-
man Language Technology Research, pages 280?285.
Masaki Murata, Toshiyuki Kanemaru, and Hitoshi Isa-
hara. 2004. Automatic paraphrase acquisition based
on matching of definition sentences in plural dictionar-
ies (written in Japanese). Journal of Natural Language
Processing, 11(5):135?149.
Roberto Navigli and Paola Velardi. 2010. Learning
word-class lattices for definition and hypernym extrac-
tion. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL
2010), pages 1318?1327.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2004), pages 142?149.
Deepak Ravichandran and Eduard H. Hovy. 2002.
Learning surface text patterns for a question answer-
ing system. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), pages 41?47.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of the Third International Workshop on
Paraphrasing (IWP-2005), pages 80?87.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proceedings of the 2nd international Con-
ference on Human Language Technology Research
(HLT2002), pages 313?318.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary template. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING2008), pages 849?856.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acquisi-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics (ACL 2007),
pages 456?463.
1097
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1619?1629,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Aid is Out There:
Looking for Help from Tweets during a Large Scale Disaster
Istva?n Varga? Motoki Sano? Kentaro Torisawa? Chikara Hashimoto?
Kiyonori Ohtake? Takao Kawai? Jong-Hoon Oh? Stijn De Saeger?
?Information Analysis Laboratory,
National Institute of Information and Communications Technology (NICT), Japan
{istvan, msano, torisawa, ch, kiyonori.ohtake, rovellia, stijn}@nict.go.jp
?Knowledge Discovery Research Laboratories, NEC Corporation, Japan
t-kawai@bx.jp.nec.com
Abstract
The 2011 Great East Japan Earthquake
caused a wide range of problems, and as
countermeasures, many aid activities were
carried out. Many of these problems and
aid activities were reported via Twitter.
However, most problem reports and corre-
sponding aid messages were not success-
fully exchanged between victims and lo-
cal governments or humanitarian organi-
zations, overwhelmed by the vast amount
of information. As a result, victims could
not receive necessary aid and humanitar-
ian organizations wasted resources on re-
dundant efforts. In this paper, we propose
a method for discovering matches between
problem reports and aid messages. Our
system contributes to problem-solving in
a large scale disaster situation by facilitat-
ing communication between victims and
humanitarian organizations.
1 Introduction
The 2011 Great East Japan Earthquake in March
11, 2011 killed 15,883 people and destroyed over
260,000 households (National Police Agency of
Japan, 2013). Accustomed way of living suddenly
became unmanageable and people found them-
selves in extreme conditions for months.
Just after the disaster, many people used Twitter
for posting problem reports and aid messages as
it functioned while most communication channels
suffered disruptions (Winn, 2011; Acar and Mu-
raki, 2011; Sano et al, 2012). Examples of such
problem reports and aid messages, translated from
Japanese tweets, are given below (P1, A1).
P1 My friend said infant formula is sold out. If
somebody knows shops in Sendai-city where
they still have it in stock, please let us know.
A1 At Jusco supermarket in Sendai, you can still
buy water and infant formula.
If A1 would have been forwarded to the sender
of P1, it could have helped since it would help
the ?friend? to obtain infant formula. But in re-
ality, the majority of such reports/messages, es-
pecially unforeseen ones went unnoticed amongst
the mass of information (Ohtake et al, 2013). In
addition, there were cases where many humani-
tarian organizations responded to the same prob-
lems and wasted precious resources. For instance,
many volunteers responded to problems which
were heavily reported by public media, leading
to oversupply (Saijo, 2012). Such waste of re-
sources could have been avoided if the organiza-
tions would have successfully shared the aid mes-
sages for the same problems.
Such observations motivated this work. We de-
veloped methods for recognizing problem reports
and aid messages in tweets and finding proper
matches between them. By browsing the discov-
ered matches, victims can be assisted to over-
come their problems, and humanitarian organiza-
tions can avoid redundant relief efforts. We define
problem reports, aid messages and their successful
matches as follows.
Problem report: A tweet that informs about the
possibility or emergence of a problem that re-
quires a treatment or countermeasure.
Aid message: A tweet that (1) informs about sit-
uations or actions that can be a remedy or so-
lution for a problem, or (2) informs that the
problem is solved or is about to be solved.
Problem-aid tweet match: A tweet pair is a
problem-aid tweet match (1) if the aid mes-
sage informs how to overcome the problem,
(2) if the aid message informs about the set-
1619
tlement of the problem, or (3) if the aid mes-
sage provides information which contributes
to the settlement of the problem.
In this work we excluded direct requests, such as
?Send us food!?, from problem reports. This is be-
cause it is relatively easy to recognize such direct
requests by checking mood types (i.e., imperative)
and their behavior is quite different from prob-
lem reports like ?People in Sendai are starving?.
Problem reports in this work do not directly state
which actions are required, only implying the ne-
cessity of a countermeasure through claiming the
existence of problems.
An underlying assumption of our method is that
we can find a noun-predicate dependency relation
that works as an indicator of problems and aids in
problem reports and aid messages, which we refer
to as problem nucleus and aid nucleus.1 An exam-
ple of problem nucleus is ?infant formula is sold
out? in P1, and that of aid nucleus is ?(can) buy
infant formula? in A1. Many problem-aid tweet
matches can be recognized through problem and
aid nuclei pairs.
We also assume that if the problem and aid nu-
clei match, they share the same noun. Then, the
semantics of predicates in the nuclei is the main
factor that decides whether the nuclei constitute
a match. We introduce a semantic classification
of predicates according to the framework of ex-
citation polarities proposed in Hashimoto et al
(2012). Our hypothesis is that excitation polarities
along with trouble expressions can characterize
problem reports, aid messages and their matches.
We developed a supervised method encoding such
information into its features.
An evident alternative to this approach is to use
sentiment analysis (Mandel et al, 2012; Tsagkali-
dou et al, 2011) assuming that problem reports
should include something ?bad? while aid mes-
sages describe something ?good?. However, we
will show that this does not work well in our exper-
iments. We think this is due to mismatch between
the concepts of problem/aid and sentiment polar-
ity. Note that previous work on ?demand? recogni-
tion also found similar tendencies (Kanayama and
Nasukawa, 2008).
Another issue in this task is, of course, the
context surrounding problem/aid nuclei. The fol-
1We found that out of 500 random tweets only 4.5% of
problem reports and 9.1% of aid messages did not contain
any problem report/aid message nuclei.
lowing (imaginary) tweets exemplify the problems
caused by contexts.
FP1 I do not believe infant formula is sold out
in Sendai.
FA1 At Jusco supermarket in Iwaki, you can still
buy infant formula.
The problem nuclei of FP1 and P1 are the same
but FP1 is not a problem report because of the ex-
pression ?I do not believe?. The aid nuclei of FA1
and A1 are the same but FA1 does not constitute
a proper match with P1 because FA1 and P1 re-
fer to different cities, ?Iwaki? and ?Sendai?. In
this work, the problems concerning the modality
and other semantic modifications to problem/aid
nuclei by context are dealt with by the introduc-
tion of features representing the text surrounding
the nuclei in machine learning. As for the loca-
tion problem, we apply a location recognizer to all
tweets and restrict the matching candidates to the
tweet pairs referring to the same location.
2 Approach
!"#$%"&"'()*&+*,*&-.(.+*,/+/0$0,/0,)-+$*#.(,'+
!"#$%&'("&!#")("&*#+,-.&"(
12001.+ 12001+/0$0,/0,)-+#0&*3",+
$#"4&0!+#0$"#1+$#"4&0!+,5)&05.+
/-0('&11/+&("&*#+,-.&"(
*(/+!0..*'0++*(/+,5)&05.+
$#"4&0!+#0$"#1+$#"4&0!+,5)&05.+ *(/+!0..*'0+*(/+,5)&05.+
!"#$%&'2/-0()3&&)('/)*4(
$#"4&0!+*,/+*(/+,"5,+*#0+1%0+.*!06+.*!0+'0"'#*$%()*&+&")*3",+
&")*3",+#0)"',(70#+
!"#$%&'2/-0('/)*4("&*#+,-.&"(
Figure 1: Problem-aid matching system overview.
We developed machine learning based systems
to recognize problem reports, aid messages and
problem-aid tweet matches. Figure 1 illustrates
the whole system. First, location names in tweets
are identified by matching tweets against our loca-
tion dictionary, described in Section 3. Then, each
tweet is paired with each dependency relation in
the tweet, which is a candidate of problem/aid nu-
clei and given to the problem report and aid mes-
sage recognizers. A tweet-nucleus-candidate pair
judged as problem report is combined with another
tweet-nucleus-candidate pair recognized as an aid
message if the two nuclei share the same noun and
the tweets share the same location name, and given
to the problem-aid match recognizer.
1620
In the following, problem and aid nuclei are
denoted by a noun-template pair. A template is
composed of a predicate and its argument posi-
tion. For instance, ?water supply stopped? in P2
is a problem nucleus, ?water supply recovered? in
A2 is an aid nucleus and they are denoted by the
noun-template pairs ?water supply, X stopped? and
?water supply, X recovered?.
P2 In Sendai city, water supply stopped.
A2 In Sendai city, water supply recovered.
Roughly speaking, we regard the tasks of prob-
lem report recognition and aid message recogni-
tion as the tasks of finding proper problem/aid
nuclei in tweets and our method performs these
tasks based on the semantic properties of nouns
and templates in problem/aid nucleus candidates
and their surrounding contexts.
The basic intuition behind this approach can
be explained using excitation polarity proposed in
Hashimoto et al (2012). Excitation polarity differ-
entiates templates into ?excitatory? or ?inhibitory?
with regard to the main function or effect of en-
tities referred to by their argument noun. While
excitatory templates (e.g., cause X, buy X, suf-
fer from X) entail that the main function or ef-
fect is activated or enhanced, inhibitory templates
(e.g., ruin X, prevent X, X runs out) entail that
the main function or effect is deactivated or sup-
pressed. The templates that do not fit into the
above categorization are classified as ?neutral?.
We observed that problem reports in general
included either of (A) a dependency relation be-
tween a noun referring to some trouble and an
excitatory template or (B) a dependency rela-
tion between a noun not referring to any trouble
and an inhibitory template. Examples of (A) in-
clude ?carbon monoxide poisoning, suffer from
X?, ?false rumor, spread X?. They refer to events
that activate troubles. On the other hand, (B) is
exemplified by ?school, X is collapsed?, ?battery,
X runs out?, which imply that some non-trouble
objects such as resources, appliances and facilities
are dysfunctional. We assume that if we can find
such dependency relations in tweets, the tweets are
likely to be problem reports.
Contrary, a tweet is more likely to be an aid
message when it includes either (C) a dependency
relation between a noun referring to some trouble
and an inhibitory template or (D) a dependency re-
lation between a noun not referring to any trou-
trouble non-trouble
excitatory (A) problem nucleus (D) aid nucleus
inhibitory (C) aid nucleus (B) problem nucleus
Table 1: Problem/aid-excitation matrix.
ble and an excitatory template. Examples of (C)
are ?flu, X was eradicated (in some shelter)? and
?debris, remove X?. They represent the dysfunc-
tion of troubles and can mean the solution or the
settlement of troubles. On the other hand, exam-
ples of (D) include ?school, X re-build? and ?baby
formula, buy X?. They entail that some resources
function properly or become available. These for-
mulations are summarized in Table 1.
As an interesting consequence of such a view
on problem/aid nucleus, we can say the following
regarding problem-aid tweet matchings: when a
problem nucleus and an aid nucleus are an ade-
quate match, the excitation polarities of their tem-
plates are opposite. Consider the following tweets.
P3 Some people were going back to Iwaki, but the
water system has not come back yet. It?s ter-
rible that bath is unusable.
A3 We open the bath for the public, located on
the 2F of Iwaki Kuhon temple. If you?re stay-
ing at a relief shelter and would like to take a
bath, you can use it.
?Bath is unusable? in P3 is a problem nucleus
while ?open the bath? in A3 is an aid nucleus.
Since the problem reported in P3 can be solved
with A3, they are a successful match. The in-
hibitory template ?X is unusable? indicates that
the function of ?bath?, a non-trouble expression,
is suppressed. The excitatory template ?open X?
indicates that the function of ?bath? is activated.
The same holds when we consider the noun re-
ferring to troubles like ?flu?. The polarity of the
template in a problem nucleus should be excita-
tory like ?flu is raging? while that of an aid nucleus
should be inhibitory like ?flu, X was eradicated?.
These examples keep the constraint that the prob-
lem and aid nucleus should have opposite polari-
ties when they constitute a match.
Note that the formulations of problem report,
aid message and their matches or the excitation
matrix (Table 1) were not presented to our anno-
tators and our test/training data may contain data
that contradict with the formulations. These for-
mulations constitute the hypothesis to be validated
in this work.
1621
An important point to be stressed here is that
there are problem-aid tweet matches that do not
fit into our formulations. For instance, we as-
sume that the problem nucleus and aid nucleus in
a proper match share the same noun. However,
tweet pairs such as ?There are many injured people
in Sendai city? and ?We are sending ambulances
to Sendai? can constitute a proper match, but there
is no proper problem-aid nuclei pair that share the
same noun in these tweets. (We can find the de-
pendency relations sharing ?Sendai? but they do
not express anything about the contents of prob-
lem and aid.) The point is that the tweet pairs can
be judged because people know ambulances can
be a countermeasure to injured people as world
knowledge. Introducing such world knowledge is
beyond the scope of this current study.
Also, we exclude direct requests from problem
reports. As mentioned in the introduction, identi-
fying direct requests is relatively easy, hence we
excluded them from our target.
3 Problem Report and Aid Message
Recognizers
We recognize problem reports and aid messages in
given tweets using a supervised classifier, SVMs
with linear kernel, which worked best in our pre-
liminary experiments. The feature set given to
the SVMs are summarized in the top part of Ta-
ble 2. Note that we used a common feature
set for both the problem report recognizer and
aid message recognizer and that it is categorized
into several types: features concerning trouble
expressions (TR), excitation polarity (EX), their
combination (TREX1) and word sentiment polar-
ity (WSP), features expressing morphological and
syntactic structures of nuclei and their context sur-
rounding problem/aid nuclei (MSA), features con-
cerning semantic word classes (SWC) appearing
in nuclei and their context, request phrases, such
as ?Please help us?, appearing in tweets (REQ),
and geographical locations in tweets recognized
by our location recognizer (GL). MSA is used to
express the modality of nuclei and other contex-
tual information surrounding nuclei. REQ was in-
troduced based on our observation that if there are
some requests in tweets, problem nuclei tend to
appear as justification for the requests.
We also attempted to represent nucleus template
IDs, noun IDs and their combinations directly in
our feature set to capture typical templates fre-
TR Whether the nucleus noun is a trouble/non-trouble expression.
EX1 The excitation polarity and the value of the excitation score of the
nucleus template.
TREX1 All possible combinations of trouble/non-trouble of TR and exci-
tation polarities of EX1.
WSP1 Whether the nucleus noun is positive/negative/not in theWord Sen-
timent Polarity (WSP) dictionary.
WSP2 Whether the nucleus template is positive/negative/not in the WSP
dictionary.
WSP3 Whether the nucleus template is followed by a positive/negative
word within the tweet.
MSA1 Morpheme n-grams, syntactic dependency n-grams in the tweet
and morpheme n-grams before and after the nucleus template.
(1 ? n ? 3)
MSA2 Character n-grams of the nucleus template to capture conjugation
and modality variations. (1 ? n ? 3)
MSA3 Morpheme and part-of-speech n-grams within the bunsetsu con-
taining the nucleus template to capture conjugation and modality
variations. (1 ? n ? 3) (A bunsetsu is a syntactic constituent
composed of a content word and several function words, the small-
est unit of syntactic analysis in Japanese.)
MSA4 The part-of-speech of the nucleus template?s head to capture
modality variations outside the nucleus template?s bunsetsu.
MSA5 The number of bunsetsu between the nucleus noun and the nucleus
template. We found that a long distance between the noun and the
template suggests parsing errors.
MSA6 Re-occurrence of the nucleus noun?s postpositional particle be-
tween the nucleus noun and the nucleus template. We found
that the re-occurrence of the same postpositional particle within
a clause suggests parsing errors.
SWC1 The semantic class n-grams in the tweet.
SWC2 The semantic class(es) of the nucleus noun.
REQ Presence of a request phrase in the tweet, identified from within
426 manually collected request phrases.
GL Geographical locations in the tweet identified using our location
recognizer. Existence/non-existence of locations in tweets are also
encoded.
EX2 Whether the problem and aid nucleus templates have the same or
opposite excitation polarities.
EX3 Product of the values of the excitation scores for the problem and
the aid nucleus template.
TREX2 All possible combinations of trouble/non-trouble of TR, excitation
polarity EX1 of the problem nucleus template and excitation po-
larity EX1 of the aid nucleus template.
SIM1 Common semantic word classes of the problem report and aid mes-
sage.
SIM2 Whether there are common nouns modifying the common nucleus
noun or not in the problem report and aid message.
SIM3 Whether the words in the same word class modify the common
nucleus noun or not in the problem report and aid message.
SIM4 The semantic similarity score between the problem nucleus tem-
plate and the aid nucleus template.
CTP Whether the problem nucleus template and the aid nucleus tem-
plate are in contradiction relation dictionary or not.
SSR1 Problem report recognizer?s SVM score of problem nucleus tem-
plate.
SSR2 Problem report recognizer?s SVM score of aid nucleus template.
SSR3 Aid message recognizer?s SVM score of the problem nucleus tem-
plate.
SSR4 Aid message recognizer?s SVM score of the aid nucleus template.
Table 2: Features used with the problem re-
port recognizer and the aid message recognizer
(above); additional features used in training the
problem-aid match recognizer (below).
quently appearing in problem and aid nuclei, but
since there was no improvement we omit them.
The other feature types need some non-trivial
dictionaries. In the following, we explain how we
created the dictionaries for each feature type along
with the motivation behind their introduction.
Trouble Expressions (TR) As mentioned previ-
ously, trouble expressions work as good evidence
for recognizing problem reports and aid messages.
The TR feature indicates whether the noun in the
problem/aid nucleus candidate is a trouble ex-
1622
pression or not. For this purpose, we created
a list of trouble expressions following the semi-
supervised procedure presented in De Saeger et al
(2008). After manual validation of the list, we ob-
tained 20,249 expressions referring to some trou-
bles, such as ?tsunami? and ?flu?. The value of the
TR feature is determined by checking whether the
nucleus noun is contained in the list.
Excitation Polarities (EX) The excitation po-
larities are also important in recognizing problem
reports and aid messages as mentioned before. For
constructing the dictionary for excitation polarities
of templates, we applied the bootstrapping proce-
dure in Hashimoto et al (2012) to 600 millionWeb
pages. Hashimoto?s method provides the value of
the excitation score in [?1, 1] for each template
indicating the polarities and their strength. Posi-
tive value indicates excitatory, negative value in-
hibitory and small absolute value neutral. After
manual checking of the results by the majority
vote of three human annotators (other than the au-
thors), we limited the templates to the ones that
have score values consistent with the majority vote
of the annotators, obtaining a dictionary consisting
of 7,848 excitatory, 836 inhibitory and 7,230 neu-
tral templates. The Fleiss? (1971) kappa-score was
0.48 (moderate agreement). We used the excita-
tion score values as feature values. Excitation has
already been used in many works, such as causal-
ity and contradiction extraction (Hashimoto et al,
2012) or Why-QA (Oh et al, 2013).
Word Sentiment Polarity (WSP) As we sug-
gested before, full-fledged sentiment analysis to
recognize the expressions, including clauses and
phrases, that refer to something good or bad was
not effective in our task. However, the sentiment
polarity, assigned to single words turned out to
be effective. To identify the sentiment polarity
of words, we employed the word sentiment polar-
ity dictionary used with a sentiment analysis tool
for Japanese, the Opinion Extraction Tool soft-
ware2, which is an implementation of Nakagawa
et al (2010). The dictionary includes 9,030 posi-
tive and 27,951 negative words. Note that we used
the Opinion Extraction Tool in the experiments to
check the effectiveness of the full-fledged senti-
ment analysis in this task.
Semantic Word Class (SWC) We assume that
nouns in the same semantic class behave simi-
2Provided at the ALAGIN Forum (http://www.alagin.jp/).
larly in crisis situations. For example, if ?infec-
tion? appears in a problem report, the tweets in-
cluding ?pulmonary embolism? are also likely to
be problem reports. Semantic word class features
are used to capture such tendencies. We applied
an EM-style word clustering algorithm in Kazama
and Torisawa (2008) to 600 millionWeb pages and
clustered 1 million nouns into 500 classes. This
algorithm has been used in many works, such as
relation extraction (De Saeger et al, 2011) and
Why-QA (Oh et al, 2012), and can generate vari-
ous kinds of semantically clean word classes, such
as foods, disease names, and natural disasters. We
used the word classes in tweets as features.3
Geographical Locations (GL) Our location
recognizer matches tweets against our loca-
tion dictionary. Location names and their
existence/non-existence in tweets constitute evi-
dence, thus we encoded such information into our
features. The location dictionary was created from
the Japan Post code data4 and Wikipedia, contain-
ing 2.7 million location names including cities,
schools and other facilities (Kazama et al, 2013).
4 Problem-Aid Match Recognizer
After problem report and aid message recogni-
tion, the positive outputs of the respective classi-
fiers are used as input in this step. The problem-
aid match recognizer classifies an aid message-
nucleus pair together with the problem report-
nucleus pair employing SVMs with linear ker-
nel, which performed best in this task again. The
problem-aid match recognizer uses all the features
used in the problem report recognizer and the aid
message recognizer along with additional features
regarding: excitation polarity (EX) and trouble
expressions (TR), distributional similarity (SIM),
contradiction (CTP) and SVM-scores of the prob-
lem report and aid message recognizers (SSR).
Here also we attempted to capture typical or fre-
quent matches of nuclei using template and noun
IDs and their combinations, but we did not observe
any improvement so we omit them from the fea-
ture set. The bottom part of Table 2 summarizes
the additional feature set, some of which are de-
scribed below in more detail.
3There is a slight complication here. For each noun n, EM
clustering estimates a probability distribution P (n|c?) for n
and semantic class c?. From this distribution we obtained
discrete semantic word classes by assigning each noun n to
semantic class c = argmaxc? p(c?|n).
4http://www.post.japanpost.jp/zipcode/download.html
1623
As for TR and EX, our intuition is that if a prob-
lem nucleus and an aid nucleus are an adequate
match, their excitation polarities are opposite, as
described in Section 2. We encode whether the ex-
citation polarities of nuclei templates are the same
or not in our features. Also, the excitation polar-
ities of problem and aid nuclei and TR are com-
bined (TREX1, TREX2) so that the classifier can
know whether the nuclei follow the constraint for
adequate matches described in Section 2.
As for SIM, if an aid message matches a prob-
lem report, besides the common nucleus noun, it is
reasonable to assume that certain contexts are se-
mantically similar. We capture this characteristic
in three ways. SIM1 looks for common semantic
word classes in the problem report and aid mes-
sage. SIM2 and SIM3 target the modifiers of the
common nucleus noun if they exist.
We also observed that if an aid message matches
a problem report, the problem nucleus template
and aid nucleus template are often distributionally
similar. A typical example is ?X is sold out? and
?buy X?. SIM4 captures this tendency. As the dis-
tributional similarity between templates, we used
a Bayesian distributional similarity measure pro-
posed by Kazama et al (2010).5
CTP indicates whether the problem and aid nu-
clei are in contradiction relation or not. This fea-
ture was implemented based on the observation
that when problem and aid nuclei are in contradic-
tion relation, they are often proper matches (e.g.,
?blackout, ?X starts?? and ?blackout, ?X ends??).
CTP indicates whether nucleus pairs are in the
one million contradiction phrase pairs6 automat-
ically obtained by applying a method proposed by
Hashimoto et al (2012) to 600 million Web pages.
5 Experiments
We evaluated our problem report recognizer and
problem-aid match recognizer. For the sake of
space, we give only the performance figures of the
aid message recognizer at the end of Section 5.1.
We collected tweets posted during and after
the 2011 Great East Japan Earthquake, between
March 10 and April 4, 2011. After applying
keyword-based filtering with a list of over 300
5The original similarity was defined over noun pairs and it
was estimated from dependency relations. Obtaining similar-
ity between template pairs, not noun pairs, is straightforward
given the same dependency relations. We used 600 million
Web pages for this similarity estimation.
6The precision of the pairs was reported as around 70%.
disaster related keywords, we obtained 55 million
tweets. After dependency parsing7, we used them
in our evaluation.
5.1 Problem Report Recognition
Firstly, we evaluated our problem report recog-
nizer. Particularly, we assessed the effect of ex-
citation polarities and trouble expressions in two
settings. The first is against a naturally distributed
gold standard data. The second targets problem
reports with problem nuclei unseen in the training
data.
In both experiments we observed that the per-
formance drops when excitation polarities and
trouble expressions are removed from the feature
set. The performance drop was larger in the sec-
ond experiment which suggests that the excitation
polarities and trouble expressions are more effec-
tive against unseen problem reports.
Training and test data for problem report recog-
nition consist of tweet-nucleus candidate pairs
randomly sampled from our 55 million tweet data.
The training data (R) and test data (T ) consist of
13,000 and 1,000 pairs, respectively, manually la-
beled by three annotators (other than the authors)
as problem or other. Final judgment was made by
majority vote. The Fleiss? kappa score for train-
ing and test data for annotation judgement is 0.74
(substantial agreement).
Our problem report recognizer and its variants
are listed in Table 3. Table 4 shows the evalua-
tion results. The proposed method achieved about
44% recall and nearly 80% precision, outperform-
ing all other systems in terms of precision, F-score
and average precision8. The improvement in pre-
cision when using TR&EX is statistically signif-
icant (p < 0.05).9 Note that F-measure dropped
PROPOSED: Our proposed method with all features used.
PROPOSED-*: The proposed method without the feature set de-
noted by ?*?. Here EX and TR denote all excitation po-
larity and trouble expression related features, respectively,
including their combinations (TREX1).
PROPOSED+OET: The proposed method incorporating the
classification results of problem nucleus candidates by the
Opinion Extraction Tool as additional binary features.
RULE-BASED: The method that regards only nuclei satisfying
the constraint in Table 1 as problem nuclei.
Table 3: Evaluated problem report recognizers.
7http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP
8We calculate average precision using the formula: aP =?n
k=1
(Prec(k)?rel(k))
n , where Prec(k) is the precision atcut-off k and rel(k) is an indicator function equaling 1 if
the item at rank k is relevant, zero otherwise.
9Throughout this paper we performed two-tailed test of
1624
Recognition system R (%) P (%) F (%) aP (%)
PROPOSED 44.26 79.41 56.83 71.82
PROPOSED-TR&EX 45.08 74.83 56.26 69.67
PROPOSED-EX 44.67 74.66 55.89 69.90
PROPOSED-TR 43.85 74.31 55.15 69.44
PROPOSED-MSA 28.69 70.71 40.81 57.74
PROPOSED-SWC 43.42 75.97 55.25 70.61
PROPOSED-WSP 43.14 77.83 55.50 70.45
PROPOSED-REQ 42.64 76.16 55.50 54.67
PROPOSED-GL 44.14 78.34 55.50 56.46
PROPOSED+OET 44.24 79.41 56.82 71.81
RULE-BASED 30.32 67.96 41.93 n/a
Table 4: Recall (R), precision (P), F-score (F) and
average precision (aP) of the problem report rec-
ognizers.
whenever each type of feature was removed, im-
plying that each type of feature is effective in this
task. Especially note the performance drop if we
remove excitation polarities (EX), trouble expres-
sion (TR) and both excitation and trouble expres-
sion features (TR&EX), confirming that they are
crucial in recognizing problem reports with high
accuracy. Also note that the performance of PRO-
POSED+OET was actually slightly worse than that
of the proposed method. This suggests that full-
fledged sentiment analysis is not effective at least
in this setting. The rule-based method achieved
relatively high precision despite of the low recall,
demonstrating the importance of problem and aid
nuclei formulations described in Section 1.
The second experiment assessed the efficiency
of our problem report recognizer against unseen
problem nuclei under the condition that every tem-
plate in nuclei has excitation polarity. We sampled
the training and test data so that the problem nu-
cleus nouns and templates in the training and test
data are disjoint. First we created a subset of the
test data by selecting the samples which had nu-
clei with excitation templates. We call this sub-
set T ?. Next, we removed samples from training
data R if either of their problem nouns or tem-
plates appeared in the nuclei of T ?. The result-
ing new training data (called R?) and test data (T ?)
consist of 6,484 and 407 tweet-nucleus candidate
pairs, respectively. We trained our problem report
recognizer using R? and tested its performance us-
ing T ?. Figure 2 shows the precision-recall curves
obtained by changing the threshold on the SVM
scores. The effectiveness of excitation polarities
and trouble expressions was more evident in this
setting. The PROPOSED?s performance was ac-
tually better in this setting (almost 50% recall at
population proportion (Ott and Longnecker, 2010) using
SVM-threshold=0.
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Pre
cisi
on
Recall
PROPOSED-TRPROPOSED-EX PROPOSED-TR&EXPROPOSED
Figure 2: Precision-recall curves of problem re-
port recognizers against unseen problem nuclei.
more that 80% precision), than the previous set-
ting, showing that excitation templates and trouble
expressions are crucial in achieving high perfor-
mance especially for unseen problem nuclei. The
same was confirmed when we removed excitation
polarity and trouble expression related features,
with performance dropping by 7.43 points in terms
of average precision. The improvement in pre-
cision when using TR&EX is statistically signif-
icant (p < 0.01). This implies, assuming that we
have a wide-coverage dictionary of templates with
excitation polarities, that excitation polarities are
important in dealing with unexpected problems in
disaster situations.
We also evaluated the aid-message recognizer,
using tweet-nucleus pairs in R and T as train-
ing and test data and the annotation scheme was
also the same. The average Fleiss? kappa score
was 0.55 (moderate agreement). Our recognizer
achieved 53.82% recall and 65.67% precision and
showed similar tendencies with the problem report
recognizer, with the excitation polarities and trou-
ble expressions contributing to higher accuracy.
We can conclude that excitation polarities and
trouble expressions are important in identifying
problem reports and aid messages during disaster
situations.
5.2 Problem-Aid Matching
Next, we evaluated the performance of the
problem-aid match recognizer. We applied our
problem report recognizer and aid message recog-
nizer to all 55 million tweets and combined the
tweet-nucleus pairs judged as problem reports and
aid messages, respectively, to create the training
and test data.
The training data consists of two parts (M1 and
M2). M1 includes many variations of the aid
messages for each problem report, while M2 en-
1625
sures diversity in nouns and templates in problem
nuclei. For M1, we randomly picked up problem
reports from the output of the problem report rec-
ognizer and to each we attached up to 30 randomly
picked, distinct aid messages that have the same
nucleus noun. Building M2 follows the construc-
tion method of M1, except that: (1) we used up
to 30 distinct problem nuclei for each noun; (2)
for each problem report we attached only one ran-
domly picked aid message.
In creating the test data T2, we followed the
construction method used for M2 to assess the
performance of our proposal with a large variety
of problems. M1, M2 and T2 consist of 3,000,
6,000 and 1,000 samples, respectively. The an-
notation was done by majority vote of three hu-
man annotators (other than the authors), the aver-
age Fleiss? kappa-score for training and test data
was 0.63 (substantial agreement).
We trained the problem-aid match recognizers
of Table 5 with M1 and M2. The evaluation
results performed on T2 are shown in Table 6.
We can observe that, among the nuclei related
features, the trouble expression (TR) and excita-
tion polarity (EX) features and their combination
(TR&EX) contribute most to the performance, al-
though the contribution of nuclei related features
is less in comparison to the problem report and aid
message recognition. The improvement in preci-
sion when using TR&EX is marginally significant
(p = 0.056). Instead, morphological and syntactic
analysis (MSA) and semantic word class (SWC)
features greatly improved performance.
As the final experiments, we evaluated top-
ranking matches of our problem-aid match recog-
nizer, where the recognizer classified all the possi-
ble combinations of tweet-nuclei pairs taken from
55 million tweets. In addition, we assessed the ef-
fectiveness of excitation polarities and trouble ex-
pressions by comparing all positive matches pro-
duced by our full problem-aid match recognizer
(PROPOSED) and those produced by the problem-
aid match recognizer (PROPOSED-TR&EX) that
PROPOSED: Our proposed method with all features used.
PROPOSED-*: The proposed method without the feature set de-
noted by ?*?. Here also EX and TR denote all excitation
polarity and trouble expression related features, respec-
tively, including their combinations (TREX1 and TREX2).
RULE-BASED: The method that judges only problem-aid nuclei
combinations with opposite excitation polarities as proper
matches.
Table 5: Evaluated problem-aid match recogniz-
ers.
Matching system R (%) P (%) F (%) aP (%)
PROPOSED 30.67 70.42 42.92 55.16
PROPOSED-TR&EX 28.83 67.14 40.33 53.99
PROPOSED-EX 31.29 67.11 42.68 54.19
PROPOSED-TR 30.56 69.33 42.42 54.85
PROPOSED-MSA 13.50 53.66 21.57 44.52
PROPOSED-SWC 26.99 67.69 38.59 52.23
PROPOSED-WSP 30.61 69.51 42.50 54.81
PROPOSED-CTP 30.06 70.00 42.05 54.94
PROPOSED-SIM 29.95 70.11 41.97 54.98
PROPOSED-REQ 30.58 70.25 42.61 54.67
PROPOSED-GL 30.61 70.31 42.65 55.02
PROPOSED-SSR 30.67 69.44 42.72 54.91
RULE-BASED 15.33 17.36 16.28 n/a
Table 6: Recall (R), precision (P), F-score (F) and
average precision (aP) of the problem-aid match
recognizers.
did not use excitation polarities and trouble ex-
pressions in its feature set. Note that PROPOSED-
TR&EX was fed by the problem report and aid
message recognizers that didn?t use excitation po-
larities and trouble expressions. For both systems?
training data we used R for the problem report
and aid message recognizers; M1 and M2 for the
problem-aid matching recognizers.
PROPOSED and PROPOSED-TR&EX output 15.2
million and 13.4 million positive matches, cover-
ing 1,691 and 1,442 nucleus nouns, respectively.
Table 7 shows match samples identified with PRO-
POSED. We observed that the output of each sys-
tem was dominated by just a handful of frequent
nucleus nouns, such as ?water? or ?gasoline?. We
preferred to assess the performance of our system
against a large variation of problem-aid nuclei,
thus we restricted the number of matches to 10
for each noun10. After this restriction the number
of matches found by PROPOSED and PROPOSED-
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000  6000  7000  8000  9000
Pre
cisi
on
Rank
PROPOSED (unseen)PROPOSED-TR&EX (unseen)PROPOSED (all)PROPOSED-TR&EX (all)
Figure 3: Problem-aid match recognition perfor-
mance for ?all? and ?unseen? problem reports.
10Note that this setting is a pessimistic estimation of our
system?s overall performance, since according to our obser-
vations problem reports with very frequent nucleus nouns had
proper matches with a higher accuracy than problem reports
with less frequent nucleus nouns.
1626
Problem report: ???????????????????
??????????????????????????
??????????????????????????
(Starting from the 17th, the Iwaki Joban Hospital, the Iwaki
Urology Clinique, the Takebayashi Sadakichi Memorial Clin-
ique and the Izumi Central Clinique have all suspended dial-
ysis sessions. Patients are advised to urgently make contact.)
Aid message: ???????????????????
??????????????????????????
(Restart of dialysis sessions: short dialysis sessions are
available at the Iwaki Urology Clinique between 9 AM and
4 PM.)
Problem report: ??????????????????
?????????????????????????
?????????????????????????
?????
(Please spread this message. According to my father in
Sendai, there are more and more people whose phones ran
out of battery. We need phone chargers!)
Aid message: ???????????????????
???????????
([Please spread] At the City Hall of Wakabayashi-ku, Sendai,
you can recharge your phone battery.)
Table 7: Examples from the output of the proposed
method in the ?all? setting. Problem report and aid
message nuclei are boldfaced in the English trans-
lations.
TR&EX was 8,484 and 7,363, respectively.
The performance of PROPOSED and
PROPOSED-TR&EX were assessed in two
settings: ?all? and ?unseen?. For ?all?, we selected
400 problem-aid matches from the outputs of the
respective systems after applying the 10-match
restriction. For ?unseen?, first we removed the
samples from the systems? outputs if either the
nucleus noun or template pair appear in the nuclei
of the problem-aid match recognizers? training
data. Next we applied the same sampling process
as with ?all?. Three annotators (other than the
authors) manually labeled the sample sets, final
judgment being made by majority vote. The
Fleiss? kappa score for all test data was 0.73
(substantial agreement).
Figure 3 shows the systems? precision curves,
drawn from the samples whose X-axis positions
represent the ranks according to SVM scores. In
both scenarios we can confirm that excitation po-
larity and trouble expression related features con-
tribute to this task. In the ?all? setting in terms
of average precision calculated over the top 7,200
matches, PROPOSED?s 62.36% is 10.48 points
higher than that of PROPOSED-TR&EX. For un-
seen problem/aid nuclei PROPOSED method?s av-
erage precision of 58.57% calculated at the top
3,800 matches is 5.47 points higher than that of
PROPOSED-TR&EX at the same data point. The
improvement in precision when using TR&EX is
statistically significant in both settings (p < 0.01).
6 Related Work
Twitter has been observed as a platform for situ-
ational awareness during various crisis situations
(Starbird et al, 2010; Vieweg et al, 2010), as sen-
sors for an earthquake reporting system (Sakaki et
al., 2010; Okazaki and Matsuo, 2010) or to de-
tect epidemics (Aramaki et al, 2011). Besides
Twitter, blogs or forums have also been the tar-
get of community response analysis (Qu et al,
2009; Torrey et al, 2007). Similar to our work
are the ones of Neubig et al (2011) and Ishino et
al. (2012), who tackle specific problems that occur
during disasters (i.e., safety information and trans-
portation information, respectively); and Munro
(2011), who extracted ?actionable messages? (re-
quests and aids, indiscriminately), matching being
performed manually. Our work differs from (Neu-
big et al, 2011) and (Ishino et al, 2012) in that we
do not restrict the range of problem reports, and as
opposed to (Munro, 2011), matching is automatic.
Systems such as that of Seki (2011)11 or Munro
(2013)12 are successful examples of crisis crowd-
sourcing, but these require extensive human inter-
vention to coordinate useful information.
Another category of related work relevant to our
task is troubleshooting. Baldwin et al (2007) and
Raghavan et al (2010) use discussion forums to
solve technical problems using supervised learn-
ing methods, but these approaches presume that
the solution of a specific problem is within the
same thread. In our work we do not employ struc-
tural characteristics of tweets as restrictions (e.g.,
a problem report and its aid message need to be in
the same tweet chain).
7 Conclusions
In this paper, we proposed a method to dis-
cover matches between problem reports and aid
messages from tweets in large-scale disasters.
Through a series of experiments, we demonstrated
that the performance of the problem-aid match-
ing can be improved with the usage of seman-
tic orientation of excitation polarities, proposed in
(Hashimoto et al, 2012), and trouble expressions.
We are planning to deploy our system and re-
lease model files of the classifiers to assist relief
efforts in future crisis scenarios.
11http://www.sinsai.info/
12http://www.mission4636.org/
1627
References
Adam Acar and Yuya Muraki. 2011. Twitter for cri-
sis communication: Lessons learned from Japan?s
tsunami disaster. International Journal of Web
Based Communities, 7(3):392?402.
Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the flu: Detecting influenza
epidemics using Twitter. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2011), pages 1568?1576.
Timothy Baldwin, David Martinez, and Richard B.
Penman. 2007. Automatic thread classification for
Linux user forum information access. In Proceed-
ings of the 12th Australasian Document Computing
Symposium (ADCS 2007), pages 72?79.
Stijn De Saeger, Kentaro Torisawa, and Jun?ichi
Kazama. 2008. Looking for trouble. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (COLING 2008), pages 185?
192.
Stijn De Saeger, Kentaro Torisawa, Masaaki Tsuchida,
Jun?ichi Kazama, Chikara Hashimoto, Ichiro Ya-
mada, Jong-Hoon Oh, Istva?n Varga, and Yulan Yan.
2011. Relation acquisition using word classes and
partial patterns. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2011), pages 825?835.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 5:378?382.
Chikara Hashimoto, Kentaro Torisawa, Stijn
De Saeger, Jong-Hoon Oh, and Jun?ichi Kazama.
2012. Excitatory or inhibitory: A new semantic
orientation extracts contradiction and causality from
the web. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL 2012), pages 619?630.
Aya Ishino, Shuhei Odawara, Hidetsugu Nanba, and
Toshiyuki Takezawa. 2012. Extracting transporta-
tion information and traffic problems from tweets
during a disaster: Where do you evacuate to? In
Proceedings of the Second International Conference
on Advances in Information Mining and Manage-
ment (IMMM 2012), pages 91?96.
Hiroshi Kanayama and Tetsuya Nasukawa. 2008. Tex-
tual demand analysis: Detection of users? wants and
needs from opinions. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (COLING 2008), pages 409?416.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Induc-
ing gazetteers for named entity recognition by large-
scale clustering of dependency relations. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08: HLT), pages 407?
415.
Jun?ichi Kazama, Stijn De Saeger, Kow Kuroda,
Masaki Murata, and Kentaro Torisawa. 2010. A
Bayesian method for robust estimation of distribu-
tional similarities. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2010), pages 247?256.
Jun?ichi Kazama, Stijn De Saeger, Kentaro Torisawa,
Jun Goto, and Istva?n Varga. 2013. Saigaiji jouhou
e no shitsumon outo shisutemu no tekiyou no koko-
romi. (An attempt for applying question-answering
system on disaster related information). In Pro-
ceeding of the Nineteenth Annual Meeting of The
Association for Natural Language Processing. (in
Japanese).
Benjamin Mandel, Aron Culotta, John Boulahanis,
Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue.
2012. A demographic analysis of online sentiment
during Hurricane Irene. In Proceedings of the Sec-
ond Workshop on Language Analysis in Social Me-
dia (LASM 2012), pages 27?36.
Robert Munro. 2011. Subword and spatiotempo-
ral models for identifying actionable information in
Haitian Kreyol. In Proceedings of the Fifteenth
Conference on Computational Natural Language
Learning (CoNLL-2011), pages 68?77.
Robert Munro. 2013. Crowdsourcing and the
crisis-affected community. Information Retrieval,
16(2):210?266.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using CRFs with hidden variables. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics (NAACL HLT
2010), pages 786?794.
National Police Agency of Japan. 2013. Damage sit-
uation and public countermeasures associated with
2011 Tohoku district ? off the Pacific Ocean Earth-
quake. http://www.npa.go.jp/archive/
keibi/biki/higaijokyo_e.pdf. (accessed
on 30 April, 2013).
Graham Neubig, Yuichiroh Matsubayashi, Masato
Hagiwara, and Koji Murakami. 2011. Safety infor-
mation mining? what can NLP do in a disaster?.
In Proceedings of the 5th International Joint Con-
ference on Natural Language Processing (IJCNLP
2011), pages 965?973.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Takuya Kawada, Stijn De Saeger, Jun?ichi Kazama,
and Yiou Wang. 2012. Why question answering
using sentiment analysis and word classes. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL 2012), pages 368?378.
1628
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake.
2013. Why-question answering using intra- and
inter-sentential causal relations. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2013).
Kiyonori Ohtake, Kentaro Torisawa, Jun Goto, and
Stijn De Saeger. 2013. Saigaiji ni okeru hi-
saisha to kyuuen kyuujosha kan no souhoko komyu-
nikeeshon. (Bi-directional communication between
victims and rescures during a crisis). In Proceeding
of the Nineteenth Annual Meeting of The Association
for Natural Language Processing. (in Japanese).
Makoto Okazaki and Yutaka Matsuo. 2010. Seman-
tic Twitter: Analyzing tweets for real-time event
notification. In Proceedings of the 2008/2009 in-
ternational conference on Social software: Re-
cent trends and developments in social software
(BlogTalk 2008), pages 63?74.
R. Lyman Ott and Michael T. Longnecker, 2010. An
Introduction to Statistical Methods and Data Analy-
sis, chapter 10.2. Brooks Cole, 6th edition.
Yan Qu, Philip Fei Wu, and Xiaoqing Wang. 2009.
Online community response to major disaster: A
study of Tianya forum in the 2008 Sichuan Earth-
quake. In 42st Hawaii International International
Conference on Systems Science (HICSS-42), pages
1?11.
Preethi Raghavan, Rose Catherine, Shajith Ikbal,
Nanda Kambhatla, and Debapriyo Majumdar. 2010.
Extracting problem and resolution information from
online discussion forums. In Proceedings of the
16th International Conference on Management of
Data (COMAD 2010).
Takeo Saijo. 2012. Hito-o tasukeru sungoi shikumi. (A
stunning system that saves people). Diamond Inc.
(in Japanese).
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: Real-time
event detection by social sensors. In Proceedings
of the 19th International Conference on World Wide
Web (WWW 2010), pages 851?860.
Motoki Sano, Istva?n Varga, Jun?ichi Kazama, and Ken-
taro Torisawa. 2012. Requests in tweets dur-
ing a crisis: A systemic functional analysis of
tweets on the Great East Japan Earthquake and
the Fukushima Daiichi nuclear disaster. In Pa-
pers from the 39th International Systemic Func-
tional Congress (ISFC39), pages 135?140.
Haruyuki Seki. 2011. Higashi-nihon daishinsai fukkou
shien platform sinsai.info no naritachi to kongo no
kadai. (The organizational structure of sinsai.info
restoration support platform for the 2011 Great East
Japan Earthquake and future challenges). Journal of
digital practices, 2(4):237?241. (in Japanese).
Kate Starbird, Leysia Palen, Amanda L. Hughes, and
Sarah Vieweg. 2010. Chatter on the red: What
hazards threat reveals about the social life of mi-
croblogged information. In Proceedings of The
2010 ACM Conference on Computer Supported Co-
operative Work (CSCW 2010), pages 241?250.
Cristen Torrey, Moira Burke, Matthew L. Lee,
Anind K. Dey, Susan R. Fussell, and Sara B. Kiesler.
2007. Connected giving: Ordinary people coordi-
nating disaster relief on the Internet. In Proceedings
of the 40th Annual Hawaii International Conference
on System Sciences (HICSS-40), pages 179?188.
Katerina Tsagkalidou, Vassiliki Koutsonikola, Athena
Vakali, and Konstantinos Kafetsios. 2011. Emo-
tional aware clustering on micro-blogging sources.
In Proceedings of the 4th international conference
on Affective computing and intelligent interaction
(ACII 2011), pages 387?396.
Sarah Vieweg, Amanda L. Hughes, Kate Starbird, and
Leysia Palen. 2010. Microblogging during two nat-
ural hazards events: What Twitter may contribute
to situational awareness. In Proceedings of the
SIGCHI Conference on Human Factors in Comput-
ing Systems (CHI 2010), pages 1079?1088.
Patrick Winn. 2011. Japan tsunami disaster: As Japan
scrambles, Twitter reigns. GlobalPost, 18 March.
1629
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1733?1743,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Why-Question Answering
using Intra- and Inter-Sentential Causal Relations
Jong-Hoon Oh? Kentaro Torisawa? Chikara Hashimoto ? Motoki Sano?
Stijn De Saeger? Kiyonori Ohtake?
Information Analysis Laboratory
Universal Communication Research Institute
National Institute of Information and Communications Technology (NICT)
{?rovellia,? torisawa,? ch,? msano,?stijn,?kiyonori.ohtake}@nict.go.jp
Abstract
In this paper, we explore the utility of
intra- and inter-sentential causal relations
between terms or clauses as evidence for
answering why-questions. To the best of
our knowledge, this is the first work that
uses both intra- and inter-sentential causal
relations for why-QA. We also propose
a method for assessing the appropriate-
ness of causal relations as answers to a
given question using the semantic orienta-
tion of excitation proposed by Hashimoto
et al (2012). By applying these ideas
to Japanese why-QA, we improved preci-
sion by 4.4% against all the questions in
our test set over the current state-of-the-
art system for Japanese why-QA. In addi-
tion, unlike the state-of-the-art system, our
system could achieve very high precision
(83.2%) for 25% of all the questions in the
test set by restricting its output to the con-
fident answers only.
1 Introduction
?Why-question answering? (why-QA) is a task to
retrieve answers from a given text archive for a
why-question, such as ?Why are tsunamis gen-
erated?? The answers are usually text fragments
consisting of one or more sentences. Although
much research exists on this task (Girju, 2003;
Higashinaka and Isozaki, 2008; Verberne et al,
2008; Verberne et al, 2011; Oh et al, 2012), its
performance remains much lower than that of the
state-of-the-art factoid QA systems, such as IBM?s
Watson (Ferrucci et al, 2010).
In this work, we propose a quite straightfor-
ward but novel approach for such difficult why-
QA task. Consider the sentence A1 in Table 1,
which represents the causal relation between the
cause, ?the ocean?s water mass ..., waves are gen-
A1 [Tsunamis that can cause large coastal inundation
are generated]effect because [the ocean?s water
mass is displaced and, much like throwing a stone
into a pond, waves are generated.]cause
A2 [Earthquake causes seismic waves which set up
the water in motion with a large force.]cause
This causes [a tsunami.]effect
A3 [Tsunamis]effect are caused by [the sudden dis-
placement of huge volumes of water.]cause
A4 [Tsunamis weaken as they pass through
forests]effect because [the hydraulic resistance of
the trees diminish their energy.]cause
A5 [Automakers in Japan suspended production for an
array of vehicles]effect because [the magnitude 9
earthquake and tsunami hit their country on Friday,
March 11, 2011.]cause
Table 1: Examples of intra/inter-sentential causal
relations. Cause and effect parts of each causal re-
lation, marked with [..]cause and [..]effect, are con-
nected by the underlined cue phrases for causality,
such as because, this causes, and are caused by.
erated,? and its effect, ?Tsunamis ... are gener-
ated.? This is a good answer to the question, ?Why
are tsunamis generated??, since the effect part is
more or less equivalent to the (propositional) con-
tent of the question. Our method finds text frag-
ments that include such causal relations with an
effect part that resembles a given question and pro-
vides them as answers.
Since this idea looks quite intuitive, many peo-
ple would probably consider it as a solution to
why-QA. However, to our surprise, we could not
find any previous work on why-QA that took this
approach. Some methods utilized the causal re-
lations between terms as evidence for finding an-
swers (i.e., matching a cause term with an answer
text and its effect term with a question) (Girju,
2003; Higashinaka and Isozaki, 2008). Other ap-
proaches utilized such clue terms for causality as
?because? as evidence for finding answers (Mu-
rata et al, 2007). However, these algorithms did
not check whether an answer candidate, i.e., a text
fragment that may be provided as an answer, ex-
plicitly contains a complex causal relation sen-
1733
tence with the effect part that resembles a ques-
tion. For example, A5 in Table 1 is an incorrect an-
swer to ?Why are tsunamis generated??, but these
previous approaches would probably choose it as a
proper answer due to ?because? and ?earthquake?
(i.e., a cause of tsunamis). At least in our exper-
imental setting, our approach outperformed these
simpler causality-based QA systems.
Perhaps this approach was previously deemed
infeasible due to two non-trivial technical chal-
lenges. The first challenge is to accurately iden-
tify a wide range of causal relations like those in
Table 1 in answer candidates. To meet this chal-
lenge, we developed a sequence labeling method
that identifies not only intra-sentential causal re-
lations, i.e., the causal relations between two
terms/phrases/clauses expressed in a single sen-
tence (e.g., A1 in Table 1), but also the inter-
sentential causal relations, which are the causal
relations between two terms/phrases/clauses ex-
pressed in two adjacent sentences (e.g., A2) in a
given text fragment.
The second challenge is assessing the appropri-
ateness of each identified causal relation as an an-
swer to a given question. This is important since
the causal relations identified in the answer candi-
dates may have nothing to do with a given ques-
tion. In this case, we have to reject these causal
relations because they are inappropriate as an an-
swer to the question. When a single answer candi-
date contains many causal relations, we also have
to select the appropriate ones. Consider the causal
relations in A1?A4. Those in A1?A3 are appro-
priate answers to ?Why are tsunamis generated??,
but not the one in A4. To assess the appropri-
ateness, the system must recognize textual entail-
ment, i.e., ?tsunamis (are) generated? in the ques-
tion is entailed by all ?tsunamis are generated? in
A1, ?cause a tsunami? in A2 and ?tsunamis are
caused? in A3 but not by ?tsunamis weaken? in
A4. This quite difficult task is currently being
studied by many researchers in the RTE field (An-
droutsopoulos and Malakasiotis, 2010; Dagan et
al., 2010; Shima et al, 2011; Bentivogli et al,
2011). To meet this challenge, we developed a
relatively simple method that can be seen as a
lightweight approximation for this difficult RTE
task, using excitation polarities (Hashimoto et al,
2012).
Through our experiments on Japanese why-QA,
we show that a combination of the above methods
can improve why-QA accuracy. In addition, our
proposed method can be successfully combined
with other approaches to why-QA and can con-
tribute to higher accuracy. As a final result, we im-
proved the precision by 4.4% against all the ques-
tions in our test set over the current state-of-the-art
system of Japanese why-QA (Oh et al, 2012). The
difference in the performance became much larger
when we only compared the highly confident an-
swers of each system. When we made our sys-
tem provide only its confident answers according
to their confidence score given by our system, the
precision of these confident answers was 83.2%
for 25% of all the questions in our test set. In the
same setting, the precision of the state-of-the-art
system (Oh et al, 2012) was only 62.4%.
2 Related Work
Although there were many previous works on the
acquisition of intra- and inter-sentential causal re-
lations from texts (Khoo et al, 2000; Girju, 2003;
Inui and Okumura, 2005; Chang and Choi, 2006;
Torisawa, 2006; Blanco et al, 2008; De Saeger et
al., 2009; De Saeger et al, 2011; Riaz and Girju,
2010; Do et al, 2011; Radinsky et al, 2012), their
application to why-QA was limited to causal re-
lations between terms (Girju, 2003; Higashinaka
and Isozaki, 2008).
As previous attempts to improve why-QA per-
formance, such semantic knowledge as Word-
Net synsets (Verberne et al, 2011), semantic
word classes (Oh et al, 2012), sentiment analy-
sis (Oh et al, 2012), and causal relations between
terms (Girju, 2003; Higashinaka and Isozaki,
2008) has been used. These previous studies took
basically bag-of-words approaches and used the
semantic knowledge to identify certain seman-
tic associations using terms and n-grams. On
the other hand, our method explicitly identifies
intra- and inter-sentential causal relations between
terms/phrases/clauses that have complex struc-
tures and uses the identified relations to answer
a why-question. In other words, our method
considers more complex linguistic structures than
those used in the previous studies. Note that our
method can complement the previous approaches.
Through our experiments, we showed that it is
possible to achieve a higher precision by combin-
ing our proposed method with bag-of-words ap-
proaches considering semantic word classes and
sentiment analysis in our previous work (Oh et al,
1734
Document	 ?retrieval	 ?from	 ?Japanese	 ?web	 ?texts	 ?
Answer	 ?candidate	 ?extrac?on	
Answer	 ?candidate	 ?extrac?on	 ?	 ?from	 ?the	 ?retrieved	 ?documents	 ?
Answer	 ?re-??ranker	 ?
Answer	 ?re-??ranking	
top-n answer	 ?candidates	 ?by	 ?answer	 ?re-??ranking	 ?
Training	 ?data	 ?for	 ?answer	 ?re-??ranking	 ?
Training	 ?data	 ?for	 ?causal	 ?rela?on	 ?recogni?on	 ?
Causal	 ?rela?on	 ?recogni?on	 ?model	 ?top-n answer	 ?candidates	 ?
training	 ?
training	 ?
Why-??ques?on	 ?
recogni?on	 ?	 ?
recogni?on	 ?	 ?
Figure 1: System architecture
2012).
3 System Architecture
We first describe the system architecture of
our QA system before describing our proposed
method. It is composed of two components: an-
swer candidate extraction and answer re-ranking
(Fig. 1). This architecture is basically the same as
that used in our previous work (Oh et al, 2012).
We extended our previous work by introducing
causal relations recognized from answer candi-
dates to the answer re-ranking. The features used
in our previous work are very different from those
in this work, and we found that combining both
improves accuracy.
Answer candidate extraction: In our previous
work, we implemented the method of Murata et
al. (2007) for our answer candidate extractor. We
retrieved documents from Japanese web texts us-
ing Boolean AND and OR queries generated from
the content words in why-questions. Then we ex-
tracted passages of five sentences from these re-
trieved documents and ranked them with the rank-
ing function proposed by Murata et al (2007).
This method ranks a passage higher when it con-
tains more query terms that are closer to each other
in the passage. We used a set of clue terms, includ-
ing the Japanese counterparts of cause and reason,
as query terms for the ranking. The top ranked
passages are regarded as answer candidates in the
answer re-ranking. See Murata et al (2007) for
more details.
Answer re-ranking: Re-ranking the answer
candidates is done by a supervised classifier
(SVMs) (Vapnik, 1995). In our previous work, we
employed three types of features for training the
re-ranker: morphosyntactic features (n-grams of
morphemes and syntactic dependency chains), se-
mantic word class features (semantic word classes
obtained by automatic word clustering (Kazama
and Torisawa, 2008)) and sentiment polarity fea-
tures (word and phrase polarities). Here, we used
semantic word classes and sentiment polarities for
identifying such semantic associations between a
why-question and its answer as ?if a disease?s
name appears in a question, then answers that in-
clude nutrient names are more likely to be correct?
by semantic word classes and ?if something un-
desirable happens, the reason is often also some-
thing undesirable? by sentiment polarities. In this
work, we propose causal relation features gener-
ated from intra- and inter-sentential causal rela-
tions in answer candidates and use them along
with the features proposed in our previous work
for training our re-ranker.
4 Causal Relations for Why-QA
We describe causal relation recognition in Sec-
tion 4.1 and describe the features (of our re-ranker)
generated from causal relations in Section 4.2.
4.1 Causal Relation Recognition
We restrict causal relations to those expressed by
such cue phrases for causality as (the Japanese
counterparts of) because and as a result like in
the previous work (Khoo et al, 2000; Inui and
Okumura, 2005) and recognize them in the fol-
lowing two steps: extracting causal relation candi-
dates and recognizing causal relations from these
candidates.
4.1.1 Extracting Causal Relation Candidates
We identify cue phrases for causality in answer
candidates using the regular expressions in Ta-
ble 2. Then, for each identified cue phrase, we
extract three sentences as a causal relation candi-
date, where one contains the cue phrase and the
other two are the previous and next sentences in
the answer candidates. When there is more than
one cue phrase in an answer candidate, we use
all of them for extracting the causal relation can-
didates, assuming that each of the cue phrases is
linked to different causal relations. We call a cue
phrase used for extracting a causal relation candi-
date a c-marker (causality marker) of the candi-
date to distinguish it from the other cue phrases in
the same causal relation candidate.
1735
Regular expressions Examples
(D|?)? ?? P? ?? (for),??? (for),????
(as a result),???? (for)
?? ?? (since or because of)
?? (??|?) ???? (from the fact that),??
? (by the fact that)
(??|??) C ??? (because),??? (It is be-
cause)
D? RCT (P|C)+ ??? (the reason is), ???
(is the cause),?????? (from
this reason)
Table 2: Regular expressions for identifying cue
phrases for causality. D, P and C represent
demonstratives (e.g., ?? (this) and ?? (that)),
postpositions (including case markers such as ?
(nominative), ? (genitive)), and copula (e.g., ?
? (is) and ??? (is)) in Japanese, respectively.
RCT, which represents Japanese terms meaning
reason, cause, or thanks to, is defined as fol-
lows: RCT = {?? (reason), ?? (cause), ?
? (cause), ??? (cause), ??? (thanks to),
?? (thanks to),?? (reason) }.
4.1.2 Recognizing Causal Relations
Next, we recognize the spans of the cause and ef-
fect parts of a causal relation linked to a c-marker.
We regard this task as a sequence labeling problem
and use Conditional Random Fields (CRFs) (Laf-
ferty et al, 2001) as a machine learning frame-
work. In our task, CRFs take three sentences
of a causal relation candidate as input and gen-
erate their cause-effect annotations with a set of
possible cause-effect IOB labels, including Begin-
Cause (B-C), Inside-Cause (I-C), Begin-Effect (B-
E), Inside-Effect (I-E), and Outside (O). Fig 2
shows an example of such sequence labeling. Al-
though this example is about sequential labeling
shown on English sentences for ease of explana-
tion, it was actually done on Japanese sentences.
We used the three types of feature sets in Table 3
for training the CRFs, where j is in the range of
i? 4 ? j ? i+4 for current position i in a causal
relation candidate.
Type Features
Morphological feature mj , mj+1j , posj , posj+1j
Syntactic feature sj , sj+1j , bj , bj+1j
C-marker feature (mj , cm), (mj+1j , cm)
(sj , cm), (sj+1j , cm)
Table 3: Features for training CRFs, where
xj+1j = xjxj+1
Morphological features: mj and posj in Ta-
ble 3 represent the jth morpheme and the POS tag.
S1:	 ?Earthquake	 ?causes	 ?seismic	 ?waves	 ?which	 ?set	 ?up	 ?the	 ?water	 ?in	 ?mo?on	 ?with	 ?a	 ?large	 ?force.	 ?EOS	 ?S2:	 ?This	 ?causes	 ?a	 ?tsunami.	 ?EOS	 ?S3:	 ?EOA	 ?
S1	 ? Earthquake	 ? causes	 ? ?	 ? with	 ? a	 ? large	 ? force	 ? .	 ? EOS	 ?IOB	 ? B-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? O	 ?S2	 ? This	 ? causes	 ? a	 ? tsunami	 ? .	 ? EOS	 ?IOB	 ? O	 ? O	 ? B-??E	 ? I-??E	 ? I-??E	 ? O	 ?S3	 ? EOA	 ?IOB	 ? O	 ?
CRFs	 ?
A	 ?causal	 ?rela?on	 ?candidate	 ?from	 ?A2	 ?
Figure 2: Recognizing causal relations by se-
quence labeling: Underlined text This causes rep-
resents a c-marker, and EOS and EOA represent
end-of-sentence and end-of-answer candidates.
??	 ? ??	 ? ???	 ? ???	 ? ???????	 ? ???	 ? ??	 ? ????????	 ?
water	 ? ice	 ? if	 ?(it)	 ?becomes	 ? its	 ?volume	 ? because	 ?(it)	 ?	 ?increases	 ? an	 ?iceberg	 ? water	 ? float	 ?on	 ?(water)	 ?
Subtree	 ?informa?on	 ?used	 ?for	 ?syntac?c	 ?features	 ?	 ?
subtree	 ? subtree	 ? child	 ? child	 ? c-??marker	 ? subtree-??of-??parent	 ?
subtree-??of-??parent	 ?
parent	 ?
???????	 ?
???	 ?
[??????????????]cause???[?????????????]effect	 ?(Because	 ?[the	 ?volume	 ?of	 ?the	 ?water	 ?increases	 ?if	 ?it	 ?becomes	 ?ice]cause,	 ?[an	 ?iceberg	 ?floats	 ?on	 ?water]effect.)	 ?
????????	 ?
root	 ?
c-??marker	 ?node	 ?
Figure 3: Example of syntactic information related
to a c-marker used for syntactic features
We use JUMAN1, a Japanese morphological ana-
lyzer, for generating our morphological features.
Syntactic features: The span of the causal rela-
tions in a given causal relation candidate strongly
depends on the c-marker in the candidate. Es-
pecially for intra-sentential causal relations, their
cause and effect parts often appear in the subtrees
of the c-marker?s node or those of the c-marker?s
parent node in a syntactic dependency tree struc-
ture. Fig. 3 shows an example that follows this ob-
servation, where the c-marker node is represented
in a hexagon and the other nodes are in a rectan-
gle. Note that each node in Fig. 3 is a word phrase
(called a bunsetsu), which is the smallest unit of
syntactic analysis in Japanese. A bunsetsu is a
syntactic constituent composed of a content word
and several function words such as postpositions
and case markers. Syntactic dependency is repre-
sented by an arrow in Fig. 3. For example, there
is syntactic dependency from word phrase ??
1 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
1736
(water) to??? (if (it) becomes), i.e.,?? dep???
???. We encode this subtree information into
sj , which is the syntactic information of a word
phrase to which the jth morpheme belongs. sj
only has one of six values: 1) the c-marker?s node
(c-marker), 2) the c-marker?s child node (child),
3) the c-marker?s parent node (parent), 4) in the c-
marker?s subtree but not the c-marker?s child node
(subtree), 5) in the subtree of the c-marker?s par-
ent node but not the c-marker?s node (subtree-of-
parent) and 6) the others (others). bj is the word
phrase information of the jth morpheme (mj) that
represents whether mj is in the beginning or in-
side a word phrase. For generating our syntactic
features, we use KNP2, a Japanese syntactic de-
pendency parser.
C-marker features: As our c-marker features,
we use a pair composed of c-marker cm and one
of the following: mj , mj+1j , sj , or sj+1j .
4.2 Causal Relation Features
We use terms, partial trees (in a syntactic depen-
dency tree structure), and the semantic orienta-
tion of excitation (Hashimoto et al, 2012) to as-
sess the appropriateness of each causal relation ob-
tained by our causal relation recognizer as an an-
swer to a given question. Finding answers with
term matching and partial tree matching has been
used in the literature of question answering (Girju,
2003; Narayanan and Harabagiu, 2004; Moschitti
et al, 2007; Higashinaka and Isozaki, 2008; Ver-
berne et al, 2008; Surdeanu et al, 2011; Verberne
et al, 2011; Oh et al, 2012), while that with the
excitation polarity is proposed in this work.
We use three types of features. Each fea-
ture type expresses the causal relations in an an-
swer candidate that are determined to be appro-
priate as answers to a given question by term
matching (tf1?tf4), partial tree matching (pf1?
pf4) and excitation polarity matching (ef1?ef4).
We call these causal relations used for generating
our causal relation features candidates of an ap-
propriate causal relation in this section. Note that
if one answer candidate has more than one candi-
date of an appropriate causal relation found by one
matching method, we generated features for each
appropriate candidate and merged all of them for
the answer candidate.
2 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP
Type Description
tf1 word n-grams of causal relations
tf2 word class version of tf1
tf3 indicator for the existence of candidates of an
appropriate causal relation identified by term
matching in an answer candidate
tf4 number of matched terms in candidates of an ap-
propriate causal relation
pf1 syntactic dependency n-grams (n dependency
chain) of causal relations
pf2 word class version of pf1
pf3 indicator for the existence of candidates of an ap-
propriate causal relation identified by partial tree
matching in an answer candidate
pf4 number of matched partial trees in candidates of
an appropriate causal relation
ef1 types of noun-polarity pairs shared by causal re-
lations and the question
ef2 ef1 coupled with each noun?s word class
ef3 indicator for the existence of candidates of an ap-
propriate causal relation identified by excitation
polarity matching in an answer candidate
ef4 number of noun-polarity pairs shared by the
question and the candidates of an appropriate
causal relation
Table 4: Causal relation features: n in n-grams
is n = {2, 3} and n-grams in an effect part are
distinguished from those in a cause part.
4.2.1 Term Matching
Our term matching method judges that a causal re-
lation is a candidate of an appropriate causal rela-
tion if its effect part contains at least one content
word (nouns, verbs, and adjectives) in the ques-
tion. For example, all the causal relations of A1?
A4 in Table 1 are candidates of an appropriate
causal relation to the question, ?Why is a tsunami
generated??, by term matching with question term
tsunami.
tf1?tf4 are generated from candidates of an ap-
propriate causal relation identified by term match-
ing. The n-grams of tf1 and tf2 are restricted
to those containing at least one content word in
a question. We distinguish this matched word
from the other words by replacing it with QW, a
special symbol representing a word in the ques-
tion. For example, word 3-gram ?this/cause/QW?
is extracted from This causes tsunamis in A2 for
?Why is a tsunami generated?? Further, we cre-
ate a word class version of word n-grams by con-
verting the words in these word n-grams into their
corresponding word class using the semantic word
classes (500 classes for 5.5 million nouns) from
our previous work (Oh et al, 2012). These word
classes were created by applying the automatic
word clustering method of Kazama and Torisawa
(2008) to 600 million Japanese web pages. For
example, the word class version of word 3-gram
1737
?this/cause/QW? is ?this/cause/QW,WCtsunami?,
where WCtsunami represents the word class of
a tsunami. tf3 is a binary feature that indi-
cates the existence of candidates of an appropri-
ate causal relation identified by term matching in
an answer candidate. tf4 represents the degree
of the relevance of the candidates of an appro-
priate causal relation measured by the number of
matched terms: one, two, and more than two.
4.2.2 Partial Tree Matching
Our partial tree matching method judges a causal
relation as a candidate of an appropriate causal re-
lation if its effect part contains at least one par-
tial tree in a question, where the partial tree covers
more than one content word. For example, only
the causal relation A1 among A1?A4 is a can-
didate of an appropriate causal relation for ques-
tion ?Why are tsunamis generated?? by partial
tree matching because only its effect part contains
partial tree ?tsunamis dep??? (are) generated? of the
question.
pf1?pf4 are generated from candidates of an
appropriate causal relation identified by the par-
tial tree matching. The syntactic dependency n-
grams in pf1 and pf2 are restricted to those that
contain at least one content word in a question. We
distinguish this matched content word from the
other content words in the n-gram by converting
it to QW, which represents a content word in the
question. For example, syntactic dependency 2-
gram ?QW dep??? cause? and its word class version
?QW,WCtsunami dep??? cause? are extracted from
Tsunamis that can cause in A1. pf3 is a binary
feature that indicates whether an answer candidate
contains candidates of an appropriate causal rela-
tion identified by partial tree matching. pf4 rep-
resents the degree of the relevance of the candi-
date of an appropriate causal relation measured by
the number of matched partial trees: one, two, and
more than two.
4.2.3 Excitation Polarity Matching
Hashimoto et al (2012) proposed a semantic ori-
entation called excitation polarities. It classifies
predicates with their argument position (called
templates) into excitatory, inhibitory and neu-
tral. In the following, we denote a template
as ?[argument position,predicate].? According to
Hashimoto?s definition, excitatory templates im-
ply that the function, effect, purpose, or the role of
an entity filling an argument position in the tem-
plates is activated/enhanced. On the contrary, in-
hibitory templates imply that the effect, purpose
or the role of an entity is deactivated/suppressed.
Neutral templates are those that neither activate
nor suppress the function of an argument.
We assume that the meanings of a text can
be roughly captured by checking whether each
noun in the text is activated or suppressed in the
sense of the excitation polarity framework, where
the activation and suppression of each entity (or
noun) can be detected by looking at the excita-
tion polarities of the templates that are filled by
the entity. For instance, effect part ?tsunamis
that can cause large coastal inundation are gen-
erated? of A1 roughly means that ?tsunamis? are
activated and ?inundation? is (or can be) acti-
vated. This activation/suppression configuration
of the nouns is consistent with sentence ?tsunamis
are caused? in which ?tsunamis? are activated.
This consistency suggests that A1 is a good an-
swer to question ?Why are tsunamis caused??, al-
though the ?tsunamis? are modified by different
predicates; ?cause? and ?generate.? On the other
hand, effect part ?tsunamis weaken as they pass
through forests? of A4 implies that ?tsunamis?
are suppressed. This suggests that A4 is not
a good answer to ?Why are tsunamis caused??
Note that the consistency checking between ac-
tivation/suppression configurations of nouns3 in
texts can be seen as a rough but lightweight ap-
proximation of the recognition of textual entail-
ments or paraphrases.
Following the definition of excitation polarity
in Hashimoto et al (2012), we manually classi-
fied templates4 to each polarity type and obtained
8,464 excitatory templates, such as [?, ???]
([subject, increase]) and [?, ????] ([sub-
ject, improve]), 2,262 inhibitory templates, such
as [?, ??] ([object, prevent]) and [?, ??]
([subject, die]), and 7,230 neutral templates such
as [?, ???] ([object, consider]). With these
templates, we obtain activation/suppression con-
figurations (including neutral) for the nouns in the
causal relations in the answer candidates and ques-
3 Because the activation/suppression configurations of
nouns come from an excitation polarity of templates, ?[argu-
ment position,predicate],? the semantics of verbs in the tem-
plates are implicitly considered in this consistency checking.
4 Varga et al (2013) has used the same templates as ours,
except they restricted their excitation/inhibitory templates to
those whose polarity is consistent with that given by the au-
tomatic acquisition method of Hashimoto et al (2012).
1738
tions.
Next, we assume that a causal relation is ap-
propriate as an answer to a question if the effect
part of the causal relation and the question share
at least one common noun with the same polarity.
More detailed information concerning the config-
urations of all the nouns in all the candidates of an
appropriate causal relation (including their cause
parts) and the question are encoded into our fea-
ture set ef1?ef4 in Table 4 and the final judgment
is done by our re-ranker.
For generating ef1 and ef2, we classified all the
nouns coupled with activation/suppression/neutral
polarities in a causal relation into three types:
SAME (the question contains the same noun with
the same polarity), DiffPOL (the question con-
tains the same noun with different polarity), and
OTHER (the others). ef1 indicates whether each
type of noun-polarity pair exists in a causal rela-
tion. Note that the types for the effect and cause
parts are represented in distinct features. ef2 is the
same as ef1 except that the types are augmented
with the word classes of the corresponding nouns.
In other words, ef2 indicates whether each type
of noun-polarity pair exists in the causal relation
for each word class. ef3 indicates the existence of
candidates of an appropriate causal relation iden-
tified by this matching scheme, and ef4 repre-
sents the number of noun-polarity pairs shared by
the question and the candidates of an appropriate
causal relations (one, two, and more than two).
5 Experiments
We experimented with causal relation recognition
and why-QA with our causal relation features.
5.1 Data Set for Why-Question Answering
For our experiments, we used the same why-QA
data set as the one used in our previous work (Oh
et al, 2012). This why-QA data set is composed
of 850 Japanese why-questions and their top-20
answer candidates obtained by answer candidate
extraction from 600 million Japanese web pages.
Three annotators checked the top-20 answer can-
didates of these 850 questions and the final judg-
ment was made by their majority vote. Their inter-
rater agreement by Fleiss? kappa reported in Oh et
al. (2012) was substantial (? = 0.634). Among the
850 questions, 250 why-questions were extracted
from the Japanese version of Yahoo! Answers,
and another 250 were created by annotators. In
our previous work, we evaluated the system with
these 500 questions and their answer candidates as
training and test data in 10-fold cross-validation.
The other 350 why-questions were manually built
from passages describing the causes or reasons of
events/phenomena. These questions and their an-
swer candidates were used as additional training
data for testing subsamples in each fold during the
10-fold cross-validation. In our why-QA experi-
ments, we evaluated our why-QA system with the
same settings.
5.2 Data Set for Causal Relation Recognition
We built a data set composed of manually anno-
tated causal relations for evaluating our causal re-
lation recognition. As source data for this data set,
we used the same 10-fold data that we used for
evaluating our why-QA (500 questions and their
answer candidates). We extracted the causal re-
lation candidates from the answer candidates in
each fold, and then our annotator (not an author)
manually marked the span of the cause and effect
parts of a causal relation for each causal relation
candidate, keeping in mind that the causal rela-
tion must be expressed in terms of a c-marker in
a given causal relation candidate. Finally, we had
a data set made of 16,051 causal relation candi-
dates, 8,117 of which had a true causal relation;
the number of intra- and inter-sentential causal re-
lations were 7,120 and 997, respectively.
Note that this data set can be partitioned into ten
folds by using the 10-fold partition of its source
data. We performed 10-fold cross validation to
evaluate our causal relation recognition with this
10-fold data.
5.3 Causal Relation Recognition
We used CRF++5 for training our causal relation
recognizer. In our evaluation, we judged a sys-
tem?s output as correct if both spans of the cause
and effect parts overlapped those in the gold stan-
dard. Evaluation was done by precision, recall,
and F1.
Precision Recall F1
BASELINE 41.9 61.0 49.7
INTRA-SENT 84.5 75.4 79.7
INTER-SENT 80.2 52.6 63.6
ALL 83.8 71.1 77.0
Table 5: Results of causal relation recognition (%)
Table 5 shows the result. BASELINE represents
5 http://code.google.com/p/crfpp/
1739
the result for our baseline system that recognizes
a causal relation by simply taking the two phrases
adjacent to a c-marker (i.e., before and after) as
cause and effect parts of the causal relation. We
assumed that the system had an oracle for judging
correctly whether each phrase is a cause part or an
effect part. In other words, we judged that a causal
relation recognized by BASELINE is correct if both
cause and effect parts in the gold standard are adja-
cent to a c-marker. INTRA-SENT and INTER-SENT
represent the results for intra- and inter-sentential
causal relations and ALL represents the result for
the both causal relations by our method. From
these results, we confirmed that our method rec-
ognized both intra- and inter-sentential causal rela-
tions with over 80% precision, and it significantly
outperformed our baseline system in both preci-
sion and recall rates.
Precision Recall F1
ALL-?MORPH? 80.8 66.4 72.9
ALL-?SYNTACTIC? 82.9 67.0 74.1
ALL-?C-MARKER? 76.3 51.4 61.4
ALL 83.8 71.1 77.0
Table 6: Ablation test results for causal relation
recognition (%)
We also investigated the contribution of the
three types of features used in our causal rela-
tion recognition to the performance. We evalu-
ated the performance when we removed one of
the three types of features (ALL-?MORPH?, ALL-
?SYNTACTIC? and ALL-?C-MARKER?) and com-
pared the results in these settings with the one
when all the feature sets were used (ALL). Ta-
ble 6 shows the result. We confirmed that all the
feature sets improved the performance, and we got
the best performance when using all of them. We
used the causal relations obtained from the 10-fold
cross validation for our why-QA experiments.
5.4 Why-Question Answering
We performed why-QA experiments to confirm
the effectiveness of intra- and inter-sentential
causal relations in a why-QA task. In
this experiment, we compared five systems:
four baseline systems (MURATA, OURCF, OH
and OH+PREVCF) and our proposed method
(PROPOSED).
MURATA corresponds to our answer candidate
extraction.
OURCF uses a re-ranker trained with only our
causal relation features.
OH, which represents our previous work (Oh et
al., 2012), has a re-ranker trained with mor-
phosyntactic, semantic word class, and senti-
ment polarity features.
OH+PREVCF is a system with a re-ranker
trained with the features used in OH and with
the causal relation feature proposed in Hi-
gashinaka and Isozaki (2008). The causal re-
lation feature includes an indicator that deter-
mines whether the causal relations between
two terms appear in a question-answer pair;
cause in an answer and its effect in a question.
We acquired the causal relation instances (be-
tween terms) from 600 million Japanese web
pages using the method of De Saeger et al
(2009) and exploited the top-100,000 causal
relation instances in this system.
PROPOSED has a re-ranker trained with our
causal relation features as well as the three
types of features proposed in Oh et al (2012).
Comparison between OH and PROPOSED re-
veals the contribution of our causal relation
features to why-QA.
We used TinySVM6 with a linear kernel
for training the re-rankers in OURCF, OH,
OH+PREVCF and PROPOSED. Evaluation was
done by P@1 (Precision of the top-answer) and
Mean Average Precision (MAP); they are the same
measures used in Oh et al (2012). P@1 measures
how many questions have a correct top-answer
candidate. MAP measures the overall quality of
the top-20 answer candidates. As mentioned in
Section 5.1, we used 10-fold cross-validation with
the same setting as the one used in Oh et al (2012)
for our experiments.
P@1 MAP
MURATA 22.2 27.0
OURCF 27.8 31.4
OH 37.4 39.1
OH+PREVCF 37.4 38.9
PROPOSED 41.8 41.0
Table 7: Why-QA results (%)
Table 7 shows the evaluation results. Our pro-
posed method outperformed the other four sys-
tems and improved P@1 by 4.4% over OH, which
is the-state-of-the-art system for Japanese why-
6 http://chasen.org/?taku/software/TinySVM/
1740
QA. OURCF showed the performance improve-
ment over MURATA. Although this suggests the
effectiveness of our causal relation features, the
overall performance of OURCF was lower than
that of OH. OH+PREVCF outperformed neither
OH nor PROPOSED. This suggests that our ap-
proach is more effective than previous causality-
based approaches (Girju, 2003; Higashinaka and
Isozaki, 2008), at least in our setting.
 20
 30
 40
 50
 60
 70
 80
 90
 100
 10  20  30  40  50  60  70  80  90  100
P
re
ci
si
on
 (%
)
% of questions
PROPOSED
OH
OurCF
Figure 4: Effect of causal relation features on the
top-answers
We also compared confident answers of
OURCF, OH, and PROPOSED by making each sys-
tem provide only the k confident top-answers (for
k questions) selected by their SVM scores given
by each system?s re-ranker. This reduces the num-
ber of questions that can be answered by a system,
but the top-answers become more reliable as k de-
creases. Fig. 4 shows this result, where the x axis
represents the percentage of questions (against all
the questions in our test set) whose top-answers
are given by each system, and the y axis repre-
sents the precision of the top-answers at a certain
point on the x axis. When both systems provided
top-answers for 25% of all the questions in our test
set, our method achieved 83.2% precision, which
is much higher than OH?s (62.4%). This exper-
iment confirmed that our causal relation features
were also effective in improving the quality of the
highly confident answers.
However, the high precision by our method was
bound to confident answers for a small number
of questions, and the difference in the precision
between OH and PROPOSED in Fig. 4 became
smaller as we considered more answers with lower
confidence. We think that one of the reasons is the
relatively small coverage of the excitation polarity
lexicon, a core resource in our excitation polarity
matching. We are planning to enlarge the lexicon
to deal with this problem.
Next, we investigated the contribution of the
intra- and inter-sentential causal relations to the
performance of our method. We used only one
of the two types of causal relations for generating
causal relation features (INTRA-SENT and INTER-
SENT) for training our re-ranker and compared the
results in these settings with the one when both
were used (ALL (PROPOSED)). Table 8 shows
the result. Both intra- and inter-sentential causal
relations contributed to the performance improve-
ment.
P@1 MAP
INTER-SENT 39.0 39.7
INTRA-SENT 40.4 40.5
ALL (PROPOSED) 41.8 41.0
Table 8: Results with/without intra- and inter-
sentential causal relations (%)
We also investigated the contributions of the
three types of causal relation features by ablation
tests (Table 9). When we do not use the fea-
tures by excitation polarity matching (ALL-{ef1?
ef4}), the performance is the worst. This implies
that the contribution of excitation polarity match-
ing exceeds the other two.
P@1 MAP
ALL-{tf1?tf4} 40.8 40.7
ALL-{pf1?pf4} 41.0 40.9
ALL-{ef1?ef4} 39.6 40.5
ALL (PROPOSED) 41.8 41.0
Table 9: Ablation test results for why-QA (%)
6 Conclusion
In this paper, we explored the utility of intra- and
inter-sentential causal relations for ranking answer
candidates to why-questions. We also proposed a
method for assessing the appropriateness of causal
relations as answers to a given question using the
semantic orientation of excitation. Through ex-
periments, we confirmed that these ideas are ef-
fective for improving why-QA, and our proposed
method achieved 41.8% P@1, which is 4.4% im-
provement over the current state-of-the-art system
of Japanese why-QA. We also showed that our
system achieved 83.2% precision for its confident
answers, when it only provided its confident an-
swers for 25% of all the questions in our test set.
1741
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entail-
ment methods. Journal of Artificial Intelligence Re-
search (JAIR), 38(1):135?187.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Dang,
and Danilo Giampiccolo. 2011. The seventh pascal
recognizing textual entailment challenge. In Pro-
ceedings of TAC.
E. Blanco, N. Castell, and Dan I. Moldovan. 2008.
Causal relation extraction. In Proceedings of
LREC?08.
Du-Seong Chang and Key-Sun Choi. 2006. Incremen-
tal cue phrase learning and bootstrapping method for
causality extraction using cue phrase and word pair
probabilities. Information Processing and Manage-
ment, 42(3):662?678.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2010. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Natural Language
Engineering, 16(1):1?17.
Stijn De Saeger, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large
scale relation acquisition using class dependent pat-
terns. In Proceedings of ICDM ?09, pages 764?769.
Stijn De Saeger, Kentaro Torisawa, Masaaki Tsuchida,
Jun?ichi Kazama, Chikara Hashimoto, Ichiro Ya-
mada, Jong Hoon Oh, Istv?n Varga, and Yulan Yan.
2011. Relation acquisition using word classes and
partial patterns. In Proceedings of EMNLP ?11,
pages 825?835.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011.
Minimally supervised event causality identification.
In Proceedings of EMNLP ?11, pages 294?303.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-
Carroll, James Fan, David Gondek, Aditya Kalyan-
pur, Adam Lally, J. William Murdock, Eric Nyberg,
John M. Prager, Nico Schlaefer, and Christopher A.
Welty. 2010. Building Watson: An overview of the
DeepQA project. AI Magazine, 31(3):59?79.
Roxana Girju. 2003. Automatic detection of causal
relations for question answering. In Proceedings of
the ACL 2003 workshop on Multilingual summariza-
tion and question answering, pages 76?83.
Chikara Hashimoto, Kentaro Torisawa, Stijn De
Saeger, Jong-Hoon Oh, and Jun?ichi Kazama. 2012.
Excitatory or inhibitory: A new semantic orientation
extracts contradiction and causality from the web. In
Proceedings of EMNLP-CoNLL ?12.
Ryuichiro Higashinaka and Hideki Isozaki. 2008.
Corpus-based question answering for why-
questions. In Proceedings of IJCNLP ?08, pages
418?425.
Takashi Inui and Manabu Okumura. 2005. Investigat-
ing the characteristics of causal relations in Japanese
text. In In Annual Meeting of the Association
for Computational Linguistics (ACL) Workshop on
Frontiers in Corpus Annotations II: Pie in the Sky.
Jun?ichi Kazama and Kentaro Torisawa. 2008. In-
ducing gazetteers for named entity recognition by
large-scale clustering of dependency relations. In
Proceedings of ACL-08: HLT, pages 407?415.
Christopher S. G. Khoo, Syin Chan, and Yun Niu.
2000. Extracting causal knowledge from a medical
database using graphical patterns. In Proceedings of
ACL ?00, pages 336?343.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML ?01, pages
282?289.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for question
answer classification. In Proceedings of ACL ?07,
pages 776?783.
Masaki Murata, Sachiyo Tsukawaki, Toshiyuki Kana-
maru, Qing Ma, and Hitoshi Isahara. 2007. A sys-
tem for answering non-factoid Japanese questions
by using passage retrieval weighted based on type
of answer. In Proceedings of NTCIR-6.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In Pro-
ceedings of COLING ?04, pages 693?701.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Takuya Kawada, Stijn De Saeger, Jun?ichi Kazama,
and Yiou Wang. 2012. Why question answering
using sentiment analysis and word classes. In Pro-
ceedings of EMNLP-CoNLL ?12, pages 368?378.
Kira Radinsky, Sagie Davidovich, and Shaul
Markovitch. 2012. Learning causality for
news events prediction. In Proceedings of WWW
?12, pages 909?918.
Mehwish Riaz and Roxana Girju. 2010. Another look
at causality: Discovering scenario-specific contin-
gency relationships with no supervision. In ICSC
?10, pages 361?368.
Hideki Shima, Hiroshi Kanayama, Cheng wei Lee,
Chuan jie Lin, Teruko Mitamura, Yusuke Miyao,
Shuming Shi, and Koichi Takeda. 2011. Overview
of NTCIR-9 RITE: Recognizing Inference in TExt.
In Proceedings of NTCIR-9.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computa-
tional Linguistics, 37(2):351?383.
1742
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using japanese coordi-
nated sentences and noun-verb co-occurrences. In
Proceedings of HLT-NAACL ?06, pages 57?64.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Istvan Varga, Motoki Sano, Kentaro Torisawa, Chikara
Hashimoto, Kiyonori Ohtake, Takao Kawai, Jong-
Hoon Oh, and Stijn De Saeger. 2013. Aid is out
there: Looking for help from tweets during a large
scale disaster. In Proceedings of ACL ?13.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and
Peter-Arno Coppen. 2008. Using syntactic infor-
mation for improving why-question answering. In
Proceedings of COLING ?08, pages 953?960.
Suzan Verberne, Lou Boves, and Wessel Kraaij. 2011.
Bringing why-qa to web search. In Proceedings of
ECIR ?11, pages 491?496.
1743
