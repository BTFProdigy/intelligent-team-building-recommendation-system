Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 273?280
Manchester, August 2008
Measuring Topic Homogeneity and its  
Application to Dictionary-based Word Sense Disambiguation 
Ann Gledson and John Keane  
School of Computer Science, University of Manchester  
Oxford Road, Manchester, UK M13 9PL  
{ann.gledson,john.keane}@manchester.ac.uk 
 
Abstract 
The use of topical features is abundant in 
Natural Language Processing (NLP), a 
major example being in dictionary-based 
Word Sense Disambiguation (WSD). Yet 
previous research does not attempt to 
measure the level of topic cohesion in 
documents, despite assertions of its ef-
fects. This paper introduces a quantitative 
measure of Topic Homogeneity using a 
range of NLP resources and not requiring 
prior knowledge of correct senses. Eval-
uation is performed firstly by using the 
WordNet::Domains package to create 
word-sets with varying levels of homo-
geneity and comparing our results with 
those expected. Additionally, to evaluate 
each measure?s potential value, the ho-
mogeneity results are correlated against 
those of 3 co-occurrence/dictionary-
based WSD techniques, tested on 1040 
Semcor and SENSEVAL sub-documents. 
Many low-moderate correlations are 
found to exist with several in the mod-
erate range (above .40). These correla-
tions surpass polysemy and sense-
entropy, the 2 most cited factors affecting 
WSD. Finally, a combined homogeneity 
measure achieves correlations of up to 
.52. 
1 Introduction 
Topical features in NLP consist of unordered 
bags of words, often the context of a target word 
or phrase. In WSD for example, the word bank in 
the sentence: ?If you?re OK being tied to one 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
bank, you can get al your financial products 
there.? might be assigned its monetary sense, 
based on the occurrence of the term financial. 
Often referred to as ?topical features?, these are 
an important part of many NLP methods, such as 
WSD (Yarowsky 1995) and Topic Area Detec-
tion (TAD) (Hearst 1997). Furthermore, in the 
SENSEVAL WSD competitions they are in-
cluded in the highest performing systems. 
We assert that the effectiveness of topical fea-
tures in NLP depends upon the level of topic 
homogeneity in the text. To illustrate two ex-
tremes: the disambiguation of the word bank 
might be more difficult if occurring in i) a work 
of fiction describing a series of activities which 
includes the phrases: ?stroll along the river? and 
?pick up her cheque book?; than in ii) a news re-
port on a bank in financial difficulty (a topically 
homogeneous text).   
This paper contributes a set of unsupervised 
Topic Homogeneity measures requiring no 
knowledge of correct senses. A variety of NLP 
resources are utilized and a set of evaluation me-
thods devised, providing useful results. The pa-
per is structured as follows: Section 2 outlines 
related work; Section 3 describes the experi-
ments focusing on the resources used and their 
associated homogeneity measures; in Section 4 
three evaluation methods are described, includ-
ing a WSD task-based evaluation; Conclusions 
and future work are presented in Section 5. 
2 Related Work 
TAD research (Hearst 1997) has revealed that 
word patterns within texts can be used to locate 
topic areas. Salton and Allan (1994) distinguish 
between homogenous texts, where the topic of 
the text might be ascertained from a small num-
ber of paragraphs, and heterogeneous texts, 
where topic areas change considerably. Unfortu-
nately, this research only detects topic homo-
geneity at inter-paragraph level. We assert that 
273
the strength of relationships between the words 
of a text can vary from one text to another and 
that this is likely to affect the usefulness of topic 
features in NLP tasks. Caracciolo et-al (2004) 
also indicate that documents can vary in topical 
structure, mentioning text homogeneity as an 
important feature but they do not evaluate these 
findings explicitly. 
Lexical cohesion (Halliday and Hasan 1976) is 
the analysis of the way the phrases and sentences 
of a text adhere to form a unified, comprehensi-
ble whole. Morris and Hirst (1991) describe a set 
of relationships between word-pairs, which in-
clude their likelihood to co-occur. These are used 
to create Lexical chains, which are sequences of 
related words in a document. The 2 main reasons 
for creating these chains are that they are ?an aid 
in the resolution of ambiguity? and they deter-
mine ?coherence and discourse structure?. We 
propose the use of lexical chains, alongside other 
methods, to measure document homogeneity. 
Measuring the semantic relatedness between 
words is an important area in NLP, and has been 
used in areas such as WSD, Lexical Chaining 
and Malapropism Detection. Budanitsky & Hirst 
(2006) evaluate 5 such measures, based on lexi-
cally assigned similarities, and conclude that an 
area of future research should be the capture of 
?non-classical? relationships not found in dictio-
naries, such as distributional similarity. Weeds 
and Weir (2006) evaluate distributional similari-
ties using document retrieval from the BNC. 
Chen et-al (2006) and Bollegala et-al (2007) use 
web search-engine results. All of the reported 
similarity measures are between two words (or 
texts) only. We extend these to calculate the top-
ic homogeneity of groups containing up to 10 
words.   
A small number of unsupervised WSD me-
thods exist which take topic areas and/or docu-
ment types into account (eg Gliozzo et-al 2004, 
McCarthy et-al 2007), but these work at corpus 
level and do not measure the level of topical ho-
mogeneity in each text. An exception is the WSD 
work by Navigli and Velardi (2005) who report 
differing results for 3 types of text: unfocussed, 
mid-technical (eg finance articles) and overly 
technical (eg computer networks). 
3 Experiments 
We propose the creation of a quantifiable meas-
ure of text homogeneity in order to predict the 
effectiveness of using co-occurrence features in 
WSD. In this initial set of experiments, docu-
ments are divided into simple ~50 content-word 
blocks, regardless of topic boundaries, to maxim-
ize the range of homogeneity levels in the texts 
and to nullify the effects of text length. The long-
term objective is for documents to be divided 
into topic areas at the pre-processing stage, using 
a TAD algorithm. We also propose to take a sin-
gle homogeneity measure of each entire sub-text, 
as opposed to using a sliding window approach, 
as the latter would be over-complex and compa-
rable with a similarity-based WSD process. 
3.1 Input Texts 
The documents of Semcor2 and SENSEVAL (2 
& 3) are used in the experiments. Each text is 
divided into sub-documents of approximately 50 
content-words 3 . All documents are converted 
into Semcor 2.1 format. 
From the resulting sub-documents, 1040 ran-
dom Semcor texts and the entire set of 73 SEN-
SEVAL 2 and 3 texts are used in the experiments. 
3.2 Text Pre-Processing 
Non-topic words are found to have a negative 
effect on NLP methods using topic features, such 
as WSD (eg Yarowsky 1993, Leacock et-al 
1998), and are therefore excluded from our topic 
homogeneity experiments. Unfortunately, no 
precise definition of non-topic words exists. Un-
der ideal conditions, where correct senses are 
known, we define non-topic words as word-
senses appearing in over 25% of all Semcor texts 
and/or being marked as factotum in the WordNet 
Domains 3.2 package (Magnini and Cavaglia 
2000). 
As the experiments described in this work as-
sume no such prior knowledge, an approximation 
of the criteria used above is made. A J48 
(pruned) decision tree is used to decide whether 
each word is non-topical. The input attributes 
were the PoS, sense-count, Semcor distribution 
(SenseEntropy) of its possible senses, whether all 
of the word?s senses are factotum, and the per-
cent of Semcor documents containing that word. 
The training and test data was the entire set of 
Semcor and SENSEVAL 2 and 3 English all-
word task data and the minimum node size was 
set to 4000 instances to minimize the tree size 
and prevent over-training. Using a 10-fold cross-
validation test mode the tree obtains 83% accura-
                                                 
2 Using the Brown-1 and Brown-2 documents only 
3 Splits are made as near as possible to the 50 content-
word length whilst keeping sentences intact. 
274
cy. The learned filter for selecting non-topical 
nouns and verbs is: 
(All-Senses = Factotum) || (Corpus-Hit-Percent >25.0%) || 
((Sense_Count > 1) &&  (PoS = v)) ||   
((Sense_Count > 3) &&  (SenseEntropy > 0.5668))   
Upon entry into the system, each sub-
document has all such words labeled as non-
topical. The remaining words are labeled as top-
ic-content. The confusion matrix output is shown 
in Table 1. 
 
Classified As ? OTHER NON-TOPIC   
OTHER 15017 5768 
NON-TOPIC 4278 34292 
Table 1: J48 Confusion Matrix 
 
In addition, only nouns and verbs are consi-
dered in the experiments as these word types are 
considered most likely to contain topical infor-
mation. 
3.3 Homogeneity Measures 
Five homogeneity measures have been created 
that cover a broad range of techniques for embo-
dying topic-area information in natural language 
texts. This is to facilitate comparisons between 
different techniques and if such a variety of as-
pects is captured, it improves the likelihood of a 
successful combination of the methods to pro-
duce an optimised measure. Each takes a full pre-
processed sub-document as input and outputs a 
single score. 
Word Entropy 
It is possible to capture topic homogeneity by 
using simple measures that require minimal re-
liance on external resources. Word entropy is 
considered as having the potential to reflect topi-
cal cohesion.  
To measure WordEntropy, the frequency of 
each topic-content lemma of an input document d 
is obtained, and Entropy(d) is measured using this 
set of frequencies, as follows: 
 - ?i=1..n  p(xi) log2 p(xi)      [1] 
Where n is the number of different topic content 
lemmas in d, and p(xi) is calculated as 
frequency(lemmai)/?j=1..nfrequency(lemmaj)   [2] 
 
As Entropy(d) is affected by n and n varies from 
one document to another, Entropy(d) is normal-
ised by dividing it by the maximum possible En-
tropy calculation for d, that is if all lemmas had 
equal frequencies. 
WordNet Similarities 
WordNet::Similarities 1.04 (Pedersen et-al 2004) 
is publicly available software which uses aspects 
of WordNet to ?measure the semantic similarity 
and relatedness between a pair of concepts?. The 
package can measure similarities between lemma 
pairs, where no knowledge of the correct sense or 
PoS is required. These similarities can be easily 
adapted to assist with the measurement of docu-
ment homogeneity, by comparing similarities 
between sets of word-PoS pairs in the document.  
Three WordNet Similarities homogeneity 
measures (AvgSimMeasure) are calculated for each 
document as follows: Step 1: Order the topic-
content lemmas of the input text firstly in des-
cending order of frequency, and then by first ap-
pearance in the text. Step 2: Take the first n 
lemmas from this list (where n is all lemmas up 
to a maximum of 10) and add to FreqLemmas. 
Step 3: Calculate the mean of all of the similarity 
measures between each pair of lemmas in Freq-
Lemmas. AvgSim can be defined as: 
Mean(?i=1..n ? j=i+1..n   SimMeasure(lemmai, lemmaj))   [3] 
 
Where SimMeasure(lemmai, lemmaj) is the Word-
Net::Similarity calculation between lemmai and 
lemmaj, where all allowable PoS combinations 
for the two lemmas when using the selected simi-
larity measure are included. 
The WordNet Similarity measures selected for 
use are Lesk, JCN4, and Lch (see Pedersen et-al 
(2004) and Patwardhan et-al (2003) for details), 
as each measure represents one of the 3 main 
algorithm types available: WordNet Gloss over-
laps, information content of the least common 
subsumer and path lengths respectively. 
Yahoo Internet Searches 
The web as a corpus has been successfully used 
for many areas in NLP such as WSD (Mihalcea 
and Moldovan 1999), obtaining frequencies for 
bigrams (Keller and Lapata 2003) and measuring 
word similarity (Bollegala et-al 2007). Such re-
liance on Web search-engine results does come 
with caveats, the most important in this context 
being that reported hit counts are not always reli-
able, mostly due to the counting of duplicate 
documents. (Kilgarriff 2007). 
Using web-searches as part of the homogenei-
ty measure is considered important to our expe-
riments, as it provides up-to-date information on 
word co-occurrence frequencies in the largest 
available collection of English language docu-
                                                 
4 TheBNC information-content file is loaded. 
275
ments. In addition, it is a measure that does not 
rely on WordNet. It is therefore necessary to 
produce a web-based homogeneity measure that 
limits the effects of inaccurate hit counts. 
The SearchYahoo homogeneity measure is 
calculated for each document d as follows: 
Steps 1 and 2: Perform steps 1 and 2 described 
above (WordNet Similarities). Step 3: Using an 
internet search-engine, obtain the hit counts of 
each member of Freqlemmas. Step 4: Order the 
resulting Frequlemmas list of n lemma/hit-counts 
combinations in descending order of hit-counts 
and save this list to IndivHitsDesc. Step 5: For 
each lemma of IndivHitsDesc, save to Combi-
HitsDesc preserving the ordering. Step 6: For 
each member of CombiHitsDesc: CombiHits-
Desci, obtain the hit counts of the associated 
lemma, along with the concatenated lemmas of 
all preceding list members of CombiHitsDesc 
(CombiHitsDesc0 to CombiHitsDesc[i-1]). This 
list of lemmas are concatenated together using ? 
AND ? as the delimiter. Step 7: Calculate the 
gradients of the best-fit lines for the hit-counts of 
IndivHitsDesc and CombiHitsDesc: creating 
gradIndiv and gradCombi respectively. Step 8: 
SearchYahoo is calculated for d as gradIndiv 
minus gradCombi.  
As SearchYahoo is taken as the difference be-
tween the two descending gradients, the measure 
is more likely to reveal the effects of the proba-
bility of the set of lemmas co-occurring in the 
same documents, rather than by influences such 
as duplicate documents. If the decline in hit-
counts from IndivHitsDesc[i-1] to IndivHitsDesc[i] 
is high, then the decline in the number of hits 
from CombiHitsDesc[i-1] to CombiHitsDesc[i] is 
also expected to be higher, and the converse for 
lower drops is also expected. Deviations from 
these expectations are reflected in the final ho-
mogeneity measure and are assumed to be caused 
by the likelihood of lemmas co-occurring togeth-
er in internet texts. 
A web-service enabled search-engine was re-
quired to create a fully automated process. The 
Google search-engine hit-counts were less suita-
ble, as they did not always decline as the number 
of query terms increased. This is perhaps because 
of the way in which Google combines the results 
of several search-engine hubs. The Yahoo web-
services were therefore selected, as these pro-
duced the necessary declines for the measure to 
work. 
Further evaluation of the Yahoo Internet 
searching homogeneity measure is presented in 
Gledson and Keane (2008), along with compari-
sons with similar methods using the Google and 
Windows LiveSearch web-services. 
WordNet Domains  
Magnini et-al (2002) describe the WordNet Do-
mains5 (Magnini and Cavaglia 2000) package as: 
?an extension of WordNet in which synsets 
have been annotated with one or more do-
main labels, selected from a hierarchically 
organized set of about two hundred labels?. 
 (Magnini et-al 2002 p.361) 
They describe a domain (eg ?Politics?) as ?a set 
of words between which there are strong seman-
tic relations?. This resource is useful for measur-
ing topic homogeneity, as it stores topic area in-
formation for word-senses directly, and comple-
ments the other measures, thus contributing to a 
diverse set of measures. 
Two WordNet Domains homogeneity meas-
ures are calculated: DomEntropy and Dom-
Top3Percent. These are calculated for each input 
document d as follows: 
Step 1: Add each topic-content lemma of d to 
the list TopicContents. Step 2: for each WordNet 
sense of each topic-content lemma in TopicCon-
tents, find all associated domains using the Do-
mains package, and add these to a Domain-
Counts list. This list contains each distinct do-
main dom present in the document, each with its 
associated count of the number of times it occurs 
in TopicContents: freq(domi). Step 3: Calculate 
DomEntropy using the equation [1] above, where 
n is the number of items in DomainCounts, and 
p(xi) = freq(domi) / ?j=1..n freq(domj)  [4] 
 
Step 4: Calculate DomTop3Percent as follows: 
100 (?i=1..3 freq(domi) / ?j=1..n freq(domj)) [5] 
Lexical Chaining 
?Lexical chains are defined as clusters of seman-
tically related words? (Doran et-al 2004). These 
words are usually related by electronic dictiona-
ries such as WordNet or Roget?s Thesaurus, and 
chains are created using a natural language text 
as input.  
The lexical chaining method used in our expe-
riments is a greedy version of the algorithm de-
scribed in Ecran and Cicekli (2007). Their me-
thod uses the WordNet dictionary and calculates 
all possible chains in the text. We adopt a greedy 
approach to chaining, as it is only necessary to 
get an overall estimate of the levels of topic ho-
mogeneity within the text, rather than producing 
                                                 
5 We use version 3.2 released Feb 2007 
276
lists of keywords or document summaries. The 
LexChain homogeneity measure is calculated for 
the input document d as follows:   
Step 1: Add each topic-content noun occur-
rence in d to the list UnusedNouns. Step 2: For 
each item in UnusedNouns, find all other items in 
UnusedNouns that it is related to and add them to 
its corresponding RelatedNouns list. Each item of 
RelatedNouns is mapped to a score (relScore) 
using the following system (Ercan and Cicekli 
2007): 10 points are awarded if the word-senses 
have identical lemmas or belong to the same 
WordNet 2.1 synset. 7 points are awarded if it is 
a hyponymy relationship and 4 points are 
awarded if it is a holonymy relationship. Step 3: 
Create chains: Iterate through UnusedNouns re-
cursively, adding all related senses to the first 
chain, until no further linked nouns can be found. 
As each new node (UnusedNouns item) is added 
to the chain, remove it from UnusedNouns. Con-
tinue creating further chains, until no more re-
lated nouns can be found. Step 4: Calculate the 
chainScore of each chain by adding together all 
of the relScores contained for each sense, at each 
node. Step 5: Set LexChain as the ChainScore of 
the highest scoring chain. 
3.4 Adjusting for Polysemy and Skew 
Each of the homogeneity measures (except Wor-
dEntropy) has the potential to be affected by the 
average polysemy and sense skews of the docu-
ment. The effects are measured statistically using 
linear regression and the resulting line of best fit 
equation is used to reverse them.  
To calculate the adjustments for each measure, 
the effects of polysemy and skew must be ap-
proximated. This is achieved by applying linear 
regression6 over the entire result set. The homo-
geneity measure is entered as the dependant vari-
able and the appropriate7 average polysemy and 
skew measures (per doc) are input as indepen-
dent variables. If the homogeneity measure is 
affected by either (or both) of the independent 
variables and the effect is statistically significant, 
a line of best fit equation is output representing 
the gradient of the effect caused by those varia-
ble(s). The appropriate homogeneity measure for 
each input document is adjusted by subtracting 
the co-efficient of the gradient multiplied by the 
appropriate variable(s): polysemy and/or skew. 
                                                 
6 Using SPSS 13.0 statistical software package. 
7 Avg. document polysemy/skews are only calculated 
for lemmas incl. in homogeneity measures. 
4 Evaluation and Discussion 
It is anticipated that the main users of a set of 
topic homogeneity measures are other NLP tech-
niques. They are, therefore, best measured in 
terms of the actual results of the processes they 
are intended to improve. Human judgments can 
be subjective (Doran et-al 2004) and are there-
fore deemed inappropriate for the evaluation of 
this task.  
Three methods are used to evaluate the homo-
geneity measures. Firstly, each measure is com-
pared with its equivalent where only correct 
senses are used. Secondly, the Word-
Net::Domains (version 3.2) hierarchy (Magnini 
and Cavaglia 2000) is used to generate sets of 
words with varying levels of topic homogeneity. 
Each set is then tested using the proposed meas-
ures, and the results compared with those ex-
pected. Finally, the usefulness of each measure is 
tested by evaluating their ability to indicate the 
likely outcome of several co-
occurrence/dictionary-based WSD measures. In 
the WSD literature, the main non-topic related 
variables reported as affecting WSD results are 
polysemy and skew, so these two measures will 
be used as the baselines.  
4.1 All Senses vs. Correct Senses  
Table 2 show the set of measures correlated 
against their correct-sense complements, where 
only correct senses are utilized. Most of the re-
sults are in the moderate range (.40-.60) with 
AvgSimLESK and LexChains achieving correla-
tions in the high and very-high ranges. Only the 
SENSEVAL DomEntropy result falls below the 
moderate level, indicating that homogeneity in-
formation is, in general, not negated by incorrect-
sense noise. 
Measure Correlation 
(Pearson?s R) 
Semcor SENSEVAL 
AvgSimLESK 0.814** 0.834** 
AvgSimJCN 0.491** 0.610** 
AvgSimLCH 0.569** 0.474** 
DomEntropy 0.575** 0.371** 
DomTop3% 0.535** 0.444** 
LexChain 0.659** 0.967** 
**Significant at the .01 level (2-tailed) 
Table 2: Correlation with correct-sense equiva-
lents. 
4.2 WordNet::Domains 
The WordNet Domains package (Magnini and 
Cavaglia 2000) assigns domains to each sense of 
277
the WordNet electronic dictionary. Therefore, for 
each domain a relevant list of words can be ex-
tracted. The domains are arranged hierarchically, 
allowing sets of words with a varied degree of 
topic homogeneity to be selected. For example, 
for a highly heterogeneous set, 10 words can be 
selected from any domain, including factotum 
(level-0: the non-topic related category). For a 
slightly less heterogeneous set, words might be 
selected randomly from a level-1 category (eg 
?Applied_Science?), and any of the categories it 
subsumes (eg Agriculture, Architecture, Build-
ings etc). The levels range from level-0 (facto-
tum) to level-4; we merge levels 3 and 4 as level-
4 domains are relatively few and are viewed as 
similar to level-3. This combined set is hence-
forth known as level-3.  
For our experiments, we have collected 2 ran-
dom samples of 10 words for every WordNet 
domain (167 domains) and then increased the 
number of sets from level-0 to level-2 domains, 
to make the number of sets from each level more 
similar. The final level counts are: levels 0 to 2 
have 100 word-sets each and level 3 has 192 
word-sets. The sets contain 10 words each.  We 
then assign an expected score to each set, equal 
to its domain level. 
**Significant at the .01 level (2-tailed) 
*Significant at the .05 level (2-tailed) 
Table 3: Correlation with expected scores for 
WordNet::Domains selected sets 
 
 The first column of results on Table 3 repre-
sents the correlations with expected results for all 
492 word-sets. The high WordNet::Domains re-
sults (DomEntropy and DomTop3%) probably 
reflect the fact that they are produced using the 
same resource as the creation of the test sets. On 
the other hand, knowledge of correct senses is 
not required for the homogeneity measures and 
these scores indicate that they are capable none-
theless of capturing topic homogeneity. The 
SearchYahoo, AvgSimsJCN and LexChain me-
thods all produce promising results with correla-
tions in the moderate range (0.40 to 0.59) and 
again indicate that they can capture topic homo-
geneity. 
To indicate whether the measures are more 
capable of distinguishing between extreme levels 
of homogeneity, we repeated the above tests, but 
included only those sets of level-0 and level-3. 
The results displayed in the final column of Ta-
ble 3 and provide evidence that this might be the 
case for the WordNet::Domains measures and 
SearchYahoo, as the correlations are significantly 
higher for these more extreme test sets. 
4.3 Dictionary-based WSD 
The three WSD algorithms selected for evalua-
tion are the technique described in Mihalcea and 
Moldovan (2000) and two WordNet Similarities 
measures: Lesk and JCN, adapted for WSD as 
described in Patwardhan et-al (2004), in which 
they are found to achieve the best performances. 
Each WSD technique uses the entire document as 
the context for each target word. The method of 
Mihalcea and Moldovan (2000) is included as it 
incorporates several techniques, all complemen-
tary to our overall set of evaluation methods. For 
our experiments it is split into three: Mih-ALL: 
covering all 8 procedures, including one that re-
lies on co-location information; Mih-4: utilising 
?procedure-4?, which involves the use of noun 
co-occurrence and WordNet hyponymy data; and 
Mih-5-8: using procedures 5 to 8, which involves 
synonymy and hyponymy. The results are ad-
justed to remove the effects of polysemy and 
SenseEntropy (section 3.4). 
In Table 4, the fine-grained WSD accuracy re-
sults (for topic-content words) are compared to 
those of the homogeneity measures, (including 
the correct-sense measures which set the high-
standard benchmark). As a baseline, non-
adjusted WSD accuracies are compared with the 
average polysemy and average Sense Entropy of 
each document. 
All of the ?all-senses? results, except Domai-
nEntropy, are statistically significant and achieve 
at least low-moderate correlations with one or 
more of the WSD measures. All of the measures 
outperform the baseline correlations for most of 
the WSD algorithms displayed.  
A COMBINED measure is calculated for the 
all-sense and the correct-sense sets of measures 
respectively.  The measures included (based on 
their individual performances and maintaining 
maximum diversity) are AvgSimsJCN, WordEn-
tropy, DomTop3Percent, LexChain and Sear-
chYahoo. Each of these result sets are ordered by 
homogeneity score (most homogeneous  
Measure Correlation 
(Pearson?s R) 
All Extreme 
SearchYahoo 0.46** 0.80** 
AvgSimLESK 0.23** 0.15* 
AvgSimJCN 0.47** 0.45** 
AvgSimLCH 0.35** 0.25** 
DomEntropy 0.70** 0.75** 
DomTop3% 0.75** 0.83** 
LexChain 0.42** 0.40** 
278
first) and banded into 5 groups making 4 cut-
points at equal percentiles and numbering them 
from 5 down to 1 respectively. The combined 
measure for each document is the sum of all such 
scores.  These measures often outperform all of 
the individual methods and achieve correlations 
of up to .52 in Semcor, the largest of the datasets. 
5 Conclusions and Further Work 
This paper presents a first attempt to measure 
Topic Homogeneity using a variety of NLP re-
sources. A set of 5 unsupervised homogeneity 
measures are presented that require no prior 
knowledge of correct senses and which exhibit 
moderate to high degrees of correlation with their 
correct-sense-only equivalents. When used to 
measure word-sets created using the Word-
Net::Domains package and which have varying 
levels of homogeneity, they are found to corre-
late well with expected results, further supporting 
our conjecture that they represent topical homo-
geneity information. Finally, when compared 
with WSD topic-content word accuracies, the 
effect of topic homogeneity is shown to surpass 
that of polysemy and sense-entropy, which have 
been recognized previously as having an influ-
ence on such results. By combining these meas-
ures, correlations are improved further, often 
outperforming the individual methods and 
achieving up to .52 for over 1000 random Sem-
cor sub-documents, again indicating their poten-
tial importance. Correlations in SENSEVAL are 
often higher, but due to the lower number of 
documents, it is more difficult to obtain statisti-
cally significant results.  
Our results provide evidence that improve-
ments could be made to WSD and other NLP 
methods which utilise topic features, by adapting 
the algorithms used depending on the level of 
topic cohesion of the input text. For example, 
window-sizes for obtaining contextual data might 
be expanded or reduced, based on the homogene-
ity level of the target text. Furthermore, non-
topical features such as collocation and grammat-
ical cues might be given more emphasis when 
disambiguating heterogeneous documents. 
Further work includes testing the measures on 
other NLP tasks. A machine learning approach 
might also be used to further optimize the com-
bination of homogeneity measures. Finally, it is 
intended that our approach should eventually be 
combined with a TAD method to improve WSD 
results. 
References  
Bollegala, Danushka, Yutaka Matuo and Mitsuru 
Ishizuka, 2007. Measuring Semantic Similarity be-
tween Words Using Web Search Engines. In Procs  
World Wide Web Conference, Banff, Alberta. 
Caracciolo, Caterina, Willem van Hage and Maarten 
de Rijke, 2004. Towards Topic Driven Access to 
Full Text Documents, in Research and Advanced 
 SENSEVAL 2 & 3 SEMCOR 
Mih 
ALL 
Mih  
4 
Mih 
5-8 
LESK JCN Mih 
ALL 
Mih  
4 
Mih 
5-8 
LESK JCN 
ALL Senses 
Word Entropy    -.325 -.301 -.419 -.442 -.206  -.286 
AvgSimLESK    .500  .138 .132 .211 .121 .097 
AvgSimJCN .286 .276 .357 .233  .255 .231 .284  .081 
AvgSimLCH .271 .211 .224 .202  .254 .248 .285  .104 
DomEntropy -.193 -.205    -.163 -.157   -.091 
DomTop3% .308 .281    .224 .215 .145  .108 
LexChain    .344  .328 .345 .296  .095 
SearchYahoo -.232 -.290 -.317 -.295  -.105 -.116  -.085 -.089 
COMBINED .382 .421 .270 .243 .171 .521 .517 .256  .350 
CORRECT 
Senses 
AvgSimLESK .461 .467  .630 .189 .186 .174 .237 .171 .115 
AvgSimJCN .257  .272 .357 .325 .218 .162 .234  .210 
AvgSimLCH .310  .483 .187  .198 .181 .247  .122 
DomEntropy -.340 -.295    -.334 -.335 -.106  -.270 
DomTop3% .376 .353   .214 .398 .393 .144  .316 
LexChain    .372 .297 .426 .440 .164 .112 .364 
COMBINED .398 .363  .398 .379 .519 .494 .294  .434 
Baseline AvgPolysemy -.100 -.030 -.234 (.252) (.113) (.146) (.169) -.004 .032 -.144 AvgSenseEntropy .031 -.044 .143 -.307 -.019 -.073 -.064 -.083 .041 -.141 
Results in bold: significant at the 0.01 level (2-tailed); Results in non-bold: significant at the 0.05 level (2-tailed) 
Italicised results: not significant at the 0.05 level but considered to be of interest  
Bracketed results: inverse to expectations 
All WSD results are adjusted for polysemy / SenseEntropy, with the exception of where compared with baselines. 
Table 4: Topic-content word WSD accuracy vs. Homogeneity: Correlations 
279
Technology for Digital Libraries, LNCS, 3232, pp 
495-500 
Chen, Hsin-Hsi, Ming-Shun Lin and Yu-Chuan Wei, 
2006. Novel Association Measures Using Web 
Search with Double Checking. In Proc.s 21st Intl. 
Conference on Computational Linguistics and 44th 
Annual Meeting of the ACL, pp. 1009-1016 
Doran, William, Nicola Stokes, Joe Carthy and John 
Dunnion, 2004. Assessing the Impact of Lexical 
Chain Scoring Methods and Sentence Extraction 
Schemes on Summarization, LNCS 2945, pp 627-
635, CICLing 2004 
Ercan, Gonenc and Ilyas Cicekli, 2007. Using Lexical 
Chains for Keyword extraction, Information 
Processing and Management, 43(2007), pp 1705-
1714. 
Gledson, Ann and John Keane, 2008. Using web-
search results to measure word-group similarity. In 
Procs 22nd Intl Conference on Computational Lin-
guistics (COLING), Manchester.  (To Appear) 
Gliozzo, Alfio, Carlo Strapparava and Ido Dagan, 
2004. Unsupervised and Supervised Exploitation of 
Semantic Domains in Lexical Disambiguation, 
Computer Speech and Language  
Halliday, Michael and Ruqaiya Hasan, 1976. Cohe-
sion in English, Longman Group 
Hearst , Marti, 1997. Text Tiling: segmenting text into 
multi-paragraph subtopic passages, Computational 
Linguistics, 23(1), pp 33-64 
Keller, Frank and Mirella Lapata, 2003. Using the 
Web to Obtain Frequencies for Unseen Bigrams, 
Computational Linguistics, 29(3) 
Kilgarriff, Adam, 2007. Googleology is bad science, 
Computational Linguistics, 33(1), pp 147-151 
Leacock, Claudia, Martin Choderow and George A. 
Miller, 1998. Using Corpus Statistics and Wordnet  
Relations for Sense Identification, Computational 
Linguistics, 24(1), pp 147-165  
McCarthy, Diana, Rob Koeling, Julie Weeds and John 
Carroll, 2007. Unsupervised Acquisition of Pre-
dominant Word Senses, Computational Linguistics, 
33(4), pp. 553-590. 
Magnini, Bernardo and Gabriela Cavagli?, 2000. 
Integrating Subject Field Codes into WordNet. 
In Procs LREC-2000, Athens, Greece, 2000, pp 
1413-1418. 
Magnini, Bernardo, Carlo Strapparava, Giovanni Pez-
zulo and Alfio Gliozzo, 2002. The Role of Domain 
Information in Word Sense Disambiguation. Natu-
ral Language Engineering, 8(4), pp 359-373 
Mihalcea, Rada and Dan Moldovan, 1999. A method 
for word sense disambiguation of unrestricted txt. 
In Procs. 37th Meeting of ACL, pp 152-158 
Mihalcea, Rada and Dan Moldovan, 2000. An Itera-
tive Approach to Word Sense Disambiguation, In 
Proc. FLAIRS 2000, pp. 219-223, Orlando 
Morris, Jane and Graeme Hirst, 1991. Lexical Cohe-
sion Computed by Thesaural Relations as an Indi-
cator of the Structure of Text, Computational Lin-
guistics, 17(1), pp. 21-48.  
Navigli, Roberto, Paola Velardi, 2005. Structural Se-
mantic Interconnections: A Knowledge-Based Ap-
proach to Word Sense Disambiguation, IEEE 
Transactions on Pattern Analysis and Machine In-
telligence, 27(7),  pp. 1075-1086,  July. 
Patwardhan, Siddharth, Satanjeev Banerjee and Ted 
Pedersen, 2003. Using Measures of Semantic Rela-
tedness for Word Sense Disambiguation, LNCS 
2588, pp 241-257, CICLing 2003 
Pedersen, Ted, Siddharth Patwardhan and Jason Mi-
chelizzi, 2004. WordNet::Similarity ? Measuring 
the Relatedness of Concepts, In Procs 19th Nation-
al Conference on Artificial Intelligence. 
Salton, Gerard and James Allan, 1994. Automatic text 
decomposition and structuring, In Procs. RIAO In-
ternational Conference, New York. 
Weeds, Julie and, David Weir, 2006, Co-occurrence 
Retreival: A Flexible Framework for Lexical Dis-
tributional Similarity. Computational Linguistics, 
31(4), pp 433-475. 
Yarowsky, David, (1995), Unsupervised word sense 
disambiguation rivaling supervised methods. In 
Procs 33rd Annual Meeting of the Association for 
Computational Linguistics (ACL) 1995, pp 189-
196 
 
280
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 281?288
Manchester, August 2008
Using Web-Search Results to Measure Word-Group Similarity 
Ann Gledson and John Keane  
School of Computer Science,  
University of Manchester  
Oxford Road, Manchester, UK M13 9PL  
{ann.gledson,john.keane}@manchester.ac.uk 
 
Abstract 
Semantic relatedness between words is 
important to many NLP tasks, and nu-
merous measures exist which use a vari-
ety of resources. Thus far, such work is 
confined to measuring similarity between 
two words (or two texts), and only a 
handful utilize the web as a corpus. This 
paper introduces a distributional similar-
ity measure which uses internet search 
counts and also extends to calculating the 
similarity within word-groups. The 
evaluation results are encouraging: for 
word-pairs, the correlations with human 
judgments are comparable with state-of-
the-art web-search page-count heuristics. 
When used to measure similarities within 
sets of 10 words, the results correlate 
highly (up to 0.8) with those expected.  
Relatively little comparison has been 
made between the results of different 
search-engines. Here, we compare ex-
perimental results from Google, Win-
dows Live Search and Yahoo and find 
noticeable differences. 
1 Introduction 
The propensity of words to appear together in 
texts, also known as their distributional similarity 
is an important part of Natural Language Proc-
essing (NLP): 
?The need to determine semantic related-
ness? between two lexically expressed 
concepts is a problem that pervades much of 
[NLP].?             (Budanitsky and Hirst 2006) 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
Such requirements are evident in word sense dis-
ambiguation (WSD) (Patwardhan et-al 2003), 
spelling correction (Budanitsky and Hirst 2006) 
and Lexical Chaining (Morris and Hirst 1991).  
As well as measuring the co-occurrence of 
word-pairs, it is also considered useful to extend 
these measures to calculate the likelihood of sets 
of words to appear together. For example, Carac-
ciolo et-al (2004) evaluate two established topic 
area detection (TAD) algorithms and indicate 
text homogeneity as a document feature affecting 
the results. Furthermore, Gliozzo et-al (2004) 
and McCarthy et-al (2007) highlight the impor-
tance of topic features for WSD and Navigli and 
Velardi (2005) report differing WSD results for 3 
types of text: unfocussed, mid-technical (eg fi-
nance articles) and overly technical (eg interop-
erability and computer networks). Measures of 
word-group similarity can be used to calculate 
the level of topic cohesion in texts, and the po-
tential for this to be used to benefit NLP areas 
such as WSD and TAD has been indicated in 
Gledson and Keane (2008). 
We consider web-searching an important part 
of measuring similarity, as it provides up-to-date 
information on word co-occurrence frequencies 
in the largest available collection of English lan-
guage documents. We propose a simple measure 
that uses internet search counts by measuring the 
decline in the number of hits as more words 
(from the word-set to be measured) are appended 
to a query string using the ?AND? operator.  To 
offset against the effect of individual word hit-
counts, the above gradient is compared to that of 
the individual word hit counts ? arranged in de-
scending order of hits returned. 
The paper is structured as follows: in Section 
2 we describe related work in the areas of word 
similarity and the use of search-engine counts in 
NLP. Section 3 outlines the algorithm to be used 
for our similarity measure which can utilise any 
search-engine that is web-service enabled. Sec-
281
tion 4 describes and discusses the results of the 
evaluation techniques used, one for evaluating 
word-pair similarities, which compares with pre-
vious work and human judgements, and three for 
evaluating word-group similarities. Section 5 
outlines the conclusions drawn from these ex-
periments and Section 6 discusses further work. 
2 Related Work 
The most commonly used similarity measures 
are based on the WordNet lexical database (eg 
Budanitsky and Hirst 2006, Hughes and Ramage 
2007) and a number of such measures have been 
made publicly available (Pedersen et-al 2004). 
The problem with such methods is that they are 
confined to measuring words in terms of the lex-
ical connections manually assigned to them. 
Language is evolving continuously and the con-
tinual maintenance and updating of such data-
bases is highly labour intensive, therefore, such 
lexical resources can never be fully up-to-date. In 
addition, the lexical connections made between 
words and concepts do not cover all possible re-
lations between words. An important relationship 
between words is distributional similarity and 
Budanitsky and Hirst (2006) conclude that the 
capture of these ?non-classical? relationships is 
an important area of future research.  
Weeds and Weir (2006) use a ?co-occurrence 
retrieval method to compute word relatedness, 
which is described as analogous to the precision 
and recall metrics of document retrieval. They 
observe the ?plausibility? of substituting word-1 
for word-2 within verb/object-noun grammatical 
relationships. This method is restricted to docu-
ment retrieval from the BNC corpus, due to the 
pre-requisite that words are part-of-speech 
tagged and that some grammatical parsing is per-
formed. The similarity measure that we have de-
veloped is simpler and does not rely on pre-
processing of texts. This distributional similarity 
measure calculates the propensity of words to 
appear together, regardless of part-of-speech or 
grammatical functions. 
We assert that the world-wide-web can be 
used to capture distributional similarity, and is 
less likely to suffer from problems of coverage, 
found in smaller corpora. The web as a corpus 
has been successfully used for many areas in 
NLP (Kilgarriff and Grefenstette 2003) such as 
WSD (Mihalcea and Moldovan 1999), obtaining 
frequencies for bigrams (Keller and Lapata 2003) 
and noun compound bracketing (Nakov and 
Hearst 2005). Such reliance on web search-
engine results does come with caveats, the most 
important (in this context) being that the reported 
hit counts may not be entirely trustworthy (Kil-
garriff 2007). 
Strube and Ponzetto?s (2006) use the Wikipe-
dia database, which includes a taxonomy of cate-
gories, and they adapt ?well established semantic 
relatedness measures originally developed for 
WordNet?. They achieve a correlation coefficient 
of 0.48 with human judgments, which is stated as 
being higher than a Google-only and WordNet-
only based measure for the largest of their test 
datasets (the 353 word-pairs of 353-TC) 
Chen et-al (2006) use the snippet?s returned 
from web-searches, in order to perform word 
similarity measurement that captures ?new usag-
es? of ever evolving, ?live? languages. A double 
checking model is utilized which combines the 
number of occurrences of the first word in the 
snippets of the second word, and vice-versa. This 
work achieves a correlation coefficient of 0.85 
with the Miller and Charles (1998) dataset of 28 
word-pairs, but to achieve the best results, 600-
700 snippets are required for each word-pair, 
requiring extra text-processing and searching.  
The work most similar to ours is the set of 
page-counting techniques of Bollegala et-al 
(2007). They combine the use of web-search 
page-counts with the analysis of the returned text 
snippets and achieve impressive correlations with 
human judgment (0.834 with the Miller and 
Charles dataset). The text-snippet analysis is 
more complex than that of Chen et-al (2006), as 
the optimal number of snippets is over 1000, and 
their process involves complex pattern matching, 
which may not scale well to measuring word-
groups similarity. Their page counting heuristics 
utilize 4 popular co-occurrence methods: Jac-
card, Overlap (Simpson), Dice and PMI (Point-
wise mutual information) but again, these tech-
niques are not designed to scale up to larger 
numbers of input words. 
Mihalcea et-al (2006) and Gabrilovich and 
Markovitch (2007) achieve good results when 
comparing texts, the former utilizing the inverse-
document frequency heuristic and the latter in-
dexing the entire Wikipedia database and com-
paring vector representations of the two inputs. 
None of the above work is adapted to be used 
on single groups of words as a measure of topic 
cohesion. They could be adapted by combining 
similarity results between many pairs, but this 
might similarly have a high computational cost. 
In addition, no comparisons of different web-
search engines are made when using web-counts. 
282
As the use of web-counts is precarious (Kigarriff 
2007), these types of comparisons are of high 
practical value. 
3 Similarity Measure 
The proposed measure can be used with any 
web-service enabled search-engine and in our 
experiments we compare three such search-
engines: Yahoo [Yahoo], Windows Live Search 
[Live-Search] and Google [Google]. The simi-
larity measure WebSim[search-engine] is calculated for 
each document d as follows:  
Step 1: Add the required set of n lemmas to the 
Lemmas list.  
Step 2: Using an internet search-engine, obtain 
the hit counts of each member of Lemmas.  
Step 3: Order the resulting list of n lemma/hit-
counts combinations in descending order of hit-
counts and save lemma/hit combinations to In-
divHitsDesc.  
Step 4: For each lemma of IndivHitsDesc, save 
to CombiHitsDesc preserving the ordering.  
Step 5: For each member of CombiHitsDesc: 
CombiHitsDesci, obtain the hit counts of the as-
sociated lemma, along with the concatenated 
lemmas of all preceding list members of Combi-
HitsDesc (CombiHitsDesc[0] to CombiHits-
Desc[i-1]). This list of lemmas are concatenated 
together using ? AND ? as the delimiter.  
Step 6: Calculate the gradients of the best-fit 
lines for the hit-counts of IndivHitsDesc and 
CombiHitsDesc: creating gradIndiv and grad-
Combi respectively.  
Step 7: WebSim[search-engine] is calculated for d as 
gradIndiv minus gradCombi. 
As WebSim[search-engine] is taken as the difference 
between the two descending gradients, the meas-
ure is more likely to reveal the affects of the 
probability of the set of lemmas co-occurring in 
the same documents, rather than by influences 
such as duplicate documents. If the decline in hit-
counts from IndivHitsDesc[i-1] to IndivHits-
Desc[i] is high, then the decline in the number of 
hits from CombiHitsDesc[i-1] to CombiHits-
Desc[i] is also expected to be higher, and the 
converse, for lower drops is also expected. Devi-
ations from these expectations are reflected in the 
final homogeneity measure and are assumed to 
be caused by the likelihood of lemmas co-
occurring together in internet texts. 
Search-engines are required that publish a set 
of web-services for a fully automated process. 
The Google, Yahoo and Windows Live Search 
search-engines have been selected and the results 
of each measure are compared. In response to 
important problems highlighted by Kilgarriff 
(2007) relating to the use of web counts in NLP 
experiments: firstly, a measure is proposed be-
tween words that does not require the pre-
annotation of part-of-speech information and 
does not rely on query syntax / meta-language. 
Secondly, our measure relies on the use of web-
search page counts (as opposed to word instance 
counts) as we are measuring the likelihood of co-
occurrence in the same text. Finally, measures 
are taken to try to avoid the problem of arbitrary 
search-engine counts. For example, each measure 
is the result of a comparison between the decline 
rates of 2 sets of hit counts and the full set of 
queries for each input text are taken within a 20 
second interval (for groups of 10 words). In addi-
tion, for the Google and Yahoo measures the 
web-service request parameter includes options 
to avoid the return of the same web-page 
(Google: ?filter_alike_results?; Yahoo: ?Al-
lowSimilar?). 
4 Evaluation 
Four methods of evaluation are used to verify 
that the similarity measure is capable of measur-
ing similarity between words. These methods are 
selected to be as varied as possible, to provide a 
fuller understanding of the usefulness of our 
technique and to verify that it is working as ex-
pected. 
4.1 Word-pairs 
The results of Rubenstein and Goodenough 
(1965) (65 word-pairs), Miller and Charles 
(1998) (30 word-pair subset of Rubenstein and 
Goodenough 1965) and Finkelstein et-al (2002) 
(353 word-pairs) are used to evaluate the ability 
of the proposed method to calculate the similarity 
between word-pairs. These results sets list the 
similarity scores of the word-pairs as assigned by 
humans. Although this does provide a useful 
benchmark, extremely high, or perfect correla-
tions are unrealistic, as those involved in the ex-
periments were asked to think about the words 
association in terms of lexical relations such as 
synonymy as opposed to the broader idea of dis-
tributional similarity. Nevertheless, in conjunc-
tion with other evaluation techniques, these com-
parisons can still be useful, as some correlation 
would be expected if our measure was function-
ing as required. 
In addition, our results are compared with the 
page-count based similarity scores of Bollegala 
283
Measure Correlation (Pearson?s R) 
WebSim[YAHOO] -0.57** 
WebSim[LIVE-SEARCH] -0.60** 
WebSim[GOOGLE] -0.43** 
Google-Jaccard (Strube & Ponzetto 2006) 0.41 
pl (path-lengths)  (Strube & Ponzetto 2006) 0.56 
wup  (Strube & Ponzetto 2006) 0.52 
lch (Strube & Ponzetto 2006) 0.54 
** Significant at the 0.01 level (2-tailed) 
Italics: Statistical significance not specified in Strube and Ponzetto (2006) 
Table 1: Correlation with Human Ratings on Rubenstein-Goodenough dataset 
 
Measure Correlation (Pearson?s R) 
WebSim[YAHOO] -0.55** 
WebSim[LIVE-SEARCH] -0.53** 
WebSim[GOOGLE] -0.39* 
Jaccard  (Bollegala et-al 2007) 0.26 
Dice  (Bollegala et-al 2007) 0.27 
Overlap  (Bollegala et-al 2007) 0.38 
PMI  (Bollegala et-al 2007) 0.55 
pl (Strube & Ponzetto 2006) 0.49 
** Significant at the 0.01 level (2-tailed) 
* Significant at the 0.05 level (2-tailed) 
Italics: Statistical significance not specified in Bollegala et-al (2007) or Strube and Ponzetto (2006) 
Table 2: Correlation with Human Ratings on Miller-Charles? dataset 
 
Measure Correlation (Pearson?s R) 
WebSim[YAHOO] -0.37** 
WebSim[LIVE-SEARCH] -0.40** 
WebSim[GOOGLE] -0.119* 
Google-Jaccard (Strube & Ponzetto 2006) 0.18 
wup (Strube & Ponzetto 2006) 0.48 
lch (Strube & Ponzetto 2006) 0.48 
** Significant at the 0.01 level (2-tailed) 
* Significant at the 0.05 level (2-tailed) 
Italics: Statistical significance not specified in Strube and Ponzetto (2006) 
Table 3: Correlation with Human Ratings on 353-TC dataset 
 
et-al (2007) and the best performing Wikipedia-
based measures of Strube and Ponzetto (2006). 
The former gauge similarity using a number of 
popular co-occurrence measures adapted for 
web-search page-counts and their method is con-
sidered closest to our approach. The individual 
results for each word-pair are shown in Tables 1, 
2 and 3. They indicate that moderate correlations 
exists, particularly in the Rubenstein and Goode-
nough set. WebSim[YAHOO] and WebSim[LIVE 
SEARCH] significantly outperform the Web-
Sim[GOOGLE] method. This may be due to 
Google?s results being more erratic. Google?s 
returned counts were sometimes found to in-
crease as extra ?AND? clauses were added to the 
query string. This is perhaps because of the way 
in which Google combines the results of several 
search-engine hubs. (This was accommodated to 
a degree by setting any negative scores to zero.)  
All three of the methods were comparable with 
the results of Bollegala et-al (2007), with the 
WebSim[LIVE SEARCH] measure performing at the 
highest levels and WebSim[YAHOO] producing the 
highest of all measures on the Miller and Charles 
(1998) dataset. (Unfortunately, Bollegala et-al 
(2007) do not compare their results with the Ru-
benstein-Goodenough and 353-TC dataset.) In 
the largest (353-TC) dataset, the WebSim[YAHOO] 
and WebSim[LIVE SEARCH] results were found to be, 
lower, but comparable with the best of Strube 
and Ponzetto?s (2006) Wikipedia-based results 
and significantly higher than their Jaccard meas-
ure, adapted to use Google web-search page-
counts. 
284
Set Words 
A law, crime, perpetrator, prison, sentence, judge, jury, police, justice, criminal 
B astrology, ?star sign", Libra, Gemini, Sagittarius, zodiac, constellation, Leo, Scorpio, birthday 
C ocean, "hydrothermal vent", fauna, botanical, biogeography, tube-worm, Atlantic, species, biology, habitat 
D economy, credit, bank, inflation, "interest rate", finance, capital, expenditure, market, profit 
E health, diet, swimming, fitness, exercise, "heart disease", stroke, fruit, jogging, work 
F football, stadium, striker, trophy, match, referee, pitch, crowd, manager, kick 
G education, exam, teacher, timetable, classroom, pupils, homework, results, parents, teenager 
H country, election, president, vote, leader, population, opposition, party, government, count 
J computer, "hard drive", keyboard, connection, monitor, RAM, speaker, "mother board", CPU, internet 
K "film star", drugs, money, millionaire, fame, music, actor, debut, beauty, paparazzi 
M "house prices", monthly, mortgage, home, borrowing, "buy-to-let", figures, homeowners, lending, trend 
N inmates, overcrowding, prisoners, custody, release, warden, cell, bars, violence, detention 
Table 4: Manually selected test sets 
 
4.2 Word-groups 
In order to indicate whether the proposed meas-
ure is capable of measuring similarity amongst a 
set of words, a broad range of evaluation meth-
ods is required. No human evaluations exist, and 
to produce such a data-set would be difficult due 
to the larger quantity of data to evaluate. The 
following three methods are used: 
Manual Selection of Word-groups 
The manual selection of word-groups, for testing 
the proposed measure is an important part of its 
evaluation as it is possible to construct word-sets 
of varying similarity, so that expected results can 
be predicted and then compared with actual re-
sults. In addition, the datasets created can be eas-
ily manipulated to range from highly homoge-
nous / similar sets of words to extremely hetero-
geneous ? where words are highly unlikely to 
appear together. The latter is achieved by sys-
tematically merging the groups together, until a 
complete mixture of words from different word-
sets is produced. 
The method used to compile the words groups 
is as follows: firstly, groups of words with a 
known propensity to appear together were se-
lected by browsing popular internet web-sites 
(see Table 4 for the words contained in each set). 
Secondly for each of these original sets, a series 
of 5 measures is taken, the first with all words 
from the original set (to illustrate, this might be 
represented as AAAAAAAAAA ? where each 
letter represent a word from the set shown), this 
therefore is the most homogeneous group. Then 
two words from this set are replaced with two 
from a new set (eg B) (AAAAAAAABB). Then 
a further two words (again originally from set A) 
are replaced with two more words from another 
new set (eg from Set C) (AAAAAABBCC), and 
so on, until the final set of 10 words to be meas-
ured consists of 5 pairs of words, each from 5 
different sets (AABBCCDDEE). These steps 
were first performed for sets A, B, C, D and E 
and then for sets F, G, H, J and K respectively. 
The results were compared (using Pearson?s cor-
relation) against the expected results. For exam-
ple: AAAAAAAAAA = 10 points, 
AAAAAAAABB = 8 points, AAAAAABBCC = 
6 points, AAAABBCCDD = 4 points and 
AABBCCDDEE = 2 points. 
On the whole, the word-groups contained in 
each of these two sets of sets are considered to be 
heterogeneous (eg A is dissimilar to B, C, D and 
E etc). To introduce more ?blurring? of these cat-
egories, a third set of sets was measured, consist-
ing of the word-sets A, D, H, M and N. It was 
considered more conceivable that the words in 
these sets could be found in the same documents.  
The expected results for this set of measures 
were modified slightly to become: 
AAAAAAAAAA = 8 points, AAAAAAAABB 
= 7 points, AAAAAABBCC = 6 points, 
AAAABBCCDD = 5 points and 
AABBCCDDEE = 4 points. This was done to 
reflect the fact that the differences between the 
highest and lowest were expected to be less. 
 
Measure 
Correlation (Pearson?s R) 
Heteroge-
neous Sets 
only: 
 
ABCDE 
FGHJK 
Homogenous 
Sets Included: 
 
ABCDE 
FGHJK 
+ADHMN 
WebSim[YAHOO] -.80** -.68** 
WebSim[LIVE-SEARCH] -.65** -.57** 
WebSim[GOOGLE] -.70** -.64** 
**Significant at the .01 level (2-tailed) 
Table 5: Correlation with expected scores for 
manually selected sets 
285
As illustrated in Table 5, the ?Heterogeneous 
Sets? group similarity scores were found to corre-
late very highly with those expected, with the 
WebSim[YAHOO] measure achieving .80. The Web-
Sim[GOOGLE] measure was also found to improve 
when tested on groups of words.  
The measures were found to perform less well 
when the third set of word-sets, containing more 
closely related members, was introduced (Table 
5, final column). This indicates that the measures 
are better at distinguishing between highly ho-
mogeneous and highly heterogeneous word-sets, 
but appear less proficient at distinguishing be-
tween word-sets with moderate levels of topic 
homogeneity. 
WordNet::Domains Selection of Word-groups 
The WordNet Domains package2 (Magnini and 
Cavaglia 2000) assigns domains to each sense of 
the WordNet electronic dictionary. Therefore, for 
each domain a relevant list of words can be ex-
tracted. The domains are arranged hierarchically, 
allowing sets of words with a varied degree of 
topic homogeneity to be selected. For example, 
for a highly heterogeneous set, 10 words can be 
selected from any domain, including factotum 
(level-0: the non-topic related category). For a 
slightly less heterogeneous set, words might be 
selected randomly from a level-1 category (eg 
?Applied_Science?), and any of the categories it 
subsumes (eg Agriculture, Architecture, Build-
ings etc). The levels range from level-0 (facto-
tum) to level-4; we merge levels 3 and 4 as level-
4 domains are relatively few and are viewed as 
similar to level-3. This combined set is hence-
forth known as level-3.  
Measure Correlation (Pearson?s R) 
All Extreme Only 
WebSim[YAHOO] -0.46** -0.80** 
WebSim[LIVE-SEARCH] -0.06 -0.23** 
WebSim[GOOGLE] -0.42** -0.71** 
**Significant at the .01 level (2-tailed) 
Table 6: Correlation with expected scores for 
WordNet::Domains selected sets 
 
For our experiments, we collected 2 random 
samples of 10 words for every WordNet domain 
(167 domains) and then increased the number of 
sets from level-0 to level-2 domains, to make the 
number of sets from each level more similar. The 
final level counts are: levels 0 to 2 have 100 
word-sets each and level 3 has 192 word-sets. 
The word-sets contain 10 words each.  We then 
                                                 
2 We use version 3.2 released Feb 2007 
assign an expected score to each set, equal to its 
domain level. 
Table 6 displays the resulting correlation be-
tween the WebSim scores and the expected scores 
(column: ?All?). In the previous section, we ob-
served that the measures are less competent at 
distinguishing between moderate levels of ho-
mogeneity, and the WordNet::Domain test sets 
contain many sets which could be described as 
having moderate homogeneity. To further test 
this, we repeated the above WordNet::Domain 
tests, but included only those sets of level-0 and 
level-3. The results displayed in the final column 
of Table 6 and provide further evidence that this 
might be the case, as the correlations are signifi-
cantly higher for these more extreme test sets. 
SENSEVAL 2 & 3 Data 
The selection of the most frequent words from 
natural language documents is another important 
part of our evaluation, as it is representative of 
real-world data-sets. As it is anticipated that our 
results could be of use for WSD, we opted to 
measure the topic homogeneity of a publicly 
available set of documents from a well estab-
lished WSD competition. The documents of the 
SENSEVAL 2 & 33 English all-words WSD task 
were divided into 73 sub-documents, each con-
taining approximately 50 content-words. The 
stop-words and non-topical 4  content words of 
each sub-document were then removed and the 
remaining words lemmatised and aggregated by 
lemma. This list of lemmas was then arranged in 
descending order of the number of occurrences in 
the sub-document. The top-10 most frequent 
lemmas were selected as input to the similarity 
measure. The results for each set alng with de-
scriptions of the documents used are displayed in 
Table 7. 
The scientific documents (d01) could be 
viewed as the most homogeneous, with the 
earthquake accounts (d002) and the work of fic-
tion (d000) being considered the most heteroge-
neous. With this in mind, it is evident in Table 7 
that the WebSim[MSN] measure performed the least 
well and other two methods performed as ex-
pected, and with identical rankings. 
Further analysis is required to compare these 
results with the WSD results for the same 
                                                 
3 See http://www.senseval.org/ 
4 Non-topical words are those that are found in over 25% of 
Semcor documents or have their correct sense(s) belonging 
to the ?Factotum? category in the WordNet Domains pack-
age by Magnini and Cavaglia (2000). 
286
Text Description Average 
WebSim 
[YAHOO] 
Average 
WebSim 
[GOOGLE] 
Average 
WebSim 
[MSN] 
d00 Change-Ringing: ? History of  Campanology, Churches, Social History 
Typical set = {bell, church, tower, "change ringing", English, peculiar-
ity, rest, stone, faithful, evensong} 
1.52 (4) 1.10 (4) 1.11 (5) 
d01 Cancer Cell Research: Typical Set = {cancer, gene, parent, protein, 
cell, growth, "suppressor gene", individual, scientist, beneficiary} 
1.19 (1) .93 (1) .89 (2) 
d02 Education: Typical Set = {child, belief, nature, parent, education, me-
dium, politician, "elementary school", creativity} 
1.36 (3) 1.05 (3) 1.03 (3) 
d000 Fiction: Typical set = {stranger, drinking, occasion, street, "moving 
van", intersection, brake, guy, mouth, truck} 
1.55 (5) 1.18 (5) 1.03 (3) 
d001 US Presidential Elections: Typical set = {district, candidate, half-
century, election, percentage, president, vote, hopeful, reminder, ticket} 
1.29 (2) .96 (2) .85 (1) 
d002 California Earthquake ? First hand accounts / stories  
Typical Set = {computer, quake, subscriber, earthquake, resident, sol-
ace, "personal computer", hundred, "check in", "bulletin board"} 
1.63 (6) 1.32 (6) 1.13 (6) 
Rankings for each measure are shown in brackets 
Table 7: Average group similarity measures of the SENSEVAL 2 and 3 datasets 
 
documents, as performed by Gledson and Keane 
(2008), and this is highlighted as part of the fur-
ther work. 
5 Conclusions 
We proposed a simple web-based similarity 
measure which relies on page-counts only, can 
be utilized to measure the similarity of entire sets 
of words in addition to word-pairs and can use 
any web-service enabled search engine.  When 
used to measure similarities between two words, 
our technique is comparable with other state-of-
the art web-search page-counting techniques (and 
outperforms most). The measure is found to cor-
relate to a moderate level (the highest being .60 
correlation) with human judgments. When used 
to measure similarities between sets of 10 words, 
the results are similarly encouraging and show 
the expected variations for word-groups with 
different levels of homogeneity. Where word-
groups are manually created, with known expec-
tations of the similarities between each word-set, 
correlations with these expected results are as 
high as .80. Noticeable differences between the 
performances of each of the 3 search-engines, for 
each evaluation method, are evident. Google per-
forms poorly for the word-pair similarity meas-
ures and Yahoo and Live Search both perform 
substantially better. For the word-set compari-
sons, Google performance improves (perhaps as 
the erratic single counts are stabilised as larger 
sets of words are used), but Yahoo is again supe-
rior and MSN performs much less well. Overall, 
our results indicate that the Yahoo measure is the 
most consistent and reliable. 
6 Further Work 
The group similarity measure calculates the pro-
pensity of words to co-occur with one-another, 
which can be described as a measure of the topic 
homogeneity of the set of input words. Gledson 
and Keane (2008) propose the use of further 
measures of topic homogeneity using a variety of 
available resources. These measures are com-
pared with the results of WSD approaches reliant 
on topic features and low to moderate correla-
tions are found to exist. It is also proposed that 
useful correlations with other NLP techniques 
utilising topical features (such as TAD and 
Malapropism Detection) might also exist. 
The word-group similarity measures per-
formed better for extreme levels of topic homo-
geneity. The measures must be improved to en-
able them to distinguish between moderate ho-
mogeneity levels. This may be achieved by com-
bining our simple measure with other word simi-
larity/relatedness techniques such as the use of 
WordNet Domains, or Lexical Chaining. 
It is expected that polysemy counts of words 
influence the outcome of these experiments, es-
pecially for the word-pairs which have less data 
and are more susceptible to erratic counts. Re-
sults might be improved by measuring and off-
setting these effects.  
In addition, an upper limit of word-set cardi-
nality should be determined, which is the maxi-
mum number of input words that can be meas-
ured. Further testing is necessary using a range of 
set cardinalities, to obtain optimal values. 
287
References 
Bollegala, D., Matsuo, Y., Ishizuka, M., 2007. Meas-
uring Semantic Similarity between Words Using 
Web Search Engines. In Proceedings of World-
Wide-Web Conference 2007 (Track: Semantic 
Web), Banff, Alberta, Canada. pp. 757-766.  
Budanitsky, A. and Hirst, G. 2006. Evaluating Word-
Net-based Measures of Lexical Semantic Related-
ness. Computational Linguistics , 32(1), pp. 13--47.  
Caracciolo, C., Willem van Hage and Maarten de 
Rijke, 2004. Towards Topic Driven Access to Full 
Text Documents, in Research and Advanced Tech-
nology for Digital Libraries, LNCS, 3232, pp 495-
500 
Chen, Hsin-Hsi, Ming-Shun Lin and Yu-Chuan Wei, 
2006. Novel Association Measures Using Web 
Search with Double Checking. In Proc.s 21st Intl. 
Conference on Computational Linguistics and 44th 
Annual Meeting of the ACL, pp. 1009-1016 
Finkelstein, Lev, Evgeniy Gabrilovich, Yossi Matias, 
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin, 2002. Placing Search in Context: The 
Concept Revisited, ACM Transactions on Informa-
tion Systems,  20(1), pp. 116-131, January 
Gabrilovich, Evgeniy and Shaul Markovitch, 2007. 
Computing Semantic Relatedness using Wikipedia-
based Explicit Semantic Analysis. In Proc.s Intl. 
Joint Conference on Artificial Intelligence 2007, 
Hyderabad, India. 
Gledson, Ann and John Keane. 2008. Measuring topic 
homogeneity and its application to word sense dis-
ambiguation. In Procs 22nd Intl Conference on 
Computational Linguistics (COLING), Manchester. 
(To Appear) 
Gliozzo, Alfio, Carlo Strapparava and Ido Dagan, 
2004. Unsupervised and Supervised Exploitation of 
Semantic Domains in Lexical Disambiguation, 
Computer Speech and Language 
Hughes, T. and D. Ramage, 2007. Lexical Semantic 
Relatedness with Random Graph Walks, In Proc.s 
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning, pp. 581-589, 
Prague. 
Keller, F. and Lapata, M., 2003. Using the Web to 
Obtain Frequencies for Unseen Bigrams, Computa-
tional Linguistics, 29(3) 
Kilgarriff, Adam, 2007. Googleology is Bad Science. 
Computational Linguistics, 33(1), pp. 147?151. 
Kilgarriff, A and Grefenstette, G., 2003. Web as Cor-
pus, In Introduction to the special issue on the web 
as corpus, Computational Linguistics, 29(3), pp. 
333--347 
Magnini, B. and Cavagli?, Gabriela. 2000. Integrating 
Subject Field Codes into WordNet. 
In Proceedings of LREC-2000, Athens, Greece, 
2000, pp 1413-1418. 
McCarthy, Diana, Rob Koeling, Julie Weeds and John 
Carroll, 2007. Unsupervised Acquisition of Pre-
dominant Word Senses, Computational Linguistics, 
33(4), pp. 553-590. 
Mihalcea, R and Moldovan, D., 1999. A method for 
word sense disambiguation of unrestricted txt. In 
Proceedings of the 37th Meeting of ACL, pp 152-
158 
Mihalcea, Rada, Courtney Corley and Carlo Strappa-
rava, 2006. Corpus-based and Knowledge-based 
Measures of Text Semantic Similarity, In Proc.s of 
American Association for Artificial Intelligence 
2006 
Miller, G. and Charles, W., 1998. Contextual corre-
lates of semantic similarity. Language and Cogni-
tive Processes, 6(1), pp. 1--28. 
Morris, J. and Hirst, G., 1991. Lexical Cohesion 
Computed by Thesaural Relations as an Indicator 
of the Structure of Text, Computational Linguis-
tics, 17(1), pp. 21-48 
Nakov, P. and Hearst, M. 2005. Search Engine Statis-
tics Beyond the n-gram: Application to Noun 
Compound Bracketing, In Proceedings of the 9th 
Conference on CoNLL, pp. 17--24, Ann Arbor, 
June 
Navigli, Roberto, Paola Velardi, 2005. Structural Se-
mantic Interconnections: A Knowledge-Based Ap-
proach to Word Sense Disambiguation, IEEE 
Transactions on Pattern Analysis and Machine In-
telligence, 27(7),  pp. 1075-1086,  July. 
Patwardhan, S., Banerjee, S. and Pedersen, T., (2003). 
Using Measures of Semantic Relatedness for Word 
Sense Disambiguation, LNCS 2588 - CICLing 
2003, pp. 241--257. 
Pedersen, Ted, Siddharth Patwardhan and Jason Mi-
chelizzi, 2004. WordNet::Similarity ? Measuring 
the Relatedness of Concepts, In Proc.s 19th Nation-
al Conference on Artificial Intelligence. 
Rubenstein, H. and Goodenough, J., 1965. Contextual 
correlates of synonymy. Communications of the 
ACM, 8, pp. 627--633. 
Strube, Michael and Paolo Ponzetto, 2006. WikiRe-
late! Computing Semantic Relatedness Using Wi-
kipedia, In Proc.s of American Association for Ar-
tificial Intelligence 2006 
Weeds, Julie and, David Weir, 2006. Co-occurrence 
Retrieval: A Flexible Framework for Lexical Dis-
tributional Similarity. Computational Linguistics, 
31(4), pp 433-475. 
288
