Learning Word Clusters from Data Types 
Pao lo  A l legr in i ,  S imonet ta  Montemagn i ,  V i to  P i r re l l i  
Ist ituto di Linguistica Comlmtazionale - CNR 
Via della Faggiola 32, Pisa, Italy 
{ allegrii),simo,vito } @ilc.pi.cnr.it 
Abst ract  
The paper illustrates a linguistic knowledge ac- 
quisition model making use of data types, in- 
finite nlenlory, and an inferential mechanism 
tbr inducing new intbrmation Dora known data. 
The mode\] is colnpared with standard stochas- 
tic lnethods applied to data tokens, and tested 
on a task of lexico semantic lassification. 
1 I n t roduct ion  and  Background 
Of late, consideral)le interest has been raised by 
the use of local syntactic contexts to automati- 
cally in&me lexico-semantic classes from parsed 
corI)ora (Pereira and Tishby 1992; Pereira ct 
al. 1993; Rooth 1995; Rooth et al 1999). This 
family of approaches takes a 1)air of words (usu- 
ally a verb plus a noun), and a syntactic rela- 
tion holding 1)etween the two in context (llSll- 
ally the ol)ject), and calculates its token distri- 
bution in a training corI)us. These (:omits de- 
line the range of more or less tyi)ical syntac- 
tic collocates elected by a verb. The semat> 
tic similarity between words is then delined in 
terms of sul)stitutability in local contexts (see 
also Grefenstette 1994; I.,in 1998): two verbs 
are semantically close if they typically share the 
same range of collocates; conversely, two nouns 
are semantically close if they take l)art in the 
same tyl)e of selection dependencies, i.e. if they 
arc selected t)y the same verbs, with the same 
function. \]~q'onl this perspective, a syntactically 
asymmetric relation (a dependency) is reinter- 
preted as a semantic co-selection, where each 
term of the relation can be defined with respect 
to the other. 
This symmetric similarity metric is often ac- 
(;omi)anied by the non trivial assuml)tion that 
th, c ,semantic classification of both vcrb,s and 
nouns be symmetric too. This is enforced by 
maximizing I7 ?tj), with  
to 
1--1 
where p(Ck) is the probability of class Ck 1)e- 
ing tbund in the training corpus, and p(vi\[Ck) 
and p(nj\[Ci~) define the probability that verb vi 
and noun nj be associated with the semantic 
dimension (or meaning component) of class C/~. 
Intuitively, the joint distribution of fimctiona.lly 
mmotated verb-noun pairs is accounted for t)y 
assuming thai; each l)air meml)er indel)endently 
correlates with the same semantic dimension, or 
selection type (Rooth 1995), a concel)tual pair 
de.fining all pairs in the class: <g. "scalar mo- 
tion", "communicative action" etc. 
The al)proach has the potential of dealing 
with polysemous words: as the same word can 
in principle belong to more than one (-lass, there 
are good reasons to expect hat the correspond- 
ing selection type positively correlates with one 
and only one sense of polysemous words. A fur- 
ther t)omls of the al)I)roach is that it makes it ex- 
i)licit the perspectivizing factor underlying the 
discovered similarity of words in a class. 
On a less positive side, poorly selective verbs 
(i.e. verbs which potentially combine with any 
1101111) Sllch as give, find or get tend to stick to- 
gether in highly probable classes, 1)ut apl)ear to 
stake out rather uIfinlbrmative senlantic dimen- 
sions, relating a motley collection of 11011118, such 
as part, way, reason and problem (Rooth 1995), 
whose only cominonality is the property of be- 
ing freely interchangeable in the context of the 
above-mentioned verbs. 
Another related issue is how many such di- 
nlens ions are necessary to account br the entire 
variety of senses attested in the training corpus. 
This is an empirical question, but we contend 
an inipor~ant one, as the usal)ility of the result;- 
lag (:lasses heavily dot)ends (m it. It is (:ommon 
knowledge that verbs can 1)c, exceedingly (-hoosy 
ill tile way they select their collocates. Hence, 
one is Mlowed to use the class Ck to make t)re - 
dictions about the set of collocates of a verb 
v~, only if P(v,:\]C~) is sufficiently high. Con- 
versely, if CI~ hal)pens to poorly (:orrelate with 
any verb, the set of nouns in Ck is unlikely 1;o 
reflect any lexical selection. This coml)ounds 
wii;h the 1)rol)lent hat the meaning of a verb vi 
can significantly involve more lihalt Olte Selltall- 
tic (timension: at i;he plesenl; stage of research 
in (:Olnlmtat;iollal lexical SO, lnanti('s, Ire scholar 
has shown what fun(:tion relal;es l;lm nmaning 
components of vi to its sehx:tional behaviom'. 
There is wide room for flu'ther resear(:h in this 
area, but truly ext)lorative tools are still needed. 
Finally, the des(:ribe(t method is a(:ul;c, ly 
t)rone to the 1)roblem of si)arse data. A ll;l)ough 
i,(cl,,,) is rightly ext)e(:ted to converg(, faster 
,:ha:, p(,,l.,), still (:o,,vergcu(:e of,(Cl ' , , )  ,:al, b(, 
ex(:(;edingly sh)w with low frequen('y nouns. It 
is moo|; i;ll;~I; sieving more and more (:()rims (tat~ 
is a solution in all (:ases, its word fl'cquen(:y is 
highly sensitive to changes in text genre, topic 
and domain (S(:hiitze and Pede, rsen 1993). 
2 The  approach  
Ih~re we illustrate a (lifl'er(mt; at)l)roa('h t() a(> 
(is|ring lexico semant;i(" (:lasses from sy~,ta('t;i- 
(:ally lo(-al ('ontexts. Like the family of sto(:has- 
tie metho(ts of se(:tion 1, we make use of ~t 
simibu:ity ntel;ric 1)ased on sul)stitui;ability ill 
(ve, rb,noun,flntction) tril)les. We also share the 
assumption that lexi(:o semantic lasses are in- 
herently multidimensional, as they heavily de- 
pend on cxis|;ence of a perspectivizing factor. 
Yet, we depart from other assmnt)tions. Clas- 
sification of verl)s an(l n(mns is asymmetric: 
two IIOIIIIS {Ll'e similar if (;hey collo(:ate with as 
many semantically diverse vcrl)s as possible in 
as many (tifli:rent syntactic ontexts as l)ossit)le. 
The converse apl)lies to verbs. In other words, 
semantic similarity of nom:s is not conditional 
on the similarity of their ac(:oml)anying verbs, 
and vicevcrsa. In a sells(',, classification brc, aks 
th, c symmetry: maximization of the silnilarity 
of nouns (verbs) may cause minimization of the 
similarity of i;heir accoml)anying verbs (nouns). 
A (:lass where a maximum of noun similarity 
correlates with a lni~ximmn of verb similarity 
cm~ be uninforniative, as exeml)litied above by 
the ease of t)oorly selective verbs. 
Secondly, we assmne (following Fodor 1998) 
that the number of t)erst)ectivizing factors gov- 
erning lexieal selection may have the order of 
magnitude of the lexicon itself. The use of 
global semantic dimensions may smooth out lex- 
ical t)refcrences. This is hardly what we need 
to semantically annotate lexical l)reti;rences. A 
more conservative al)proa(:h to the t)rol)lem, in- 
ducing local semantic ('lasses, (:an (:oml)ine al)- 
1)licability to real language 1)recessing l)roblen~s 
with the fln'l;lmr t)omls of exploring a relatively 
mmharted territory. 
Thirdly, p(vi, nj) ai)t)ears to be too sensitive 
to changes in text genre, tol)ic lind domain to 
be eXl)ectcd to converge reliably. We prefer to 
ground a similarity metric oIx measuring the cor- 
relation among verb noun, type, s ratlw, r than |e- 
ke, as, tbr two basic reasons: i) verl) noun types 
arc (tis(:retc, ;m(t less l)rone t;o random varia- 
tion in it (parsed) (:orpus, ii) verl) noun tyl)eS 
(:ml reliably l)e a(:(tuir(xl from highly intbrm~ttive 
trot hardly redundant knowledge sources uch as 
h~xi(:a n(1 encyclot)aedias. 
Finally, our information refit tbr measuring 
wor(t similarity is not a (:Oul)le of context 
sharing pairs (e.g. (set, sta.ndard,obj) and 
(sct, re.cord,obj)) but a qv, ad'r'uplc, of such con- 
text;s, tbrme(t 1)y (:ombining two verbs with two 
. ( , . ) ) s  ( , . , .  
a.d  ), 
such that they enter an av, ah)gical proportion. 
2.1 The  analogica l  p ropor t ion  
In the t)resenl; conl;ext, an anah)gieal prot)ortion 
(hereafl;er AP)  is a quadrui)le of flmctionally 
mmotated t)airs resulting from tile combination 
of any two ltOllllS 'l~, i and ~3.j wit;h any two verbs 
v/,. and vt su(:h as (2) holds: 
(v~.,ni,f,,,) : (v~.,nj,L,,)= 
(v,,ni, fn) : (vt,nj,fiz), (2) 
where terms along the two diagonals can sw~p 
t)lace in the 1)rot)ortion , and identity of sub- 
s(:ript indi(:ates identity of wflues. Three aspects 
of (2) are worth eml)hasizing in this context. 
First, it does not require that the stone syn- 
tactic time|ion hold 1)etween all pairs, but only 
that time|ions be pairwisc identical. Moreover, 
(2) does not cover all possible syntactic ontexts 
where hi, uj, "vk and vt may coral)|he, but only 
th, ose where verb and .function values co-vary. 
(.set, standard, obj) : (,set, record, obj) = 
(meet, standard, obj) : (,,,.tet:,record, x) (3) 
We call this constraint ile "same-verb-same 
flmction" principle. As we will see in section 2.3, 
the principle has important consequences on tile 
sort of similarity induced by (2). Finally, if one 
uses subscripts as tbrmal constraints on type 
identity, then any term can be derived from (2) 
if the values of all other terms are known. For 
example given tile partially instantiated propof  
tion in (3), the last term is filled in unambigu- 
ously by substituting x = fn = obj. 
AP  is an important generalization of the 
inter-substitutabil ity assumption, as it extends 
tile assumption to cases of flmctionally hetero- 
geneous verb-nonn pairs. Intuitively, an AP 
says that, for two nouns to be taken as sys- 
tematically similar, one has to be ready to 'ase 
th, cm interchangeably in at lea,st wo different lo- 
cal contexts. This is where the inferential and 
the classificatory perspectives meet. 
2.2 Mathemat ica l  background 
We gave reasons for defining the similarity met- 
ric as a flmction of verb-noun type correlation 
rather than verb noun token correlation. In 
this section we sketch the mathematical frame- 
work underlying this assumption, to show that, 
tbr a set of verb nonn pairs with a unique syn- 
tactic function, AP  is the smallest C that sat- 
isfies eq.(1). 
Eq.(1) says that vi and nj are conditionally 
independent given C, meaning that their corre- 
lation only det)ends on the probability of their 
belonging to C, as tbrmally restated in eq.(4). 
p(n, vlC) = p(nlC)p(vlC) (4) 
In passing from token to type frequency, we as- 
sume that a projection operator simply assigns 
a mfitbrm type probability to each event (pair) 
with a nonzero token probability in the train- 
ing corpus. From a learning perspective, this 
corresponds to the assumption that an infinite 
memory filters out events already seen during 
training. The type probability pT(n,v) is de- 
fined as in eq.(5), where Np is the number of 
different pairs attested in the training corpus. 
pT(n,v) = 1/Np if the pair is attested, 
pT(n,v) = 0 otherwise. (5) 
By eq.(4), pT(n, vlC ) ? 0 if and only if 
pT(nlC) ~ 0 and pT(vlC) ~ O. This amounts 
to saying that all verbs in C are freely inter- 
changeable in the context of all nouns in C, and 
viceversa. We will hereafter efer to C as a sub- 
stitutability island ( SI). AP  can accordingly be 
looked at as the minimal SI. 
The strength of correlation of nouns and 
verbs in each S I  can be measured as a sum- 
mation over the strength of all APs where they 
enter. Formally, one can define a correlation 
score or(v, n) as the probability of v and n be- 
ing attested in a pair. This can be derived from 
our definition of pr(v ,n) ,  as shown in eq.(6), 
by substituting pT(n,v) = pr(v)pT(nlv) and 
pT(nlv ) = 1/w(v), where w(a) is tile type fi'e- 
quency of a (i.e. number of different attested 
pairs containing a). 
1 
- = = 
G 
(6) N,, G 
Eq.(6), after simplification, yields the tbllowing 
o< (7) 
By the same token, the correlation flmction 
c7(AP) relative to the 4 possit)le pairs in AP 
is calculated as 
cr(dP) = 1)7'('/)11~3,1 )PSF (~?,2 \[V\] )pT(V2 I'rl,2)PT(7~l \[?)2 ) 
(3( \[C0(~%1)C0(Vl )C0(~'1,2)C0('02)\] -1. (g) 
Eq.(8) captures the intuition that the corre- 
lation score between verbs and nouns in AP 
is an inverse function of their type frequency. 
Nouns and verbs with high type frequency oc- 
cur in many different pairs: the less selective 
they are, the smaller their semantic contribu- 
tion to cr(AP). 
Our preference tbr a(AP) over el(v, n) under- 
lies the definition of correlation score of S I  given 
in eq.(9) (see also section 4). 
-_- Z (9) 
APESI 
2.3 Breaking the symmetry  
In section 2.2 we assumed, for the sake of sim- 
plicity, that verbs and nouns are possibly re- 
lated through one syntactic function only. In a 
10 
proportion like (2), however, l;he syntactic time- 
tion is allowed to wtry. Nonetheless each rclal;ed 
S\] contains nouns which always combine with 
a given verb with one and the .same syntactic 
./:unction. Clearly, the Sallle is no|; true of verbs. 
Suppose that an S1 contains two verbs vk and vt 
(say drive and pierce) and two nouns ni and nj 
(say nail and peg) that ~re respecl;ively el)jeer 
and subject of 'vl~ and yr. The type of similar- 
ity in the resulting n(mn and verb clusters is 
of a completely ditti;rent nature: in the case of 
n(mns, we acquire dist'rib,utionally parallel words 
(e.g. nail and peg); in the case of verbs, we 
get distrib'utionaIly correlated words (say drive 
and pierce) which are not interchangeable in the 
same conl;exl;. Mixing the two types of distribu- 
tional similarity in the same class makes little 
sense. Hereafter, we will aim at maximizing the 
similarity of disl;ributionally parallel nouns. In 
doing so, we will use functionally hel;erogencous 
contexts as in (2). This breaks classitication 
symlne|,ry, and there is no guarantee I;hal; se- 
mantically coher(mt verb clust;ers be rcl;m'ncd. 
3 The  method  
The section illusl;ratcs an at)plication of the 
principles of section 2 1;() 1;t5o task of clustering 
the set of object;s ot! a vo, rl) on t;he basis of a 
repository of flmctionally mmol;al;ed cont(;xts. 
3.1 The  knowledge  base 
The training evidence is a Knowledge. Base 
(KB)  of flm('tionally anno|;ated verb noun 
1)airs, ins|;mltiating a wide rallg~e of syntactic re- 
lations: 
a) vert)objecl;, e.g. (,~'a,,,.~,,,'c, ~, 'ol, lc,,z, obj) 
'cause-1)roblenl'; 
b) verb subject, e.g. (capitare, pwblcma subj) 
'occur-problem'; 
c) verb prepositional_complement, e.g. (recap- 
pare, probIcma, in,) 'run_into-problenl'. 
The KaY contains 43,000 pair types, auto- 
matically extracted from different knowledge 
sources: dictionaries, both bilingual and mono- 
lingual (Montemagni 1995), and a corpus of ti- 
nancial newspapers (Federici st al. 1998). The 
two sources rettect two ditt'erent modes of lexi- 
cal usage: dictionaries give typical examples of 
use of a word, and rmming corpora attest ac- 
tual usage of words in specific enfl)edding do- 
mains. These differences have all impact on 
the typology of senses which the two sources 
1)rovi(le evidence for. General dictionaries tes- 
titly all possible senses of a given word; tyl)ical 
word collocates acquired from dictionaries tend 
to cover the entire range of possible senses of 
a headword. On the other hand, unrestricted 
texts reIlect actual usage and possibly bear wit- 
hess to senses which are relevant to a specific 
domain only. 
3.2 The  i nput  words  
There is abundant psycholinguistic evidence 
that semantic similarity between words is em- 
inently conI;exl; sensitive (Miller and Charles 
1991). Moreover, in many language processing 
tasks, word similarity is typically judged rela- 
tive to an actual context, as in the cases of 
syntactic disambiguation (both structural and 
fulwtional), word sense disambiguation, and se- 
lection of the contextually approt)riate transla- 
1;ion equiwflent of a word given its neighl)ouring 
words. Finally, close examination of real data 
shows that (titl'erellt word senses elect classes of 
complements according to different dilnensions 
of semantic similarity. This is so pervasive, that 
it soon be(:omes impossit)le to t)rovide an efl'ec- 
live account of these, dimensions independently 
of the sense in question. 
Ewduation of botll accuracy and usability of 
any autontatic classitication of words into se- 
mantic clusters cammt lint artificially elude th(; 
1)asic question "similar in what respc, ct;?". Our 
choice of input words retlects these concerns. 
\?e automatically clustered the set of objects of 
a given verb, as they arc attested in a test co l  
pus. This yields local lexico seman{;ic lasses, 
i.e. conditional on the selected verb head, as 
opposed to global classes, i.e. built once and 
tbr all to accomlt tbr the collocates of any verb. 
Among the practical advantages of local clas- 
sitication we should at least mention the follow- 
ing two. Choice of a verb head as a perspec- 
tivizing factor considerably reduces the possi- 
bility that the same polysemous object collocate 
is used in different senses with the same verb. 
Fnrthermore, the resulting clusters can give in- 
tormal;ion about the senses, or meaning facets, 
of the verb head. 
a.a Ident i f icat ion and ranking of  noun 
clusters 
For the sake of concreteness, let us consider the 
tbllowing object-collocates of the Italian verb 
11 
{APPESANTIMENTO~ CRESCITA, 
FLESSIONE, RIALZO} 
{CRESCITA, FLESSIONE} 
{CRESCITA, GUAI0} 
{CRESCITA, PROBLEMA} 
{CRESCITA, RITARD0} 
{FLESSIONE, PROBLEMA} 
{FLESSIONE, RIALZO} 
{GUAI0, PROBLEMA} 
{RIDIMENSIONAMENT0, RITARD0} 
Figure h Some Sis 
headword causarc. 
: CAUSARE/O, REGISTRARE/O 
: CAUSARE/O, EVIDENZIARE/S, 
MEDIARE/O, MOSTRARE/O~ 
PRESENTARE/S, PRESENTIRE/O 
REGISTRARE/O, REGISTRARE/$ 
: CAUSARE/0, PROVOCARE/0 
AVERE/S~ CAUSARE/0, EVIDENZIARE/0 
PORRE/S, PRESENTARE/S 
CAUSARE/O, USARE/S 
CAUSARE/0, PRESENTARE/S, STARE/S 
CAUSARE/0, REGISTRARE/0 
REGISTRARE/S, SUBIRE/0 
CAUSARE/0, CAVARE -- SI/S, INCAPPARE/S 
CAUSARE/O, GIUSTIFICARE/O 
relative to the collocates of the 
causare 'cause', as they are found in a test cor- 
pus: 
appcsantimento 'increase in weight', 
crescita 'growth', flessionc 'decrease', 
guaio 'trouble', p~vblcma 'prol)lem', rialzo 
'rise', ridimensionamcnto 'reduction', 
ritardo 'delay', turbolenza 'turbulence'. 
Clustering these input words requires prelim- 
inary identification of Substitutability Islands 
(Sis). An example of SI  is the quadruple 
tbrmed by the verb pair causare 'cause' and in- 
eapparc 'rnn into' and the noun pair guaio 'trou- 
ble' and problema 'problem', where menfl)ers of 
the same pair are inter-substitutable in context, 
give:: the constraints entbrced by the AP type 
in (2). Note that guaio and problema are ob- 
jects of eausare, and prepositional complements 
(headed by in 'in') of incappare. This makes it 
possible to maximize the sinfilarity of trouble 
and problem across fimctionally heterogeneous 
contexts. 
Bigger Sis than the one just shown will form 
as many APs as there are quadruples of col:- 
textually interchangeable nouns and verbs. We 
consider a lexico-semantic cluster of nouns the 
projection of an SI  onto the set of nouns. Fig.1 
illustrates a sample of noun clusters (between 
curly brackets) projected from a set of Sis, to- 
gether with a list of the verbs tbund in the same 
Sis (the suffix 'S' stands tbr subject, and 'O' 
for object). Due to the asymmetry of classifica- 
tion, verbs in Sis are not taken to tbrm part of 
a lexico-semantic cluster in the same sense as 
nonns  are .  
7.04509e-O5{GUAIO,PKOBLEMA} 
7.01459e-O5{RIDIMENSIONAHENTO,RITARDO} 
4.65858e-O5{CRESCITA,FLESSIONE} 
I.T5699e-O5{FLESSIONE,RIALZO} 
9.49509e-O6{APPESANTIMENTO,CRESClTA,FLESSIONE,RIALZO} 
1.88964e-O6{CRESCITA,GUAIO} 
1.19814e-O6{CRESCITA,RITARDO} 
8.84254e-OT{CRESCITA,PROBLEMA} 
6,TI41e-OT{FLESSIONE,PROBLEMA} 
Figure 2: Nine top-most scored noun clusters 
Not all projected noun clusters exhibit the 
same degree of semantic oherence. Intuitively, 
the cluster {appesantimento crescita flessione 
riaIzo} 'increase in weight, growth, decrease, 
rise' is semantically more appeMing tlmn the 
cluster {crescita problema} 'growth problem' 
(Fig.l). 
A quantitative measure of the sen:antic ohe- 
sion of a noun cluster CN is give:: by the con'e- 
lation score ~(SI) of the SI  of which UN is a 
projection. In Fig.2 noun clusters are ranked by 
decreasing vahms of cr(SI), calculated according 
to eq.(9). 
3.4 Cent ro id  ident i f i ca t ion  
Noun clusters of Figs.1 and 2 are admittedly 
considerably fine grained. A coarser grain can 
be attained trivially through set union of inte:'- 
secting clusters. In fact, what we want to obtain 
is a set of mazimally orthogonal and semanti- 
cally coherent noun classes, under the assump- 
tion that these (:lasses highly correlate with the 
principal meaning components of the verb head 
of which input nouns are objects. 
In the algorithm ewfluated here this is 
achieved in two steps: i) first, we select the 
best possible centroids of the prospective classes 
among the noun clusters of Fig.2; secondly, 
ii) we lmnp outstanding clusters (i.e. clusters 
which have not been selected in step i)) around 
the identified centroids. In what tbllows, we will 
only focus on step i). Results and evaluation of 
step ii) are reported in (Allegrini et al 2000). 
In step i) we assume that centroids are 
disjunctively defined, maximally coherent 
classes; hence, there exists no pair of intersect- 
ing centroids. The best possible selection of 
centroids will include non intersecting clusters 
with the highest possible cumulative score. In 
practice, the best centroid corresponds to the 
12 
cluster with the topmost cr(SI). The second 
best centroid is the cluster with the second 
highest o-(SI) and no intersection with the first; 
centroid, and so on (the i-th centroid is 1;11(', 
i.-th highest chlster with no intersection with 
the tirst i - 1. centroids) until all clusters in the 
rank are used Ill). Clusters selected as centroids 
in the causare example above ~tre: {GUAIO 
PROBLEMA}, {RIDIMENSIONAMENTO RITARDO}, 
{CaP.SCITA FL~.SS-rONE}. 
Clearly, this is not the only t)ossible strategy 
t'or centroid selection, lint certainly a suitabh; 
one giv(;n our assulnl)tions mM goals. To stun 
Ul) , the targeted classification is local, i.('., con- 
ditional on a specific verl~ head, and orthogonal , 
i.c. it aims at identifying maximally disjulmtive 
classes with high correlation with the principal 
meaning ('Oral)orients of the vert) head. This 
strategy h;ads to identificalfion of the different 
senses, or possibly me,ruing facets, of a vert). 
In tlteir turn, noun clusters may capture sul)- 
tle semm~tic distinctions. For instance,, a dis- 
tinction is made between incremental eveni;s or 
results of incrt'anental e, vents, which 1)resut)l)OSe 
~ scalar dimension (as in the ease of {cre,,s'cita 
,/lcssione} 'growth, decrease') and re, scheduling 
eve, nts, where a change occurs with respect to 
a previously planned event or object (see the 
centroid {ridimensiov, amento  ritardo} :reduc- 
tion debw' ). 
4 Exper iment  and  eva luat ion  
We, were able to extract all S l 's relative to the 
entire K\]3.  However, we report here an intr ins ic 
evaluation of the accuracy of acquired ce.ntroids 
which involves ()lily a small subset of our results, 
since provision of a refhrence class tyl)ology is 
extremely labour intensive. 1 
We consider 20 Italian verbs and their object 
collocates.gThe object collocates were automat- 
ically extracted fi'om the "Italian SPAIIKLE 
Reference Corpus", a corpus of Italian financial 
1For an extrinsic evahlation of the proposed similar- 
ity measure the reader is referred to (Montemagni et aI. 
1996; Briscoe ct al. 1999; Federici ct al. 1999a). 
2The |x,'st verbs are: a.qgiungcrc 'add', aiutare 'hell)' , 
a,sl)cttarc 'expect', cambiarc 'change', C(t*taO,?'t: t(:}tllSe:~ 
chicdcrc 'ask', considc.rarc 'consider', dare. 'give', dc- 
ciderc 'decide', fornive 'provide', muoverc. 'move', pcrm~'.- 
ttere 'allow', portarc 'bring', p~wlurrc 'produce', sccglicrc 
'choose', sentirc 'feel', stabilire 'establislF, tagliarc 'cut', 
terminate 'end', trovarc 'find'. 
newspapers of about one million word tokens 
(Federici ct al. 1998). 
l?or each test verb, an indetmndent classifica- 
tion of its collocates was created lnanually, by 
partitioning the collocates into disjoint sets of 
semantically coherent lexical preferences, each 
set pointing to distinct senses of the test; verb, 
according to a reference monolingual dictionary 
(Garzanti 1984). This considerably reduces the 
anlount of subjectivity inevitably involved in 
the creation of a reference partition, and mini- 
mizes the probability that more than one sense 
of a t)olysemous noun can appear in the same 
class of collocates. 
The inferred centroids, selected from clusters 
ranked by c~(SI) defined as in (9), are t)rojected 
~gainst he reference classification. Precision is 
delined as the ratio between 1;t1(; mmflmr of con- 
troids t)roperly inchlded in one reference class 
and l;he nmnber of inferred centroids. Recall is 
defined as the ratio between the number of relhr- 
time classes which properly inchlde at least one 
centroid an(t the nmnber of all reference classes. 
Fig.3 shows results for the sets of object collo- 
cates of 1)olysemous {;est verbs only, as lttOltOSe- 
mous verbs trivially yMd 100% precision recall. 
An average, wflue over the sets of object col- 
locates of all verbs is also shown, with 86% 
88% of precision recall. Another average value 
is also l)lotted (as a black ul)right triangle), ol)- 
l:ained \])y ranking n(mn clusters l)y ~(S\]) calcu- 
lated as ill (10). This average wflue (53% 53% 
precision recall) provides a sort of baseline of 
the difliculty of the task, and sheds consider- 
able light on the use of APs ,  rather than simple 
verb noun pairs, as inforlnation units ibr mea- 
suring internal cohesion of centroids. 
 (si) =_ (10) 
(n,v)G91 
5 Conc lus ion  
We described a linguistic knowledge acquisition 
model and tested it; on a word classification task. 
The main points of our prot)osal are: 
? classification is asymmetric, grounded on 
principles of machine learning with intinite 
memory; 
? the algorithm is explorative and non- 
reductionist; no a priori  model of class dis- 
13 
' i . . . . . . . . . . . .  T L . . . . . . . . .  i:i:  
! AIUTARE 
I ICAMBIARE 
0.9 CHIEDERE 
CONSIDERARE ? 
- bARE 
' ,?DECIDERE 
0.8 PORTARE 
PRODURRE 
? SCEGLIERE ~ SENTIRE STABILIRE 
~TAGLIARE \ [ :1~ ? 
~:TFIOVA RE 
0.6 ~'AVERAGE(AP) 
~AVERAGE(pai 0 
i ? i 
o.5 ::.i ? ' 1  
I 
i 
0.3 0.4 0.5 0.6 0,7 0.8 0.9 1 
PRECISION 
d < ~ 0.7 
Figure 3: Centroid precision and recall for object 
collocates of polysemous verbs. 
tril)ution is assmned; 
? classification is modelled z~s the task of 
forming a web of context dependent se- 
mantic associations among words; 
? the approach uses a context--sensitive no- 
tion of semantic similarity; 
? the approach rests on the notion of analog- 
ical proportion, which proves to t)e a reli- 
able intbrmation refit for measuring seman- 
tic similarity; 
? analogical t)roportions are harder to track 
down than simple pairs, and interconnected 
in a highly complex way; yet, reliance on 
data types, as opposed to token frequen- 
cies, makes the proposed method comtm- 
rationally tractable and resistant o data 
sparseness. 
References  
Allegrini P., Montemagni S., Pirrelli V. (2000) Con- 
trolled Bootstrapt)ing of Lexico-semantic Classes 
as a Bridge between Paradigmatic and Syntag- 
matic Knowledge: Methodology and Evaluation. 
In Precccdings of LREC-2000, Athens, Greece 
May-June 2000, pp. 601-608. 
Briseoe T., McCarthy D., Carroll J., Allegrini P., 
Calzolari N., Federici S., Montemagni S., Pir- 
relli V., Abney S., Beil F., Carroll G., Light M., 
Prescher D., Riezler S., Rooth M. (5999) Acqui- 
sition System for Syntactic and Semantic Type 
and Selection. Deliverable 7.2. WP 7, EC project 
SPARKLE "Shallow Parsing and Knowledge Ex- 
traction for Language Engineering" (LE-2111). 
Federici, S., Montemagni, S., Pirrelli, V. (1999) 
SENSE: an Analogy based Word Sense Disam- 
biguation System. In M. Light and M. Pahner 
(eds.), Special Issue of Natural Language Engi- 
neerin 9on Lexical Semantic Tagging. 
Federici, S., Montemagni, S., Pirrelli, V., Calzolari, 
N. (1998) Analogy-based Extraction of Lexical 
Knowledge from Corpora: the SPARKLE Expe- 
rience. In Proceedings of LREC-1998, Granada, 
SP, May 1998. 
Fodor, J.A. (1998) Concepts. Where Co.qnitivc Sci- 
ence Went Wzvng. Clarendon Press, Oxtbrd, 
1998. 
Garzanti (1984) ll Nuovo Dizionario Italiano 
Garzanti. Garzanti, Milano, 1984. 
Grefenstette, G. (1994) Explorations in Automatic 
Thesaurus Discovery. Kluwer Acadenfic Publish- 
ers, Boston, 1994. 
Lin D. (1998) Automatic Retrieval and Clustering 
of Similar Words. In Proceedings of COLING- 
A CL'98, Montreal, Canada, August 1998. 
Miller, G.A., Charles, W.G. (1991) Contextual Cor- 
relates of Semantic SimilariW. In Language and 
Cognitive Processes, 6 (1), pp. 1-28. 
Montemagni, S. (1.995) Subject and Object in Italian 
Sentence Processing. PhD Dissertation, UMIST, 
Manchester, UK, 1995. 
MontemagIfi, S., Federici, S., Pirrelli, V. (1996) 
Resolving syntactic ambiguities with lexico 
semantic patterns: an analogy-based apl)roach. 
In Proceedings of COLING-96, Copenhagen, Au- 
gust 1996, pp. 376 381. 
Pereira, F., Tishby, N. (1992) Distributional Sim- 
ilarity, Phase Transitions and Hierarchical Clus- 
tering. In Working Notes, Fall Symposium Series. 
AAAI, pp. 54 64. 
Pereira, F., Tishby, N., Lee, L. (1993) Distrilmtional 
Clustering Of English Words. In Proceedings of 
the 30th Annual Meeting of the Association for 
Computational Linguistics, pp. 183 190. 
Rooth, M. (Ms) Two-dimensional clusters in gram- 
matical relations. In Symposium on 12epresenta- 
tion and Acquisition of Lcxical Knowledge: Pol- 
ysemy, Ambiguity and Gcncrativity, AAAI 1995 
Spring Symposium Series, Stanford University. 
Rooth, M., Riezler, S., Prescher, D., Carroll, G., 
Bell, F. (1999) Inducing aSemanticallt Annotated 
Lexicon via EM-Based Clustering. In Procccdings 
of the 37th Annual Meeting of the Association for' 
Computational Linguistics, Maryland, USA, June 
1999, I)P. 104-111. 
Schfitze, H., Pedersen, J. (1993) A vector model 
for syntagmatic and paradigmatic relatedness. In 
Proceedings of the 9th Annual Confcrcncc of the 
UW Centrc for" the New OED and Text 12cscarch, 
Oxford, England, 1993, pp. 104-113. 
14 
Grammar and Lexicon in the Robust Parsing of Italian 
Towards a Non-Na?ve Interplay 
 
Roberto  
BARTOLINI 
Istituto di Linguistica 
Computazionale CNR 
Area della Ricerca 
Via Moruzzi 1 
56100 PISA (Italy) 
Alessandro  
LENCI 
Universit? di Pisa 
Via Santa Maria 36 
56100 PISA (Italy) 
Simonetta  
MONTEMAGNI 
Istituto di Linguistica Com-
putazionale CNR 
Area della Ricerca 
Via Moruzzi 1 
56100 PISA (Italy) 
Vito  
PIRRELLI 
Istituto di Linguistica 
Computazionale CNR 
Area della Ricerca 
Via Moruzzi 1 
56100 PISA (Italy) 
 
{roberto.bartolini, alessandro.lenci, simonetta.montemagni, vito.pirrelli}@ilc.cnr.it 
 
Abstract 
In the paper we report a qualitative evalua-
tion of the performance of a dependency 
analyser of Italian that runs in both a non-
lexicalised and a lexicalised mode. Results 
shed light on the contribution of types of 
lexical information to parsing.    
Introduction 
It is widely assumed that rich computational 
lexicons form a fundamental component of reli-
able parsing architectures and that lexical infor-
mation can only have beneficial effects on 
parsing. Since the beginning of work on broad-
coverage parsing  (Jensen 1988a, 1988b), the 
key issue has been how to make effective use of 
lexical information. In this paper we put these 
assumptions to the test by addressing the follow-
ing questions: to what extent should a lexicon be 
trusted for parsing? What is the neat contribution 
of lexical information to overall parse success? 
We present here the results of a preliminary 
evaluation of the interplay between lexical and 
grammatical information in parsing Italian using 
a robust parsing system based on an incremental 
approach to shallow syntactic analysis. The sys-
tem can run in both a non-lexicalised and a lexi-
calised mode. Careful analysis of the results 
shows that contribution of lexical information to 
parse success is more selective than commonly 
assumed,  thus raising the parallel issues of how 
to promote a more effective integration between 
parsers and lexicons and how to develop better 
lexicons for parsing.  
1 Syntactic parsing lexicons 
Syntactic lexical information generally feeds 
parsing systems distilled in subcategorization 
frames. Subcategorization is a formal specifica-
tion of a predicate phrasal context in terms of the 
type of arguments syntactically selected by the 
predicate entry (e.g. the verb hit selects for a 
subject NP and an object NP). Lexical frames 
commonly include: i.) number of selected argu-
ments, ii.) syntactic categories of their possible 
realization (NP, PP, etc.), iii.) lexical constraints 
on the argument realization (e.g. the preposition 
heading a PP complement), and iv.) the argu-
ment functional role. Other types of syntactic in-
formation that are also found in syntactic 
lexicons are: argument optionality, verb control, 
auxiliary selection, order constraints, etc. On the 
other hand, collocation-based lexical informa-
tion is only rarely provided by computational 
lexicons, a gap often lamented in robust parsing 
system development. 
A number of syntactic computational lexi-
cons are nowadays available to the NLP com-
munity. Important examples are LDOCE 
(Procter 1987), ComLex (Grishman et al 1994), 
PAROLE (Ruimy et al 1998). These lexicons 
are basically hand-crafted by expert lexicogra-
phers, and their natural purpose is to provide 
general purpose, domain-independent syntactic 
information, covering the most frequent entries 
and frames. On the other hand, parsing systems 
often complement general lexicons with corpus-
driven, automatically harvested syntactic infor-
mation (Federici et al 1998b, Briscoe 2001, 
Korhonen 2002). Automatic acquisition of sub-
categorization frames allows systems to access 
highly context dependent constructions, to fill in 
possible lexical gaps and eventually rely on fre-
quency information to tune the relative impact of 
specific frames (Carroll et al 1998). 
Lexicon coverage is usually regarded as the 
main parameter affecting use of lexical informa-
tion for parsing. However, the real comparative 
impact of the type (rather than the mere quan-
tity) of lexical information has been seldom dis-
cussed. Our results show that the contribution of 
various lexical information types to parse suc-
cess is not uniform. The experiment focuses on a 
particular subset of the information available in 
syntactic lexicons - the representation of PP 
complements in lexical frames - tested on the 
task of PP-attachment. The reason for this 
choice is that this piece of information occupies 
a central and dominant position in existing lexi-
cons. For instance in the Italian PAROLE lexi-
con, more than one third of verb frames contain 
positions realized by a PP, and this percentage 
raises up to the near totality noun-headed 
frames. 
2 Robust Parsing of Italian 
The general architecture of the Italian parsing 
system used for testing adheres to the following 
principles: 1) modular approach to parsing, 2) 
underspecified output (whenever required), 3) 
cautious use of lexical information, generally re-
sorted to in order to refine and/or further specify 
analyses already produced on the basis of 
grammatical information. These principles un-
derlie other typical robust parsing architectures 
(Chanod 2001, Briscoe and Carroll 2002). 
The system consists of i.) CHUNK-IT 
(Federici et al 1998a), a battery of finite state 
automata for non-recursive text segmentation 
(chunking), and ii.) IDEAL (Lenci et al 2001), a 
dependency-based analyser of the full range of 
intra-sentential functional relations (e.g. subject, 
object, modifier, complement, etc.). CHUNK-IT 
requires a minimum of lexical knowledge: 
lemma, part of speech and morpho-syntactic fea-
tures. IDEAL includes in turn two main compo-
nents: (i.) a Core Dependency Grammar of 
Italian; (ii.) a syntactic lexicon of ~26,400 sub-
categorization frames for nouns, verbs and ad-
jectives derived from the Italian PAROLE 
syntactic lexicon (Ruimy et al 1998). The 
IDEAL Core Grammar is formed by ~100 rules 
(implemented as finite state automata) covering 
major syntactic phenomena,1 and organized into 
structurally-based rules and lexically-based 
rules. IDEAL adopts a slightly simplified ver-
sion of the FAME annotation scheme (Lenci et 
al. 2000), where functional relations are head-
based and hierarchically organised to make pro-
vision for underspecified representations of 
highly ambiguous functional analyses. This fea-
ture allows IDEAL to tackle cases where lexical 
information is incomplete, or where functional 
relations cannot be disambiguated conclusively 
(e.g. in the case of the argument vs. adjunct dis-
tinction). A ?confidence score? is associated 
with some of the identified dependency relations 
to determine a plausibility ranking among dif-
ferent possible analyses. 
In IDEAL, lexico-syntactic information inter-
venes only after possibly underspecified de-
pendency relations have been identified on the 
basis of structural information only. At this sec-
ond stage, the lexicon is accessed to provide ex-
tra conditions on parsing, so that the first stage 
parse can be non-monotonically altered in vari-
ous ways (see section 3.3). This strategy mini-
mises the impact of lexical gaps (whether at the 
level of lemma or of the associated subcategori-
zation frames) on the system performance (in 
particular on its coverage). 
3 The Experiment 
3.1 The Test Corpus (TC) 
The test corpus contains a selection of sentences 
extracted from the balanced partition of the Ital-
ian Syntactic Semantic Treebank (ISST, Mon-
temagni et al 2000), including articles from 
 
1 Adjectival and adverbial modification; negation; (non-
extraposed) sentence arguments (subject, object, indirect 
object); causative and modal constructions; predicative 
constructions; PP complementation and modification; em-
bedded finite and non-finite clauses; control of infinitival 
subjects; relative clauses (main cases); participial construc-
tions; adjectival coordination; noun-noun coordination 
(main cases); PP-PP coordination (main cases); cliticiza-
tion. 
contemporary Italian newspapers and periodicals 
covering a high variety of topics (politics, econ-
omy, culture, science, health, sport, leisure, etc.). 
TC consists of 23,919 word tokens, correspond-
ing to 721 sentences (with a mean sentence 
length of 33.18 words, including punctuation to-
kens). The mean number of grammatical rela-
tions per sentence is 18. 
3.2 The Baseline Parser (BP) 
The baseline parser is a non-lexicalised version 
of IDEAL including structurally-based rules 
only. The mean number of grammatical relations 
per sentence detected by BP in TC is 15. 
The output of the baseline parser is shallow in 
different respects. First, it contains underspeci-
fied analyses, resorted to whenever available 
structural information does not allow for a more 
specific syntactic interpretation: e.g. at this level, 
no distinction is made between arguments and 
modifiers, which are all generically tagged as 
?complements?. Concerning attachment, the sys-
tem tries all structurally-compatible attachment 
hypotheses and ranks them according to a confi-
dence score. Strong preference is given to 
rightmost attachments: e.g. a prepositional com-
plement is attached with the highest confidence 
score (50) to the closest, or rightmost, available 
lexical head. In the evaluation reported in section 
4, we consider top-ranked dependents only, i.e. 
those enforcing rightmost attachment. Moreover, 
in matching the relations yielded by the parser 
with the ISST relations in TC we make allowance 
for one level of subsumption, i.e. a BP relation can 
be one level higher than its ISST counterpart in 
the hierarchy of dependency relations. Finally, the 
BP output is partial with respect to those depend-
encies (e.g. a that-clause or a direct object) that 
would be very difficult to identify with a suffi-
cient degree of confidence through structurally-
based rules only.  
3.3 The Lexically-Augmented Parser (LAP) 
The lexically-augmented version of IDEAL in-
cludes both structurally-based and lexically-
based rules (using the PAROLE lexicon). In this 
lexically-augmented configuration, IDEAL first 
tries to identify as many dependencies as possi-
ble with structural information. Lexically-based 
rules intervene later to refine and/or complete 
structurally-based analyses. Those structurally-
based hypotheses that find support in the lexicon 
are assigned the highest score (60). The contri-
bution of lexically-based rules is non-monotonic:
old relations can eventually be downgraded, as 
they happen to score, in the newly ranked list of 
possible relations, lower than their lexically-
based alternatives. Furthermore, specification of 
a former underspecified relation is always ac-
companied by a re-ranking of the relations iden-
tified for a given sentence; from this re-ranking, 
restructuring (e.g. reattachment of complements) 
of the final output may follow. 
LAP output thus includes: 
a) fully specified dependency relations: e.g. an 
underspecified dependency relation such as 
?complement? (COMP), identified by a struc-
turally-based rule, is rewritten, when lexi-
cally-supported, as ?indirect object? (OBJI) 
and assigned a higher confidence value; 
b) new dependency relations: this is the case, 
for instance, of that-clauses, direct objects 
and other relation types whose identification 
is taken to be too difficult and noisy without 
support of lexical evidence; 
c) underspecified dependency relations, for 
those cases that find no lexical support. 
The mean number of grammatical relations per 
sentence detected by LAP in TC is 16. In the 
evaluation of section 4, we consider top-ranked 
dependents only (confidence score  50), corre-
sponding to either lexically-supported dependency 
relations or ? in their absence ? to rightmost at-
tachments. Again, in matching the relations 
yielded by the parser with the ISST relations in 
TC we make allowance for one level of subsump-
tion. 
4 Analysis of Results 
The parsing outputs of BP and LAP were com-
pared and projected against ISST annotation to 
assess the contribution of lexical information to 
parse success. In this paper, we focus on the 
evaluation of how and to which extent lexico-
syntactic information contributes to identifica-
tion of the proper attachment of prepositional 
complements. For an assessment of the role and 
impact of lexical information in the analysis of 
dependency pairs headed by specific words, the 
interested reader is referred to Bartolini et al
(2002). 
4.1 Quantitative Evaluation 
Table 1 summarises the results obtained by the 
two different parsing configurations (BP and 
LAP) on the task of attaching prepositional 
complements (PC). Prepositional complements 
are classified with respect to the governing head: 
PC_VNA refers to all prepositional comple-
ments governed by V(erbal), N(ominal) or 
A(djectival) heads. PC_V is the subset with a 
V(erbal) head and PC_N the subset with a 
N(ominal) head. For each PC class, precision, 
recall and f score figures are given for the differ-
ent parsing configurations. Precision is defined 
as the ratio of correctly identified dependency 
relations over all relations found by the parser 
(prec = correctly identified relations / total num-
ber of identified relations); recall refers to the ra-
tio of correctly identified dependency relations 
over all relations in ISST (recall = correctly 
identified relations / ISST relations). Finally, the 
overall performance of the parsing systems is 
described in terms of the f score, computed as 
follows: 2 prec recall / prec + recall. 
 
BP LAP ISST 
Prec recall F score Prec recall f score 
PC_VNA 3458 75,53 57,40 65,23 74,82 61,02 67,22
PC_V 1532 75,43 45,50 56,76 74,23 49,50 61,22
PC_N 1835 73,53 80,82 77,00 72,76 81,36 76,82
Table 1. Prepositional complement attachment in BP and LAP 
 
Table 2. Lexicalised attachments 
 
To focus on the role of the lexicon in either con-
firming or revising structure-based dependen-
cies, lexically-supported attachments are singled 
out for evaluation in Table 2. Their cumulative 
frequency counts are reported in the first three 
columns of Table 2 (?Lexicalised attachments?), 
together with their distribution per head catego-
ries. Lexicalised attachments include both those 
structure-based attachments that happen to be 
confirmed lexically (?Confirmed attachments?), 
and restructured attachments, i.e. when a prepo-
sitional complement previously attached to the 
closest available head to its left is eventually re-
assigned as the dependent of a farther head, on 
the basis of lexicon look-up (?Restructured at-
tachments?). Table 2 thus shows the impact of 
lexical information on the task of PP attachment. 
In most cases, 89% of the total of lexicalised at-
tachments, LAP basically confirms dependency 
relations already assigned at the previous stage. 
Newly discovered attachments, which are de-
tected thanks to lexicon look-up and re-ranking,  
amount to only 11% of all lexicalised attach-
ments, less than 3% of all PP attachments 
yielded by LAP.  
4.3 Discussion 
4.3.1 Recall and precision on noun and verb 
heads 
Let us consider the output of BP first. The strik-
ing difference in the recall of noun-headed vs 
verb-headed prepositional attachments (on com-
parable levels of precision, rows 2 and 3 of Ta-
ble 1) prompts the suggestion that the typical 
context of use of a noun is more easily described 
in terms of local, order-contingent criteria (e.g. 
rightmost attachment) than a verb context is. We 
can give at least three reasons for that. First, 
frame bearing nouns tend to select fewer argu-
 Lexicalised atts Confirmed atts Restructured atts 
total OK prec Total OK prec total OK prec 
PP_VNA 919 819 89,12 816 771 94,49 103 65 63,11
PP_V 289 244 84,43 201 194 96,52 88 61 69,32
PP_N 629 575 91,41 614 577 93,97 15 4 26,67
ments than verbs do. In our lexicon, 1693 verb-
headed frames out of 6924 have more than one 
non subject argument (24.4%), while there being 
only 1950 noun-headed frames out of 15399 
with more than one argument (12.6%). In TC, of 
2300 head verb tokens, 328 exhibit more than 
one non subject argument (14%). Rightmost at-
tachment trivially penalises such argument 
chains, where some arguments happen to be 
overtly realised in context one or more steps re-
moved from their heads. The second reason is 
sensitive to language variation: verb arguments 
tend to be dislocated more easily than noun ar-
guments, as dislocation heavily depends on sen-
tence-level (hence main verb-level) phenomena 
such as shift of topic or emphasis. In Italian, 
topic-driven argument dislocation in preverbal 
position is comparatively frequent and repre-
sents a problem for the baseline parser, which 
works on a head-first assumption. Thirdly, verbs 
are typically modified by a wider set of syntactic 
satellites than nouns are, such as temporal and 
circumstantial modifiers (Dik 1989). For exam-
ple, deverbal nouns do not inherit the possible 
temporal modifiers of their verb base (I run the 
marathon in three hours, but *the run of the 
marathon in three hours). Modifiers of this sort 
tend to be distributed in the sentence much more 
freely than ordinary arguments.  
4.3.2 Impact of the lexicon on recall 
Of the three above mentioned factors, only the 
first one has an obvious lexical character. We 
can provide a rough estimate of the impact of 
lexical information on the performance of LAP. 
The lexicon filter contributes a 9% increase of 
recall on verb complements (4% over 45.5%), 
by correctly reattaching to the verbal head those 
arguments (61) that were wrongly attached to 
their immediately preceding constituent by BP. 
This leads to an overall 49.5% recall. All re-
maining false negatives (about 48%) are i) either 
verb modifiers or ii) proper verb arguments ly-
ing out of the reach of structure-based criteria, 
due to syntactic phenomena such as complement 
dislocation, complex coordination, parenthetic 
constructions and ellipsis. We shall return to a 
more detailed analysis of false negatives in sec-
tion 4.3.4. In the case of noun complements, use 
of lexical information produces a negligible in-
crease of recall: 0.6% ( 0.5% over 80.8%). This 
is not surprising, as our test corpus contains very 
few cases of noun-headed argument chains, 
fewer than we could expect if the probability of 
their occurrence reflected the (uniform) type dis-
tribution of noun frames in the lexicon. The vast 
majority of noun-headed false negatives, as we 
shall see in more detail in a moment, is repre-
sented by modifiers. 
4.3.3 Impact of the lexicon on precision 
Reattachment is enforced by LAP when the 
preposition introducing a candidate complement 
in context is found in the lexical frame of its 
head. Table 2 shows that ~37% of the 103 re-
structured attachments proposed by the lexicon 
are wrong. Even more interestingly, there is a 
strong asymmetry between nouns and verbs. 
With verb heads, precision of lexically-driven 
reattachments is fairly high (~70%), nonetheless 
lower than precision of rightmost attachment 
(~75%). In the case of noun heads, the number 
of lexically reattached dependencies is instead 
extremely low. The percentage of mistakes is  
high, with precision dropping to 26.6%. 
The difference in the total number of restruc-
tured attachment may be again due to the richer 
complementation patterns exhibited by verbs in 
the lexicon. However, while in the case of verbs 
lexical information produces a significant im-
provement on restructured attachment precision, 
this contribution drops considerably for nouns. 
The main reason for this situation is that nouns 
tend to select semantically vacuous prepositions 
such as of much more often than verbs do. In our 
lexicon, out of 4157 frames headed by a noun, 
4015 contain the preposition di as an argument 
introducer (96.6%). Di is in fact an extremely 
polysemous preposition, heading, among others, 
also possessive phrases and other kinds of modi-
fiers. This trivially increases the number of cases 
of attachment ambiguity and eventually the pos-
sibility of getting false positives. Conversely, as 
shown by the number of confirmed attachments 
in Table 2, the role of lexical information in fur-
ther specifying an attachment with no restructur-
ing is almost uniform across nouns and verbs. 
4.3.4 False negatives  
The vast majority of undetected verb comple-
ments (80.6%) are modifiers of various kind. 
The remaining set of false negatives consists of 
48 complements (7.7%), 30 indirect objects 
(4.8%) and 43 oblique arguments (6.9%). Most 
such complements are by-phrases in passive 
constructions which are not as such very diffi-
cult to detect but just happen to fall out of the 
current coverage of LAP. More interestingly, 2/3 
of the remaining false negatives elude LAP be-
cause they are overtly realised far away from 
their verb head, often to its left. Most of these 
constructions involve argument dislocation and 
ellipsis. We can thus preliminarily conclude that 
argument dislocation and ellipsis accounts for 
about 14% of false negatives (7% over 50%). 
Finally, the number of false negatives due to at-
tachment ambiguity is almost negligible in the 
case of verbal heads. 
On the other hand, the impact of undetected 
modifiers of a verbal head on attachment recall 
is considerable. The most striking feature of this 
large subset is the comparative sparseness of 
modifiers introduced by di (of): 31 out of 504 
(6.2%). At a closer scrutiny, the majority of 
these di-phrases are either phraseological adver-
bial modifiers (di recente ?of late?, del resto ?be-
sides? etc.) or quasi-arguments headed by 
participle forms. Notably, 227 undetected modi-
fiers (45% of the total) are selected by semanti-
cally heavy and complex (possibly 
discontinuous) prepositions (davanti a ?in front 
of?, in mezzo a ?amid?, verso ?towards?, intorno 
a ?around?, contro ?against?, da ... a ?from ... to? 
etc.). As to the remaining 241 undetected modi-
fiers (48%), they are introduced by ?light? 
prepositions such as a ?to?, in ?in? and da ?from?. 
Although this 48% contains a number of diffi-
cult attachments, one can identify subsets of 
fairly reliable modifiers by focusing on the noun 
head introduced by the preposition, which usu-
ally gives a strong indication of the nature of the 
modifier, especially in the case of measure, tem-
poral and locative expressions.  
4.3.5 False positives 
Table 2 shows a prominent asymmetry in the 
precision of confirmed and restructured attach-
ments. Wrong restructured attachments are 
mainly due to a misleading match between the 
preposition introducing a PC and that introduc-
ing a slot in the lexical frame of its candidate 
head (~85%). This typically occurs with ?light? 
prepositions (e.g. di, a, etc.). Most notably, in a 
relevant subset of these mistakes, the verb or 
noun head belongs to an idiomatic multi-word 
expression. In the case of confirmed attach-
ments, about one third of false positives (~5%) 
involve multi-word expressions, in particular 
compound terms such as presidente del consig-
lio ?prime minister?, where the rightmost ele-
ment of the compound is wrongly selected as the 
head of the immediately following PP. In both 
restructured and confirmed attachments, the re-
maining cases (on average ~4%) are due to 
complex syntactic structures (e.g. appositive 
constructions, complex coordination, ellipsis 
etc.) which are outside the coverage of the cur-
rent grammar.  
Conclusion 
Larger lexicons are not necessarily better for 
parsing. The issue of the interplay of lexicon and 
grammar, although fairly well understood at the 
level of linguistic theory, still remains to be fully 
investigated at the level of parsing. In this paper, 
we tried to scratch the surface of the problem 
through a careful analysis of the performance of 
an incremental dependency analyser of Italian, 
which can run in both a non-lexicalised and a 
lexicalised mode.  
The contribution of lexical information to 
parse success is unevenly distributed over both 
part of speech categories and frame types. For 
reasons abundantly illustrated in section 4, the 
frames of noun heads are not quite as useful as 
those of verb heads, especially when available 
information is only syntactic. Moreover, while 
information on verb transitivity or clause em-
bedding is crucial to filter out noisy attachments, 
information on the preposition introducing the 
oblique complement or the indirect object of a 
verb can be misleading, and should thus be used 
for parsing with greater care. The main reason is 
that failure to register in the lexicon all possible 
prepositions actually found in real texts may 
cause undesired over-filtering of genuine 
arguments (false negatives). In many cases, 
argument prepositions are actually selected by 
the lexical head of the subcategorised argument, 
rather than by its subcategorising verb. Simi-
larly, while information about argument option-
ality vs obligatoriness is seldom confirmed in 
real language use, statistical preferences on the 
order of argument realisation can be very useful. 
Most current lexicons say very little about 
temporal and circumstantial modifiers, but much 
more can be said about them that is useful to 
parsing. First, some prepositions only occur to 
introduce verb modifiers. These semantically 
heavy prepositions, often consisting of more 
than one lexical item, play a fundamental role in 
the organization of written texts, and certainly 
deserve a special place in a parsing-oriented 
lexicon. Availability of this type of lexical in-
formation could pave the way to the develop-
ment of specialised ?mini-parsers? of those 
satellite modifiers whose structural position in 
the sentence is subject to considerable variation. 
These mini-parsers could benefit from informa-
tion about semantically-based classes of nouns, 
such as locations, measure terms, or temporal 
expressions, which should also contain indica-
tion of the preposition they are typically intro-
duced by. Clearly, this move requires 
abandoning the prejudice that lexical informa-
tion should only flow from the head to its 
dependents. Finally, availability of large 
repertoires of multi word units (both complex 
prepositions and compound terms) appears to 
have a large impact on improving parse preci-
sion.  
There is no doubt that harvesting such a wide 
range of lexical information in the quantity 
needed for accurate parsing will require exten-
sive recourse to bootstrapping methods of lexi-
cal knowledge acquisition from real texts.     
References  
Bartolini R., Lenci A., Montemagni S, Pirrelli V. 
(2002) The Lexicon-Grammar Balance in Robust 
Parsing of Italian, in Proceedings of the 3rd Inter-
national Conference on Language Resources and 
Evaluation, Las Palmas, Gran Canaria. 
Briscoe, E.J. (2001) From dictionary to corpus to 
self-organizing dictionary: learning valency asso-
ciations in the face of variation and change, in 
Proceedings of Corpus Linguistics 2001, Lancaster 
University, pp. 79-89. 
Briscoe T., Carroll J., (2002) Robust Accurate Statis-
tical Annotation of General Text, in Proceedings of 
the 3rd International Conference on Language Re-
sources and Evaluation, Las Palmas, Gran Canaria. 
Carroll, J., Minnen G., Briscoe E.J. (1998) Can sub-
categorisation probabilities help a statistical 
parser?, in Proceedings of the 6th ACL/SIGDAT 
Workshop on Very Large Corpora, Montreal, Can-
ada. 118-126. 
Chanod J.P. (2001) Robust Parsing and Beyond, in 
J.C. Junqua and G. van Noord (eds.) Robustness in 
Language and Speech Technology, Dordrecht, 
Kluwer, pp. 187-204. 
Federici, S., Montemagni, S., Pirrelli, V. (1998a) 
Chunking Italian: Linguistic and Task-oriented 
Evaluation, in Proceedings of the LREC Workshop 
on ?Evaluation of Parsing Systems?, Granada, 
Spain. 
Federici, S., Montemagni, S., Pirrelli, V., Calzolari, 
N. (1998b) Analogy-based Extraction of Lexical 
Knowledge from Corpora: the SPARKLE Experi-
ence, in Proceedings of the 1st International Con-
ference on Language resources and Evaluation, 
Granada, Spain. 
Grishman, R., Macleod C., Meyers A. (1994) 
COMLEX Syntax: Building a Computational Lexi-
con, in Proceedings of Coling 1994, Kyoto. 
Jensen K. (1988a) Issues in Parsing, in A. Blaser 
(ed.), Natural Language at the Computer, Springer 
Verlag, Berlin, pp. 65-83. 
Jensen K. (1988b) Why computational grammarians 
can be skeptical about existing linguistic theories,
in Proceedings of COLING-88, pp. 448-449. 
Lenci, A., Bartolini, R., Calzolari, N., Cartier, E. 
(2001) Document Analysis, MLIS-5015 MUSI, De-
liverable D3.1,. 
Lenci, A., Montemagni, S., Pirrelli, V., Soria, C. 
(2000) Where opposites meet. A Syntactic Meta-
scheme for Corpus Annotation and Parsing 
Evaluation, in Proceedings of the 2nd International 
Conference on Language Resources and Evalua-
tion, Athens, Greece. 
Montemagni S., Barsotti F., Battista M., Calzolari N., 
Corazzari O., Zampolli A., Fanciulli F., Massetani 
M., Raffaelli R., Basili R., Pazienza M.T., Saracino 
D., Zanzotto F., Mana N., Pianesi F., Delmonte R. 
(2000) The Italian Syntactic-Semantic Treebank: 
Architecture, Annotation, Tools and Evaluation, in 
Proceedings of the COLING Workshop on ?Lin-
guistically Interpreted Corpora (LINC-2000)?, 
Luxembourg, 6 August 2000, pp. 18-27. 
Procter, P. (1987) Longman Dictionary of Contempo-
rary English, Longman, London. 
Ruimy, N., Corazzari, O., Gola, E., Spanu, A., Cal-
zolari, N., Zampolli, A. (1998) The European LE-
PAROLE Project: The Italian Syntactic Lexicon, in 
Proceedings of the 1st International Conference on 
Language resources and Evaluation, Granada, 
Spain, 1998. 
 
Proceedings of the Second Workshop on Psychocomputational Models of Human Language Acquisition, pages 72?81,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Climbing the path to grammar: a maximum entropy model of 
subject/object learning 
 
 
Felice Dell?Orletta Alessandro Lenci Simonetta Montemagni Vito Pirrelli 
Dept. of Computer Science Dept. of Linguistics ILC-CNR ILC-CNR 
University of Pisa University of Pis a Area della Ricerca Area della Ricerca 
Largo Pontecorvo 3 
56100 Pisa (Italy) 
Via Santa Maria 36 
56100 Pisa (Italy) 
Via Moruzzi 1 
56100 Pisa (Italy) 
Via Moruzzi 1 
56100 Pisa (Italy) 
 
{felice.dellorletta, alessandro.lenci, simonetta.montemagni, vito.pirrelli}@ilc.cnr.it 
 
 
 
 
Abstract 
In this paper, we discuss an applic ation of 
Maximum Entropy to modeling the acqui-
sition of subject and object processing in 
Italian. The model is able to learn from 
corpus data a set of experimentally and 
theoretically well-motivated linguistic 
constraints, as well as their relative sali-
ence in Italian grammar development and 
processing. The model is also shown to 
acquire robust syntactic generalizations 
by relying on the evidence provided by a 
small number of high token frequency 
verbs only. These results are consistent 
with current research focusing on the role 
of high frequency verbs in allowing chil-
dren to converge on the most salient con-
straints in the grammar. 
1 Introduction 
Current research in language learning supports the 
view that developing grammatical competence in-
volve mastering and integrating multiple, parallel, 
probabilistic constraints defined over different 
types of linguistic (and non linguistic) information 
(Seidenberg and MacDonald 1999, MacWhinney 
2004). This is particularly clear when we focus on 
the core of grammatical deve lopment, namely the 
ability to properly identify syntactic relations. Psy-
cholinguistic evidence shows that children learn to 
identify sentence subjects and direct objects by 
combining various types of probabilistic cues, such 
as word order, noun animacy, definiteness, agree-
ment, etc. The relative prominence of each of these 
cues during the development of a child?s syntactic 
competence can considerably vary cross-
linguistically, mirroring their relative salience in 
the adult grammar system (cf. Bates et al 1984). 
If grammatical constraints are inherently prob-
abilistic (Manning 2003), the path through which 
the child acquires adult grammar competence can 
be viewed as the process of building a stochastic 
model out of the linguistic input. Consistently with 
?usage-based? approaches to language acquisition 
(cf. Tomasello, 2000) grammatical constraints 
would thus emerge from language use thanks to the 
child?s ability to keep track of statistical regulari-
ties in linguistic cues. In turn, this raises the issue 
of how children are able to exploit the statistical 
distribution of cues in the linguistic input. Various 
types of cross-linguistic evidence converge on the 
hypothesis that children are actually able to take 
great advantage of the highly skewed distribution 
of naturalistic language data. Goldberg et al 
(2004), Matthews et al (2003), Ninio (1999) 
among the others argue that verbs with high token 
frequency in the input have a facilitatory effect in 
allowing children to derive robust syntactic gener-
alizations even from surprisingly minimal input. 
According to this model, syntactic learning is 
driven by a small pool of verbs occurring with the 
highest token frequency: they approximately corre-
spond to so-called ?light verbs? such as English 
go, give , want etc. These verbs would act as ?cata-
72
lysts? in allowing children to converge on the most 
salient grammar constraints of the language they 
are acquiring. 
In computational linguistics, Maximum Entropy 
models have proven to be robust statistical learning 
algorithms that perform well in a number of proc-
essing tasks (cf. Ratnaparkhi 1998). In this paper, 
we discuss successful application of a Maximum 
Entropy (ME) model to the processing of Italian 
syntactic relations. We believe that this discussion 
is of general interest for two basic reasons. First, 
the model is able to learn, from corpus data, a set 
of experimentally and theoretically well-motivated 
linguistic constraints, as well as their relative sali-
ence in the processing of Italian. This suggests that 
it is possible for a child to bootstrap and use this 
type of knowledge on the basis of a specific distri-
bution of real language data, a conclusion that 
bears on the question of the role and type of innate 
inductive biases. Secondly, the model is also 
shown to acquire robust syntactic generalizations 
by relying on the evidence provided by a small 
number of high token frequency verbs only. With 
some qualifications, this evidence sheds light on 
the interaction between highly skewed language 
data distributions and language maturation. Robust 
grammar generalizations emerge on the basis of 
exposure to early, statist ically stable and lexically 
underspecified evidence, thus providing a reliable 
backbone to children?s syntactic development and 
later lexical organization.  
In the following section we first broach the 
general problem of parsing subjects and objects in 
Italian. Section 3 describes an ME model of the 
problem. Section 4 and 5 are devoted to a detailed 
empirical analysis of the interaction of different 
feature configurations and of the interplay between 
verb token frequency and relevant generalizations. 
Conclusions are drawn in the final discussion. 
2 Subjects and Objects in Italian 
Children that learn how to process subjects and 
objects in Italian are confronted with a twofold 
challenge: i) the relatively free order of Italian sen-
tence constituents and ii) the possible absence of 
an overt subject. The existence of a preferred Sub-
ject Verb Object (SVO) order in Italian main 
clauses does not rule out all other possible permu-
tations of these units: in fact, they are all attested, 
albeit with considerable differences in distribution 
and degree of markedness (Bartolini et al 2004).1 
Moreover, because of pro-drop, an Italian Verb 
Noun (VN) sequence can either be interpreted as a 
VO construction with subject omission (e.g. ha 
dichiarato guerra ?(he) declared war?) or as an 
instance of postverbal subject (VS, e.g. ha di-
chiarato Giovanni ?John declared?). Symmetri-
cally, an NV sequence is potentially ambiguous 
between SV and OV: compare il bambino ha man-
giato  ?the child ate? with il gelato ha mangiato ?the 
ice-cream, (he) ate?. 
These grammatical facts are in keeping with 
what we know about Italian children?s parsing 
strategies. Bates et al (1984) show that while, in 
English, word order is by and large the most effec-
tive cue for subject-object identification (hence-
forth SOI) both in syntactic processing and during 
the child?s syntactic development, the same cue 
plays second fiddle in Italian. Bates and colleagues 
bring empirical evidence supporting the hypothesis 
that Italian children show extreme reliance on NV 
agreement and, secondly, on noun animacy, rather 
than word order. They conclude that the following 
syntactic constraints dominance hierarchy is opera-
tive in Italian: agreement > animacy > word order. 
The fact that animacy can reliably be resorted 
to in Italian SOI receives indirect confirmation 
from corpus data. We looked at the distribution of 
animate subjects and objects in the Italian Syntac-
tic Semantic Treebank (ISST, Montemagni et al, 
2003), a 300,000 tokens syntactically annotated 
corpus, including articles from contemporary Ita l-
ian newspapers and periodicals covering a broad 
variety of topics. Subjects and objects in ISST 
were automatically annotated for animacy using 
the SIMPLE Italian computational lexicon (Lenci 
et al 2000) as a background semantic resource. 
The annotation was then checked manually. Cor-
pus analysis highlights a strong asymmetry in the 
distribution of animate nouns in subject and object 
roles: over 56.6% of ISST subjects are animate 
(out of a total number of 12,646), while only the 
11.1% of objects are animate (out of a total number 
of 5,559). Such an overwhelming preference for 
inanimate ob jects in adult language data makes 
animacy play a very important role in SOI, both as 
a key developmental factor in the bootstrapping of 
the syntax-semantics mapping and as a reliable 
                                                               
1 In the present paper we restrict ourselves to the case of de-
clarative main clauses. 
73
processing cue, consistently with psycholinguistic 
data. 
On the other hand, the distribution of word or-
der configurations in the same corpus shows an-
other interesting asymmetry. NV sequences receive 
an SV interpretation in 95.6% of the cases, and an 
object interpretation in the remaining 4.4% (most 
of which are clitic and relative pronouns, whose 
preverbal pos ition is grammatically constrained). 
The situation is quite different when we turn to VN 
sequences, where verb-object pairs represent 
73.4% of the cases, with verb-subject pairs repre-
senting the remaining 26.6%. We infer that ? at 
least in standard written Italian ? VS is a much 
more consistently used construction than OV, and 
that the role of word order in Italian parsing is not 
a marginal one across the board, but rather relative 
to VN contexts only. In NV constructions there is a 
strong preference for a subject interpretation, and 
this suggests a more dynamic dominance hierarchy 
of Italian syntactic constraints than the one pro-
vided above. 
As for agreement, it represents conclusive evi-
dence for SOI only when a nominal constituent and 
a verb do not agree in number and/or person (as in 
leggono il libro ?(they) read the book?). On the 
contrary, when noun and verb share the same per-
son and number the impact of agreement on SOI is 
neutralised, as in il bambino legge il libro ?the 
child reads the book? or in ha dichiarato il presi-
dente ?the president declared?. Although this ambi-
guity arises in specific contexts (i.e. when the verb 
is used in the third person singular or plural and the 
subject/object candidate agrees with it), it is inter-
esting to note that in ISST: third person verb forms 
cover 95.6% of all finite verb forms; and, more 
interestingly for our present concerns, 87.9% of all 
VN and NV pairs involving a third person verb 
form contains an agreeing noun. From this we con-
clude that the contribution of agreement to our 
problem is fairly limited, as lack  of agreement 
shows up only in a limited number of contexts. 
All in all, corpus data lend support to the idea 
that in Italian SOI is governed by a complex inter-
play of probabilistic constraints of a different na-
ture (morpho-syntactic, semantic, word order etc.). 
Moreover, distributional asymmetries in language 
data seem to provide a fairly reliable statistical ba-
sis upon which relevant probabilistic constraints 
can be bootstrapped and combined consistently. In 
the following section we shall present a ME model 
of how constraints and their interaction can be 
bootstrapped from language data. 
3 A Maximum Entropy model of SOI 
The Maximum Entropy (ME) framework offers a 
mathematically  sound way to build a probabilistic 
model for SOI, which combines different linguistic 
cues. Given a linguistic context c and an outcome 
a?A that depends on c, in the ME framework the 
conditional probability distribution p(a|c) is esti-
mated on the basis of the assumption that no a pri-
ori constraints must be met other than those related 
to a set of features f j(a,c) of c, whose distribution is 
derived from the training data. It can be proven 
that the probability distribution p satisfying the 
above assumption is the one with the highest en-
tropy, is unique and has the following exponential 
form (Berger et al 1996): 
(1) ?
=
=
k
j
cajf
jcZ
cap
1
),(
)(
1)|( a  
where Z(c) is a normalization factor, f j(a,c) are the 
values of k features of the pair (a,c) and correspond 
to the linguistic cues of c that are relevant to pre-
dict the outcome a. Features are extracted from the 
training data and define the constraints that the 
probabilistic model p must satisfy. The parameters 
of the distribution a1, ?, ak correspond to weights 
associated with the features, and determine the 
relevance of each feature in the overall model. In 
the experiments reported below feature weights 
have been estimated with the Generative Iterative 
Scaling (GIS) algorithm implemented in the AMIS 
software (Miyao and Tsujii 2002). 
We model SOI as the task of predicting the cor-
rect syntactic function f  ? {subject, object} of a 
noun occurring in a given syntactic context s. This 
is equivalent to build the conditional probability 
distribution p(f |s) of having a syntactic function f  
in a syntactic context s . Adopting the ME ap-
proach, the distribution p can be rewritten in the 
parametric form of (1), with features correspond-
ing to the linguistic contextual cues relevant to 
SOI. The context s  is a pair <vs , ns>, where vs is 
the verbal head and ns its nominal dependent in s. 
This notion of s departs from more traditional 
ways of describing an SOI context as a triple of 
one verb and two nouns in a certain syntactic con-
figuration (e.g, SOV or VOS, etc.). In fact, we as-
sume that SOI can be stated in terms of the more 
74
local task of establishing the grammatical function 
of a noun n observed in a verb-noun pair. This 
simplifying assumption is consistent with the claim 
in MacWhinney et al (1984) that SVO word order 
is actually derivative from SV and VO local pat-
terns and downplays the role of the transitive com-
plex construction in sentence processing. Evidence 
in favour of this hypothesis also comes from cor-
pus data: in ISST, there are 4,072 complete sub-
ject-verb-object-configurations, a small number if 
compared to the 11,584 verb tokens appearing with 
either a subject or an object only. Due to the com-
parative sparseness of canonical SVO constructions 
in Italian, it seems more reasonable to assume that 
children should pay a great deal of attention to 
both SV and VO units as cues in sentence percep-
tion (Matthews et al 2004). Reconstruction of the 
whole lexical SVO pattern can accordingly be seen 
as the end point of an acquisition process whereby 
smaller units are re-analyzed as being part of more 
comprehensive constructions. This hypothesis is 
more in line with a distributed view of canonical 
constructions as derivative of more basic local po-
sitional patterns, working together to yield more 
complex and abstract constructions. Last but not 
least, assuming verb-noun pairs as the relevant 
context for SOI allows us to simultaneously model 
the interaction of word order variation with pro-
drop in Italian. 
4 Feature selection 
The most important part of any ME model is the 
selection of the context features whose weights are 
to be estimated from data distributions. Our feature 
selection strategy is grounded on the main assump-
tion that features should correspond to linguisti-
cally and psycholinguistically well-motivated 
contextual cues. This allows us to evaluate the 
probabilistic model also with respect to its ability 
to replicate psycholinguistic experimental results 
and to be consistent with linguistic generalizations. 
Features are binary functions fki,f  (f ,s), which 
test whether a certain cue ki for the function f  oc-
curs in the context s . For our ME model of SOI, 
we have selected the following types of features: 
Word order tests the position of the noun wrt the 
verb, for instance: 
(2)
?
?
? ==
otherwise
postposnounif
subjf subjpost 0
.1
),(,
ss  
Animacy  tests whether the noun in s  is animate or 
inanimate (cf. ?.2). The centrality of this cue in 
Italian is widely supported by psycholinguistic 
evidence. Another source of converging evidence 
comes from functional and typological linguistic 
research. For instance, Aissen (2003) argues for 
the universal value of the following hierarchy rep-
resenting the relative markedness of the associa-
tions between grammatical functions and animacy 
degrees (with each item in these scale been less 
marked than the elements to its right): 
Animacy Markedness Hierarchy 
Subj/Human > Subj/Animate > Subj/Inanimate 
Obj/Inanimate > Obj/Animate > Obj/Human 
Markedness hierarchies have also been interpreted 
as probabilistic constraints estimated form corpus 
data (Bresnan et al 2001, ?vrelid 2004). In our 
ME model we have used a reduced version of the 
animacy markedness hierarchy in which human 
and animate nouns have been both subsumed under 
the general class animate. 
Definiteness tests the degree of ?referentiality? of 
the noun in a context pair s . Like for animacy, 
definiteness has been claimed to be associated with 
grammatical functions, giving rise to the following 
universal markedness hierarchy Aissen (2003): 
Definiteness Markedness Hierarchy 
Subj/Pro > Subj/Name > Subj/Def > Subj/Indef 
Obj/Indef > Obj/Def > Obj/Name > Obj/Pro 
According to this hierarchy, subjects with a low 
degree of definiteness are more marked than sub-
jects with a high degree of definiteness (for objects 
the reverse pattern holds). Given the importance 
assigned to the definiteness markedness hierarchy 
in current linguistic research, we have included the 
definiteness cue in the ME model. It is worth re-
marking that, unlike animacy, in psycholinguistic 
experiments definiteness has not been assigned any 
effective role in SOI. This makes testing this cue in 
a computational model even more interesting, as a 
way to evaluate its effective contribution to Italian 
SOI. In our experiments, we have used a ?com-
pact? version of the definiteness scale: the defi-
niteness cue tests whether the noun in the context 
75
pair i) is a name or a pronoun ii) has a definite arti-
cle iii), has an indefinite article or iv) is a ?bare? 
noun (i.e. with no article). It is worth saying that 
?bare? nouns are usually placed at the bottom end 
of the definiteness scale. 
The three types of features above only refer to 
nominal cues in the context pairs. Nevertheless, 
specific lexical properties of the verb can also be 
resorted to in SOI. The probability for ns to be sub-
ject or object may also depend on the specific lexi-
cal preferences of vs. To take this lexical factor 
into account, we add a set of lexical cues to the 
three general feature types above. Lexical cues test 
animacy with respect to a specific verb vk: 
(3) 
?
?
?
?
?
=?==
otherwise
animnvvif
subjf ksubjkvanim
0
1
),(,,
sss  
Lexical features provide evidence of the propensity 
of a given verb to have an animate (inanimate) 
subject or object. In fact, the verb argument struc-
ture and thematic properties may well influence the 
possible distribution of animate (inanimate) sub-
jects and objects, thus overriding more general 
tendencies. By including lexical cues, we are thus 
able to test the interplay of lexical constraints with 
general grammatical ones. 
Note that in our ME model we have not in-
cluded agreement as a feature, in spite of its 
prominent role in Italian. The fact that agreement 
is often inconclusive for SOI (?.2) suggests that 
children must also acquire the ability to deal with 
the interplay of various concurrent constraints, 
none of which is singularly sufficient for the task 
completion this type of competence. It is exactly 
this area of syntactic competence that we wanted to 
explore with the experiments reported below (cf. 
MacWhinney et al 1984, who similarly abstract 
from the dominant role of case in German SOI). 
5 Testing feature configurations for SOI 
The ME model for Italian SOI has been trained on 
18,205 verb-subject/object pairs extracted from 
ISST. The training set was obtained by extracting 
all verb-subject and verb-object dependencies 
headed by an active verb occurring in a finite ver-
bal construction and by excluding all cases where 
the position of the nominal constituent was gram-
matically constrained (e.g. clitic objects, relative 
clauses). Two different feature configurations have 
been used for training: 
-  non-lexical feature configuration (NLC), in-
cluding only general features acting as global 
constraints: namely word order, noun animacy 
and noun definiteness; 
- lexical feature configuration (LC), including 
word order, noun animacy and definiteness, 
and information about the verb head.  
The test corpus consists of 645 verb-noun pairs 
extracted from contexts where agreement happens 
to be neutralized. Of them, 446 contained a subject 
(either pre- or post-verbal) and 199 contained an 
object (either pre- or post-verbal). The two feature 
configurations were evaluated by calculating the 
percentage of correctly assigned relations over the 
total number of test pairs (accuracy). As our model 
always assigns one syntactic relation to each test 
pair, accuracy equals both standard precision and 
recall. Finally, we have assumed a baseline score 
of 69%, corresponding to the result yielded by a 
dumb model assigning to each test pair the most 
frequent relation in the training corpus, i.e. subject. 
5.1 Non-lexical feature configuration 
Our first experiment was carried out with NLC. 
The accuracy on the test corpus is 91.5%; most 
errors (i.e. 96.4%) relate to the postverbal position, 
with 44 mistaken subjects (42 inanimate) and 9 
mistaken objects (all animate). The score was con-
firmed by a 10-fold cross-validation on the whole 
training set (89.3% accuracy). 
A further way to evaluate the goodness of the 
model is by inspecting the weights associated with 
feature values (Table 1). 
 Subj Obj 
Preverbal 1,34E+00 2,10E-02 
Postverbal 5,21E-01 1,47E+00 
Anim 1,28E+00 3,34E-01 
Inanim 8,60E-01 1,21E+00 
PronName  1,22E+00 5,75E-01 
DefArt 1,05E+00 1,00E+00 
IndefArt 8,33E-01 1,16E+00 
NoArticle 9,46E-01 1,07E+00 
Table 1 ? Feature value weights in NLC  
The grey cells in Table 1 highlight the preference 
of each feature value for either subject or object 
identif ication: e.g. preverbal subjects are strongly 
preferred over preverbal objects; animate subjects 
76
are preferred over animate objects, etc. Interest-
ingly, if we rank the Anim and Inanim values for 
subjects and objects, we can observe tha t they dis-
tribute consistently with the Animacy Markedness 
Hierarchy reported in ?.4: Subj /Anim > 
Subj/Inanim and Obj/Inanim > Obj/Anim. Simi-
larly, by ranking the values of the definiteness fea-
tures in the Subj column by decreasing weight 
values we obtain the following ordering: Pron-
Name > DefArt > IndefArt > NoArt, which nicely 
fits in with the Definiteness Markedness Hierarchy 
in ?.4. The so-called ?markedness reversal? is ob-
served if we focus on the values for the same fea-
tures in the Obj column: the PronName feature 
represents the most marked option, followed by 
DefArt. The only exception is represented by the 
relative ordering of IndefArt and NoArt which 
however show very close values. 
Evaluating feature salience 
In order to evaluate the most reliable cues in Italian 
SOI, we have analysed the model predictions for 
different bundles of feature values. For each of the 
16 different bundles (b) attested in the data, we 
have estimated p(subj|b) and p(obj|b): 
b p(subj|b) p(obj|b) 
Pre Anim IndefArt 0,994 0,006
Pre Anim DefArt 0,996 0,004
Pre Anim NoArt 0,995 0,005
Pre Anim PronName 0,998 0,002
Pre Inanim IndefArt 0,970 0,030
Pre Inanim DefArt 0,979 0,021
Pre Inanim NoArt 0,976 0,024
Pre Inanim PronName 0,990 0,010
Post Anim IndefArt 0,495 0,505
Post Anim DefArt 0,589 0,411
Post Anim NoArt 0,546 0,454
Post Anim PronName  0,743 0,257
Post Inanim IndefArt 0,153 0,847
Post Inanim DefArt 0,209 0,791
Post Inanim NoArt 0,182 0,818
Post Inanim PronName 0,348 0,652
Table 2 ? Subj/obj probabilities by different bundles 
The model shows a neat preference for subject 
when the noun is preverbal. Instead, when the noun 
is postverbal, function assignment is de facto de-
cided by the noun animacy. Conversely, definite-
ness features have a much more secondary role: 
they can re-enforce (or weaken) the preference ex-
pressed by animacy, but they do not have the 
strength to determine SOI. 
The relative salience of the different constraints 
acting on SOI can also be inferred by comparing 
the weights associated with individual feature val-
ues. For instance, Goldwater and Johnson (2003) 
show that ME can be successfully applied to learn 
constraint rankings in Optimality Theory, by as-
suming the parameter weights a1, ?, ak as the 
ranking values of the constraints. The following 
table lists the 16 general constraints of the model 
by increasing weight values: 
 
Feature Weight 
Preverbal_Obj 2,10E-02
Anim_Obj 3,34E-01
Postverbal_Subj 5,21E-01
ProName_Obj 5,75E-01
IndefArt_Subj 8,33E-01
Inanim_Subj 8,60E-01
NoArticle_Subj 9,46E-01
ArtDef_Obj 1,00E+00
DefArt_Subj 1,05E+00
NoArticle_Obj 1,07E+00
IndefArt_Obj 1,16E+00
Inanim_Obj 1,21E+00
PronName_Subj 1,22E+00
Anim_Subj 1,28E+00
Preverbal_Subj 1,34E+00
Postverbal_Obj 1,47E+00
Table 3 ? Constraint weights ranking 
The rankings in Table 3 can be used to derive the 
relative salience of each constraint. Lower ranked 
constraints correspond to more marked syntactic 
configurations that are then disfavoured in SOI. 
Notice that the two animacy constraints Anim_Obj 
and Anim_Subj are respectively placed near the 
bottom and the top end of the scale. Notwithstand-
ing the low position of Postverbal_Subj, animacy 
is thus able to override the word order constraint 
and to produce a strong tendency to identify ani-
mate nouns as subjects, even when they appear in 
postverbal position (cf. Table 2 above). The con-
straint ranking thus confirms the interplay between 
animacy and word order in Italian, with the former 
playing a decisive role in assigning the syntactic 
function of postverbal nouns. On the other hand, 
77
the constraints involving noun definiteness occupy 
a more intermediate position in the general rank-
ing, with very close values. This is again consistent 
with the less decisive role of this feature type in 
SOI, as shown above. 
5.2 Lexical feature configuration 
In this experiment the general features reported in 
Table 1 have been integrated with 4,316 verb-
specific features as the ones exemplified below for 
the verb dire ?say?: 
dire_animSog 1.228213e+00 
dire_noanimSog 7.028484e-01 
dire_animOgg 3.645964e-01 
dire_noanimOgg 1.321887e+00 
whose associated weights show the strong prefer-
ence of this verb to take animate subjects as op-
posed to inanimate ones as well as a preference for 
inanimate objects with respect to animate ones. 
The results achieved with LC on the test corpus 
show a significant improvement with respect to 
those obtained with NLC: the accuracy is now 
95.5%, with a  4% improvement, confirmed by a 
10-fold cross-validation (94.9%). Also in this case, 
most of the errors relate to the pos tverbal position 
(i.e. 27 out of 29), partitioned into 26 mistaken 
subjects and 1 mistaken object. Lexical features 
have been resorted to to solve most of the NLC 
errors (i.e. 34 out of 55). It is interesting to note 
however that lexical features can also be mislead-
ing. The LC results include 8 new errors, suggest-
ing that lexical features do not always provide 
conclusive evidence: in fact, in 185 cases out of 
645 test VN pairs (i.e. 28.7% of the cases) general 
features are preferred over lexical ones. It is also 
worth mentioning that the ranking of general ani-
macy and definiteness features in LC actually fits 
in with the respective markedness hierarchies even 
with a better approximation than the one produced 
by NLC.  Finally, the relative prominence of the 
different global features confirms the trend in Ta-
ble 2, with word order being predominant in pre-
verbal pos ition and animacy playing a major role 
with postverbal nouns. 
Both feature configurations of the ME model 
thus appear to comply with linguistic and psycho-
linguistic generalizations on SOI. On the linguistic 
side, the constraints learnt by the model are consis-
tent with universal markedness hierarchies for 
grammatical relations. Secondly, the prominence 
of the various constraints in the model fits in well 
with psycholinguistic data. Consistently with the 
results in Bates et al (1984), the model confirms 
the great impact of noun animacy in Italian, al-
though in this case its key role seems to be more 
directly limited to the postverbal position. Con-
versely, the preverbal position is by itself a very 
strong cue for subject interpretation. 
6 High frequency verbs and SOI  
Frequency is known to play a major influence in 
language learning. In morphology, for example, 
highly frequent lexical items tend to be shorter 
forms, more readily accessible in the mental lexi-
con, independently stored as whole items (Stem-
berger and MacWhinney 1986) and fairly resistant 
to morphological overgeneralization through time, 
thus establishing a correlation between irregular 
inflected forms and frequency. Frequency has also 
been assigned a key role in the acquisition of syn-
tactic constructions. In fact, Goldberg (1998) and 
Ninio (1999) have independently argued for the 
existence of a causal relation between early expo-
sure to highly frequent light verbs and acquisition 
of abstract syntax-semantics mappings (construc-
tions). Light verbs such as want, put and go tend to 
be very frequent, because they are applicable in a 
wider range of contexts and are learned and used at 
an early language maturation stage The main idea 
is that children?s early use of these high frequency 
verbs is conducive to the acquisition of abstract 
constructional properties generalizing over particu-
lar instances. 
Goldberg et al (2004) motivate this hypothesis 
by observing that light verbs have high input fre-
quency in the child?s developmental environment 
and, at the same time, exhibit a low degree of se-
mantic specialization. Hence, she argues, it takes a 
little abstraction step for a child to jump from ac-
tual instances of use of light verbs to the syntax-
semantics association of their underlying construc-
tion. On the other hand, Ninio (1999) grounds the 
facilitatory role of highly frequent verbs on their 
being ?pathbreaking? prototypes of the construc-
tion they instantiate, since they are the best models 
of the relevant combinatorial and semantic proper-
ties of their construction in a relatively undiluted 
fashion. However, in the case of light verb con-
structions, the correlation between high frequency 
78
and construction prototypicality and extension is 
tenuous. In fact, it is difficult to argue that frequent 
light verbs such as see, want or do exhibit a high 
degree of both semantic and constructional trans i-
tivity (Goldberg et al 2004). This is reminiscent of 
the morphological behaviour of very frequent word 
forms in inflectional languages, as most of these 
forms are highly fused and show a general ten-
dency towards irregular inflection and low mor-
phological prototypicality. Furthermore, it is 
difficult to reconcile the ?pathbreaking? view with 
the observation that frequently observed linguistic 
units are memorized in full, as unanalyzed wholes. 
6.1 Testing the role of frequency 
To address these open issues and put the alleged 
?pathbreaking? role of light verbs to the challeng-
ing test of a probabilistic model, we carried out a 
second battery of experiments to learn the general, 
non-lexical constraints from two training corpora 
of roughly equivalent size where overall type and 
token verb frequencies were controlled for. Both 
corpora are a subset of the original training set: 
1. skewed frequency corpus (SF) ? it includes 
5,261 context pairs, obtained by selecting 15 verbs 
occurring more than 100 times in ISST (figures in 
parentheses give their token frequency): essere 
?be? (2406), avere ?have? (708), fare  ?do, make? 
(527), dire ?say, tell? (275), dare ?give? (173), ve-
dere ?see? (134), andare ?go? (126), sembrare 
?seem? (124), cercare ?try? (122), mettere ?put? 
(122), portare ?take? (121), trovare ?find? (112), 
volere ?want? (105), lasciare ?leave? (105), riuscire 
?manage? (101). It is worth noticing that this set 
includes typical ?pathbreaking? verbs; 
2. balanced frequency corpus (BF) ? this corpus 
includes 5,373 context pairs selected in such a way 
to ensure that every verb type in the original train-
ing set is attested in BF and occurs at most 6 times. 
For verbs occurring with a higher frequency, the 
pairs to be included in BF have been randomly se-
lected. 
Thus SF and BF represent two opposite training 
situations: SF contains few types with very high 
token frequencies, while BF contains a high num-
ber of verb types (i.e. 1457), with very low and 
uniform token frequency. These training sets re-
semble the structure of linguistic input used by 
Goldberg et al (2004) for their experiments. In 
that case, one group of subjects was exposed to 
linguistic inputs in which some verbs occurred 
with a much higher frequency than the others; a 
second group of subjects was instead exposed to 
linguistic stimuli in which every verb occurred 
with roughly equal frequency. Therefore, by train-
ing our ME model on SF and BF we are able  to 
evaluate the effective role of high token frequency 
verbs in driving syntactic learning.  
The ME model with the general features only 
(i.e. NLC) was first trained on SF, and then tested 
on the 645-pair corpus in ?.5, showing a 90% ac-
curacy. The same ME model was then trained on 
BF, and then tested on the 645-pair corpus, scoring 
a 87% accuracy. The ME model trained on the 
skewed frequency data thus outperforms the model 
trained on BF in a statistically significant way (?2 = 
4.97; a=0.05; p-value = 0.025). 
By using a training set formed only by the verbs 
with the highest token frequency, the model has 
thus been able to acquire robust syntactic con-
straints for SOI. Once these constraints have been 
applied to unseen events, the model has achieved a 
performance comparable to the one of the general 
models in ?.5. This is somehow even more signif i-
cant if we consider that the training set was now 
formed by less than one-third of the pairs on which 
the models in ?.5 were trained. Data quantity aside, 
the most relevant fact is that it is the way verb fre-
quencies are distributed to determine the learning 
path, with a significant positive effect produced by 
high token frequency verbs. In the model trained 
on SF, feature ranking is also governed by mark-
edness relations, and the relative prominence of the 
various constraints is utterly similar to the one dis-
cussed in ?.5. In other terms, the results of this ex-
periment prove that frequent verbs are actually 
able to act as ?catalysts? of the syntactic acquis i-
tion process. It is possible for children to converge 
on the correct generalizations governing SOI in 
Italian, just by relying on the linguistic evidence 
provided by the most frequent verbs. 
This view suggests a way out of the apparent 
paradox of the ?pathbreaking? hypothesis: highly 
frequent verbs can be assumed to provide stable 
and consistent multiple probabilistic cues for the 
assignment of subject/object relations. The exis-
tence of pos itional patterns that occur with high 
token frequency may well provide a deeply en-
trenched and highly salient set of distributional 
cues that act as probabilistic constraints on con-
structional generalizations. We hypothesize that 
similar constructions of other less frequent verbs 
79
are processed, for lack of more specific overriding 
information, in the light of these constraints. Since 
processing is the result of a ?conspiracy? of dis-
tributed constraints, ?pathbreaking? prototypes 
need not be real construction exemplars but highly 
schematic patterns. We proved that highly frequent 
local positional patterns offer the right sort of con-
straint conspiracy. 
7 General discussion 
It appears that the distributional evidence of high 
frequency light verbs may well provide a solid 
cognitive anchor for sweeping perceptual generali-
zations on the syntax-semantics mapping. These 
generalizations are local, in that they involve pos i-
tional NV and VN pairs only, and are perceptual as 
they address the issue of identifying appropriate 
syntactic relations by relying on perceptual fea-
tures of linguistic contexts, such as position, ani-
macy, etc. On the basis of these findings, one can 
reasonably argue that complex lexical construc-
tions (in the sense of Goldberg 1998) are built 
upon these local patterns, by combining them in 
those contexts where the presence of a particular 
verb licenses such a combination.  
The two feature configurations discussed in ?.5 
(i.e. NLC and LC) can thus be viewed as two suc-
cessive steps along the path that leads towards the 
emergence of complex, lexically-driven construc-
tions. This can actually be modeled as the incre-
mental process of adding more and more lexical 
constraints to early lexicon-free generalizations 
(based on word order, animacy, definiteness etc.). 
As a result of such additional constraints, the pres-
ence of an intransitive verb may completely rule 
out the object interpretation of a VN pattern, flying 
in the face of a general bias towards viewing VN as 
a transitive pattern. This picture is compatible with 
the well-known observation that constructions are 
used rather conservatively by children at early 
stages of language maturation (Tomasello 2000). 
In fact, if early generalizations are mainly percep-
tual and local, we do not expect them to be used in 
production, at least until the child reaches a stage 
where they are combined into bigger lexically-
driven constructions. 
ME has proven to be a sound computational 
learning framework to simulate the interplay of 
complex probabilistic constraints in language. Our 
experiments confirm linguistic generalizations and 
psycholinguistic data for subjects and objects in 
Italian, while raising new interesting issues at the 
same time. This is the case of the role of definite-
ness in SOI. In fact, the model features neatly re-
produce the definiteness markedness hierarchy, but 
definiteness does not appear to be really influential 
for subject and object processing. Various hy-
potheses are compatible with such results, inclu d-
ing that definiteness is not a cue on which speakers 
rely for SOI in Italian. Another more interesting 
possibility is that definiteness constraints may in-
deed play a decisive role when the learner is asked 
to assign subject and object relations in the context 
of a more complex construction than a simple NV 
pair. Suppose that both nouns of a noun-noun-verb 
triple are amenable to a subject interpretation, but 
that one of them is a more likely subject than the 
other due to its being part of a definite noun 
phrase. Then, it is reasonable to expect that the 
model would select the definite noun phrase as the 
subject in the triple and opt for an object interpre-
tation of the other candidate noun phrase.  
As part of our future work, we plan to train the 
ME model on a more realistic corpus of parental 
input to Italian children, available in the CHILDES 
database (MacWhinney, 2000: http://childes.psy. 
cmu.edu/data/Romance/Italian). In fact, there is 
converging evidence that the use of particular con-
structions in parental speech is largely dominated 
by the use of each construction with one specific, 
highly frequent verb (e.g. go for the intransitive 
construction). The same trends noted in mother?s 
speech to children are mirrored in children?s early 
speech (Goldberg et al, 2004). Quochi (in prepara-
tion) reports a similar distributional pattern for the 
caused motion and intransitive motion verbs in two 
Italian CHILDES corpora (named ?Italian-
Antelmi? and ?Italian-Calambrone?). If these find-
ings are confirmed, the high accuracy of our ME 
model trained on the skewed frequency corpus 
(SF) allows us to expect an equally high accuracy 
when training the model on evidence from Italian 
parental speech.  
This brings us to another related point: lack of 
correction/supervision in parental input. Since our 
ME model heavily relies on previously classified 
noun-verb pairs, we can legitimately wonder how 
easily it can be extended to simulate child language 
learning in an unsupervised mode. In fact, it should 
be appreciated that, in our experiments, compar-
tively little rests on supervised classification. Iden-
80
tification of the contextually-relevant subject is, for 
lack of explicit morphosyntactic clues such as 
agreement and diathesis, simply a matter of guess-
ing the more likely agent of the action expressed 
by the verb on the basis of semantic and pragmatic 
features such as animacy, definiteness and noun 
position to the verb. Mutatis mutandis, the same 
holds for object identification. It is then highly 
likely that salient evidence for the correct sub-
ject/object classific ation comes to the child from 
direct observation of the situation described by a 
sentence. It is such systematic coupling of linguis-
tic evidence from the sentence with perceptual evi-
dence of the situation described by the sentence 
that can assist the child in developing interface 
notions such as subject, object and the like.  
References 
Aissen J., 2003. Differential object marking: iconicity 
vs. economy. Natural Language and Linguistic The-
ory, 21: 435-483. 
Bartolini R., Lenci A., Montemagni S., Pirrelli V., 2004. 
Hybrid constraints for robust parsing: First experi-
ments and evaluation. LREC2004: 859-862. 
Bates E., MacWhinney B., Caselli C., Devescovi A., 
Natale F., Venza V., 1984. A crosslinguistic study of 
the development of sentence interpretation strategies. 
Child Development, 55: 341-354. 
Berger A., Della Pietra S., Della Pietra V., 1996. A 
maximum entropy approach to natural language 
processing. Computational Linguistics 22(1): 39-71 
Bresnan J., Dingare D., Manning C. D., 2001. Soft con-
straints mirror hard constraints: voice and person in 
English and Lummi. Proceedings of the LFG01 Con-
ference , Hong Kong: 13-32. 
Goldberg A. E., 1998. The emergence of the semantics 
of argument structure constructions. In B. MacWhin-
ney (e d.), The Emergence of Language . Lawrence 
Erlbaum Associates, Hillsdale, N. J.: 197-212. 
Goldberg A. E., Casenhiser D., Sethuraman N., 2004. 
Learning argument structure generalizations, Cogni-
tive Linguistics. 
Goldwater S., Johnson M. 2003. Learning OT Con-
straint Rankings Using a Maximum Entropy Model. 
In Spenader J., Eriksson A., Dahl ?. (eds.), Proceed-
ings of the Stockholm Workshop on Variation within 
Optimality Theory. April 26-27, 2003, Stockholm 
University: 111-120. 
Lenci A. et al, 2000. SIMPLE: A Ge neral Framework 
for the Development of Multilingual Lexicons. Inter-
national Journal of Lexicography, 13 (4): 249-263. 
Manning C. D., 2003. Probabilistic syntax. In R. Bod, J. 
Hay, S. Jannedy (eds), Probabilistic Linguistics,  
MIT Press, Cambridge MA: 289-341. 
MacWhinney, B., 2000. The CHILDES project: Tools 
for analyzing talk. Third Edition. Mahwah, NJ: La w-
rence Erlbaum Associates 
MacWhinney B., Bates E., Kliegl R., 1984. Cue validity 
and sentence interpretation in English, German, and 
Italian. Journal of Verbal Learning and Verbal Be-
havior, 23: 127-150. 
MacWhinney B., 2004. A unified model of language 
acquisition. In J. Kroll & A. De Groot (eds.), Hand-
book of bilingualism: Psycholinguistic approaches, 
Oxford University Press, Oxford. 
Matthews D., Lieven E., Theakston A., Tomasello M., 
in press, The role of frequency in the acquisition of 
English word order, Cognitive Development. 
Miyao Y., Tsujii J., 2002. Maximum entropy estimation 
for feature forests. Proc. HLT2002. 
Montemagni S. et al 2003. Building the Italian syntac-
tic-semantic treebank. In Abeill? A. (ed.) Treebanks. 
Building and Using Parsed Corpora , Kluwer, 
Dordrecht: 189-210. 
Ninio, A. 1999. Pathbreaking verbs in syntactic devel-
opment and the question of prototypical transitivity. 
Journal of Child Language, 26: 619- 653. 
?vrelid L., 2004. Disambiguation of syntactic functions 
in Norwegian: modeling variation in word order in-
terpretations conditioned by animacy and definite-
ness. Proceedings of the 20th Scandinavian 
Conference of Linguistics, Helsinki. 
Quochi, V., (in preparation). A constructional analysis 
of parental speech: The role of frequency and predic-
tion in language acquisition, evidence from Italian. 
Ratnaparkhi A., 1998. Maximum Entropy Models for 
Natural Language Ambiguity Resolution. Ph.D. Dis-
sertation, University of Pennsylvania. 
Seidenberg M. S., MacDonald M. C. 1999. A probabil-
istic constraints approach to language acquisition and 
processing. Cognitive Science  23(4): 569-588. 
Stemberger, J., MacWhinney, B. 1986. Frequency and 
the lexical storage of regularly inflected forms. 
Memory and Cognition, 14:17-26. 
Tomasello M., 2000. Do young children have adult syn-
tactic competence? Cognition , 74: 209-253. 
81
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 21?28,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Probing the space of grammatical variation: 
induction of cross-lingual grammatical constraints from treebanks 
 
 
Felice Dell?Orletta 
Universit? di Pisa, Dipartimento di 
Informatica - Largo B. Pontecorvo 3 
ILC-CNR - via G. Moruzzi 1 
56100 Pisa, Italy 
felice.dellorletta@ilc.cnr.it 
Alessandro Lenci 
Universit? di Pisa, Dipartimento di 
Linguistica - Via Santa Maria 36 
56100 Pisa, Italy 
 
alessandro.lenci@ilc.cnr.it 
Simonetta Montemagni 
ILC-CNR - via G. Moruzzi 1 
56100 Pisa, Italy 
simonetta.montemagni@ilc.cnr.it 
Vito Pirrelli 
ILC-CNR - via G. Moruzzi 1 
56100 Pisa, Italy 
vito.pirrelli@ilc.cnr.it 
 
  
Abstract 
The paper reports on a detailed 
quantitative analysis of distributional 
language data of both Italian and Czech, 
highlighting the relative contribution of a 
number of distributed grammatical 
factors to sentence-based identification of 
subjects and direct objects. The work 
uses a Maximum Entropy model of 
stochastic resolution of conflicting 
grammatical constraints and is 
demonstrably capable of putting 
explanatory theoretical accounts to the 
test of usage-based empirical verification. 
1 Introduction 
The paper illustrates the application of a 
Maximum Entropy (henceforth MaxEnt) model 
(Ratnaparkhi 1998) to the processing of subjects 
and direct objects in Italian and Czech. The 
model makes use of richly annotated Treebanks 
to determine the types of linguistic factors 
involved in the task and weigh up their relative 
salience. In doing so,  we set ourselves a two-
fold goal. On the one hand, we intend to discuss 
the use of Treebanks to discover typologically 
relevant and linguistically motivated factors and 
assess the relative contribution of the latter to 
cross-linguistic parsing issues. On the other 
hand, we are interested in testing the empirical 
plausibility of constraint-resolution models of 
language processing (see infra) when confronted 
with real language data.  
Current research in natural language learning 
and processing supports the view that 
grammatical competence consists in mastering 
and integrating multiple, parallel constraints 
(Seidenberg and MacDonald 1999, MacWhinney 
2004). Moreover, there is growing consensus on 
two major properties of grammatical constraints: 
i.) they are probabilistic ?soft constraints? 
(Bresnan et al 2001), and ii.) they have an 
inherently functional nature, involving different 
types of linguistic (and non linguistic) 
information (syntactic, semantic, etc.). These 
features emerge particularly clearly in dealing 
with one of the core aspects of grammar 
learning: the ability to identify syntactic relations 
in text. Psycholinguistic evidence shows that 
speakers learn to identify sentence subjects and 
direct objects by combining various types of 
probabilistic, functional cues, such as word 
order, noun animacy, definiteness, agreement, 
etc. An important observation is that the relative 
prominence of each such cue can considerably 
vary cross-linguistically. Bates et al (1984), for 
example, argue that while, in English, word order 
is the most effective cue for Subject-Object 
Identification (henceforth SOI) both in syntactic 
processing and during the child?s syntactic 
development, the same cue plays second fiddle in 
relatively free phrase-order languages such as 
Italian or German. 
If grammatical constraints are inherently 
probabilistic (Manning 2003), the path through 
which adult grammar competence is acquired can 
be viewed as the process of building a stochastic 
model out of the linguistic input. In 
computational linguistics, MaxEnt models have 
21
proven to be robust statistical learning algorithms 
that perform well in a number of processing 
tasks. Being supervised learning models, they  
require richly annotated data as training input. 
Before we turn to the use of Treebanks for 
training a MaxEnt model for SOI, we first 
analyse the range of linguistic factors that are 
taken to play a significant role in the task.   
2 Subjects and objects in Czech and 
Italian 
Grammatical relations - such as subject (S) and 
direct object (O) - are variously encoded in 
languages, the two most widespread strategies 
being: i) structural encoding through word order, 
and ii) morpho-syntactic marking. In turn, 
morpho-syntactic marking can apply either on 
the noun head only, in the form of case 
inflections, or on both the noun and the verb, in 
the form of agreement marking (Croft 2003). 
Besides formal coding, the distribution of 
subjects and object is also governed by semantic 
and pragmatic factors, such as noun animacy, 
definiteness, topicality, etc. As a result, there 
exists a variety of linguistic clues jointly co-
operating in making a particular noun phrase the 
subject or direct object of a sentence. Crucially 
for our present purposes, cross-linguistic 
variation does not only concern the particular 
strategy used to encode S and O, but also the 
relative strength that each factor plays in a given 
language. For instance, while English word order 
is by and large the dominant clue to identify S 
and O, in other languages the presence of a rich 
morphological system allows word order to have 
a much looser connection with the coding of 
grammatical relations, thus playing a secondary 
role in their identification. Moreover, there are 
languages where semantic and pragmatic 
constraints such as animacy and/or definiteness 
play a predominant role in the processing of 
grammatical relations. A large spectrum of 
variations exists, ranging from languages where 
S must have a higher degree of animacy and/or 
definiteness relative to O, to languages where 
this constraint only takes the form of a softer 
statistical preference (cf. Bresnan et al 2001). 
The goal of this paper is to explore the area of 
this complex space of grammar variation through 
careful assessment of the distribution of S and O 
tokens in Italian and Czech. For our present 
analysis, we have used a MaxEnt statistical 
model trained on data extracted from two 
syntactically annotated corpora: the Prague 
Dependency Treebank (PDT, Bohmova et al 
2003) for Czech, and the Italian Syntactic 
Semantic Treebank (ISST, Montemagni et al 
2003) for Italian. These corpora have been 
chosen not only because they are the largest 
syntactically annotated resources for the two 
languages, but also because of their high degree 
of comparability, since they both adopt a 
dependency-based annotation scheme. 
Czech and Italian provide an interesting 
vantage point for the cross-lingual analysis of 
grammatical variation. They are both Indo-
European languages, but they do not belong to 
the same family: Czech is a West Slavonic 
language, while Italian is a Romance language. 
For our present concerns, they appear to share 
two crucial features: i) the free order of 
grammatical relations with respect to the verb; ii) 
the possible absence of an overt subject. 
Nevertheless, they also greatly differ due to: the 
virtual non-existence of case marking in Italian 
(with the only marginal exception of personal 
pronouns), and the degree of phrase-order 
freedom in the two languages. Empirical 
evidence supporting the latter claim is provided 
in Table 1, which reports data extracted from 
PDT and ISST. Notice that although in both 
languages S and O can occur either pre-verbally 
or post-verbally, Czech and Italian greatly differ 
in their propensity to depart from the (unmarked) 
SVO order. While in Italian preverbal O is 
highly infrequent (1.90%), in Czech more than 
30% of O tokens occur before the verb. The 
situation is similar but somewhat more balanced 
in the case of S, which occurs post-verbally in 
22.21% of the Italian cases, and  in  40% of 
Czech ones. For sure, one can argue that, in 
spoken Italian, the number of pre-verbal objects 
is actually higher, because of the greater number 
of left dislocations and topicalizations occurring 
in informal speech. However reasonable, the 
observation does not explain away the 
distributional differences in the two corpora, 
since both PDT and ISST contain written 
language only. We thus suggest that there is clear 
empirical evidence in favour of a systematic, 
higher phrase-order freedom in Czech, arguably 
related to the well-known correlation of Czech 
constituent placement with sentence information 
structure, with the element carrying new 
information showing a tendency to occur 
sentence-finally (Stone 1990). For our present 
concerns, however, aspects of information 
structure, albeit central in Czech grammar, were 
not taken into  account, as they  happen not to  be 
22
 
   Czech Italian 
    Subj Obj Subj Obj 
Pre 59.82% 30.27% 77.79% 1.90% 
Post 40.18% 69.73% 22.21% 98.10% Pos 
All 100.00% 100.00% 100.00% 100.00% 
Agr 98.50% 56.54% 97.73% 58.33% 
NoAgr 1.50% 43.46% 2.27% 41.67% Agr 
All 100.00% 100.00% 100.00% 100.00% 
Anim 34.10% 15.42% 50.18% 10.67% 
NoAnim 65.90% 84.58% 49.82% 89.33% Anim 
All 100.00% 100.00% 100.00% 100.00% 
Table 1 ?Distribution of Czech and Italian S and O wrt word order, 
agreement and noun animacy 
 Czech 
 Subj Obj 
Nominative 53.83% 0.65% 
Accusative 0.15% 28.30% 
Dative 0.16% 9.54% 
Genitive 0.22% 2.03% 
Instrumental 0.01% 3.40% 
Ambiguous 45.63% 56.08% 
All 100.00% 100.00% 
Table 2 - Distribution of Czech S and O 
wrt case 
marked-up in the Italian corpus.  
According to the data reported in Table 1, 
Czech and Italian show similar correlation 
patterns between animacy and grammatical 
relations. S and O in ISST were automatically 
annotated for animacy using the SIMPLE Italian 
computational lexicon (Lenci et al 2000) as a 
background semantic resource. The annotation 
was then checked manually. Czech S and O were 
annotated for animacy using Czech WordNet 
(Pala and Smrz 2004); it is worth remarking that 
in Czech animacy annotation was done only 
automatically, without any manual revision. 
Italian shows a prominent asymmetry in the 
distribution of animate nouns in subject and 
object roles: over 50% of ISST subjects are 
animate, while only 10% of the objects are 
animate. Such a trend is also confirmed in Czech 
? although to a lesser extent - with 34.10% of 
animate subjects vs. 15.42% of objects.1 Such an 
overwhelming preference for animate subjects in 
corpus data suggests that animacy may play a 
very important role for S and O identification in 
both languages. 
Corpus data also provide interesting evidence 
concerning the actual role of morpho-syntactic 
constraints in the distribution of grammatical 
relations. Prima facie, agreement and case are 
the strongest and most directly accessible clues 
for S/O processing, as they are marked both 
overtly and locally. This is also confirmed by 
psycholinguistic evidence, showing that subjects 
tend to rely on these clues to identify S/O. 
However, it should be observed that agreement 
can be relied upon conclusively in S/O 
processing only when a nominal constituent and 
                                               
1 In fact, the considerable difference in animacy distribution 
between the two languages might only be an artefact of the 
way we annotated Czech nouns semantically, on the basis of 
their context-free classification in the Czech WordNet. 
a verb do not agree in number and/or person (as 
in leggono il libro ?(they) read the book?). 
Conversely, when N and V share the same 
person and number, no conclusion can be drawn, 
as trivially shown by a sentence like il bambino 
legge il libro ?the child reads the book?. In ISST, 
more than 58% of O tokens agree with their 
governing V, thus being formally 
indistinguishable from S on the basis of 
agreement features. PDT also exhibits a similar 
ratio, with 56% of O tokens agreeing with their 
verb head. Analogous considerations apply to 
case marking, whose perceptual reliability is 
undermined by morphological syncretism,  
whereby different  cases are realized through the 
same marker. Czech data reveal the massive 
extent of this phenomenon and its impact on SOI. 
As reported in Table 2, more than 56% of O 
tokens extracted from PDT are formally 
indistinguishable from S in case ending. 
Similarly, 45% of S tokens are formally 
indistinguishable from O uses on the same 
ground. All in all, this means that in 50% of the 
cases a Czech noun can not be understood as the 
S/O of a sentence by relying on overt case 
marking only. 
To sum up, corpus data lend support to the 
idea that in both Italian and in Czech SOI is 
governed by a complex interplay of probabilistic 
constraints of a different nature (morpho-
syntactic, semantic, word order, etc.) as the latter 
are neither singly necessary nor jointly sufficient 
to attack the processing task at hand. It is 
tempting to hypothesize that the joint distribution 
of these data can provide a statistically reliable 
basis upon which relevant probabilistic 
constraints are bootstrapped and combined 
consistently. This should be possible due to i) the 
different degrees of clue salience in the two 
languages and ii) the functional need to minimize 
23
processing ambiguity in ordinary communicative 
exchanges. With reference to the latter point, for 
example, we may surmise that a speaker will be 
more inclined to violate one constraint on S/O 
distribution (e.g. word order) when another clue 
is available (e.g. animacy) that strongly supports 
the intended interpretation only. The following 
section illustrates how a MaxEnt model can be 
used to model these intuitions by bootstrapping 
constraints and their interaction from language 
data. 
3 Maximum Entropy modelling 
The MaxEnt framework offers a mathematically 
sound way to build a probabilistic model for SOI, 
which combines different linguistic cues. Given 
a linguistic context c and an outcome a?A that 
depends on c, in the MaxEnt framework the 
conditional probability distribution p(a|c) is 
estimated on the basis of the assumption that no 
a priori constraints must be met other than those 
related to a set of features fj(a,c) of c, whose 
distribution is derived from the training data. It 
can be proven that the probability distribution p 
satisfying the above assumption is the one with 
the highest entropy, is unique and has the 
following exponential form (Berger et al 1996): 
(1) ?
=
=
k
j
caf
j
j
cZcap 1
),(
)(
1)|( a  
where Z(c) is a normalization factor, fj(a,c) are 
the values of k features of the pair (a,c) and 
correspond to the linguistic cues of c that are 
relevant to predict the outcome a. Features are 
extracted from the training data and define the 
constraints that the probabilistic model p must 
satisfy. The parameters of the distribution ?1, ?, 
?k correspond to weights associated with the 
features, and determine the relevance of each 
feature in the overall model. In the experiments 
reported below feature weights have been 
estimated with the Generative Iterative Scaling 
(GIS) algorithm implemented in the AMIS 
software (Miyao and Tsujii 2002). 
We model SOI as the task of predicting the 
correct syntactic function ? ? {subject, object} 
of a noun occurring in a given syntactic context 
?. This is equivalent to building the conditional 
probability distribution p(?|?) of having a 
syntactic function ? in a syntactic context ?. 
Adopting the MaxEnt approach, the distribution 
p can be rewritten in the parametric form of (1), 
with features corresponding to the linguistic 
contextual cues relevant to SOI. The context ? is 
a pair <v?, n?>, where v? is the verbal head and n? 
its nominal dependent in ?. This notion of ? 
departs from more traditional ways of describing 
an SOI context as a triple of one verb and two 
nouns in a certain syntactic configuration (e.g, 
SOV or VOS, etc.). In fact, we assume that SOI 
can be stated in terms of the more local task of 
establishing the grammatical function of a noun 
n observed in a verb-noun pair. This simplifying 
assumption is consistent with the claim in 
MacWhinney et al (1984) that SVO word order 
is actually derivative from SV and VO local 
patterns and downplays the role of the transitive 
complex construction in sentence processing. 
Evidence in favour of this hypothesis also comes 
from corpus data: for instance, in ISST complete 
subject-verb-object configurations represent only 
26% of the cases, a small percentage if compared 
to the 74% of verb tokens appearing with either a 
subject or an object only; a similar situation can 
be observed in PDT where complete subject-
verb-object configurations occur in only 20% of 
the cases. Due to the comparative sparseness of 
canonical SVO constructions in Czech and 
Italian, it seems more reasonable to assume that 
children should pay a great deal of attention to 
both SV and VO units as cues in sentence 
perception (Matthews et al in press). 
Reconstruction of the whole lexical SVO pattern 
can accordingly be seen as the end point of an 
acquisition process whereby smaller units are re-
analyzed as being part of more comprehensive 
constructions. This hypothesis is more in line 
with a distributed view of canonical 
constructions as derivative of more basic local 
positional patterns, working together to yield 
more complex and abstract constructions. Last 
but not least, assuming verb-noun pairs as the 
relevant context for SOI allows us to 
simultaneously model the interaction of word 
order variation with pro-drop. 
4 Feature selection 
The most important part of any MaxEnt model is 
the selection of the context features whose 
weights are to be estimated from data 
distributions. Our feature selection strategy is 
grounded on the main assumption that features 
should correspond to theoretically and 
typologically well-motivated contextual cues. 
This allows us to evaluate the probabilistic 
model also with respect to its consistency with 
current linguistic generalizations. In turn, the 
model can be used as a probe into the 
correspondence between theoretically motivated 
24
generalizations and usage-based empirical 
evidence.  
Features are binary functions fki,? (?,?), which 
test whether a certain cue ki for the feature ? 
occurs in the context ?. For our MaxEnt model, 
we have selected different features types that test 
morpho-syntactic, syntactic, and semantic key 
dimensions in determining the distribution of S 
and O. 
 
Morpho-syntactic features. These include N-V 
agreement, for Italian and Czech, and case, only 
for Czech. The combined use of such features 
allow us not only to test the impact of morpho-
syntactic information on SOI, but also to analyze 
patterns of cross-lingual variation stemming 
from language specific morphological 
differences, e.g. lack of case marking in Italian. 
 
Word order. This feature essentially test the 
position of the noun wrt the verb, for instance: 
(2)
??
? == otherwise
postposnounifsubjf subjpost 0
.1),(, ss  
 
Animacy. This is the main semantic feature, 
which tests whether the noun in ? is animate or 
inanimate (cf. section 2). The centrality of this 
cue for grammatical relation assignment is 
widely supported by typological evidence (cf. 
Aissen 2003, Croft 2003). The Animacy 
Markedness Hierarchy - representing the relative 
markedness of the associations between 
grammatical functions and animacy degrees ? is 
actually assigned the role of a functional 
universal principle in grammar. The hierarchy is 
reported below, with each item in these scales 
been less marked than the elements to its right: 
 
Animacy Markedness Hierarchy 
Subj/Human > Subj/Animate > Subj/Inanimate 
Obj/Inanimate > Obj/Animate > Obj/Human 
 
Markedness hierarchies have also been 
interpreted as probabilistic constraints estimated 
from corpus data (Bresnan et al 2001). In our 
MaxEnt model we have used a reduced version 
of the animacy markedness hierarchy in which 
human and animate nouns have been both 
subsumed under the general class animate. 
 
Definiteness tests the degree of ?referentiality? of 
the noun in a context pair ?. Like for animacy, 
definiteness has been claimed to be associated 
with grammatical functions, giving rise to the 
following universal markedness hierarchy Aissen 
(2003): 
 
Definiteness Markedness Hierarchy 
Subj/Pro > Subj/Name > Subj/Def > Subj/Indef 
Obj/Indef > Obj/Def > Obj/Name > Obj/Pro 
 
According to this hierarchy, subjects with a low 
degree of definiteness are more marked than 
subjects with a high degree of definiteness (for 
objects the reverse pattern holds). Given the 
importance assigned to the definiteness 
markedness hierarchy in current linguistic 
research, we have included the definiteness cue 
in the MaxEnt model. In our experiments, for 
Italian we have used a compact version of the 
definiteness scale: the definiteness cue tests 
whether the noun in the context pair i) is a name 
or a pronoun ii) has a definite article iii), has an 
indefinite article or iv) is a bare noun (i.e. with 
no article). It is worth saying that bare nouns are 
usually placed at the bottom end of the 
definiteness scale. Since in Czech there is no 
article, we only make a distinction between 
proper names and common nouns. 
5 Testing the model 
The Italian MaxEnt model was trained on 14,643 
verb-subject/object pairs extracted from ISST. 
For Czech, we used a training corpus of 37,947 
verb-subject/object pairs extracted from PDT. In 
both cases, the training set was obtained by 
extracting all verb-subject and verb-object 
dependencies headed by an active verb, with the 
exclusion of all cases where the position of the 
nominal constituent was grammatically 
determined (e.g. clitic objects, relative clauses). 
It is interesting to note that in both training sets 
the proportion of subjects and objects relations is 
nearly the same: 63.06%-65.93% verb-subject 
pairs and 36.94%-34.07% verb-object pairs for 
Italian and Czech respectively. 
The test corpus consists of a set of verb-noun 
pairs randomly extracted from the reference 
Treebanks: 1,000 pairs for Italian and 1,373 for 
Czech. For Italian, 559 pairs contained a subject 
and 441 contained an object; for Czech, 905 
pairs contained a subject and 468 an object. 
Evaluation was carried out by calculating the 
percentage of  correctly  assigned  relations  over 
the total number of test pairs (accuracy). As our 
model always assigns one syntactic relation to 
each test pair, accuracy equals both standard 
precision and recall. 
25
  Czech Italian 
  Subj Obj Subj Obj 
Preverb 1.99% 19.40% 0.00% 6.90% 
Postverb 71.14% 7.46% 71.55% 21.55% 
Anim 0.50% 3.98% 6.90% 21.55% 
Inanim 72.64% 22.89% 64.66% 6.90% 
Nomin 0.00% 1.00% 
Genitive 0.50% 0.00% 
Dative 1.99% 0.00% 
Accus 0.00% 0.00% 
Instrum 0.00% 0.00% 
Ambig 70.65% 25.87% 
Na 
Agr 70.15% 25.87% 61.21% 12.07% 
NoAgr 2.99% 0.50% 7.76% 1.72% 
NAAgr 0.00% 0.50% 2.59% 14.66% 
Table 3 ? Types of errors for Czech and Italian 
 
 Czech Italian 
 Subj Obj Subj Obj 
Preverb 1.24E+00 5.40E-01 1.31E+00 2.11E-02 
Postverb 8.77E-01 1.17E+00 5.39E-01 1.38E+00 
Anim 1.16E+00 6.63E-01 1.28E+00 3.17E-01 
Inanim 1.03E+00 9.63E-01 8.16E-01 1.23E+00 
PronName 1.13E+00 7.72E-01 1.13E+00 8.05E-01 
DefArt 1.01E+00 1.02E+00 
IndefArt 6.82E-01 1.26E+00 
NoArticle 
1.05E+00 9.31E-01 
9.91E-01 1.02E+00 
Nomin 1.23E+00 2.22E-02 
Genitive 2.94E-01 1.51E+00 
Dative 2.85E-02 1.49E+00 
Accus 8.06E-03 1.39E+00 
Instrum 3.80E-03 1.39E+00 
Na 
Agr 1.18E+00 6.67E-01 1.28E+00 4.67E-01 
NoAgr 7.71E-02 1.50E+00 1.52E-01 1.58E+00 
NAAgr 3.75E-01 1.53E+00 2.61E-01 1.84E+00 
Table 4 - Feature value weights in NLC for Czech and 
Italian
We have assumed a baseline score of 56% for 
Italian and of 66% for Czech, corresponding to 
the result yielded by a naive model   assigning   
to  each   test   pair  the   most frequent relation 
in the training corpus, i.e. subject. Experiments 
were carried out with the general features 
illustrated in section 4: verb agreement, case (for 
Czech only), word order, noun animacy and 
noun definiteness. 
Accuracy on the test corpus is 88.4% for 
Italian and 85.4% for Czech. A detailed error 
analysis for the two languages is reported in 
Table 3, showing that in both languages subject 
identification appears to be particularly 
problematic. In Czech, it appears that the 
prototypically mistaken subjects are post-verbal 
(71.14%), inanimate (72.64%), ambiguously 
case-marked (70.65%) and agreeing with the 
verb (70.15%), where reported percentages refer 
to the whole error set. Likewise, Italian mistaken 
subjects can be described thus: they typically 
occur in post-verbal position (71.55%), are 
mostly inanimate (64.66%) and agree with the 
verb (61.21%). Interestingly, in both languages, 
the highest number of errors occurs when a) N 
has the least prototypical syntactic and semantic 
properties for O or S (relative to word order and 
noun animacy) and b) morpho-syntactic features 
such as agreement and case are neutralised. This 
shows that MaxEnt is able to home in on the core 
linguistic properties that govern the distribution 
of S and O in Italian and Czech, while remaining 
uncertain in the face of somewhat peripheral and 
occasional cases. 
A further way to evaluate the goodness of fit 
of our model is by inspecting the weights 
associated with feature values for the two 
languages. They are reported in Table 4, where 
grey cells highlight the preference of each 
feature value for either subject or object 
identification. In both languages agreement with 
the verb strongly relates to the subject relation. 
For Czech, nominative case is strongly 
associated with subjects while the other cases 
with objects. Moreover, in both languages 
preverbal subjects are strongly preferred over 
preverbal objects; animate subjects are preferred 
over animate objects; pronouns and proper 
names are typically subjects.  
Let us now try to relate these feature values to 
the Markedness Hierarchies reported in section 
4. Interestingly enough, if we rank the Italian 
Anim and Inanim values for subjects and objects, 
we observe that they distribute consistently with 
the Animacy Markedness Hierarchy: Subj/Anim 
> Subj/Inanim and Obj/Inanim > Obj/Anim. This 
is confirmed by the Czech results. Similarly, by 
ranking the Italian values for the definiteness 
features in the Subj column by decreasing weight 
values we obtain the following ordering: 
PronName > DefArt > IndefArt > NoArt, which 
nicely fits in with the Definiteness Markedness 
Hierarchy in section 4. The so-called 
?markedness reversal? is replicated with a good 
degree of approximation, if we focus on the 
values for the same features in the Obj column: 
the PronName feature represents the most 
marked option, followed by IndefArt, DefArt and 
NoArt (the latter two showing the same feature 
value). The exception here is represented by the 
relative ordering of IndefArt and DefArt which 
however show very close values. The same 
26
seems to hold for Czech, where the feature 
ordering for Subj is PronName > 
DefArt/IndefArt/NoArt and the reverse is 
observed for Obj.  
5.1 Evaluating comparative feature salience 
The relative salience of the different constraints 
acting on SOI can be inferred by comparing the 
weights associated with individual feature 
values. For instance, Goldwater and Johnson 
(2003) show that MaxEnt can successfully be 
applied to learn constraint rankings in Optimality 
Theory, by assuming the parameter weights <?1, 
?, ?k> as the ranking values of the constraints.  
Table 5 illustrates the constraint ranking for 
the two languages, ordered by decreasing weight 
values for both S and O. Note that, although not 
all constraints are applicable in both languages, 
the weights associated with applicable 
constraints exhibit the same relative salience in 
Czech and Italian. This seems to suggest the 
existence of a rather dominant (if not universal) 
salience scale of S and O processing constraints, 
in spite of the considerable difference in the 
marking strategies adopted by the two languages. 
As the relative weight of each constraint 
crucially depends on its overall interaction with 
other constraints on a given processing task, 
absolute weight values can considerably vary 
from language to language, with a resulting 
impact on the distribution of S and O 
constructions. For example, the possibility of 
overtly and unambiguously marking a direct 
object with case inflection makes wider room for 
preverbal use of objects in Czech. Conversely, 
lack of case marking in Italian considerably 
limits the preverbal distribution of direct objects.   
This evidence, however, appears to be an 
epiphenomenon of the interaction of fairly stable 
and invariant preferences, reflecting common 
functional tendencies in language processing. As 
shown in Table 5, if constraint ranking largely 
confirms the interplay between animacy and 
word order in Italian, Czech does not contradict 
it but rather re-modulate it somewhat, due to the 
?perturbation? factors introduced by its richer 
battery of case markers. 
6 Conclusions 
Probabilistic language models, machine language 
learning algorithms and linguistic theorizing all 
appear to support a view of language processing 
as a process of dynamic, on-line resolution of 
conflicting grammatical constraints. We begin to 
gain considerable insights into the complex 
process of bootstrapping nature and behaviour of 
these constraints upon observing their actual 
distribution in perceptually salient contexts. In 
our view of things, this trend outlines a 
promising framework providing fresh support to 
usage-based models of language acquisition 
through mathematical and computational 
simulations. Moreover, it allows scholars to 
investigate patterns of cross-linguistic 
typological variation that crucially depend on the 
appropriate setting of model parameters. Finally, 
it promises to solve, on a principled basis, 
traditional performance-oriented cruces of 
grammar theorizing such as degrees of human 
acceptability of ill-formed grammatical 
constructions (Hayes 2000) and the inherently 
graded compositionality of linguistic 
constructions such as morpheme-based words 
and word-based phrases (Bybee 2002, Hay and 
Baayen 2005).  
We argue that the current availability of 
comparable, richly annotated corpora and of 
mathematical tools and models for corpus 
exploration make time ripe for probing the space 
of grammatical variation, both intra- and inter-
linguistically, on unprecedented levels of 
sophistication and granularity. All in all, we 
anticipate that such a convergence is likely to 
have a twofold impact: it is bound to shed light 
on the integration of performance and 
competence factors in language study; it will 
make mathematical models of language 
increasingly able to accommodate richer and 
richer language evidence, thus putting 
explanatory theoretical accounts to the test of a 
usage-based empirical verification. 
In the near future, we intend to pursue two 
parallel lines of development. First we would 
like to increase the context-sensitiveness of our 
processing task by integrating binary 
grammatical constraints into the broader context 
of multiply conflicting grammar relations. This 
way, we will be in a position to capture the 
constraint that a (transitive) verb has at most one 
subject and one object, thus avoiding multiple 
assignment of subject (object) relations in the 
same context. Suppose, for example, that both 
nouns in a noun-noun-verb triple are amenable to 
a subject interpretation, but that one of them is a 
more likely subject than the other. Then, it is 
reasonable to expect the model to process the 
less likely subject candidate as the object of the 
verb in the triple. Another promising line of 
development is based on the observation that the 
27
order in which verb arguments appear in context 
is also lexically governed: in Italian, for 
example, report verbs show a strong tendency to 
select subjects post-verbally. Dell?Orletta et al 
(2005) report a substantial improvement on the 
model performance on Italian SOI when lexical 
information is taken into account, as a lexicalized 
MaxEnt model appears to integrate general 
constructional and semantic biases with 
lexically-specific preferences. In a cross-lingual 
perspective, comparable evidence of lexical 
constraints on word order would allow us to 
discover language-wide invariants in the lexicon-
grammar interplay.   
References 
Bates E., MacWhinney B., Caselli C., Devescovi A., 
Natale F., Venza V. 1984. A crosslinguistic study 
of the development of sentence interpretation 
strategies. Child Development, 55: 341-354. 
Bohmova A., Hajic J., Hajicova E., Hladka B. 2003. 
The Prague Dependency Treebank: Three-Level 
Annotation Scenario, in A. Abeille (ed.) 
Treebanks: Building and Using Syntactically 
Annotated Corpora, Kluwer Academic Publishers, 
pp. 103-128. 
Bybee J. 2002. Sequentiality as the basis of 
constituent structure. in T. Giv?n and B. Malle 
(eds.) The Evolution of Language out of Pre-
Language, Amsterdam: John Benjamins. 107-132. 
Croft W. 2003. Typology and Universals. Second 
Edition, Cambridge University Press, Cambridge. 
Bresnan J., Dingare D., Manning C. D. 2001. Soft 
constraints mirror hard constraints: voice and 
person in English and Lummi. Proceedings of the 
LFG01 Conference, Hong Kong: 13-32. 
Dell?Orletta F., Lenci A., Montemagni S., Pirrelli V. 
2005. Climbing the path to grammar: a maximum 
entropy model of subject/object learning. 
Proceedings of the ACL-2005 Workshop 
?Psychocomputational Models of Human 
Language Acquisition?, University of Michigan, 
Ann Arbour (USA), 29-30 June 2005. 
Hay J., Baayen R.H. 2005. Shifting paradigms: 
gradient structure in morphology, Trends in 
Cognitive Sciences, 9(7): 342-348. 
Hayes B. 2000. Gradient Well-Formedness in 
Optimality Theory, in Joost Dekkers, Frank van 
der Leeuw and Jeroen van de Weijer (eds.) 
Optimality Theory: Phonology, Syntax, and 
Acquisition, Oxford University Press, pp. 88-120. 
Lenci A. et al 2000. SIMPLE: A General Framework 
for the Development of Multilingual Lexicons. 
International Journal of Lexicography, 13 (4): 
249-263. 
MacWhinney B. 2004. A unified model of language 
acquisition. In J. Kroll & A. De Groot (eds.), 
Handbook of bilingualism: Psycholinguistic 
approaches, Oxford University Press, Oxford. 
Manning C. D. 2003. Probabilistic syntax. In R. Bod, 
J. Hay, S. Jannedy (eds), Probabilistic Linguistics,  
MIT Press, Cambridge MA: 289-341. 
Miyao Y., Tsujii J. 2002. Maximum entropy 
estimation for feature forests. Proc. HLT2002. 
Montemagni S. et al 2003. Building the Italian 
syntactic-semantic treebank. In Abeill? A. (ed.) 
Treebanks. Building and Using Parsed Corpora, 
Kluwer, Dordrecht: 189-210. 
Ratnaparkhi A. 1998. Maximum Entropy Models for 
Natural Language Ambiguity Resolution. Ph.D. 
Dissertation, University of Pennsylvania. 
   
Constraints for S  Constraints for O 
Feature Italian Czech  Feature Italian Czech 
Preverbal 1.31E+00 1.24E+00  Genitive na 1.51E+00 
Nomin na 1.23E+00  NoAgr 1.58E+00 1.50E+00 
Agr 1.28E+00 1.18E+00  Dative na 1.49E+00 
Anim 1.28E+00 1.16E+00  Accus na 1.39E+00 
Inanim 8.16E-01 1.03E+00  Instrum na 1.39E+00 
Postverbal 5.39E-01 8.77E-01  Postverbal 1.38E+00 1.17E+00 
Genitive na 2.94E-01  Inanim 1.23E+00 9.63E-01 
NoAgr 1.52E-01 7.71E-02  Agr 4.67E-01 6.67E-01 
Dative na 2.85E-02  Anim 3.17E-01 6.63E-01 
Accus na 8.06E-03  Preverbal 2.11E-02 5.40E-01 
Instrum na 3.80E-03  Nomin na 2.22E-02 
Table 5 ? Ranked constraints for S and O in Czech and Italian 
28
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 77?80,
Beijing, August 2010
Contrastive Filtering of Domain-Specific Multi-Word Terms from
Different Types of Corpora
Francesca Bonin? ?, Felice Dell?Orletta?, Giulia Venturi? and Simonetta Montemagni?
? Istituto di Linguistica Computazionale ?Antonio Zampolli? (ILC-CNR)
?Dipartimento di Informatica, Universita` di Pisa,
?CLIC Language Interaction and Computation Lab
{francesca.bonin, felice.dellorletta,
giulia.venturi, simonetta.montemagni}@ilc.cnr.it
Abstract
In this paper we tackle the challenging
task of Multi-word term (MWT) extrac-
tion from different types of specialized
corpora. Contrastive filtering of previ-
ously extracted MWTs results in a con-
siderable increment of acquired domain-
specific terms.
1 Introduction
Multi-word term (MWT) extraction is a challeng-
ing and well-known automatic term recognition
(ATR) subtask, aimed at retrieving complex do-
main terminology from specialized corpora. Al-
though domain sublanguages are characterized by
specific vocabularies, a well-defined border be-
tween specific sublanguages (SLs) and general
language (GL) vocabularies is difficult to establish
since lexicon shifts in a continuum from a highly
specialized area to a transition area between GL
and SLs (Rondeau et al, 1984). Within this con-
tinuum, Cabre? (1999) identifies three types of lex-
ical items: a. GL lexical items; b. SL terms, c.
lexical items belonging to a borderline area be-
tween GL and SL. The proportion of these dif-
ferent types of lexical items varies depending on
the text type. To our knowledge, automatic term
recognition methods proposed so far in the litera-
ture focussed on highly specialized corpora (typ-
ically, technical and scientific literature), mainly
characterized by SL terminology. However, the
same ATR methods may not be equally effective
when dealing with corpora characterized by a dif-
ferent proportion of term types; e.g. from texts
such as Wikipedia articles, which are conceived
for a more extended audience, both SL terms and
common words are acquired as long as they show
a statistically significant distribution. In this pa-
per, we claim that the contrastive approach to
MWT extraction described in Bonin et al (2010)
can be effectively exploited to distinguish be-
tween common words and domain-specific termi-
nology in different types of corpora as well as to
identify terms belonging to different SLs when oc-
curring in the same text. The latter is the case of
legal texts, characterized by a mixture of differ-
ent SLs, the legal and the regulated-domain SLs
(Breuker et al, 2004). Effectiveness and flexibil-
ity of the proposed ATR approach has been tested
with different experiments aimed at the extrac-
tion of domain terminology from corpora charac-
terized by different degrees of difficulty as far as
ATR is concerned, namely (i) environmental sci-
entific literature, (ii) Wikipedia environmental ar-
ticles, and (iii) a corpus of legal texts on environ-
mental domain.
2 General Extraction Method
The MWT extraction methodology we follow is
organized in two steps, described in detail in
Bonin et al (2010). Firstly, a shortlist of well-
formed and relevant candidate MWTs is extracted
from a given target corpus and secondly a con-
trastive method is applied against the selected
MWTs only. In fact, in the first stage, candi-
date MWTs are searched for in an automatically
POS-tagged and lemmatized text and they are then
weighted with the C-NC Value method (Frantzi et
al., 1999). In the second stage, the list of MWTs
extracted is revised and re-ranked with a con-
trastive score, based on the distribution of terms
across corpora of different domains; in particu-
77
lar, the Contrastive Selection of multi-word terms
(CSmw) function, newly introduced in Bonin et
al. (2010), was used, which proved to be partic-
ularly suitable for handling variation in low fre-
quency events. The main benefit of such an ap-
proach consists in its modularity; by first selecting
valid MWTs which have significant distributional
tendencies, and then by assessing their domain-
relevance using a contrastive function, the MWT
sparsity problem is overcome or at lest signifi-
cantly reduced.
3 Experiments
The MWT extraction methodology described
above has been followed in order to acquire envi-
ronmental terminology from three different kinds
of domain corpora. The first experiment has been
carried out on a corpus of scientific articles con-
cerning climate change research of Italian Na-
tional Research Council (CNR), of 397,297 to-
kens, while the second experiment has been car-
ried out on a corpus of Wikipedia articles from
the Italian Portal ?Ecologia e Ambiente? (Ecol-
ogy and Environment) (174,391 tokens). As gen-
eral contrastive corpus, we used, in both cases,
the PAROLE Corpus (Marinelli et al, 2003)1, in
order to filter out GL lexical items. The third
and more challenging experiment has been car-
ried out on a collection of Italian European legal
texts concerning the environmental domain for a
total of 394,088 word tokens. In this case, as con-
trastive corpus we exploited a collection of Ital-
ian European legal texts regulating a domain other
than the environmental one2, in order to extract
MWTs belonging to the environmental domain,
but also to single out legal-domain terms, used in
legal texts. For each acquisition corpus we fol-
lowed the two-layered approach described above,
selecting, firstly, a top list of 2000 environmental
MWTs from the candidate term list ranked on the
C-NC Value score and, secondly, re-ranking this
2000-term list on the basis of the CSmw function;
then we extracted the final top list of 300 envi-
ronmental MWTs. In order to assess the effec-
1It is made up of about 3 million word tokens and it in-
cludes Italian texts of different types.
2A corpus of Italian European Directives on consumer
protection domain for a total of 74,210 word tokens.
tiveness of the approach against different types of
corpora, we analyzed the two 300-term top lists
of MWTs acquired respectively after the first and
the second extraction steps. In both cases, we
divided the 300-term top lists in 30-term groups
which show domain-specific terms? distribution,
so that they could be easily compared. The eval-
uation has been carried out by comparing the lists
of MWTs extracted against a gold standard re-
source, i.e. the thesaurus EARTh (Environmen-
tal Applications Reference Thesaurus).3. In ad-
dition, a second resource has been used in the
third experiment for evaluating legal terms: the
Dizionario giuridico (Edizioni Simone)4. Those
terms which could not find a positive matching
against the gold standard resources were manually
validated by domain experts.
Scient.Lit. Wikipedia
Group C-NC CSmw C-NC CSmw
0-30 22 27 27 29
30-60 28 25 28 26
60-90 24 30 25 25
90-120 19 28 23 27
120-150 25 29 23 24
Sub-TOT 118 139 126 131
150-180 25 25 22 20
180-210 23 27 20 30
210-240 24 29 23 26
240-270 23 25 24 24
270-300 21 19 15 25
TOT 234 264 230 256
Table 1: Environmental terms in the 300-term top
lists from scientific articles (columns 2 and 3) and
from Wikipedia (columns 4 and 5).
3.1 Discussion of Results
Achieved experimental results highlight two main
issues. Firstly, they show that the proposed con-
trastive approach to domain-specific MWTs ex-
traction has a general good performance. As Fig-
ures 1, 2 and 3 show, the amount of environ-
mental MWTs after the contrastive stage increases
with respect to the amount of MWTs acquired af-
ter the candidate MWT extraction stage carried
3http://uta.iia.cnr.it/earth.htm#EARTh%202002. Con-
taining 12,398 environmental terms.
4Available online: http://www.simone.it/newdiz and in-
cluding 1,800 terms.
78
C-NC Value CSmw
Group Env Leg Env Leg
0-30 12 12 21 4
30-60 10 8 16 4
60-90 11 10 20 3
90-120 22 1 19 3
120-150 10 13 13 6
Sub-TOT 65 44 89 20
150-180 9 13 14 6
180-210 13 10 17 6
210-240 16 5 11 9
240-270 11 9 16 9
270-300 12 8 9 13
TOT 126 90 156 63
Table 2: Env(ironmental) and Leg(al) MWTs in
the 300-term top list from the legal corpus.
Type of text % relative increment
Wikipedia 11.30%
Scientific articles 12.82%
Legal texts 23.81%
Table 3: Relative increment of environmental
MWTs in the contrastive re-ranking stage.
out with the C-NC Value method. Secondly, re-
ported results witness that such performances are
differently affected by the different types of in-
put corpora: as summarized in Table 3, the rela-
tive increment of environmental MWTs after the
contrastive filtering stage ranges from 11.3% to
23.81%. Interestingly, as shown in Table 1, the
results obtained in the first and second experi-
ments show similar trends. This is due to the over-
whelming occurrence in the two input corpora of
specialized terminology with respect to the GL
items. Differently from what could have been
Figure 1: Scientific articles. Comparative pro-
gressive trend of environmental extracted terms.
expected, Wikipedia texts contain highly special-
ized terminology. However, a qualititative evalu-
ation of MTWs extracted revealed that this latter
corpus includes terms which belong to that bor-
derline area between GL and SL (case c. in the
Cabre? (1999) classification). It follows that in
the Wikipedia case the contrastive stage filtered
out not only common words, such as milione di
dollari ?a million dollars?, but also terms such as
unita` immobiliare ?real estate? belonging to such
borderline area of terminology; their difficult clas-
sification slightly decreases the contrastive stage
performance.
In the third experiment, the total amount of
environmental MWTs percentually increased by
23.81% after the second stage of contrastive re-
ranking. Differently from the previous experi-
ments, in this case we faced the need for dis-
cerning terms belonging to the vocabulary of two
SLs, i.e. regulated domain (i.e. environmental)
terms and legal ones (e.g. norma nazionale, na-
tional rule): this emerges clearly from the results
reported in Table 2 where it is shown that the
same number of environmental and legal MWTs
(i.e. 12 terms) are extracted at the first stage in
the first 30-term group, and that the contrastive
re-ranking allows the emergence of 21 environ-
mental MWTs against 4 legal MWTs only. This
trend can be observed in Figure 4, where the di-
vergent lines show the different distributions of
environmental and legal terms: interestingly, lines
cross each other where legal terms outnumber en-
vironmental terms, i.e. in the last 30-term group.
Such a relative increment with respect to the C-
NC Value ranking can be easily explained in terms
of the main features of the two methods, where C-
NC Value method is overtly aimed at extracting
domain-specific terminology (both environmental
and legal terms), and the contrastive re-ranking
step is specifically aimed at distinguishing the rel-
evance of acquired MWTs with respect to the in-
volved domains.
4 Conclusion
In this paper we tackled the challenging task of
MWT extraction from different kinds of domain
79
Figure 2: Wikipedia articles. Comparative pro-
gressive trend of environmental extracted terms.
Figure 3: Legal texts. Comparative progressive
trend of environmental extracted terms.
corpora, characterized by different types of termi-
nologies. We demonstrated that the multi-layered
approach proposed in Bonin et al (2010) can be
successfully exploited in distinguishing between
GL and SL items and in assessing the domain-
relevance of extracted terms. The latter is the case
of type of multi-domain corpora, characterized by
the occurrence of terms belonging to different SLs
(e.g. legal texts). Moreover, the results obtained
from different text types proved that the perfor-
mance of the contrastive filtering stage is dramat-
ically influenced by the nature of the acquisition
corpus.
5 Acknowledgments
The research has been supported in part by a grant
from the Italian FIRB project RBNE07C4R9.
Thanks are also due to Angela D?Angelo (Scuola
Figure 4: Legal texts. Trend of contrastive func-
tion.
Superiore Sant?Anna, Pisa) and Paolo Plini (EKO-
Lab, CNR, Rome), who contributed as domain ex-
perts to the evaluation.
References
Bonin, Francesca, Felice Dell?Orletta, Giulia Venturi,
and Simonetta Montemagni, 2010. A Contrastive
Approach to Multi-word Term Extraction from Do-
main Corpora, in Proceedings of the ?7th Interna-
tional Conference on Language Resources and Eval-
uation?, Malta, 19-21 May, 3222-3229.
Breuker, Joost, and Rinke Hoekstra, 2004. Epistemol-
ogy and Ontology in Core Ontologies: FOLaw and
LRI-Core, two core ontologies for law, in Proceed-
ings of the ?Workshop on Core Ontologies in Ontol-
ogy Engineering?, UK, 15-27.
Cabre?, M.Teresa, 1999. The terminology. Theory,
methods and applications. John Benjamins Publish-
ing Company.
Frantzi, Katerina, and Sofia Ananiadou, 1999. The
C-value / NC Value domain independent method for
multi-word term extraction. In Journal of Natural
Language Processing, 6(3):145-179.
Marinelli, Rita, et al, 2003. The Italian PAROLE cor-
pus: an overview. In A. Zampolli et al (eds.), Com-
putational Linguistics in Pisa, XVI-XVII, IEPI., I,
401-421.
Rondeau, Guy, and Juan Sager, 1984. Introduction
a` la terminologie (2nd ed.). Chicoutimi, Gatan
Morin.
80
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 115?124,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
ULISSE:
an Unsupervised Algorithm for Detecting Reliable Dependency Parses
Felice Dell?Orletta, Giulia Venturi and Simonetta Montemagni
Istituto di Linguistica Computazionale ?Antonio Zampolli? (ILC?CNR)
via G. Moruzzi, 1 ? Pisa (Italy)
{felice.dellorletta,giulia.venturi,simonetta.montemagni}@ilc.cnr.it
Abstract
In this paper we present ULISSE, an unsu-
pervised linguistically?driven algorithm to se-
lect reliable parses from the output of a de-
pendency parser. Different experiments were
devised to show that the algorithm is robust
enough to deal with the output of different
parsers and with different languages, as well
as to be used across different domains. In
all cases, ULISSE appears to outperform the
baseline algorithms.
1 Introduction
While the accuracy of state?of?the?art parsers is in-
creasing more and more, this is still not enough for
their output to be used in practical NLP?based ap-
plications. In fact, when applied to real?world texts
(e.g. the web or domain?specific corpora such as
bio?medical literature, legal texts, etc.) their accu-
racy decreases significantly. This is a real problem
since it is broadly acknowledged that applications
such as Information Extraction, Question Answer-
ing, Machine Translation, and so on can benefit sig-
nificantly from exploiting the output of a syntactic
parser. To overcome this problem, over the last few
years a growing interest has been shown in assessing
the reliability of automatically produced parses: the
selection of high quality parses represents nowadays
a key and challenging issue. The number of stud-
ies devoted to detecting reliable parses from the out-
put of a syntactic parser is spreading. They mainly
differ with respect to the kind of selection algo-
rithm they exploit. Depending on whether training
data, machine learning classifiers or external parsers
are exploited, existing algorithms can be classified
into i) supervised?based, ii) ensemble?based and iii)
unsupervised?based methods.
The first is the case of the construction of a ma-
chine learning classifier to predict the reliability of
parses on the basis of different feature types. Yates
et al (2006) exploited semantic features derived
from the web to create a statistical model to de-
tect unreliable parses produced by a constituency
parser. Kawahara and Uchimoto (2008) relied on
features derived from the output of a supervised de-
pendency parser (e.g. dependency lengths, num-
ber of unknown words, number of coordinated con-
junctions, etc.), whereas Ravi et al (2008) exploited
an external constituency parser to extract text?based
features (e.g. sentence length, unknown words, etc.)
as well as syntactic features to develop a super-
vised predictor of the target parser accuracy. The
approaches proposed by Reichart and Rappoport
(2007a) and Sagae and Tsujii (2007) can be classi-
fied as ensemble?based methods. Both select high
quality parses by computing the level of agreement
among different parser outputs: wheras the former
uses several versions of a constituency parser, each
trained on a different sample from the training data,
the latter uses the parses produced by different de-
pendency parsing algorithms trained on the same
data. However, a widely acknowledged problem of
both supervised?based and ensemble?based meth-
ods is that they are dramatically influenced by a) the
selection of the training data and b) the accuracy and
the typology of errors of the used parser.
To our knowledge, Reichart and Rappoport
(2009a) are the first to address the task of high qual-
115
ity parse selection by resorting to an unsupervised?
based method. The underlying idea is that syntactic
structures that are frequently created by a parser are
more likely to be correct than structures produced
less frequently. For this purpose, their PUPA (POS?
based Unsupervised Parse Assessment Algorithm)
uses statistics about POS tag sequences of parsed
sentences produced by an unsupervised constituency
parser.
In this paper, we address this unsupervised sce-
nario with two main novelties: unlike Reichart and
Rappoport (2009a), a) we address the reliable parses
selection task using an unsupervised method in a
supervised parsing scenario, and b) we operate on
dependency?based representations. Similarly to Re-
ichart and Rappoport (2009a) we exploit text inter-
nal statistics: but whereas they rely on features that
are closely related to constituency representations,
we use linguistic features which are dependency?
motivated. The proposed algorithm has been eval-
uated for selecting reliable parses from English and
Italian corpora; to our knowledge, this is the first
time that such a task has been applied to a less re-
sourced language such as Italian. The paper is or-
ganised as follows: in Section 2 we illustrate the
ULISSE algorithm; sections 3 and 4 are devoted to
the used parsers and baselines. Section 5 describes
the experiments and discusses achieved results.
2 The ULISSE Algorithm
The ULISSE (Unsupervised LInguiStically?driven
Selection of dEpendency parses) algorithm takes as
input a set of parsed sentences and it assigns to each
dependency tree a score quantifying its reliability. It
operates in two different steps: 1) it collects statis-
tics about a set of linguistically?motivated features
extracted from a corpus of parsed sentences; 2) it
calculates a quality (or reliability) score for each an-
alyzed sentence using the feature statistics extracted
from the whole corpus.
2.1 Selection of features
The features exploited by ULISSE are all linguis-
tically motivated and rely on the dependency tree
structure. Different criteria guided their selection.
First, as pointed out in Roark et al (2007), we
needed features which could be reliably identified
within the automatic output of a parser. Second,
we focused on dependency structures that are widely
agreed in the literature a) to reflect sentences? syn-
tactic and thus parsing complexity and b) to impose
a high cognitive load on the parsing of a complete
sentence.
Here follows the list of features used in the exper-
iments reported in this paper, which turned out to be
the most effective ones for the task at hand.
Parse tree depth: this feature is a reliable indicator
of sentence complexity due to the fact that, with sen-
tences of approximately the same length, parse tree
depth can be indicative of increased sentence com-
plexity (Yngve, 1960; Frazier, 1985; Gibson, 1998;
Nenkova, 2010).
Depth of embedded complement ?chains?: this
feature is a subtype of the previous one, focusing on
the depth of chains of embedded complements, ei-
ther prepositional complements or nominal and ad-
jectival modifiers. Long chains of embedded com-
plements make the syntactic structure more complex
and their analysis much more difficult.
Arity of verbal predicates: this feature refers to the
number of dependency links sharing the same ver-
bal head. Here, there is no obvious relation between
the number of dependents and sentence complexity:
both a small number and a high number of depen-
dents can make the sentence processing quite com-
plex, although for different reasons (elliptical con-
structions in the former case, a high number of mod-
ifiers in the latter).
Verbal roots: this feature counts the number of ver-
bal roots with respect to number of all sentence roots
in the target corpus.
Subordinate vs main clauses: subordination is gen-
erally considered to be an index of structural com-
plexity in language. Two distinct features are con-
sidered for monitoring this aspect: one measuring
the ratio between main and subordinate clauses and
the other one focusing on the relative ordering of
subordinate clauses with respect to the main clause.
It is a widely acknowledged fact that highly com-
plex sentences contain deeply embedded subordi-
nate clauses; however, subordinate clauses are easier
to process if they occur in post?verbal rather than in
pre?verbal position (Miller, 1998).
Length of dependency links: McDonald and Nivre
(2007) report that statistical parsers have a drop in
116
accuracy when analysing long distance dependen-
cies. This is in line with Lin (1996) and Gibson
(1998) who claim that the syntactic complexity of
sentences can be predicted with measures based on
the length of dependency links, given the memory
overhead of very long distance dependencies. Here,
the dependency length is measured in terms of the
words occurring between the syntactic head and the
dependent.
Dependency link plausibility (henceforth, Arc-
POSFeat): this feature is used to calculate the plausi-
bility of a dependency link given the part?of?speech
of the dependent and the head, by also considering
the PoS of the head father and the dependency link-
ing the two.
2.2 Computation Score
The quality score (henceforth, QS) of parsed sen-
tences results from a combination of the weights as-
sociated with the monitored features. ULISSE is
modular and can use several weights combination
strategies, which may be customised with respect to
the specific task exploiting the output of ULISSE.
For this study, QS is computed as a simple prod-
uct of the individual feature weights. This follows
from the necessity to recognize high quality parses
within the input set of parsed sentences: the prod-
uct combination strategy is able to discard low qual-
ity parse trees even in presence of just one low
weight feature. Therefore, QS for each sentence i
in the set of input parsed sentences I is QS(Si) =?n
y=1 Weight(Si, fy), where Si is the i?th sentence
of I , n is the total number of selected features and
Weight(Si, fy) is the computed weight for the y?th
feature.
Selected features can be divided into two classes,
depending on whether they are computed with re-
spect to each sentence and averaged over all sen-
tences in the target corpus (global features), or they
are computed with respect to individual dependency
links and averaged over all of them (local features).
The latter is the case of the ArcPOSFeat feature,
whereas the all other ones represent global features.
For the global features, the Weight(Si, fy) is de-
fined as:
Weight(Si, fy) = F (V (fy), range(L(Si), r))|range(L(Si), r)| ,
(1)
where V (fy) is the value of the y?th feature (ex-
tracted from Si), L(Si) is the length of the sen-
tence Si, range(L(Si), r) defines a range cov-
ering values from L(Si) ? r and L(Si) + r,
F (V (fy), range(L(Si), r)) is the frequency of
V (fy) in all sentences in I that has a value of
length in range(L(Si), r1) and |range(L(Si), r)|
is the total number of sentences in I with length
in range(L(Si), r). For what concerns the lo-
cal feature ArcPOSFeat, ULISSE assigns a weight
for each arc in Si: in principle different strate-
gies can be used to compute a unique weight for
this feature for Si. Here, the sentence weight
for the feature ArcPOSFeat is computed as the
minimum weight among the weights of all arcs
of Si. Therefore, Weight(Si, ArcPOSFeat) =
min{weight((Pd, Ph, t)), ?(Pd, Ph, t) ? Si},
where the triple (Pd, Ph, t) is an arc in Si in which
Pd is the POS of the dependent, Ph is the POS
of the syntactic head and t is the type of the de-
pendency relation and weight((Pd, Ph, t)) is the
weight of the specific arc (Pd, Ph, t). The individ-
ual arc weight is computed as follows:
weight((Pd, Ph, t)) = F ((Pd, Ph, t))F ((Pd,X, t)) ?
? F ((Pd, Ph, t))F ((X,Ph, t)) ?
? F (((Pd, Ph, t)(Ph, Ph2, t2)))F ((Pd, Ph, t)) ?
? F (((Pd, Ph, t)(Ph, Ph2, t2)))F ((Ph, Ph2, t2)) ?
? F (((Pd, Ph, t)(Ph, Ph2, t2)))F ((((Pd,X, t))(X,Ph2, t2))) ,
where F (x) is the frequency of x in I , X is a vari-
able and (arc1 arc2) represent two consecutive arcs
in the tree.
3 The Parsers
ULISSE was tested against the output of two really
different data?driven parsers: the first?order Max-
imum Spanning Tree (MST) parser (McDonald et
al., 2006) and the DeSR parser (Attardi, 2006) using
Support Vector Machine as learning algorithm. The
1We set r=0 in the in?domain experiments and r=2 in the
out?of?domain experiment reported in Sec 5.3.
117
former is a graph?based parser (following the so?
called ?all?pairs? approach Buchholz et al (2006))
where every possible arc is considered in the con-
struction of the optimal parse tree and where depen-
dency parsing is represented as the search for a max-
imum spanning tree in a directed graph. The latter
is a Shift?Reduce parser (following a ?stepwise? ap-
proach, Buchholz et al (2006)), where the parser
is trained and learns the sequence of parsing actions
required to build the parse tree.
Although both parser models show a similar accu-
racy, McDonald and Nivre (2007) demonstrate that
the two types of models exhibit different behaviors.
Their analysis exemplifies how different the two
parsers behave when their accuracies are compared
with regard to some linguistic features of the ana-
lyzed sentences. To mention only a few, the Shift?
Reduce parser tends to perform better on shorter
sentences, while the MST parser guarantees a higher
accuracy in identifying long distance dependencies.
As regards the identification of dependency types,
the MST parser shows a better ability to identify
the dependents of the sentences? roots whereas the
Shift?Reduce tends to better recognize specific rela-
tions (e.g. Subject and Object).
McDonald and Nivre (2007) describe how the
systems? behavioral differences are due to the dif-
ferent parsing algorithms implemented by the Shift?
Reduce and the MST parsing models. The Shift
Reduce parser constructs a dependency tree by per-
forming a sequence of parser actions or transitions
through a greedy parsing strategy. As a result of
this parsing procedure, a Shift Reduce parser cre-
ates shorter arcs before longer arcs. The latter could
be the reason for the lower accuracy in identifying
longer arcs when compared to the MST parser. This
also influences a lower level of accuracy in the anal-
ysis of longer sentences that usually contain longer
arcs than shorter sentences. The MST parser?s abil-
ity to analyze both short and long arcs is invariant
as it employs a graph-based parsing method where
every possible arc is considered in the construction
of the dependency tree.
4 The Baselines
Three different increasingly complex baseline mod-
els were used to evaluate the performance of
ULISSE.
The first baseline is constituted by a Random Se-
lection (RS ) of sentences from the test sets. This
baseline is calculated in terms of the scores of the
parser systems on the test set.
The second baseline is represented by the Sen-
tence Length (SL ), starting from the assumption,
demonstrated by McDonald and Nivre (2007), that
long sentences are harder to analyse using statistical
dependency parsers than short ones. This is a strong
unsupervised baseline based on raw text features,
ranking the parser results from the shortest sentence
to the longest one.
The third and most advanced baseline, exploiting
parse features, is the PUPA algorithm (Reichart and
Rappoport, 2007a). PUPA uses a set of parsed sen-
tences to compute the statistics on which its scores
are based. The PUPA algorithm operates on a con-
stituency based representation and collects statistics
about the POS tags of the words in the yield of the
constituent and of the words in the yields of neigh-
boring constituents. The sequences of POS tags that
are more frequent in target corpus receive higher
scores after proper regularization is applied to pre-
vent potential biases. Therefore, the final score as-
signed to a constituency tree results from a combina-
tion of the scores of its extracted sequences of POSs.
In order to use PUPA as a baseline, we imple-
mented a dependency?based version, hencefoth re-
ferred to as dPUPA. dPUPA uses the same score
computation of PUPA and collects statistics about
sequences of POS tags: the difference lies in the fact
that in this case the POS sequences are not extracted
from constituency trees but rather from dependency
trees. To be more concrete, rather than represent-
ing a sentence as a collection of constituency?based
sequences of POSs, dPUPA represents each sen-
tence as a collection of sequences of POSs cov-
ering all identified dependency subtrees. In par-
ticular, each dependency tree is represented as the
set of all subtrees rooted by non?terminal nodes.
Each subtree is then represented as the sequence
of POS tags of the words in the subtree (reflect-
ing the word order of the original sentence) inte-
grated with the POS of the leftmost and rightmost
in the sentence (NULL when there are no neigh-
bors). Figure 1 shows the example of the depen-
dency tree for the sentence I will give you the ball.
118
Figure 1: Example of dependency tree.
If we consider the subtree rooted by give (in the
dotted circle), the resulting POS sequence is as
follows: POS2 POS3 POS4 POS5 POS6 NULL,
where POS3 POS4 POS5 POS6 is the sequence of
POS tags in the subtree, POS2 is the left neighbor
POS tag and NULL marks the absence of a right
neighbor.
5 Experiments and Results
The experiments were organised as follows: a target
corpus was automatically POS tagged (Dell?Orletta,
2009) and dependency?parsed; the ULISSE and
baseline algorithms of reliable parse selection were
run on the POS?tagged and dependency?parsed tar-
get corpus in order to identify high quality parses;
results achieved by the selection algorithms were
evaluated with respect to a subset of the target cor-
pus of about 5,000 word?tokens (henceforth referred
to as ?test set?) for which gold-standard annotation
was available. Different sets of experiments were
devised to test the robustness of our algorithm. They
were performed with respect to i) the output of the
parsers described in Section 3, ii) two different lan-
guages, iii) different domains.
For what concerns the languages, we chose Italian
and English for two main reasons. First of all, they
pose different challenges to a parser since they are
characterised by quite different syntactic features.
For instance, Italian, as opposed to English, is char-
acterised by a relatively free word order (especially
for what concerns subject and object relations with
respect to the verb) and by the possible absence of
an overt subject. Secondly, as it is shown in Section
5.1, Italian is a less resourced language with respect
to English. This is a key issue, since as demonstrated
by Reichart and Rappoport (2007b) and McClosky
et al (2008), small and big treebanks pose different
problems in the reliable parses selection.
Last but not least, we aimed at demonstrating that
ULISSE can be successfully used not only with texts
belonging to the same domain as the parser train-
ing corpus. For this purpose, ULISSE was tested
on a target corpus of Italian legislative texts, whose
automatic linguistic analysis poses domain?specific
challenges (Venturi, 2010). Out?of?domain experi-
ments are being carried out also for English.
5.1 The Corpora
The Italian corpora Both parsers were trained on
ISST?TANL2, a dependency annotated corpus used
in Evalita?093, an evaluation campaign carried out
for Italian (Bosco et al, 2009). ISST?TANL in-
cludes 3,109 sentences (71,285 tokens) and consists
of articles from newspapers and periodicals.
Two different target corpora were used for the
in?domain and out?of?domain experiments. For
the former, we used a corpus of 1,104,237 sen-
tences (22,830,739 word?tokens) of newspapers
texts which was extracted from the CLIC-ILC Cor-
pus (Marinelli et al, 2003); for the legal domain,
we used a collection of Italian legal texts (2,697,262
word?tokens; 97,564 sentences) regulating a vari-
ety of domains, ranging from environment, human
rights, disability rights, freedom of expression to pri-
vacy, age disclaimer, etc. In the two experiments,
the test sets were represented respectively by: a) the
test set used in the Evalita?09 evaluation campaign,
constituted by 260 sentences and 5,011 tokens from
newpapers text; b) a set of 102 sentences (corre-
sponding to 5,691 tokens) from legal texts.
The English corpora For the training of parsers
we used the dependency?based version of Sections
2?11 of the Wall Street Journal partition of the
Penn Treebank (Marcus et al, 2003), which was de-
veloped for the CoNLL 2007 Shared Task on De-
pendency Parsing (Nivre et al, 2007): it includes
447,000 word tokens and about 18,600 sentences.
As target data we took a corpus of news, specif-
ically the whole Wall Street Journal Section of the
2http://medialab.di.unipi.it/wiki/SemaWiki
3http://evalita.fbk.eu/index.html
119
Penn Treebank4, from which the portion of text cor-
responding to the training corpus was removed; the
English target corpus thus includes 39,285,425 to-
kens (1,625,606 sentences). For testing we used
the test set of the CoNLL 2007 Shared Task, cor-
responding to a subset of Section 23 of the Wall
Street Journal partition of the Penn Treebank (5,003
tokens, 214 sentences).
5.2 Evaluation Methodology
Performances of the ULISSE algorithm have been
evaluated i) with respect to the accuracy of ranked
parses and ii) in terms of Precision and Recall. First,
for each experiment we evaluated how the ULISSE
algorithm and the baselines classify the sentences in
the test set with respect to the ?Labelled Attachment
Score? (LAS) obtained by the parsers, i.e. the per-
centage of tokens for which it has predicted the cor-
rect head and dependency relation. In particular, we
computed the LAS score of increasingly wider top
lists of k tokens, where k ranges from 500 word to-
kens to the whole size of the test set (with a step size
of 500 word tokens, i.e. k=500, k=1000, k=1500,
etc.).
As regards ii), we focused on the set of ranked
sentences showing a LAS ? ?. Since imposing
a 100% LAS was too restrictive, for each experi-
ment we defined a different ? threshold taking into
account the performance of each parser across the
different languages and domains. In particular, we
took the top 25% and 50% of the list of ranked sen-
tences and calculated Precision and Recall for each
of them. To this specific end, a parse tree showing
a LAS ? ? is considered as a trustworthy analysis.
Precision has been computed as the ratio of the num-
ber of trustworthy analyses over the total number of
sentences in each top list. Recall has been computed
as the ratio of the number of trustworthy analyses
which have been retrieved over the total number of
trustworthy analyses in the whole test set.
In order to test how the ULISSE algorithm is able
to select reliable parses by relying on parse fea-
tures rather than on raw text features, we computed
the accuracy score (LAS) of a subset of the top list
of sentences parsed by both parsers and ranked by
4This corpus represents to the unlabelled data set distributed
for the CoNLL 2007 Shared Task on Dependency Parsing, do-
main adaptation track.
ULISSE: in particular, we focused on those sen-
tences which were not shared by the MST and DeSR
top lists.
5.3 Results
We will refer to the performed experiments as fol-
lows: ?IT in?domain? and ?IT out?of?domain? for
the Italian experiments using respectively the ISST?
TANL test set (henceforth ISST TS) and the Legal-
Corpus test set (henceforth Legal TS); ?EN in?
domain? for the English experiment using the PTB
test set (PTB TS).
As a starting point let us consider the accuracy
of DeSR and MST parsers on the whole test sets,
reported in Table 1. The accuracy has been com-
puted in terms of LAS and of Unlabelled Attach-
ment Score (UAS), i.e. the percentage of tokens with
a correctly identified syntactic head. It can be no-
ticed that the performance of the two parsers is quite
similar for Italian (i.e. wrt ISST TS and Legal TS),
whereas there is a 2.3% difference between the MST
and DeSR accuracy as far as English is concerned.
ISST TS Legal TS PTB TS
Parser LAS UAS LAS UAS LAS UAS
DeSR 80.22 84.96 73.40 76.12 85.95 87.25
MST 79.52 85.43 73.99 78.72 88.25 89.55
Table 1: Overall accuracy of DeSR and MST parsers.
The plots in Figure 2 show the LAS of parses
ranked by ULISSE and the baselines across the dif-
ferent experiments. Each plot reports the results of a
single experiment: plots in the same row report the
LAS of DeSR and MST parsers with respect to the
same test set. In all experiments, ULISSE turned out
to be the best ranking algorithm since it appears to
select top lists characterised by higher LAS scores
than the baselines. As Figure 2 shows, all ranking
algorithms perform better than Random Selection
(RS), i.e. all top lists (for each k value) show a LAS
higher than the accuracy of DeSR and MST parsers
on the whole test sets. In the EN in?domain experi-
ment, the difference between the results of ULISSE
and the other ranking algorithms is smaller than in
the corresponding Italian experiment, a fact result-
ing from the higher accuracy of DeSR and MST
parsers (i.e. LAS 85.95% and 88.25% respectively)
on the PTB TS. It follows that, for example, the
first top list (with k=500) of the SL baseline has a
120
1000 2000 3000 4000 5000
80
82
84
86
88
90
 
 
 ULISSE
 LS
 dPUPA
 RS
(a) IT in?domain experiment (DeSR).
1000 2000 3000 4000 5000
79
80
81
82
83
84
85
86
 
 
 ULISSE
 LS
 dPUPA
 RS
(b) IT in?domain experiment (MST).
1000 2000 3000 4000 5000
74
76
78
80
82
 
 
 ULISSE
 LS
 dPUPA
 RS
(c) IT out?of?domain experiment (DeSR).
1000 2000 3000 4000 5000
66
68
70
72
74
76
78
80
82
84
 
 
 ULISSE
 LS
 dPUPA
 RS
(d) IT out?of?domain experiment (MST).
1000 2000 3000 4000 5000
86
88
90
92
94
 
 
 ULISSE
 LS
 dPUPA
 RS
(e) EN in?domain experiment (DeSR).
1000 2000 3000 4000 5000
88
89
90
91
92
93
94
 
 
 ULISSE
 LS
 dPUPA
 RS
(f) EN in?domain experiment (MST).
Figure 2: LAS of parses ranked by ULISSE algorithm and by the three baselines.
LAS accuracy of 93.36% and 93.96% respectively
for DeSR and MST: even in this case, ULISSE out-
performs all baselines. This is also the case in the
IT out?of?domain experiment. As reported in Table
1, parsing legal texts is a quite challenging task due
to a number of domain?specific peculiarities at the
level of syntax: this is testified by the average sen-
tence length which in the Legal TS is 56 word to-
kens. Nevertheless, ULISSE is able also in this case
to highly rank long sentences showing a high LAS.
For example, while in the first top list of 500 word
tokens the sentences parsed by DeSR and ordered by
SL have an average sentence length of 24 words and
a LAS of 79.37%, ULISSE includes in the same top
list longer sentences (with average sentence length =
29) with a higher LAS (82.72%). Also dPUPA ranks
in the same top list quite long sentences (with 27 av-
erage sentence length), but compared to ULISSE it
shows a lower LAS (i.e. 73.56%).
IT in?domain IT out?of?domain EN in?domain
DeSR MST DeSR MST DeSR MST
MST top?list 80.93 80.27 68.84 74.58 83.37 90.39
DeSR top?list 82.46 77.82 75.47 74.88 86.50 86.74
Table 3: LAS of not?shared sentences in the DeSR and
MST top?lists.
Results in Table 2 show that in the top 25% of
the ranked sentences with a LAS ? ? ULISSE has
the highest Precision and Recall in all experiments.
We believe that the low performance of dPUPA with
respect to all other ranking algorithms can be due to
121
DeSR MST
25% 50% 25% 50%
Prec Rec LAS AvgSL Prec Rec LAS AvgSL Prec Rec LAS AvgSL Prec Rec LAS AvgSL
IT in?domain: LAS ? 85% (DeSR: 120 sentences; MST: 112 sentences)
ULISSE 66.15 35.83 88.25 5.25 59.23 64.17 84.30 14.60 60 34.82 86.16 5.68 55.38 64.29 83.39 15.27
LS 63.08 34.17 84.54 4.15 53.08 57.50 82.07 11.90 58.46 33.93 82.73 4.45 53.08 61.61 82.14 12.75
dPUPA 61.54 33.33 86.89 6.68 59.23 64.17 84.36 14.82 53.85 31.25 82.26 8.61 50.00 58.04 79.94 17.04
IT out?of?domain: LAS ? 75% (DeSR: 51 sentences; MST: 57 sentences)
ULISSE 73.08 37.25 80.75 16.71 69.23 70.59 79.17 41.80 69.23 31.58 81.47 13.63 67.31 61.40 78.36 36
LS 53.85 27.45 76.71 12.63 67.31 68.63 78.34 34.14 61.54 28.07 78.42 11.30 69.23 63.16 79.78 30.54
dPUPA 57.69 29.41 73.97 15.67 61.54 62.74 75.24 40.39 46.15 21.05 72.08 22.56 57.69 52.63 74.86 42.91
EN in?domain: LAS ? 90% (DeSR: 118 sentences; MST: 120 sentences)
ULISSE 81.48 37.29 94.50 6.31 69.44 63.56 90.93 16.36 77.78 35 93.74 5.82 69.44 62.5 91.20 16.48
LS 77.78 35.59 93.39 4.87 65.74 60.17 91.01 13.67 75.92 34.17 93.55 4.79 68.52 61.67 90.84 13.44
dPUPA 74.07 33.90 89.76 7.95 65.74 60.17 88.37 18.14 77.78 35 93.43 5.08 68.52 61.67 91.03 14.49
Table 2: In all Tables: the number of sentences with a LAS ? ? parsed by DeSr and MST parsers (first row); Precision
(Prec), Recall (Rec), the corresponding parser accuracy (LAS) of the top 25% and 50% of the list of sentences and
ranked by the ULISSE algorithm, Length of Sentence (LS) and dependency PUPA (dPUPA) and the corresponding
average length in tokens of ranked sentence (AvgSL).
the fact that PUPA is based on constituency?specific
features that once translated in terms of dependency
structures may be not so effective.
In order to show that the ranking of sentences
does not follow from raw text features but rather
from parse features, we evaluated the accuracy of
parsed sentences that are not?shared by MST and
DeSR top?lists selected by ULISSE. For each test
set we selected a different top list: a set of 100
sentences in the IT and EN in?domain experiments
and of 50 sentences in the IT out?of?domain exper-
iment. For each of them we have a different number
of not?shared sentences: 24, 15 and 16 in the IT
in?domain, IT out?of?domain and EN in?domain
experiments respectively. Table 3 reports the LAS
of DeSR and MST for these sentences: it can be
observed that the LAS of not?shared sentences in
the DeSR top list is always higher than the LAS
assigned by the same parser to the not?shared sen-
tences in the MST top list, and viceversa. For in-
stance, in the English experiment the LAS achieved
by DeSR on the not?shared top list is higher (86.50)
than the LAS of DeSR on the not?shared MST top
list (83.37); viceversa, the LAS of MST on the not?
shared DeSR top list is higher (86.74) than the LAS
of MST on the not?shared MST top list (90.39). The
unique exception is MST in the IT out?of?domain
experiment, but the difference in terms of LAS be-
tween the parses is not statistically relevant (p?value
< 0.05). These results demonstrate that ULISSE is
able to select parsed sentences on the basis of the
reliability of the analysis produced by each parser.
6 Conclusion
ULISSE is an unsupervised linguistically?driven
method to select reliable parses from the output of
dependency parsers. To our knowledge, it repre-
sents the first unsupervised ranking algorithm oper-
ating on dependency representations which are more
and more gaining in popularity and are arguably
more useful for some applications than constituency
parsers. ULISSE shows a promising performance
against the output of two supervised parsers se-
lected for their behavioral differences. In all experi-
ments, ULISSE outperforms all baselines, including
dPUPA and Sentence Length (SL), the latter repre-
senting a very strong baseline selection method in a
supervised scenario, where parsers have a very high
performance with short sentences. The fact of car-
rying out the task of reliable parse selection in a su-
pervised scenario represents an important novelty:
however, the unsupervised nature of ULISSE could
also be used in an unsupervised scenario (Reichart
and Rappoport, 2010). Current direction of research
include a careful study of a) the quality score func-
tion, in particular for what concerns the combination
of individual feature weights, and b) the role and ef-
fectivess of the set of linguistic features. This study
is being carried out with a specific view to NLP tasks
which might benefit from the ULISSE algorithm.
This is the case, for instance, of the domain adap-
tation task in a self?training scenario (McClosky et
al., 2006), of the treebank construction process by
minimizing the human annotators? efforts (Reichart
and Rappoport, 2009b), of n?best ranking methods
for machine translation (Zhang, 2006).
122
References
Giuseppe Attardi. 2006. Experiments with a multilan-
guage non-projective dependency parser. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning (CoNLL-X ?06), New York
City, New York, pp. 166?170.
Cristina Bosco, Simonetta Montemagni, Alessandro
Mazzei, Vincenzo Lombardo, Felice Dell?Orletta and
Alessandro Lenci. 2009. Parsing Task: comparing
dependency parsers and treebanks. In Proceedings of
Evalita?09, Reggio Emilia.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Felice Dell?Orletta. 2009. Ensemble system for Part-of-
Speech tagging. In Proceedings of Evalita?09, Eval-
uation of NLP and Speech Tools for Italian, Reggio
Emilia, December.
Lyn Frazier. 1985. Syntactic complexity. In D.R.
Dowty, L. Karttunen and A.M. Zwicky (eds.), Natural
Language Parsing, Cambridge University Press, Cam-
bridge, UK.
Edward Gibson. 1998. Linguistic complexity: Locality
of syntactic dependencies. In Cognition, 68(1), pp. 1-
76.
Daisuke Kawahara and Kiyotaka Uchimoto. 2008.
Learning Reliability of Parses for Domain Adaptation
of Dependency Parsing. In Proceedings of IJCNLP
2008, pp. 709?714.
Dekan Lin. 1996. On the structural complexity of nat-
ural language sentences. In Proceedings of COLING
1996, pp. 729?733.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of CoNLL
2006.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the Errors of Data-Driven Dependency Parsing
Models. In Proceedings of EMNLP-CoNLL, 2007, pp.
122-131.
Mitchell P. Marcus, Mary Ann Marcinkiewicz and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the penn treebank. In Comput. Lin-
guist.,vol. 19, issue 2, MIT Press, pp. 313?330.
Rita Marinelli, et al 2003. The Italian PAROLE cor-
pus: an overview. In A. Zampolli et al (eds.), Compu-
tational Linguistics in Pisa, XVI?XVII, Pisa?Roma,
IEPI., I, 401?421.
David McClosky, Eugene Charniak and Mark Johnson.
2006. Reranking and self?training for parser adap-
tation. In Proceedings of ICCL?ACL 2006, pp. 337?
344.
David McClosky, Eugene Charniak and Mark Johnson.
2008. When is Self?Trainig Effective for parsing?. In
Proceedings of COLING 2008, pp. 561?568.
Jim Miller and Regina Weinert. 1998. Spontaneous spo-
ken language. Syntax and discourse. Oxford, Claren-
don Press.
Ani Nenkova, Jieun Chae, Annie Louis, and Emily Pitler.
2010. Structural Features for Predicting the Linguistic
Quality of Text Applications to Machine Translation,
Automatic Summarization and Human?Authored Text.
In E. Krahmer, M. Theune (eds.), Empirical Methods
in NLG, LNAI 5790, Springer-Verlag Berlin Heidel-
berg, pp. 222241.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, Deniz Yuret.
2007. The CoNLL 2007 Shared Task on Dependency
Parsing. In Proceedings of the EMNLP-CoNLL, pp.
915?932.
Sujith Ravi, Kevin Knight and Radu Soricut. 2008. Auto-
matic Prediction of Parser Accuracy. In Proceedings
of the EMNLP 2008, pp. 887?896.
Roi Reichart and Ari Rappoport. 2007a. An ensemble
method for selection of high quality parses. In Pro-
ceedings of ACL 2007, pp. 408?415.
Roi Reichart and Ari Rappoport. 2007b. Self?Training
for Enhancement and Domain Adaptation of Statisti-
cal Parsers Trained on Small Datasets. In Proceedings
of ACL 2007, pp. 616?623.
Roi Reichart and Ari Rappoport. 2009a. Automatic Se-
lection of High Quality Parses Created By a Fully Un-
supervised Parser. In Proceedings of CoNLL 2009,
pp. 156?164.
Roi Reichart and Ari Rappoport. 2009b. Sample Selec-
tion for Statistical Parsers: Cognitively Driven Algo-
rithms and Evaluation Measures. In Proceedings of
CoNLL 2009, pp. 3?11.
Roi Reichart and Ari Rappoport. 2010. Improved Fully
Unsupervised Parsing with Zoomed Learning. In Pro-
ceedings of EMNLP 2010.
Brian Roark, Margaret Mitchell and Kristy Hollingshead.
2007. Syntactic complexity measures for detecting
Mild Cognitive Impairment. In Proceedings of ACL
Workshop on Biological, Translational, and Clinical
Language Processing (BioNLP?07), pp. 1?8.
Kenji Sagae and Junichi Tsujii. 2007. Dependency
Parsing and Domain Adaptation with LR Models and
Parser Ensemble. In Proceedings of the EMNLP?
CoNLL 2007, pp. 1044?1050.
Giulia Venturi. 2010. Legal Language and Legal Knowl-
edge Management Applications. In E. Francesconi, S.
Montemagni, W. Peters and D. Tiscornia (eds.), Se-
mantic Processing of Legal Texts, Lecture Notes in
Computer Science, Springer Berlin / Heidelberg, vol.
6036, pp. 3-26.
123
Alexander Yates, Stefan Schoenmackers and Oren Et-
zioni. 2006. Detecting Parser Errors Using Web?
based Semantic Filters. In Proceedings of the EMNLP
2006, pp. 27?34.
Victor H.A. Yngve. 1960. A model and an hypothesis for
language structure. In Proceedings of the American
Philosophical Society, pp. 444-466.
Ying Zhang, Almut Hildebrand and Stephan Vogel.
2006. Distributed language modeling for N-best list
re-ranking. In Proceedings of the EMNLP 2006, pp.
216?223.
124
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 73?83,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
READ?IT:
Assessing Readability of Italian Texts with a View to Text Simplification
Felice Dell?Orletta, Simonetta Montemagni and Giulia Venturi
Istituto di Linguistica Computazionale ?Antonio Zampolli? (ILC?CNR)
via G. Moruzzi, 1 ? Pisa (Italy)
{felice.dellorletta,simonetta.montemagni,giulia.venturi}@ilc.cnr.it
Abstract
In this paper, we propose a new approach to
readability assessment with a specific view to
the task of text simplification: the intended
audience includes people with low literacy
skills and/or with mild cognitive impairment.
READ?IT represents the first advanced read-
ability assessment tool for what concerns Ital-
ian, which combines traditional raw text fea-
tures with lexical, morpho-syntactic and syn-
tactic information. In READ?IT readability
assessment is carried out with respect to both
documents and sentences where the latter rep-
resents an important novelty of the proposed
approach creating the prerequisites for align-
ing the readability assessment step with the
text simplification process. READ?IT shows
a high accuracy in the document classification
task and promising results in the sentence clas-
sification scenario.
1 Introduction
Recently, there has been increasing interest in the
exploitation of results from Natural Language Pro-
cessing (NLP) for the development of assistive tech-
nologies. Here, we address this topic by reporting
the first but promising results in the development
of a software architecture for the Italian language
aimed at assisting people with low literacy skills
(both native and foreign speakers) or who have lan-
guage disabilities in reading texts.
Within an information society, where everyone
should be able to access all available information,
improving access to written language is becoming
more and more a central issue. This is the case, for
instance, of administrative and governmental infor-
mation which should be accessible to all members of
the society, including people who have reading dif-
ficulties for different reasons: because of a low edu-
cation level or because of the fact that the language
in question is not their mother tongue, or because
of language disabilities. Health related information
represents another crucial domain which should be
accessible to a large and heterogenous target group.
Understandability in general and readability in par-
ticular is also an important issue for accessing infor-
mation over the web as stated in the Web Content
Accessibility Guidelines (WCAG) proposed by the
Web Accessibility Initiative of the W3C.
In this paper, we describe the approach we devel-
oped for automatically assessing the readability of
newspaper texts with a view to the specific task of
text simplification. The paper is organized as fol-
lows: Section 2 describes the background literature
on the topic; Section 3 introduces the main features
of our approach to readability assessment, with Sec-
tion 4 illustrating its implementation in the READ-
IT prototype; Sections 5 and 6 describe the experi-
mental setting and discuss achieved results.
2 Background
Readability assessment has been a central research
topic for the past 80 years which is still attracting
considerable interest nowadays. Over the last ten
years, within the NLP community the automatic as-
sessment of readability has received increasing at-
tention: if on the one hand the availability of sophis-
ticated NLP technologies makes it possible to moni-
tor a wide variety of factors affecting the readability
73
of a text, on the other hand there is a wide range
of both human- and machine-oriented applications
which can benefit from it.
Traditional readability formulas focus on a lim-
ited set of superficial text features which are taken
as rough approximations of the linguistic factors at
play in readability assessment. For example, the
Flesch-Kincaid measure (the most common reading
difficulty measure still in use, Kincaid (1975)) is a
linear function of the average number of syllables
per word and of the average number of words per
sentence, where the former and latter are used as
simple proxies for lexical and syntactic complexity
respectively. For Italian, there are two readability
formulas: an adaptation of the Flesh-Kincaid for En-
glish to Italian known as the Flesch-Vacca formula
(Franchina and Vacca, 1986); the GulpEase index
(Lucisano and Piemontese, 1988), assessing read-
ability on the basis of the average number of char-
acters per word and the average number of words
per sentence.
A widely acknowledged fact is that all traditional
readability metrics are quick and easy to calculate
but have drawbacks. For example, the use of sen-
tence length as a measure of syntactic complexity as-
sumes that a longer sentence is more grammatically
complex than a shorter one, which is often but not
always the case. Word syllable count is used start-
ing from the assumption that more frequent words
are more likely to have fewer syllables than less fre-
quent ones (an association that is related to Zipf?s
Law, Zipf (1935)); yet, similarly to the previous
case, word length does not necessarily reflects its
difficulty. The unreliability of these metrics has been
experimentally demonstrated by several recent stud-
ies in the field: to mention only a few Si and Callan
(2001), Petersen and Ostendorf (2006), Feng (2009).
On the front of the assessment of the lexical dif-
ficulty of a given text, a first step forward is rep-
resented by vocabulary-based formulas such as the
Dale-Chall formula (Chall and Dale, 1995), using
a combination of average sentence length and word
frequency counts. In particular, for what concerns
the latter it reconstructs the percentage of words
not on a list of 3000 ?easy? words by matching its
own list to the words in the material being evalu-
ated, to determine the appropriate reading level. If
vocabulary-based measures represent an improve-
ment in assessing the readability of texts which was
possible due to the availability of frequency dictio-
naries and reference corpora, they are still unsatis-
factory for what concerns sentence structure.
Over the last ten years, work on readability de-
ployed sophisticated NLP techniques, such as syn-
tactic parsing and statistical language modeling, to
capture more complex linguistic features and used
statistical machine learning to build readability as-
sessment tools. A variety of different NLP-based
approaches to the automatic readability assessment
has been proposed so far, differing with respect to:
a) the typology of features taken into account (e.g.
lexical, syntactic, semantic, discourse), and, for each
type, at the level of the inventory of used individual
features; b) the intended audience of the texts under
evaluation, which strongly influences the readability
assessment, and last but not least c) the application
within which readability assessment is carried out.
Interesting alternatives to static vocabulary-based
measures have been put forward by Si and Callan
(2001) who used unigram language models com-
bined with sentence length to capture content in-
formation from scientific web pages, or by Collins-
Thompson and Callan (2004) who adopted a sim-
ilar language modeling approach (Smoothed Uni-
gram model) to predict reading difficulty of short
passages and web documents. These approaches can
be seen as a generalization of the vocabulary-based
approach, aimed at capturing finer-grained and more
flexible information about vocabulary usage. If un-
igram language models help capturing important
content information and variation of word usage,
they do not cover other types of features which are
reported to play a significant role in the assessment
of readability. More recently, the role of syntac-
tic features started being investigated (Schwarm and
Ostendorf, 2005; Heilman et al, 2007; Petersen and
Ostendorf, 2009): in these studies syntactic structure
is tracked through a combination of features from n-
gram (trigram, bigram and unigram) language mod-
els and parse trees (parse tree height, number of
noun phrases, verb phrases and subordinated clauses
or SBARs) with more traditional features.
Yet, besides lexical and syntactic complexity fea-
tures there are other important factors, such as the
structure of the text, the definition of discourse topic,
discourse cohesion and coherence and so on, play-
74
ing a central role in determining the reading diffi-
culty of a text. More recent approaches esplored
the role of these features in readability assessment:
this is the case, for instance, of Barzilay and Lap-
ata (2008) or Feng (2010). The last few years have
been characterised by approaches based on the com-
bination of features ranging over different linguistic
levels, namely lexical, syntactic and discourse (see
e.g. Pitler and Nenkova (2008), Kate (2010)).
Another important factor determining the typol-
ogy of features to be considered for assessing read-
ability has to do with the intended audience of
readers: it is commonly agreed that reading ease
does not follow from intrinsic text properties alone,
but it is also affected by the expected audience.
Among the studies addressing readability with re-
spect to specific audiences, it is worth mentioning
here: Schwarm and Ostendorf (2005) and Heilman
et al (2007) dealing with language learners, or Feng
(2009) focussing on people with mild intellectual
disabilities. Interestingly,Heilman et al (2007) dif-
ferentiate the typology of used features when ad-
dressing first (L1) or second (L2) language learn-
ers: they argue that grammatical features are more
relevant for L2 than for L1 learners. Feng (2009)
propose a set of cognitively motivated features op-
erating at the discourse level specifically addressing
the cognitive characteristics of the expected users.
When readability is targeted towards adult compe-
tent language users a more prominent role is played
by discourse features (Pitler and Nenkova, 2008).
Applications which can benefit from an automatic
readability assessment range from the selection of
reading material tailored to varying literacy levels
(e.g. for L1/L2 students or low literacy people)
and the ranking of documents by reading difficulty
(e.g. in returning the results of web queries) to NLP
tasks such as automatic document summarization,
machine translation as well as text simplification.
Again, also the application making use of the read-
ability assessment, which is in turn strictly related to
the intended audience of readers, strongly influences
the typology of features to be taken into account.
Advanced NLP?based readability metrics devel-
oped so far typically deal with English, with a few
attempts devoted to other languages, namely French
(Collins-Thompson and Callan, 2004), Portuguese
(Aluisio et al, 2010) and German (Bru?ck, 2008).
3 Our Approach
Our approach to readability assessment was devel-
oped with a specific application in mind, i.e. text
simplification, and addresses a specific target audi-
ence of readers, namely people characterised by low
literacy skills and/or by mild cognitive impairment.
Following the most recent approaches, we treat read-
ability assessment as a classification task: in partic-
ular, given the available corpora for the Italian lan-
guage as well as the type of target audience, we re-
sorted to a binary classification aimed at discerning
easy?to?read textual objects from difficult?to?read
ones. The language dealt with is Italian: to our
knowledge, this is the first attempt of an advanced
methodology for readability assessment for this lan-
guage. Our approach focuses on lexical and syntac-
tic features, whose selection was influenced by the
application, the intended audience and the language
dealt with (both for its intrinsic linguistic features
and for the fact of being a less resourced language).
Following Roark (2007), in the features selection
process we preferred easy-to-identify features which
could be reliably identified within the output of NLP
tools. Last but not least, as already done by Aluisio
et al (2010) the set of selected syntactic features also
includes simplification oriented ones, with the final
aim of aligning the readability assessment step with
the text simplification process.
Another qualifying feature of our approach to
readability assessment consists in the fact that we
are dealing with two types of textual objects: docu-
ments and sentences. The latter represents an impor-
tant novelty of our work since so far most research
focused on readability classification at the document
level (Skory and Eskenazi, 2010). When the tar-
get application is text simplification, we strongly be-
lieve that also assessing readability at the sentence
level could be very useful. We know that methods
developed so far perform well to characterize the
level of an entire document, but they are unreliable
for short texts and thus also for single sentences.
Sentence-based readability assessment thus repre-
sents a further challenge we decided to tackle: in
fact, if all sentences occurring in simplified texts can
be assumed to be easy-to-read sentences, the reverse
does not necessarily hold since not all sentences oc-
curring in complex texts are difficult-to-read sen-
75
tences. Since there are no training data at the sen-
tence level, it becomes difficult ? if not impossible
? to evaluate the effectiveness of our approach, i.e.
erroneous readability assessments within the class
of difficult-to-read texts may either correspond to
those easy?to?read sentences occurring within com-
plex texts or represent real classification errors. In
order to overcome this problem in the readability as-
sessment of individual sentences, we introduced a
notion of distance with respect to easy-to-read sen-
tences. In this way, the prerequisites are created for
the integration of the two processes of readability as-
sessment and text simplification. Before, text read-
ability was assessed with respect to the entire doc-
ument and text simplification was carried out at the
sentence level: due to the decoupling of the two pro-
cesses, the impact of simplification operations on the
overall readability level of the text was not always
immediately clear. With sentence-based readability
assessment, this should be no longer a problem.
4 READ?IT
Our approach to readability assessment has been im-
plemented in a software prototype, henceforth re-
ferred to as READ?IT. READ?IT operates on syn-
tactically (i.e. dependency) parsed texts and it as-
signs to each considered reading object - either a
document or a sentence - a score quantifying its
readability. READ?IT is a classifier based on Sup-
port Vector Machines using LIBSVM (Chang and
Lin, 2001) that, given a set of features and a training
corpus, creates a statistical model using the feature
statistics extracted from the training corpus. Such
a model is used in the assessment of readability of
unseen documents and sentences.
The set of features used to build the statistical
model can be parameterized through a configura-
tion file: as we will see, the set of relevant fea-
tures used for readability assessment at the docu-
ment level differs from the those used at the sen-
tence level. This also creates the prerequisites for
specialising the readability assessment measure with
respect to more specific target audiences: as pointed
out in Heilman et al (2007) different types of fea-
tures come into play e.g. when addressing L1 or L2
language learners. Here follows the complete list of
features used in the reported experiments.
4.1 Features
The features used for predicting readability are or-
ganised into four main categories: namely, raw text
features, lexical features as well as morpho-syntactic
and syntactic features. This proposed four?fold par-
tition closely follows the different levels of linguis-
tic analysis automatically carried out on the text be-
ing evaluated, i.e. tokenization, lemmatization, PoS
tagging and dependency parsing. Such a partition
was meant to identify those easy to extract features
with high discriminative power in order to reduce
the linguistic pre-processing of texts guaranteeing at
the same time a reliable readability assessment.
Raw Text Features
They refer to those features typically used within tra-
ditional readability metrics. They include Sentence
Length, calculated as the average number of words
per sentence, and Word Length, calculated as the av-
erage number of characters per words.
Lexical Features
Basic Italian Vocabulary rate features: these fea-
tures refer to the internal composition of the vocab-
ulary of the text. To this end, we took as a refer-
ence resource the Basic Italian Vocabulary by De
Mauro (2000), including a list of 7000 words highly
familiar to native speakers of Italian. In particular,
we calculated two different features corresponding
to: i) the percentage of all unique words (types)
on this reference list (calculated on a per?lemma
basis); ii) the internal distribution of the occurring
basic Italian vocabulary words into the usage clas-
sification classes of ?fundamental words? (very fre-
quent words), ?high usage words? (frequent words)
and ?high availability words? (relatively lower fre-
quency words referring to everyday objects or ac-
tions and thus well known to speakers). Whereas
the latter represents a novel feature in the readability
assessment literature, the former originates from the
Dale-Chall formula (Chall and Dale, 1995) and, as
implemented here, it can be seen as the complement
of the type out-of-vocabulary rate features used by
Petersen and Ostendorf (2009).
Type/Token Ratio: this feature refers to the ratio
between the number of lexical types and the num-
ber of tokens. This feature, which can be consid-
ered as an indicator of expressive language delay or
76
disorder as shown in Wright (2003) for adults and
in Retherford (2003) for children, has already been
used for readability assessment purposes by Aluisio
et al (2010). Due to its sensitivity to sample size,
this feature has been computed for text samples of
equivalent length.
Morpho?syntactic Features
Language Model probability of Part-Of-Speech
unigrams: this feature is based on a unigram lan-
guage model assuming that the probability of a token
is independent of its context. The model is simply
defined by a list of types (POS) and their individual
probabilities. This feature has already been shown
to be a reliable indicator for automatic readability
assessment (see, for example, Pitler and Nenkova
(2008) and Aluisio et al (2010)).
Lexical density: this feature refers to the ratio of
content words (verbs, nouns, adjectives and adverbs)
to the total number of lexical tokens in a text. Con-
tent words have already been used for readability as-
sessment by Aluisio et al (2010) and Feng (2010).
Verbal mood: this feature refers to the distribution
of verbs according to their mood. It is a novel and
language?specific feature exploiting the predictive
power of the Italian rich verbal morphology.
Syntactic Features
Unconditional probability of dependency types:
this feature refers to the unconditional probability of
different types of syntactic dependencies (e.g. sub-
ject, direct object, modifier, etc.) and can be seen
as the dependency-based counterpart of the ?phrase
type rate? feature used by Nenkova (2010).
Parse tree depth features: parse tree depth can be
indicative of increased sentence complexity as stated
by, to mention only a few, Yngve (1960), Frazier
(1985) and Gibson (1998). This set of features is
meant to capture different aspects of the parse tree
depth and includes the following measures: a) the
depth of the whole parse tree, calculated in terms of
the longest path from the root of the dependency tree
to some leaf; b) the average depth of embedded com-
plement ?chains? governed by a nominal head and
including either prepositional complements or nomi-
nal and adjectival modifiers; c) the probability distri-
bution of embedded complement ?chains? by depth.
The first feature has already been used in syntax-
based readability assessment studies (Schwarm and
Ostendorf, 2005; Heilman et al, 2007; Nenkova,
2010); the latter two are reminiscent of the ?head
noun modifiers? feature used by Nenkova (2010).
Verbal predicates features: this set of features cap-
tures different aspects of the behaviour of verbal
predicates. They range from the number of verbal
roots with respect to number of all sentence roots
occurring in a text to their arity. The arity of verbal
predicates is calculated as the number of instanti-
ated dependency links sharing the same verbal head
(covering both arguments and modifiers). Although
there is no obvious relation between the number of
verb dependents and sentence complexity, we be-
lieve that both a low and a high number of depen-
dents can make sentence readability quite complex,
although for different reasons (elliptical construc-
tions in the former case, a high number of modifiers
in the latter). Within this feature set we also con-
sidered the distribution of verbal predicates by arity.
To our knowledge, this set of features has never been
used so far for readability assessment purposes.
Subordination features: subordination is widely
acknowledged to be an index of structural complex-
ity in language. As in Aluisio et al (2010), this set
of features has been introduced here with a specific
view to the text simplification task. A first feature
was meant to measure the distribution of subordi-
nate vs main clauses. For subordinates, we also
considered their relative ordering with respect to
the main clause: according to Miller and Weinert
(1998), sentences containing subordinate clauses in
post?verbal rather than in pre?verbal position are
easier to read. Two further features were intro-
duced to capture the depth of embedded subordinate
clauses since it is a widely acknowledged fact that
highly complex sentences contain deeply embedded
subordinate clauses: in particular, a) the average
depth of ?chains? of embedded subordinate clauses
and b) the probability distribution of embedded sub-
ordinate clauses ?chains? by depth.
Length of dependency links feature: both Lin
(1996) and Gibson (1998) showed that the syntactic
complexity of sentences can be predicted with mea-
sures based on the length of dependency links. This
is also demonstrated in McDonald and Nivre (2007)
who claim that statistical parsers have a drop in ac-
curacy when analysing long dependencies. Here, the
77
dependency length is measured in terms of the words
occurring between the syntactic head and the depen-
dent. This feature is the dependency-based counter-
part of the ?phrase length? feature used for readabil-
ity assessment by Nenkova (2010) and Feng (2010).
5 The Corpora
One challenge in this work was finding an appropri-
ate corpus. Although a possibly large collection of
texts labelled with their target grade level (such as
the Weekly Reader for English) would be ideal, we
are not aware of any such collection that exists for
Italian in electronic form. Instead, to test our ap-
proach to automatically identify the readability of a
given text, we used two different corpora: a news-
paper corpus, La Repubblica (henceforth, ?Rep?),
and an easy?to?read newspaper, Due Parole (hence-
forth, ?2Par?) which was specifically written for an
audience of adults with a rudimentary literacy level
or with mild intellectual disabilities. The articles in
2Par were written by Italian linguists expert in text
simplification using a controlled language both at
the lexicon and sentence structure levels (Piemon-
tese, 1996) .
There are different motivations underlying the se-
lection of these two corpora for our study. On the
practical side, to our knowledge 2Par is the only
available corpus of simplified texts addressing a
wide audience characterised by a low literacy level.
So, the use of 2Par represented the only possible op-
tion on the front of simplified texts. For the selection
of the second corpus we opted for texts belonging
to the same class, i.e. newspapers: this was aimed
at avoiding interferences due to textual genre varia-
tion in the measure of text readability. This is con-
firmed by the fact that the two corpora show a sim-
ilar behaviour with respect to a number of different
parameters, which according to the literature on reg-
ister variation (Biber, 2009) are indicative of textual
genre differences: e.g. lexical density, the noun/verb
ratio, the percentage of verbal roots, etc. On the
other hand, the two corpora differ significantly with
respect to the distribution of features typically cor-
related with text complexity, e.g. the composition of
the used vocabulary (e.g. the percentage of words
belonging to the Basic Italian Vocabulary in Rep is
4.14% and in 2Par is 48.04%) or, from the syntactic
point of view, the average parse tree height (which in
Rep is 5.71 and in 2Par 3.67), the average number of
verb phrases per sentence (which in Rep is 2.40 and
in 2Par 1.25), the depth of nested structures (e.g. the
average depth of embedded complement ?chains? in
Rep is 1.44 and in 2Par is 1.30), the proportion of
main vs subordinate clauses (in Rep main and sub-
ordinate clauses represent respectively 65.11% and
34.88% of the cases; in 2Par there is 79.66% of main
clauses and 20.33% of subordinate clauses).
The Rep/2Par pair of corpora is somehow remi-
niscent of corpora used in other readability studies,
such as Encyclopedia Britannica and Britannica El-
ementary, but with a main difference: whereas the
English corpora consist of paired original/simplified
texts, which we might define as ?parallel monolin-
gual corpora?, the selected Italian corpora rather
present themselves as ?comparable monolingual
corpora?, without any pairing of the full?simplified
versions of the same article. Comparability is guar-
anteed here by the inclusion of texts belonging to the
same textual genre: we expect such comparable cor-
pora to be usefully exploited for readability assess-
ment because of the emphasis on style over topic.
Although these corpora do not provide an ex-
plicit grade?level ranking for each article, broad
categories are distinguished, namely easy?to?read
vs difficult?to?read texts. The two paired com-
plex/simplified corpora were used to train and test
different language models described in Section 6.
As already pointed out, such a distinction is reliable
in a document classification scenario, while at the
sentence classification level it poses the remarkable
issue of discerning easy?to?read sentences within
difficult?to?read documents (i.e. Rep).
6 Experiments and Results
READ?IT was tested on the 2Par and Rep cor-
pora automatically POS tagged by the Part?Of?
Speech tagger described in Dell?Orletta (2009) and
dependency?parsed by the DeSR parser (Attardi,
2006) using Support Vector Machine as learning al-
gorithm. Three different sets of experiments were
devised to test the performance of READ-IT in the
following subtasks: i) document readability classifi-
cation, ii) sentence readability classification and iii)
detection of easy?to?read sentences within difficult?
78
to?read texts.
For what concerns the document classification
subtask, we used a corpus made up of 638 docu-
ments of which 319 were extracted from 2Par (taken
as representative of the class easy?to?read texts) and
319 from Rep (representing the class of difficult?
to?read texts). We have followed a 5?fold cross?
validation process: the corpus was randomly split
into 5 training and test sets. The test sets consisted
of 20% of the individual documents belonging to the
two considered readability levels, with each docu-
ment being included in one test set only. With re-
gard to the sentence classification subtask, we used a
training set of about 3,000 sentences extracted from
2Par and of about 3,000 sentences from Rep and a
test corpus of 1,000 sentences of which 500 were ex-
tracted from 2Par (hereafter, 2Par test set) and 500
from Rep (hereafter, Rep test set). In the third ex-
periment, readability assessment was carried out by
READ?IT with respect to a much bigger corpus of
2,5 milion of words extracted from the newspaper
La Repubblica (hereafter, Rep 2.5), for a total of
123,171 sentences, with the final aim of detecting
easy?to?read sentences.
All the experiments were carried out using four
different readability models, described as follows:
1. Base Model, using raw text features only;
2. Lexical Model, using a combination of raw
text and lexical features;
3. MorphoS Model: using raw text, lexical and
morpho?syntactic features;
4. Syntax Model: combining all feature types,
namely raw text, lexical, morpho?syntactic and
syntactic features.
Note that in the Lexical and Syntax Models, dif-
ferent sets of features were selected for the subtasks
of document and sentence classification. In particu-
lar, for sentence?based readability assesment we did
not take into account the Type/Token Ratio feature,
all features concerning the distribution of ?chains?
of embedded complements and subordinate clauses
and the distribution of verbal predicates by arity.
Since, to our knowledge, a machine learning read-
ability classifier does not exist for the Italian lan-
guage we consider the Base Model as our baseline:
this can be seen as an approximation of the Gul-
pEase index, which is based on the same raw text
features (i.e. sentence and word length).
6.1 Evaluation Methodology
Different evaluation methods have been defined in
order to assess achieved results in the three afore-
mentioned experiment sets. The performance of
both document and sentence classification experi-
ments have been evaluated in terms of i) overall Ac-
curacy of the system and ii) Precision and Recall.
In particular, Accuracy is a global score referring
to the percentage of documents or sentences cor-
rectly classified, either as easy?to?read or difficult?
to?read objects. Precision and Recall have been
computed with respect to two the target reading lev-
els: in particular, Precision is the ratio of the number
of correctly classified documents or sentences over
the total number of documents and sentences classi-
fied by READ?IT as belonging to the easy?to?read
(i.e. 2Par) or difficult?to?read (i.e. Rep) classes; Re-
call has been computed as the ratio of the number of
correctly classified documents or sentences over the
total number of documents or sentences belonging
to each reading level in the test sets. For each set of
experiments, evaluation was carried out with respect
to the four models of the classifier.
Following from the assumption that 2Par con-
tains only easy?to?read sentences while Rep does
not necessarily contain only difficult?to?read ones,
we consider READ?IT errors in the classification of
2Par sentences as erroneously classified sentences.
On the other hand, classification errors within the
set of Rep sentences deserve an in?depth error anal-
ysis, since we need to discern real errors from mis-
classifications due to the fact that we are in front of
easy?to?read sentences occurring in a difficult?to?
read context. In order discern errors from ?correct?
misclassifications, we introduced a new evaluation
methodology, based on the notion of Euclidean dis-
tance between feature vectors. Each feature vec-
tor is a n?dimensional vector of linguistic features
(see Section 4.1) that represents a set of sentences.
Two vectors with 0 distance represent the same set
of sentences, i.e. those sentences sharing the same
values for the monitored linguistic features. Con-
versely, the bigger the distance between two vectors
is, the more distant are the two represented sets of
79
sentences with respect to the monitored features.
The same notion of distance has also been used
to test which model was more effective in predicting
the readability of n?word long sentences.
6.2 Results
In Table 1, the Accuracy, Precision and Recall scores
achieved with the different READ?IT models in the
document classification subtask are reported. It can
be noticed that the Base Model shows the lowest per-
formance, while the MorphoS Model outperforms
all other models. Interestingly, the Lexical Model
shows a high accuracy for what concerns the doc-
ument classification subtask (95.45%), by signifi-
cantly improving the accuracy score of the Base
Model (about +19%). This result demonstrates that
for assessing the readability of documents a combi-
nation of raw and lexical features provides reliable
results which can be further improved (about +3%)
by also taking into account morpho-syntax.
2Par Rep
Model Accuracy Prec Rec Prec Rec
Base 76.65 74.71 80.56 78.91 72.73
Lexical 95.45 95.60 95.30 95.31 95.61
MorphoS 98.12 98.12 98.12 98.12 98.12
Syntax 97.02 97.17 96.87 96.88 97.18
Table 1: Document classification results
Consider now the sentence classification subtask.
Table 2 shows that in this case the most reliable re-
sults are achieved with the Syntax Model. It is inter-
esting to note that the morpho?syntactic and syntac-
tic features allow a much higher increment in terms
of Accuracy, Precision and Recall scores than in the
document classification scenario: i.e. the difference
between the performance of the Lexical Model and
the best one in the document classification experi-
ment (i.e. the MorphoS Model) is equal to 2.6%,
while in the sentence classification case (i.e. Syntax
Model) is much higher, namely 17% .
In Table 3, we detail the performance of the best
READ?IT model (i.e. the Syntax Model) on the
Rep test set. In order to evaluate those sentences
which were erroneously classified as belonging to
2Par, we calculated the distance between 2Par and i)
these sentences (140 sentences referred to as wrong
in the Table), ii) the correctly classified sentences
2Par Rep
Model Accuracy Prec Rec Prec Rec
Base 59.6 55.6 95.0 82.9 24.2
Lexical 61.6 57.3 91.0 78.1 32.2
MorphoS 76.1 72.8 83.4 80.6 68.8
Syntax 78.2 75.1 84.4 82.2 72.0
Table 2: Sentence classification results
(360 sentences, referred to as correct in the Table),
iii) the whole Rep test set. As we can see, the dis-
tance between the wrong sentences and 2Par is much
lower than the distance holding between 2Par and
the correcly classified sentences (correct). This en-
tails that the sentences which were erroneously clas-
sified as easy?to?read sentences (i.e. belonging to
2Par) are in fact more readable than the correctly
classified ones (as belonging to Rep).It is obvious
that the Rep test set, which contains both correct and
wrong sentences, has an intermediate distance value
with respect to 2Par.
Distance
Correct 52.072
Rep test set 45.361
Wrong 37.843
Table 3: Distances between 2Par and Rep on the basis of
the Syntax Model
In Table 4, the percentage of Rep 2.5 sentences
classified as difficult?to?read is reported. The re-
sults show that the Syntax Model classifies the higher
number of sentences as difficult?to?read, but from
these results we cannot say whether this model is the
best one or not since Rep 2.5 sentences are not anno-
tated with readability information. Therefore, in or-
der to compare the performance of the four READ?
IT models and to identify which is the best one, we
computed the distance between the sentences clas-
sified as easy?to?read and 2Par, which is reported,
for each model, in Table 5. It can be noticed that
the Syntax Model appears to be the best one since it
shows the lowest distance with respect to 2Par; on
the other hand, the whole Rep 2.5 corpus shows a
higher distance since it contains both difficult? and
easy?to?read sentences. Obviously, the sentences
classified as difficult?to?read by the Syntax Model
(Diff Syntax in the Table) show the broader distance.
80
Accuracy
Base 0.234
Lexical 0.387
MorphoS 0.705
Syntax 0.755
Table 4: Accuracy in sentence classification of Rep 2.5.
Distance
Diff Syntax 66.526
Rep 2.5 64.040
Base 61.135
Lexical 60.529
MorphoS 55.535
Syntax 51.408
Table 5: Distance between 2Par and i) difficult?to?read
sentences according to the Syntax Model, ii) Rep 2.5, iii)
easy?to?read sentences by the four models.
In order to gain an in?depth insight into the
different behaviour of the four READ?IT models,
we evaluated their performances for sentences of a
fixed length. We considered sentences whose length
ranges between 8 and 30. For every set of sentences
of the same legth, we compared the easy?to-read
sentences of Rep 2.5 classified by the four models
with respect to 2Par. In Figure 1, each point rep-
resents the distance between a set of sentences of
the same length and the same n?word long set of
sentences in the 2Par corpus. As it can be seen, the
bottom line which represents the sentences classified
as easy?to?read by the Syntax Model is the closest
to the 2Par sentences of the same length. On the
contrary, the line representing the sentences classi-
fied by the Base Model is the most distant amongst
the four READ?IT models. Interestingly, it over-
laps with the line representing the Rep 2.5 sentences:
this suggests that a classification model based only
on raw text features (i.e. sentence and word length)
is not able to identify easy?to?read sentences if we
consider sets of sentences of a fixed length. Obvi-
ously, the line representing the sentences classified
as difficult?to?read by the Syntax Model shows the
broadest distance. This experiment has shown that
linguistically motivated features (and in particular
syntactic ones) have a fundamental role in the sen-
tence readability assessment subtask.
Figure 1: Distance between 2Par and i) difficult?to?read
sentences according to the Syntax Model, ii) Rep 2.5, iii)
easy?to?read sentences by the four models for sets of sen-
tences of fixed length
7 Conclusion
In this paper, we proposed a new approach to read-
ability assessment with a specific view to the task of
text simplification: the intended audience includes
people with low literacy skills and/or with mild cog-
nitive impairment. The main contributions of this
work can be summarised as follows: i) READ?
IT represents the first advanced readability assess-
ment tool for what concerns Italian; ii) it combines
traditional raw text features with lexical, morpho-
syntactic and syntactic information; iii) readability
assessment is carried out with respect to both doc-
uments and sentences. Sentence?based readability
assessment is an important novelty of our approach
which creates the prerequisites for aligning readabil-
ity assessment with text simplification. READ?IT
shows a high accuracy in the document classification
task and promising results in the sentence classifica-
tion scenario. The two different tasks appear to en-
force different requirements at the level of the under-
lying linguistic features. To overcome the lack of an
Italian reference resource annotated with readability
information at the sentence level we introduced the
notion of distance to assess READ?IT performance.
81
Acknowledgements
The research reported in the paper has been partly
supported by the national project ?Migrations? of
the National Council of Research (CNR) in the
framework of the line of research Advanced tech-
nologies and linguistic and cultural integration in
the school. In particular the authors would like
to thank Eva Maria Vecchi who contributed to the
prerequisites of the proposed readability assessment
methodology reported in the paper.
References
Sandra Aluisio, Lucia Specia, Caroline Gasperin and
Carolina Scarton. 2010. Readability assessment for
text simplification. In Proceedings of the NAACL HLT
2010 Fifth Workshop on Innovative Use of NLP for
Building Educational Applications, pp. 1?9.
Giuseppe Attardi. 2006. Experiments with a multilan-
guage non-projective dependency parser. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning (CoNLL-X ?06), New York
City, New York, pp. 166?170.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. In Com-
putational Linguistics, 34(1), pp. 1?34.
Douglas Biber and Susan Conrad. 2009. Register, genre,
and style. Cambridge, Cambridge University Press.
Tim vor der Bru?ck, Sven Hartrumpf, Hermann Helbig.
2008. A Readability Checker with Supervised Learn-
ing using Deep Syntactic and Semantic Indicators. In
Proceedings of the 11th International Multiconference:
Information Society - IS 2008 - Language Technolo-
gies, Ljubljana, Slovenia, pp. 92?97.
Jeanne S. Chall and Edgar Dale. 1995. Readability
Revisited: The New Dale?Chall Readability Formula.
Brookline Books, Cambridge, MA.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm
Kevyn Collins-Thompson and Jamie Callan. 2004. A
language modeling approach to predicting reading dif-
ficulty. In Proceedings of the HLT / NAACL, pp. 193?
200.
Felice Dell?Orletta. 2009. Ensemble system for Part-of-
Speech tagging. In Proceedings of Evalita?09, Eval-
uation of NLP and Speech Tools for Italian, Reggio
Emilia, December.
Tullio De Mauro. 2000. Il dizionario della lingua ital-
iana. Torino, Paravia.
Lijun Feng, Martin Jansche, Matt Huenerfauth and
Noe?mie Elhadad. 2010. A comparison of features for
automatic readability assessment. In Proceedings of
the 23rd International Conference on Computational
Linguistics (COLING 2010), pp. 276?284.
Lijun Feng, Noe?mie Elhadad and Matt Huenerfauth.
2009. Cognitively motivated features for readability
assessment. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL ?09), pp. 229?237.
V. Franchina and Roberto Vacca. 1986. Adaptation of
Flesh readability index on a bilingual text written by
the same author both in Italian and English languages.
In Linguaggi (3), pp. 47?49.
Lyn Frazier. 1985. Syntactic complexity. In D.R.
Dowty, L. Karttunen and A.M. Zwicky (eds.), Natural
Language Parsing, Cambridge University Press, Cam-
bridge, UK.
Edward Gibson. 1998. Linguistic complexity: Locality
of syntactic dependencies. In Cognition, 68(1), pp. 1-
76.
Michael J. Heilman, Kevyn Collins and Jamie Callan.
2007. Combining Lexical and Grammatical Features
to Improve Readability Measures for First and Second
Language Texts. In Proceedings of the Human Lan-
guage Technology Conference, pp. 460?467.
Michael J. Heilman, Kevyn Collins and Maxine Eske-
nazi. 2008. An analysis of statistical models and fea-
tures for reading difficulty prediction. In Proceedings
of the Third Workshop on Innovative Use of NLP for
Building Educational Applications (EANL ?08), pp.
71?79.
Rohit J. Kate, Xiaoqiang Luo, Siddharth Patwardhan,
Martin Franz, Radu Florian, Raymond J. Mooney,
Salim Roukos, Chris Welty. 2010. Learning to Pre-
dict Readability using Diverse Linguistic Features. In
Proceedings of the 23rd International Conference on
Computational Linguistics (COLING 2010), pp. 546?
554.
J. Peter Kincaid, Lieutenant Robert P. Fishburne, Richard
L. Rogers and Brad S. Chissom. 1975. Derivation
of new readability formulas for Navy enlisted person-
nel. Research Branch Report, Millington, TN: Chief
of Naval Training, pp. 8?75.
George Kingsley Zipf. 1988. The Psychobiology of Lan-
guage. Houghton?Miflin, Boston.
Dekan Lin. 1996. On the structural complexity of nat-
ural language sentences. In Proceedings of COLING
1996, pp. 729?733.
Pietro Lucisano and Maria Emanuela Piemontese. 1988.
Gulpease. Una formula per la predizione della diffi-
colta` dei testi in lingua italiana. In Scuola e Citta` (3),
pp. 57?68.
82
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the Errors of Data-Driven Dependency Parsing
Models. In Proceedings of EMNLP-CoNLL, 2007, pp.
122-131.
Jim Miller and Regina Weinert. 1998. Spontaneous spo-
ken language. Syntax and discourse. Oxford, Claren-
don Press.
Ani Nenkova, Jieun Chae, Annie Louis, and Emily Pitler.
2010. Structural Features for Predicting the Linguistic
Quality of Text Applications to Machine Translation,
Automatic Summarization and Human?Authored Text.
In E. Krahmer, M. Theune (eds.), Empirical Methods
in NLG, LNAI 5790, Springer-Verlag Berlin Heidel-
berg, pp. 222?241.
Sarah E. Petersen and Mari Ostendorf. 2006. A machine
learning approach to reading level assessment. Uni-
versity of Washington CSE Technical Report.
Sarah E. Petersen and Mari Ostendorf. 2009. A ma-
chine learning approach to reading level assessment.
In Computer Speech and Language (23), pp. 89?106.
Maria Emanuela Piemontese. 1996. Capire e farsi
capire. Teorie e tecniche della scrittura controllata
Napoli, Tecnodid.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Processing, pp.
186?195.
Kristine Retherford. 2003. Normal development: a
database of communication and related behaviors.
Eau Claire, WI: Thinking Publications.
Brian Roark, Margaret Mitchell and Kristy Hollingshead.
2007. Syntactic complexity measures for detecting
mild cognitive impairment. In Proceedings of the
Workshop on BioNLP 2007: Biological, Translational,
and Clinical Language Processing, pp. 1?8.
Sarah E. Schwarm and Mari Ostendorf. 2005. Read-
ing level assessment using support vector machines
and statistical language models. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics (ACL 05), pp. 523?530.
Luo Si and Jamie Callan. 2001. A statistical model for
scientific readability. In Proceedings of the tenth in-
ternational conference on Information and knowledge
management, pp. 574?576.
Adam Skory and Maxine Eskenazi. 2010. Predicting
cloze task quality for vocabulary training. In Proceed-
ings of the NAACL HLT 2010 Fifth Workshop on In-
novative Use of NLP for Building Educational Appli-
cations, pp. 49?56.
Victor H.A. Yngve. 1960. A model and an hypothesis for
language structure. In Proceedings of the American
Philosophical Society, pp. 444-466.
Heather Harris Wright, Stacy W. Silverman and Marilyn
Newhoff. 2003. Measures of lexical diversity in apha-
sia. In Aphasiology, 17(5), pp. 443-452.
83
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 207?215,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Linguistic Profiling based on General?purpose Features and Native
Language Identification
Andrea Cimino, Felice Dell?Orletta, Giulia Venturi and Simonetta Montemagni
Istituto di Linguistica Computazionale ?Antonio Zampolli? (ILC?CNR)
ItaliaNLP Lab - www.italianlp.it
via G. Moruzzi, 1 ? Pisa (Italy)
{name.surname}@ilc.cnr.it
Abstract
In this paper, we describe our approach to na-
tive language identification and discuss the re-
sults we submitted as participants to the First
NLI Shared Task. By resorting to a wide set of
general?purpose features qualifying the lexi-
cal and grammatical structure of a text, rather
than to ad hoc features specifically selected
for the NLI task, we achieved encouraging re-
sults, which show that the proposed approach
is general?purpose and portable across differ-
ent tasks, domains and languages.
1 Introduction
Since the seminal work by Koppel et al (2005),
within the Computational Linguistics community
there has been a growing interest in the NLP?based
Native Language Identification (henceforth, NLI)
task. However, so far, due to the unavailability
of balanced and wide?coverage benchmark corpora
and the lack of evaluation standards it has been dif-
ficult to compare the results achieved for this task
with different methods and techniques (Tetreault et
al., 2013). The First Shared Task on Native Lan-
guage Identification (Tetreault et al, 2013) can be
seen as an answer to the above mentioned problems.
In this paper, we describe our approach to na-
tive language identification and discuss the results
we submitted as participants to the First NLI Shared
Task. Following the guidelines by the Shared Task
Organizers based on the previous literature on this
topic, Native Language Identification is tackled as
a text classification task combining NLP?enabled
feature extraction and machine learning: see e.g.
Tetreault et al (2013) and Brooke and Hirst (2012).
Interestingly, the same methodological paradigm is
shared by other tasks like e.g. author recognition and
verification (see e.g. van Halteren (2004), author-
ship attribution (see Juola (2008) for a survey), genre
identification (Mehler et al, 2011) as well as read-
ability assessment (see Dell?Orletta et al (2011a) for
an updated survey), all relying on feature extraction
from automatically parsed texts and state?of?the?art
machine learning algorithms. Besides obvious dif-
ferences at the level of the typology of selected lin-
guistic features and of learning techniques, these dif-
ferent tasks share a common approach to the prob-
lems they tackle: i.e. they succeed in determining
the language variety, the author, the text genre or the
level of readability of a text by exploiting the distri-
bution of different types of linguistic features auto-
matically extracted from texts.
Our approach to NLI relies on multi?level lin-
guistic analysis, covering morpho?syntactic tagging
and dependency parsing. In the NLI literature, the
range of features used is wide and includes char-
acteristics of the linguistic structure underlying the
L2 text, encoded in terms of sequences of charac-
ters, words, grammatical categories or of syntac-
tic constructions, as well as of the document struc-
ture: note however that, in most part of the cases,
the exploited features are task?specific. In our ap-
proach, we decided to resort to a wide set of fea-
tures ranging across different levels of linguistic de-
scription (i.e. lexical, morpho?syntactic and syntac-
tic) without any a priori selection: the same set of
features was successfully exploited in NLI?related
tasks, i.e. focusing on the linguistic form rather than
207
the content of texts, such as readability assessment
(Dell?Orletta et al, 2011a) or the classification of
textual genres (Dell?Orletta et al, 2012).
The exploitation of general features qualifying the
lexical and grammatical structure of a text, rather
than ad hoc features specifically selected for the task
at hand, is not the only peculiarity of our approach
to NLI. Following Biber (1993), we start from the
assumption that ?linguistic features from all levels
function together as underlying dimensions of vari-
ation?. This choice stems from studies on linguis-
tic variation, in particular from Biber and Conrad
(2009) who claim that linguistic varieties ? called
?registers? from a functional perspective ? differ ?in
their characteristic distributions of pervasive linguis-
tic features, not the single occurrence of an indi-
vidual feature?. This is to say that by carrying out
the linguistic analysis of collections of essays each
written by different L1 native speakers, we need to
quantify the extent to which a given feature occurs
in each collection, in order to reconstruct the lin-
guistic profile underlying each L1 collection: dif-
ferences lie at the level of the distribution of linguis-
tic features, which can be common and pervasive in
some L1 collections but comparatively rare in oth-
ers. This approach is the basis of so?called ?linguis-
tic profiling? of texts, within which ?the occurrences
of a large number of linguistic features in a text, ei-
ther individual items or combinations of items, are
counted? (van Halteren, 2004) with the final aim of
reconstructing the profile of a text.
We carried out native language identification in
two steps. The first step consisted of the identifi-
cation of the set of linguistic features characteriz-
ing the essays written by different L1 native speak-
ers, i.e. the linguistic profiling of the different sec-
tions of TOEFL11 corpus (Blanchard et al, 2013)
distributed as training and development data. In
the second step, the features which turned out to
have highly discriminative power were used for the
classification of essays written by different L1 na-
tive speakers. Essay classification has been carried
out by experimenting with different approaches: i.e.
a single?classifier method and two different multi?
model ensemble approaches.
The paper is organised as follows: after introduc-
ing the set of used linguistic features (Section 2),
Section 3 illustrates a selection of the linguistic
profiling results obtained with respect to the train-
ing section of the TOEFL11 corpus; Section 4 de-
scribes the different classification approaches we
followed and the feature selection process; in Sec-
tion 5 achieved results are reported and discussed.
2 Features
In this study, we focused on a wide set of features
ranging across different levels of linguistic descrip-
tion. Differing from previous work on NLI, no a
priori selection of features was carried out. Instead
of focusing on particular classes of errors or on dif-
ferent types of stylistic idiosyncrasies, we took into
account a wide range of features which are typically
used in studies focusing on the ?form? of a text,
e.g. on issues of genre, style, authorship or read-
ability. As previously pointed out, this represents a
peculiarity of our approach. This choice makes the
selected features language?independent, domain?
independent and reusable across different types of
tasks, as empirically demonstrated in Dell?Orletta
et al (2011a) where the same set of features has
been successfully exploited for readability assess-
ment, and in Dell?Orletta et al (2012) where the fea-
tures have been used for the classification of differ-
ent types of textual genre. Note that in both cases the
language dealt with was Italian: for the NLI Shared
Task we had to specialize the feature extraction pro-
cess with respect to the English language as well as
to the annotation scheme used to represent the un-
derlying linguistic structure.
The whole set of features we started with is de-
scribed below, organised into four main categories:
namely, raw text and lexical features as well as
morpho-syntactic and syntactic features. This pro-
posed four?fold partition closely follows the differ-
ent levels of linguistic analysis automatically car-
ried out on the text being evaluated, i.e. tokeniza-
tion, lemmatization, morpho-syntactic tagging and
dependency parsing.
2.1 Raw and Lexical Text Features
Sentence Length, calculated as the average number
of words per sentence.
Word Length, calculated as the average number of
characters per word.
Document Length, calculated as the total number
208
of words per document.
Character bigrams.
Word n-grams, including both unigrams and bi-
grams.
Type/Token Ratio: the Type/Token Ratio (TTR) is
a measure of vocabulary variation which has shown
to be a helpful measure of lexical variety within
a text as well as style marker in an authorship at-
tribution scenario: a text characterized by a low
type/token ratio will contain a great deal of repeti-
tion whereas a high type/token ratio reflects vocabu-
lary richness and variation. Due to its sensitivity to
sample size, TTR has been computed for text sam-
ples of equivalent length (the first 50 tokens).
2.2 Morpho?syntactic Features
Coarse grained Part-Of-Speech n-grams: distri-
bution of unigrams and bigrams of coarse?grained
PoS, corresponding to the main grammatical cate-
gories (e.g. noun, verb, adjective, etc.).
Fine grained Part-Of-Speech n-grams: distribu-
tion of unigrams and bigrams of fine?grained PoS,
which represent subdivisions of the coarse?grained
tags (e.g. the class of nouns is subdivided into proper
vs common nouns, verbs into main verbs, gerund
forms, past particles, etc.).
Verbal chunks: distribution of sequences of verbal
PoS (also including adverbs). This feature can be
seen as a proxy to capture different aspects of verbal
predication, with particular attention to idiosyncratic
usages of verbal mood, tense, person and adverbial
modification.
Lexical density: ratio of content words (verbs,
nouns, adjectives and adverbs) to the total number
of lexical tokens in a text.
2.3 Syntactic Features
Dependency types n-grams: distribution of uni-
grams and bigrams of dependency types calculated
with respect to i) the hierarchical parse tree structure
and ii) the surface linear ordering of words.
Dependency triples: distribution of triplets repre-
senting a dependency relation consisting of a syn-
tactic head (h), the dependency relation type (t) and
the dependent (d). Two different variants of this fea-
ture are distinguished, based on the fact that either
the coarse?grained PoS or the word?form of h and d
is considered: we will refer to the former as Coarse
grained Part-Of-Speech dependency triples and to
the latter as Lexical dependency triples. In both
cases, the relative ordering of h and d, i.e. whether h
precedes or follows d at the level of the linear order-
ing of words within the sentence, is also considered.
Dependency Subtrees: distribution of dependency
subtrees consisting of a dependency relation (repre-
sented as the dependency triple {h, t, d}), the head
father and the dependency relation linking the two.
As in the previous case, two different variants of this
feature are distinguished, based on the fact that ei-
ther the coarse grained PoS or the word?forms of
the nodes in the dependency subtree are considered.
Parse tree depth features: this set of features is
meant to capture different aspects of the parse tree
depth and includes: a) the depth of the whole parse
tree, calculated in terms of the longest path from
the root of the dependency tree to some leaf; b)
the average depth of embedded complement ?chains?
governed by a nominal head and including either
prepositional complements or nominal and adjecti-
val modifiers; c) the probability distribution of em-
bedded complement ?chains? by depth. These fea-
tures represent reliable indicators of sentence com-
plexity, as stated by, among others, Yngve (1960),
Frazier (1985) and Gibson (1998), and they can thus
allow capturing specific difficulties of L2 learners.
Coarse grained Part-Of-Speech of sentence root:
this feature refers to coarse grained POS of the syn-
tactic root of a sentence.
Arity of verbal predicates: this feature refers to
the number of dependencies (corresponding to either
subcategorized arguments or modifiers) governed by
the same verbal head. In the NLI context, it can al-
low capturing improper verbal usage by L2 learners
due to language transfer (e.g. with pro?drop lan-
guages as L1).
Subordination features: this set of features is
meant to capture different aspects of the use of sub-
ordination and includes: a) the distribution of sub-
ordinate vs main clauses; b) the average depth of
?chains? of embedded subordinate clauses and c)
the probability distribution of embedded subordinate
clauses ?chains? by depth. Similarly to parse tree
depth, this set of features can be taken to reflect the
structural complexity of sentences and can thus be
indicative of specific difficulties of L2 learners.
Length of dependency links: measured in terms
209
of the words occurring between the syntactic head
and the dependent. This is another feature which
reflects the syntactic complexity of sentences (Lin,
1996; Gibson, 1998) and which can be successfully
exploited to capture syntactic idiosyncracies of L2
learners due to L1 interferences.
2.4 Other features
Two further features have been considered for NLI
purposes, which were included in the distributed
datasets. For each document, we have also consid-
ered i) the English language proficiency level (high,
medium, or low) based on human assessment by lan-
guage specialists, and ii) the topic of the essays.
3 Linguistic Profiling of TOEFL11 Corpus
In this section, we illustrate the results of linguis-
tic profiling carried out on the training and devel-
opment sets extracted from the TOEFL11 corpus.
This corpus, described in Blanchard et al (2013),
contains 1,100 essays per 11 languages (for a to-
tal of 12,100 essays) sampled as evenly as possi-
ble from 8 prompts (i.e., topics) along with score
levels (low/medium/high) for each essay. The con-
sidered L1s are: Arabic, Chinese, French, German,
Hindi, Italian, Japanese, Korean, Spanish, Telugu,
and Turkish. For the specific purposes of the NLI
Shared Task, a total of 9,900 essays has been dis-
tributed as training data (900 essays per L1), 1,100
as development data (100 per L1) and the remaining
1,100 essays have been used as test data.
We started from the automatic linguistic annota-
tion of training and development data whose output
has been searched for with respect to the features il-
lustrated in Section 2.
3.1 Linguistic Pre?processing
Both training and development data were au-
tomatically morpho-syntactically tagged by the
POS tagger described in Dell?Orletta (2009) and
dependency?parsed by the DeSR parser using
Multi?Layer Perceptron as learning algorithm (At-
tardi et al, 2009), a state?of?the?art linear?time
Shift?Reduce dependency parser. Feature extraction
is carried out against the output of the multi?level
automatic linguistic analysis carried out during the
pre?processing stage: lexical and grammatical pat-
terns corresponding to the wide typology of selected
features are looked for within each annotation layer
and quantified.
3.2 Linguistic Profiling
Generally speaking, linguistic profiling makes it
possible to identify (groups of) texts which are sim-
ilar, at least with respect to the ?profiled? features
(van Halteren, 2004). In what follows we report
the results of linguistic profiling obtained with re-
spect to the 11 L1 sub?corpora considered in this
study. Figure 1 shows the results obtained with re-
spect to a selection of the features described in Sec-
tion 2. These results refer to the combined training
and development data sets: note, however, that we
also calculated the values of these features in the two
datasets separately and it turned out that they do not
vary significantly between the two sets. This fact
can be taken as a proof both of the reliability of our
approach to linguistic profiling and of the relevance
of these features for NLI purposes.
Starting from raw textual features (Figures 1(a)
and 1(b)), both average sentence length and aver-
age word length vary significantly across L1s. In
particular, if on the one hand the essays written by
Arabic and Spanish L1 speakers contain the shortest
words and the longest sentences, on the other hand
the Hindi and Telugu L1 essays are characterized by
the longest words; the L1 Japanese and Korean cor-
pora contain the shortest sentences.
Let us focus now on the distribution of unigrams
of coarse grained Parts?Of?Speech. If we consider
the distributions of determiners and nouns, two fea-
tures typically used for NLI purposes (Wong and
Dras, 2009) which also represent stylistic markers
associated with different linguistic varieties (Biber
and Conrad, 2009), it can be noticed (see Fig-
ures 1(c) and 1(d)) that for Japanese and Korean the
essays show the lowest percentage of determiners,
while for Hindi and Telugu they are characterized
by the highest percentage of nouns.
For what concerns syntactic features, we observe
that essays by Japanese and Korean speakers are
characterized by quite a different distribution with
respect to the other L1 corpora. In particular, they
show the shallowest parse trees, the shortest depen-
dency links as well as the shortest ?chains? of em-
bedded complements governed by a nominal head.
On the other hand, the essays by Spanish and Ara-
210
(a) Average word length (b) Average sentence length
(c) Distribution of Determiners (d) Distribution of Nouns
(e) Average parse tree depth (f) Average depth of embedded complement ?chains?
(g) Average length of the longest dependency link (h) Arity of verbal predicates
Figure 1: Results of linguistic profiling carried out on the combined training and development sections of the TOEFL11
corpus.
211
bic speakers contain the deepest parse trees, for Ital-
ian and Spanish we observe the longest dependency
links and for Hindi and Telugu the longest sequences
of embedded complements. Moreover, while the
essays by Italians are characterised by the highest
value of arity of verbal predicates, for Hindi, Telugu
and Korean essays much lower values are recorded.
Interestingly, these linguistic profiling results
show similar trends across the 11 languages at dif-
ferent levels of linguistic analysis. For instance, it
can be noted that Japanese and Korean or Italian
and Spanish, which belong to two different language
families, show similar distributions of features. Sim-
ilarities have also been recorded in the sub?corpora
by Hindi and Telugu speakers, even if these lan-
guages do not belong to the same family; we can
hypothesize that this might originate from language
contact phenomena.
4 System Description
4.1 Machine Learning Classifier
Our approach to Native Language Identification has
been implemented in a software prototype, i.e. a
classifier operating on mopho?syntactically tagged
and dependency parsed texts which assigns to each
document a score expressing its probability of be-
longing to a given L1 class. The highest score rep-
resents to the most probable class. Given a set of
features and a training corpus, the classifier creates a
statistical model using the feature statistics extracted
from the training corpus. This model is used in the
classification of unseen documents. The set of fea-
tures and the machine learning algorithm can be pa-
rameterized through a configuration file.
For each feature, we have implemented three dif-
ferent variants, depending on whether the feature
value is encoded in terms of: i ) presence/absence
of the feature (binary variant), ii ) the normalized
frequency (normalized frequency variant), and iii )
the normalized tf*idf value (normalized tf*idf vari-
ant). Since the binary feature variant outperformed
the other two, in all the experiments carried out on
the development set reported in Section 5 we illus-
trate the results obtained using this variant only. This
is in line with the results obtained by Brooke and
Hirst (2012) and Tetreault et al (2013). According
to (Brooke and Hirst, 2012), a possible explanation
is that ?in these relatively short texts, there is high
variability in normalized frequencies, and a simpler
metric, by having less variability, is easier for the
classifier to leverage?. Support Vector Machines
(SVM) using LIBSVM (Chang and Lin, 2001) and
Maximum Entropy (ME) using MaxEnt1 have been
used as machine learning algorithms.
We experimented two classification approaches: a
single classifier method and two ensemble systems,
combining the output of several classifiers.
The single classifier uses the set of features re-
sulting from the feature selection process described
in Section 4.2 and the SVM using linear kernel as
machine learning algorithm. This choice is due to
the fact that in all the experiments the linear SVM
outperformed the SVM using polynomial kernel.
There are two possible explanations for this fact,
namely: a) the number of features is much higher
than the number of training instances, accordingly
it might not be necessary to map data to a higher
dimensional space, therefore the nonlinear mapping
does not improve the performance; b) Weston et al
(2000) showed that SVMs can indeed suffer in high
dimensional spaces where many features are irrele-
vant. Note that in Section 5, we report the results of
this classifier using different sets of features corre-
sponding to the lexical, morpho?syntactic and syn-
tactic levels of linguistic analysis.
The two ensemble systems combine the outputs
of the component classifiers following two different
strategies. The first one is based on the majority vot-
ing method (henceforth, VoteComb ): the combina-
tion strategy is seen as a classical voting problem
where for each essay is assigned the L1 class that
has been selected from the majority of classifiers. In
case of ties, the L1 class predicted from the best indi-
vidual model (as resulting from the experiments car-
ried out on the development set) is selected. The sec-
ond strategy combines the outputs of the component
classifiers via another classifier (henceforth referred
to as meta?classifier): we will refer to this second
strategy as ClassComb. The meta?classifier uses as
a feature the probability score predicted from each
component classifier for each L1 class. Differently
from the component classifiers, the meta?classifier
is based on polynomial kernel SVM. In both en-
1https://github.com/lzhang10/maxent#readme
212
semble systems, the component classifiers use linear
SVM and ME as machine learning algorithms and
exploit different sets of features among the ones re-
sulting from the feature selection process described
below.
4.2 Features Selection Process
Since our approach to NLI relies on a wide num-
ber of general?purpose features, a feature selection
process was necessary in order to prune irrelevant
and redundant features which could negatively af-
fect the classification results. The selection process
starts taking into account all the n features described
in Section 2. In each iteration, for each feature fi we
generate a configuration ci such that fi is disabled
and all the other features are enabled. When an it-
eration finishes, we obtain for each ci a correspond-
ing accuracy score score(ci) which is computed as
the average of the accuracy obtained by the classi-
fier on the development set (ad) and on an internal
development set (ai), corresponding to the 10% of
the training set, used in order to reduce the overfit-
ting risk. Being cb the best configuration among all
the ci configurations, if score(cb) ? of the accuracy
scores resulting from the previous iterations the pro-
cess stops. Otherwise:
1. store in F the pair ?fb, disabled? ;
2. for each configuration ci, if score(ci) ? of the
accuracy scores resulting from the previous it-
erations, we store in F the pair ?fi, enabled?;
3. set C = ?cb, score(cb)?
where F is a map containing elements
feature ? {disabled, enabled} and C is a
pair that contains the current best configuration cb
and the corresponding score score(cb). In each
iteration, we consider only the features which do
not occur in F . At the initialization step F is empty
and C contains the configuration where all the
considered features are enabled.
In spite of the fact that the described selection
process does not guarantee to obtain the global opti-
mum, it however permitted us to obtain an improve-
ment of about 8% with respect to the starting model
indiscriminately using all features.
Table 1 lists the features resulting from the fea-
ture selection process. It can be noted that some
Lexical features:
Word n-grams
Morpho?syntactic features:
Coarse grained Part-Of-Speech unigrams
Fine grained Part-Of-Speech bigrams
Syntactic features:
Dependency types unigrams
Lexical dependency triples
Parse tree depth features
Coarse grained Part-Of-Speech of sentence root
Arity of verbal predicates
Subordination features
Length of dependency links
Table 1: Features resulting from the feature selection pro-
cess.
of them coincide with those typically used for NLI
purposes: this is the case of n?grams of words,
Parts-Of-Speech and syntactic dependencies. Inter-
estingly, to our knowledge, other features such as ar-
ity of verbal predicates, length of dependency links
as well as subordination and parse tree depth fea-
tures have not been used for NLI so far, in spite of
their being widely exploited in the syntactic com-
plexity literature (as discussed in Section 2).
5 Results
Table 2 reports the overall Accuracy achieved with
the different classifier models in the NLI classifi-
cation task on the official test set as well as the
F-measure score recorded for each L1 class. The
first two lines show the accuracies of the two com-
bination models, while the last three report the re-
sults obtained by the single classifier using i) the set
of features resulting by the features selection pro-
cess (Best Single), ii) the selected lexical features
only (see Table 1) (Lexical ) and iii) the lexical and
morpho?syntactic features (Lex+Morph ).
The two combination models outperform all
the single model classifiers: note that ClassComb
achieved much better results with respect to Vote-
Comb. By comparing these results with the F-
measure scores obtained on the distributed develop-
ment data (see Table 3), it can be seen that the rank-
ing of the scores achieved by the different classifiers
remains the same even if on the test data we obtained
a performance of -2,2% with respect to the develop-
213
Accuracy ARA CHI FRE GER HIN ITA JAP KOR SPA TEL TUR
ClassComb 77,9 73,8 77,5 83,2 87,3 71,1 86,0 78,8 74,2 70,8 76,2 78,0
VoteComb 77,2 74,3 77,0 80,0 87,0 72,8 81,6 79,6 73,8 67,7 77,6 77,6
Best Single 76,6 71,9 77,6 75,8 85,7 73,2 82,0 80,0 74,0 69,0 76,9 76,5
Lex+Morph 76,4 77,2 76,2 78,6 85,9 72,1 80,4 76,8 71,9 68,0 76,4 76,4
Lexical 76,2 71,1 76,5 79,0 87,6 74,5 80,8 77,7 70,8 66,7 79,2 73,4
Table 2: Classification results of different classifiers on official test data.
ment test set.
Let us consider now the results obtained by the
single model classifiers. In all cases the Best Single
outperforms the other two models demonstrating the
reliability of the features selection process and that
a combination of lexical, morpho?syntactic and syn-
tactic features leads to better results.
Although the best performing model is the Class-
Comb, this is not true for all the 11 languages. In
Table 2, the best results for each L1 are bolded. In-
terestingly, even though Lexical is the worst model,
it is the best performing one for three L1s while the
best model, i.e. ClassComb, for five only.
It can be noted that with respect to the devel-
opment data set the syntactic features used by the
Best Single model allow an increment of +1% as
opposed to the Lexical model: this represents a
much higher increase if compared with the result
obtained on the test data, which is +0,4%. This is
an unexpected result since the feature selection de-
scribed in Section 4.2 was carried out on an internal
development set in order to prevent the risk of over-
fitting on the distributed development data.
Classifier Accuracy
ClassComb 80,1
VoteComb 79,3
Best Single 78,8
Lex+Morph 78,2
Lexical 77,8
Table 3: Classification results of different classifiers on
distributed development data.
6 Conclusion
In this paper, we reported our participation results
to the First Native Language Identification Shared
Task. By resorting to a wide set of general?
purpose features qualifying the lexical and grammat-
ical structure of a text, rather than to ad hoc fea-
tures specifically selected for the task at hand, we
achieved encouraging results. After a feature se-
lection process, new features which to our knowl-
edge have never been exploited so far for NLI pur-
poses turned out to contribute significantly to the
task. Interestingly, the same set of features we
started from has been previously successfully ex-
ploited in other related tasks, such as readability
assessment and genre classification, operating on
the Italian language. The obtained results suggest
that our approach is general?purpose and portable
across different domains and languages. Further di-
rections of research currently include: i) comparison
of results obtained with general purpose features and
with NLI?specific features (e.g. typical errors or dif-
ferent types of stylistic idiosyncrasies specific to L2
learners), with a view to combining them to achieve
better results; ii) design and development of new en-
semble classification methods as well as new fea-
ture selection methods considering not only classes
of features but also individual features; iii) testing
our approach to NLI on different L2s (e.g. Italian) .
References
Giuseppe Attardi, Felice Dell?Orletta, Maria Simi and
Joseph Turian. 2009. Accurate dependency parsing
with a stacked multilayer perceptron. In Proceedings
of EVALITA, Evaluation of NLP and Speech Tools for
Italian, Reggio Emilia, Italy.
Douglas Biber. 1993. Using Register?diversified Cor-
pora for General Language Studies. Computational
Linguistics Journal, 19(2): 219?241.
Douglas Biber and Susan Conrad. 2009. Genre, Register,
Style. Cambridge: CUP.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Educational Testing
Service.
214
Julian Brooke and Graeme Hirst. 2012. Robust, Lexical-
ized Native Language Identification. In Proceedings
of COLING 2012, Mumbai, India, 391?408.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM:
a library for support vector machines. Software avail-
able at http://www.csie.ntu.edu.tw/ cjlin/libsvm
Walter Daelemans. 2012. Explanation in Computational
Stylometry. In A. Gelbukh (ed.) CICLing 2012, Part
II, LNCS 7817, Springer?Verlag, 451?462.
Felice Dell?Orletta. 2009. Ensemble system for Part-of-
Speech tagging. In Proceedings of Evalita?09, Eval-
uation of NLP and Speech Tools for Italian, Reggio
Emilia, December.
Felice Dell?Orletta, Simonetta Montemagni and Giulia
Venturi. 2011a. READ-IT: Assessing Readability of
Italian Texts with a View to Text Simplification. In
Proceedings of the Workshop on ?Speech and Lan-
guage Processing for Assistive Technologies? (SLPAT
2011), Edinburgh, July 30, 73?83.
Felice Dell?Orletta, Simonetta Montemagni and Giulia
Venturi. 2012. Genre?oriented Readability Assess-
ment: a Case Study. In Proceedings of the Workshop
on Speech and Language Processing Tools in Educa-
tion (SLP-TED), 91?98.
Lyn Frazier. 1985. Syntactic complexity. In D.R.
Dowty, L. Karttunen and A.M. Zwicky (eds.), Natural
Language Parsing, Cambridge University Press, Cam-
bridge, UK.
Edward Gibson. 1998. Linguistic complexity: Locality
of syntactic dependencies. In Cognition, 68(1), pp. 1?
76.
Patrick Juola. 2008. Authorship Attribution. Now Pub-
lishers Inc.
Moshe Koppel, Jonathan Schler and Kfir Zigdon. 2005.
Automatically determining an anonymous author?s na-
tive language. In Intelligence and Security Informat-
ics, vol. 3495, LNCS, Springer?Verlag, 209?217.
Dekan Lin. 1996. On the structural complexity of natural
language sentences. In Proceedings of COLING 1996,
pp. 729?733.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the Errors of Data-Driven Dependency Parsing
Models. In Proceedings of EMNLP-CoNLL, 2007,
122?131.
Alexander Mehler, Serge Sharoff and Marina Santini
(Eds.). 2011. Genres on the Web. Computational
Models and Empirical Studies. Springer Series: Text,
Speech and Language Technology.
Sze?Meng Jojo Wong and Mark Dras. 2009. Contrastive
Analysis and Native Language Identification. In Pro-
ceedings of the Australasian Language Technology As-
sociation Workshop.
Hans van Halteren. 2004. Linguistic profiling for author
recognition and verification. In Proceedings of the
Association for Computational Linguistics (ACL04),
200?207.
Joel Tetreault, Daniel Blanchard, Aoife Cahill and Mar-
tin Chodorow. 2012. Native Tongues, Lost and
Found: Resources and Empirical Evaluations in Na-
tive Language Identification. In Proceedings of COL-
ING 2012, Mumbai, India, 2585?2602.
Joel Tetreault, Daniel Blanchard and Aoife Cahill. 2013.
Summary Report on the First Shared Task on Native
Language Identification. In Proceedings of the Eighth
Workshop on Building Educational Applications Us-
ing NL, Atlanta, GA, USA.
Victor H.A. Yngve. 1960. Amodel and an hypothesis for
language structure. In Proceedings of the American
Philosophical Society, 444?466.
Jason Weston, Sayan Mukherjee, Oliver Chapelle, Mas-
similiano Pontil, Tomaso Poggio and Vladimir Nau-
movich Vapnik. 2000. Feature selection for SVMs. In
Advances in Neural Information Processing Systems
13, MIT Press, 668?674.
215
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 45?53,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Unsupervised Linguistically-Driven Reliable Dependency Parses
Detection and Self-Training for Adaptation to the Biomedical Domain
Felice Dell?Orletta, Giulia Venturi, Simonetta Montemagni
Istituto di Linguistica Computazionale ?Antonio Zampolli? (ILC?CNR)
Via G. Moruzzi, 1 ? Pisa (Italy)
{felice.dellorletta,giulia.venturi,simonetta.montemagni}@ilc.cnr.it
Abstract
In this paper, a new self?training method
for domain adaptation is illustrated, where
the selection of reliable parses is car-
ried out by an unsupervised linguistically?
driven algorithm, ULISSE. The method
has been tested on biomedical texts with
results showing a significant improve-
ment with respect to considered baselines,
which demonstrates its ability to capture
both reliability of parses and domain?
specificity of linguistic constructions.
1 Introduction
As firstly demonstrated by (Gildea, 2001), pars-
ing systems have a drop of accuracy when tested
against domain corpora outside of the data from
which they were trained. This is a real prob-
lem in the biomedical domain where, due to the
rapidly expanding body of biomedical literature,
the need for increasingly sophisticated and effi-
cient biomedical text mining systems is becom-
ing more and more pressing. In particular, the ex-
istence of natural language parsers reliably deal-
ing with biomedical texts represents the prerequi-
ste for identifying and extracting knowledge em-
bedded in them. Over the last years, this prob-
lem has been tackled within the biomedical NLP
community from different perspectives. The de-
velopment of a domain?specific annotated corpus,
i.e. the Genia Treebank (Tateisi, Yakushiji, Ohta,
& Tsujii, 2005), played a key role by providing a
sound basis for empirical performance evaluation
as well as training of parsers. On the other hand,
several attempts have been made to adapt general
parsers to the biomedical domain. First experi-
ments in this direction are reported in (Clegg &
Shepherd, 2005) who first compared the perfor-
mance of three different parsers against the Ge-
nia treebank and a sample of the Penn Treebank
(PTB) (Mitchell P. Marcus & Santorini, 1993) in
order to carry out an inter?domain analysis of
the typology of errors made by each parser and
demonstrated that by integrating the output of the
three parsers they achieved statistically significant
performance gains. Three different methods of
parser adaptation for the biomedical domain have
been proposed by (Lease & Charniak, 2005) who,
starting from the results of unknown word rate
experiments carried out on the Genia treebank,
adapted a PTB?trained parser by improving the
Part?Of?Speech tagging accuracy and by relying
on an external domain?specific lexicon. More re-
cently, (McClosky, Charniak, & Johnson, 2010)
and (Plank & van Noord, 2011) devised adaptation
methods based on domain similarity measures. In
particular, both of them adopted lexical similar-
ity measures to automatically select from an anno-
tated collection of texts those training data which
is more relevant, i.e. lexically closer, to adapt the
parser to the target domain.
A variety of semi?supervised approaches,
where unlabeled data is used in addition to labeled
training data, have been recently proposed in the
literature in order to adapt parsing systems to new
domains. Among these approaches, the last few
years have seen a growing interest in self?training
for domain adaptation, i.e. a method for using au-
tomatically annotated data from a target domain
when training supervised models. Self?training
methods proposed so far mainly differ at the level
of the selection of parse trees to be added to the
in?domain gold trees as further training data. De-
pending on whether or not external supervised
classifiers are used to select the parses to be added
to the gold?training set, two types of methods are
envisaged in the literature. The first is the case,
among others, of: (Kawahara & Uchimoto, 2008),
using a machine learning classifier to predict the
reliability of parses on the basis of different fea-
ture types; or (Sagae & Tsujii, 2007), selecting
45
identical analyses for the same sentence within the
output of different parsing models trained on the
same dataset; or (McClosky, Charniak, & John-
son, 2006), using a discriminative reranker against
the output of a n?best generative parser for select-
ing the best parse for each sentence to be used
as further training data. Yet, due to the fact that
several supervised classifiers are resorted to for
improving the base supervised parser, this class
of methods cannot be seen as a genuine istance
of self?training. The second type of methods is
exemplified, among others, by (Reichart & Rap-
poport, 2007) who use the whole set of automat-
ically analyzed sentences, and by (McClosky &
Charniak, 2008) and (Sagae, 2010) who add dif-
ferent amounts of automatically parsed data with-
out any selection strategy. Note that (McClosky
& Charniak, 2008) tested their self?training ap-
proach on the Genia Treebank: they self?trained
a PTB?trained costituency parser using a random
selection of Medline abstracts.
In this paper, we address the second scenario
with a main novelty: we use an unsupervised ap-
proach to select reliable parses from automatically
parsed target domain texts to be combined with the
gold?training set. Two unsupervised algorithms
have been proposed so far in the literature for
selecting reliable parses, namely: PUPA (POS?
based Unsupervised Parse Assessment Algorithm)
(Reichart & Rappoport, 2009) and ULISSE (Un-
supervised LInguiStically?driven Selection of dE-
pendency parses) (Dell?Orletta, Venturi, & Mon-
temagni, 2011). Both algorithms assign a qual-
ity score to each parse tree based on statistics
collected from a large automatically parsed cor-
pus, with a main difference: whereas PUPA oper-
ates on costituency trees and uses statistics about
sequences of part?of?speech tags, ULISSE uses
statistics about linguistic features checked against
dependency?based representations. The self?
training strategy presented in this paper is based
on an augmented version of ULISSE. The reasons
for this choice are twofold: if on the one hand
ULISSE appears to outperform PUPA (namely, a
dependency?based version of PUPA implemented
in (Dell?Orletta et al, 2011)), on the other hand the
linguistically?driven nature of ULISSE makes our
self?training strategy for domain adaptation able
to capture reliable parses which are also represen-
tative of the syntactic peculiarities of the target do-
main.
After introducing the in? and out?domain cor-
pora used in this study (Section 2), we discuss
the results of the multi?level linguistic analysis of
these corpora carried out (Section 3) with a view
to identifying the main features differentiating the
biomedical language from ordinary language. In
Section 4, the algorithm used to select reliable
parses from automatically parsed domain?specific
texts is described. In Section 5 the proposed self?
training method is illustrated, followed by a dis-
cussion of achieved results (Section 6).
2 Corpora
Used domain corpora include i) the two out?
domain datasets used for the ?Domain Adaptation
Track? of the CoNLL 2007 Shared Task (Nivre
et al, 2007) and ii) the dependency?based version
of the Genia Treebank (Tateisi et al, 2005). The
CoNLL 2007 datasets are represented by chemical
(CHEM) and biomedical abstracts (BIO), made of
5,001 tokens (195 sentences) and of 5,017 tokens
(200 sentences) respectively. The dependency?
based version of Genia includes?493k tokens and
?18k sentences which was generated by convert-
ing the PTB version of Genia created by Illes Solt1
using the (Johansson & Nugues, 2007) tool with
the -conll2007 option to produce annotations in
line with the CoNLL 2007 data set2. As unla-
belled data, we used the datasets distributed in the
framework of the CoNLL 2007 Domain Adapta-
tion Track. For CHEM the set of unlabelled data
consists of 10,482,247 tokens (396,128 sentences)
and for BIO of 9,776,890 tokens (375,421 sen-
tences). For the experiments using Genia as test
set, we used the BIO unlabelled data. This was
possible due to the fact that both the Genia Tree-
bank and the BIO dataset consist of biomedical
abstracts extracted (though using different query
terms) from PubMed.com.
As in?domain training data we have used the
CoNLL 2007 dependency?based version of Sec-
tions 2?11 of the Wall Street Journal (WSJ) par-
tition of the Penn Treebank (PTB), for a total of
447,000 tokens and about 18,600 sentences. For
testing, we used the subset of Section 23 of WSJ
consisting of 5,003 tokens (214 sentences).
All corpora have been morpho?syntactically
tagged and lemmatized by a customized version
1http://categorizer.tmit.bme.hu/?illes/genia ptb/
2In order to be fully compliant with the PTB PoS tagset,
we changed the PoS label of all punctuation marks.
46
of the pos?tagger described in (Dell?Orletta, n.d.)
and dependency parsed by the DeSR parser us-
ing Multi?Layer Perceptron (MLP) as learning al-
gorithm (Attardi, Dell?Orletta, Simi, & Turian,
n.d.), a state?of?the?art linear?time Shift?Reduce
dependency parser following a ?stepwise? ap-
proach (Buchholz & Marsi, 2006).
3 Linguistic analysis of biomedical
abstrats vs newspaper articles
For the specific concerns of this study, we carried
out a comparative linguistic analysis of four dif-
ferent corpora, taken as representative of ordinary
language and biomedical language. In each case,
we took into account a gold (i.e. manually an-
notated) corpus, and an unlabelled corpus, which
was automatically annotated. By comparing the
results obtained with respect to gold and automat-
ically annotated texts, we intend to demonstrate
the reliability of features extracted from automat-
ically annotated texts. As data representative of
ordinary language we took i) the whole WSJ sec-
tion of the Penn Treebank3 including 39,285,425
tokens (1,625,606 sentences) and ii) the sections
2?11 of the WSJ. For what concerns the biomed-
ical domain, we relied on the Genia Treebank in
order to guarantee comparability of the results of
our linguistic analysis with previous studies car-
ried out on this reference corpus. As automatically
annotated data we used the corpus of biomedical
abstract (BIO) distributed as out?domain dataset
used for the ?Domain Adaptation Track? of the
CoNLL 2007 Shared Task.
In order to get evidence of the differences hold-
ing between the WSJ newspaper articles and the
selected biomedical abstracts, the four corpora
have been compared with respect to a wide typol-
ogy of features (i.e. raw text, lexical, morpho?
syntactic and syntactic). Let us start from raw
text features, in particular from average sen-
tence length (calculated as the average number
of words per sentence): as Figure 1 shows, both
the corpus of automatically parsed newspaper ar-
ticles (WSJ unlab) and the manually annotated
one (WSJ gold) contain shorter sentences with re-
spect to both the automatically parsed biomedi-
cal abstrats (BIO unlab) and the manually anno-
tated ones (Genia gold), a result which is in line
3This corpus represents to the unlabelled data set dis-
tributed for the CoNLL 2007 Shared Task on Dependency
Parsing, domain adaptation track.
Figure 1: Average sentence length in biomedical
and newspaper corpora.
with (Clegg & Shepherd, 2005) findings. When
we focus on the lexical level, BIO unlab and
Genia gold appear to have quite a similar per-
centage of lexical items which is not contained
in WSJ gold (23.13% and 26.14% respectively)
while the out?of?vocabulary rate of WSJ unlab
is much lower, i.e. 8.69%. Similar results were
recorded by (Lease & Charniak, 2005) who report
the unknown word rate for various genres of tec-
nical literature.
Let us focus now on the morpho?syntactic level.
If we consider the distribution of nouns, verbs
and adjectives, three features typically represent-
ing stylistic markers associated with different lin-
guistic varieties (Biber & Conrad, 2009), it can
be noticed (see Figures 2(a) and 2(c)) that the
biomedical abstracts contain a higher percentage
of nouns and adjectives while showing a signif-
icantly lower percentage of verbs (Figure 2(b)).
The syntactic counterpart of the different distri-
bution of morpho?syntactic categories can be ob-
served in Table 1, reporting the percentage distri-
bution of the first ten Parts?of?Speech dependency
triplets occurring in the biomedical and newspaper
corpora: each triplet is described as the sequence
of the PoS of a dependent and a head linked by a
depedency arc, by also considering the PoS of the
head father. It turned out that biomedical abstracts
are characterized by nominal dependency triplets,
e.g. two nouns and a preposition (NN?NN?IN)
or noun, preposition, noun (NN?IN?NN) or ad-
jective, noun and preposition (JJ?NN?IN), which
occur more frequently than verbal triplets, such
as determiner, noun and verb (DT?NN?VBZ) or a
verbal root (.?VBD?ROOT)4. Interestingly, in Ge-
nia gold no verbal triplet occurs within the top ten
triplets, which cover the 21% of the total amount
4We named ?ROOT? the artificial root node.
47
(a) Distribution of Nouns (b) Distribution of Verbs (c) Distribution of Adjectives
Figure 2: Distribution of some Parts?of?Speech in biomedical and newspaper corpora.
of triplets occurring in the corpus. By contrast,
the same top ten triplets represent only ?11% in
WSJ gold, testifying the wider variety of syntac-
tic constructions occurring in newspaper articles
with respect to texts of the biomedical domain.
This is also proved by the total amount of differ-
ent PoS dependency triplets occurring in the two
gold datasets, i.e. 7,827 in WSJ gold and 5,064
in Genia gold even though the Genia Treebank is
?50,000?tokens bigger.
Further differences can be observed at a deeper
syntactic level of analysis. This is the case of the
average depth of embedded complement ?chains?
governed by a nominal head. Figure 3(a) shows
that biomedical abstracts are characterized by an
average depth which is higher than the one ob-
served in newspaper articles. A similar trend
can be observed for what concerns the distribu-
tion of ?chains? by depth. In Figure 3(b) shows
that WSJ unlab and WSJ gold ?chains?, on the one
hand, and BIO unlab and Genia gold ?chains?,
on the other hand, overlap. The corpora have
also been compared with respect to i) the av-
erage length of dependency links, measured in
terms of the words occurring between the syntac-
tic head and the dependent (excluding punctua-
tion marks), and ii) the average depth of the whole
parse tree, calculated in terms of the longest path
from the root of the dependency tree to a leaf. In
both cases it can be noted that i) the biomedical
abstracts contain much longer dependency links
than newswire texts (Figure 3(c)) and ii) the av-
erage depth of BIO unlab and Genia gold parse
trees is higher than in the case of the WSJ unlab
and WSJ gold (Figure 3(d)). A further distin-
guishing feature of the biomedical abstracts con-
cerns the average depth of ?chains? of embed-
ded subordinate clauses, calculated here by taking
into account both clausal arguments and comple-
ments linked to a verbal sentence root. As Figure
3(e) shows, both BIO unlab and Genia gold have
shorter ?chains? with respect to the ones contained
in the newspaper articles. Interestingly, a careful
analysis of the distributions by depth of ?chains?
of embedded subordinate clauses shows that the
biomedical abstracts appear to have i) a higher oc-
currence of ?chains? including just one subordinate
clause and ii) a lower percentage of deep ?chains?
with respect to newswire texts. Finally, we com-
pared the two types of corpora with respect to the
distribution of verbal roots. The biomedical ab-
stracts resulted to be characterised by a lower per-
centage of verbal roots with respect to newspaper
articles (see Figure 3(f)). This is in line with the
distribution of verbs as well as of the nominal de-
pendency triplets observed in the biomedical ab-
stracts at the morpho?syntactic level of analysis.
Interestingly, the results obtained with respect
to automatically parsed and manually annotated
data show similar trends for both considered in?
and out?domain corpora, thus demonstrating the
reliability of features monitored against automat-
ically annotated data. In what follows, we will
show how detected linguistic peculiarities can be
exploited in a domain adaptation scenario.
4 Linguistically?driven Unsupervised
Ranking of Parses for Self?training
In the self?training approach illustrated in this pa-
per, the selection of parses from the automatically
annotated target domain dataset is guided by an
augmented version of ULISSE, an unsupervised
linguistically?driven algorithm to select reliable
parses from the output of dependency annotated
texts (Dell?Orletta et al, 2011) which has shown
a good performance for two different languages
48
WSJ gold WSJ unlab Genia gold BIO unlab
Triplet % Freq Triplet % Freq Triplet % Freq Triplet % Freq
DT-NN-IN 2.03 DT-NN-IN 1.72 NN-NN-IN 3.66 DT-NN-IN 2.87
.-VBD-ROOT 1.61 .-VBD-ROOT 1.30 NN-IN-NN 2.93 NN-IN-NN 2.39
NN-IN-NN 1.11 JJ-NN-IN 0.99 DT-NN-IN 2.48 JJ-NN-IN 2.08
JJ-NN-IN 1.10 NN-IN-NN 0.97 JJ-NN-IN 1.96 NN-NN-IN 1.73
.-VBZ-ROOT 1.09 NNP-NNP-IN 0.87 NN-NNS-IN 1.88 IN-NN-IN 1.72
NNP-NNP-IN 0.95 DT-NN-VBD 0.85 JJ-NNS-IN 1.77 JJ-NNS-IN 1.36
DT-NN-VBZ 0.89 NN-VBD-ROOT 0.80 IN-NN-IN 1.65 .-VBD-ROOT 1.33
DT-NN-VBD 0.87 JJ-NNS-IN 0.79 NN-CC-IN 1.64 NNS-IN-NN 1.13
JJ-NNS-IN 0.87 NNP-NNP-VBD 0.78 NNS-IN-NN 1.56 NNP-NN-IN 1.03
IN-NN-IN 0.87 .-VBZ-ROOT 0.75 NN-NN-CC 1.47 NN-IN-VBN 0.93
Table 1: Frequency distribution of the first ten Parts?of?Speech dependency triplets in biomedical and
newspaper corpora.
(a) Depth of embedded complement
?chains?
(b) Distribution of embedded com-
plement ?chains? by depth
(c) Length of dependency links
(d) Parse tree depth (e) Depth of embedded subordinate
clauses ?chains?
(f) Distribution of verbal roots
Figure 3: Syntactic features in biomedical and newspaper corpora.
(English and Italian) against the output of two su-
pervised parsers (MST, (McDonald, Lerman, &
Pereira, 2006) and DeSR, (Attardi, 2006)) se-
lected for their behavioral differences (McDonald
& Nivre, 2007). ULISSE assigns to each depen-
dency tree a score quantifying its reliability based
on a wide range of linguistic features. After col-
lecting statistics about selected features from a
corpus of automatically parsed sentences, for each
newly parsed sentence ULISSE computes a relia-
bility score using the previously extracted feature
statistics. In its reliability assessment, ULISSE ex-
ploits both global and local features, where global
features (listed in Table 2 and discussed in Sec-
tion 3) are computed with respect to each sen-
tence and averaged over all sentences in the cor-
pus, and the local features with respect to indi-
vidual dependency links and averaged over all of
them. Local features include the plausibility of
a dependency link calculated by considering se-
lected features of the dependent and its govern-
ing head as well as of the head father: whereas
in ULISSE the selected features were circum-
scribed to part?of?speech information (so?called
?ArcPOSFeat? feature), in this version of the al-
gorithm a new local feature has been introduced,
named ?ArcLemmaFeat?, which exploits lemma
information. ?ArcPOSFeat? is able to capture the
different distribution of PoS dependency triplets
(see Table 1), along with the type of dependency
link, while the newly introduced ?ArcLemmaFeat?
is meant to capture the lexical peculiarities of the
target domain (see Section 3). As demonstrated
in (Dell?Orletta et al, 2011), both global and lo-
49
cal linguistic features contribute to the selection
of reliable parses. Due to the typology of linguis-
tic features underlying ULISSE, selected reliable
parses typically include domain?specific construc-
tions. This peculiarity of the ULISSE algorithm
turned out to be particularly useful to maximize
the self?training effect in improving the parsing
performance in a domain adaptation scenario.
The reliability score assigned by this augmented
version of ULISSE to newly parsed sentences re-
sults from a combination of the weights associ-
ated with individual features, both global and local
ones. In this study, the reliability score was com-
puted as a simple product of the individual feature
weights: in this way, one low weight feature is suf-
ficient to qualify a parse as low quality and thus to
exclude it from the self?training dataset5.
Feature
Parse tree depth
Embedded complement ?chains? headed by a noun
- Average depth
- Distribution by depth
Verbal roots
Arity of verbal predicates
- Distribution by arity
Subordinate vs main clauses
- Relative ordering of subordinate clauses with respect to the main clause
- Average depth of ?chains? of embedded subordinate clauses
- Distribution of embedded subordinate clauses ?chains? by depth
Length of dependency links
Table 2: Global features underlying ULISSE.
5 Experimental set?up
In the reported experiments, we used the DeSR
parser. Its performance using the proposed domain
adaptation strategy was tested against i) the two
out?domain datasets distributed for the ?Domain
Adaptation Track? of the CoNLL 2007 Shared
Task and ii) the dependency?based version of the
Genia Treebank, described in Section 2. For test-
ing purposes, we selected from the dependency?
based version of the Genia Treebank sentences
with a maximum length of 39 tokens (for a total
of 375,912 tokens and 15,623 sentences).
Results achieved with respect to the CHEM and
BIO test sets were evaluated in terms of ?Labelled
Attachment Score? (LAS), whereas for Genia the
only possible evaluation was in terms of ?Un-
labelled Attachment Score? (UAS). This follows
from the fact that, as reported by Illes, this ver-
sion of Genia is annotated with a Penn Treebank?
style phrase?structure, where a number of func-
tional tags are missing: this influences the type
5See (Dell?Orletta et al, 2011) for a detailed description
of the quality score computation.
Test corpus LAS UAS
PTB 86.09% 87.29%
CHEM 78.50% 81.10%
BIO 78.65% 79.97%
GENIA n/a 80.25%
Table 3: The BASE model tested on PTB, CHEM,
BIO and GENIA.
of evaluation which can be carried out against the
Genia test set.
Achieved results were compared with two base-
lines, represented by: i) the Baseline model
(BASE), i.e. the parsing model trained on the PTB
training set only; ii) the Random Selection (RS) of
parses from automatically parsed out?domain cor-
pora, calculated as the mean of a 10?fold cross?
validation process. As proved by (Sagae, 2010)
and by (McClosky & Charniak, 2008) for the
biomedical domain, the latter represents a strong
unsupervised baseline showing a significant accu-
racy improvement which was obtained by adding
incremental amounts of automatically parsed out?
domain data to the training dataset without any se-
lection strategy.
The experiments we carried out to test the ef-
fectiveness of our self?training strategy were or-
ganised as follows. ULISSE and the baseline al-
gorithms were used to produce different rankings
of parses of the unlabelled target domain corpora.
From the top of these rankings different pools of
parses were selected to be used for training. In
particular, two different sets of experiments were
carried out, namely: i) using only automatically
parsed data as training corpus and ii) combining
automatically parsed data with the PTB training
set. For each set of experiments, different amounts
of unlabelled data were used to create the self?
training models.
6 Results
Table 3 reports the results of the BASE model
tested on PTB, CHEM, BIO and GENIA. When
applied without adaptation to the out?domain
CHEM, BIO and GENIA test sets, the BASE pars-
ing model has a drop of about 7.5% of LAS in both
CHEM and BIO cases. For what concerns UAS,
the drop is about 6% for CHEM and about 7% for
BIO and GENIA.
The results of the performed experiments are
shown in Figures 4 and 5, where each plot re-
ports the accuracy scores (LAS and UAS respec-
tively) of the self?trained parser using the ULISSE
50
(a) LAS for CHEM without PTB training set. (b) LAS for BIO without the PTB training set.
(c) LAS for CHEM with the PTB training set. (d) LAS for BIO with the PTB training set.
Figure 4: LAS of the different self?training models in the two sets of performed experiments.
(a) UAS for GENIA without the PTB training set. (b) UAS for GENIA with the PTB training set.
Figure 5: UAS of the different self?training models for GENIA.
algorithm (henceforth, ULISSE?Stp) and of the
baseline models (RS and BASE). The parser ac-
curacy was computed with respect to different
amounts of automatically parsed data which were
used to create the self?trained parsing model. For
this purpose, we considered six different pools of
250k, 450k, 900k, 1000k, 1350k and 1500k to-
kens. Plots are organized by experiment type: i.e.
the results in subfigures 4(a), 4(b) and 5(a) are
achieved by using only automatically parsed data
as training corpus, whereas those reported in the
other subfigures refer to models trained on auto-
matically parsed data combined with PTB. Note
that in all figures the line named best?RS repre-
sents the best RS score for each pool of k tokens in
the 10?fold cross?validation process.
For what concerns BIO and CHEM, in the first
set of experiments ULISSE?Stp turned out to be
the best self?training algorithm: this is always the
case for CHEM (see subfigure 4(a)), whereas for
BIO (see subfigure 4(b)) it outperforms all base-
lines only when pools of tokens >= 900k are
added. When 900k tokens are used, ULISSE?Stp
allows a LAS improvement of 0.81% on CHEM
and of 0.61% on BIO with respect to RS, and
of 0.62% on CHEM and of 0.48% on BIO with
respect to BASE. It is interesting to note that
ULISSE?Stp using only automatically parsed data
for training achieves better results than BASE (us-
ing the PTB gold training set): to our knowledge,
a similar result has never been reported in the lit-
erature. The behaviour is similar also when the
51
experiments are evaluated in terms of UAS6.
The results achieved in the first set of experi-
ments carried out on the GENIA test set (see 5(a))
differ significantly from what we observed for
CHEM and BIO: in this case, the BASE model ap-
pears to outperform all the other algorithms, with
the ULISSE?Stp algorithm being however more
effective than the RS baselines.
Figures 4(c), 4(d) and 5(b) report the results of
the second set of experiments, i.e. those carried
out by also including PTB in the training set. Note
that in these plots the RS+PTB lines represent the
score of the parser models trained on the pools
of tokens used to obtain the best?RS line com-
bined with the PTB gold training set. It can be
observed that the ULISSE?Stp+PTB self?training
model outperforms all baselines for CHEM, BIO
and GENIA for all the different sizes of pools of
selected tokens. For each pool of parsed data, Ta-
ble 4 records the improvement and the error reduc-
tion observed with respect to the BASE model.
Pool of tokens CHEM Err. red. BIO Err. red. GENIA Err. red.
250k 0.8 3.72 0.76 3.55 0.97 4.91
450k 1.1 5.12 0.54 2.53 1.52 7.7
900k 1.14 5.3 1.02 4.77 1.31 6.63
1000k 0.8 3.72 1.56 7.29 1.2 6.08
1350k 0.4 1.49 1.46 6.82 0.94 4.76
1500k 0.78 3.62 0.75 3.37 0.66 3.34
Table 4: % improvement of ULISSE?Stp+PTB vs
BASE reported in terms of LAS for CHEM and
BIO and of UAS for GENIA.
Differently from (Sagae, 2010) (with a
constituency?based parser), in this set of experi-
ments the self?training approach based on random
selection of sentences (i.e. the best?RS+PTB
baseline) doesn?t achieve any improvement with
respect to the BASE model with only minor
exceptions (observed e.g. with 250k and 450k
pools of added tokens for CHEM and with 250k
for GENIA). Moreover, even when the best?RS
LAS is higher than the ULISSE?Stp score (e.g.
in the first pools of k of Figure 4(b)), ULISSE?
Stp+PTB turns out to be more effective than
the best?RS+PTB baseline (Figure 4(d)). These
results may follow from the fact that ULISSE?Stp
is able to capture not only reliable parses but also,
and more significantly here, parses which reflect
the syntactic peculiarities of the target domain.
Table 5 shows the results of the different
ULISSE?Stp+PTB models tested on the PTB test
6In this paper, for CHEM and BIO experiments we report
only the LAS scores since this is the standard evaluation met-
ric for dependency parsing.
set: no LAS improvement is observed with respect
to the results obtained with the BASE model, i.e.
86.09% (see Table 3). This result is in line with
(McClosky et al, 2010) and (Plank & van No-
ord, 2011) who proved that parsers trained on the
union of gold corpora belonging to different do-
mains achieve a lower accuracy with respect to the
same parsers trained on data belonging to a sin-
gle target domain. Last but not least, it should be
noted that the performance of ULISSE?Stp across
the experiments carried out with pools of automat-
ically parsed tokens of different sizes is in line
with the behaviour of the ULISSE ranking algo-
rithm (Dell?Orletta et al, 2011), where increas-
ingly wider top lists of parsed tokens show de-
creasing LAS scores. This helps explaining why
the performance of ULISSE?Stp starts decreasing
after a certain threshold when wider top?lists of
tokens are added to the parser training data.
Pool of tokens CHEM BIO
250k 83.53 85.55
450k 85.53 86.01
900k 85.95 84.79
1000k 86.03 85.45
1350k 85.49 85.71
1500k 85.67 86.39
Table 5: ULISSE?Stp+PTB on PTB test set with
automatically parsed data.
Conclusion
In this paper we explored a new self?training
method for domain adaptation where the selec-
tion of reliable parses within automatically an-
notated texts is carried out by an unsupervised
linguistically?driven algorithm, ULISSE. Results
achieved for the CoNLL 2007 datasets as well
as for the larger test set represented by GENIA
show a significant improvement with respect to
considered baselines. This demonstrates a two?
fold property of ULISSE, namely its reliablity
and effectiveness both in capturing peculiarities
of biomedical texts, and in selecting high quality
parses. Thanks to these properties the proposed
self?training method is able to improve the parser
performances when tested in an out?domain sce-
nario. The same approach could in principle be
applied to deal with biomedical sub?domain varia-
tion: as reported by (Lippincott, Se?aghdha, & Ko-
rhonen, 2011), biomedical texts belonging to dif-
ferent sub?domains do vary along many linguistic
dimensions, with a potential negative impact on
biomedical NLP tools.
52
References
Attardi, G. (2006). Experiments with a multi-
language non-projective dependency parser.
In Proceedings of CoNLL-X ?06 (pp. 166?
170). New York City, New York.
Attardi, G., Dell?Orletta, F., Simi, M., & Turian, J.
(n.d.). Accurate dependency parsing with a
stacked multilayer perceptron. In Proceed-
ings of EVALITA 2009.
Biber, D., & Conrad, S. (2009). Genre, regis-
ter, style. Cambridge: Cambridge Univer-
sity Press.
Buchholz, S., & Marsi, E. (2006). CoNLL-
X shared task on multilingual dependency
parsing. In Proceedings of CoNLL 2006.
Clegg, A. B., & Shepherd, A. J. (2005). Evalu-
ating and integrating treebank parsers on a
biomedical corpus. In In proceedings of the
ACL 2005 workshop on software.
Dell?Orletta, F. (n.d.). Ensemble system for
part-of-speech tagging. In Proceedings of
EVALITA 2009.
Dell?Orletta, F., Venturi, G., & Montemagni, S.
(2011). Ulisse: an unsupervised algorithm
for detecting reliable dependency parses. In
Proceedings of CoNLL 2011 (pp. 115?124).
Portland, Oregon.
Gildea, D. (2001). Corpus variation and parser
performance. In Proceedings of EMNLP
2001 (p. 167-202). Pittsburgh, PA.
Johansson, R., & Nugues, P. (2007). Ex-
tended constituent-to-dependency conver-
sion for english. In Proceedings of NODAL-
IDA 2007. Tartu, Estonia.
Kawahara, D., & Uchimoto, K. (2008). Learning
reliability of parses for domain adaptation
of dependency parsing. In Proceedings of
IJCNLP 2008 (pp. 709?714).
Lease, M., & Charniak, E. (2005). Parsing
biomedical literature. In Proceedings of
the second international joint conference on
natural language processing (IJCNLP-05),
Jeju Island, Korea (pp. 58?69).
Lippincott, T., Se?aghdha, D. O?., & Korhonen, A.
(2011). Exploring subdomain variation in
biomedical language. BMC Bioinformatics,
12, 212?233.
McClosky, D., & Charniak, E. (2008). Self-
training for biomedical parsing. In Proceed-
ings of ACL?HLT 2008 (pp. 101?104).
McClosky, D., Charniak, E., & Johnson, M.
(2006). Reranking and self-training for
parser adaptation. In Proceedings of ACL
2006 (pp. 337?344). Sydney, Australia.
McClosky, D., Charniak, E., & Johnson, M.
(2010). Automatic domain adaptation for
parsing. In Proceedings of NAACL?HLT
2010 (pp. 28?36). Los Angeles, California.
McDonald, R., Lerman, K., & Pereira, F. (2006).
Multilingual dependency analysis with a
two-stage discriminative parser. In Proceed-
ings of CoNLL 2006.
McDonald, R., & Nivre, J. (2007). Character-
izing the errors of data-driven dependency
parsing models. In Proceedings of EMNLP-
CoNLL 2007 (p. 122-131).
Mitchell P. Marcus, M. A. M., & Santorini, B.
(1993). Building a large annotated corpus
of english: the penn treebank. Comput. Lin-
guist., 19(2), 313?330.
Nivre, J., Hall, J., Ku?bler, S., McDonald, R., Nils-
son, J., Riedel, S., & Yuret, D. (2007).
The conll 2007 shared task on dependency
parsing. In Proceedings of EMNLP-CoNLL
2007 (pp. 915?932).
Plank, B., & van Noord, G. (2011). Effective mea-
sures of domain similarity for parsing. In
Proceedings of ACL 2011 (pp. 1566?1576).
Portland, Oregon.
Reichart, R., & Rappoport, A. (2007). Self?
training for enhancement and domain adap-
tation of statistical parsers trained on small
datasets. In Proceedings of ACL 2007 (pp.
616?623).
Reichart, R., & Rappoport, A. (2009). Automatic
selection of high quality parses created by a
fully unsupervised parser. In Proceedings of
CoNLL 2009 (pp. 156?164).
Sagae, K. (2010). Self-training without reranking
for parser domain adaptation and its impact
on semantic role labeling. In Proceedings
of the 2010 workshop on domain adaptation
for natural language processing (DANLP
2010) (pp. 37?44). Uppsala, Sweden.
Sagae, K., & Tsujii, J. (2007). Dependency pars-
ing and domain adaptation with lr models
and parser ensemble. In Proceedings of
EMNLP?CoNLL 2007 (pp. 1044?1050).
Tateisi, Y., Yakushiji, A., Ohta, T., & Tsujii, J.
(2005). Syntax annotation for the GENIA
corpus. In Proceedings of IJCNLP?05 (pp.
222?227). Jeju Island, Korea.
53
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 61?69,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Converting Italian Treebanks:
Towards an Italian Stanford Dependency Treebank
Cristina Bosco
Dipartimento di Informatica
Universita` di Torino
cristina.bosco@unito.it
Simonetta Montemagni
Istituto di Linguistica
Computazionale
?Antonio Zampolli?
(ILC?CNR)
simonetta.montemagni@ilc.cnr.it
Maria Simi
Dipartimento di Informatica
Universita` di Pisa
simi@unipi.it
Abstract
The paper addresses the challenge of con-
verting MIDT, an existing dependency?
based Italian treebank resulting from the
harmonization and merging of smaller re-
sources, into the Stanford Dependencies
annotation formalism, with the final aim
of constructing a standard?compliant re-
source for the Italian language. Achieved
results include a methodology for con-
verting treebank annotations belonging
to the same dependency?based family,
the Italian Stanford Dependency Treebank
(ISDT), and an Italian localization of the
Stanford Dependency scheme.
1 Introduction
The limited availability of training resources is
a widely acknowledged bottleneck for machine
learning approaches for Natural Language Pro-
cessing (NLP). This is also the case of dependency
treebanks within statistical dependency parsing.
Moreover, the availability of a treebank in a stan-
dard format strongly improves its usefulness, in-
creasing the number of tasks for which it can be
exploited and allowing the application of a larger
variety of tools. It also has an impact on the relia-
bility of achieved results, and, last but not least, it
permits comparability with other resources.
This motivated a variety of initiatives devoted
to the definition of standards for the linguistic an-
notation of corpora. Since the early 1990s, dif-
ferent initiatives have been devoted to the defi-
nition of standards for the linguistic annotation
of corpora with a specific view to re?using and
merging existing treebanks. The starting point
is represented by the EAGLES (Expert Advisory
Groups on Language Engineering Standards) ini-
tiative, which ended up with providing provisional
standard guidelines (Leech et al, 1996), operat-
ing at the level of both content (i.e. the linguistic
categories) and encoding format. More recent ini-
tiatives, e.g. LAF/GrAF (Ide and Romary, 2006;
Ide and Suderman, 2007) and SynAF (Declerck,
2008) representing on?going ISO TC37/SC4 stan-
dardization activities1, rather focused on the def-
inition of a pivot format capable of representing
diverse annotation types of varying complexity
without providing specifications for the annotation
of content categories (i.e., the labels describing the
associated linguistic phenomena), for which stan-
dardization appeared since the beginning to be a
much trickier matter. Recently, other standard-
ization efforts such as ISOCat (Kemps-Snijders et
al., 2009) tackled this latter issue by providing a
set of data categories at various levels of granu-
larity, each accompanied by a precise definition of
its linguistic meaning. Unfortunately, the set of
dependency categories within ISOCat is still basic
and restricted. We can thus conclude that as far as
content categories are concerned de jure standards
are not suitable at the moment for being used in
the harmonization and merging of real dependency
treebanks.
The alternative to de jure standards is repre-
sented by de facto standards. For what concerns
dependency?based annotation, which in the recent
past has been increasingly exploited for a wide
range of NLP?based information extraction tasks,
the Stanford Dependency (SD) scheme (de Marn-
effe et al, 2006) is gaining popularity as a de
facto standard. Among the contexts where SD has
been applied, we can observe e.g. parsers and
corpora exploited in biomedical information ex-
traction, where it has been suggested to be a suit-
able unifying syntax formalism for several incom-
patible syntactic annotation schemes (Pyysalo et
al., 2007). SD has already been applied to differ-
ent languages, e.g. Finnish in the Turku treebank
(Haverinen et al, 2010), Swedish in the Talbanken
1
http://www.tc37sc4.org/
61
treebank2, Chinese in the Classical Chinese Liter-
ature treebank (Seraji et al, 2012) or Persian in the
Uppsala Persian Dependency Treebank (Lee and
Kong, 2012).
In this paper, we describe the conversion of
an existing Italian resource into the SD annota-
tion scheme, with the final aim of developing a
standard?compliant treebank, the Italian Stanford
Dependency Treebank (ISDT). The reference re-
source, called Merged Italian Dependency Tree-
bank (MIDT)3 (Bosco et al, 2012), is the re-
sult of a previous effort in the direction of im-
proving interoperability of data sets available for
Italian by harmonizing and merging two exist-
ing dependency?based resources, i.e. TUT and
ISST?TANL, adopting incompatible annotation
schemes. The two conversion steps are visual-
ized in Figure 1: note that in both of them the
focus is on the conversion and merging of the con-
tent of linguistic annotation; for what concerns the
representation format, all involved treebanks fol-
low the CoNLL tab?separated format (Buchholz
and Marsi, 2006) which nowadays represents a de
facto standard within the international dependency
parsing community. In this paper, we deal with the
second step, focusing on the MIDT to ISDT con-
version.
Starting from a comparative analysis of the
MIDT and SD annotation schemes, we developed
a methodology for converting treebank annota-
tions belonging to the same dependency?based
family based on:
? a comparative analysis of the source and tar-
get annotation schemes, carried out with re-
spect to different dimensions of variation,
ranging from head selection criteria, depen-
dency tagset granularity to defined annotation
criteria;
? the analysis of the performance of a state?of?
the?art dependency parser by using as train-
ing the source and the target treebanks;
? the mapping of the MIDT annotation scheme
onto the SD data categories.
2
http://stp.lingfil.uu.se/?nivre/swedish treebank/
talbanken-stanford-1.2.tar.gz
3MIDT was developed within the project PARLI
(http://parli.di.unito.it/project en.html) partially
funded in 2008-2012 by the Italian Ministry for Univer-
sity and Research, for fostering the development of new
resources and tools that can operate together, and the
harmonization of existing ones. MIDT is documented at
http://medialab.di.unipi.it/wiki/MIDT/.
Figure 1: Merging and conversion process from
TUT and ISST?TANL to MIDT and ISDT.
In this conversion process, we had to deal
with the peculiarities of the Italian language: the
tackled issues range from morphological richness,
presence of clitic pronouns to relatively free word
order and pro?drop, all properties requiring spe-
cific annotation strategies to be dealt with. There-
fore, a by product of this conversion process is rep-
resented by the specialization of the SD annotation
scheme with respect to Italian.
In the following sections, after briefly describ-
ing the methodology applied for the development
of the MIDT resource (Section 2), we focus on a
comparative analysis of the MIDT and SD anno-
tation schemes (Section 3) followed by a descrip-
tion of the implemented conversion process (Sec-
tion 4). Finally, we present the results obtained by
training a parsing system on the newly developed
resource (Section 5).
2 The starting point: MIDT
ISDT originates from the conversion towards the
SD standard of the MIDT resource, whose origins
and development are summarised below (for more
details on this harmonization and merging step the
interested reader is referred to Bosco et al (2012)).
2.1 The ancestors: TUT and ISST?TANL
The TUT and ISST?TANL resources differ under
different respects, at the level of both corpus com-
position and adopted annotation schemes.
For what concerns size and composition, TUT
(Bosco et al, 2000)4 currently includes 3,452 Ital-
ian sentences (i.e. 102,150 tokens in TUT native,
4
http://www.di.unito.it/?tutreeb/
62
and 93,987 in CoNLL) and represents five dif-
ferent text genres (newspapers, Italian Civil Law
Code, JRC-Acquis Corpus5, Wikipedia and the
Costituzione Italiana), while ISST?TANL includes
3,109 sentences (71,285 tokens in CoNLL for-
mat), which were extracted from the ?balanced?
ISST partition (Montemagni et al, 2003) exem-
plifying general language usage as testified in arti-
cles from newspapers and periodicals, selected to
cover a high variety of topics (politics, economy,
culture, science, health, sport, leisure, etc.).
As far as the annotation scheme is concerned,
TUT applies the major principles of the Word
Grammar theoretical framework (Hudson, 1984)
using a rich set of dependency relations, but it in-
cludes null elements to deal with non?projective
structures, long distance dependencies, equi phe-
nomena, pro?drop and elliptical structures6. The
ISST?TANL annotation scheme originates from
FAME (Lenci et al, 2008), an annotation scheme
which was developed starting from de facto stan-
dards and which was specifically conceived for
complying with the basic requirements of parsing
evaluation, and ? later ? for the annotation of un-
restricted Italian texts.
2.2 Creating the merged MIDT resource
The challenge we tackled in the development of
MIDT was to translate between different annota-
tion schemes and merging them. We focused on
the harmonization and merging of content cate-
gories. To this specific end, we defined a set of
linguistic categories to be used as a ?bridge? be-
tween the specific TUT and ISST?TANL schemes.
First of all, we analyzed similarities and dif-
ferences of the underlying schemes, which led to
identify a core of syntactic constructions for which
the annotations agreed, but also to highlight vari-
ations in head selection criteria, inventory of de-
pendency types and their linguistic interpretation,
projectivity constraint and analysis of specific syn-
tactic constructions. For instance, TUT always
assigns heads on the basis of syntactic criteria,
i.e. the head role is played by the function word
in all constructions where one function word and
one content word are involved (e.g. determiner?
noun, verb?auxiliary), while in ISST?TANL head
selection follows from a combination of syntactic
5
http://langtech.jrc.it/JRC-Acquis.html
6The CoNLL format does not include null elements, but
the projectivity constraint is maintained at the cost of a loss
of information with respect to native TUT in some cases.
and semantic criteria (e.g. in determiner?noun and
auxiliary?verb relations the head role is played by
the content word). Both schemes assume differ-
ent inventories of dependency types and degrees
of granularity in the representation of specific re-
lations. Moreover, whereas ISST?TANL allows
for non?projective representations, TUT assumes
the projectivity constraint. Further differences are
concerned with the treatment of coordination and
punctuation, which are particularly problematic to
deal with in the dependency framework.
As a second step, we defined a bridge anno-
tation, i.e. the MIDT dependency tagset, fol-
lowing practical considerations: bridge categories
should be automatically reconstructed by exploit-
ing morpho?syntactic and dependency informa-
tion contained in the original resources; for some
constructions, the MIDT representation is parame-
terizable, i.e. the tagset provides two different op-
tions, corresponding to the TUT and ISST?TANL
annotation styles (e.g. for determiner?noun or
preposition?noun relations).
The final MIDT tagset contains 21 dependency
tags (as opposed to the 72 tags of TUT and the
29 of ISST?TANL), including the different op-
tions provided for the same type of construction.
CoNLL is used as encoding format.
3 Comparing the MIDT and SD schemes
The MIDT and SD annotation schemes are both
dependency?based and therefore fall within the
same broader family. This fact, however, does
not guarantee per se an easy and linear conver-
sion process from one to the other: as pointed out
in Bosco et al (2012), harmonizing and convert-
ing annotation schemes can be quite a challenging
task, even when this process is carried out within
a same paradigm and with respect to the same lan-
guage. In the case at hand, this task is made easier
thanks to the fact that the MIDT and SD schemes
share similar design principles: for instance, in
both cases preference is given a) to relations which
are semantically contentful and useful to appli-
cations, or b) to relations linking content words
rather than being indirectly mediated via function
words (see design principles 2 and 5 respectively
in de Marneffe and Manning (2008a)). Another
peculiarity shared by MIDT and SD consists in the
fact that they both neutralize the argument/adjunct
distinction for what concerns prepositional com-
plements, which is taken to be ?largely useless
63
in practice? as de Marneffe and Manning (2008a)
claim. In spite of their sharing similar design prin-
ciples, there are also important differences con-
cerning the inventory of dependency types and
their linguistic interpretation, the head selection
criteria as well as the treatment of specific syn-
tactic constructions. In what follows, we summa-
rize the main dimensions of variation between the
MIDT and SD annotation schemes, with a specific
view to the conversion issues they arise.
3.1 Granularity and inventory of dependency
types
MIDT and SD annotation schemes assume differ-
ent inventories of dependency types characterized
by different degrees of granularity in the repre-
sentation of specific relations: the adopted depen-
dency tagset includes 21 dependency types in the
case of MIDT and 48 in the case of SD. Interest-
ingly however, it is not always the case that the
finer grained annotation scheme ? i.e. SD ? is the
one providing more granular distinctions: whereas
this is typically the case, there are also cases in
which more granular distinction are adopted in the
MIDT annotation scheme.
Consider first SD relational distinctions which
are neutralized at the level of the MIDT annota-
tion. As reported in de Marneffe and Manning
(2008a), so?called NP?internal relations are crit-
ical in real world applications: the SD scheme
therefore includes many relations of this kind,
e.g. appos (appositive modifier), nn (noun com-
pound), num (numeric modifier), number (ele-
ment of compound number) and abbrev (abbre-
viation). In MIDT all these relation types are
lumped together under the general heading of mod
(modifier). To deal with these cases, the MIDT to
SD conversion has to simultaneously combine de-
pendency and morpho?syntactic information (e.g.
the morpho?syntactic category of the nodes in-
volved in the relation), which however is not al-
ways sufficient as in the case of appositive modi-
fiers for which further evidence is needed.
Let us consider now the reverse case, i.e. in
which MIDT adopts finer?grained distinctions
with respect to SD. For instance, MIDT envis-
ages different relation types for auxiliary?verb and
preposition?verb (within infinitive clauses, be they
modifiers or subcategorized arguments) construc-
tions, which are aux and prep respectively. By
contrast, SD represents both cases in terms of the
same relation type, i.e. aux. Significant differ-
ences between English and Italian justify the dif-
ferent strategies adopted in SD and MIDT respec-
tively: in English, open clausal complements are
always introduced by the particle ?to?, whereas in
Italian different prepositions can introduce them
(i.e. ?a?, ?di?, ?da?), which are selected by the gov-
erning head. The SD representation of the element
introducing infinitival complements and modifiers
in terms of aux might not be appropriate as far as
Italian is concerned and it would be preferable to
have a specific relation for dealing with introduc-
ers of infinitival complements (like complm in the
case of finite clausal complements): as reported
in Section 4, we are currently evaluating different
representational options with a specific view to the
syntactic peculiarities of the Italian language.
Another interesting and more complex exam-
ple can be found for what concerns the parti-
tioning of the space of sentential complements.
MIDT distinguishes between mod(ifiers) on the
one hand and subcategorised arg(uments) on the
other hand: note that whereas arg is restricted
to clausal complements subcategorized for by the
governing head, the mod relation covers different
types of modifiers (nominal, adjectival, clausal,
adverbial, etc.). By contrast, SD resorts to spe-
cific relations for dealing with sentential comple-
ments: in particular, distinct relation types are en-
visaged depending on e.g. whether the clause is
a subcategorized complement or a modifier (see
e.g. ccomp vs advcl), or whether the gov-
ernor is a verb or a noun (see e.g. xcomp vs
infmod), or whether the clausal complement is
headed by a finite or non?finite verb (see e.g.
ccomp vs xcomp). Starting from MIDT, the
finer?grained distinctions adopted by SD for deal-
ing with clausal complements can be recovered by
combining dependency information with morpho-
syntactic one (e.g. the mood of the verbal head of
the clausal complements or the morpho?syntactic
category of the governing head).
3.2 Head selection
Criteria for distinguishing the head and the de-
pendent within relations have been widely dis-
cussed in the linguistic literature in all frameworks
where the notion of syntactic head plays an im-
portant role. Unfortunately, different criteria have
been proposed, some syntactic and some seman-
tic, which do not lead to a single coherent notion
64
of dependency (Ku?bler et al, 2009). Head se-
lection thus represents an important and unavoid-
able dimension of variation among dependency
annotation schemes, especially for what con-
cerns constructions involving grammatical func-
tion words. MIDT and SD agree on the treat-
ment of tricky cases such as the determiner?noun
relation within nominal groups, the preposition?
noun relation within prepositional phrases as well
as the auxiliary?main verb relation in complex
verbal groups. In both schemes, head selection
follows from a combination of syntactic and se-
mantic criteria: i.e. whereas in the determiner?
noun and auxiliary?verb constructions the head
role is assigned to the semantic head (noun/verb),
in preposition?noun constructions the head role is
played by the element which is subcategorized for
by the governing head, i.e. the preposition which
is the syntactic head but can also be seen as as a
kind of role marker. In this area, the only but not
negligible difference is concerned with subordi-
nate clauses whose head in SD is assumed to be the
verb, rather than the introducing element (whether
a preposition or a subordinating conjunction) as in
MIDT: in this case, the MIDT to SD conversion
requires restructuring of the dependency tree.
3.3 Coordination and punctuation
In both MIDT and SD schemes, coordinate con-
structions are considered as asymmetric structures
with a main difference: while in MIDT both
the conjunction and conjuncts starting from the
second one are linked to the immediately pre-
ceding conjunct, in SD the conjunction(s) and
the subsequent conjunct(s) are all linked to the
first one. Also the treatment of punctuation is
quite problematic in the framework of a depen-
dency annotation scheme, although this has not
been specifically dealt with in the linguistic liter-
ature. Whereas MIDT has its own linguistically?
motivated strategy to deal with punctuation, SD
does not appear to provide explicit and detailed
annotation guidelines in this respect.
3.4 MIDT? or SD?only relations
It is not always the case that a dependency type
belonging to the MIDT or SD annotation scheme
has a counterpart in the other. Let us start from SD
relation types which are not explicitly encoded in
the MIDT source annotation, due to constraints of
the CoNLL representation format. This is the case
of the ref dependency linking the relative word
introducing the relative clause and its antecedent,
or of the xsubj relation which in spite of its being
part of the original TUT and ISST resources have
been omitted from the most recent and CoNLL?
compliant versions, which represent the starting
point of in MIDT: in both cases, the ?one head
per dependent? constraint of the CoNLL repre-
sentation format is violated. From this, it fol-
lows that ISDT won?t include these dependency
types. Other SD relations which were part of the
MIDT?s ancestors but were neutralized in MIDT
are concerned with semantically?oriented distinc-
tions which turned out to be problematic to be
reliably identified in parsing in spite of their be-
ing explicitly encoded in both source annotation
schemes (Bosco et al, 2012). This is the case of
the indirect object relation (iobj) or of temporal
modifiers (tmod).
The MIDT relation types which instead do not
have a corresponding relation in SD are those
that typically represent Italian?specific peculiari-
ties. This is the case of the clit(ic) dependency,
linking clitic pronouns to the verbal head they re-
fer to. In MIDT, whenever appropriate clitic pro-
nouns are assigned a label that reflects their gram-
matical function (e.g. ?dobj? or ?iobj?): this is the
case of reflexive constructions (Maria si lava lit.
?Maria her washes? meaning that ?Maria washes
herself?) or of complements overtly realized as
clitic pronouns (Giovanni mi ha dato un libro lit.
?Giovanni to?me has given a book? meaning that
?Giovanni gave me a book?). With pronominal
verbs, in which the clitic can be seen as part of
the verbal inflection, a specific dependency rela-
tion (clit) is resorted to link the clitic pronoun
to the verbal head: for instance, in a sentence like
la sedia si e` rotta lit. ?the chair it is broken? mean-
ing that ?the chair broke?, the dependency linking
the clitic si to the verbal head is clit.
4 The MIDT to SD conversion
The conversion process followed to generate the
Italian Stanford Dependency Treebank (ISDT)
starting from MIDT is based on the results of the
comparative analysis reported in the previous sec-
tion. It is organized in two different steps: the
first one aimed at generating an enriched version
of the MIDT resource, henceforth referred to as
MIDT++, including SD?relevant distinctions neu-
tralized in MIDT, and the second one in charge
of converting the MIDT++ annotation in terms
65
of the Stanford Dependencies as described in de
Marneffe and Manning (2008b) specialized with
respect to the Italian language syntactic peculiar-
ities. Note that also the resulting ISDT resource
adheres to the CoNLL tabular format.
The first step relied on previous harmonization
work leading to the construction of the MIDT re-
source starting from the CoNLL?compliant TUT
and ISST?TANL treebanks (described in Bosco
et al (2012)). During this step, we recovered
from the native resources relevant distinctions
that have been neutralized in MIDT, because of
choices made in the design of the MIDT anno-
tation scheme (e.g. indirect objects or temporal
modifiers which are assigned an underspecified
representation in MIDT, see Section 3) or simply
because the harmonization of the source annota-
tion schemes was not possible without manual re-
vision (this is the case of appositions, explicitly
annotated only in TUT).
Other issues tackled during this first pre?
processing step include the treatment of coordi-
nation and multi?word expressions. Since in SD
conjunctions and conjuncts, after the first one, are
all linked to the first conjunct, exactly as it was
in ISST?TANL, the intermediate MIDT++ is gen-
erated according to this scheme, with no conver-
sion for ISST?TANL and by restructuring the dif-
ferent cascading coordination style of TUT. For
what concerns multi?word expressions, we unified
the multi?word repertoires of the two resources.
Another area that required some pre?processing
with manual revision is concerned with the anno-
tation of the parataxis relation. The augmented re-
source resulting from this pre?processing step, i.e.
MIDT++, is used as a ?bridge? towards the SD
representation format.
Starting from the results of the comparative
analysis detailed in Section 3, we defined conver-
sion patterns which can be grouped into two main
classes according to whether they refer to individ-
ual dependencies (case A) or they involve depen-
dency subtrees due to head reassignment (case B).
A) Structure?preserving mapping rules involv-
ing dependency retyping without restructur-
ing of the tree:
A.1) 1:1 mapping requiring dependency retyp-
ing only (e.g. MIDT prep > SD pobj, or
MIDT subj > SD nsubj);
A.2) 1:n mapping requiring finer?grained de-
pendency retyping (e.g. MIDT mod > SD
abbrev | amod | appos | nn | nnp |
npadvmod | num | number | partmod |
poss | preconj | predet | purplcl |
quantmod | tmod);
B) Tree restructuring mapping rules involving
head reassignment and dependency retyping.
Focusing on dependency retyping we distin-
guish the following cases:
B.1) head reassignment with 1:1 dependency
mapping (e.g. MIDT subj > SD csubj
in the case of clausal subjects);
B.2) head reassignment with 1:n dependency
mapping based on finer?grained distinctions
(e.g. MIDT arg> SD xcomp? ccomp, or
MIDT mod (with verbal head) > SD advcl
| infmod | prepc | purpcl).
In what follows, we will exemplify how the ab-
stract patterns described above have been trans-
lated into MIDT to SD conversion rules. The
conversion of the MIDT arg relation, referring
to clausal complements subcategorized for by the
governing head, represents an interesting example
of 1:n dependency mapping with tree restructuring
(case B.2 above). In MIDT, clausal complements,
either finite or non?finite clauses, are linked to the
governing head (which can be a verb, a noun or an
adjective) as arg(uments), with a main difference
with respect to SD, i.e. that the head of the clausal
complement is the word introducing it (be it a
preposition or a subordinating conjunction) rather
than the verb of the clausal complement. The main
conversion rules to SD can be summarised as fol-
lows, where the? separates the left from the right
hand side of the rule, the notation x ?dep label y
denotes that token y is governed by token x with
the dependency label specifying the relation hold-
ing between the two (a MIDT tag is found on the
left side of the rule, whereas an SD one occurs on
the right side):
1. $1[S|V |A] ?arg $2[E] ?prep $3[Vinfinitive] ?
$1 ?xcomp $3; $3 ?aux $2
2. $1[S|V |A] ?arg $2[CS] ?sub $3[Vfinite] ?
$1 ?ccomp $3; $3 ?complm $2
In the rules, the $ followed by a number is a vari-
able indentifying a given dependency node. Con-
straints on tokens in the left?hand side of the rule
66
(a) MIDT representation (b) SD representation
Figure 2: MIDT vs SD annotation of the same sentence
are reported within square brackets: they are typi-
cally concerned with the grammatical category of
the token (CS stands for subordinative conjunc-
tion, E for preposition, S for noun, V for verb).
Rule 1 above handles the transformation of the in-
finitival clause from the MIDT representation to
SD. Consider as an example the MIDT depen-
dency tree in Figure 2(a) for the sentence Gio-
vanni ha dichiarato ai giudici di avere pagato i
terroristi, lit. ?Giovanni told to?the judges to have
paid the terrorists? ?Giovanni told the judges that
he has paid the terrorists? whose SD conversion is
reported in Figure 2(b). By comparing the trees,
we see that head restructuring and dependency re-
typing have both been performed in the conversion
of the infinitival clause representation: in MIDT
the head of the infinitival clause is the preposition
whereas in SD it is the verb; the relation linking
the governing head and the head of the infinitival
clause is arg in MIDT and xcomp in SD.
Currently, the conversion script implements
over 100 rules which are still being tested with the
final aim of finding the most appropriate represen-
tation with respect to the Italian syntactic pecu-
liarities. The problematic area of sentential com-
plements is still being explored to find out ade-
quate representational solutions. Consider as an
example the case of the word introducing infiniti-
val complements: Figure 2(b) above, reporting the
result of the SD conversion, shows that the same
aux relation is used to link the preposition to the
verb heading the infinitival complement as well as
the auxiliary avere ?to have? to the main verb. This
solution might not be so appropriate given the pe-
culiarities of the Italian language, where different
prepositions (lexically selected by the governing
head) can introduce infinitival complements.
During the conversion step, the SD scheme
has been specialized with respect to the Italian
language. There are SD dependency relations
which were excluded from the Italian localization
of the standard scheme, either because not ap-
propriate given the syntactic peculiarities of this
language (this is the case e.g. of the prt re-
lation) or because they could not be recovered
from the CoNLL?compliant versions of the re-
sources we started from (see e.g. the relations
ref or xsubj). The SD tagset was also extended
with new dependency types: this is the case of
the clit relation used for dealing with clitics in
pronominal verbs, or of the nnp relation specifi-
cally defined for compound proper nouns. Other
specializations are concerned with the use of un-
derspecified categories: rather than resorting to the
most generic relation, i.e. dep used when it is im-
possible to determine a more precise dependency
relation, we exploited the hierarchical organiza-
tion of SD typed dependencies, i.e. we used the
comp and mod relations when we could not find
an appropriate relation within the set of their de-
pendency subtypes.
5 Using ISDT as training corpus
In this section, we report the results achieved
by using ISDT for training a dependency parser,
namely DeSR (Dependency Shift Reduce), a
transition?based statistical parser (Attardi, 2006),
where it is possible to specify, through a config-
uration file, the set of features to use (e.g. POS
tag, lemma, morphological features) and the clas-
sification algorithm (e.g. Multi-Layer Perceptron
(Attardi and Dell?Orletta, 2009), Support Vector
Machine, Maximum Entropy). DeSR has been
trained on TUT and ISST?TANL in the frame-
work of the evaluation campaigns Evalita, for
the last time in 2011 (Bosco and Mazzei, 2012;
Dell?Orletta et al, 2012). More recently DeSR has
been trained and tested on MIDT: the results ob-
67
Table 1: Parsing results with ISDT resources
TRAINING TEST PARSER LAS LAS no punct
TUT?SDT train TUT?SDT test DeSR MLP 84.14% 85.57%
ISST?TANL?SDT train ISST?TANL?SDT test DeSR MLP 80.55% 82.11%
TUT+ISST?TANL?SDT train TUT+ISST?TANL?SDT test DeSR MLP 83.34% 84.16%
TUT+ISST?TANL?SDT train TUT?SDT test DeSR MLP 84.14% 85.79%
TUT+ISST?TANL?SDT train ISST?TANL?SDT test DeSR MLP 79.94% 81.86%
tained on both the MIDT version of the individual
TUT and ISST?TANL resources and the merged
resource are reported in (Bosco et al, 2012): the
best scores, achieved applying a parser combina-
tion strategy and training on TUT inMIDT format,
are LAS 90.11% and LAS 91.58% without punc-
tuation.
For the experiments on the ISDT resource we
used a basic and fast variant of the DeSR parser
based on Multi-Layer Perceptron (MLP). In fact,
the purpose of the experiment was not to optimize
the parser for the new resource but to compare
relative performances of the same parser on dif-
ferent versions of the same resources. As a re-
sult, the substantial drop in performance observed
with respect to the MIDT resource is in part due to
this factor, and cannot be totally attributed to the
greater complexity of the SD scheme or quality of
the conversion output.
Table 1 reports, in the first two rows, the val-
ues of Labeled Attachment Score (LAS, with and
without punctuation) obtained against the TUT?
ISDT and ISST?TANL?ISDT datasets. The differ-
ent performance of the parser on the two converted
datasets (TUT?ISDT and ISST?TANL?ISDT) is in
line with what was observed in previous exper-
iments with native resources and MIDT (Bosco
et al, 2010; Bosco et al, 2012); therefore, the
composition of the training and test corpora can
still be identified as possible causes for such a dif-
ference. The results reported in rows 3?5 have
been obtained by training DeSR with the larger
resource including both TUT?ISDT and ISST?
TANL?ISDT. As test set, we used a combination
of the two test sets (row 3) and test sets from the
two data sets separately (rows 4 and 5). The pre-
liminary results achieved by using ISDT are en-
couraging, in line with what was obtained on the
WSJ for English and reported in (Cer et al, 2010),
where the best results in labeled attachment preci-
sion, achieved by a fast dependency parser (Nivre
Eager feature Extract), is 81.7. For the time being,
training with the larger combined resource does
not seem to provide a substantial advantage, con-
firming results obtained with MIDT, despite the
fact that in the conversion from MIDT to ISDT
a substantial effort was spent to further harmonize
the two resources.
6 Conclusion
In this paper, we addressed the challenge of con-
verting MIDT, an existing dependency?based Ital-
ian treebank resulting from the harmonization and
merging of smaller resources adopting incompati-
ble annotation schemes, into the Stanford Depen-
dencies annotation formalism, with the final aim
of constructing a standard?compliant resource for
the Italian language. SD, increasingly acknowl-
edged within the international NLP community as
a de facto standard, was selected for its being de-
fined with a specific view to supporting informa-
tion extraction tasks.
The outcome of this still ongoing effort is three?
fold. Starting from a comparative analysis of
the MIDT and SD annotation schemes, we devel-
oped a methodology for converting treebank anno-
tations belonging to the same dependency?based
family. Second, Italian has now a new standard?
compliant treebank, i.e. the Italian Stanford De-
pendency Treebank (ISDT, 200,516 tokens)7: we
believe that this conversion will significantly im-
prove the usability of the resource. Third, but not
least important, we specialized the Stanford De-
pendency annotation scheme to deal with the pe-
culiarities of the Italian language.
7 Acknowledgements
This research was supported by a Google ?gift?.
Giuseppe Attardi helped with the experiments
with the DeSR parser, Roberta Montefusco pro-
duced the converter to the collapsed/propagated
version of ISDT and in so doing helped us to re-
duce inconsistencies and errors in the resource.
7Both the MIDT and ISDT resources are released by
the authors under the Creative Commons Attribution?
NonCommercial-ShareAlike 3.0 Unported licence
(http://creativecommons.org/licenses/by-nc-sa/3.0/
legalcode.txt).
68
References
G. Attardi and F. Dell?Orletta. 2009. Reverse revision
and linear tree combination for dependency parsing.
In Proceedings of of NAACL HLT (2009).
G. Attardi. 2006. Experiments with a multilanguage
non?projective dependency parser. In Proceedings
of the CoNLL-X ?06, New York City, New York.
C. Bosco and A. Mazzei. 2012. The evalita 2011 pars-
ing task: the dependency track. In Working Notes of
Evalita?11, Roma, Italy.
C. Bosco, V. Lombardo, L. Lesmo, and D. Vassallo.
2000. Building a treebank for italian: a data-driven
annotation schema. In Proceedings of the LREC?00,
Athens, Greece.
C. Bosco, S. Montemagni, A. Mazzei, V. Lombardo,
F. Dell?Orletta, A. Lenci, L. Lesmo, G. Attardi,
M. Simi, A. Lavelli, J. Hall, J. Nilsson, and J. Nivre.
2010. Comparing the influence of different treebank
annotations on dependency parsing. In Proceedings
of the LREC?10, Valletta, Malta.
C. Bosco, M. Simi, and S. Montemagni. 2012. Harmo-
nization and merging of two italian dependency tree-
banks. In Proceedings of the LREC 2012 Workshop
on Language Resource Merging, Istanbul, Turkey.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
In Proc. of CoNLL, pages 149?164.
D. Cer, M.C. de Marneffe, D. Jurafsky, and C.D. Man-
ning. 2010. Parsing to stanford dependencies:
Trade-offs between speed and accuracy. In Proceed-
ings of the LREC?10), Valletta, Malta.
M.C. de Marneffe and C. Manning. 2008a. The stan-
ford typed dependencies representation. In Col-
ing 2008: Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation,
pages 1?8, Stroudsburg, PA, USA. Association for
Computational Linguistics.
M.C. de Marneffe and C.D. Manning. 2008b. Stan-
ford typed dependencies manual. Technical report,
Stanford University.
M.C. de Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of 5th In-
ternational Conference on Language Resources and
Evaluation (LREC 2006).
T. Declerck. 2008. A framework for standardized syn-
tactic annotation. In Proceedings of the LREC?08,
Marrakech, Morocco.
F. Dell?Orletta, S. Marchi, S. Montemagni, G. Venturi,
T. Agnoloni, and E. Francesconi. 2012. Domain
adaptation for dependency parsing at evalita 2011.
In Working Notes of Evalita?11, Roma, Italy.
K. Haverinen, T. Viljanen, V. Laippala, S. Kohonen,
F. Ginter, and T. Salakoski. 2010. Treebanking
Finnish. In Proceedings of the 9th Workshop on
Treebanks and Linguistic Theories (TLT-9), pages
79?90, Tartu, Estonia.
R. Hudson. 1984. Word Grammar. Basil Blackwell,
Oxford and New York.
N. Ide and L. Romary. 2006. Representing linguistic
corpora and their annotations. In Proceedings of the
LREC?06, Genova, Italy.
N. Ide and K. Suderman. 2007. GrAF: A graph-based
format for linguistic annotations. In Proceedings of
the Linguistic Annotation Workshop, Prague, Czech
Republic.
M. Kemps-Snijders, M. Windhouwer, P. Wittenburg,
and S.E. Wright. 2009. Isocat: remodelling meta-
data for language resources. IJMSO, 4(4):261?276.
S. Ku?bler, R.T. McDonald, and J. Nivre. 2009. De-
pendency Parsing. Morgan & Claypool Publishers,
Oxford and New York.
John Lee and Yin Hei Kong. 2012. A dependency
treebank of classical chinese poems. In Proceed-
ings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
191?199, Montre?al, Canada, June. Association for
Computational Linguistics.
G. Leech, R. Barnett, and P. Kahrel. 1996. Eagles rec-
ommendations for the syntactic annotation of cor-
pora. Technical report, EAG-TCWG-SASG1.8.
A. Lenci, S. Montemagni, V. Pirrelli, and C. Soria.
2008. A syntactic meta?scheme for corpus anno-
tation and parsing evaluation. In Proceedings of the
LREC?00, Athens, Greece.
S. Montemagni, F. Barsotti, M. Battista, N. Calzo-
lari, O. Corazzari, A. Lenci, A. Zampolli, F. Fan-
ciulli, M. Massetani, R. Raffaelli, R. Basili, M. T.
Pazienza, D. Saracino, F. Zanzotto, N. Mana, F. Pi-
anesi, and R. Delmonte. 2003. Building the Italian
Syntactic-Semantic Treebank. In A. Abeille?, editor,
Building and Using syntactically annotated corpora.
Kluwer, Dordrecht.
S. Pyysalo, F. Ginter, K. Haverinen, J. Heimonen,
T. Salakoski, and V. Laippala. 2007. On the uni-
fication of syntactic annotations under the Stanford
dependency scheme: A case study on Bioinfer and
GENIA. In BioNLP 2007: Biological, transla-
tional, and clinical language processing, pages 25?
32, Prague.
M. Seraji, B. Megyesi, and J. Nivre. 2012. Bootstrap-
ping a persian dependency treebank. Special Issue
of Linguistic Issues in Language Technology (LiLT)
on Treebanks and Linguistic Theories, 7.
69
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 163?173,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Assessing the Readability of Sentences: Which Corpora and Features?
Felice Dell?Orletta

, Martijn Wieling
??
, Andrea Cimino

, Giulia Venturi

and Simonetta Montemagni


Istituto di Linguistica Computazionale ?Antonio Zampolli? (ILC?CNR)
ItaliaNLP Lab - www.italianlp.it
{name.surname}@ilc.cnr.it
?
Department of Humanities Computing, University of Groningen, The Netherlands
?
Department of Quantitative Linguistics, University of T?ubingen, Germany
wieling@gmail.com
Abstract
The paper investigates the problem of
sentence readability assessment, which is
modelled as a classification task, with a
specific view to text simplification. In par-
ticular, it addresses two open issues con-
nected with it, i.e. the corpora to be used
for training, and the identification of the
most effective features to determine sen-
tence readability. An existing readabil-
ity assessment tool developed for Italian
was specialized at the level of training cor-
pus and learning algorithm. A maximum
entropy?based feature selection and rank-
ing algorithm (grafting) was used to iden-
tify to the most relevant features: it turned
out that assessing the readability of sen-
tences is a complex task, requiring a high
number of features, mainly syntactic ones.
1 Introduction
Over the last ten years, work on automatic read-
ability assessment employed sophisticated NLP
techniques (such as syntactic parsing and statisti-
cal language modeling) to capture highly complex
linguistic features, and used statistical machine
learning to build readability assessment tools. A
variety of different NLP?based approaches has
been proposed so far in the literature, differing
at the level of the number of identified readabil-
ity classes, the typology of features taken into ac-
count, the intended audience of the texts under
evaluation, or the application within which read-
ability assessment is carried out, etc.
Research focused so far on readability assess-
ment at the document level. However, as pointed
out by Skory and Eskenazi (2010), methods devel-
oped perform well when the task is characterizing
the readability level of an entire document, while
they are unreliable for short texts, including single
sentences. Yet, for specific applications, assessing
the readability level of individual sentences would
be desirable. This is the case, for instance, for text
simplification: in current approaches, text read-
ability is typically assessed with respect to the en-
tire document, while text simplification is carried
out at the sentence level, as e.g. done in Alu??sio
et al. (2010), Bott and Saggion (2011) and Inui et
al. (2003). By decoupling the readability assess-
ment and simplification processes, the impact of
simplification operations on the overall readabil-
ity level of a given text may not always be clear.
With sentence?based readability assessment, this
is expected to be no longer a problem. Sentence
readability assessment thus represents an open is-
sue in the literature which is worth being further
explored. To our knowledge, the only attempts
in this direction are represented by Dell?Orletta et
al. (2011) and Sj?oholm (2012) for the Italian and
Swedish languages respectively, followed more
recently by Vajjala and Meurers (2014) dealing
with English.
In this paper, we tackle the challenge of assess-
ing the readability of individual sentences as a first
step towards text simplification. The task is mod-
elled as a classification task, with the final aim
of shedding light on two open issues connected
with it, namely the reference corpora to be used
for training (i.e. collections of sentences classified
according to their readability level), and the iden-
tification of the most effective features to deter-
mine sentence readability. For what concerns the
former, sentence readability assessment poses the
remarkable issue of classifying sentences accord-
ing to their difficulty: if all sentences occurring in
simplified texts can be assumed to be easy?to?read
sentences, the reverse does not necessarily hold
since not all sentences occurring in complex texts
are to be assumed difficult?to?read. This fact has
important implications at the level of the composi-
tion of the corpora to be used for training. The sec-
163
ond issue is concerned with whether and to what
extent the features playing a significant role in the
assessment of readability at the sentence level co-
incide with those exploited at the level of docu-
ment. In particular, the following research ques-
tions are addressed:
1. in assessing sentence readability, is it bet-
ter to use a small gold standard training cor-
pus of manually classified sentences or a
much bigger training corpus automatically
constructed from readability?tagged docu-
ments possibly containing misclassified sen-
tences?
2. which are the features maximizing sentence
readability assessment?
3. to what extent do important features for sen-
tence readability classification match those
playing a role in the document readability
classification?
We will try to answer these questions by work-
ing on Italian, which is a less?resourced language
as far as readability is concerned. To this end,
READ?IT (Dell?Orletta et al., 2011; Dell?Orletta
et al., 2014), which represents the first NLP?based
readability assessment tool for Italian, was spe-
cialized in different respects, namely at the level of
the training corpus and of the learning algorithm;
to investigate questions 2. and 3. above, a maxi-
mum entropy?based feature selection and ranking
algorithm (i.e. grafting) was selected. The specific
target audience of readers addressed in this study
is represented by people characterised by low lit-
eracy skills and/or by mild cognitive impairment.
The paper is organized as follows: Section 2 de-
scribes the background literature, Section 3 intro-
duces our approach to the task, in terms of used
corpora, features and learning algorithm. Finally,
Sections 4 and 5 describe the experimental setting
and discuss achieved results.
2 Background
In spite of the acknowledged need of perform-
ing readability assessment at the sentence level,
so far very few attempts have been made to sys-
tematically investigate the issues and challenges
concerned with the readability assessment of sen-
tences (as opposed to documents). The first two
studies in this direction focused on languages
other than English, namely Italian (Dell?Orletta
et al., 2011) and Swedish (Sj?oholm, 2012). In
both cases, the authors start from the assump-
tion that while all sentences occurring in simpli-
fied texts can be assumed to be easy?to?read sen-
tences, the reverse is not true, since not all sen-
tences occurring in complex texts are difficult?to?
read. This has important consequences at the level
of the evaluation of sentence classification results:
i.e. erroneous readability assessments within the
class of difficult?to?read texts may either corre-
spond to those easy?to?read sentences occurring
within complex texts or represent real classifi-
cation errors. To overcome this problem in the
readability assessment of individual sentences, a
notion of distance with respect to easy-to-read
sentences was introduced by Dell?Orletta et al.
(2011). Focusing on English, a similar issue is
addressed more recently by Vajjala and Meur-
ers (2014) who developed a binary sentence clas-
sifier trained on Wikipedia and Simple English
Wikipedia: they showed that the low accuracy ob-
tained by their classifier stems from the incorrect
assumption that all Wikipedia sentences are more
complex than the Simple Wikipedia ones.
Besides readability, sentence?based analyses
are reported in the literature for related tasks: for
instance, in a text simplification scenario by Drn-
darevi?c et al. (2013), Alu??sio et al. (2008),
?
Stajner
and Saggion (2013) and Barlacchi and Tonelli
(2013); or to predict writing quality level by Louis
and Nenkova (2013). Sheikha and Inkpen (2012)
report the results of both document? and sentence?
based classification in the different but related task
of assessing formal vs. informal style of a docu-
ment/sentence. For students learning English, An-
dersen et al. (2013) made a self?assessment and
tutoring system available which was able to assign
a quality score for each individual sentence they
write: this provides automated feedback on learn-
ers? writing.
A further important issue, largely investigated
in previous readability assessment studies, is the
identification of linguistic factors playing a role
in assessing the readability of documents. If tra-
ditional readability metrics (see e.g., Kincaid et
al. (1975)) typically rely on raw text characteris-
tics, such as word and sentence length, the new
NLP?based readability indices exploit wider sets
of features ranging across different linguistic lev-
els. Starting from Schwarm and Ostendorf (2005)
and Heilman et al. (2007), the role of syntactic
164
features in this task was considered, and more re-
cently, the role of discourse features (e.g., dis-
course topic, discourse cohesion and coherence)
has also been taken into account (see e.g., Barzi-
lay and Lapata (2008), Pitler and Nenkova (2008),
Kate et al. (2010), Feng et al. (2010) and Tonelli
et al. (2012)). Many of these studies also explored
the usefulness of features belonging to individual
levels of linguistic description in predicting text
readability. For example, Feng et al. (2010) sys-
tematically evaluated a wide range of features and
compared the results of different statistical classi-
fiers trained on different classes of features. Sim-
ilarly, the correlation between level?specific fea-
tures has been calculated by Pitler and Nenkova
(2008) with respect to human readability judg-
ments, and by Franc?ois and Fairon (2012) with
respect to readability levels. In both cases, the
classes of features which turned out to be highly
correlated with readability judgments were used
in a readability assessment tool to test their effi-
cacy. Note, however, that in all cases the predic-
tive power of the selected features was evaluated
at the document level only.
3 Our Approach
In this section, we introduce the main ingredi-
ents of our approach to sentence readability as-
sessment, corpora used for training and testing,
selected features and the learning and feature se-
lection algorithm.
3.1 Corpora
We relied on two different corpora: a newspaper
corpus, La Repubblica (henceforth, Rep), and an
easy?to?read newspaper, Due Parole (henceforth,
2Par). 2Par includes articles specifically written
by Italian linguists experts in text simplification
for an audience of adults with a rudimentary lit-
eracy level or with mild intellectual disabilities
(Piemontese, 1996), which represents the target
audience of this study. The two corpora ? selected
as representative of complex vs. simplified texts
within the journalistic genre ? differ significantly
with respect to the distribution of features typi-
cally correlated with text complexity (Dell?Orletta
et al., 2011) and thus represent reliable training
datasets. However, whereas such a distinction is
valid as far as documents are concerned, it appears
to be a simplistic generalization when the focus is
on sentences. In other words, whereas we can con-
sider all sentences of 2Par as easy?to?read, not all
Rep sentences are expected to be difficult?to?read.
From this it follows that whereas the internal com-
position of 2Par is homogeneous at the sentence
level, this is not the case for Rep.
To overcome this asymmetry and in particular
to assess the impact of the noise in the Rep train-
ing corpus, we constructed different training sets
differing in size and internal composition, going
from a noisy set which assumes all Rep sentences
to be difficult?to?read to a clean but smaller set
in which the easy?to?read sentences occurring in
Rep were manually filtered out. These training
sets were used in different experiments whose re-
sults are reported in Section 4.2.
The corpus containing only difficult?to?read
sentences was manually built by annotating Rep
sentences according to their readability (i.e. easy
vs. difficult). The annotation process was car-
ried out by two annotators with a background in
computational linguistics. In order to assess the
reliability of their judgements, we started with a
small annotation experiment: the two annotators
were provided with the same 5 articles from the
Rep corpus (for a total of 107 sentences) and were
asked to extract the difficult?to?read sentences (as
opposed to both easy?to?read and not?easy?to?
classify sentences). The first annotator carried out
the task in 5 minutes and 46 seconds, while the
second annotator took 9 minutes and 8 seconds.
The two annotators agreed on the classification of
81 difficult?to?read sentences out of 107 consid-
ered ones (in particular, the first annotator iden-
tified 90 difficult?to?read?sentences and the sec-
ond one 93 sentences). The agreement between
the two annotators was calculated in terms of pre-
cision, by taking one of the annotation sets as the
gold standard and the other as response: on aver-
age, we obtained a precision of 0.88 in the retrieval
of sentences definitely classified as difficult?to?
read. Given the high level of agreement, the two
annotators were asked to select difficult sentences
from two sets of distinct Rep articles. This re-
sulted in a set of 1,745 difficult?to?read sentences
which were used together with a random selection
of easy?to?read sentences from 2Par for training
and testing.
1
1
The collection can be downloaded from
www.italianlp.it/?page id=22.
165
Feature Ranking position Feature Ranking position
Sent. class. Doc. class. Sent. class. Doc. class.
Raw text features:
[1] Sentence length 1 1 [2] Word length 2 2
Lexical features:
[3] Word types in the Basic Italian Vocabu-
lary
14 42 [6] ?High availability words? 21 22
[4] ?Fundamental words? 10 9 [7] TTR (form) 7
[5] ?High usage words? 22 38 [8] TTR (lemma) 53
Morpho?syntactic features:
[9] Adjective 46 [26] Aux. verb ? inf. mood 64
[10] Adverb 29 59 [27] Aux. verb ? part. mood 51
[11] Article 49 25 [28] Aux. verb ? subj. mood 55
[12] Conjunction 40 [29] Main verb ? cond. mood 40 43
[13] Determiner 43 54 [30] Main verb ? ger. mood 48 48
[14] Interjection [31] Main verb ? imp. mood 37 57
[15] Noun 12 19 [32] Main verb ? indic. mood 16 11
[16] Number 65 44 [33] Main verb ? inf. mood 13 13
[17] Predeterminer [34] Main verb ? part. mood 26 28
[18] Preposition 61 [35] Main verb ? subj. mood 46 32
[19] Pronoun 27 30 [36] Modal verb - inf. mood 54 56
[20] Punctuation 35 [37] Modal verb ? cond. mood 41 36
[21] Residual [38] Modal verb ? imp. mood
[22] Verb 63 34 [39] Modal verb ? indic. mood 18 23
[23] Lexical density 34 33 [40] Modal verb ? part. mood
[24] Aux. verb ? cond. mood 59 60 [41] Modal verb ? subj. mood 60 58
[25] Aux. verb ? indic. mood 17 17
Syntactic features:
[42] Argument 62 [65] Sentence root 35 62
[43] Auxiliary 70 [66] Subject 39 52
[44] Clitic 63 [67] Subordinate clause 64
[45] Complement 28 29 [68] Temporal complement 45 55
[46] Concatenation 66 [69] Temporal modifier
[47] Conjunct in a disjunctive compound 58 67 [70] Temporal predicate
[48] Conjunct linked by a copulative con-
junction
38 37 [71] Parse tree depth 5 4
[49] Copulative conjunction 31 39 [72] Embedded complement ?chains? 8 24
[50] Determiner 50 26 [73] Verbal Root 6 3
[51] Direct object 44 27 [74] Arity of verbal predicates 3 15
[52] Disjunctive conjunction 57 68 [75] Pre?verbal subject 4 12
[53] Indirect complement/object 66 [76] Post?verbal subject 25 16
[54] Locative complement 52 51 [77] Pre?verbal object 36 41
[55] Locative modifier [78] Post?verbal object 9 21
[56] Locative predicate [79] Main clauses 23 14
[57] Modal verb 61 [80] Subordinate clauses 42 45
[58] Modifier 20 47 [81] Subordinate clauses in pre?verbal posi-
tion
32 10
[59] Negative 56 69 [82] Subordinate clauses in post?verbal po-
sition
19 20
[60] Passive subject [83] ?Chains? of embedded subordinate
clauses
11 5
[61] Predicative complement 49 [84] Finite complement clauses 30 18
[62] Preposition [85] Infinitive clauses 53 50
[63] Punctuation 24 31 [86] Length of dependency links 15 8
[64] Relative modifier 47 65 [87] Maximum length of dependency links 7 6
Table 1: Typology of features and ranking position in sentence and document readability assessment
experiments. Only about 14 features are needed for an adequate model of document readability, whereas
this number increases to 30 for sentence readability (marked in boldface). Features which were not
selected during ranking have no rank.
3.2 Linguistic Features
The set of features used in the experiments re-
ported in this paper is wide, spanning across dif-
ferent levels of linguistic analysis. They can
be broadly classified into four main classes, as
reported in Table 1: raw text features, lexical
features, morpho?syntactic features and syntactic
features, shortly described below.
2
2
For an exhaustive discussion including the motivations
underlying this selection of features, the interested reader is
Raw text features (Features [1?2] in Table 1)
refer to those features typically used within tra-
ditional readability metrics and include sentence
length, calculated as the average number of words
per sentence, and word length, calculated as the
average number of characters per words.
The cover category of lexical features (Features
[3?8] in Table 1) includes features referring to
referred to Dell?Orletta et al. (2011, 2014) where these fea-
tures were successfully used for assessing the readability of
Italian texts.
166
both the internal composition of the vocabulary
and the lexical richness of the text. For what con-
cerns the former, the Basic Italian Vocabulary by
De Mauro (2000) was taken as a reference re-
source, including a list of 7000 words highly fa-
miliar to native speakers of Italian. In particular,
we consider: a) the percentage of all unique words
(types) on this reference list occurring in the text,
and b) the internal distribution of the occurring ba-
sic Italian vocabulary words into the usage classi-
fication classes of ?fundamental words? (very fre-
quent words), ?high usage words? (frequent words)
and ?high availability words? (relatively lower fre-
quency words referring to everyday life). Lexical
richness of texts is monitored by computing the
Type/Token Ratio (TTR), which refers to the ratio
between the number of lexical types and the num-
ber of tokens within a text. Due to its sensitivity
to sample size, this feature is computed for text
samples of equivalent length.
The set of morpho?syntactic features (Features
[9?41] in Table 1) is aimed at capturing differ-
ent aspects of the linguistic structure affecting in
one way or another the readability of a text. They
range from the probability distribution of part?
of?speech (POS) types, to the lexical density of
the text, calculated as the ratio of content words
(verbs, nouns, adjectives and adverbs) to the to-
tal number of lexical tokens in a text. This class
also includes features referring to the distribution
of verbs by mood and/or tense, which can be seen
as a language?specific feature exploiting the pre-
dictive power of the Italian rich morphology.
The set of syntactic features (Features [42?87]
in Table 1) captures different aspects of the syntac-
tic structure which are taken as reliable indicators
for automatic readability assessment, namely:
? the unconditional probability of syntactic de-
pendency types, e.g. subject, direct object,
modifier, etc. (Features 42?70);
? parse tree depth features (71?72), going from
the depth of the whole parse tree, calculated
in terms of the longest path from the root
of the dependency tree to some leaf, to a
more specific feature referring to the aver-
age depth of embedded complement ?chains?
governed by a nominal head and including
either prepositional complements or nominal
and adjectival modifiers;
? verbal predicate features (73?78) aimed at
capturing different aspects of the behaviour
of verbal predicates: they range from the
number of verbal roots with respect to num-
ber of all sentence roots occurring in a text,
to more specific features such as the arity
of verbs, meant as the number of instanti-
ated dependency links sharing the same ver-
bal head (covering both arguments and modi-
fiers) and the relative ordering of subject and
object with respect to the verbal head;
? as subordination is widely acknowledged
to be an index of structural complexity
in language, subordination features (79?
85) include: the distribution of subordinate
vs. main clauses; for subordinates, the dis-
tribution of infinitives vs finite complement
clauses, their relative ordering with respect
to the main clause and the average depth of
?chains? of embedded subordinate clauses;
? the length of dependency links is another
characteristic connected with the syntactic
complexity of sentences. Features 86?87
measure dependency length in terms of the
words occurring between the syntactic head
and the dependent: they focus on all depen-
dency links vs. maximum dependency links
only.
3.3 Model Training and Feature Ranking
Given the twofold goal of this study, i.e. re-
liably assessing sentence readability and finding
the most predictive features undelying it, we used
GRAFTING (Perkins et al., 2003), as this approach
allows to train a maximum entropy model while si-
multaneously including incremental feature selec-
tion. The method uses a gradient?based heuristic
to select the most promising feature (to add to the
set of selected features S), and then performs a full
weight optimization over all features in S. This
process is repeated until a certain stopping crite-
rion is reached. As the grafting approach we use
integrates the l
1
regularization (preventing overfit-
ting), features are only included (i.e. have a non-
zero weight) when the reduction of the objective
function is greater than a certain treshold. In our
case, the l
1
prior we use was selected on the basis
of evaluating maximum entropy models with vary-
ing l
1
values (range: 1e-11, 1e-10, ..., 0.1, 1) via
10?fold cross validation. We used TINYEST
3
, a
3
http://github.com/danieldk/tinyest
167
grafting-capable maximum entropy parameter es-
timator for ranking tasks (de Kok, 2011; de Kok,
2013), to select the features and estimate their
weights. Whereas our task is not a ranking task,
but rather a binary classification problem, we were
able to model it as a ranking task by assigning a
high score (1) to difficult?to?read sentences and a
low score (0) to easy?to?read sentences. Conse-
quently, a sentence having a score < 0.5 was in-
terpreted as an easy?to?read sentence, whereas a
sentence which was assigned a score ? 0.5 was
interpreted to be a difficult?to?read sentence.
4 Experiments and Results
4.1 Experimental Setup
In all experiments, the corpora were automatically
tagged by the part?of?speech tagger described
in Dell?Orletta (2009) and dependency?parsed by
the DeSR parser (Attardi, 2006) using Support
Vector Machines as learning algorithm. We de-
vised two different experiments, aimed at explor-
ing the research questions investigated in this pa-
per. To this end, READ?IT was adapted by inte-
grating a specialized training corpus and a maxi-
mum entropy?based feature selection and ranking
algorithm (i.e. grafting).
Experiment 1
This experiment, investigating the first research
question, is aimed at identifying what is the most
effective training data for sentence readability as-
sessment. In particular, the goal is to compare
the results on the basis of using a small set of
gold standard data with respect to a (potentially
larger, but) noisy data set (i.e. without manual re-
vision) where every Rep sentence was assumed to
be difficult?to?read. In particular, the comparison
involved four datasets:
? a collection of gold standard data consisting
of 1,310 easy?to?read sentences randomly
extracted from the 2Par corpus and 1,310
manually selected difficult?to-read sentences
from the Rep corpus;
? a large and unbalanced collection of uncor-
rected data consisting of the whole 2Par cor-
pus (3,910 easy?to?read sentences) and the
whole Rep corpus (8,452 sentences, classi-
fied a priori as difficult?to?read);
? a balanced collection of uncorrected sen-
tences, consisting of 3,910 sentences from
2Par and 3,910 sentences from Rep;
? a balanced collection of uncorrected sen-
tences having the same size as the gold stan-
dard dataset, namely 1,310 sentences from
2Par and 1,310 sentences from Rep.
To assess similarities and differences at the level
of the different corpora used for training in this
experiment, in Table 2 we report a selection of
linguistic features (see Section 3.2) characterizing
the four datasets with respect to the whole 2Par
corpus.We can observe that 2Par differs from all
four Rep corpora for all reported features, and that
the four Rep corpora show similar trends. Inter-
estingly, however, the Rep Gold corpus is almost
always the most distant one from 2Par (i.e. at the
level of sentence length, word length, distribution
of adjectives and subjects, average length of de-
pendency links and parse tree depth).
On the basis of the four Rep datasets, four mod-
els were built which we evaluated using a held?
out test set consisting of 435 sentences from 2Par
and 435 manually classified difficult?to?read sen-
tences from Rep. Using the grafting method, we
calculated the classification score for each sen-
tence in our test set on the basis of an increasing
number of features (ranging from 1 to all non-zero
weighted features for the specific dataset): sen-
tences with a score below 0.5 were classified as
easy?to?read, whereas sentences having a score
greater or equal to 0.5 were classified as difficult?
to?read. This procedure was repeated for each of
the four models.
Experiment 2
The second experiment is aimed at answering our
second and third research questions, focusing on
the features relevant for sentence readability, and
the relationship of those features with document
readability classification. For this purpose, we
compared sentence? and document?based read-
ability classification results. In particular, we com-
pared the features used by the sentence?based
readability model trained on the gold standard
data and the features used by the document?based
model trained on Rep and 2Par. With respect
to the document classification, we used a cor-
pus of 638 documents (319 extracted from 2Par
representating easy?to?read texts, and 319 ex-
tracted from Rep representing difficult?to?read
texts) with 20% of the documents constituting the
held?out test set.
168
Features Rep Unbalan. large Rep Balan. small Rep Balan. large Rep Gold 2Par
Sentence length 24.98 26.03 25.26 28.14 18.66
Word length 5.14 5.24 5.14 5.28 5
?Fundamental words? 75.05% 75.08% 74.83% 74.99% 76.38
Adjective 6.19% 6.25% 6.36% 6.42% 6.03%
Noun 25.65% 27.09% 25.74% 26.10% 29.13%
Subject 4.62% 4.75% 4.64% 4.42% 6%
Max. length of dependency links 9.73 10.13 9.85 10.98 7.67
Parse tree depth 6.18 6.57 6.30 6.83 5.2
Table 2: Distribution of some linguistic features in Rep and 2Par training data
Accuracy Precision (all ft)
Training data 2 ft 10 ft 30 ft 50 ft all ft Easy Difficult
Unbalanced large 50 63.7 74.9 78.4 78.9 (85 ft) 69.2 88.5
Balanced small 64 67.9 79.2 80.8 82.5 (82 ft) 82.5 82.5
Balanced large 63.9 70.6 79.7 81.0 82.3 (85 ft) 83.0 81.6
Gold data 65.6 69.8 79.9 81.3 83.7 (66 ft) 84.8 82.5
Table 3: Sentence classification results using four training datasets and a varying number of features
4.2 Which Training Corpus for Sentence
Classification?
Table 3 reports the results for the sentence classi-
fication task using the four training datasets de-
scribed above. Results are reported in terms of
both overall accuracy (calculated as the proportion
of correct answers against all answers) and preci-
sion within each readability class (when using all
features), defined as the number of easy or diffi-
cult sentences correctly identified as such (in their
respective columns).
Accuracy was computed for all training models
tested using an increasing number of features (2,
10, 30, 50 and all features) as resulting from the
GRAFTING?based ranking and detailed in Table 1.
Note that the first two features correspond in all
cases to the traditional readability features of sen-
tence length and word length. The classification
model trained on the small gold standard dataset
turned out to almost always outperform all other
models: it achieved the best accuracy (83.7%) us-
ing a relatively small number of features (66), and
also for a fixed number of features (i.e. 2, 30
and 50). Only when using the top?10 features,
the uncorrected balanced large dataset slightly out-
performed the gold standard dataset. The accu-
racy when using the unbalanced dataset for train-
ing was always significantly (p < 0.05) worse (us-
ing McNemar?s test) than the accuracy based on
the other training data. The only other significant
difference existed between the balanced small and
large dataset for 10 features. All other differences
are non?significant.
It is also interesting to note that in the results
reported in column 2 ft of Table 3 a significant
difference is observed when comparing the accu-
racy achieved using the unbalanced large data set
with that achieved with the gold standard data: i.e.
about 15.5 percentage points of difference for the
2 ft model against 3 ? 6% using higher numbers
of features. This result originates from the fact that
the unbalanced corpus contains to a larger extent
sentences which are short and complex at the same
time whose correct readability assessment requires
linguistically?grounded features (see below).
The last two columns of Table 3 report preci-
sion results for easy? vs. difficult?to?read sen-
tences for each of the four training datasets (all
features). It is clear that for the class of difficult?
to?read sentences the highest precision (88.5%) is
obtained when using the whole 2Par and Rep cor-
pora for training (i.e. unbalanced large), whereas
for the class of easy?to?read sentences the best
precision results (84.8%) are obtained with the
system trained on the gold standard dataset. In-
terestingly, the worst precision results (69.2%) are
reported for the class of easy?to?read sentences
with the unbalanced large training data set.
These results suggest that the advantages of us-
ing the gold standard data over the uncorrected
training data sets are limited. From this it fol-
lows that treating the whole Rep corpus as a col-
lection of difficult?to?read sentences is not com-
pletely unjustified: this is in line with the satisfac-
tory results reported by Dell?Orletta et al. (2011)
where Rep was used for training a sentence read-
169
ability classifier without any manual filtering of
sentences. Nevertheless, the results of this ex-
periment demonstrate that readability assessment
accuracy and in particular the precision in identi-
fying easy?to?read sentences can be improved by
using a manually selected training dataset. Bal-
ancing the size of larger but potentially noisy (i.e.
without manual revision) data sets appears to cre-
ate a positive trade?off between accuracy and pre-
cision for both classes, thus representing a viable
alternative to the construction of a gold standard
dataset.
4.3 Sentence vs. Document Classification:
which and how many features?
To identify the typology of features needed for
sentence readability assessment and compare them
to those needed for assessing document read-
ability, we compared the results obtained by the
grafting?based feature selection in the sentence
classification task (using the gold standard dataset
for training, see Table 3) to those obtained in the
document classification task whose accuracy on
the test set is reported in Table 4 for increasing
numbers of features selected via GRAFTING.
Train. data 2 ft 10 ft 30 ft 50 ft 70 ft (all)
Rep - 2Par 80 93.3 96.6 96.6 95
Table 4: Accuracy of document classification for
a varying number of features
By comparing the document classification re-
sults with respect to those obtained for sentences,
it can be noticed that the best accuracy is achieved
using a set of 30 features: in contrast to sentence
classification where adding features keeps increas-
ing the performance, more features do not appear
to help for document classification. Sentence read-
ability classification thus seems to be a more com-
plex task, requiring a higher amount of features.
This trend emerges more clearly in Figures 1(a)
and 1(b), where the classification results on the
training set (using 10?fold cross?validation) and
the held?out test set are visualized for increas-
ing amounts of features selected via GRAFTING.
As Figure 1(a) shows, the document classifica-
tion task requires about 14 features after which
the performance appears to stabilize (97.4% accu-
racy for the ten?fold cross-validation and 96.7%
for the held?out test set). In contrast, Figure
1(b) shows that sentence classification requires at
least 30 features (83.4% accuracy for the ten?fold
cross-validation and 79.9% for the test set).
Noticeable differences can also be observed in
the typology of features playing a prominent role
in the two tasks. For each feature taken into ac-
count, Table 1 reports its ranking as resulting from
sentence? and document?based classification ex-
periments (columns ?Sent. class.? and ?Doc.
class.? respectively). Note that in interpreting
the rank associated with each feature it should
be considered that in sentence? and document?
classification the number of required features is
significantly different, i.e. 30 and 14 respectively:
this is to say that approximately the same rank as-
sociated to the same feature does not entail a com-
parable role across the two classification tasks.
As already pointed out, for both sentences and
documents raw text features (i.e. Sentence length
and Word length) turned out to be the top features,
leading however to significantly different results:
i.e. 80% accuracy for documents vs. 65% for
sentences. Among the remaining features, graft-
ing results show that syntactic features do play
a central role in both sentence? and document?
based readability assessment: many of these are
highly ranked, with some differences. Syntactic
features playing a similar role in both readabil-
ity classification tasks include: Verbal root [73],
Parse tree depth [71], ?Chains? of embedded sub-
ordinate clauses [83] and Max. length of depen-
dency links [87], covering important aspects of
syntactic complexity such as depth of the syntactic
dependency (sub?)tree and length of dependency
links. Features that are mainly useful for sentence
readability turned out to be Arity of verbal pred-
icates [74], Pre?verbal subject [75], Post?verbal
object [78] and Embedded complement ?chains?
[72], which can all be seen as representing local
features referring to sentence parts. The feature
Subordinate clauses in pre?verbal position [81],
focusing on the global distribution of pre?verbal
subordinate clauses within the document, is rele-
vant for document classification only. It is interest-
ing to note that features capturing different facets
of the same phenomenon can play quite a different
role for assessing the readability of sentences vs.
documents: this is the case of dependency length,
measured in terms of the words occurring between
the syntactic head and the dependent, where fea-
ture [86] refers to the average length of all de-
pendency links and [87] to the average length of
170
(a) Document classification (b) Sentence classification
Figure 1: Document vs Sentence classification results
maximum dependency links from each sentence.
Whereas [86] plays a similar role for sentences
and documents (in both cases it is a middle rank
feature), [87] is a global feature playing a more
prominent role in document classification.
At the morpho?syntactic level, the feature rank-
ing is more comparable. However, it is interest-
ing to note that very few morpho?syntactic fea-
tures were selected by the feature selection pro-
cess: this is particularly true for document classi-
fication. This can follow from the fact that these
features can be considered as proxies of the syn-
tactic structure which in these experiments was
represented through specific features: in this situ-
ation, the grafting process preferred syntactic fea-
tures over morpho?syntactic ones, in spite of the
lower accuracy of the dependency parser with re-
spect to the part?of?speech tagger. Interestingly,
this result is in contrast with what reported by
Falkenjack and J?onsson (2014) for what concerns
document readability assessment, who claim that
an optimal subset of text features for readability
based document classification does not need fea-
tures induced via parsing. Among the morpho?
syntactic features, it appears that verbal features
play an important role: this can follow both by the
language dealt with which is a a morphologically
rich language, and by the fact that these features
do not have a counterpart at the syntactic level.
Lexical features show a much more mixed re-
sult. Type?Token Ratio (TTR) is only important
for document classification, whereas most of the
other features are important for sentence readabil-
ity, but not for document readability (with the ex-
ception of the presence of ?fundamental words? of
the Basic Italian Vocabulary).
5 Discussion
In this study we have focused on three research
questions. First, we asked which type of train-
ing corpus is best to assess sentence readability.
Whereas we found that using a set of manually
selected complex sentences was better than using
a simple corpus?based distinction, the extra ef-
fort needed to construct the training corpus might
not be worthwhile as observed improvements were
quite modest. However, we did not consider a
more sensitive measure of the difficulty of a sen-
tence (such as a number ranging between 0 and
1), and this might be able to offer a more sub-
stantial improvement (at the cost of needing more
time to create the training material). Of course,
when the goal is to identify the best features for
assessing sentence readability, it does make sense
to have high?quality training data to prevent se-
lecting inadequate features. The second research
question involved identifying which features were
most useful for assessing sentence readability. Be-
sides raw text features, syntactic but also morpho?
syntactic features turned out to play a central role
to achieve adequate performance. The third re-
search question investigated the overlap between
the features needed for document and sentence
readability classification. Whereas there certainly
was overlap between the top features (with dif-
ferent levels of performance), most of the fea-
tures had a different rank across the two tasks,
with local features being more predictive for sen-
tence classification and global ones for documents.
This suggests that the sentence readability task is
more complex than assessing document readabil-
ity, given that there is much less information avail-
able for a sentence than for a document.
171
Acknowledgments
The research reported in this paper was carried out
in the framework of the Short Term Mobility pro-
gram of international exchanges funded by CNR
(Italy). We thank Dani?el de Kok for his help in
applying TINYEST to our data and Giulia Benotto
for her help in manual revision of training data.
References
Sandra M. Alu??sio, Lucia Specia, Thiago A.S. Pardo,
Erick G. Maziero, and Renata P.M. Fortes. 2008.
Towards brazilian portuguese automatic text simpli-
fication systems. In Proceedings of the Eighth ACM
Symposium on Document Engineering, pages 240?
248.
Sandra Alu??sio, Lucia Specia, Caroline Gasperin, and
Carolina Scarton. 2010. Readability assessment for
text simplification. In Proceedings of the NAACL
HLT 2010 Fifth Workshop on Innovative Use of NLP
for Building Educational Applications, pages 1?9.
?istein E. Andersen, Helen Yannakoudakis, Fiona
Barker, and Tim Parish. 2013. Developing and
testing a self-assessment and tutoring system. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 32?41.
Giuseppe Attardi. 2006. Experiments with a multi-
language non-projective dependency parser. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning (CoNLL-X ?06), New
York City, New York, pages 166?170.
Gianni Barlacchi and Sara Tonelli. 2013. Ernesta: A
sentence simplification tool for children?s stories in
italian. In Proceedings of the 14th Conferences on
Computational Linguistics and Natural Language
Processing (CICLing 2013), pages 476?487.
Regina Barzilay and Mirella Lapata. 2008. Model-
ing local coherence: An entity-based approach. vol-
ume 34.
Stefan Bott and Horacio Saggion. 2011. An un-
supervised alignment algorithm for text simplifica-
tion corpus construction. In Proceedings of the
Workshop on Monolingual Text-To-Text Generation,
pages 20?26.
Dani?el de Kok. 2011. Discriminative features in
reversible stochastic attribute-value grammars. In
Proceedings of the EMNLP Workshop on Language
Generation and Evaluation, pages 54?63. Associa-
tion for Computational Linguistics.
Dani?el de Kok. 2013. Reversible Stochastic Attribute-
Value Grammars. Ph.D. thesis, Rijksuniversiteit
Groningen.
Tullio De Mauro. 2000. Il dizionario della lingua ital-
iana. Paravia, Torino.
Felice Dell?Orletta, Simonetta Montemagni, and Giu-
lia Venturi. 2011. Read-it: Assessing readability
of italian texts with a view to text simplification. In
Proceedings of the Workshop on Speech and Lan-
guage Processing for Assistive Technologies (SLPAT
2011), pages 73?83.
Felice Dell?Orletta, Simonetta Montemagni, and Giulia
Venturi. 2014. Assessing document and sentence
readability in less resourced languages and across
textual genres. In International Journal of Applied
Linguistics (ITL). Special Issue on Readability and
Text Simplification. To appear.
Felice Dell?Orletta. 2009. Ensemble system for part-
of-speech tagging. In Proceedings of Evalita?09,
Evaluation of NLP and Speech Tools for Italian,
Reggio Emilia, December.
Biljana Drndarevi?c, Sanja
?
Stajner, Stefan Bott, Susana
Bautista, and Horacio Saggion. 2013. Automatic
text simplification in spanish: A comparative evalu-
ation of complementing modules. In Computational
Linguistics and Intelligent Text Processing, pages
488?500. Springer Berlin Heidelberg.
Johan Falkenjack and Arne J?onsson. 2014. Classify-
ing easy-to-read texts without parsing. In Proceed-
ings of the Proceedings of the 3rd Workshop on Pre-
dicting and Improving Text Readability for Target
Reader Populations (PITR), Gothenburg, Sweden.
Association for Computational Linguistics.
Lijun Feng, Martin Jansche, Matt Huenerfauth, and
No?emie Elhadad. 2010. A comparison of features
for automatic readability assessment. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (COLING 2010), pages 276?
284.
Thomas Franc?ois and C?edrick Fairon. 2012. An ?AI
readability? formula for french as a foreign lan-
guage. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, Jeju Island, Korea, pages 466?477.
Michael J. Heilman, Kevyn Collins, and Jamie Callan.
2007. Combining lexical and grammatical features
to improve readability measures for first and second
language texts. In Proceedings of the Human Lan-
guage Technology Conference, pages 460?467.
Kentaro Inui, Atsushi Fujita, Tetsuro Takahashi, Ryu
Iida, and Tomoya Iwakura. 2003. Text simplifica-
tion for reading assistance: A project note. In Pro-
ceedings of the Second International Workshop on
Paraphrasing, pages 9?16.
Rohit J. Kate, Xiaoqiang Luo, Siddharth Patwardhan,
Martin Franz, Radu Florian, Raymond J. Mooney,
Salim Roukos, and Chris Welty. 2010. Learning to
172
predict readability using diverse linguistic features.
In oceedings of the 23rd International Conference
on Computational Linguistics, pages 546?554.
J. Peter Kincaid, Lieutenant Robert P. Fishburne,
Richard L. Rogers, and Brad S. Chissom. 1975.
Derivation of new readability formulas for navy
enlisted personnel. In Research Branch Report,
Millington, TN: Chief of Naval Training, pages 8?
75.
Annie Louis and Ani Nenkova. 2013. A corpus of sci-
ence journalism for analysing writing quality. vol-
ume 4.
Simon Perkins, Kevin Lacker, and James Theiler.
2003. Grafting: Fast, incremental feature selection
by gradient descent in function space. The Journal
of Machine Learning Research, 3:1333?1356.
Maria Emanuela Piemontese. 1996. Capire e farsi
capire. Teorie e tecniche della scrittura controllata.
Tecnodid, Napoli.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 186?195.
Sarah E. Schwarm and Mari Ostendorf. 2005. Reading
level assessment using support vector machines and
statistical language models. In Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics (ACL 05), pages 523?530.
Fadi Abu Sheikha and Diana Inkpen. 2012. Learning
to classify documents according to formal and infor-
mal style. volume 8.
Johan Sj?oholm. 2012. Probability as readability: A
new machine learning approach to readability as-
sessment for written Swedish. LiU Electronic Press,
Master thesis.
Adam Skory and Maxine Eskenazi. 2010. Predicting
cloze task quality for vocabulary training. In Pro-
ceedings of the NAACL HLT 2010 Fifth Workshop
on Innovative Use of NLP for Building Educational
Applications, pages 49?56.
Sara Tonelli, Ke Tran Manh, and Emanuele Pianta.
2012. Making readability indices readable. In Pro-
ceedings of the First Workshop on Predicting and
Improving Text Readability for Target Reader Popu-
lations, pages 40?48.
Sowmya Vajjala and Detmar Meurers. 2014. On as-
sessing the reading level of individual sentences for
text simplification. In Proceedings of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL-14), Gothen-
burg, Sweden. Association for Computational Lin-
guistics.
Sanja
?
Stajner and Horacio Saggion. 2013. Readabil-
ity indices for automatic evaluation of text simplifi-
cation systems: A feasibility study for spanish. In
Proceedings of the International Joint Conference
on Natural Language Processing.
173
