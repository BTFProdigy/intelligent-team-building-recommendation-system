Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 208?215,
New York, June 2006. c?2006 Association for Computational Linguistics
Learning to Detect Conversation Focus of Threaded Discussions 
 
 
Donghui Feng        Erin Shaw        Jihie Kim        Eduard Hovy 
Information Sciences Institute 
University of Southern California 
Marina del Rey, CA, 90292 
{donghui, shaw, jihie, hovy}@isi.edu 
 
 
Abstract 
In this paper we present a novel feature-
enriched approach that learns to detect the 
conversation focus of threaded discus-
sions by combining NLP analysis and IR 
techniques. Using the graph-based algo-
rithm HITS, we integrate different fea-
tures such as lexical similarity, poster 
trustworthiness, and speech act analysis of 
human conversations with feature-
oriented link generation functions. It is 
the first quantitative study to analyze hu-
man conversation focus in the context of 
online discussions that takes into account 
heterogeneous sources of evidence. Ex-
perimental results using a threaded dis-
cussion corpus from an undergraduate 
class show that it achieves significant per-
formance improvements compared with 
the baseline system. 
1 Introduction 
Threaded discussion is popular in virtual cyber 
communities and has applications in areas such as 
customer support, community development, inter-
active reporting (blogging) and education. Discus-
sion threads can be considered a special case of 
human conversation, and since we have huge re-
positories of such discussion, automatic and/or 
semi-automatic analysis would greatly improve the 
navigation and processing of the information.  
A discussion thread consists of a set of messages 
arranged in chronological order. One of the main 
challenges in the Question Answering domain is 
how to extract the most informative or important 
message in the sequence for the purpose of answer-
ing the initial question, which we refer to as the 
conversation focus in this paper. For example, 
people may repeatedly discuss similar questions in 
a discussion forum and so it is highly desirable to 
detect previous conversation focuses in order to 
automatically answer queries (Feng et al, 2006).  
Human conversation focus is a hard NLP (Natu-
ral Language Processing) problem in general be-
cause people may frequently switch topics in a real 
conversation. The threaded discussions make the 
problem manageable because people typically fo-
cus on a limited set of issues within a thread of a 
discussion. Current IR (Information Retrieval) 
techniques are based on keyword similarity meas-
ures and do not consider some features that are 
important for analyzing threaded discussions. As a 
result, a typical IR system may return a ranked list 
of messages based on keyword queries even if, 
within the context of a discussion, this may not be 
useful or correct. 
Threaded discussion is a special case of human 
conversation, where people may express their 
ideas, elaborate arguments, and answer others? 
questions; many of these aspects are unexplored by 
traditional IR techniques. First, messages in 
threaded discussions are not a flat document set, 
which is a common assumption for most IR sys-
tems. Due to the flexibility and special characteris-
tics involved in human conversations, messages 
within a thread are not necessarily of equal impor-
tance. The real relationships may differ from the 
analysis based on keyword similarity measures, 
e.g., if a 2nd message ?corrects? a 1st one, the 2nd 
message is probably more important than the 1st. 
IR systems may give different results. Second, 
messages posted by different users may have dif-
ferent degrees of correctness and trustworthiness, 
which we refer to as poster trustworthiness in this 
paper. For instance, a domain expert is likely to be 
more reliable than a layman on the domain topic.  
208
In this paper we present a novel feature-enriched 
approach that learns to detect conversation focus of 
threaded discussions by combining NLP analysis 
and IR techniques. Using the graph-based algo-
rithm HITS (Hyperlink Induced Topic Search, 
Kleinberg, 1999), we conduct discussion analysis 
taking into account different features, such as lexi-
cal similarity, poster trustworthiness, and speech 
act relations in human conversations. We generate 
a weighted threaded discussion graph by applying 
feature-oriented link generation functions. All the 
features are quantified and integrated as part of the 
weight of graph edges. In this way, both quantita-
tive features and qualitative features are combined 
to analyze human conversations, specifically in the 
format of online discussions. 
To date, it is the first quantitative study to ana-
lyze human conversation that focuses on threaded 
discussions by taking into account heterogeneous 
evidence from different sources. The study de-
scribed here addresses the problem of conversation 
focus, especially for extracting the best answer to a 
particular question, in the context of an online dis-
cussion board used by students in an undergraduate 
computer science course. Different features are 
studied and compared when applying our approach 
to discussion analysis. Experimental results show 
that performance improvements are significant 
compared with the baseline system. 
The remainder of this paper is organized as fol-
lows: We discuss related work in Section 2. Sec-
tion 3 presents thread representation and the 
weighted HITS algorithm. Section 4 details fea-
ture-oriented link generation functions. Compara-
tive experimental results and analysis are given in 
Section 5. We discuss future work in Section 6. 
2 Related Work 
Human conversation refers to situations where two 
or more participants freely alternate in speaking 
(Levinson, 1983). What makes threaded discus-
sions unique is that users participate asynchro-
nously and in writing. We model human 
conversation as a set of messages in a threaded 
discussion using a graph-based algorithm. 
Graph-based algorithms are widely applied in 
link analysis and for web searching in the IR com-
munity. Two of the most prominent algorithms are 
Page-Rank (Brin and Page, 1998) and the HITS 
algorithm (Kleinberg, 1999). Although they were 
initially proposed for analyzing web pages, they 
proved useful for investigating and ranking struc-
tured objects. Inspired by the idea of graph based 
algorithms to collectively rank and select the best 
candidate, research efforts in the natural language 
community have applied graph-based approaches 
on keyword selection (Mihalcea and Tarau, 2004), 
text summarization (Erkan and Radev, 2004; Mi-
halcea, 2004), word sense disambiguation (Mihal-
cea et al, 2004; Mihalcea, 2005), sentiment 
analysis (Pang and Lee, 2004), and sentence re-
trieval for question answering (Otterbacher et al, 
2005). However, until now there has not been any 
published work on its application to human con-
versation analysis specifically in the format of 
threaded discussions. In this paper, we focus on 
using HITS to detect conversation focus of 
threaded discussions.  
Rhetorical Structure Theory (Mann and Thom-
son, 1988) based discourse processing has attracted 
much attention with successful applications in sen-
tence compression and summarization. Most of the 
current work on discourse processing focuses on 
sentence-level text organization (Soricut and 
Marcu, 2003) or the intermediate step (Sporleder 
and Lapata, 2005). Analyzing and utilizing dis-
course information at a higher level, e.g., at the 
paragraph level, still remains a challenge to the 
natural language community. In our work, we util-
ize the discourse information at a message level. 
Zhou and Hovy (2005) proposed summarizing 
threaded discussions in a similar fashion to multi-
document summarization; but then their work does 
not take into account the relative importance of 
different messages in a thread. Marom and Zuker-
man (2005) generated help-desk responses using 
clustering techniques, but their corpus is composed 
of only two-party, two-turn, conversation pairs, 
which precludes the need to determine relative im-
portance as in a multi-ply conversation. 
In our previous work (Feng et al, 2006), we im-
plemented a discussion-bot to automatically an-
swer student queries in a threaded discussion but 
extract potential answers (the most informative 
message) using a rule-based traverse algorithm that 
is not optimal for selecting a best answer; thus, the 
result may contain redundant or incorrect informa-
tion. We argue that pragmatic knowledge like 
speech acts is important in conversation focus 
analysis. However, estimated speech act labeling 
between messages is not sufficient for detecting 
209
human conversation focus without considering 
other features like author information. Carvalho 
and Cohen (2005) describe a dependency-network 
based collective classification method to classify 
email speech acts. Our work on conversation focus 
detection can be viewed as an immediate step fol-
lowing automatic speech act labeling on discussion 
threads using similar collective classification ap-
proaches. 
We next discuss our approach to detect conver-
sation focus using the graph-based algorithm HITS 
by taking into account heterogeneous features. 
3 Conversation Focus Detection 
In threaded discussions, people participate in a 
conversation by posting messages. Our goal is to 
be able to detect which message in a thread con-
tains the most important information, i.e., the focus 
of the conversation. Unlike traditional IR systems, 
which return a ranked list of messages from a flat 
document set, our task must take into account 
characteristics of threaded discussions.  
First, messages play certain roles and are related 
to each other by a conversation context. Second, 
messages written by different authors may vary in 
value. Finally, since postings occur in parallel, by 
various people, message threads are not necessarily 
coherent so the lexical similarity among the mes-
sages should be analyzed. To detect the focus of 
conversation, we integrate a pragmatics study of 
conversational speech acts, an analysis of message 
values based on poster trustworthiness and an 
analysis of lexical similarity. The subsystems that 
determine these three sources of evidence comprise 
the features of our feature-based system. 
Because each discussion thread is naturally rep-
resented by a directed graph, where each message 
is represented by a node in the graph, we can apply 
a graph-based algorithm to integrate these sources 
and detect the focus of conversation. 
3.1 Thread Representation 
A discussion thread consists of a set of messages 
posted in chronological order. Suppose that each 
message is represented by mi, i =1,2,?, n. Then 
the entire thread is a directed graph that can be rep-
resented by G= (V, E), where V is the set of nodes 
(messages), V= {mi,i=1,...,n}, and E is the set of 
directed edges. In our approach, the set V is auto-
matically constructed as each message joins in the 
discussion. E is a subset of VxV. We will discuss 
the feature-oriented link generation functions that 
construct the set E in Section 4. 
We make use of speech act relations in generat-
ing the links. Once a speech act relation is identi-
fied between two messages, links will be generated 
using generation functions described in next sec-
tion. When mi is a message node in the thread 
graph, VmF i ?)( represents the set of nodes that 
node mi points to (i.e., children of mi), and 
VmB i ?)( represents the set of nodes that point to 
mi (i.e., parents of mi). 
3.2 Graph-Based Ranking Algorithm: HITS 
Graph-based algorithms can rank a set of objects in 
a collective way and the affect between each pair 
can be propagated into the whole graph iteratively. 
Here, we use a weighted HITS (Kleinberg, 1999) 
algorithm to conduct message ranking. 
Kleinberg (1999) initially proposed the graph-
based algorithm HITS for ranking a set of web 
pages. Here, we adjust the algorithm for the task of 
ranking a set of messages in a threaded discussion. 
In this algorithm, each message in the graph can be 
represented by two identity scores, hub score and 
authority score. The hub score represents the qual-
ity of the message as a pointer to valuable or useful 
messages (or resources, in general). The authority 
score measures the quality of the message as a re-
source itself. The weighted iterative updating com-
putations are shown in Equations 1 and 2. ?
?
+ =
)(
1 )(*)(
ij mFm
j
r
iji
r mauthoritywmhub           (1) 
?
?
+ =
)(
1 )(*)(
ij mBm
j
r
jii
r mhubwmauthority           (2) 
where r and r+1 are the numbers of iterations. 
The number of iterations required for HITS to 
converge depends on the initialization value for 
each message node and the complexity of the 
graph. Graph links can be induced with extra 
knowledge (e.g. Kurland and Lee, 2005). To help 
integrate our heterogeneous sources of evidence 
with our graph-based HITS algorithm, we intro-
duce link generation functions for each of the three 
features, (gi, i=1, 2, 3), to add links between mes-
sages. 
4 Feature-Oriented Link Generation 
210
Conversation structures have received a lot of at-
tention in the linguistic research community (Lev-
inson, 1983). In order to integrate conversational 
features into our computational model, we must 
convert a qualitative analysis into quantitative 
scores. For conversation analysis, we adopted the 
theory of Speech Acts proposed by (Austin, 1962; 
Searle, 1969) and defined a set of speech acts (SAs) 
that relate every pair of messages in the corpus. 
Though a pair of messages may only be labeled 
with one speech act, a message can have multiple 
SAs with other messages. 
We group speech acts by function into three 
categories, as shown in Figure 1. Messages may 
involve a request (REQ), provide information 
(INF), or fall into the category of interpersonal 
(INTP) relationship. Categories can be further di-
vided into several single speech acts.  
 
Figure 1. Categories of Message Speech Act. 
The SA set for our corpus is given in Table 1. A 
speech act may a represent a positive, negative or 
neutral response to a previous message depending 
on its attitude and recommendation. We classify 
each speech act as a direction as POSITIVE (+), 
NEGATIVE (?) or NEUTRAL, referred to as SA 
Direction, as shown in the right column of Table 1. 
The features we wish to include in our approach 
are lexical similarity between messages, poster 
trustworthiness, and speech act labels between 
message pairs in our discussion corpus.  
The feature-oriented link generation is con-
ducted in two steps. First, our approach examines 
in turn all the speech act relations in each thread 
and generates two types of links based on lexical 
similarity and SA strength scores. Second, the sys-
tem iterates over all the message nodes and assigns 
each node a self-pointing link associated with its 
poster trustworthiness score. The three features are 
integrated into the thread graph accordingly by the 
feature-oriented link generation functions. Multiple 
links with the same start and end points are com-
bined into one. 
Speech
Act Name Description Dir. 
ACK Acknowl-edge 
Confirm or             
acknowledge + 
CANS Complex Answer 
Give answer requiring a 
full description of pro-
cedures, reasons, etc. 
 
COMM Command Command or            announce  
COMP Compli-ment 
Praise an argument or 
suggestion + 
CORR Correct Correct a wrong answer or solution ? 
CRT Criticize Criticize an argument ? 
DESC Describe Describe a fact or    situation  
ELAB Elaborate Elaborate on a previous argument or question  
OBJ Object Object to an argument or suggestion ? 
QUES Question Ask question about a specific problem  
SANS Simple Answer 
Answer with a short 
phrase or few words      
(e.g. factoid, yes/no) 
 
SUG Suggest Give advice or suggest a solution  
SUP Support Support an argument or suggestion + 
Table 1. Types of message speech acts in corpus. 
4.1 Lexical Similarity 
Discussions are constructed as people express 
ideas, opinions, and thoughts, so that the text itself 
contains information about what is being dis-
cussed. Lexical similarity is an important measure 
for distinguishing relationships between message 
pairs. In our approach, we do not compute the lexi-
cal similarity of any arbitrary pair of messages, 
instead, we consider only message pairs that are 
present in the speech act set. The cosine similarity 
between each message pair is computed using the 
TF*IDF technique (Salton, 1989). 
Messages with similar words are more likely to 
be semantically-related. This information is repre-
sented by term frequency (TF). However, those 
Inform:    
INF 
Interpersonal: 
INTP 
COMM  
QUES  
Speech 
Act Request: 
REQ 
ACK 
COMP 
CRT  
OBJ  
SUP  
CANS 
CORR 
DESC 
ELAB 
SANS 
SUG 
211
with more general terms may be unintentionally 
biased when only TF is considered so Inverse 
Document Frequency (IDF) is introduced to miti-
gate the bias. The lexical similarity score can be 
calculated using their cosine similarity. 
),(cos_ ji
l mmsimW =                      (3) 
For a given a speech act, SAij(mi?mj), connect-
ing message mi and mj, the link generation function 
g1 is defined as follows:  
)()(1
l
ijij WarcSAg =                          (4) 
The new generated link is added to the thread 
graph connecting message node mi and mj with a 
weight of Wl. 
4.2 Poster Trustworthiness 
Messages posted by different people may have dif-
ferent degrees of trustworthiness. For example, 
students who contributed to our corpus did not 
seem to provide messages of equal value. To de-
termine the trustworthiness of a person, we studied 
the responses to their messages throughout the en-
tire corpus. We used the percentage of POSITIVE 
responses to a person?s messages to measure that 
person?s trustworthiness. In our case, POSITIVE 
responses, which are defined above, included SUP, 
COMP, and ACK. In addition, if a person?s mes-
sage closed a discussion, we rated it POSITIVE. 
Suppose the poster is represented by kperson , 
the poster score, pW , is a weight calculated by 
))((
))(_(
)(
k
k
k
p
personfeedbackcount
personfeedbackpositivecount
personW =  
                                                                        (5) 
For a given single speech act, SAij(mi?mj), the 
poster score indicates the importance of message 
mi by itself and the generation function is given by  
)()(2
p
iiij WarcSAg =                               (6) 
The generated link is self-pointing, and contains 
the strength of the poster information. 
4.3 Speech Act Analysis 
We compute the strength of each speech act in a 
generative way, based on the author and trustwor-
thiness of the author. The strength of a speech act 
is a weighted average over all authors. 
)(
)(
)(
)()( k
P
person
persons personW
SAcount
SAcount
dirsignSAW
k
k?= (7) 
where the sign function of direction is defined with 
Equation 8. 
??
??=
             Otherwise     1
NEGATIVE isdir  if     1
)(dirsign                      (8) 
All SA scores are computed using Equation 7 
and projected to [0, 1]. For a given speech act, 
SAij(mi?mj), the generation function will generate 
a weighted link in the thread graph as expressed in 
Equation 9. 
??
??
?=
               Otherwise      )(
NEUTRAL is  if      )(
)(3 s
ij
ij
s
ii
ij Warc
SAWarc
SAg     (9) 
The SA scores represent the strength of the rela-
tionship between the messages. Depending on the 
direction of the SA, the generated link will either 
go from message mi to mj or from message mi to mi 
(i.e., to itself). If the SA is NEUTRAL, the link will 
point to itself and the score is a recommendation to 
itself. Otherwise, the link connects two different 
messages and represents the recommendation de-
gree of the parent to the child message. 
5 Experiments 
5.1 Experimental Setup  
We tested our conversation-focus detection ap-
proach using a corpus of threaded discussions from 
three semesters of a USC undergraduate course in 
computer science. The corpus includes a total of 
640 threads consisting of 2214 messages, where a 
thread is defined as an exchange containing at least 
two messages. 
Length of thread Number of threads 
3 139 
4 74 
5 47 
6 30 
7 13 
8 11 
Table 2. Thread length distribution. 
From the complete corpus, we selected only 
threads with lengths of greater than two and less 
than nine (messages). Discussion threads with 
lengths of only two would bias the random guess 
of our baseline system, while discussion threads 
with lengths greater than eight make up only 3.7% 
of the total number of threads (640), and are the 
least coherent of the threads due to topic-switching 
and off-topic remarks. Thus, our evaluation corpus 
included 314 threads, consisting of 1307 messages, 
with an average thread length of 4.16 messages per 
212
thread. Table 2 gives the distribution of the lengths 
of the threads. 
The input of our system requires the identifica-
tion of speech act relations between messages. Col-
lective classification approaches, similar to the 
dependency-network based approach that Carvalho 
and Cohen (2005) used to classify email speech 
acts, might also be applied to discussion threads. 
However, as the paper is about investigating how 
an SA analysis, along with other features, can 
benefit conversation focus detection, so as to avoid 
error propagation from speech act labeling to sub-
sequent processing, we used manually-annotated 
SA relationships for our analysis. 
Code Frequency Percentage (%) 
ACK 53 3.96 
CANS 224 16.73 
COMM 8 0.6 
COMP 7 0.52 
CORR 20 1.49 
CRT 23 1.72 
DESC 71 5.3 
ELAB 105 7.84 
OBJ 21 1.57 
QUES 450 33.61 
SANS 23 1.72 
SUG 264 19.72 
SUP 70 5.23 
Table 3. Frequency of speech acts. 
The corpus contains 1339 speech acts. Table 3 
gives the frequencies and percentages of speech 
acts found in the data set. Each SA generates fea-
ture-oriented weighted links in the threaded graph 
accordingly as discussed previously. 
 
Number of best     
answers 
Number of threads 
1 250 
2 56 
3 5 
4 3 
Table 4. Gold standard length distribution. 
We then read each thread and choose the mes-
sage that contained the best answer to the initial 
query as the gold standard. If there are multiple 
best-answer messages, all of them will be ranked 
as best, i.e., chosen for the top position. For exam-
ple, different authors may have provided sugges-
tions that were each correct for a specified 
situation. Table 4 gives the statistics of the num-
bers of correct messages of our gold standard. 
We experimented with further segmenting the 
messages so as to narrow down the best-answer 
text, under the assumption that long messages 
probably include some less-than-useful informa-
tion. We applied TextTiling (Hearst, 1994) to seg-
ment the messages, which is the technique used by 
Zhou and Hovy (2005) to summarize discussions. 
For our corpus, though, the ratio of segments to 
messages was only 1.03, which indicates that our 
messages are relatively short and coherent, and that 
segmenting them would not provide additional 
benefits. 
5.2 Baseline System 
To compare the effectiveness of our approach with 
different features, we designed a baseline system 
that uses a random guess approach. Given a dis-
cussion thread, the baseline system randomly se-
lects the most important message. The result was 
evaluated against the gold standard. The perform-
ance comparisons of the baseline system and other 
feature-induced approaches are presented next. 
5.3 Result Analysis and Discussion 
We conducted extensive experiments to investigate 
the performance of our approach with different 
combinations of features. As we discussed in Sec-
tion 4.2, each poster acquires a trustworthiness 
score based on their behavior via an analysis of the 
whole corpus. Table 5 is a sample list of some 
posters with their poster id, the total number of 
responses (to their messages), the total number of 
positive responses, and their poster scores pW . 
 
Poster 
ID 
    Total 
Response 
  Positive 
Response  
pW  
193 1 1 1 
93 20 18 0.9 
38 15 12 0.8 
80 8 6 0.75 
47 253 182 0.719 
22 3 2 0.667 
44 9 6 0.667 
91 6 4 0.667 
147 12 8 0.667 
32 10 6 0.6 
190 9 5 0.556 
97 20 11 0.55 
12 2 1 0.5 
Table 5. Sample poster scores. 
213
Based on the poster scores, we computed the 
strength score of each SA with Equation 7 and pro-
jected them to [0, 1]. Table 6 shows the strength 
scores for all of the SAs. Each SA has a different 
strength score and those in the NEGATIVE cate-
gory have smaller ones (weaker recommendation). 
 
SA )(SAWs  SA )(SAWs  
CANS 0.8134 COMM 0.6534 
DESC 0.7166 ELAB 0.7202 
SANS 0.8281 SUG 0.8032 
QUES 0.6230   
ACK 0.6844 COMP 0.8081 
SUP 0.8057   
CORR 0.2543 CRT 0.1339 
OBJ 0.2405   
Table 6. SA strength scores. 
We tested the graph-based HITS algorithm with 
different feature combinations and set the error rate 
to be 0.0001 to get the algorithm to converge. In 
our experiments, we computed the precision score 
and the MRR (Mean Reciprocal Rank) score 
(Voorhees, 2001) of the most informative message 
chosen (the first, if there was more than one). Ta-
ble 7 shows the performance scores for the system 
with different feature combinations. The perform-
ance of the baseline system is shown at the top. 
The HITS algorithm assigns both a hub score 
and an authority score to each message node, re-
sulting in two sets of results. Scores in the HITS_ 
AUTHORITY rows of Table 7 represent the re-
sults using authority scores, while HITS_HUB 
rows represent the results using hub scores.  
Due to the limitation of thread length, the lower 
bound of the MRR score is 0.263. As shown in the 
table, a random guess baseline system can get a 
precision of 27.71% and a MRR score of 0.539.  
When we consider only lexical similarity, the 
result is not so good, which supports the notion 
that in human conversation context is often more 
important than text at a surface level. When we 
consider poster and lexical score together, the per-
formance improves. As expected, the best per-
formances use speech act analysis. More features 
do not always improve the performance, for exam-
ple, the lexical feature will sometimes decrease 
performance. Our best performance produced a 
precision score of 70.38% and an MRR score of 
0.825, which is a significant improvement over the 
baseline?s precision score of 27.71% and its MRR 
score of 0.539. 
Algorithm  & 
Features 
Correct   
(out of 314) 
Precision 
(%) MRR 
Baseline 87 27.71 0.539 
Lexical 65 20.70 0.524 
Poster 90 28.66 0.569 
SA 215 68.47 0.819 
Lexical +  
Poster 91 28.98 0.565 
Lexical +      
SA 194 61.78 0.765 
Poster +        
SA 221 70.38 0.825 H
IT
S_
A
U
T
H
O
R
IT
Y
 
Lexical +  
Poster + 
SA 
212 67.52 0.793 
Lexical 153 48.73 0.682 
Poster 79 25.16 0.527 
SA 195 62.10 0.771 
Lexical +  
Poster 158 50.32 0.693 
Lexical +      
SA 177 56.37 0.724 
Poster +       
SA 207 65.92 0.793 
H
IT
S_
H
U
B
 
Lexical + 
Poster + 
SA 
196 62.42 0.762 
Table 7. System Performance Comparison. 
Another widely-used graph algorithm in IR is 
PageRank (Brin and Page, 1998). It is used to in-
vestigate the connections between hyperlinks in 
web page retrieval. PageRank uses a ?random 
walk? model of a web surfer?s behavior. The surfer 
begins from a random node mi and at each step 
either follows a hyperlink with the probability of d, 
or jumps to a random node with the probability of 
(1-d). A weighted PageRank algorithm is used to 
model weighted relationships of a set of objects. 
The iterative updating expression is 
? ??
?
+ +?=
)(
)(
1 )(*)1()(
ij
jk
mBm
j
r
mFm
jk
ji
i
r mPR
w
w
ddmPR  (10) 
where r and r+1 are the numbers of iterations. 
 
We also tested this algorithm in our situation, 
but the best performance had a precision score of 
only 47.45% and an MRR score of 0.669. It may 
be that PageRank?s definition and modeling ap-
proach does not fit our situation as well as the 
HITS approach. In HITS, the authority and hub- 
214
based approach is better suited to human conversa-
tion analysis than PageRank, which only considers 
the contributions from backward links of each 
node in the graph. 
6 Conclusions and Future Work 
We have presented a novel feature-enriched ap-
proach for detecting conversation focus of threaded 
discussions for the purpose of answering student 
queries. Using feature-oriented link generation and 
a graph-based algorithm, we derived a unified 
framework that integrates heterogeneous sources 
of evidence. We explored the use of speech act 
analysis, lexical similarity and poster trustworthi-
ness to analyze discussions. 
From the perspective of question answering, this 
is the first attempt to automatically answer com-
plex and contextual discussion queries beyond fac-
toid or definition questions. To fully automate 
discussion analysis, we must integrate automatic 
SA labeling together with our conversation focus 
detection approach. An automatic system will help 
users navigate threaded archives and researchers 
analyze human discussion. 
Supervised learning is another approach to de-
tecting conversation focus that might be explored. 
The tradeoff and balance between system perform-
ance and human cost for different learning algo-
rithms is of great interest. We are also exploring 
the application of graph-based algorithms to other 
structured-objects ranking problems in NLP so as 
to improve system performance while relieving 
human costs. 
Acknowledgements 
The work was supported in part by DARPA grant DOI-
NBC Contract No. NBCHC050051, Learning by Read-
ing, and in part by a grant from the Lord Corporation 
Foundation to the USC Distance Education Network. 
The authors want to thank Deepak Ravichandran, Feng 
Pan, and Rahul Bhagat for their helpful suggestions 
with the manuscript. We would also like to thank the 
HLT-NAACL reviewers for their valuable comments. 
References 
Austin, J. 1962. How to do things with words. Cam-
bridge, Massachusetts: Harvard Univ. Press. 
Brin, S. and Page, L. 1998. The anatomy of a large-
scale hypertextual web search engine. Computer 
Networks and ISDN Systems, 30(1-7):107--117. 
Carvalho, V.R. and Cohen, W.W. 2005. On the collec-
tive classification of email speech acts. In Proceed-
ings of SIGIR-2005, pp. 345-352. 
Erkan, G. and Radev, D. 2004. Lexrank: graph-based 
centrality as salience in text summarization. Journal 
of Artificial Intelligence Research (JAIR). 
Feng, D., Shaw, E., Kim, J., and Hovy, E.H. 2006. An 
intelligent discussion-bot for answering student que-
ries in threaded discussions. In Proceedings of Intel-
ligent User Interface (IUI-2006), pp. 171-177.  
Hearst, M.A. 1994. Multi-paragraph segmentation of 
expository text. In Proceedings of ACL-1994.  
Kleinberg, J. 1999. Authoritative sources in a hyper-
linked environment. Journal of the ACM, 46(5). 
Kurland, O. and Lee L. 2005. PageRank without hyper-
links: Structural re-ranking using links induced by 
language models. In Proceedings of SIGIR-2005.  
Levinson, S. 1983. Pragmatics. Cambridge Univ. Press. 
Mann, W.C. and Thompson, S.A. 1988. Rhetorical 
structure theory: towards a functional theory of text 
organization. Text, 8 (3), pp. 243-281. 
Marom, Y. and Zukerman, I. 2005. Corpus-based gen-
eration of easy help-desk responses. Technical Re-
port, Monash University. Available at: 
http://www.csse.monash.edu.au/publications/2005/tr-
2005-166-full.pdf. 
Mihalcea, R. 2004. Graph-based ranking algorithms for 
sentence extraction, applied to text summarization. In 
Companion Volume to ACL-2004. 
Mihalcea, R. 2005. unsupervised large-vocabulary word 
sense disambiguation with graph-based algorithms 
for sequence data labeling. In HLT/EMNLP 2005. 
Mihalcea, R. and Tarau, P. 2004. TextRank: bringing 
order into texts. In Proceedings of EMNLP 2004. 
Mihalcea, R., Tarau, P. and Figa, E. 2004. PageRank on 
semantic networks, with application to word sense 
disambiguation. In Proceedings of COLING 2004. 
Otterbacher, J., Erkan, G., and  Radev, D. 2005. Using 
random walks for question-focused sentence re-
trieval. In Proceedings of HLT/EMNLP 2005.  
Pang, B. and Lee, L. 2004. A sentimental education: 
sentiment analysis using subjectivity summarization 
based on minimum cuts. In ACL-2004. 
Salton, G. 1989. Automatic Text Processing, The Trans-
formation, Analysis, and Retrieval of Information by 
Computer. Addison-Wesley, Reading, MA, 1989. 
Searle, J. 1969. Speech Acts. Cambridge: Cambridge 
Univ. Press. 
Soricut, R. and Marcu, D. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In Proceedings of HLT/NAACL-2003. 
Sporleder, C. and Lapata, M. 2005. Discourse chunking 
and its application to sentence compression. In Pro-
ceedings of HLT/EMNLP 2005. 
Voorhees, E.M. 2001. Overview of the TREC 2001 
question answering track. In TREC 2001. 
Zhou, L. and Hovy, E.H. 2005. Digesting virtual ?geek
?culture: the summarization of technical internet re-
lay chats. In Proceedings of ACL 2005. 
215
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 11?12,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Modeling Message Roles and Influence in Q&A Forums  Jeonhyung Kang and Jihie Kim University of Southern California  Information Sciences Institute 4676 Admiralty Was, Marina del Rey, CA, U.S.A {jeonhyuk, jihie}@isi.edu  Abstract 
We are modeling roles of individual messages and participants in Q&A discussion forums. In this paper, we present a mixed network model that represents message exchanges and message influences within a discussion thread.  We first model individual message roles and thread-level user roles using discussion content features. We then combine the resulting message roles and the user roles to generate the overall influence net-work. Message influences and their aggregation over the network are analyzed using B-centrality measures.  We use the results in identifying the most influential message in answering the initial question of the thread.  
1 Introduction Online discussion boards play an important role in various fields, including science, politics, and edu-cation. Understanding patterns of group interac-tions can be important in many applications. For example, in Q&A forums, some messages contain more useful or influential answers than others. Identifying useful content can help future discus-sions with similar issues. That is, useful informa-tion on a topic can be sent to related discussions (Kim et al, 2009). There has been some work on analyzing dia-logue patterns in online discussion boards (Feng et al 2006). Some of these model message roles us-ing dialogue acts such as question act or answer act. Most of these focus on modeling individual messages, often using surface forms.  There has been limited study on thread-level modeling of the true roles of the messages, whether they provide information (source) or seek information (sink), or which message in the thread is most useful or in-fluential as the source. In this paper, we present a novel model of mes-sage influence within a discussion thread.  In mod-
eling the thread-level influence of the messages, besides the sink/source role of the individual mes-sages, we take into account the roles of the mes-sage posters within the thread.  In Q&A discussion threads, since the roles of the posters as an infor-mation provider or an information seeker often do not change within the same thread, such informa-tion can help us identify the true roles or influence of the messages.  We combine the message roles and the user roles with a network model. Message influences and their aggregation over the network are ana-lyzed using B-centrality measures.  We use the resulting influence scores in identifying the most influential message in answering the initial ques-tion of the thread. 2 Modeling Message Influence  We use discussion data from an Operating Systems course in the Computer Science department at the University of Southern California. Students use a discussion board, most commonly, to seek help on the project assignments. For this study, we use data from the Fall 2007 semester, with 177 discussion threads (randomly choose 133 for training and 44 for test) with 580 messages (randomly choose 451 for training and 129 for test). 2.1 Sink and source roles of a message For each pair of messages where one is a reply to the other, we model the roles of the latter, as a sink or a source with respect to the former message or the message author. Some messages, especially long ones, can have both roles. Figure 1 shows an influence model of sink and source. A node repre-sents either a user or a message. An edge is either a reply-to relation between two messages or an own-ership of a message by a user. The direction of each edge indicates the direction of influence. A source is a message that provides information and it generates influence. In the top graph, B responds 
11
to A?s message as an information source. A sink message requests information from others so the edge direction goes towards it.  Note that sinks and sources are different from questions and answers since some of sources can take a form of a question (e.g. have you checked the manual?).       
Figure 1: Sink and source role of a message 2.2 Information seeker and information pro-vider: Thread level roles Figure 2 shows an influence model of an informa-tion seeker and an information provider when they exchange messages. The information flows from the information provider to the information seeker, as indicated by the direction of the edges. In gen-eral, the initial poster seeks information and his or her role does not usually change within the thread. Without loss of generality, we assume that within a discussion thread, a user?s role doesn?t change for a certain amount of time, although he or she can post both Sink and Source messages, as shown in Figure 2.  Using this model, we can capture the intention of the message based on who posted the message.           Figure 2: Thread-level user intention and message roles 2.3 Message and Participant Role Classifiers  Individual messages were annotated with sink and source information. The same message can have both sink and source roles with respect to the prior message or its poster.  
The features used include cue phases (n-grams) and their positions, message position in the thread, author change information, the relative po-sition of the message in the thread, a user?s partici-pation frequency (normalized), n-gram of previous messages with their positions and the message length. The details of these features are described in Kim et al(2009).  We used a Support Vector Machine (Chang and Lin 2001) to create two binary classifiers (since one message can have both roles) that iden-tify message roles: source and sink, and one binary classifier for user roles: information seeker and information provider. The precisions/recalls range from 0.89 to 0.96. F-scores are within 0.92-0.93. 3 Profiling Influence of Message with Cen-trality Measures  We generate a message-role graph and a user-role graph using the above model, and generate influ-ence network combining user and message roles. Message influences and their aggregation over the network are analyzed using B-centrality measures. To evaluate our source score accuracy (Ghosh and Lerman 2009), we annotated the most influential source message for the initial question (sink) in each thread. We use Mean Reciprocal Rank Score (MRR) to evaluate our results. The combined model provides better results with an MRR score of 0.90. Ranking strategy Information used MRR Influence Network Model score User role + msg role 0.90 Earlier source msg  msg role + msg location 0.74 Earlier msg from info providers  User role + msg location 0.68 Table 2: MRR scores for different strategies Acknowledgement This work was supported by National Science Foundation, CCLI Phase II (#0618859).  References  Feng, D. Shaw, E. Kim, J. and Hovy, E. An Intelligent Discussion-Bot for Answering Student Queries in Threaded Discussions, Proc. of IUI-2006. Ghosh, R. and Lerman, K. The structure of heterogeneous networks. Proc. of IEEE Social Comp. Conf, 2009. Kim, J., Li, J., and Kim, T. Identifying student online dis-cussions with unanswered questions,Proc. K-CAP 2009.  
12
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 13?14,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Towards Modeling Social and Content Dynamics in Discussion Forums
Jihie Kim and Aram Galstyan
Information Sciences Institute / Univesity of Southern California
Marina del Rey, CA, USA
{jihie,galstyan}@isi.edu
Extended Abstract
Recent years have witnessed the transformation of
the World Wide Web from an information-gathering
and processing tool into an interactive communica-
tion medium in the form of online discussion fo-
rums, chat?rooms, blogs, and so on. There is strong
evidence suggesting that social networks facilitate
new ways to interact with information in such me-
dia. Understanding the mechanisms and the patterns
of such interactions can be important for many ap-
plications. Currently, there is not much work that
adequately models interaction between social net-
works and information content. From the perspec-
tive of social network analysis, most existing work
is concerned with understanding static topological
properties of social networks represented by such
forums. For instance, Park and Maurer (2009) ap-
plied node clustering to identify consensus and con-
sensus facilitators, while Kang et al (2009) uses
discussion thread co-participation relations to iden-
tify (static) groups in discussions. On discussion
content analysis research side, there have been ap-
proaches for classifying messages with respect to di-
alogue roles (Carvalho and Cohen, 2005; Ravi and
Kim, 2007), but they often ignore the role and the
impact of underlying social interactions.
Thus, the current static network and content anal-
ysis approaches provide limited support for
? Capturing dynamics of social interactions: the
sequence of communication or who is respond-
ing to whom is important in understanding the
nature of interactions.
? Relating social interactions to content analysis:
the content can give hint on the nature of the in-
teraction and vice versa (e.g., users with more
social interactions are more likely to have com-
mon interests).
To address the above issues, one needs to go
beyond the static analysis approach, and develop
dynamical models that will explicitly account for
the interplay between the content of communication
(topics) and the structure of communications (social
networks). Such framework and corresponding al-
gorithmic base will allow us to infer ?polarizing?
topics discussed in forums, identify evolving com-
munities of interests, and examine the link between
social and content dynamics.
To illustrate the advantages and the need for more
fine?grained analysis, we now turn to a concrete ex-
ample. Figure 1(a) provides a sample of discus-
sion co-participation network from an online discus-
sion forum. Each oval node represents a user and
each square shows a discussion thread, while each
arrow represents users participating in the thread.
The numbers on the arrow represent the number of
messages contributed to the thread. Ten discussion
threads with 127 messages from 43 users are cap-
tured. Based on this network, we can identify users
that have similar interests, cluster topics and/or users
according to similarities, and so on. However, this
network is too coarse?grained to get additional in-
formation about the social interactions. For instance,
it does not say anything whether co?participating
users have similar or conflicting views.
We now contrast the above network with a more
fine?grained representation of forum dynamics. We
performed a thorough manual analysis of threads, by
taking into account the sequence of messages to con-
struct response?to graph, and then manually anno-
tating the attitudes of each message towards the one
it was responding to. Figure 1(b) provides a signed
attitude network from the same dataset as the one
used for Figure 1(a). Each node represents a user
and an arrow shows how one replies to the other.
13
 : a thread 
(a)
+ + 
+ 
+ 
- - 
(b)
Figure 1: (a) Thread participation network; (b) Signed
attitude network. In (b), the circles show two triangle
relationships suggested by structural balance theory.
The numbers on the arrow represent the number of
the reply?to occurrences, while the color of the link
represents the attitude. Here we use a very loose
definition of ?attitude?. Namely, positive (blue) at-
titude means that the posting user agrees with the
previous comment or message, or expresses friendly
sentiments. And negative attitude means disagree-
ing with the previous message or using outright of-
fensive language. The resulting signed network dif-
ferentiates links between the users (friends or foes).
Clearly, the resulting network is much more infor-
mative about the social interactions among the users.
Remarkably, even for the small manually collected
data-set, the resulting network reproduces some of
the known features of signed networks from social
sciences (Lescovec et. al., 2010; Wassermanc and
Faust, 1994). For instance, the highlighted ovals
show balanced triads: two friends with a common
enemy and three mutual friends. This with struc-
tural balance theory, which suggests that in signed
network particular triads with odd number of posi-
tive links (three mutual friends or two friends with a
common enemy) are more plausible than other cases
(e.g. three mutual foes). As we add more data, we
expect more occurrences of such triads.
Our current research focuses on automating
the above process of network construction and
analysis. To this end, we have been developing
approaches based on Dynamic Bayesian Networks
where the nodes correspond to participating users
and messages, and the edges encode probabilistic
dependence between message content and user
attitudes. In this framework, the content of a
message depends on the previous message as
well as on the attitude of the posting user to-
wards both the content and the other user. The
observables in this model are the messages (and
in some cases, some user?attributes such as age,
location etc). And the unobservables such as
users? attitudes and social preferences are modeled
through latent variables that need to be inferred.
To be more specific, let u1 and u2 denote the
variables describing the users, and m1,m2, ...
denote the message sequence. Within the proposed
generative framework, the goal is to calculate
the posterior probability P (u1, u2|m1,m2, ...) ?
pi(u1)pi(u2)pi(m1|u1)
?K
t=2 P (mt|mt?1, ui=1,2).
Here pi(.) are the priors, and P (mt|mt?1, ui) is a
probability of seeing a particular response by the
user ui to a message mt?1, which will be estimated
using annotated data and further refined through
EM?type approach.
References
Carvalho, V. and Cohen, W., On the collective classifica-
tion of email speech acts. Proc. of SIGIR (2005).
Kang, J., Kim, J. and Shaw, E., Profiling Student Groups
in Online Discussion with Network Analysis, Proc. of
K-CAP wsp on Analyzing Social Media (2009).
Leskovec, J. Huttenlocher, D. Kleinberg. J. Signed Net-
works in Social Media. ACM SIGCHI Conference on
Human Factors in Computing Systems (2010).
Park, S. Maurer F. A. Network Analysis of Stakeholders
in Tool Visioning Process for Story Test Driven De-
velopment, Proc. IEEE Int?l Conf. on Engineering of
Complex Computer Systems, (2009)
Ravi, S., Kim, J., Profiling Student Interactions in
Threaded Discussions with Speech Act Classifiers.
Proc. AI in Education (2007).
Wasserman, S. and Faust. K. Social Network Analysis:
Methods and Applications. Camb. U. Press, (1994).
14
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 84?91,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Towards Identifying Unresolved Discussions in Student Online Forums 

Jihie Kim, Jia Li, and Taehwan Kim
University of Southern California  
Information Sciences Institute 
4676 Admiralty Was, Marina del Rey, CA, U.S.A 
{jihie, jiali, taehwan}@isi.edu 
Abstract
Automatic tools for analyzing student online 
discussions are highly desirable for providing 
better assistance and promoting discussion 
participation. This paper presents an approach 
for identifying student discussions with unre-
solved issues or unanswered questions. In or-
der to handle highly incoherent data, we 
perform several data processing steps. We 
then apply a two-phase classification algo-
rithm. First, we classify ?speech acts? of indi-
vidual messages to identify the roles that the 
messages play, such as question, issue raising, 
and answers. We then use the resulting speech 
acts as features for classifying discussion 
threads with unanswered questions or unre-
solved issues. We performed a preliminary 
analysis of the classifiers and the system 
shows an average F score of 0.76 in discus-
sion thread classification. 
1 Introduction*
Online discussion boards have become a popular 
and important medium for distance education.  
Students use discussion forums to collaborate, to 
exchange information, and to seek answers to 
problems from their instructors and classmates.  
Making use of the dialog to assess student under-
standing is an open research problem. As the class 
size increases and online interaction becomes 
heavier, automatic tools for analyzing student dis-
cussions are highly desirable for providing better 
assistance and promoting discussion participation. 
In this paper, we present an approach for automati-
cally identifying discussions that have unresolved 
issues or unanswered questions. The resulting dis-
                                                          
*
cussions can be reported to instructors for further 
assistance. 
We present a two-phase machine learning ap-
proach where the first phase identifies high level 
dialogue features (speech acts such as question, 
issue raising, answer, and acknowledgement) that 
are appropriate for assessing student interactions. 
The second phase uses speech acts as features in 
creating thread classifiers that identify discussions 
with unanswered questions or unresolved issues. 
We also describe an approach where thread classi-
fiers are created directly from the features in dis-
cussion messages. The preliminary results indicate 
that although the direct learning approach can 
identify threads with unanswered questions well, 
SA based learning provide a little better results in 
identifying threads with issues and threads with 
unresolved issues.
2 Modeling Student Discussions
Our study takes place in the context of an under-
graduate course discussion board that is an integral 
component of an Operating Systems course in the 
Computer Science Department at the University of 
Southern California. We obtain our data from an 
existing online discussion board that hosts student 
technical discussions. Total 291 discussion threads 
(219 for training and 72 for test) with 1135 mes-
sages (848 for training and 287 for test) from two 
semesters? discussions were used for this study. 
168 students participated in the discussions.  
2.1 Discussion Threads 
Unlike prototypical collaborative argumentation 
where a limited number of members take part in 
the conversation with a strong focus on solving 
specific problems, student online discussions have 
much looser conversational structure, possibly in-
volving multiple anonymous discussants. Student 
84
discussions are very informal and noisy with re-
spect to grammar, syntax and punctuation. There is 
a lot of variance in the way that students present 
similar information. Messages about programming 
assignments include various forms of references to 
programming code. Figure 1 shows an example 
discussion thread that is relatively technical and 
formal. The raw data include humorous messages 
and personal announcements as well as technical 
questions and answers.
Figure 1. An example discussion thread 
The average number of messages per discussion 
thread in our undergraduate course is 3.9, and 
many discussion threads contain only two or three 
messages. Discussions often start with a question 
from a student on a project or an assignment. In 
some cases, the discussion ends with an answer 
that follows the question. In some other cases, the 
original poster may raise additional issues or ask 
questions about the answer. The discussion can 
continue with the following answer from another 
student as in Figure 1.  However, sometimes the 
discussion ends with hanging issues or questions 
without an answer.  
2.2 Speech Acts in messages: Identifying roles 
that a message plays 
For conversation analysis, we adopted the theory 
of Speech Acts (SAs) to capture relations between 
messages (Austin, 1962; Searle, 1969). Each mes-
sage within a discussion thread may play a differ-
ent role.  A message could include a question for a 
particular problem, or it could contain an answer or 
suggestion with respect to a previous question in 
the thread. Messages can include question, answer, 
acknowledgement, and objection. Since SAs are 
useful in understanding contributions made by stu-
dents in discussions, and are natural indicators for 
unanswered questions or unresolved issues, we use 
SAs as features for classifying discussion threads 
in a two phase learning as described below. 
Table 1. Speech Act Categories and Kappa values 
SA
Category Description  kappa
QUES 
A question about a problem, in-
cluding question about a previous 
message 
0.94
ANS 
A simple or complex answer to a 
previous question. Suggestion or 
advice
0.72
ISSUE 
Report misunderstanding, unclear 
concepts or issues in solving prob-
lems 
0.88
Pos-Ack
An acknowledgement, compliment 
or support in response to a prev. 
message 
0.87
Neg-Ack
A correction or objection (or com-
plaint) to/on a previous message 
0.85
We divide message roles into several SA cate-
gories, extending the approaches presented in (Kim 
et al, 2006; Kim and Ravi 2007). We focus on the 
categories that are relevant to the problem of iden-
tifying discussion threads with unanswered ques-
tion or unresolved issues.  
The message might contain a question about a 
particular problem (QUES) or report a misunder-
standing, unclear concepts or issues in solving a 
problem (ISSUE). It might propose an answer or 
suggestion with respect to a previous question in 
the thread (ANS). Finally, a message might ac-
knowledge the previous message with support 
Message1: QUES
Message2: ANS
Poster1: I am still confused.  I understand it is in the
same address space as the parent process, where do we
allocate the 8 pages of mem for it? And how do we
keep track of .....?  I am sure it is a simple concept that
I am just missing.
Poster2: Have you read the student documentation for
the Fork syscall?
?
Poster1: The Professor gave us 2 methods for forking
threads from the main program.  One was .......  The
other was to ......... When you fork a thread where does
it get created and take its 8 pages from? Do you have to
calculate ......? If so how?  Where does it store its
PCReg .......?    Any suggestions would be helpfule.
Poster3: If you use the first implementation....,
then you'll have a hard limit on the number of
threads....If you use the second implementation,
you need to....
Either way, you'll need to implement the
AddrSpace::NewStack() function and make sure
that there is memory available.
?
Message3: ISSUE, QUES
Message4: ANS
85
(Pos-Ack) or show disagreement or objection 
(Neg-Ack). SAs relate a pair of messages that has a 
?reply-to? relation. A pair of messages can be la-
beled with multiple SAs, and a message can have 
multiple SAs with more than one messages. This 
allows us to capture various relations among mes-
sages. Table 1 describes the categories we are fo-
cusing on and the kappa values from two 
annotators. Figure 1 shows SA relations between 
message pairs.  
During annotation of the corpus, the annotators 
marked the cues that are relevant to a particular SA 
category as well as the SA categories themselves. 
Such information provides hints on the kinds of 
features that are useful. We also interviewed the 
annotators to capture additional cues or indicators 
that they used during the annotation. We iterated 
with several different annotation approaches until 
we reach enough agreement among the annotators 
on a new dataset that was not seen by the annota-
tors before. 
Table 2 shows the distribution statistics of each 
SA category among the whole training and test 
corpus. Since a message may have more than one 
SA, the percentage sum of all SAs doesn?t equal to 
1. As we can see, Pos-Ack and Neg-Ack are ex-
periencing lacking data problem which is one of 
the challenges we are facing for SA classification. 
Table 2. Statistics for each Speech Act Category 
Training set Test set SA
Category # of msgs Percentage # of msgs Percentage
QUES 469 55.31% 146 50.87% 
ANS 508 59.91% 176 61.32% 
ISSUE 136 16.03% 46 16.03% 
Pos-Ack 78 9.20% 30 10.45% 
Neg-Ack 23 2.71% 8 2.79% 
3 Message Speech Act Classifiers  
In this section, we first describe how raw discus-
sion data is processed and show the features gener-
ated from the data, and we then present the current 
SA classifiers.  
3.1 Discussion Data Pre-processing
Besides typical data preprocessing steps, such 
as stemming and filtering, which are taken by most 
NLP systems, our system performs additional steps 
to reduce noise and variance (Ravi and Kim 2007).  
We first remove the text from previous mes-
sages that is automatically inserted by the discus-
sion board system starting with righ angle braket 
(>) when the user clicks on a ?Reply to? button. 
We also apply a simple stemming algorithm that 
removes ?s? and ?es? for plurals. Apostrophes are 
also converted to their original forms. E.g., ?I?m? 
is converted to ?I am?. For discussions on pro-
gramming assignment, the discussion included pro-
gramming code fragments. Each section of 
programming code or code fragment is replaced 
with a single term called code.  Similar substitution 
patterns were used for a number of categories like 
filetype extensions (?.html?, ?.c?, ?.c++?, 
?.doc?), URL links and others. Students also tend 
to use informal words (e.g. ?ya?, ?yeah?, ?yup?).
We substitute some of such words with one form 
(?yes?). For words like ?which?,  ?where?, 
?when?, ?who? and ?how?, we used the term 
categ_wh. We do not replace pronouns (?I?, ?we?, 
?they?,) since they may be useful for identifying 
some SAs. For example, ?You can? may be a cue 
for ANS but ?I can? may not.  
We also apply a simple sentence divider with 
simple cues (punctuation and white spaces such as 
newline) in order to captures the locations of the 
features in the message, such as cue words in the 
first sentence vs. cues in the last sentence.  
3.2 Features for Speech Act Classification
We have used six different types of features based 
on input from the annotators.  
F1: cue phases and their positions: In addition to 
SAs (e.g. QUES), the human annotators marked 
the parts within the message (cue phrases or sen-
tences), which helped them identify the SAs in the 
message. In order to overcome data sparseness, we 
generate features from the marked phrases. That is, 
from each phrase, we extract all the unigrams, bi-
grams, trigrams (sequence of 1/2/3 words) and add 
them to the feature set. We also added two separate 
unigrams, three separate unigrams and a unigram 
and a bigram combinations since the annotations in 
the training data indicated that they could be a use-
ful pattern.  All the cues including separate cues 
such as two unigrams are captured and used for a 
single sentence. Positions of the cues are included 
since in longer messages the cues in the beginning 
86
sentences and the ones in the end sentences can 
indicate different SAs. For example, THANK in 
the beginning indicates a positive answer but 
THANK in latter part of the message usually 
means politeness (thank in advance). 
F2: Message Position: Position of current message 
within the discussion thread (e.g. the first message, 
the last message, or middle in the thread). 
F3: Previous Message Information: SAs in the 
previous message that the current message is reply-
ing to. 
F4: Poster Class: Student or Instructor. 
F5: Poster Change: Was the current message 
posted by the same person who posted the message 
that the current message is replying to? 
F6: Message Length: Values include Short(1-
5words), Medium(6-30 words), and Long(>30 
words).
F1 is a required feature since the annotators in-
dicated cues as useful feature in most cases. All the 
others are optional.
3.3 Speech Act Classifiers 
We applied SVM in creating binary classifiers for 
each SA category using Chang and Lin (2001). 
Also, Transformation-based Learning (TBL) was 
applied as it has been successfully used in spoken 
dialogue act classification (Samuel 2000; Brill 
1995). It starts with the unlabeled corpus and 
learns the best sequence of admissible ?transforma-
tion rules? that must be applied to the training cor-
pus to minimize the error for the task.  The 
generated rules are easy to understand and useful 
for debugging the features used.  TBL results are 
also used in generating dependencies among SA 
categories for F3, i.e. which SAs tend to follow 
which other SAs1, as describe below.
SA Classification with TBL
Each rule iRule is composed of two parts - (1) 
iRuleLHS  - A combination of features that should 
be checked for applicability to the current message 
(2)
iRuleTAG  - SA tag to apply, if the feature com-
bination is applicable to the current message. 
                                                          
1 It is possible to collect related clues from SVM with distribution of 
feature values and information gain although dependencies can be 
easily recognized in TBL rules. 
iii RuleTAGRuleLHSRule !"::
Where ii XRuleLHS !
)654321(; FFFFFFXXX i #####$%
The iRuleLHS  component can be instantiated 
from all the combination of the features F1, ?,F6. 
iRuleTAG  is any SA (single SA) chosen from a list 
of all the SA categories. An example rule used in 
Speech Act learning is shown below: 
Rule1 :: IF cue-phrase = {?not?, ?work?} 
& poster-info = Student 
& post-length = Long 
=> ISSUE 
Rule1 means if the post contains two unigrams 
?not? and ?work?, the poster is a student, and the 
post length is long, then the Speech Act for the 
post is ISSUE.
We apply each rule in the potential rule set on 
all the posts in the training corpus and transform 
the post label if the post is applicable. The rule 
with highest improvement by F score is selected 
into the optimal rule set and moved from the po-
tential rule set. The iteration continues until there 
is no significant improvement with any rule.  
The training corpus was divided into 3 parts for 
3-fold cross validation. The rules from 3 rule sets 
are merged and sorted by weighted Mean Recipro-
cal Rank (MRR) (Voorhees, 2001). For example, if 
we have 5 rules among 3 rule sets as follows, 
Rule set 1 (0.85 on test): R1 R2 R3 
Rule set 2 (0.88 on test): R2 R1 R4 
Rule set 3 (0.79 on test): R1 R4 R5 
For R1, we calculate the weighted MRR as 
(0.85*1 + 0.88*(1/2) + 0.79*1) / 3. After sorting, 
we get top N rules from the merged rule set. Table 
3 shows some of the rules learned. 
Table 3. TBL rule examples 
IF cue-phrase = {???}  => QUES 
IF cue-phrase = {?yes you can?} 
& poster-info = Instructor 
& post-length = Medium  => ANS
IF cue-phrase = {?yes?} 
& cue-position = CP_BEGIN  
& prev-SA = QUES 
=> ANS
IF cue-phrase = {?not know?}  
87
& poster-info = student  
& poster-change = YES  => ISSUE 
Based on the rules generated from TBL, we 
analyze dependencies among the SA categories for 
F3 (previous message SAs). In TBL rules, ANS 
depends on ISSUE and QUES, i.e. some ANS 
rules have QUES and ISSUE for F3. Also Pos-Ack 
and Neg-Ack tend to follow ANS. Both SVM and 
TBL classifiers use this information during testing. 
That is, we apply independent classifiers first and 
then use dependent classifiers according to the de-
pendency order as following:  
Currently there is no loop in the selected rules 
but we plan to address potential issues with loops 
in SA dependencies.
SA Classification with SVM 
Table 4. Some of the top selected features by Infor-
mation Gain 
SA  
Category Top features 
QUES
??? 
POST_POSITION 
?_category_wh_ ? ?? 
PREV_SA_FIRST_NONE 
?to ? ?? 
ANS
POST_POSITION 
PREV_SA_QUESTION 
??? 
POSTER_INFO 
ISSUE 
POSTER_INFO 
?not ? sure? 
POST_POSITION 
FEATURE_LENGTH 
?error?
Pos-Ack 
PREV_SA_ANSWER 
POST_POSITION 
PREV_SA_FIRST_NONE 
?thanks? & cue-position = CP_BEGIN 
?ok? & cue-position = CP_BEGIN 
Neg-Ack
 ?yes,  ? ?, but? 
POST_POSITION 
?, but? 
?are ? wrong?
Given all the combination of the features F1,?, 
F6, we use Information Gain (Yang and Pederson 
1997) for pruning the feature space and selecting 
features. For each Speech Act, we sort all the fea-
tures (lexical and non-lexical) by Information Gain 
and use the top N (=200) features. Table 4 shows 
the top features selected by Information Gain. The 
resulting features are used in representing a mes-
sage in a vector format.  
We did 5-fold cross validation in the training. 
RBF (Radial Basis Function) is used as the kernel 
function. We performed grid search to get the best 
parameter (C and gamma) in training and applied 
them to the test corpus.  
Table 5. SA classification results 
SVM TBL 
SA Cat-
egory 
Prec.
Re-
call
F
score 
Prec. 
Re-
call
F
score
QUES 0.95 0.90 0.94 0.96 0.91 0.95 
ANS 0.87 0.80 0.85 0.83 0.64 0.78 
ISSUE 0.65 0.54 0.62 0.46 0.76 0.50 
Pos- 
Ack
0.57 0.44 0.54 0.58 0.56 0.57 
Neg-Ack 0 0 0 0.5 0.38 0.47 
Table 5 shows the current classification accura-
cies with SVM and TBL. The main reason that 
ISSUE, Pos-Ack and Neg-Ack show low scores is 
that they have relatively small number of examples 
(see statistics in Table 2). We plan to add more 
examples as we collect more discussion annota-
tions. For thread classification described below, we 
use features with QUES, ANS, ISSUE and 
Pos_Ack only. 
4 Identifying Discussions with Unan-
swered or Unresolved Questions: 
Thread Classification 
Figure 2 shows typical patterns of interactions in 
our corpus. Many threads follow pattern (a) where 
the first message includes a question and the sub-
sequent message provides an answer.  In (b), after 
an answer, the student presents an additional ques-
tion or misunderstanding (ISSUE), which is fol-
lowed by another answer.  Often students provide 
positive acknowledgement when an answer is sat-
ISSUE
ANS
QUES
Pos-Ack
Neg-Ack
88
isfying.  Pattern (c) covers cases for when the 
question is unanswered.   
Figure 2. Example patterns in student discussion 
threads 
We are interested in the following assessment 
questions.
(Q1) Were all questions answered? (Y/N) 
(Q2) Were there any issues or confusion? (Y/N) 
(Q3) Were those issues or confusions resolved? (Y/N) 
There can be multiple questions, and Q1 is false 
if there is any question that does not have a corre-
sponding answer. That is, even when some ques-
tions were resolved, it could   
still be False (not resolved) if some were not re-
solved.  If Q2 is False (i.e. there is no issue or 
question), then Q3 is also False.  
These questions are useful for distinguishing 
different interaction patterns, including threads 
with unanswered questions. In the second phase of 
learning, we use SA-based features.  Our initial 
analysis of student interactions as above indicates 
that the following simple features can be useful in 
answering such questions:  
(T-F1) Whether there was an [SA] in the thread  
(T-F2) Whether the last message in the thread in-
cluded [SA]  
We used TBL rules for Pos-Ack and SVM clas-
sifiers for other SA categories because of relatively 
higher score of Pos-Ack from TBL and other cate-
gories from SVM. We use 8 (2 x 4) features cre-
ated from T-F1 and T-F2. SVM settings are similar 
to the ones used in the SA classification.  
Table 6 shows the thread classification results. 
We checked SVM classification results with hu-
man annotated SAs since they can show how use-
ful SA-based features are (T-F1 and T-F2 in 
particular) in answering Q1?Q3. The results 
shown in Table 6-(a) indicate that the features (T-
F1 and T-F2) are in fact useful for the questions.  
When we used the SA classifiers and SVM in a 
pipeline, the system shows precisions (recalls) of 
83%(84%), 77%(74%) and 68%(69%) for Q1, Q2, 
and Q3 respectively.  
         Table 6. Thread Classification Results 
Precision Recall F score 
Q1 0.93    0.93 0.93 
Q2 0.93 0.93 0.93 
Q3 0.89 0.89 0.89 
(a) Classification results with human annotated SAs 
Precision Recall F score 
Q1 0.83 0.84 0.83 
Q2 0.77 0.74 0.76 
Q3 0.68 0.69 0.68 
(b) SVM classification results with system generated 
SAs
The results with system generated SAs provide 
an average F score of 0.76. Although the ISSUE 
classifier has F score of 0.62, the score for Q2 is 
0.76. Q2 checks one or more occurrences of 
ISSUE rather than identifying existence of ISSUE 
in a message, and it may become an easier problem 
when there are multiple occurrences of ISSUEs.  
5 Direct Thread Classification without 
SAs
As an alternative to the SA-based two-phase learn-
ing, we crated thread classifiers directly from the 
features in discussion messages. We used SVM 
with the following features that we can capture 
directly from a discussion thread.  
F1?: cue phases and their positions in the 
thread:  we use the same cue features in F1 but we 
use an optional thread level cue position: 
Last_message and Dont_Care. For example, for a 
given cue ?ok?, if it appears in the the last message 
of the thread, we generate two features, 
"ok"_Last_message and "ok"_Dont_Care.  
Given a set of candidate features, we use In-
formation Gain to select the top N (=200) features. 
The resulting features are used in creating vectors 
as described inS 3.3. Similar cross-validation and 
SVM settings are applied.
89
   Table 7. Results from Direct Thread Classification  
Precision Recall F score 
Q1 0.86 0.86 0.86 
Q2 0.81 0.62 0.70 
Q3 0.75 0.33 0.46 
Table 7 shows the classification results. Al-
though the direct learning approach can identify 
threads with unanswered questions well, SA based 
learning provides a little better results in identify-
ing threads with issues (Q2) and unresolved issues 
(Q3). It seems that SA-based features may help 
performing more difficult tasks (e.g. assessment 
for ISSUEs in discussions) We need further inves-
tigation on different types of assessment tasks.  
6 Related Work 
Rhetorical Structure Theory (Mann and Thom-
son, 1988) based discourse processing has attracted 
much attention with successful applications in sen-
tence compression and summarization. Most of the 
current work on discourse processing focuses on 
sentence-level text organization (Soricut and 
Marcu, 2003) or the intermediate step (Sporleder 
and Lapata, 2005). Analyzing and utilizing dis-
course information at a higher level, e.g., at the 
paragraph level, still remains a challenge to the 
natural language community. In our work, we util-
ize the discourse information at a message level.  
There has been prior work on dialogue act 
analysis and associated surface cue words (Samuel 
2000; Hirschberg and Litman 1993). There have 
also been Dialogue Acts modeling approaches for 
automatic tagging and recognition of conversa-
tional speech (Stolcke et al, 2000) and related 
work in corpus linguistics where machine learning 
techniques have been used to find conversational 
patterns in spoken transcripts of dialogue corpus 
(Shawar and Atwell, 2005). Although spoken dia-
logue is different from message-based conversa-
tion in online discussion boards, they are closely 
related to our thread analysis work, and we plan to 
investigate potential use of conversation patterns in 
spoken dialogue in threaded discussions.  
Carvalho and Cohen (2005) present a depend-
ency-network based collective classification 
method to classify email speech acts. However, 
estimated speech act labeling between messages is 
not sufficient for assessing contributor roles or 
identifying help needed by the participants. We 
included other features like participant profiles. 
Also our corpus consists of less informal student 
discussions rather than messages among project 
participants, which tend to be more technically 
coherent.
Requests and commitments of email exchange 
are analyzed in (Lampert et al, 2008). As in their 
analysis, we have a higher kappa value for ques-
tions than answers, and some sources of ambiguity 
in human annotations such as different forms of 
answers also appear in our data. However, student 
discussions tend to focus on problem solving rather 
than task request and commitment as in project 
management applications, and their data show dif-
ferent types of ambiguity due to different nature of 
participant interests.  
There also has been work on non-traditional, 
qualitative assessment of instructional discourse 
(Graesser et al, 2005; McLaren et al, 2007; Boyer 
et al, 2008). The assessment results can be used in 
finding features for student thinking skills or level 
of understanding. Although the existing work has 
not been fully used for discussion thread analysis, 
we are investigating opportunities for using such 
features to cover additional discourse analysis ca-
pabilities.  Similar approaches for classifying 
speech acts were investigated (Kim and Ravi 
2007). Our work captures more features that are 
relevant to analyzing noisy student discussion 
threads and support a full automatic analysis of 
student discussions instead of manual generation of 
thread analysis rules.  
7 Summary and Future Work 
We have presented an approach for automatically 
classifying student discussions to identify discus-
sions that have unanswered questions and need 
instructor attention. We applied a multi-phase 
learning approach, where the first phase classifies 
individual messages with SAs and the second 
phase classifies discussion threads with SA-based 
features.  We also created thread classifiers directly 
from features in discussion messages. The prelimi-
nary results indicate that SA-based features may 
help difficult classification tasks. We plan to per-
form more analysis on different types of thread 
classification tasks.  
We found that automatic classification of un-
dergraduate student discussions is very challenging 
90
due to incoherence and noise in the data. Espe-
cially messages that contain long sentences, infor-
mal statements with uncommon words, answers in 
form of question, are difficult to classify.  In order 
to use other SA categories such as Neg-Ack and 
analyze various types of student interactions, we 
plan to use more annotated discussion data.  
A deeper assessment of online discussions re-
quires a combination with other information such 
as discussion topics (Feng et al, 2006). For exam-
ple, classification of discussion topics can be used 
in identifying topics that participants have more 
confusion about.  Furthermore, such information 
can also be used in profiling participants such as 
identifying mentors or help seekers on a particular 
topic as in (Kim and Shaw 2009).  We are investi-
gating several extensions in order to generate more 
useful instructional tools.
Acknowledgments 
This work was supported by National Science 
Foundation, CCLI Phase II grant (#0618859). 
References 
Austin, J., How to do things with words. 1962. Cam-
bridge, Massachusetts: Harvard Univ. Press.  
Boyer, K., Phillips, R., Wallis M., Vouk M., Lester, J., 
Learner Characteristics and Feedback in Tutorial 
Dialogue. 2008. ACL workshop on Innovative Use of 
NLP for Building Educational Applications.
Brill, E. 1962. Transformation-based error-driven learn-
ing and natural language processing: a case study in 
part-of-speech tagging. Comput. Linguist., 21(4). 
Carvalho, V.R. and Cohen, W.W. 2005. On the collec-
tive classification of email speech acts. Proceedings 
of SIGIR.
Chang, C.-C. and Lin, C.-J. 2001. LIBSVM: a library 
for support vector machines.   
Feng, D., Kim, J., Shaw, E., Hovy E., 2006. Towards 
Modeling Threaded Discussions through Ontology-
based Analysis. Proceedings of National Confer-
ence on Artificial Intelligence.
Graesser, A. C., Olney, A., Ventura, M., Jackson, G. T. 
2005. AutoTutor's Coverage of Expectations during 
Tutorial Dialogue. Proceedings of the FLAIRS Con-
ference.
Hirschberg, J. and Litman, D. 1993. Empirical Studies 
on the Disambiguation of Cue Phrases?, Computa-
tional Linguistics, 19 (3). 
Kim, J., Chern, G., Feng, D., Shaw, E., and Hovy, E. 
2006. Mining and Assessing Discussions on the 
Web through Speech Act Analysis. Proceedings of 
the ISWC'06 Workshop on Web Content Miningwith 
Human Language Technologies (2006).
Kim J. and Shaw E. 2009. Pedagogical Discourse: Con-
necting Students to Past Discussions and Peer Men-
tors within an Online Discussion Board, Innovative 
Applications of Artificial Intelligence Conference.
Lampert, A., Dale, R., and Paris, C. 2008. The Nature of 
Requests and Commitments in Email Messages, 
AAAI workshop on Enhanced Messaging.
Mann, W.C. and Thompson, S.A. 1988. Rhetorical 
structure theory: towards a functional theory of text 
organization. Text: An Interdisciplinary Journal for 
the Study of Text, 8 (3). 
McLaren, B. et al,2007. Using Machine Learning 
Techniques to Analyze and Support Mediation of 
Student E!Discussions, Proc. of AIED 2007.
Ravi, S., Kim, J., 2007. Profiling Student Interactions in 
Threaded Discussions with Speech Act Classifiers. 
Proceedings of AI in Education.
Samuel, K. 2000.An Investigation of Dialogue Act Tag-
ging using Transformation-Based Learning, PhD 
Thesis, University of Delaware.  
Searle, J. 1969. Speech Acts. Cambridge: Cambridge 
Univ. Press. 
Soricut, R. and Marcu, D. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. Proceedings of HLT/NAACL-2003.
Sporleder, C. and Lapata, M., 2005. Discourse chunking 
and its application to sentence compression. In 
Proceedings of Human Language Technology con-
ference ? EMNLP.
Stolcke, A. , Coccaro, N. , Bates, R. , Taylor, P. , et al, 
2000. Dialogue act modeling for automatic tagging 
and recognition of conversational speech, Compu-
tational Linguistics, v.26 n.3. 
Shawar, B. A. and Atwell, E. 2005. Using corpora in 
machine-learning chatbot systems.? International 
Journal of Corpus Linguistics, vol. 10. 
Witten, I. H., and Frank, E. 2005. Data Mining: Practi-
cal machine learning tools and techniques, 2nd 
Edition, Morgan Kaufmann, San Francisco. 
Yang, Y. and Pedersen, J.  1997. A Comparative Study 
on Feature Selection in Text Categorization. Proc. 
International Conference on Machine Learning.
91
