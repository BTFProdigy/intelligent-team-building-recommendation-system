Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 65?72,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Pipeline Framework for Dependency Parsing
Ming-Wei Chang Quang Do Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{mchang21, quangdo2, danr}@uiuc.edu
Abstract
Pipeline computation, in which a task is
decomposed into several stages that are
solved sequentially, is a common compu-
tational strategy in natural language pro-
cessing. The key problem of this model
is that it results in error accumulation and
suffers from its inability to correct mis-
takes in previous stages. We develop
a framework for decisions made via in
pipeline models, which addresses these
difficulties, and presents and evaluates it
in the context of bottom up dependency
parsing for English. We show improve-
ments in the accuracy of the inferred trees
relative to existing models. Interestingly,
the proposed algorithm shines especially
when evaluated globally, at a sentence
level, where our results are significantly
better than those of existing approaches.
1 Introduction
A pipeline process over the decisions of learned
classifiers is a common computational strategy in
natural language processing. In this model a task
is decomposed into several stages that are solved
sequentially, where the computation in the ith
stage typically depends on the outcome of com-
putations done in previous stages. For example,
a semantic role labeling program (Punyakanok et
al., 2005) may start by using a part-of-speech tag-
ger, then apply a shallow parser to chunk the sen-
tence into phrases, identify predicates and argu-
ments and then classify them to types. In fact,
any left to right processing of an English sentence
may be viewed as a pipeline computation as it pro-
cesses a token and, potentially, makes use of this
result when processing the token to the right.
The pipeline model is a standard model of
computation in natural language processing for
good reasons. It is based on the assumption that
some decisions might be easier or more reliable
than others, and their outcomes, therefore, can be
counted on when making further decisions. Nev-
ertheless, it is clear that it results in error accu-
mulation and suffers from its inability to correct
mistakes in previous stages. Researchers have re-
cently started to address some of the disadvantages
of this model. E.g., (Roth and Yih, 2004) suggests
a model in which global constraints are taken into
account in a later stage to fix mistakes due to the
pipeline. (Punyakanok et al, 2005; Marciniak
and Strube, 2005) also address some aspects of
this problem. However, these solutions rely on the
fact that all decisions are made with respect to the
same input; specifically, all classifiers considered
use the same examples as their input. In addition,
the pipelines they study are shallow.
This paper develops a general framework for
decisions in pipeline models which addresses
these difficulties. Specifically, we are interested
in deep pipelines ? a large number of predictions
that are being chained.
A pipeline process is one in which decisions
made in the ith stage (1) depend on earlier deci-
sions and (2) feed on input that depends on earlier
decisions. The latter issue is especially important
at evaluation time since, at training time, a gold
standard data set might be used to avoid this issue.
We develop and study the framework in the con-
text of a bottom up approach to dependency pars-
ing. We suggest that two principles to guide the
pipeline algorithm development:
(i) Make local decisions as reliable as possible.
(ii) Reduce the number of decisions made.
Using these as guidelines we devise an algo-
65
rithm for dependency parsing, prove that it satis-
fies these principles, and show experimentally that
this improves the accuracy of the resulting tree.
Specifically, our approach is based on a shift-
reduced parsing as in (Yamada and Matsumoto,
2003). Our general framework provides insights
that allow us to improve their algorithm, and to
principally justify some of the algorithmic deci-
sions. Specifically, the first principle suggests to
improve the reliability of the local predictions,
which we do by improving the set of actions taken
by the parsing algorithm, and by using a look-
ahead search. The second principle is used to jus-
tify the control policy of the parsing algorithm ?
which edges to consider at any point of time. We
prove that our control policy is optimal in some
sense, and that the decisions we made, guided by
these, principles lead to a significant improvement
in the accuracy of the resulting parse tree.
1.1 Dependency Parsing and Pipeline Models
Dependency trees provide a syntactic reresenta-
tion that encodes functional relationships between
words; it is relatively independent of the grammar
theory and can be used to represent the structure
of sentences in different languages. Dependency
structures are more efficient to parse (Eisner,
1996) and are believed to be easier to learn, yet
they still capture much of the predicate-argument
information needed in applications (Haghighi et
al., 2005), which is one reason for the recent in-
terest in learning these structures (Eisner, 1996;
McDonald et al, 2005; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004).
Eisner?s work ? O(n3) parsing time generative
algorithm ? embarked the interest in this area.
His model, however, seems to be limited when
dealing with complex and long sentences. (Mc-
Donald et al, 2005) build on this work, and use
a global discriminative training approach to im-
prove the edges? scores, along with Eisner?s algo-
rithm, to yield the expected improvement. A dif-
ferent approach was studied by (Yamada and Mat-
sumoto, 2003), that develop a bottom-up approach
and learn the parsing decisions between consecu-
tive words in the sentence. Local actions are used
to generate a dependency tree using a shift-reduce
parsing approach (Aho et al, 1986). This is a
true pipeline approach, as was done in other suc-
cessful parsers, e.g. (Ratnaparkhi, 1997), in that
the classifiers are trained on individual decisions
rather than on the overall quality of the parser, and
chained to yield the global structure. Clearly, it
suffers from the limitations of pipeline process-
ing, such as accumulation of errors, but neverthe-
less, yields very competitive parsing results. A
somewhat similar approach was used in (Nivre and
Scholz, 2004) to develop a hybrid bottom-up/top-
down approach; there, the edges are also labeled
with semantic types, yielding lower accuracy than
the works mentioned above.
The overall goal of dependency parsing (DP)
learning is to infer a tree structure. A common
way to do that is to predict with respect to each
potential edge (i, j) in the tree, and then choose a
global structure that (1) is a tree and that (2) max-
imizes some score. In the context of DPs, this
?edge based factorization method? was proposed
by (Eisner, 1996). In other contexts, this is similar
to the approach of (Roth and Yih, 2004) in that
scoring each edge depends only on the raw data
observed and not on the classifications of other
edges, and that global considerations can be used
to overwrite the local (edge-based) decisions.
On the other hand, the key in a pipeline model
is that making a decision with respect to the edge
(i, j) may gain from taking into account deci-
sions already made with respect to neighboring
edges. However, given that these decisions are
noisy, there is a need to devise policies for reduc-
ing the number of predictions in order to make the
parser more robust. This is exemplified in (Ya-
mada and Matsumoto, 2003) ? a bottom-up ap-
proach, that is most related to the work presented
here. Their model is a ?traditional? pipeline model
? a classifier suggests a decision that, once taken,
determines the next action to be taken (as well as
the input the next action observes).
In the rest of this paper, we propose and jus-
tify a framework for improving pipeline process-
ing based on the principles mentioned above: (i)
make local decisions as reliably as possible, and
(ii) reduce the number of decisions made. We
use the proposed principles to examine the (Ya-
mada and Matsumoto, 2003) parsing algorithm
and show that this results in modifying some of
the decisions made there and, consequently, better
overall dependency trees.
2 Efficient Dependency Parsing
This section describes our DP algorithm and jus-
tifies its advantages as a pipeline model. We pro-
66
pose an improved pipeline framework based on the
mentioned principles.
For many languages such as English, Chinese
and Japanese (with a few exceptions), projective
dependency trees (that is, DPs without edge cross-
ings) are sufficient to analyze most sentences. Our
work is therefore concerned only with projective
trees, which we define below.
For words x, y in the sentence T we introduce
the following notations:
x ? y: x is the direct parent of y.
x ?? y: x is an ancestor of y;
x ? y: x ? y or y ? x.
x < y: x is to the left of y in T .
Definition 1 (Projective Language) (Nivre,
2003) ?a, b, c ? T, a ? b and a < c < b imply
that a ?? c or b ?? c.
2.1 A Pipeline DP Algorithm
Our parsing algorithm is a modified shift-reduce
parser that makes use of the actions described be-
low and applies them in a left to right manner
on consecutive pairs of words (a, b) (a < b) in
the sentence. This is a bottom-up approach that
uses machine learning algorithms to learn the pars-
ing decisions (actions) between consecutive words
in the sentences. The basic actions used in this
model, as in (Yamada and Matsumoto, 2003), are:
Shift: there is no relation between a and b, or
the action is deferred because the relationship be-
tween a and b cannot be determined at this point.
Right: b is the parent of a,
Left: a is the parent of b.
This is a true pipeline approach in that the clas-
sifiers are trained on individual decisions rather
than on the overall quality of the parsing, and
chained to yield the global structure. And, clearly,
decisions make with respect to a pair of words af-
fect what is considered next by the algorithm.
In order to complete the description of the algo-
rithm we need to describe which edge to consider
once an action is taken. We describe it via the no-
tion of the focus point: when the algorithm con-
siders the pair (a, b), a < b, we call the word a the
current focus point.
Next we describe several policies for determin-
ing the focus point of the algorithm following an
action. We note that, with a few exceptions, de-
termining the focus point does not affect the cor-
rectness of the algorithm. It is easy to show that
for (almost) any focus point chosen, if the correct
action is selected for the corresponding edge, the
algorithm will eventually yield the correct tree (but
may require multiple cycles through the sentence).
In practice, the actions selected are noisy, and a
wasteful focus point policy will result in a large
number of actions, and thus in error accumulation.
To minimize the number of actions taken, we want
to find a good focus point placement policy.
After S, the focus point always moves one word
to the right. After L or R there are there natural
placement policies to consider:
Start Over: Move focus to the first word in T .
Stay: Move focus to the next word to the right.
That is, for T = (a, b, c), and focus being a, an
L action will result is the focus being a, while R
action results in the focus being b.
Step Back: The focus moves to the previous word
(on the left). That is, for T = (a, b, c), and focus
being b, in both cases, a will be the focus point.
In practice, different placement policies have a
significant effect on the number of pairs consid-
ered by the algorithm and, therefore, on the fi-
nal accuracy1. The following analysis justifies the
Step Back policy. We claim that if Step Back
is used, the algorithm will not waste any action.
Thus, it achieves the goal of minimizing the num-
ber of actions in pipeline algorithms. Notice that
using this policy, when L is taken, the pair (a, b) is
reconsidered, but with new information, since now
it is known that c is the child of b. Although this
seems wasteful, we will show this is a necessary
movement to reduce the number of actions.
As mentioned above, each of these policies
yields the correct tree. Table 1 compares the three
policies in terms of the number of actions required
to build a tree.
Policy #Shift #Left #Right
Start over 156545 26351 27918
Stay 117819 26351 27918
Step back 43374 26351 27918
Table 1: The number of actions required to build
all the trees for the sentences in section 23 of Penn
Treebank (Marcus et al, 1993) as a function of
the focus point placement policy. The statistics are
taken with the correct (gold-standard) actions.
It is clear from Table 1 that the policies result
1Note that (Yamada and Matsumoto, 2003) mention that
they move the focus point back after R, but do not state what
they do after executing L actions, and why. (Yamada, 2006)
indicates that they also move focus point back after L.
67
Algorithm 2 Pseudo Code of the dependency
parsing algorithm. getFeatures extracts the fea-
tures describing the word pair currently consid-
ered; getAction determines the appropriate action
for the pair; assignParent assigns a parent for the
child word based on the action; and deleteWord
deletes the child word in T at the focus once the
action is taken.Let t represents for a word token
For sentence T = {t1, t2, . . . , tn}
focus= 1
while focus< |T | do
~v = getFeatures(tfocus, tfocus+1)
? = getAction(tfocus, tfocus+1, ~v)if ? = L or ? = R then
assignParent(tfocus, tfocus+1, ?)
deleteWord(T, focus, ?)
// performing Step Back here
focus = focus ? 1
else
focus = focus + 1
end if
end while
in very different number of actions and that Step
Back is the best choice. Note that, since the ac-
tions are the gold-standard actions, the policy af-
fects only the number of S actions used, and not
the L and R actions, which are a direct function
of the correct tree. The number of required ac-
tions in the testing stage shows the same trend and
the Step Back also gives the best dependency ac-
curacy. Algorithm 2 depicts the parsing algorithm.
2.2 Correctness and Pipeline Properties
We can prove two properties of our algorithm.
First we show that the algorithm builds the de-
pendency tree in only one pass over the sentence.
Then, we show that the algorithm does not waste
actions in the sense that it never considers a word
pair twice in the same situation. Consequently,
this shows that under the assumption of a perfect
action predictor, our algorithm makes the smallest
possible number of actions, among all algorithms
that build a tree sequentially in one pass.
Note that this may not be true if the action clas-
sifier is not perfect, and one can contrive examples
in which an algorithm that makes several passes on
a sentence can actually make fewer actions than a
single pass algorithm. In practice, however, as our
experimental data shows, this is unlikely.
Lemma 1 A dependency parsing algorithm that
uses the Step Back policy completes the tree when
it reaches the end of the sentence for the rst time.
In order to prove the algorithm we need the fol-
lowing definition. We call a pair of words (a, b) a
free pair if and only if there is a relation between
a and b and the algorithm can perform L or R ac-
tions on that pair when it is considered. Formally,
Definition 2 (free pair) A pair (a, b) considered
by the algorithm is a free pair, if it satises the
following conditions:
1. a ? b
2. a, b are consecutive in T (not necessary in
the original sentence).
3. No other word in T is the child of a or b. (a
and b are now part of a complete subtree.)
Proof. : It is easy to see that there is at least one
free pair in T , with |T | > 1. The reason is that
if no such pair exists, there must be three words
{a, b, c} s.t. a ? b, a < c < b and ?(a ? c ?
b ? c). However, this violates the properties of a
projective language.
Assume {a, b, d} are three consecutive words in
T . Now, we claim that when using Step Back, the
focus point is always to the left of all free pairs in
T . This is clearly true when the algorithm starts.
Assume that (a, b) is the first free pair in T and let
c be just to the left of a and b. Then, the algorithm
will not make a L or R action before the focus
point meets (a, b), and will make one of these ac-
tions then. It?s possible that (c, a ? b) becomes a
free pair after removing a or b in T so we need
to move the focus point back. However, we also
know that there is no free pair to the left of c.
Therefore, during the algorithm, the focus point
will always remain to the left of all free pairs. So,
when we reach the end of the sentence, every free
pair in the sentence has been taken care of, and the
sentence has been completely parsed. 2
Lemma 2 All actions made by a dependency
parsing algorithm that uses the Step Back policy
are necessary.
Proof. : We will show that a pair (a, b) will never
be considered again given the same situation, that
is, when there is no additional information about
relations a or b participate in. Note that if R or
68
L is taken, either a or b will become a child word
and be eliminate from further consideration by the
algorithm. Therefore, if the action taken on (a, b)
is R or L, it will never be considered again.
Assume that the action taken is S, and, w.l.o.g.
that this is the rightmost S action taken before a
non-S action happens. Note that it is possible that
there is a relation between a and b, but we can-
not perform R or L now. Therefore, we should
consider (a, b) again only if a child of a or b has
changed. When Step Back is used, we will con-
sider (a, b) again only if the next action is L. (If
next action is R, b will be eliminated.) This is true
because the focus point will move back after per-
forming L, which implies that b has a new child
so we are indeed in a new situation. Since, from
Lemma 1, the algorithm only requires one round.
we therefore consider (a, b) again only if the situ-
ation has changed. 2
2.3 Improving the Parsing Action Set
In order to improve the accuracy of the action pre-
dictors, we suggest a new (hierarchical) set of ac-
tions: Shift, Left, Right, WaitLeft, WaitRight. We
believe that predicting these is easier due to finer
granularity ? the S action is broken to sub-actions
in a natural way.
WaitLeft: a < b. a is the parent of b, but it?s
possible that b is a parent of other nodes. Action is
deferred. If we perform Left instead, the child of b
can not find its parents later.
WaitRight: a < b. b is the parent of a, but it?s
possible that a is a parent of other nodes. Similar
to WL, action is deferred.
Thus, we also change the algorithm to perform
S only if there is no relationship between a and b2.
The new set of actions is shown to better support
our parsing algorithm, when tested on different
placement policies. When WaitLeft or WaitRight
is performed, the focus will move to the next word.
It is very interesting to notice that WaitRight is
not needed in projective languages if Step Back
is used. This give us another strong reason to use
Step Back, since the classification becomes more
accurate ? a more natural class of actions, with a
smaller number of candidate actions.
Once the parsing algorithm, along with the fo-
cus point policy, is determined, we can train the
2Interestingly, (Yamada and Matsumoto, 2003) mention
the possibility of an additional single Wait action, but do not
add it to the model.
action classifiers. Given an annotated corpus, the
parsing algorithm is used to determine the action
taken for each consecutive pair; this is used to train
a classifier to predict one of the five actions. The
details of the classifier and the feature used are
given in Section 4.
When the learned model is evaluated on new
data, the sentence is processed left to right and the
parsing algorithm, along with the action classifier,
are used to produce the dependency tree. The eval-
uation process is somewhat more involved, since
the action classifier is not used as is, but rather via
a look ahead inference step described next.
3 A Pipeline Model with Look Ahead
The advantage of a pipeline model is that it can use
more information, based on the outcomes of previ-
ous predictions. As discussed earlier, this may re-
sult in accumulating error. The importance of hav-
ing a reliable action predictor in a pipeline model
motivates the following approach. We devise a
look ahead algorithm and use it as a look ahead
policy, when determining the predicted action.
This approach can be used in any pipeline
model but we illustrate it below in the context of
our dependency parser.
The following example illustrates a situation in
which an early mistake in predicting an action
causes a chain reaction and results in further mis-
takes. This stresses the importance of correct early
decisions, and motivates our look ahead policy.
Let (w, x, y, z) be a sentence of four words, and
assume that the correct dependency relations are
as shown in the top part of Figure 1. If the system
mistakenly predicts that x is a child of w before y
and z becomes x?s children, we can only consider
the relationship between w and y in the next stage.
Consequently, we will never find the correct parent
for y and z. The previous prediction error propa-
gates and impacts future predictions. On the other
hand, if the algorithm makes a correct prediction,
in the next stage, we do not need to consider w and
y. As shown, getting useful rather than misleading
information in a pipeline model, requires correct
early predictions. Therefore, it is necessary to uti-
lize some inference framework to that may help
resolving the error accumulation problem.
In order to improve the accuracy of the action
prediction, we might want to examine all possible
combinations of action sequences and choose the
one that maximizes some score. It is clearly in-
69
X YW Z
X
YW Z
Figure 1: Top figure: the correct dependency rela-
tions between w, x, y and z. Bottom figure: if the
algorithm mistakenly decides that x is a child of w
before deciding that y and z are x?s children, we
cannot find the correct parent for y and z.
tractable to find the global optimal prediction se-
quences in a pipeline model of the depth we con-
sider. Therefore, we use a look ahead strategy,
implemented via a local search framework, which
uses additional information but is still tractable.
The local search algorithm is presented in Algo-
rithm 3. The algorithm accepts three parameters,
model, depth and State. We assume a classifier
that can give a confidence in its prediction. This is
represented here by model.
As our learning algorithm we use a regularized
variation of the perceptron update rule, as incorpo-
rated in SNoW (Roth, 1998; Carlson et al, 1999),
a multi-class classifier that is tailored for large
scale learning tasks and has been used successfully
in a large number of NLP tasks (e.g., (Punyakanok
et al, 2005)). SNoW uses softmax over the raw
activation values as its confidence measure, which
can be shown to produce a reliable approximation
of the labels? conditional probabilities.
The parameter depth is to determine the depth
of the search procedure. State encodes the config-
uration of the environment (in the context of the
dependency parsing this includes the sentence, the
focus point and the current parent and children for
each word). Note that State changes when a pre-
diction is made and that the features extracted for
the action classifier also depend on State.
The search algorithm will perform a search of
length depth. Additive scoring is used to score
the sequence, and the first action in this sequence
is selected and performed. Then, the State is up-
dated, the new features for the action classifiers are
computed and search is called again.
One interesting property of this framework is
that it allows that use of future information in ad-
dition to past information. The pipeline model nat-
urally allows access to all the past information.
Algorithm 3 Pseudo code for the look ahead algo-
rithm. y represents a action sequence. The func-
tion search considers all possible action sequences
with |depth| actions and returns the sequence with
the highest score.
Algo predictAction(model, depth, State)
x = getNextFeature(State)
y = search(x, depth, model, State)
lab = y[1]
State = update(State, lab)
return lab
Algo search(x, depth, model, State)
maxScore = ??
F = {y | ?y? = depth}
for y in F do
s = 0, TmpState = State
for i = 1 . . . depth do
x = getNextFeature(TmpState)
s = s+ score(y[i], x, model)
TmpState = update(TmpState, y[i])
end for
if s > maxScore then
y? = y
maxScore = s
end if
end for
return y?
Since the algorithm uses a look ahead policy, it
also uses future predictions. The significance of
this becomes clear in Section 4.
There are several parameters, in addition to
depth that can be used to improve the efficiency of
the framework. For example, given that the action
predictor is a multi-class classifier, we do not need
to consider all future possibilities in order to de-
cide the current action. For example, in our exper-
iments, we only consider two actions with highest
score at each level (which was shown to produce
almost the same accuracy as considering all four
actions).
4 Experiments and Results
We use the standard corpus for this task, the Penn
Treebank (Marcus et al, 1993). The training set
consists of sections 02 to 21 and the testing set is
section 23. The POS tags for the evaluation data
sets were provided by the tagger of (Toutanova et
al., 2003) (which has an accuracy of 97.2% section
70
23 of the Penn Treebank).
4.1 Features for Action Classification
For each word pair (w1, w2) we use the words,their POS tags and also these features of the chil-
dren of w1 and w2. We also include the lexiconand POS tags of 2 words before w1 and 4 wordsafter w2 (as in (Yamada and Matsumoto, 2003)).The key additional feature we use, relative to (Ya-
mada and Matsumoto, 2003), is that we include
the previous predicted action as a feature. We
also add conjunctions of above features to ensure
expressiveness of the model. (Yamada and Mat-
sumoto, 2003) makes use of polynomial kernels
of degree 2 which is equivalent to using even more
conjunctive features. Overall, the average number
of active features in an example is about 50.
4.2 Evaluation
We use the same evaluation metrics as in (McDon-
ald et al, 2005). Dependency accuracy (DA) is the
proportion of non-root words that are assigned the
correct head. Complete accuracy (CA) indicates
the fraction of sentences that have a complete cor-
rect analysis. We also measure that root accuracy
(RA) and leaf accuracy (LA), as in (Yamada and
Matsumoto, 2003). When evaluating the result,
we exclude the punctuation marks, as done in (Mc-
Donald et al, 2005) and (Yamada and Matsumoto,
2003).
4.3 Results
We present the results of several of the experi-
ments that were intended to help us analyze and
understand several of the design decisions in our
pipeline algorithm.
To see the effect of the additional action, we
present in Table 2 a comparison between a system
that does not have the WaitLeft action (similar
to the (Yamada and Matsumoto, 2003) approach)
with one that does. In both cases, we do not use the
look ahead procedure. Note that, as stated above,
the action WaitRight is never needed for our pars-
ing algorithm. It is clear that adding WaitLeft in-
creases the accuracy significantly.
Table 3 investigates the effect of the look ahead,
and presents results with different depth param-
eters (depth= 1 means ?no search?), showing a
consistent trend of improvement.
Table 4 breaks down the results as a function
of the sentence length; it is especially noticeable
that the system also performs very well for long
method DA RA CA LA
w/o WaitLeft 90.27 90.73 39.28 93.87
w WaitLeft 90.53 90.76 39.74 93.94
Table 2: The significant of the action WaitLeft .
method DA RA CA LA
depth=1 90.53 90.76 39.74 93.94depth=2 90.67 91.51 40.23 93.96depth=3 90.69 92.05 40.52 93.94depth=4 90.79 92.26 40.68 93.95
Table 3: The effect of different depth settings.
sentences, another indication for its global perfor-
mance robustness.
Table 5 shows the results with three settings of
the POS tagger. The best result is, naturally, when
we use the gold standard also in testing. How-
ever, it is worthwhile noticing that it is better to
train with the same POS tagger available in test-
ing, even if its performance is somewhat lower.
Table 6 compares the performances of several
of the state of the art dependency parsing systems
with ours. When comparing with other depen-
dency parsing systems it is especially worth notic-
ing that our system gives significantly better accu-
racy on completely parsed sentences.
Interestingly, in the experiments, we allow the
parsing algorithm to run many rounds to parse a
sentece in the testing stage. However, we found
that over 99% sentences can be parsed in a single
round. This supports for our justification about the
correctness of our model.
5 Further Work and Conclusion
We have addressed the problem of using learned
classifiers in a pipeline fashion, where a task is de-
composed into several stages and stage classifiers
are used sequentially, where each stage may use
the outcome of previous stages as its input. This
is a common computational strategy in natural lan-
guage processing and is known to suffer from error
accumulation and an inability to correct mistakes
in previous stages.
Sent. Len. DA RA CA LA
<11 93.4 96.7 85.2 94.6
11-20 92.4 93.7 56.1 94.7
21-30 90.4 91.8 32.5 93.4
31-40 90.4 89.8 16.8 94.0
>40 89.7 87.9 8.7 93.3
Table 4: The effect of sentences length. The ex-
periment is done with depth = 4.
71
Train-Test DA RA CA LA
gold?pos 90.7 92.0 40.8 93.8
pos?pos 90.8 92.3 40.7 94.0
gold?gold 92.0 93.9 43.6 95.0
Table 5: Comparing different sources of POS tag-
ging in a pipeline model. We set depth= 4 in all
the experiments of this table.
System DA RA CA LA
Y&M03 90.3 91.6 38.4 93.5
N&S04 87.3 84.3 30.4 N/A
M&C&P05 90.9 94.2 37.5 N/ACurrent Work 90.8 92.3 40.7 94.0
Table 6: The comparison between the current
work with other dependency parsing systems.
We abstracted two natural principles, one which
calls for making the local classifiers used in the
computation more reliable and a second, which
suggests to devise the pipeline algorithm in such
a way that minimizes the number of decisions (ac-
tions) made.
We study this framework in the context of de-
signing a bottom up dependency parsing. Not only
we manage to use this framework to justify several
design decisions, but we also show experimentally
that following these results in improving the accu-
racy of the inferred trees relative to existing mod-
els. Interestingly, we can show that the trees pro-
duced by our algorithm are relatively good even
for long sentences, and that our algorithm is do-
ing especially well when evaluated globally, at a
sentence level, where our results are significantly
better than those of existing approaches ? perhaps
showing that the design goals were achieved.
Our future work includes trying to generalize
this work to non-projective dependency parsing,
as well as attempting to incorporate additional
sources of information (e.g., shallow parsing in-
formation) into the pipeline process.
6 Acknowledgements
We thank Ryan McDonald for providing the anno-
tated data set and to Vasin Punyakanok for useful
comments and suggestions.
This research is supported by the Advanced
Research and Development Activity (ARDA)?s
Advanced Question Answering for Intelligence
(AQUAINT) Program and a DOI grant under the
Reflex program.
References
A. V. Aho, R. Sethi, and J. D. Ullman. 1986. Compilers:
Principles, techniques, and tools. In Addison-Wesley Pub-lishing Company, Reading, MA.
A. Carlson, C. Cumby, J. Rosen, and D. Roth. 1999.
The SNoW learning architecture. Technical Report
UIUCDCS-R-99-2101, UIUC Computer Science Depart-
ment, May.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. the Inter-national Conference on Computational Linguistics (COL-ING), pages 340?345, Copenhagen, August.
A. Haghighi, A. Ng, and C. Manning. 2005. Robust textual
inference via graph matching. In Proceedings of HumanLanguage Technology Conference and Conference on Em-pirical Methods in Natural Language Processing, pages
387?394, Vancouver, British Columbia, Canada, October.
Association for Computational Linguistics.
T. Marciniak and M. Strube. 2005. Beyond the pipeline: Dis-
crete optimization in NLP. In Proceedings of the NinthConference on Computational Natural Language Learn-ing (CoNLL-2005), pages 136?143, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
M. P. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330,
June.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc. ofthe Annual Meeting of the ACL, pages 91?98, Ann Arbor,
Michigan.
J. Nivre and M. Scholz. 2004. Deterministic dependency
parsing of english text. In COLING2004, pages 64?70.
Joakim Nivre. 2003. An efficient algorithm for projective
dependency parsing. In IWPT, Nancy, France.
V. Punyakanok, D. Roth, and W. Yih. 2005. The necessity
of syntactic parsing for semantic role labeling. In Proc.of the International Joint Conference on Artificial Intelli-gence (IJCAI), pages 1117?1123.
A. Ratnaparkhi. 1997. A linear observed time statistical
parser based on maximum entropy models. In EMNLP-97, The Second Conference on Empirical Methods in Nat-ural Language Processing, pages 1?10.
D. Roth and W. Yih. 2004. A linear programming for-
mulation for global inference in natural language tasks.
In Hwee Tou Ng and Ellen Riloff, editors, Proc. of theAnnual Conference on Computational Natural LanguageLearning (CoNLL), pages 1?8. Association for Computa-
tional Linguistics.
D. Roth. 1998. Learning to resolve natural language ambi-
guities: A unified approach. In Proc. National Conferenceon Artificial Intelligence, pages 806?813.
K. Toutanova, D. Klein, and C. Manning. ?2003?. Feature-
rich part-of-speech tagging with a cyclic dependency net-
work. In Proceedings of HLT-NAACL 03.
H. Yamada and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In IWPT2003.
H. Yamada. 2006. Private communication.
72
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 186?190, New York City, June 2006. c?2006 Association for Computational Linguistics
A Pipeline Model for Bottom-Up Dependency Parsing
Ming-Wei Chang Quang Do Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{mchang21, quangdo2, danr}@uiuc.edu
Abstract
We present a new machine learning frame-
work for multi-lingual dependency pars-
ing. The framework uses a linear, pipeline
based, bottom-up parsing algorithm, with
a look ahead local search that serves to
make the local predictions more robust.
As shown, the performance of the first
generation of this algorithm is promising.
1 System Description
1.1 Parsing as a Pipeline
Pipeline computation is a common computational
strategy in natural language processing, where a task
is decomposed into several stages that are solved se-
quentially. For example, a semantic role labeling
program may start by using a part-of-speech tagger,
than apply a shallow parser to chunk the sentence
into phrases, and continue by identifying predicates
and arguments and then classifying them.
(Yamada and Matsumoto, 2003) proposed a
bottom-up dependency parsing algorithm, where the
local actions, chosen from among Shift, Left, Right,
are used to generate a dependency tree using a
shift-reduce parsing approach. Moreover, they used
SVMs to learn the parsing decisions between pairs
of consecutive words in the sentences 1. This is
a true pipeline approach in that the classifiers are
trained on individual decisions rather than on the
overall quality of the parser, and chained to yield the
1A pair of words may become consecutive after the words
between them become the children of these two words
global structure. It suffers from the limitations of
pipeline processing, such as accumulation of errors,
but nevertheless, yields very competitive parsing re-
sults.
We devise two natural principles for enhancing
pipeline models. First, inference procedures should
be incorporated to make robust prediction for each
stage. Second, the number of predictions should
be minimized to prevent error accumulation. Ac-
cording to these two principles, we propose an im-
proved pipeline framework for multi-lingual depen-
dency parsing that aims at addressing the limitations
of the pipeline processing. Specifically, (1) we use
local search, a look ahead policy, to improve the ac-
curacy of the predicted actions, and (2) we argue that
the parsing algorithm we used minimizes the num-
ber of actions (Chang et al, 2006).
We use the set of actions: Shift, Left, Right, Wait-
Left, WaitRight for the parsing algorithm. The pure
Wait action was suggested in (Yamada and Mat-
sumoto, 2003). However, here we come up with
these five actions by separating actions Left into
(real) Left and WaitLeft, and Right into (real) Right
and WaitRight. Predicting these turns out to be eas-
ier due to finer granularity. We then use local search
over consecutive actions and better exploit the de-
pendencies among them.
The parsing algorithm is a modified shift-reduce
parser (Aho et al, 1986) that makes use of the ac-
tions described above and applies them in a left
to right manner on consecutive word pairs (a, b)
(a < b) in the word list T . T is initialized as the full
sentence. Latter, the actions will change the contents
of T . The actions are used as follows:
186
Shift: there is no relation between a and b.
Right: b is the parent of a,
Left: a is the parent of b
WaitLeft: a is the parent of b, but it?s possible that
b is a parent of other nodes. Action is deferred.
The actions control the procedure of building
trees. When Left or Right is performed, the algo-
rithm has found a parent and a child. Then, the func-
tion deleteWord will be called to eliminate the child
word, and the procedure will be repeated until the
tree is built. In projective languages, we discovered
that action WaitRight is not needed. Therefore, for
projective languages, we just need 4 actions.
In order to complete the description of the algo-
rithm we need to describe which pair of consecu-
tive words to consider once an action is taken. We
describe it via the notion of the focus point, which
represents the index of the current word in T . In
fact, determining the focus point does not affect the
correctness of the algorithm. It is easy to show that
any pair of consecutive words in the sentence can
be considered next. If the correct action is chosen
for the corresponding pair, this will eventually yield
the correct tree (but may necessitate multiple cycles
through the sentence).
In practice, however, the actions chosen will be
noisy, and a wasteful focus point policy will result
in a large number of actions, and thus in error accu-
mulation. To minimize the number of actions taken,
we want to find a good focus point placement policy.
There are many natural placement policies that we
can consider (Chang et al, 2006). In this paper, ac-
cording to the policy we used, after S and WL, the
focus point moves one word to the right. After L or
R, we adopt the policy Step Back: the focus moves
back one word to the left. Although the focus place-
ment policy here is similar to (Yamada and Mat-
sumoto, 2003), they did not explain why they made
this choice. In (Chang et al, 2006), we show that
the policy movement used here minimized the num-
ber of actions during the parsing procedure. We can
also show that the algorithm can parse a sentence
with projective relationships in only one round.
Once the parsing algorithm, along with the focus
point policy, is determined, we can train the action
classifiers. Given an annotated corpus, the parsing
algorithm is used to determine the action taken for
each consecutive pair; this is used to train a classifier
Algorithm 1 Pseudo Code of the dependency pars-
ing algorithm. getFeatures extracts the features
describing the currently considered pair of words;
getAction determines the appropriate action for the
pair; assignParent assigns the parent for the child
word based on the action; and deleteWord deletes the
word which become child once the action is taken.
Let t represents for a word and its part of speech
For sentence T = {t1, t2, . . . , tn}
focus= 1
while focus< |T | do
~v = getFeatures(tfocus, tfocus+1)
? = getAction(tfocus, tfocus+1, ~v)
if ? = L or ? = R then
assignParent(tfocus, tfocus+1, ?)
deleteWord(T, focus, ?)
// performing Step Back here
focus = focus ? 1
else
focus = focus + 1
end if
end while
to predict one of the four actions. The details of the
classifier and the features are given in Section 3.
When we apply the trained model on new data,
the sentence is processed from left to right to pro-
duce the predicted dependency tree. The evaluation
process is somewhat more involved, since the action
classifier is not used as it is, but rather via a local
search inference step. This is described in Section 2.
Algorithm 1 depicts the pseudo code of our parsing
algorithm.
Our algorithm is designed for projective lan-
guages. For non-projective relationships in some
languages, we convert them into near projective
ones. Then, we directly apply the algorithm on mod-
ified data in training stage. Because the sentences in
some language, such as Czech, etc. , may have multi
roots, in our experiment, we ran multiple rounds of
Algorithm 1 to build the tree.
1.2 Labeling the Type of Dependencies
In our work, labeling the type of dependencies is
a post-task after the phase of predicting the head
for the tokens in the sentences. This is a multi-
class classification task. The number of the de-
187
pendency types for each language can be found in
the organizer?s introduction paper of the shared task
of CoNLL-X. In the phase of learning dependency
types, the parent of the tokens, which was labeled
in the first phase, will be used as features. The pre-
dicted actions can help us to make accurate predic-
tions for dependency types.
1.3 Dealing with Crossing Edges
The algorithm described in previous section is pri-
marily designed for projective languages. To deal
with non-projective languages, we use a similar ap-
proach of (Nivre and Nilsson, 2005) to map non-
projective trees to projective trees. Any single
rooted projective dependency tree can be mapped
into a projective tree by the Lift operation. The
definition of Lift is as follows: Lift(wj ? wk) =
parent(wj) ? wk, where a ? b means that a is the
parent of b, and parent is a function which returns
the parent word of the given word. The procedure is
as follows. First, the mapping algorithm examines if
there is a crossing edge in the current tree. If there is
a crossing edge, it will perform Lift and replace the
edge until the tree becomes projective.
2 Local Search
The advantage of a pipeline model is that it can use
more information that is taken from the outcomes
of previous prediction. However, this may result in
accumulating error. Therefore, it is essential for our
algorithm to use a reliable action predictor. This mo-
tivates the following approach for making the local
prediction in a pipeline model more reliable. Infor-
mally, we devise a local search algorithm and use it
as a look ahead policy, when determining the pre-
dicted action.
In order to improve the accuracy, we might want
to examine all the combinations of actions proposed
and choose the one that maximizes the score. It is
clearly intractable to find the global optimal predic-
tion sequence in a pipeline model of the depth we
consider. The size of the possible action sequence
increases exponentially so that we can not examine
every possibility. Therefore, a local search frame-
work which uses additional information, however, is
suitable and tractable.
The local search algorithm is presented in Al-
Algorithm 2 Pseudo code for the local search al-
gorithm. In the algorithm, y represents the a action
sequence. The function search considers all possible
action sequences with |depth| actions and returns
the sequence with highest score.
Algo predictAction(model, depth, State)
x = getNextFeature(State)
y = search(x, depth, model, State)
lab = y[1]
State = update(State, lab)
return lab
Algo search(x, depth, model, State)
maxScore = ??
F = {y | ?y? = depth}
for y in F do
s = 0, TmpState = State
for i = 1 . . . depth do
x = getNextFeature(TmpState)
s = s + log(score(y[i], x))
TmpState = update(TmpState, y[i])
end for
if s > maxScore then
y? = y
maxScore = s
end if
end for
return y?
gorithm 2. The algorithm accepts two parameters,
model and depth. We assume a classifier that can
give a confidence in its prediction. This is repre-
sented here by model. depth is the parameter de-
termining the depth of the local search. State en-
codes the configuration of the environment (in the
context of the dependency parsing this includes the
sentence, the focus point and the current parent and
children for each node). Note that the features ex-
tracted for the action classifier depends on State, and
State changes by the update function when a predic-
tion is made. In this paper, the update function cares
about the child word elimination, relationship addi-
tion and focus point movement.
The search algorithm will perform a search of
length depth. Additive scoring is used to score the
sequence, and the first action in this sequence is per-
formed. Then, the State is updated, determining the
188
next features for the action classifiers and search is
called again.
One interesting property of this framework is that
we use future information in addition to past infor-
mation. The pipeline model naturally allows access
to all the past information. But, since our algorithm
uses the search as a look ahead policy, it can produce
more robust results.
3 Experiments and Results
In this work we used as our learning algorithm a
regularized variation of the perceptron update rule
as incorporated in SNoW (Roth, 1998; Carlson et
al., 1999), a multi-class classifier that is specifically
tailored for large scale learning tasks. SNoW uses
softmax over the raw activation values as its confi-
dence measure, which can be shown to be a reliable
approximation of the labels? probabilities. This is
used both for labeling the actions and types of de-
pendencies. There is no special language enhance-
ment required for each language. The resources pro-
vided for 12 languages are described in: (Hajic? et
al., 2004; Chen et al, 2003; Bo?hmova? et al, 2003;
Kromann, 2003; van der Beek et al, 2002; Brants
et al, 2002; Kawata and Bartels, 2000; Afonso et
al., 2002; Dz?eroski et al, 2006; Civit Torruella and
Mart?? Anton??n, 2002; Nilsson et al, 2005; Oflazer et
al., 2003; Atalay et al, 2003).
3.1 Experimental Setting
The feature set plays an important role in the qual-
ity of the classifier. Basically, we used the same
feature set for the action selection classifiers and
for the label classifiers. In our work, each exam-
ple has average fifty active features. For each word
pair (w1, w2), we used their LEMMA, the POSTAG
and also the POSTAG of the children of w1 and
w2. We also included the LEMMA and POSTAG
of surrounding words in a window of size (2, 4).
We considered 2 words before w1 and 4 words af-
ter w2 (we agree with the window size in (Yamada
and Matsumoto, 2003)). The major difference of
our feature set compared with the one in (Yamada
and Matsumoto, 2003) is that we included the pre-
vious predicted action. We also added some con-
junctions of the above features to ensure expressive-
ness of the model. (Yamada and Matsumoto, 2003)
made use of the polynomial kernel of degree 2 so
they in fact use more conjunctive features. Beside
these features, we incorporated the information of
FEATS for the languages when it is available. The
columns in the data files we used for our work are
the LEMMA, POSTAG, and the FEATS, which is
treated as atomic. Due to time limitation, we did not
apply the local search algorithm for the languages
having the FEATS features.
3.2 Results
Table 1 shows our results on Unlabeled Attachment
Scores (UAS), Labeled Attachment Scores (LAS),
and Label Accuracy score (LAC) for 12 languages.
Our results are compared with the average scores
(AV) and the standard deviations (SD), of all the sys-
tems participating in the shared task of CoNLL-X.
Our average UAS for 12 languages is 83.54%
with the standard deviation 6.01; and 76.80% with
the standard deviation 9.43 for average LAS.
4 Analysis and Discussion
We observed that our UAS for Arabic is generally
lower than for other languages. The reason for the
low accuracy of Arabic is that the sentence is very
long. In the training data for Arabic, there are 25%
sentences which have more than 50 words. Since
we use a pipeline model in our algorithm, it required
more predictions to complete a long sentence. More
predictions in pipeline models may result in more
mistakes. We think that this explains our relatively
low Arabic result. Moreover, in our current system,
we use the same window size (2,4) for feature ex-
traction in all languages. Changing the windows size
seems to be a reasonable step when the sentences are
longer.
For Czech, one reason for our relatively low result
is that we did not use the whole training corpus due
to time limitation 2 . Actually, in our experiment
on the development set, when we increase the size
of training data in the training phase we got signif-
icantly higher result than the system trained on the
smaller data. The other problem for Czech is that
Czech is one of the languages with many types of
part of speech and dependency types, and also the
2Training our system for most languages takes 30 minutes
or 1 hour for both phases of labeling HEAD and DEPREL. It
takes 6-7 hours for Czech with 50% training data.
189
Language UAS LAS LAC
Ours AV SD Ours AV SD Ours AV SD
Arabic 76.09 73.48 4.94 60.92 59.94 6.53 75.69 75.12 5.49
Chinese 89.60 84.85 5.99 85.05 78.32 8.82 87.28 81.66 7.92
Czech 81.78 77.01 6.70 72.88 67.17 8.93 80.42 76.59 7.69
Danish 86.85 84.52 8.97 80.60 78.31 11.34 86.51 84.50 4.35
Dutch 76.25 75.07 5.78 72.91 70.73 6.66 80.15 77.57 5.92
German 86.90 82.60 6.73 84.17 78.58 7.51 91.03 86.26 6.01
Japanese 90.77 89.05 5.20 89.07 85.86 7.09 92.18 89.90 5.36
Portuguese 88.60 86.46 4.17 83.99 80.63 5.83 88.84 85.35 5.45
Slovene 80.32 76.53 4.67 69.52 65.16 6.78 79.26 76.31 6.40
Spanish 83.09 77.76 7.81 79.72 73.52 8.41 89.26 85.71 4.56
Swedish 89.05 84.21 5.45 82.31 76.44 6.46 84.82 80.00 6.24
Turkish 73.15 69.35 5.51 60.51 55.95 7.71 73.75 69.59 7.94
Table 1: Our results are compared with the average scores. UAS=Unlabeled Attachment Score,
LAS=Labeled Attachment Score, LAC=Label Accuracy, AV=Average score, and SD=standard deviation.
length of the sentences in Czech is relatively long.
These facts make recognizing the HEAD and the
types of dependencies more difficult.
Another interesting aspect is that we have not
used the information about the syntactic and/or mor-
phological features (FEATS) properly. For the lan-
guages for which FEATS is available, we have a
larger gap, compared with the top system.
5 Further Work and Conclusion
In the shared task of CoNLL-X, we have shown that
our dependency parsing system can do well on mul-
tiple languages without requiring special knowledge
for each of the languages.
From a technical perspective, we have addressed
the problem of using learned classifiers in a pipeline
fashion, where a task is decomposed into several
stages and classifiers are used sequentially to solve
each stage. This is a common computational strat-
egy in natural language processing and is known to
suffer from error accumulation and an inability to
correct mistakes in previous stages. We abstracted
two natural principles, one which calls for making
the local classifiers used in the computation more
reliable and a second, which suggests to devise the
pipeline algorithm in such a way that it minimizes
the number of actions taken.
However, since we tried to build a single approach
for all languages, we have not fully utilized the capa-
bilities of our algorithms. In future work we will try
to specify both features and local search parameters
to the target language.
Acknowledgement This research is supported by
NSF ITR IIS-0428472, a DOI grant under the Reflex
program and ARDA?s Advanced Question Answer-
ing for Intelligence (AQUAINT) program.
References
A. V. Aho, R. Sethi, and J. D. Ullman. 1986. Compilers:
Principles, techniques, and tools. In Addison-Wesley
Publishing Company, Reading, MA.
A. Carlson, C. Cumby, J. Rosen, and D. Roth. 1999.
The SNoW learning architecture. Technical Report
UIUCDCS-R-99-2101, UIUC Computer Science De-
partment, May.
M. Chang, Q. Do, and D. Roth. 2006. Local search
for bottom-up dependency parsing. Technical report,
UIUC Computer Science Department.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05).
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
pages 806?813.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
IWPT2003.
190
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1099?1109,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Constraints based Taxonomic Relation Classification
Quang Xuan Do Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{quangdo2,danr}@illinois.edu
Abstract
Determining whether two terms in text have
an ancestor relation (e.g. Toyota and car) or
a sibling relation (e.g. Toyota and Honda) is
an essential component of textual inference in
NLP applications such as Question Answer-
ing, Summarization, and Recognizing Textual
Entailment. Significant work has been done
on developing stationary knowledge sources
that could potentially support these tasks, but
these resources often suffer from low cover-
age, noise, and are inflexible when needed to
support terms that are not identical to those
placed in them, making their use as general
purpose background knowledge resources dif-
ficult. In this paper, rather than building a sta-
tionary hierarchical structure of terms and re-
lations, we describe a system that, given two
terms, determines the taxonomic relation be-
tween them using a machine learning-based
approach that makes use of existing resources.
Moreover, we develop a global constraint opti-
mization inference process and use it to lever-
age an existing knowledge base also to enforce
relational constraints among terms and thus
improve the classifier predictions. Our exper-
imental evaluation shows that our approach
significantly outperforms other systems built
upon existing well-known knowledge sources.
1 Introduction
Taxonomic relations that are read off of structured
ontological knowledge bases have been shown to
play important roles in many computational linguis-
tics tasks, such as document clustering (Hotho et
al., 2003), navigating text databases (Chakrabarti et
al., 1997), Question Answering (QA) (Saxena et al,
2007) and summarization (Vikas et al, 2008). It
is clear that the recognition of taxonomic relation
between terms in sentences is essential to support
textual inference tasks such as Recognizing Textual
Entailment (RTE) (Dagan et al, 2006). For exam-
ple, it may be important to know that a blue Toy-
ota is neither a red Toyota nor a blue Honda, but
that all are cars, and even Japanese cars. Work in
Textual Entailment has argued quite convincingly
(MacCartney and Manning, 2008; MacCartney and
Manning, 2009) that many such textual inferences
are largely compositional and depend on the ability
to recognize some basic taxonomic relations such
as the ancestor or sibling relations between terms.
To date, these taxonomic relations can be read off
manually generated ontologies such as Wordnet that
explicitly represent these, and there has also been
some work trying to extend the manually built re-
sources using automatic acquisition methods result-
ing in structured knowledge bases such as the Ex-
tended WordNet (Snow et al, 2006) and the YAGO
ontology (Suchanek et al, 2007).
However, identifying when these relations hold
using fixed stationary hierarchical structures may
be impaired by noise in the resource and by uncer-
tainty in mapping targeted terms to concepts in the
structures. In addition, for knowledge sources de-
rived using bootstrapping algorithms and distribu-
tional semantic models such as (Pantel and Pen-
nacchiotti, 2006; Kozareva et al, 2008; Baroni and
Lenci, 2010), there is typically a trade-off between
precision and recall, resulting either in a relatively
accurate resource with low coverage or a noisy re-
1099
source with broader coverage. In the current work,
we take a different approach, identifying directly
whether a pair of terms hold a taxonomic relation.
Fixed resources, as we observe, are inflexible
when dealing with targeted terms not being cov-
ered. This often happens when targeted terms have
the same meaning, but different surface forms, than
the terms used in the resources (e.g. Toyota Camry
and Camry). We argue that it is essential to have a
classifier that, given two terms, can build a semantic
representation of the terms and determines the tax-
onomic relations between them. This classifier will
make use of existing knowledge bases in multiple
ways, but will provide significantly larger coverage
and more precise results. We make use of a dynamic
resource such as Wikipedia to guarantee increased
coverage without changing our model and also per-
form normalization-to-Wikipedia to find appropri-
ate Wikipedia replacements for outside-Wikipedia
terms. Moreover, stationary resources are usually
brittle because of the way most of them are built:
using local relational patterns (e.g. (Hearst, 1992;
Snow et al, 2005)). Infrequent terms are less likely
to be covered, and some relations may not be sup-
ported well by these methods because their cor-
responding terms rarely appear in close proximity
(e.g., an Israeli tennis player Dudi Sela and Roger
Federrer). Our approach uses search techniques to
gather relevant Wikipedia pages of input terms and
performs a learning-based classification w.r.t. to the
features extracted from these pages as a way to get
around this brittleness.
Motivated by the needs of NLP applications such
as RTE, QA, Summarization, and the composition-
ality argument alluded to above, we focus on identi-
fying two fundamental types of taxonomic relations
- ancestor and sibling. An ancestor relation and its
directionality can help us infer that a statement with
respect to the child (e.g. cannabis) holds for an
ancestor (e.g. drugs) as in the following example,
taken from a textual entailment challenge dataset:
T: Nigeria?s NDLEA has seized 80 metric
tonnes of cannabis in one of its largest ever
hauls, officials say.
H: Nigeria seizes 80 tonnes of drugs.
Similarly, it is important to know of a sibling re-
lation to infer that a statement about Taiwan may
(without additional information) contradict a simi-
lar statement with respect to Japan since these are
different countries, as in the following:
T: A strong earthquake struck off the southern
tip of Taiwan at 12:26 UTC, triggering a warn-
ing from Japan?s Meteorological Agency that
a 3.3 foot tsunami could be heading towards
Basco, in the Philippines.
H: An earthquake strikes Japan.
Several recent TE studies (Abad et al, 2010; Sam-
mons et al, 2010) suggest to isolate TE phenomena,
such as recognizing taxonomic relations, and study
them separately; they discuss some of characteristics
of phenomena such as contradiction from a similar
perspective to ours, but do not provide a solution.
In this paper, we present TAxonomic RElation
Classifier (TAREC), a system that classifies taxo-
nomic relations between a given pair of terms us-
ing a machine learning based classifier. An inte-
gral part of TAREC is also our inference model that
makes use of relational constraints to enforce co-
herency among several related predictions. TAREC
does not aim at building or extracting a hierarchi-
cal structure of concepts and relations, but rather to
directly recognize taxonomic relations given a pair
of terms. Target terms are represented using vector
of features that are extracted from retrieved corre-
sponding Wikipedia pages. In addition, we make
use of existing stationary ontologies to find related
terms to the target terms, and classify those too. This
allows us to make use of a constraint-based infer-
ence model (following (Roth and Yih, 2004; Roth
and Yih, 2007) that enforces coherency of decisions
across related pairs (e.g., if x is-a y and y is-a z, it
cannot be that x is a sibling of z).
In the rest of the paper, after discussing re-
lated work in Section 2, we present an overview of
TAREC in Section 3. The learning component and
the inference model of TAREC are described in Sec-
tions 4 and 5. We experimentally evaluate TAREC
in Section 6 and conclude our paper in Section 7.
2 Related Work
There are several works that aim at building tax-
onomies and ontologies which organize concepts
and their taxonomic relations into hierarchical struc-
tures. (Snow et al, 2005; Snow et al, 2006) con-
1100
structed classifiers to identify hypernym relation-
ship between terms from dependency trees of large
corpora. Terms with recognized hypernym rela-
tion are extracted and incorporated into a man-made
lexical database, WordNet (Fellbaum, 1998), re-
sulting in the extended WordNet, which has been
augmented with over 400, 000 synsets. (Ponzetto
and Strube, 2007) and (Suchanek et al, 2007) both
mined Wikipedia to construct hierarchical structures
of concepts and relations. While the former ex-
ploited Wikipedia category system as a conceptual
network and extracted a taxonomy consisting of sub-
sumption relations, the latter presented the YAGO
ontology, which was automatically constructed by
mining and combining Wikipedia and WordNet. A
natural way to use these hierarchical structures to
support taxonomic relation classification is to map
targeted terms onto the hierarchies and check if
they subsume each other or share a common sub-
sumer. However, this approach is limited because
constructed hierarchies may suffer from noise and
require exact mapping (Section 6). TAREC over-
comes these limitations by searching and selecting
the top relevant articles in Wikipedia for each input
term; taxonomic relations are then recognized based
on the features extracted from these articles.
On the other hand, information extraction boot-
strapping algorithms, such as (Pantel and Pennac-
chiotti, 2006; Kozareva et al, 2008), automatically
harvest related terms on large corpora by starting
with a few seeds of pre-specified relations (e.g. is-
a, part-of). Bootstrapping algorithms rely on some
scoring function to assess the quality of terms and
additional patterns extracted during bootstrapping it-
erations. Similarly, but with a different focus, Open
IE, (Banko and Etzioni, 2008; Davidov and Rap-
poport, 2008), deals with a large number of relations
which are not pre-specified. Either way, the out-
put of these algorithms is usually limited to a small
number of high-quality terms while sacrificing cov-
erage (or vice versa). Moreover, an Open IE sys-
tem cannot control the extracted relations and this is
essential when identifying taxonomic relations. Re-
cently, (Baroni and Lenci, 2010) described a gen-
eral framework of distributional semantic models
that extracts significant contexts of given terms from
large corpora. Consequently, a term can be repre-
sented by a vector of contexts in which it frequently
appears. Any vector space model could then use the
terms? vectors to cluster terms into categories. Sib-
ling terms (e.g. Honda, Toyota), therefore, have very
high chance to be clustered together. Nevertheless,
this approach cannot recognize ancestor relations.
In this paper, we compare TAREC with this frame-
work only on recognizing sibling vs. no relation, in
a strict experimental setting which pre-specifies the
categories to which the terms belong.
3 An Overview of the TAREC Algorithm
3.1 Preliminaries
In the TAREC algorithm, a term refers to any men-
tion in text, such as mountain, George W. Bush, bat-
tle of Normandy. TAREC does not aim at extracting
terms and building a stationary hierarchical structure
of terms, but rather recognize the taxonomic relation
between any two given terms. TAREC focuses on
classifying two fundamental types of taxonomic re-
lations: ancestor and sibling. Determining whether
two terms hold a taxonomic relation depends on a
pragmatic decision of how far one wants to climb up
a taxonomy to find a common subsumer. For exam-
ple, George W. Bush is a child of Presidents of the
United States as well as people, even more, that term
could also be considered as a child of mammals or
organisms w.r.t. the Wikipedia category system; in
that sense, George W. Bush may be considered as a
sibling of oak because they have organisms as a least
common subsumer. TAREC makes use of a hierar-
chical structure as background knowledge and con-
siders two terms to hold a taxonomic relation only
if the relation can be recognized from information
acquired by climbing up at most K levels from the
representation of the target terms in the structure. It
is also possible that the sibling relation can be rec-
ognized by clustering terms together by using vector
space models. If so, two terms are siblings if they
belong to the same cluster.
To cast the problem of identifying taxonomic rela-
tions between two terms x and y in a machine learn-
ing perspective, we model it as a multi-class classi-
fication problem. Table 1 defines four relations with
some examples in our experiment data sets.
This paper focuses on studying a fundamental
problem of recognizing taxonomic relations (given
well-segmented terms) and leaves the orthogonal is-
1101
Examples
Relation Meaning Term x Term y
x? y x is an ancestor actor Mel Gibson
of y food rice
x? y x is a child Makalu mountain
of y Monopoly game
x? y x and y are Paris London
siblings copper oxygen
x= y x and y have Roja C++
no relation egg Vega
Table 1: Taxonomic relations and some examples in our
data sets.
sues of how to take contexts into account and how it
should be used in applications to a future work.
3.2 The Overview of TAREC
Assume that we already have a learned local clas-
sifier that can classify taxonomic relations between
any two terms. Given two terms, TAREC uses
Wikipedia and the local classifier in an inference
model to make a final prediction on the taxonomic
relation between these two. To motivate the need for
an inference model, beyond the local classifier itself,
we observe that the presence of other terms in addi-
tion to the two input terms, can provide some natural
constraints on the possible taxonomic relations and
thus can be used to make the final prediction (which
we also refer as global prediction) more coherent. In
practice, we first train a local classifier (Section 4),
then incorporate it into an inference model (Section
5) to classify taxonomic relations between terms.
The TAREC algorithm consists of three steps and
is summarized in Figure 1 and explained below.
1. Normalizing input terms to Wikipedia: Al-
though most commonly used terms have corre-
sponding Wikipedia articles, there are still a lot of
terms with no corresponding Wikipedia articles. For
a non-Wikipedia term, we make an attempt to find
a replacement by using Web search. We wish to
find a replacement such that the taxonomic relation
is unchanged. For example, for input pair (Lojze
Kovac?ic?, Rudi S?eligo), there is no English Wikipedia
page for Lojze Kovac?ic?, but if we can find Marjan
Roz?anc and use it as a replacement of Lojze Kovac?ic?
(two terms are siblings and refer to two writers), we
can continue classifying the taxonomic relation of
the pair (Marjan Roz?anc, Rudi S?eligo). This part
of the algorithm was motivated by (Sarmento et al,
TAxonomic RElation Classifier (TAREC)
INPUT: A pair of terms (x, y)
A learned local classifierR (Sec. 4)
WikipediaW
OUTPUT: Taxonomic relation r? between x and y
1. (x, y)? NormalizeToWikipedia(x, y,W)
2. Z ? GetAddionalTerms(x, y) (Sec. 5.2)
3. r? = ClassifyAndInference(x, y,Z,R,W) (Sec. 5.1)
RETURN: r?;
Figure 1: The TAREC algorithm.
2007). We first make a query with the two input
terms (e.g. ?Lojze Kovac?ic?? AND ?Rudi S?eligo?)
to search for list-structure snippets in Web docu-
ments1 such as ?... ?delimiter? ca ?delimiter? cb
?delimiter? cc ?delimiter? ...? (the two input terms
should be among ca, cb, cc, ...). The delimiter could
be commas, periods, or asterisks2. For snippets that
contain the patterns of interest, we extract ca, cb, cc
etc. as replacement candidates. To reduce noise,
we empirically constrain the list to contain at least
4 terms that are no longer than 20 characters each.
The candidates are ranked based on their occurrence
frequency. The top candidate with Wikipedia pages
is used as a replacement.
2. Getting additional terms (Section 5.2): TAREC
leverage an existing knowledge base to extract addi-
tional terms related to the input terms, to be used in
the inference model in step 3.
3. Making global prediction with relational con-
straints (Section 5.1): TAREC performs several lo-
cal predictions using the local classifier R (Section
4) on the two input terms and these terms with the
additional ones. The global prediction is then in-
ferred by enforcing relational constraints among the
terms? relations.
4 Learning Taxonomic Relations
The local classifier of TAREC is trained on the
pairs of terms with correct taxonomic relation labels
(some examples are showed in Table 1). The trained
classifier when applied on a new input pair of terms
will return a real valued number which can be inter-
preted as the probability of the predicted label. In
this section, we describe the learning features used
1We use http://developer.yahoo.com/search/web/
2Periods and asterisks capture enumerations.
1102
Title/Term Text Categories
President of
the United
States
The President of the United States is the head of state and head of government of the United States and is the
highest political official in the United States by influence and recognition. The President leads the executive
branch of the federal government and is one of only two elected members of the executive branch...
Presidents of the United States, Presidency of
the United States
George W.
Bush
George Walker Bush; born July 6, 1946) served as the 43rd President of the United States from 2001 to 2009.
He was the 46th Governor of Texas from 1995 to 2000 before being sworn in as President on January 20, 2001...
Children of Presidents of the United States, Gov-
ernors of Texas, Presidents of the United States,
Texas Republicans...
Gerald Ford Gerald Rudolff Ford (born Leslie Lynch King, Jr.) (July 14, 1913 December 26, 2006) was the 38th President
of the United States, serving from 1974 to 1977, and the 40th Vice President of the United States serving from
1973 to 1974.
Presidents of the United States, Vice Presidents
of the United States, Republican Party (United
States) presidential nominees...
Table 2: Examples of texts and categories of Wikipedia articles.
by our local taxonomic relation classifier.
Given two input terms, we first build a semantic
representation for each term by using a local search
engine3 to retrieve a list of top articles in Wikipedia
that are relevant to the term. To do this, we use the
following procedure: (1) Using both terms to make a
query (e.g. ?George W. Bush? AND ?Bill Clinton?)
to search in Wikipedia ; (2) Extracting important
keywords in the titles and categories of the retrieved
articles using TF-IDF (e.g. president, politician); (3)
Combining each input term with the extracted key-
words (e.g. ?George W. Bush? AND ?president?
AND ?politician?) to create a final query used to
search for the term?s relevant articles in Wikipedia.
This is motivated by the assumption that the real
world applications calling TAREC typically does so
with two terms that are related in some sense, so our
procedure is designed to exploit that. For example,
it?s more likely that term Ford in the pair (George
W. Bush, Ford) refers to the former president of the
United States, Gerald Ford, than the founder of Ford
Motor Company, Henry Ford.
Once we have a semantic representation of each
term, in the form of the extracted articles, we extract
from it features that we use as the representation of
the two input terms in our learning algorithm. It is
worth noting that a Wikipedia page usually consists
of a title (i.e. the term), a body text, and a list of
categories to which the page belongs. Table 2 shows
some Wikipedia articles. From now on, we use the
titles of x, the texts of x, and the categories of x to
refer to the titles, texts, and categories of the asso-
ciated articles in the representation of x. Below are
the learning features extracted for input pair (x,y).
Bags-of-words Similarity: We use cosine simi-
larity metric to measure the degree of similarity be-
tween bags of words. We define four bags-of-words
features as the degree of similarity between the texts
3E.g. http://lucene.apache.org/
Degree of similarity
texts(x) vs. categories(y)
categories(x) vs. texts(y)
texts(x) vs. texts(y)
categories(x) vs. categories(y)
Table 3: Bag-of-word features of the pair of terms (x,y);
texts(.) and categories(.) are two functions that extract
associated texts and categories from the semantic repre-
sentation of x and y.
and categories associated with two input terms x and
y in Table 3. To collect categories of a term, we take
the categories of its associated articles and go up K
levels in the Wikipedia category system. In our ex-
periments, we use abstracts of Wikipedia articles in-
stead of whole texts.
Association Information: This features repre-
sents a measure of association between the terms
by considering their information overlap. We cap-
ture this feature by the pointwise mutual informa-
tion (pmi) which quantifies the discrepancy between
the probability of two terms appearing together ver-
sus the probability of each term appearing indepen-
dently4. The pmi of two terms x and y is estimated
as follows:
pmi(x, y) = log
p(x, y)
p(x)p(y)
= log
Nf(x, y)
f(x)f(y)
,
where N is the total number of Wikipedia articles,
and f(.) is the function which counts the number of
appearances of its argument.
Overlap Ratios: The overlap ratio features cap-
ture the fact that the titles of a term usually overlap
with the categories of its descendants. We measure
this overlap as the ratio of the number of common
phrases used in the titles of one term and the cate-
gories of the other term. In our context, a phrase is
4pmi is different than mutual information. The former ap-
plies to specific outcomes, while the latter is to measure the
mutual dependence of two random variables.
1103
considered to be a common phrase if it appears in the
titles of one term and the categories of the other term
and it is also of the following types: (1) the whole
string of a category, or (2) the head in the root form
of a category, or (3) the post-modifier of a category.
We use the Noun Group Parser from (Suchanek et
al., 2007) to extract the head and post-modifier from
a category. For example, one of the categories of an
article about Chicago is Cities in Illinois. This cate-
gory can be parsed into a head in its root form City,
and a post-modifier Illinois. Given term pair (City,
Chicago), we observe that City matches the head of
the category Cities in Illinois of term Chicago. This
is a strong indication that Chicago is a child of City.
We also use a feature that captures the overlap
ratio of common phrases between the categories of
two input terms. For this feature, we do not use the
post-modifier of the categories. We use Jaccard sim-
ilarity coefficient to measure these overlaps ratios.
5 Inference with Relational Constraints
Once we have a local multi-class classifier that maps
a given pair of terms to one of the four possible rela-
tions, we use a constraint-based optimization algo-
rithm to improve this prediction. The key insight
behind the way we model the inference model is
that if we consider more than two terms, there are
logical constraints that restrict the possible relations
among them. For instance, George W. Bush can-
not be an ancestor or sibling of president if we are
confident that president is an ancestor of Bill Clin-
ton, and Bill Clinton is a sibling of George W. Bush.
We call the combination of terms and their relations
a term network. Figure 2 shows some n-term net-
works consisting of two input terms (x, y), and ad-
ditional terms z, w, v.
The aforementioned observations show that if we
can obtain additional terms that are related to the
two target terms, we can enforce such coherency
relational constraints and make a global prediction
that would improve the prediction of the taxonomic
relation between the two given terms. Our infer-
ence model follows constraint-based formulations
that were introduced in the NLP community and
were shown to be very effective in exploiting declar-
ative background knowledge (Roth and Yih, 2004;
Denis and Baldridge, 2007; Punyakanok et al, 2008;
Chang et al, 2008).
George W.
Bush
President
Bill Clinton
x
y
z
Red Green
Blue
x
y
z
(a) (b)
Honda Toyota
car
manufacturer
x
y
z
w
BMW
Celcius meter
temperature
x
y
z
w
length
(d)
(c)
v
physical
quantities
Figure 2: Examples of n-term networks with two input
term x and y. (a) and (c) show valid combinations of
edges, whereas (b) and (d) are two relational constraints.
For simplicity, we do not draw no relation edges in (d).
5.1 Enforcing Coherency through Inference
Let x, y be two input terms, and Z =
{z1, z2, ..., zm} be a set of additional terms. For a
subset Z ? Z , we construct a set of term networks
whose nodes are x, y and all elements in Z, and the
edge, e, between every two nodes is one of four tax-
onomic relations whose weight, w(e), is given by
a local classifier (Section 4). If l = |Z|, there are
n = 2 + l nodes in each network, and 4[
1
2n(n?1)]
term networks can be constructed. In our experi-
ments we only use 3-term networks (i.e. l = 1).
For example, for the input pair (red, green) and
Z = {blue, yellow}, we can construct 64 networks
for the triple ?red, green, Z = {blue}? and 64 net-
works for ?red, green, Z = {yellow}? by trying all
possible relations between the terms.
A relational constraint is defined as a term net-
work consisting of only its ?illegitimate? edge set-
tings, those that belongs to a pre-defined list of in-
valid edge combinations. For example, Figure 2b
shows an invalid network where red is a sibling of
both green and blue, and green is an ancestor of blue.
In Figure 2d, Celcius and meter cannot be siblings
because they are children of two sibling terms tem-
perature and length. The relational constraints used
in our experiments are manually constructed.
Let C be a list of relational constraints. Equation
(1) defines the network scoring function, which is a
linear combination of the edge weights, w(e), and
the penalties, ?k, of term networks matching con-
straint Ck ? C.
score(t) =
?
e?t
w(e)?
|C|?
k=1
?kdCk(t) (1)
function dCk(t) indicates if t matches Ck. In our
work, we use relational constraints as hard con-
1104
YAGO Query Patterns
INPUT: term ?x?
OUTPUT: lists of ancestors, siblings, and children of ?x?
Pattern 1 Pattern 2 Pattern 3
?x? MEANS ?A ?x? MEANS ?A ?x? MEANS ?D
?A SUBCLASSOF ?B ?A TYPE ?B ?E TYPE ?D
?C SUBCLASSOF ?B ?C TYPE ?B
RETURN: ?B, ?C, ?E as
lists of ancestors, siblings, and children, respectively.
Figure 3: Our YAGO query patterns used to obtain related
terms for ?x?.
straints and set their penalty ?k to ?. For a set of
term networks formed by ?x, y, Z? and all possible
relations between the terms, we select the best net-
work, t? = argmaxtscore(t).
After picking the best term network t? for every
Z ? Z , we make the final decision on the taxonomic
relation between x and y. Let r denote the relation
between x and y in a particular t? (e.g. r = x? y.)
The set of all t? is divided into 4 groups with respect
to r (e.g. a group of all t? having r = x ? y, a
group of all t? having r = x ? y.) We denote a
group with term networks holding r as the relation
between x and y by Tr. To choose the best taxo-
nomic relation, r?, of x and y, we solve the objective
function defined in Equation 2.
r? = argmaxr
1
|Tr|
?
t??Tr
?t?score(t
?) (2)
where ?t is the weight of term network t, defined
as the occurrence probability of t (regarding only its
edges? setting) in the training data, which is aug-
mented with additional terms. Equation (2) finds the
best taxonomic relation of two input terms by com-
puting the average score of every group of the best
term networks representing a particular relation of
two input terms.
5.2 Extracting Related Terms
In the inference model, we need to obtain other
terms that are related to the two input terms. Here-
after, we refer to additional terms as related terms.
The related term space is a space of direct ancestors,
siblings and children in a particular resource.
We propose an approach that uses the YAGO on-
tology (Suchanek et al, 2007) to provide related
terms. It is worth noting that YAGO is chosen over
the Wikipedia category system used in our work be-
cause YAGO is a clean ontology built by carefully
combining Wikipedia and WordNet.5
In YAGO model, all objects (e.g. cities, people,
etc.) are represented as entities. To map our input
terms to entities in YAGO, we use the MEANS re-
lation defined in the YAGO ontology. Furthermore,
similar entities are grouped into classes. This allows
us to obtain direct ancestors of an entity by using
the TYPE relation which gives the entity?s classes.
Furthermore, we can get ancestors of a class with
the SUBCLASSOF relation6. By using three relations
MEANS, TYPE and SUBCLASSOF in YAGO model,
we can obtain Proposals for direct ancestors, sib-
lings, and children, if any, for any input term. We
then evaluate our classifier on all pairs, and run the
inference to improve the prediction using the co-
herency constraints. Figure 3 presents three patterns
that we used to query related terms from YAGO.
6 Experimental Study
In this section, we evaluate TAREC against several
systems built upon existing well-known knowledge
sources. The resources are either hierarchical struc-
tures or extracted by using distributional semantic
models. We also perform several experimental anal-
yses to understand TAREC?s behavior in details.
6.1 Comparison to Hierarchical Structures
We create and use two main data sets in our ex-
periments. Dataset-I is generated from 40 seman-
tic classes of about 11,000 instances. The orig-
inal semantic classes and instances were manu-
ally constructed with a limited amount of manual
post-filtering and were used to evaluate informa-
tion extraction tasks in (Pas?ca, 2007; Pas?ca and
Van Durme, 2008) (we refer to this original data as
OrgData-I). This dataset contains both terms with
Wikipedia pages (e.g. George W. Bush) and non-
Wikipedia terms (e.g. hindu mysticism). Pairs of
terms are generated by randomly pairing seman-
tic class names and instances. We generate dis-
joint training and test sets of 8,000 and 12,000 pairs
of terms, respectively. We call the test set of this
5However, YAGO by itself is weaker than our approach in
identifying taxonomic relations (see Section 6.)
6These relations are defined in the YAGO ontology.
1105
dataset Test-I. Dataset-II is generated from 44 se-
mantic classes of more than 10,000 instances used
in (Vyas and Pantel, 2009)7. The original semantic
classes and instances were extracted from Wikipedia
lists. This data, therefore, only contains terms with
corresponding Wikipedia pages. We also generate
disjoint training and test sets of 8,000 and 12,000
pairs of terms, respectively, and call the test set of
this dataset Test-II.8
Several semantic class names in the original data
are written in short forms (e.g. chemicalelem,
proglanguage). We expand these names to some
meaningful names which are used by all systems in
our experiments. For example, terroristgroup is ex-
panded to terrorist group, terrorism. Table 1 shows
some pairs of terms which are generated. Four types
of taxonomic relations are covered with balanced
numbers of examples in all data sets. To evaluate our
systems, we use a snapshot of Wikipedia from July,
2008. After cleaning and removing articles without
categories (except redirect pages), 5,503,763 articles
remain. We index these articles using Lucene9. As
a learning algorithm, we use a regularized averaged
Perceptron (Freund and Schapire, 1999).
We compare TAREC with three systems that we
built using recently developed large-scale hierarchi-
cal structures. Strube07 is built on the latest ver-
sion of a taxonomy, TStrube, which was derived from
Wikipedia (Ponzetto and Strube, 2007). It is worth
noting that the structure of TStrube is similar to the
page structure of Wikipedia. For a fair comparison,
we first generate a semantic representation for each
input term by following the same procedure used in
TAREC described in Section 4. The titles and cat-
egories of the articles in the representation of each
input term are then extracted. Only titles and their
corresponding categories that are in TStrube are con-
sidered. A term is an ancestor of the other if at
least one of its titles is in the categories of the other
term. If two terms share a common category, they
are considered siblings; and no relation, otherwise.
The ancestor relation is checked first, then sibling,
and finally no relation. Snow06 uses the extended
7There were 50 semantic classes in the original dataset. We
grouped some semantically similar classes for the purpose of
classifying taxonomic relations.
8Published at http://cogcomp.cs.illinois.edu/page/software
9http://lucene.apache.org, version 2.3.2
Test-I Test-II
Strube07 24.32 25.63
Snow06 41.97 36.26
Yago07 65.93 70.63
TAREC (local) 81.89 84.7
TAREC 85.34 86.98
Table 4: Evaluating and comparing performances, in ac-
curacy, of the systems on Test-I and Test-II. TAREC (lo-
cal) uses only our local classifier to identify taxonomic re-
lations by choosing the relation with highest confidence.
WordNet (Snow et al, 2006). Words in the extended
WordNet can be common nouns or proper nouns.
Given two input terms, we first map them onto the
hierarchical structure of the extended WordNet by
exact string matching. A term is an ancestor of the
other if it can be found as an hypernym after going
up K levels in the hierarchy from the other term. If
two terms share a common subsumer within some
levels, then they are considered as siblings. Oth-
erwise, there is no relation between the two input
terms. Similar to Strube07, we first check ancestor,
then sibling, and finally no relation. Yago07 uses
the YAGO ontology (Suchanek et al, 2007) as its
main source of background knowledge. Because the
YAGO ontology is a combination of Wikipedia and
WordNet, this system is expected to perform well at
recognizing taxonomic relations. To access a term?s
ancestors and siblings, we use patterns 1 and 2 in
Figure 3 to map a term to the ontology and move up
on the ontology. The relation identification process
is then similar to those of Snow06 and Strube07. If
an input term is not recognized by these systems,
they return no relation.
Our overall algorithm, TAREC, is described in
Figure 1. We manually construct a pre-defined list
of 35 relational constraints to use in the inference
model. We also evaluate our local classifier (Section
4), which is referred as TAREC (local). To make
classification decision with TAREC (local), for a
pair of terms, we choose the predicted relation with
highest confidence returned by the classifier.
In all systems compared, we vary the value ofK10
from 1 to 4. The best result of each system is re-
ported. Table 4 shows the comparison of all sys-
tems evaluated on both Test-I and Test-II. Our sys-
tems, as shown, significantly outperform the other
10See Section 3.1 for the meaning of K.
1106
systems. In Table 4, the improvement of TAREC
over TAREC (local) on Test-I shows the contribu-
tion of both the normalization procedure (that is, go-
ing outside Wikipedia terms) and the global infer-
ence model to the classification decisions, whereas
the improvement on Test-II shows only the contribu-
tion of the inference model, because Test-II contains
only terms with corresponding Wikipedia articles.
Observing the results we see that our algorithms
is doing significantly better that fixed taxonomies
based algorithms. This is true both for TAREC (lo-
cal) and for TAREC. We believe that our machine
learning based classifier is very flexible in extract-
ing features of the two input terms and thus in pre-
dicting their taxonomic Relation. On the other hand,
other system rely heavily on string matching tech-
niques to map input terms to their respective ontolo-
gies, and these are very inflexible and brittle. This
clearly shows one limitation of using existing struc-
tured resources to classify taxonomic relations.
We do not use special tactics to handle polyse-
mous terms. However, our procedure of building se-
mantic representations for input terms described in
Section 4 ties the senses of the two input terms and
thus, implicitly, may get some sense information.
We do not use this procedure in Snow06 because
WordNet and Wikipedia are two different knowl-
edge bases. We also do not use this procedure in
Yago07 because in YAGO, a term is mapped onto the
ontology by using the MEANS operator (in Pattern 1,
Figure 3). This cannot follow our procedure.
6.2 Comparison to Harvested Knowledge
As we discussed in Section 2, the output of
bootstrapping-based algorithms is usually limited to
a small number of high-quality terms while sacri-
ficing coverage (or vice versa). For example, the
full Espresso algorithm in (Pantel and Pennacchiotti,
2006) extracted 69,156 instances of is-a relation
with 36.2% precision. Similarly, (Kozareva et al,
2008) evaluated only a small number (a few hun-
dreds) of harvested instances. Recently, (Baroni
and Lenci, 2010) proposed a general framework to
extract properties of input terms. Their TypeDM
model harvested 5,000 significant properties for
each term out of 20,410 noun terms. For exam-
ple, the properties of marine include ?own, bomb?,
?use, gun?. Using vector space models we could
measure the similarity between terms using their
property vectors. However, since the information
available in TypeDM does not support predicting the
ancestor relation between terms, we only evaluate
TypeDM in classifying sibling vs. no relation. We
do this by giving a list of semantic classes using the
following procedure: (1) For each semantic class,
use some seeds to compute a centroid vector from
the seeds? vectors in TypeDM, (2) each term in an
input pair is classified into its best semantic class
based on the cosine similarity between its vector and
the centroid vector of the category, (3) two terms are
siblings if they are classified into the same category;
and have no relation, otherwise. Out of 20,410 noun
terms in TypeDM, there are only 345 terms overlap-
ping with the instances in OrgData-I and belonging
to 10 significant semantic classes. For each seman-
tic class, we randomly pick 5 instances as its seeds to
make a centroid vector. The rest of the overlapping
instances are randomly paired to make a dataset of
4,000 pairs of terms balanced in the number of sib-
ling and no relation pairs. On this dataset, TypeDM
achieves the accuracy of 79.75%. TAREC (local),
with the local classifier trained on the training set
(with 4 relation classes) of Dataset-I, gives 78.35%
of accuracy. The full TAREC system with relational
constraints achieves 82.65%. We also re-train and
evaluate the local classifier of TAREC on the same
training set but without ancestor relation pairs. This
local classifier has an accuracy of 81.08%.
These results show that although the full TAREC
system gives better performance, TypeDM is very
competitive in recognizing sibling vs. no relation.
However, TypeDM can only work in a limited set-
ting where semantic classes are given in advance,
which is not practical in real-world applications; and
of course, TypeDM does not help to recognize an-
cestor relations between two terms.
6.3 Experimental Analysis
In this section, we discuss some experimental anal-
yses to better understand our systems.
Precision and Recall: We want to study TAREC
on individual taxonomic relations using Precision
and Recall. Table 5 shows that TAREC performs
very well on ancestor relation. Sibling and no rela-
tion are the most difficult relations to classify. In
the same experimental setting on Test-I, Yago07
1107
TAREC
Test-I Test-II
Prec Rec Prec Rec
x? y 95.82 88.01 96.46 88.48
x? y 94.61 89.29 96.15 88.86
x? y 79.23 84.01 83.15 81.87
x= y 73.94 79.9 75.54 88.27
Average 85.9 85.3 87.83 86.87
Table 5: Performance of TAREC on individual taxo-
nomic relation.
Wiki WordNet non-Wiki
Strube07 24.59 24.13 21.18
Snow06 41.23 46.91 34.46
Yago07 69.95 70.42 34.26
TAREC (local) 89.37 89.72 31.22
TAREC 91.03 91.2 45.21
Table 6: Performance of the systems on special data sets,
in accuracy. On the non-Wikipedia test set, TAREC (lo-
cal) simply returns sibling relation.
achieves 79.34% and 66.03% of average Precision
and Recall, respectively. These numbers on Test-II
are 81.33% and 70.44%.
Special Data Sets: We evaluate all systems that
use hierarchical structures as background knowl-
edge on three special data sets derived from Test-I.
From 12,000 pairs in Test-I, we created a test set,
Wiki, consisting of 10, 456 pairs with all terms in
Wikipedia. We use the rest of 1, 544 pairs with at
least one non-Wikipedia term to build a non-Wiki
test set. The third dataset, WordNet, contains 8, 625
pairs with all terms in WordNet and Wikipedia. Ta-
ble 6 shows the performance of the systems on these
data sets. Unsurprisingly, Yago07 gets better results
on Wiki than on Test-I. Snow06, as expected, gives
better performance on the WordNet test set. TAREC
still significantly outperforms these systems. The
improvement of TAREC over TAREC (local) on the
Wiki and WordNet test sets shows the contribution
of the inference model, whereas the improvement on
the non-Wikipedia test set shows the contribution of
normalizing input terms to Wikipedia.
Contribution of Related Terms in Inference:
We evaluate TAREC when the inference procedure
is fed by related terms that are generated using a
?gold standard? source instead of YAGO. To do this,
we use the original data which was used to generate
Test-I. For each term in the examples of Test-I, we
get its ancestors, siblings, and children, if any, from
K=1 K=2 K=3 K=4
TAREC 82.93 85.34 85.23 83.95
TAREC (Gold Infer.) 83.46 86.18 85.9 84.93
Table 7: Evaluating TAREC with different sources pro-
viding related terms to do inference.
the original data and use them as related terms in the
inference model. This system is referred as TAREC
(Gold Infer.). Table 7 shows the results of the two
systems on different K as the number of levels to
go up on the Wikipedia category system. We see
that TAREC gets better results when doing inference
with better related terms. In this experiment, the two
systems use the same number of related terms.
7 Conclusions
We studied an important component of many com-
putational linguistics tasks: given two target terms,
determine that taxonomic relation between them.
We have argued that static structured knowledge
bases cannot support this task well enough, and pro-
vided empirical support for this claim. We have de-
veloped TAREC, a novel algorithm that leverages in-
formation from existing knowledge sources and uses
machine learning and a constraint-based inference
model to mitigate the noise and the level of uncer-
tainty inherent in these resources. Our evaluations
show that TAREC significantly outperforms other
systems built upon existing well-known knowledge
sources. Our approach generalizes and handles non-
Wikipedia term well across semantic classes. Our
future work will include an evaluation of TAREC in
the context of textual inference applications.
Acknowledgments
The authors thank Mark Sammons, Vivek Srikumar, James
Clarke and the anonymous reviewers for their insightful com-
ments and suggestions. University of Illinois at Urbana-
Champaign gratefully acknowledges the support of Defense
Advanced Research Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory (AFRL)
prime contract No. FA8750-09-C-0181. The first author also
thanks the Vietnam Education Foundation (VEF) for its spon-
sorship. Any opinions, findings, and conclusion or recommen-
dations expressed in this material are those of the authors and
do not necessarily reflect the view of the VEF, DARPA, AFRL,
or the US government.
1108
References
A. Abad, L. Bentivogli, I. Dagan, D. Giampiccolo,
S. Mirkin, E. Pianta, and A. Stern. 2010. A resource
for investigating the impact of anaphora and corefer-
ence on inference. In LREC.
M. Banko and O. Etzioni. 2008. The tradeoffs between
open and traditional relation extraction. In ACL-HLT.
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36.
S. Chakrabarti, B. Dom, R. Agrawal, and P. Raghavan.
1997. Using taxonomy, discriminants, and signatures
for navigating in text databases. In VLDB.
M. Chang, L. Ratinov, and D. Roth. 2008. Constraints as
prior knowledge. In ICML Workshop on Prior Knowl-
edge for Text and Language Processing.
D. Davidov and A. Rappoport. 2008. Unsupervised dis-
covery of generic relationships using pattern clusters
and its evaluation by automatically generated sat anal-
ogy questions. In ACL.
P. Denis and J. Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In NAACL.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning.
M. A. Hearst. 1992. Acquisition of hyponyms from large
text corpora. In COLING.
A. Hotho, S. Staab, and G. Stumme. 2003. Ontologies
improve text document clustering. In ICDM.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Seman-
tic class learning from the web with hyponym pattern
linkage graphs. In ACL-HLT.
B. MacCartney and C. D. Manning. 2008. Modeling se-
mantic containment and exclusion in natural language
inference. In COLING.
B. MacCartney and C. D. Manning. 2009. An extended
model of natural logic. In IWCS-8.
M. Pas?ca and B. Van Durme. 2008. Weakly-supervised
acquisition of open-domain classes and class attributes
from web documents and query logs. In ACL-HLT.
M. Pas?ca. 2007. Organizing and searching the world
wide web of facts step two: Harnessing the wisdom
of the crowds. In WWW.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging generic patterns for automatically harvesting se-
mantic relations. In ACL, pages 113?120.
S. P. Ponzetto and M. Strube. 2007. Deriving a large
scale taxonomy from wikipedia. AAAI.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
CoNLL.
D. Roth and W. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
M. Sammons, V.G. Vydiswaran, and D. Roth. 2010. Ask
not what textual entailment can do for you... In ACL.
L. Sarmento, V. Jijkuon, M. de Rijke, and E. Oliveira.
2007. ?more like these?: growing entity classes from
seeds. In CIKM.
A. K. Saxena, G. V. Sambhu, S. Kaushik, and L. V. Sub-
ramaniam. 2007. Iitd-ibmirl system for question an-
swering using pattern matching, semantic type and se-
mantic category recognition. In TREC.
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning
syntactic patterns for automatic hypernym discovery.
In NIPS.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
ACL.
F. M. Suchanek, G. Kasneci, and G. Weikum. 2007.
Yago: A Core of Semantic Knowledge. In WWW.
O. Vikas, A. K. Meshram, G. Meena, and A. Gupta.
2008. Multiple document summarization using princi-
pal component analysis incorporating semantic vector
space model. In Computational Linguistics and Chi-
nese Language Processing.
V. Vyas and P. Pantel. 2009. Semi-automatic entity set
refinement. In NAACL-HLT.
D. Yarowsky. 1995. Unsupervised woed sense disam-
biguation rivaling supervied methods. In Proceedings
of ACL-95.
1109
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 294?303,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Minimally Supervised Event Causality Identification
Quang Xuan Do Yee Seng Chan Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{quangdo2,chanys,danr}@illinois.edu
Abstract
This paper develops a minimally supervised
approach, based on focused distributional sim-
ilarity methods and discourse connectives,
for identifying of causality relations between
events in context. While it has been shown
that distributional similarity can help identify-
ing causality, we observe that discourse con-
nectives and the particular discourse relation
they evoke in context provide additional in-
formation towards determining causality be-
tween events. We show that combining dis-
course relation predictions and distributional
similarity methods in a global inference pro-
cedure provides additional improvements to-
wards determining event causality.
1 Introduction
An important part of text understanding arises from
understanding the semantics of events described in
the narrative, such as identifying the events that are
mentioned and how they are related semantically.
For instance, when given a sentence ?The police
arrested him because he killed someone.?, humans
understand that there are two events, triggered by
the words ?arrested? and ?killed?, and that there is
a causality relationship between these two events.
Besides being an important component of discourse
understanding, automatically identifying causal re-
lations between events is important for various nat-
ural language processing (NLP) applications such
as question answering, etc. In this work, we auto-
matically detect and extract causal relations between
events in text.
Despite its importance, prior work on event
causality extraction in context in the NLP litera-
ture is relatively sparse. In (Girju, 2003), the au-
thor used noun-verb-noun lexico-syntactic patterns
to learn that ?mosquitoes cause malaria?, where the
cause and effect mentions are nominals and not nec-
essarily event evoking words. In (Sun et al, 2007),
the authors focused on detecting causality between
search query pairs in temporal query logs. (Beamer
and Girju, 2009) tried to detect causal relations be-
tween verbs in a corpus of screen plays, but limited
themselves to consecutive, or adjacent verb pairs.
In (Riaz and Girju, 2010), the authors first cluster
sentences into topic-specific scenarios, and then fo-
cus on building a dataset of causal text spans, where
each span is headed by a verb. Thus, their focus was
not on identifying causal relations between events in
a given text document.
In this paper, given a text document, we first iden-
tify events and their associated arguments. We then
identify causality or relatedness relations between
event pairs. To do this, we develop a minimally su-
pervised approach using focused distributional sim-
ilarity methods, such as co-occurrence counts of
events collected automatically from an unannotated
corpus, to measure and predict existence of causal-
ity relations between event pairs. Then, we build on
the observation that discourse connectives and the
particular discourse relation they evoke in context
provide additional information towards determining
causality between events. For instance, in the ex-
ample sentence provided at the beginning of this
section, the words ?arrested? and ?killed? probably
have a relatively high apriori likelihood of being ca-
294
sually related. However, knowing that the connec-
tive ?because? evokes a contingency discourse re-
lation between the text spans ?The police arrested
him? and ?he killed someone? provides further ev-
idence towards predicting causality. The contribu-
tions of this paper are summarized below:
? Our focus is on identifying causality between
event pairs in context. Since events are of-
ten triggered by either verbs (e.g. ?attack?) or
nouns (e.g. ?explosion?), we allow for detec-
tion of causality between verb-verb, verb-noun,
and noun-noun triggered event pairs. To the
best of our knowledge, this formulation of the
task is novel.
? We developed a minimally supervised ap-
proach for the task using focused distributional
similarity methods that are automatically col-
lected from an unannotated corpus. We show
that our approach achieves better performance
than two approaches: one based on a frequently
used metric that measures association, and an-
other based on the effect-control-dependency
(ECD) metric described in a prior work (Riaz
and Girju, 2010).
? We leverage on the interactions between event
causality prediction and discourse relations
prediction. We combine these knowledge
sources through a global inference procedure,
which we formalize via an Integer Linear Pro-
gramming (ILP) framework as a constraint op-
timization problem (Roth and Yih, 2004). This
allows us to easily define appropriate con-
straints to ensure that the causality and dis-
course predictions are coherent with each other,
thereby improving the performance of causality
identification.
2 Event Causality
In this work, we define an event as an action or oc-
currence that happens with associated participants
or arguments. Formally, we define an event e
as: p(a1, a2, . . . , an), where the predicate p is the
word that triggers the presence of e in text, and
a1, a2, . . . , an are the arguments associated with
e. Examples of predicates could be verbs such as
?attacked?, ?employs?, nouns such as ?explosion?,
?protest?, etc., and examples of the arguments of
?attacked? could be its subject and object nouns.
To measure the causality association between a
pair of events ei and ej (in general, ei and ej
could be extracted from the same or different doc-
uments), we should use information gathered about
their predicates and arguments. A simple approach
would be to directly calculate the pointwise mu-
tual information (PMI)1 between pi(ai1, ai2, . . . , ain)
and pj(aj1, aj2, . . . , ajm). However, this leads to very
sparse counts as the predicate pi with its list of ar-
guments ai1, . . . , ain would rarely co-occur (within
some reasonable context distance) with predicate pj
and its entire list of arguments aj1, . . . , ajm. Hence,
in this work, we measure causality association us-
ing three separate components and focused distribu-
tional similarity methods collected about event pairs
as described in the rest of this section.
2.1 Cause-Effect Association
We measure the causality or cause-effect association
(CEA) between two events ei and ej using the fol-
lowing equation:
CEA(ei, ej) =
spp(ei, ej) + spa(ei, ej) + saa(ei, ej) (1)
where spp measures the association between event
predicates, spa measures the association between the
predicate of an event and the arguments of the other
event, and saa measures the association between
event arguments. In our work, we regard each event
e as being triggered and rooted at a predicate p.
2.1.1 Predicate-Predicate Association
We define spp as follows:
spp(ei, ej) = PMI(pi, pj)?max(ui, uj)
?IDF (pi, pj)?Dist(pi, pj) (2)
which takes into account the PMI between pred-
icates pi and pj of events ei and ej respectively,
as well as various other pieces of information. In
Suppes? Probabilistic theory of Casuality (Suppes,
1970), he highlighted that event e is a possible cause
of event e?, if e? happens more frequently with e than
1PMI is frequently used to measure association between
variables.
295
by itself, i.e. P (e?|e) > P (e?). This can be easily
rewritten as P (e,e?)P (e)P (e?) > 1, similar to the definitionof PMI:
PMI(e, e?) = log P (e, e
?)
P (e)P (e?)
which is only positive when P (e,e?)P (e)P (e?) > 1.
Next, we build on the intuition that event predi-
cates appearing in a large number of documents are
probably not important or discriminative. Thus, we
penalize these predicates when calculating spp by
adopting the inverse document frequency (idf):
IDF (pi, pj) = idf(pi)? idf(pj)? idf(pi, pj),
where idf(p) = log D1+N , D is the total number ofdocuments in the collection and N is the number of
documents that p occurs in.
We also award event pairs that are closer together,
while penalizing event pairs that are further apart in
texts, by incorporating the distance measure of Lea-
cock and Chodorow (1998), which was originally
used to measure similarity between concepts:
Dist(pi, pj) = ?log |sent(p
i)? sent(pj)|+ 1
2? ws ,
where sent(p) gives the sentence number (index) in
which p occurs and ws indicates the window-size
(of sentences) used. If pi and pj are drawn from the
same sentence, the numerator of the above fraction
will return 1. In our work, we set ws to 3 and thus,
if pi occurs in sentence k, the furthest sentence that
pj will be drawn from, is sentence k + 2.
The final component of Equation 2, max(ui, uj),
takes into account whether predicates (events) pi and
pj appear most frequently with each other. ui and uj
are defined as follows:
ui = P (p
i, pj)
maxk[P (pi, pk)]? P (pi, pj) + 
uj = P (p
i, pj)
maxk[P (pk, pj)]? P (pi, pj) +  ,
where we set  = 0.01 to avoid zeros in the denom-
inators. ui will be maximized if there is no other
predicate pk having a higher co-occurrence proba-
bility with pi, i.e. pk = pj . uj is treated similarly.
2.1.2 Predicate-Argument and
Argument-Argument Association
We define spa as follows:
spa(ei, ej) =
1
|Aej |
?
a?Aej
PMI(pi, a)
+ 1|Aei |
?
a?Aei
PMI(pj , a), (3)
where Aei and Aej are the sets of arguments of ei
and ej respectively.
Finally, we define saa as follows:
saa(ei, ej) =
1
|Aei ||Aej |
?
a?Aei
?
a??Aej
PMI(a, a?) (4)
Together, spa and saa provide additional contexts
and robustness (in addition to spp) for measuring the
cause-effect association between events ei and ej .
Our formulation of CEA is inspired by the ECD
metric defined in (Riaz and Girju, 2010):
ECD(a, b) = max(v, w)??log dis(a, b)2?maxDistance , (5)
where
v = P (a, b)P (b)? P (a, b) +  ?
P (a, b)
maxt[P (a, bt)]? P (a, b) + 
w= P (a, b)P (a)? P (a, b) +  ?
P (a, b)
maxt[P (at, b)]? P (a, b) +  ,
where ECD(a,b) measures the causality between two
events a and b (headed by verbs), and the sec-
ond component in the ECD equation is similar to
Dist(pi, pj). In our experiments, we will evaluate
the performance of ECD against our proposed ap-
proach.
So far, our definitions in this section are generic
and allow for any list of event argument types. In
this work, we focus on two argument types: agent
(subject) and patient (object), which are typical core
arguments of any event. We describe how we extract
event predicates and their associated arguments in
the section below.
3 Verbal and Nominal Predicates
We consider that events are not only triggered by
verbs but also by nouns. For a verb (verbal predi-
cate), we extract its subject and object from its as-
sociated dependency parse. On the other hand, since
296
events are also frequently triggered by nominal pred-
icates, it is important to identify an appropriate list
of event triggering nouns. In our work, we gathered
such a list using the following approach:
? We first gather a list of deverbal nouns from the
set of most frequently occurring (in the Giga-
word corpus) 3,000 verbal predicate types. For
each verb type v, we go through all its Word-
Net2 senses and gather all its derivationally re-
lated nouns Nv 3.
? From Nv, we heuristically remove nouns that
are less than three characters in length. We also
remove nouns whose first three characters are
different from the first three characters of v. For
each of the remaining nouns in Nv, we mea-
sured its Levenstein (edit) distance from v and
keep the noun(s) with the minimum distance.
When multiple nouns have the same minimum
distance from v, we keep all of them.
? To further prune the list of nouns, we next re-
moved all nouns ending in ?er?, ?or?, or ?ee?,
as these nouns typically refer to a person, e.g.
?writer?, ?doctor?, ?employee?. We also re-
move nouns that are not hyponyms (children)
of the first WordNet sense of the noun ?event?4.
? Since we are concerned with nouns denoting
events, FrameNet (Ruppenhofer et al, 2010)
(FN) is a good resource for mining such nouns.
FN consists of frames denoting situations and
events. As part of the FN resource, each FN
frame consists of a list of lexical units (mainly
verbs and nouns) representing the semantics of
the frame. Various frame-to-frame relations are
also defined (in particular the inheritance re-
lation). Hence, we gathered all the children
frames of the FN frame ?Event?. From these
children frames, we then gathered all their noun
lexical units (words) and add them to our list of
2http://wordnet.princeton.edu/
3The WordNet resource provides derivational information
on words that are in different syntactic (i.e. part-of-speech) cat-
egories, but having the same root (lemma) form and that are
semantically related.
4The first WordNet sense of the noun ?event? has the mean-
ing: ?something that happens at a given place and time?
nouns. Finally, we also add a few nouns denot-
ing natural disaster from Wikipedia5.
Using the above approach, we gathered a list of
about 2,000 noun types. This current approach is
heuristics based which we intend to improve in the
future, and any such improvements should subse-
quently improve the performance of our causality
identification approach.
Event triggering deverbal nouns could have as-
sociated arguments (for instance, acting as subject,
object of the deverbal noun). To extract these ar-
guments, we followed the approach of (Gurevich
et al, 2008). Briefly, the approach uses linguistic
patterns to extract subjects and objects for deverbal
nouns, using information from dependency parses.
For more details, we refer the reader to (Gurevich et
al., 2008).
4 Discourse and Causality
Discourse connectives are important for relating dif-
ferent text spans, helping us to understand a piece of
text in relation to its context:
[The police arrested him] because [he killed someone].
In the example sentence above, the discourse con-
nective (?because?) and the discourse relation it
evokes (in this case, the Cause relation) allows read-
ers to relate its two associated text spans, ?The po-
lice arrested him? and ?he killed someone?. Also,
notice that the verbs ?arrested? and ?killed?, which
cross the two text spans, are causally related. To
aid in extracting causal relations, we leverage on the
identification of discourse relations to provide addi-
tional contextual information.
To identify discourse relations, we use the Penn
Discourse Treebank (PDTB) (Prasad et al, 2007),
which contains annotations of discourse relations
in context. The annotations are done over the
Wall Street Journal corpus and the PDTB adopts a
predicate-argument view of discourse relations. A
discourse connective (e.g. because) takes two text
spans as its arguments. In the rest of this section,
we briefly describe the discourse relations in PDTB
and highlight how we might leverage them to aid in
determining event causality.
5http://en.wikipedia.org/wiki/Natural disaster
297
Coarse-grained relations Fine-grained relations
Comparison Concession, Contrast, Pragmatic-concession, Pragmatic-contrast
Contingency Cause, Condition, Pragmatic-cause, Pragmatic-condition
Expansion Alternative, Conjunction, Exception, Instantiation, List, Restatement
Temporal Asynchronous, Synchronous
Table 1: Coarse-grained and fine-grained discourse relations.
4.1 Discourse Relations
PDTB contains annotations for four coarse-grained
discourse relation types, as shown in the left column
of Table 1. Each of these are further refined into
several fine-grained discourse relations, as shown in
the right column of the table.6 Next, we briefly de-
scribe these relations, highlighting those that could
potentially help to determine event causality.
Comparison A Comparison discourse relation
between two text spans highlights prominent differ-
ences between the situations described in the text
spans. An example sentence is:
Contrast: [According to the survey, x% of Chinese Inter-
net users prefer Google] whereas [y% prefer Baidu].
According to the PDTB annotation manual
(Prasad et al, 2007), the truth of both spans is in-
dependent of the established discourse relation. This
means that the text spans are not causally related and
thus, the existence of a Comparison relation should
imply that there is no causality relation across the
two text spans.
Contingency A Contingency relation between
two text spans indicates that the situation described
in one text span causally influences the situation in
the other. An example sentence is:
Cause: [The first priority is search and rescue] because
[many people are trapped under the rubble].
Existence of a Contingency relation potentially
implies that there exists at least one causal event
pair crossing the two text spans. The PDTB an-
notation manual states that while the Cause and
Condition discourse relations indicate casual influ-
ence in their text spans, there is no causal in-
fluence in the text spans of the Pragmatic-cause
and Pragmatic-condition relations. For instance,
Pragmatic-condition indicates that one span pro-
6PDTB further refines these fine-grained relations into a fi-
nal third level of relations, but we do not use them in this work.
vides the context in which the description of the sit-
uation in the other span is relevant; for example:
Pragmatic-condition: If [you are thirsty], [there?s beer in
the fridge].
Hence, there is a need to also identify fine-grained
discourse relations.
Expansion Connectives evoking Expansion dis-
course relations expand the discourse, such as by
providing additional information, illustrating alter-
native situations, etc. An example sentence is:
Conjunction: [Over the past decade, x women were
killed] and [y went missing].
Most of the Expansion fine-grained relations (ex-
cept for Conjunction, which could connect arbitrary
pieces of text spans) should not contain causality re-
lations across its text spans.
Temporal These indicate that the situations de-
scribed in the text spans are related temporally. An
example sentence is:
Synchrony: [He was sitting at his home] when [the whole
world started to shake].
Temporal precedence of the (cause) event over the
(effect) event is a necessary, but not sufficient req-
uisite for causality. Hence by itself, Temporal re-
lations are probably not discriminative enough for
determining event causality.
4.2 Discourse Relation Extraction System
Our work follows the approach and features de-
scribed in the state-of-the-art Ruby-based discourse
system of (Lin et al, 2010), to build an in-
house Java-based discourse relation extraction sys-
tem. Our system identifies explicit connectives in
text, predict their discourse relations, as well as their
associated text spans. Similar to (Lin et al, 2010),
we achieved a competitive performance of slightly
over 80% F1-score in identifying fine-grained rela-
tions for explicit connectives. Our system is devel-
oped using the Learning Based Java modeling lan-
298
guage (LBJ) (Rizzolo and Roth, 2010) and will be
made available soon. Due to space constraints, we
refer interested readers to (Lin et al, 2010) for de-
tails on the features, etc.
In the example sentences given thus far in this sec-
tion, all the connectives were explicit, as they appear
in the texts. PDTB also provides annotations for im-
plicit connectives, which we do not use in this work.
Identifying implicit connectives is a harder task and
incorporating these is a possible future work.
5 Joint Inference for Causality Extraction
To exploit the interactions between event pair
causality extraction and discourse relation identifi-
cation, we define appropriate constraints between
them, which can be enforced through the Con-
strained Conditional Models framework (aka ILP for
NLP) (Roth and Yih, 2007; Chang et al, 2008). In
doing this, the predictions of CEA (Section 2.1) and
the discourse system are forced to cohere with each
other. More importantly, this should improve the
performance of using only CEA to extract causal
event pairs. To the best of our knowledge, this ap-
proach for causality extraction is novel.
5.1 CEA & Discourse: Implementation Details
Let E denote the set of event mentions in a docu-
ment. Let EP = {(ei, ej) ? E ? E | ei ? E , ej ?
E , i < j, |sent(ei) ? sent(ej)| ? 2} denote the
set of event mention pairs in the document, where
sent(e) gives the sentence number in which event e
occurs. Note that in this work, we only extract event
pairs that are at most two sentences apart. Next, we
define LER = {?causal?, ?? causal?} to be the set of
event relation labels that an event pair ep ? EP can
be associated with.
Note that the CEA metric as defined in Section 2.1
simply gives a score without it being bounded to be
between 0 and 1.0. However, to use the CEA score
as part of the inference process, we require that it be
bounded and thus can be used as a binary prediction,
that is, predicting an event pair as causal or ?causal.
To enable this, we use a few development documents
to automatically find a threshold CEA score that sep-
arates scores indicating causal vs ?causal. Based
on this threshold, the original CEA scores are then
rescaled to fall within 0 to 1.0. More details on this
are in Section 6.2.
Let C denote the set of connective mentions in a
document. We slightly modify our discourse sys-
tem as follows. We define LDR to be the set of
discourse relations. We initially add all the fine-
grained discourse relations listed in Table 1 to LDR.
In the PDTB corpus, some connective examples are
labeled with just a coarse-grained relation, with-
out further specifying a fine-grained relation. To
accommodate these examples, we add the coarse-
grained relations Comparison, Expansion, and Tem-
poral to LDR. We omit the coarse-grained Con-
tingency relation from LDR, as we want to sepa-
rate Cause and Condition from Pragmatic-cause and
Pragmatic-condition. This discards very few exam-
ples as only a very small number of connective ex-
amples are simply labeled with a Contingency label
without further specifying a fine-grained label. We
then retrained our discourse system to predict labels
in LDR.
5.2 Constraints
We now describe the constraints used to support
joint inference, based on the predictions of the CEA
metric and the discourse classifier. Let sc(dr) be
the probability that connective c is predicated to be
of discourse relation dr, based on the output of our
discourse classifier. Let sep(er) be the CEA pre-
diction score (rescaled to range in [0,1]) that event
pair ep takes on the causal or ?causal label er. Let
x?c,dr? be a binary indicator variable which takes on
the value 1 iff c is labeled with the discourse relation
dr. Similarly, let y?ep,er? be a binary variable which
takes on the value 1 iff ep is labeled as er. We then
define our objective function as follows:
max
[
|LDR|
?
c?C
?
dr?LDR
sc(dr) ? x?c,dr?
+|LER|
?
ep?EP
?
er?LER
sep(er) ? y?ep,er?
]
(6)
subject to the following constraints:
?
dr?LDR
x?c,dr? = 1 ?c ? C (7)
?
er?LER
y?ep,er? = 1 ?ep ? EP (8)
x?c,dr? ? {0, 1} ?c ? C, dr ? LDR (9)
y?ep,er? ? {0, 1} ?ep ? EP, er ? LER(10)
299
Equation (7) requires that each connective c can
only be assigned one discourse relation. Equation
(8) requires that each event pair ep can only be
causal or ?causal. Equations (9) and (10) indicate
that x?c,dr? and y?ep,er? are binary variables.
To capture the relationship between event pair
causality and discourse relations, we use the follow-
ing constraints:
x?c,?Cause?? ?
?
ep?EPc
y?ep,?causal?? (11)
x?c,?Condition?? ?
?
ep?EPc
y?ep,?causal??, (12)
where both equations are defined ?c ? C. EPc is
defined to be the set of event pairs that cross the two
text spans associated with c. For instance, if the first
text span of c contains two event mentions ei, ej ,
and there is one event mention ek in the second text
span of c, then EPc = {(ei, ek), (ej , ek)}. Finally,
the logical form of Equation (11) can be written as:
x?c,?Cause?? ? y?epi,?causal?? ? . . . ? y?epj ,?causal??,
where epi, . . . , epj are elements in EPc. This states
that if we assign the Cause discourse label to c,
then at least one of epi, . . . , epj must be assigned as
causal. The interpretation of Equation (12) is simi-
lar.
We use two more constraints to capture the inter-
actions between event causality and discourse rela-
tions. First, we defined Cep as the set of connectives
c enclosing each event of ep in each of its text spans,
i.e.: one of the text spans of c contain one of the
event in ep, while the other text span of c contain the
other event in ep. Next, based on the discourse rela-
tions in Section 4.1, we propose that when an event
pair ep is judged to be causal, then the connective
c that encloses it should be evoking one of the dis-
course relations in LDRa = {?Cause?, ?Condition?,
?Temporal?, ?Asynchronous?, ?Synchrony?, ?Con-
junction?}. We capture this using the following con-
straint:
y?ep,?causal?? ?
?
dra?LDRa
x?c,dra? ?c ? Cep (13)
The logical form of Equation (13) can be written as:
y?ep,?causal?? ? x?c,?Cause?? ? x?c,?Condition?? . . . ?
x?c,?Conjunction??. This states that if we assign ep as
causal, then we must assign to c one of the labels in
LDRa .
Finally, we propose that for any connectives evok-
ing discourse relations LDRb = {?Comparison?,
?Concession?, ?Contrast?, ?Pragmatic-concession?,
?Pragmatic-contrast?, ?Expansion?, ?Alternative?,
?Exception?, ?Instantiation?, ?List?, ?Restate-
ment?}, any event pair(s) that it encloses should be
?causal. We capture this using the following con-
straint:
x?c,drb? ? y?ep,??causal??
? drb ? LDRb , ep ? EPc, (14)
where the logical form of Equation (14) can be writ-
ten as: x?c,drb? ? y?ep,??causal??.
6 Experiments
6.1 Experimental Settings
To collect the distributional statistics for measuring
CEA as defined in Equation (1), we applied part-
of-speech tagging, lemmatization, and dependency
parsing (Marneffe et al, 2006) on about 760K docu-
ments in the English Gigaword corpus (LDC catalog
number LDC2003T05).
We are not aware of any benchmark corpus for
evaluating event causality extraction in contexts.
Hence, we created an evaluation corpus using the
following process: Using news articles collected
from CNN7 during the first three months of 2010, we
randomly selected 20 articles (documents) as evalu-
ation data, and 5 documents as development data.
Two annotators annotated the documents for
causal event pairs, using two simple notions for
causality: the Cause event should temporally pre-
cede the Effect event, and the Effect event occurs be-
cause the Cause event occurs. However, sometimes
it is debatable whether two events are involved in a
causal relation, or whether they are simply involved
in an uninteresting temporal relation. Hence, we al-
lowed annotations of C to indicate causality, and R
to indicate relatedness (for situations when the exis-
tence of causality is debatable). The annotators will
simply identify and annotate the C or R relations be-
tween predicates of event pairs. Event arguments are
not explicitly annotated, although the annotators are
free to look at the entire document text while mak-
ing their annotation decisions. Finally, they are free
7http://www.cnn.com
300
System Rec% Pre% F1%
PMIpp 26.6 20.8 23.3
ECDpp &PMIpa,aa 40.9 23.5 29.9
CEA 62.2 28.0 38.6
CEA+Discourse 65.1 30.7 41.7
Table 2: Performance of baseline systems and our ap-
proaches on extracting Causal event relations.
System Rec% Pre% F1%
PMIpp 27.8 24.9 26.2
ECDpp &PMIpa,aa 42.4 28.5 34.1
CEA 63.1 33.7 43.9
CEA+Discourse 65.3 36.5 46.9
Table 3: Performance of the systems on extracting Causal
and Related event relations.
to annotate relations between predicates that have
any number of sentences in between and are not re-
stricted to a fixed sentence window-size.
After adjudication, we obtained a total of 492
C+R relation annotations, and 414C relation anno-
tations on the evaluation documents. On the devel-
opment documents, we obtained 92 C+R and 71 C
relation annotations. The annotators overlapped on
10 evaluation documents. On these documents, the
first (second) annotator annotated 215 (199) C + R
relations, agreeing on 166 of these relations. To-
gether, they annotated 248 distinct relations. Us-
ing this number, their agreement ratio would be 0.67
(166/248). The corresponding agreement ratio for
C relations is 0.58. These numbers highlight that
causality identification is a difficult task, as there
could be as many as N2 event pairs in a document
(N is the number of events in the document). We
plan to make this annotated dataset available soon.8
6.2 Evaluation
As mentioned in Section 5.1, to enable translat-
ing (the unbounded) CEA scores into binary causal,
?causal predictions, we need to rescale or calibrate
these scores to range in [0,1]. To do this, we first
rank all the CEA scores of all event pairs in the de-
velopment documents. Most of these event pairs will
be ?causal. Based on the relation annotations in
these development documents, we scanned through
8http://cogcomp.cs.illinois.edu/page/publication view/663
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 5  10  15  20  25  30  35  40
Pr
ec
is
io
n(
%
)
K (number of causality predictions)
Precision(%) on top K event causality predictions
CEA
ECDpp & PMIpa,aa
PMIpp
Figure 1: Precision of the top K causality C predictions.
this ranked list of scores to locate the CEA score
t that gives the highest F1-score (on the develop-
ment documents) when used as a threshold between
causal vs ?causal decisions. We then ranked all
the CEA scores of all event pairs gathered from the
760K Gigaword documents, discretized all scores
higher than t into B bins, and all scores lower than
t into B bins. Together, these 2B bins represent the
range [0,1]. We used B = 500. Thus, consecu-
tive bins represent a difference of 0.001 in calibrated
scores.
To measure the causality between a pair of
events ei and ej , a simple baseline is to calculate
PMI(pi, pj). Using a similar thresholding and cali-
bration process to translate PMI(pi, pj) scores into
binary causality decisions, we obtained a F1 score of
23.1 when measured over the causality C relations,
as shown in the row PMIpp of Table 2.
As mentioned in Section 2.1.2, Riaz and Girju
(2010) proposed the ECD metric to measure
causality between two events. Thus, as a point of
comparison, we replaced spp of Equation (1) with
ECD(a, b) of Equation (5), substituting a = pi and
b = pj . After thresholding and calibrating the scores
of this approach, we obtained a F1-score of 29.7, as
shown in the row ECDpp&PMIpa,aa of Table 2.
Next, we evaluated our proposed CEA approach
and obtained a F1-score of 38.6, as shown in the row
CEA of Table 2. Thus, our proposed approach ob-
tained significantly better performance than the PMI
baseline and the ECD approach. Next, we per-
formed joint inference with the discourse relation
predictions as described in Section 5 and obtained
301
an improved F1-score of 41.7. We note that we ob-
tained improvements in both recall and precision.
This means that with the aid of discourse relations,
we are able to recover more causal relations, as well
as reduce false-positive predictions.
Constraint Equations (11) and (12) help to re-
cover causal relations. For improvements in pre-
cision, as stated in the last paragraph of Section
5.2, identifying other discourse relations such as
?Comparison?, ?Contrast?, etc., provides counter-
evidence to causality. Together with constraint
Equation (14), this helps to eliminate false-positive
event pairs as classified by CEA and contributes
towards CEA+Discourse having a higher precision
than CEA.
The corresponding results for extracting both
causality and relatedness C + R relations are given
in Table 3. For these experiments, the aim was for a
more relaxed evaluation and we simply collapsed C
and R into a single label.
Finally, we also measured the precision of the
top K causality C predictions, showing the preci-
sion trends in Figure 1. As shown, CEA in general
achieves higher precision when compared toPMIpp
and ECDpp&PMIpa,aa. The trends for C+R pre-
dictions are similar.
Thus far, we had included both verbal and nom-
inal predicates in our evaluation. When we repeat
the experiments for ECDpp&PMIpa,aa and CEA
on just verbal predicates, we obtained the respective
F1-scores of 31.8 and 38.3 on causality relations.
The corresponding F1-scores for casuality and relat-
edness relations are 35.7 and 43.3. These absolute
F1-scores are similar to those in Tables 2 and 3, dif-
fering by 1-2%.
7 Analysis
We randomly selected 50 false-positive predictions
and 50 false-negative causality relations to analyze
the mistakes made by CEA.
Among the false-positives (precision errors), the
most frequent error type (56% of the errors) is that
CEA simply assigns a high score to event pairs that
are not causal; more knowledge sources are required
to support better predictions in these cases. The next
largest group of error (22%) involves events contain-
ing pronouns (e.g. ?he?, ?it?) as arguments. Ap-
plying coreference to replace these pronouns with
their canonical entity strings or labeling them with
semantic class information might be useful.
Among the false-negatives (recall errors), 23%
of the errors are due to CEA simply assigning a
low score to causal event pairs and more contex-
tual knowledge seems necessary for better predic-
tions. 19% of the recall errors arises from causal
event pairs involving nominal predicates that are not
in our list of event evoking noun types (described in
Section 3). A related 17% of recall errors involves
nominal predicates without any argument. For these,
less information is available for CEA to make pre-
dictions. The remaining group (15% of errors) in-
volves events containing pronouns as arguments.
8 Related Work
Although prior work in event causality extraction
in context is relatively sparse, there are many prior
works concerning other semantic aspects of event
extraction. Ji and Grishman (2008) extracts event
mentions (belonging to a predefined list of target
event types) and their associated arguments. In other
prior work (Chen et al, 2009; Bejan and Harabagiu,
2010), the authors focused on identifying another
type of event pair semantic relation: event corefer-
ence. Chambers and Jurafsky (2008; 2009) chain
events sharing a common (protagonist) participant.
They defined events as verbs and given an existing
chain of events, they predict the next likely event in-
volving the protagonist. This is different from our
task of detecting causality between arbitrary event
pairs that might or might not share common argu-
ments. Also, we defined events more broadly, as
those that are triggered by either verbs or nouns. Fi-
nally, although our proposed CEA metric has resem-
blance the ECD metric in (Riaz and Girju, 2010), our
task is different from theirs and our work differs in
many aspects. They focused on building a dataset of
causal text spans, whereas we focused on identifying
causal relations between events in a given text doc-
ument. They considered text spans headed by verbs
while we considered events triggered by both verbs
and nouns. Moreover, we combined event causality
prediction and discourse relation prediction through
a global inference procedure to further improve the
performance of event causality prediction.
302
9 Conclusion
In this paper, using general tools such as the depen-
dency and discourse parsers which are not trained
specifically towards our target task, and a minimal
set of development documents for threshold tuning,
we developed a minimally supervised approach to
identify causality relations between events in con-
text. We also showed how to incorporate discourse
relation predictions to aid event causality predictions
through a global inference procedure. There are sev-
eral interesting directions for future work, including
the incorporation of other knowledge sources such
as coreference and semantic class predictions, which
were shown to be potentially important in our er-
ror analysis. We could also use discourse relations
to aid in extracting other semantic relations between
events.
Acknowledgments
The authors thank the anonymous reviewers for their
insightful comments and suggestions. University of
Illinois at Urbana-Champaign gratefully acknowl-
edges the support of Defense Advanced Research
Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract No. FA8750-09-C-0181. The first
author thanks the Vietnam Education Foundation
(VEF) for its sponsorship. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the authors and do not nec-
essarily reflect the view of the VEF, DARPA, AFRL,
or the US government.
References
Brandon Beamer and Roxana Girju. 2009. Using a bi-
gram event model to predict causal potential. In CI-
CLING.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010. Un-
supervised event coreference resolution with rich lin-
guistic features. In ACL.
Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised learning of narrative event chains. In ACL-HLT.
Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised learning of narrative schemas and their partici-
pants. In ACL.
Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and
Dan Roth. 2008. Learning and inference with con-
straints. In AAAI.
Zheng Chen, Heng Ji, and Robert Haralick. 2009. A
pairwise event coreference model, feature impact and
evaluation for event coreference resolution. In RANLP
workshop on Events in Emerging Text Types.
Roxana Girju. 2003. Automatic detection of causal re-
lations for question answering. In ACL workshop on
Multilingual Summarization and Question Answering.
Olga Gurevich, Richard Crouch, Tracy Holloway King,
and Valeria de Paiva. 2008. Deverbal nouns in knowl-
edge representation. Journal of Logic and Computa-
tion, 18, June.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through unsupervised cross-document infer-
ence. In ACL.
Claudia Leacock and Martin Chodorow, 1998. Combin-
ing Local Context and WordNet Similarity for Word
Sense Identification. MIT Press.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
A pdtb-styled end-to-end discourse parser. Tech-
nical report. http://www.comp.nus.edu.sg/ linzi-
hen/publications/tech2010.pdf.
Marie-catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh,
Alan Lee, Aravind Joshi, Livio Robaldo, and
Bonnie Webber. 2007. The penn discourse tree-
bank 2.0 annotation manual. Technical report.
http://www.seas.upenn.edu/ pdtb/PDTBAPI/pdtb-
annotation-manual.pdf.
Mehwish Riaz and Roxana Girju. 2010. Another look at
causality: Discovering scenario-specific contingency
relationships with no supervision. In ICSC.
N. Rizzolo and D. Roth. 2010. Learning based java for
rapid development of nlp systems. In LREC.
Dan Roth and Wen Tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In CoNLL.
Dan Roth and Wen Tau Yih. 2007. Global inference for
entity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Scheffczyk.
2010. FrameNet II: Extended Theory and Practice.
http://framenet.icsi.berkeley.edu.
Yizhou Sun, Ning Liu, Kunqing Xie, Shuicheng Yan,
Benyu Zhang, and Zheng Chen. 2007. Causal rela-
tion of queries from temporal logs. In WWW.
Patrick Suppes. 1970. A Probabilistic Theory of Causal-
ity. Amsterdam: North-Holland Publishing Company.
303
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 677?687, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Joint Inference for Event Timeline Construction
Quang Xuan Do Wei Lu Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{quangdo2,luwei,danr}@illinois.edu
Abstract
This paper addresses the task of construct-
ing a timeline of events mentioned in a given
text. To accomplish that, we present a novel
representation of the temporal structure of a
news article based on time intervals. We then
present an algorithmic approach that jointly
optimizes the temporal structure by coupling
local classifiers that predict associations and
temporal relations between pairs of tempo-
ral entities with global constraints. Moreover,
we present ways to leverage knowledge pro-
vided by event coreference to further improve
the system performance. Overall, our experi-
ments show that the joint inference model sig-
nificantly outperformed the local classifiers by
9.2% of relative improvement in F1. The ex-
periments also suggest that good event coref-
erence could make remarkable contribution to
a robust event timeline construction system.
1 Introduction
Inferring temporal relations amongst a collection of
events in a text is a significant step towards vari-
ous important tasks such as automatic information
extraction and document comprehension. Over the
past few years, with the development of the Time-
Bank corpus (Pustejovsky et al2003) , there have
been several works on building automatic systems
for such a task (Mani et al2006; Chambers and
Jurafsky, 2008; Yoshikawa et al2009; Denis and
Muller, 2011).
Most previous works devoted much efforts to the
task of identifying relative temporal relations (such
as before, or overlap) amongst events (Chambers
?
??
|
?
t1
|
?
t2
|
?
t3
?
|
t4
|
?
+?
|
Time
??
I1
I2
I3
e1
e2
e4
e3/e5
e7 ? e6
Figure 1: A graphical illustration of our timeline representation.
The e?s, t?s and I?s are events, time points and time intervals,
respectively.
and Jurafsky, 2008; Denis and Muller, 2011), with-
out addressing the task of identifying correct asso-
ciations between events and their absolute time of
occurrence. Even if this issue is addressed, certain
restrictions are often imposed for efficiency reasons
(Yoshikawa et al2009; Verhagen et al2010). In
practice, however, being able to automatically infer
the correct time of occurrence associated with each
event is crucial. Such information not only leads to
better text comprehension, but also enables fusion
of event structures extracted from multiple articles
or domains.
In this work, we are specifically interested in map-
ping events into an universal timeline representa-
tion. Besides inferring the relative temporal rela-
tions amongst the events, we would also like to au-
tomatically infer a specific absolute time of occur-
rence for each event mentioned in the text. Unlike
previous work, we associate each event with a spe-
cific absolute time interval inferred from the text. An
example timeline representation is illustrated in Fig.
677
1. Further details of our timeline representation are
given in Sec. 2.3.
We perform global inference by combining a col-
lection of local pairwise classifiers through the use
of an Integer Linear Programming (ILP) formula-
tion that promotes global coherence among local de-
cisions. The formulation allows our model to pre-
dict both event-event relations and event-time inter-
val associations simultaneously. We show that, with
the use of time intervals instead of time points, our
approach leads to a more concise ILP formulation
with reduced number of variables and constraints.
Moreover, we observed that event coreference can
reveal important information for such a task. We
propose that different event mentions that refer to
the same event can be grouped together before clas-
sification and performing global inference. This can
reduce the amount of efforts in both classification
and inference stages and can potentially eliminate
mistakes that would be made otherwise without such
coreference information. To the best of our knowl-
edge, our proposal of leveraging event coreference
to support event timeline construction is novel.
Our experiments on a collection of annotated
news articles from the standard ACE dataset demon-
strate that our approach produces robust timelines of
events. We show that our algorithmic approach is
able to combine various local evidences to produce
a global coherent temporal structure, with improved
overall performance. Furthermore, the experiments
show that the overall performance can be further im-
proved by exploiting knowledge from event corefer-
ence.
2 Background
We focus on the task of mapping event mentions in
a news article to a timeline. We first briefly describe
and define several basic concepts.
2.1 Events
Following the annotation guidelines of the ACE
project, we define an event as an action or occur-
rence that happens with associated participants or
arguments. We also distinguish between events and
event mentions, where a unique event can be core-
ferred to by a set of explicit event mentions in an
article. Formally, an event Ei is co-referred to by
a set of event mentions (ei1, e
i
2, . . . , e
i
k). Each event
mention e can be written as p(a1, a2, . . . , al), where
the predicate p is the word that triggers the presence
of e in text, and a1, a2, . . . al are the arguments asso-
ciated with e. In this work we focus on four tempo-
ral relations between two event mentions including
before, after, overlap and no relation.
2.2 Time Intervals
Similar to Denis and Muller (2011), we define time
intervals as pairs of time endpoints. Each time in-
terval I is denoted by [t?, t+], where t? and t+ are
two time endpoints representing the lower and upper
bound of the interval I , respectively, with t? ? t+.
The general form of a time endpoint is written as
?YYYY-MM-DD hh:mm:ss?. An endpoint can be un-
defined, in which case it is set to an infinity value:
??, or +?. There are two types of time intervals:
Explicit intervals are time intervals that can be
extracted directly from a given text. For example,
consider the following snippet of an article in our
data set: The litigation covers buyers in auctions
outside the United States between January 1, 1993
and February 7, 2000. In this example, we can ex-
tract and normalize two time intervals which are ex-
plicitly written, including January 1, 1993? [1993-
01-01 00:00:00, 1993-01-01 23:59:59] and Febru-
ary 7, 2000 ? [2000-02-07 00:00:00, 2000-02-07
23:59:59]. Moreover, an explicit interval can also
be formed by one or more separate explicit temporal
expressions. In the example above, the connective
term between relates the two expressions to form a
single time interval: between January 1, 1993 and
February 7, 2000 ? [1993-01-01 00:00:00, 2000-
02-07 23:59:59]. To extract explicit time intervals
from text, we use the time interval extractor de-
scribed in Zhao et al2012).
Implicit intervals are time intervals that are not
explicitly mentioned in the text. We observed that
there are events that cannot be assigned to any pre-
cise time interval but are roughly known to occur
in the past or in the future relative to the Doc-
ument Creation Time (DCT) of the article. We
introduce two implicit time intervals to represent
the past and the future events as (??, t?DCT ] and
[t+DCT ,+?), respectively. In addition, we also al-
low an event mention to be assigned into the entire
timeline, which is denoted by (??,+?) if we can-
678
not identify its time of occurrence. We also consider
DCT as an implicit interval.
We say that the time interval Ii precedes the time
interval Ij on a timeline if and only if t
+
i ? t
?
j ,
which also implies that Ii succeeds Ij if and only if
t?i ? t
+
j . The two intervals overlap, otherwise.
2.3 Timeline
We define a timeline as a partially ordered set of time
intervals. Fig. 1 gives a graphical illustration of an
example timeline, where events are annotated and
associated with time intervals. Relations amongst
events can be properly reflected in the timeline rep-
resentation. For example, in the figure, the events e1
and e2 are both associated with the interval I1. The
relation between them is no relation, since it is un-
clear which occurs first. On the other hand, e5 and
e3 both happen in the interval I2 but they form an
overlap relation. The events e6 and e7 occur within
the same interval I3, but e7 precedes (i.e. before) e6
on the timeline. The event e4 is associated with the
interval (??,+?), indicating there is no knowl-
edge about its time of occurrence.
We believe that such a timeline representation
for temporally ordering events has several advan-
tages over the temporal graph representations used
in previous works (Chambers and Jurafsky, 2008;
Yoshikawa et al2009; Denis and Muller, 2011).
Unlike previous works, in our model the events are
partially ordered in a single timeline, where each
event is associated with a precise time interval. This
improves human interpretability of the temporal re-
lations amongst events and time. This property of
our timeline representation, thus, facilitates merg-
ing multiple timelines induced from different arti-
cles. Furthermore, as we will show later, the use
of time intervals within the timeline representation
simplifies the global inference formulation and thus
the inference process.
3 A Joint Timeline Model
Our task is to induce a globally coherent timeline
for a given article. We thus adopt a global infer-
ence model for performing the task. The model
consists of two components: (1) two local pairwise
classifiers, one between event mentions and time in-
tervals (the E?T classifier) and one between event
mentions themselves (the E?E classifier), and (2)
a joint inference module that enforces global co-
herency constraints on the final outputs of the two
local classifiers. Fig. 2 shows a simplified temporal
structure of event mentions and time intervals of an
article in our model.
Our E?T classifier is different from previous
work (Chambers and Jurafsky, 2008; Yoshikawa et
al., 2009; Denis and Muller, 2011), where such clas-
sifiers were trained to identify temporal relations be-
tween event mentions and a temporal expression. In
our work, in order to construct absolute timeline of
event mentions, temporal expressions are captured
and normalized as absolute time intervals. The E?T
classifiers are then used to assign event mentions to
their contextually corresponding time intervals.
We also lifted several restrictions imposed in pre-
vious work (Bethard et al2007; Yoshikawa et al
2009; Verhagen et al2010). Specifically, we do
not require that event mentions and time expressions
have to appear in the same sentence, and we do not
require two event mentions have to appear very close
to each other (e.g., main event mentions in adjacent
sentences) in order to be considered as candidate
pairs for classification. Instead, we performed clas-
sifications over all pairs of event mentions and time
intervals as well as over all pairs of event mentions.
We show through experiments that lifting these re-
strictions is indeed important (see Sec. 5).
Another important improvement over previous
work is our global inference model We would like
to highlight that our work is also distinct from most
previous works in the global inference component.
Specifically, our global inference model jointly op-
timizes the E-E relations amongst event mentions
and their associations, E-T, with temporal informa-
tion (intervals in our case). Previous work (Cham-
bers and Jurafsky, 2008; Denis and Muller, 2011),
on the other hand, assumed that the E-T information
is given and only tried to improve E-E.
3.1 The Pairwise Classifiers
We first describe our local classifiers that associate
event mention with time interval and classify tempo-
ral relations between event mentions, respectively.
CE?T : is the E?T classifier that associates an
event mention with a time interval. Given an event
mention and a time interval, the classifier predicts
679
e1
e
2
e
3
e
4
e
n-1
e
n
e
5
? ? ?
I
1
I
2
I
3
I
m? ? ?
Figure 2: A simplified temporal structure of an article. There
are m time intervals I1 ? ? ? Im and n event mentions e1 ? ? ? en.
A solid edge indicates an association between an interval and
an event mention, whereas a dash edge illustrates a temporal
relation between two event mentions.
whether the former associates with the latter.
CE?T (ei, Ij)? {0, 1},
?i, j, 1 ? i ? n, 1 ? j ? m, (1)
where n and m are the number of event mentions
and time intervals in an article, respectively.
CE?E : is the E?E classifier that identifies
the temporal relation between two event mentions.
Given a pair of event mentions, the classifier predicts
one of the four temporal relations between them:
b?efore, a?fter, o?verlap and n?o relation. Specifically:
CE?E(ei, ej)? {b?, a?, o?, n?},
?i, j, 1 ? i, j ? n, i 6= j, (2)
For training of the classifiers, we define a set of
features following some previous work (Bethard et
al., 2007; Chambers and Jurafsky, 2008; Yoshikawa
et al2009), together with some additional features
that we believe to be helpful for the interval-based
representation. We describe the base features below
and use ? and ? to denote the features used for CE?T
and CE?E , respectively. We use the term temporal
entity (or entity, for short) to refer to either an event
mention or a time interval.
Lexical Features: A set of lexical features related
to the temporal entities: (i)?? the word, lemma and
part-of-speech of the input event mentions and the
context surrounding them, where the context is de-
fined as a window of 2 words before and after the
mention; (ii)? the modal verbs to the left and to the
right of the event mention; (iii)? the temporal con-
nectives between the event mentions1.
1We define a list of temporal connectives including before,
after, since, when, meanwhile, lately, etc.
Syntactic Features: (i)?? which entity appears
first in the text; (ii)?? whether the two entities appear
in the same sentence; (iii)?? the quantized number of
sentences between the two entities2; (iv)?? whether
the input event mentions are covered by preposi-
tional phrases and what are the heads of the phrases;
(v)?? if the entities are in the same sentence, what is
their least common constituent on the syntactic parse
tree; (vi)? whether there is any other temporal entity
that is closer to one of the two entities.
Semantic Features?: A set of semantic features,
mostly related to the input event mentions: (i)
whether the input event mentions have a common
synonym from their synsets in WordNet (Fellbaum,
1998); (ii) whether the input event mentions have a
common derivational form derived from WordNet.
Linguistic Features??: The tense and the aspect
of the input event mentions. We use an in-house
rule-based recognizer to extract these features.
Time Interval Features?: A set of features re-
lated to the input time interval: (i) whether the
interval is implicit; (ii) if it is implicit, identify
its interval type: ?dct? = [t?DCT , t
+
DCT ], ?past? =
(??, t?DCT ], ?feature? = [t
+
DCT ,+?), and ?en-
tire? = (??,+?); (iii) the interval is before, after
or overlapping with the DCT.
We note that unlike many previous work (Mani et
al., 2006; Chambers and Jurafsky, 2008; Denis and
Muller, 2011), our classifiers do not use any gold
annotations of event attributes (event class, tense, as-
pect, modal and polarity) provided in the TimeBank
corpus as features.
In our work, we use a regularized averaged Per-
ceptron (Freund and Schapire, 1999) as our classifi-
cation algorithm3. We used the one-vs.-all scheme
to transform a set of binary classifiers into a multi-
class classifier (for CE?E). The raw prediction
scores were converted into probability distribution
using the Softmax function (Bishop 1996). If there
are n classes and the raw score of class i is acti, the
posterior estimation for class i is:
P? (i) =
eacti
?
1?j?n e
actj
2We quantize the number of sentences between two entities
to 0, 1, 2, less than 5 and greater than or equal to 5
3Other algorithm (e.g. SVM) gave comparable or worse re-
sults, so we only show the results from Averaged Perceptron.
680
3.2 Joint Inference for Event Timeline
To exploit the interaction among the temporal enti-
ties in an article, we optimize the predicted tempo-
ral structure, formed by predictions from CE?T and
CE?E , w.r.t. a set of global constraints that enforce
coherency on the final structure. We perform exact
inference using Integer Linear Programming (ILP)
as in (Roth and Yih, 2007; Clarke and Lapata, 2008).
We use the Gurobi Optimizer4 as a solver.
Let I = {I1, I2, . . . , Im} denote the set of time
intervals extracted from an article, and let E =
{e1, e2, . . . , en} denote all event mentions in the
same article. Let EI = {(ei, Ij) ? E ? I|ei ?
E , Ij ? I} denote the set of all pairs of event
mentions and time intervals. We also denote the
set of event mention pairs by EE = {(ei, ej) ?
E ? E|ei ? E , ej ? E , i 6= j}. The prediction prob-
ability of an association of a pair eI ? EI, given
by classifier CE?T , is denoted by p?eI,1?
5. Now, let
R = {b?, a?, o?, n?} be the set of temporal relations be-
tween two event mentions. The prediction proba-
bility of an event mention pair ee ? EE that takes
temporal relation r, given by CE?E , is denoted by
p?ee,r?. Furthermore, we define x?eI,1? to be a binary
indicator variable that takes on the value 1 iff an as-
sociation is predicted between e and I . Similarly,
we define a binary indicator variable y?ee,r? of a pair
of event mentions ee that takes on the value 1 iff ee
is predicted to hold the relation r.
The objective function is then defined as a linear
combination of the prediction probabilities from the
two local classifiers as follows:
arg max
x,y
[
?
?
eI?EI
p?eI,1? ? x?eI,1?
+ (1? ?)
?
ee?EE
?
r?R
p?ee,r? ? y?ee,r?
]
(3)
subject to the following constraints:
x?eI,1? ? {0, 1}, ?eI ? EI (4)
y?ee,r? ? {0, 1}, ?ee ? EE , r ? R (5)
?
r?R
y?ee,r? = 1, ?ee ? EE (6)
4http://gurobi.com/
5This value is complementary to the non-association proba-
bility, denoted by p?eI,0? = 1? p?eI,1?
We use the single parameter ? to balance the over-
all contribution of two components E-T and E-E.
? is determined through cross validation tuning on
a development set. We use (4) and (5) to make sure
x?eI,1? and y?ee,r? are binary values. The equality
constraint (6) ensures that exactly one particular re-
lation can be assigned to each event mention pair.
In addition, we also require that each event is as-
sociated with only one time interval. These con-
straints are encoded as follows:
?
I?I
x?eI,1? = 1, ?e ? E (7)
Our model also enforces reflexivity and transitiv-
ity constraints on the relations among event men-
tions as follows:
y?eiej ,r? ? y?ejei,r?? = 0,
?eiej = (ei, ej) ? EE , i 6= j (8)
y?eiej ,r1? + y?ejek,r2? ? y?eiek,r3? ? 1,
?eiej , ejek, eiek ? EE , i 6= j 6= k (9)
The equality constraints in (8) encode reflexive
property of event-event relations, where the rela-
tion r? denotes the inversion of the relation r. The
set of possible (r, r?) pairs is defined as follows:
{
(b?, a?), (a?, b?), (o?, o?), (n?, n?)
}
. Following the work
of (Bramsen et al2006; Chambers and Jurafsky,
2008), we encode transitive closure of relations be-
tween event mentions with inequality constraints in
(9), which states that if the pair (ei, ej) has a certain
relation r1, and the pair (ej , ek) has the relation r2,
then the relation r3 must be satisfied between ei and
ek. Examples of such triple (r1, r2, r3) include (b?, b?,
b?) and (a?, a?, a?).
Finally, to capture the interactions between our
local pairwise classifiers we add the following con-
straints:
x?eiIk,1? + x?ejIl,1? ? y?eiej ,b?? ? 1,
?eiIk, ejIl ? EI, ?eiej ? EE ,
Ik precedes Il, i 6= j, k 6= l (10)
Intuitively, the inequality constraints in (10) spec-
ify that a temporal relation between two event men-
tions can be inferred from their respective associated
681
time intervals. Specifically, if two event mentions ei
and ej are associated with two time intervals Ik and
Il respectively, and Ik precedes Il in the timeline,
then ei must happen before ej .
It is important to note that our interval-based for-
mulation is more concise in terms of the number of
variables and constraints needed in the ILP relative
to time expression-based (or timepoint-based) for-
mulations used in previous work (Chambers and Ju-
rafsky, 2008). Specifically, in such timepoint-based
formulations, the relation between each event men-
tion and each time expression needs to be inferred,
resulting in |E||T ||RT | variables, where |E|, |T |,
and |RT | are the numbers of event mentions, time
points, and temporal relations respectively. In con-
trast, only |E||I| variables are required in our for-
mulation, where |I| is the number of intervals (since
we extract intervals explicitly, |I| is roughly equal
to |T |). Furthermore, performing inference with the
timepoint-based formulation would require |E||T |
equality constraints to enforce that each event men-
tion can take only one relation inRT for a particular
time point, whereas our interval-based model only
requires |E| constraints, since each event is strictly
associated with one interval (see Eqn. (7)). We jus-
tify the benefits of our formulation later in Sec. 5.4.
4 Incorporating Knowledge from Event
Coreference
One of the key contributions of our work is using
event coreference information to enhance the time-
line construction performance. This is motivated by
the following two principles:
(P1) All mentions of a unique event are associ-
ated with the same time interval, and overlap with
each other.
(P2) All mentions of an event have the same tem-
poral relation with all mentions of another event.
The example below, extracted from an article pub-
lished on 03/11/2003 in the Automatic Content Ex-
traction (ACE), 2005, corpus6 serves to illustrate the
significance of event coreference to our task.
6http://www.itl.nist.gov/iad/mig/tests/ace/2005/
The world?s most powerful fine art auction houses,
Sotheby?s and Christie?s, have agreed to [e11 =
pay] 40 million dollars to settle an international
price-fixing scam, Sotheby?s said. The [e12 = pay-
ment], if approved by the courts, would settle a
slew of [e21 = suits] by clients over auctions held
between 1993 and 2000 outside the US. ... Sotheby?s
and Christie?s will each [e13 = pay] 20 million dol-
lars,? said Sotheby?s, which operates in 34 countries.
In this example, there are 4 event mentions, whose
trigger words are highlighted in bold face. The un-
derlined text gives an explicit time interval: I1 =
[1993-01-01 00:00:00, 2000-12-31 23:59:59] (we
ignore 2 other intervals given by 1993 and 2000
to simplify the illustration). Now if we consider
the event mention e12, it actually belongs to the im-
plicit future interval I2 = [2003-03-11 23:59:59,
+?). Nevertheless, there is a reasonable chance
that CE?T associates it with I1, given that they both
appear in the same sentence, and there is no di-
rect evident feature indicating the event will actu-
ally happen in the future. In such a situation, using
a local classifier to identify the correct temporal as-
sociation could be challenging.
Fortunately, precise knowledge from event coref-
erence may help alleviate such a problem. The
knowledge reveals that the 4 event mentions can be
grouped into 2 distinct events: E1 = {e11, e
1
2, e
1
3},
E2 = {e21}. If CE?T can make a strong prediction
in associating the event mention e11 (or e
1
3) to I2, in-
stead of I1, the system will have a high chance to
re-assign e12 to I2 based on principle (P1). Similarly,
if CE?E is effective in figuring out that some men-
tion of event E1 occurs after some mention of E2,
then all the mentions of E1 would be predicted to
occur after all mentions in E2 according to (P2).
To incorporate knowledge from event coreference
into our classifiers and the joint inference model, we
use the following procedure: (1) performing classi-
fication with CE?T and CE?E on the data, (2) using
the knowledge from event coreference to overwrite
the prediction probabilities obtained by the two lo-
cal classifiers in step (1), and (3) applying the joint
inference model on the new prediction probabilities
obtained from (2). We note that if we stop at step (2),
we get the outputs of the local classifiers enhanced
by event coreference knowledge.
To overwrite the classification probabilities using
682
event coreference knowledge, we propose two ap-
proaches as follows:
MaxScore: We define the probability between
any mention e ? Ei and an interval I as follows:
p?eI,1? = max
e??Ei
P? (e?, I) (11)
where P? (e?, I) is the classifier (CE?T ) probability
for associating event mention e? to the time interval.
On the other hand, the probabilities for associat-
ing the set of temporal relations, R, to each pair of
mentions in Ei?Ej , is given by the following pair:
(ei, ej)? = arg max
(ei? ,ej? )?Ei?Ej ,r?R
P?
(
(ei
?
, ej
?
), r)
)
p?ee,r? = P?
(
(ei, ej)?, r
)
,?r ? R (12)
In other words, over all possible event mention
pairs and relations, we first pick the pair who glob-
ally obtains the highest probability for some rela-
tion. Next, we simply take the probability distri-
bution of that event mention pair as the distribution
over the relations, for the event pair.
SumScore: The probability between any mention
e ? Ei and an interval I is obtained by:
p?eI,1? =
1
|Ei|
?
e??Ei
P? (e?, I) (13)
To obtain the probability distribution over the set
of temporal relations,R, for any pair of mentions in
Ei ? Ej , we used the following procedure:
r? = arg max
r?R
?
ei?Ei
?
ej?Ej
P?
(
(ei, ej), r
)
(ei, ej)? = arg max
(ei? ,ej? )?Ei?Ej
P?
(
(ei
?
, ej
?
), r?
)
p?ee,r? = P?
(
(ei, ej)?, r
)
,?r ? R (14)
In other words, given two groups of event men-
tions, we first compute the total score of each rela-
tion, and select the relation which has the highest
score. Next from the list of pairs of event mentions
from the two groups, we select the pair which has the
relation r* with highest score compared to all other
pairs. The probability distribution of this pair will
be used as the probability distribution of all event
mention pairs between the two events.
In both approaches, we assign the overlap rela-
tions to all pairs of event mentions in the same event
with probability 1.0.
5 Experimental Study
We first describe the experimental data and then
present and discuss the experimental results.
5.1 Data and Setup
Most previous works in temporal reasoning used
the TimeBank corpus as a benchmark. The cor-
pus contains a fairly diverse collection of anno-
tated event mentions, without any specific focus on
certain event types. According to the annotation
guideline of the corpus, most of verbs, nominal-
izations, adjectives, predicative clauses and preposi-
tional phrases can be tagged as events. However, in
practice, when performing temporal reasoning about
events in a given text, one is typically interested in
significant and typed events, such as Killing, Leg-
islation, Election. Furthermore, event mentions in
TimeBank are annotated with neither event argu-
ments nor event coreference information.
We noticed that the ACE 2005 corpus contains the
annotation that we are interested in. The corpus con-
sists of articles annotated with event mentions (with
event triggers and arguments) and event coreference
information. To create an experimental data set for
our work, we selected from the corpus 20 newswire
articles published in March 2003. To extract time
intervals from the articles, we used the time inter-
val extractor described in (Zhao et al2012) with
minimal post-processing. Implicit intervals are also
added according to Sec. 2.2. We then hired an anno-
tator with expertise in the field to annotate the data
with the following information: (i) event mention
and time interval association, and (ii) the temporal
relations between event mentions, including {b?, a?,
o?}. The annotator was not required to annotate all
pairs of event mentions, but as many as possible.
Next, we saturated the relations based on the ini-
tial annotations as follows: (i) event mentions that
had not been associated with any time intervals were
assigned to the entire timeline interval (??,+?),
and (ii) added inferred temporal relations between
event mentions with reflectivity and transitivity. Ta-
ble 1 shows the data statistics before and after sat-
uration. There are totally 8312 event pairs from 20
documents, including no relation pairs. We note that
in a separate experiment, we still evaluated CE?E
on the TimeBank corpus and got better performance
683
Data #Intervals #E-mentions #E-T #E-E
Initial 232 324 305 376
Saturated 232 324 324 5940
Table 1: The statistics of our experimental data set.
than a corresponding classifier in an existing work
(see Sec. 5.4).
We conducted all experiments with 5-fold cross
validation at the instance level on our data set after
saturation. The global inference model was applied
on a whole document. The results of the systems are
reported in averaged precision, recall and F1 score
on the association performance, for CE?T , and the
temporal relations (we excluded the n? relation, for
CE?E). We also measured the overall performance
of the systems by computing the average of the per-
formance of the classifiers.
5.2 A Baseline
We developed a baseline system that works as fol-
lows. It associates an event mention with the closest
time interval found in the same sentence. If such
an interval is not found, the baseline associates the
mention with the closest time interval to the left.
If the interval is again not found, the mention will
be associated with the DCT interval. The baseline
is based on the intuition of natural reading order:
events that are mentioned earlier are likely to pre-
cede those mentioned later. For the temporal rela-
tion between a pair of event mentions, the baseline
treats the event mention that appears earlier in the
text as temporally happening before the other men-
tion. The baseline performance is shown in the first
group of results in Table 2.
5.3 Our Systems
For our systems, we first evaluated the performance
of our local pairwise classifiers and the global in-
ference model. The second group of results in Ta-
ble 2 shows the systems? performance. Overall,
the results show that our global inference model
relatively outperformed the baseline and the local
classifiers by 57.8% and 9.2% in F1, respectively.
We perform a bootstrap resampling significance test
(Koehn, 2004) on the output predictions of the lo-
cal classifiers with and without the inference model.
The test shows that the overall improvement with
the inference model is statistically significant (p <
0.01). This indicates the effectiveness of our joint
inference model with global coherence constraints.
Next, we integrated event coreference knowledge
into our systems (as described in Sec. 4) and eval-
uated their performance. Our experiments showed
that the SumScore approach works better for CE?T ,
while MaxScore is more suitable for CE?E . Our ob-
servations showed that event mentions of an event
may appear in close proximity with multiple time
intervals in the text, making CE?T produce high
prediction scores for many event mention-interval
pairs. This, consequently, confuses MaxScore on
the best association of the event and the time inter-
vals, whereas SumScore overcomes the problem by
averaging out the association scores. On the other
hand, CE?E gets more benefit from MaxScore be-
causeCE?E works better on pairs of event mentions
that appear closely in the text, which activate more
valuable learning features. We will report the results
using the best approach of each classifier.
To evaluate our systems with event coreference
knowledge, we first experimented our systems with
gold event coreference as given by the ACE 2005
corpus. Table 2 shows the contribution of event
coreference to our systems in the third group of the
results. The results show that injecting knowledge
from event coreference remarkably improved both
the local classifiers and the joint inference model.
Overall, the system that combined event corefer-
ence and the global inference model achieved the
best performance, which significantly overtook all
other compared systems. Specifically, it outper-
formed the baseline system, the local classifiers, and
the joint inference model without event coreference
with 80%, 25%, and 14% of relative improvement in
F1, respectively. It also consistently outperformed
the local classifiers enhanced with event corefer-
ence. We note that the precision and recall of CE?T
in the joint inference model are the same because
the inference model enforced each event mention to
be associated with exactly one time interval. This
is also true for the systems integrated with event
coreference because our integration approaches as-
sign only one time interval to an event mention.
We next move to experimenting with automati-
cally learned event coreference systems. In this ex-
684
Model
CE?T CE?E Overall
Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
1 Baseline 33.29 33.29 33.29 20.86 32.81 25.03 27.06 33.05 29.16
2
No Event Coref.
Local classifiers 62.70 34.50 43.29 40.46 42.42 40.96 51.58 38.46 42.13
Global inference 47.88 47.88 47.88 41.42 48.04 44.14 44.65 47.96 46.01
3
With Gold Event Coref.
Local classifiers 50.88 50.88 50.88 43.86 52.65 47.46 47.37 51.77 49.17
Global inference 50.88 50.88 50.88 48.04 62.45 54.05 49.46 56.67 52.47
4
With Learned Event Coref.
Local classifiers 46.37 46.37 46.37 40.83 45.28 42.60 43.60 45.83 44.49
Global inference 46.37 46.37 46.37 42.09 52.50 46.47 44.23 49.44 46.42
Table 2: Performance under various evaluation settings. All figures are averaged scores from 5-fold cross-validation experiments.
periment, we re-trained the event coreference sys-
tem described in Chen et al2009) on all arti-
cles in the ACE 2005 corpus, excluding the 20 ar-
ticles used in our data set. The performance of these
systems are shown in the fourth group of the re-
sults in Table 2. The results show that by using a
learned event coreference system, we achieved the
same improvement trends as with gold event coref-
erence. However, we did not obtain significant im-
provement when comparing with global inference
without event coreference information. This result
shows that the performance of an event coreference
system can have a significant impact on the over-
all performance. While this suggests that a better
event coreference system could potentially help the
task more, it also opens the question whether event
coreference can be benefited from our local classi-
fiers through the use of a joint inference framework.
We would like to leave this for future investigations.
5.4 Previous Work-Related Experiments
We also performed experiments using the same set-
ting as in (Yoshikawa et al2009), which followed
the guidelines of the TempEval challenges (Verha-
gen et al2007; Verhagen et al2010), on our sat-
urated data. Several assumptions were made to sim-
plify the task. For example, only main events in
adjacent sentences are considered when identifying
event-event relations. See (Yoshikawa et al2009)
for more details. We performed 5-fold cross valida-
tion without event coreference. Overall, the system
achieved 29.99 F1 for the local classifiers and 34.69
when the global inference is used. These results are
better than the baseline but underperform our full
models where those simplification assumptions are
not imposed, as shown in Table 2, indicating the im-
portance of relaxing their assumptions in practice.
We also evaluated our CE?E on the TimeBank
corpus. We followed the settings of Chambers and
Jurafsky (2008) to extract all event mention pairs
that were annotated with before (or ibefore, ?imme-
diately before?) and after (or iafter) relations in 183
news articles in the corpus. We trained and evalu-
ated ourCE?E on these examples with the same fea-
ture set that we evaluated in our experiments above,
with gold tense and aspect features but without event
type. Following their work, we performed 10-fold
cross validation. Our classifier achieved a micro-
averaged accuracy of 73.45%, whereas Chambers
and Jurafsky (2008) reported 66.8%. We next in-
jected the knowledge of an event coreference sys-
tem trained on the ACE2005 corpus into our CE?E ,
and obtained a micro-averaged accuracy of 73.39%.
It was not surprising that event coreference did not
help in this dataset because: (i) different domains
? the event coreference was trained on ACE 05 but
applied on TimeBank, and (ii) different annotation
guidelines on events in ACE 2005 and TimeBank.
Finally, we conducted an experiment that justi-
fies the advantages of our interval-based inference
model over a time point-based inference. To do this,
we first converted our data in Table 1 from inter-
vals to time points and infer the temporal relations
between the annotated event mentions and the time
points: before, after, overlap, and unknown. We
modified the first component in the objective func-
tion in (3) to accommodate these temporal relations.
We also made several changes to the constraints,
including removing those in (7) since they are no
longer required, and adding constraints that ensure
685
the relation between a time point and an event men-
tion takes exactly one value. Proper changes were
also made to other constraints in (10) to reflect the
fact that time points are considered rather than inter-
vals. We observed that experiment with such a for-
mulation was unable to finish within 5 hours (we ter-
minated the ILP inference after waiting for 5 hours),
whereas our interval-based model finished the ex-
periment with an average of 21 seconds per article.
6 Related Work
Research in temporal reasoning recently received
much attention. Allen (1983) introduced an interval
based temporal logic which has been used widely
in the field. Recent efforts in building an annotated
temporal corpus (Pustejovsky et al2003) has pop-
ularized the use of machine learning techniques for
the task (Mani et al2006; Bethard et al2007).
This corpus was later used (with simplifications) in
two TempEval challenges (Verhagen et al2007;
Verhagen et al2010). In these challenges, several
temporal-related tasks were defined including the
tasks of identifying the temporal relation between an
event mention and a temporal expression in the same
sentence, and recognizing temporal relations of pairs
of event mentions in adjacent sentences. However,
with several restrictions imposed to these tasks, the
developed systems were not practical.
Recently, there has been much work attempting
to leverage Allen?s interval algebra of temporal re-
lations to enforce global constraints on local pre-
dictions. The work of Tatu and Srikanth (2008)
used global relational constraints to not only expand
the training data but also identifies temporal incon-
sistencies to improve local classifiers. They used
greedy search to select the most appropriate config-
uration of temporal relations among events and tem-
poral expressions. For exact inferences, Bramsen et
al. (2006), Chambers and Jurafsky (2008), Denis
and Muller (2011), and Talukdar et al2012) for-
mulated the temporal reasoning problem in an ILP.
However, the inference models in their work were
not a joint model involving multiple local classifiers
but only one local classifier was involved in their ob-
jective functions.
The work of Yoshikawa et al2009) did formu-
late a joint inference model with Markov Logic Net-
work (MLN). They, however, used the same setting
as the TempEval challenges, thus only pairs of tem-
poral entities in the same or adjacent sentences are
considered. Our work, on the other hand, focuses on
constructing an event timeline with time intervals,
taking multiple local pairwise predictions into a joint
inference model and removing the restrictions on the
positions of the temporal entities. Furthermore, we
propose for the first time to use event coreference
and evaluate the importance of its role in the task of
event timeline construction.
7 Conclusions and Future Work
We proposed an interval-based representation of the
timeline of event mentions in an article. Our rep-
resentation allowed us to formalize the joint infer-
ence model that can be solved efficiently, compared
to a time point-based inference model, thus open-
ing up the possibility of building more practical
event temporal inference systems. Our inference
model achieved significant improvement over the lo-
cal classifiers. We also showed that event coref-
erence can naturally support timeline construction,
and good event coreference led to significant im-
provement in the system performance. Specifically,
when such gold event coreference knowledge was
injected into the model, a significant improvement
in the overall performance could be obtained. While
our experiments suggest that the temporal classi-
fiers can potentially help enhance the performance
of event coreference, in future work we would like
to investigate into coupling event coreference with
other components in a global inference framework.
Acknowledgments
The authors gratefully acknowledge the support
of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
No. FA8750-09-C-0181, and the Army Research
Laboratory (ARL) under agreement W911NF-09-2-
0053. The first author also thanks the Vietnam Ed-
ucation Foundation (VEF) for its sponsorship. Any
opinions, findings, and conclusion or recommenda-
tions expressed in this material are those of the au-
thors and do not necessarily reflect the view of the
VEF, DARPA, AFRL, ARL, or the US government.
686
References
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM.
Steven Bethard, James H. Martin, and Sara Klingenstein.
2007. Timelines from text: Identification of syntactic
temporal relations. In ICSC.
P. Bramsen, P. Deshpande, Y. K. Lee, and R. Barzilay.
2006. Inducing temporal graphs. In EMNLP.
N. Chambers and D. Jurafsky. 2008. Jointly combin-
ing implicit constraints improves temporal ordering.
In EMNLP.
Zheng Chen, Heng Ji, and Robert Haralick. 2009. A
pairwise event coreference model, feature impact and
evaluation for event coreference resolution. In Work-
shop on Events in Emerging Text Types.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear programming
approach. Journal of Artificial Intelligence Research.
Pascal Denis and Philippe Muller. 2011. Predicting
globally-coherent temporal structures from texts via
endpoint inference and graph decomposition. In IJ-
CAI.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine learning
of temporal relations. In ACL.
J. Pustejovsky, P. Hanks, R. Sauri, A. See, R. Gaizauskas,
A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro,
and M. Lazo. 2003. The TIMEBANK corpus. In
Corpus Linguistics.
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Introduction to Statistical Relational
Learning.
Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchell.
2012. Coupled temporal scoping of relational facts. In
WSDM.
Marta Tatu and Munirathnam Srikanth. 2008. Experi-
ments with reasoning for temporal relations between
events. In COLING.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal re-
lation identification. In SemEval-2007.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In SemEval-2010.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identify-
ing temporal relations with markov logic. In ACL-
IJCNLP.
Ran Zhao, Quang Do, and Dan Roth. 2012. A robust
shallow temporal reasoning system. In NAACL-HLT
Demo.
687
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 29?32,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
A Robust Shallow Temporal Reasoning System
Ran Zhao Quang Xuan Do Dan Roth
Computer Science Department
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{ranzhao1,quangdo2,danr}@illinois.edu
Abstract
This paper presents a demonstration of a tem-
poral reasoning system that addresses three
fundamental tasks related to temporal expres-
sions in text: extraction, normalization to time
intervals and comparison. Our system makes
use of an existing state-of-the-art temporal ex-
traction system, on top of which we add sev-
eral important novel contributions. In addi-
tion, we demonstrate that our system can per-
form temporal reasoning by comparing nor-
malized temporal expressions with respect
to several temporal relations. Experimental
study shows that the system achieves excellent
performance on all the tasks we address.
1 Introduction
Performing temporal reasoning with respect to tem-
poral expressions is important in many NLP tasks
such as text summarization, information extraction,
discourse understanding and information retrieval.
Recently, the Knowledge Base Population track (Ji
et al, 2011) introduced the temporal slot filling task
that requires identifying and extracting temporal in-
formation for a limited set of binary relations such as
(person, employee of), (person, spouse). In the work
of (Wang et al, 2010), the authors presented the
Timely Yago ontology, which extracted and incorpo-
rated temporal information as part of the description
of the events and relations in the ontology. Temporal
reasoning is also essential in supporting the emerg-
ing temporal information retrieval research direction
(Alonso et al, 2011).
In this paper, we present a system that addresses
three fundamental tasks in temporal reasoning:
? Extraction: Capturing the extent of time expres-
sions in a given text. This task is based on task A in
the TempEval-2 challenge (Verhagen et al, 2010).
Consider the following sentence:
Seventy-five million copies of the rifle have been
built since it entered production in February 1947.
In this sentence, February 1947 is a basic temporal
expression that should be extracted by the extraction
module. More importantly, we further extend the
task to support also the extraction of complex tem-
poral expressions that are not addressed by existing
systems. In the example above, it is important to rec-
ognize and capture the phrase since it entered pro-
duction in February 1947 as another temporal ex-
pression that expresses the time period of the manu-
facturing event (triggered by built.) For the best of
our knowledge, this extension is novel.
? Normalization: Normalizing temporal expres-
sions, which are extracted by the extraction module,
to a canonical form. Our system normalizes tem-
poral expressions (including complex ones) to time
intervals of the form [start point, end point]. The
endpoints follow a standard date and time format:
YYYY-MM-DD hh:mm:ss. Our system accounts for
an input reference date when performing the normal-
ization. For example, given March 20th, 1947 as a
reference date, our system normalizes the temporal
expressions extracted in the example above as fol-
lows: [1947-02-01 00:00:00, 1947-02-28 23:59:59]
and [1947-02-01 00:00:00, 1947-03-20 23:59:59],
respectively.
? Comparison: Comparing two time intervals
(i.e. normalized temporal expressions). This mod-
ule identifies the temporal relation that holds be-
29
tween intervals, including the before, before-and-
overlap, containing, equal, inside , after and after-
and-overlap relations. For example, when compar-
ing the two normalized time intervals above, we get
the following result: [1947-02-01 00:00:00, 1947-
02-28 23:59:59] is inside [1947-02-01 00:00:00,
1947-03-20 23:59:59].
There has been much work addressing the prob-
lems of temporal expression extraction and normal-
ization, i.e. the systems developed in TempEval-2
challenge (Verhagen et al, 2010). However, our sys-
tem is different from them in several aspects. First,
we extend the extraction task to capture complex
temporal expressions. Second, our system normal-
izes temporal expressions (including complex ones)
to time intervals instead of time points. Finally, our
system performs temporal comparison of time inter-
vals with respect to multiple relations. We believe
that with the rapid progress in NLP and IR, more
tasks will require temporal information and reason-
ing, and a system that addresses these three funda-
mental tasks well will be able to support and facili-
tate temporal reasoning systems efficiently.
2 The System
2.1 Temporal Expression Extraction
We built the temporal expression extraction module
on top of the Heideltime system (Stro?tgen and Gertz,
2010) to take advantage of a state-of-the-art tempo-
ral extraction system in capturing basic expressions.
We use the Illinois POS tagger1 (Roth and Zelenko,
1998) to provide part-of-speech tags for the input
text before passing it to HeidelTime. Below is an
example of the HeidelTime output of the example in
the previous section:
Seventy-five million copies of the rifle have been
built since it entered production in <TIMEX3
tid=?t2? type=?DATE? value=?1947-02?>February
1947</TIMEX3>
In this example, HeidelTime captures a basic tem-
poral expression: February 1947. However, Heidel-
Time cannot capture the complex temporal expres-
sion since it entered production in February 1947,
which expresses a period of time from February
1947 until the document creation time. This is ac-
tually the time period of the manufacturing event
1http://cogcomp.cs.illinois.edu/page/software view/POS
NP
PP
VP
  SBAR
Seventy-five million copies of the rifle have been built   since it entered production in Feburary 1947
VP
NP
S
Figure 1: The SBAR constituent in the parse tree de-
termines an extended temporal expression given that in
February 1947 is already captured by HeidelTime.
(triggered by built). To capture complex phrases, we
make use of a syntactic parse tree2 as illustrated in
Figure 1. A complex temporal expression is recog-
nized if it satisfies the following conditions:
? It is covered by a PP or SBAR constituent
in the parse tree.
? The constituent starts with a temporal con-
nective. In this work, we focus on an impor-
tant subset of temporal connectives, consist-
ing of since, between, from, before and after.
? It contains at least one basic temporal ex-
pression extracted by HeidelTime.
In addition, our extraction module also handles
holidays in several countries. For example, in
the sentence ?The gas price increased rapidly after
Christmas.?, we are able to extract two temporal ex-
pressions Christmas and after Christmas, which re-
fer to different time intervals.
2.2 Normalization to Time Intervals
Our system normalizes a temporal expression to a
time interval of the form [start point, end point],
where start point? end point. Each time endpoint of
an interval follows a standard date and time format:
YYYY-MM-DD hh:mm:ss. It is worth noting that this
format augments the date format in TimeML, used
by HeidelTime and other existing systems. Our date
and time format of each time endpoint refer to an
absolute time point on a universal timeline, making
our time intervals absolute as well. Furthermore, we
take advantage of the predicted temporal value of
each temporal expression from the HeidelTime out-
put. For instance, in the HeidelTime output example
above, we extract 1947-02 as the normalized date
of February 1947 and then convert it to the inter-
val [1947-02-01 00:00:00, 1947-02-28 23:59:59]. If
HeidelTime cannot identify an exact date, month or
year, we then resort to our own temporal normalizer,
2We use nlparser (Charniak and Johnson, 2005)
30
which consists of a set of conversion rules, regard-
ing to the document creation time of the input text.
An interval endpoint can get infinity value if its tem-
poral boundary cannot be specified.
2.3 Comparison
To compare two time intervals (i.e. normalized
temporal expressions), we define six temporal rela-
tions: before, before-and-overlap, contains, equals,
inside, after and after-and-overlap. The temporal
relation between two normalized intervals is deter-
mined by a set of comparison rules that take the four
interval endpoints into consideration. For example,
A = [sA, eA] contains B = [sB, eB] if and only if
(sA < sB)? (eA > eB), where s and e are intervals
start and end points, respectively.
3 Experimental Study
In this section, we present an evaluation of our ex-
tended temporal extractor, the normalizer and the
comparator. We do not evaluate the HeidelTime
temporal extractor again because its performance
was reported in the TempEval-2 challenge (Verha-
gen et al, 2010), where it achieved 0.86 F1 score on
the TimeBank data sets (Pustejovsky et al, 2003).
3.1 Data Preparation
We focus on scaling up temporal systems to deal
with complex expressions. Therefore, we prepared
an evaluation data set that consists of a list of sen-
tences containing at least one of the five temporal
connectives since, betwen, from, before and after.
To do this, we extract all sentences that satisfy the
condition from 183 articles in the TimeBank 1.2
corpus3. This results in a total of 486 sentences.
Each sentence in the data set comes with the doc-
ument creation time (DCT) of its corresponding ar-
ticle. The second and the third columns of Table
1 summarize the number of sentences and appear-
ances of each temporal connective.
We use this data set to evaluate the extended tem-
poral extractor, the normalizer and also the com-
parator of our system. We note that although this
data set is driven by our focused temporal connec-
tives, it does not lose the generality of evaluating
3http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?
catalogId=LDC2006T08
Connective # sent. # appear. Prec Rec F1
since 31 31 1.0 1.0 1.0
between 32 33 1.0 1.0 1.0
from 340 366 0.8 1.0 0.89
before 33 33 0.8 1.0 0.89
after 78 81 0.72 1.0 0.84
Avg. 0.86 1.0 0.92
Table 1: The performance of our extended temporal ex-
tractor on complex expressions which contain at least one
of the connectives shown in the first column. These ex-
pressions cannot be identified by existing systems.
Module Correct Incorrect Acc
Normalizer 191 16 0.92
Comparator 191 0 1.0
Table 2: The performance of the normalization and com-
parison modules. We only compare the 191 correctly
identified time intervals with their corresponding docu-
ment creation time.
the normalization and comparison modules because
the sentences in this data set alo contain many ba-
sic temporal expressions. Moreover, there are many
cases where the connectives in our data are not actu-
ally temporal connectives. Our system is supposed
to not capture them as temporal expressions. This is
also reflected in the experimental results.
3.2 Experimental Results
We report the performance of our extended tem-
poral extraction module using precision, recall and
F1 score as shown in the last three columns of Ta-
ble 1. We evaluate the normalization module on
the correctly extracted temporal expressions, includ-
ing basic expressions captured by HeidelTime and
the extended expressions identified by our extrac-
tor. A normalization is correct if and only if both
time interval endpoints are correctly identified. We
study the comparison module by evaluating it on
the comparisons of the correctly normalized expres-
sions against the corresponding DCT of the sen-
tences from which they are extracted. Because the
normalization and comparison outputs are judged as
correct or incorrect, we report the performance of
these modules in accuracy (Acc) as shown in Ta-
ble 2. Overall, the experimental study shows that
all modules in our system are robust and achieve ex-
cellent performance.
31
Figure 2: A screenshot of the input panel.
Figure 3: A screenshot of the output panel.
4 The Demonstration
4.1 Visualization
We have implemented our system in a web-based
demo4. Figure 2 shows a screenshot of the input
panel of the system. The input panel includes a main
text box that allows users to input the text, and some
other input fields that allow users to customize the
system?s outputs. Among the fields, the reference
date serves as the document creation time (DCT) of
the input text. All temporal expressions captured
from the text will be normalized based on the ref-
erence date and compared also to the reference date
as illustrated in Figure 3.
4.2 Script Outline
First, we will give an overview of existing temporal
reasoning systems. Then we will introduce the novel
contributions of our system. After that, we will go
over our web-based demonstration, including (i) the
input panel: reference date and the text to be ana-
lyzed, and (ii) the output panel: the extracted basic
and extended temporal expressions, the normalized
intervals, and the comparison results.
5 Conclusions
In this demonstration paper, we introduced a tempo-
ral reasoning system that addresses three fundamen-
tal problems related to temporal expressions in text,
4http://cogcomp.cs.illinois.edu/page/demo view/TempSys
including extraction, normalization and comparison.
Our system consists of a temporal expression ex-
tractor capable of dealing with complex temporal
phrases, a time interval normalizer and a time inter-
val comparator. The experimental study shows that
our system achieves a high level of performance,
which will allow it to support other systems that re-
quire complicated temporal reasoning.
Acknowledgement
This research is supported by the Army Research Laboratory
(ARL) under agreement W911NF-09-2-0053 and the Defense
Advanced Research Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. The second author also
thanks the Vietnam Education Foundation (VEF) for its spon-
sorship. Any opinions, findings, and conclusion or recommen-
dations expressed in this material are those of the authors and
do not necessarily reflect the view of the VEF, ARL, DARPA,
AFRL, or the US government.
References
Omar Alonso, Jannik Stro?tgen, Ricardo Baeza-Yates, and
Michael Gertz. 2011. Temporal information retrieval:
Challenges and opportunities. In TWAW.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In ACL.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the tac2011 knowledge base population
track. In TAC.
James Pustejovsky, Jose Castano, Robert Ingria, Roser
Sauri, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003. Timeml: Robust specification of event
and temporal expressions in text. In IWCS-5.
D. Roth and D. Zelenko. 1998. Part of speech tagging us-
ing a network of linear separators. In COLING-ACL,
The 17th International Conference on Computational
Linguistics.
Jannik Stro?tgen and Michael Gertz. 2010. Heideltime:
High quality rule-based extraction and normalization
of temporal expressions. In Proceedings of the 5th
International Workshop on Semantic Evaluation, Se-
mEval ?10.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation.
Yafang Wang, Mingjie Zhu, Lizhen Qu, Marc Spaniol,
and Gerhard Weikum. 2010. Timely yago: harvesting,
querying, and visualizing temporal knowledge from
wikipedia. In EDBT.
32
