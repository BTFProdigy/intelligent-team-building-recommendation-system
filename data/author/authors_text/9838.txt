Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 66?73,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
SVD Feature Selection for Probabilistic Taxonomy Learning
Fallucchi Francesca
Disp, University ?Tor Vergata?
Rome, Italy
fallucchi@info.uniroma2.it
Fabio Massimo Zanzotto
Disp, University ?Tor Vergata?
Rome, Italy
zanzotto@info.uniroma2.it
Abstract
In this paper, we propose a novel way
to include unsupervised feature selection
methods in probabilistic taxonomy learn-
ing models. We leverage on the computa-
tion of logistic regression to exploit unsu-
pervised feature selection of singular value
decomposition (SVD). Experiments show
that this way of using SVD for feature se-
lection positively affects performances.
1 Introduction
Taxonomies are extremely important knowledge
repositories in a variety of applications for nat-
ural language processing and knowledge repre-
sentation. Yet, manually built taxonomies such
as WordNet (Miller, 1995) often lack in cover-
age when used in specific knowledge domains.
Automatically creating or extending taxonomies
for specific domains is then a very interesting
area of research (O?Sullivan et al, 1995; Magnini
and Speranza, 2001; Snow et al, 2006). Auto-
matic methods for learning taxonomies from cor-
pora often use distributional hypothesis (Harris,
1964) and exploit some induced lexical-syntactic
patterns (Hearst, 1992; Pantel and Pennacchiotti,
2006). In these models, within a very large set,
candidate word pairs are selected as new word
pairs in hyperonymy and added to an existing tax-
onomy. Candidate pairs are represented in some
feature space. Often, these feature spaces are
huge and, then, models may take into considera-
tion noisy features.
In machine learning, feature selection has been
often used to reduce the dimensions in huge fea-
ture spaces. This has many advantages, e.g., re-
ducing the computational cost and improving per-
formances by removing noisy features (Guyon and
Elisseeff, 2003).
In this paper, we propose a novel way to in-
clude unsupervised feature selection methods in
probabilistic taxonomy learning models. Given
the probabilistic taxonomy learning model intro-
duced by (Snow et al, 2006), we leverage on the
computation of logistic regression to exploit sin-
gular value decomposition (SVD) as unsupervised
feature selection. SVD is used to compute the
pseudo-inverse matrix needed in logistic regres-
sion.
To describe our idea, we firstly review how
SVD can be used as unsupervised feature selec-
tion (Sec. 2). In Section 3 we then describe the
probabilistic taxonomy learning model introduced
by (Snow et al, 2006). We will then shortly re-
view the logistic regression used to compute the
taxonomy learning model to describe where SVD
can be naturally used. We will describe our ex-
periments in Sec. 4. Finally, we will draw some
conclusions and describe our future work (Sec. 5).
2 Unsupervised feature selection with
Singular Value Decomposition
Singular value decomposition (SVD) is one of the
possible factorization of a rectangular matrix that
has been largely used in information retrieval for
reducing the dimension of the document vector
space (Deerwester et al, 1990).
The decomposition can be defined as follows.
Given a generic rectangular n ? m matrix A, its
singular value decomposition is:
A = U?V T
where U is a matrix n ? r, V T is a r ?m and ?
is a diagonal matrix r ? r. The two matrices U
and V are unitary, i.e., UTU = I and V TV = I .
The diagonal elements of the ? are the singular
values such as ?1 ? ?2 ? ... ? ?r > 0 where r is
the rank of the matrix A. For the decomposition,
SVD exploits the linear combination of rows and
columns of A.
A first trivial way of using SVD as unsupervised
feature reduction is the following. Given E as set
66
of training examples represented in a feature space
of n features, we can observe it as a matrix, i.e.
a sequence of examples E = (??e1 ...??em). With
SVD, the n ? m matrix E can be factorized as
E = U?V T . This factorization implies we can
focus the learning problem on a new space using
the transformation provided by the matrix U . This
new space is represented by the matrix:
E? = UTE = ?V T (1)
where each example is represented with r new fea-
tures. Each new feature is obtained as a linear
combination of the original features, i.e. each fea-
ture vector ??el can be seen as a new feature vector
??el
? = UT??el . When the target feature space is big
whereas the cardinality of the training set is small,
i.e., n >> m, the application of SVD results in a
reduction of the original feature space as the rank
r of the matrix E is r ? min(n,m).
A more interesting way of using SVD as unsu-
pervised feature selection model is to exploit its
approximated computations, i.e. :
A ? Ak = Um?k?k?kV
T
k?n
where k is smaller than the rank r. The compu-
tation algorithm (Golub and Kahan, 1965) is al-
lowed to stop at a given k different from the real
rank r. The property of the singular values, i.e.,
?1 ? ?2 ? ... ? ?r > 0, guarantees that the
first k are bigger than the discarded ones. There
is a direct relation between the informativeness of
the dimension and the value of the singular value.
High singular values correspond to dimensions of
the new space where examples have more vari-
ability whereas low singular values determine di-
mensions where examples have a smaller variabil-
ity (see (Liu, 2007)). These dimensions can not
be used as discriminative features in learning al-
gorithms. The possibility of computing the ap-
proximated version of the matrix gives a power-
ful method for feature selection and filtering as
we can decide in advance how many features or,
better, linear combination of original features we
want to use.
As feature selection model, SVD is unsuper-
vised in the sense that the feature selection is done
without taking into account the final classes of the
training examples. This is not always the case,
feature selection models such as those based on
Information Gain largely use the final classes of
training examples. SVD as feature selection is in-
dependent from the classification problem.
3 Probabilistic Taxonomy Learning and
SVD feature selection
Recently, Snow et al (2006) introduced a prob-
abilistic model for learning taxonomies form cor-
pora. This probabilistic formulation exploits the
two well known hypotheses: the distributional hy-
pothesis (Harris, 1964) and the exploitation of
the lexico-syntactic patterns as in (Robison, 1970;
Hearst, 1992). Yet, in this formulation, we can
positively and naturally introduce our use of SVD
as feature selection model.
In the rest of this section we will firstly intro-
duce the probabilistic model (Sec. 3.1) and, then,
we will describe how SVD is used as feature se-
lector in the logistic regression that estimates the
probabilities of the model. To describe this part we
need to go in depth into the definition of the logis-
tic regression (Sec. 3.2) and the way of estimating
the regression coefficients (Sec. 3.3). This will
open the possibility of describing how we exploit
SVD (Sec. 3.4)
3.1 Probabilistic model
In the probabilistic formulation (Snow et al,
2006), the task of learning taxonomies from a cor-
pus is seen as a probability maximization prob-
lem. The taxonomy is seen as a set T of asser-
tions R over pairs Ri,j . If Ri,j is in T , i is a con-
cept and j is one of its generalization (i.e., the di-
rect or the indirect generalization). For example,
Rdog,animal ? T describes that dog is an animal.
The main innovation of this probabilistic method
is the ability of taking into account in a single
probability the information coming from the cor-
pus and an existing taxonomy T .
The main probabilities are then: (1) the prior
probability P (Ri,j ? T ) of an assertion Ri,j to
belong to the taxonomy T and (2) the posterior
probability P (Ri,j ? T |??e i,j) of an assertion Ri,j
to belong to the taxonomy T given a set of evi-
dences ??e i,j derived from the corpus. Evidences
is a feature vector associated with a pair (i, j). For
examples, a feature may describe how many times
i and j are seen in patterns like ?i as j? or ?i is
a j?. These among many other features are in-
dicators of an is-a relation between i and j (see
(Hearst, 1992)).
Given a set of evidences E over all the relevant
word pairs, in (Snow et al, 2006), the probabilis-
tic taxonomy learning task is defined as the prob-
lem of finding the taxonomy T? that maximizes the
67
probability of having the evidences E, i.e.:
T? = arg max
T
P (E|T )
In (Snow et al, 2006), this maximization prob-
lem is solved with a local search. What is max-
imized at each step is the increase of the probabil-
ity P (E|T ) of the taxonomy when the taxonomy
changes from T to T ? = T ? N where N are the
relations added at each step. This increase of prob-
abilities is defined as multiplicative change ?(N)
as follows:
?(N) = P (E|T ?)/P (E|T ) (2)
The main innovation of the model in (Snow et al,
2006) is the possibility of adding at each step the
best relation N = {Ri,j} as well as N = I(Ri,j)
that is Ri,j with all the relations by the existing
taxonomy. We will then experiment with our fea-
ture selection methodology in the two different
models:
flat: at each iteration step, a single relation is
added, i.e. R?i,j = arg maxRi,j ?(Ri,j)
inductive: at each iteration step, a set of re-
lations is added, i.e. I(R?i,j) where R?i,j =
arg maxRi,j ?(I(Ri,j)).
The last important fact is that it is possible to
demonstrate that
?(Ei,j) = k ?
P (Ri,j ? T |??e i,j)
1? P (Ri,j ? T |??e i,j)
=
= k ? odds(Ri,j)
where k is a constant (see (Snow et al, 2006))
that will be neglected in the maximization process.
This last equation gives the possibility of using the
logistic regression as it is. In the next sections we
will see how SVD and the related feature selection
can be used to compute the odds.
3.2 Logistic Regression
Logistic Regression (Cox, 1958) is a particular
type of statistical model for relating responses Y
to linear combinations of predictor variables X . It
is a specific kind of Generalized Linear Model (see
(Nelder and Wedderburn, 1972)) where its func-
tion is the logit function and the independent vari-
able Y is a binary or dicothomic variable which
has a Bernoulli distribution. The dependent vari-
able Y takes value 0 or 1. The probability that
Y has value 1 is function of the regressors x =
(1, x1, ..., xk).
The probabilistic taxonomy learner model in-
troduced in the previous section falls in the cat-
egory of probabilistic models where the logistic
regression can be applied as Ri,j ? T is the bi-
nary dependent variable and ??e i,j is the vector of
its regressors. In the rest of the section we will see
how the odds, i.e., the multiplicative change, can
be computed.
We start from formally describing the Logistic
Regression Model. Given the two stochastic vari-
ables Y and X , we can define as p the probability
of Y to be 1 given that X=x, i.e.:
p = P (Y = 1|X = x)
The distribution of the variable Y is a Bernulli dis-
tribution, i.e.:
Y ? Bernoulli(p)
Given the definition of the logit(p) as:
logit(p) = ln
(
p
1? p
)
(3)
and given the fact that Y is a Bernoulli distribution,
the logistic regression foresees that the logit is a
linear combination of the values of the regressors,
i.e.,
logit(p) = ?0 + ?1x1 + ...+ ?kxk (4)
where ?0, ?1, ..., ?k are called regression coeffi-
cients of the variables x1, ..., xk respectively.
Given the regression coefficients, it is possible
to compute the probability of a given event where
we observe the regressors x to be Y = 1 or in our
case to belong to the taxonomy. This probability
can be computed as follows:
p(x) =
exp(?0 + ?1x1 + ...+ ?kxk)
1 + exp(?0 + ?1x1 + ...+ ?kxk)
It is obviously trivial to determine the
odds(Ri,j) related to the multiplicative change
of the probabilistic taxonomy model. The odds
is the ratio between the positive and the negative
event. It is defined as follows:
odds(Ri,j) =
P (Ri,j?T |
??e i,j)
1?P (Ri,j?T |
??e i,j)
(5)
Then, it is strictly related with the logit, i.e.:
odds(Ri,j) = exp(?0 +??e
T
i,j?) (6)
The relationship between the possible values of
the probability, odds and logit is show in the Table
1.
68
Probability Odds Logit
0 ? p < 0.5 [0, 1) (??, 0]
0.5 < p ? 1 [1,?) [0,?)
Table 1: Relationship between probability, odds
and logit
3.3 Estimating Regression Coefficients
The remaining problem is how to estimate the re-
gression coefficients. This estimation is done us-
ing the maximal likelihood estimation to prepare a
set of linear equations using the above logit defini-
tion and, then, solving a linear problem. This will
give us the possibility of introducing the necessity
of determining a pseudo-inverse matrix where we
will use the singular value decomposition and its
natural possibility of performing feature selection.
Once we have the regression coefficients, we have
the possibility of assigning estimating a probabil-
ity P (Ri,j ? T |??e i,j) given any configuration of
the values of the regressors??e i,j , i.e., the observed
values of the features. For sake of simplicity we
will hereafter refer to ??e i,j as ??e l.
Let assume we have a multiset O of observa-
tions extracted from Y ?E where Y ? {0, 1} and
we know that some of them are positive observa-
tions (i.e., Y = 1) and some of them are negative
observations (i.e., Y = 0).
For each pairs the relative configuration ??e l ?
E that appeared at least once in O, we can de-
termine using the maximal likelihood estimation
P (Y = 1|??e l). Then, from the equation of the
logit (Eq. 4), we have a linear equation system,
i.e.:
??????
logit(p) = Q? (7)
where Q is a matrix that includes a constant col-
umn of 1, necessary for the ?0 of the linear combi-
nation of the values of the regression. Moreover it
includes the transpose of the evidence matrix, i.e.
E = (??e 1...??e m). Therefore the matrix will be:
Q =
?
?
?
?
?
?
1 e11 e12 ? ? ? e1n
1 e21 e22 ? ? ? e2n
...
...
...
. . .
...
1 em1 em2 ? ? ? emn
?
?
?
?
?
?
The set of equations in Eq. 7 can be solved us-
ing multiple linear regression.
In their general form, the equations of multiple
linear regression may be written as (Caron et al,
1988):
y = X? + ?
where:
? y is a column vector n ? 1 that includes the
observed values of the dependent variables
Y1, ..., Yk;
? X is a matrix n ?m of the values of the re-
gressors that we have observed;
? ? is a column vector m? 1 of the regression
coefficients;
? ? is a column vector including the stochastic
components that have not been observed and
that will not be considered later.
In the case X is a rectangular and singular matrix,
the system y = X? has not a solution. Yet, it is
possible to use the principle of the Least Square
Estimation. This principle determines the solution
? that minimize the residual norm, i.e.:
?? = arg min ?X? ? y?2 (8)
This problem can be solved by the Moore-
Penrose pseudoinverse X+ (Penrose, 1955).
Then, the final equation to determine the ? is
?? = X+y
It is important to remark that if the inverse matrix
exist X+ = X?1 and that X+X and XX+ are
symmetric.
For our case, the following equation is valid:
?? = Q+
??????
logit(p)
3.4 Computing Pseudoinverse Matrix with
SVD Analysis
We finally reached the point where it is possible
to explain our idea that is naturally using singular
value decomposition (SVD) as feature selection in
a probabilistic taxonomy learner. In the previous
sections we described how the probabilities of the
taxonomy learner can be estimated using logistic
regressions and we concluded that a way to de-
termine the regression coefficients ? is computing
the Moore-Penrose pseudoinverse Q+. It is pos-
sible to compute the Moore-Penrose pseudoin-
verse using the SVD in the following way (Pen-
rose, 1955). Given an SVD decomposition of the
69
matrixQ = U?V T the pseudo-inverse matrix that
minimizes the Eq. 9 is:
Q+ = V ?+UT (9)
The diagonal matrix ?+ is a matrix r? r obtained
first transposing ? and then calculating the recip-
rocals of the singular value of ?. So the diagonal
elements of the ?+ are 1?1 ,
1
?2
, ..., , 1?r .
We have now our opportunity of using SVD as
natural feature selector as we can compute differ-
ent approximations of the pseudo-inverse matrix.
As we saw in Sec. 2, the algorithm for computing
the singular value decomposition can be stopped a
different dimensions. We called k the number of
dimensions. As we can obtain different SVD as
approximations of the original matrix (Eq. 2), we
can define different approximations of :
Q+ ? Q+k = Vn?k?
+
k?kU
T
k?m
In our experiments we will use different values
of k to explore the benefits of SVD as feature se-
lector.
4 Experimental Evaluation
In this section, we want to empirically explore
whether our use of SVD feature selection pos-
itively affects performances of the probabilistic
taxonomy learner. The best way of determining
how a taxonomy learner is performing is to see if it
can replicate an existing ?taxonomy?. We will ex-
periment with the attempt of replicating a portion
of WordNet (Miller, 1995). In the experiments, we
will address two issues: 1) determining to what
extent SVD feature selection affect performances
of the taxonomy learner; 2) determining if SVD
as unsupervised feature selection is better for the
task than some simpler model for taxonomy learn-
ing. We will explore the effects on both the flat
and the inductive probabilistic taxonomy learner.
The rest of the section is organized as follows.
In Sec. 4.1 we will describe the experimental set-
up in terms of: how we selected the portion of
WordNet, the description of the corpus used to ex-
tract evidences, a description of the feature space
we used, and, finally, the description of a baseline
models for taxonomy learning we have used. In
Sec. 4.2 we will present the results of the experi-
ments in term of performance.
4.1 Experimental Set-up
To completely define the experiments we need to
describe some issues: how we defined the taxon-
omy to replicate, which corpus we have used to
extract evidences for pairs of words, which feature
space we used, and, finally, the baseline model we
compared our feature selection model against.
As target taxonomy we selected a portion of
WordNet1 (Miller, 1995). Namely, we started
from the 44 concrete nouns listed in (McRae et
al., 2005) and divided in 3 classes: animal, arti-
fact, and vegetable. For sake of comprehension,
this set is described in Tab. 2. For each word w,
we selected the synset sw that is compliant with
the class it belongs to. We then obtained a set S of
synsets (see Tab. 2). We then expanded the set to
S? adding the siblings (i.e., the coordinate terms)
for each synset in S. The set S? contains 265 co-
ordinate terms plus the 44 original concrete nouns.
For each element in S we collected its hyperonym,
obtaining the setH . We then removed from the set
H the 4 topmosts: entity, unit, object, and whole.
The set H contains 77 hyperonyms. For the pur-
pose of the experiments we both derived from the
previous sets a taxonomy T and produced a set of
negative examples T . The two sets have been ob-
tained as follows. The taxonomy T is the portion
of WordNet implied by O = H ? S?, i.e., T con-
tains all the (s, h) ? O ? O that are in WordNet.
On the contrary, T contains all the (s, h) ? O?O
that are not in WordNet. We then have 5108 posi-
tive pairs in T and 52892 negative pairs in T .
We then split the set T ?T in two parts, training
and testing. As we want to see if it is possible to
attach the set S? to the right hyperonym, the split
has been done as follows. We randomly divided
the set S? in two parts Str and Sts, respectively,
of 70% and 30% of the original S?. We then se-
lected as training Ttr all the pairs in T containing
a synset in Str and as testing set Tts those pairs of
T containing a synset of Sts. For the probabilistic
model, Ttr is the initial taxonomy whereas Tts?T
is the unknown set.
As corpus we used the English Web as Corpus
(ukWaC) (Ferraresi et al, 2008). This is a web
extracted corpus of about 2700000 web pages con-
taining more than 2 billion words. The corpus con-
tains documents of different topics such as web,
computers, education, public sphere, etc.. It has
been largely demonstrated that the web documents
1We used the version 3.0
70
Concrete nouns Clas Sense Concrete nouns Clas Sense
1 banana Vegetable 1 23 boat Artifact 0
2 bottle Artifact 0 24 bowl Artifact 0
3 car Artifact 0 25 cat Animal 0
4 cherry Vegetable 2 26 chicken Animal 1
5 chisel Artifact 0 27 corn Vegetable 2
6 cow Animal 0 28 cup Artifact 0
7 dog Animal 0 29 duck Animal 0
8 eagle Animal 0 30 elephant Animal 0
9 hammer Artifact 1 31 helicopter Artifact 0
10 kettle Artifact 0 32 knife Artifact 0
11 lettuce Vegetable 2 33 lion Animal 0
12 motorcycle Artifact 0 34 mushroom Vegetable 4
13 onion Vegetable 2 35 owl Animal 0
14 peacock Animal 1 36 pear Vegetable 0
15 pen Artifact 0 37 pencil Artifact 0
16 penguin Animal 0 38 pig Animal 0
17 pineapple Vegetable 1 39 potato Vegetable 2
18 rocket Artifact 0 40 scissors Artifact 0
19 screwdriver Artifact 0 41 ship Artifact 0
20 snail Animal 0 42 spoon Artifact 0
21 swan Animal 0 43 telephone Artifact 1
22 truck Artifact 0 44 turtle Animal 1
Table 2: Concrete nouns, Classes and senses selected in WordNet
are good models for natural language (Lapata and
Keller, 2004).
As the focus of the paper is the analysis of the
effect of the SVD feature selection, we used as fea-
ture spaces both n-grams and bag-of-words. Out
of the T ? T , we selected only those pairs that
appeared at a distance of at most 3 tokens. Us-
ing these 3 tokens, we generated three spaces:
(1) 1-gram that contains monograms, (2) 2-gram
that contains monograms and bigrams, and (3) the
3-gram space that contains monograms, bigrams,
and trigrams. For the purpose of this experiment,
we used a reduced stop list as classical stop words
as punctuation, parenthesis, the verb to be are very
relevant in the context of features for learning a
taxonomy.
Finally, we want to describe our baseline model
for taxonomy learning. This model only contains
Heart?s patterns (Hearst, 1992) as features. The
feature value is the point-wise mutual information.
These features are in some sense the best features
for the task as these have been manually selected
after a process of corpus analysis. These baseline
features are included in our 3-gram model. We can
then compare our best models with this baseline
features in order to see if our SVD feature selec-
tion model outperforms manual feature selection.
4.2 Results
In the first set of experiments we want to focus on
the issue whether or not performances of the prob-
abilistic taxonomy learner is positively affected
by the proposed feature selection model based on
the singular value decomposition. We then deter-
mined the performance with respect to different
values of k. This latter represents the number of
surviving dimensions where the pseudo-inverse is
computed. Then, it represents the number of fea-
tures the model adopts. We performed this first set
of experiments in the 1-gram feature space. Punc-
tuation has been considered. Figure 1 plots the ac-
curacy of the probabilistic learner with respect to
the size of the feature set, i.e. the number k of sin-
gle values considered for computing the pseudo-
inverse matrix. To determine if the effect of the
feature selection is preserved during the iteration
of the local search algorithm, we report curves at
different sizes of the set of added pairs. Curves are
71
0.1
0.2
0.3
0.4
0.5
0.6
0 200 400 600 800 1000
accuracy
k: dimension of the reduced space
flat
20 added pairs
40 added pairs
60 added pairs
80 added pairs
100 added pairs
0.1
0.2
0.3
0.4
0.5
0.6
0 200 400 600 800 1000
accuracy
k: dimension of the reduced space
inductive
40 added pairs
80 added pairs
130 added pairs
Figure 1: Accuracy over different cuts of the feature space
0.1
0.2
0.3
0.4
0.5
0.6
0 100 200 300 400 500 600
accuracy
added pairs
baseline
1-gram
2-gram
3-gram
Figure 2: Comparison of different feature spaces
with k=400
reported for both the flat model and the inductive
model. The flat algorithm adds one pair at each
iteration. Then, we reported curves for each 20
added pairs. Each curve shows that accuracy does
not increase after a dimension of k=700. This size
of the space is necessary only for the first 20 added
pairs. Accuracy keeps increasing to k=700 and
then decreases. When we add more pairs, the opti-
mal size of the space is around k=200. For the in-
ductive model we report the accuracies for around
40, 80, 130 added pairs. Here, at each iteration,
more than one pair is added. The optimal dimen-
sion of the feature space seems to be around 500
as after that value performances decrease or stay
stable. SVD feature selection has then a positive
effect for both the flat and the inductive probabilis-
tic taxonomy learners. This has beneficial effects
both on the performances and on the computation
time.
In the second set of experiments we want to de-
termine whether or not SVD feature selection for
the probabilistic taxonomy learner behaves better
than a reduced set of known features. We then
fixed the dimension k to 400 and we compared the
baseline model with different probabilistic models
with different feature sets: 1-gram, 2-gram, and
3-gram. We can consider that the trigram model
before the cut on its dimensions contains feature
subsuming the baseline model. Figure 2 shows re-
sults. Curves report accuracy after n added pairs.
All the probabilistic models outperform the base-
line model. As what happened for the first series of
experiments (see Fig. 1) more informative spaces
such as 3-gram behaves better when the number of
72
added pairs is small. Performances of the three re-
duced pairs become similar after 100 added pairs.
These experiments show that SVD feature selec-
tion has a positive effect on performances as re-
sulting models are always better with respect to
the baseline.
5 Conclusions and Future Work
We presented a model to naturally introduce
SVD feature selection in a probabilistic taxonomy
learner. The method is effective as allows the de-
signing of better probabilistic taxonomy learners.
We still need to explore at least two issues. First,
we need to determine whether or not the posi-
tive effect of SVD feature selection is preserved
in more complex feature spaces such as syntactic
feature spaces as those used in (Snow et al, 2006).
Second, we need to compare the SVD feature se-
lection with other unsupervised feature selection
models to determine whether or not this is the best
method to use in the case of probabilistic taxon-
omy learning.
References
D. Caron, W. Hospital, and P. N. Corey. 1988.
Variance estimation of linear regression coefficients
in complex sampling situation. Sampling Error:
Methodology, Software and Application, pages 688?
694.
D. R. Cox. 1958. The regression analysis of binary
sequences. Journal of the Royal Statistical Society.
Series B (Methodological), 20(2):215?242.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. L, and Richard Harshman. 1990. In-
dexing by latent semantic analysis. Journal of the
American Society for Information Science, 41:391?
407.
A. Ferraresi, E. Zanchetta, M. Baroni, and S. Bernar-
dini. 2008. Introducing and evaluating ukwac, a
very large web-derived corpus of english. In In
Proceed-ings of the WAC4 Workshop at LREC 2008,
Marrakesh, Morocco.
G. Golub and W. Kahan. 1965. Calculating the singu-
lar values and pseudo-inverse of a matrix. Journal of
the Society for Industrial and Applied Mathematics,
Series B: Numerical Analysis, 2(2):205?224.
Isabelle Guyon and Andre? Elisseeff. 2003. An intro-
duction to variable and feature selection. Journal of
Machine Learning Research, 3:1157?1182, March.
Zellig Harris. 1964. Distributional structure. In Jer-
rold J. Katz and Jerry A. Fodor, editors, The Philos-
ophy of Linguistics, New York. Oxford University
Press.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 15th International Conference on Computational
Linguistics (CoLing-92), Nantes, France.
Mirella Lapata and Frank Keller. 2004. The web as
a baseline: Evaluating the performance of unsuper-
vised web-based models for a range of nlp tasks.
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics, Boston,
MA.
Bing Liu. 2007. Web Data Mining: Exploring Hy-
perlinks, Contents, and Usage Data. Data-Centric
Systems and Applications. Springer.
Bernardo Magnini and Manuela Speranza. 2001. In-
tegrating generic and specialized wordnets. In In
Proceedings of the Euroconference RANLP 2001,
Tzigov Chark, Bulgaria.
K. McRae, G.S. Cree, M.S. Seidenberg, and C. McNor-
gan. 2005. Semantic feature production norms for a
large set of living and nonliving things. pages 547?
559, Behavioral Research Methods, Instruments,
and Computers.
George A. Miller. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39?41, November.
J. A. Nelder and R. W. M. Wedderburn. 1972. Gener-
alized linear models. Journal of the Royal Statistical
Society. Series A (General), 135(3):370?384.
Donie O?Sullivan, A. McElligott, and Richard F. E.
Sutcliffe. 1995. Augmenting the princeton wordnet
with a domain specific ontology. In Proceedings of
the Workshop on Basic Issues in Knowledge Sharing
at the 14th International Joint Conference on Artifi-
cial Intelligence. Montreal, Canada.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
113?120, Sydney, Australia, July. Association for
Computational Linguistics.
R. Penrose. 1955. A generalized inverse for matrices.
In Proc. Cambridge Philosophical Society.
Harold R. Robison. 1970. Computer-detectable se-
mantic structures. Information Storage and Re-
trieval, 6(3):273?288.
Rion Snow, Daniel Jurafsky, and A. Y. Ng. 2006. Se-
mantic taxonomy induction from heterogenous evi-
dence. In In ACL, pages 801?808.
73
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1263?1271,
Beijing, August 2010
Estimating Linear Models for Compositional Distributional Semantics
Fabio Massimo Zanzotto1
(1) Department of Computer Science
University of Rome ?Tor Vergata?
zanzotto@info.uniroma2.it
Ioannis Korkontzelos
Department of Computer Science
University of York
johnkork@cs.york.ac.uk
Francesca Fallucchi1,2
(2) Universita` Telematica
?G. Marconi?
f.fallucchi@unimarconi.it
Suresh Manandhar
Department of Computer Science
University of York
suresh@cs.york.ac.uk
Abstract
In distributional semantics studies, there
is a growing attention in compositionally
determining the distributional meaning of
word sequences. Yet, compositional dis-
tributional models depend on a large set
of parameters that have not been explored.
In this paper we propose a novel approach
to estimate parameters for a class of com-
positional distributional models: the addi-
tive models. Our approach leverages on
two main ideas. Firstly, a novel idea for
extracting compositional distributional se-
mantics examples. Secondly, an estima-
tion method based on regression models
for multiple dependent variables. Experi-
ments demonstrate that our approach out-
performs existing methods for determin-
ing a good model for compositional dis-
tributional semantics.
1 Introduction
Lexical distributional semantics has been largely
used to model word meaning in many fields as
computational linguistics (McCarthy and Carroll,
2003; Manning et al, 2008), linguistics (Harris,
1964), corpus linguistics (Firth, 1957), and cogni-
tive research (Miller and Charles, 1991). The fun-
damental hypothesis is the distributional hypoth-
esis (DH): ?similar words share similar contexts?
(Harris, 1964). Recently, this hypothesis has been
operationally defined in many ways in the fields of
physicology, computational linguistics, and infor-
mation retrieval (Li et al, 2000; Pado and Lapata,
2007; Deerwester et al, 1990).
Given the successful application to words, dis-
tributional semantics has been extended to word
sequences. This has happened in two ways: (1)
via the reformulation of DH for specific word se-
quences (Lin and Pantel, 2001); and (2) via the
definition of compositional distributional seman-
tics (CDS) models (Mitchell and Lapata, 2008;
Jones and Mewhort, 2007). These are two differ-
ent ways of addressing the problem.
Lin and Pantel (2001) propose the pattern dis-
tributional hypothesis that extends the distribu-
tional hypothesis for specific patterns, i.e. word
sequences representing partial verb phrases. Dis-
tributional meaning for these patterns is derived
directly by looking to their occurrences in a cor-
pus. Due to data sparsity, patterns of different
length appear with very different frequencies in
the corpus, affecting their statistics detrimentally.
On the other hand, compositional distributional
semantics (CDS) propose to obtain distributional
meaning for sequences by composing the vectors
of the words in the sequences (Mitchell and Lap-
ata, 2008; Jones and Mewhort, 2007). This ap-
proach is fairly interesting as the distributional
meaning of sequences of different length is ob-
tained by composing distributional vectors of sin-
gle words. Yet, many of these approaches have a
large number of parameters that cannot be easily
estimated.
In this paper we propose a novel approach to es-
1263
timate parameters for additive compositional dis-
tributional semantics models. Our approach lever-
ages on two main ideas. Firstly, a novel way for
extracting compositional distributional semantics
examples and counter-examples. Secondly, an es-
timation model that exploits these examples and
determines an equation system that represents a
regression problem with multiple dependent vari-
ables. We propose a method to estimate a solu-
tion of this equation system based on the Moore-
Penrose pseudo-inverse matrices (Penrose, 1955).
The rest of the paper is organised as follows:
Firstly, we shortly review existing compositional
distributional semantics (CDS) models (Sec. 2).
Then we describe our model for estimating CDS
models parameters (Sec. 3). In succession, we
introduce a way to extract compositional dis-
tributional semantics examples from dictionaries
(Sec. 4). Then, we discuss the experimental set up
and the results of our linear CDS model with es-
timated parameters with respect to existing CDS
models (Sec. 5).
2 Models for compositional
distributional semantics (CDS)
A CDS model is a function  that computes the
distributional vector of a sequence of words s by
combining the distributional vectors of its com-
ponent words w1 . . .wn. Let(s) be the distribu-
tional vector describing s and ~wi the distributional
vectors describing its component word wi. Then,
the CDS model can be written as:
(s) = (w1 . . .wn) = ~w1  . . . ~wn (1)
This generic model has been fairly studied and
many different functions have been proposed and
tested.
Mitchell and Lapata (2008) propose the fol-
lowing general CDS model for 2-word sequences
s = xy:
(s) = (xy) = f(~x, ~y,R,K) (2)
where ~x and ~y are respectively the distributional
vectors of x and y, R is the particular syntactic
and/or semantic relation connecting x and y, and,
K represents the amount of background knowl-
edge that the vector composition process takes
vector dimensions
betwe
en
gap proce
ss
socialtwo
contact < 11, 0, 3, 0, 11 >
x: close < 27, 3, 2, 5, 24 >
y: interaction < 23, 0, 3, 8, 4 >
Table 1: Example of distributional
frequency vectors for the triple t =
( ~contact, ~close, ~interaction)
into account. Two specialisations of the gen-
eral CDS model are proposed: the basic additive
model and the basic multiplicative model.
The basic additive model (BAM) is written as:
(s) = ?~x+ ?~y (3)
where ? and ? are two scalar parameters. The
simplistic parametrisation is ? = ? = 1. For
example, given the vectors ~x and ~y of Table 1,
BAM (s) =< 50, 3, 5, 13, 28 >.
The basic multiplicative model (BMM) is writ-
ten as:
si = xiyi (4)
where si, xi, and yi are the i-th dimensions of
the vectors (s), ~x, and ~y, respectively. For
the example of Table 1, BMM (s) =< 621, 0,
6, 40, 96 >.
Erk and Pado? (2008) look at the problem in a
different way. Let the general distributional mean-
ing of the word w be ~w. Their model computes a
different vector ~ws that represents the specific dis-
tributional meaning of w with respect to s, i.e.:
~ws = (w, s) (5)
In general, this operator gives different vectors for
each word wi in the sequence s, i.e. (wi, s) 6=
(wj , s) if i 6= j. It also gives different vectors
for a word wi appearing in different sequences sk
and sl, i.e. (wi, sk) 6= (wi, sl) if k 6= l.
The model of Erk and Pado? (2008) was de-
signed to disambiguate the distributional mean-
ing of a word w in the context of the sequence
s. However, substituting the word w with the se-
mantic head h of s, allows to compute the distri-
butional meaning of sequence s as shaped by the
1264
word that is governing the sequence (c.f. Pollard
and Sag (1994)). For example, the distributional
meaning of the word sequence eats mice is gov-
erned by the verb eats. Following this model, the
distributional vector (s) can be written as:
(s) ? (h, s) (6)
The function (h, s) explicitly uses the re-
lation R and the knowledge K of the general
equation 2, being based on the notion of selec-
tional preferences. We exploit the model for se-
quences of two words s=xy where the two words
are related with an oriented syntactic relation r
(e.g. r=adj modifier). For making the syntac-
tic relation explicit, we indicate the sequence as:
s = x r?? y.
Given a word w, the model has to keep track
of its selectional preferences. Consequently, each
word w is represented with a triple:
(~w,Rw, R?1w ) (7)
where ~w is the distributional vector of the word w,
Rw is the set of the vectors representing the direct
selectional preferences of the word w, and R?1w is
the set of the vectors representing the indirect se-
lectional preferences of the word w. Given a set of
syntactic relationsR, the set Rw and R?1w contain
respectively a selectional preference vectorRw(r)
and Rw(r)?1 for each r ? R. Selectional prefer-
ences are computed as in Erk (2007). If x is the
semantic head of sequence s, then the model can
be written as:
(s) = (x, x r?? y) = ~xRy(r) (8)
Otherwise, if y is the semantic head:
(s) = (y, x r?? y) = ~y R?1x (r) (9)
 is in both cases realised using BAM or BMM.
We will call these models: basic additive model
with selectional preferences (BAM-SP) and basic
multiplicative model with selectional preferences
(BMM-SP).
Both Mitchell and Lapata (2008) and Erk and
Pado? (2008) experimented with few empirically
estimated parameters. Thus, the general additive
CDS model has not been adequately explored.
3 Estimating Additive Compositional
Semantics Models from Data
The generic additive model sums the vectors ~x
and ~y in a new vector ~z:
(s) = ~z = A~x+B~y (10)
where A and B are two square matrices captur-
ing the relation R and the background knowledge
K of equation 2. Writing matrices A and B by
hand is impossible because of their large size. Es-
timating these matrices is neither a simple classi-
fication learning problem nor a simple regression
problem. It is a regression problem with multiple
dependent variables. In this section, we propose
our model to solve this regression problem using
a set of training examples E.
The set of training examples E contains triples
of vectors (~z, ~x, ~y). ~x and ~y are the two distribu-
tional vectors of the words x and y. ~z is the ex-
pected distributional vector of the composition of
~x and ~y. Note that for an ideal perfectly perform-
ing CDS model we can write ~z = (xy). How-
ever, in general the expected vector ~z is not guar-
anteed to be equal to the composed one (xy).
Figure 1 reports an example of these triples, i.e.,
t = ( ~contact, ~close, ~interaction), with the re-
lated distributional vectors. The construction of
E is discussed in section 4.
In the rest of the section, we describe how the
regression problem with multiple dependent vari-
ables can be solved with a linear equation system
and we give a possible solution of this equation
system. In the experimental section, we refer to
our model as the estimated additive model (EAM).
3.1 Setting the linear equation system
The matrices A and B of equation 10 can be
joined in a single matrix:
~z =
(
A B
)(~x
~y
)
(11)
For the triple t of table 1, equation 11 is:
~contact =
(
A B
)
(
~close
~interaction
)
(12)
1265
and it can be rewritten as:
?
?????
11
0
3
0
11
?
?????
=
(
A5?5 B5?5
)
?
??????????
27
3
2
5
24
23
0
3
8
4
?
??????????
(13)
Focusing on matrix (AB), we can transpose the
matrices as follows:
~zT =
((
A B
)(~x
~y
))T
=
(
~xT ~yT
)(AT
BT
)
(14)
Matrix (~xT ~yT ) is known and matrix
(
AT
BT
)
is
to be estimated.
Equation 14 is the prototype of our final equa-
tion system. The larger the matrix (AB) to be
estimated, the more equations like 14 are needed.
Given set E that contains n triples (~z, ~x, ~y), we
can write the following system of equations:
?
????
~zT1
~zT2...
~zTn
?
???? =
?
????
(
~xT1 ~yT1
)
(
~xT2 ~yT2
)
...(
~xTn ~yTn
)
?
????
(
AT
BT
)
(15)
The vectors derived from the triples can be seen as
two matrices of n rows, Z and (XY ) related to ~zTi
and (~xTi ~yTi
), respectively. The overall equation
system is then the following:
Z =
(
X Y
)(AT
BT
)
(16)
This equation system represents the constraints
that matrices A and B have to satisfy in order to
be a possible linear CDS model that can at least
describe seen examples. We will hereafter call
? =
(
A B
) and Q = (X Y ). The system
in equation 16 can be simplified as:
Z = Q?T (17)
As Q is a rectangular and singular matrix, it is
not invertible and the system in equation 16 has
no solutions. It is possible to use the principle
of Least Square Estimation for computing an ap-
proximation solution. The idea is to compute the
solution ?? that minimises the residual norm, i.e.:
??T = arg min
?T
?Q?T ? Z?2 (18)
One solution for this problem is the Moore-
Penrose pseudoinverse Q+ (Penrose, 1955) that
gives the following final equation:
??T = Q+Z (19)
In the next section, we discuss how the Moore-
Penrose pseudoinverse is obtained using singular
value decomposition (SVD).
3.2 Computing the pseudo-inverse matrix
The pseudo-inverse matrix can provide an approx-
imated solution even if the equation system has no
solutions. We here compute the Moore-Penrose
pseudoinverse using singular value decomposi-
tion (SVD) that is widely used in computational
linguistics and information retrieval for reducing
spaces (Deerwester et al, 1990).
Moore-Penrose pseudoinverse (Penrose, 1955)
is computed in the following way. Let the original
matrix Q have n rows and m columns and be of
rank r. The SVD decomposition of the original
matrix Q is Q = U?V T where ? is a square di-
agonal matrix of dimension r. Then, the pseudo-
inverse matrix that minimises the equation 18 is:
Q+ = V ?+UT (20)
where the diagonal matrix ?+ is the r ? r trans-
posed matrix of ? having as diagonal elements the
reciprocals of the singular values 1?1 , 1?2 , ..., 1?r of
?.
Using SVD to compute the pseudo-inverse ma-
trix allows for different approximations (Fallucchi
and Zanzotto, 2009). The algorithm for comput-
ing the singular value decomposition is iterative
(Golub and Kahan, 1965). Firstly derived dimen-
sions have higher singular value. Then, dimension
k is more informative than dimension k? > k. We
can consider different values for k to obtain differ-
ent SVD for the approximations Q+k of the origi-nal matrix Q+ in equation 20), i.e.:
Q+k = Vn?k?+k?kUTk?m (21)
1266
where Q+k is a matrix n by m obtained consider-ing the first k singular values.
4 Building positive and negative
examples
As explained in the previous section, estimating
CDS models, needs a set of triples E, similar to
triple t of table 1. This set E should contain pos-
itive examples in the form of triples (~zi, ~xi, ~yi).
Examples are positive in the sense that ~zi =
(xy) for an ideal CDS. There are no available
sets to contain such triples, with the exception of
the set used in Mitchell and Lapata (2008) which
is designed only for testing purposes. It contains
similar and dissimilar pairs of sequences (s1,s2)
where each sequence is a verb-noun pair (vi,ni).
From the positive part of this set, we can only de-
rive quadruples where (v1n1) ? (v2n2) but
we cannot derive the ideal resulting vector of the
composition (vini). Sets used to test multi-
word expression (MWE) detection models (e.g.,
(Schone and Jurafsky, 2001; Nicholson and Bald-
win, 2008; Kim and Baldwin, 2008; Cook et
al., 2008; Villavicencio, 2003; Korkontzelos and
Manandhar, 2009)) are again not useful as con-
taining only valid MWE that cannot be used to
determine the set of training triples needed here.
As a result, we need a novel idea to build sets
of triples to train CDS models. We can leverage
on knowledge stored in dictionaries. In the rest of
the section, we describe how we build the positive
example set E and a control negative example set
NE. Elements of the two sets are pairs (t,s) where
t is a target word s is a sequence of words. t is the
word that represent the distributional meaning of
s in the case ofE. Contrarily, t is totally unrelated
to the distributional meaning of s inNE. The sets
E and NE can be used both for training and for
testing. In the testing phase, we can use these sets
to determine whether a CDS model is good or not
and to compare different CDS models.
4.1 Building Positive Examples using
Dictionaries
Dictionaries as natural repositories of equivalent
expressions can be used to extract positive exam-
ples for training and testing CDS models. The
basic idea is the following: dictionary entries are
declarations of equivalence. Words or, occasion-
ally, multi-word expressions t are declared to be
semantically similar to their definition sequences
s. This happens at least for some sense of the
defined words. We can then observe that t ? s.
For example, we report some sample definitions
of contact and high life:
target word (t) definition sequence (s)
contact close interaction
high life excessive spending
In the first case, a word, i.e. contact, is semanti-
cally similar to a two-word expression, i.e. close
interaction. In the second case, two two-word ex-
pressions are semantically similar.
Then, the pairs (t, s) can be used to model
positive cases of compositional distributional se-
mantics as we know that the word sequence s
is compositional and it describes the meaning of
the word t. The distributional meaning ~t of t is
the expected distributional meaning of s. Conse-
quently, the vector ~t is what the CDS model (s)
should compositionally obtain from the vectors of
the components ~s1 . . . ~sm of s. This way of ex-
tracting similar expressions has some interesting
properties:
First property Defined words t are generally
single words. Thus, we can extract stable and
meaningful distributional vectors for these words
and then compare them to the distributional vec-
tors composed by CDS model. This is an impor-
tant property as we cannot compare directly the
distributional vector ~s of a word sequence s and
the vector (s) obtained by composing its com-
ponents. As the word sequence s grows in length,
the reliability of the vector ~s decreases since the
sequence s becomes rarer.
Second property Definitions s have a large va-
riety of different syntactic structures ranging from
simple structures as Adjective-Noun to more com-
plex ones. This gives the possibility to train and
test CDS models that take into account syntax.
Table 2 represents the distribution of the more
frequent syntactic structures in the definitions of
WordNet1 (Miller, 1995).
1Definitions were extracted from WordNet 3.0 and were
parsed with the Charniak parser (Charniak, 2000)
1267
Freq. Structure
2635 (FRAG (PP (IN) (NP (DT) (JJ) (NN))))
833 (NP (DT) (JJ) (NN))
811 (NP (NNS))
645 (NP (NNP))
623 (S (VP (VB) (ADVP (RB))))
610 (NP (JJ) (NN))
595 (NP (NP (DT) (NN)) (PP (IN) (NP (NN))))
478 (NP (NP (DT) (NN)) (PP (IN) (NP (NNP))))
451 (FRAG (PP (IN) (NP (NN))))
419 (FRAG (RB) (ADJP (JJ)))
375 (S (VP (VB) (PP (IN) (NP (DT) (NN)))))
363 (S (VP (VB) (PP (IN) (NP (NN)))))
342 (NP (NP (DT) (NN)) (PP (IN) (NP (DT) (NN))))
341 (NP (DT) (JJ) (JJ) (NN))
330 (ADJP (RB) (JJ))
307 (NP (JJ) (NNS))
244 (NP (DT) (NN) (NN))
241 (S (NP (NN)) (NP (NP (NNS)) (PP (IN) (NP (DT) (NNP)))))
239 (NP (NP (DT) (JJ) (NN)) (PP (IN) (NP (DT) (NN))))
Table 2: Top 20 syntactic structures of WordNet
definitions
4.2 Extracting Negative Examples from
Word Etymology
In order to devise complete training and testing
sets for CDS models, we need to find a sensible
way to extract negative examples. An option is to
randomly generate totally unrelated triples for the
negative examples set, NE. In this case, due to
data sparseness NE would mostly contain triples
(~z, ~x, ~y) where it is expected that ~z 6= (xy). Yet,
these can be too generic and too loosely related to
be interesting cases.
Instead we attempt to extract sets of negative
pairs (t,s) comparable with the one used for build-
ing the training set E. The target word t should
be a single word and s should be a sequence of
words. The latter should be a sequence of words
related by construction to t but the meaning of t
and s should be unrelated.
The idea is the following: many words are et-
ymologically derived from very old or ancient
words. These words represent a collocation which
is in general not related to the meaning of the
target word. For example, the word philosophy
derives from two Greek words philos (beloved)
and sophia (wisdom). However, the use of the
word philosophy in not related to the collocation
beloved wisdom. This word has lost its origi-
nal compositional meaning. The following table
shows some more etymologically complex words
along with the compositionally unrelated colloca-
tions:
target word compositionally unrelated seq.
municipal receive duty
octopus eight foot
As the examples suggest, we are able to build a
set NE with features similar to the features of
N . In particular, each target word is paired with
a related word sequence derived from its etymol-
ogy. These etymologically complex words are un-
related to the corresponding compositional collo-
cations. To derive a set NE with the above char-
acteristics we can use dictionaries containing ety-
mological information as Wiktionary2.
5 Experimental evaluation
In the previous sections, we presented the esti-
mated additive model (EAM): our approach to es-
timate the parameters of a generic additive model
for CDS. In this section, we experiment with this
model to determine whether it performs better
than existing models: the basic additive model
(BAM), the basic multiplicative model (BMM),
the basic additive model with selectional pref-
erences (BAM-SP), and the basic multiplicative
model with selectional preferences (BMM-SP)
(c.f. Sec. 2). In succession, we explore whether
our estimated additive model (EAM) is better than
any possible BAM obtained with parameter ad-
justment. In the rest of the section, we firstly give
the experimental setup and then we discuss the ex-
periments and the results.
5.1 Experimental setup
Our experiments aim to compare compositional
distributional semantic (CDS) models  with re-
spect to their ability of detecting statistically sig-
nificant difference between sets E and NE. In
particular, the average similarity sim(~z,(xy))
for (~z, ~x, ~y) ? E should be significantly different
from sim(~z,(xy)) for (~z, ~x, ~y) ? NE. In this
section, we describe the chosen similarity mea-
sure sim, statistical significance testing and con-
struction details for the training and testing set.
Cosine similarity was used to compare the con-
text vector ~z representing the target word z with
the composed vector (xy) representing the con-
text vector of sequence x y. Cosine similarity be-
2http://www.wiktionary.org
1268
tween two vectors ~x and ~y of the same dimension
is defined as:
sim(~x, ~y) = ~x ? ~y?~x? ?~y? (22)
where ? is the dot product and ?~a? is the magni-
tude of vector ~a computed the Euclidean norm.
To evaluate whether a CDS model distinguishes
positive examples E from negative examples
NE, we test if the distribution of similarities
sim(~z,(xy)) for (~z, ~x, ~y) ? E is statistically
different from the distribution of the same simi-
larities for (~z, ~x, ~y) ? NE. For this purpose, we
used Student?s t-test for two independent samples
of different sizes. t-test assumes that the two dis-
tributions are Gaussian and determines the prob-
ability that they are similar, i.e., derive from the
same underlying distribution. Low probabilities
indicate that the distributions are highly dissimilar
and that the corresponding CDS model performs
well, as it detects statistically different similarities
for the positive set E and the negative set NE.
Based on the null hypothesis that the means of
the two samples are equal, ?1 = ?2, Student?s t-
test takes into account the sizes N , means M and
variances s2 of the two samples to compute the
following value:
t = (M1 ?M2) ?1
?
2(s21 + s22)
df ?Nh
(23)
where df = N1 + N2 ? 2 stands for the degrees
of freedom and Nh = 2(N?11 + N?12 )?1 is the
harmonic mean of the sample sizes. Given the
statistic t and the degrees of freedom df , we can
compute the corresponding p-value, i.e., the prob-
ability that the two samples derive from the same
distribution. The null hypothesis can be rejected if
the p-value is below the chosen threshold of statis-
tical significance (usually 0.1, 0.05 or 0.01), oth-
erwise it is accepted. In our case, rejecting the
null hypothesis means that the similarity values of
instances of E are significantly different from in-
stances of NE, and that the corresponding CDS
model perform well. p-value can be used as a per-
formance ranking function for CDS models.
We constructed two sets of instances: (a) a
set containing Adjective-Noun or Noun-Noun se-
NN set VN set
BAM 0.05690 0.50753
BMM 0.20262 0.37523
BAM-SP 0.42574 0.01710
BMM-SP <1.00E-10 0.23552
EAM (k=20) 0.00431 0.00453
Table 3: Probability of confusing E and NE with
different CDS models
quences (NN set); and (b) a set containing Verb-
Noun sequences (VN set). Capturing different
syntactic relations, these two sets can support that
our results are independent from the syntactic re-
lation between the words of each sequence. For
each set, we used WordNet for extracting positive
examples E and Wiktionary for extracting nega-
tive examples NE as described in Section 4. We
obtained the following sets: (a) NN consists of
1065 word-sequence pairs from WordNet defini-
tions and 377 pairs extracted from Wiktionary;
and (b) VN consists of 161 word-sequence pairs
from WordNet definitions and 111 pairs extracted
from Wiktionary. We have then divided these two
sets in two parts of 50% each, for training and
testing. Instances of the training part of E have
been used to estimate matricesA andB for model
EAM , while the testing parts have been used for
testing all models. Frequency vectors for all sin-
gle words occurring in the above pairs were con-
structed from the British National Corpus using
sentences as contextual windows and words as
features. The resulting space has 689191 features.
5.2 Results and Analysis
The first set of experiments compares EAM with
other existing CDS models: BAM, BMM, BAM-
SP, and BMM-SP. Results are shown in Table 3.
The table reports the p-value, i.e., the probability
of confusing the positive set E and the negative
set NE for all models. Lower probabilities char-
acterise better models. Probabilities below 0.05
indicate that the model detects a statistically sig-
nificant difference between setsE andNE. EAM
has been computed with k = 20 different dimen-
sions for the pseudo-inverse matrix. The two basic
additive models (BAM and BAM-SP) have been
computed for ? = ? = 1.
1269
NN set V N set
Figure 1: p-values of BAM with different values for parameter ? (where ? = 1 ? ?) and of EAM for
different approximations of the SVD pseudo-inverse matrix (k)
The first observation is that EAM models sig-
nificantly separate positive from negative exam-
ples for both sets. This is not the case for any
of the other models. Only, the selectional prefer-
ences based models in two cases have this prop-
erty, but this cannot be generalised: BAM-SP on
the VN set and BMM-SP on the NN set. In gen-
eral, these models do not offer the possibility of
separating positive from negative examples.
In the second set of experiments, we attempt to
investigate whether simple parameter adjustment
of BAM can perform better than EAM. Results are
shown in figure 1. Plots show the basic additive
model (BAM) with different values for parameter
? (where ? = 1??) and EAM computed for dif-
ferent approximations of the SVD pseudo-inverse
matrix (i.e., with different k). The x-axis of the
plots represents parameter ? and the y-axis repre-
sents the probability of confusing the positive set
E and the negative setNE. The representation fo-
cuses on the performance ofBAM with respect to
different ? values. The performance of EAM for
different k values is represented with horizontal
lines. Probabilities of different models are directly
comparable. Line SS represents the threshold of
statistical significance; the value below which the
detected difference between the E and NE sets
becomes statistically significant.
Experimental results show some interesting
facts: While BAM for ? > 0 perform better than
EAM computed with k = 1 in the NN set, they
do not perform better in the VN set. EAM with
k = 1 has 1 degree of freedom corresponding to
1 parameter, the same as BAM. The parameter of
EAM is tuned on the training set, in contrast to
?, the parameter of BAM. Increasing the number
of considered dimensions, k of EAM, estimated
models outperform BAM for all values of param-
eter ?. Moreover, EAM detect a statistically sig-
nificant difference between theE and theNE sets
for k ? 10 and k = 20 for the NN set and the
VN set set, respectively. Simple parametrisation
of a BAM does not outperform the proposed esti-
mated additive model.
6 Conclusions
In this paper, we presented an innovative method
to estimate linear compositional distributional se-
mantics models. The core of our approach con-
sists on two parts: (1) providing a method to es-
timate the regression problem with multiple de-
pendent variables and (2) providing a training set
derived from dictionary definitions. Experiments
showed that our model is highly competitive with
respect to state-of-the-art models for composi-
tional distributional semantics.
References
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In proceedings of the 1st NAACL,
pages 132?139, Seattle, Washington.
Cook, Paul, Afsaneh Fazly, and Suzanne Stevenson.
2008. The VNC-Tokens Dataset. In proceedings
of the LREC Workshop: Towards a Shared Task for
Multiword Expressions (MWE 2008), Marrakech,
Morocco.
1270
Deerwester, Scott C., Susan T. Dumais, Thomas K.
Landauer, George W. Furnas, and Richard A. Harsh-
man. 1990. Indexing by latent semantic analysis.
Journal of the American Society of Information Sci-
ence, 41(6):391?407.
Erk, Katrin and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 897?
906. Association for Computational Linguistics.
Erk, Katrin. 2007. A simple, similarity-based model
for selectional preferences. In proceedings of ACL.
Association for Computer Linguistics.
Fallucchi, Francesca and Fabio Massimo Zanzotto.
2009. SVD feature selection for probabilistic tax-
onomy learning. In proceedings of the Workshop on
Geometrical Models of Natural Language Seman-
tics, pages 66?73. Association for Computational
Linguistics, Athens, Greece.
Firth, John R. 1957. Papers in Linguistics. Oxford
University Press, London.
Golub, Gene and William Kahan. 1965. Calculat-
ing the singular values and pseudo-inverse of a ma-
trix. Journal of the Society for Industrial and Ap-
plied Mathematics, Series B: Numerical Analysis,
2(2):205?224.
Harris, Zellig. 1964. Distributional structure. In Katz,
Jerrold J. and Jerry A. Fodor, editors, The Philos-
ophy of Linguistics, New York. Oxford University
Press.
Jones, Michael N. and Douglas J. K. Mewhort. 2007.
Representing word meaning and order information
in a composite holographic lexicon. Psychological
Review, 114:1?37.
Kim, Su N. and Timothy Baldwin. 2008. Standard-
ised evaluation of english noun compound inter-
pretation. In proceedings of the LREC Workshop:
Towards a Shared Task for Multiword Expressions
(MWE 2008), pages 39?42, Marrakech, Morocco.
Korkontzelos, Ioannis and Suresh Manandhar. 2009.
Detecting compositionality in multi-word expres-
sions. In proceedings of ACL-IJCNLP 2009, Sin-
gapore.
Li, Ping, Curt Burgess, and Kevin Lund. 2000. The
acquisition of word meaning through global lexical
co-occurrences. In proceedings of the 31st Child
Language Research Forum.
Lin, Dekang and Patrick Pantel. 2001. DIRT-
discovery of inference rules from text. In Proceed-
ings of the ACM Conference on Knowledge Discov-
ery and Data Mining (KDD-01). San Francisco, CA.
Manning, Christopher D., Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to Information
Retrieval. Cambridge University Press, Cambridge,
UK.
McCarthy, Diana and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Compu-
tational Linguistics, 29(4):639?654.
Miller, George A. and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, VI:1?28.
Miller, George A. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39?41.
Mitchell, Jeff and Mirella Lapata. 2008. Vector-based
models of semantic composition. In proceedings
of ACL-08: HLT, pages 236?244, Columbus, Ohio.
Association for Computational Linguistics.
Nicholson, Jeremy and Timothy Baldwin. 2008. Inter-
preting compound nominalisations. In proceedings
of the LREC Workshop: Towards a Shared Task for
Multiword Expressions (MWE 2008), pages 43?45,
Marrakech, Morocco.
Pado, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?
199.
Penrose, Roger. 1955. A generalized inverse for ma-
trices. In Proceedings of Cambridge Philosophical
Society.
Pollard, Carl J. and Ivan A. Sag. 1994. Head-driven
Phrase Structured Grammar. Chicago CSLI, Stan-
ford.
Schone, Patrick and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictio-
nary headwords a solved problem? In Lee, Lil-
lian and Donna Harman, editors, proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 100?108.
Villavicencio, Aline. 2003. Verb-particle construc-
tions and lexical resources. In proceedings of
the ACL 2003 workshop on Multiword expressions,
pages 57?64, Morristown, NJ, USA. Association for
Computational Linguistics.
1271
