Proceedings of the ACL Student Research Workshop, pages 52?58,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Automated Collocation Suggestion for Japanese Second Language 
Learners 
Lis W. Kanashiro Pereira Erlyn Manguilimotan Yuji Matsumoto 
Nara Institute of Science and Technology 
Department of Information Science 
{lis-k, erlyn-m, matsu}@is.naist.jp 
 
Abstract 
This study addresses issues of Japanese lan-
guage learning concerning word combinations 
(collocations). Japanese learners may be able 
to construct grammatically correct sentences, 
however, these may sound ?unnatural?. In this 
work, we analyze correct word combinations 
using different collocation measures and 
word similarity methods. While other methods 
use well-formed text, our approach makes use 
of a large Japanese language learner corpus for 
generating collocation candidates, in order to 
build a system that is more sensitive to con-
structions that are difficult for learners. Our 
results show that we get better results com-
pared to other methods that use only well-
formed text. 
1 Introduction 
Automated grammatical error correction is 
emerging as an interesting topic of natural lan-
guage processing (NLP). However, previous re-
search in second language learning are focused 
on restricted types of learners? errors, such as 
article and preposition errors (Gamon, 2010; 
Rozovskaya and Roth, 2010; Tetreault et al, 
2010). For example, research for Japanese lan-
guage mainly focuses on Japanese case particles 
(Suzuki and Toutanova, 2006; Oyama and 
Matsumoto, 2010). It is only recently that NLP 
research has addressed issues of collocation er-
rors. 
Collocations are conventional word combina-
tions in a language. In Japanese, ocha wo ireru 
???????1 [to make tea]? and yume wo 
miru ? ????2 [to have a dream]? are exam-
ples of collocations. Even though their accurate 
use is crucial to make communication precise 
and to sound like a native speaker, learning them 
                                               
1 lit. to put in tea 
2 lit. to see a dream 
is one of the most difficult tasks for second lan-
guage learners. For instance, the Japanese collo-
cation yume wo miru [lit. to see a dream] is un-
predictable, at least, for native speakers of Eng-
lish, because its constituents are different from 
those in the Japanese language. A learner might 
create the unnatural combination yume wo suru, 
using the verb suru (a general light verb meaning 
?do? in English) instead of miru ?to see?. 
 In this work, we analyze various Japanese 
corpora using a number of collocation and word 
similarity measures to deduce and suggest the 
best collocations for Japanese second language 
learners. In order to build a system that is more 
sensitive to constructions that are difficult for 
learners, we use word similarity measures that 
generate collocation candidates using a large 
Japanese language learner corpus. By employing 
this approach, we could obtain a better result 
compared to other methods that use only well-
formed text. 
The remainder of the paper is organized as fol-
lows. In Section 2, we introduce related work on 
collocation error correction. Section 3 explains 
our method, based on word similarity and 
association measures, for suggesting collocations. 
In Section 4, we describe different word 
similarity and association measures, as well as 
the corpora used in our experiments. The exper-
imental setup and the results are described in 
Sections 5 and 6, respectively. Section 7 points 
out the future directions for our research. 
2 Related Work 
Collocation correction currently follows a similar 
approach used in article and preposition correc-
tion. The general strategy compares the learner's 
word choice to a confusion set generated from 
well-formed text during the training phase.  If 
one or more alternatives are more appropriate to 
the context, the learner's word is flagged as an 
error and the alternatives are suggested as correc-
tions. To constrain the size of the confusion set, 
52
similarity measures are used. To rank the best 
candidates, the strength of association in the 
learner?s construction and in each of the 
generated alternative construction are measured. 
For example, Futagi et al (2008) generated 
synonyms for each candidate string using 
WordNet and Roget?s Thesaurus and used the 
rank ratio measure to score them by their 
semantic similarity.  Liu et al (2009) also used 
WordNet to generate synonyms, but used 
Pointwise Mutual Information as association 
measure to rank the candidates. Chang et al 
(2008) used bilingual dictionaries to derive 
collocation candidates and used the log-
likelihood measure to rank them. One drawback 
of these approaches is that they rely on resources 
of limited coverage, such as dictionaries, thesau-
rus or manually constructed databases to gener-
ate the candidates. Other studies have tried to 
offer better coverage by automatically deriving 
paraphrases from parallel corpora (Dahlmeier 
and Ng, 2011), but similar to Chang et al (2008), 
it is essential to identify the learner?s first lan-
guage and to have bilingual dictionaries and par-
allel corpora for every first language (L1) in or-
der to extend the resulting system. Another prob-
lem is that most research does not actually take 
the learners' tendency of collocation errors into 
account; instead, their systems are trained only 
on well-formed text corpora. Our work follows 
the general approach, that is, uses similarity 
measures for generating the confusion set and 
association measures for ranking the best candi-
dates. However, instead of using only well-
formed text for generating the confusion set, we 
use a large learner corpus created by crawling the 
revision log of a language learning social net-
working service (SNS), Lang-83. Another work 
that also uses data from Lang-8 is Mizumoto et 
al. (2011), which uses Lang-8 in creating a large-
scale Japanese learner?s corpus. The biggest ben-
efit of using such kind of data is that we can ob-
tain in large scale pairs of learners? sentences and 
their corrections assigned by native speakers. 
3 Combining Word Similarity and As-
sociation Measures to Suggest Collo-
cations  
In our work, we focus on suggestions for noun 
and verb collocation errors in ?noun wo verb 
(noun-?-verb)? constructions, where noun is the 
direct object of verb. Our approach consists of 
                                               
3www.lang-8.com  
three steps: 1) for each extracted tuple in the se-
cond learner?s composition, we created a set of 
candidates by substituting words generated using 
word similarity algorithms; 2) then, we measured 
the strength of association in the writer?s phrase 
and in each generated candidate phrase using 
association measures to compute collocation 
scores; 3) the highest ranking alternatives are 
suggested as corrections. In our evaluation, we 
checked if the correction given in the learner 
corpus matches one of the suggestions given by 
the system. Figure 1 illustrates the method used 
in this study. 
 
Figure 1 Word Similarity and Association 
Measures combination method for suggesting 
col-locations. 
 
We considered only the tuples that contain 
noun or verb error. A real application, however, 
should also deal with error detection. For each 
example of the construction on the writer?s text, 
the system should create the confusion set with 
alternative phrases, measure the strength of asso-
ciation in the writer?s phrase and in each gener-
ated alternative phrase and flag as error only if 
the association score of the writer?s phrase is 
lower than one or more of the alternatives gener-
ated and suggest the higher-ranking alternatives 
as corrections. 
4 Approaches to Word Similarity and 
Word Association Strength 
4.1 Word Similarity 
Similarity measures are used to generate the col-
location candidates that are later ranked using 
association measures. In our work, we used the 
following three measures to analyze word simila- 
53
    Confusion Set 
Word ?? ???  ??? ?? ?? ?? ??? ?? ?? 
Meaning do accept begin make write  say eat do carry 
Word ?? ??? ???? ?? ? ?? ?? ??   ??? 
Meaning building beer draft beer money bill amount 
of  
money 
scenery fee building 
 
Table 1 Confusion Set example for the words suru (??) and biru (??) 
 
 ?? 
write 
?? 
read 
??? 
put on 
??? 
diary 
15 11 8 
 
Table 2 Context of a particular noun represented 
as a co-occurrence vector 
 
 ??? 
rice 
????? 
ramen noodle soup 
???? 
curry 
??? 
eat 
164 53 39 
 
Table 3 Context of a particular noun represented 
as a co-occurrence vector 
rity: 1) thesaurus-based word similarity, 2) dis-
tributional similarity and 3) confusion set derived 
from learner corpus. The first two measures gen-
erate the collocation candidates by finding words 
that are analogous to the writer?s choice, a com-
mon approach used in the related work on 
collocation error correction (Liu et al, 2009; 
?stling and O. Knutsson, 2009; Wu et al, 2010) 
and the third measure generates the candidates 
based on the corrections given by native speakers 
in the learner corpus. 
Thesaurus-based word similarity: The intui-
tion of this measure is to check if the given 
words have similar glosses (definitions). Two 
words are considered similar if they are near 
each other in the thesaurus hierarchy (have a path 
within a pre-defined threshold length).  
Distributional Similarity: Thesaurus-based 
methods produce weak recall since many words, 
phrases and semantic connections are not cov-
ered by hand-built thesauri, especially for verbs 
and adjectives.  As an alternative, distributional 
similarity models are often used since it gives 
higher recall. On the other hand, distributional 
similarity models tend to have lower precision 
(Jurafsky et al, 2009), because the candidate set 
is larger. The intuition of this measure is that two 
words are similar if they have similar word con-
texts. In our task, context will be defined by 
some grammatical dependency relation, specifi-
cally, ?object-verb? relation. Context is repre-
sented as co-occurrence vectors that are based on 
syntactic dependencies. We are interested in 
computing similarity of nouns and verbs and 
hence the context of a particular noun is a vector 
of verbs that are in an object relation with that 
noun. The context of a particular verb is a vector 
of nouns that are in an object relation with that 
verb. Table 2 and Table 3 show examples of part 
of co-occurrence vectors for the noun ??? [dia-
ry]? and the verb ???? [eat]?, respectively. 
The numbers indicate the co-occurrence frequen-
cy in the BCCWJ corpus (Maekawa, 2008). We 
computed the similarity between co-occurrence 
vectors using different metrics: Cosine Similarity, 
Dice coefficient (Curran, 2004), Kullback-
Leibler divergence or KL divergence or relative 
entropy (Kullback and Leibler, 1951) and the 
Jenson-Shannon divergence (Lee, 1999). 
Confusion Set derived from learner corpus: 
In order to build a module that can ?guess? 
common construction errors, we created a confu-
sion set using Lang-8 corpus. Instead of generat-
ing words that have similar meaning to the learn-
er?s written construction, we extracted all the 
possible noun and verb corrections for each of 
the nouns and verbs found in the data. Table 1 
shows some examples extracted. For instance, 
the confusion set of the verb suru ??? [to do]? 
is composed of verbs such as ukeru ???? [to 
accept]?, which does not necessarily have similar 
meaning with suru. The confusion set means that 
in the corpus, suru was corrected to either one of 
these verbs, i.e., when the learner writes the verb 
suru, he/she might actually mean to write one of 
the verbs in the confusion set.  For the noun 
biru??? [building]?, the learner may have, for 
example, misspelled the word b?ru ???? 
[beer]?, or may have got confused with the trans-
lation of the English words bill (???[money]?,   
?? [bill]?, ??? [amount of money]?, ??? 
[fee]?) or view (??? [scenery]?) to Japanese. 
54
4.2 Word Association Strength 
After generating the collocation candidates using 
word similarity, the next step is to identify the 
?true collocations? among them.  Here, the 
association strength was measured, in such a way 
that word pairs generated by chance from the 
sampling process can be excluded. An 
association measure assigns an association score 
to each word pair. High scores indicate strong 
association, and can be used to select the ?true 
collocations?. We adopted the Weighted Dice 
coefficient (Kitamura and Matsumoto, 1997) as 
our association measurement. We also tested us-
ing other association measures (results are omit-
ted): Pointwise Mutual Information (Church and 
Hanks, 1990), log-likelihood ratio (Dunning, 
1993) and Dice coefficient (Smadja et al, 1996), 
but Weighted Dice performed best. 
5 Experiment setup 
We divided our experiments into two parts: verb 
suggestion and noun suggestion. For verb sug-
gestion, given the learners? ?noun wo verb? con-
struction, our focus is to suggest ?noun wo verb? 
collocations with alternative verbs other than the 
learner?s written verb. For noun suggestion, giv-
en the learners? ?noun wo verb? construction, our 
focus is to suggest ?noun wo verb? collocations 
with alternative nouns other than the learner?s 
written noun. 
5.1 Data Set 
For computing word similarity and association 
scores for verb suggestion, the following re-
sources were used:  
1) Bunrui Goi Hyo Thesaurus (The National 
Institute for Japanese Language, 1964): a Japa-
nese thesaurus, which has a vocabulary size of 
around 100,000 words, organized into 32,600 
unique semantic classes. This thesaurus was used 
to compute word similarity, taking the words that 
are in the same subtree as the candidate word. By 
subtree, we mean the tree with distance 2 from  
the leaf node (learner?s written word) doing the 
pre-order tree traversal. 
2) Mainichi Shimbun Corpus (Mainichi 
Newspaper Co., 1991): one of the ma-
jor newspapers in Japan that provides raw text of 
newspaper articles used as linguistic resource. 
One year data (1991) were used to extract the 
?noun wo verb? tuples to compute word similari-
ty (using cosine similarity metric) and colloca-
tion scores. We extracted 224,185 tuples com-
posed of 16,781 unique verbs and 37,300 unique 
nouns. 
3) Balanced Corpus of Contemporary Written 
Japanese, BCCWJ Corpus (Maekawa, 2008): 
composed of one hundred million words, por-
tions of this corpus used in our experiments in-
clude magazine, newspaper, textbooks, and blog 
data4. Incorporating a variety of topics and styles 
in the training data helps minimize the domain 
gap problem between the learner?s vocabulary 
and newspaper vocabulary found in the Mainichi 
Shimbun data. We extracted 194,036 ?noun wo 
verb? tuples composed of 43,243 unique nouns 
and 18,212 unique verbs. These data are neces-
sary to compute the word similarity (using cosine 
similarity metric) and collocation scores. 
4) Lang-8 Corpus: Consisted of two year data 
(2010 and 2011):  
 A) Year 2010 data, which contain 
1,288,934 pairs of learner?s sentence and its 
correction, was used to: i) Compute word sim-
ilarity (using cosine similarity metric) and col-
location scores: We took out the learners? sen-
tences and used only the corrected sentences. 
We extracted 163,880 ?noun wo verb? tuples 
composed of 38,999 unique nouns and 16,086 
unique verbs. ii) Construct the confusion set 
(explained in Section 4.1): We constructed the 
confusion set for all the 16,086 verbs and 
38,999 nouns that appeared in the data.  
 B) Year 2011 data were used to con-
struct the test set (described in Section 5.2). 
5.2 Test set selection 
We used Lang-8 (2011 data) for selecting our 
test set. For the verb suggestion task, we extract-
ed all the ?noun wo verb? tuples with incorrect 
verbs and their correction. From the tuples ex-
tracted, we selected the ones where the verbs 
were corrected to the same verb 5 or more times 
by the native speakers. Similarly, for the noun 
suggestion task, we extracted all the ?noun wo 
verb? tuples with incorrect nouns and their cor-
rection. There are cases where the learner?s con-
struction sounds more acceptable than its correc-
tion, cases where in the corpus, they were cor-
rected due to some contextual information. For 
our application, since we are only considering 
                                               
4 Although the language used in blog data is usually 
more informal than the one used in newspaper, 
maganizes, etc., and might contain errors like spelling 
and grammar, collocation errors are much less fre-
quent compared to spelling and grammar errors, since 
combining words appropriately is one the vital com-
petencies of a native speaker of a language.  
55
the noun, particle and verb that the learner wrote, 
there was a need to filter out such contextually 
induced corrections.  To solve this problem, we 
used the Weighted Dice coefficient to compute 
the association strength between the noun and all 
the verbs, filtering out the pairs where the learn-
er?s construction has a higher score than the cor-
rection. After applying those conditions, we ob-
tained 185 tuples for the verb suggestion test set 
and 85 tuples for the noun suggestion test set. 
5.3 Evaluation Metrics 
We compared the verbs in the confusion set 
ranked by collocation score suggested by the sys-
tem with the human correction verb and noun in 
the Lang-8 data. A match would be counted as a 
true positive (tp). A false negative (fn) occurs 
when the system cannot offer any suggestion. 
The metrics we used for the evaluation are: 
precision, recall and the mean reciprocal rank 
(MRR).  We report precision at rank k, k=1, 5, 
computing the rank of the correction when a true 
positive occurs. The MRR was used to assess 
whether the suggestion list contains the correc-
tion and how far up it is in the list. It is calculat-
ed as follows:    
?
?
?
N
i irankNMRR 1 )(
11            (1) 
where N is the size of the test set. If the system 
did not return the correction for a test instance, 
we set 
)(
1
irank
to zero. Recall rate is calculated 
with the formula below: 
fntp
tp
?
        (2) 
6 Results 
Table 4 shows the ten models derived from com-
bining different word similarity measures and the 
Weighted Dice measure as association measure, 
using different corpora. In this table, for instance, 
we named M1 the model that uses thesaurus for 
computing word similarity and uses Mainichi 
Shimbun corpus when computing collocation 
scores using the association measure adopted, 
Weighted Dice. M2 uses Mainichi Shimbun cor-
pus for computing both word similarity and col-
location scores. M10 computes word similarity 
using the confusing set from Lang-8 corpus and 
uses BCCWJ and Lang-8 corpus when compu-
ting collocation scores. 
Considering that the size of the candidate set 
generated by different word similarity measures 
vary considerably, we limit the size of the confu-
sion set to 270 for verbs and 160 for nouns, 
which correspond to the maximum values of the 
confusion set size for nouns and verbs when us-
ing Lang-8 for generating the candidate set. Set-
ting up a threshold was necessary since the size 
of the candidate set generated when using Distri-
butional Similarity methods may be quite large, 
affecting the system performance. When compu-
ting Distributional Similarity, scores are also as-
signed to each candidate, thus, when we set up a 
threshold value n, we consider the list of n can-
didates with highest scores. Table 4 reports the 
precision of the k-best suggestions, the recall rate 
and the MRR for verb and noun suggestion. 
6.1 Verb Suggestion 
Table 4 shows that the model using thesaurus 
(M1) achieved the highest precision rate among 
the other models; however, it had the lowest re-
call. The model could suggest for cases where 
the wrong verb written by the learner and the 
correction suggested in Lang-8 data have similar 
meaning, as they are near to each other in the 
thesaurus hierarchy. However, for cases where 
the wrong verb written by the learner and the 
correction suggested in Lang-8 data do not have 
similar meaning, M1 could not suggest the cor-
rection. 
In order to improve the recall rate, we generat-
ed models M2-M6, which use distributional simi-
larity (cosine similarity) and also use corpora 
other than Mainichi Shimbun corpus to minimize 
the domain gap problem between the learner?s 
vocabulary and the newspaper vocabulary found 
in the Mainichi Shimbun data. The recall rate 
improved significantly but the precision rate de-
creased. In order to compare it with other distri-
butional similarity metrics (Dice, KL-Divergence 
and Jenson-Shannon Divergence) and with the 
method that uses Lang-8 for generating the con-
fusion set, we chose the model with the highest 
recall value as baseline, which is the one that 
uses BCCWJ and Lang-8 (M6) and generated 
other models (M7-M10). The best MRR value 
obtained among all the Distributional Similarity 
methods was obtained by Jenson-Shannon diver-
gence.  The highest recall and MRR values are 
achieved when Lang-8 data were used to gener-
ate the confusion set (M10). 
56
 S
im
il
a
ri
ty
 u
se
d
 
fo
r 
C
o
n
fu
si
o
n
 
S
et
s 
T
h
es
a
u
ru
s 
Cosine Similarity 
D
ic
e 
 C
o
ef
fi
ci
en
t 
K
L
  
D
iv
er
g
en
ce
 
J
en
so
n
-
S
h
a
n
n
o
n
  
 
D
iv
er
g
en
ce
 
C
o
n
fu
si
o
n
 S
et
 
fr
o
m
  
L
a
n
g
-8
 
 
K
-B
es
t 
M
ai
n
ic
h
i 
S
h
im
b
u
n
 
M
ai
n
ic
h
i 
S
h
im
b
u
n
 
B
C
C
W
J 
L
an
g
-8
 
M
ai
n
ic
h
i 
S
h
im
b
u
n
 +
 
B
C
C
W
J 
B
C
C
W
J 
+
 
L
an
g
-8
 
B
C
C
W
J 
+
 
L
an
g
-8
 
B
C
C
W
J 
+
 
L
an
g
-8
 
B
C
C
W
J 
+
 
L
an
g
-8
 
B
C
C
W
J 
+
 
L
an
g
-8
 
 M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 
V
er
b
 
S
u
g
g
es
ti
o
n
 1 0.94 0.48 0.42 0.60 0.62 0.56 0.59 0.63 0.60 0.64 
5 1 0.91 0.94 0.90 0.90 0.86 0.86 0.84 0.88 0.95 
Recall 0.20 0.40 0.30 0.68 0.49 0.71 0.81 0.35 0.74 0.97 
MRR 0.19 0.26 0.19 0.49 0.36 0.50 0.58 0.26 0.53 0.75 
N
o
u
n
 
S
u
g
g
es
ti
o
n
 1 0.16 0.20 0.42 0.58 0.50 0.55 0.30 0.63 0.57 0.73 
5 1 0.66 0.94 0.89 1 0.91 0.83 1 0.84 0.98 
Recall 0.07 0.17 0.22 0.45 0.04 0.42 0.35 0.12 0.38 0.98 
MRR 0.03 0.06 0.13 0.33 0.02 0.29 0.18 0.10 0.26 0.83 
 
Table 4 The precision and recall rate and MRR of the Models of Word Similarity and Association 
Strength method combination. 
6.2 Noun Suggestion 
Similar to the verb suggestion experiments, the 
best recall and MRR values are achieved when 
Lang-8 data were used to generate the confusion 
set (M10).  
For noun suggestion, our automatically con-
structed test set includes a number of spelling 
correction cases, such as cases for the combina-
tion eat ice cream, where the learner wrote 
aisukurimu wo taberu ??????????
?? and the correction is aisukur?mu wo taberu ?
????????????. Such phenomena 
did not occur with the test set for verb suggestion. 
For those cases, the fact that only spelling cor-
rection is necessary in order to have the right 
collocation may also indicate that the learner is 
more confident regarding the choice of the noun 
than the verb. This also justifies the even lower 
recall rate obtained (0.07) when using a thesau-
rus for generating the candidates 
7 Conclusion and Future Work 
We analyzed various Japanese corpora using a 
number of collocation and word similarity 
measures to deduce and suggest the best colloca-
tions for Japanese second language learners.  In 
order to build a system that is more sensitive to 
constructions that are difficult for learners, we 
use word similarity measures that generate collo-
cation candidates using a large Japanese lan-
guage learner corpus, instead of only using well-
formed text. By employing this approach, we 
could obtain better recall and MRR values com-
pared to thesaurus based method and distribu-
tional similarity methods. 
Although only noun-wo-verb construction is 
examined, the model is designed to be applicable 
to other types of constructions, such as adjective-
noun and adverb-noun. Another straightforward 
extension is to pursue constructions with other 
particles, such as ?noun ga verb (subject-verb)?, 
?noun ni verb (dative-verb)?, etc. In our experi-
ments, only a small context information is con-
sidered (only the noun, the particle wo (?) and 
the verb written by the learner). In order to verify 
our approach and to improve our current results, 
considering a wider context size and other types 
of constructions will be the next steps of this re-
search. 
 
Acknowledgments 
 
57
Special thanks to Yangyang Xi for maintaining Lang-
8. 
References  
Y. C. Chang, J. S. Chang, H. J. Chen, and H. C. Liou. 
2008. An automatic collocation writing assistant for 
Taiwanese EFL learners: A case of corpus-based 
NLP technology. Computer Assisted Language 
Learning,  21(3):283?299. 
K. Church, and P. Hanks. 1990. Word Association 
Norms, Mutual Information and Lexicogra-
phy, Computational Linguistics, Vol. 16:1, pp. 22-
29. 
J. R. Curran. 2004. From Distributional to Semantic 
Similarity.  Ph.D. thesis, University of Edinburgh. 
D. Dahlmeier, H. T. Ng. 2011. Correcting Semantic 
Collocation Errors with L1-induced Paraphrases. In 
Proceedings of the 2011 Conference on Empirical 
Methods in Natural Language Processing, pages 
107?117, Edinburgh, Scotland, UK, July.  Associa-
tion for Computational Linguistics  
T. Dunning. 1993. Accurate methods for the statistics 
of surprise and coincidence. Computational Lin-
guistics 19.1 (Mar. 1993), 61-74. 
Y. Futagi, P. Deane, M. Chodorow, and J. Tetreault. 
2008. A computational approach to detecting collo-
cation errors in the writing of non-native speakers 
of English. Computer Assisted Language Learning, 
21, 4 (October 2008), 353-367. 
M. Gamon. 2010. Using mostly native data to correct 
errors in learners? writing: A meta-classifier ap-
proach. In Proceedings of Human Language Tech-
nologies: The 2010 Annual Conference of the 
North American Chapter of the ACL, pages 163?
171, Los Angeles, California, June. Association for 
Computational Linguistics. 
D. Jurafsky and J. H. Martin. 2009. Speech and Lan-
guage Processing: An Introduction to Natural Lan-
guage Processing, Speech Recognition, and Com-
putational Linguistics. 2nd edition. Prentice-Hall. 
K. Maekawa. 2008. Balanced corpus of contemporary 
written japanese. In Proceedings of the 6th 
Workshop on Asian Language Resources (ALR), 
pages 101?102. 
M. Kitamura, Y. Matsumoto. 1997. Automatic extrac-
tion of translation patterns in parallel corpora. In 
IPSJ, Vol. 38(4), pp.108-117, April. In Japanese. 
S. Kullback, R.A. Leibler. 1951. On Information and 
Sufficiency. Annals of Mathematical Statis-
tics 22 (1): 79?86. 
L. Lee. 1999. Measures of Distributional Similarity. 
In Proc of the 37th annual meeting of the ACL, 
Stroudsburg, PA, USA, 25. 
A. L. Liu, D. Wible, and N. L. Tsao. 2009. Automated 
suggestions for miscollocations. In Proceedings of 
the NAACL HLT Workshop on Innovative Use of 
NLP for Building Educational Applications, pages 
47?50, Boulder, Colorado, June. Association for 
Computational Linguistics. 
Mainichi Newspaper Co. 1991. Mainichi Shimbun 
CD-ROM 1991. 
T. Mizumoto, K. Mamoru, M. Nagata, Y. Matsumoto. 
2011. Mining Revision Log of Language Learning 
SNS for Automated Japanese Error Correction of 
Second Language Learners. In Proceedings of the 
5th International Joint Conference on Natural 
Language Processing, pp.147-155. Chiang Mai, 
Thailand, November. AFNLP. 
R. ?stling and O. Knutsson. 2009. A corpus-based 
tool for helping writers with Swedish collocations. 
In Proceedings of the Workshop on Extracting and 
Using Constructions in NLP, Nodalida, Odense, 
Denmark. 70, 77. 
H. Oyama and Y. Matsumoto. 2010. Automatic Error 
Detection Method for Japanese Case Particles in 
Japanese Language Learners. In Corpus, ICT, and 
Language Education, pages 235?245. 
A. Rozovskaya and D. Roth. 2010. Generating confu-
sion sets for context-sensitive error correction. In 
Proceedings of the 2010 Conference on Empirical 
Methods in Natural Language Processing, pages 
961?970, MIT, Massachusetts, USA, October. As-
sociation for Computational Linguistics. 
F. Smadja, K. R. Mckeown, V. Hatzivassiloglou. 1996. 
Translation collocations for bilingual lexicons:  a 
statistical approach. Computational Linguistics, 
22:1-38. 
H. Suzuki and K. Toutanova. 2006. Learning to Pre-
dict Case Markers in Japanese. In Proceedings of 
the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the 
ACL, pages 1049?1056 , Sydney, July. Association 
for Computational Linguistics.J. Tetreault, J. Foster, 
and M. Chodorow. 2010. Using parse features for 
preposition selection and error detection. In Pro-
ceedings of ACL 2010 Conference Short Papers, 
pages 353-358, Uppsala, Sweden, July.  Associa-
tion for Computational Linguistics. 
The National Institute for Japanese Language, editor. 
1964. Bunrui-Goi-Hyo. Shuei shuppan. In Japanese. 
J. C. Wu, Y. C. Chang, T. Mitamura, and J. S. Chang. 
2010. Automatic collocation suggestion in academ-
ic writing. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 115-119, Uppsala, 
Sweden, July. Association for Computational Lin-
guistics. 
58
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 109?113,
Gothenburg, Sweden, 26-27 April 2014.
c?2014 Association for Computational Linguistics
Identifying collocations using cross-lingual association measures
Lis Pereira
1
, Elga Strafella
2
, Kevin Duh
1
and Yuji Matsumoto
1
1
Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara 630-0192, Japan
{lis-k, kevinduh, matsu}@is.naist.jp
2
National Institute for Japanese Language and Linguistics, 10-2 Midoricho, Tachikawa, Tokyo 190-8561, Japan
strafelga@gmail.com
Abstract
We introduce a simple and effective cross-
lingual approach to identifying colloca-
tions. This approach is based on the obser-
vation that true collocations, which cannot
be translated word for word, will exhibit
very different association scores before
and after literal translation. Our exper-
iments in Japanese demonstrate that our
cross-lingual association measure can suc-
cessfully exploit the combination of bilin-
gual dictionary and large monolingual cor-
pora, outperforming monolingual associa-
tion measures.
1 Introduction
Collocations are part of the wide range of linguis-
tic phenomena such as idioms (kick the bucket),
compounds (single-mind) and fixed phrases (by
and large) defined as Multiword Expressions
(MWEs). MWEs, and collocations, in particu-
lar, are very pervasive not only in English, but
in other languages as well. Although handling
MWEs properly is crucial in many natural lan-
guage processing (NLP) tasks, manually annotat-
ing them is a very costly and time consuming task.
The main goal of this work-in-progress is,
therefore, to evaluate the effectiveness of a simple
cross-lingual approach that allows us to automat-
ically identify collocations in a corpus and subse-
quently distinguish them according to one of their
intrinsic properties: the meaning of the expression
cannot be predicted from the meaning of the parts,
i.e. they are characterized by limited composition-
ality (Manning and Sch?utze, 1999). Given an ex-
pression, we predict whether the expression(s) re-
sulted from the word by word translation is also
commonly used in another language. If not, that
might be evidence that the original expression is
a collocation (or an idiom). This can be cap-
tured by the ratio of association scores, assigned
by association measures, in the target vs. source
language. The results indicate that our method
improves the precision comparing with standard
methods of MWE identification through monolin-
gual association measures.
2 Related Work
Most previous works on MWEs and, more specifi-
cally, collocation identification (Evert, 2008; Sere-
tan, 2011; Pecina, 2010; Ramisch, 2012) employ
a standard methodology consisting of two steps:
1) candidate extraction, where candidates are ex-
tracted based on n-grams or morphosyntactic pat-
terns and 2) candidate filtering, where association
measures are applied to rank the candidates based
on association scores and consequently remove
noise. One drawback of such method is that as-
sociation measures might not be able to perform a
clear-cut distinction between collocation and non-
collocations, since they only assign scores based
on statistical evidence, such as co-occurrence fre-
quency in the corpus. Our cross-lingual associa-
tion measure ameliorates this problem by exploit-
ing both corpora in two languages, one of which
may be large.
A few studies have attempted to identify non-
compositional MWE?s using parallel corpora and
dictionaries. Melamed (1997) investigates how
non-compositional compounds can be detected
from parallel corpus by identifying translation di-
vergences in the component words. Pichotta and
DeNero (2013) analyses the frequency statistics
of an expression and its component words, us-
ing many bilingual corpora to identifying phrasal
verbs in English. The disadvantage of such ap-
proach is that large-scale parallel corpora is avail-
able for only a few language pairs. On the other
hand, monolingual data is largely and freely avail-
able for many languages. Our approach requires
only a bilingual dictionary and non-parallel mono-
lingual corpora in both languages.
109
Salehi and Cook (2013) predict the degree of
compositionality using the string distance between
the automatic translation into multiple languages
of an expression and the individual translation
of its components. They use an online database
called Panlex (Baldwin et al., 2010), that can
translate words and expressions from English into
many languages. Tsvetkov and Wintner (2013) is
probably the closest work to ours. They trained
a Bayesian Network for identfying MWE?s and
one of the features used is a binary feature that
assumes value is 1 if the literal translation of the
MWE candidate occurs more than 5 times in a
large English corpus.
3 Identifying Collocations
In this research, we predict whether the expres-
sion(s) resulted from the translation of the com-
ponents of a Japanese collocation candidate is/are
also commonly used in English. For instance, if
we translate the Japanese collocation ????
? mendou-wo-miru ?to care for someone? (care-
?-see)1 into English word by word, we obtain
?see care?, which sounds awkward and may not
appear in an English corpus very often. On the
other hand, the word to word translation of the free
combination ????? eiga-wo-miru ?to see a
movie? (movie-?-see) is more prone to appear
in an English corpus, since it corresponds to the
translation of the expression as well. In our work,
we focus on noun-verb expressions in Japanese.
Our proposed method consists of three steps:
1) Candidate Extraction: We focus on noun-
verb constructions in Japanese. We work with
three construction types: object-verb, subject-verb
and dative-verb constructions, represented respec-
tively as ?noun wo verb (noun-?-verb)?, ?noun ga
verb (noun-?-verb)? and ?noun ni verb (noun-?-
verb)?, respectively. The candidates are extracted
from a Japanese corpus using a dependency parser
(Kudo and Matsumoto, 2002) and ranked by fre-
quency.
2) Translation of the component words: for
each noun-verb candidate, we automatically ob-
tain all the possible English literal translations of
the noun and the verb using a Japanese/English
dictionary. Using that information, all the possi-
ble verb-noun combinations in English are then
generated. For instance, for the candidate ??
1
In Japanese,? is a case marker that indicates the object-
verb dependency relation.
?? hon-wo-kau ?to buy a book? (buy-?-book),
we take the noun ? hon and the verb ?? kau
and check their translation given in the dictio-
nary. ? has translations like ?book?, ?main? and
?head? and ?? is translated as ?buy?. Based
on that, possible combinations are ?buy book? or
?buy main? (we filter out determiners, pronouns,
etc.).
3) Ranking of original and derived word to
word translated expression: we compare the
association score of the original expression in
Japanese (calculated using a Japanese corpus) and
its corresponding derived word to word translated
expressions. If the original expression has a much
higher score than its literal translations, it might be
a good evidence that we are dealing with a collo-
cation, instead of a free combination.
There is no defined criteria in choosing one par-
ticular association measure when applying it in
a specific task, since different measures highlight
different aspects of collocativity (Evert, 2008). A
state-of-the-art, language independent framework
that employs the standard methodology to identify
MWEs is mwetoolkit (Ramisch, 2012). It ranks
the extracted candidates using four different as-
sociation measures: log-likelihood-ratio, Dice co-
efficient, pointwise mutual information and Stu-
dent?s t-score. We previously conducted exper-
iments with these four measures for Japanese
(results are ommited), and Dice coefficient per-
formed best. Using Dice coefficient, we calcu-
late the ratio between the score of the original ex-
pression and the average score of its literal trans-
lations. Finally, the candidates are ranked by the
ratio value. Those that have a high value are ex-
pected to be collocations, while those with a low
value are expected to be free combinations.
4 Experiment Setup
4.1 Data Set
The following resources were used in our experi-
ments:
Japanese/English dictionary: we used Edict
(Breen, 1995), a freely available Japanese/English
Dictionary in machine-readable form, containing
110,424 entries. This dictionary was used to find
all the possible translations of each Japanese word
involved in the candidate (noun and verb). For our
test set, all the words were covered by the dictio-
nary. We obtained an average of 4.5 translations
per word. All the translations that contains more
110
than three words are filtered out. For the transla-
tions of the Japanese noun, we only consider the
first noun appearing in each translation. For the
translations of the Japanese verb, we only consider
the first verb/phrasal verb appearing in each trans-
lation. For instance, in the Japanese collocation?
???? koi-ni-ochiru ?to fall in love? (love-?-
fall down)
2
, the translations in the dictionary and
the ones we consider (shown in bold type) of the
noun? koi ?love? and the verb??? ochiru ?to
fall down? are:
?: love , tender passion
???: to fall down, to fail, to crash, to
degenerate, to degrade
Bilingual resource: we used Hiragana Times
corpus, a Japanese-English bilingual corpus of
magazine articles of Hiragana Times
3
, a bilingual
magazine written in Japanese and English to intro-
duce Japan to non-Japanese, covering a wide range
of topics (culture, society, history, politics, etc.).
The corpus contains articles from 2003-2102, with
a total of 117,492 sentence pairs. We used the
Japanese data to extract the noun-verb collocation
candidates using a dependency parser, Cabocha
(Kudo and Matsumoto, 2002). For our work, we
focus on the object-verb, subject-verb and dative-
verb dependency relations. The corpus was also
used to calculate the Dice score of each Japanese
candidate, using the Japanese data.
Monolingual resource: we used 75,377 En-
glish Wikipedia articles, crawled in July 2013. It
contains a total of 9.5 million sentences. The data
was used to calculate the Dice score of each can-
didate?s derived word to word translated expres-
sions. The corpus was annotated with Part-of-
Speech (POS) information, from where we de-
fined POS patterns to extract all the verb-noun
and noun-verb sequences, using the MWE toolkit
(Ramisch, 2012), which is an integrated frame-
work for MWE treatment, providing corpus pre-
processing facilities.
Table 1 shows simple statistics on the Hiragana
Times corpus and on the Wikipedia corpus.
4.2 Test set
In order to evaluate our system, the top 100 fre-
quent candidates extracted from Hiragana Times
corpus were manually annotated by 4 Japanese
native speakers. The judges were asked to make
2? is the dative case marker in Japanese.
3
http://www.hiraganatimes.com
Hiragana
Times
Wikipedia
# jp sentences 117,492 -
# en sentences 117,492 9,500,000
# jp tokens 3,949,616 -
# en tokens 2,107,613 247,355,886
# jp noun-verb 31,013 -
# en noun-verb - 266,033
# en verb-noun - 734,250
Table 1: Statistics on the Hiragana Times corpus
and Wikipedia corpus, showing the number of sen-
tences, number of words and number of noun-
verb and verb-noun expressions in English and
Japanese.
a ternary judgment for each of the candidates on
whether the candidate is a collocation, idiom or
free combination. For each category, a judge was
shown the definition and some examples. We de-
fined collocations as all those expressions where
one of the component words preserves its lit-
eral meaning, while the other element assumes a
slightly different meaning and its use is blocked
(i.e. it cannot be substituted by a synonym). Id-
ioms were defined as the semantically and syntac-
tically fixed expressions where all the component
words loose their original meaning. Free combi-
nations were defined as all those expressions fre-
quently used where the components preserve their
literal meaning. The inter-annotator agreement
is computed using Fleiss? Kappa statistic (Fleiss,
1971), since it involves more than 2 annotators.
Since our method does not differentiate colloca-
tions from idioms (although we plan to work on
that as future work), we group collocations and id-
ioms as one class. We obtained a Kappa coeffi-
cient of 0.4354, which is considered as showing
moderate agreement according to Fleiss (1971).
Only the candidates identically annotated by the
majority of judges (3 or more) were added to the
test set, resulting in a number of 87 candidates
(36 collocations and 51 free combinations). Af-
ter that, we obtained a new Kappa coefficient of
0.5427, which is also considered as showing mod-
erate agreement (Fleiss, 1971).
4.3 Baseline
We compare our proposed method with two base-
lines: an association measure based system and
a Phrase-Based Statistical Machine Translation
111
(SMT) based system.
Monolingual Association Measure: The sys-
tem ranks the candidates in the test set according
to their Dice score calculated using the Hiragana
Times Japanese data.
Phrase-Based SMT system: a standard non-
factored phrase-based SMT system was built us-
ing the open source Moses toolkit (Koehn et al.,
2007) with parameters set similar to those of Neu-
big (2011), who provides a baseline system pre-
viously applied to a Japanese-English corpus built
from Wikipedia articles. For training, we used Hi-
ragana Times bilingual corpus. The Japanese sen-
tences were word-segmented and the English sen-
tences were tokenized and lowercased. All sen-
tences with size greater than 60 tokens were previ-
ously eliminated. The whole English corpus was
used as training data for a 5-gram language model
built with the SRILM toolkit (Stolcke, 2002).
Similar to what we did for our proposed
method, for each candidate in the test set, we find
all the possible literally translated expressions (as
described in Section 3). In the phrase-table gen-
erated after the training step, we look for all the
entries that contain the original candidate string
and check if at least one of the possible literal
translations appear as their corresponding transla-
tion. For the entries found, we compute the av-
erage of the sum of the candidate?s direct and in-
verse phrase translation probability scores. The di-
rect phrase translation probability and the inverse
phrase translation probability (Koehn et al., 2003)
are respectively defined as:
?(e|f) =
count(f, e)
?
f
count(f, e)
(1)
?(f |e) =
count(f, e)
?
e
count(f, e)
(2)
Where f and e indicate a foreign phrase and a
source phrase, independently.
The candidates are ranked according to the av-
erage score as described previously.
5 Evaluation
In our evaluation, we average the precision con-
sidering all true collocations and idioms as thresh-
old points, obtaining the mean average precision
(MAP). Differently from the traditional approach
used to evaluate an association measure, using
MAP we do not need to set a hard threshold.
Table 2 presents the MAP values for our pro-
posed method and for the two baselines. Our
cross-lingual method performs best in terms of
MAP values against the two baselines. We found
out that it performs statistically better only com-
pared to the Monolingual Association Measure
baseline
4
. The Monolingual Association Measure
baseline performed worst, since free combinations
were assigned high scores as well, and the system
was not able to perform a clear separation into col-
locations and non-collocations. The Phrase-Based
SMT system obtained a higher MAP value than
Monolingual Association measure, but the score
may be optimistic since we are testing in-domain.
One concern is that there are only a very few bilin-
gual/parallel corpora for the Japanese/English lan-
guage pair, in case we want to test with a different
domain and larger test set. The fact that our pro-
posed method outperforms SMT implies that us-
ing such readily-available monolingual data (En-
glish Wikipedia) is a better way to exploit cross-
lingual information.
Method MAP value
Monolingual Association Measure 0.54
Phrase-Based SMT 0.67
Proposed Method 0.71
Table 2: Mean average precision of proposed
method and baselines.
Some cases where the system could not per-
form well include those where a collocation can
also have a literal translation. For instance,
in Japanese, there is the collocation ????
kokoro-wo-hiraku ?to open your heart? (heart-?-
open), where the literal translation of the noun ?
kokoro ?heart? and the verb ?? hiraku ?open?
correspond to the translation of the expression as
well.
Another case is when the candidate expression
has both literal and non-literal meaning. For in-
stance, the collocation ???? hito-wo-miru
(person-?-see) can mean ?to see a person?, which
is the literal meaning, but when used together with
the noun ? me ?eye?, for instance, it can also
can mean ?to judge human character?. When an-
notating the data, the judges classified as idioms
some of those expressions, for instance, because
the non-literal meaning is mostly used compared
4
Statistical significance was calculated using a two-tailed
t-test for a confidence interval of 95%.
112
with the literal meaning. However, our system
found that the literal translated expressions are
also commonly used in English, which caused the
performance decrease.
6 Conclusion and Future Work
In this report of work in progress, we propose
a method to distinguish free combinations and
collocations (and idioms) by computing the ratio
of association measures in source and target lan-
guages. We demonstrated that our method, which
can exploit existing monolingual association mea-
sures on large monolingual corpora, performed
better than techniques previously applied in MWE
identification.
In the future work, we are interested in increas-
ing the size of the corpus and test set used (for
instance, include mid to low frequent MWE?s),
as well as applying our method to other collo-
cational patterns like Noun-Adjective, Adjective-
Noun, Adverb-Verb, in order to verify our ap-
proach. We also believe that our approach can
be used for other languages as well. We intend to
conduct a further investigation on how we can dif-
ferentiate collocations from idioms. Another step
of our research will be towards the integration of
the acquired data into a web interface for language
learning and learning materials for foreign learn-
ers as well.
Acknowledgments
We would like to thank Masashi Shimbo, Xi-
aodong Liu, Mai Omura, Yu Sawai and Yoriko
Nishijima for their valuable help and anonymous
reviewers for the helpful comments and advice.
References
Timothy Baldwin, Jonathan Pool, and Susan M Colow-
ick. 2010. Panlex and lextract: Translating all
words of all languages of the world. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics: Demonstrations, pages 37?
40. Association for Computational Linguistics.
Jim Breen. 1995. Building an electronic japanese-
english dictionary. In Japanese Studies Association
of Australia Conference. Citeseer.
Stefan Evert. 2008. Corpora and collocations. Corpus
Linguistics. An International Handbook, 2.
Joseph L Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological bul-
letin, 76(5):378.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
proceedings of the 6th conference on Natural lan-
guage learning-Volume 20, pages 1?7. Association
for Computational Linguistics.
Christopher D Manning and Hinrich Sch?utze. 1999.
Foundations of statistical natural language process-
ing, volume 999. MIT Press.
Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. EMNLP.
Graham Neubig. 2011. The kyoto free translation task.
Available on line at http://www. phontron. com/kftt.
Pavel Pecina. 2010. Lexical association measures
and collocation extraction. Language resources and
evaluation, 44(1-2):137?158.
Karl Pichotta and John DeNero. 2013. Identifying
phrasal verbs using many bilingual corpora. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 636?
646.
Carlos Ramisch. 2012. A generic framework for mul-
tiword expressions treatment: from acquisition to
applications. In Proceedings of ACL 2012 Student
Research Workshop, pages 61?66. Association for
Computational Linguistics.
Bahar Salehi and Paul Cook. 2013. Predicting
the compositionality of multiword expressions using
translations in multiple languages.
Violeta Seretan. 2011. Syntax-based collocation ex-
traction, volume 44. Springer.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In INTERSPEECH.
Yulia Tsvetkov and Shuly Wintner. 2013. Identifica-
tion of multi-word expressions by combining multi-
ple linguistic information sources.
113
