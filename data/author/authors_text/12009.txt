Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 66?71,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Syntactic Tree-based Relation Extraction Using a Generalization of 
Collins and Duffy Convolution Tree Kernel 
Mahdy Khayyamian Seyed Abolghasem 
Mirroshandel 
Hassan Abolhassani 
Sharif University of Technology Sharif University of Technology Sharif University of Technology 
khayyamian@ce.sharif.edu mirroshandel@ce.sharif.edu abolhassani@sharif.edu 
 
 
 
 
 
 
Abstract 
Relation extraction is a challenging task in 
natural language processing. Syntactic 
features are recently shown to be quite 
effective for relation extraction. In this 
paper, we generalize the state of the art 
syntactic convolution tree kernel 
introduced by Collins and Duffy. The 
proposed generalized kernel is more 
flexible and customizable, and can be 
conveniently utilized for systematic 
generation of more effective application 
specific syntactic sub-kernels. Using the 
generalized kernel, we will also propose a 
number of novel syntactic sub-kernels for 
relation extraction. These kernels show a 
remarkable performance improvement over 
the original Collins and Duffy kernel in the 
extraction of ACE-2005 relation types. 
1 Introduction 
One of the contemporary demanding NLP tasks is 
information extraction, which is the procedure of 
extracting structured information such as entities, 
relations, and events from free text documents. As 
an information extraction sub-task, semantic 
relation extraction is the procedure of finding 
predefined semantic relations between textual 
entity mentions. For instance, assuming a semantic 
relation with type Physical and subtype Located 
between an entity of type Person and another 
entity of type Location, the sentence "Police 
arrested Mark at the airport last week." conveys 
two mentions of this relation between "Mark" and 
"airport" and also between "police" and "airport" 
that can be shown in the following format. 
Phys.Located(Mark, airport) 
Phys.Located(police, airport) 
 Relation extraction is a key step towards 
question answering systems by which vital 
structured data is acquired from underlying free 
text resources. Detection of protein interactions in 
biomedical corpora (Li et al, 2008) is another 
valuable application of relation extraction. 
 Relation extraction can be approached by a 
standard classification learning method. We 
particularly use SVM (Boser et al, 1992; Cortes 
and Vapnik, 1995) and kernel functions as our 
classification method. A kernel is a function that 
calculates the inner product of two transformed 
vectors of a high dimensional feature space using 
the original feature vectors as shown in eq. 1. 
)().(),( jiji XXXXK ??=  (1) 
Kernel functions can implicitly capture a large 
amount of features efficiently; thus, they have been 
widely used in various NLP tasks.  
 Various types of features have been exploited so 
far for relation extraction. In (Bunescu and 
Mooney, 2005b) sequence of words features are 
utilized using a sub-sequence kernel. In (Bunescu 
and Mooney, 2005a) dependency graph features 
are exploited, and in (Zhang et al, 2006a) syntactic 
features are employed for relation extraction. 
Although in order to achieve the best performance, 
it is necessary to use a proper combination of these 
features (Zhou et al, 2005), in this paper, we will 
concentrate on how to better capture the syntactic 
features for relation extraction. 
66
 In CD?01 (Collins and Duffy, 2001) a 
convolution syntactic tree kernel is proposed that 
generally measures the syntactic similarity 
between parse trees. In this paper, a generalized 
version of CD?01 convolution tree kernel is 
proposed by associating generic weights to the 
nodes and sub-trees of the parse tree. These 
weights can be used to incorporate domain 
knowledge into the kernel and make it more 
flexible and customizable. The generalized kernel 
can be conveniently used to generate a variety of 
syntactic sub-kernels (including the original CD?01 
kernel), by adopting appropriate weighting 
mechanisms.  
 As a result, in this paper, novel syntactic sub-
kernels are generated from the generalized kernel 
for the task of relation extraction. Evaluations 
demonstrate that these kernels outperform the 
original CD?01 kernel in the extraction of ACE-
2005 main relation types  
 The remainder of this paper is structured as 
follows. In section 2, the most related works are 
briefly reviewed. In section 3, CD?01 tree kernel is 
described. The proposed generalized convolution 
tree kernel is explained in section 4 and its 
produced sub-kernels for relation extraction are 
illustrated in section 5. The experimental results 
are discussed in section 6. Our work is concluded 
in section 7 and some possible future works are 
presented in section 8. 
2 Related Work 
In (Collins and Duffy, 2001), a convolution parse 
tree kernel has been introduced. This kernel is 
generally designed to measure syntactic similarity 
between parse trees and is especially exploited for 
parsing English sentences in their paper. Since 
then, the kernel has been widely used in different 
applications such as semantic role labeling 
(Moschitti, 2006b) and relation extraction (Zhang 
et al, 2006a; Zhang et al, 2006b; Zhou et al, 
2007; Li et al 2008). 
 For the first time, in (Zhang et al, 2006a), this 
convolution tree kernel was used for relation 
extraction. Since the whole syntactic parse tree of 
the sentence that holds the relation arguments 
contains a plenty of misleading features, several 
parse tree portions are studied to find the most 
feature-rich portion of the syntactic tree for 
relation extraction, and Path-Enclosed Tree (PT) is 
finally found to be the best performing tree 
portion. PT is a portion of parse tree that is 
enclosed by the shortest path between the two 
relation arguments. Moreover, this tree kernel is 
combined with an entity kernel to form a 
reportedly high quality composite kernel in (Zhang 
et al, 2006b). 
3 CD?01 Convolution Tree Kernel  
In (Collins and Duffy, 2001), a convolution tree 
kernel has been introduced that measures the 
syntactic similarity between parse trees. This 
kernel computes the inner products of the 
following feature vector. 
10
))(#
),...,(#),...,(#()(
2
2
1
2
1
?<
=
?
?
??
TsubTree
TsubTreeTsubTreeTH
n
size
i
sizesize
n
i
 
(2) 
Each feature of this vector is the occurrence count 
of a sub-tree type in the parse tree decayed 
exponentially by the parameter ? . Without this 
decaying mechanism used to retain the kernel 
values within a fairly small range, the value of the 
kernel for identical trees becomes far higher than 
its value for different trees. Term isize  is defined 
to be the number of rules or internal nodes of the ith 
sub-tree type. Samples of such sub-trees are shown 
in Fig. 1 for a simple parse tree. Since the number 
of sub-trees of a tree is exponential in its size 
(Collins and Duffy, 2001), direct inner product 
calculation is computationally infeasible. 
Consequently, Collins and Duffy (2001) proposed 
an ingenious kernel function that implicitly 
calculates the inner product in )( 21 NNO ?  time 
on the trees of size 1N  and 2N . 
4 A Generalized Convolution Tree 
Kernel  
In order to describe the kernel, a feature vector 
over the syntactic parse tree is firstly defined in eq. 
(3), in which the ith feature equals the weighted 
sum of the number of instances of sub-tree type ith 
in the tree. 
Function )(nI
isubtree
 is an indicator function that 
returns 1 if the isubtree  occurs with its root at 
node n and 0 otherwise. As described in eq. (4), 
67
function tw(T) (which stands for "tree weight") 
assigns a weight to a tree T which is equal to the 
product of the weights of all its nodes. 
)))](()([
,...,))](()([
))],...,(()([()( 11
?
?
?
?
?
?
?
?
?=
Tn
msubtree
Tn
isubtree
Tn
subtree
nsubtreetwnI
nsubtreetwnI
nsubtreetwnITH
m
i
 (3) 
??
??
?=
)()(
)()()(
TdesExternalNonTdesInternalNon
nenwninwTtw  
(4) 
 
Figure 1. Samples of sub-trees used in convolution tree 
kernel calculation. 
 
 Since each node of the whole syntactic tree can 
either happen as an internal node or as an external 
node of a supposed sub-tree (presuming its 
existence in the sub-tree), two types of weights are 
respectively associated to each node by the 
functions )(ninw  and )(nenw  (which respectively 
stand for "internal node weight" and "external node 
weight"). For instance, in Fig. 1, the node with 
label PP is an external node for sub-trees (1) and 
(7) while it is an internal node of sub-trees (3) and 
(4). 
??
?? ?
?
? ?
? ?
? ?
?
?
=
?
??=
??
?=
><=
11 22
11 22
22
11
),(
))](())((
)()([
)]])(()([
]))(()([[
)(),(),(
21
21
21
22
11
2121
Tn Tn
gc
ii
Tn Tn i
subtreesubtree
Tn
isubtree
i Tn
isubtree
nnC
nsubTreetwnsubTreetw
nInI
nsubTreetwnI
nsubTreetwnI
THTHTTK
ii
i
i
 
(5) 
 As shown in eq. (5), A similar procedure to 
(Collins and Duffy, 2001) can be employed to 
develop a kernel function for the calculation of dot 
products on H(T) vectors. According to eq. (5) the 
calculation of the kernel finally leads to the sum of 
a ),( 21 nnCgc  function over all tree node pairs of T1 
and T2. Function ),( 21 nnCgc  is the weighted sum of 
the common sub-trees rooted at 1n  and n2, and can 
be recursively computed in a similar way to 
function ),( 21 nnC  of (Collins and Duffy, 2001) as 
follows. 
(1) if the production rules of nodes n1 and n2 are 
different then 0),( 21 =nnCgc  
(2) else if n1 and n2 are the same pre-terminals (the 
same part of speeches) then 
))(()(
))(()(),(
22
1121
nchildenwninw
nchildenwninwnnCgc
?
??=
 
(3) else if both n1 and n2 have the same production 
rules then 
))](),(())(())(([
)()(),(
2121
2121
nchildnchildCnchildenwnchildenw
ninwninwnnC
iigc
i
ii
gc
? +?
??=
 
 In the first case, when the two nodes represent 
different production rules they can't accordingly 
have any sub-trees in common. In the second case, 
there is exactly one common sub-tree of size two. 
It should be noted that all the leaf nodes of the tree 
(or words of the sentence) are considered identical 
in the calculation of the tree kernel. The value of 
the function in this case is the weight of this 
common sub-tree. In the third case, when the nodes 
generally represent the same production rules the 
weighted sum of the common sub-trees are 
calculated recursively. The equation holds because 
the existence of common sub-trees rooted at n1 and 
n2 implies the existence of common sub-trees 
rooted at their corresponding children, which can 
be combined multiplicatively to form their parents' 
common sub-trees. 
 Due to the equivalent procedure of kernel 
calculation, this generalized version of the tree 
kernel preserves the nice )( 21 NNO ?  time 
complexity property of the original kernel. It is 
worthy of note that in (Moschitti, 2006b) a sorting 
based method is proposed for the fast 
implementation of such tree kernels that reduces 
the average running time to )( 21 NNO + . 
 The generalized kernel can be converted to 
CD?01 kernel by defining ?=)(ninw  and 
1)( =nenw . Likewise, other definitions can be 
utilized to produce other useful sub-kernels. 
DT 
the 
airport 
NN 
NP 
NP PP 
NP 
NP 
NP PP 
IN NNP 
Mark 
airport 
NP 
NP PP 
IN NP 
DT NN 
NNP 
Mark at 
the 
PP 
airport 
IN NP 
DT NN 
at 
the 
(4) (5) 
(6) (7) 
DT 
the airport 
NP 
NN 
NP 
NP PP 
NNP 
(1) (2) (3) 
68
5 Kernels for Relation Extraction 
In this section, three sub-kernels of the generalized 
convolution tree kernel will be proposed for 
relation extraction. Using the embedded weights of 
the generalized kernel, these sub-kernels 
differentiate among sub-trees based on their 
expected relevance to semantic relations. More 
specifically, the sub-trees are weighted according 
to how their nodes interact to the arguments of the 
relation. 
5.1 Argument Ancestor Path Kernel (AAP) 
Definition of weighting functions is shown in eq. 
(6) and (7). Parameter 10 ?< ?  is a decaying 
parameter similar to ? . 
 
??
??
?
=
otherwise
itonnodeaofchilddirectaor
pathancestorumenttheonisnif
ninw
0
arg
)(
?
 (6) 
??
??
?
=
otherwise
itonnodeaofchilddirectaor
pathancestorumenttheonisnif
nenw
0
arg1
)(
 
(7) 
This weighting method is equivalent to applying 
CD?01 tree kernel (by setting 2?? = ) on a portion 
of the parse tree that exclusively includes the 
arguments ancestor nodes and their direct children. 
5.2 Argument Ancestor Path Distance Kernel 
(AAPD) 
DISTMAX
nAAPDistnAAPDistMin
ninw _
))arg,(),arg,(( 21
)( ?=
 
(8) 
DISTMAX
nAAPDistnAAPDistMin
nenw _
))arg,(),arg,(( 21
)( ?=
 
(9) 
Definition of weighting functions is shown in eq. 
(8) and (9). Both functions have identical 
definitions for this kernel. 
Function AAPDist(n,arg) calculates the distance of 
the node n from the argument arg on the parse tree 
as illustrated by Fig. 2. MAX_DIST is used for 
normalization, and is the maximum of 
AAPDist(n,arg) in the whole tree. In this way, the 
closer a tree node is to one of the arguments 
ancestor path, the less it is decayed by this 
weighting method.  
 
 
 
 
 
 
5.3 Threshold Sensitive Argument Ancestor 
Path Distance Kernel (TSAAPD) 
This kernel is intuitively similar to the previous 
kernel but uses a rough threshold based decaying 
technique instead of a smooth one. The definition 
of weighting functions is shown in eq. (10) and 
(11). Both functions are again identical in this case.   
 
??
?
?
?
=
ThresholdnAAPDist
ThresholdnAAPDist
ninw )(
)(1)(
?  (10) 
??
?
?
?
=
ThresholdnAAPDist
ThresholdnAAPDist
nenw )(
)(1)(
?
 (11) 
6 Experiments 
6.1 Experiments Setting 
The proposed kernels are evaluated on ACE-2005 
multilingual corpus (Walker et al, 2006). In order 
to avoid parsing problems, the more formal parts 
of the corpus in "news wire" and "broadcast news" 
sections are used for evaluation as in (Zhang et al, 
2006b). 
 
 
AAPDist(airport, NP)=1 
 
S 
NN 
airport 
NP VP 
NNP 
Police 
VBN 
arrested 
NP 
NP PP 
IN NP 
DT NN 
NNP 
Mark at 
the 
NP 
JJ 
last week 
Figure 2. The syntactic parse tree of the sentence 
"Police arrested Mark at the airport last week" that 
conveys a Phys.Located(Mark, airport) relation. The 
ancestor path of the argument "airport" (dashed 
curve) and the distance of the node NP of "Mark" 
from it (dotted curve) is shown. 
69
  PER-SOC ART GEN-AFF ORG-AFF PART-WHOLE PHYS 
CD?01 0.62 0.51 0.09 0.43 0.30 0.32 
AAP 0.58 0.49 0.10 0.43 0.28 0.36 
AAPD 0.70 0.50 0.12 0.43 0.29 0.29 
TSAAPD-0 0.63 0.48 0.11 0.43 0.30 0.33 
TSAAPD-1 0.73 0.47 0.11 0.45 0.28 0.33 
Table 1: The F1-Measure value is shown for every kernel on each ACE-2005 main relation type. For every relation 
type the best result is shown in bold font. 
 
 We have used LIBSVM (Chang and Lin 2001) 
java source for the SVM classification and 
Stanford NLP package1 for tokenization, sentence 
segmentation and parsing.  
 Following [Bunescu and Mooney, 2007], every 
pair of entities within a sentence is regarded as a 
negative relation instance unless it is annotated as a 
positive relation in the corpus. The total number of 
negative training instances, constructed in this 
way, is about 20 times more than the number of 
annotated positive instances. Thus, we also 
imposed the restriction of maximum argument 
distance of 10 words. This constraint eliminates 
half of the negative constructed instances while 
slightly decreases positive instances. Nevertheless, 
since the resulted training set is still unbalanced, 
we used LIBSVM weighting mechanism. 
Precisely, if there are P positive and N negative 
instances in the training set, a weight value of 
PN /  is used for positive instances while the 
default weight value of 1 is used for negative ones. 
 A binary SVM is trained for every relation type 
separately, and type compatible annotated and 
constructed relation instances are used to train it. 
For each relation type, only type compatible 
relation instances are exploited for training. For 
example to learn an ORG-AFF relation (which 
applies to (PER, ORG) or (ORG, ORG) argument 
types) it is meaningless to use a relation instance 
between two entities of type PERSON. Moreover, 
the total number of training instances used for 
training every relation type is restricted to 5000 
instances to shorten the duration of the evaluation 
process. The reported results are achieved using a 
5-fold cross validation method. 
 The kernels AAP, AAPD and TSAAPD-0 
(TSAAPD with threshold = 0) and TSAAPD-1 
(TSAAPD with threshold = 1) are compared with 
CD?01 convolution tree kernel. All the kernels 
                                                          
1 http://nlp.stanford.edu/software/index.shtml 
except for AAP are computed on the PT portion 
described in section 2. AAP is computed over the 
MCT tree portion which is also proposed by 
(Zhang et al, 2006a) and is the sub-tree rooted at 
the first common ancestor of relation arguments.  
 For the proposed kernels ?  is set to 0.44 which 
is tuned on a development set that contained 5000 
instances of type PHYS. The ?  parameter of 
CD?01 kernel is set to 0.4 according to (Zhang et 
al., 2006a). The C parameter of SVM classification 
is set to 2.4 for all the kernels after tuning it 
individually for each kernel on the mentioned 
development set. 
6.2 Experiments Results 
The results of the experiments are shown in Table 
1. The proposed kernels outperform the original 
CD?01 kernel in four of the six relation types. The 
performance of TSAAPD-1 is especially 
remarkable because it is the best kernel in ORG-
AFF and PER-SOC relations. It particularly 
performs very well in the extraction of PER-SOC 
relation with an F1-measure of 0.73. It should be 
noted that the general low performance of all the 
kernels on the GEN-AFF type is because of its 
extremely small number of annotated instances in 
the training set (40 in 5000). The AAPD kernel has 
the best performance with a remarkable 
improvement over the Collins kernel in GEN-AFF 
relation type. 
 The results clearly demonstrate that the nodes 
closer to the ancestor path of relation arguments 
contain the most useful syntactic features for 
relation extraction 
7 Conclusion  
In this paper, we proposed a generalized 
convolution tree kernel that can generate various 
syntactic sub-kernels including the CD?01 kernel. 
Kernel 
Relation 
70
The kernel is generalized by assigning weights to 
the sub-trees. The weight of a sub-tree is the 
product of the weights assigned to its nodes by two 
types of weighting functions. In this way, impacts 
of the tree nodes on the kernel value can be 
discriminated purposely based on the application. 
Context information can also be injected to the 
kernel via context sensitive weighting mechanisms. 
 Using the generalized kernel, various sub-
kernels can be produced by different definitions of 
the two weighting functions. We consequently 
used the generalized kernel for systematic 
generation of useful kernels in relation extraction. 
In these kernels, the closer a node is to the relation 
arguments ancestor paths, the less it is decayed by 
the weighting functions. Evaluation on the ACE-
2005 main relation types demonstrates the 
effectiveness of the proposed kernels. They show 
remarkable performance improvement over CD?01 
kernel.  
8 Future Work 
Although the path-enclosed tree portion (PT) 
(Zhang et al, 2006a) seems to be an appropriate 
portion of the syntactic tree for relation extraction, 
it only takes into account the syntactic information 
between the relation arguments, and discards many 
useful features (before and after the arguments 
features). It seems that the generalized kernel can 
be used with larger tree portions that contain 
syntactic features before and after the arguments, 
because it can be more easily targeted to related 
features. 
 Currently, the proposed weighting mechanisms 
are solely based on the location of the tree nodes in 
the parse tree; however other useful information 
such as labels of nodes can also be used in 
weighting. 
 Another future work can be utilizing the 
generalized kernel for other applicable NLP tasks 
such as co-reference resolution. 
Acknowledgement 
This work is supported by Iran Telecommunication 
Research Centre under contract No. 500-7725.  
References 
Boser B. E., Guyon I., and Vapnik V. 1992. A training 
algorithm for optimal margin classifiers. In 
Proceedings of the Fifth Annual Workshop on 
Computational Learning Theory, pages 144-152. 
ACM Press.  
Bunescu R. C. and Mooney R. J. 2005a. A Shortest Path 
Dependency Kernel for Relation Extraction. 
EMNLP-2005 
Bunescu R. C. and Mooney R. J. 2005b. Subsequence 
kernels for relation extraction. NIPS-2005. 
Bunescu R. C. and Mooney R. J. 2007. Learning for 
Information Extraction: From Named Entity 
Recognition and Disambiguation to Relation 
Extraction, Ph.D. Thesis. Department of Computer 
Sciences, University of Texas at Austin. 
Chang, C.-C. and C.-J. Lin 2001. LIBSVM: a library for 
support vector machines. Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
Collins M. and Duffy N. 2001. Convolution Kernels for 
Natural Language. NIPS-2001 
Cortes C. and Vapnik V. 1995. Support-vector network. 
Machine Learning. 20, 273-297. 
Li J., Zhang Z., Li X. and Chen H. 2008. Kernel-based 
learning for biomedical relation extraction. J. Am. 
Soc. Inf. Sci. Technol. 59, 5, 756?769. 
Moschitti A. 2006a. Making tree kernels practical for 
natural language learning. EACL-2006. 
Moschitti A. 2006b. Syntactic kernels for natural 
language learning: the semantic role labeling case. 
HLT-NAACL-2006 (short paper) 
Walker, C., Strassel, S., Medero J. and Maeda, K. 2006. 
ACE 2005 Multilingual Training Corpus. Linguistic 
Data Consortium, Philadelphia. 
Zhang M., Zhang J. and SU j. 2006a. Exploring 
syntactic features for relation extraction using a 
convolution tree kernel. HLT-NAACL-2006. 
Zhang M., Zhang J., Su J. and Zhou G.D. 2006b. A 
Composite Kernel to Extract Relations between 
Entities with both Flat and Structured 
COLINGACL-2006: 825-832. 
Zhou G.D., Su J, Zhang J. and Zhang M. 2005. 
Exploring Various Knowledge in Relation 
Extraction. ACL-2005 
Zhou G.D., Zhang M., Ji D.H. and Zhu Q.M. 2007. Tree 
Kernel-based Relation Extraction with Context-
Sensitive Structured Parse Tree Information. 
EMNLP-CoNLL-2007 
71
Proceedings of NAACL-HLT 2013, pages 239?247,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Enforcing Subcategorization Constraints in a Parser Using Sub-parses
Recombining
Seyed Abolghasem Mirroshandel?,? Alexis Nasr? Beno??t Sagot
?Laboratoire d?Informatique Fondamentale de Marseille- CNRS - UMR 7279
Universite? Aix-Marseille, Marseille, France
Alpage, INRIA & Universite? Paris-Diderot, Paris, France
?Computer Engineering Department, Faculty of Engineering,
University of Guilan, Rasht, Iran
(ghasem.mirroshandel@lif.univ-mrs.fr, alexis.nasr@lif.univ-mrs.fr,
benoit.sagot@inria.fr)
Abstract
Treebanks are not large enough to adequately
model subcategorization frames of predica-
tive lexemes, which is an important source of
lexico-syntactic constraints for parsing. As
a consequence, parsers trained on such tree-
banks usually make mistakes when selecting
the arguments of predicative lexemes. In this
paper, we propose an original way to correct
subcategorization errors by combining sub-
parses of a sentence S that appear in the list
of the n-best parses of S. The subcatego-
rization information comes from three differ-
ent resources, the first one is extracted from
a treebank, the second one is computed on a
large corpora and the third one is an existing
syntactic lexicon. Experiments on the French
Treebank showed a 15.24% reduction of er-
roneous subcategorization frames (SF) selec-
tions for verbs as well as a relative decrease of
the error rate of 4% Labeled Accuracy Score
on the state of the art parser on this treebank.
1 Introduction
Automatic syntactic parsing of natural languages
has witnessed many important changes in the last
fifteen years. Among these changes, two have mod-
ified the nature of the task itself. The first one is
the availability of treebanks such as the Penn Tree-
bank (Marcus et al, 1993) or the French Treebank
(Abeille? et al, 2003), which have been used in the
parsing community to train stochastic parsers, such
as (Collins, 1997; Petrov and Klein, 2008). Such
work remained rooted in the classical language the-
oretic tradition of parsing, generally based on vari-
ants of generative context free grammars. The sec-
ond change occurred with the use of discriminative
machine learning techniques, first to rerank the out-
put of a stochastic parser (Collins, 2000; Charniak
and Johnson, 2005) and then in the parser itself (Rat-
naparkhi, 1999; Nivre et al, 2007; McDonald et al,
2005a). Such parsers clearly depart from classical
parsers in the sense that they do not rely anymore on
a generative grammar: given a sentence S, all pos-
sible parses for S1 are considered as possible parses
of S. A parse tree is seen as a set of lexico-syntactic
features which are associated to weights. The score
of a parse is computed as the sum of the weights of
its features.
This new generation of parsers allows to reach
high accuracy but possess their own limitations. We
will focus in this paper on one kind of weakness
of such parser which is their inability to properly
take into account subcategorization frames (SF) of
predicative lexemes2, an important source of lexico-
syntactic constraints. The proper treatment of SF is
actually confronted to two kinds of problems: (1)
the acquisition of correct SF for verbs and (2) the
integration of such constraints in the parser.
The first problem is a consequence of the use of
treebanks for training parsers. Such treebanks are
composed of a few thousands sentences and only a
small subpart of acceptable SF for a verb actually
1Another important aspect of the new parsing paradigm is
the use of dependency trees as a means to represent syntactic
structure. In dependency syntax, the number of possible syn-
tactic trees associated to a sentence is bounded, and only de-
pends on the length of the sentence, which is not the case with
syntagmatic derivation trees.
2We will concentrate in this paper on verbal SF.
239
occur in the treebank.
The second problem is a consequence of the pars-
ing models. For algorithmic complexity as well as
data sparseness reasons, the parser only considers
lexico-syntactic configurations of limited domain of
locality (in the parser used in the current work, this
domain of locality is limited to configurations made
of one or two dependencies). As described in more
details in section 2, SF often exceed in scope such
domains of locality and are therefore not easy to in-
tegrate in the parser. A popular method for intro-
ducing higher order constraints in a parser consist in
reranking the n best output of a parser as in (Collins,
2000; Charniak and Johnson, 2005). The reranker
search space is restricted by the output of the parser
and high order features can be used. One draw-
back of the reranking approach is that correct SF for
the predicates of a sentence can actually appear in
different parse trees. Selecting complete trees can
therefore lead to sub-optimal solutions. The method
proposed in this paper merges parts of different trees
that appear in an n best list in order to build a new
parse.
Taking into account SF in a parser has been a ma-
jor issue in the design of syntactic formalisms in the
eighties and nineties. Unification grammars, such
as Lexical Functional Grammars (Bresnan, 1982),
Generalized Phrase Structure Grammars (Gazdar et
al., 1985) and Head-driven Phrase Structure Gram-
mars (Pollard and Sag, 1994), made SF part of the
grammar. Tree Adjoining Grammars (Joshi et al,
1975) proposed to extend the domain of locality of
Context Free Grammars partly in order to be able
to represent SF in a generative grammar. More
recently, (Collins, 1997) proposed a way to intro-
duce SF in a probabilistic context free grammar and
(Arun and Keller, 2005) used the same technique
for French. (Carroll et al, 1998), used subcate-
gorization probabilities for ranking trees generated
by unification-based phrasal grammar and (Zeman,
2002) showed that using frame frequency in a de-
pendency parser can lead to a significant improve-
ment of the performance of the parser.
The main novelties of the work presented here is
(1) the way a new parse is built by combining sub-
parses that appear in the n best parse list and (2)
the use of three very different resources that list the
possible SF for verbs.
The organization of the paper is the following: in
section 2, we will briefly describe the parsing model
that we will be using for this work and give accuracy
results on a French corpus. Section 3 will describe
three different resources that we have been using to
correct SF errors made by the parser and give cov-
erage results for these resources on a development
corpus. Section 4 will propose three different ways
to take into account, in the parser, the resources de-
scribed in section 3 and give accuracy results. Sec-
tion 5 concludes the paper.
2 The Parser
The parser used in this work is the second order
graph based parser (McDonald et al, 2005b) imple-
mentation of (Bohnet, 2010). The parser was trained
on the French Treebank (Abeille? et al, 2003) which
was transformed into dependency trees by (Candito
et al, 2009). The size of the treebank and its de-
composition into train, development and test sets are
represented in table 1.
nb of sentences nb of tokens
TRAIN 9 881 278 083
DEV 1 239 36 508
TEST 1 235 36 340
Table 1: Size and decomposition of the French Treebank
The parser gave state of the art results for parsing
of French, reported in table 2. Table 2 reports the
standard Labeled Accuracy Score (LAS) and Unla-
beled Accuracy Score (UAS) which is the ratio of
correct labeled (for LAS) or unlabeled (for UAS) de-
pendencies in a sentence. We also defined a more
specific measure: the SF Accuracy Score (SAS)
which is the ratio of verb occurrences that have been
paired with the correct SF by the parser. We have
introduced this quantity in order to measure more
accurately the impact of the methods described in
this paper on the selection of a SF for the verbs of a
sentence.
TEST DEV
SAS 80.84 79.88
LAS 88.88 88.53
UAS 90.71 90.37
Table 2: Subcategorization Frame Accuracy, Labeled and
Unlabeled Accuracy Score on TEST and DEV.
240
We have chosen a second order graph parser in
this work for two reasons. The first is that it is the
parsing model that obtained the best results on the
French Treebank. The second is that it allows us
to impose structural constraints in the solution of
the parser, as described in (Mirroshandel and Nasr,
2011), a feature that will reveal itself precious when
enforcing SF in the parser output.
3 The Resources
Three resources have been used in this work in order
to correct SF errors. The first one has been extracted
from a treebank, the second has been extracted from
an automatically parsed corpus that is several order
of magnitude bigger than the treebank. The third one
has been extracted from an existing lexico-syntactic
resource. The three resources are respectively de-
scribed in sections 3.2, 3.3 and 3.4. Before describ-
ing the resources, we describe in details, in section
3.1 our definition of SF. In section 3.5, we evalu-
ate the coverage of these resources on the DEV cor-
pus. Coverage is an important characteristic of a re-
source: in case of an SF error made by the parser, if
the correct SF that should be associated to a verb, in
a sentence, does not appear in the resource, it will be
impossible to correct the error.
3.1 Subcat Frames Description
In this work, a SF is defined as a couple (G,L)
where G is the part of speech tag of the element that
licenses the SF. This part of speech tag can either
be a verb in infinitive form (VINF), a past participle
(VPP), a finite tense verb (V) or a present participle
(VPR). L is a set of couples (f, c) where f is a syn-
tactic function tag chosen among a set F and c is
a part of speech tag chosen among the set C. Cou-
ple (f, c) indicates that function f can be realized as
part of speech tag c. Sets F and C are respectively
displayed in top and bottom tables of figure 1. An
anchored SF (ASF) is a couple (v, S) where v is a
verb lemma and S is a SF, as described above.
A resource is defined as a collection of ASF
(v, S), each associated to a count c, to represent the
fact that verb v has been seen with SF S c times. In
the case of the resource extracted form an existing
lexicon (section 3.4), the notion of count is not ap-
plicable and we will consider that it is always equal
SUJ subject
OBJ object
A OBJ indirect object introduced by the preposition a`
DE OBJ indirect object introduced by the preposition de
P OBJ indirect object introduced by another preposition
ATS attribute of the subject
ATO attribute of the direct object
ADJ adjective
CS subordinating conjunction
N noun
V verb finite tense
VINF verb infinitive form
VPP verb past participle
VPR verb present participle
Figure 1: Syntactic functions of the arguments of the SF
(top table). Part of speech tags of the arguments of the SF
(bottom table)
to one.
Below is an example of three ASF for the french
verb donner (to give). The first one is a transitive SF
where both the subject and the object are realized as
nouns as in Jean donne un livre (Jean gives a book.).
The second one is ditransitive, it has both a direct
object and an indirect one introduced by the prepo-
sition a` as in Jean donne un livre a` Marie. (Jean
gives a book to Marie). The third one corresponds
to a passive form as in le livre est donne? a` Marie par
Jean (The book is given to Marie by Jean).
(donner,(V,(suj,N),(obj,N)))
(donner,(V,(suj,N),(obj,N),(a_obj,N)))
(donner,(VPP,(suj,N),(aux_pass,V),
(a_obj,N),(p_obj,N)))
One can note that when an argument corresponds
to an indirect dependent of the verb (introduced ei-
ther by a preposition or a subordinating conjunc-
tion), we do not represent in the SF, the category
of the element that introduces the argument, but the
category of the argument itself, a noun or a verb.
Two important choices have to be made when
defining SF. The first one concerns the dependents
of the predicative element that are in the SF (argu-
ment/adjunct distinction) and the second is the level
of abstraction at which SF are defined.
In our case, the first choice is constrained by the
treebank annotation guidelines. The FTB distin-
guishes seven syntactic functions which can be con-
sidered as arguments of a verb. They are listed in
the top table of figure 1. Most of them are straight-
241
forward and do not deserve an explanation. Some-
thing has to be said though on the syntactic function
P OBJ which is used to model arguments of the verb
introduced by a preposition that is neither a` nor de,
such as the agent in passive form, which is intro-
duced by the preposition par.
We have added in the SF two elements that do not
correspond to arguments of the verb: the reflexive
pronoun, and the passive auxiliary. The reason for
adding these elements to the SF is that their pres-
ence influences the presence or absence of some ar-
guments of the verb, and therefore the SF.
The second important choice that must be made
when defining SF is the level of abstraction, or, in
other words, how much the SF abstracts away from
its realization in the sentence. In our case, we have
used two ways to abstract away from the surface re-
alization of the SF. The first one is factoring sev-
eral part of speech tags. We have factored pronouns,
common nouns and proper nouns into a single cat-
egory N. We have not gathered verbs in different
modes into one category since the mode of the verb
influences its syntactic behavior and hence its SF.
The second means of abstraction we have used is
the absence of linear order between the arguments.
Taking into account argument order increases the
number of SF and, hence, data sparseness, without
adding much information for selecting the correct
SF, this is why we have decided to to ignore it. In
our second example above, each of the three argu-
ments can be realized as one out of eight parts of
speech that correspond to the part of speech tag N
and the 24 possible orderings are represented as one
canonical ordering. This SF therefore corresponds
to 12 288 possible realizations.
3.2 Treebank Extracted Subcat Frames
This resource has been extracted from the TRAIN
corpus. At a first glance, it may seen strange to ex-
tract data from the corpus that have been used for
training our parser. The reason is that, as seen in
section 1, SF are not directly modeled by the parser,
which only takes into account subtrees made of, at
most, two dependencies.
The extraction procedure of SF from the treebank
is straightforward : the tree of every sentence is vis-
ited and, for every verb of the sentence, its daughters
are visited, and, depending whether they are consid-
ered as arguments of the verb (with respect to the
conventions or section 3.1), they are added to the SF.
The number of different verbs extracted, as well as
the number of different SF and the average number
of SF per verb are displayed in table 3. Column T
(for Train) is the one that we are interested in here.
T L A0 A5 A10
nb of verbs 2058 7824 23915 4871 3923
nb of diff SF 666 1469 12122 2064 1355
avg. nb of SF 4.83 52.09 14.26 16.16 13.45
Table 3: Resources statistics
The extracted resource can directly be compared
with the TREELEX resource (Kupsc and Abeille?,
2008), which has been extracted from the same tree-
bank. The result that we obtain is different, due to
the fact that (Kupsc and Abeille?, 2008) have a more
abstract definition of SF. As a consequence, they de-
fine a smaller number of SF: 58 instead of 666 in
our case. The smaller number of SF yields a smaller
average number of SF per verb: 1.72 instead of 4.83
in our case.
3.3 Automatically computed Subcat Frames
The extraction procedure described above has been
used to extract ASF from an automatically parsed
corpus. The corpus is actually a collection of three
corpora of slightly different genres. The first one
is a collection of news reports of the French press
agency Agence France Presse, the second is a col-
lection of newspaper articles from a local French
newspaper : l?Est Re?publicain. The third one is
a collection of articles from the French Wikipedia.
The size of the different corpora are detailed in ta-
ble 4.
The corpus was first POS tagged with the MELT
tagger (Denis and Sagot, 2010), lemmatized with the
MACAON tool suite (Nasr et al, 2011) and parsed
in order to get the best parse for every sentence.
Then the ASF have been extracted.
The number of verbs, number of SF and average
number of SF per verb are represented in table 3,
in column A0 (A stands for Automatic). As one
can see, the number of verbs and SF are unrealis-
tic. This is due to the fact that the data that we ex-
tract SF from is noisy: it consists of automatically
produced syntactic trees which contain errors (recall
242
CORPUS Sent. nb. Tokens nb.
AFP 2 041 146 59 914 238
EST REP 2 998 261 53 913 288
WIKI 1 592 035 33 821 460
TOTAL 5 198 642 147 648 986
Table 4: sizes of the corpora used to collect SF
that the LAS on the DEV corpus is 88, 02%). There
are two main sources of errors in the parsed data: the
pre-processing chain (tokenization, part of speech
tagging and lemmatization) which can consider as
a verb a word that is not, and, of course, parsing
errors, which tend to create crazy SF. In order to
fight against noise, we have used a simple thresh-
olding: we only collect ASF that occur more than a
threshold i. The result of the thresholding appears
in columns A5 and A10 , where the subscript is the
value of the threshold. As expected both the number
of verbs and SF decrease sharply when increasing
the value of the threshold.
Extracting SF for verbs from raw data has been
an active direction of research for a long time, dat-
ing back at least to the work of (Brent, 1991) and
(Manning, 1993). More recently (Messiant et al,
2008) proposed such a system for French verbs. The
method we use for extracting SF is not novel with
respect to such work. Our aim was not to devise
new extraction techniques but merely to evaluate the
resource produced by such techniques for statistical
parsing.
3.4 Using an existing resource
The third resource that we have used is the Lefff
(Lexique des formes fle?chies du franc?ais ? Lexicon
of French inflected form), a large-coverage syntac-
tic lexicon for French (Sagot, 2010). The Lefff was
developed in a semi-automatic way: automatic tools
were used together with manual work. The latest
version of the Lefff contains 10,618 verbal entries
for 7,835 distinct verbal lemmas (the Lefff covers all
categories, but only verbal entries are used in this
work).
A sub-categorization frame consists in a list of
syntactic functions, using an inventory slightly more
fine-grained than in the French Treebank, and for
each of them a list of possible realizations (e.g.,
noun phrase, infinitive clause, or null-realization if
the syntactic function is optional).
For each verbal lemma, we extracted all sub-
categorization frames for each of the four verbal
part-of-speech tags (V, VINF, VPR, VPP), thus cre-
ating an inventory of SFs in the same sense and for-
mat as described in Section 3.1. Note that such SFs
do not contain alternatives concerning the way each
syntactic argument is realized or not: this extraction
process includes a de-factorization step. Its output,
hereafter L, contains 801,246 distinct (lemma, SF)
pairs.
3.5 Coverage
In order to be able to correct SF errors, the three
resources described above must possess two impor-
tant characteristics: high coverage and high accu-
racy. Coverage measures the presence, in the re-
source, of the correct SF of a verb, in a given sen-
tence. Accuracy measures the ability of a resource
to select the correct SF for a verb in a given context
when several ones are possible.
We will give in this section coverage result, com-
puted on the DEV corpus. Accuracy will be de-
scribed and computed in section 4. The reason why
the two measures are not described together is due
to the fact that coverage can be computed on a ref-
erence corpus while accuracy must be computed on
the output of a parser, since it is the parser that will
propose different SF for a verb in a given context.
Given a reference corpus C and a resource R,
two coverage measures have been computed, lexi-
cal coverage, which measures the ratio of verbs of C
that appear in R and syntactic coverage, which mea-
sures the ratio of ASF of C that appear in R. Two
variants of each measures are computed: on types
and on occurrences. The values of these measures
computed on the DEV corpus are summarized in ta-
ble 5.
T L A0 A5 A10
Lex. types 89.56 99.52 99.52 98.56 98.08
cov. occ 96.98 99.85 99.85 99.62 99.50
Synt. types 62.24 78.15 95.78 91.08 88.84
cov. occ 73.54 80.35 97.13 93.96 92.39
Table 5: Lexical and syntactic coverage of the three re-
sources on DEV
The figures of table 5 show that lexical cover-
age of the three resources is quite high, ranging
243
from 89.56 to 99.52 when computed on types and
from 96.98 to 99.85 when computed on occurrences.
The lowest coverage is obtained by the T resource,
which does not come as a surprise since it is com-
puted on a rather small number of sentences. It
is also interesting to note that lexical coverage of
A does not decrease much when augmenting the
threshold, while the size of the resource decreases
dramatically (as shown in table 3). This validates
the hypothesis that the resource is very noisy and
that a simple threshold on the occurrences of ASF is
a reasonable means to fight against noise.
Syntactic coverage is, as expected, lower than lex-
ical coverage. The best results are obtained by A0:
95.78 on types and 97.13 on occurrences. Thresh-
olding on the occurrences of anchored SF has a big-
ger impact on syntactic coverage than it had on lexi-
cal coverage. A threshold of 10 yields a coverage of
88.84 on types and 92.39 on occurrences.
4 Integrating Subcat Frames in the Parser
As already mentioned in section 1, SF usually ex-
ceed the domain of locality of the structures that are
directly modeled by the parser. It is therefore dif-
ficult to integrate directly SF in the model of the
parser. In order to circumvent the problem, we have
decided to work on the n-best output of the parser:
we consider that a verb v, in a given sentence S,
can be associated to any of the SF that v licenses in
one of the n-best trees. The main weakness of this
method is that an SF error can be corrected only if
the right SF appears at least in one of the n-best parse
trees.
In order to estimate an upper bound of the SAS
that such methods can reach (how many SF errors
can actually be corrected), we have computed the
oracle SAS on the 100 best trees of the DEV corpus
DEV (for how many verbs the correct SF appears
in at least one of the n-best parse trees). The oracle
score is equal to 95.16, which means that for 95.16%
of the verb occurrences of the DEV, the correct SF
appears somewhere in the 100-best trees. 95.16 is
therefore the best SAS that we can reach. Recall
that the baseline SAS is equal to 79.88% the room
for progress is therefore equal to 15.28% absolute.
Three experiments are described below. In the
first one, section 4.1, a simple technique, called Post
Processing is used. Section 4.2 describes a second
technique, called Double Parsing, which is a is a
refinement of Post Processing. Both sections 4.1
and 4.2 are based on single resources. Section 4.3
proposes a simple way to combine the different re-
sources.
4.1 Post Processing
The post processing method (PP) is the simplest one
that we have tested. It takes as input the different
ASF that occur in the n-best output of the parser as
well as a resource R. Given a sentence, let?s note
T1 . . . Tn the trees that appear in the n-best output
of the parser, in decreasing order of their score. For
every verb v of the sentence, we note S(v) the set
of all the SF associated to v that appear in the trees
T1 . . . Tn.
Given a verb v and a SF s, we define the following
functions:
C(v, s) is the number of occurrences of the ASF
(v, s) in the trees T1 . . . Tn.
F(v) is the SF associated to v in T1
CR(v, s) the number of occurrences of the ASF
(v, s) in the resource R.
We define a selection function as a function that
selects a SF for a given verb in a given sentence.
A selection function has to take into account the in-
formation given by the resource (whether an SF is
acceptable/frequent for a given verb) as well as the
information given by the parser (whether the parser
has a strong preference to associate a given SF to a
given verb).
In our experiments, we have tested two simple
selection functions. ?R which selects the first SF
s ? S(v), such that CR(v, s) > 0 when traversing
the trees T1 . . . Tn in the decreasing order of score
(best tree first).
The second function, ?R(v) compares the most
frequent SF for v in the resourceRwith the SF of the
first parse. If the ratio of the number of occurrences
in the n-best of the former and the latter is above a
threshold ?, the former is selected. More formally:
?R(v) =
?
????
????
s? = argmaxs?S(v) CR(v, s)
if C(v,s?)C(v,F(v)) > ?
F(v)
otherwise
244
The coefficient? has been optimized on DEV cor-
pus. Its value is equal to 2.5 for the Automatic re-
source, 2 for the Train resource and 1.5 for the Lefff.
The construction of the new solution proceeds as
follows: for every verb v of the sentence, a SF is se-
lected with the selection function. It is important to
note, at this point, that the SF selected for different
verbs of the sentence can pertain to different parse
trees. The new solution is built based on tree T1. For
every verb v, its arguments are potentially modified
in agreement with the SF selected by the selection
function. There is no guarantee at this point that the
solution is well formed. We will return to this prob-
lem in section 4.2.
We have evaluated the PP method with different
selection functions on the TEST corpus. The results
of applying function ?R were more successful. As
a result we just report the results of this function in
table 6. Different levels of thresholding for resource
A gave almost the same results, we therefore used
A10 which is the smallest one.
B T L A
SAS 80.84 83.11 82.14 82.17
LAS 88.88 89.14 89.03 89.03
UAS 90.71 90.91 90.81 90.82
Table 6: LAS and UAS on TEST using PP
The results of table 6 show two interesting facts.
First, the SAS is improved, it jumps from 80.84 to
83.11. PP therefore corrects some SF errors made
by the parser. It must be noted however that this im-
provement is much lower than the oracle score. The
second interesting fact is the very moderate increase
of both LAS and UAS. This is due to the fact that
the number of dependencies modified is small with
respect to the total number of dependencies. The
impact on LAS and UAS is therefore weak.
The best results are obtained with resource T . Al-
though the coverage of T is low, the resource is very
close to the train data, this fact probably explains the
good results obtained with this resource.
It is interesting, at this point, to compare our
method with a reranking approach. In order to do so,
we have compared the upper bound of the number of
SF errors that can be corrected when using rerank-
ing and our approach. The results of the comparison
computed on a list of 100 best trees is reported in
table 7 which shows the ratio of subcat frame errors
that could be corrected with a reranking approach
and the ratio of errors sub-parse recombining could
reach.
DEV TEST
reranking 53.9% 58.5%
sub-parse recombining 75.5% 76%
Table 7: Correction rate for subcat frames errors with dif-
ferent methods
Table 7 shows that combining sub-parses can, in
theory, correct a much larger number of wrong SF
assignments than reranking.
4.2 Double Parsing
The post processing method shows some improve-
ment over the baseline. But it has an important draw-
back: it can create inconsistent parses. Recall that
the parser we are using is based on a second order
model. In other words, the score of a dependency
depends on some neighboring dependencies. When
building a new solution, the post processing method
modifies some dependencies independently of their
context, which may give birth to very unlikely con-
figurations.
In order to compute a new optimal parse tree
that preserves the modified dependencies, we have
used a technique proposed in (Mirroshandel and
Nasr, 2011) that modifies the scoring function of the
parser in such a way that the dependencies that we
want to keep in the parser output get better scores
than all competing dependencies. The new solution
is therefore the optimal solution that preserves the
dependencies modified by the PP method.
The double parsing (DP) method is therefore a
three stage method. First, sentence S is parsed, pro-
ducing the n-best parses. Then, the post processing
method is used, modifying the first best parse. Let?s
note D the set of dependencies that were changed in
this process. In the last stage, a new parse is pro-
duced, that preserves D.
B T L A
SAS 80.84 83.11 82.14 82.17
LAS 88.88 89.30 89.25 89.31
UAS 90.71 91.07 91.05 91.08
Table 8: LAS and UAS on TEST using DP
245
The results of DP on TEST are reported in table
8. SAS did not change with respect to PP, because
DP keeps the SF selected by PP. As expected DP
does increase LAS and UAS. Recomputing an op-
timal solution therefore increases the quality of the
parses. Table 8 also shows that the three resources
get alost the same LAS and UAS although SAS is
better for resource T.
4.3 Combining Resources
Due to the different generation techniques of our
three resources, another direction of research is
combining them. We did different experiments con-
cerning all possible combination of resources: A and
L (AL), T and L (TL), T and A (TA), and all tree
(TAL) resources. The results of these combinations
for PP and DP methods are shown in tables 9 and
10, respectively.
The resource are combined in a back-off schema:
we search for a candidate ASF in a first resource. If
it is found, the search stops. Otherwise, the next re-
source(s) are probed. One question that arises is:
which sequence is the optimal one for combining
the resources. To answer this question, we did sev-
eral experiments on DEV set. Our experiments have
shown that it is better to search T resource, then
A, and, eventually, L. The results of this combining
method, using PP are reported in table 9. The best
results are obtained for the TL combination. The
SAS jumps from 83.11 to 83.76. As it was the case
with single resources, the LAS and UAS increase is
moderate.
B AL TL TA TAL
SAS 80.84 82.12 83.76 83.50 83.50
LAS 88.88 89.03 89.22 89.19 89.19
UAS 90.71 90.79 90.98 90.95 90.95
Table 9: LAS and UAS on TEST using PP with resource
combination
With DP (table 9), the order of resource combina-
tion is exactly the same as with PP. As was the case
with single resources, DP has a positive, but moder-
ate, impact on LAS and UAS.
The results of tables 9 and 10 do not show con-
siderable improvement over single resources. This
might be due to the large intersection between our
resources. In other words, they do not have comple-
mentary information, and their combination will not
B AL TL TA TAL
SAS 80.84 82.12 83.76 83.50 83.50
LAS 88.88 89.22 89.31 89.34 89.34
UAS 90.71 91.02 91.05 91.08 91.09
Table 10: LAS and UAS on TEST using DP with resource
combination
introduce much information. Another possible rea-
son for this result is the combination technique used.
More sophisticated techniques might yield better re-
sults.
5 Conclusions
Subcategorization frames for verbs constitute a rich
source of lexico-syntactic information which is hard
to integrate in graph based parsers. In this paper, we
have used three different resources for subcatego-
rization frames. These resources are from different
origins with various characteristics. We have pro-
posed two different methods to introduce the useful
information from these resources in a second order
model parser. We have conducted different exper-
iments on French Treebank that showed a 15.24%
reduction of erroneous SF selections for verbs. Al-
though encouraging, there is still plenty of room
for better results since the oracle score for 100 best
parses is equal to 95.16% SAS and we reached
83.76%. Future work will concentrate on more elab-
orate selection functions as well as more sophisti-
cated ways to combine the different resources.
Acknowledgments
This work has been funded by the French Agence
Nationale pour la Recherche, through the project
EDYLEX (ANR-08-CORD-009).
References
A. Abeille?, L. Cle?ment, and F. Toussenel. 2003. Building
a treebank for french. In Anne Abeille?, editor, Tree-
banks. Kluwer, Dordrecht.
A. Arun and F. Keller. 2005. Lexicalization in crosslin-
guistic probabilistic parsing: The case of french. In
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 306?313.
Association for Computational Linguistics.
B. Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of ACL, pages 89?97.
246
Michael Brent. 1991. Automatic acquisition of subcate-
gorization frames from untagged text. In Proceedings
of ACL.
Joan Bresnan, editor. 1982. The Mental Representation
of Grammatical Relations. MIT Press.
M. Candito, B. Crabbe?, P. Denis, and F. Gue?rin. 2009.
Analyse syntaxique du franc?ais : des constituants aux
de?pendances. In Proceedings of Traitement Automa-
tique des Langues Naturelles.
J. Carroll, G. Minnen, and T. Briscoe. 1998. Can sub-
categorisation probabilities help a statistical parser?
Arxiv preprint cmp-lg/9806013.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of ACL.
Michael Collins. 1997. Three Generative, Lexicalised
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the ACL.
Michael Collins. 2000. Discriminative Reranking for
Natural Language Parsing. In Proceedings of ICML.
P. Denis and B. Sagot. 2010. Exploitation d?une
ressource lexicale pour la construction d?un e?tiqueteur
morphosyntaxique e?tat-de-l?art du franc?ais. In Pro-
ceedings of Traitement Automatique des Langues Na-
turelles.
Gerald Gazdar, Ewan Klein, Geoffrey K. Pullum, and
Ivan Sag. 1985. Generalized Phrase Structure Gram-
mar. Harvard University Press.
Aravind Joshi, Leon Levy, and M Takahashi. 1975. Tree
adjunct grammars. Journal of Computer and System
Sciences, 10:136?163.
Anna Kupsc and Anne Abeille?. 2008. Treelex: A subcat-
egorisation lexicon for french verbs. In Proceedings of
the First International Conference on Global Interop-
erability for Language Resources.
Christopher Manning. 1993. Automatic acquisition of
a large subcategorization dictionary from corpora. In
Proceedings of ACL.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational linguistics,
19(2):313?330.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 91?98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proceedings of HLT-EMNLP,
pages 523?530.
C. Messiant, A. Korhonen, T. Poibeau, et al 2008.
Lexschem: A large subcategorization lexicon for
french verbs. In Proceedings of the Language Re-
sources and Evaluation Conference.
S.A. Mirroshandel and A. Nasr. 2011. Active learning
for dependency parsing using partially annotated sen-
tences. In Proceedings of International Conference on
Parsing Technologies.
A. Nasr, F. Be?chet, J-F. Rey, B. Favre, and Le Roux J.
2011. MACAON: An NLP tool suite for processing
word lattices. In Proceedings of ACL.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Kbler, S. Marinov, and E. Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Slav Petrov and Dan Klein. 2008. Discriminative Log-
Linear Grammars with Latent Variables. In J.C. Platt,
D. Koller, Y. Singer, and S. Roweis, editors, Advances
in Neural Information Processing Systems 20 (NIPS),
pages 1153?1160, Cambridge, MA. MIT Press.
Carl Pollard and Ivan Sag. 1994. Head-driven Phrase
Structure Grammmar. CSLI Series. University of
Chicago Press.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
learning, 34(1):151?175.
Beno??t Sagot. 2010. The Lefff, a freely available and
large-coverage morphological and syntactic lexicon
for french. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), pages 2744?2751, Valletta, Malta.
D. Zeman. 2002. Can subcategorization help a statistical
dependency parser? In Proceedings of the 19th in-
ternational conference on Computational linguistics-
Volume 1, pages 1?7. Association for Computational
Linguistics.
247
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777?785,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Semi-supervised Dependency Parsing using Lexical Affinities
Seyed Abolghasem Mirroshandel?,? Alexis Nasr? Joseph Le Roux
?Laboratoire d?Informatique Fondamentale de Marseille- CNRS - UMR 7279
Universite? Aix-Marseille, Marseille, France
LIPN, Universite? Paris Nord & CNRS,Villetaneuse, France
?Computer Engineering Department, Sharif university of Technology, Tehran, Iran
(ghasem.mirroshandel@lif.univ-mrs.fr, alexis.nasr@lif.univ-mrs.fr,
leroux@univ-paris13.fr)
Abstract
Treebanks are not large enough to reliably
model precise lexical phenomena. This de-
ficiency provokes attachment errors in the
parsers trained on such data. We propose
in this paper to compute lexical affinities,
on large corpora, for specific lexico-syntactic
configurations that are hard to disambiguate
and introduce the new information in a parser.
Experiments on the French Treebank showed
a relative decrease of the error rate of 7.1% La-
beled Accuracy Score yielding the best pars-
ing results on this treebank.
1 Introduction
Probabilistic parsers are usually trained on treebanks
composed of few thousands sentences. While this
amount of data seems reasonable for learning syn-
tactic phenomena and, to some extent, very frequent
lexical phenomena involving closed parts of speech
(POS), it proves inadequate when modeling lexical
dependencies between open POS, such as nouns,
verbs and adjectives. This fact was first recognized
by (Bikel, 2004) who showed that bilexical depen-
dencies were barely used in Michael Collins? parser.
The work reported in this paper aims at a better
modeling of such phenomena by using a raw corpus
that is several orders of magnitude larger than the
treebank used for training the parser. The raw cor-
pus is first parsed and the computed lexical affinities
between lemmas, in specific lexico-syntactic config-
urations, are then injected back in the parser. Two
outcomes are expected from this procedure, the first
is, as mentioned above, a better modeling of bilexi-
cal dependencies and the second is a method to adapt
a parser to new domains.
The paper is organized as follows. Section 2 re-
views some work on the same topic and highlights
their differences with ours. In section 3, we describe
the parser that we use in our experiments and give
a detailed description of the frequent attachment er-
rors. Section 4 describes how lexical affinities be-
tween lemmas are calculated and their impact is then
evaluated with respect to the attachment errors made
by the parser. Section 5 describes three ways to in-
tegrate the lexical affinities in the parser and reports
the results obtained with the three methods.
2 Previous Work
Coping with lexical sparsity of treebanks using raw
corpora has been an active direction of research for
many years.
One simple and effective way to tackle this prob-
lem is to put together words that share, in a large
raw corpus, similar linear contexts, into word clus-
ters. The word occurrences of the training treebank
are then replaced by their cluster identifier and a new
parser is trained on the transformed treebank. Us-
ing such techniques (Koo et al, 2008) report signi-
ficative improvement on the Penn Treebank (Marcus
et al, 1993) and so do (Candito and Seddah, 2010;
Candito and Crabbe?, 2009) on the French Treebank
(Abeille? et al, 2003).
Another series of papers (Volk, 2001; Nakov
and Hearst, 2005; Pitler et al, 2010; Zhou et al,
2011) directly model word co-occurrences. Co-
occurrences of pairs of words are first collected in a
777
raw corpus or internet n-grams. Based on the counts
produced, lexical affinity scores are computed. The
detection of pairs of words co-occurrences is gen-
erally very simple, it is either based on the direct
adjacency of the words in the string or their co-
occurrence in a window of a few words. (Bansal
and Klein, 2011; Nakov and Hearst, 2005) rely on
the same sort of techniques but use more sophisti-
cated patterns, based on simple paraphrase rules, for
identifying co-occurrences.
Our work departs from those approaches by the
fact that we do not extract the lexical information
directly on a raw corpus, but we first parse it and
then extract the co-occurrences on the parse trees,
based on some predetermined lexico-syntactic pat-
terns. The first reason for this choice is that the lin-
guistic phenomena that we are interested in, such as
as PP attachment, coordination, verb subject and ob-
ject can range over long distances, beyond what is
generally taken into account when working on lim-
ited windows. The second reason for this choice was
to show that the performances that the NLP commu-
nity has reached on parsing, combined with the use
of confidence measures allow to use parsers to ex-
tract accurate lexico-syntactic information, beyond
what can be found in limited annotated corpora.
Our work can also be compared with self train-
ing approaches to parsing (McClosky et al, 2006;
Suzuki et al, 2009; Steedman et al, 2003; Sagae
and Tsujii, 2007) where a parser is first trained on
a treebank and then used to parse a large raw cor-
pus. The parses produced are then added to the ini-
tial treebank and a new parser is trained. The main
difference between these approaches and ours is that
we do not directly add the output of the parser to the
training corpus, but extract precise lexical informa-
tion that is then re-injected in the parser. In the self
training approach, (Chen et al, 2009) is quite close
to our work: instead of adding new parses to the tree-
bank, the occurrence of simple interesting subtrees
are detected in the parses and introduced as new fea-
tures in the parser.
The way we introduce lexical affinity measures in
the parser, in 5.1, shares some ideas with (Anguiano
and Candito, 2011), who modify some attachments
in the parser output, based on lexical information.
The main difference is that we only take attachments
that appear in an n-best parse list into account, while
they consider the first best parse and compute all po-
tential alternative attachments, that may not actually
occur in the n-best forests.
3 The Parser
The parser used in this work is the second order
graph based parser (McDonald et al, 2005; Ku?bler
et al, 2009) implementation of (Bohnet, 2010). The
parser was trained on the French Treebank (Abeille?
et al, 2003) which was transformed into dependency
trees by (Candito et al, 2009). The size of the tree-
bank and its decomposition into train, development
and test sets is represented in table 1.
nb of sentences nb of words
FTB TRAIN 9 881 278 083
FTB DEV 1 239 36 508
FTB TEST 1 235 36 340
Table 1: Size and decomposition of the French Treebank
The part of speech tagging was performed with
the MELT tagger (Denis and Sagot, 2010) and lem-
matized with the MACAON tool suite (Nasr et al,
2011). The parser gave state of the art results for
parsing of French, reported in table 2.
pred. POS tags gold POS tags
punct no punct punct no punct
LAS 88.02 90.24 88.88 91.12
UAS 90.02 92.50 90.71 93.20
Table 2: Labeled and unlabeled accuracy score for auto-
matically predicted and gold POS tags with and without
taking into account punctuation on FTB TEST.
Figure 1 shows the distribution of the 100 most
common error types made by the parser. In this
figure, x axis shows the error types and y axis
shows the error ratio of the related error type
( number of errors of the specific typetotal number of errors ). We define an error
type by the POS tag of the governor and the POS
tag of the dependent. The figure presents a typical
Zipfian distribution with a low number of frequent
error types and a large number of unfrequent error
types. The shape of the curve shows that concen-
trating on some specific frequent errors in order to
increase the parser accuracy is a good strategy.
778
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 10  20  30  40  50  60  70  80  90  100
er
ro
r 
ra
ti
o
Error Type
Figure 1: Distribution of the types of errors
Table 3 gives a finer description of the most com-
mon types of error made by the parser. Here we
define more precise patterns for errors, where some
lexical values are specified (for prepositions) and, in
some cases, the nature of the dependency is taken
into account. Every line of the table corresponds to
one type of error. The first column describes the
error type. The second column indicates the fre-
quency of this type of dependency in the corpus. The
third one displays the accuracy for this type of de-
pendency (the number of dependencies of this type
correctly analyzed by the parser divided by the to-
tal number of dependencies of this type). The fourth
column shows the contribution of the errors made on
this type of dependency to the global error rate. The
last column associates a name with some of the error
types that will prove useful in the remainder of the
paper to refer to the error type.
Table 3 shows two different kinds of errors that
impact the global error rate. The first one concerns
very common dependencies that have a high accu-
racy but, due to their frequency, hurt the global er-
ror rate of the parser. The second one concerns low
frequency, low accuracy dependency types. Lines 2
and 3, respectively attachment of the preposition a` to
a verb and the subject dependency illustrate such a
contrast. They both impact the total error rate in the
same way (2.53% of the errors). But the first one
is a low frequency low accuracy type (respectively
0.88% and 69.11%) while the second is a high fre-
quency high accuracy type (respectively 3.43% and
93.03%). We will see in 4.2.2 that our method be-
haves quite differently on these two types of error.
dependency freq. acc. contrib. name
N?N 1.50 72.23 2.91
V? a` 0.88 69.11 2.53 VaN
V?suj? N 3.43 93.03 2.53 SBJ
N? CC 0.77 69.78 2.05 NcN
N? de 3.70 92.07 2.05 NdeN
V? de 0.66 74.68 1.62 VdeN
V?obj? N 2.74 90.43 1.60 OBJ
V? en 0.66 81.20 1.24
V? pour 0.46 67.78 1.10
N? ADJ 6.18 96.60 0.96 ADJ
N? a` 0.29 70.64 0.72 NaN
N? pour 0.12 38.64 0.67
N? en 0.15 47.69 0.57
Table 3: The 13 most common error types
4 Creating the Lexical Resource
The lexical resource is a collection of tuples
?C, g, d, s? where C is a lexico-syntactic configu-
ration, g is a lemma, called the governor of the
configuration, d is another lemma called the depen-
dent and s is a numerical value between 0 and 1,
called the lexical affinity score, which accounts for
the strength of the association between g and d in
the context C. For example the tuple ?(V, g)
obj
?
(N, d), eat , oyster , 0.23? defines a simple configu-
ration (V, g)
obj
? (N, d) that is an object depen-
dency between verb g and noun d. When replac-
ing variables g and d in C respectively with eat
and oyster , we obtain the fully specified lexico syn-
tactic pattern(V, eat)
obj
? (N, oyster), that we call
an instantiated configuration. The numerical value
0.23 accounts for how much eat and oyster like
to co-occur in the verb-object configuration. Con-
figurations can be of arbitrary complexity but they
have to be generic enough in order to occur fre-
quently in a corpus yet be specific enough to model
a precise lexico syntactic phenomenon. The context
(?, g)
?
? (?, d), for example is very generic but does
not model a precise linguistic phenomenon, as selec-
tional preferences of a verb, for example. Moreover,
configurations need to be error-prone. In the per-
spective of increasing a parser performances, there
is no point in computing lexical affinity scores be-
tween words that appear in a configuration for which
779
the parser never makes mistakes.
The creation of the lexical resource is a three stage
process. The first step is the definition of configura-
tions, the second one is the collection of raw counts
from the machine parsed corpora and the third one is
the computation of lexical affinities based on the raw
counts. The three steps are described in the follow-
ing subsection while the evaluation of the created
resource is reported in subsection 4.2.
4.1 Computing Lexical Affinities
A set of 9 configurations have been defined. Their
selection is a manual process based on the analysis
of the errors made by the parser, described in sec-
tion 3, as well as on the linguistic phenomena they
model. The list of the 9 configurations is described
in Table 4. As one can see on this table, configu-
rations are usually simple, made up of one or two
dependencies. Linguistically, configurations OBJ
and SBJ concern subject and object attachments,
configuration ADJ is related to attachments of ad-
jectives to nouns and configurations NdeN, VdeN,
VaN, and NaN indicate prepositional attachments.
We have restricted ourselves here to two common
French prepositions a` and de. Configurations NcN
and VcV deal respectively with noun and verb coor-
dination.
Name Description
OBJ (V, g)
obj
? (N, d)
SBJ (V, g)
subj
? (N, d)
ADJ (N, g) ? ADJ
NdeN (N, g) ? (P, de)? (N, d)
VdeN (V, g) ? (P, de)? (N, d)
NaN (N, g) ? (P, a`)? (N, d)
VaN (V, g) ? (P, a`)? (N, d)
NcN (N, g) ? (CC, ?)? (N, d)
VcV (V, g) ? (CC, ?)? (V, d)
Table 4: List of the 9 configurations.
The computation of the number of occurrences of
an instantiated configuration in the corpus is quite
straightforward, it consists in traversing the depen-
dency trees produced by the parser and detect the
occurrences of this configuration.
At the end of the counts collection, we have gath-
CORPUS Sent. nb. Tokens nb.
AFP 1 024 797 31 486 618
EST REP 1 103 630 19 635 985
WIKI 1 592 035 33 821 460
TOTAL 3 720 462 84 944 063
Table 5: sizes of the corpora used to gather lexical counts
ered for every lemma l its number of occurrences as
governor (resp. dependent) of configurationC in the
corpus, noted C(C, l, ?) (resp. C(C, ?, l)), as well as
the number of occurrences of configuration C with
lemma lg as a governor and lemma ld as a depen-
dent, noted C(C, lg, ld). We are now in a position
to compute the score s(C, lg, ld). This score should
reflect the tendency of lg and ld to appear together
in configuration C. It should be maximal if when-
ever lg occurs as the governor of configuration C,
the dependent position is occupied by ld and, sym-
metrically, if whenever ld occurs as the dependent of
configuration C, the governor position is occupied
by lg. A function that conforms such a behavior is
the following:
s(C, lg, ld) =
1
2
(
C(C, lg, ld)
C(C, lg, ?)
+
C(C, lg, ld)
C(C, ?, ld)
)
it takes its values between 0 (lg and ld never
co-occur) and 1 (g and d always co-occur). This
function is close to pointwise mutual information
(Church and Hanks, 1990) but takes its values be-
tween 0 and 1.
4.2 Evaluation
Lexical affinities were computed on three corpora of
slightly different genres. The first one, is a collection
of news report of the French press agency Agence
France Presse, the second is a collection of news-
paper articles from a local French newspaper : l?Est
Re?publicain. The third one is a collection of articles
from the French Wikipedia. The size of the different
corpora are detailed in table 5. The corpus was first
POS tagged, lemmatized and parsed in order to get
the 50 best parses for every sentence. Then the lexi-
cal resource was built, based on the 9 configurations
described in table 4.
The lexical resource has been evaluated on
FTB DEV with respect to two measures: coverage
780
and correction rate, described in the next two sec-
tions.
4.2.1 Coverage
Coverage measures the instantiated configura-
tions present in the evaluation corpus that are in the
resource. The results are presented in table 6. Every
line represents a configuration, the second column
indicates the number of different instantiations of
this configuration in the evaluation corpus, the third
one indicates the number of instantiated configura-
tions that were actually found in the lexical resource
and the fourth column shows the coverage for this
configuration, which is the ratio third column over
the second. Last column represents the coverage of
the training corpus (the lexical resource is extracted
on the training corpus) and the last line represents
the same quantities computed on all configurations.
Table 6 shows two interesting results: firstly the
high variability of coverage with respect to configu-
rations, and secondly the low coverage when the lex-
ical resource is computed on the training corpus, this
fact being consistent with the conclusions of (Bikel,
2004). A parser trained on a treebank cannot be ex-
pected to reliably select the correct governor in lex-
ically sensitive cases.
Conf. occ. pres. cov. T cov.
OBJ 1017 709 0.70 0.21
SBJ 1210 825 0.68 0.24
ADJ 1791 1239 0.69 0.33
NdeN 1909 1287 0.67 0.31
VdeN 189 107 0.57 0.16
NaN 123 61 0.50 0.20
VaN 422 273 0.65 0.23
NcN 220 55 0.25 0.10
VcV 165 93 0.56 0.04
? 7046 4649 0.66 0.27
Table 6: Coverage of the lexical resource over FTB DEV.
4.2.2 Correction Rate
While coverage measures how many instantiated
configurations that occur in the treebank are actu-
ally present in the lexical resource, it does not mea-
sure if the information present in the lexical resource
can actually help correcting the errors made by the
parser.
We define Correction Rate (CR) as a way to ap-
proximate the usefulness of the data. Given a word
d present in a sentence S and a configuration C, the
set of all potential governors of d in configuration
C, in all the n-best parses produced by the parser is
computed. This set is noted G = {g1, . . . , gj}. Let
us note GL the element of G that maximizes the lex-
ical affinity score. When the lexical resource gives
no score to any of the elements of G, GL is left un-
specified.
Ideally, G should not be the set of governors in
the n-best parses but the set of all possible governors
for d in sentence S. Since we have no simple way
to compute the latter, we will content ourselves with
the former as an approximation of the latter.
Let us note GH the governor of d in the (first)
best parse produced and GR the governor of d in the
correct parse. CR measures the effect of replacing
GH with GL.
We have represented in table 7 the different sce-
narios that can happen when comparing GH , GR
and GL.
GL = GR or GL unspec. CC
GH = GR GL 6= GR CE
GL = GR EC
GH 6= GR GL 6= GR or GL unspec. EE
GR /? G NA
Table 7: Five possible scenarios when comparing the
governor of a word produced by the parser (GH ), in
the reference parse (GR) and according to the lexical re-
source (GL).
In scenarios CC and CE, the parser did not make
a mistake (the first letter, C, stands for correct). In
scenario CC, the lexical affinity score was compat-
ible with the choice of the parser or the lexical re-
source did not select any candidate. In scenario CE,
the lexical resource introduced an error. In scenar-
ios EC and EE, the parser made an error. In EC,
the error was corrected by the lexical resource while
in EE, it wasn?t. Either because the lexical resource
candidate was not the correct governor or it was un-
specified. The last case, NA, indicates that the cor-
rect governor does not appear in any of the n-best
parses. Technically this case could be integrated in
EE (an error made by the parser was not corrected
by the lexical resource) but we chose to keep it apart
781
since it represents a case where the right solution
could not be found in the n-best parse list (the cor-
rect governor is not a member of set G).
Let?s note nS the number of occurrences of sce-
nario S for a given configuration. We compute CR
for this configuration in the following way:
CR =
old error number - new error number
old error number
=
nEC ? nCE
nEE + nEC + nNA
When CR is equal to 0, the correction did not have
any impact on the error rate. When CR> 0, the error
rate is reduced and if CR < 0 it is increased1.
CR for each configuration is reported in table 8.
The counts of the different scenarios have also been
reported.
Conf. nCC nCE nEC nEE nNA CR
OBJ 992 30 51 5 17 0.29
SBJ 1131 35 61 16 34 0.23
ADJ 2220 42 16 20 6 -0.62
NdeN 2083 93 42 44 21 -0.48
VdeN 150 2 49 1 13 0.75
NaN 89 5 21 10 2 0.48
VaN 273 19 132 8 11 0.75
NcN 165 17 12 31 12 -0.09
VcN 120 21 14 11 5 -0.23
? 7223 264 398 146 121 0.20
Table 8: Correction Rate of the lexical resource with re-
spect to FTB DEV.
Table 8 shows very different results among con-
figurations. Results for PP attachments VdeN, VaN
and NaN are quite good (a CR of 75% for a given
configuration, as VdeN indicates that the number of
errors on such a configuration is decreased by 25%).
It is interesting to note that the parser behaves quite
badly on these attachments: their accuracy (as re-
ported in table 3) is, respectively 74.68, 69.1 and
70.64. Lexical affinity helps in such cases. On
the other hand, some attachments like configuration
ADJ and NdeN, for which the parser showed very
good accuracy (96.6 and 92.2) show very poor per-
formances. In such cases, taking into account lexical
affinity creates new errors.
1One can note, that contrary to coverage, CR does not mea-
sure a characteristic of the lexical resource alone, but the lexical
resource combined with a parser.
On average, using the lexical resource with this
simple strategy of systematically replacing GH with
GL allows to decrease by 20% the errors made on
our 9 configurations and by 2.5% the global error
rate of the parser.
4.3 Filtering Data with Ambiguity Threshold
The data used to extract counts is noisy: it con-
tains errors made by the parser. Ideally, we would
like to take into account only non ambiguous sen-
tences, for which the parser outputs a single parse
hypothesis, hopefully the good one. Such an ap-
proach is obviously doomed to fail since almost ev-
ery sentence will be associated to several parses.
Another solution would be to select sentences for
which the parser has a high confidence, using confi-
dence measures as proposed in (Sa?nchez-Sa?ez et al,
2009; Hwa, 2004). But since we are only interested
in some parts of sentences (usually one attachment),
we don?t need high confidence for the whole sen-
tence. We have instead used a parameter, defined on
single dependencies, called the ambiguity measure.
Given the n best parses of a sentence and a depen-
dency ?, present in at least one of the n best parses,
let us note C(?) the number of occurrences of ? in
the n best parse set. We note AM(?) the ambiguity
measure associated to ?. It is computed as follows:
AM(?) = 1?
C(?)
n
An ambiguity measure of 0 indicates that ? is non
ambiguous in the set of the n best parses (the word
that constitutes the dependent in ? is attached to the
word that constitutes the governor in ? in all the n-
best analyses). When n gets large enough this mea-
sure approximates the non ambiguity of a depen-
dency in a given sentence.
Ambiguity measure is used to filter the data when
counting the number of occurrences of a configura-
tion: only occurrences that are made of dependen-
cies ? such that AM(?) ? ? are taken into account.
? is called the ambiguity threshold.
The results of coverage and CR given above were
computed for ? equal to 1, which means that, when
collecting counts, all the dependencies are taken into
account whatever their ambiguity is. Table 9 shows
coverage and CR for different values of ? . As ex-
pected, coverage decreases with ? . But, interest-
782
ingly, decreasing ? , from 1 down to 0.2 has a posi-
tive influence on CR. Ambiguity threshold plays the
role we expected: it allows to reduce noise in the
data, and corrects more errors.
? = 1.0 ? = 0.4 ? = 0.2 ? = 0.0
cov/CR cov/CR cov/CR cov/CR
OBJ 0.70/0.29 0.58/0.36 0.52/0.36 0.35/0.38
SBJ 0.68/0.23 0.64/0.23 0.62/0.23 0.52/0.23
ADJ 0.69/-0.62 0.61/-0.52 0.56/-0.52 0.43/-0.38
NdeN 0.67/-0.48 0.58/-0.53 0.52/-0.52 0.38/-0.41
VdeN 0.57/0.75 0.44/0.73 0.36/0.73 0.20/0.30
NaN 0.50/0.48 0.34/0.42 0.28/0.45 0.15/0.48
VaN 0.65/0.75 0.50/0.8 0.41/0.80 0.26/0.48
NcN 0.25/-0.09 0.19/0 0.16/0.02 0.07/0.13
VcV 0.56/-0.23 0.42/-0.07 0.28/0.03 0.08/0.07
Avg 0.66/0.2 0.57/0.23 0.51/0.24 0.38/0.17
Table 9: Coverage and Correction Rate on FTB DEV for
several values of ambiguity threshold.
5 Integrating Lexical Affinity in the Parser
We have devised three methods for taking into ac-
count lexical affinity scores in the parser. The first
two are post-processing methods, that take as input
the n-best parses produced by the parser and mod-
ify some attachments with respect to the information
given by the lexical resource. The third method in-
troduces the lexical affinity scores as new features in
the parsing model. The three methods are described
in 5.1, 5.2 and 5.3. They are evaluated in 5.4.
5.1 Post Processing Method
The post processing method is quite simple. It is
very close to the method that was used to compute
the Correction Rate of the lexical resource, in 4.2.2:
it takes as input the n-best parses produced by the
parser and, for every configuration occurrence C
found in the first best parse, the set (G) of all po-
tential governors of C, in the n-best parses, is com-
puted and among them, the word that maximizes the
lexical affinity score (GL) is identified.
Once GL is identified, one can replace the choice
of the parser (GH ) with GL. This method is quite
crude since it does not take into account the confi-
dence the parser has in the solution proposed. We
observed, in 4.2.2 that CR was very low for configu-
rations for which the parser achieves good accuracy.
In order to introduce the parser confidence in the fi-
nal choice of a governor, we compute C(GH) and
C(GL) which respectively represent the number of
times GH and GL appear as the governor of config-
uration C. The choice of the final governor, noted
G?, depends on the ratio of C(GH) and C(GL). The
complete selection strategy is the following:
1. if GH = GL or GL is unspecified, G? = GH .
2. if GH 6= GL, G? is determined as follows:
G? =
{
GH if
C(GH)
C(GL)
> ?
GL otherwise
where ? is a coefficient that is optimized on the
development data set.
We have reported, in table 10 the values of CR,
for the 9 different features, using this strategy, for
? = 1. We do not report the values of CR for other
values of ? since they are very close to each other.
The table shows several noticeable facts. First, the
new strategy performs much better than the former
one (crudely replacing GH by GL), the value of CR
increased from 0.2 to 0.4, which means that the er-
rors made on the nine configurations are now de-
creased by 40%. Second, CR is now positive for ev-
ery configuration: the number of errors is decreased
for every configuration.
Conf. OBJ SUJ ADJ NdeN VdeN
CR 0.45 0.46 0.14 0.05 0.73
Conf. NaN VaN NcN VcV ?
CR 0.12 0.8 0.12 0.1 0.4
Table 10: Correction Rate on FTB DEV when taking into
account parser confidence.
5.2 Double Parsing Method
The post processing method performs better than the
naive strategy that was used in 4.2.2. But it has an
important drawback: it creates inconsistent parses.
Recall that the parser we are using is based on a sec-
ond order model, which means that the score of a de-
pendency depends on some neighboring ones. Since
with the post processing method only a subset of the
dependencies are modified, the resulting parse is in-
consistent: the score of some dependencies is com-
puted on the basis of other dependencies that have
been modified.
783
In order to compute a new optimal parse tree
that preserves the modified dependencies, we have
used a technique proposed in (Mirroshandel and
Nasr, 2011) that modifies the scoring function of the
parser in such a way that the dependencies that we
want to keep in the parser output get better scores
than all competing dependencies.
The double parsing method is therefore a three
stage method. First, sentence S is parsed, producing
the n-best parses. Then, the post processing method
is used, modifying the first best parse. Let?s note
D the set of dependencies that were changed in this
process. In the last stage, a new parse is produced,
that preserves D.
5.3 Feature Based Method
In the feature based method, new features are
added to the parser that rely on lexical affinity
scores. These features are of the following form:
?C, lg, ld, ?C(s)?, where C is a configuration num-
ber, s is the lexical affinity score (s = s(C, lg, ld))
and ?c(?) is a discretization function.
Discretization of the lexical affinity scores is nec-
essary in order to fight against data sparseness. In
this work, we have used Weka software (Hall et al,
2009) to discretize the scores with unsupervised bin-
ning. Binning is a simple process which divides
the range of possible values a parameter can take
into subranges called bins. Two methods are im-
plemented in Weka to find the optimal number of
bins: equal-frequency and equal-width. In equal-
frequency binning, the range of possible values are
divided into k bins, each of which holds the same
number of instances. In equal-width binning, which
is the method we have used, the range are divided
into k subranges of the same size. The optimal num-
ber of bins is the one that minimizes the entropy of
the data. Weka computes different number of bins
for different configurations, ranging from 4 to 10.
The number of new features added to the parser is
equal to
?
C B(C) where C is a configuration and
B(C) is the number of bins for configuration C.
5.4 Evaluation
The three methods described above have been evalu-
ated on FTB TEST. Results are reported in table 11.
The three methods outperformed the baseline (the
state of the art parser for French which is a second
order graph based method) (Bohnet, 2010). The best
performances were obtained by the Double Parsing
method that achieved a labeled relative error reduc-
tion of 7, 1% on predicted POS tags, yielding the
best parsing results on the French Treebank. It per-
forms better than the Post Processing method, which
means that the second parsing stage corrects some
inconsistencies introduced in the Post Processing
method. The performances of the Feature Based
method are disappointing, it achieves an error reduc-
tion of 1.4%. This result is not easy to interpret. It
is probably due to the limited number of new fea-
tures introduced in the parser. These new features
probably have a hard time competing with the large
number of other features in the training process.
pred. POS tags gold POS tags
punct no punct punct no punct
BL LAS 88.02 90.24 88.88 91.12
UAS 90.02 92.50 90.71 93.20
PP LAS 88.45 90.73 89.46 91.78
UAS 90.61 93.20 91.44 93.86
DP LAS 88.87 91.10 89.72 91.90
UAS 90.84 93.30 91.58 93.99
FB LAS 88.19 90.33 89.29 91.43
UAS 90.22 92.62 91.09 93.46
Table 11: Parser accuracy on FTB TEST using the
standard parser (BL) the post processing method (PP),
the double parsing method (DP) and the feature based
method.
6 Conclusion
Computing lexical affinities, on large corpora, for
specific lexico-syntactic configurations that are hard
to disambiguate has shown to be an effective way
to increase the performances of a parser. We have
proposed in this paper one method to compute lexi-
cal affinity scores as well as three ways to introduce
this new information in a parser. Experiments on a
French corpus showed a relative decrease of the er-
ror rate of 7.1% Labeled Accuracy Score.
Acknowledgments
This work has been funded by the French Agence
Nationale pour la Recherche, through the projects
SEQUOIA (ANR-08-EMER-013) and EDYLEX
(ANR-08-CORD-009).
784
References
A. Abeille?, L. Cle?ment, and F. Toussenel. 2003. Building
a treebank for french. In Anne Abeille?, editor, Tree-
banks. Kluwer, Dordrecht.
E.H. Anguiano and M. Candito. 2011. Parse correction
with specialized models for difficult attachment types.
In Proceedings of EMNLP.
M. Bansal and D. Klein. 2011. Web-scale features for
full-scale parsing. In Proceedings of ACL, pages 693?
702.
D. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
B. Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of ACL, pages 89?97.
M. Candito and B. Crabbe?. 2009. Improving generative
statistical parsing with semi-supervised word cluster-
ing. In Proceedings of the 11th International Confer-
ence on Parsing Technologies, pages 138?141.
M. Candito and D. Seddah. 2010. Parsing word clusters.
In Proceedings of the NAACL HLT Workshop on Sta-
tistical Parsing of Morphologically-Rich Languages,
pages 76?84.
M. Candito, B. Crabbe?, P. Denis, and F. Gue?rin. 2009.
Analyse syntaxique du franc?ais : des constituants aux
de?pendances. In Proceedings of Traitement Automa-
tique des Langues Naturelles.
W. Chen, J. Kazama, K. Uchimoto, and K. Torisawa.
2009. Improving dependency parsing with subtrees
from auto-parsed data. In Proceedings of EMNLP,
pages 570?579.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational linguistics, 16(1):22?29.
P. Denis and B. Sagot. 2010. Exploitation d?une
ressource lexicale pour la construction d?un e?tiqueteur
morphosyntaxique e?tat-de-l?art du franc?ais. In Pro-
ceedings of Traitement Automatique des Langues Na-
turelles.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The WEKA data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
R. Hwa. 2004. Sample selection for statistical parsing.
Computational Linguistics, 30(3):253?276.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proceedings of the
ACL HLT, pages 595?603.
S. Ku?bler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Synthesis Lectures on Human Lan-
guage Technologies, 1(1):1?127.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational linguistics,
19(2):313?330.
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In Proceedings of
HLT NAACL, pages 152?159.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In Proceedings of HLT-EMNLP,
pages 523?530.
S.A. Mirroshandel and A. Nasr. 2011. Active learning
for dependency parsing using partially annotated sen-
tences. In Proceedings of International Conference on
Parsing Technologies.
P. Nakov and M. Hearst. 2005. Using the web as an
implicit training set: application to structural ambigu-
ity resolution. In Proceedings of HLT-EMNLP, pages
835?842.
A. Nasr, F. Be?chet, J-F. Rey, B. Favre, and Le Roux J.
2011. MACAON: An NLP tool suite for processing
word lattices. In Proceedings of ACL.
E. Pitler, S. Bergsma, D. Lin, and K. Church. 2010. Us-
ing web-scale N-grams to improve base NP parsing
performance. In Proceedings of COLING, pages 886?
894.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with lr models and parser ensem-
bles. In Proceedings of the CoNLL shared task session
of EMNLP-CoNLL, volume 7, pages 1044?1050.
R. Sa?nchez-Sa?ez, J.A. Sa?nchez, and J.M. Bened??. 2009.
Statistical confidence measures for probabilistic pars-
ing. In Proceedings of RANLP, pages 388?392.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of EACL, pages 331?338.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In Proceed-
ings of EMNLP, pages 551?560.
M. Volk. 2001. Exploiting the WWW as a corpus to
resolve PP attachment ambiguities. In Proceedings of
Corpus Linguistics.
G. Zhou, J. Zhao, K. Liu, and L. Cai. 2011. Exploiting
web-derived selectional preference to improve statisti-
cal dependency parsing. In Proceedings of HLT-ACL,
pages 1556?1565.
785
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 89?99,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Generative Constituent Parsing and Discriminative Dependency Reranking:
Experiments on English and French
Joseph Le Roux Beno?t Favre? Alexis Nasr? Seyed Abolghasem Mirroshandel?,?
LIPN, Universit? Paris Nord ? CNRS UMR 7030, Villetaneuse, France
?LIF, Universit? Aix-Marseille ? CNRS UMR 7279, Marseille, France
?Computer Engineering Department, Sharif university of Technology, Tehran, Iran
leroux@univ-paris13.fr, benoit.favre@lif.univ-mrs.fr,
alexis.nasr@lif.univ-mrs.fr, ghasem.mirroshandel@lif.univ-mrs.fr
Abstract
We present an architecture for parsing in two
steps. A phrase-structure parser builds for
each sentence an n-best list of analyses which
are converted to dependency trees. These de-
pendency structures are then rescored by a dis-
criminative reranker. Our method is language
agnostic and enables the incorporation of ad-
ditional information which are useful for the
choice of the best parse candidate. We test
our approach on the the Penn Treebank and
the French Treebank. Evaluation shows a sig-
nificative improvement on different parse met-
rics.
1 Introduction
Two competing approaches exist for parsing natural
language. The first one, called generative, is based
on the theory of formal languages and rewriting sys-
tems. Parsing is defined here as a process that trans-
forms a string into a tree or a tree forest. It is of-
ten grounded on phrase-based grammars ? although
there are generative dependency parsers ? in partic-
ular context-free grammars or one of their numer-
ous variants, that can be parsed in polynomial time.
However, the independence hypothesis that under-
lies this kind of formal system does not allow for
precise analyses of some linguistic phenomena, such
as long distance and lexical dependencies.
In the second approach, known as discriminative,
the grammar is viewed as a system of constraints
over the correct syntactic structures, the words of the
sentence themselves being seen as constraints over
the position they occupy in the sentence. Parsing
boils down to finding a solution that is compatible
with the different constraints. The major problem of
this approach lies in its complexity. The constraints
can, theoretically, range over any aspect of the final
structures, which prevents from using efficient dy-
namic programming techniques when searching for
a global solution. In the worst case, final structures
must be enumerated in order to be evaluated. There-
fore, only a subset of constraints is used in imple-
mentations for complexity reasons. This approach
can itself be divided into formalisms relying on logic
to describe constraints, as the model-theoretic syn-
tax (Pullum and Scholz, 2001), or numerical for-
malisms that associate weights to lexico-syntactic
substructures. The latter has been the object of some
recent work thanks to progresses achieved in the
field of Machine Learning. A parse tree is repre-
sented as a vector of features and its accuracy is
measured as the distance between this vector and the
reference.
One way to take advantage of both approaches
is to combine them sequentially, as initially pro-
posed by Collins (2000). A generative parser pro-
duces a set of candidates structures that constitute
the input of a second, discriminative module, whose
search space is limited to this set of candidates.
Such an approach, parsing followed by reranking,
is used in the Brown parser (Charniak and Johnson,
2005). The approach can be extended in order to
feed the reranker with the output of different parsers,
as shown by (Johnson and Ural, 2010; Zhang et al,
2009).
In this paper we are interested in applying rerank-
ing to dependency structures. The main reason is
that many linguistic constraints are straightforward
to implement on dependency structures, as, for ex-
ample, subcategorization frames or selectional con-
straints that are closely linked to the notion of de-
89
pendents of a predicate. On the other hand, depen-
dencies extracted from constituent parses are known
to be more accurate than dependencies obtained
from dependency parsers. Therefore the solution we
choose is an indirect one: we use a phrase-based
parser to generate n-best lists and convert them to
lists of dependency structures that are reranked. This
approach can be seen as trade-off between phrase-
based reranking experiments (Collins, 2000) and the
approach of Carreras et al (2008) where a discrimi-
native model is used to score lexical features repre-
senting unlabelled dependencies in the Tree Adjoin-
ing Grammar formalism.
Our architecture, illustrated in Figure 1, is based
on two steps. During the first step, a syntagmatic
parser processes the input sentence and produces n-
best parses as well as their probabilities. They are
annotated with a functional tagger which tags syn-
tagms with standard syntactic functions subject, ob-
ject, indirect object . . . and converted to dependency
structures by application of percolation rules. In the
second step, we extract a set of features from the
dependency parses and the associated probabilities.
These features are used to reorder the n-best list
and select a potentially more accurate parse. Syn-
tagmatic parses are produced by the implementation
of a PCFG-LA parser of (Attia et al, 2010), simi-
lar to (Petrov et al, 2006), a functional tagger and
dependency converter for the target language. The
reranking model is a linear model trained with an
implementation of the MIRA algorithm (Crammer et
al., 2006)1.
Charniak and Johnson (2005) and Collins (2000)
rerank phrase-structure parses and they also include
head-dependent information, in other words unla-
belled dependencies. In our approach we take into
account grammatical functions or labelled depen-
dencies.
It should be noted that the features we use are very
generic and do not depend on the linguistic knowl-
edge of the authors. We applied our method to En-
glish, the de facto standard for testing parsing tech-
nologies, and French which exhibits many aspects of
a morphologically rich language. But our approach
could be applied to other languages, provided that
1This implementation is available at https://github.
com/jihelhere/adMIRAble.
the resources ? treebanks and conversion tools ? ex-
ist.
(1) PCFG-LA n-best constituency parses
(2) Function annotation
(3) Conversion to dependency parses
(4) Feature extraction
(5) MIRA reranking
w
Final constituency & dependency parse
Input text
Figure 1: The parsing architecture: production of the n-
best syntagmatic trees (1) tagged with functional labels
(2), conversion to a dependency structure (3) and feature
extraction (4), scoring with a linear model (5). The parse
with the best score is considered as final.
The structure of the paper is the following: in
Section 2 we describe the details of our generative
parser and in Section 3 our reranking model together
with the features templates. Section 4 reports the re-
sults of the experiments conducted on the Penn Tree-
bank (Marcus et al, 1994) as well as on the Paris 7
Treebank (Abeill? et al, 2003) and Section 5 con-
cludes the paper.
2 Generative Model
The first part of our system, the syntactic analysis
itself, generates surface dependency structures in a
sequential fashion (Candito et al, 2010b; Candito
et al, 2010a). A phrase structure parser based on
Latent Variable PCFGs (PCFG-LAs) produces tree
structures that are enriched with functions and then
converted to labelled dependency structures, which
will be processed by the parse reranker.
90
2.1 PCFG-LAs
Probabilistic Context Free Grammars with Latent
Annotations, introduced in (Matsuzaki et al, 2005)
can be seen as automatically specialised PCFGs
learnt from treebanks. Each symbol of the gram-
mar is enriched with annotation symbols behaving
as subclasses of this symbol. More formally, the
probability of an unannotated tree is the sum of the
probabilities of its annotated counterparts. For a
PCFG-LA G, R is the set of annotated rules, D(t)
is the set of (annotated) derivations of an unanno-
tated tree t, and R(d) is the set of rules used in a
derivation d. Then the probability assigned by G to
t is:
PG(t) =
?
d?D(t)
PG(d) =
?
d?D(t)
?
r?R(d)
PG(r) (1)
Because of this alternation of sums and products
that cannot be optimally factorised, there is no ex-
act polynomial dynamic programming algorithm for
parsing. Matsuzaki et al (2005) and Petrov and
Klein (2007) discuss approximations of the decod-
ing step based on a Bayesian variational approach.
This enables cubic time decoding that can be fur-
ther enhanced with coarse-to-fine methods (Char-
niak and Johnson, 2005).
This type of grammars has already been tested
on a variety of languages, in particular English
and French, giving state-of-the-art results. Let us
stress that this phrase-structure formalism is not lex-
icalised as opposed to grammars previously used in
reranking experiments (Collins, 2000; Charniak and
Johnson, 2005). The notion of lexical head is there-
fore absent at parsing time and will become avail-
able only at the reranking step.
2.2 Dependency Structures
A syntactic theory can either be expressed with
phrase structures or dependencies, as advocated for
in (Rambow, 2010). However, some information
may be simpler to describe in one of the representa-
tions. This equivalence between the modes of repre-
sentations only stands if the informational contents
are the same. Unfortunately, this is not the case
here because the phrase structures that we use do
not contain functional annotations and lexical heads,
whereas labelled dependencies do.
This implies that, in order to be converted
into labelled dependency structures, phrase struc-
ture parses must first be annotated with functions.
Previous experiments for English and French as
well (Candito et al, 2010b) showed that a sequential
approach is better than an integrated one for context-
free grammars, because the strong independence hy-
pothesis of this formalism implies a restricted do-
main of locality which cannot express the context
needed to properly assign functions. Most func-
tional taggers, such as the ones used in the following
experiments, rely on classifiers whose feature sets
can describe the whole context of a node in order to
make a decision.
3 Discriminative model
Our discriminative model is a linear model
trained with the Margin-Infused Relaxed Algorithm
(MIRA) (Crammer et al, 2006). This model com-
putes the score of a parse tree as the inner product
of a feature vector and a weight vector represent-
ing model parameters. The training procedure of
MIRA is very close to that of a perceptron (Rosen-
blatt, 1958), benefiting from its speed and relatively
low requirements while achieving better accuracy.
Recall that parsing under this model consists in
(1) generating a n-best list of constituency parses
using the generative model, (2) annotating each of
them with function tags, (3) converting them to de-
pendency parses, (4) extracting features, (5) scoring
each feature vector against the model, (6) selecting
the highest scoring parse as output.
For training, we collect the output of feature ex-
traction (4) for a large set of training sentences and
associate each parse tree with a loss function that de-
notes the number of erroneous dependencies com-
pared to the reference parse tree. Then, model
weights are adjusted using MIRA training so that the
parse with the lowest loss gets the highest score. Ex-
amples are processed in sequence, and for each of
them, we compute the score of each parse according
to the current model and find an updated weight vec-
tor that assigns the first rank to the best parse (called
oracle). Details of the algorithm are given in the fol-
lowing sections.
91
3.1 Definitions
Let us consider a vector space of dimensionmwhere
each component corresponds to a feature: a parse
tree p is represented as a sparse vector ?(p). The
model is a weight vector w in the same space where
each weight corresponds to the importance of the
features for characterizing good (or bad) parse trees.
The score s(p) of a parse tree p is the scalar product
of its feature vector ?(p) and the weight vector w.
s(p) =
m?
i=1
wi?i(p) (2)
Let L be the n-best list of parses produced by the
generative parser for a given sentence. The highest
scoring parse p? is selected as output of the reranker:
p? = argmax
p?L
s(p) (3)
MIRA learning consists in using training sen-
tences and their reference parses to determine the
weight vector w. It starts with w = 0 and modifies
it incrementally so that parses closest to the refer-
ence get higher scores. Let l(p), loss of parse p,
be the number of erroneous dependencies (governor,
dependent, label) compared to the reference parse.
We define o, the oracle parse, as the parse with the
lowest loss in L.
Training examples are processed in sequence as
an instance of online learning. For each sentence,
we compute the score of each parse in the n-best
list. If the highest scoring parse differs from the or-
acle (p? 6= o), the weight vector can be improved.
In this case, we seek a modification of w ensuring
that o gets a better score than p? with a difference
at least proportional to the difference between their
loss. This way, very bad parses get pushed deeper
than average parses. Finding such weight vector
can be formulated as the following constrained opti-
mization problem:
minimize: ||w||2 (4)
subject to: s(o)? s(p?) ? l(o)? l(p?) (5)
Since there is an infinity of weight vectors that
satisfy constraint 5, we settle on the one with the
smallest magnitude. Classical constrained quadratic
optimization methods can be applied to solve this
problem: first, Lagrange multipliers are used to in-
troduce the constraint in the objective function, then
the Hildreth algorithm yields the following analytic
solution to the non-constrained problem:
w? = w + ? (?(o)? ?(p?)) (6)
? = max
[
0,
l(o)? l(p?)? [s(o)? s(p?)]
||?(o)? ?(p?)||2
]
(7)
Here, w? is the new weight vector, ? is an up-
date magnitude and [?(o)? ?(p?)] is the difference
between the feature vector of the oracle and that of
the highest scoring parse. This update, similar to
the perceptron update, draws the weight vector to-
wards o while pushing it away from p?. Usual tricks
that apply to the perceptron also apply here: (a) per-
forming multiple passes on the training data, and (b)
averaging the weight vector over each update2. Al-
gorithm 1 details the instructions for MIRA training.
Algorithm 1 MIRA training
for i = 1 to t do
for all sentences in training set do
Generate n-best list L from generative parser
for all p ? L do
Extract feature vector ?(p)
Compute score s(p) (eq. 2)
end for
Get oracle o = argminp l(p)
Get best parse p? = argmaxp s(p)
if p? 6= o then
Compute ? (eq. 7)
Update weight vector (eq. 6)
end if
end for
end for
Return average weight vector over updates.
3.2 Features
The quality of the reranker depends on the learning
algorithm as much as on the feature set. These fea-
tures can span over any subset of a parse tree, up to
the whole tree. Therefore, there are a very large set
of possible features to choose from. Relevant fea-
tures must be general enough to appear in as many
2This can be implemented efficiently using two weight vec-
tors as for the averaged perceptron.
92
parses as possible, but specific enough to character-
ize good and bad configurations in the parse tree.
We extended the feature set from (McDonald,
2006) which showed to be effective for a range of
languages. Our feature templates can be categorized
in 5 classes according to their domain of locality.
In the following, we describe and exemplify these
templates on the following sentence from the Penn
treebank, in which we target the PMOD dependency
between ?at? and ?watch.?
Probability Three features are derived from the
PCFG-LA parser, namely the posterior proba-
bility of the parse (eq. 1), its normalized prob-
ability relative to the 1-best, and its rank in the
n-best list.
Unigram Unigram features are the most simple as
they only involve one word. Given a depen-
dency between position i and position j of type
l, governed by xi, denoted xi
l
? xj , two fea-
tures are created: one for the governor xi and
one for the dependent xj . They are described
as 6-tuples (word, lemma, pos-tag, is-governor,
direction, type of dependency). Variants with
wildcards at each subset of tuple slots are also
generated in order to handle sparsity.
In our example, the dependency between
?looked? and ?at? generates two features:
[at, at, IN, G, R, PMOD] and
[looked, look, NN, D, L, PMOD]
And also wildcard features such as:
[-, at, IN, G, R, PMOD], [at,
-, IN, G, R, PMOD] ...
[at, -, -, -, -, PMOD]
This wildcard feature generation is applied to
all types of features. We will omit it in the re-
mainder of the description.
Bigram Unlike the previous template, bigram fea-
tures model the conjunction of the governor
and the dependent of a dependency relation,
like bilexical dependencies in (Collins, 1997).
Given dependency xi
l
? xj , the feature cre-
ated is (word xi, lemma xi, pos-tag xi, word
xj , lemma xj , pos-tag xj , distance3 from i to
j, direction, type).
The previous example generates the following
feature:
[at, at, IN, watch, watch, NN,
2, R, PMOD]
Where 2 is the distance between ?at? and
?watch?.
Linear context This feature models the linear con-
text between the governor and the dependent
of a relation by looking at the words between
them. Given dependency xi
l
? xj , for each
word from i + 1 to j ? 1, a feature is created
with the pos-tags of xi and xj , and the pos tag
of the word between them (no feature is create
if j = i + 1). An additional feature is created
with pos-tags at positions i? 1, i, i+ 1, j ? 1,
j, j +1. Our example yields the following fea-
tures:
[IN, PRP$, NN], and [VBD, IN,
PRP$, PRP$, NN, .].
Syntactic context: siblings This template and the
next one look at two dependencies in two con-
figurations. Given two dependencies xi
l
? xj
and xi
m
? xk, we create the feature (word,
lemma, pos-tag for xi, xj and xk, distance from
i to j, distance from i to k, direction and type of
each of the two dependencies). In our example:
[looked, look, VBD, I, I, PRP,
at, at, IN, 1, 1, L, SBJ, R,
ADV]
Syntactic context: chains Given two dependencies
xi
l
? xj
m
? xk, we create the feature (word,
lemma, pos-tag of xi, xj and xk, distance from
i to j, distance from i to k, direction and type of
each of the two dependencies). In our example:
[looked, look, VBD, at, at, IN,
watch, watch, NN, 1, 2, R, ADV,
3In every template, distance features are quantified in 7
classes: 1, 2, 3, 4, 5, 5 to 10, more.
93
R, PMOD]
It is worth noting that our feature templates only
rely on information available in the training set, and
do not use any external linguistic knowledge.
4 Experiments
In this section, we evaluate our architecture on
two corpora, namely the Penn Treebank (Marcus et
al., 1994) and the French Treebank (Abeill? et al,
2003). We first present the corpora and the tools
used for annotating and converting structures, then
the performances of the phrase structure parser alone
and with the discriminative reranker.
4.1 Treebanks and Tools
For English, we use the Wall Street Journal sections
of the Penn Treebank. We learn the PCFG-LA from
sections 02-214. We then use FUNTAG (Chrupa?a
et al, 2007) to add functions back to the PCFG-LA
analyses. For the conversion to dependency struc-
tures we use the LTH tool (Johansson and Nugues,
2007). In order to get the gold dependencies, we run
LTH directly on the gold parse trees. We use sec-
tion 22 for development and section 23 for the final
evaluation.
For French, we use the Paris 7 Treebank (or
French Treebank, FTB). As in several previous ex-
periments we decided to divide the 12,350 phrase
structure trees in three sets: train (80%), develop-
ment (10%) and test (10%). The syntactic tag set for
French is not fixed and we decided to use the one
described in (Candito and Crabb?, 2009) to be able
to compare this system with recent parsing results
on French. As for English, we learn the PCFG-LA
without functional annotations which are added af-
terwards. We use the dependency structures devel-
oped in (Candito et al, 2010b) and the conversion
toolkit BONSA?. Furthermore, to test our approach
against state of the art parsing results for French
we use word clusters in the phrase-based parser as
in (Candito and Crabb?, 2009).
For both languages we constructed 10-fold train-
ing data from train sets in order to avoid overfitting
the training data. The trees from training sets were
divided into 10 subsets and the parses for each sub-
set were generated by a parser trained on the other
4Functions are omitted.
9 subsets. Development and test parses are given by
a parser using the whole training set. Development
sets were used to choose the best reranking model.
For lemmatisation, we use the MATE lemmatiser
for English and a home-made lemmatiser for French
based on the lefff lexicon (Sagot, 2010).
4.2 Generative Model
The performances of our parser are summarised in
Figure 2, (a) and (b), where F-score denotes the Par-
seval F-score5, and LAS and UAS are respectively
the Labelled and Unlabelled Attachment Score of
the converted dependency structures6. We give or-
acle scores (the score that our system would get if
it selected the best parse from the n-best lists) when
the parser generates n-best lists of depth 10, 20, 50
and 100 in order to get an idea of the effectiveness
of the reranking process.
One of the issues we face with this approach is
the use of an imperfect functional annotator. For
French we evaluate the loss of accuracy on the re-
sulting dependency structure from the gold develop-
ment set where functions have been omitted. The
UAS is 100% but the LAS is 96.04%. For English
the LAS from section 22 where functions are omit-
ted is 95.35%.
From the results presented in this section we can
make two observations. First, the results of our
parser are at the state of the art on English (90.7%
F-score) and on French (85.7% F-score). So the
reranker will be confronted with the difficult task of
improving on these scores. Second, the progression
margin is sensible with a potential LAS error reduc-
tion of 41% for English and 40.2% for French.
4.3 Adding the Reranker
4.3.1 Learning Feature Weights
The discriminative model, i.e. template instances
and their weights, is learnt on the training set parses
obtained via 10-fold cross-validation. The genera-
tive parser generates 100-best lists that are used as
learning example for the MIRA algorithm. Feature
extraction produces an enormous number of fea-
tures: about 571 millions for English and 179 mil-
5We use a modified version of evalb that gives the ora-
cle score when the parser outputs a list of candidates for each
sentence.
6All scores are measured without punctuation.
94
(a) Oracle Scores on PTB dev set (b) Oracle Scores on FTB dev set
8990
9192
9394
9596
97
1 10 20 50 100Size of n-best list
UASLASF-score
Oracle s
core
86
88
90
92
94
86
88
90
92
94
1 10 20 50 100Size of n?best list
Oracle s
core
UASLASF-score
(c) Reranker scores on PTB dev set (d) Reranker scores on FTB dev set
899
091
929
394
1 10 20 50 100Size of n-best list
UASLASF-score
Reranke
d score
868
788
899
091
1 10 20 50 100Size of n?best list
UASLASF-score
Rerank
er score
Figure 2: Oracle and reranker scores on PTB and FTB data on the dev. set, according to the depth of the n-best.
lions for French. Let us remark that this large set of
features is not an issue because our discriminative
learning algorithm is online, that is to say it consid-
ers only one example at a time, and it only gives
non-null weights to useful features.
4.3.2 Evaluation
In order to test our system we first tried to eval-
uate the impact of the length of the n-best list over
the reranking predictions7. The results are shown in
Figure 2, parts (c) and (d).
For French, we can see that even though the LAS
and UAS are consistently improving with the num-
ber of candidates, the F-score is maximal with 50
candidates. However the difference between 50 can-
didates and 100 candidates is not statistically signifi-
cant. For English, the situation is simpler and scores
improve continuously on the three metrics.
Finally we run our system on the test sets for both
treebanks. Results are shown8 in Table 1 for En-
glish, and Table 2 for French. For English the im-
provement is 0.9% LAS, 0.7% Parseval F-score and
7The model is always trained with 100 candidates.
8F < 40 is the parseval F-score for sentences with less than
40 words.
0.8% UAS.
Baseline Reranker
F 90.4 91.1
F < 40 91.0 91.7
LAS 88.9 89.8
UAS 93.1 93.9
Table 1: System results on PTB Test set
For French we have improvements of 0.3/0.7/0.9.
If we add a template feature indicating the agree-
ment between part-of-speech provided by the PCFG-
LA parser and a part-of-speech tagger (Denis
and Sagot, 2009), we obtain better improvements:
0.5/0.8/1.1.
Baseline Reranker Rerank + MElt
F 86.6 87.3 87.4
F < 40 88.7 89.0 89.2
LAS 87.9 89.0 89.2
UAS 91.0 91.9 92.1
Table 2: System results on FTB Test set
95
4.3.3 Comparison with Related Work
We compare our results with related parsing re-
sults on English and French.
For English, the main results are shown in Ta-
ble 3. From the presented data, we can see that
indirect reranking on LAS may not seem as good
as direct reranking on phrase-structures compared to
F-scores obtained in (Charniak and Johnson, 2005)
and (Huang, 2008) with one parser or (Zhang et
al., 2009) with several parsers. However, our sys-
tem does not rely on any language specific feature
and can be applied to other languages/treebanks. It
is difficult to compare our system for LAS because
most systems evaluate on gold data (part-of-speech,
lemmas and morphological information) like Bohnet
(2010).
Our system also compares favourably with the
system of Carreras et al (2008) that relies on a more
complex generative model, namely Tree Adjoining
Grammars, and the system of Suzuki et al (2009)
that makes use of external data (unannotated text).
F LAS UAS
Huang, 2008 91.7 ? ?
Bohnet, 2010 ? 90.3 ?
Zhang et al 2008 91.4 ? 93.2
Huang and Sagae, 2010 ? ? 92.1
Charniak et al 2005 91.5 90.0 94.0
Carreras et al 2008 ? ? 93.5
Suzuki et al 2009 ? ? 93.8
This work 91.1 89.8 93.9
Table 3: Comparison on PTB Test set
For French, see Table 4, we compare our system
with the MATE parser (Bohnet, 2010), an improve-
ment over the MST parser (McDonald et al, 2005)
with hash kernels, using the MELT part-of-speech
tagger (Denis and Sagot, 2009) and our own lemma-
tiser.
We also compare the French system with results
drawn from the benchmark performed by Candito et
al. (2010a). The first system (BKY-FR) is close to
ours without the reranking module, using the Berke-
ley parser adapted to French. The second (MST-
FR) is based on MSTParser (McDonald et al, 2005).
These two system use word clusters as well.
The next section takes a close look at the models
of the reranker and its impact on performance.
F < 40 LAS UAS
This work 89.2 89.2 92.1
MATE + MELT ? 89.2 91.8
BKY-FR 88.2 86.8 91.0
MST-FR ? 88.2 90.9
Table 4: Comparison on FTB Test set
4.3.4 Model Analysis
It is interesting to note that in the test sets, the
one-best of the syntagmatic parser is selected 52.0%
of the time by the reranker for English and 34.3% of
the time for French. This can be explained by the
difference in the quantity of training data in the two
treebanks (four times more parses are available for
English) resulting in an improvement of the quality
of the probabilistic grammar.
We also looked at the reranking models, specifi-
cally at the weight given to each of the features. It
shows that 19.8% of the 571 million features have
a non-zero weight for English as well as 25.7% of
the 179 million features for French. This can be ex-
plained by the fact that for a given sentence, features
that are common to all the candidates in the n-best
list are not discriminative to select one of these can-
didates (they add the same constant weight to the
score of all candidates), and therefore ignored by the
model. It also shows the importance of feature engi-
neering: designing relevant features is an art (Char-
niak and Johnson, 2005).
We took a closer look at the 1,000 features of
highest weight and the 1,000 features of lowest
weight (negative) for both languages that represent
the most important features for discriminating be-
tween correct and incorrect parses. For English,
62.0% of the positive features are backoff features
which involve at least one wildcard while they are
85.9% for French. Interestingly, similar results hold
for negative features. The difference between the
two languages is hard to interpret and might be due
in part to lexical properties and to the fact that these
features may play a balancing role against towards
non-backoff features that promote overfitting.
Expectedly, posterior probability features have
the highest weight and the n-best rank feature has the
highest negative weight. As evidenced by Table 5,
96
en (+) en (-) fr (+) fr (-)
Linear 30.4 36.1 44.8 44.0
Unigram 20.7 16.3 9.7 8.2
Bigram 27.4 29.1 20.8 24.4
Chain 15.4 15.3 13.7 19.4
Siblings 5.8 3.0 10.8 3.6
Table 5: Repartition of weight (in percentage) in the
1,000 highest (+) and lowest (-) weighted features for En-
glish and French.
among the other feature templates, linear context oc-
cupies most of the weight mass of the 1,000 highest
weighted features. It is interesting to note that the
unigram and bigram templates are less present for
French than for English while the converse seems to
be true for the linear template. Sibling features are
consistently less relevant.
In terms of LAS performance, on the PTB test
set the reranked output is better than the baseline
on 22.4% of the sentences while the opposite is true
for 10.4% of the sentences. In 67.0% of the sen-
tences, they have the same LAS (but not necessar-
ily the same errors). This emphasises the difficulty
of reranking an already good system and also ex-
plains why oracle performance is not reached. Both
the baseline and reranker output are completely cor-
rect on 21.3% of the sentences, while PCFG-LA cor-
rectly parses 23% of the sentences and the MIRA
brings that number to 26%.
Figures 3 and 4 show hand-picked sentences for
which the reranker selected the correct parse. The
French sentence is a typical difficult example for
PCFGs because it involves a complex rewriting rule
which might not be well covered in the training
data (SENT ? NP VP PP PONCT PP PONCT PP
PONCT). The English example is tied to a wrong
attachment of the prepositional phrase to the verb
instead of the date, which lexicalized features of the
reranker handle easily.
5 Conclusion
We showed that using a discriminative reranker, on
top of a phrase structure parser, based on converted
dependency structures could lead to significant im-
provements over dependency and phrase structure
parse results. We experimented on two treebanks
for two languages, English and French and we mea-
sured the improvement of parse quality on three dif-
ferent metrics: Parseval F-score, LAS and UAS,
with the biggest error reduction on the latter. How-
ever the gain is not as high as expected by looking
at oracle scores, and we can suggest several possible
improvements on this method.
First, the sequential approach is vulnerable to cas-
cading errors. Whereas the generative parser pro-
duces several candidates, this is not the case of the
functional annotators: these errors are not amend-
able. It should be possible to have a functional tag-
ger with ambiguous output upon which the reranker
could discriminate. It remains an open question as
how to integrate ambiguous output from the parser
and from the functional tagger. The combination
of n-best lists would not scale up and working on
the ambiguous structure itself, the packed forest as
in (Huang, 2008), might be necessary. Another pos-
sibility for future work is to let the phrase-based
parser itself perform function annotation, but some
preliminary tests on French showed disappointing
results.
Second, designing good features, sufficiently gen-
eral but precise enough, is, as already coined
by Charniak and Johnson (2005), an art. More for-
mally, we can see several alternatives. Dependency
structures could be exploited more thoroughly using,
for example, tree kernels. The restricted number of
candidates enables the use of more global features.
Also, we haven?t used any language-specific syntac-
tic features. This could be another way to improve
this system, relying on external linguistic knowledge
(lexical preferences, subcategorisation frames, cop-
ula verbs, coordination symmetry . . . ). Integrating
features from the phrase-structure trees is also an op-
tion that needs to be explored.
Third this architecture enables the integration of
several systems. We experimented on French using a
part-of-speech tagger but we could also use another
parser and either use the methodology of (Johnson
and Ural, 2010) or (Zhang et al, 2009) which fu-
sion n-best lists form different parsers, or use stack-
ing methods where an additional parser is used as
a guide for the main parser (Nivre and McDonald,
2008).
Finally it should be noted that this system does not
rely on any language specific feature, and thus can
be applied to languages other that French or English
97
NNSStocks
NP
VBDwere CD698 CDmillion
QP
NNSbushels
NP
INon NNPMay CD31
NP
INof DTthis NNyear
NP
PP
NP
PP
VP
..
S
NNSStocks
NP
VBDwere CD698 CDmillion
QP
NNSbushels
NP
INon NNPMay CD31
NP
PP
INof DTthis NNyear
NP
PP
VP
..
S
depen
dency parse
s
synta
gmati
c
parse
s
Before reranking After reranking
Figure 3: English sentence from the WSJ test set for which the reranker selected the correct tree while the first
candidate of the n-best list suffered from an incorrect attachment.
SENT
NP VN PP
PONCT
NP
PONCTNPP NPP V VPP P AP DET ADJ PONCT P ADJADJ
SENT
NP VN PP
PONCT NP PONCTNPP NPP V VPP P AP P NC PONCT P ADJADJ NP
PP PP
NPP NPP V VPP P ADJ PONCT DET ADJ PONCT P ADJ PONCT NPP NPP V VPP P ADJ PONCT P NC PONCT P ADJ PONCTdepen
dency parses
syntag
matic parses
Before reranking After reranking
Figure 4: Sentence from the FTB for which the best parse according to baseline was incorrect, probably due to the
tendency of the PCFG-LA model to prefer rules with more support. The reranker selected the correct parse.
without re-engineering new reranking features. This
makes this architecture suitable for morphologically
rich languages.
Acknowledgments
This work has been funded by the French Agence
Nationale pour la Recherche, through the project
SEQUOIA (ANR-08-EMER-013).
References
Anne Abeill?, Lionel Cl?ment, and Toussenel Fran?ois,
2003. Treebanks, chapter Building a treebank for
French. Kluwer, Dordrecht.
M. Attia, J. Foster, D. Hogan, J. Le Roux, L. Tounsi, and
J. van Genabith. 2010. Handling Unknown Words in
Statistical Latent-Variable Parsing Models for Arabic,
English and French. In Proceedings of SPMRL.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In Proceedings
of COLING.
M.-H. Candito and B. Crabb?. 2009. Improving Gen-
erative Statistical Parsing with Semi-Supervised Word
Clustering. In Proceedings of IWPT 2009.
M.-H. Candito, J. Nivre, P. Denis, and E. Henestroza An-
guiano. 2010a. Benchmarking of Statistical Depen-
dency Parsers for French. In Proceedings of COL-
ING?2010.
Marie Candito, Beno?t Crabb?, and Pascal Denis. 2010b.
Statistical French Dependency Parsing : Treebank
Conversion and First Results. In Proceedings of
LREC2010.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, Dynamic Programming and the Perceptron for
Efficient, Feature-rich Parsing. In CONLL.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of ACL.
98
Grzegorz Chrupa?a, Nicolas Stroppa, Josef van Genabith,
and Georgiana Dinu. 2007. Better training for func-
tion labeling. In Proceedings of RANLP, Borovets,
Bulgaria.
Michael Collins. 1997. Three Generative, Lexicalised
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the ACL.
Michael Collins. 2000. Discriminative Reranking for
Natural Language Parsing. In Proceedings of ICML.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
ShalevShwartz, and Yoram Singer. 2006. Online
Passive-Aggressive Algorithm. Journal of Machine
Learning Research.
Pascal Denis and Beno?t Sagot. 2009. Coupling an anno-
tated corpus and a morphosyntactic lexicon for state-
of-the-art pos tagging with less human effort. In Pro-
ceedings PACLIC 23, Hong Kong, China.
Liang Huang. 2008. Forest Reranking: Discriminative
Parsing with Non-Local Features. In Proceedings of
ACL.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of NODALIDA 2007, pages 105?112,
Tartu, Estonia, May 25-26.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the Berkeley and Brown Parsers. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 665?668, Los An-
geles, California, June. Association for Computational
Linguistics.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: Annotating predicate argument structure. In
Proceedings of the ARPA Speech and Natural Lan-
guage Workshop.
Takuya Matsuzaki, Yusuke Miyao, and Jun ichi Tsujii.
2005. Probabilistic CFG with Latent Annotations. In
Proceedings of ACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online Large-Margin Training of Dependency
Parsers. In Association for Computational Linguistics
(ACL).
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL, pages 950?958.
Slav Petrov and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In HLT-NAACL, pages
404?411.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In ACL.
Geoffrey K. Pullum and Barbara C. Scholz. 2001. On the
distinction between model-theoretic and generative-
enumerative syntactic frameworks. In Logical Aspects
of Computational Linguistics.
Owen Rambow. 2010. The Simple Truth about Depen-
dency and Phrase Structure Representations: An Opin-
ion Piece. In NAACL HLT.
Frank Rosenblatt. 1958. The Perceptron: A Probabilistic
Model for Information Storage and Organization in the
Brain. Psychological Review.
Beno?t Sagot. 2010. The lefff, a freely available and
large-coverage lexicon for french. In Proceedings of
LREC 2010, La Valette, Malta.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2-Volume 2,
pages 551?560. Association for Computational Lin-
guistics.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In Proceedings of EMNLP.
99
