Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 1?4,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Language Dynamics and Capitalization using Maximum Entropy
Fernando Batistaa,b, Nuno Mamedea,c and Isabel Trancosoa,c
a L2F ? Spoken Language Systems Laboratory - INESC ID Lisboa
R. Alves Redol, 9, 1000-029 Lisboa, Portugal
http://www.l2f.inesc-id.pt/
b ISCTE ? Instituto de Ci?ncias do Trabalho e da Empresa, Portugal
c IST ? Instituto Superior T?cnico, Portugal.
{fmmb,njm,imt}@l2f.inesc-id.pt
Abstract
This paper studies the impact of written lan-
guage variations and the way it affects the cap-
italization task over time. A discriminative
approach, based on maximum entropy mod-
els, is proposed to perform capitalization, tak-
ing the language changes into consideration.
The proposed method makes it possible to use
large corpora for training. The evaluation is
performed over newspaper corpora using dif-
ferent testing periods. The achieved results
reveal a strong relation between the capital-
ization performance and the elapsed time be-
tween the training and testing data periods.
1 Introduction
The capitalization task, also known as truecasing
(Lita et al, 2003), consists of rewriting each word
of an input text with its proper case information.
The capitalization of a word sometimes depends on
its current context, and the intelligibility of texts is
strongly influenced by this information. Different
practical applications benefit from automatic capi-
talization as a preprocessing step: when applied to
speech recognition output, which usually consists
of raw text, automatic capitalization provides rele-
vant information for automatic content extraction,
named entity recognition, and machine translation;
many computer applications, such as word process-
ing and e-mail clients, perform automatic capital-
ization along with spell corrections and grammar
check.
The capitalization problem can be seen as a se-
quence tagging problem (Chelba and Acero, 2004;
Lita et al, 2003; Kim and Woodland, 2004), where
each lower-case word is associated to a tag that de-
scribes its capitalization form. (Chelba and Acero,
2004) study the impact of using increasing amounts
of training data as well as a small amount of adap-
tation. This work uses a Maximum Entropy Markov
Model (MEMM) based approach, which allows to
combine different features. A large written news-
paper corpora is used for training and the test data
consists of Broadcast News (BN) data. (Lita et al,
2003) builds a trigram language model (LM) with
pairs (word, tag), estimated from a corpus with case
information, and then uses dynamic programming to
disambiguate over all possible tag assignments on a
sentence. Other related work includes a bilingual
capitalization model for capitalizing machine trans-
lation (MT) outputs, using conditional random fields
(CRFs) reported by (Wang et al, 2006). This work
exploits case information both from source and tar-
get sentences of the MT system, producing better
performance than a baseline capitalizer using a tri-
gram language model. A preparatory study on the
capitalization of Portuguese BN has been performed
by (Batista et al, 2007).
One important aspect related with capitalization
concerns the language dynamics: new words are in-
troduced everyday in our vocabularies and the usage
of some other words decays with time. Concerning
this subject, (Mota, 2008) shows that, as the time
gap between training and test data increases, the per-
formance of a named tagger based on co-training
(Collins and Singer, 1999) decreases.
This paper studies and evaluates the effects of lan-
guage dynamics in the capitalization of newspaper
1
corpora. Section 2 describes the corpus and presents
a short analysis on the lexicon variation. Section 3
presents experiments concerning the capitalization
task, either using isolated training sets or by retrain-
ing with different training sets. Section 4 concludes
and presents future plans.
2 Newspaper Corpus
Experiments here described use the RecPub news-
paper corpus, which consists of collected editions
of the Portuguese ?P?blico? newspaper. The corpus
was collected from 1999 to 2004 and contains about
148Million words. The corpus was split into 59 sub-
sets of about 2.5 Million words each (between 9 to
11 per year). The last subset is only used for testing,
nevertheless, most of the experiments here described
use different training and test subsets for better un-
derstanding the time effects on capitalization. Each
subset corresponds to about five weeks of data.
2.1 Data Analysis
The number of unique words in each subset is
around 86K but only about 50K occur more than
once. In order to assess the relation between the
word usage and the time gap, we created a number
of vocabularies with the 30K more frequent words
appearing in each training set (roughly corresponds
to a freq > 3). Then, the first and last corpora subsets
were checked against each one of the vocabularies.
Figure 1 shows the correspondent results, revealing
that the number of OOVs (Out of VocabularyWords)
decreases as the time gap between the train and test
periods gets smaller.
??
??
???
???
???
???
???
???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ????
??
????? ?????
???? ????
Figure 1: Number of OOVs using a 30K vocabulary.
3 Capitalization
The present study explores only three ways of
writing a word: lower-case, all-upper, and first-
capitalized, not covering mixed-case words such as
?McLaren? and ?SuSE?. In fact, mixed-case words
are also being treated by means of a small lexicon,
but they are not evaluated in the scope of this paper.
The following experiments assume that the capi-
talization of the first word of each sentence is per-
formed in a separated processing stage (after punc-
tuation for instance), since its correct graphical form
depends on its position in the sentence. Evaluation
results may be influenced when taking such words
into account (Kim and Woodland, 2004).
The evaluation is performed using the met-
rics: Precision, Recall and SER (Slot Error Rate)
(Makhoul et al, 1999). Only capitalized words (not
lowercase) are considered as slots and used by these
metrics. For example: Precision is calculated by di-
viding the number of correct capitalized words by
the number of capitalized words in the testing data.
The modeling approach here described is discrim-
inative, and is based on maximum entropy (ME)
models, firstly applied to natural language problems
in (Berger et al, 1996). An ME model estimates
the conditional probability of the events given the
corresponding features. Therefore, all the infor-
mation must be expressed in terms of features in
a pre-processing step. Experiments here described
only use features comprising word unigrams and bi-
grams: wi (current word), ?wi?1, wi? and ?wi, wi+1?
(bigrams). Only words occurring more than once
were included for training, thus reducing the number
of misspelled words. All the experiments used the
MegaM tool (Daum? III, 2004), which uses conju-
gate gradient and a limited memory optimization of
logistic regression. The following subsections de-
scribe the achieved results.
3.1 Isolated Training
In order to assess how time affects the capitalization
performance, the first experiments consist of pro-
ducing six isolated language models, one for each
year of training data. For each year, the first 8 sub-
sets were used for training and the last one was used
for evaluation. Table 1 shows the corresponding
capitalization results for the first and last testing sub-
2
Train 1999-12 test set 2004-12 test set
Prec Rec SER Prec Rec SER
1999 94% 81% 0.240 92% 76% 0.296
2000 94% 81% 0.242 92% 77% 0.291
2001 94% 79% 0.262 93% 76% 0.291
2002 93% 79% 0.265 93% 78% 0.277
2003 94% 77% 0.276 93% 78% 0.273
2004 93% 77% 0.285 93% 80% 0.264
Table 1: Using 8 subsets of each year for training.
???
???
???
???
???
??? ??? ??? ??? ??? ???
??
?????????
????????? ?????????
Figure 2: Performance for different training periods.
sets, revealing that performance is affected by the
time lapse between the training and testing periods.
The best results were always produced with nearby
the testing data. A similar behavior was observed on
the other four testing subsets, corresponding to the
last subset of each year. Results also reveal a degra-
dation of performance when the training data is from
a time period after the evaluation data.
Results from previous experiment are still worse
than results achieved by other work on the area
(Batista et al, 2007) (about 94% precision and 88%
recall), specially in terms of recall. This is caused
by a low coverage of the training data, thus reveal-
ing that each training set (20Million words) does not
provide sufficient data for the capitalization task.
One important problem related with this discrim-
inative approach concerns memory limitations. The
memory required increases with the size of the cor-
pus (number of observations), preventing the use
of large corpora, such as RecPub for training, with
Evaluation Set Prec Rec SER
2004-12 test set 93% 82% 0.233
Table 2: Training with all RecPub training data.
Checkpoint LM #lines Prec Rec SER
1999-12 1.27 Million 92% 77% 0.290
2000-12 1.86 Million 93% 79% 0.266
2001-12 2.36 Million 93% 80% 0.257
2002-12 2.78 Million 93% 81% 0.247
2003-12 3.10 Million 93% 82% 0.236
2004-08 3.36 Million 93% 83% 0.225
Table 3: Retraining from Jan. 1999 to Sep. 2004.
available computers. For example, four million
events require about 8GB of RAM to process. This
problem can be minimized using a modified train-
ing strategy, based on the fact that scaling the event
by the number of occurrences is equivalent to multi-
ple occurrences of that event. Accordingly to this,
our strategy to use large training corpora consists
of counting all n-gram occurrences in the training
data and then use such counts to produce the cor-
responding input features. This strategy allows us
to use much larger corpora and also to remove less
frequent n-grams if desired. Table 2 shows the per-
formance achieved by following this strategy with
all the RecPub training data. Only word frequen-
cies greater than 4 were considered, minimizing the
effects of misspelled words and reducing memory
limitations. Results reveal the expected increase of
performance, specially in terms of recall. However,
these results can not be directly compared with pre-
vious work on this subject, because of the different
corpora used.
3.2 Retraining
Results presented so far use isolated training. A new
approach is now proposed, which consists of train-
ing with new data, but starting with previously cal-
culated models. In other words, previously trained
models provide initialized models for the new train.
As the training is still performed with the new data,
the old models are iteratively adjusted to the new
data. This approach is a very clean framework for
language dynamics adaptation, offering a number of
advantages: (1) new events are automatically con-
sidered in the new models; (2) with time, unused
events slowly decrease in weight; (3) by sorting the
trained models by their relevance, the amount of data
used in next training stage can be limited without
much impact in the results. Table 3 shows the re-
3
???
???
???
???
???
???
???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ???? ????
??
?????????
??????????? ???????????
Figure 3: Training forward and backwards
sults achieved with this approach, revealing higher
performance as more training data is available.
The next experiment shows that the training or-
der is important. In fact, from previous results, the
increase of performance may be related only with
the number of events seen so far. For this reason,
another experiment have been performed, using the
same training data, but retraining backwards. Corre-
sponding results are illustrated in Figure 3, revealing
that: the backwards training results are worse than
forward training results, and that backward training
results do not allways increase, rather stabilize af-
ter a certain amount of data. Despite the fact that
both training use all training data, in the case of for-
ward training the time gap between the training and
testing data gets smaller for each iteration, while in
the backwards training is grows. From these results
we can conclude that a strategy based on retraining
is suitable for using large amounts of data and for
language adaptation.
4 Conclusions and Future Work
This paper shows that maximum entropy models
can be used to perform the capitalization task, spe-
cially when dealing with language dynamics. This
approach provides a clean framework for learning
with new data, while slowly discarding unused data.
The performance achieved is almost as good as us-
ing generative approaches, found in related work.
This approach also allows to combine different data
sources and to explore different features. In terms
of language changes, our proposal states that differ-
ent capitalization models should be used for differ-
ent time periods.
Future plans include the application of this work
to BN data, automatically produced by our speech
recognition system. In fact, subtitling of BN has led
us into using a baseline vocabulary of 100K words
combined with a daily modification of the vocabu-
lary (Martins et al, 2007) and a re-estimation of the
language model. This dynamic vocabulary provides
an interesting scenario for our experiments.
Acknowledgments
This work was funded by PRIME National Project
TECNOVOZ number 03/165, and FCT project
CMU-PT/0005/2007.
References
F. Batista, N. J. Mamede, D. Caseiro, and I. Trancoso.
2007. A lightweight on-the-fly capitalization system
for automatic speech recognition. In Proc. of the
RANLP 2007, Borovets, Bulgaria, September.
A. L. Berger, S. A. Della Pietra, and V. J. Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39?71.
C. Chelba and A. Acero. 2004. Adaptation of maxi-
mum entropy capitalizer: Little data can help a lot.
EMNLP04.
M. Collins and Y. Singer. 1999. Unsupervised models
for named entity classification. In Proc. of the Joint
SIGDAT Conference on EMNLP.
H. Daum? III. 2004. Notes on CG and LM-BFGS opti-
mization of logistic regression.
J. Kim and P. C. Woodland. 2004. Automatic capitalisa-
tion generation for speech input. Computer Speech &
Language, 18(1):67?90.
L. V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla.
2003. tRuEcasIng. In Proc. of the 41st annual meet-
ing on ACL, pages 152?159, Morristown, NJ, USA.
J. Makhoul, F. Kubala, R. Schwartz, and R. Weischedel.
1999. Performance measures for information extrac-
tion. In Proceedings of the DARPA Broadcast News
Workshop, Herndon, VA, Feb.
C. Martins, A. Teixeira, and J. P. Neto. 2007. Dynamic
language modeling for a daily broadcast news tran-
scription system. In ASRU 2007, December.
Cristina Mota. 2008. How to keep up with language
dynamics? A case study on Named Entity Recognition.
Ph.D. thesis, IST / UTL.
Wei Wang, Kevin Knight, and Daniel Marcu. 2006. Cap-
italizing machine translation. In HLT-NAACL, pages
1?8, Morristown, NJ, USA. ACL.
4
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 962?971, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Entropy-based Pruning for Phrase-based Machine Translation
Wang Ling, Joa?o Grac?a, Isabel Trancoso, Alan Black
L2F Spoken Systems Lab, INESC-ID, Lisboa, Portugal
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
{wang.ling,joao.graca,isabel.trancoso}@inesc-id.pt
awb@cs.cmu.edu
Abstract
Phrase-based machine translation models
have shown to yield better translations than
Word-based models, since phrase pairs en-
code the contextual information that is needed
for a more accurate translation. However,
many phrase pairs do not encode any rele-
vant context, which means that the transla-
tion event encoded in that phrase pair is led
by smaller translation events that are indepen-
dent from each other, and can be found on
smaller phrase pairs, with little or no loss in
translation accuracy. In this work, we pro-
pose a relative entropy model for translation
models, that measures how likely a phrase pair
encodes a translation event that is derivable
using smaller translation events with similar
probabilities. This model is then applied to
phrase table pruning. Tests show that con-
siderable amounts of phrase pairs can be ex-
cluded, without much impact on the transla-
tion quality. In fact, we show that better trans-
lations can be obtained using our pruned mod-
els, due to the compression of the search space
during decoding.
1 Introduction
Phrase-based Machine Translation Models (Koehn
et al 2003) model n-to-m translations of n source
words to m target words, which are encoded in
phrase pairs and stored in the translation model.
This approach has an advantage over Word-based
Translation Models (Brown et al 1993), since trans-
lating multiple source words allows the context for
each source word to be considered during trans-
lation. For instance, the translation of the En-
glish word ?in? by itself to Portuguese is not ob-
vious, since we do not have any context for the
word. This word can be translated in the con-
text of ?in (the box)? to ?dentro?, or in the con-
text of ?in (China)? as ?na?. In fact, the lexical
entry for ?in? has more than 10 good translations
in Portuguese. Consequently, the lexical translation
entry for Word-based models splits the probabilis-
tic mass between different translations, leaving the
choice based on context to the language model. On
the other hand, in Phrase-based Models, we would
have a phrase pair p(in the box, dentro da caixa)
and p(in china, na china), where the words ?in the
box? and ?in China? can be translated together to
?dentro da caixa? and ?na China?, which substan-
tially reduces the ambiguity. In this case, both the
translation and language models contribute to find
the best translation based on the local context, which
generally leads to better translations.
However, not all words add the same amount of
contextual information. Using the same example for
?in?, if we add the context ?(hid the key) in?, it is
still not possible to accurately identify the best trans-
lation for the word ?in?. The phrase extraction algo-
rithm (Ling et al 2010) does not discriminate which
phrases pairs encode contextual information, and ex-
tracts all phrase pairs with consistent alignments.
Hence, phrases that add no contextual information,
such as, p(hid the key in, escondeu a chave na)
and p(hid the key in, escondeu a chave dentro)
are extracted. This is undesirable because we are
populating translation models with redundant phrase
pairs, whose translations can be obtained using com-
962
binations of other phrases with the same probabil-
ities, namely p(hid the key, escondeu a chave),
p(in, dentro) and p(in, na). This is a problem
that is also found in language modeling, where
large amounts of redundant higher-order n-grams
can make the model needlessly large. For backoff
language models, multiple pruning strategies based
on relative entropy have been proposed (Seymore
and Rosenfeld, 1996) (Stolcke, 1998), where the ob-
jective is to prune n-grams in a way to minimize the
relative entropy between the model before and after
pruning.
While the concept of using relative entropy for
pruning is not new and frequently used in backoff
language models, there are no such models for ma-
chine translation. Thus, the main contribution of
our work is to propose a relative entropy pruning
model for translation models used in Phrase-based
Machine Translation. It is shown that our pruning
algorithm can eliminate phrase pairs with little or
no impact in the predictions made in our translation
model. In fact, by reducing the search space, less
search errors are made during decoding, which leads
to improvements in translation quality.
This paper is organized as follows. We describe
and contrast the state of the art pruning algorithms
in section 2. In section 3, we describe our relative-
entropy model for machine translation. Afterwards,
in section 4, we apply our model for pruning in
Phrase-based Machine Translation systems. We per-
form experiments with our pruning algorithm based
on phrase pair independence and analyse the results
in section 5. Finally, we conclude in section 6.
2 Phrase Table Pruning
Phrase table pruning algorithms are important in
translation, since they efficiently reduce the size of
the translation model, without having a large nega-
tive impact in the translation quality. This is espe-
cially relevant in environments where memory con-
straints are imposed, such as translation systems for
small devices like cellphones, and also when time
constraints for the translation are defined, such as
online Speech-to-Speech systems.
2.1 Significance Pruning
A relevant reference in phrase table pruning is the
work of (Johnson and Martin, 2007), where it is
shown that a significant portion of the phrase ta-
ble can be discarded without a considerable negative
impact on translation quality, or even positive one.
This work computes the probability, named p-value,
that the joint occurrence event of the source phrase
s and target phrase t occurring in same sentence pair
happens by chance, and are actually statistically in-
dependent. Phrase pairs that have a high p-value,
are more likely to be spurious and more prone to
be pruned. This work is followed in (Tomeh et al
2009), where phrase pairs are treated discriminately
based on their complexity. Significance-based prun-
ing has also been successfully applied in language
modeling in (Moore and Quirk, 2009).
Our work has a similar objective, but instead
of trying to predict the independence between the
source and target phrases in each phrase pair, we at-
tempt to predict the independence between a phrase
pair and other phrase pairs in the model.
2.2 Relevance Pruning
Another proposed approach (Matthias Eck and
Waibel, 2007) consists at collecting usage statistics
for phrase pairs. This algorithm decodes the train-
ing corpora and extracts the number of times each
phrase pair is used in the 1-best translation hypoth-
esis. Thus, phrase pairs that are rarely used during
decoding are excluded first during pruning.
This method considers the relationship between
phrase pairs in the model, since it tests whether
the decoder is more prone to use some phrase pairs
than others. However, it leads to some undesirable
pruning choices. Let us consider a source phrase
?the box in China? and 2 translation hypotheses,
where the first hypothesis uses the phrase transla-
tion p(the key in China, a chave na China) with
probability 70%, and the second hypothesis uses
two phrase translations p(the key, a chave) and
p(in China, na China) with probability 65%. This
approach will lean towards pruning the phrase pairs
in the second hypothesis, since the decoder will use
the first hypothesis. This is generally not desired,
since the 2 smaller phrase pairs can be used to trans-
late the same source sentence with a small probabil-
963
ity loss (5%), even if the longer phrase is pruned.
On the other hand, if the smaller phrases are pruned,
the longer phrase can not be used to translate smaller
chunks, such as ?the key in Portugal?. This matter is
aggravated due to the fact that the training corpora is
used to decode, so longer phrase pairs will be used
more frequently than when translating unseen sen-
tences, which will make the model more biased into
pruning shorter phrase pairs.
3 Relative Entropy Model For
Phrase-based Translation Models
In this section, we shall define our entropy model
for phrase pairs. We start by introducing some no-
tation to distinguish different types of phrase pairs
and show why some phrase pairs are more redun-
dant than others. Afterwards, we illustrate our no-
tion of relative entropy between phrase pairs. Then,
we describe our entropy model, its computation and
its application to phrase table pruning.
3.1 Atomic and Composite Phrase Pairs
We discriminate between 2 types of phrase pairs:
atomic phrase pairs and composite phrase pairs.
Atomic phrase pairs define the smallest transla-
tion units, such that given an atomic phrase pair that
translates from s to t, the same translation cannot
be obtained using any combination of other phrase
pairs. Removing these phrase pairs reduces the
range of translations that our model is capable of
translating and also the possible translations.
Composite phrase pairs define translations of a
given sequence of words that can also be obtained
using atomic or other smaller composite phrase
pairs. Each combination is called a derivation or
translation hypothesis. Removing these phrase pairs
does not change the amount of sentences that the
model can translate, since all translations encoded
in these phrases can still be translated using other
phrases, but these will lead to different translation
probabilities.
Considering table 1, we can see that atomic
phrases encode one elementary translation event,
while composite phrases encode joint events that are
encoded in atomic phrase pairs. If we look at the
source phrase ?in?, there is a multitude of possible
translations for this word in most target languages.
Taking Portuguese as the target language, the proba-
bility that ?in? is translated to ?em? is relatively low,
since it can also be translated to ?no?, ?na?, ?den-
tro?, ?dentro de? and many others.
However, if we add another word such as ?Por-
tugal? forming ?in Portugal?, it is more likely that
?in? is translated to ?em?. Thus, we define the
joint event of ?in? translating to ?em? (A1) and
?Portugal? to ?Portugal? (B1), denoted as A1 ? B1,
in the phrase pair p(in Portugal, em Portugal).
Without this phrase pair it is assumed that these
are independent events with probability given by
P (A1)P (B1)1, which would be 10%, leading to a
60% reduction. In this case, it would be more likely,
that in Portugal is translated to no Portugal or
na Portugal, which would be incorrect.
Some words, such as ?John?, forming ?John in?,
do not influence the translations for the word ?in?,
since it can still be translated to ?em?, ?no?, ?na?,
?dentro? or ?dentro de? depending on the word that
follows. By definition, if the presence of phrase
p(John, John) does not influence the translation of
p(in, em) and viceversa, we can say that probability
of the joint event P (A1?C1) is equal to the product
of the probabilities of the events P (A1)P (C1).
If we were given a choice of pruning either the
composite phrase pairs p(John in, John em) or
p(in Portugal, em Portugal), the obvious choice
would be the former, since the probability of the
event encoded in that phrase pair is composed by 2
independent events, in which case the decoder will
inherently consider the hypothesis that ?John in? is
translated to ?John em? with the same probability. In
another words, the model?s predictions even, with-
out this phrase pair will remain the same.
The example above shows an extreme case,
where the event encoded in the phrase pair
p(John in, John em) is decomposed into indepen-
dent events, and can be removed without chang-
ing the model?s prediction. However, finding and
pruning phrase pairs that are independent, based on
smaller events is impractical, since most translation
events are not strictly independent. However, many
phrase pairs can be replaced with derivations using
smaller phrases with a small loss in the model?s pre-
1For simplicity, we assume at this stage that no reordering
model is used
964
Phrase Pair Prob Event
Atomic Phrase Pairs
in? em 10% A1
in? na 20% A2
in? no 20% A3
in? dentro 5% A4
in? dentro de 5% A5
Portugal? Portugal 100% B1
John? John 100% C1
Composite Phrase Pairs
in Portugal? em Portugal 70% A1 ?B1
John in? John em 10% C1 ?A1
John in? John na 20% C1 ?A2
John in? John no 20% C1 ?A3
John in? John dentro 5% C1 ?A4
John in? John dentro de 5% C1 ?A5
Table 1: Phrase Translation Table with associated events
dictions.
Hence, we would like to define a metric for phrase
pairs that allows us evaluate how discarding each
phrase pair will affect the pruned model?s predic-
tions. By removing phrase pairs that can be derived
using smaller phrase pairs with similar probability,
it is possible to discard a significant portion of the
translation model, while minimizing the impact on
the model?s predictions.
3.2 Relative Entropy Model for Machine
Translation
For each phrase pair pa, we define the supporting
set SP (pa(s, t)) = S1, ..., Sk, where each element
Si = pi, ..., pj is a distinct derivation of pa(s, t) that
translates s to t, with probability P (Si) = P (pi) ?
...?P (pj). A phrase pair can have multiple elements
in its supporting set. For instance, the phrase pair
p(John in Portugal, John em Portugal), has 3
elements in the support set:
? S1 = {p(John, John), p(in, em), p(Portugal, Portugal)}
? S2 = {p(John, John), p(in Portugal, em Portugal)}
? S3 = {p(John in, John em), p(Portugal, Portugal)}
S1, S2 and S3 encode 3 different assumptions
about the event of translating ?John in Portugal?
to ?John em Portugal?. S1 assumes that the event
is composed by 3 independent events A1, B1 and
C1, S2 assumes that A1 and B1 are dependent, and
groups them into a single composite event A1 ?B1,
which is independent from C1, and S3 groups A1
and C1 independently from B1. As expected, the
event encoded in the phrase pair p itself isA1?B1?
C1, which assumes thatA1,B1 andC1 are all depen-
dent. We can see that if any of the events S1, S2 or
S3 has a ?similar probability? as the event coded in
the phrase pair, we can remove this phrase pair with
a minimal impact in the phrase prediction.
To formalize our notion of ?similar probabil-
ity?, we apply the relative entropy or the Kullback-
Leibler divergence, and define the divergence be-
tween a pruned translation model Pp(s, t) and the
unpruned model P (s, t) as:
D(Pp||P ) = ?
?
s,t
P (s, t)log
Pp(t|s)
P (t|s)
(1)
Where Pp(t|s)P (t|s) , measures the deviation from the
probability emission from the pruned model and the
original probability from the unpruned model, for
each source-target pair s, t. This is weighted by
the frequency that the pair s, t is observed, given by
P (s, t).
Our objective is to minimize D(Pp||P ), which
can be done locally by removing phrase pairs p(s, t)
with the lowest values for ?P (s, t)logPp(t|s)P (t|s) . Ide-
ally, we would want to minimize the relative entropy
for all possible source and target sentences, rather
than all phrases in our model. However, minimiz-
ing such an objective function would be intractable
due to reordering, since the probability assigned to a
phrase pair in a sentence pair by each model would
depend on the positioning of all other phrase pairs
used in the sentence. Because of these dependen-
cies, we would not be able to reduce this problem to
a local minimization problem. Thus, we assume that
all phrase pairs have the same probability regardless
of their context in a sentence.
Thus, our pruning algorithm takes a threshold ?
and prunes all phrase pairs that fail to meet the fol-
lowing criteria:
?P (s, t)log
Pp(t|s)
P (t|s)
> ? (2)
The main components of this function is the ratio
between the emission from the pruned model and
965
unpruned models given by Pp(t|s)P (t|s) , and the weight
given to each s, t pair given by P (s, t). In the re-
mainder of this section, we will focus on how to
model each of these components in equation 2.
3.3 Computing P (s, t)
The term P (s, t) can be seen as a weighting function
for each s, t pair. There is no obvious optimal dis-
tribution to model P (s, t). In this work, we apply 2
different distributions for P (s, t). First, an uniform
distribution, where all phrases are weighted equally.
Secondly, a multinomial function defined as:
P (s, t) =
N(s, t)
N
(3)
whereN is the number of sentence pairs in the paral-
lel data, and N(s, t) is the number of sentence pairs
where s was observed in the source sentence and t
was observed in the target sentence. Using this dis-
tribution, the model is more biased in pruning phrase
pairs with s, t pairs that do not occur frequently.
3.4 Computing Pp(t|s)P (t|s)
The computation of Pp(t|s)P (t|s) depends on how the de-
coder adapts when a phrase pair is pruned from the
model. In the case of back-off language models,
this can be solved by calculating the difference of
the logs between the n-gram estimate and the back-
off estimate. However, a translation decoder gen-
erally functions differently. In our work, we will
assume that the decoding will be performed using
a Viterbi decoder, such as MOSES (Koehn et al
2007), where the translation with the highest score
is chosen.
In the example above, where s=?John in Portu-
gal? and t=?John em Portugal?, the decoder would
choose the derivation with the highest probability
from s to t. Using the unpruned model, the possi-
ble derivations are either using phrase p(s, t) or one
element of its support set S1, S2 or S3. On the other
hand, on the pruned model where p(s, t) does not
exist, only S1, S2 and S3 can be used. Thus, given
a s, t pair one of three situations may occur. First, if
the probability of the phrase pair p(s, t) is lower than
the highest probability element in SP (p(s, t)), then
both the models will choose that element, in which
case, Pp(t|s)P (t|s) = 1. This can happen, if we define
features that penalize longer phrase pairs, such as
lexical weighting, or if we apply smoothing (Foster
et al 2006). Secondly, if the probability of p(s, t)
is equal to the most likely element in SP (p(s, t)),
regardless of whether the unpruned model choses to
use p(s, t) or that element, the probability emissions
of the pruned and unpruned model will be identi-
cal. Thus, for this case Pp(t|s)P (t|s) = 1. Finally, if the
probability of p(s, t) is higher than other possible
derivations, the unpruned model will choose to emit
the probability of p(s, t), while the pruned model
will emit the most likely element in SP (p(s, t)).
Hence, the probability loss between the 2 models,
will be the ratio between the probability of p(s, t)
and the probability of the most likely element in
SP (p(s, t)).
From the example above, we can generalize the
function for Pp(t|s)P (t|s) as:
?
p??argmax(SP (p(s,t))) P (p
?)
P (p(s, t))
(4)
Where P (p(s, t)) denotes the probability of
p(s, t) and
?
p??argmax(SP (p(s,t))) P (p
?) the most
likely sequence of phrasal translations that translates
s to t, with the probability equal to the product of all
phrase translation probabilities in that sequence.
Replacing in equation 2, our final condition that
must be satisfied for keeping a phrase pair is:
?P (s, t)log
?
p??argmax(SP (p(s,t))) P (p
?)
P (p(s, t))
> ? (5)
4 Application for Phrase-based Machine
Translation
We will now show how we apply our entropy prun-
ing model in the state-of-the-art phrase-based trans-
lation system MOSES and describe the problems
that need to be addressed during the implementation
of this model.
4.1 Translation Model
The translation model in Moses is composed by
a phrase translation model and a phrase reorder-
ing model. The first one models, for each phrase
pair p(s, t), the probability of translating the s to
t by combining multiple features ?i, weighted by
966
wTi , as PT (p) =
?n
i=1 ?i(p)
wTi . The reordering
model is similar, but models the local reordering be-
tween p, given the previous and next phrase accord-
ing to the target side, pP and pN , or more formally,
PR(p|pP , pN ) =
?m
i=1 ?i(p|pP , pP )
wRi
4.2 Building the Support Set
Essentially, implementing our model is equiva-
lent to calculating the components described in
equation 5. These are P (s, t), P (p(s|t)) and
argmax(SP (p(s, t))). Calculating the uniform dis-
tribution and multinomial distributions for P (s, t)
is simple, the uniform distribution just assumes the
same value for all s and t, and the multinomial dis-
tribution can be modeled by extracting counts from
the parallel corpora.
Calculating P (s|t) is also trivial, since it only en-
volves calculating PT (p(s, t)), which can be done
by retrieving the translation features of p and apply-
ing the weights for each feature.
The most challenging task is to calculate
argmax(SP (p(s, t))), which is similar to the de-
coding task in machine translation, where we need to
find the best translation t? for a sentence s, that is, t? =
argmaxtP (s|t)P (t). In practice, we are not search-
ing in the space of possible translations, but in the
space of possible derivations, which are sequences
of phrase translations p1(s1, t1), ..., pn(sn, tn) that
can be applied to s to generate an output t with the
score given by P (t)
?n
i=1 P (si, ti).
Our algorithm to determine SP (p(s, t)) can be
described as an adaptation to the decoding algorithm
in Moses, where we restrict the search space to the
subspace SP (p(s, t)), that is, our search space is
only composed by derivations that output t, with-
out using p itself. This can be done using the forced
decoding algorithm proposed in (Schwartz, 2008).
Secondly, the score of a given translation hypothesis
does not depend on the language model probability
P (t), since all derivations in this search space have
the same t, thus we discard this probability from
the score function. Finally, rather than using beam
search, we exhaustively search all the search space,
to reduce the hypothesis of incurring a search error
at this stage. This is possible, since phrase pairs are
generally smaller than text (less than 8 words), and
because we are constraining the search space to t,
which is an order of magnitude smaller than the reg-
ular search space with all possible translations.
4.3 Pruning Algorithm
The algorithm to generate a pruned translation
model is shown in 1. We iterate over all phrase pairs
p1(s1, t1), ..., pn(sn, tn), decode using our forced
decoding algorithm from si to ti, to obtain the best
path S. If no path is found then it means that the pi
is atomic. Then, we prune pi based on condition 5.
Algorithm 1 Independence Pruning
Require: pruning threshold ?,
unpruned model {p1(s1, t1), ..., pn(sn, tn)}
for pi(si, ti) ? {p1(s1, t1), ..., pn(sn, tn)} do
S := argmax(SP (pi)) \ pi
score :=?
if S 6= {} then
score := ?P (s, t)log
?
p?(s?,t?)?S P (s
?|t?)
P (s|t)
end if
if score ? ? then
prune(pi)
end if
end for
return pruned model
The main bottle neck in this algorithm is find-
ing argmax(SP (pi)). While this appears relatively
simple and similar to a document decoding task, the
size of our task is on a different order of magni-
tude, since we need to decode every phrase pair in
the translation model, which might not be tractable
for large models with millions of phrase pairs. We
address this problem in section 5.3.
Another problem with this algorithm is that the
decision to prune each phrase pair is made assuming
that all other phrase pairs will remain in the model.
Thus, there is a chance a phrase pair p1 is pruned
because of a derivation using p2 and p3 that leads to
the same translation. However, if p3 also happens to
be pruned, such a derivation will no longer be pos-
sible. One possible solution to address this problem
is to perform pruning iteratively, from the smallest
phrase pairs (number of words) and increase the size
at each iteration. However, we find this undesirable,
since the model will be biased into removing smaller
phrase pairs, which are generally more useful, since
they can be used in multiple derivation to replace
larger phrase pairs. In the example above, the model
967
would eliminate p3 and keep p1, yet the best deci-
sion could be to keep p3 and remove p1, if p3 is also
frequently used in derivations of other phrase pairs.
Thus, we leave the problem of finding the best set of
phrases to prune as future work.
5 Experiments
We tested the performance of our system under two
different environments. The first is the small scale
DIALOG translation task for IWSLT 2010 evalua-
tion (Paul et al 2010) using a small corpora for
the Chinese-English language pair (henceforth re-
ferred to as ?IWSLT?). The second one is a large
scale test using the complete EUROPARL (Koehn,
2005) corpora for the Portuguese-English language
pair, which we will denote by ?EUROPARL?.
5.1 Corpus
The IWSLT model was trained with 30K training
sentences. The development corpus and test corpus
were taken from the evaluation dataset in IWSLT
2006 (489 tuning and 500 test sentences with 7 ref-
erences). The EUROPARL model was trained using
the EUROPARL corpora with approximately 1.3M
sentence pairs, leaving out 1K sentences for tuning
and another 1K sentences for tests.
5.2 Setup
In the IWSLT experiment, word alignments were
generated using an HMM model (Vogel et al 1996),
with symmetric posterior constraints (V. Grac?a et
al., 2010), using the Geppetto toolkit2. This setup
was used in the official evaluation in (Ling et al
2010). For the EUROPARL experiment the word
alignments were generated using IBM model 4. In
both experiments, the translation model was built
using the phrase extraction algorithm (Paul et al
2010), with commonly used features in Moses (Ex:
probability, lexical weighting, lexicalized reordering
model). The optimization of the translation model
weights was done using MERT tuning (Och, 2003)
and the results were evaluated using BLEU-4.
5.3 Pruning Setup
Our pruning algorithm is applied after the translation
model weight optimization with MERT. We gener-
2http://code.google.com/p/geppetto/
ate multiple translation models by setting different
values for ?, so that translation models of different
sizes are generated at intervals of 5%. We also run
the significance pruning (Johnson and Martin, 2007)
algorithm in these conditions.
While the IWSLT translation model has only
88,424 phrase pairs, for the EUROPARL exper-
iment, the translation model was composed by
48,762,372 phrase pairs, which had to be decoded.
The average time to decode each phrase pair us-
ing the full translation model is 4 seconds per sen-
tence, since the table must be read from disk due to
its size. This would make translating 48M phrase
pairs unfeasible. To address this problem, we di-
vide the phrase pairs in the translation model into
blocks of K phrase pairs, that are processed sepa-
rately. For each block, we resort to the approach
used in MERT tuning, where the model is filtered to
only include the phrase pairs that are used for trans-
lating tuning sentences. We filter each block with
phrase pairs fromK to 2K with the source sentences
sK , ..., s2K . Furthermore, since we are force de-
coding using the target sentences, we also filter the
remaining translation models using the target sen-
tences tK , ..., t2K . We used blocks of 10,000 phrase
pairs and each filtered table was reduced to less than
1% of the translation table on average, reducing the
average decoding time to 0.03 seconds per sentence.
Furthermore, each block can be processed in parallel
allowing multiple processes to be used for the task,
depending on the resources that are available.
5.4 Results
Figure 1 shows the BLEU results for different sizes
of the translation model for the IWSLT experiment
using the uniform and multinomial distributions for
P (s, t). We observe that there is a range of values
from 65% to 95% where we actually observe im-
provements caused by our pruning algorithm, with
the peak at 85% for the uniform distribution, where
we improve from 15.68 to 15.82 (0.9% improve-
ment). Between 26% and 65%, the BLEU score is
lower than the baseline at 100%, with the minimum
at 26% with 15.54, where only atomic phrase pairs
remain and both the multinomial and uniform distri-
bution have the same performance, obviously. This
is a considerable reduction in phrase table size by
sacrificing 0.14 BLEU points. Regarding the com-
968
15.5	 ?
15.55	 ?
15.6	 ?
15.65	 ?
15.7	 ?
15.75	 ?
15.8	 ?
15.85	 ?
25
%	 ?
30
%	 ?
35
%	 ?
40
%	 ?
45
%	 ?
50
%	 ?
55
%	 ?
60
%	 ?
65
%	 ?
70
%	 ?
75
%	 ?
80
%	 ?
85
%	 ?
90
%	 ?
95
%	 ?
10
0%
	 ?
IWSLT	 ?Results	 ?
Uniform	 ?
Mul?nomial	 ?
Figure 1: Results for the IWSLT experiment. The x-
axis shows the percentage of the phrase table used. The
BLEU scores are shown in the y-axis. Two distributions
for P (s, t) were tested Uniform and Multinomial.
parison between the uniform and multinomial distri-
bution, we can see that both distributions yield sim-
ilar results, specially when a low number of phrase
pairs is pruned. In theory, the multinomial distri-
bution should yield better results, since the pruning
model will prefer to prune phrase pairs that are more
likely to be observed. However, longer phrase pairs,
which tend compete with other long phrase pairs on
which get pruned first. These phrase pairs gener-
ally occur only once or twice, so the multinomial
model will act similarly to the uniform model re-
garding longer phrase pairs. On the other hand, as
the model size reduces, we can see that using multi-
nomial distribution seems to start to improve over
the uniform distribution.
The comparison between our pruning model and
pruning based on significance is shown in table 2.
These models are hard to compare, since not all
phrase table sizes can be obtained using both met-
rics. For instance, the significance metric can ei-
ther keep or remove all phrase pairs that only appear
once, leaving a large gap of phrase table sizes that
cannot be attained. In the EUROPARL experiment
the sizes of the table suddenly drops from 60% to
8%. The same happens with our metric that cannot
distinguish atomic phrase pairs. In the EUROPARL
experiment, we cannot generate phrase tables with
sizes smaller than 15%. Thus, we only show re-
sults at points where both algorithms can produce
a phrase table.
Significant improvements are observed in the
Table size Significance Entropy (u) Entropy (m)
Pruning Pruning Pruning
IWSLT
57K (65%) 14.82 15.77 15.78
71K (80%) 15.14 15.76 15.77
80K (90%) 15.31 15.73 15.72
88K (100%) 15.68 15.68 15.68
EUROPARL
29M (60%) 28.64 28.82 28.91
34M (70%) 28.84 28.94 28.99
39M (80%) 28.86 28.99 28.99
44M (90%) 28.91 29.00 29.02
49M (100%) 29.18 29.18 29.18
Table 2: Comparison between Significance Pruning (Sig-
nificance Pruning) and Entropy-based pruning using the
uniform (Entropy (u) Pruning) and multinomial distribu-
tions (Entropy (m) Pruning).
IWSLT experiment, where significance pruning
does not perform as well. On the other hand, on the
EUROPARL experiment, our model only achieves
slightly higher results. We believe that this is re-
lated by the fact the EUROPARL corpora is gener-
ated from automatically aligning documents, which
means that there are misaligned sentence pairs.
Thus, many spurious phrase pairs are extracted. Sig-
nificance pruning performs well under these condi-
tions, since the measure is designed for this purpose.
In our metric, we do not have any means for detect-
ing spurious phrase pairs, in fact, spurious phrase
pairs are probably kept in the phrase table, since
each distinct spurious phrase pair is only extracted
once, and thus, they have very few derivations in
its support set. This suggests, that the significance
score can be integrated in our model to improve our
model, which we leave as future work.
John married Portugal
married 
in
in 
Portugal
married 
married 
in
John 
in 
Portugal
Portugal
a)
b)
Figure 2: Translation order in for different reordering
starting from left to right.
We believe that in language pairs such as Chinese-
969
English with large distance reorderings between
phrases are more prone to search errors and benefit
more from our pruning algorithm. To illustrate this,
let us consider the source sentence ?John married
in Portugal?, and translating either using the blocks
?John?, ?married? and ?in Portugal? or the blocks
?John?, ?married in?, ?Portugal?, the first hypoth-
esis would be much more viable, since the word
?Portugal? is more relevant as the context for the
word ?in?. Thus, the key choice for the decoder is
to decide whether to translate using ?married? with
or without ?in?, and it is only able to predict that
it is better to translate ?married? by itself until it
finds that ?in? is better translated with ?Portugal?.
Thus, a search error occurs if the hypothesis where
?married? is translated by itself is removed. In fig-
ure 2, we can see the order that blocks are consid-
ered for different reorderings, starting from left to
right. In a), we illustrate the case for a monotonous
translation. We observe that the correct decision be-
tween translating ?married in? or just ?married? is
found immediately, since the blocks ?Portugal? and
?in Portugal? are considered right afterwards. In this
case, it is unlikely that the hypothesis using ?mar-
ried? is removed. However, if we consider that due
to reordering, ?John? is translated after ?married?
and before ?Portugal?, which is shown in b). Then,
the correct decision can only be found after consid-
ering ?John?. In this case, ?John? does not have
many translations, so the likelihood of eliminating
the correct hypothesis. However, if there were many
translations for John, it is highly likely that the cor-
rect partial hypothesis is eliminated. Furthermore,
the more words exist between ?married? and ?Portu-
gal?, the more likely will the correct hypothesis not
exist when we reach ?Portugal?. By pruning the hy-
pothesis ?married in? a priori, we contribute in pre-
venting such search errors.
We observe that some categories of phrase pairs
that are systematically pruned, but these cannot
be generalized in rules, since there are many ex-
ceptions. The most obvious type of phrase pairs
are phrases with punctuations, such as ???.? to
?thanks .? and ?. ??? to ?thanks .?, since ?.?
is translated independently from most contextual
words. However, this rule should not be general-
ized, since in some cases ?.? is a relevant contextual
marker. For instance, the word ?please? is translated
to ??? in the sentence ?open the door, please.? and
translated to ????? in ?please my advisors?. An-
other example are sequences of numbers, which are
generally translated literally. For instance, ??(8)
?(3)?(8)? is translated to ?eight three eight? (Ex:
?room eight three eight?). Thus, phrase pairs for
number sequences can be removed, since those num-
bers can be translated one by one. However, for se-
quences such as ??(1)?(8)?, we need a phrase pair
to represent this specifically. This is because ??(1)?
can be translated to ?one?, but also to ?a?, ?an?, ?sin-
gle?. Other exceptions include ??(1)?(1)?, which
tends to be translated as ?eleven?, and which tends to
be translated to ?o?, rather than ?zero? in sequences
(?room eleven o five?).
6 Conclusions
We present a pruning algorithm for Machine Trans-
lation based on relative entropy, where we assess
whether the translation event encoded in a phrase
pair can be decomposed into combinations of events
encoded in other phrase pairs. We show that such
phrase pairs can be removed from the translation
model with little negative impact or even a positive
one in the overall translation quality. Tests show that
our method yields comparable or better results with
state of the art pruning algorithms.
As future work, we would like to combine our
approach with significance pruning, since both ap-
proaches are orthogonal and address different issues.
We also plan to improve the pruning step of our algo-
rithm to find the optimal set of phrase pairs to prune
given the pruning threshold.
The code used in this work will be made available.
7 Acknowledgements
This work was partially supported by FCT (INESC-
ID multiannual funding) through the PIDDAC Pro-
gram funds, and also through projects CMU-
PT/HuMach/0039/2008 and CMU-PT/0005/2007.
The PhD thesis of Wang Ling is supported by FCT
grant SFRH/BD/51157/2010. The authors also wish
to thank the anonymous reviewers for many helpful
comments.
970
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19:263?311, June.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?06, pages 53?61, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
J Howard Johnson and Joel Martin. 2007. Improv-
ing translation quality by discarding most of the
phrasetable. In In Proceedings of EMNLP-CoNLL?07,
pages 967?975.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 48?54, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-burch, Richard Zens, Rwth Aachen, Alexan-
dra Constantin, Marcello Federico, Nicola Bertoldi,
Chris Dyer, Brooke Cowan, Wade Shen, Christine
Moran, and Ondrej Bojar. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Summit,
pages 79?86, Phuket, Thailand. AAMT, AAMT.
Wang Ling, Tiago Lu??s, Joa?o Grac?a, Lu??sa Coheur, and
Isabel Trancoso. 2010. Towards a general and ex-
tensible phrase-extraction algorithm. In IWSLT ?10:
International Workshop on Spoken Language Transla-
tion, pages 313?320, Paris, France.
Stephen Vogal Matthias Eck and Alex Waibel. 2007. Es-
timating phrase pair relevance for translation model
pruning. MTSummit XI.
Robert C. Moore and Chris Quirk. 2009. Less is more:
significance-based n-gram selection for smaller, bet-
ter language models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 2 - Volume 2, EMNLP ?09,
pages 746?755, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Michael Paul, Marcello Federico, and Sebastian Stu?ker.
2010. Overview of the iwslt 2010 evaluation cam-
paign. In IWSLT ?10: International Workshop on Spo-
ken Language Translation, pages 3?27.
Lane Schwartz. 2008. Multi-source translation methods.
In Proceedings of AMTA, pages 279?288.
Kristie Seymore and Ronald Rosenfeld. 1996. Scalable
backoff language models. In In Proceedings of ICSLP,
pages 232?235.
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In In Proc. DARPA Broad-
cast News Transcription and Understanding Work-
shop, pages 270?274.
Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.
2009. Complexity-based phrase-table filtering for sta-
tistical machine translation. MTSummit XII, Aug.
Joa?o V. Grac?a, Kuzman Ganchev, and Ben Taskar. 2010.
Learning Tractable Word Alignment Models with
Complex Constraints. Comput. Linguist., 36:481?504.
S. Vogel, H. Ney, and C. Tillmann. 1996. Hmm-
based word alignment in statistical translation. In
Proceedings of the 16th conference on Computational
linguistics-Volume 2, pages 836?841. Association for
Computational Linguistics.
971
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 73?84,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Paraphrasing 4 Microblog Normalization
Wang Ling Chris Dyer Alan W Black Isabel Trancoso
L2F Spoken Systems Lab, INESC-ID, Lisbon, Portugal
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
Instituto Superior Te?cnico, Lisbon, Portugal
{lingwang,cdyer,awb}@cs.cmu.edu
isabel.trancoso@inesc-id.pt
Abstract
Compared to the edited genres that have
played a central role in NLP research, mi-
croblog texts use a more informal register with
nonstandard lexical items, abbreviations, and
free orthographic variation. When confronted
with such input, conventional text analysis
tools often perform poorly. Normalization
? replacing orthographically or lexically id-
iosyncratic forms with more standard variants
? can improve performance. We propose a
method for learning normalization rules from
machine translations of a parallel corpus of
microblog messages. To validate the utility of
our approach, we evaluate extrinsically, show-
ing that normalizing English tweets and then
translating improves translation quality (com-
pared to translating unnormalized text) using
three standard web translation services as well
as a phrase-based translation system trained
on parallel microblog data.
1 Introduction
Microblogs such as Twitter, Sina Weibo (a popular
Chinese microblog service) and Facebook have re-
ceived increasing attention in diverse research com-
munities (Han and Baldwin, 2011; Hawn, 2009, in-
ter alia). In contrast to traditional text domains that
use carefully controlled, standardized language, mi-
croblog content is often informal, with less adher-
ence to conventions regarding punctuation, spelling,
and style, and with a higher proportion of dialect
or pronouciation-derived orthography. While this
diversity itself is an important resource for study-
ing, e.g., sociolinguistic variation (Eisenstein et al,
2011; Eisenstein, 2013), it poses challenges to NLP
applications developed for more formal domains. If
retaining variation due to sociolinguistic or phono-
logical factors is not crucial, text normalization can
improve performance on downstream tasks (?2).
This paper introduces a data-driven approach to
learning normalization rules by conceiving of nor-
malization as a kind of paraphrasing and taking
inspiration from the bilingual pivot approach to
paraphrase detection (Bannard and Callison-Burch,
2005) and the observation that translation is an
inherently ?simplifying? process (Laviosa, 1998;
Volansky et al, 2013). Starting from a parallel cor-
pus of microblog messages consisting of English
paired with several other languages (Ling et al,
2013), we use standard web machine translation sys-
tems to re-translate the non-English segment, pro-
ducing ?English original,English MT? pairs (?3).
These are our normalization examples, with MT out-
put playing the role of normalized English. Sev-
eral techniques for identifying high-precision nor-
malization rules are proposed, and we introduce a
character-based normalization model to account for
predictable character-level processes, like repetition
and substitution (?4). We then describe our decod-
ing procedure (?5) and show that our normaliza-
tion model improve translation quality for English?
Chinese microblog translation (?6).1
2 Why Normalize?
Consider the English tweet shown in the first row of
Table 1 which contains several elements that NLP
1The datasets used in this paper are available from http:
//www.cs.cmu.edu/?lingwang/microtopia.
73
Table 1: Translations of an English microblog message
into Mandarin, using three web translation services.
orig. To DanielVeuleman yea iknw imma work on that
MT1 ?iknw DanielVeuleman?????
MT2 DanielVeuleman?iknw???????
MT3 ?DanielVeuleman??iknw imma??????
systems trained on edited domains may not handle
well. First, it contains several nonstandard abbre-
viations, such as, yea, iknw and imma (abbrevia-
tions of yes, I know and I am going to). Second,
there is no punctuation in the text although stan-
dard convention would dictate that it should be used.
To illustrate the effect this can have, consider now
the translations produced by Google Translate,2 Mi-
crosoft Bing,3 and Youdao,4 shown in rows 2?4.
Even with no knowledge of Chinese, it is not hard
to see that all engines have produced poor transla-
tions: the abbreviation iknw is left translated by all
engines, and imma is variously deleted, left untrans-
lated, or transliterated into the meaningless sequence
?? (pronounced y?? ma?).
While normalization to a form like To Daniel
Veuleman: Yes, I know. I am going to work on that.
does indeed lose some information (information im-
portant for an analysis of sociolinguistic or phono-
logical variation clearly goes missing), it expresses
the propositional content of the original in a form
that is more amenable to processing by traditional
tools. Translating the normalized form with Google
Translate produces ????Veuleman?????
???????????, which is a substantial
improvement over all translations in Table 1.
3 Obtaining Normalization Examples
We want to treat normalization as a supervised learn-
ing problem akin to machine translation, and to do
so, we need to obtain pairs of microblog posts and
their normalized forms. While it would be possible
to ask annotators to create such a corpus, it would
be quite expensive to obtain large numbers of ex-
amples. In this section, we propose a method for
creating normalization examples without any human
2http://translate.google.com/
3http://www.bing.com/translator
4http://fanyi.youdao.com/
Table 2: Translations of Chinese original post to English
using web-based service.
orig. To DanielVeuleman yea iknw imma work on that
orig. ?DanielVeuleman?????????
?????????
MT1 Right DanielVeuleman say, yes, I know, I?m
Xiangna efforts
MT2 DanielVeuleman said, Yes, I know, I?m that hard
MT3 Said to DanielVeuleman, yes, I know, I?m to
that effort
annotation, by leveraging existing tools and data re-
sources.
The English example sentence in Table 1 was se-
lected from the ?topia parallel corpus (Ling et
al., 2013), which consists of self-translated mes-
sages from Twitter and Sina Weibo (i.e., each mes-
sage contains a translation of itself). Row 2 of
Table 2 shows the Mandarin self-translation from
the corpus. The key observation is what happens
when we automatically translate the Mandarin ver-
sion back into English. Rows 3?5 shows automatic
translations from three standard web MT engines.
While not perfect, the translations contain several
correctly normalized subphrases. We will use such
re-translations as a source of (noisy) normalization
examples. Since such self-translations are relatively
numerous on microblogs, this technique can provide
a large amount of data.
Of course, to motivate this paper, we argued that
NLP tools ? like the very translation systems we
propose to use ? often fail on unnormalized input.
Is this a problem? We argue that it is not for the
following two reasons.
Normalization in translation. Work in transla-
tion studies has observed that translation tends to
be a generalizing process that ?smooths out? author-
and work-specific idiosyncrasies (Laviosa, 1998;
Volansky et al, 2013). Assuming this observa-
tion is robust, we expect that dialectal variant forms
found in microblogs to be normalized in translation.
Therefore, if the parallel segments in our microblog
parallel corpus did indeed originate through a trans-
lation process (rather than, e.g., being generated as
two independent utterances from a bilingual), we
may then state the following assumption about the
distribution of variant forms in a parallel segment
74
?e, f?: if e contains nonstandard lexical variants,
then f is likely to be a normalized translation using
with fewer nonstandard lexical variants (and vice-
versa).
Uncorrelated orthographic variants. Any writ-
ten language has the potential to make creative use
of orthography: alphabetic scripts can render ap-
proximations of pronunciation variants; logographic
scripts can use homophonic substitutions. However,
the kinds of innovations used in particular languages
will be language specific (depending on details of
the phonology, lexicon, and orthography of the lan-
guage). However, for language pairs that differ sub-
stantially in these dimensions, it may not always
be possible (or at least easy) to preserve particular
kinds of nonstandard orthographic forms in trans-
lation. Consider the (relatively common) pronoun-
verb compounds like iknw and imma from our mo-
tivating example: since Chinese uses a logographic
script without spaces, there is no obvious equivalent.
3.1 Variant?Normalized Parallel Corpus
For the two reasons outlined above, we argue that
we will be able to translate back into English us-
ing MT, even when the underlying English part of
the parallel corpus has a great deal of nonstandard
content. We leverage this fact to build the normal-
ization corpus, where the original English tweet is
treated as the variant form, and the automatic trans-
lation obtained from another language is considered
a potential normalization.5
Our process is as follows. The microblog cor-
pus of Ling et al (2013) contains sentence pairs ex-
tracted from Twitter and Sina Weibo, for multiple
language pairs. We use all corpora that include En-
glish as one of the languages in the pair. The respec-
tive non-English side is translated into English using
different translation engines. The different sets we
used and the engines we used to translate are shown
in Table 3. Thus, for each original English post o,
we obtain n paraphrases {pi}
n
i=1, from n different
translation engines.
5We additionally assume that the translation engines are
trained to output more standardized data, so there will be addi-
tional normalizing effect from the machine translation system.
Table 3: Corpora Used for Paraphrasing.
Lang. Pair Source Segs. MT Engines
ZH-EN Weibo 800K Google, Bing, Youdao
ZH-EN Twitter 113K Google, Bing, Youdao
AR-EN Twitter 114K Google, Bing
RU-EN Twitter 119K Google, Bing
KO-EN Twitter 78K Google, Bing
JA-EN Twitter 75K Google, Bing
3.2 Alignment and Filtering
Our parallel microblog corpus was crawled automat-
ically and contains many misaligned sentences. To
improve precision, we attempt to find the similar-
ity between the (unnormalized) original and each
of the normalizations using an alignment based on
the one used in METEOR (Denkowski and Lavie,
2011), which computes the best alignment between
the original tweet and each of the normalizations
but modified to permit domain-specific approximate
matches. To address lexical variants, we allow fuzzy
word matching, that is, we allow lexically similar,
such as yea and yes to be aligned (similarity is de-
termined by the Levenshtein distance). We also per-
form phrasal matchings, such as ikwn to i know. To
do so, we extend the alignment algorithm from word
to phrasal alignments. More precisely, given the
original post o and a candidate normalization n, we
wish to find the optimal segmentation producing a
good alignment. A segmentation s = ?s1, . . . , s|s|?
is a sequence of segments that aligns as a block to a
source word. For instance, for the sentence yea iknw
imma work on that, one possible segmentation could
be s1 =yea ikwn, s2 =imma and s3 =work on that.
Model. We define the score of an alignment a and
segmentation s in using a model that makes semi-
Markov independence assumptions, similar to the
work in (Bansal et al, 2011), u(a, s | o,n) =
|s|?
i=1
[
ue(si, ai | n)? ut(ai | ai?1)? u`(|si|)
]
In this model, the maximal scoring segmentation
and alignment can be found using a polynomial time
dynamic programming algorithm. Each segment
can be aligned to any word or segment in o. The
aligned segment for sk is defined as ak. For the
75
score of a segment correspondence ue(s, a | n), we
assume that this can be estimated using the lexical
similarity between segments, which we define to be
1? L(sk,ak)max{|sk|,|ak|} , where L(x, y) denotes the Leven-
shtein distance between strings x and y, normalized
by the highest possible distance between those seg-
ments.
For the alignment score ut, we assume that the
relative order of the two sequences will be mostly
monotonous. Thus, we approximate ut with the fol-
lowing density poss(ak) ? pose(ak?1) ? N (1, 1),
where the poss is the index of the first word in the
segment and pose the one of the last word.
After finding the Viterbi alignments, we compute
the similarity measure ? = |A||A|+|U | , used in (Resnik
and Smith, 2003), where |A| and |U | are the number
of words that were aligned and unaligned, respec-
tively. In this work, we extract the pair if ? > 0.2.
4 Normalization Model
From the normalization corpus, we learn a nor-
malization model that generalizes the normalization
process. That is, from the data we observe that To
DanielVeuleman yea iknw imma work on that is nor-
malized to To Daniel Veuleman: yes, I know. I
am going to work on that. However, this is not
useful, since the chances of the exact sentence To
DanielVeuleman yea iknw imma work on that occur-
ring in the data is low. We wish to learn a process to
convert the original tweet into the normalized form.
There are two mechanisms that we use in our
model. The first (?4.1) learns word?word and
phrase?phrase mappings. That is, we wish to find
that DanielVeuleman is normalized to Daniel Veule-
man, that iknw is normalized to I know and that
imma is normalized to I am going. These mappings
are more useful, since whenever iknw occurs in the
data, we have the option to normalize it to I know.
The second (?4.2) learns character sequence map-
pings. If we look at the normalization DanielVeule-
man to Daniel Veuleman, we can see that it is only
applicable when the exact word DanielVeuleman oc-
curs. However, we wish to learn that it is uncom-
mon for the letters l and v to occur in the same word
sequentially, so that be can add missing spaces in
words that contain the lv character sequence, such as
normalizing phenomenalvoter to phenomenal voter.
I wanna go 4 pizza 2day
I want go for pizza todayto
Figure 1: Variant?normalized alignment with the variant
form above and the normalized form below; solid lines
show potential normalizations, while dashed lines repre-
sent identical translations.
However, there are also cases where this is not true,
for instance, in the word velvet, we do not wish to
separate the letters l and v. Thus, we shall describe
the process we use to decide when to apply these
transformations.
4.1 From Sentences To Phrases
The process to find phrases from sentences has been
throughly studied in Machine Translation. This is
generally done in two steps, Word Alignments and
Phrase Extraction.
Alignment. The first step is to find the word-level
alignments between the original post and its nor-
malization. This is a well studied problem in MT,
referred as Word Alignment (Brown et al, 1993).
Many alignment models have been proposed, such
as, the HMM-based word alignment models (Vo-
gel et al, 1996) and the IBM models (Och and
Ney, 2003). Generally, a symmetrization step is per-
formed, where the bidirectional alignments are com-
bined heuristically. In our work, we use the fast
aligner proposed in (Dyer et al, 2013) to obtain the
word alignments. Figure 1 shows an example of an
word aligned pair of a tweet and its normalization.
Phrase Extraction. The phrasal extraction
step (Ling et al, 2010), uses the word aligned
sentences and extracts phrasal mappings between
the original tweet and its normalization, named
phrase pairs. For instance, in Figure 1, we would
like to extract the phrasal mapping from go 4 to go
for, so that we learn that the word 4 in the context of
go is normalized to the proposition for. To do this,
the most common approach is to use the template
proposed in (Och and Ney, 2004), which allows
phrase pairs to be extracted, if there is at least one
word alignment within the pair, and there are no
76
Table 4: Fragment of the phrase normalization model
built, for each original phrase o, we present the top-3 nor-
malized forms ranked by f(n | o).
Original (o) Normalization (n) f(n | o)
wanna want to 0.4679
wanna will 0.0274
wanna going to 0.0114
4 4 0.5641
4 for 0.01795
go 4 go for 1.0000
words inside the pair that are aligned to words not
in the pair. For instance, in the example above, the
phrase pair that normalizes wanna to want to would
be extracted, but the phrase pair normalizing wanna
to want to go would not, because the word go in the
normalization is aligned to a word not in the pair.
Phrasal Features. After extracting the phrase
pairs, a model is produced with features derived
from phrase pair occurrences during extraction. This
model is equivalent to phrasal translation model in
MT, but we shall refer to it as the normalization
model. For a phrase pair ?o,n?, where o is the origi-
nal phrase, and n is the normalized phrase, we com-
pute the normalization relative frequency f(n | o) =
C(n,o)
C(o) , where C(n, o) denotes the number of times
o was normalized to n and C(o) denotes the number
of times o was seen in the extracted phrase pairs. Ta-
ble 4 gives a fragment of the normalization model.
The columns represent the original phrase, its nor-
malization and the probability, respectively.
In Table 4, we observe that the abbreviation
wanna is normalized to want to with a relatively
high probability, but it can also be normalized to
other equivalent expressions, such as will and go-
ing to. The word 4 by itself has a low probability
to be normalized to the preposition for. This is ex-
pected, since this decision cannot be made without
context. However, we see that the phrase go 4 is
normalized to go for with a high probability, which
specifies that within the context of go, 4 is generally
used as a preposition.
4.2 From Phrases to Characters
While we can learn lexical variants that are in the
corpora using the phrase model, we can only address
word forms that have been observed in the corpora.
Table 5: Fragment of the character normalization model
where examples representative of the lexical variant gen-
eration process are encoded in the model.
Original (o) Normalization (n) f(n | o)
o o o o o 0.0223
o o o o 0.0439
s c 0.0331
z s 0.0741
s h c h 0.019
2 t o 0.014
4 f o r 0.0013
0 o 0.0657
i n g f o r i n g <space> f o r 0.4545
g f g <space> f 0.01028
This is quite limited, since we cannot expect all the
word forms to be present, such as all the possible
orthographic errors for the word cat, such as catt,
kat and caaaat. Thus, we will build a character-
based model that learns the process lexical variants
are generated at the subword level.
Our character-based model is similar to the
phrase-based model, except that, rather than learn-
ing word-based mappings from the original tweet
and the normalization sentences, we learn character-
based mappings from the original phrases to the nor-
malizations of those phrases. Thus, we extract the
phrase pairs in the phrasal normalization model, and
use them as a training corpora. To do this, for each
phrase pair, we add a start token, <start>, and a
end token, <end>, at the beginning and ending of
the phrase pair. Afterwards, we separate all charac-
ters by space and add a space token <space> where
spaces were originally. For instance, the phrase
pair normalizing DanielVeuleman to Daniel Veule-
man would be converted to <start> d a n i e l v e u
l e m a n <end> and <start> d a n i e l <space> v
e u l e m a n <end>.
Character-based Normalization Model - To
build the character-based model, we proceed using
the same approach as in the phrasal normalization
model. We first align characters using Word Align-
ment Models, and then we perform phrase extrac-
tion to retrieve the phrasal character segments, and
build the character-based model by collecting statis-
tics. Once again, we provide examples of entries in
the model in Table 5.
77
We observe that many of the normalizations dealt
with in the previous model by memorizing phrases
are captured with string transformations. For in-
stance, from phrase pairs such as tooo to too and
sooo to so, we learn that sequences of o?s can be
reduced to 2 or 1 o. Other examples include or-
thographic substitutions, such as 2 for to and 4
for for (as found in 2gether, 2morrow, 4ever and
4get). Moreover, orthographic errors can be gener-
ated from mistaking characters with similar phonetic
properties, such as, s to c, z to s and sh to ch, gener-
ating lexical variants such as reprecenting. Finally,
we learn that the number 0 that resembles the letter
o, can be used as a replacement, as in g00d. Finally,
we can see that the rule ingfor to ing for attempts to
find segmentation errors, such as goingfor, where a
space between going and for was omitted.6
5 Normalization Decoder
In section 4, we built two models to learn the process
of normalization, the phrase-based model and the
character-based model. In this section, we describe
the decoder we used to normalize the sentences.
The advantage of the phrase-based model is that it
can make decisions for normalization based on con-
text. That is, it contains phrasal units, such as, go
4, that determine, when the word 4 should be nor-
malized to the preposition for and when to leave it
as a number. However, it cannot address words that
are unseen in the corpora. For instance, if the word
form 4ever is not seen in the training corpora, it is
not be able to normalize it, even if it has seen the
word 4get normalized to forget. On the other hand,
the character-based model learns subword normal-
izations, for instance, if we see the word nnnnno
normalized to no, we can learn that repetitions of
the letter n are generally shorted to n, which al-
lows it to generate new word forms. This model
has strong generalization potential, but the weak-
ness of the character-based model is that it fails to
6Note that this captures the context in which such transfor-
mations are likely to occur: there are not many words that con-
tain the sequence ingfor, so the probability that these should be
normalized by inserting a space is high. On the other hand, we
cannot assume that if we observe the sequence gf, we can safely
separate these with a space. This is because, there are many
words that contain this sequence, such as the abbreviation of
gf (girlfriend), dogfight, and bigfoot.
consider the context of the normalization that the
phrase-based model uses to make normalization de-
cisions. Thus, our goal in this section is describe a
decoder that uses both models to improve the quality
of the normalizations.
5.1 Phrasal Decoder
We use Moses, an off-the-shelf phrase-based MT
system (Koehn et al, 2007), to ?translate? the orig-
inal tweet its normalized form using the phrasal
model (?4.1). Aside form the normalization prob-
ability, we also use the common features used in
MT. These are the reverse normalization probabil-
ity, the lexical and reverse lexical probabilities and
the phrase penalty. We also use the MSD reorder-
ing model proposed in (Koehn et al, 2005), which
adds reordering features.7 The final score of each
phrase pair is given as a sum of weighted log fea-
tures. The weights for these features are optimized
using MERT (Och, 2003). In our work, we sampled
150 tweets randomly from Twitter and normalized
them manually, and used these samples as devel-
opment data for MERT. As for the character-based
model features, we simply rank the training phrase
pairs by their relative frequency the f(n | o), and use
the top-1000 phrase pairs as development set. Fi-
nally, a language model is required during decoding
as a prior, since it defines the type of language that
is produced by the output. We wish to normalized
to formal language, which is generally better pro-
cessed by NLP tools. Thus, for the phrase model,
we use the English NIST dataset composed of 8M
sentences in English from the news domain to build
a 5-gram Kneser-Ney smoothed language model.
5.2 Character and Phrasal Decoder
We now turn to how to apply the character-based
(?4.2), together with the phrasal model. For this
model, we again use Moses, treating each charac-
ter as a ?word?. The simplest way to combine both
methods is first to decode the input o sentence with
the character-based decoder, normalizing each word
independently and then normalizing the resulting
output using the phrase-based decoder, which en-
ables the phrase model to score the outputs of the
character model in context.
7Reordering helps find lexical variants that are generated by
transposing characters, such as, mabye to maybe.
78
0 1 2 3 4 5 6
I
wanna
want to meeeeet
meet
met
DanielVeuleman
Daniel Veuleman
Figure 2: Example output lattice of the character-based decoder, for the sentence I wanna meeeeet DanielVeuleman.
Our process is as follows. Given the input sen-
tence o, with the words o1, . . . , om, where m is
the number of words in the input, we generate for
each word oi a list of n-best normalization candi-
dates z1oi , . . . , z
n
oi . We further filter the candidates
using two criteria. We start by filtering each can-
didate zjoi that occurs less frequently than the orig-
inal word oi. This is motivated by our observation
that lexical variants occur far less than the respec-
tive standard form. Second, we build a corpus of
English language Twitter consisting of 70M tweets,
extract the unigram counts, and perform Brown clus-
tering (Brown et al, 1992) with k = 3000 clusters.
Next, we calculate the cluster similarity between oi
and each surviving candidate, zjoi . We filter the can-
didate if the similarity is less than 0.8. The similar-
ity between two clusters represented as bit strings,
S[c(oi), c(z
j
oi)], calculated as:
S(x, y) =
2 ? |lpm{x, y)}|
|x|+ |y|
,
where lpm computes the longest common prefix of
the contexts and |x| is the length of the bit string.8
If a candidate contains more than one word (because
a space was inserted), we set its count as the mini-
mum count among its words. To find the cluster for
multiple word units, we concatenate the words to-
gether, and find the cluster with the resulting word if
it exists. This is motivated by the fact that it is com-
mon for missing spaces to exist in microblog cor-
pora, generating new word forms, such as wantto,
goingfor, and given a large enough corpora as the
one we used, these errors occur frequently enough to
be placed in the correct cluster. In fact, the variants
such as wanna and tmi, occur in the same clusters as
the words wantto and toomuchinformation.
Remaining candidates are combined into a word
lattice, enabling us to perform lattice-based decod-
8Brown clusters are organized such that more words with
more similar distributions share common prefixes.
ing with the phrasal model (Dyer et al, 2008). Fig-
ure 2, provides an example of such a lattice for the
variant sentence I wanna meeeet DanielVeuleman.
5.3 Learning Variants from Monolingual Data
Until now, we learned normalizations from pairs of
original tweets and their normalizations. We shall
now describe a process to leverage monolingual doc-
uments to learn new normalizations, since the mono-
lingual data is far easier to obtain than parallel data.
This process is similar to the work in (Han et al,
2012), where confusion sets of contextually simi-
lar words are built initially as potential normaliza-
tion candidates. We again use the k = 3000 Brown
clusters,9 and this time consider the contents of each
cluster as a set of possible normalization variants.
For instance, we find that the cluster that includes the
word never, also includes the variant forms neverrrr,
neva and nevahhh. However, the cluster also con-
tains non-variant forms, such as gladly and glady.
Thus, we want to find that neverrrr maps to never,
while glady maps to gladly in the same cluster. Our
work differs from previous work in that, rather than
defining features manually, we use our character-
based decoder to find the mappings between lexical
variants and their normalizations.
For every word type wi in cluster c(wi) =
{w1, . . . , wn}, we generate a set of possible candi-
dates for each word w1i , . . . , w
m
i . Then, we build
a directed acyclic graph (DAG), where every word.
We add an edge between wi and wj , if wi can be
decoded into wj using the character model from the
previous section, and also if wi occurs less than wj ;
the second condition guarantees that the graph will
be acyclic. Sample graphs are shown in Figure 3.
Afterwards, we find the number of paths between
all nodes in the graph (this can be computed effi-
ciently in O(|V | + |E|) time). Then, for each word
9The Brown clustering algorithm groups words together
based on contextual similarity.
79
neverr
neva neve
nevar
never
glady
gladly
cladly
Figure 3: Example DAGs, built from the cluster contain-
ing the words never and gladly.
wi, we find the wj to which it has the highest num-
ber of paths to and extract the normalization of wi
to wj . In case of a tie, we choose the word wj that
occurs more often in the monolingual corpora. This
is motivated by the fact that normalizations are tran-
sitive. Thus, even if neva cannot be decoded directly
to never, we can use nevar as an intermediate step to
find the correct normalization. This is performed for
all the clusters, and the resulting dictionary of lexi-
cal variants mapped to their standard forms is added
to the training data of the character-based model.
6 Experiments
We evaluate our normalization model intrinsically
by testing whether our normalizations more closely
resemble standardized data, and then extrinsically
by testing whether we can improve the translation
quality of in-house as well as online Machine Trans-
lation systems by normalizing the input.
6.1 Setup
We use the gold standard by Ling et al (2013), com-
posed by 2581 English-Mandarin microblog sen-
tence pairs. From this set, we randomly select 1290
pairs for development and 1291 pairs for testing.
The normalizer model is trained on the corpora
extracted and filtered in section 3, in total, there
were 1.3M normalization pairs used during training.
The test sentences are normalized using four differ-
ent setups. The first setup leaves the input sentence
unchanged, which we call No Norm. The second
uses the phrase-based model to normalize the input
sentence, which we will denote Norm+phrase. The
third uses the character-based model to output lat-
tices, and then decodes with the phrase based model,
which we will denote Norm+phrase+char. Finally,
we test the same model after adding the training data
extracted using monolingual documents, which we
will refer as Norm+phrase+char+mono.
To test the normalizations themselves, we used
Google Translate to translate the Mandarin side of
the 1291 test sentence pairs back to English and use
the original English tweet. While, this is by itself
does not guarantee that the normalizations are cor-
rect, since the normalizations could be syntactically
and semantically incorrect, it will allow us to check
whether the normalizations are closer to those pro-
duced by systems trained on news data. This exper-
iment will be called Norm.
As an application and extrinsic evaluation for our
normalizer, we test if we can obtain gains on the
MT task on microblog data by using our normalizer
prior to translation. We build two MT systems us-
ing Moses. Firstly, we build a out-of-domain model
using the full 2012 NIST Chinese-English dataset
(approximately 8M sentence pairs), which is dataset
from the news domain, and we will denote this sys-
tem as Inhouse+News. Secondly, we build a in-
domain model using the 800K sentence pairs from
?topia corpora (Ling et al, 2013). We also add
the NIST dataset to improve coverage. We call this
system Inhouse+News+Weibo. To train these sys-
tems, we use the Moses phrase-based MT system
with standard features (Koehn et al, 2003). For re-
ordering, we use the MSD reordering model (Axel-
rod et al, 2005). As the language model, we train
a 5-gram model with Kneser-ney smoothing using a
10M tweets from twitter. Finally, the weights were
tuned using MERT (Och, 2003). As for online sys-
tems, we consider the systems used to generate the
paraphrase corpora in section 3, which we will de-
note as Online A, Online B and Online C10
The normalization and MT results are evaluated
with BLEU-4 (Papineni et al, 2002) comparing the
produced translations or normalizations with the ap-
propriate reference.
6.2 Results
Results are shown in Table 6. In terms of the normal-
izations, we observe a much better match between
10The names of the systems are hidden to not violate the pri-
vacy issues in the terms and conditions of these online systems.
80
Table 6: Normalization and MT Results. Rows denote different normalizations, and columns different translation
systems, except the first column (Norm), which denotes the normalization experiment. Cells display the BLEU score
of that experiment.
Moses Moses
Condition Norm (News) (News+Weibo) Online A Online B Online C
baseline 19.90 15.10 24.37 20.09 17.89 18.79
norm+phrase 21.96 15.69 24.29 20.50 18.13 18.93
norm+phrase+char 22.39 15.87 24.40 20.61 18.22 19.08
norm+phrase+char+mono 22.91 15.94 24.46 20.78 18.37 19.21
the normalized text with the reference, than the orig-
inal tweets. In most cases, adding character-based
models improves the quality of the normalizations.
We observe that better normalizations tend to lead
to better translations. The relative improvements
are most significant, when moving from No Norm
to norm+phrase normalization. This is because,
we are normalizing words that are not seen in gen-
eral MT system?s training data, but occur frequently
in microblog data, such as wanna to want to, u to
you and im to i?m. The only exception is in the In-
house+News+Weibo system, where the normaliza-
tion deteriorates the results. This is to be expected,
since this system is trained on the same microblog
data used to learn the normalizations. However, we
can observe on norm+phrase+char that if we add
the character-based model, we can observe improve-
ments for this system as well as for all other ones.
This is because the model is actually learning nor-
malizations that are unseen in the data. Some ex-
amples of these normalization include, normalizing
lookin to looking, nutz to nuts and maimi to miami
but also separating peaceof to peace of. The fact
that these improvements are obtained for all sys-
tems is strong evidence that we are actually produc-
ing good normalizations, and not overfitting to one
of the systems that we used to generate our data.
The gains are much smaller from norm+phrase
to norm+phrase+char, since the improvements we
obtain come from normalizing less frequent words.
Finally, we can obtain another small improvement
by adding monolingual data to the character-based
model in norm+phrase+char+mono.
7 Related Work
Most of the work in microblog normalization is fo-
cused on finding the standard forms of lexical vari-
ants (Yang and Eisenstein, 2013; Han et al, 2013;
Han et al, 2012; Kaufmann, 2010; Han and Bald-
win, 2011; Gouws et al, 2011; Aw et al, 2006). A
lexical variant is a variation of a standard word in
a different lexical form. This ranges from minor or
major spelling errors, such as jst, juxt and jus that
are lexical variants of just, to abbreviations, such as
tmi and wanna, which stand for too much informa-
tion and want to, respectively. Jargon can also be
treated as variants, for instance cday is a slang word
for birthday, in some groups.
There are many rules that govern the process lex-
ical variants are generated. Some variants are gener-
ated from orthographic errors, caused by some mis-
take from the user when writing. For instance, the
variants representin, representting, or reprecenting
can be generated by a spurious letter swap, insertion
or substitution by the user. One way to normalize
these types of errors is to attempt to insert, remove
and swap words in a lexical variant until a word in
a dictionary of standard words is found (Kaufmann,
2010). Contextual features are another way to find
lexical variants, since variants generally occur in the
same context as their standard form. This includes
orthographic errors, abbreviations and slang. How-
ever, this is generally not enough to detect lexical
variants, as many words share similar contexts, such
as already, recently and normally. Consequently,
contextual features are generally used to generate a
confusion set of possible normalizations of a lexical
variant, and then more features are used to find the
correct normalization (Han et al, 2012). One simple
approach is to compute the Levenshtein distance to
find lexical similarities between words, which would
effectively capture the mappings between represent-
ting, reprecenting and representin to representing.
However, a pronunciation model (Tang et al, 2012)
81
would be needed to find the mapping between g8,
2day and 4ever to great, today and forever, respec-
tively. Moreover, visual character similarity features
would be required to find the mapping between g00d
and? to good and i.
Clearly, learning this process is a challenging
task, and addressing each different case individually
would require vast amounts of resources. Further-
more, once we change the language to normalize
to another language, the types of rules that generate
lexical variants would radically change and a new set
of features would have to be engineered. We believe
that to be successful in normalizing microblogs,
the process to learn new lexical variants should be
learned from data, making as few assumptions as
possible. We learn our models without using any
type of predefined features, such as phonetic fea-
tures or lexical features. In fact, we will not assume
that most words and characters map to themselves,
as it is assumed in methods using the Levenshtein
distance (Kaufmann, 2010; Han et al, 2012; Wang
and Ng, 2013). All these mappings are learned from
our data. Furthermore, in the work above, the dictio-
naries built using these methods assume that lexical
variants are mapped to standard forms in a word-to-
word mapping. Thus, variants such as wanna, gonna
and imma are not normalizable, since they are nor-
malized to multiple words want to, going to and I
am gonna. Moreover, there are segmentation errors
that occur from missing spaces, such as sortof and
goingfor, which also map to more than one word to
sort of and going for. These cases shall also be ad-
dressed in our work.
Wang and Ng (2013) argue that microblog nor-
malization is not simply to map lexical variants into
standard forms, but that other tasks, such as punctua-
tion correction and missing word recovery should be
performed. Consider the example tweet you free?,
while there are no lexical variants in this message,
the authors consider that it is the normalizer should
recover the missing article are and normalize this
tweet to are you free?. To do this, the authors train a
series of models to detect and correct specific errors.
While effective for narrow domains, training models
to address each specific type of normalization is not
scalable over all types of normalizations that need to
be performed within the language, and the fact that a
set of new models must be implemented for another
language limits the applicability of this work.
Another strong point of the work above is that
a decoder is presented, while the work on build-
ing dictionaries only normalize out of vocabu-
lary (OOV) words. The work on (Han et al, 2012)
trains a classifier to decide whether to normalize a
word or not, but is still preconditioned on the fact
that the word in question is OOV. Thus, lexical vari-
ants, such as, 4 and u, with the standard forms for
and you, are left untreated, since they occur in other
contexts, such as u in u s a. Inspired by the work
above, we also propose a decoder based on the exist-
ing off-the-self decoder Moses (Koehn et al, 2007).
Finally, the work in (Xu et al, 2013) obtains para-
phrases from Twitter, by finding tweets that contain
common entities, such as Obama, that occur during
the same period by matching temporal expressions.
The resulting paraphrase corpora can also be used to
train a normalizer.
8 Conclusion
We introduced a data-driven approach to microblog
normalization based on paraphrasing. We build a
corpora of tweets and their normalizations using par-
allel corpora from microblogs using MT techniques.
Then, we build two models that learn generalizations
of the normalization process, one the phrase level
and on the character level. Then, we build a de-
coder that combines both models during decoding.
Improvements on multiple MT systems support the
validity of our method.
In future work, we shall attempt to build normal-
izations for other languages. We shall also attempt
to learn an unsupervised normalization model with
only monolingual data, similar to the work for MT
in (Ravi and Knight, 2011).
Acknowledgements
The PhD thesis of Wang Ling is supported by FCT ?
Fundac?a?o para a Cie?ncia e a Tecnologia, under project
SFRH/BD/51157/2010. This work was supported by na-
tional funds through FCT ? Fundac?a?o para a Cie?ncia e a
Tecnologia, under project PEst-OE/EEI/LA0021/2013.
The authors also wish to express their gratitude to the
anonymous reviewers for their comments and insight.
82
References
[Aw et al2006] AiTi Aw, Min Zhang, Juan Xiao, and
Jian Su. 2006. A phrase-based statistical model for
SMS text normalization. In Proceedings of the ACL,
COLING-ACL ?06, pages 33?40, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Axelrod et al2005] Amittai Axelrod, Ra Birch Mayne,
Chris Callison-burch, Miles Osborne, and David Tal-
bot. 2005. Edinburgh system description for the 2005
iwslt speech translation evaluation. In In Proc. Inter-
national Workshop on Spoken Language Translation
(IWSLT.
[Bannard and Callison-Burch2005] Colin Bannard and
Chris Callison-Burch. 2005. Paraphrasing with bilin-
gual parallel corpora. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 597?604, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
[Bansal et al2011] Mohit Bansal, Chris Quirk, and
Robert C. Moore. 2011. Gappy phrasal alignment by
agreement. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies - Volume 1, HLT ?11,
pages 1308?1317, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Brown et al1992] Peter F Brown, Peter V Desouza,
Robert L Mercer, Vincent J Della Pietra, and Jenifer C
Lai. 1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4):467?479.
[Brown et al1993] Peter F. Brown, Vincent J. Della
Pietra, Stephen A. Della Pietra, and Robert L. Mer-
cer. 1993. The mathematics of statistical machine
translation: parameter estimation. Comput. Linguist.,
19:263?311, June.
[Denkowski and Lavie2011] Michael Denkowski and
Alon Lavie. 2011. Meteor 1.3: Automatic metric
for reliable optimization and evaluation of machine
translation systems. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
85?91, Edinburgh, Scotland, July. Association for
Computational Linguistics.
[Dyer et al2008] Chris Dyer, Smaranda Muresan, and
Philip Resnik. 2008. Generalizing word lattice trans-
lation. In Proceedings of HLT-ACL.
[Dyer et al2013] Chris Dyer, Victor Chahuneau, and
Noah A Smith. 2013. A simple, fast, and effective
reparameterization of ibm model 2. In Proceedings of
NAACL-HLT, pages 644?648.
[Eisenstein et al2011] Jacob Eisenstein, Noah A. Smith,
and Eric P. Xing. 2011. Discovering sociolinguis-
tic associations with structured sparsity. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies - Volume 1, HLT ?11, pages 1365?1374,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
[Eisenstein2013] Jacob Eisenstein. 2013. What to do
about bad language on the internet. In Proceedings
of NAACL-HLT, pages 359?369.
[Gouws et al2011] Stephan Gouws, Dirk Hovy, and Don-
ald Metzler. 2011. Unsupervised mining of lexical
variants from noisy text. In Proceedings of the First
Workshop on Unsupervised Learning in NLP, EMNLP
?11, pages 82?90, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Han and Baldwin2011] Bo Han and Timothy Baldwin.
2011. Lexical normalisation of short text messages:
makn sens a #twitter. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume
1, HLT ?11, pages 368?378, Stroudsburg, PA, USA.
Association for Computational Linguistics.
[Han et al2012] Bo Han, Paul Cook, and Timothy Bald-
win. 2012. Automatically constructing a normalisa-
tion dictionary for microblogs. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, EMNLP-CoNLL ?12, pages 421?
432, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
[Han et al2013] Bo Han, Paul Cook, and Timothy Bald-
win. 2013. Lexical normalization for social media
text. ACM Transactions on Intelligent Systems and
Technology (TIST), 4(1):5.
[Hawn2009] Carleen Hawn. 2009. Take two aspirin and
tweet me in the morning: how twitter, facebook, and
other social media are reshaping health care. Health
affairs, 28(2):361?368.
[Kaufmann2010] M. Kaufmann. 2010. Syntactic Nor-
malization of Twitter Messages. studies, 2.
[Koehn et al2003] Philipp Koehn, Franz Josef Och, and
Daniel Marcu. 2003. Statistical phrase-based trans-
lation. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy - Volume 1, NAACL ?03, pages 48?54, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
[Koehn et al2005] Philipp Koehn, Amittai Axelrod,
Alexandra Birch Mayne, Chris Callison-Burch, Miles
Osborne, David Talbot, and Michael White. 2005.
Edinburgh system description for the 2005 nist mt
evaluation. In Proceedings of Machine Translation
Evaluation Workshop 2005.
[Koehn et al2007] Philipp Koehn, Hieu Hoang, Alexan-
dra Birch, Chris Callison-burch, Richard Zens, Rwth
83
Aachen, Alexandra Constantin, Marcello Federico,
Nicola Bertoldi, Chris Dyer, Brooke Cowan, Wade
Shen, Christine Moran, and Ondrej Bojar. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
[Laviosa1998] Sara Laviosa. 1998. Core patterns of
lexical use in a comparable corpus of English lexical
prose. Meta, 43(4):557?570.
[Ling et al2010] Wang Ling, Tiago Lu??s, Joa?o Grac?a,
Lu??sa Coheur, and Isabel Trancoso. 2010. Towards a
general and extensible phrase-extraction algorithm. In
IWSLT ?10: International Workshop on Spoken Lan-
guage Translation, pages 313?320, Paris, France.
[Ling et al2013] Wang Ling, Guang Xiang, Chris Dyer,
Alan Black, and Isabel Trancoso. 2013. Microblogs
as parallel corpora. In Proceedings of the 51st An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?13. Association for Computational Lin-
guistics.
[Och and Ney2003] Franz Josef Och and Hermann Ney.
2003. A systematic comparison of various statis-
tical alignment models. Computational linguistics,
29(1):19?51.
[Och and Ney2004] Franz Josef Och and Hermann Ney.
2004. The alignment template approach to statistical
machine translation. Comput. Linguist., 30(4):417?
449, December.
[Och2003] Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL ?03,
pages 160?167, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Papineni et al2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine transla-
tion. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics, ACL ?02,
pages 311?318, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Ravi and Knight2011] Sujith Ravi and Kevin Knight.
2011. Deciphering foreign language. In ACL, pages
12?21.
[Resnik and Smith2003] Philip Resnik and Noah A
Smith. 2003. The web as a parallel corpus. Com-
putational Linguistics, 29(3):349?380.
[Tang et al2012] Hao Tang, Joseph Keshet, and Karen
Livescu. 2012. Discriminative pronunciation mod-
eling: A large-margin, feature-rich approach. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers-Volume
1, pages 194?203. Association for Computational Lin-
guistics.
[Vogel et al1996] S. Vogel, H. Ney, and C. Tillmann.
1996. Hmm-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics-Volume 2, pages 836?841. Asso-
ciation for Computational Linguistics.
[Volansky et al2013] Vered Volansky, Noam Ordan, and
Shuly Wintner. 2013. On the features of transla-
tionese. Literary and Linguistic Computing.
[Wang and Ng2013] Pidong Wang and Hwee Ng. 2013.
A beam-search decoder for normalization of social
media text with application to machine translation. In
Proceedings of NAACL-HLT 2013, NAACL ?13. As-
sociation for Computational Linguistics.
[Xu et al2013] Wei Xu, Alan Ritter, and Ralph Grish-
man. 2013. Gathering and generating paraphrases
from twitter with application to normalization. In Pro-
ceedings of the Sixth Workshop on Building and Us-
ing Comparable Corpora, pages 121?128, Sofia, Bul-
garia, August. Association for Computational Linguis-
tics.
[Yang and Eisenstein2013] Yi Yang and Jacob Eisenstein.
2013. A log-linear model for unsupervised text nor-
malization. In Proc. of EMNLP.
84
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 450?454,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Reordering Modeling using Weighted Alignment Matrices
Wang Ling, Tiago Lu??s, Joa?o Grac?a, Lu??sa Coheur and Isabel Trancoso
L2F Spoken Systems Lab
INESC-ID Lisboa
{wang.ling,tiago.luis,joao.graca}@inesc-id.pt
{luisa.coheur,isabel.trancoso}@inesc-id.pt
Abstract
In most statistical machine translation sys-
tems, the phrase/rule extraction algorithm uses
alignments in the 1-best form, which might
contain spurious alignment points. The usage
of weighted alignment matrices that encode all
possible alignments has been shown to gener-
ate better phrase tables for phrase-based sys-
tems. We propose two algorithms to generate
the well known MSD reordering model using
weighted alignment matrices. Experiments on
the IWSLT 2010 evaluation datasets for two
language pairs with different alignment algo-
rithms show that our methods produce more
accurate reordering models, as can be shown
by an increase over the regular MSD models
of 0.4 BLEU points in the BTEC French to
English test set, and of 1.5 BLEU points in the
DIALOG Chinese to English test set.
1 Introduction
The translation quality of statistical phrase-based
systems (Koehn et al, 2003) is heavily dependent
on the quality of the translation and reordering mod-
els generated during the phrase extraction algo-
rithm (Ling et al, 2010). The basic phrase extrac-
tion algorithm uses word alignment information to
constraint the possible phrases that can be extracted.
It has been shown that better alignment quality gen-
erally leads to better results (Ganchev et al, 2008).
However the relationship between the word align-
ment quality and the results is not straightforward,
and it was shown in (Vilar et al, 2006) that better
alignments in terms of F-measure do not always lead
to better translation quality.
The fact that spurious word alignments might oc-
cur leads to the use of alternative representations for
word alignments that allow multiple alignment hy-
potheses, rather than the 1-best alignment (Venu-
gopal et al, 2009; Mi et al, 2008; Christopher
Dyer et al, 2008). While using n-best alignments
yields improvements over using the 1-best align-
ment, these methods are computationally expen-
sive. More recently, the method described in (Liu
et al, 2009) produces improvements over the meth-
ods above, while reducing the computational cost
by using weighted alignment matrices to represent
the alignment distribution over each parallel sen-
tence. However, their results were limited by the
fact that they had no method for extracting a reorder-
ing model from these matrices, and used a simple
distance-based model.
In this paper, we propose two methods for gener-
ating the MSD (Mono Swap Discontinuous) reorder-
ing model from the weighted alignment matrices.
First, we test a simple approach by using the 1-best
alignment to generate the reordering model, while
using the alignment matrix to produce the translation
model. This reordering model is a simple adaptation
of the MSD model to read from alignment matrices.
Secondly, we develop two algorithms to infer the re-
ordering model from the weighted alignment matrix
probabilities. The first one uses the alignment infor-
mation within phrase pairs, while the second uses
contextual information of the phrase pairs.
This paper is organized as follows: Section 2 de-
scribes the MSD model; Section 3 presents our two
algorithms; in Section 4 we report the results from
the experiments conducted using these algorithms,
450
and comment on the results; we conclude in Sec-
tion 5.
2 MSD models
Moses (Koehn et al, 2007) allows many config-
urations for the reordering model to be used. In
this work, we will only refer to the default config-
uration (msd-bidirectional-fe), which uses the MSD
model, and calculates the reordering orientation for
the previous and the next word, for each phrase pair.
Other possible configurations are simpler than the
default one. For instance, the monotonicity model
only considers monotone and non-monotone orien-
tation types, whereas the MSD model also considers
the monotone orientation type, but distinguishes the
non-monotone orientation type between swap and
discontinuous. The approach presented in this work
can be adapted to the other configurations.
In the MSD model, during the phrase extraction,
given a source sentence S and a target sentence T ,
the alignment set A, where aji is an alignment from i
to j, the phrase pair with words in positions between
i and j in S, Sji , and n and m in T , T
m
n , can be
classified with one of three orientations with respect
to the previous word:
? The orientation is monotonous if only the pre-
vious word in the source is aligned with the pre-
vious word in the target, or, more formally, if
an?1i?1 ? A ? a
n?1
j+1 /? A.
? The orientation is swap, if only the next word
in the source is aligned with the previous word
in the target, or more formally, if an?1j+1 ? A ?
an?1i?1 /? A.
? The orientation is discontinuous if neither of
the above are true, which means, (an?1i?1 ?
A ? an?1j+1 ? A) ? (a
n?1
i?1 /? A ? a
n?1
j+1 /? A).
The orientations with respect to the next word are
given analogously. The reordering model is gener-
ated by grouping the phrase pairs that are equal, and
calculating the probabilities of the grouped phrase
pair being associated each orientation type and di-
rection, based on the orientations for each direction
that are extracted. Formally, the probability of the
phrase pair p having a monotonous orientation is
prev 
word(s)
source phrase
target phrase
prev 
word(t)
next 
word(s)
source phrase
target phrase
prev 
word(t)
a) b)
c)
source phrase
target phrase
prev 
word(t)
d)
next 
word(s)
source phrase
target phrase
prev 
word(t)
prev 
word(s)
Figure 1: Enumeration of possible reordering cases with
respect to the previous word. Case a) is classified as
monotonous, case b) is classified as swap and cases c)
and d) are classified as discontinuous.
given by:
P (p,mono) = C(mono)C(mono)+C(swap)+C(disc) (1)
Where C(o) is the number of times a phrase is ex-
tracted with the orientation o in that group of phrase
pairs. Moses also provides many options for this
stage, such as types of smoothing. We use the de-
fault smoothing configuration which adds the fixed
value of 0.5 to all C(o).
3 Weighted MSD Model
When using a weighted alignment matrix, rather
than working with alignments points, we use the
probability of each word in the source aligning with
each word in the target. Thus, the regular MSD
model cannot be directly applied here.
One obvious solution to solve this problem is to
produce a 1-best alignment set alng with the align-
ment matrix, and use the 1-best alignment to gen-
erate the reordering model, while using the align-
ment matrix to produce the translation model. How-
ever, this method would not be taking advantage of
the weighted alignment matrix. The following sub-
sections describe two algorithms that are proposed
to make use of the alignment probabilities.
3.1 Score-based
Each phrase pair that is extracted using the algorithm
described in (Liu et al, 2009) is given a score based
on its alignments. This score is higher if the align-
ment points in the phrase pair have high probabili-
ties, and if the alignment is consistent. Thus, if an
451
extracted phrase pair has better quality, its orienta-
tion should have more weight than phrase pairs with
worse quality. We implement this by changing the
C(o) function in equation 1 from being the number
of the phrase pairs with the orientation o, to the sum
of the scores of those phrases. We also need to nor-
malize the scores for each group, due to the fixed
smoothing that is applied, since if the sum of the
scores is much lower (e.g. 0.1) than the smoothing
factor (0.5), the latter will overshadow the weight
of the phrase pairs. The normalization is done by
setting the phrase pair with the highest value of the
sum of all MSD probabilities to 1, and readjusting
other phrase pairs accordingly. Thus, a group of 3
phrase pairs that have the MSD probability sums of
0.1, 0.05 and 0.1, are all set to 1, 0.5 and 1.
3.2 Context-based
We propose an alternative algorithm to calculate
the reordering orientations for each phrase pair.
Rather than classifying each phrase pair with either
monotonous (M ), swap (S) or discontinuous (D),
we calculate the probability for each orientation, and
use these as weighted counts when creating the re-
ordering model. Thus, for the previous word, given
a weighted alignment matrix W , the phrase pair be-
tween the indexes i and j in S, Sji , and n and m in
T , Tmn , the probability values for each orientation
are given by:
? Pc(M) = W
n?1
i?1 ? (1?W
n?1
j+1 )
? Pc(S) = W
n?1
j+1 ? (1?W
n?1
i?1 )
? Pc(D) = W
n?1
i?1 ?W
n?1
j+1
+ (1?Wn?1i?1 )? (1?W
n?1
j+1 )
These formulas derive from the adaptation of con-
ditions of each orientation presented in 2. In the
regular MSD model, the previous orientation for a
phrase pair is monotonous if the previous word in
the source phrase is aligned with the previous word
in the target phrase and not aligned with the next
word. Thus, the probability of a phrase pair to have a
monotonous orientation Pc(M) is given by the prob-
ability of the previous word in the source phrase
being aligned with the previous word in the target
phrase Wn?1i?1 , and the probability of the previous
word in the source to not be aligned with the next
word in the target (1 ? Wn?1j+1 ). Also, the sum of
the probabilities of all orientations (Pc(M), Pc(S),
Pc(D)) for a given phrase pair can be trivially shown
to be 1. The probabilities for the next word are
given analogously. Following equation 1, the func-
tion C(o) is changed to be the sum of all Pc(o), from
the grouped phrase pairs.
4 Experiments
4.1 Corpus
Our experiments were performed over two datasets,
the BTEC and the DIALOG parallel corpora from
the latest IWSLT evaluation 2010 (Paul et al, 2010).
BTEC is a multilingual speech corpus that contains
sentences related to tourism, such as the ones found
in phrasebooks. DIALOG is a collection of human-
mediated cross-lingual dialogs in travel situations.
The experiments performed with the BTEC cor-
pus used only the French-English subset, while the
ones perfomed with the DIALOG corpus used the
Chinese-English subset. The training corpora con-
tains about 19K sentences and 30K sentences, re-
spectively. The development corpus for the BTEC
task was the CSTAR03 test set composed by 506
sentences, and the test set was the IWSLT04 test set
composed by 500 sentences and 16 references. As
for the DIALOG task, the development set was the
IWSLT09 devset composed by 200 sentences, and
the test set was the CSTAR03 test set with 506 sen-
tences and 16 references.
4.2 Setup
We use weighted alignment matrices based on Hid-
den Markov Models (HMMs), which are produced
by the the PostCAT toolkit1, based on the poste-
rior regularization framework (V. Grac?a et al, 2010).
The extraction algorithm using weighted alignment
matrices employs the same method described in (Liu
et al, 2009), and the phrase pruning threshold was
set to 0.1. For the reordering model, we use the
distance-based reordering, and compare the results
with the MSD model using the 1-best alignment.
Then, we apply our two methods based on align-
ment matrices. Finally, we combine our two meth-
ods above by adapting the function C(o), to be the
1http://www.seas.upenn.edu/ strctlrn/CAT/CAT.html
452
sum of all Pc(o), weighted by the scores of the re-
spective phrase pairs. The optimization of the trans-
lation model weights was done using MERT, and
each experiment was run 5 times, and the final score
is calculated as the average of the 5 runs, in order to
stabilize the results. Finally, the results were eval-
uated using BLEU-4, METEOR, TER and TERp.
The BLEU-4 and METEOR scores were computed
using 16 references. The TER and TERp were com-
puted using a single reference.
4.3 Reordering model comparison
Tables 1 and 2 show the scores using the differ-
ent reordering models. Consistent improvements in
the BLEU scores may be observed when changing
from the MSD model to the models generated us-
ing alignment matrices. The results were consis-
tently better using our models in the DIALOG task,
since the English-Chinese language pair is more de-
pendent on the reordering model. This is evident
if we look at the difference in the scores between
the distance-based and the MSD models. Further-
more, in this task, we observe an improvement on all
scores from the MSD model to our weighted MSD
models, which suggests that the usage of alignment
matrices helps predict the reordering probabilities
more accurately.
We can also see that the context based reordering
model performs better than the score based model
in the BTEC task, which does not perform sig-
nificantly better than the regular MSD model in
this task. Furthermore, combining the score based
method with the context based method does not lead
to any improvements. We believe this is because the
alignment probabilities are much more accurate in
the English-French language pair, and phrase pair
scores remain consistent throughout the extraction,
making the score based approach and the regular
MSD model behave similarly. On the other hand,
in the DIALOG task, score based model has bet-
ter performance than the regular MSD model, and
the combination of both methods yields a significant
improvement over each method alone.
Table 3 shows a case where the context based
model is more accurate than the regular MSD model.
The alignment is obviously faulty, since the word
?two? is aligned with both ?deux?, although it
should only be aligned with the first occurrence.
BTEC BLEU METEOR TERp TER
Distance-based 61.84 65.38 27.60 22.40
MSD 62.02 65.93 27.40 22.80
score MSD 62.15 66.18 27.30 22.20
context MSD 62.42 66.29 27.00 22.00
combined MSD 62.42 66.14 27.10 22.20
Table 1: Results for the BTEC task.
DIALOG BLEU METEOR TERp TER
Distance-based 36.29 45.15 49.00 41.20
MSD 39.56 46.85 47.20 39.60
score MSD 40.2 47.16 46.52 38.80
context MSD 40.14 47.14 45.88 39.00
combined MSD 41.03 47.69 46.20 38.20
Table 2: Results for the DIALOG task.
Furthermore, the word ?twin? should be aligned
with ?a` deux lit?, but it is aligned with ?cham-
bres?. If we use the 1-best alignment to compute
the reordering type of the sentence pair ?Je voudrais
re?server deux? / ?I?d like to reserve two?, the re-
ordering type for the following orientation would
be monotonous, since the next word ?chambres?
is falsely aligned with ?twin?. However, it should
clearly be discontinuous, since the right alignment
for ?twin? is ?a` deux lit?. This problem is less seri-
ous when we use the weighted MSD model, since
the orientation probability mass would be divided
between monotonous and discontinuous since the
probability weighted matrix for the wrong alignment
is 0.5. On the BTEC task, some of the other scores
are lower than the MSD model, and we suspect that
this stems from the fact that our tuning process only
attempts to maximize the BLEU score.
5 Conclusions
In this paper we addressed the limitations of the
MSD reordering models extracted from the 1-best
alignments, and presented two algorithms to ex-
tract these models from weighted alignment matri-
ces. Experiments show that our models perform bet-
ter than the distance-based model and the regular
MSD model. The method based on scores showed a
good performance for the Chinese-English language
pair, but the performance for the English-French pair
was similar to the MSD model. On the other hand,
the method based on context improves the results on
453
Alignment Je vo
ud
ra
is
re?
se
rv
er
de
ux
ch
am
br
es
a` de
ux
lit
s
.
I 1
?d 0.7
like 0.7
to
reserve 1
two 1 0.5
twin 0.5 0.5
rooms 1
. 1
Table 3: Weighted alignment matrix for a training sen-
tence pair from BTEC, with spurious alignment proba-
bilities. Alignment points with 0 probabilities are left
empty.
both pairs. Finally, on the Chinese-English test, by
combining both methods we can achieve a BLEU
improvement of approximately 1.5%. The code used
in this work is currently integrated with the Geppetto
toolkit2 , and it will be made available in the next
version for public use.
6 Acknowledgements
This work was partially supported by FCT (INESC-
ID multiannual funding) through the PIDDAC Pro-
gram funds, and also through projects CMU-
PT/HuMach/0039/2008 and CMU-PT/0005/2007.
The PhD thesis of Tiago Lu??s is supported by
FCT grant SFRH/BD/62151/2009. The PhD the-
sis of Wang Ling is supported by FCT grant
SFRH/BD/51157/2010. The authors also wish to
thank the anonymous reviewers for many helpful
comments.
References
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing Word Lattice Translation. Tech-
nical Report LAMP-TR-149, University of Maryland,
College Park, February.
Kuzman Ganchev, Joa?o V. Grac?a, and Ben Taskar. 2008.
Better alignments = better translations? In Proceed-
ings of ACL-08: HLT, pages 986?993, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
2http://code.google.com/p/geppetto/
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 48?54, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-burch, Richard Zens, Rwth Aachen, Alexan-
dra Constantin, Marcello Federico, Nicola Bertoldi,
Chris Dyer, Brooke Cowan, Wade Shen, Christine
Moran, and Ondrej Bojar. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Wang Ling, Tiago Lu??s, Joao Grac?a, Lu??sa Coheur, and
Isabel Trancoso. 2010. Towards a general and ex-
tensible phrase-extraction algorithm. In IWSLT ?10:
International Workshop on Spoken Language Transla-
tion, pages 313?320, Paris, France.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 2 - Volume 2, EMNLP ?09, pages 1017?1026,
Morristown, NJ, USA. Association for Computational
Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192?199, Columbus, Ohio, June. Association
for Computational Linguistics.
Michael Paul, Marcello Federico, and Sebastian Stu?ker.
2010. Overview of the iwslt 2010 evaluation cam-
paign. In IWSLT ?10: International Workshop on Spo-
ken Language Translation, pages 3?27.
Joa?o V. Grac?a, Kuzman Ganchev, and Ben Taskar. 2010.
Learning Tractable Word Alignment Models with
Complex Constraints. Comput. Linguist., 36:481?504.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Wider pipelines: N-best
alignments and parses in MT training.
David Vilar, Maja Popovic, and Hermann Ney. 2006.
Aer: Do we need to ?improve? our alignments? In
International Workshop on Spoken Language Transla-
tion (IWSLT), pages 205?212.
454
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 176?186,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Microblogs as Parallel Corpora
Wang Ling123 Guang Xiang2 Chris Dyer2 Alan Black2 Isabel Trancoso 13
(1)L2F Spoken Systems Lab, INESC-ID, Lisbon, Portugal
(2)Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
(3)Instituto Superior Te?cnico, Lisbon, Portugal
{lingwang,guangx,cdyer,awb}@cs.cmu.edu
isabel.trancoso@inesc-id.pt
Abstract
In the ever-expanding sea of microblog data, there
is a surprising amount of naturally occurring par-
allel text: some users create post multilingual mes-
sages targeting international audiences while oth-
ers ?retweet? translations. We present an efficient
method for detecting these messages and extract-
ing parallel segments from them. We have been
able to extract over 1M Chinese-English parallel
segments from Sina Weibo (the Chinese counter-
part of Twitter) using only their public APIs. As a
supplement to existing parallel training data, our
automatically extracted parallel data yields sub-
stantial translation quality improvements in trans-
lating microblog text and modest improvements
in translating edited news commentary. The re-
sources in described in this paper are available at
http://www.cs.cmu.edu/?lingwang/utopia.
1 Introduction
Microblogs such as Twitter and Facebook have
gained tremendous popularity in the past 10 years.
In addition to being an important form of commu-
nication for many people, they often contain ex-
tremely current, even breaking, information about
world events. However, the writing style of mi-
croblogs tends to be quite colloquial, with fre-
quent orthographic innovation (R U still with me
or what?) and nonstandard abbreviations (idk!
shm)?quite unlike the style found in more tra-
ditional, edited genres. This poses considerable
problems for traditional NLP tools, which were
developed with other domains in mind, which of-
ten make strong assumptions about orthographic
uniformity (i.e., there is just one way to spell you).
One approach to cope with this problem is to an-
notate in-domain data (Gimpel et al, 2011).
Machine translation suffers acutely from the
domain-mismatch problem caused by microblog
text. On one hand, standard models are probably
suboptimal since they (like many models) assume
orthographic uniformity in the input. However,
more acutely, the data used to develop these sys-
tems and train their models is drawn from formal
and carefully edited domains, such as parallel web
pages and translated legal documents. MT training
data seldom looks anything like microblog text.
This paper introduces a method for finding nat-
urally occurring parallel microblog text, which
helps address the domain-mismatch problem.
Our method is inspired by the perhaps surpris-
ing observation that a reasonable number of mi-
croblog users tweet ?in parallel? in two or more
languages. For instance, the American entertainer
Snoop Dogg regularly posts parallel messages on
Sina Weibo (Mainland China?s equivalent of Twit-
ter), for example, watup Kenny Mayne!! - Kenny
Mayne?????????, where an English
message and its Chinese translation are in the
same post, separated by a dash. Our method is able
to identify and extract such translations. Briefly,
this requires determining if a tweet contains more
than one language, if these multilingual utterances
contain translated material (or are due to some-
thing else, such as code switching), and what the
translated spans are.
The paper is organized as follows. Section 2
describes the related work in parallel data extrac-
tion. Section 3 presents our model to extract par-
allel data within the same document. Section 4
describes our extraction pipeline. Section 5 de-
scribes the data we gathered from both Sina Weibo
(Chinese-English) and Twitter (Chinese-English
and Arabic-English). We then present experiments
showing that our harvested data not only substan-
tially improves translations of microblog text with
176
existing (and arguably inappropriate) translation
models, but that it improves the translation of
more traditional MT genres, like newswire. We
conclude in Section 6.
2 Related Work
Automatic collection of parallel data is a well-
studied problem. Approaches to finding par-
allel web documents automatically have been
particularly important (Resnik and Smith, 2003;
Fukushima et al, 2006; Li and Liu, 2008; Uszko-
reit et al, 2010; Ture and Lin, 2012). These
broadly work by identifying promising candidates
using simple features, such as URL similarity or
?gist translations? and then identifying truly par-
allel segments with more expensive classifiers.
More specialized resources were developed using
manual procedures to leverage special features of
very large collections, such as Europarl (Koehn,
2005).
Mining parallel or comparable messages from
microblogs has mainly relied on Cross-Lingual In-
formation Retrieval techniques (CLIR). Jelh et al
(2012) attempt to find pairs of tweets in Twitter us-
ing Arabic tweets as search queries in a CLIR sys-
tem. Afterwards, the model described in (Xu et al,
2001) is applied to retrieve a set of ranked trans-
lation candidates for each Arabic tweet, which are
then used as parallel candidates.
The work on mining parenthetical transla-
tions (Lin et al, 2008), which attempts to find
translations within the same document, has some
similarities with our work, since parenthetical
translations are within the same document. How-
ever, parenthetical translations are generally used
to translate names or terms, which is more lim-
ited than our work which extracts whole sentence
translations.
Finally, crowd-sourcing techniques to obtain
translations have been previously studied and ap-
plied to build datasets for casual domains (Zbib
et al, 2012; Post et al, 2012). These approaches
require remunerated workers to translate the mes-
sages, and the amount of messages translated per
day is limited. We aim to propose a method that
acquires large amounts of parallel data for free.
The drawback is that there is a margin of error in
the parallel segment identification and alignment.
However, our system can be tuned for precision or
for recall.
3 Parallel Segment Retrieval
We will first abstract from the domain of Mi-
croblogs and focus on the task of retrieving par-
allel segments from single documents. Prior work
on finding parallel data attempts to reason about
the probability that pairs of documents (x, y) are
parallel. In contrast, we only consider one doc-
ument at a time, defined by x = x1, x2, . . . , xn,
and consisting of n tokens, and need to deter-
mine whether there is parallel data in x, and if
so, where are the parallel segments and their lan-
guages. For simplicity, we assume that there are
at most 2 continuous segments that are parallel.
As representation for the parallel seg-
ments within the document, we use the tuple
([p, q], l, [u, v], r, a). The word indexes [p, q] and
[u, v] are used to identify the left segment (from
p to q) and right segment (from u to v), which
are parallel. We shall refer [p, q] and [u, v] as the
spans of the left and right segments. To avoid
overlaps, we set the constraint p ? q < u ? v.
Then, we use l and r to identify the language of
the left and right segments, respectively. Finally, a
represents the word alignment between the words
in the left and the right segments.
The main problem we address is to find the
parallel data when the boundaries of the parallel
segments are not defined explicitly. If we knew
the indexes [p, q] and [u, v], we could simply run
a language detector for these segments to find l
and r. Then, we would use an word alignment
model (Brown et al, 1993; Vogel et al, 1996),
with source s = xp, . . . , xq, target t = xu, . . . , xv
and lexical table ?l,r to calculate the Viterbi align-
ment a. Finally, from the probability of the word
alignments, we can determine whether the seg-
ments are parallel.
Thus, our model will attempt to find the opti-
mal values for the segments [p, q][u, v], languages
l, r and word alignments a jointly. However, there
are two problems with this approach. Firstly, word
alignment models generally attribute higher prob-
abilities to smaller segments, since these are the
result of a smaller product chain of probabilities.
In fact, because our model can freely choose the
segments to align, choosing only one word as the
left segment that is well aligned to a word in the
right segment would be the best choice. This
is obviously not our goal, since we would not
obtain any useful sentence pairs. Secondly, in-
ference must be performed over the combination
of all latent variables, which is intractable using
177
a brute force algorithm. We shall describe our
model to solve the first problem in 3.1 and our
dynamic programming approach to make the in-
ference tractable in 3.2.
3.1 Model
We propose a simple (non-probabilistic) three-
factor model that models the spans of the parallel
segments, their languages, and word alignments
jointly. This model is defined as follows:
S([u, v], r, [p, q],l, a | x) =
S?S ([p, q], [u, v] | x)?
S?L(l, r | [p, q], [u, v], x)?
S?T (a | [p, q], l, [u, v], r, x)
Each of the components is weighted by the pa-
rameters ?, ? and ?. We set these values empiri-
cally ? = 0.3, ? = 0.3 and ? = 0.4, and leave the
optimization of these parameters as future work.
We discuss the components of this model in turn.
Span score SS . We define the score of hypothe-
sized pair of spans [p, q], [u, v] as:
SS([p, q], [u, v] | x) =
(q ? p+ 1) + (v ? u+ 1)?
0<p??q?<u??v??n(q? ? p? + 1) + (v? ? u? + 1)
?
?([p, q], [u, v], x)
The first factor is a distribution over all spans that
assigns higher probability to segmentations that
cover more words in the document. It is highest
for segmentations that cover all the words in the
document (this is desirable since there are many
sentence pairs that can be extracted but we want
to find the largest sentence pair in the document).
The function ? takes on values of 0 or 1 depend-
ing on whether certain constraints are violated,
these include: parenthetical constraints that en-
force that spans must not break text within par-
enthetical characters and language constraints that
ensure that we do break a sequence of Mandarin
characters, Arabic words or Latin words.
Language score SL. The language score
SL(l, r | [p, q], [u, v], x) indicates whether the lan-
guage labels l, r are appropriate to the document
contents:
SL(l, r | [p, q], [u, v], x) =?q
i=p L(l, xi) +
?v
i=u L(r, xi)
n
where L(l, x) is a language detection function that
yields 1 if the word xi is in language l, and 0 oth-
erwise. We build the function simply by consid-
ering all words that are composed of Latin char-
acters as English, Arabic characters as Arabic and
Han characters as Mandarin. This approach is not
perfect, but it is simple and works reasonably well
for our purposes.
Translation score ST . The translation score
ST (a | [p, q], l, [u, v], r) indicates whether [p, q]
is a reasonable translation of [u, v] with the align-
ment a. We rely on IBM Model 1 probabilities for
this score:
ST (a | [p, q], l, [u, v], r, x) =
1
(q ? p+ 1)v?u+2
v?
i=u
PM1(xi | xai).
The lexical tables PM1 for the various language
pairs are trained a priori using available parallel
corpora. While IBM Model 1 produces worse
alignments than other models, in our problem, we
need to efficiently consider all possible spans, lan-
guage pairs and word alignments, which makes
the problem intractable. We will show that dy-
namic programing can be used to make this prob-
lem tractable, using Model 1. Furthermore, IBM
Model 1 has shown good performance for sen-
tence alignment systems previously (Xu et al,
2005; Braune and Fraser, 2010).
3.2 Inference
Our goal is to find the spans, language pair and
alignments such that:
argmax
[p,q],l,[u,v],r,a
S([p, q], l, [u, v], r, a | x) (1)
A high score indicates that the predicted bispan is
likely to correspond to a valid parallel span, so we
set a constant threshold ? to determine whether a
document has parallel data, i.e., the value of z:
z? = max
[u,v],r,[p,q],l,a
S([u, v], r, [p, q], l, a | x) > ?
Naively maximizing Eq. 1 would require
O(|x|6) operations, which is too inefficient to be
practical on large datasets. To process millions
of documents, this process would need to be op-
timized.
The main bottleneck of the naive algorithm is
finding new Viterbi Model 1 word alignments ev-
ery time we change the spans. Thus, we propose
178
an iterative approach to compute the Viterbi word
alignments for IBM Model 1 using dynamic pro-
gramming.
Dynamic programming search. The insight we
use to improve the runtime is that the Viterbi
word alignment of a bispan can be reused to cal-
culate the Viterbi word alignments of larger bis-
pans. The algorithm operates on a 4-dimensional
chart of bispans. It starts with the minimal valid
span (i.e., [0, 0], [1, 1]) and progressively builds
larger spans from smaller ones. Let Ap,q,u,v rep-
resent the Viterbi alignment (under ST ) of the bis-
pan [p, q], [u, v]. The algorithm uses the follow-
ing recursions defined in terms of four operations
?{+v,+u,+p,+q} that manipulate a single dimension
of the bispan to construct larger spans:
? Ap,q,u,v+1 = ?+v(Ap,q,u,v) adds one token to
the end of the right span with index v + 1 and
find the viterbi alignment for that token. This
requires iterating over all the tokens in the left
span, [p, q] and possibly updating their align-
ments. See Fig. 1 for an illustration.
? Ap,q,u+1,v = ?+u(Ap,q,u,v) removes the first to-
ken of the right span with index u, so we only
need to remove the alignment from u, which can
be done in time O(1).
? Ap,q+1,u,v = ?+q(Ap,q,u,v) adds one token to
the end of the left span with index q + 1, we
need to check for each word in the right span, if
aligning to the word in index q+1 yields a better
translation probability. This update requires n?
q + 1 operations.
? Ap+1,q,u,v = ?+p(Ap,q,u,v) removes the first
token of the left span with index p. After re-
moving the token, we need to find new align-
ments for all tokens that were aligned to p.
Thus, the number of operations for this update
is K ? (q ? p + 1), where K is the number of
words that were aligned to p. In the best case, no
words are aligned to the token in p, and we can
simply remove it. In the worst case, if all target
words were aligned to p, this update will result
in the recalculation of all Viterbi Alignments.
The algorithm proceeds until all valid cells have
been computed. One important aspect is that the
update functions differ in complexity, so the se-
quence of updates we apply will impact the per-
formance of the system. Most spans are reach-
able using any of the four update functions. For
instance, the span A2,3,4,5 can be reached us-
ing ?+v(A2,3,4,4), ?+u(A2,3,3,5), ?+q(A2,2,4,5) or
?+p(A1,3,4,5). However, we want to use ?+u
a b - A B
a
b
-
A
B
a b - A B
p
qu v
p
qu v?+v
Figure 1: Illustration of the ?+v operator. The
light gray boxes show the parallel span and the
dark boxes show the span?s Viterbi alignment.
In this example, the parallel message contains a
?translation? of a b to A B.
whenever possible, since it only requires one op-
eration, although that is not always possible. For
instance, the state A2,2,2,4 cannot be reached us-
ing ?+u, since the state A2,2,1,4 is not valid, be-
cause the spans overlap. If this happens, incre-
mentally more expensive updates need to be used,
such as ?+v, then ?+q, which are in the same order
of complexity. Finally, we want to minimize the
use of ?+p, which is quadratic in the worst case.
Thus, we use the following recursive formulation
that guarantees the optimal outcome:
Ap,q,u,v =
?
????
????
?+u(Ap,q,u?1,v) if u > q + 1
?+v(Ap,q,u,v?1) else if v > q + 1
?+p(Ap?1,q,u,v) else if q = p+ 1
?+q(Ap,q?1,u,v) otherwise
This transition function applies the cheapest
possible update to reach state Ap,q,u,v.
Complexity analysis. We can see that ?+u
is only needed in the following the cases
[0, 1][2, 2], [1, 2][3, 3], ? ? ? , [n ? 2, n ? 1][n, n].
Since, this update is quadratic in the worst
case, the complexity of this operations is
O(n3). The update ?+q, is applied to the cases
[?, 1][2, 2], [?, 2][3, 3], ? ? ? , [?, n?1], [n, n], where
? denotes any number within the span constraints
but not present in previous updates. Since, the
update is linear and we need to iterate through
all tokens twice, this update takes O(n3) opera-
tions. The update ?+v is applied for the cases
[?, 1][2, ?], [?, 2][3, ?], ? ? ? , [?, n? 1], [n, ?]. Thus,
with three degrees of freedom and a linear update,
it runs in O(n4) time. Finally, update ?+u runs in
constant time, but is run for all remaining cases,
which constitute O(n4) space. By summing the
179
executions of all updates, we observe that the or-
der of magnitude of our exact inference process is
O(n4). Note that for exact inference, it is not pos-
sible to get a lower order of magnitude, since we
need to at least iterate through all possible span
values once, which takes O(n4) time.
4 Parallel Data Extraction
We will now describe our method to extract par-
allel data from Microblogs. The target domains
in this work are Twitter and Sina Weibo, and
the main language pair is Chinese-English. Fur-
thermore, we also run the system for the Arabic-
English language pair using the Twitter data.
For the Twitter domain, we use a previously
crawled dataset from the years 2008 to 2013,
where one million tweets are crawled every day.
In total, we processed 1.6 billion tweets.
Regarding Sina Weibo, we built a crawler that
continuously collects tweets from Weibo. We start
from one seed user and collect his posts, and then
we find the users he follows that we have not con-
sidered, and repeat. Due to the rate limiting es-
tablished by the Weibo API1, we are restricted in
terms of number of requests every hour, which
greatly limits the amount of messages we can col-
lect. Furthermore, each request can only fetch up
to 100 posts from a user, and subsequent pages of
100 posts require additional API calls. Thus, to
optimize the number of parallel posts we can col-
lect per request, we only crawl all messages from
users that have at least 10 parallel tweets in their
first 100 posts. The number of parallel messages
is estimated by running our alignment model, and
checking if ? > ?, where ? was set empirically
initially, and optimized after obtaining annotated
data, which will be detailed in 5.1. Using this
process, we crawled 65 million tweets from Sina
Weibo within 4 months.
In both cases, we first filter the collection of
tweets for messages containing at least one trigram
in each language of the target language pair, deter-
mined by their Unicode ranges. This means that
for the Chinese-English language pair, we only
keep tweets with more than 3 Mandarin charac-
ters and 3 latin words. Furthermore, based on the
work in (Jelh et al, 2012), if a tweet A is iden-
tified as a retweet, meaning that it references an-
other tweetB, we also consider the hypothesis that
these tweets may be mutual translations. Thus, if
A and B contain trigrams in different languages,
1http://open.weibo.com/wiki/API??/en
these are also considered for the extraction of par-
allel data. This is done by concatenating tweets A
and B, and adding the constraint that [p, q] must
be within A and [u, v] must be within B. Finally,
identical duplicate tweets are removed.
After filtering, we obtained 1124k ZH-EN
tweets from Sina Weibo, 868k ZH-EN and 136k
AR-EN tweets from Twitter. These language pairs
are not definite, since we simply check if there is
a trigram in each language.
Finally, we run our alignment model described
in section 3, and obtain the parallel segments and
their scores, which measure how likely those seg-
ments are parallel. In this process, lexical tables
for EN-ZH language pair used by Model 1 were
built using the FBIS dataset (LDC2003E14) for
both directions, a corpus of 300K sentence pairs
from the news domain. Likewise, for the EN-
AR language pair, we use a fraction of the NIST
dataset, by removing the data originated from UN,
which leads to approximately 1M sentence pairs.
5 Experiments
We evaluate our method in two ways. First, intrin-
sically, by observing how well our method identi-
fies tweets containing parallel data, the language
pair and what their spans are. Second, extrinsi-
cally, by looking at how well the data improves
a translation task. This methodology is similar to
that of Smith et al (2010).
5.1 Parallel Data Extraction
Data. Our method needs to determine if a given
tweet contains parallel data, and if so, what is
the language pair of the data, and what segments
are parallel. Thus, we had a native Mandarin
speaker, also fluent in English, to annotate 2000
tweets sampled from crawled Weibo tweets. One
important question of answer is what portion of
the Microblogs contains parallel data. Thus, we
also use the random sample Twitter and annotated
1200 samples, identifying whether each sample
contains parallel data, for the EN-ZH and AR-EN
filtered tweets.
Metrics. To test the accuracy of the score S, we
ordered all 2000 samples by score. Then, we cal-
culate the precision, recall and accuracy at increas-
ing intervals of 10% of the top samples. We count
as a true positive (tp) if we correctly identify a par-
allel tweet, and as a false positive (fp) spuriously
detect a parallel tweet. Finally, a true negative (tn)
occurs when we correctly detect a non-parallel
180
tweet, and a false negative (fn) if we miss a par-
allel tweet. Then, we set the precision as tptp+fp ,
recall as tptp+fn and accuracy as tp+tntp+fp+tn+fn . Forlanguage identification, we calculate the accuracy
based on the number of instances that were iden-
tified with the correct language pair. Finally, to
evaluate the segment alignment, we use the Word
Error Rate (WER) metric, without substitutions,
where we compare the left and right spans of our
system and the respective spans of the reference.
We count an insertion error (I) for each word in
our system?s spans that is not present in the refer-
ence span and a deletion error (D) for each word
in the reference span that is not present in our sys-
tem?s spans. Thus, we set WER = D+IN , where
N is the number of tokens in the tweet. To com-
pute this score for the whole test set, we compute
the average of the WER for each sample.
Results. The precision, recall and accuracy
curves are shown in Figure 2. The quality of the
parallel sentence detection did not vary signifi-
cantly with different setups, so we will only show
the results for the best setup, which is the baseline
model with span constraints.
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
0.7	 ?
0.8	 ?
0.9	 ?
1	 ?
10%	 ? 20%	 ? 30%	 ? 40%	 ? 50%	 ? 60%	 ? 70%	 ? 80%	 ? 90%	 ? 100%	 ?
Precision	 ?
Recall	 ?
Accuracy	 ?
Figure 2: Precision, recall and accuracy curves
for parallel data detection. The y-axis denotes the
scores for each metric, and the x-axis denotes the
percentage of the highest scoring sentence pairs
that are kept.
From the precision and recall curves, we ob-
serve that most of the parallel data can be found
at the top 30% of the filtered tweets, where 5 in 6
tweets are detected correctly as parallel, and only
1 in every 6 parallel sentences is lost. We will de-
note the score threshold at this point as ?, which is
a good threshold to estimate on whether the tweet
is parallel. However, this parameter can be tuned
for precision or recall. We also see that in total,
30% of the filtered tweets are parallel. If we gen-
eralize this ratio for the complete set with 1124k
tweets, we can expect approximately 337k paral-
lel sentences. Finally, since 65 million tweets were
extracted to generate the 337k tweets, we estimate
that approximately 1 parallel tweet can be found
for every 200 tweets we process using our tar-
geted approach. On the other hand, from the 1200
tweets from Twitter, we found that 27 had parallel
data in the ZH-EN pair, if we extrapolate for the
whole 868k filtered tweets, we expect that we can
find 19530. 19530 parallel sentences from 1.6 bil-
lion tweets crawled randomly, represents 0.001%
of the total corpora. For AR-EN, a similar re-
sult was obtained where we expect 12407 tweets
out of the 1.6 billion to be parallel. This shows
that targeted approaches can substantially reduce
the crawling effort required to find parallel tweets.
Still, considering that billions of tweets are posted
daily, this is a substantial source of parallel data.
The remainder of the tests will be performed on
the Weibo dataset, which contains more parallel
data. Tests on the Twitter data will be conducted
as future work, when we process Twitter data on a
larger scale to obtain more parallel sentences.
For the language identification task, we had an
accuracy of 99.9%, since distinguishing English
and Mandarin is trivial. The small percentage of
errors originated from other latin languages (Ex:
French) due to our naive language detector.
As for the segment alignment task. Our base-
line system with no constraints obtains a WER of
12.86%, and this can be improved to 11.66% by
adding constraints to possible spans. This shows
that, on average, approximately 1 in 9 words on
the parallel segments is incorrect. However, trans-
lation models are generally robust to such kinds of
errors and can learn good translations even in the
presence of imperfect sentence pairs.
Among the 578 tweets that are parallel, 496
were extracted within the same tweet and 82 were
extracted from retweets. Thus, we see that the ma-
jority of the parallel data comes from within the
same tweet.
Topic analysis. To give an intuition about the
contents of the parallel data we found, we looked
at the distribution over topics of the parallel
dataset inferred by LDA (Blei et al, 2003). Thus,
we grouped the Weibo filtered tweets by users,
and ran LDA over the predicted English segments,
with 12 topics. The 7 most interpretable topics are
shown in Table 1. We see that the data contains a
181
# Topic Most probable words in topic
1 (Dating) love time girl live mv back word night rt wanna
2 (Entertainment) news video follow pong image text great day today fans
3 (Music) cr day tour cn url amazon music full concert alive
4 (Religion) man god good love life heart would give make lord
5 (Nightlife) cn url beijing shanqi party adj club dj beijiner vt
6 (Chinese News) china chinese year people world beijing years passion country government
7 (Fashion) street fashion fall style photo men model vogue spring magazine
Table 1: Most probable words inferred using LDA in several topics from the parallel data extracted from
Weibo. Topic labels (in parentheses) were assigned manually for illustration purposes.
variety of topics, both formal (Chinese news, reli-
gion) and informal (entertainment, music).
Example sentence pairs. To gain some perspec-
tive on the type of sentence pairs we are extract-
ing, we will illustrate some sentence pairs we
crawled and aligned automatically. Table 2 con-
tains 5 English-Mandarin and 4 English-Arabic
sentence pairs that were extracted automatically.
These were chosen, since they contain some as-
pects that are characteristic of the text present in
Microblogs and Social Media. These are:
? Abbreviations - In most sentence pairs exam-
ples, we can witness the use of abbreviated
forms of English words, such as wanna, TMI,
4 and imma. These can be normalized as want
to, too much information, for and I am going
to, respectively. In sentence 5, we observe that
this phenomena also occurs in Mandarin. We
find that TMD is a popular way to write???
whose Pinyin rendering is ta? ma? de. The mean-
ing of this expression depends on the context it
is used, and can convey a similar connotation
as adding the intensifier the hell to an English
sentence.
? Jargon - Another common phenomena is the
appearance of words that are only used in sub-
communities. For instance, in sentence pair 4,
we the jargon word cday is used, which is a col-
loquial variant for birthday.
? Emoticons - In sentence 8, we observe the pres-
ence of the emoticon :), which is frequently
used in this media. We found that emoticons are
either translated as they are or simply removed,
in most cases.
? Syntax errors - In the domain of microblogs, it
is also common that users do not write strictly
syntactic sentences, for instance, in sentence
pair 7, the sentence onni this gift only 4 u, is
clearly not syntactically correct. Firstly, onni
is a named entity, yet it is not capitalized. Sec-
ondly, a comma should follow onni. Thirdly, the
verb is should be used after gift. Having exam-
ples of these sentences in the training set, with
common mistakes (intentional or not), might
become a key factor in training MT systems that
can be robust to such errors.
? Dialects - We can observe a much broader range
of dialects in our data, since there are no di-
alect standards in microblogs. For instance, in
sentence pair 6, we observe an arabic word (in
bold) used in the spoken Arabic dialect used in
some countries along the shores of the Persian
Gulf, which means means the next. In standard
Arabic, a significantly different form is used.
We can also see in sentence pair 9 that our
aligner does not alway make the correct choice
when determining spans. In this case, the segment
RT @MARYAMALKHAWAJA: was included in the
English segment spuriously, since it does not cor-
respond to anything in the Arabic counterpart.
5.2 Machine Translation Experiments
We report on machine translation experiments us-
ing our harvested data in two domains: edited
news and microblogs.
News translation. For the news test, we cre-
ated a new test set from a crawl of the Chinese-
English documents on the Project Syndicate web-
site2, which contains news commentary articles.
We chose to use this data set, rather than more
standard NIST test sets to ensure that we had re-
cent documents in the test set (the most recent
NIST test sets contain documents published in
2007, well before our microblog data was created).
We extracted 1386 parallel sentences for tuning
and another 1386 sentences for testing, from the
manually aligned segments. For this test set, we
used 8 million sentences from the full NIST par-
allel dataset as the language model training data.
We shall call this test set Syndicate.
2http://www.project-syndicate.org/
182
ENGLISH MANDARIN
1 i wanna live in a wes anderson world ??????Wes Anderson????
2 Chicken soup, corn never truly digests. TMI. ??????????????????.??
3 To DanielVeuleman yea iknw imma work on that ?DanielVeuleman?????????????????
4 msg 4 Warren G his cday is today 1 yr older. ????Warren G????????????????
5 Where the hell have you been all these years? ????TMD????
ENGLISH ARABIC
6 It?s gonna be a warm week! Qk ?


AJ
? @ ? ?J.?B@
7 onni this gift only 4 u ?? ?? 	? ?K
Y?? @ ? 	Y? ?

	
G?

@
8 sunset in aqaba :) (: ?J. ???@ ?

	
? ?? ??@ H. ?Q
	
?
9 RT @MARYAMALKHAWAJA: there is a call @Y 	? ??A 	J? ?Y? ?


	
? H@Q?A 	??? Z @Y 	K ?A 	J?for widespread protests in #bahrain tmrw
Table 2: Examples of English-Mandarin and English-Arabic sentence pairs. The English-Mandarin
sentences were extracted from Sina Weibo and the English-Arabic sentences were extracted from Twitter.
Some messages have been shorted to fit into the table. Some interesting aspects of these sentence pairs
are marked in bold.
Microblog translation. To carry out the mi-
croblog translation experiments, we need a high
quality parallel test set. Since we are not aware
of such a test set, we created one by manually se-
lecting parallel messages from Weibo. Our proce-
dure was as follows. We selected 2000 candidate
Weibo posts from users who have a high num-
ber of parallel tweets according to our automatic
method (at least 2 in every 5 tweets). To these, we
added another 2000 messages from our targeted
Weibo crawl, but these had no requirement on the
proportion of parallel tweets they had produced.
We identified 2374 parallel segments, of which we
used 1187 for development and 1187 for testing.
We refer to this test set as Weibo.3
Obviously, we removed the development and
test sets from our training data. Furthermore, to
ensure that our training data was not too similar to
the test set in the Weibo translation task, we fil-
tered the training data to remove near duplicates
by computing edit distance between each paral-
lel sentence in the heldout set and each training
instance. If either the source or the target sides
of the a training instance had an edit distance of
less than 10%, we removed it.4 As for the lan-
guage models, we collected a further 10M tweets
from Twitter for the English language model and
another 10M tweets from Weibo for the Chinese
language model.
3We acknowledge that self-translated messages are prob-
ably not a typically representative sample of all microblog
messages. However, we do not have the resources to produce
a carefully curated test set with a more broadly representative
distribution. Still, we believe these results are informative as
long as this is kept in mind.
4Approximately 150,000 training instances removed.
Syndicate Weibo
ZH-EN EN-ZH ZH-EN EN-ZH
FBIS 9.4 18.6 10.4 12.3
NIST 11.5 21.2 11.4 13.9
Weibo 8.75 15.9 15.7 17.2
FBIS+Weibo 11.7 19.2 16.5 17.8
NIST+Weibo 13.3 21.5 16.9 17.9
Table 3: BLEU scores for different datasets in dif-
ferent translation directions (left to right), broken
with different training corpora (top to bottom).
Baselines. We report results on these test sets us-
ing different training data. First, we use the FBIS
dataset which contains 300K high quality sentence
pairs, mostly in the broadcast news domain. Sec-
ond, we use the full 2012 NIST Chinese-English
dataset (approximately 8M sentence pairs, includ-
ing FBIS). Finally, we use our crawled data (re-
ferred as Weibo) by itself and also combined with
the two previous training sets.
Setup. We use the Moses phrase-based MT sys-
tem with standard features (Koehn et al, 2003).
For reordering, we use the MSD reordering
model (Axelrod et al, 2005). As the language
model, we use a 5-gram model with Kneser-
Ney smoothing. The weights were tuned using
MERT (Och, 2003). Results are presented with
BLEU-4 (Papineni et al, 2002).
Results. The BLEU scores for the different par-
allel corpora are shown in Table 3 and the top 10
out-of-vocabulary (OOV) words for each dataset
are shown in Table 4. We observe that for the
Syndicate test set, the NIST and FBIS datasets
183
Syndicate (test) Weibo (test)
FBIS NIST Weibo FBIS NIST Weibo
obama (83) barack (59) democracies (15) 2012 (24) showstudio (9) submissions (4)
barack (59) namo (6) imbalances (13) alanis (13) crue (9) ivillage (4)
princeton (40) mitt (6) mahmoud (12) crue (9) overexposed (8) scola (3)
ecb (8) guant (6) millennium (9) showstudio (9) tweetmeian (5) rbst (3)
bernanke (8) fairtrade (6) regimes (8) overexposed (8) tvd (5) curitiba (3)
romney (7) hollande (5) wolfowitz (7) itunes (8) iheartradio (5) zeman (2)
gaddafi (7) wikileaks (4) revolutions (7) havoc (8) xoxo (4) @yaptv (2)
merkel (7) wilders (3) qaddafi (7) sammy (6) snoop (4) witnessing (2)
fats (7) rant (3) geopolitical (7) obama (6) shinoda (4) whoohooo (2)
dialogue (7) esm (3) genome (7) lol (6) scrapbook (4) wbr (2)
Table 4: The most frequent out-of-vocabulary (OOV) words and their counts for the two English-source
test sets with three different training sets.
perform better than our extracted parallel data.
This is to be expected, since our dataset was ex-
tracted from an extremely different domain. How-
ever, by combining the Weibo parallel data with
this standard data, improvements in BLEU are ob-
tained. Error analysis indicates that one major fac-
tor is that names from current events, such as Rom-
ney and Wikileaks do not occur in the older NIST
and FBIS datasets, but they are represented in the
Weibo dataset. Furthermore, we also note that the
system built on the Weibo dataset does not per-
form substantially worse than the one trained on
the FBIS dataset, a further indication that harvest-
ing parallel microblog data yields a diverse collec-
tion of translated material.
For the Weibo test set, a significant improve-
ment over the news datasets can be achieved us-
ing our crawled parallel data. Once again newer
terms, such as iTunes, are one of the reasons older
datasets perform less well. However, in this case,
the top OOV words of the news domain datasets
are not the most accurate representation of cov-
erage problems in this domain. This is because
many frequent words in microblogs, e.g., nonstan-
dard abbreviations, like u and 4 are found in the
news domain as words, albeit with different mean-
ings. Thus, the OOV table gives an incomplete
picture of the translation problems when using
the news domain corpora to translate microblogs.
Also, some structural errors occur when training
with the news domain datasets, one such example
is shown in table 5, where the character ? is in-
correctly translated to said. This occurs because
this type of constructions is infrequent in news
datasets. Furthermore, we can see that compound
expressions, such as the translation from ???
? to party time are also learned.
Finally, we observe that combining the datasets
Source ?sam farrar??????
Reference to sam farrar , party time
FBIS farrar to sam said , in time
NIST to sam farrar said , the moment
WEIBO to sam farrar , party time
Table 5: Translation Examples using different
training sets.
yields another gain over individual datasets, both
in the Syndicate and in the Weibo test sets.
6 Conclusion
We presented a framework to crawl parallel data
from microblogs. We find parallel data from sin-
gle posts, with translations of the same sentence
in two languages. We show that a considerable
amount of parallel sentence pairs can be crawled
from microblogs and these can be used to improve
Machine Translation by updating our translation
tables with translations of newer terms. Further-
more, the in-domain data can substantially im-
prove the translation quality on microblog data.
The resources described in this paper and fur-
ther developments are available to the general pub-
lic at http://www.cs.cmu.edu/?lingwang/utopia.
Acknowledgements
The PhD thesis of Wang Ling is supported by FCT
grant SFRH/BD/51157/2010. The authors wish
to express their gratitude to thank William Cohen,
Noah Smith, Waleed Ammar, and the anonymous
reviewers for their insight and comments. We are
also extremely grateful to Brendan O?Connor for
providing the Twitter data and to Philipp Koehn
and Barry Haddow for providing the Project Syn-
dicate data.
184
References
[Axelrod et al2005] Amittai Axelrod, Ra Birch Mayne,
Chris Callison-burch, Miles Osborne, and David
Talbot. 2005. Edinburgh system description for the
2005 iwslt speech translation evaluation. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT.
[Blei et al2003] David M. Blei, Andrew Y. Ng, and
Michael I. Jordan. 2003. Latent dirichlet alocation.
J. Mach. Learn. Res., 3:993?1022, March.
[Braune and Fraser2010] Fabienne Braune and Alexan-
der Fraser. 2010. Improved unsupervised sentence
alignment for symmetrical and asymmetrical paral-
lel corpora. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 81?89, Stroudsburg, PA, USA.
Association for Computational Linguistics.
[Brown et al1993] Peter F. Brown, Vincent J. Della
Pietra, Stephen A. Della Pietra, and Robert L. Mer-
cer. 1993. The mathematics of statistical machine
translation: parameter estimation. Comput. Lin-
guist., 19:263?311, June.
[Fukushima et al2006] Ken?ichi Fukushima, Kenjiro
Taura, and Takashi Chikayama. 2006. A fast and
accurate method for detecting English-Japanese par-
allel texts. In Proceedings of the Workshop on Mul-
tilingual Language Resources and Interoperability,
pages 60?67, Sydney, Australia, July. Association
for Computational Linguistics.
[Gimpel et al2011] Kevin Gimpel, Nathan Schneider,
Brendan O?Connor, Dipanjan Das, Daniel Mills, Ja-
cob Eisenstein, Michael Heilman, Dani Yogatama,
Jeffrey Flanigan, and Noah A. Smith. 2011. Part-
of-speech tagging for twitter: annotation, features,
and experiments. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers - Volume 2, HLT ?11, pages 42?47, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
[Jelh et al2012] Laura Jelh, Felix Hiebel, and Stefan
Riezler. 2012. Twitter translation using translation-
based cross-lingual retrieval. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 410?421, Montre?al, Canada, June. Asso-
ciation for Computational Linguistics.
[Koehn et al2003] Philipp Koehn, Franz Josef Och,
and Daniel Marcu. 2003. Statistical phrase-based
translation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, NAACL ?03, pages 48?54,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
[Koehn2005] Philipp Koehn. 2005. Europarl: A Par-
allel Corpus for Statistical Machine Translation. In
Proceedings of the tenth Machine Translation Sum-
mit, pages 79?86, Phuket, Thailand. AAMT, AAMT.
[Li and Liu2008] Bo Li and Juan Liu. 2008. Mining
Chinese-English parallel corpora from the web. In
Proceedings of the 3rd International Joint Confer-
ence on Natural Language Processing (IJCNLP).
[Lin et al2008] Dekang Lin, Shaojun Zhao, Benjamin
Van Durme, and Marius Pas?ca. 2008. Mining par-
enthetical translations from the web by word align-
ment. In Proceedings of ACL-08: HLT, pages 994?
1002, Columbus, Ohio, June. Association for Com-
putational Linguistics.
[Och2003] Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL ?03,
pages 160?167, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Papineni et al2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine trans-
lation. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 311?318, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
[Post et al2012] Matt Post, Chris Callison-Burch, and
Miles Osborne. 2012. Constructing parallel cor-
pora for six indian languages via crowdsourcing. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 401?409, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
[Resnik and Smith2003] Philip Resnik and Noah A.
Smith. 2003. The web as a parallel corpus. Compu-
tational Linguistics, 29:349?380.
[Smith et al2010] Jason R. Smith, Chris Quirk, and
Kristina Toutanova. 2010. Extracting parallel sen-
tences from comparable corpora using document
level alignment. In Proceedings of the 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics.
[Ture and Lin2012] Ferhan Ture and Jimmy Lin. 2012.
Why not grab a free lunch? mining large corpora for
parallel sentences to improve translation modeling.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 626?630, Montre?al, Canada, June. Associa-
tion for Computational Linguistics.
[Uszkoreit et al2010] Jakob Uszkoreit, Jay Ponte,
Ashok C. Popat, and Moshe Dubiner. 2010. Large
scale parallel document mining for machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 1101?
1109.
[Vogel et al1996] Stephan Vogel, Hermann Ney, and
Christoph Tillmann. 1996. Hmm-based word align-
ment in statistical translation. In Proceedings of the
16th conference on Computational linguistics - Vol-
ume 2, COLING ?96, pages 836?841, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
[Xu et al2001] Jinxi Xu, Ralph Weischedel, and Chanh
Nguyen. 2001. Evaluating a probabilistic model
185
for cross-lingual information retrieval. In Proceed-
ings of the 24th annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ?01, pages 105?110, New
York, NY, USA. ACM.
[Xu et al2005] Jia Xu, Richard Zens, and Hermann
Ney. 2005. Sentence segmentation using ibm word
alignment model 1. In Proceedings of EAMT 2005
(10th Annual Conference of the European Associa-
tion for Machine Translation, pages 280?287.
[Zbib et al2012] Rabih Zbib, Erika Malchiodi, Jacob
Devlin, David Stallard, Spyros Matsoukas, Richard
Schwarz, John Makhoul, Omar F. Zaidan, and Chris
Callison-Burch. 2012. Machine translation of Ara-
bic dialects. In Proceedings of the 2012 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies.
186
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 770?779,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Graph-based Semi-Supervised Model for Joint Chinese Word
Segmentation and Part-of-Speech Tagging
Xiaodong Zeng? Derek F. Wong? Lidia S. Chao? Isabel Trancoso?
?Department of Computer and Information Science, University of Macau
?INESC-ID / Instituto Superior Te?cnico, Lisboa, Portugal
nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo,
isabel.trancoso@inesc-id.pt
Abstract
This paper introduces a graph-based semi-
supervised joint model of Chinese word
segmentation and part-of-speech tagging.
The proposed approach is based on a
graph-based label propagation technique.
One constructs a nearest-neighbor simi-
larity graph over all trigrams of labeled
and unlabeled data for propagating syn-
tactic information, i.e., label distribution-
s. The derived label distributions are re-
garded as virtual evidences to regular-
ize the learning of linear conditional ran-
dom fields (CRFs) on unlabeled data. An
inductive character-based joint model is
obtained eventually. Empirical results on
Chinese tree bank (CTB-7) and Microsoft
Research corpora (MSR) reveal that the
proposed model can yield better result-
s than the supervised baselines and other
competitive semi-supervised CRFs in this
task.
1 Introduction
Word segmentation and part-of-speech (POS) tag-
ging are two critical and necessary initial proce-
dures with respect to the majority of high-level
Chinese language processing tasks such as syn-
tax parsing, information extraction and machine
translation. The traditional way of segmentation
and tagging is performed in a pipeline approach,
first segmenting a sentence into words, and then
assigning each word a POS tag. The pipeline ap-
proach is very simple to implement, but frequently
causes error propagation, given that wrong seg-
mentations in the earlier stage harm the subse-
quent POS tagging (Ng and Low, 2004). The join-
t approaches of word segmentation and POS tag-
ging (joint S&T) are proposed to resolve these t-
wo tasks simultaneously. They effectively allevi-
ate the error propagation, because segmentation
and tagging have strong interaction, given that
most segmentation ambiguities cannot be resolved
without considering the surrounding grammatical
constructions encoded in a POS sequence (Qian
and Liu, 2012).
In the past years, several proposed supervised
joint models (Ng and Low, 2004; Zhang and
Clark, 2008; Jiang et al, 2009; Zhang and Clark,
2010) achieved reasonably accurate results, but the
outstanding problem among these models is that
they rely heavily on a large amount of labeled data,
i.e., segmented texts with POS tags. However, the
production of such labeled data is extremely time-
consuming and expensive (Jiao et al, 2006; Jiang
et al, 2009). Therefore, semi-supervised join-
t S&T appears to be a natural solution for easily in-
corporating accessible unlabeled data to improve
the joint S&T model. This study focuses on using
a graph-based label propagation method to build
a semi-supervised joint S&T model. Graph-based
label propagation methods have recently shown
they can outperform the state-of-the-art in sever-
al natural language processing (NLP) tasks, e.g.,
POS tagging (Subramanya et al, 2010), knowl-
edge acquisition (Talukdar et al, 2008), shallow
semantic parsing for unknown predicate (Das and
Smith, 2011). As far as we know, however, these
methods have not yet been applied to resolve
the problem of joint Chinese word segmentation
(CWS) and POS tagging.
Motivated by the works in (Subramanya et al,
2010; Das and Smith, 2011), for structured prob-
lems, graph-based label propagation can be em-
ployed to infer valuable syntactic information (n-
gram-level label distributions) from labeled data
to unlabeled data. This study extends this intui-
tion to construct a similarity graph for propagating
trigram-level label distributions. The derived label
distributions are regarded as prior knowledge to
regularize the learning of a sequential model, con-
ditional random fields (CRFs) in this case, on both
770
labeled and unlabeled data to achieve the semi-
supervised learning. The approach performs the
incorporation of the derived labeled distributions
by manipulating a ?virtual evidence? function as
described in (Li, 2009). Experiments on the da-
ta from the Chinese tree bank (CTB-7) and Mi-
crosoft Research (MSR) show that the proposed
model results in significant improvement over oth-
er comparative candidates in terms of F-score and
out-of-vocabulary (OOV) recall.
This paper is structured as follows: Section
2 points out the main differences with the re-
lated work of this study. Section 3 reviews the
background, including supervised character-based
joint S&T model based on CRFs and graph-based
label propagation. Section 4 presents the details of
the proposed approach. Section 5 reports the ex-
periment results. The conclusion is drawn in Sec-
tion 6.
2 Related Work
Prior supervised joint S&T models present ap-
proximate 0.2% - 1.3% improvement in F-score
over supervised pipeline ones. The state-of-the-
art joint models include reranking approaches (Shi
and Wang, 2007), hybrid approaches (Nakagawa
and Uchimoto, 2007; Jiang et al, 2008; Sun,
2011), and single-model approaches (Ng and Low,
2004; Zhang and Clark, 2008; Kruengkrai et al,
2009; Zhang and Clark, 2010). The proposed ap-
proach in this paper belongs to the single-model
type.
There are few explorations of semi-supervised
approaches for CWS or POS tagging in previ-
ous works. Xu et al (2008) described a Bayesian
semi-supervised CWS model by considering the
segmentation as the hidden variable in machine
translation. Unlike this model, the proposed ap-
proach is targeted at a general model, instead of
one oriented to machine translation task. Sun and
Xu (2011) enhanced a CWS model by interpolat-
ing statistical features of unlabeled data into the
CRFs model. Wang et al (2011) proposed a semi-
supervised pipeline S&T model by incorporating
n-gram and lexicon features derived from unla-
beled data. Different from their concern, our em-
phasis is to learn the semi-supervised model by
injecting the label information from a similarity
graph constructed from labeled and unlabeled da-
ta.
The induction method of the proposed approach
also differs from other semi-supervised CRFs al-
gorithms. Jiao et al (2006), extended by Mann
and McCallum (2007), reported a semi-supervised
CRFs model which aims to guide the learning
by minimizing the conditional entropy of unla-
beled data. The proposed approach regularizes the
CRFs by the graph information. Subramanya et
al. (2010) proposed a graph-based self-train style
semi-supervised CRFs algorithm. In the proposed
approach, an analogous way of graph construction
intuition is applied. But overall, our approach dif-
fers in three important aspects: first, novel feature
templates are defined for measuring the similari-
ty between vertices. Second, the critical property,
i.e., sparsity, is considered among label propaga-
tion. And third, the derived label information from
the graph is smoothed into the model by optimiz-
ing a modified objective function.
3 Background
3.1 Supervised Character-based Model
The character-based joint S&T approach is oper-
ated as a sequence labeling fashion that each Chi-
nese character, i.e., hanzi, in the sequence is as-
signed with a tag. To perform segmentation and
tagging simultaneously in a uniform framework,
according to Ng and Low (2004), the tag is com-
posed of a word boundary part, and a POS part,
e.g., ?B NN? refers to the first character in a word
with POS tag ?NN?. In this paper, 4 word bound-
ary tags are employed: B (beginning of a word),
M (middle part of a word), E (end of a word) and
S (single character). As for the POS tag, we shal-
l use the 33 tags in the Chinese tree bank. Thus,
the potential composite tags of joint S&T consist
of 132 (4?33) classes.
The first-order CRFs model (Lafferty et al,
2001) has been the most common one in this
task. Given a set of labeled examples Dl =
{(xi, yi)}li=1, where xi = x1ix2i ...xNi is the se-
quence of characters in the ith sentence, and yi =
y1i y2i ...yNi is the corresponding label sequence.
The goal is to learn a CRFs model in the form,
p(yi|xi; ?) =
1
Z(xi; ?)
exp{
N?
j=1
K?
k=1
?kfk(yj?1i , y
j
i , xi, j)}
(1)
where Z(xi; ?) is the partition function that nor-
malizes the exponential form to be a probability
distribution, and fk(yj?1i , yji , xi, j). In this study,
771
the baseline feature templates of joint S&T are
the ones used in (Ng and Low, 2004; Jiang et al,
2008), as shown in Table 1. ? = {?1?2...?K} ?
RK are the weight parameters to be learned. In su-
pervised training, the aim is to estimate the ? that
maximizes the conditional likelihood of the train-
ing data while regularizing model parameters:
L(?) =
l?
i=1
log p(yi|xi; ?)?R(?) (2)
R(?) can be any standard regularizer on parame-
ters, e.g., R(?) =? ? ? /2?2, to limit overfitting
on rare features and avoid degeneracy in the case
of correlated features. This objective function can
be optimized by the stochastic gradient method or
other numerical optimization methods.
Type Font Size
Unigram Cn(n = ?2,?1, 0, 1, 2)
Bigram CnCn+1(n = ?2,?1, 0, 1)
Date, Digit and
Alphabetic Letter
T (C?2)T (C?1)T (C0)
T (C1)T (C2)
Table 1: The feature templates of joint S&T.
3.2 Graph-based Label Propagation
Graph-based label propagation, a critical subclass
of semi-supervised learning (SSL), has been wide-
ly used and shown to outperform other SSL meth-
ods (Chapelle et al, 2006). Most of these algo-
rithms are transductive in nature, so they cannot
be used to predict an unseen test example in the fu-
ture (Belkin et al, 2006). Typically, graph-based
label propagation algorithms are run in two main
steps: graph construction and label propagation.
The graph construction provides a natural way to
represent data in a variety of target domains. One
constructs a graph whose vertices consist of la-
beled and unlabeled examples. Pairs of vertices
are connected by weighted edges which encode
the degree to which they are expected to have the
same label (Zhu et al, 2003). Popular graph con-
struction methods include k-nearest neighbors (k-
NN) (Bentley, 1980; Beygelzimer et al, 2006),
b-matching (Jebara et al, 2009) and local recon-
struction (Daitch et al, 2009). Label propaga-
tion operates on the constructed graph. The pri-
mary objective is to propagate labels from a few
labeled vertices to the entire graph by optimiz-
ing a loss function based on the constraints or
properties derived from the graph, e.g., smooth-
ness (Zhu et al, 2003; Subramanya et al, 2010;
Talukdar et al, 2008), or sparsity (Das and Smith,
2012). State-of-the-art label propagation algo-
rithms include LP-ZGL (Zhu et al, 2003), Ad-
sorption (Baluja et al, 2008), MAD (Talukdar
and Crammer, 2009) and Sparse Inducing Penal-
ties (Das and Smith, 2012).
4 Method
The emphasis of this work is on building a joint
S&T model based on two different kinds of data
sources, labeled and unlabeled data. In essence,
this learning problem can be treated as incorporat-
ing certain gainful information, e.g., prior knowl-
edge or label constraints, of unlabeled data into
the supervised model. The proposed approach em-
ploys a transductive graph-based label propagation
method to acquire such gainful information, i.e.,
label distributions from a similarity graph con-
structed over labeled and unlabeled data. Then,
the derived label distributions are injected as vir-
tual evidences for guiding the learning of CRFs.
Algorithm 1 semi-supervised joint S&T induction
Input:
Dl = {(xi, yi)}li=1 labeled sentences
Du = {(xi)}l+ui=l+1 unlabeled sentencesOutput:
?: a set of feature weights
1: Begin
2: {G} = construct graph (Dl,Du)
3: {q0} = init labelDist ({G})
4: {q} = propagate label ({G}, {q0})
5: {?} = train crf (Dl ? Du, {q})
6: End
The model induction includes the following
steps (see Algorithm 1): firstly, given labeled
and unlabeled data, i.e., Dl = {(xi, yi)}li=1
with l labeled sentences and Du = {(xi)}l+ui=l+1with u unlabeled sentences, a specific similarity
graph G representing Dl and Du is constructed
(construct graph). The vertices (Section 4.1) in
the constructed graph consist of all trigrams that
occur in labeled and unlabeled sentences, and edge
weights between vertices are computed using the
cosine distance between pointwise mutual infor-
mation (PMI) statistics. Afterwards, the estimated
label distributions q0 of vertices in the graph G are
randomly initialized (init labelDist). Subsequently,
772
the label propagation procedure (propagate label)
is conducted for projecting label distributions q
from labeled vertices to the entire graph, using
the algorithm of Sparse-Inducing Penalties (Das
and Smith, 2012) (Section 4.2). The final step
(train crf) of the induction is incorporating the in-
ferred trigram-level label distributions q into CRFs
model (Section 4.3).
4.1 Graph Construction
In most graph-based label propagation tasks, the
final effect depends heavily on the quality of
the graph. Graph construction thus plays a cen-
tral role in graph-based label propagation (Zhu et
al., 2003). For character-based joint S&T, unlike
the unstructured learning problem whose vertices
are formed directly by labeled and unlabeled in-
stances, the graph construction is non-trivial. Das
and Petrov (2011) mentioned that taking individu-
al characters as the vertices would result in various
ambiguities, whereas the similarity measurement
is still challenging if vertices corresponding to en-
tire sentences.
This study follows the intuitions of graph con-
struction from Subramanya et al (2010) in which
vertices are represented by character trigrams oc-
curring in labeled and unlabeled sentences. For-
mally, given a set of labeled sentences Dl, and un-
labeled onesDu, whereD , {Dl,Du}, the goal is
to form an undirected weighted graph G = (V,E),
where V is defined as the set of vertices which
covers all trigrams extracted from Dl and Du.
Here, V = Vl ? Vu, where Vl refers to trigrams
that occurs at least once in labeled sentences and
Vu refers to trigrams that occur only in unlabeled
sentences. The edges E ? Vl ? Vu, connect all
the vertices. This study makes use of a symmet-
ric k-NN graph (k = 5) and the edge weights are
measured by a symmetric similarity function (E-
quation (3)):
wi,j =
{
sim(xi, xj) if j ? K(i) or i ? K(j)
0 otherwise
(3)
where K(i) is the set of the k nearest neighbors of
xi(|K(i) = k, ?i|) and sim(xi, xj) is a similari-
ty measure between two vertices. The similarity
is computed based on the co-occurrence statistic-
s over the features in Table 2. Most features we
adopted are selected from those of (Subramanya
et al, 2010). Note that a novel feature in the last
row encodes the classes of surrounding character-
s, where four types are defined: number, punctu-
ation, alphabetic letter and other. It is especially
helpful for the graph to make connections with tri-
grams that may not have been seen in labeled data
but have similar label information. The pointwise
mutual information values between the trigram-
s and each feature instantiation that they have in
common are summed to sparse vectors, and their
cosine distances are computed as the similarities.
Description Feature
Trigram + Context x1x2x3x4x5
Trigram x2x3x4
Left Context x1x2
Right Context x4x5
Center Word x3
Trigram - Center Word x2x4
Left Word + Right Context x2x4x5
Right Word + Left Context x1x2x3
Type of Trigram: number,
punctuation, alphabetic letter
and other
t(x2)t(x3)t(x4)
Table 2: Features employed to measure the sim-
ilarity between two vertices, in a given tex-
t ?x1x2x3x4x5?, where the trigram is ?x2x3x4?.
The nature of the similarity graph enforces that
the connected trigrams with high weight appearing
in different texts should have similar syntax con-
figurations. Thus, the constructed graph is expect-
ed to provide additional information that cannot
be expressed directly in a sequence model (Subra-
manya et al, 2010). One primary benefit of this
property is on enriching vocabulary coverage. In
other words, the new features of various trigram-
s only occurring in unlabeled data can be discov-
ered. As the excerpt in Figure 1 shows, the trigram
????? (Tianjin port) has no any label informa-
tion, as it only occurs in unlabeled data, but for-
tunately its neighborhoods with similar syntax in-
formation, e.g., ????? (Shanghai port), ???
?? (Guangzhou port), can assist to infer the cor-
rect tag ?M NN?.
4.2 Label Propagation
In order to induce trigram-level label distributions
from the graph constructed by the previous step,
a label propagation algorithm, Sparsity-Inducing
Penalties, proposed by Das and Smith (2012), is
employed. This algorithm is used because it cap-
tures the property of sparsity that only a few labels
773
Figure 1: An excerpt from the similarity graph
over trigrams on labeled and unlabeled data.
are typically associated with a given instance. In
fact, the sparsity is also a common phenomenon
among character-based CWS and POS tagging.
The following convex objective is optimized on
the similarity graph in this case:
argmin
q
l?
j=1
? qj ? rj ?2
+?
l+u?
i=1,k?N (i)
wik ? qi ? qk ?2 +?
l+u?
i=1
? qi ?2
s.t. qi ? 0, ?i ? V
(4)
where rj denotes empirical label distributions of
labeled vertices, and qi denotes unnormalized es-
timate measures in every vertex. The wik refers to
the similarity between the ith trigram and the kth
trigram, and N (i) is a set of neighbors of the ith
trigram. ? and ? are two hyperparameters whose
values are discussed in Section 5. The squared-
loss criterion1 is used to formulate the objective
function. The first term in Equation (4) is the seed
match loss which penalizes the estimated label dis-
tributions qj , if they go too far away from the em-
pirical labeled distributions rj . The second term
is the edge smoothness loss that requires qi should
be smooth with respect to the graph, such that two
vertices connected by an edge with high weight
should be assigned similar labels. The final term
is a regularizer to incorporate the prior knowledge,
e.g., uniform distributions used in (Talukdar et al,
2008; Das and Smith, 2011). This study applies
the squared norm of q to encourage sparsity per
vertex. Note that the estimated label distribution
1It can be seen as a multi-class extension of quadratic cost
criterion (Bengio et al, 2006) or as a variant of the objective
in (Zhu et al, 2003). An entropic distance measure could also
be used, e.g., KL-divergence (Subramanya et al, 2010; Das
and Smith, 2012).
qi in Equation (4) is relaxed to be unnormalized,
which simplifies the optimization. Thus, the objec-
tive function can be optimized by L-BFGS-B (Zhu
et al, 1997), a generic quasi-Newton gradient-
based optimizer. The partial derivatives of Equa-
tion (4) are computed for each parameter of q and
then passed on to the optimizer that updates them
such that Equation (4) is maximized.
4.3 Semi-Supervised CRFs Training
The trigram-level label distributions inferred in the
propagation step can be viewed as a kind of valu-
able ?prior knowledge? to regularize the learning
on unlabeled data. The final step of the induc-
tion is thus to incorporate such prior knowledge
into CRFs. Li (2009) generalizes the use of vir-
tual evidence to undirected graphical models and,
in particular, to CRFs for incorporating external
knowledge. By extending the similar intuition, as
illustrated in Figure 2, we modify the structure of
a regular linear-chain CRFs on unlabeled data for
smoothing the derived label distributions, where
virtual evidences, i.e., q in our case, are donated
by {v1, v2, . . . , vT }, in parallel with the state vari-
ables {y1, y2, . . . , yT }. The modified CRFs model
allows us to flexibly define the interaction between
estimated state values and virtual evidences by po-
tential functions. Therefore, given labeled and un-
labeled data, the learning objective is defined as
follows:
L(?) +
l+u?
i=l+1
Ep(yi|xi,vi;?g)[log p(yi, vi|xi; ?)]
(5)
where the conditional probability in the second
term is denoted as
p(yi, vi|xi; ?) =
1
Z ?(xi; ?)
exp{
N?
j=1
K?
k=1
?kfk(yj?1i , y
j
i , xi, j)
+?
N?
t=1
s(yti , vti)}
(6)
The first term in Equation (5) is the same as E-
quation (2), which is the traditional CRFs learn-
ing objective function on the labeled data. The
second term is the expected conditional likelihood
of unlabeled data. It is directed to maximize the
conditional likelihood of hidden states with the
derived label distributions on unlabeled data, i.e.,
p(y, v|x), where y and v are jointly modeled but
774
the probability is still conditional on x. Here,
Z ?(x; ?) is the partition function of normalization
that is achieved by summing the numerator over
both y and v. A virtual evidence feature function
of s(yti , vti) with pre-defined weight ? is defined
to regularize the conditional distributions of states
over the derived label distributions. The learning
is impacted by the derived label distributions as E-
quation (7): firstly, if the trigram xt?1i xtixt+1i at
current position does have no corresponding de-
rived label distributions (vti = null), the value of
zero is assigned to all state hypotheses so that the
posteriors would not affected by the derived infor-
mation. Secondly, if it does have a derived label
distribution, since the virtual evidence in this case
is a distribution instead of a specific label, the la-
bel probability in the distribution under the current
state hypothesis is assigned. This means that the
values of state variables are constrained to agree
with the derived distributions.
s(yti , vti) =
{
qxt?1i xtixt+1i (y
t
i) if vti 6= null
0 else
(7)
The second term in Equation (5) can be op-
timized by using the expectation maximization
(EM) algorithm in the same fashion as in the
generative approach, following (Li, 2009). One
can iteratively optimize the Q function Q(?) =?
y p(yi|xi; ?g) log p(yi, vi|xi; ?), in which ?g is
the model estimated from the previous iteration.
Here the gradient of the Q function can be mea-
sured by:
?Q(?)
??k
=
?
t
?
yt?1i ,yti
fk(yt?1i , yti , xi, t).
(p(yt?1i , yti |xi, vi; ?)? p(yt?1i , yti |xi; ?))
(8)
The forward-backward algorithm is used to mea-
sure p(yt?1i , yti |xi, vi; ?) and p(yt?1i , yti |xi; ?).
Thus, the objective function Equation (5) is op-
timized as follows: for the instances i = 1, 2, ..., l,
the parameters ? are learned as the supervised
manner; for the instances i = l+1, l+2, ..., u+ l,
in the E-step, the expected value of Q function is
computed, based on the current model ?g. In the
M-step, the posteriors are fixed and updated ? that
maximizes Equation (5).
Figure 2: Modified linear-chain CRFs integrating
virtual evidences on unlabeled data.
5 Experiment
5.1 Setting
The experimental data are mainly taken from the
Chinese tree bank (CTB-7) and Microsoft Re-
search (MSR)2. CTB-7 consists of over one mil-
lion words of annotated and parsed text from Chi-
nese newswire, magazine news, various broadcast
news and broadcast conversation programs, web
newsgroups and weblogs. It is a segmented, POS
tagged3 and fully bracketed corpus. The train, de-
velopment and test sets4 from CTB-7 and their
corresponding statistics are reported in Table 3.
To satisfy the characteristic of the semi-supervised
learning problem, the train set, i.e., the labeled da-
ta, is formed by a relatively small amount of an-
notated texts sampled from CTB-7. For the un-
labeled data in this experiment, a greater amount
of texts is extracted from CTB-7 and MSR, which
contains 53,108 sentences with 2,418,690 charac-
ters.
The performance measurement indicators for
word segmentation and POS tagging (joint S&T)
are balance F-score, F = 2PR/(P+R), the harmon-
ic mean of precision (P) and recall (R), and out-
of-vocabulary recall (OOV-R). For segmentation,
a token is regarded to be correct if its boundaries
match the ones of a word in the gold standard.
For the POS tagging, it is correct only if both the
boundaries and the POS tags are perfect matches.
The experimental platform is implemented
based on two toolkits: Mallet (McCallum and
Kachites, 2002) and Junto (Talukdar and Pereira,
2010). Mallet is a java-based package for s-
tatistical natural language processing, which in-
cludes the CRFs implementation. Junto is a graph-
2It can be download at: www.sighan.org/bakeoff2005.
3There is a total of 33 POS tags in CTB-7.
4The extracted sentences in train, development and test set
were assigned with the composite tags as described in Section
3.1.
775
based label propagation toolkit that provides sev-
eral state-of-the-art algorithms.
Data #Sent #Word #Char #OOV
Train 17,968 374,697 596,360
Develop 1,659 46,637 79,283 0.074
Test 2,037 65,219 104,502 0.089
Table 3: Training, development and testing data.
5.2 Baseline and Proposed Models
In the experiment, the baseline supervised pipeline
and joint S&T models are built only on the train
data. The proposed model will also be compared
with the semi-supervised pipeline S&T model de-
scribed in (Wang et al, 2011). In addition, two
state-of-the-art semi-supervised CRFs algorithms,
Jiao?s CRFs (Jiao et al, 2006) and Subramanya?s
CRFs (Subramanya et al, 2010), are also used to
build joint S&T models. The corresponding set-
tings of the above candidates are listed below:
? Baseline I: a supervised CRFs pipeline S&T
model. The feature templates are from Zhao
et al (2006) and Wu et al (2008).
? Wang?s model: a semi-supervised CRFs
pipeline S&T model. The same feature tem-
plates in (Wang et al, 2011) are used, i.e.,
?+n-gram+cluster+lexicon?.
? Baseline II: a supervised CRFs joint S&T
model. The feature templates introduced in
Section 3.1 are used.
? Jiao?s model: a semi-supervised CRFs joint
S&T model trained using the entropy regular-
ization (ER) criteria (Jiao et al, 2006). The
optimization method proposed by Mann and
McCallum (2007) is applied.
? Subramanya?s model: a self-train style
semi-supervised CRFs joint S&T model
based on the same parameters used in (Sub-
ramanya et al, 2010).
? Our model: several parameters in our model
are needed to tune based on the development
set, e.g., ?, ? and ?.
In all the CRFs models above, the Gaussian reg-
ularizer and stochastic gradient descent method
are employed.
5.3 Main Results
This experiment yielded a similarity graph that
consists of 462,962 trigrams from labeled and un-
labeled data. The majority (317,677 trigrams) oc-
curred only in unlabeled data. Based on the de-
velopment data, the hyperparameters of our mod-
el were tuned among the following settings: for
the graph propagation, ? ? {0.2, 0.5, 0.8} and
? ? {0.1, 0.3, 0.5, 0.8}; for the CRFs training,
? ? {0.1, 0.3, 0.5, 0.7, 0.9}. The best performed
joint settings are ? = 0.5, ? = 0.3 and ? = 0.7.
With the chosen set of hyperparameters, the test
data was used to measure the final performance.
Model Segmentation POS TaggingF1 OOV-R F1 OOV-R
Baseline I 94.27 60.12 91.08 51.72
Wang?s 95.17 63.10 91.64 53.29
Baseline II 95.14 61.52 91.61 52.29
Jiao?s 95.58 63.05 92.11 53.27
Subramanya?s 96.30 67.12 92.46 57.15
Our model 96.85 68.09 92.89 58.36
Table 4: The performance of segmentation and
POS tagging on testing data.
Table 4 summarizes the performance of seg-
mentation and POS tagging on the test data, in
comparison with the other five models. First-
ly, as expected, for the two supervised baselines,
the joint model outperforms the pipeline one, e-
specially on segmentation. It obtains 0.92% and
2.32% increase in terms of F-score and OOV-R
respectively. This outcome verifies the commonly
accepted fact that the joint model can substantially
improve the pipeline one, since POS tags provide
additional information to word segmentation (Ng
and Low, 2004). Secondly, it is also noticed that
all four semi-supervised models are able to benefit
from unlabeled data and greatly improve the re-
sults with respect to the baselines. On the whole,
for segmentation, they achieve average improve-
ments of 1.02% and 6.8% in F-score and OOV-R;
whereas for POS tagging, the average increments
of F-sore and OOV-R are 0.87% and 6.45%. An
interesting phenomenon is found among the com-
parisons with baselines that the supervised joint
model (Baseline II) is even competitive with semi-
supervised pipeline one (Wang et al, 2011). This
illustrates the effects of error propagation in the
pipeline approach. Thirdly, in what concerns the
semi-supervised approaches, the three joint S&T
models, i.e., Jiao?s, Subramanya?s and our mod-
el, are superior to the pipeline model, i.e., Wang?s
776
model. Moreover, the two graph-based approach-
es, i.e., Subramanya?s and our model, outperform
the others. Most importantly, the boldface num-
bers in the last row illustrate that our model does
achieve the best performance. Overall, for word
segmentation, it obtains average improvements of
1.43% and 8.09% in F-score and OOV-R over oth-
ers; for POS tagging, it achieves average improve-
ments of 1.09% and 7.73%.
0 10,000 20,000 30,000 40,000 50,000
94.0
94.5
95.0
95.5
96.0
96.5
97.0
97.5
 Wang's
 Jiao's
 Subramanya's
 Our
F
-
s
c
o
r
e
Number of unlabeled sentences
0 10,000 20,000 30,000 40,000 50,000
91.0
91.5
92.0
92.5
93.0
93.5
 Wang's
 Jiao's
 Subramanya's
 Our
F
-
s
c
o
r
e
Number of unlabeled sentences
0 10,000 20,000 30,000 40,000 50,000
60.0
62.5
65.0
67.5
70.0
 Wang's
 Jiao's
 Subramanya's
 Our
O
O
V
-
R
Number of unlabeled sentences
0 10,000 20,000 30,000 40,000 50,000
51.0
52.5
54.0
55.5
57.0
58.5
 Wang's
 Jiao's
 Subramanya's
 Our
O
O
V
-
R
Number of unlabeled sentences
Figure 3: The learning curves of semi-supervised
models on unlabeled data, where left graphs are
segmentation and the right ones are tagging.
5.4 Learning Curve
An additional experiment was conducted to inves-
tigate the impact of unlabeled data for the four
semi-supervised models. Figure 3 illustrates the
curves of F-score and OOV-R for segmentation
and tagging respectively, as the unlabeled data
size is progressively increased in steps of 6,000
sentences. It can be clearly observed that al-
l curves of our model are able to mount up steadi-
ly and achieve better gains over others consistent-
ly. The most competitive performance of the oth-
er three candidates is achieved by Subramanya?s
model. This strongly reveals that the knowledge
derived from the similarity graph does effectively
strengthen the model. But in Subramanya?s mod-
el, when the unlabeled size ascends to approxi-
mately 30,000 sentences the curves become nearly
asymptotic. The semi-supervised pipeline model,
Wang?s model, presents a much slower growth on
all curves over the others and also begins to over-
fit with large unlabeled data sizes (>25,000 sen-
tences). The figure also shows an erratic fluctu-
ation of Jiao?s model. Since this approach aims
at minimizing conditional entropy over unlabeled
data and encourages finding putative labelings for
unlabeled data, it results in a data-sensitive mod-
el (Li et al, 2009).
5.5 Analysis & Discussion
A statistical analysis of the segmentation and tag-
ging results of the supervised joint model (Base-
line II) and our model is carried out to comprehend
the influence of the graph-based semi-supervised
behavior. For word segmentation, the most signif-
icant improvement of our model is mainly concen-
trated on two kinds of words which are known for
their difficulties in terms of CWS: a) named enti-
ties (NE), e.g., ????? (Tianjin port) and ???
?? (free tax zone); and b) Chinese numbers (CN),
e.g., ?????? (eight hundred and fifty million)
and ???????? (seventy two percent). Very
often, these words do not exist in the labeled data,
so the supervised model is hard to learn their fea-
tures. Part of these words, however, may occur in
the unlabeled data. The proposed semi-supervised
approach is able to discover their label information
with the help of a similarity graph. Specifically, it
learns the label distributions from similar words
(neighborhoods), e.g., ????? (Shanghai port),
????? (protection zone), ?????? (nine
hundred and seventy million). The statistics in Ta-
ble 5 demonstrate significant error reductions of
50.44% and 48.74% on test data, corresponding to
NE and CN respectively.
Type #word #baErr #gbErr ErrDec%
NE 471 226 112 50.44
CN 181 119 61 48.74
Table 5: The statistics of segmentation error for
named entities (NE) and Chinese numbers (CN)
in test data. #baErr and #gbErr denote the count
of segmentations by Baseline II and our model;
ErrDec% denotes the error reduction.
On the other hand, to better understand the tag-
ging results, we summarize the increase and de-
crease of the top five common tagging error pat-
terns of our model over Baseline II for the cor-
rectly segmented words, as shown in Table 6. The
error pattern is defined by ?A?B? that refers the
true tag of ?A? is annotated by a tag of ?B?. The
obvious improvement brought by our model oc-
curs with the tags ?NN?, ?CD?, ?NR?, ?JJ? and
?NR?, where errors are reduced 60.74% on aver-
777
Pattern #baErr ? Pattern #baErr ?
NN?VV 58 38 NN?NR 13 6
CD?NN 41 27 IJ?ON 9 5
NR?VV 29 17 VV?NN 4 3
JJ?NN 18 11 NR?NN 1 3
NR?VA 19 10 JJ?AD 1 2
Table 6: The statistics of POS tagging error pat-
terns in test data. #baErr denote the count of tag-
ging error by Baseline II, while ? and ? denotes
the number of error reduced or increased by our
model.
age. More impressively, there is a large portion of
fixed error pattern instances stemming from OOV
words. Meanwhile, it is also observed that the dis-
ambiguation of error patterns in the right portion
of the table slightly suffers from our approach. In
reality, it is impossible and unrealistic to request
a model to be ?no harms but only benefits? under
whatever circumstances.
6 Conclusion
This study introduces a novel semi-supervised ap-
proach for joint Chinese word segmentation and
POS tagging. The approach performs the semi-
supervised learning in the way that the trigram-
level distributions inferred from a similarity graph
are used to regularize the learning of CRFs model
on labeled and unlabeled data. The empirical re-
sults indicate that the similarity graph information
and the incorporation manner of virtual evidences
present a positive effect to the model induction.
Acknowledgments
The authors are grateful to the Science and Tech-
nology Development Fund of Macau and the Re-
search Committee of the University of Macau
for the funding support for our research, un-
der the reference No. 017/2009/A and RG060/09-
10S/CS/FST. The authors also wish to thank the
anonymous reviewers for many helpful comments.
References
Shumeet Baluja, Rohan Seth, D. Sivakumar, Yushi
Jing, Jay Yagnik, Shankar Kumar, Deepak Ravich,
and Mohamed Aly. 2008. Video suggestion and
discovery for youtube: taking random walks through
the view graph. In Proceedings of WWW, pages 895-
904, Beijing, China.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization. Journal of machine
learning research, 7:2399?2434.
Yoshua Bengio, Olivier Delalleau, and Nicolas Le
Roux. 2006. Label propogation and quadratic crite-
rion. MIT Press.
Jon Louis Bentley. 1980. Multidimensional divide-and
-conquer. Communications of the ACM, 23(4):214 -
229.
Alina Beygelzimer, Sham Kakade, and John Langford.
2006. Cover trees for nearest neighbor. In Proceed-
ings of ICML, pages 97-104, New York, USA
Olivier Chapelle, Bernhard Scho? lkopf, and Alexander
Zien. 2006. Semi-supervised learning. MIT Press.
Samuel I. Daitch, Jonathan A. Kelner, and Daniel A.
Spielman. 2009. Fitting a graph to vector data. In
Proceedings of ICML, 201-208, NY, USA.
Dipanjan Das and Noah A. Smith. 2011. Semi-
supervised framesemantic parsing for unknown
predicates. In Proceedings of ACL, pages 1435-
1444, Portland, Oregon, USA.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
Part-of-Speech Tagging with Bilingual Graph-based
Projections. In Proceedings of ACL, pages 1435-
1444, Portland, Oregon, USA.
Dipanjan Das and Noah A. Smith. 2012. Graph-based
lexicon expansion with sparsity-inducing penalties.
In Proceedings of NAACL, pages 677-687, Montre?al,
Canada.
Tony Jebara, Jun Wang, and Shih-Fu Chang. 2009.
Graph construction and b-matching for semi-
supervised learning. In Proceedings of ICML, 441-
448, New York, USA.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Liu.
2008. A Cascaded Linear Model for Joint Chinese
Word Segmentation and Part-of-Speech Tagging. In
Proceedings of ACL, pages 897-904, Columbus, O-
hio.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic Adaptation of Annotation Standards: Chi-
nese Word Segmentation and POS Tagging ? A Case
Study. In Proceedings of he ACL and the 4th IJC-
NLP of the AFNLP, pages 522?530, Suntec, Singa-
pore.
Feng Jiao, Shaojun Wang, and Chi-Hoon Lee. 2006.
Semi-supervised conditional random fields for im-
proved sequence segmentation and labeling. In In
Proceedings of ACL, pages 209?216, Sydney, Aus-
tralia.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint Chinese word segmentation and
POS tagging. In Proceedings of ACL and IJCNLP
of the AFNLP, pages 513- 521, Suntec, Singapore
August.
778
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Field: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of ICML, pages 282-
289, Williams College, USA.
Xiao Li. 2009. On the use of virtual evidence in con-
ditional random fields. In Proceedings of EMNLP,
pages 1289-1297, Singapore.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extract-
ing structured information from user queries with
semi-supervised conditional random fields In Pro-
ceedings of ACM SIGIR, pages 572-579, Boston,
USA.
Gideon S. Mann and Andrew McCallum. 2007. Ef-
ficient computation of entropy gradient for semi-
supervised conditional random fields. In Proceed-
ings of NAACL, pages 109-112, New York, USA.
McCallum and Andrew Kachites. 2002. MALLET: A
Machine Learning for Language Toolkit. Software
at http://mallet.cs.umass.edu.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A
hybrid approach to word segmentation and POS tag-
ging. In Proceedings of ACL Demo and Poster Ses-
sion, pages 217?220, Prague, Czech Republic.
Hwee Tou Ng and Jin Kiat Low 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
EMNLP, Barcelona, Spain.
Xian Qian and Yang Liu. 2012. Joint Chinese Word
Segmentation, POS Tagging and Parsing. In Pro-
ceedings of EMNLP-CoNLL, pages 501-511, Jeju Is-
land, Korea.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
CRF based joint decoding method for cascade seg-
mentation and labelling tasks. In Proceedings of IJ-
CAI, Hyderabad, India.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-
supervised learning of structured tagging models.
In Proceedings of EMNLP, pages 167-176, Mas-
sachusetts, USA.
Weiwei Sun. 2011. A Stacked Sub-Word Model
for Joint Chinese Word Segmentation and Part-of-
Speech Tagging. In Proceedings of ACL, pages
1385?1394, Portland, Oregon.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of EMNLP, pages 970-979, Scotland, UK.
Partha Pratim Talukdar, Joseph Reisinger, Marius Pas-
ca, Deepak Ravichandran, Rahul Bhagat, and Fer-
nando Pereira. 2008. Weakly Supervised Acquisi-
tion of Labeled Class Instances using Graph Ran-
dom Walks. In Proceedings of EMNLP, pages 582-
590, Hawaii, USA.
Partha Pratim Talukdar and Koby Crammer. 2009.
New Regularized Algorithms for Transductive
Learning. In Proceedings of ECML-PKDD, pages
442 - 457, Bled, Slovenia.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learn-
ing methods for class-instance acquisition. In Pro-
ceedings of ACL, pages 1473-1481, Uppsala, Swe-
den.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Torisa-
wa. 2011. Improving Chinese word segmentation
and POS tagging with semi-supervised methods us-
ing large auto-analyzed data. In Proceedings of IJC-
NLP, pages 309?317, Chiang Mai, Thailand.
Yu-Chieh Wu Jie-Chi Yang, and Yue-Shi Lee. 2008.
Description of the NCU Chinese Word Segmenta-
tion and Part-of-Speech Tagging for SIGHAN Bake-
off. In Proceedings of the SIGHAN Workshop on
Chinese Language Processing, pages 161-166, Hy-
derabad, India.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Herman-
n Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In Proceedings of COLING, pages 1017-1024,
Manchester, UK.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of EMNLP, pages 888-896,
Columbus, Ohio.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of
EMNLP, pages 843-852, Massachusetts, USA.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in Chinese
word segmentation via conditional random field
modeling. In Proceedings of PACLIC, pages 87-94,
Wuhan, China.
Xiaojin Zhu, Zoubin Ghahramani, and John Laffer-
ty. 2003. Semi-supervised learning using Gaussian
fields and harmonic functions. In Proceedings of
ICML, pages 912?919, Washington DC, USA.
Ciyou Zhu, Richard H. Byrd, Peihuang Lu, and Jorge
Nocedal. 1997. L-BFGS-B: Fortran subroutines for
large scale bound constrained optimization. ACM
Transactions on Mathematical Software, 23:550-
560.
779
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 171?176,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Co-regularizing character-based and word-based models for
semi-supervised Chinese word segmentation
Xiaodong Zeng? Derek F. Wong? Lidia S. Chao? Isabel Trancoso?
?Department of Computer and Information Science, University of Macau
?INESC-ID / Instituto Superior Te?cnico, Lisboa, Portugal
nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo,
isabel.trancoso@inesc-id.pt
Abstract
This paper presents a semi-supervised
Chinese word segmentation (CWS) ap-
proach that co-regularizes character-based
and word-based models. Similarly to
multi-view learning, the ?segmentation
agreements? between the two differen-
t types of view are used to overcome the
scarcity of the label information on unla-
beled data. The proposed approach train-
s a character-based and word-based mod-
el on labeled data, respectively, as the ini-
tial models. Then, the two models are con-
stantly updated using unlabeled examples,
where the learning objective is maximiz-
ing their segmentation agreements. The a-
greements are regarded as a set of valuable
constraints for regularizing the learning of
both models on unlabeled data. The seg-
mentation for an input sentence is decod-
ed by using a joint scoring function com-
bining the two induced models. The e-
valuation on the Chinese tree bank reveals
that our model results in better gains over
the state-of-the-art semi-supervised mod-
els reported in the literature.
1 Introduction
Chinese word segmentation (CWS) is a critical
and a necessary initial procedure with respect to
the majority of high-level Chinese language pro-
cessing tasks such as syntax parsing, informa-
tion extraction and machine translation, since Chi-
nese scripts are written in continuous characters
without explicit word boundaries. Although su-
pervised CWS models (Xue, 2003; Zhao et al,
2006; Zhang and Clark, 2007; Sun, 2011) pro-
posed in the past years showed some reasonably
accurate results, the outstanding problem is that
they rely heavily on a large amount of labeled da-
ta. However, the production of segmented Chi-
nese texts is time-consuming and expensive, since
hand-labeling individual words and word bound-
aries is very hard (Jiao et al, 2006). So, one can-
not rely only on the manually segmented data to
build an everlasting model. This naturally pro-
vides motivation for using easily accessible raw
texts to enhance supervised CWS models, in semi-
supervised approaches. In the past years, however,
few semi-supervised CWS models have been pro-
posed. Xu et al (2008) described a Bayesian semi-
supervised model by considering the segmentation
as the hidden variable in machine translation. Sun
and Xu (2011) enhanced the segmentation result-
s by interpolating the statistics-based features de-
rived from unlabeled data to a CRFs model. An-
other similar trial via ?feature engineering? was
conducted by Wang et al (2011).
The crux of solving semi-supervised learning
problem is the learning on unlabeled data. In-
spired by multi-view learning that exploits redun-
dant views of the same input data (Ganchev et
al., 2008), this paper proposes a semi-supervised
CWS model of co-regularizing from two dif-
ferent views (intrinsically two different models),
character-based and word-based, on unlabeled da-
ta. The motivation comes from that the two types
of model exhibit different strengths and they are
mutually complementary (Sun, 2010; Wang et al,
2010). The proposed approach begins by train-
ing a character-based and word-based model on
labeled data respectively, and then both models
are regularized from each view by their segmen-
tation agreements, i.e., the identical outputs, of
unlabeled data. This paper introduces segmenta-
tion agreements as gainful knowledge for guiding
the learning on the texts without label information.
Moreover, in order to better combine the strengths
of the two models, the proposed approach uses a
joint scoring function in a log-linear combination
form for the decoding in the segmentation phase.
171
2 Segmentation Models
There are two classes of CWS models: character-
based and word-based. This section briefly re-
views two supervised models in these categories,
a character-based CRFs model, and a word-based
Perceptrons model, which are used in our ap-
proach.
2.1 Character-based CRFs Model
Character-based models treat word segmentation
as a sequence labeling problem, assigning label-
s to the characters in a sentence indicating their
positions in a word. A 4 tag-set is used in this
paper: B (beginning), M (middle), E (end) and
S (single character). Xue (2003) first proposed
the use of CRFs model (Lafferty et al, 2001) in
character-based CWS. Let x = (x1x2...x|x|) ? X
denote a sentence, where each character and y =
(y1y2...y|y|) ? Y denote a tag sequence, yi ? T
being the tag assigned to xi. The goal is to achieve
a label sequence with the best score in the form,
p?c(y|x) =
1
Z(x; ?c)
exp{f(x, y) ? ?c} (1)
where Z(x; ?c) is a partition function that normal-
izes the exponential form to be a probability distri-
bution, and f(x, y) are arbitrary feature functions.
The aim of CRFs is to estimate the weight param-
eters ?c that maximizes the conditional likelihood
of the training data:
??c = argmax
?c
l?
i=1
log p?c(yi|xi)? ???c?22 (2)
where ???c?22 is a regularizer on parameters to
limit overfitting on rare features and avoid degen-
eracy in the case of correlated features. In this
paper, this objective function is optimized by s-
tochastic gradient method. For the decoding, the
Viterbi algorithm is employed.
2.2 Word-based Perceptrons Model
Word-based models read a input sentence from left
to right and predict whether the current piece of
continuous characters is a word. After one word
is identified, the method moves on and searches
for a next possible word. Zhang and Clark (2007)
first proposed a word-based segmentation mod-
el using a discriminative Perceptrons algorithm.
Given a sentence x, let us denote a possible seg-
mented sentence as w ? w, and the function that
enumerates a set of segmentation candidates as
GEN:w = GEN(x) for x. The objective is to
maximize the following problem for all sentences:
??w = argmax
w=GEN(x)
|w|?
i=1
?(x,wi) ? ?w (3)
where it maps the segmented sentencew to a glob-
al feature vector ? and denotes ?w as its cor-
responding weight parameters. The parameter-
s ?w can be estimated by using the Perceptron-
s method (Collins, 2002) or other online learning
algorithms, e.g., Passive Aggressive (Crammer et
al., 2006). For the decoding, a beam search decod-
ing method (Zhang and Clark, 2007) is used.
2.3 Comparison Between Both Models
Character-based and word-based models present
different behaviors and each one has its own
strengths and weakness. Sun (2010) carried out a
thorough survey that includes theoretical and em-
pirical comparisons from four aspects. Here, two
critical properties of the two models supporting
the co-regularization in this study are highlight-
ed. Character-based models present better predic-
tion ability for new words, since they lay more
emphasis on the internal structure of a word and
thereby express more nonlinearity. On the oth-
er side, it is easier to define the word-level fea-
tures in word-based models. Hence, these models
have a greater representational power and conse-
quently better recognition performance for in-of-
vocabulary (IV) words.
3 Semi-supervised Learning via
Co-regularizing Both Models
As mentioned earlier, the primary challenge of
semi-supervised CWS concentrates on the unla-
beled data. Obviously, the learning on unlabeled
data does not come for ?free?. Very often, it is
necessary to discover certain gainful information,
e.g., label constraints of unlabeled data, that is in-
corporated to guide the learner toward a desired
solution. In our approach, we believe that the seg-
mentation agreements (? 3.1) from two differen-
t views, character-based and word-based models,
can be such gainful information. Since each of the
models has its own merits, their consensuses signi-
fy high confidence segmentations. This naturally
leads to a new learning objective that maximizes
segmentation agreements between two models on
unlabeled data.
172
This study proposes a co-regularized CWS
model based on character-based and word-based
models, built on a small amount of segmented sen-
tences (labeled data) and a large amount of raw
sentences (unlabeled data). The model induction
process is described in Algorithm 1: given labeled
dataset Dl and unlabeled dataset Du, the first t-
wo steps are training a CRFs (character-based) and
Perceptrons (word-based) model on the labeled
data Dl , respectively. Then, the parameters of
both models are continually updated using unla-
beled examples in a learning cycle. At each iter-
ation, the raw sentences in Du are segmented by
current character-based model ?c and word-based
model ?w. Meanwhile, all the segmentation agree-
ments A are collected (? 3.1). Afterwards, the
agreements A are used as a set of constraints to
bias the learning of CRFs (? 3.2) and Perceptron
(? 3.3) on the unlabeled data. The convergence
criterion is the occurrence of a reduction of seg-
mentation agreements or reaching the maximum
number of learning iterations. In the final segmen-
tation phase, given a raw sentence, the decoding
requires both induced models (? 3.4) in measuring
a segmentation score.
Algorithm 1 Co-regularized CWS model induction
Require: n labeled sentencesDl;m unlabeled sentencesDu
Ensure: ?c and ?w
1: ?0c ? crf train(Dl)
2: ?0w ? perceptron train(Dl)
3: for t = 1...Tmax do
4: At ? agree(Du, ?t?1c , ?t?1w )
5: ?tc ? crf train constraints(Du,At, ?t?1c )
6: ?tw ? perceptron train constraints(Du,At, ?t?1w )
7: end for
3.1 Agreements Between Two Models
Given a raw sentence, e.g., ?????????
?????(I am watching the opening ceremony
of the Olympics in Beijing.)?, the two segmenta-
tions shown in Figure 1 are the predictions from
a character-based and word-based model. The
segmentation agreements between the two mod-
els correspond to the identical words. In this ex-
ample, the five words, i.e. ?? (I)?, ??? (Bei-
jing)?, ?? (watch)?, ???? (opening ceremony)?
and ??(.)?, are the agreements.
3.2 CRFs with Constraints
For the character-based model, this paper fol-
lows (Ta?ckstro?m et al, 2013) to incorporate the
segmentation agreements into CRFs. The main
idea is to constrain the size of the tag sequence
lattice according to the agreements for achieving
simplified learning. Figure 2 demonstrates an ex-
ample of the constrained lattice, where the bold
node represents that a definitive tag derived from
the agreements is assigned to the current charac-
ter, e.g., ?? (I)? has only one possible tag ?S?
because both models segmented it to a word with
a single character. Here, if the lattice of all admis-
sible tag sequences for the sentence x is denoted
as Y(x), the constrained lattice can be defined by
Y?(x, y?), where y? refers to tags inferred from the
agreements. Thus, the objective function on unla-
beled data is modeled as:
???c = argmax
?c
m?
i=1
log p?c(Y?(xi, y?i)|xi)? ???c?22
(4)
It is a marginal conditional probability given by
the total probability of all tag sequences consistent
with the constrained lattice Y?(x, y?). This objec-
tive can be optimized by using LBFGS-B (Zhu et
al., 1997), a generic quasi-Newton gradient-based
optimizer.
Figure 1: The segmentations given by character-
based and word-based model, where the words in
?2? refer to the segmentation agreements.
Figure 2: The constrained lattice representation
for a given sentence, ????????????
???.
3.3 Perceptrons with Constraints
For the word-based model, this study incorporates
segmentation agreements by a modified parame-
ter update criterion in Perceptrons online training,
as shown in Algorithm 2. Because there are no
?gold segmentations? for unlabeled sentences, the
output sentence predicted by the current model is
compared with the agreements instead of the ?an-
swers? in the supervised case. At each parameter
173
update iteration k, each raw sentence xu is decod-
ed with the current model into a segmentation zu.
If the words in output zu do not match the agree-
ments A(xu) of the current sentence xu, the pa-
rameters are updated by adding the global feature
vector of the current training example with the a-
greements and subtracting the global feature vec-
tor of the decoder output, as described in lines 3
and 4 of Algorithm 2.
Algorithm 2 Parameter update in word-based model
1: for k = 1...K, u = 1...m do
2: calculate zu = argmax
w=GEN(x)
?|w|
i=1 ?(xu, wi) ? ?k?1w
3: if zu 6= A(xu)
4: ?kw = ?k?1w + ?(A(xu))? ?(zu)
5: end for
3.4 The Joint Score Function for Decoding
There are two co-regularized models as results of
the previous induction steps. An intuitive idea is
that both induced models are combined to conduct
the segmentation, for the sake of integrating their
strengths. This paper employs a log-linear inter-
polation combination (Bishop, 2006) to formulate
a joint scoring function based on character-based
and word-based models in the decoding:
Score(w) = ? ? log(p?c(y|x))
+(1? ?) ? log(?(x,w) ? ?w) (5)
where the two terms of the logarithm are the s-
cores of character-based and word-based model-
s, respectively, for a given segmentation w. This
composite function uses a parameter ? to weight
the contributions of the two models. The ? value
is tuned using the development data.
4 Experiment
4.1 Setting
The experimental data is taken from the Chinese
tree bank (CTB). In order to make a fair compar-
ison with the state-of-the-art results, the versions
of CTB-5, CTB-6, and CTB-7 are used for the e-
valuation. The training, development and testing
sets are defined according to the previous works.
For CTB-5, the data split from (Jiang et al, 2008)
is employed. For CTB-6, the same data split as
recommended in the CTB-6 official document is
used. For CTB-7, the datasets are formed accord-
ing to the way in (Wang et al, 2011). The cor-
responding statistic information on these data s-
plits is reported in Table 1. The unlabeled data in
our experiments is from the XIN CMN portion of
Chinese Gigaword 2.0. The articles published in
1991-1993 and 1999-2004 are used as unlabeled
data, with 204 million words.
The feature templates in (Zhao et al, 2006)
and (Zhang and Clark, 2007) are used in train-ing
the CRFs model and Perceptrons model, respec-
tively. The experimental platform is implement-
ed based on two popular toolkits: CRF++ (Kudo,
2005) and Zpar (Zhang and Clark, 2011).
Data #Sent-train
#Sent-
dev
#Sent-
test
OOV-
dev
OOV-
test
CTB-5 18,089 350 348 0.0811 0.0347
CTB-6 23,420 2,079 2,796 0.0545 0.0557
CTB-7 31,131 10,136 10,180 0.0549 0.0521
Table 1: Statistics of CTB-5, CTB-6 and CTB-7
data.
4.2 Main Results
The development sets are mainly used to tune the
values of the weight factor ? in Equation 5. We
evaluated the performance (F-score) of our model
on the three development sets by using differen-
t ? values, where ? is progressively increased in
steps of 0.1 (0 < ? < 1.0). The best performed
settings of ? for CTB-5, CTB-6 and CTB-7 on de-
velopment data are 0.7, 0.6 and 0.6, respectively.
With the chosen parameters, the test data is used
to measure the final performance.
Table 2 shows the F-score results of word seg-
mentation on CTB-5, CTB-6 and CTB-7 testing
sets. The line of ?ours? reports the performance
of our semi-supervised model with the tuned pa-
rameters. We first compare it with the supervised
?baseline? method which joints character-based
and word-based model trained only on the training
set1. It can be observed that our semi-supervised
model is able to benefit from unlabeled data and
greatly improves the results over the supervised
baseline. We also compare our model with two
state-of-the-art semi-supervised methods of Wang
?11 (Wang et al, 2011) and Sun ?11 (Sun and X-
u, 2011). The performance scores of Wang ?11 are
directly taken from their paper, while the results of
Sun ?11 are obtained, using the program provided
by the author, on the same experimental data. The
1The ?baseline? uses a different training configuration so
that the ? values in the decoding are also need to be tuned on
the development sets. The tuned ? values are {0.6, 0.6, 0.5}
for CTB-5, CTB-6 and CTB-7.
174
bold scores indicate that our model does achieve
significant gains over these two semi-supervised
models. This outcome can further reveal that us-
ing the agreements from these two views to regu-
larize the learning can effectively guide the mod-
el toward a better solution. The third compari-
son candidate is Hatori ?12 (Hatori et al, 2012)
which reported the best performance in the litera-
ture on these three testing sets. It is a supervised
joint model of word segmentation, POS tagging
and dependency parsing. Impressively, our model
still outperforms Hatori ?12 on all three datasets.
Although there is only a 0.01 increase on CTB-5,
it can be seen as a significant improvement when
considering Hatori ?12 employs much richer train-
ing resources, i.e., sentences tagged with syntactic
information.
Method CTB-5 CTB-6 CTB-7
Ours 98.27 96.33 96.72
Baseline 97.58 94.71 94.87
Wang ?11 98.11 95.79 95.65
Sun ?11 98.04 95.44 95.34
Hatori ?12 98.26 96.18 96.07
Table 2: F-score (%) results of five CWS models
on CTB-5, CTB-6 and CTB-7.
5 Conclusion
This paper proposed an alternative semi-
supervised CWS model that co-regularizes a
character- and word-based model by using their
segmentation agreements on unlabeled data. We
perform the agreements as valuable knowledge
for the regularization. The experiment results
reveal that this learning mechanism results in a
positive effect to the segmentation performance.
Acknowledgments
The authors are grateful to the Science and Tech-
nology Development Fund of Macau and the Re-
search Committee of the University of Macau for
the funding support for our research, under the ref-
erence No. 017/2009/A and MYRG076(Y1-L2)-
FST13-WF. The authors also wish to thank the
anonymous reviewers for many helpful comments.
References
Christopher M. Bishop. 2006. Pattern recognition and
machine learning.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1-8, Philadelphia, USA.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of ma-chine
learning research, 7:551-585.
Kuzman Ganchev, Joao Graca, John Blitzer, and Ben
Taskar. 2008. Multi-View Learning over Struc-
tured and Non-Identical Outputs. In Proceedings of
CUAI, pages 204-211, Helsinki, Finland.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2012. Incremental Joint Approach
to Word Segmentation, POS Tagging, and Depen-
dency Parsing in Chinese. In Proceedings of ACL,
pages 1045-1053, Jeju, Republic of Korea.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Liu.
2008. A Cascaded Linear Model for Joint Chinese
Word Segmentation and Part-of-Speech Tagging. In
Proceedings of ACL, pages 897-904, Columbus, O-
hio.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic Adaptation of Annotation Standards: Chi-
nese Word Segmentation and POS Tagging - A Case
Study. In Proceedings of ACL and the 4th IJCNLP
of the AFNLP, pages 522-530, Suntec, Singapore.
Feng Jiao, Shaojun Wang and Chi-Hoon Lee. 2006.
Semi-supervised conditional random fields for im-
proved sequence segmentation and labeling. In Pro-
ceedings of ACL and the 4th IJCNLP of the AFNLP,
pages 209-216, Strouds-burg, PA, USA.
Taku Kudo. 2005. CRF++: Yet another CRF toolkit.
Software available at http://crfpp.sourceforge. net.
John Lafferty, Andrew McCallum, and Fernando Pe-
reira. 2001. Conditional Random Field: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of ICML, pages 282-
289, Williams College, USA.
Weiwei Sun. 2001. Word-based and character-based
word segmentation models: comparison and com-
bination. In Proceedings of COLING, pages 1211-
1219, Bejing, China.
Weiwei Sun. 2011. A stacked sub-word model for
joint Chinese word segmentation and part-of-speech
tagging. In Proceedings of ACL, pages 1385-1394,
Portland, Oregon.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of EMNLP, pages 970-979, Scotland, UK.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan M-
cDonald, and Joakim Nivre. 2013. Token and Type
Constraints for Cross-Lingual Part-of-Speech Tag-
ging. In Transactions of the Association for Compu-
tational Linguistics, 1:1-12.
175
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010.
A Character-Based Joint Model for Chinese Word
Segmentation. In Proceedings of COLING, pages
1173-1181, Bejing, China.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Torisawa.
2011. Improving Chinese word segmentation and
POS tagging with semi-supervised methods using
large auto-analyzed data. In Proceedings of IJC-
NLP, pages 309-317, Hyderabad, India.
Jia Xu, Jianfeng Gao, Kristina Toutanova and Her-
mann Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In Proceedings of COLING, pages 1017-1024,
Manchester, UK.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29-48.
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation using a word-based perceptron algorithm. In
Proceedings of ACL, pages 840-847, Prague, Czech
Republic.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of
EMNLP, pages 843-852, Massachusetts, USA.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105-151.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in Chinese
word segmentation via conditional random field
modeling. In Proceedings of PACLIC, pages 87-94,
Wuhan, China.
Ciyou Zhu, Richard H. Byrd, Peihuang Lu, and Jorge
Nocedal. 2006. L-BFGS-B: Fortran subroutines for
large scale bound constrained optimization. ACM
Transactions on Mathematical Software, 23:550-
560.
176
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 61?66,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Meet EDGAR, a tutoring agent at MONSERRATE
Pedro Fialho, Lu??sa Coheur, Se?rgio Curto, Pedro Cla?udio
A?ngela Costa, Alberto Abad, Hugo Meinedo and Isabel Trancoso
Spoken Language Systems Lab (L2F), INESC-ID
Rua Alves Redol 9
1000-029 Lisbon, Portugal
name.surname@l2f.inesc-id.pt
Abstract
In this paper we describe a platform for
embodied conversational agents with tu-
toring goals, which takes as input written
and spoken questions and outputs answers
in both forms. The platform is devel-
oped within a game environment, and cur-
rently allows speech recognition and syn-
thesis in Portuguese, English and Spanish.
In this paper we focus on its understand-
ing component that supports in-domain in-
teractions, and also small talk. Most in-
domain interactions are answered using
different similarity metrics, which com-
pare the perceived utterances with ques-
tions/sentences in the agent?s knowledge
base; small-talk capabilities are mainly
due to AIML, a language largely used by
the chatbots? community. In this paper
we also introduce EDGAR, the butler of
MONSERRATE, which was developed in
the aforementioned platform, and that an-
swers tourists? questions about MONSER-
RATE.
1 Introduction
Several initiatives have been taking place in the
last years, targeting the concept of Edutainment,
that is, education through entertainment. Fol-
lowing this strategy, virtual characters have ani-
mated several museums all over the world: the
3D animated Hans Christian Andersen is ca-
pable of establishing multimodal conversations
about the writer?s life and tales (Bernsen and
Dybkjr, 2005), Max is a virtual character em-
ployed as guide in the Heinz Nixdorf Museums
Forum (Pfeiffer et al, 2011), and Sergeant Black-
well, installed in the Cooper-Hewitt National De-
sign Museum in New York, is used by the U.S.
Army Recruiting Command as a hi-tech attrac-
tion and information source (Robinson et al,
Figure 1: EDGAR at MONSERRATE.
2008). DuARTE Digital (Mendes et al, 2009)
and EDGAR are also examples of virtual charac-
ters for the Portuguese language with the same
edutainment goal: DuARTE Digital answers ques-
tions about Custo?dia de Bele?m, a famous work of
the Portuguese jewelry; EDGAR is a virtual butler
that answers questions about MONSERRATE (Fig-
ure 1).
Considering the previous mentioned agents,
they all cover a specific domain of knowledge (al-
though a general Question/Answering system was
integrated in Max (Waltinger et al, 2011)). How-
ever, as expected, people tend also to make small
talk when interacting with these agents. There-
fore, it is important that these systems properly
deal with it. Several strategies are envisaged to
this end and EDGAR is of no exception. In this
paper, we describe the platform behind EDGAR,
which we developed aiming at the fast insertion of
in-domain knowledge, and to deal with small talk.
This platform is currently in the process of being
industrially applied by a company known for its
expertise in building and deploying kiosks. We
will provide the hardware and software required
to demonstrate EDGAR, both on a computer and
on a tablet.
This paper is organized as follows: in Sec-
tion 2 we present EDGAR?s development platform
61
Figure 2: EDGAR architecture
and describe typical interactions, in Section 3 we
show how we move from in-domain interactions
to small talk, and in Section 4 we present an anal-
ysis on collected logs and their initial evaluation
results. Finally, in Section 5 we present some con-
clusions and point to future work.
2 The Embodied Conversational Agent
platform
2.1 Architecture overview
The architecture of the platform, generally de-
signed for the development of Embodied Con-
versational Agents (ECAs) (such as EDGAR), is
shown in Figure 2. In this platform, several mod-
ules intercommunicate by means of well defined
protocols, thus leveraging the capabilities of inde-
pendent modules focused on specific tasks, such
as speech recognition or 3D rendering/animation.
This independence allows us to use subsets of this
platform modules in scenarios with different re-
quirements (for instance, we can record characters
uttering a text).
Design and deployment of the front end of
EDGAR is performed in a game engine, which has
enabled the use of computer graphics technologies
and high quality assets, as seen in the video game
industry.
2.2 Multimodal components
The game environment, where all the interac-
tion with EDGAR takes place, is developed in the
Unity1 platform, being composed of one highly
1http://unity3d.com/
detailed character, made and animated by Rocket-
box studios2, a virtual keyboard and a push-while-
talking button.
In this platform, Automatic Speech Recogni-
tion (ASR) is performed by AUDIMUS (Meinedo
et al, 2003) for all languages, using generic acous-
tic and language models, recently compiled from
broadcast news data (Meinedo et al, 2010). Lan-
guage models were interpolated with all the do-
main questions defined in the Natural Language
Understanding (NLU) framework (see below),
while ASR includes features such as speech/non-
speech (SNS) detection and automatic gain control
(AGC). Speech captured in a public space raises
several ASR robustness issues, such as loudness
variability of spoken utterances, which is partic-
ularly bound to happen in a museological envi-
ronment (such as MONSERRATE) where silence is
usually incited. Thus, we have added a bounded
amplication to the captured signal, despite the
AGC mechanism, ensuring that too silent sounds
are not discarded by the SNS mechanism.
Upon a spoken input, AUDIMUS translates it
into a sentence, with a confidence value. An
empty recognition result, or one with low con-
fidence, triggers a control tag (? REPEAT ?) to
the NLU module, which results in a request for
the user to repeat what was said. The answer re-
turned by the NLU module is synthesized in a lan-
guage dependent Text To Speech (TTS) system,
with DIXI (Paulo et al, 2008) being used for Por-
tuguese, while a recent version of FESTIVAL (Zen
et al, 2009) covers both English and Spanish. The
2http://www.rocketbox-libraries.com/
62
synthesized audio is played while the correspond-
ing phonemes are mapped into visemes, repre-
sented as skeletal animations, being synchronized
according to phoneme durations, available in all
the employed TTS engines.
Emotions are declared in the knowledge sources
of the agent. As shown in Figure 3, they are coor-
dinated with viseme animations.
Figure 3: The EDGAR character in a joyful state.
2.3 Interacting with EDGAR
In a typical interaction, the user enters a ques-
tion with a virtual keyboard or says it to the mi-
crophone while pressing a button (Figure 4), in
the language chosen in the interface (as previously
said, Portuguese, English or Spanish).
Figure 4: A question written in the EDGAR inter-
face.
Then, the ASR will transcribe it and the NLU
module will process it. Afterwards, the answer,
chosen by the NLU module, is heard through
the speakers, due to the TTS, and sequentially
written in a talk bubble, according to the pro-
duced speech. The answer is accompanied with
visemes, represented by movements of the char-
acter?s mouth/lips, and by facial emotions as
marked in the answers of the NLU knowledge
base. A demo of EDGAR, only for English interac-
tions, can be tested in https://edgar.l2f.
inesc-id.pt/m3/edgar.php.
3 The natural language understanding
component
3.1 In-domain knowledge sources
The in-domain knowledge sources of the agent
are XML files, hand-crafted by domain experts.
This XML files have multilingual pairs consti-
tuted by different paraphrases of the same ques-
tion and possible answers. The main reason to
follow this approach (and contrary to other works
where grammars are used), is to ease the process
of creating/enriching the knowledge sources of the
agent being developed, which is typically done
by non experts in linguistics or computer science.
Thus, we opted for following a similar approach
of the work described, for instance, in (Leuski et
al., 2006), where the agents knowledge sources are
easy to create and maintain. An example of a ques-
tions/answers pair is:
<questions>
<q en="How is everything?"
es="Todo bien?">
Tudo bem?</q>
</questions>
<answers>
<a en="I am ok, thank you."
es="Estoy bien, gracias."
emotion="smile_02">
Estou bem, obrigado.</a>
</answers>
As it can been see from this example, emotions
are defined in these files, associated to each ques-
tion/answer pair (emotion=?smile? in the exam-
ple, one of the possible smile emotions).
These knowledge sources can be (automati-
cally) extended with ?synonyms?. We call them
?synonyms?, because they do not necessarily fit
in the usual definition of synonyms. Here we fol-
low a broader approach to this concept and if two
words, within the context of a sentence from the
knowledge source, will lead to the same answer,
then we consider them to be ?synonyms?. For
instance ?palace? or ?castle? are not synonyms.
However, people tend to refer to MONSERRATE in
both forms. Thus, we consider them to be ?syn-
onyms? and if one of these is used in the orig-
inal knowledge sources, the other is used to ex-
pand them. It should be clear that we will gener-
ate many incorrect questions with this procedure,
but empirical tests (out of the scope of this paper)
show that these questions do not hurt the system
performance. Moreover, they are useful for ASR
language model interpolation, which is based on
N-grams.
63
3.2 Out-of-domain knowledge sources
The same format of the previously described
knowledge sources can be used to represent out-
of-domain knowledge. Here, we extensively used
the ?synonyms? approach. For instance, words
wife and girlfriend are considered to be ?syn-
onyms? as all the personal questions with these
words should be answered with the same sentence:
I do not want to talk about my private life.
Nevertheless, and taking into consideration the
work around small talk developed by the chat-
bots community (Klwer, 2011), we decided to
use the most popular language to build chat-
bots: the ?Artificial Intelligence Markup Lan-
guage?, widely known as AIML, a derivative of
XML. With AIML, knowledge is coded as a set
of rules that will match the user input, associ-
ated with templates, the generators of the out-
put. A detailed description of AIML syntax can
be found in http://www.alicebot.org/
aiml.html. In what respects AIML inter-
preters, we opted to use Program D (java), which
we integrated in our platform. Currently, we use
AIML to deal with slang and to answer questions
that have to do with cinema and compliments.
As a curiosity, we should explain that we deal
with slang when input came from the keyboard,
and not when it is speech, as the language models
are not trained with this specific lexicon. The rea-
son we do that is because if the language models
were trained with slang, it would be possible to er-
roneously detect it in utterances and then answer
them accordingly, which could be extremely un-
pleasant. Therefore, EDGAR only deals with slang
when the input is the keyboard.
The current knowledge sources have 152 ques-
tion/answer pairs, corresponding to 763 questions
and 206 answers. For Portuguese, English and
Spanish the use of 226, 219 and 53 synonym re-
lations, led to the generation of 22 194, 16 378
and 1 716 new questions, respectively.
3.3 Finding the appropriate answer
The NLU module is responsible for the answer se-
lection process. It has three main components.
The first one, STRATEGIES, is responsible to
choose an appropriate answer to the received inter-
action. Several strategies are implemented, includ-
ing the ones based on string matching, string dis-
tances (as for instance, Levenshtein, Jaccard and
Dice), N-gram Overlap and support vector ma-
chines (seeing the answer selection as a classifica-
tion problem). Currently, best results are attained
using a combination of Jaccard and bigram Over-
lap measures and word weight through the use of
tf-idf statistic. In this case, Jaccard takes into ac-
count how many words are shared between the
user?s interaction and the knowledge source en-
try, bigram Overlap gives preference to the shared
sequences of words and tf-idf contributes to the
results attained by previous measures, by given
weight to unfrequent words, which should have
more weight on the decision process (for example,
the word MONSERRATE occurs in the majority of
the questions in the corpus, so it is not very infor-
mative and should not have the same weight as, for
instance, the word architect or owner).
The second component, PLUGINS, deals with
two different situations. First, it accesses Pro-
gram D when interactions are not answered by the
STRATEGIES component. That is, when the tech-
nique used by STRATEGIES returns a value that
is lower than a threshold (dependent of the used
technique), the PLUGIN component runs Program
D in order to try to find an answer to the posed
question. Secondly, when the ASR has no confi-
dence of the attained transcription (and returns the
? REPEAT ? tag) or Program D is not able to find
an answer, the PLUGINS component does the fol-
lowing (with the goal of taking the user again to
the agent topic of expertise):
? In the first time that this occurs, a sentence
such as Sorry, I did not understand you. is
chosen as the answer to be returned.
? The second time this occurs, EDGAR asks the
user I did not understand you again. Why
don?t you ask me X?, being X generated in
run time and being a question from a subset
of the questions from the knowledge sources.
Obviously, only in-domain (not expanded)
questions are considered for replacing X.
? The third time there is a misunderstanding,
EDGAR says We are not understanding each
other, let me talk about MONSERRATE. And
it randomly choses some answer to present to
the user.
The third component is the HISTORY-
TRACKER, which handles the agent knowledge
about previous interactions (kept until a default
time without interactions is reached).
64
4 Preliminary evaluation
Edgar is more a domain-specific Question An-
swering (QA) than a task-oriented dialogue sys-
tem. Therefore, we evaluated it with the metrics
typically used in QA. The mapping of the dif-
ferent situations in true/false positives/negatives is
explained in the following.
We have manually transcribed 1086 spoken ut-
terances (in Portuguese), which were then labeled
with the following tags, some depending on the
answer given by EDGAR:
? 0: in-domain question incorrectly answered,
although there was information in the knowl-
edge sources (excluding Program D) to an-
swer it;
? 1: out-of-domain question, incorrectly an-
swered;
? 2: question correctly answered by Program
D;
? 3: question correctly answered by using
knowledge sources (excluding Program D);
? 4: in-domain question, incorrectly answered.
There is no information in the knowledge
source to answer it, but it should be;
? 5: multiple questions, partially answered;
? 6: multiple questions, unanswered;
? 7: question with implicit information (there,
him, etc.), unanswered;
? 8: question which is not ?ipsis verbis? in the
knowledge source, but has a paraphrase there
and was not correctly answered;
? 9: question with a single word (garden,
palace), unanswered;
? 10: question that we do not want the system
to answer (some were answered, some were
not).
The previous tags were mapped into:
? true positives: questions marked with 2, 3
and 5;
? true negatives: questions marked with 0 and
10 (the ones that were not answered by the
system);
? false positives: questions marked with 0 and
10 (the ones that were answered by the sys-
tem);
? false negatives: questions marked with 4, 6,
7, 8 and 9.
Then, two experiments were conducted: in the
first, the NLU module was applied to the manual
transcriptions; in the second, directly to the output
of the ASR. Table 1 shows the results.
NLU input = manual transcriptions
Precision Recall F-measure
0.92 0.60 0.72
acNLU input = ASR
Precision Recall F-measure
0.71 0.32 0.45
Table 1: NLU results
The ASR Word Error Rate (WER) is of 70%.
However, we detect some problems in the way we
were collecting the audio, and in more recent eval-
uations (by using 363 recent logs where previous
problems were corrected), that error decreased to a
WER of 52%, including speech from 111 children,
21 non native Portuguese speakers (thus, with a
different pronunciation), 23 individuals not talking
in Portuguese and 27 interactions where multiple
speakers overlap. Here, we should refer the work
presented in (Traum et al, 2012), where an eval-
uation of two virtual guides in a museum is pre-
sented. They also had to deal with speakers from
different ages and with question off-topic, and re-
port a ASR with 57% WER (however they major-
ity of their user are children: 76%).
We are currently preparing a new corpus for
evaluating the NLU module, however, the follow-
ing results remain: in the best scenario, if tran-
scription is perfect, the NLU module behaves as
indicated in Table 1 (manual transcriptions).
5 Conclusions and Future Work
We have described a platform for developing
ECAs with tutoring goals, that takes both speech
and text as input and output, and introduced
EDGAR, the butler of MONSERRATE, which was
developed in that platform. Special attention was
given to EDGAR?s NLU module, which couples
techniques that try to find distances between the
user input and sentences in the existing knowledge
65
sources, with a framework imported from the chat-
bots community (AIML plus Program D). EDGAR
has been tested with real users for the last year and
we are currently performing a detailed evaluation
of it. There is much work to be done, including to
be able to deal with language varieties, which is an
important source of recognition errors. Moreover,
the capacity of dealing with out-of-domain ques-
tions is still a hot research topic and one of our
priorities in the near future. We have testified that
people are delighted when EDGAR answers out-
of-domain questions (Do you like soccer?/I rather
have a tea and read a good criminal book) and we
cannot forget that entertainment is also one of this
Embodied Conversational Agent (ECA)?s goal.
Acknowledgments
This work was supported by national
funds through FCT ? Fundac?a?o para a
Cie?ncia e a Tecnologia, under project PEst-
OE/EEI/LA0021/2013. Pedro Fialho, Se?rgio
Curto and Pedro Cla?udio scholarships were sup-
ported under project FALACOMIGO (ProjectoVII
em co-promoc?a?o, QREN n 13449).
References
N. O. Bernsen and L. Dybkjr. 2005. Meet hans chris-
tian andersen. In In Proceedings of Sixth SIGdial
Workshop on Discourse and Dialogue, pages 237?
241.
Tina Klwer. 2011. ?i like your shirt? ? dialogue acts
for enabling social talk in conversational agents. In
Proceedings of the 11th International Conference on
Intelligent Virtual Agents. International Conference
on Intelligent Virtual Agents (IVA), 11th, September
17-19, Reykjavik, Iceland. Springer.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective ques-
tion answering characters. In 7th SIGdial Workshop
on Discourse and Dialogue, Sydney, Australia.
Hugo Meinedo, Diamantino Caseiro, Joa?o Neto, and
Isabel Trancoso. 2003. Audimus.media: a broad-
cast news speech recognition system for the euro-
pean portuguese language. In Proceedings of the 6th
international conference on Computational process-
ing of the Portuguese language, PROPOR?03, pages
9?17, Berlin, Heidelberg. Springer-Verlag.
H. Meinedo, A. Abad, T. Pellegrini, I. Trancoso, and
J. P. Neto. 2010. The l2f broadcast news speech
recognition system. In Proceedings of Fala2010,
Vigo, Spain.
Ana Cristina Mendes, Rui Prada, and Lu??sa Coheur.
2009. Adapting a virtual agent to users? vocabu-
lary and needs. In Proceedings of the 9th Interna-
tional Conference on Intelligent Virtual Agents, IVA
?09, pages 529?530, Berlin, Heidelberg. Springer-
Verlag.
Se?rgio Paulo, Lu??s C. Oliveira, Carlos Mendes, Lu??s
Figueira, Renato Cassaca, Ce?u Viana, and Helena
Moniz. 2008. Dixi ? a generic text-to-speech sys-
tem for european portuguese. In Proceedings of the
8th international conference on Computational Pro-
cessing of the Portuguese Language, PROPOR ?08,
pages 91?100, Berlin, Heidelberg. Springer-Verlag.
Thies Pfeiffer, Christian Liguda, Ipke Wachsmuth, and
Stefan Stein. 2011. Living with a virtual agent:
Seven years with an embodied conversational agent
at the heinz nixdorf museumsforum. In Proceedings
of the International Conference Re-Thinking Tech-
nology in Museums 2011 - Emerging Experiences,
pages 121 ? 131. thinkk creative & the University of
Limerick.
Susan Robinson, David Traum, Midhun Ittycheriah,
and Joe Henderer. 2008. What would you ask a
conversational agent? observations of human-agent
dialogues in a museum setting. In International
Conference on Language Resources and Evaluation
(LREC), Marrakech, Morocco.
David Traum, Priti Aggarwal, Ron Artstein, Susan
Foutz, Jillian Gerten, Athanasios Katsamanis, Anton
Leuski, Dan Noren, and William Swartout. 2012.
Ada and grace: Direct interaction with museum
visitors. In The 12th International Conference on
Intelligent Virtual Agents (IVA), Santa Cruz, CA,
September.
Ulli Waltinger, Alexa Breuing, and Ipke Wachsmuth.
2011. Interfacing virtual agents with collaborative
knowledge: Open domain question answering us-
ing wikipedia-based topic models. In IJCAI, pages
1896?1902.
Heiga Zen, Keiichiro Oura, Takashi Nose, Junichi Ya-
magishi, Shinji Sako, Tomoki Toda, Takashi Ma-
suko, Alan W. Black, and Keiichi Tokuda. 2009.
Recent development of the HMM-based speech syn-
thesis system (HTS). In Proc. 2009 Asia-Pacific
Signal and Information Processing Association (AP-
SIPA), Sapporo, Japan, October.
66
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1360?1369,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Toward Better Chinese Word Segmentation for SMT via Bilingual
Constraints
Xiaodong Zeng
?
Lidia S. Chao
?
Derek F. Wong
?
Isabel Trancoso
?
Liang Tian
?
?
NLP
2
CT Lab / Department of Computer and Information Science, University of Macau
?
INESC-ID / Instituto Superior T?enico, Lisboa, Portugal
nlp2ct.samuel@gmail.com, {lidiasc, derekfw}@umac.mo,
isabel.trancoso@inesc-id.pt,tianliang0123@gmail.com
Abstract
This study investigates on building a
better Chinese word segmentation mod-
el for statistical machine translation. It
aims at leveraging word boundary infor-
mation, automatically learned by bilin-
gual character-based alignments, to induce
a preferable segmentation model. We
propose dealing with the induced word
boundaries as soft constraints to bias the
continuous learning of a supervised CRF-
s model, trained by the treebank data (la-
beled), on the bilingual data (unlabeled).
The induced word boundary information
is encoded as a graph propagation con-
straint. The constrained model induction
is accomplished by using posterior reg-
ularization algorithm. The experiments
on a Chinese-to-English machine transla-
tion task reveal that the proposed model
can bring positive segmentation effects to
translation quality.
1 Introduction
Word segmentation is regarded as a critical pro-
cedure for high-level Chinese language process-
ing tasks, since Chinese scripts are written in con-
tinuous characters without explicit word bound-
aries (e.g., space in English). The empirical works
show that word segmentation can be beneficial to
Chinese-to-English statistical machine translation
(SMT) (Xu et al, 2005; Chang et al, 2008; Zhao
et al, 2013). In fact most current SMT models
assume that parallel bilingual sentences should be
segmented into sequences of tokens that are meant
to be ?words? (Ma and Way, 2009). The practice
in state-of-the-art MT systems is that Chinese sen-
tences are tokenized by a monolingual supervised
word segmentation model trained on the hand-
annotated treebank data, e.g., Chinese treebank
(CTB) (Xue et al, 2005). These models are con-
ducive to MT to some extent, since they common-
ly have relatively good aggregate performance and
segmentation consistency (Chang et al, 2008).
But one outstanding problem is that these mod-
els may leave out some crucial segmentation fea-
tures for SMT, since the output words conform to
the treebank segmentation standard designed for
monolingually linguistic intuition, rather than spe-
cific to the SMT task.
In recent years, a number of works (Xu et al,
2005; Chang et al, 2008; Ma and Way, 2009;
Xi et al, 2012) attempted to build segmentation
models for SMT based on bilingual unsegment-
ed data, instead of monolingual segmented data.
They proposed to learn gainful bilingual knowl-
edge as golden-standard segmentation supervi-
sions for training a bilingual unsupervised mod-
el. Frequently, the bilingual knowledge refers to
the mappings of an individual English word to one
or more consecutive Chinese characters, generat-
ed via statistical character-based alignment. They
leverage such mappings to either constitute a Chi-
nese word dictionary for maximum-matching seg-
mentation (Xu et al, 2004), or form labeled data
for training a sequence labeling model (Paul et al,
2011). The prior works showed that these models
help to find some segmentations tailored for SMT,
since the bilingual word occurrence feature can be
captured by the character-based alignment (Och
and Ney, 2003). However, these models tend to
miss out other linguistic segmentation patterns as
monolingual supervised models, and suffer from
the negative effects of erroneously alignments to
word segmentation.
This paper proposes an alternative Chinese
Word Segmentation (CWS) model adapted to the
SMT task, which seeks not only to maintain the
advantages of a monolingual supervised model,
having hand-annotated linguistic knowledge, but
also to assimilate the relevant bilingual segmenta-
1360
tion nature. We propose leveraging the bilingual
knowledge to form learning constraints that guide
a supervised segmentation model toward a better
solution for SMT. Besides the bilingual motivat-
ed models, character-based alignment is also em-
ployed to achieve the mappings of the successive
Chinese characters and the target language word-
s. Instead of directly merging the characters in-
to concrete segmentations, this work attempts to
extract word boundary distributions for character-
level trigrams (types) from the ?chars-to-word?
mappings. Furthermore, these word boundaries
are encoded into a graph propagation (GP) expres-
sion, in order to widen the influence of the induced
bilingual knowledge among Chinese texts. The G-
P expression constrains similar types having ap-
proximated word boundary distributions. Crucial-
ly, the GP expression with the bilingual knowledge
is then used as side information to regularize a
CRFs (conditional random fields) model?s learn-
ing over treebank and bitext data, based on the
posterior regularization (PR) framework (Ganchev
et al, 2010). This constrained learning amounts to
a jointly coupling of GP and CRFs, i.e., integrating
GP into the estimation of a parametric structural
model.
This paper is structured as follows: Section 2
points out the main differences with the related
works of this study. Section 3 presents the de-
tails of the proposed segmentation model. Section
4 reports the experimental results of the proposed
model for a Chinese-to-English MT task. The con-
clusion is drawn in Section 5.
2 Related Work
In the literature, many approaches have been pro-
posed to learn CWS models for SMT. They can
be put into two categories, monolingual-motivated
and bilingual-motivated. The former primarily op-
timizes monolingual supervised models according
to some predefined segmentation properties that
are manually summarized from empirical MT e-
valuations. Chang et al (2008) enhanced a CRF-
s segmentation model in MT tasks by tuning the
word granularity and improving the segmentation
consistence. Zhang et al (2008) produced a bet-
ter segmentation model for SMT by concatenat-
ing various corpora regardless of their differen-
t specifications. Distinct from their behaviors,
this work uses automatically learned constraints
instead of manually defined ones. Most impor-
tantly, the constraints have a better learning guid-
ance since they originate from the bilingual texts.
On the other hand, the bilingual-motivated CWS
models typically rely on character-based align-
ments to generate segmentation supervisions. Xu
et al (2004) proposed to employ ?chars-to-word?
alignments to generate a word dictionary for max-
imum matching segmentation in SMT task. The
works in (Ma and Way, 2009; Zhao et al, 2013)
extended the dictionary extraction strategy. Ma
and Way (2009) adopted co-occurrence frequency
metric to iteratively optimize ?candidate words?
extract from the alignments. Zhao et al (2013) at-
tempted to find an optimal subset of the dictionary
learned by the character-based alignment to maxi-
mize the MT performance. Paul et al (2011) used
the words learned from ?chars-to-word? align-
ments to train a maximum entropy segmentation
model. Rather than playing the ?hard? uses of
the bilingual segmentation knowledge, i.e., direct-
ly merging ?char-to-word? alignments to words
as supervisions, this study extracts word bound-
ary information of characters from the alignments
as soft constraints to regularize a CRFs model?s
learning.
The graph propagation (GP) technique provides
a natural way to represent data in a variety of tar-
get domains (Belkin et al, 2006). In this tech-
nique, the constructed graph has vertices consist-
ing of labeled and unlabeled examples. Pairs of
vertices are connected by weighted edges encod-
ing the degree to which they are expected to have
the same label (Zhu et al, 2003). Many recent
works, such as by Subramanya et al (2010), Das
and Petrov (2011), Zeng et al (2013; 2014) and
Zhu et al (2014), proposed GP for inferring the la-
bel information of unlabeled data, and then lever-
age these GP outcomes to learn a semi-supervised
scalable model (e.g., CRFs). These approaches are
referred to as pipelined learning with GP. This s-
tudy also works with a similarity graph, encoding
the learned bilingual knowledge. But, unlike the
prior pipelined approaches, this study performs a
joint learning behavior in which GP is used as a
learning constraint to interact with the CRFs mod-
el estimation.
One of our main objectives is to bias CRF-
s model?s learning on unlabeled data, under a
non-linear GP constraint encoding the bilingual
knowledge. This is accomplished by the poste-
rior regularization (PR) framework (Ganchev et
1361
al., 2010). PR performs regularization on poste-
riors, so that the learned model itself remains sim-
ple and tractable, while during learning it is driven
to obey the constraints through setting appropriate
parameters. The closest prior study is constrained
learning, or learning with prior knowledge. Chang
et al (2008) described constraint driven learning
(CODL) that augments model learning on unla-
beled data by adding a cost for violating expec-
tations of constraint features designed by domain
knowledge. Mann and McCallum (2008) and M-
cCallum et al (2007) proposed to employ gener-
alized expectation criteria (GE) to specify prefer-
ences about model expectations in the form of lin-
ear constraints on some feature expectations.
3 Methodology
This work aims at building a CWS model adapted
to the SMT task. The model induction is shown in
Algorithm 1. The input data requires two type-
s of training resources, segmented Chinese sen-
tences from treebank D
c
l
and parallel unsegment-
ed sentences of Chinese and foreign language D
c
u
and D
f
u
. The first step is to conduct character-
based alignment over bitexts D
c
u
and D
f
u
, where
every Chinese character is an alignment target.
Here, we are interested on n-to-1 alignment pat-
terns, i.e., one target word is aligned to one or
more source Chinese characters. The second step
aims to collect word boundary distributions for al-
l types, i.e., character-level trigrams, according to
the n-to-1 mappings (Section 3.1). The third step
is to encode the induced word boundary informa-
tion into a k-nearest-neighbors (k-NN) similarity
graph constructed over the entire set of types from
D
c
l
and D
c
u
(Section 3.2). The final step trains a
discriminative sequential labeling model, condi-
tional random fields, on D
c
l
and D
c
u
under bilin-
gual constraints in a graph propagation expression
(Section 3.3). This constrained learning is carried
out based on posterior regularization (PR) frame-
work (Ganchev et al, 2010).
3.1 Word Boundaries Learned from
Character-based Alignments
The gainful supervisions toward a better segmen-
tation solution for SMT are naturally extracted
from MT training resources, i.e., bilingual parallel
data. This study employs an approximated method
introduced in (Xu et al, 2004; Ma and Way, 2009;
Chung and Gildea, 2009) to learn bilingual seg-
Algorithm 1 CWS model induction with bilingual
constraints
Require:
Segmented Chinese sentences from treebank
D
c
l
; Parallel sentences of Chinese and foreign
language D
c
u
and D
f
u
Ensure:
?: the CRFs model parameters
1: D
c?f
? char align bitext (D
c
u
,D
f
u
)
2: r ? learn word bound (D
c?f
)
3: G ? encode graph constraint (D
c
l
,D
c
u
, r)
4: ? ? pr crf graph (D
c
l
,D
c
u
,G)
mentation knowledge. This relies on statistical
character-based alignment: first, every Chinese
character in the bitexts is divided by a white s-
pace so that individual characters are regarded as
special ?words? or alignment targets, and second,
they are connected with English words by using
a statistical word aligner, e.g., GIZA++ (Och and
Ney, 2003). Note that the aligner is restricted to
use an n-to-1 alignment pattern. The primary idea
is that consecutive Chinese characters are grouped
to a candidate word, if they are aligned to the same
foreign word. It is worth mentioning that prior
works presented a straightforward usage for can-
didate words, treating them as golden segmenta-
tions, either dictionary units or labeled resources.
But this study treats the induced candidate word-
s in a different way. We propose to extract the
word boundary distributions
1
for character-level
trigrams (type)
2
, as shown in Figure 1, instead of
the very specific words. There are two main rea-
sons to do so. First, it is a more general expression
which can reduce the impact amplification of er-
roneous character alignments. Second, boundary
distributions can play more flexible roles as con-
straints over labelings to bias the model learning.
The type-level word boundary extraction is for-
mally described as follows. Given the ith sen-
tence pair ?x
c
i
, x
f
i
,A
c?f
i
? of the aligned bilin-
gual corpus D
c?f
, the Chinese sentence x
c
i
con-
sisting of m characters {x
c
i,1
, x
c
i,2
, ..., x
c
i,m
}, and
the foreign language sentence x
f
i
, consisting of
1
The distribution is on four word boundary labels indi-
cating the character positions in a word, i.e., B (begin), M
(middle), E (end) and S (single character).
2
A word boundary distribution corresponds to the center
character of a type. In fact, it aims at reducing label ambi-
guities to collect boundary information of character trigrams,
rather than individual characters (Altun et al, 2006).
1362
n words {x
f
i,1
, x
f
i,2
, ..., x
f
i,n
}, A
c?f
i
represents a
set of alignment pairs a
j
= ?C
j
, x
f
i,j
? that de-
fines connections between a few Chinese char-
acters C
j
= {x
c
i,j
1
, x
c
i,j
2
, ..., x
c
i,j
k
} and a sin-
gle foreign word x
f
i,j
. For an alignment a
j
=
?C
j
, x
f
i,j
?, only the sequence of characters C
j
=
{x
c
i,j
1
, x
c
i,j
2
, ..., x
c
i,j
k
} ?d ? [1, k?1], j
d+1
? j
d
=
1 constitutes a valid candidate word. For the w-
hole bilingual corpus, we assign each character
in the candidate words with a word boundary tag
T ? {B,M,E, S}, and then count across the en-
tire corpus to collect the tag distributions r
i
=
{r
i,t
; t ? T} for each type x
c
i,j?1
x
c
i,j
x
c
i,j+1
.
???
??
Beijin
g   Ol
ympu
s
Chara
cter-b
ased a
lignm
ent
???
??
BE 
   B   
M   E
Beijin
g   Ol
ympu
s
Word
 boun
daries
??? ??? ? Type-level W
ord 
bound
ary di
stribu
tions
BeiP
ing S
hi ???
BeiJi
ng Re
n ??? BeiJing
 Di ???
Quan
Yun H
ui ???
BeiJi
ng Sh
i ???
0.8 0
.6
0.3
0.2
0.9
AoYu
n Hui ??? 0.2
Figure 1: An example of similarity graph over
character-level trigrams (types).
3.2 Constraints Encoded by Graph
Propagation Expression
The previous step contributes to generate bilingual
segmentation supervisions, i.e., type-level word
boundary distributions. An intuitive manner is to
directly leverage the induced boundary distribu-
tions as label constraints to regularize segmenta-
tion model learning, based on a constrained learn-
ing algorithm. This study, however, makes further
efforts to elevate the positive effects of the bilin-
gual knowledge via the graph propagation tech-
nique. We adopt a similarity graph to encode
the learned type-level word boundary distribution-
s. The GP expression will be defined as a PR con-
straint in Section 3.3 that reflects the interactions
between the graph and the CRFs model. In other
words, GP is integrated with estimation of para-
metric structural model. This is greatly different
from the prior pipelined approaches (Subramanya
et al, 2010; Das and Petrov, 2011; Zeng et al,
2013), where GP is run first and its propagated
outcomes are then used to bias the structural mod-
el. This work seeks to capture the GP benefits dur-
ing the modeling of sequential correlations.
In what follows, the graph setting and propa-
gation expression are introduced. As in conven-
tional GP examples (Das and Smith, 2012), a sim-
ilarity graph G = (V,E) is constructed over N
types extracted from Chinese training data, includ-
ing treebank D
c
l
and bitexts D
c
u
. Each vertex V
i
has a |T |-dimensional estimated measure v
i
=
{v
i,t
; t ? T} representing a probability distribu-
tion on word boundary tags. The induced type-
level word boundary distributions r
i
= {r
i,t
; t ?
T} are empirical measures for the corresponding
M graph vertices. The edges E ? V
i
?V
j
connect
all the vertices. Scores between pairs of graph ver-
tices (types), w
ij
, refer to the similarities of their
syntactic environment, which are computed fol-
lowing the method in (Subramanya et al, 2010;
Das and Petrov, 2011; Zeng et al, 2013). The
similarities are measured based on co-occurrence
statistics over a set of predefined features (intro-
duced in Section 4.1). Specifically, the point-wise
mutual information (PMI) values, between ver-
tices and each feature instantiation that they have
in common, are summed to sparse vectors, and
their cosine distances are computed as the sim-
ilarities. The nature of this similarity graph en-
forces that the connected types with high weight-
s appearing in different texts should have similar
word boundary distributions.
The quality (smoothness) of the similarity graph
can be estimated by using a standard propagation
function, as shown in Equation 1. The square-loss
criterion (Zhu et al, 2003; Bengio et al, 2006) is
used to formulate this function:
P(v) =
T
?
t=1
(
M
?
i=1
(v
i,t
? r
i,t
)
2
+?
N
?
j=1
N
?
i=1
w
ij
(v
i,t
? v
j,t
)
2
+ ?
N
?
i=1
(v
i,t
)
2
)
(1)
The first term in this equation refers to seed match-
es that compute the distances between the estimat-
ed measure v
i
and the empirical probabilities r
i
.
The second term refers to edge smoothness that
measures how vertices v
i
are smoothed with re-
spect to the graph. Two types connected by an
edge with high weight should be assigned similar
word boundary distributions. The third term, a `
2
norm, evaluates the distribution sparsity (Das and
1363
Smith, 2012) per vertex. Typically, the GP process
amounts to an optimization process with respect
to parameter v such that Equation 1 is minimized.
This propagation function can be used to reflect
the graph smoothness, where the higher the score,
the lower the smoothness.
3.3 PR Learning with GP Constraint
Our learning problem belongs to semi-supervised
learning (SSL), as the training is done on treebank
labeled data (X
L
,Y
L
) = {(x
1
, y
1
), ..., (x
l
, y
l
)},
and bilingual unlabeled data (X
U
) = {x
1
, ..., x
u
}
where x
i
= {x
1
, ..., x
m
} is an input word se-
quence and y
i
= {y
1
, ..., y
m
}, y ? T is its corre-
sponding label sequence. Supervised linear-chain
CRFs can be modeled in a standard conditional
log-likelihood objective with a Gaussian prior:
L(?) = p
?
(y
i
|x
i
)?
???
2
2?
(2)
The conditional probabilities p
?
are expressed as a
log-linear form:
p
?
(y
i
|x
i
) =
exp(
m
?
k=1
?
T
f(y
k?1
i
, y
k
i
, x
i
))
Z
?
(x
i
)
(3)
Where Z
?
(x
i
) is a partition function that normal-
izes the exponential form to be a probability dis-
tribution, and f(y
k?1
i
, y
k
i
, x
i
) are arbitrary feature
functions.
In our setting, the CRFs model is required
to learn from unlabeled data. This work em-
ploys the posterior regularization (PR) frame-
work
3
(Ganchev et al, 2010) to bias the CRFs
model?s learning on unlabeled data, under a con-
straint encoded by the graph propagation expres-
sion. It is expected that similar types in the graph
should have approximated expected taggings un-
der the CRFs model. We follow the approach in-
troduced by (He et al, 2013) to set up a penalty-
based PR objective with GP: the CRFs likelihood
is modified by adding a regularization term, as
shown in Equation 4, representing the constraints:
R
U
(?, q) = KL(q||p
?
) + ?P(v) (4)
Rather than regularize CRFs model?s posteriors
p
?
(Y|x
i
) directly, our model uses an auxiliary
distribution q(Y|x
i
) over the possible labelings
3
The readers are refered to the original paper of Ganchev
et al (2010).
Y for x
i
, and penalizes the CRFs marginal log-
likelihood by a KL-divergence term
4
, represent-
ing the distance between the estimated posteriors
p and the desired posteriors q, as well as a penal-
ty term, formed by the GP function. The hy-
perparameter ? is used to control the impacts of
the penalty term. Note that the penalty is fired
if the graph score computed based on the expect-
ed taggings given by the current CRFs model is
increased vis-a-vis the previous training iteration.
This nature requires that the penalty term P(v)
should be formed as a function of posteriors q over
CRFs model predictions
5
, i.e., P(q). To state this,
a mappingM : ({1, ..., u}, {1, ...,m})? V from
words in the corpus to vertices in the graph is de-
fined. We can thus decompose v
i,t
into a function
of q as follows:
v
i,t
=
u?
a=1
m?
b=1;
M(a,b)=V
i
T?
c=1
?
y?Y
1(y
b
= t, y
b?1
= c)q(y|x
a
)
u?
a=1
m?
b=1
1(M(a, b) = V
i
)
(5)
The final learning objective combines the CRF-
s likelihood with the PR regularization term:
J (?, q) = L(?) + R
U
(?, q). This joint objec-
tive, over ? and q, can be optimized by an expecta-
tion maximization (EM) style algorithm as report-
ed in (Ganchev et al, 2010). We start from ini-
tial parameters ?
0
, estimated by supervised CRFs
model training on treebank data. The E-step is to
minimize R
U
(?, q) over the posteriors q that are
constrained to the probability simplex. Since the
penalty term P(v) is a non-linear form, the opti-
mization method in (Ganchev et al, 2010) via pro-
jected gradient descent on the dual is inefficient
6
.
This study follows the optimization method (He et
al., 2013) that uses exponentiated gradient descent
(EGD) algorithm. It allows that the variable up-
date expression, as shown in Equation 6, takes a
multiplicative rather than an additive form.
q
(w+1)
(y|x
i
) = q
(w)
(y|x
i
) exp(??
?R
?q
(w)
(y|x
i
)
)
(6)
where the parameter ? controls the optimization
rate in the E-step. With the contributions from
4
The form of KL term: KL(q||p) =
?
q?Y
q(y) log
q(y)
p(y)
.
5
The original PR setting also requires that the penalty ter-
m should be a linear (Ganchev et al, 2010) or non-linear (He
et al, 2013) function on q.
6
According to (He et al, 2013), the dual of quadratic pro-
gram implies an expensive matrix inverse.
1364
the E-step that further encourage q and p to agree,
the M-step aims to optimize the objective J (?, q)
with respect to ?. The M-step is similar to the stan-
dard CRFs parameter estimation, where the gradi-
ent ascent approach still works. This EM-style ap-
proach monotonically increases J (?, q) and thus
is guaranteed to converge to a local optimum.
E-step: q
(t+1)
= argmin
q
R
U
(?
(t)
, q
(t)
)
M-step: ?
(t+1)
= argmax
?
L(?)
+?
u
?
i=1
?
y?Y
q
(t+1)
(y|x
i
) log p
?
(y|x
i
)
(7)
4 Experiments
4.1 Data and Setup
The experiments in this study evaluated the per-
formances of various CWS models in a Chinese-
to-English translation task. The influence of
the word segmentation on the final translation
is our main investigation. We adopted three
state-of-the-art metrics, BLEU (Papineni et al,
2002), NIST (Doddington et al, 2000) and ME-
TEOR (Banerjee and Lavie, 2005), to evaluate the
translation quality.
The monolingual segmented data, train
TB
, is
extracted from the Penn Chinese Treebank (CTB-
7) (Xue et al, 2005), containing 51,447 sentences.
The bilingual training data, train
MT
, is formed
by a large in-house Chinese-English parallel cor-
pus (Tian et al, 2014). There are in total 2,244,319
Chinese-English sentence pairs crawled from on-
line resources, concentrated in 5 different domains
including laws, novels, spoken, news and miscel-
laneous
7
. This in-house bilingual corpus is the
MT training data as well. The target-side lan-
guage model is built on over 35 million mono-
lingual English sentences, train
LM
, crawled from
online resources. The NIST evaluation campaign
data, MT-03 and MT-05, are selected to comprise
the MT development data, dev
MT
, and testing da-
ta, test
MT
, respectively.
For the settings of our model, we adopted the
standard feature templates introduced by Zhao et
al. (2006) for CRFs. The character-based align-
ment for achieving the ?chars-to-word? mappings
is accomplished by GIZA++ aligner (Och and
Ney, 2003). For the GP, a 10-NNs similarity graph
7
The in-house corpus has been manually validated, in a
long process that exceeded 500 hours.
was constructed
8
. Following (Subramanya et al,
2010; Zeng et al, 2013), the features used to
compute similarities between vertices were (Sup-
pose given a type ?w
2
w
3
w
4
? surrounding contexts
?w
1
w
2
w
3
w
4
w
5
?): unigram (w
3
), bigram (w
1
w
2
,
w
4
w
5
, w
2
w
4
), trigram (w
2
w
3
w
4
, w
2
w
4
w
5
,
w
1
w
2
w
4
), trigram+context (w
1
w
2
w
3
w
4
w
5
) and
character classes in number, punctuation, alpha-
betic letter and other (t(w
2
)t(w
3
)t(w
4
)). There
are four hyperparameters in our model to be tuned
by using the development data (dev
MT
) among
the following settings: for the graph propagation,
? ? {0.2, 0.5, 0.8} and ? ? {0.1, 0.3, 0.5, 0.8};
for the PR learning, ? ? {0 ? ?
i
? 1} and ? ?
{0 ? ?
i
? 1} where the step is 0.1. The best per-
formed joint settings, ? = 0.5, ? = 0.5, ? = 0.9
and ? = 0.8, were used to measure the final per-
formance.
The MT experiment was conducted based on
a standard log-linear phrase-based SMT model.
The GIZA++ aligner was also adopted to obtain
word alignments (Och and Ney, 2003) over the
segmented bitexts. The heuristic strategy of grow-
diag-final-and (Koehn et al, 2007) was used to
combine the bidirectional alignments for extract-
ing phrase translations and reordering tables. A
5-gram language model with Kneser-Ney smooth-
ing was trained with SRILM (Stolcke, 2002) on
monolingual English data. Moses (Koehn et al,
2007) was used as decoder. The Minimum Error
Rate Training (MERT) (Och, 2003) was used to
tune the feature parameters on development data.
4.2 Various Segmentation Models
To provide a thorough analysis, the MT experi-
ments in this study evaluated three baseline seg-
mentation models and two off-the-shelf models,
in addition to four variant models that also employ
the bilingual constraints. We start from three base-
line models:
? Character Segmenter (CS): this model sim-
ply divides Chinese sentences into sequences
of characters.
? Supervised Monolingual Segmenter (SM-
S): this model is trained by CRFs on treebank
training data (train
TB
). The same feature
templates (Zhao et al, 2006) are used. The
standard four-tags (B, M, E and S) were used
8
We evaluated graphs with top k (from 3 to 20) nearest
neighbors on development data, and found that the perfor-
mance converged beyond 10-NNs.
1365
as the labels. The stochastic gradient descent
is adopted to optimize the parameters.
? Unsupervised Bilingual Segmenter (UBS):
this model is trained on the bitexts (trainMT)
following the approach introduced in (Ma
and Way, 2009). The optimal set of the mod-
el parameter values was found on dev
MT
to
be k = 3, t
AC
= 0.0 and t
COOC
= 15.
The comparison candidates also involve two pop-
ular off-the-shelf segmentation models:
? Stanford Segmenter: this model, trained by
Chang et al (2008), treats CWS as a binary
word boundary decision task. It covers sev-
eral features specific to the MT task, e.g., ex-
ternal lexicons and proper noun features.
? ICTCLAS Segmenter: this model, trained
by Zhang et al (2003), is a hierarchical
HMM segmenter that incorporates parts-of-
speech (POS) information into the probabili-
ty models and generates multiple HMM mod-
els for solving segmentation ambiguities.
This work also evaluated four variant models
9
that perform alternative ways to incorporate the
bilingual constraints based on two state-of-the-art
graph-based SSL approaches.
? Self-training Segmenters (STS): two vari-
ant models were defined by the approach re-
ported in (Subramanya et al, 2010) that us-
es the supervised CRFs model?s decodings,
incorporating empirical and constraint infor-
mation, for unlabeled examples as additional
labeled data to retrain a CRFs model. One
variant (STS-NO-GP) skips the GP step, di-
rectly decoding with type-level word bound-
ary probabilities induced from bitexts, while
the other (STS-GP-PL) runs the GP at first
and then decodes with GP outcomes. The
optimal hyperparameter values were found to
be: STS-NO-GP (? = 0.8) and ? = 0.6) and
STS-GP-PL (? = 0.5, ? = 0.3, ? = 0.8 and
? = 0.6).
? Virtual Evidences Segmenters (VES): T-
wo variant models based on the approach
in (Zeng et al, 2013) were defined. The type-
level word boundary distributions, induced
9
Note that there are two variant models working with GP.
To be fair, the same similarity graph settings introduced in
this paper were used.
by the character-based alignment (VES-NO-
GP), and the graph propagation (VES-GP-
PL), are regarded as virtual evidences to bias
CRFs model?s learning on the unlabeled da-
ta. The optimal hyperparameter values were
found to be: VES-NO-GP (? = 0.7) and
VES-GP-PL (? = 0.5, ? = 0.3 and ? = 0.7).
4.3 Main Results
Table 1 summarizes the final MT performance on
the MT-05 test data, evaluated with ten different
CWS models. In what follows, we summarized
four major observations from the results. First-
ly, as expected, having word segmentation does
help Chinese-to-English MT. All other nine CWS
models outperforms the CS baseline which does
not try to identify Chinese words at all. Second-
ly, the other two baselines, SMS and UBS, are on
a par with each other, showing less than 0.36 av-
erage performance differences on the three eval-
uation metrics. This outcome validated that the
models, trained by either the treebank or the bilin-
gual data, performed reasonably well. But they
only capture partial segmentation features so that
less gains for SMT are achieved when compar-
ing to other sophisticated models. Thirdly, we no-
tice that the two off-the-shelf models, Stanford and
ICTCLAS, just brought minor improvements over
the SMS baseline, although they are trained us-
ing richer supervisions. This behaviour illustrates
that the conventional optimizations to the mono-
lingual supervised model, e.g., accumulating more
supervised data or predefined segmentation prop-
erties, are insufficient to help model for achiev-
ing better segmentations for SMT. Finally, high-
lighting the five models working with the bilingual
constraints, most of them can achieve significant
gains over the other ones without using the bilin-
gual constraints. This strongly demonstrates that
bilingually-learned segmentation knowledge does
helps CWS for SMT. The models working with G-
P, STS-GP-PL, VES-GP-PL and ours outperform
all others. We attribute this to the role of GP in
assisting the spread of bilingual knowledge on the
Chinese side. Importantly, it can be observed that
our model outperforms STS-GP, VES-GP, which
greatly supports that joint learning of CRFs and
GP can alleviate the error transfer by the pipelined
models. This is one of the most crucial findings
in this study. Overall, the boldface numbers in the
last row illustrate that our model obtains average
improvements of 1.89, 1.76 and 1.61 on BLEU,
1366
NIST and METEOR over others.
Models BLEU NIST METEOR
CS 29.38 59.85 54.07
SMS 30.05 61.33 55.95
UBS 30.15 61.56 55.39
Stanford 30.40 61.94 56.01
ICTCLAS 30.29 61.26 55.72
STS-NO-GP 31.47 62.35 56.12
STS-GP-PL 31.94 63.20 57.09
VES-NO-GP 31.98 62.63 56.59
VES-GP-PL 32.04 63.49 57.34
Our Model 32.75 63.72 57.64
Table 1: Translation performances (%) on MT-05
testing data by using ten different CWS models.
4.4 Analysis & Discussion
This section aims to further analyze the three pri-
mary observations concluded in Section 4.3: i)
word segmentation is useful to SMT; ii) the tree-
bank and the bilingual segmentation knowledge
are helpful, performing segmentation of differen-
t nature; and iii) the bilingual constraints lead to
learn segmentations better tailored for SMT.
The first observation derives from the compar-
isons between the CS baseline and other model-
s. Our results, showing the significant CWS ben-
efits to SMT, are consistent with the works re-
ported in the literature (Xu et al, 2004; Chang
et al, 2008). In our experiment, two additional
evidences found in the translation model are pro-
vided to further support that NO tokenization of
Chinese (i.e., the CS model?s output) could har-
m the MT system. First, the SMT phrase extrac-
tion, i.e., building ?phrases? on top of the char-
acter sequences, cannot fully capture all meaning-
ful segmentations produced by the CS model. The
character based model leads to missing some use-
ful longer phrases, and to generate many meaning-
less or redundant translations in the phrase table.
Moreover, it is affected by translation ambiguities,
caused by the cases where a Chinese character has
very different meanings in different contextual en-
vironments.
The second observation shifts the emphasis to
SMS and UBS, based on the treebank and the
bilingual segmentation, respectively. Our result-
s show that both segmentation patterns can bring
positive effects to MT. Through analyzing both
models? segmentations for train
MT
and test
MT
,
we attempted to get a closer inspection on the seg-
mentation preferences and their influence on MT.
Our first finding is that the segmentation consen-
suses between SMS and UBS are positive to MT.
There have about 35% identical segmentations
produced by the two models. If these identical
segmentations are removed, and the experiments
are rerun, the translation scores decrease (on av-
erage) by 0.50, 0.85 and 0.70 on BLEU, NIST
and METEOR, respectively. Our second finding
is that SMS exhibits better segmentation consis-
tency than UBS. One representative example is the
segmentations for ???? (lonely)?. All the out-
puts of SMS were ?????, while UBS generat-
ed three ambiguous segmentations, ??(alone) ?
?(double zero)?, ???(lonely) ?(zero)? and
??(alone) ?(zero) ?(zero)?. The segmentation
consistency of SMS rests on the high-quality tree-
bank data and the robust CRFs tagging mod-
el. On the other hand, the advantage of UB-
S is to capture the segmentations matching the
aligned target words. For example, UBS grouped
??(country) ?(border) ?(between)? to a word
????(international)?, rather than two word-
s ???(international) ?(between)? (as given by
SMS), since these three characters are aligned to
a single English word ?international?. The above
analysis shows that SMS and UBS have their own
merits and combining the knowledge derived from
both segmentations is highly encouraged.
The third observation concerns the great im-
pact of the bilingual constraints to the segmenta-
tion models in the MT task. The use of the bilin-
gual constraints is the prime objective of this s-
tudy. Our first contribution for this purpose is
on using the word boundary distributions to cap-
ture the bilingual segmentation supervisions. This
representation contributes to reduce the negative
impacts of erroneous ?chars-to-word? alignments.
The ambiguous types (having relatively uniform
boundary distribution), caused by alignment er-
rors, cannot directly bias the model tagging pref-
erences. Furthermore, the word boundary distri-
butions are convenient to make up the learning
constraints over the labelings among various con-
strained learning approaches. They have success-
fully played in three types of constraints for our
experiments: PR penalty (Our model), decoding
constraints in self-training (STS) and virtual evi-
dences (VES). The second contribution is the use
of GP, illustrated by STS-GP-PL, VES-GP-PL and
1367
Our model. The major effect is to multiply the im-
pacts of the bilingual knowledge through the sim-
ilarity graph. The graph vertices (types)
10
, with-
out any supervisions, can learn the word bound-
ary information from their similar types (neigh-
borhoods) having the empirical boundary prob-
abilities. The segmentations given by the three
GP models show about 70% positive segmenta-
tion changes, affected by the unlabeled graph ver-
tices, with respect to the ones given by the NO-
GP models, STS-NO-GP and VES-NO-GP. In our
opinion, the learning mechanism of our approach,
joint coupling of GP and CRFs, rather than the
pipelined one as the other two models, contributes
to maximizing the graph smoothness effects to the
CRFs estimation so that the error propagation of
the pipelined approaches is alleviated.
5 Conclusion
This paper proposed a novel CWS model for the
SMT task. This model aims to maintain the lin-
guistic segmentation supervisions from treebank
data and simultaneously integrate useful bilingual
segmentations induced from the bitexts. This ob-
jective is accomplished by three main steps: 1)
learn word boundaries from character-based align-
ments; 2) encode the learned word boundaries into
a GP constraint; and 3) training a CRFs model, un-
der the GP constraint, by using the PR framework.
The empirical results indicate that the proposed
model can yield better segmentations for SMT.
Acknowledgments
The authors are grateful to the Science and
Technology Development Fund of Macau and
the Research Committee of the University of
Macau (Grant No. MYRG076 (Y1-L2)-FST13-
WF and MYRG070 (Y1-L2)-FST12-CS) for the
funding support for our research. The work of
Isabel Trancoso was supported by national funds
through FCT-Fundac??ao para a Ci?ecia e a Tecnolo-
gia, under project PEst-OE/EEI/LA0021/2013.
The authors also wish to thank the anonymous re-
viewers for many helpful comments.
10
This experiment yielded a similarity graph that consists
of 11,909,620 types from train
TB
and train
MT
, where there
have 8,593,220 (72.15%) types without any empirical bound-
ary distributions.
References
Yasemin Altun, David McAllester, and Mikhail Belkin.
2006. Maximum margin semi-supervised learning
for structured variables. Advances in Neural Infor-
mation Processing Systems, 18:33.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings
of the ACL Workshop on Intrinsic and Extrinsic E-
valuation Measures for Machine Translation and/or
Summarization, pages 65?72. Association for Com-
putational Linguistics.
Yoshua Bengio, Olivier Delalleau, and Nicolas
Le Roux. 2006. Label propagation and quadrat-
ic criterion. Semi-Supervised Learning, pages 193?
216.
Pi-Chuan Chang, Michel Galley, and Christopher D
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of WMT, pages 224?232. Association for
Computational Linguistics.
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Pro-
ceedings of EMNLP, pages 718?726. Association
for Computational Linguistics.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL, pages 600?609.
Association for Computational Linguistics.
Dipanjan Das and Noah A Smith. 2012. Graph-based
lexicon expansion with sparsity-inducing penalties.
In Proceedings of NAACL, pages 677?687. Associa-
tion for Computational Linguistics.
George R. Doddington, Mark A. Przybocki, Alvin F.
Martin, and Douglas A. Reynolds. 2000. The nist
speaker recognition evaluation?overview, methodol-
ogy, systems, results, perspective. Speech Commu-
nication, 31(2):225?254.
Kuzman Ganchev, J?oao Grac?a, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. The Journal of
Machine Learning Research, 11:2001?2049.
Luheng He, Jennifer Gillenwater, and Ben Taskar.
2013. Graph-based posterior regularization for
semi-supervised structured prediction. In Proceed-
ings of CoNLL, page 38. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of ACL on Interactive Poster and Demon-
stration Sessions, pages 177?180. Association for
Computational Linguistics.
1368
Yanjun Ma and Andy Way. 2009. Bilingually motivat-
ed domain-adapted word segmentation for statistical
machine translation. In Proceedings of EACL, pages
549?557. Association for Computational Linguistic-
s.
Gideon S. Mann and Andrew McCallum. 2008.
Generalized expectation criteria for semi-supervised
learning of conditional random fields. In Proceed-
ings of ACL, pages 870?878. Association for Com-
putational Linguistics.
Andrew McCallum, Gideon Mann, and Gregory
Druck. 2007. Generalized expectation criteri-
a. Computer Science Technical Note, University of
Massachusetts, Amherst, MA.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of A-
CL, pages 160?167. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic e-
valuation of machine translation. In Proceedings of
ACL, pages 311?318. Association for Computation-
al Linguistics.
Michael Paul, Finch Andrew, and Sumita Eiichiro.
2011. Integration of multiple bilingually-trained
segmentation schemes into statistical machine trans-
lation. IEICE Transactions on Information and Sys-
tems, 94(3):690?697.
Andreas Stolcke. 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of Inter-
speech.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-
supervised learning of structured tagging models. In
Proceedings of EMNLP, pages 167?176. Associa-
tion for Computational Linguistics.
Liang Tian, Derek F. Wong, Lidia S. Chao, Paulo
Quaresma, Francisco Oliveira, Shuo Li, Yiming
Wang, and Yi Lu. 2014. UM-Corpus: A large
English-Chinese parallel corpus for statistical ma-
chine translation. In Proceedings of LREC. Euro-
pean Language Resources Association.
Ning Xi, Guangchao Tang, Xinyu Dai, Shujian Huang,
and Jiajun Chen. 2012. Enhancing statistical ma-
chine translation with character alignment. In Pro-
ceedings of ACL, pages 285?290. Association for
Computational Linguistics.
Jia Xu, Richard Zens, and Hermann Ney. 2004. Do
we need Chinese word segmentation for statistical
machine translation? In Proceedings of the Third
SIGHAN Workshop on Chinese Language Learning,
pages 122?128. Association for Computational Lin-
guistics.
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann
Ney. 2005. Integrated Chinese word segmentation
in statistical machine translation. In Proceedings of
IWSLT, pages 216?223. Association for Computa-
tional Linguistics.
Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and Is-
abel Trancoso. 2013. Graph-based semi-supervised
model for joint Chinese word segmentation and part-
of-speech tagging. In Proceedings of ACL, pages
770?779. Association for Computational Linguistic-
s.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, Is-
abel Trancoso, Liangye He, and Qiuping Huang.
2014. Lexicon expansion for latent variable gram-
mars. Pattern Recognition Letters, 42:47?55.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. HHMM-based Chinese lexical analyzer
ICTCLAS. In Proceedings of the Second SIGHAN
Workshop on Chinese Language Processing, pages
184?187. Association for Computational Linguistic-
s.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008. Improved statistical machine translation by
multiple Chinese word segmentation. In Proceed-
ings of WMT, pages 216?223. Association for Com-
putational Linguistics.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
improved Chinese word segmentation system with
conditional random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing. Association for Computational Linguistics.
Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-
Liang Lu. 2013. An empirical study on word seg-
mentation for Chinese machine translation. In Com-
putational Linguistics and Intelligent Text Process-
ing, pages 248?263. Springer.
Xiaojin Zhu, Zoubin Ghahramani, and John Laffer-
ty. 2003. Semi-supervised learning using gaussian
fields and harmonic functions. In Proceedings of
ICML, volume 3, pages 912?919.
Ling Zhu, Derek F. Wong, and Lidia S. Chao. 2014.
Unsupervised chunking based on graph propagation
from bilingual corpus. The Scientific World Journal,
2014(401943):10.
1369
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 1?10,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
An on-line system for remote treatment of aphasia
Anna Pompili, Alberto Abad,
Isabel Trancoso
L2F - Spoken Language Systems Lab
INESC-ID/IST, Lisbon, Portugal
{anna,alberto,imt}@l2f.inesc-id.pt
Jose? Fonseca, Isabel P. Martins,
Gabriela Leal, Luisa Farrajota
LEL - Language Research Laboratory
Lisbon Faculty of Medicine, Portugal
jfonseca@fm.ul.pt
Abstract
Aphasia treatment for the recovery of lost
communication functionalities is possible
through frequent and intense speech therapy
sessions. In this sense, speech and language
technology may provide important support in
improving the recovery process. The aim of
the project Vithea (Virtual Therapist for Apha-
sia Treatment) is to develop an on-line sys-
tem designed to behave as a virtual thera-
pist, guiding the patient in performing train-
ing exercises in a simple and intuitive fashion.
In this paper, the fundamental components of
the Vithea system are presented, with particu-
lar emphasis on the speech recognition mod-
ule. Furthermore, we report encouraging au-
tomatic word naming recognition results using
data collected from speech therapy sessions.
1 Introduction
Aphasia is a communication disorder that can af-
fect various aspects of language, including hearing
comprehension, speech production, and reading and
writing fluency. It is caused by damage to one or
more of the language areas of the brain. Many times
the cause of the brain injury is a cerebral vascular
accident (CVA), but other causes can be brain tu-
mors, brain infections and severe head injury due
to an accident. Unfortunately, in the last decades
the number of individuals that suffer CVAs has dra-
matically increased, with an estimated 600.000 new
cases each year in the EU. Typically, a third of
these cases present language deficiencies (Pedersen
et al, 1995). This kind of language disorder in-
volves countless professional, family and economic
problems, both from the point of view of the individ-
ual and the society. In this context, two remarkable
considerations have led to the development of the
Portuguese national project Vithea (Virtual Thera-
pist for Aphasia treatment).
First are the enormous benefits that speech and
language technology (SLT) may bring to the daily
lives of people with physical impairment. Informa-
tion access and environment control are two areas
where SLT has been beneficially applied, but SLT
also has great potential for diagnosis, assessment
and treatment of several speech disorders (Hawley
et al, 2005). For instance, a method for speech in-
telligibility assessment using both automatic speech
recognition and prosodic analysis is proposed in
(Maier et al, 2009). This method is applied to the
study of patients that have suffered a laryngotomy
and to children with cleft lip and palate. (Castillo-
Guerra and Lovey, 2003) presents a method for
dysarthria assessment using features extracted from
pathological speech signals. In (Yin et al, 2009), the
authors describe an approach to pronunciation veri-
fication for a speech therapy application.
The second reason for undertaking the Vithea
project is that several aphasia studies have demon-
strated the positive effect of speech therapy activi-
ties for the improvement of social communication
abilities. These have focused on specific linguistic
impairments at the phonemic, semantic or syntac-
tic levels (Basso, 1992). In fact, it is believed more
and more that the intensity of speech therapy pos-
itively influences language recovery in aphasic pa-
tients (Bhogal et al, 2003).
These compelling reasons have motivated the de-
1
velopment of an on-line system for the treatment
of aphasic patients incorporating recent advances in
speech and language technology in Portuguese. The
system will act as a ?virtual therapist?, simulating an
ordinary speech therapy session, where by means of
the use of automatic speech recognition (ASR) tech-
nology, the virtual therapist will be able to recognize
what was said by the patient and to validate if it was
correct or not. As a result of this novel and special-
ized stimulation method for the treatment of aphasia,
patients will have access to word naming exercises
from their homes at any time, which will certainly
cause an increase in the number of training hours,
and consequently it has the potential to bring signif-
icant improvements to the rehabilitation process.
In section 2 we provide a brief description of dif-
ferent aphasia syndromes, provide an overview of
the most commonly adopted therapies for aphasia,
and describe the therapeutic focus of our system.
Section 3 is devoted to an in depth description of
the functionalities that make up the system, while
section 4 aims at detailing its architecture. Finally,
section 5 describes the automatic speech recognition
module and discusses the results achieved within the
automatic naming recognition task.
2 About the aphasia disorder
2.1 Classification of aphasia
It is possible to distinguish two different types of
aphasia on the basis of the fluency of the speech pro-
duced: fluent and non-fluent aphasia. The speech
of someone with fluent aphasia has normal articula-
tion and rhythm, but is deficient in meaning. Typi-
cally, there are word-finding problems that most af-
fect nouns and picturable action words. Non-fluent
aphasic speech is slow and labored, with short ut-
terance length. The flow of speech is more or less
impaired at the levels of speech initiation, the find-
ing and sequencing of articulatory movements, and
the production of grammatical sequences. Speech is
choppy, interrupted, and awkwardly articulated.
Difficulty of recalling words or names is the most
common language disorder presented by aphasic in-
dividuals (whether fluent or non-fluent). In fact, it
can be the only residual defect after rehabilitation of
aphasia (Wilshire and Coslett, 2000).
2.2 Common therapeutic approaches
There are several therapeutic approaches for the
treatment of the various syndromes of aphasia. Of-
ten these methods are focused on treating a specific
disorder caused from aphasia. The most commonly
used techniques are output focused, such as the stim-
ulation method and the Melodical Intonation Ther-
apy (MIT) (Albert et al, 1994). Other methods are
linguistic-oriented learning approaches, such as the
lexical-semantic therapy or the mapping technique
for the treatment of agrammatism. Still, several
non-verbal methods for the treatment of some se-
vere cases of non-fluent aphasia, such as the visual
analog communication, iconic communication, vi-
sual action and drawing therapies, are currently used
(Sarno, 1981; Albert, 1998).
Although there is an extensive list of treatments
specifically designed to recover from particular dis-
orders caused by aphasia, one class of rehabilita-
tion therapy especially important aims to improve
the recovery from word retrieval problems, given the
widespread difficulty of recalling words or names.
Naming ability problems are typically treated with
semantic exercises like Naming Objects or Naming
common actions (Adlam et al, 2006). The approach
typically followed is to subject the patient to a set of
exercises comprising a set of stimuli in a variety of
tasks. The stimuli are chosen based on their seman-
tic content. The patient is asked to name the subject
that has been shown.
2.3 Therapeutic focus of the Vithea system
The focus of the Vithea system is on the recovery
of word naming ability for aphasic patients. So far,
experiments have only been made with fluent apha-
sia patients, but even for this type of aphasia, major
differences may be found. Particularly, patients with
Transcortical sensorial aphasia, Conduction aphasia
and Anomic aphasia (Goodglass, 1993) have been
included in our studies.
Although the system has been specifically de-
signed for aphasia treatment, it may be easily
adapted to the treatment or diagnosis of other dis-
orders in speech production. In fact, two of the
patients that have participated in our experimen-
tal study were diagnosed with acquired apraxia of
speech (AOS), which typically results from a stroke,
2
Figure 1: Comprehensive overview of the Vithea system.
tumor, or other known neurological illness or injury,
and is characterized by inconsistent articulatory er-
rors, groping oral movements to locate the correct
articulatory position, and increasing errors with in-
creasing word and phrase length.
3 The Vithea System
The overall flow of the system can be described as
follows: when a therapy session starts, the virtual
therapist will show to the patient, one at a time, a
series of visual or auditory stimuli. The patient is
required to respond verbally to these stimuli by nam-
ing the contents of the object or action that is repre-
sented. The utterance produced is recorded, encoded
and sent via network to the server side. Here, a web
application server receives the audio file via a servlet
that serves as an interface to the ASR system, which
takes as input the audio file encoding the patient?s
answer and generates a textual representation of it.
This result is then compared with a set of predeter-
mined textual answers (for that given question, of
course) in order to verify the correctness of the pa-
tient?s input. Finally, feedback is sent back to the
patient. Figure 1 shows a comprehensive view of
this process.
The system comprises two specific modules, dedi-
cated respectively to the patients for carrying out the
therapy sessions and to the clinicians for the admin-
istration of the functionalities related to them. The
two modules adhere to different requirements that
have been defined for the particular class of user for
which they have been developed. Nonetheless they
share the set of training exercises, that are built by
the clinicians and performed by the patients.
3.1 Speech therapy exercises
Following the common therapeutic approach for
treatment of word finding difficulties, a training ex-
ercise is composed of several semantic stimuli items.
The stimuli may be of several different types: text,
audio, image and video. Like in ordinary speech
therapy sessions, the patient is asked to respond to
the stimuli verbally, describing the imaging he/she
sees or completing a popular saying (which was pre-
sented verbally or in text).
Exercise categories
The set of therapeutic exercises integrated in
Vithea has been designed by the Language Research
Laboratory of the Department of Clinical Neuro-
science of the Lisbon Faculty of Medicine (LEL).
LEL has provided a rich battery of exercises that can
be classified into two macro-categories according to
the main modality of the stimulus, namely:
A) Image or video: Naming object picture, Naming
of verbs with action pictures, and Naming verbs
given pictures of objects.
B) Text or speech: Responsive Naming, Complete
Sayings, Part-whole Associations, What name
is given to. . . , Generic Designation, Naming by
function, Phonological Evocation, and Seman-
tics Evocation.
Exercises can be also classified according to
Themes, in order to immerse the individual in a prag-
matic, familiar environment: a) Home b) Animals
c) Tools d) Food e) Furniture f) Professions g) Ap-
pliances h) Transportation i) Alive/Not Alive j) Ma-
nipulable/Not Manipulable k) Clothing l) Random.
Evaluation exercises
In addition to the set of training exercises, which
are meant to be used on a daily basis by the apha-
sic patient, the Vithea system also supports a dif-
ferent class of exercises: Evaluation Exercises. Un-
like training exercises, evaluation exercises are used
by human therapists to periodically assess the pa-
tient?s progress and his/her current degree of apha-
sia via an objective metric denoted as Aphasia Quo-
tient (AQ). Evaluation exercises are chosen from a
3
subset of the previously mentioned classes of ther-
apeutic exercises, namely: Naming object picture,
Naming of verbs with action pictures, and Naming
verbs given pictures of objects.
3.2 Patient Module
The patient module is meant to be used by aphasic
individuals to perform the therapeutic exercises.
Visual design considerations
Most of the users for whom this module is in-
tended have had a CVA. Because of this, they may
have some forms of physical disabilities such as re-
duced arm mobility, and therefore they may experi-
ence problems using a mouse. Acknowledging this
eventuality, particular attention has been given to the
design of the graphical user interface (GUI) for this
module, making it simple to use both at the presen-
tation level and in terms of functionality provided.
Driven by the principle of accessibility, we designed
the layout in an easy to use and understand fashion,
such that the interaction should be predictable and
unmistakable.
Moreover, even though aphasia is increasing in
the youngest age groups, it still remains a predomi-
nant disorder among elderly people. This age group
is prone to suffer from visual impairments. Thus,
we carefully considered the graphic elements cho-
sen, using big icons for representing our interface
elements. Figure 2 illustrates some screenshots of
the Patient Module on the top.
Exercise protocol
Once logged into the system, the virtual therapist
guides the patient in carrying out the training ses-
sions, providing a list of possible exercises to be per-
formed. When the patient choses to start a training
exercise, the system will present target stimuli one
at a time in a random way. After the evaluation of
the patient?s answer by the system, the patient can
listen again to his/her previous answer, record again
an utterance (up to a number of times chosen before
starting the exercise) or pass to the next exercise.
Patient tracking
Besides permitting training sessions, the patient
module has the responsibility of storing statistical
and historical data related to user sessions. User ut-
terances and information about each user access to
the system are stored in a relational database. Par-
ticularly, start and end time of the whole training
session, of a training exercise, and of each stimulus
are collected. On the one hand, we log every access
in order to evaluate the impact and effectiveness of
the program by seeing the frequency with which it
is used. On the other hand, we record the total time
needed to accomplish a single stimulus or to end a
whole exercise in order to estimate user performance
improvements.
3.3 Clinician Module
The clinician module is specifically designed to al-
low clinicians to manage patient data, to regulate
the creation of new stimuli and the alteration of the
existing ones, and to monitor user performance in
terms of frequency of access to the system and user
progress. The module is composed by three sub-
modules: User, Exercise, Statistic.
User sub-module
This module allows the management of a knowl-
edge base of patients. Besides basic information
related to the user personal profile, the database
also stores for each individual his/her type of apha-
sia, his/her aphasia severity (7-level subjective scale)
and AQ information.
Exercise sub-module
This module allows the clinician to create, update,
preview and delete stimuli from an exercise. An ex-
ercise is composed of a varying number of stimuli.
In addition to the canonical valid answer, the system
accepts for each stimulus an extended word list com-
prising three extra valid answers. This list allows the
system to consider the most frequent synonyms and
diminutives.
Since the stimuli are associated with a wide as-
sortment of multimedia files, besides the manage-
ment of the set of stimuli, the sub-module also pro-
vides a rich Web based interface to manage the
database of multimedia resources used within the
stimuli. Figure 2c shows a screenshot listing some
multimedia files. From this list, it is possible to se-
lect a desired file in order to edit or delete it.
In this context, a preview feature has also been
provided. The system is capable of handling a wide
range of multimedia encoding: audio (accepted file
4
Figure 2: Vithea system screenshots: a) Interface with preview of the stimuli constituting an exercise in the patients
module (top-left), b) interface for performing a specific stimulus in the patients module (top-right), c) interface for
the management of multimedia resources in the clinician module (bottom-left) and d) interface for the creation of new
stimulus in the clinician module (bottom-right).
types: wav, mp3), video (accepted file types: wmv,
avi, mov, mp4, mpe, mpeg, mpg, swf), and images
(accepted file types: jpe, jpeg, jpg, png, gif, bmp, tif,
tiff).
Given the diversity of the various file types ac-
cepted by the system, a conversion to a unique file
type was needed, in order to show them all with only
one external tool. Audio files are therefore converted
to mp3 file format, while video files are converted to
flv file format.
Finally, a custom functionality has been designed
to create new stimuli in an intuitive fashion similar
in style to a WYSIWYG editor. Figure 2d illustrates
the stimuli editor, showing how to insert a multime-
dia resource.
Statistics sub-module
This module allows the clinician both to monitor
statistical information related to user-system inter-
actions and to access the utterances produced by the
patient during the therapeutic sessions. The statisti-
cal information comprises data related to the user?s
progress and to the frequency with which users ac-
cess the system. On the one hand, we provide all
the attempts recorded by the patients in order to
allow a re-evaluation by clinicians. This data can
be used to identify possible weaknesses or errors
from the recognition engine. On the other hand, we
thought that monitoring the utilization of the appli-
cation from the users could be an important piece of
feedback about the system?s feasibility. This is moti-
vated by common concerns about the fact that some
users abandon their therapeutic sessions when they
are not able to see quick results in terms of improve-
ments.
4 Architectural Overview
Considering the aforementioned requirements and
features that will make up the system, Learning
Management Systems (LMSs) software applications
were initially considered. LMSs automate the ad-
5
ministration of training events, manage the log-in of
registered users, manage course catalog, record data
from learners and provide reports to the manage-
ment (Aydin and Tirkes, 2010). Thus, an in-depth
evaluation of the currently widespread solutions was
carried out (Pompili, 2011). Concretely, eight differ-
ent LMSs (Atutor, Chamilo, Claroline, eFront, Ilias,
Moodle, Olat, Sakai) were studied in detail. Unfor-
tunately, the outcome of this study revealed impor-
tant drawbacks.
The main problem noticed is that LMSs are typi-
cally feature-rich tools that try to be of general pur-
pose use, sometimes resulting in the loss of their
usefulness to the average user. Often the initial user
reaction to the interface of these tools is confusion:
the most disorienting challenge is figuring out where
to get the information needed. As previously men-
tioned, patients who have had a CVA may experi-
ence physical deficiencies, thus the Vithea system
needs an easy to use and understandable interface.
We dedicated some effort trying to personalize LMS
solutions, but most of them do not allow easy sim-
plification of the presentation layout.
Moreover, while there were several differences
between the functionalities that the evaluated LMSs
provided in terms of training exercises, they all pre-
sented various limitations in their implementation.
Eventually, we had to acknowledge that it would
have been extremely complex to customize the eval-
uated frameworks to meet the Vithea project require-
ments without introducing major structural changes
to the code.
Besides, the average user for whom the Vithea
system is intended is not necessarily accustomed
with computers and even less with these tools, which
in most cases are developed for environments such
as universities or huge organizations. This means
that our users may lack the technical skills neces-
sary to work with an LMS, and the extra effort of
understanding the system would result in a loss of
motivation.
Therefore, considering the conclusions from this
study, we have opted to build a modular, portable
application which will totally adhere to our require-
ments. With these purposes in mind, the system has
been designed as a multi-tier web application, being
accessible everywhere from a web browser. The im-
plementation of the whole system has been achieved
by integrating different technologies of a heteroge-
neous nature. In fact, the presentation tier exploits
Adobe R?Flash R?technology in order to support rich
multimedia interaction. The middle tier comprises
the integration of our own speech recognition sys-
tem, AUDIMUS, and some of the most advanced
open source frameworks for the development of web
applications, Apache Tiles, Apache Struts 2, Hiber-
nate and Spring. In the data tier, the persistence
of the application data is delegated to the relational
database MySQL. This is where the system main-
tains information related to patient clinical data, ut-
terances produced during therapeutic sessions, train-
ing exercises, stimuli and statistical data related both
to the frequency with which the system is used, and
to the patient progress.
4.1 Speech-related components of the system
Audio Recorder
In order to record the patient?s utterances, the
Vithea system takes advantage of opportunities of-
fered by Adobe R?Flash R?technology. This allows
easy integration in most browsers without any re-
quired extra plugin, while avoiding the need for se-
curity certificates to attest to the reliability of an
external component running in the client machine
within the browser. This choice was mainly moti-
vated from the particular kind of users who will use
the system, allowing them to enjoy the advantages
of the virtual therapist without the frustration of ad-
ditional configuration. A customized component has
been developed following the aforementioned prin-
ciples of usability in terms of designing the user in-
terface. Keeping simplicity and understandability
as our main guidelines, we used a reduced set of
large symbols and we tried to keep the number of
interactions required to a bare minimum. Therefore,
recording and sending an utterance to the server re-
quires only that the patient starts the recording when
ready, and then stops it when finished. Another ac-
tion is required to play back the recorded audio.
Automatic Speech Recognition Engine
AUDIMUS is the Automatic Speech Recognition
engine integrated into the Vithea system. The AU-
DIMUS framework has been developed during the
last years of research at the Spoken Language Pro-
cessing Lab of INESC-ID (L2F), it has been success-
6
fully used for the development of several ASR appli-
cations such as the recognition of Broadcast News
(BN) (Meinedo et al, 2010). It represents an essen-
tial building block, being the component in charge
of receiving the patient answers and validating the
correctness of the utterances with respect to the ther-
apeutic exercises. In the following section, this spe-
cific module of the Vithea architecture is assessed
and described in more detail.
5 The Vithea speech recognition module
5.1 The AUDIMUS hybrid speech recognizer
AUDIMUS is a hybrid recognizer that follows the
connectionist approach (Boulard and Morgan, 1993;
Boulard and Morgan, 1994). It combines the tem-
poral modeling capacity of Hidden Markov Mod-
els (HMMs) with the pattern discriminative classi-
fication of multilayer perceptrons (MLP). A Markov
process is used to model the basic temporal nature
of the speech signal, while an artificial neural net-
work is used to estimate posterior phone probabili-
ties given the acoustic data at each frame. Each MLP
is trained on distinct feature sets resulting from dif-
ferent feature extraction processes, namely Percep-
tual Linear Predictive (PLP), log-RelAtive SpecTrAl
PLP (RASTA-PLP) and Modulation SpectroGram
(MSG).
The AUDIMUS decoder is based on the Weighted
Finite State Transducer (WFST) approach to large
vocabulary speech recognition (Mohri et al, 2002).
The current version of AUDIMUS for the Euro-
pean Portuguese language uses an acoustic model
trained with 57 hours of downsampled Broadcast
News data and 58 hours of mixed fixed-telephone
and mobile-telephone data (Abad and Neto, 2008).
5.2 Word Naming Recognition task
We refer to word recognition as the task that per-
forms the evaluation of the utterances spoken by the
patients, in a similar way to the role of the thera-
pist in a rehabilitation session. This task represents
the main challenge addressed by the virtual ther-
apist system. Its difficulty is related to the utter-
ances produced by aphasic individuals that are fre-
quently interleaved with disfluencies like hesitation,
repetitions, and doubts. In order to choose the best
approach to accomplish this critical task, prelimi-
nary evaluations were performed with two sub-sets
of the Portuguese Speech Dat II corpus. These con-
sist of word spotting phrases using embedded key-
words: the development set is composed of 3334
utterances, while the evaluation set comprises 481
utterances. The number of keywords is 27. Two dif-
ferent approaches were compared: the first based
on large vocabulary continuous speech recognition
(LVCSR), the second based on the acoustic match-
ing of speech with keyword models in contrast to
a background model. Experimental results showed
promising performance indicators by the latter ap-
proach, both in terms of Equal Error Rate (EER),
False Alarm (FA) and False Rejection (FR). Thus,
on the basis of these outcomes, background model-
ing based keyword spotting (KWS) was considered
more appropriate for this task.
Background modeling based KWS
In this work, an equally-likely 1-gram model
formed by the possible target keywords and a com-
peting background model is used for word detec-
tion. While keyword models are described by their
sequence of phonetic units provided by an auto-
matic grapheme-to-phoneme module, the problem
of background modeling must be specifically ad-
dressed. The most common method consists of
building a new phoneme classification network that
in addition to the conventional phoneme set, also
models the posterior probability of a background
unit representing ?general speech?. This is usually
done by using all the training speech as positive ex-
amples for background modeling and requires re-
training the acoustic networks. Alternatively, the
posterior probability of the background unit can be
estimated based on the posterior probabilities of the
other phones (Pinto et al, 2007). We followed the
second approach, estimating the posterior probabil-
ity of a garbage unit as the mean probability of the
top-6 most likely outputs of the phonetic network at
each time frame. In this way there is no need for
acoustic network re-training. Then, a likelihood-
dependent decision threshold (determined with tele-
phonic data for development) is used to prune the
best recognition hypotheses to a reduced set of sen-
tences where the target keyword is searched for.
7
5.3 Experiments with real data
Corpus of aphasic speech
A reduced speech corpus composed of data col-
lected during therapy sessions of eight different pa-
tients has been used to assess the performance of
the speech recognition module. As explained above,
two of them (patients 2 and 7) were diagnosed with
AOS. Each of the sessions consists of naming ex-
ercises with 103 objects per patient. Each object is
shown with an interval of 15 seconds from the pre-
vious. The objects and the presentation order are the
same for all patients. Word-level transcription and
segmentation were manually produced for the pa-
tient excerpts in each session, totaling 996 segments.
The complete evaluation corpus has a duration of ap-
proximately 1 hour and 20 minutes.
Evaluation criteria
A word naming exercise is considered to be com-
pleted correctly whenever the targeted word is said
by the patient (independently of its position, amount
of silence before the valid answer, etc...). It is
worth noticing that this is not necessarily the crite-
rion followed in therapy tests by speech therapists.
In fact, doubts, repetitions, corrections, approxima-
tion strategies and other similar factors are usually
considered unacceptable in word naming tests, since
their presence is an indicator of speech pathologies.
However, for the sake of comparability between a
human speech therapist evaluation and an automatic
evaluation, we keep this simplified evaluation crite-
rion. In addition to the canonical valid answer to
every exercise, an extended word list containing the
most frequent synonyms and diminutives has been
defined, for a total KWS vocabulary of 252 words.
Only answers included in this list have been ac-
cepted as correct in both manual and automatic eval-
uation.
Results
Word naming scores are calculated for each
speaker as the number of positive word detections
divided by the total number of exercises (leftmost
plot of Figure 3). The correlation between the hu-
man evaluation assessed during ordinary therapeu-
tic sessions and the automatic evaluation assessed
with the word recognition task has resulted in a Per-
son?s coefficient of 0.9043. This result is considered
quite promising in terms of global evaluation. As
concerning individual evaluations (rightmost plot of
Figure 3), it can be seen that the system shows re-
markable performance variability in terms of false
alarms and misses depending on the specific patient.
In this sense, the adaptation to the specific user pro-
file may be interesting in terms of adjusting the sys-
tem?s operation point to the type and level of apha-
sia. As a preliminary attempt to tackle the cus-
tomization issue, the word detector has been indi-
vidually calibrated for each speaker following a 5-
fold cross-validation strategy with the correspond-
ing patient exercises. The calibration is optimized to
the minimum false alarm operation point for patients
with high false-alarm rates (2, 3, 4, 5 and 8) and to
the minimum miss rate for patients with a high num-
ber of misses (1, 6 and 7). Figure 4 shows results for
this customized detector. In this case, the correlation
between human and automatic evaluation is 0.9652
and a more balanced performance (in terms of false
alarm and false rejection ratios) is observed for most
speakers.
1 2 3 4 5 6 7 80
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Patient
Avera
ge wor
d nam
ing sco
re
 
 HumanAuto
1 2 3 4 5 6 7 80
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Patient
False 
alarm/
false r
ejection
 rates
 
 False alarmFalse rejection
Figure 3: On the left side, average word naming scores of
the human and automatic evaluations. On the right side,
false alarm and false rejection rates.
1 2 3 4 5 6 7 80
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Patient
Avera
ge wor
d nam
ing sco
re
 
 HumanAuto
1 2 3 4 5 6 7 80
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Patient
False 
alarm/
false r
ejection
 rates
 
 False alarmFalse rejection
Figure 4: On the left side, average word naming scores of
the human and automatic evaluations with the customized
detector. On the right side, false alarm and false rejection
rates of the customized detector.
8
Analysis of word detection errors
The most common cause for false alarms is the
presence of many ?invented? nonexistent words
without semantic meaning, which are very often
phonetically very close to the target words. These
paraphasic errors were present in all types of fluent
aphasia and AOS that we have observed, but not for
all patients. In many of these errors, the stressed syl-
lable is often pronounced right, or at least its rhyme.
As the typical stress pattern in Portuguese is in the
penultimate syllable, most often the last syllable is
also pronounced correctly (e.g. borco / porco). In
patients that try to say the word by approximation,
that is, by successive attempts to get closer to the
target word, but using only existent words, the dif-
ferences between the percentages of miss and false
alarms are not so remarkable.
One characteristic of aphasic patients that some-
times causes keywords to be missed (both when cor-
rectly or incorrectly pronounced) is pauses in be-
tween syllables. This may justify the inclusion of
alternative pronunciations, in case such pronuncia-
tions are considered acceptable by therapists. Ad-
ditionally, more sophisticated speech tools may also
be integrated, such as tools for computing the good-
ness of pronunciation (Witt, 1999). This would al-
low a different type of assessment of the pronuncia-
tion errors, which may provide useful feedback for
the therapist and the patients.
6 Conclusions and future work
6.1 Conclusions
This paper described how automatic speech recog-
nition technology has contributed to build up a sys-
tem that will act as a virtual therapist, being capa-
ble of facilitating the recovery of people who have a
particular language disorder: aphasia. Early experi-
ments conducted to evaluate ASR performance with
speech from aphasic patients yielded quite promis-
ing results.
The virtual therapist has been designed follow-
ing relevant accessibility principles tailored to the
particular category of users targeted by the system.
Special attention has been devoted to the user in-
terface design: web page layout and graphical ele-
ments have been chosen keeping in mind the possi-
bility that a user may experience reduced arm mobil-
ity and the technology that has been integrated was
selected with the idea of minimizing possible diffi-
culties in using the system. A pedagogical approach
has been followed in planning the functionalities of
the virtual therapist. This has been mainly driven
by the fundamental idea of avoiding an extra feature
rich tool which could have resulted in frustration for
some patients, who seek help for recovery and do
not need to learn how to use complex software.
Overall, since the system is a web application, it
allows therapy sessions anywhere at anytime. Thus,
we expect that this will bring significant improve-
ments to the quality of life of the patients allowing
more frequent, intense rehabilitation sessions and
thus a faster recovery.
6.2 Future work
The Vithea system has recently achieved the first
phase of a project which still entails several im-
provements. Even though, Naming objects and Nam-
ing common actions are the most commonly used
exercises during the rehabilitation therapies, the sys-
tem has been designed to allow a more comprehen-
sive set of therapeutic exercises which will be im-
plemented during the next refinement phase. Also,
at this stage, we plan to make available the current
version of the system to real patients in order to re-
ceive effective feedback on the system.
In the subsequent improvement phase, we will in-
tegrate the possibility of providing help, both seman-
tic and phonological to the patient whenever the vir-
tual therapist is asked for. Hints could be given both
in the form of a written solution or as a speech syn-
thesized production based on Text To Speech (TTS).
Furthermore, we are considering the possibility of
incorporating an intelligent animated agent that to-
gether with the exploitation of synthesized speech,
will behave like a sensitive and effective clinician,
providing positive encouragements to the user.
Acknowledgements
This work was funded by the FCT project
RIPD/ADA/109646/2009, and partially supported
by FCT (INESC-ID multiannual funding) through
the PIDDAC Program funds. The authors would like
to thank to Prof. Dr. M. T. Pazienza, A. Costa and the
reviewers for their precious comments.
9
References
A. Abad and J. P. Neto. 2008. International Confer-
ence on Computational Processing of Portuguese Lan-
guage, Portugal. Automatic classification and tran-
scription of telephone speech in radio broadcast data.
A. L. R. Adlam, K. Patterson, T. T. Rogers, P. J. Nestor,
C. H. Salmond, J. Acosta-Cabronero and J. R. Hodges.
2006. Brain. Semantic dementia and Primary
Progressive Aphasia: two side of the same coin?,
129:3066?3080.
M. L. Albert, R. Sparks and N. A. Helm. 1994. Neu-
rology. Report of the Therapeutics and Technology
Assessment Subcommittee of the American Academy
of Neurology. Assesment: melodic intonation therapy,
44:566?568.
M. L. Albert. 1998. Arch Neurol-Chicago Treatment of
aphasia, 55:1417?1419.
C. C. Aydin and G. Tirkes. 2010. Education Engineer-
ing. Open source learning management systems in e-
learning and Moodle, 54:593?600.
A. Basso. 1992. Aphasiology. Prognostic factors in
aphasia, 6(4):337?348.
S. K. Bhogal, R. Teasell and M. Speechley. 2003.
Stroke. Intensity of aphasia therapy, impact on recov-
ery, 34:987?993.
H. Bourlard and N. Morgan. 1993. IEEE Transactions
on Neural Networks. Continuous speech recognition
by connectionist statistical methods, 4(6):893?909.
H. Bourlard and N. Morgan. 1994. Springer. Connec-
tionist speech recognition: a hybrid approach.
D. Caseiro, I. Trancoso, C. Viana and M. Barros.
2003. International Congress of Phonetic Sciences,
Barcelona, Spain. A Comparative Description of GtoP
Modules for Portuguese and Mirandese Using Finite
State Transducers.
E. Castillo-Guerra and D. F. Lovey. 2003. 25th Annual
Conference IEEE Engineering in Medicine and Biol-
ogy Society. A Modern Approach to Dysarthria Clas-
sification.
H. Goodglass. 1993. Understanding aphasia: technical
report. Academy Press, University of Califo?rnia. San
Diego.
M. S. Hawley, P. D. Green, P. Enderby, S. P. Cunningham
and R. K. Moore. 2005. Interspeech. Speech technol-
ogy for e-inclusion of people with physical disabilities
and disordered speech, 445?448.
A. Maier, T. Haderlein, U. Eysholdt, F. Rosanowski,
A. Batliner, M. Schuster and E. No?th. 2009. Speech
Communication. PEAKS - A System for the Automatic
Evaluation of Voice and Speech Disorders, 51(5):425?
437.
H. Meinedo and J. P. Neto. 2000. International Con-
ference on Spoken Language Processing, Beijing,
China. Combination Of Acoustic Models In Contin-
uous Speech Recognition Hybrid Systems, 2:931?934.
H. Meinedo, A. Abad, T. Pellegrini, I. Trancoso and
J. P. Neto. 2010. Fala 2010, Vigo, Spain. The L2F
Broadcast News Speech Recognition System.
M. Mohri, F. Pereira and M. Riley. 2002. Computer
Speech and Language. Weighted Finite-State Trans-
ducers in Speech Recognition, 16:69?88.
P. M. Pedersen, H. S. J?rgensen, H. Nakayama,
H. O. Raaschou and T. S. Olsen. 1995. Ann Neu-
rol Aphasia in acute stroke: incidence, determinants,
and recovery, 38(4):659?666.
J. Pinto, A. Lovitt and H. Hermansky. 2007. Inter-
speech. Exploiting Phoneme Similarities in Hybrid
HMM-ANN Keyword Spotting, 1817?1820.
A. Pompili. 2011. Thesis, Department of Computer Sci-
ence, University of Rome. Virtual therapist for apha-
sia treatment.
M. T. Sarno. 1981. Recovery and rehabilitation in apha-
sia, 485?530. Acquired Aphasia, Academic Press,
New York.
C. E. Wilshire and H. B. Coslett. 2000. Disorders of
word retrieval in aphasia theories and potential appli-
cations, 82?107. Aphasia and Language: Theory to
practice, The Guilford Press, New York.
S. M. Witt. 1999. Use of speech recognition in Computer
assisted Language Learning. PhD thesis, Department
of Engineering, University of Cambridge.
S. -C. Yin, R. Rose, O. Saz and E. Lleida. 2009. IEEE In-
ternational Conference on Acoustics, Speech and Sig-
nal Processing. A study of pronunciation verification
in a speech therapy application, 4609?4612.
10
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 426?436,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Crowdsourcing High-Quality Parallel Data Extraction from Twitter
?
Wang Ling
123
Lu?s Marujo
123
Chris Dyer
2
Alan Black
2
Isabel Trancoso
13
(1)L
2
F Spoken Systems Lab, INESC-ID, Lisbon, Portugal
(2)Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
(3)Instituto Superior T?cnico, Lisbon, Portugal
{lingwang,lmarujo,cdyer,awb}@cs.cmu.edu
isabel.trancoso@inesc-id.pt
Abstract
High-quality parallel data is crucial for a
range of multilingual applications, from
tuning and evaluating machine translation
systems to cross-lingual annotation pro-
jection. Unfortunately, automatically ob-
tained parallel data (which is available
in relative abundance) tends to be quite
noisy. To obtain high-quality parallel data,
we introduce a crowdsourcing paradigm
in which workers with only basic bilin-
gual proficiency identify translations from
an automatically extracted corpus of par-
allel microblog messages. For less than
$350, we obtained over 5000 parallel seg-
ments in five language pairs. Evaluated
against expert annotations, the quality of
the crowdsourced corpus is significantly
better than existing automatic methods:
it obtains an performance comparable to
expert annotations when used in MERT
tuning of a microblog MT system; and
training a parallel sentence classifier with
it leads also to improved results. The
crowdsourced corpora will be made avail-
able in http://www.cs.cmu.edu/
~lingwang/microtopia/.
1 Introduction
High-quality parallel data is essential for tun-
ing and evaluating statistical MT systems, and
it plays a role in a wide range of multilingual
NLP applications, such as word sense disambigua-
tion (Gale et al., 1992; Ng et al., 2003; Specia
et al., 2005), paraphrasing (Bannard and Callison-
burch, 2005; Ganitkevitch et al., 2012), annota-
tion projection (Das and Petrov, 2011), and other
language-specific applications (Schwarck et al.,
?
A sample of the crowdsourced corpora and the inter-
faces used are available as supplementary material.
2010; Liu et al., 2011). While large amounts
of parallel data can be easily obtained by mining
the web (Resnik and Smith, 2003), comparable
corpora (Munteanu and Marcu, 2005), and even
social media sites (Ling et al., 2013), automati-
cally extracted parallel tends to be noisy, and, as a
result, ?evaluation-quality? parallel corpora have
generally been produced at considerable expense
by targeted translation efforts (Bojar et al., 2013,
inter alia). Unfortunately, in some domains such
as microblogs, the only corpora that are available
are automatically extracted and noisy.
While phrase-based translation models can ef-
fectively learn translation rules from noisy parallel
data (Goutte et al., 2012), having a subset of high-
quality parallel segments is nevertheless crucial.
Firstly, the automatic parallel data extraction sys-
tem?s parameters can be tuned by optimizing on
the gold standard data. Secondly, even though the
parallel data used to train MT systems can contain
a considerable amount of noise, it is conventional
to use human annotated parallel data to tune and
evaluate the system. Finally, other NLP applica-
tions may not be as noise-robust as MT.
We introduce a new crowdsourcing protocol for
obtaining high-quality parallel data from noisy,
automatically extracted parallel data (?3), focus-
ing on the challenging case of identifying par-
allel data in microblog messages (Ling et al.,
2013). In contrast to previous attempts to use
crowdsourcing to obtain parallel data, in which
workers performed translation (Ambati and Vo-
gel, 2010; Zaidan and Callison-Burch, 2011; Post
et al., 2012; Ambati et al., 2012), our approach
only requires that they identify whether a candi-
date message contains a translation, and if so, what
the spans of the translated segments are. This is
a much simpler task than translation, and one that
can often be completed by workers with only a ba-
sic proficiency in the source and target languages.
For evaluation (?4), we use our protocol to build
426
parallel datasets on a Chinese-English corpus orig-
inally extracted from Sina Weibo and for which we
have expert annotations. This lets us quantify the
effectiveness of our method under different task
variations. We also show that the crowdsourced
corpus performs as well as expert annotation (and
better than the automatically extracted corpus) for
tuning an MT system with MERT. We next apply
our method on a corpus of five language pairs (en-
ar, en-ja, en-ko, en-ru, en-zh) extracted from Twit-
ter (?5), for which we have no gold-standard data.
Using this data in a cross-validation setup, we train
and evaluate a maxent classifier for detecting par-
allel data (?6), and then we conclude (?7).
2 Related Work
Our work crosses crowdsourcing techniques and
automatic parallel data extraction from mi-
croblogs. In this section, we shall provide back-
ground information and analysis of the work per-
formed in these two fields.
2.1 Parallel Data Extraction from Microblogs
Many sources of parallel data exist on the
web. The most popular choice are parallel web
pages (Resnik and Smith, 2003), while other
work have looked at specific domains with large
amounts of data, such as Wikipedia (Smith et
al., 2010). Microblogs, such as Twitter and Sina
Weibo, represent a subdomain of the Web. Some
of its characteristics is the informal language used
and the short nature of the messages that are
posted. Due to its large size and growing pop-
ularity, work has been done on parallel data ex-
traction from this domain. Ling et al. (2013) at-
tempt to find naturally occurring parallel data from
Sina Weibo and Twitter. Some examples of what
is found are illustrated in Figure 1. The extrac-
tion process starts by finding the parallel segments
within the same message and the word alignments
between those segments that maximize a hand-
tuned model score.
Another method (Jehl et al., 2012) leverages
CLIR (Cross Lingual Information Retrieval) tech-
niques to find pairs of tweets that are translations.
The main challenge in this approach is the large
amount of pairs of tweets that must be considered,
which raises some scalability issues when process-
ing billions of tweets.
Our crowdsourcing method can be applied to
annotate data from any naturally occurring source.
In this paper, we will use the corpus developed
by Ling et al. (2013), since it is publicly available
and has parallel data for 6 languages from Twitter,
and for 10 languages from Sina Weibo.
2.2 Parallel Data using Crowdsourcing
Most of the work done in building parallel data
using crowdsourcing (Ambati and Vogel, 2010;
Zaidan and Callison-Burch, 2011; Post et al.,
2012; Ambati et al., 2012) relies on using crowd-
sourcing workers to translate. These methods
must address the fact that workers may produce
poor and sometimes incorrect translations. Thus,
in order to find good translations, subsequent
postediting and/or ranking is generally necessary.
In contrast, in our work, crowdsourcing is used
for data extraction rather than translation, a sub-
stantially simpler task than translation (in particu-
lar, translation of informal text) that requires less
expertise in the language pair (basic proficiency in
the two languages is generally sufficient to suc-
cessfully complete the task). Furthermore, assess-
ing whether a worker performed the task correctly
and combining the outputs of different workers is
simpler. The time spent per item is also reduced:
our annotation interface only requires the worker
to make a few clicks on the tweet to complete
each annotation, meaning that tasks are completed
faster and with less effort, allowing us to obtain
translations at lower cost. On the other hand,
the main drawback of our method is that it can
only obtain parallel data from translations that ex-
ist, which corresponds to the amount of posts that
have been translated and posted. This limits the
potential coverage of our method. Furthermore,
the resulting datasets may not be fully representa-
tive of the Twitter domain, since not all types of
content are translated and follow the same distri-
bution as the data in Twitter.
3 Proposed Crowdsourcing Protocol
As discussed above, automatically extracted par-
allel is often noisy. The sources of error range
from language detection errors, to errors determin-
ing if material is actually translation, and errors in
extracting the appropriate spans of the translated
material. Consider the fragment of the microblog
parallel corpus mined by Ling et al. (2013), which
is shown in Figure 1. In the Korean-English mes-
sage, the system may incorrectly added the un-
translated word Hahah in the English segment,
427
and missed the translated word Weather. At a high
level, the task faced by annotators will be to iden-
tify and resolve such errors.
3.1 Overview
We separate the tasks of identifying the parallel
posts, which we shall denote by identification,
and of locating the parallel segments, which we
will call location. The justification for this is that
the majority of the tweets are not parallel, as re-
ported by Ling et al. (2013), and the location of
the parallel data is only applicable if the tweet
actually contains parallel data. This is also de-
sirable because the identification task is simpler
than the location task. Firstly, identifying whether
a tweet contains translations requires much less
proficiency in the respective languages than locat-
ing the parallel segments, since it only requires
the worker to understand parts of the message.
This means we can have more potential workers
capable of performing this task. Secondly, the
first task is a binary decision, and each annota-
tion can be completed with only one action, which
means that the average required time for this task
is much lower than the second task and the pay-
ment required for each hit will naturally be lower
as well. Finally, combining worker results for a
binary decision is simpler than combining transla-
tions, since the space of possible answers is sev-
eral orders of magnitude lower.
As crowdsourcing platform, we use Amazon?s
Mechanical Turk. In this platform, the requesters
can submit tasks, where one can define the num-
ber of workers n that will complete each task and
what is the payment p for each task submission,
henceforth denoted as job. In our work, we had to
consider the following components:
? Interface - To submit a task, an interface
must be provided, which workers will be us-
ing to complete the job.
? Worker Quality Prediction - After submit-
ting a job, the requester can accept and pay
the agreed fee or reject the task. It is cru-
cial to have a method to automatically pre-
dict whether workers have performed the job
properly, and reject them otherwise.
? Result Combination - It is common for mul-
tiple workers to complete the same task with
different results. Thus, a method must be im-
plemented to combine multiple responses for
correctly predicting the desired response.
We structured each of our tasks as a series of q
questions, which include a small number of refer-
ences r, for which we know the answers. Thus,
the amount of answers we obtain for each dollar is
given by
q?r
np
, where n is the number of workers
per task and p is the payment for each task. In or-
der to maximize this quotient, we can either reduce
the number of reference question r, the number of
workers per task n, or the payment p. However,
reducing r will also limit our capability of esti-
mating the quality of the worker results, since we
will have less data to make such prediction. For
the same reason, reducing n will limit our abil-
ity to combine results properly. As for the pay-
ment p, while there is no direct effect on our task,
it has been noted that workers will perform the
task faster for higher payments (Post et al., 2012).
In our work, we will propose methods to predict
quality and combine results that will minimize the
requirements for n and r, while maximizing the
quality of the final results.
3.2 Parallel Post Identification
In the identification task, for each question, we
will show a post, and solicit the worker to detect if
it contains translations in a given language pair.
Interface The interface for this task is straight-
forward. We present to the worker each tweet in-
dividually, together with a checkbox to be checked
in case the tweet contains parallel data. The navi-
gation between tweets is done by adding next and
previous buttons, allowing the user to go back and
review previous answers. Finally, the worker can
only submit the HIT after traversing all 25 ques-
tions. Unlike the work in crowdsourcing transla-
tion (Zaidan and Callison-Burch, 2011), where au-
tomatic translation systems are discouraged, since
it produces poor output, we allow its usage as long
as this leads to correct annotations. In fact, we add
a button to automatically translate the tweet into
English from the non-English language.
Worker Quality Prediction We accept the job
if it answers enough reference questions correctly.
We consider two different approaches to select ref-
erences. A random sampler that selects tweets
randomly and a balanced sampler that selects
the same number of positive and negative sam-
ples. As notation, we will denote as acceptor
428
Figure 1: Parallel microblog posts in 5 language pairs. Shaded backgrounds mark the parallel segments
(annotated manually), non shaded parts do not have translations.
accept(rand, c, r) a setup where the worker?s job
is accepted if c out of r randomly sampled refer-
ences are correctly answered. Likewise, acceptor
accept(bal, c, r) denotes the same setup using bal-
anced reference questions.
Result Combination Given n jobs with answers
for a question that can be either positive or nega-
tive, we calculate the weighted ratio of positive an-
swers, given by
?
i=1..n
?
p
(i)w(i)
?
i=1..n
w(i)
, where ?
p
is one if
answer i is positive and 0 otherwise, and w(i) is
the weight of the worker. w(i) is defined as the
ratio of correct answers from job i in the reference
set. If the weighted ratio is higher than 0.5, we la-
bel the tweet as positive and otherwise as negative.
3.3 Parallel Data Location
In the location task, we also present one tweet per
question, where the worker will be asked to iden-
tify the parallel segments. The worker can also
define that there are no translations in the tweet.
Interface The interface for this task presents the
user with one tweet at a time, and allows the user
to break the tweet into segments, by clicking be-
tween characters. Each segment can then be clas-
sified as English, the non-English language (Ex:
Mandarin), or non-parallel, which is the default
option. To understand the concept of non-parallel
segments, notice that when we are locating par-
allel data in tweets, we are essentially breaking
the tweet into the structure ?N
left
P
left
N
middle
P
right
N
right
", where P
left
and P
right
are the par-
allel segments and N
left
, N
middle
and N
right
are
textual segments that are non-parallel. These may
not exist, for instance, the Arabic tweet in Fig-
ure 1 (line 1) does not contain any non-parallel text
and does not require any non-parallel segments
to delineate the parallel data. The Korean tweet
(line 2), on the other hand, has an N
middle
corre-
sponding to????????????????????????????* and an
N
right
corresponding to Hahah and requires two
non-parallel segments to locate the parallel data.
Thus, if the worker does not commit any errors,
each question can be answered with at most four
clicks, when all five segments exist, and two op-
tion choices for identifying the parallel segments.
In the easiest case, when only the parallel seg-
ments exist, only one click and two option choices
are needed. If there are no translations, the button
no translations can be clicked.
For instance, to annotate the Korean tweet in
Figure 1, the worker must click immediately be-
fore????, then before Weather and finally before
Hahah. Then on the drop-down box of the first
and and third segments, the worker must choose
Korean and English, respectively. The interface
after these operations is show in Figure 2.
Work Quality Prediction To score the worker?s
jobs, we use the scoring function devised in (Ling
et al., 2013), which measures the word overlap
between the reference parallel segments segments
and the predicted segments. However, setting the
score threshold to accept a job is a challenge, since
scores are bound to change for different language-
pairs and domains. Moreover, some tweets are
harder to annotate than others. Learning this
threshold automatically requires annotated data,
which we do not have for all language pairs and
domains. Thus, we propose a method to generate
thresholds specifically for each sample.
We consider a ?smart but lazy" pseudo worker,
who will complete the same jobs automatically
and generate scores that the real worker?s jobs
must beat to be accepted. We say he is ?smart",
429
Figure 2: Location Interface (After the annotation is performed)
since he knows the reference annotation, and
?lazy" because he will only define a new non-
parallel segment if it is significant, otherwise it
will just be left in the parallel segments. By sig-
nificant, we will define whether it is at least 20%
larger (in number of characters) than the parallel
segments. For instance, in the Korean example in
Figure 1, Hahah would be left in the English par-
allel segment, while ???????????????????????
??? ??* would not be in the Korean segment. We
will accept a job if the average of the scores in the
reference set is higher or equal than the pseudo
worker?s scores. This acceptor shall be denoted as
accept(lazy, a), where a is the number of refer-
ences used.
Another option is to use the automatic system?s
output as a baseline that workers must improve to
be accepted. We will also test this option and call
this acceptor accept(auto, a).
Result Combination Unlike the identification
task, where the result is binary and combining
multiple decisions is straightforward, the range of
results from this task is larger and combining them
is a challenge. Thus, we score each job based on
the WER on the reference set and use annotations
of the highest scoring job.
4 Experiments
To obtain results on the effectiveness of the meth-
ods described in Section 3, we will first perform
experiments using pre-annotated data. We use the
annotated dataset with tweets in Mandarin-English
from Sina Weibo created in (Ling et al., 2013).
It consists of approximately 4000 tweets crawled
from Sina Weibo that were annotated on whether
they contained parallel data and the location of the
parallel segments. In our experiment, we sample
1000 tweets from this dataset, where 602 tweets
were parallel and 398 were not.
1
We will not submit the same tasks using differ-
ent setups, since we would have to pay the cost of
the tasks multiple times. Furthermore, we know
the answers for all the questions in this controlled
experiment, the quality of a job can be evalu-
ated precisely by using all questions as references.
Thus, we will perform the task once, with a larger
number of workers and accepting and rejecting
jobs based on their real quality. Then, we will use
the resulting datasets and simulate the conditions
using different setups.
430
Acceptor avg(a) avg(r) d
accept(rand, 2, 2) 0.44 0.00 0.44
accept(rand, 3, 4) 0.44 0.00 0.44
accept(rand, 4, 4) 0.55 0.04 0.51
accept(bal, 2, 2) 0.69 0.09 0.60
accept(bal, 3, 4) 0.64 0.03 0.61
accept(bal, 4, 4) 0.76 0.15 0.61
Table 1: Agreement with the expert annotations
for different acceptors.
4.1 Identification Task
The 1000 tweets were distributed into 40 tasks
with 25 questions each (q = 25). Each task is
to be performed by 5 workers (n = 5) and upon
acceptance, a worker would be rewarded with 6
cents (p = 0.06). As we know the answers for
all the questions in this case, we will calculate the
Cohen?s Kappa between the responses of each job
and the expert annotator, and accept a job if it is
higher than 0.5. We decided to use Cohen?s kappa
to evaluate a job, rather than accuracy, since each
set of 25 questions does not contain the same num-
ber of positive and negative samples. For instance,
in a set of 20 negative samples, a worker would
achieve an accuracy of 80% if he simply answers
negatively to all questions, which is not an ade-
quate assessment of the job?s quality. On the other
hand, the Cohen?s Kappa balances the positive and
negative question in each task by using their prior
probabilities. In total, there were 566 jobs, where
200 where accepted and 366 were rejected.
Next, we pretended that we only have access to
4 references, which will be used for quality es-
timation and simulate the acceptances and rejec-
tions for each strategy. Table 1 shows the aver-
ages of the real Kappa values of accepted (col-
umn avg(a)) and rejected jobs (column avg(r))
using different acceptors. Our goal is to maximize
the number of acceptances with high Kappa val-
ues and minimize those that have low Kappa val-
ues. Thus, we define d as the difference between
avg(a) and avg(r). From the results, we observe
that using a balanced reference yields a much bet-
ter estimation of the jobs quality using our metric
d. Similar conclusions can be reached by compar-
ing accept(rand, 3, 4) with accept(bal, 3, 4) and
accept(rand, 4, 4) with accept(bal, 4, 4). Quality
predictors that use balanced reference sets achieve
1
We wished to annotate a sample where the number of
parallel posts is high, so that we would have enough samples
to perform the location task.
Acceptor prec recall F1 acc ?
Automatic 0.87 0.69 0.77 0.75 0.51
All jobs 0.75 0.84 0.8 0.74 0.44
accept(rand, 2, 2) 0.85 0.92 0.88 0.86 0.69
accept(rand, 3, 4) 0.84 0.93 0.88 0.85 0.68
accept(rand, 4, 4) 0.91 0.95 0.93 0.92 0.82
accept(bal, 2, 2) 0.94 0.94 0.94 0.92 0.84
accept(bal, 3, 4) 0.93 0.95 0.94 0.93 0.85
accept(bal, 4, 4) 0.94 0.93 0.93 0.92 0.84
Table 2: Parallel post prediction scores using dif-
ferent acceptors.
approximately the same results for d. However,
the setup accept(bal, 3, 4) has a lower Kappas for
both avg(a) and avg(r), which means that it is
less likely to reject good jobs at the cost of accept-
ing more bad jobs. This is desirable from an ethi-
cal perspective, since workers are not responsible
for errors in our quality prediction. Furthermore,
rejecting good jobs has a negative impact on the
progress of the task, since good workers may be
discouraged to perform more tasks.
Results on the identification task, obtained for
n = 3, are shown in Table 2. Naturally, us-
ing a balanced reference set yields better results,
since these have a higher d value. We can also
see the importance of quality prediction, since not
performing quality estimation (row All jobs) will
yield worse results than the automatic system.
Next, we will compare results using different
numbers of workers. We fix the quality predic-
tion methodology to accept(bal, 3, 4) and results
are shown in Table 3. We observe that in gen-
eral, using more workers will generate better re-
sults, but score gains from adding another worker
becomes lower as n increases. One problem for
n = 2 is the fact that there are many cases where
two workers with the same weight chose a posi-
tive and a negative answer, in which case, no de-
cision can be made, and we simply choose false
by default. This explains the high recall and low
precision values. However, this problem seems to
occur much less with higher values of n.
4.2 Location Task
For the location task, we used the predicted par-
allel posts the identification task with the setup
accept(bal, 3, 4) and n = 5. We preferred to use
this rather than using the expert annotations, since
it would not contain false positives, which does not
simulate a real situation. Then, we used 500 out of
431
# workers prec recall F1 acc ?
Automatic 0.87 0.69 0.77 0.75 0.51
1 0.86 0.85 0.85 0.82 0.64
2 0.85 0.95 0.90 0.87 0.72
3 0.93 0.95 0.94 0.93 0.85
4 0.94 0.96 0.95 0.94 0.87
5 0.96 0.96 0.96 0.95 0.90
Table 3: Identification scores for different n.
the 607 identified positive samples. This makes
20 tasks in total, with 25 questions (q = 25), and
each task would be run until 5 jobs are accepted
(n = 5). For this task, we set a payment of 30
cents (p = 0.3), since it is a more complex task.
Again, since we have the expert annotations for all
questions, we calculated the average WER on all
answers and rejected jobs scoring less than 0.6
2
.
This task is mainly focused on the quality pre-
diction of the workers, as the result combination
is done by finding the job with the highest score
in the reference set. This means, for an arbitrary
large n, all quality estimation methods will pro-
duce the same result, since we will find the best
job on the references eventually. However, bet-
ter quality estimation will allow us to find the best
jobs with lower n, which makes the task less ex-
pensive. Table 4 shows results using different se-
tups. In these results, we set aside 4 questions to
be used as references. We can see that for low n
(1 or 2), if we simply accept all jobs, the quality
of the results will be lower than the automatic sys-
tem. For n = 4, this approach can achieve a WER
score of 0.06. However, if we use the automatic
system as a baseline that jobs must surpass, we can
achieve this WER score with only two jobs, which
reduces the cost of this task by half. Yet, this is
strongly dependent on the automatic system, as a
worse system will be easier to match for the work-
ers. On the other hand, using the smart but lazy
pseudo worker, where we degrade the reference
annotations slightly, we can see that we can obtain
the 0.06 WER score using only the first worker. At
n = 2, we can see that the WER improves to 0.05,
which is lost for n = 3. This is because the pre-
diction of the quality of the job using the workers
is not always precise.
4.3 Machine Translation Results
Finally, we will perform an extrinsic test to see
how the improvements obtained by using crowd-
2
Determined empirically
Number of jobs 1 2 3 4 5
Automatic 0.16 0.16 0.16 0.16 0.16
All Jobs 0.23 0.21 0.07 0.06 0.06
accept(auto, 4) 0.09 0.06 0.06 0.06 0.06
accept(lazy, 4) 0.06 0.05 0.06 0.06 0.06
Table 4: Parallel data location scores for different
acceptors (rows) and different numbers of work-
ers. Each cell denotes the WER for that setup.
Auto (Pos) Crowd Expert Auto (All)
Size 483 479 483 908
EN-ZH 10.21 10.49 10.51 10.71
ZH-EN 7.59 7.87 7.82 8.02
Table 5: BLEU score comparison using different
corpora for MERT tuning. The Size row denotes
the number of sentences of each corpus, and the
EN-ZH and ZH-EN rows denote the BLEU scores
of the respective language pair and tuning dataset.
sourcing map to Machine Translations. We will
build an out of domain MT system using the FBIS
dataset (LDC2003E14), a corpus of 300K sen-
tence pairs from the news domain in the Chinese-
English pair using the Moses (Koehn et al., 2007)
pipeline. Due to the small size of our crowd-
sourced corpus, we will use it in the MERT tun-
ing (Och, 2003), and test its effects compared to
automatically extracted parallel data and the ex-
perts judgements. As the test set, we will use
1,500 sentence pairs from the Weibo gold standard
from Ling et al. (2013), that were not used in our
crowdsourcing experiment to prevent data over-
lap. For reordering, we use the MSD reordering
model (Axelrod et al., 2005) and as the language
model, we use a 5-gram model with Kneser-Ney
smoothing (Heafield, 2011). Finally, results are
presented with BLEU-4 (Papineni et al., 2002).
We build 3 tuning corpora, the automatically ex-
tracted corpus (denoted Auto), the crowdsourced
corpus (denoted Crowd) and the corpus annotated
by the expert (denoted Expert). This is done by
taking the 1000 tweets used in this experiment, se-
lect those that were identified as parallel accord-
ing to each criteria. For the automatic extraction,
the authors in (Ling et al., 2013) simply use all
tweets as parallel, which may influence the tun-
ing results. Thus, we test two versions of this cor-
pus, one where we take all samples as parallel (de-
noted Auto (All)), and one where we use the ex-
pert?s decision for the identification task only (de-
432
Pair Parallel Avg(en) cost(I) cost(L) total
en-ar 1512 8.3 $35.7 $43.2 $76.2
en-zh 1302 8.7 $35.7 $37.2 $70.2
en-ja 1155 7.9 $35.7 $33.0 $68.7
en-ko 1008 7.1 $35.7 $28.8 $64.5
en-ru 798 6.3 $35.7 $22.8 $58.5
all 5775 ? $178.5 $165.0 $343.5
Table 7: AMT costs for crowdsourced corpora
from Twitter.
noted Auto (Pos)). In the crowdsourcing case, we
use the accept(bal, 3, 4) setup, with n = 5, for the
identification task and the accept(lazy, 4) setup,
with n = 2, for the location task. From the re-
sulting parallel tweets, we also remove all tweets
that were used as reference in the accept(lazy, 4)
quality estimator, as this would give an unfair ad-
vantage to the crowdsourced corpora.
Results are shown in Table 5, where each cell
contains the average BLEU score in 5 MERT runs,
using a different tuning dataset. Surprisingly, us-
ing the whole set of automatically extracted cor-
pora actually achieves better results than using
carefully selected data that are parallel. We be-
lieve that is because many non-parallel segments
actually contain comparable information that can
be used to improve the weights during MERT tun-
ing. However, this does not mean that the qual-
ity of the automatically crawled corpus is better
than the crowdsourced and expert annotated cor-
pus. When using a similar number of parallel sen-
tences, we observe that using the crowdsourced
corpus yields better scores than the automatically
extracted corpora, comparable to experts annota-
tions. While results are not significantly better
than automatically extracted corpora, this suggests
that the crowdsourced corpora has a better overall
quality than automatically extracted corpora.
5 Five Language Twitter Parallel Corpus
Now that we have established the effectiveness of
our technique for extracting high-quality parallel
data in a scenario where we have gold standard
annotations, we apply it to creating parallel cor-
pora in five languages on Twitter, for which we
have no gold-standard parallel data: Arabic, Man-
darin, Japanese, Korean and Russian. Once again,
we use the extracted automatically Twitter cor-
pus from Ling et al. (2013) and deploy the task
in Mechanical Turk. We use the setup that ob-
tained the best results in Section 4. For the identi-
fication task, we used the accept(bal, 3, 4) setup,
with n = 5. The payment for each task was
0.06 dollars. Thus, for this task, each dollar spent
yields 70 annotated tweets. For the location task,
we used the accept(lazy, 4) setup, with n = 2
and each task was rewarded with 0.3 dollars. To
obtain the tweet sample, we filtered the corpora
in Ling et al. (2013) for tweets with alignment
scores higher than 0.1. Then, we uniformly ex-
tracted 2500 tweets for each language. To gener-
ate gold standard references, the authors manually
annotated 40 samples for each pair.
Table 7 contains information about the result-
ing corpora. The number of parallel sentences ex-
tracted from the 2500 tweets in each language pair
is shown in column Parallel and we can see that
this differs given the language pair. We can also
see in column Avg(en) that the average number of
English words is much smaller than what is seen
in more formal domains. Finally, Arabic parallel
data seems more predominant from our samples
followed by Mandarin, while Russian parallel data
seem scarcer.
6 Discriminative Parallel Data Detection
While the work in (Ling et al., 2013) used a linear
combination of three models, the alignment, lan-
guage and segment features, these weights were
determined manually. However, using the crowd-
sourced corpus (in Section 5), we will apply previ-
ously proposed methods that learn a classifier with
machine learning techniques as in related work
on finding parallel data (Resnik and Smith, 2003;
Munteanu and Marcu, 2005). In our work, we use
a max entropy classifier model, similar to that pre-
sented by Munteanu and Marcu (2005) to detect
parallel data in tweets. Our features are:
? Alignment feature - The baseline feature is
the alignment score from the work in (Ling et
al., 2013), and measures how well the paral-
lel segments align, which is derived from the
content-based matching methods for detect-
ing parallel data (Resnik and Smith, 2003).
? User features - An observation in (Ling et
al., 2013) is that a user that frequently posts
in parallel is likely to post more parallel mes-
sages. Based on this, we added the aver-
age alignment score from all messages of the
same user and the ratio of messages that are
predicted to be parallel as features.
433
Weibo (en-zh) Twitter (en-zh) Twitter (en-ar) Twitter (en-ru) Twitter (en-ko) Twitter (en-ja)
Alignment 0.781 0.599 0.721 0.692 0.635 0.570
+User 0.814 0.598 0.721 0.705 0.650 0.566
+Length 0.839 0.603 0.725 0.706 0.650 0.569
+Repetition 0.849 0.652 0.763 0.729 0.655 0.579
+Language 0.849 0.668 0.782 0.737 0.747 0.584
Table 6: Classification Results using a 10-fold cross validation over different datasets. Each cell contains
the F-measure using a given dataset and an incremental set of features.
? Repetition features - There are many words
that are not translated, such as hashtags, at
mentions, numbers and named entities. So, if
we see these repeated twice in the same post,
it can be used as a strong cue that this was
the result of a translation. Hence, we define
features for each of these cases, that trigger if
either of these occur in multiples of two times
in the same post. Named Entities were iden-
tified using a naive approach by considering
words with capital letters.
? Length feature - It is known that the length
differences between parallel sentences can
be modelled by a normal distribution (Gale
and Church, 1991). Hence, we used parallel
data in the respective language to determine
(??, ??
2
), which lets us calculate the likelihood
of two hypothesized segments being parallel.
Since we did not have annotated parallel data
for this domain, we used the top 2000 scoring
parallel sentences from the respective Twitter
dataset in (Ling et al., 2013).
? Language feature - It is common for non-
English words to be found in English seg-
ments, such as names of foreign celebri-
ties, numbers and hashtags. However, when
this happens to the majority of the words in
a segment that is supposed to be English,
it may indicated that there was an error in
the language detection. The same happens
with non-English segments. We used the
same naive approach to detect languages as
in (Ling et al., 2013), where we calculate the
ratio of number of words in the English seg-
ment and the total number of words from the
segment detected as English and the ratio of
the number of Foreign words and the total
number of words in the Foreign segment ,de-
tected by their unicode ranges. This was also
included in the work in (Ling et al., 2013).
Results using a 10 fold cross-validation are
shown in Table 6. In general, we can see that the
classifier performs worse in Twitter datasets com-
pared to the Weibo dataset. We believe that this is
because parallel sentences extracted from Twitter
are smaller, due to the 140 character limit, which
does not hold in Sina Weibo. Each parallel En-
glish segment from the Sina Weibo parallel data
contains 15.4 words on average. On other hand,
we see in Table 7 that this number is smaller in
the parallel data from Twitter. This means that the
aligner will have a much smaller range of words to
align when detecting parallel data, which makes it
more difficult to find parallel segments.
As for the features, we observe that by defin-
ing these simple features, we can get a signifi-
cant improvement over previous baselines. For
the User feature, we see that the improvements
in the Weibo dataset are much larger than in
the Twitter datasets. This is because the Twitter
dataset was crawled uniformly, whereas the Weibo
dataset was focused on users that post parallel
data frequently. Thus, in the Weibo dataset there
more posts that were posted by the same user,
which does not happen as frequently in the Twitter
dataset. As for the Length feature, we can see that
it yields a small but consistent improvement over
all datasets. Repetition based features also lead to
improvements across all datasets, and produces a
5% improvement in the English-Mandarin Twitter
dataset. Finally, language based features also add
another improvement over previous results.
7 Conclusions
We presented a crowdsourcing approach to extract
parallel data from tweets. As opposed to meth-
ods to crowdsource translations, our tasks do not
require workers to translate sentences, but to find
them in tweets. Our method is divided into two
tasks. First, we identify which tweets contain
translations, and we show that multiple worker?s
jobs can be combined to obtain results compara-
434
ble to those of expert annotators. Secondly, tweets
that are found to contain translations are given
to other workers to locate the parallel segments,
where we can also obtain high quality results.
Then, we use our method to extract high quality
parallel data from Twitter in 5 language pairs. Fi-
nally, we improve the automatic identification of
tweets with translations by using a max entropy
classifier trained on the crowdsourced data.
We are currently extracting more data and the
crowdsourced parallel data from Twitter will made
be available to the public.
References
[Ambati and Vogel2010] Vamshi Ambati and Stephan
Vogel. 2010. Can crowds build parallel corpora
for machine translation systems? In Proceedings
of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, Stroudsburg, PA, USA. Association for
Computational Linguistics.
[Ambati et al.2012] Vamshi Ambati, Stephan Vogel,
and Jaime Carbonell. 2012. Collaborative workflow
for crowdsourcing translation. In Proceedings of the
ACM 2012 Conference on Computer Supported Co-
operative Work, CSCW ?12, pages 1191?1194, New
York, NY, USA. ACM.
[Axelrod et al.2005] Amittai Axelrod, Ra Birch Mayne,
Chris Callison-burch, Miles Osborne, and David
Talbot. 2005. Edinburgh system description for the
2005 iwslt speech translation evaluation. In Pro-
ceedings International Workshop on Spoken Lan-
guage Translation (IWSLT.
[Bannard and Callison-burch2005] Colin Bannard and
Chris Callison-burch. 2005. Paraphrasing with
bilingual parallel corpora. In In ACL-2005, pages
597?604.
[Bojar et al.2013] Ond
?
rej Bojar, Christian Buck, Chris
Callison-Burch, Christian Federmann, Barry Had-
dow, Philipp Koehn, Christof Monz, Matt Post,
Radu Soricut, and Lucia Specia. 2013. Find-
ings of the 2013 Workshop on Statistical Machine
Translation. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 1?
44, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
[Das and Petrov2011] Dipanjan Das and Slav Petrov.
2011. Unsupervised part-of-speech tagging with
bilingual graph-based projections. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies - Volume 1, HLT ?11, pages 600?609,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
[Gale and Church1991] William A. Gale and Ken-
neth W. Church. 1991. A program for aligning
sentences in bilingual corpora. In Proceedings of
the 29th Annual Meeting on Association for Com-
putational Linguistics, ACL ?91, pages 177?184,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
[Gale et al.1992] William A. Gale, Kenneth W. Church,
and David Yarowsky. 1992. Using bilingual materi-
als to develop word sense disambiguation methods.
[Ganitkevitch et al.2012] Juri Ganitkevitch, Yuan Cao,
Jonathan Weese, Matt Post, and Chris Callison-
Burch. 2012. Joshua 4.0: Packing, PRO, and para-
phrases. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 283?291,
Montr?al, Canada, June. Association for Computa-
tional Linguistics.
[Goutte et al.2012] Cyril Goutte, Marine Carpuat, and
George Foster. 2012. The impact of sentence
alignment errors on phrase-based machine transla-
tion performance. In Proc. of AMTA.
[Heafield2011] Kenneth Heafield. 2011. KenLM:
faster and smaller language model queries. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 187?197, Edinburgh, Scot-
land, United Kingdom, July.
[Jehl et al.2012] Laura Jehl, Felix Hieber, and Stefan
Riezler. 2012. Twitter translation using translation-
based cross-lingual retrieval. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 410?421, Montr?al, Canada, June. Asso-
ciation for Computational Linguistics.
[Koehn et al.2007] Philipp Koehn, Hieu Hoang,
Alexandra Birch, Chris Callison-burch, Richard
Zens, Rwth Aachen, Alexandra Constantin, Mar-
cello Federico, Nicola Bertoldi, Chris Dyer, Brooke
Cowan, Wade Shen, Christine Moran, and Ondrej
Bojar. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume
Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic, June.
Association for Computational Linguistics.
[Ling et al.2013] Wang Ling, Guang Xiang, Chris Dyer,
Alan Black, and Isabel Trancoso. 2013. Microblogs
as parallel corpora. In Proceedings of the 51st An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?13. Association for Computational
Linguistics.
[Liu et al.2011] Feifan Liu, Fei Liu, and Yang Liu.
2011. Learning from chinese-english parallel data
for chinese tense prediction. In IJCNLP, pages
1116?1124.
[Munteanu and Marcu2005] Dragos Munteanu and
Daniel Marcu. 2005. Improving machine transla-
tion performance by exploiting comparable corpora.
Computational Linguistics, 31(4):477?504.
435
[Ng et al.2003] Hwee Tou Ng, Bin Wang, and Yee Seng
Chan. 2003. Exploiting parallel texts for word sense
disambiguation: An empirical study. In Proceedings
of ACL03, pages 455?462.
[Och2003] Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL ?03,
pages 160?167, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Papineni et al.2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine trans-
lation. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 311?318, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
[Post et al.2012] Matt Post, Chris Callison-Burch, and
Miles Osborne. 2012. Constructing parallel cor-
pora for six indian languages via crowdsourcing. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 401?409, Montr?al,
Canada, June. Association for Computational Lin-
guistics.
[Resnik and Smith2003] Philip Resnik and Noah A.
Smith. 2003. The web as a parallel corpus. Compu-
tational Linguistics, 29:349?380.
[Schwarck et al.2010] Florian Schwarck, Alexander
Fraser, and Hinrich Sch?tze. 2010. Bitext-based
resolution of german subject-object ambiguities. In
Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
?10, pages 737?740, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
[Smith et al.2010] Jason R. Smith, Chris Quirk, and
Kristina Toutanova. 2010. Extracting parallel sen-
tences from comparable corpora using document
level alignment. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 403?411, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Specia et al.2005] Lucia Specia, Maria Das Gra?as,
Volpe Nunes, and Mark Stevenson. 2005. Exploit-
ing parallel texts to produce a multilingual sense
tagged corpus for word sense disambiguation. In
Proceedings of RANLP-05, Borovets, pages 525?
531.
[Zaidan and Callison-Burch2011] Omar F. Zaidan and
Chris Callison-Burch. 2011. Crowdsourcing trans-
lation: professional quality from non-professionals.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, HLT ?11, pages
1220?1229, Stroudsburg, PA, USA. Association for
Computational Linguistics.
436
