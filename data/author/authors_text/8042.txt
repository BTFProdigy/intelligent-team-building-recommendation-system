Shallow language processing architecture for Bulgarian
Hristo Tanev
ITC-Irst,
Centro per la Ricerca Scientifica e Tecnologica
Povo,Trento, Italy 38050
tanev@itc.it
Ruslan Mitkov
School of Humanities,
Languages and Social Studies
Wolverhampton WV1 1SB UK
R.Mitkov@wlv.ac.uk
Abstract
This paper describes LINGUA - an architec-
ture for text processing in Bulgarian. First, the
pre-processing modules for tokenisation, sen-
tence splitting, paragraph segmentation, part-
of-speech tagging, clause chunking and noun
phrase extraction are outlined. Next, the pa-
per proceeds to describe in more detail the
anaphora resolution module. Evaluation results
are reported for each processing task.
1 Introduction
The state of the art of today?s full parsing and
knowledge-based automatic analysis still falls
short of providing a reliable processing frame-
work for robust, real-world applications such
as automatic abstracting or information ex-
traction. The problem is especially acute for
languages which do not benefit from a wide
range of processing programs such as Bulgar-
ian. There have been various projects which
address different aspects of the automatic anal-
ysis in Bulgarian such as morphological analy-
sis (Krushkov, 1997), (Simov et al, 1992), mor-
phological disambiguation (Simov et al, 1992)
and parsing (Avgustinova et al, 1989), but no
previous work has pursued the development of
a knowledge-poor, robust processing environ-
ment with a high level of component integrity.
This paper reports the development and im-
plementation of a robust architecture for lan-
guage processing in Bulgarian referred to as
LINGUA, which includes modules for POS tag-
ging, sentence splitting, clause segmentation,
parsing and anaphora resolution. Our text pro-
cessing framework builds on the basis of con-
siderably shallower linguistic analysis of the in-
put, thus trading off depth of interpretation for
breadth of coverage and workable, robust solu-
tion. LINGUA uses knowledge poor, heuristi-
cally based algorithms for language analysis, in
this way getting round the lack of resources for
Bulgarian.
2 LINGUA - an architecture for
language processing in Bulgarian
LINGUA is a text processing framework for
Bulgarian which automatically performs tokeni-
sation, sentence splitting, part-of-speech tag-
ging, parsing, clause segmentation, section-
heading identification and resolution for third
person personal pronouns (Figure 1). All mod-
ules of LINGUA are original and purpose-
built, except for the module for morphological
analysis which uses Krushkov?s morphological
analyser BULMORPH (Krushkov, 1997). The
anaphora resolver is an adaptation for Bulgar-
ian of Mitkovs knowledge-poor pronoun resolu-
tion approach (Mitkov, 1998).
LINGUA was used in a number of projects
covering automatic text abridging, word seman-
tic extraction (Totkov and Tanev, 1999) and
term extraction. The following sections outline
the basic language processing functions, pro-
vided by the language engine.
2.1 Text segmentation: tokenisation,
sentence splitting and paragraph
identification
The first stage of every text processing task is
the segmentation of text in terms of tokens, sen-
tences and paragraphs.
LINGUA performs text segmentation by op-
erating within an input window of 30 tokens, ap-
plying rule-based algorithm for token synthesis,
sentence splitting and paragraph identification.
2.1.1 Tokenisation and token stapling
Tokens identified from the input text serve as
input to the token stapler. The token stapler
forms more complex tokens on the basis of a
Figure 1: General architecture of LINGUA
token grammar. With a view to improving to-
kenisation, a list of abbreviations has been in-
corporated into LINGUA.
2.1.2 Sentence splitting
LINGUA?s sentence splitter operates to iden-
tify sentence boundaries on the basis of 9 main
end-of-sentence rules and makes use of a list of
abbreviations. Some of the rules consist of sev-
eral finer sub-rules. The evaluation of the per-
formance of the sentence splitter on a text of
190 sentences reports a precision of 92% and
a recall of 99%. Abbreviated names such as
J.S.Simpson are filtered by special constraints.
The sentence splitting and tokenising rules were
adapted for English. The resulting sentence
splitter was then employed for identifying sen-
tence boundaries in the Wolverhampton Corpus
of Business English project.
2.1.3 Paragraph identification
Paragraph identification is based on heuristics
such as cue words, orthography and typograph-
ical markers. The precision of the paragraph
splitter is about 94% and the recall is 98% (Ta-
ble 3).
2.2 Morphological analysis and
part-of-speech tagging
2.2.1 Morphological analysis
Bulgarian morphology is complex, for exam-
ple the paradigm of a verb has over 50
forms. Krushkov?s morphological analyser
BULMORPH (Krushkov, 1997) is integrated in
the language engine with a view to processing
Bulgarian texts at morphological level.
2.2.2 Morphological disambiguation
The level of morphological ambiguity for
Bulgarian is not so high as it is in other
languages. As a guide, we measured the ratio:
Number of all tags/Number of all words.
The results show that this ratio is compara-
tively low and for a corpus of technical texts
of 9000 words the ratio tags per word is 1,26,
whereas for a 13000-word corpus from the genre
of fiction this ratio is 1,32. For other languages
such as Turkish this ratio is about 1,9 and for
certain English corpora 2,0 1.
We used 33 hand-crafted rules for disam-
biguation. Since large tagged corpora in Bul-
garian are not widely available, the development
of a corpus-based probabilistic tagger was an
unrealistic goal for us. However, as some stud-
ies suggest (Voutilainen, 1995), the precision of
rule-based taggers may exceed that of the prob-
abilistic ones.
2.3 Parsing
Seeking a robust flexible solution for pars-
ing we implemented two alternative approaches
in LINGUA: a fast-working NP extractor and
more general parser, which works more slowly,
but delivers better results both in accuracy and
coverage. As no syntactically annotated Bulgar-
ian corpora were available to us, using statistical
data to implement probabilistic algorithm was
not an option.
The NP extraction algorithm is capable of
analysing nested NPs, NPs which contain left
1Kemal Oflazer, personal communication
modifiers, prepositional phrases and coordinat-
ing phrases. The NP extractor is based on a
simple unification grammar for NPs and APs.
The recall of NP extraction, measured against
352 NPs from software manuals, was 77% and
the precision - 63.5%.
A second, better coverage parser was im-
plemented which employs a feature grammar
based on recent formal models for Bulgarian,
(Penchev, 1993), (Barkalova, 1997). All basic
types of phrases such as NP, AP, PP, VP and
AdvP are described in this grammar. The
parser is supported by a grammar compiler,
working on grammar description language
for representation of non context unification
grammars. For example one of the rules for
synthesis of NP phrases has the form:
NP (def :Y full art :F ext :+ rex :? nam :?)
?AP (gender :X def :Y full art:F number: L )
NP (ext:? def:? number:L gender:X rex:?)
The features and values in the rules are not
fixed sets and can be changed dynamically. The
flexibility of this description allows the gram-
mar to be extended easily. The parser uses a
chart bottom-up strategy, which allows for par-
tial parsing in case full syntactic tree cannot be
built over the sentence.
There are currently about 1900 syntac-
tic rules in the grammar which are encoded
through 70 syntactic formulae.
Small corpus of 600 phrases was syntactically
annotated by hand. We used this corpus to
measure the precision of the parsing algorithm
(Table 3).
We found that the precision of NP extraction
performed by the chart parser is higher than
the precision of the standalone NP extraction -
74.8% vs. 63.5% while the recall improves by
only 0.9% - 77.9% vs. 77% .
The syntactic ambiguity is resolved using syn-
tactic verb frames and heuristics, similar to the
ones described in (Allen, 1995).
The parser reaches its best performance for
NPs (74.8% precision and 77.9% recall) and low-
est for VPs (33% precision, 26% recall) and
Ss (20% precision and 5.9% recall) (Table 3).
The overall (measured on all the 600 syntac-
tic phrases) precision and recall, are 64.9% and
60.5% respectively. This is about 20% lower,
compared with certain English parsers (Murat
and Charniak, 1995), which is due to the in-
sufficient grammar coverage, as well as the lack
of reliable disambiguation algorithm. However
the bracket crossing accuracy is 80%, which is
comparable to some probabilistic approaches. It
should be noted that in our experiments we re-
stricted the maximal number of arcs up to 35000
per sentence to speed up the parsing.
3 Anaphora resolution in Bulgarian
3.1 Adaptation of Mitkovs
knowledge-poor approach for
Bulgarian
The anaphora resolution module is imple-
mented as the last stage of the language pro-
cessing architecture (Figure 1). This module re-
solves third-person personal pronouns and is an
adaptation of Mitkov?s robust, knowledge-poor
multilingual approach (Mitkov, 1998) whose lat-
est implementation by R. Evans is referred to
as MARS 2 (Orasan et al, 2000). MARS does
not make use of parsing, syntactic or seman-
tic constraints; nor does it employ any form
of non-linguistic knowledge. Instead, the ap-
proach relies on the efficiency of sentence split-
ting, part-of-speech tagging, noun phrase iden-
tification and the high performance of the an-
tecedent indicators; knowledge is limited to a
small noun phrase grammar, a list of (indicat-
ing) verbs and a set of antecedent indicators.
The core of the approach lies in activating the
antecedent indicators after filtering candidates
(from the current and two preceding sentences)
on the basis of gender and number agreement
and the candidate with the highest composite
score is proposed as antecedent 3. Before that,
the text is pre-processed by a sentence split-
ter which determines the sentence boundaries, a
part-of-speech tagger which identifies the parts
of speech and a simple phrasal grammar which
detects the noun phrases. In the case of com-
plex sentences, heuristic ?clause identification?
rules track the clause boundaries.
LINGUA performs the pre-processing,
needed as an input to the anaphora resolution
algorithm: sentence, paragraph and clause
splitters, NP grammar, part-of-speech tagger,
2MARS stands for Mitkov?s Anaphora Resolution
System.
3For a detailed procedure how candidates are handled
in the event of a tie, see (Mitkov, 1998).
Text Pronouns Weight set
Standard Optimised Baseline
most recent
Software manuals Success rate 221 75.0% 78.8% 58.0%
Critical succ. rate 70.0% 73.0% 54.0%
Non trivial succ. rate 70.0% 78.8% 58.0%
Tourist guides Success rate 116 68.1% 69.8% 65.0%
Critical succ. rate 63.3% 64.4% 58.8%
Non trivial succ. rate 67.2% 69.0% 65.0%
All texts Success rate 337 72.6% 75.7% 60.4%
Critical succ. rate 67.7% 70.0% 55.7%
Non trivial succ. rate 72.3% 75.4% 60.4%
Table 1: Success rate of anaphora resolution
section heading identification heuristics. Since
one of the indicators that Mitkov?s approach
uses is term preference, we manually developed4
a small term bank containing 80 terms from
the domains of programming languages, word
processing, computer hardware and operating
systems 5. This bank additionally featured 240
phrases containing these terms.
The antecedent indicators employed in
MARS are classified as boosting (such indica-
tors when pointing to a candidate, reward it
with a bonus since there is a good probability
of it being the antecedent) or impeding (such in-
dicators penalise a candidate since it does not
appear to have high chances of being the an-
tecedent). The majority of indicators are genre-
independent and are related to coherence phe-
nomena (such as salience and distance) or to
structural matches, whereas others are genre-
specific (e.g. term preference, immediate refer-
ence, sequential instructions). Most of the indi-
cators have been adopted in LINGUA without
modification from the original English version
(see (Mitkov, 1998) for more details). How-
ever, we have added 3 new indicators for Bul-
garian: selectional restriction pattern, adjectival
NPs and name preference.
The boosting indicators are
First Noun Phrases: A score of +1 is assigned
to the first NP in a sentence, since it is deemed
4This was done for experimental purposes. In future
applications, we envisage the incorporation of automatic
term extraction techniques.
5Note that MARS obtains terms automatically using
TF.IDF.
to be a good candidate for the antecedent.
Indicating verbs: A score of +1 is assigned
to those NPs immediately following the verb
which is a member of a previously defined set
such as discuss, present, summarise etc.
Lexical reiteration: A score of +2 is assigned
those NPs repeated twice or more in the
paragraph in which the pronoun appears, a
score of +1 is assigned to those NP, repeated
once in the paragraph.
Section heading preference: A score of +1 is
assigned to those NPs that also appear in the
heading of the section.
Collocation match: A score of +2 is assigned
to those NPs that have an identical collocation
pattern to the pronoun.
Immediate reference: A score of +2 is as-
signed to those NPs appearing in constructions
of the form ? ...V1 NP < CB > V2 it ? , where
< CB > is a clause boundary.
Sequential instructions: A score of +2 is
applied to NPs in the NP1 position of con-
structions of the form: ?To V1 NP1 ... To V2 it
...?
Term preference: a score of +1 is applied to
those NPs identified as representing domain
terms.
Selectional restriction pattern: a score of
Text Pronouns Intrasentential: Average Average Average Average
Intersentential number of distance from distance from distance from
anaphors candidates the antecedent the antecedent the antecedent
per anaphor in clauses in sentences in NP
Sofware 221 106 : 115 3.29 1.10 0.62 3.30
manuals
Tourist 116 17 : 99 3.35 1.74 0.98 5.13
guides
Table 2: Complexity of the evaluation data
+2 is applied to noun phrases occurring in
collocation with the verb preceding or following
the anaphor. This preference is different
from the collocation match preference in that
it operates on a wider range of ?selectional
restriction patterns? associated with a specific
verb 6 and not on exact lexical matching. If
the verb preceding or following the anaphor
is identified to be in a legitimate collocation
with a certain candidate for antecedent, that
candidate is boosted accordingly. As an il-
lustration, assume that ?Delete file? has been
identified as a legitimate collocation being a
frequent expression in a domain specific corpus
and consider the example ?Make sure you save
the file in the new directory. You can now
delete it. ? Whereas the ?standard? collocation
match will not be activated here, the selectional
restriction pattern will identify ?delete file? as
an acceptable construction and will reward the
candidate ?the file?.
Adjectival NP: a score of +1 is applied to
NPs which contain adjectives modifying the
head. Empirical analysis shows that Bulgarian
constructions of that type are more salient
than NPs consisting simply of a noun. Recent
experiments show that the success rate of the
anaphora resolution is improved by 2.20%,
using this indicator. It would be interesting
to establish if this indicator is applicable for
English.
Name preference: a score +2 is applied to
names of entities (person, organisation, product
6At the moment these patterns are extracted from a
list of frequent expressions involving the verb and do-
main terms in a purpose-built term bank but in gener-
ally they are automatically collected from large domain-
specific corpora.
names).
The impeding indicator is Prepositional
Noun Phrases: NPs appearing in prepositional
phrases are assigned a score of -1.
Two indicators, Referential distance and
Indefiniteness may increase or decrease a
candidate?s score.
Referential distance gives scores of +2 and
+1 for the NPs in the same and in the previous
sentence respectively, and -1 for the NPs two
sentences back. This indicator has strong influ-
ence on the anaphora resolution performance,
especially in the genre of technical manuals.
Experiments show that its switching off can
decrease the success rate by 26% .
Indefiniteness assigns a score of -1 to indefi-
nite NPs, 0 to the definite (not full article) and
+1 to these which are definite, containing the
definite ?full? article in Bulgarian.
4 Evaluation of the anaphora
resolution module
The precision of anaphora resolution measured
on corpus of software manuals containing 221
anaphors, is 75.0%. Given that the anaphora
resolution system operates in a fully automatic
mode, this result could be considered very
satisfactory. It should be noted that some of
the errors arise from inaccuracy of the pre-
processing modules such as clause segmentation
and NP extraction (see Table 3).
We also evaluated the anaphora resolution
system in the genre of tourist texts. As ex-
pected, the success rate dropped to 68.1%
which, however, can still be regarded as a very
Language processing module Precision % Recall % Evaluation data
sentence splitter 92.00 99.00 190 sentences
paragraph splitter 94.00 98.00 268 paragraphs
clause chunker 93.50 93.10 232 clauses
POS tagger 95.00 95.00 303 POS tags
NP extractor 63.50 77.00 352 NPs
chart parsing
NP 74.84 77.89 294 NPs
AP 65.15 67.19 64 APs
AdvP 37.14 50.00 26 AdvPs
VP 33.33 26.39 72 VPs
PP 70.00 60.21 93 PPs
S 20.00 5.88 51 Ss
Total 64.93 60.50 600 phrases
Bracket crossing accuracy 80.33 - 600 phrases
Anaphora resolution 72.60 - 337 anaphors
Table 3: Summary of LINGUA performance
good result, given the fact that neither man-
ual pre-editing of the input text, nor any post-
editing of the output of the pre-processing tools
were undertaken. The main reason for the de-
cline of performance is that some of the origi-
nal indicators such as term preference, immedi-
ate reference and sequential instructions of the
knowledge-poor approach, are genre specific.
The software manuals corpus featured 221
anaphoric third person pronouns, whereas the
tourist text consisted of 116 such pronouns. For
our evaluation we used the measures success
rate, critical success rate and non-trivial suc-
cess rate (Mitkov, 2001). Success rate is the
ratio SR = AC/A, where AC is the number of
correctly resolved and A is the number of all
anaphors. Critical success rate is the success
rate for the anaphors which have more than one
candidates for antecedent after the gender and
number agreement filter is applied. Non-trivial
success rate is calculated for those anaphors
which have more than one candidates for an-
tecedent before the gender and number agree-
ment is applied. We also compared our ap-
proach with the typical baseline model Baseline
most recent which takes as antecedent the most
recent NP matching the anaphor in gender and
number. The results are shown in the Table 1.
These results show that the performance of
LINGUA in anaphora resolution is comparable
to that of MARS (Orasan et al, 2000). An opti-
mised version 7 of the indicator weights scored a
success rate of 69,8% on the tourist guide texts,
thus yielding an improvement of 6,1%.
Table 2 illustrates the complexity of the eval-
uation data by providing simple quantifying
measures such as average number of candi-
dates per anaphor, average distance from the
anaphor to the antecedent in terms of sentences,
clauses, intervening NPs, number of intrasen-
tential anaphors as opposed to intersentential
ones etc.
5 Conclusion
This paper outlines the development of the
first robust and shallow text processing frame-
work in Bulgarian LINGUA which includes
modules for tokenisation, sentence splitting,
paragraph segmentation, part-of-speech tag-
ging, clause chunking, noun phrases extraction
and anaphora resolution (Figure 1). Apart
from the module on pronoun resolution which
was adapted from Mitkov?s knowledge-poor ap-
proach for English and the incorporation of
BULMORPH in the part-of-speech tagger, all
modules were specially built for LINGUA. The
evaluation shows promising results for each of
the modules.
7The optimisation made use of genetic algorithms in
a manner similar to that described in (Orasan et al,
2000).
References
J. Allen. 1995. Natural Language Understand-
ing. The Benjamin/Cummings Publishing
Company, Inc.
T. Avgustinova, K. Oliva, and E. Paskaleva.
1989. An HPSG-based parser for bulgar-
ian. In International Seminar on Machine
Translation ?Computer and Translation 89?,
Moscow, Russia.
P. Barkalova. 1997. Bulgarian syntax - known
and unknown. Plovdiv University Press,
Plovdiv. in Bulgarian.
H. Krushkov. 1997. Modelling and building of
machine dictionaries and morphological pro-
cessors. Ph.D. thesis, University of Plovdiv.
in Bulgarian.
R. Mitkov. 1998. Robust pronoun reso-
lution with limited knowledge. In Pro-
ceedings of the 18.th International Confer-
ence on Computational Linguistics (COL-
ING?98)/ACL?98 Conference, pages 869?875,
Montreal,Canada.
R. Mitkov. 2001. Towards a more consistent
and comprehensive evaluation of anaphora
resolution algorithms and systems. Towards
a more consistent and comprehensive evalu-
ation of anaphora resolution algorithms and
systems, (15):253?276.
E. Murat and E. Charniak. 1995. A statistical
syntactic disambiguation program and what
it learns. CS, 29-95.
C. Orasan, R. Evans, and R. Mitkov. 2000.
Enhancing preference-based anaphora resolu-
tion with genetic algorithms. In Proceedings
of NLP?2000, Patras, Greece.
J. Penchev. 1993. Bulgarian Syntax - Govern-
ment and Binding. Plovdiv University Press,
Plovdiv. in Bulgarian.
K. Simov, E. Paskaleva, M. Damova, and
M. Slavcheva. 1992. Morpho-assistant - a
knowledge based system for bulgarian mor-
phology. In Proceedings of the Third Confer-
ence on Applied Natural Language Process-
ing, Trento, Italy.
G. Totkov and Ch. Tanev. 1999. Computerized
extraction of word semantics through con-
nected text analysis. In Proc. of the Interna-
tional Workshop DIALOGUE ?99, pages 360
? 365.
A. Voutilainen. 1995. A syntax-based part-of-
speech tagger. In Proceedings of the 7th con-
ference of the European Chapter of EACL,
Dublin, Ireland.
Weakly Supervised Approaches for Ontology Population
Hristo Tanev Tanev
ITC-irst
38050, Povo
Trento, Italy
htanev@yahoo.co.uk
Bernardo Magnini
ITC-irst
38050, Povo
Trento, Italy
magnini@itc.it
Abstract
We present a weakly supervised approach
to automatic Ontology Population from
text and compare it with other two unsu-
pervised approaches. In our experiments
we populate a part of our ontology of
Named Entities. We considered two high
level categories - geographical locations
and person names and ten sub-classes for
each category. For each sub-class, from
a list of training examples and a syntac-
tically parsed corpus, we automatically
learn a syntactic model - a set of weighted
syntactic features, i.e. words which typ-
ically co-occur in certain syntactic posi-
tions with the members of that class. The
model is then used to classify the unknown
Named Entities in the test set. The method
is weakly supervised, since no annotated
corpus is used in the learning process. We
achieved promising results, i.e. 65% accu-
racy, outperforming significantly previous
unsupervised approaches.
1 Introduction
Automatic Ontology Population (OP) from texts
has recently emerged as a new field of application
for knowledge acquisition techniques (see, among
others, (Buitelaar et al, 2005)). Although there
is no a univocally accepted definition for the OP
task, a useful approximation has been suggested
(Bontcheva and Cunningham, 2003) as Ontology
Driven Information Extraction, where, in place of
a template to be filled, the goal of the task is the ex-
traction and classification of instances of concepts
and relations defined in a Ontology. The task has
been approached in a variety of similar perspec-
tives, including term clustering (e.g. (Lin, 1998a)
and (Almuhareb and Poesio, 2004)) and term cat-
egorization (e.g. (Avancini et al, 2003)).
A rather different task is Ontology Learning
(OL), where new concepts and relations are sup-
posed to be acquired, with the consequence of
changing the definition of the Ontology itself (see,
for instance, (Velardi et al, 2005)).
In this paper OP is defined in the following sce-
nario. Given a set of terms T = t1, t2, ..., tn, a
document collection D, where terms in T are sup-
posed to appear, and a set of predefined classes
C = c1, c2, ..., cm denoting concepts in an Ontol-
ogy, each term ti has to be assigned to the proper
class in C. For the purposes of the experiments
presented in this paper we assume that (i) classes
in C are mutually disjoint and (ii) each term is as-
signed to just one class.
As we have defined it, OP shows a strong sim-
ilarity with Named Entity Recognition and Clas-
sification (NERC). However, a major difference is
that in NERC each occurrences of a recognized
term has to be classified separately, while in OP it
is the term, independently of the context in which
it appears, that has to be classified.
While Information Extraction, and NERC in
particular, have been addressed prevalently by
means of supervised approaches, Ontology Popu-
lation is typically attacked in an unsupervised way.
As many authors have pointed out (e.g. (Cimiano
and Vo?lker, 2005)), the main motivation is the fact
that in OP the set of classes is usually larger and
more fine grained than in NERC (where the typ-
ical set includes Person, Location, Organization,
GPE, and a Miscellanea class for all other kind
of entities). In addition, by definition, the set of
classes in C changes as a new ontology is consid-
ered, making the creation of annotated data almost
impossible practically.
17
According with the demand for weakly super-
vised approaches to OP, we propose a method,
called Class ? Example, which learns a classi-
fication model from a set of classified terms, ex-
ploiting lexico-syntactic features. Unlike most of
the approaches which consider pair wise similarity
between terms ((Cimiano and Vo?lker, 2005); (Lin,
1998a)), the Class-Example method considers the
similarity between a term ti and a set of training
examples which represent a certain class. This re-
sults in a great number of class features and opens
the possibility to exploit more statistical data, such
as the frequency of appearance of a class feature in
different training terms.
In order to show the effectiveness of the Class-
Example approach, it has been compared against
two different approaches: (i) aClass-Pattern unsu-
pervised approach, in the style of (Hearst, 1998);
(ii) an unsupervised approach that considers the
word of the class as a pivot word for acquiring
relevant contexts for the class (we refer to this
method as Class?Word). Results of the compar-
ison show that the Class-Example method outper-
forms significantly the other two methods, making
it appealing even considering the need of supervi-
sion.
Although the Class-Example method we pro-
pose is applicable in general, in this paper we
show its usefulness when applied to terms denot-
ing Named Entities. The motivation behind this
choice is the practical value of Named Entity clas-
sifications, as, for instance, in applications such as
Questions Answering and Information Extraction.
Moreover, some Named Entity classes, including
names of writers, athletes and organizations, dy-
namically change over the time, which makes it
impossible to capture them in a static Ontology.
The rest of the paper is structured as follows.
Section 2 describes the state-of-the-art methods in
Ontology Population. Section 3 presents the three
approaches to the task we have compared. Section
4 introduces Syntactic Network, a formalism used
for the representation of syntactic information and
exploited in both the Class-Word and the Class-
Example approaches. Section 5 reports on the
experimental settings, results obtained, and dis-
cusses the three approaches. Section 6 concludes
the paper and suggests directions for future work.
2 Related Work
There are two main paradigms distinguishing On-
tology Population approaches. In the first one
Ontology Population is performed using patterns
(Hearst, 1998) or relying on the structure of terms
(Velardi et al, 2005). In the second paradigm the
task is addressed using contextual features (Cimi-
ano and Vo?lker, 2005).
Pattern-based approaches search for phrases
which explicitly show that there is an ?is-a? re-
lation between two words, e.g. ?the ant is an in-
sect? or ?ants and other insects?. However, such
phrases do not appear frequently in a text cor-
pus. For this reason, some approaches use theWeb
(Schlobach et al, 2004). (Velardi et al, 2005) ex-
perimented several head-matching heuristics ac-
cording to which if a term1 is in the head of
term2, then there is an ?is-a? relation between
them: For example ?Christmas tree? is a kind of
?tree?.
Context feature approaches use a corpus to ex-
tract features from the context in which a se-
mantic class tends to appear. Contextual features
may be superficial (Fleischman and Hovy, 2002)
or syntactic (Lin, 1998a), (Almuhareb and Poe-
sio, 2004). Comparative evaluation in (Cimiano
and Vo?lker, 2005) shows that syntactic features
lead to better performance. Feature weights can
be calculated either by Machine Learning algo-
rithms (Fleischman and Hovy, 2002) or by statisti-
cal measures, like Point Wise Mutual Information
or the Jaccard coefficient (Lin, 1998a).
A hybrid approach using both pattern-based,
term structure, and contextual feature methods is
presented in (Cimiano et al, 2005).
State-of-the-art approaches may be divided in
two classes, according to different use of train-
ing data: Unsupervised approaches (see (Cimi-
ano et al, 2005) for details) and supervised ap-
proaches which use manually tagged training data,
e.g. (Fleischman and Hovy, 2002). While state-
of-the-art unsupervised methods have low perfor-
mance, supervised approaches reach higher ac-
curacy, but require the manual construction of a
training set, which impedes them from large scale
applications.
3 Weakly supervised approaches for
Ontology Population
In this Section we present three Ontology Popula-
tion approaches. Two of them are unsupervised:
18
(i) a pattern-based approach described in (Hearst,
1998), which we refer to as Class-Pattern and (ii)
a feature similarity method reported in (Cimiano
and Vo?lker, 2005) to which we will refer as Class-
Word. Finally, we describe a new weakly super-
vised approach for ontology population which ac-
cepts as a training data lists of instances for each
class under consideration. This method we call
Class-Example.
3.1 Class-Pattern approach
This approach was described first in (Hearst,
1998). The main idea is that if a term t belongs
to a class c, then in a text corpus we may expect
the occurrence of phrases like such c as t,.... In our
experiments for ontology population we used the
patterns described in the Hearst?s paper plus the
pattern t is (a | the) c:
1. t is (a | the) c
2. such c as t
3. such c as (NP,)*, (and | or) t
4. t (,NP)* (and | or) other c
5. c, (especially | including) (NP, )* t
For each instance from the test set t and for each
concept c we instantiated the patterns and searched
with them in the corpus. If a pattern which is in-
stantiated with a concept c and a term t appears
in the corpus, then we assume the t belongs to c.
For example, if the term to be classified is ?Etna?
and the concept is ?mountain?, one of the instan-
tiated patterns will be ?mountains such as Etna?;
if this pattern is found in the text, then ?Etna? is
considered to be a ?mountain?. If the algorithm
assigns a term to several categories, we choose the
one which co-occurs most often with the term.
3.2 Class-Word approach
(Cimiano and Vo?lker, 2005) describes an unsu-
pervised approach for ontology population based
on vector-feature similarity between each concept
c and a term to be classified t. For example,
in order to conclude how much ?Etna? is an ap-
propriate instance of the class ?mountain?, this
method finds the feature-vector similarity between
the word ?Etna? and the word ?mountain?. Each
instance from the test set T is assigned to one of
the classes in the set C. Features are collected
from Corpus and the classification algorithm on
classify(T , C, Corpus)
foreach(t in T ) do{
vt = getContextV ector(t, Corpus);}
foreach(c in C) do{
vc = getContextV ector(c, Corpus);}
foreach(t in T ) do{
classes[t] = argmaxc?Csim(vt, vc);}
return classes[];
end classify
Figure 1: Unsupervised algorithm for Ontology
Population.
figure 1 is applied. The problem with this ap-
proach is that the context distribution of a name
(e.g. ?Etna?) is sometimes different than the con-
text distribution of the class word (e.g. ?moun-
tain?). Moreover, a single word provides a limited
quantity of contextual data.
In this algorithm the context vectors vt and
vc are feature vectors whose elements represent
weighted context features from Corpus of the
term t (e.g. ?Etna?) or the concept word c (e.g.
?mountain?). Cimiano and Vo?lker evaluate differ-
ent context features and prove that syntactic fea-
tures work best. Therefore, in our experimen-
tal settings we considered only such features ex-
tracted from a corpus parsed with a dependency
parser. Unlike the original approach which relies
on pseudo-syntactic features, we used features ex-
tracted from dependency parse trees. Moreover,
we used virtually all the words connected syntacti-
cally to a term, not only the modifiers. A syntactic
feature is a pair: (word, syntactic relation) (Lin,
1998a). We use two feature types: First order fea-
tures, which are directly connected to the training
or test examples in the dependency parse trees of
Corpus; second order features, which are con-
nected to the training or test instances indirectly
by skipping one word (the verb) in the dependency
tree. As an example, let?s consider two sentences:
?Edison invented the phonograph? and ?Edison
created the phonograph?. If ?Edison? is a name
to be classified, then two first order features of this
name exist - (?invent?, subject-of) and (?create?,
subject-of). One second order feature can be ex-
tracted - (?phonograph?, object-of+subject); it co-
occurs two times with the word ?Edison?. In our
experiments second order features are considered
only those words which are governed by the same
verb whose subject is the name which is a training
19
or test instance (in this example ?Edison?).
3.3 Weakly Supervised Class-Example
Approach
The approach we put forward here uses the same
processing stages as the one presented in Fig-
ure 1 and relies on syntactic features extracted
from a corpus. However, the Class-Example al-
gorithm receives as an additional input parame-
ter the sets of training examples for each class
c ? C. These training sets are simple lists of
instances (i.e. terms denoting Named Entities),
without context, and can be acquired automati-
cally or semi-automatically from an existing on-
tology or gazetteer. To facilitate their acquisition,
the Class-Example approach imposes no restric-
tions to the training examples - they can be am-
biguous and have different frequencies. However,
they have to appear in Corpus (in our experimen-
tal settings - at least twice). For example, for the
class ?mountain? training examples are: ?Ever-
est?, ?Mauna Loa?, etc.
The algorithm learns from each training set
Train(c) a single feature vector vc, called the syn-
tactic model of the class. Therefore, in our algo-
rithm, the statement
vc = getContextV ector(c, Corpus)
in Figure 1 is substituted with
vc = getSyntacticModel(Train(c), Corpus).
For each class c, a set of syntactic features F (c)
are collected by finding the union of the features
extracted from each occurrence in the corpus of
each training instance in Train(c). Next, the fea-
ture vector vc is constructed: If a feature is not
present in F (c), then its corresponding coordinate
in vc has value 0; otherwise, it has a value equal to
the feature weight.
The weight of a feature fc ? F (c) is calculated
in three steps:
1. First, the co-occurrence of fc with the train-
ing set is calculated:
weight1(fc) =
?
t?Train(c)
?.log( P (fc, t)P (fc).P (t)
)
where P (fc, t) is the probability that feature
fc co-occurs with t, P (fc) and P (t) are the
probabilities that fc and t appear in the cor-
pus, ? = 14 for syntactic features with lexi-
cal element noun and ? = 1 for all the other
syntactic features. The ? parameter reflects
the linguistic intuition that nouns are more in-
formative than verbs and adjectives which in
most cases represent generic predicates. The
values of ? were automatically learned from
the training data.
2. We normalize the feature weights, since we
observed that they vary a lot between dif-
ferent classes: for each class c we find the
feature with maximal weight and denote its
weight with mxW (c),
mxW (c) = maxfc?F (c)weight1(fc)
Next, the weight of each feature fc ? F (c) is
normalized by dividing it with mxW (c):
weightN (fc) =
weight1(fc)
mxW (c)
3. To obtain the final weight of fc, we divide
weightN (fc) by the number of classes in
which this feature appears. This is motivated
by the intuition that a feature which appears
in the syntactic models of many classes is not
a good class predictor.
weight(fc) =
weightN (fc)
|Classes(fc)|
where |Classes(fc)| is the number of classes
for which fc is present in the syntactic model.
As shown in Figure 1, the classification uses a
similarity function sim(vt, vc) whose arguments
are the feature vector of a term vt and the feature
vector vc for a class c. We defined the similarity
function as the dot product of the two feature vec-
tors: sim(vt, vc) = vc.vt. Vectors vt are binary
(i.e. the feature value is 1 if the feature is present
and, 0-otherwise), while the features in the syntac-
tic model vectors vc receive weights according to
the approach described in this Section.
4 Representing Syntactic Information
Since both the Class-Word and the Class-Example
methods work with syntactic features, the main
source of information is a syntactically parsed cor-
pus. We parsed about half a gigabyte of a news
corpus with MiniPar (Lin, 1998b). It is a statis-
tically based dependency parser which is reported
to reach 89% precision and 82% recall on press re-
portage texts. MiniPar generates syntactic depen-
dency structures - directed labeled graphs whose
20
g1 g2 SyntNet(g1, g2)
loves|1
s
??
o
%%JJ
J
J
J
J
J
J
J
J
J
loves|4 o //
s
??
Jane|6 loves|1,4
(1,2)(4,5)
??
(4,6)
o
//
(1,3)
o
**TTT
T
T
T
T
T
T
T
T
T
T
T
T
T
T
Jane|6
John|2 Mary|3 John|5 John|2,5 Mary|3
Figure 2: Two syntactic graphs and their Syntactic Network.
vertices represent words and the edges between
them represent syntactic relations like subject, ob-
ject, modifier, etc. Examples for two dependency
structures - g1 and g2, are shown in Figure 2:
They represent the sentences ?John loves Mary?
and ?John loves Jane?; labels s and o on their
edges stand for subject and object respectively.
The syntactic structures generated by MiniPar are
dendroid (tree-like), but still cycles appear in some
cases.
In order to extract information from the parsed
corpus, we had to choose a model for represent-
ing dependency trees which allows to search ef-
ficiently for syntactic structures and to calculate
their frequencies. Building a classic index at word
level was not an option, since we have to search for
syntactic structures, not words. On the other hand,
indexing syntactic relations (i.e. word pair and the
relation between the words) would be useful, but
still does not resolve the problem, since in many
cases we search for more complex structures than
a relation between two words: for example, when
we have to find which words are syntactically re-
lated to a Named Entity composed by two words,
we have to search for syntactic structures which
consists of three vertices and two edges.
In order to trace efficiently more complex struc-
tures in the corpus, we put forward a model for
representation of a set of labeled graphs, called
Syntactic Network (SyntNet for short). The model
is inspired by a model presented earlier in (Szpek-
tor et al, 2004), however our model allows more
efficient construction of the representation. The
scope of SyntNet is to represent a set of labeled
graphs through one aggregate structure in which
the isomorphic sub-structures overlap. When
SyntNet represents a syntactically parsed text cor-
pus, its vertices are labeled with words from the
text while edges represent syntactic relations from
the corpus and are labeled accordingly.
An example is shown in Figure 2, where two
syntactic graphs, g1 and g2, are merged into
one aggregate representation SyntNet(g1, g2).
Vertices labeled with equal words in g1 and
g2 are merged into one generalizing vertex in
SyntNet(g1, g2). For example, the vertices with
label John in g1 and g2 are merged into one vertex
John in SyntNet(g1, g2).
Edges are merged in a similar way:
(loves, John) ? g1 and (loves, John) ? g2
are represented through one edge (loves, John)
in SyntNet(g1, g2).
Each vertex in g1 and g2 is labeled addition-
ally with a numerical index which is unique
for the graph set. Numbers on vertices in
SyntNet(g1, g2) show which vertices from g1
and g2 are merged in the corresponding Synt-
Net vertices. For example, vertex loves ?
SyntNet(g1, g2) has a set {1, 4} which means
that vertices 1 and 4 are merged in it. In a similar
way the edge (loves, John) ? SyntNet(g1, g2)
is labeled with two pairs of indices (4, 5) and
(1, 2), which shows that it represents two edges:
the edge between vertices 4 and 5 and the edge
between 1 and 2.
Two properties of SyntNet are important: first
isomorphic sub-structures from all the graphs rep-
resented via a SyntNet are mapped into one struc-
ture. This allows us to easily find all the oc-
currences of multiword terms and named enti-
ties. Second, using the numerical indices on ver-
tices and edges, we can efficiently calculate which
structures are connected syntactically to the train-
ing and test terms. As an example, let?s try to cal-
culate in which constructions the word ?Mary? ap-
pears considering SyntNet in Figure 2. First, in
SyntNet we can directly observe that there is the
relation loves?Mary labeled with the pair 1 ? 3
- therefore this relation appears once in the corpus.
Next, tracing the numerical indices on the ver-
tices and edges we can find a path from ?Mary? to
?John? through ?loves?. The path passes through
the following numerical indices: 3 ? 1 ? 2: this
means that there is one appearance of the structure
21
?John lovesMary? in the corpus, spanning through
vertices 1, 2, and 3. Such a path through the nu-
merical indices cannot be found between ?Mary?
and ?Jane? which means that they do not appear in
the same syntactic construction in the corpus.
SyntNet is built incrementally in a straightfor-
ward manner: Each new vertex or edge added to
the network is merged with the identical vertex or
edge, if such already exists in SyntNet. Otherwise,
a new vertex or edge is added to the network. The
time necessary for building a SyntNet is propor-
tional to the number of the vertices and the edges
in the represented graphs (and does not otherwise
depend on their complexity).
The efficiency of the SyntNet model when
representing and searching for labeled structures
makes it very appropriate for the representation of
a syntactically parsed corpus. We used the prop-
erties of SyntNet in order to trace efficiently the
occurrences of Named Entities in the parsed cor-
pus, to calculate their frequencies, to find the syn-
tactic features which co-occur with these Named
Entities, as well as the frequencies of these co-
occurrences. Moreover, the SyntNet model al-
lowed us to extract more complex, second order
syntactic features which are connected indirectly
to the terms in the training and the test set.
5 Experimental settings and results
We have evaluated all the three approaches de-
scribed in Section 3. The same evaluation settings
were used for the three experiments. The source
of features was a news corpus of about half a gi-
gabyte. The corpus was parsed with MiniPar and
a Syntactic Network representation was built from
the dependency parse trees produced by the parser.
Syntactic features were extracted from this Synt-
Net.
We considered two high-level Named Entity
categories: Locations and Persons. For each of
them five fine-grained sub-classes were taken into
consideration. For locations: mountain, lake,
river, city, and country; for persons: statesman,
writer, athlete, actor, and inventor.
For each class under consideration we created
a test set of Named Entities using WordNet 2.0
and Internet sites like Wikipedia. For the Class-
Example approach we also provided training data
using the same resources. WordNet was the pri-
mary data source for training and test data. The ex-
amples from it were extracted automatically. We
P (%) R (%) F (%)
mountain 58 78 67
lake 75 50 60
river 69 55 61
city 56 76 65
country 86 93 89
locations macro 69 70 68
locations micro 78 78 78
statesman 42 72 53
writer 93 55 69
athlete 90 47 62
actor 90 73 80
inventor 12 33 18
persons macro 65 56 57
persons micro 57 57 57
total macro 67 63 62
total micro 65 65 65
category location 83 91 87
category person 95 89 92
Table 1: Performance of the Class-Example ap-
proach.
used Internet to get additional examples for some
classes. To do this, we created automatic text ex-
traction scripts for Web pages and manually fil-
tered their output when it was necessary.
Totally, the test data comprised 280 Named En-
tities which were not ambiguous and appeared at
least twice in the corpus.
For the Class-Example approach we provided
a training set of 1194 names. The only require-
ment to the names in the training set was that
they appear at least twice in the parsed corpus.
They were allowed to be ambiguous and no man-
ual post-processing or filtering was carried out on
this data.
For both context feature approaches (i.e. Class-
Word and Class-Example), we used the same type
of syntactic features and the same classification
function, namely the one described in Section 3.3.
This was done in order to compare better the ap-
proaches.
Results from the comparative evaluation are
shown in Table 2. For each approach we mea-
sured macro average precision, macro average re-
call, macro average F-measure and micro average
F; for Class-Word and Class-Example micro F is
equal to the overall accuracy, i.e. the percent of the
instances classified correctly. The first row shows
22
macro P (%) macro R (%) macro F (%) micro F(%)
Class-Patterns 18 6 9 10
Class-Word 32 41 33 42
Class-Example 67 63 62 65
Class-Example (sec. ord.) 65 61 62 68
Table 2: Comparison of different approaches.
the results obtained with superficial patterns. The
second row presents the results from the Class-
Word approach. The third row shows the results
of our Class-Example method. The fourth line
presents the results for the same approach but us-
ing second-order features for the person category.
The Class-Pattern approach showed low perfor-
mance, similar to the random classification, for
which macro and micro F=10%. Patterns suc-
ceeded to classify correctly only instances of the
classes ?river? and ?city?. For the class ?city?
the patterns reached precision of 100% and recall
65%; for the class ?river? precision was high (i.e.
75%), but recall was 15%.
The Class-Word approach showed significantly
better performance (macro F=33%, micro F=42%)
than the Class-Pattern approach.
The performance of the Class-Example (62%
macro F and 65%-68% micro F) is much higher
than the performance of Class-Word (29% in-
crease in macro F and 23% in micro F). The last
row of the table shows that second-order syntactic
features augment further the performance of the
Class-Example method in terms of micro average
F (68% vs. 65%).
A more detailed evaluation of the Class-
Example approach is shown in Table 1. In this
table we show the performance of the approach
without the second-order features. Results vary
between different classes: The highest F is mea-
sured for the class ?country? - 89% and the low-
est is for the class ?inventor? - 18%. However,
the class ?inventor? is an exception - for all the
other classes the F measure is over 50%. Another
difference may be observed between the Location
and Person classes: Our approach performs sig-
nificantly better for the locations (68% vs. 57%
macro F and 78% vs. 57% micro F). Although
different classes had different number of training
examples, we observed that the performance for
a class does not depend on the size of its training
set. We think, that the variation in performance be-
tween categories is due to the different specificity
of their textual contexts. As a consequence, some
classes tend to co-occur with more specific syn-
tactic features, while for other classes this is not
true.
Additionally, we measured the performance
of our approach considering only the macro-
categories ?Location? and ?Person?. For this pur-
pose we did not run another experiment, we rather
used the results from the fine-grained classifica-
tion and grouped the already obtained classes. Re-
sults are shown in the last two rows of table 1: It
turns out that the Class-Example method makes
very well the difference between ?location? and
?person? - 90% of the test instances were classi-
fied correctly between these categories.
6 Conclusions and future work
In this paper we presented a new weakly super-
vised approach for Ontology Population, called
Class-Example, and confronted it with two other
methods. Experimental results show that the
Class-Example approach has best performance. In
particular, it reached 65% of accuracy, outper-
forming in our experimental framework the state-
of-the-art Class-Word method by 42%. Moreover,
for location names the method reached accuracy
of 78%. Although the experiments are not com-
parable, we would like to state that some super-
vised approaches for fine-grained Named Entity
classification, e.g. (Fleischman, 2001), have sim-
ilar accuracy. On the other hand, the presented
weakly supervised Class-Example approach re-
quires as a training data only a list of terms for
each class under consideration. Training exam-
ples can be automatically acquired from existing
ontologies or other sources, since the approach
imposes virtually no restrictions on them. This
makes our weakly supervised methodology appli-
cable on larger scale than supervised approaches,
still having significantly better performance than
the unsupervised ones.
In our experimental framework we used syntac-
tic features extracted from dependency parse trees
23
and we put forward a novel model for the repre-
sentation of a syntactically parsed corpus. This
model allows for performing a comprehensive ex-
traction of syntactic features from a corpus includ-
ing more complex second-order ones, which re-
sulted in an improvement of performance. This
and other empirical observations not described in
this paper lead us to the conclusion that the per-
formance of an Ontology Population system im-
proves with the increase of the types of syntactic
features under consideration.
In our future work we consider applying our
Ontology Population methodology to more se-
mantic categories and to experiment with other
types of syntactic features, as well as other types
of feature-weighting formulae and learning algo-
rithms. We consider also the integration of the
approach in a Question Answering or Information
Extraction system, where it can be used to perform
fine-grained type checking.
References
A. Almuhareb and M. Poesio. 2004. Attribute-
based and value-based clustering: An evaluation.
In Proceedings of EMNLP 2004, pages 158?165,
Barcelona, Spain.
H. Avancini, A. Lavelli, B. Magnini, F. Sebastiani, and
R. Zanoli. 2003. Expanding Domain-Specific Lex-
icons by Term Categorization. In Proceedings of
SAC 2003, pages 793?79.
K. Bontcheva and H. Cunningham. 2003. The Se-
mantic Web: A New Opportunity and Challenge for
HLT. In Proceedings of the Workshop HLT for the
Semantic Web and Web Services at ISWC 2003.
P. Buitelaar, P. Cimiano, and B. Magnini, editors.
2005. Ontology Learning from Text: Methods, Eval-
uation and Applications. IOS Press, Amsterdam,
The Netherlands.
P. Cimiano and J. Vo?lker. 2005. Towards large-scale,
open-domain and ontology-based named entity clas-
sification. In Proceedings of RANLP?05, pages 166?
172, Borovets, Bulgaria.
P. Cimiano, A. Pivk, L.S. Thieme, and S. Staab. 2005.
Learning Taxonomic Relations from Heterogeneous
Sources of Evidence. In Ontology Learning from
Text: Methods, Evaluation and Applications. IOS
Press.
M. Fleischman and E. Hovy. 2002. Fine Grained
Classification of Named Entities. In Proceedings of
COLING 2002, Taipei, Taiwan, August.
M. Fleischman. 2001. Automated Subcategorization
of Named Entities. In 39th Annual Meeting of the
ACL, Student Research Workshop, Toulouse, France,
July.
M. Hearst. 1998. Automated Discovery of Word-
Net Relations. In WordNet: An Electronic Lexical
Database. MIT Press.
D. Lin. 1998a. Automatic Retrieval and Clustering of
Similar Words. In Proceedings of COLING-ACL98,
Montreal, Canada, August.
D. Lin. 1998b. Dependency-based Evaluation of Mini-
Par. In Proceedings of Workshop on the Evaluation
of Parsing Systems, Granada, Spain.
S. Schlobach, M. Olsthoorn, and M. de Rijke. 2004.
Type Checking in Open-Domain Question Answer-
ing. In Proceedings of ECAI 2004.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004.
Scaling Web-based Acquisition of Entailment Rela-
tions. In Proceedings of EMNLP 2004, Barcelona,
Spain.
P. Velardi, R.Navigli, A. Cuchiarelli, and F.Neri. 2005.
Evaluation of Ontolearn, a Methodology for Auto-
matic Population of Domain Ontologies. In P. Buite-
laar, P. Cimiano, and B. Magnini, editors, Ontology
Learning from Text: Methods, Evaluation and Ap-
plications. IOS Press.
24
Is It the Right Answer?
Exploiting Web Redundancy for Answer Validation
Bernardo Magnini, Matteo Negri, Roberto Prevete and Hristo Tanev
ITC-Irst, Centro per la Ricerca Scientifica e Tecnologica
[magnini,negri,prevete,tanev]@itc.it
Abstract
Answer Validation is an emerging topic
in Question Answering, where open do-
main systems are often required to rank
huge amounts of candidate answers. We
present a novel approach to answer valida-
tion based on the intuition that the amount
of implicit knowledge which connects an
answer to a question can be quantitatively
estimated by exploiting the redundancy of
Web information. Experiments carried out
on the TREC-2001 judged-answer collec-
tion show that the approach achieves a
high level of performance (i.e. 81% suc-
cess rate). The simplicity and the effi-
ciency of this approach make it suitable to
be used as a module in Question Answer-
ing systems.
1 Introduction
Open domain question-answering (QA) systems
search for answers to a natural language question
either on the Web or in a local document collec-
tion. Different techniques, varying from surface pat-
terns (Subbotin and Subbotin, 2001) to deep seman-
tic analysis (Zajac, 2001), are used to extract the text
fragments containing candidate answers. Several
systems apply answer validation techniques with the
goal of filtering out improper candidates by check-
ing how adequate a candidate answer is with re-
spect to a given question. These approaches rely
on discovering semantic relations between the ques-
tion and the answer. As an example, (Harabagiu
and Maiorano, 1999) describes answer validation as
an abductive inference process, where an answer is
valid with respect to a question if an explanation for
it, based on background knowledge, can be found.
Although theoretically well motivated, the use of se-
mantic techniques on open domain tasks is quite ex-
pensive both in terms of the involved linguistic re-
sources and in terms of computational complexity,
thus motivating a research on alternative solutions
to the problem.
This paper presents a novel approach to answer
validation based on the intuition that the amount of
implicit knowledge which connects an answer to a
question can be quantitatively estimated by exploit-
ing the redundancy of Web information. The hy-
pothesis is that the number of documents that can
be retrieved from the Web in which the question and
the answer co-occur can be considered a significant
clue of the validity of the answer. Documents are
searched in the Web by means of validation pat-
terns, which are derived from a linguistic process-
ing of the question and the answer. In order to test
this idea a system for automatic answer validation
has been implemented and a number of experiments
have been carried out on questions and answers pro-
vided by the TREC-2001 participants. The advan-
tages of this approach are its simplicity on the one
hand and its efficiency on the other.
Automatic techniques for answer validation are
of great interest for the development of open do-
main QA systems. The availability of a completely
automatic evaluation procedure makes it feasible
QA systems based on generate and test approaches.
In this way, until a given answer is automatically
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 425-432.
                         Proceedings of the 40th Annual Meeting of the Association for
proved to be correct for a question, the system will
carry out different refinements of its searching crite-
ria checking the relevance of new candidate answers.
In addition, given that most of the QA systems rely
on complex architectures and the evaluation of their
performances requires a huge amount of work, the
automatic assessment of the relevance of an answer
with respect to a given question will speed up both
algorithm refinement and testing.
The paper is organized as follows. Section 2
presents the main features of the approach. Section 3
describes how validation patterns are extracted from
a question-answer pair by means of specific question
answering techniques. Section 4 explains the basic
algorithm for estimating the answer validity score.
Section 5 gives the results of a number of experi-
ments and discusses them. Finally, Section 6 puts
our approach in the context of related works.
2 Overall Methodology
Given a question   and a candidate answer  the an-
swer validation task is defined as the capability to as-
sess the relevance of  with respect to   . We assume
open domain questions and that both answers and
questions are texts composed of few tokens (usually
less than 100). This is compatible with the TREC-
2001 data, that will be used as examples throughout
this paper. We also assume the availability of the
Web, considered to be the largest open domain text
corpus containing information about almost all the
different areas of the human knowledge.
The intuition underlying our approach to an-
swer validation is that, given a question-answer pair
([   ,  ]), it is possible to formulate a set of valida-
tion statements whose truthfulness is equivalent to
the degree of relevance of  with respect to   . For
instance, given the question ?What is the capital of
the USA??, the problem of validating the answer
?Washington? is equivalent to estimating the truth-
fulness of the validation statement ?The capital of
the USA is Washington?. Therefore, the answer
validation task could be reformulated as a problem
of statement reliability. There are two issues to be
addressed in order to make this intuition effective.
First, the idea of a validation statement is still insuf-
ficient to catch the richness of implicit knowledge
that may connect an answer to a question: we will
attack this problem defining the more flexible idea
of a validation pattern. Second, we have to design
an effective and efficient way to check the reliability
of a validation pattern: our solution relies on a pro-
cedure based on a statistical count of Web searches.
Answers may occur in text passages with low
similarity with respect to the question. Passages
telling facts may use different syntactic construc-
tions, sometimes are spread in more than one sen-
tence, may reflect opinions and personal attitudes,
and often use ellipsis and anaphora. For instance, if
the validation statement is ?The capital of USA is
Washington?, we have Web documents containing
passages like those reported in Table 1, which can
not be found with a simple search of the statement,
but that nevertheless contain a significant amount of
knowledge about the relations between the question
and the answer. We will refer to these text fragments
as validation fragments.
1. Capital Region USA: Fly-Drive Holidays in
and Around Washington D.C.
2. the Insider?s Guide to the Capital Area Music
Scene (Washington D.C., USA).
3. The Capital Tangueros (Washington, DC
Area, USA)
4. I live in the Nation?s Capital, Washington
Metropolitan Area (USA).
5. in 1790 Capital (also USA?s capital): Wash-
ington D.C. Area: 179 square km
Table 1: Web search for validation fragments
A common feature in the above examples is the
co-occurrence of a certain subset of words (i.e.
?capital?,?USA? and ?Washington?). We will make
use of validation patterns that cover a larger portion
of text fragments, including those lexically similar
to the question and the answer (e.g. fragments 4 and
5 in Table 1) and also those that are not similar (e.g.
fragment 2 in Table 1). In the case of our example
a set of validation statements can be generalized by
the validation pattern:
[capital  text  USA  text  Washington]
where  text  is a place holder for any portion of
text with a fixed maximal length.
To check the correctness of  with respect to  
we propose a procedure that measures the number
of occurrences on the Web of a validation pattern
derived from  and   . A useful feature of such pat-
terns is that when we search for them on the Web
they usually produce many hits, thus making statis-
tical approaches applicable. In contrast, searching
for strict validation statements generally results in a
small number of documents (if any) and makes sta-
tistical methods irrelevant. A number of techniques
used for finding collocations and co-occurrences of
words, such as mutual information, may well be
used to search co-occurrence tendency between the
question and the candidate answer in the Web. If we
verify that such tendency is statistically significant
we may consider the validation pattern as consistent
and therefore we may assume a high level of correla-
tion between the question and the candidate answer.
Starting from the above considerations and given
a question-answer pair     , we propose an answer
validation procedure based on the following steps:
1. Compute the set of representative keywords
	
  and 	  both from   and from  ; this step is
carried out using linguistic techniques, such as
answer type identification (from the question)
and named entities recognition (from the an-
swer);
2. From the extracted keywords compute the vali-
dation pattern for the pair [    ];
3. Submit the patterns to the Web and estimate an
answer validity score considering the number
of retrieved documents.
3 Extracting Validation Patterns
In our approach a validation pattern consists of two
components: a question sub-pattern (Qsp) and an
answer sub-pattern (Asp).
Building the Qsp. A Qsp is derived from the input
question cutting off non-content words with a stop-
words filter. The remaining words are expanded
with both synonyms and morphological forms in
order to maximize the recall of retrieved docu-
ments. Synonyms are automatically extracted from
the most frequent sense of the word in WordNet
(Fellbaum, 1998), which considerably reduces the
risk of adding disturbing elements. As for morphol-
ogy, verbs are expanded with all their tense forms
(i.e. present, present continuous, past tense and past
participle). Synonyms and morphological forms are
added to the Qsp and composed in an OR clause.
The following example illustrates how the Qsp
is constructed. Given the TREC-2001 question
?When did Elvis Presley die??, the stop-words filter
removes ?When? and ?did? from the input. Then
synonyms of the first sense of ?die? (i.e. ?decease?,
?perish?, etc.) are extracted from WordNet. Finally,
morphological forms for all the corresponding verb
tenses are added to the Qsp. The resultant Qsp will
be the following:
[Elvis  text  Presley  text  (die OR died OR
dying OR perish OR ...)]
Building the Asp. An Asp is constructed in two
steps. First, the answer type of the question is iden-
tified considering both morpho-syntactic (a part of
speech tagger is used to process the question) and
semantic features (by means of semantic predicates
defined on the WordNet taxonomy; see (Magnini et
al., 2001) for details). Possible answer types are:
DATE, MEASURE, PERSON, LOCATION, ORGANI-
ZATION, DEFINITION and GENERIC. DEFINITION
is the answer type peculiar to questions like ?What
is an atom?? which represent a considerable part
(around 25%) of the TREC-2001 corpus. The an-
swer type GENERIC is used for non definition ques-
tions asking for entities that can not be classified as
named entities (e.g. the questions: ?Material called
linen is made from what plant?? or ?What mineral
helps prevent osteoporosis??)
In the second step, a rule-based named entities
recognition module identifies in the answer string
all the named entities matching the answer type cat-
egory. If the category corresponds to a named en-
tity, an Asp for each selected named entity is cre-
ated. If the answer type category is either DEFINI-
TION or GENERIC, the entire answer string except
the stop-words is considered. In addition, in order
to maximize the recall of retrieved documents, the
Asp is expanded with verb tenses. The following
example shows how the Asp is created. Given the
TREC question ?When did Elvis Presley die?? and
the candidate answer ?though died in 1977 of course
some fans maintain?, since the answer type category
is DATE the named entities recognition module will
select [1977] as an answer sub-pattern.
4 Estimating Answer Validity
The answer validation algorithm queries the Web
with the patterns created from the question and an-
swer and after that estimates the consistency of the
patterns.
4.1 Querying the Web
We use a Web-mining algorithm that considers the
number of pages retrieved by the search engine. In
contrast, qualitative approaches to Web mining (e.g.
(Brill et al, 2001)) analyze the document content,
as a result considering only a relatively small num-
ber of pages. For information retrieval we used the
AltaVista search engine. Its advanced syntax allows
the use of operators that implement the idea of vali-
dation patterns introduced in Section 2. Queries are
composed using NEAR, OR and AND boolean opera-
tors. The NEAR operator searches pages where two
words appear in a distance of no more than 10 to-
kens: it is used to put together the question and the
answer sub-patterns in a single validation pattern.
The OR operator introduces variations in the word
order and verb forms. Finally, the AND operator is
used as an alternative to NEAR, allowing more dis-
tance among pattern elements.
If the question sub-pattern 
 does not return
any document or returns less than a certain thresh-
old (experimentally set to 7) the question pattern
is relaxed by cutting one word; in this way a new
query is formulated and submitted to the search en-
gine. This is repeated until no more words can be
cut or the returned number of documents becomes
higher than the threshold. Pattern relaxation is per-
formed using word-ignoring rules in a specified or-
der. Such rules, for instance, ignore the focus of the
question, because it is unlikely that it occurs in a
validation fragment; ignore adverbs and adjectives,
because are less significant; ignore nouns belonging
to the WordNet classes ?abstraction?, ?psychologi-
cal feature? or ?group?, because usually they specify
finer details and human attitudes. Names, numbers
and measures are preferred over all the lower-case
words and are cut last.
4.2 Estimating pattern consistency
The Web-mining module submits three searches to
the search engine: the sub-patterns [Qsp] and [Asp]
and the validation pattern [QAp], this last built as
the composition [Qsp NEAR Asp]. The search en-
gine returns respectively: 
 ,    
		 Scaling Web-based Acquisition of Entailment Relations
Idan Szpektor
idan@szpektor.net
?ITC-Irst, Via Sommarive, 18 (Povo) - 38050 Trento, Italy
?DIT - University of Trento, Via Sommarive, 14 (Povo) - 38050 Trento, Italy
?Department of Computer Science, Bar Ilan University - Ramat Gan 52900, Israel
Department of Computer Science, Tel Aviv University - Tel Aviv 69978, Israel
Hristo Tanev?
tanev@itc.it
Ido Dagan?
dagan@cs.biu.ac.il
Bonaventura Coppola??
coppolab@itc.it
Abstract
Paraphrase recognition is a critical step for nat-
ural language interpretation. Accordingly, many
NLP applications would benefit from high coverage
knowledge bases of paraphrases. However, the scal-
ability of state-of-the-art paraphrase acquisition ap-
proaches is still limited. We present a fully unsuper-
vised learning algorithm for Web-based extraction
of entailment relations, an extended model of para-
phrases. We focus on increased scalability and gen-
erality with respect to prior work, eventually aiming
at a full scale knowledge base. Our current imple-
mentation of the algorithm takes as its input a verb
lexicon and for each verb searches the Web for re-
lated syntactic entailment templates. Experiments
show promising results with respect to the ultimate
goal, achieving much better scalability than prior
Web-based methods.
1 Introduction
Modeling semantic variability in language has
drawn a lot of attention in recent years. Many ap-
plications like QA, IR, IE and Machine Translation
(Moldovan and Rus, 2001; Hermjakob et al, 2003;
Jacquemin, 1999) have to recognize that the same
meaning can be expressed in the text in a huge vari-
ety of surface forms. Substantial research has been
dedicated to acquiring paraphrase patterns, which
represent various forms in which a certain meaning
can be expressed.
Following (Dagan and Glickman, 2004) we ob-
serve that a somewhat more general notion needed
for applications is that of entailment relations (e.g.
(Moldovan and Rus, 2001)). These are directional
relations between two expressions, where the mean-
ing of one can be entailed from the meaning of the
other. For example ?X acquired Y? entails ?X owns
Y?. These relations provide a broad framework for
representing and recognizing semantic variability,
as proposed in (Dagan and Glickman, 2004). For
example, if a QA system has to answer the question
?Who owns Overture?? and the corpus includes the
phrase ?Yahoo acquired Overture?, the system can
use the known entailment relation to conclude that
this phrase really indicates the desired answer. More
examples of entailment relations, acquired by our
method, can be found in Table 1 (section 4).
To perform such inferences at a broad scale, ap-
plications need to possess a large knowledge base
(KB) of entailment patterns. We estimate such a
KB should contain from between a handful to a few
dozens of relations per meaning, which may sum
to a few hundred thousands of relations for a broad
domain, given that a typical lexicon includes tens of
thousands of words.
Our research goal is to approach unsupervised ac-
quisition of such a full scale KB. We focus on de-
veloping methods that acquire entailment relations
from the Web, the largest available resource. To
this end substantial improvements are needed in or-
der to promote scalability relative to current Web-
based approaches. In particular, we address two
major goals: reducing dramatically the complexity
of required auxiliary inputs, thus enabling to apply
the methods at larger scales, and generalizing the
types of structures that can be acquired. The algo-
rithms described in this paper were applied for ac-
quiring entailment relations for verb-based expres-
sions. They successfully discovered several rela-
tions on average per each randomly selected expres-
sion.
2 Background and Motivations
This section provides a qualitative view of prior
work, emphasizing the perspective of aiming at a
full-scale paraphrase resource. As there are still
no standard benchmarks, current quantitative results
are not comparable in a consistent way.
The major idea in paraphrase acquisition is often
to find linguistic structures, here termed templates,
that share the same anchors. Anchors are lexical
elements describing the context of a sentence. Tem-
plates that are extracted from different sentences
and connect the same anchors in these sentences,
are assumed to paraphrase each other. For example,
the sentences ?Yahoo bought Overture? and ?Yahoo
acquired Overture? share the anchors {X=Yahoo,
Y =Overture}, suggesting that the templates ?X buy
Y? and ?X acquire Y? paraphrase each other. Algo-
rithms for paraphrase acquisition address two prob-
lems: (a) finding matching anchors and (b) identify-
ing template structure, as reviewed in the next two
subsections.
2.1 Finding Matching Anchors
The prominent approach for paraphrase learning
searches sentences that share common sets of mul-
tiple anchors, assuming they describe roughly the
same fact or event. To facilitate finding many
matching sentences, highly redundant comparable
corpora have been used. These include multiple
translations of the same text (Barzilay and McKe-
own, 2001) and corresponding articles from multi-
ple news sources (Shinyama et al, 2002; Pang et
al., 2003; Barzilay and Lee, 2003). While facilitat-
ing accuracy, we assume that comparable corpora
cannot be a sole resource due to their limited avail-
ability.
Avoiding a comparable corpus, (Glickman and
Dagan, 2003) developed statistical methods that
match verb paraphrases within a regular corpus.
Their limited scale results, obtaining several hun-
dred verb paraphrases from a 15 million word cor-
pus, suggest that much larger corpora are required.
Naturally, the largest available corpus is the Web.
Since exhaustive processing of the Web is not feasi-
ble, (Duclaye et al, 2002) and (Ravichandran and
Hovy, 2002) attempted bootstrapping approaches,
which resemble the mutual bootstrapping method
for Information Extraction of (Riloff and Jones,
1999). These methods start with a provided known
set of anchors for a target meaning. For example,
the known anchor set {Mozart, 1756} is given as in-
put in order to find paraphrases for the template ?X
born in Y?. Web searching is then used to find occur-
rences of the input anchor set, resulting in new tem-
plates that are supposed to specify the same relation
as the original one (?born in?). These new templates
are then exploited to get new anchor sets, which
are subsequently processed as the initial {Mozart,
1756}. Eventually, the overall procedure results in
an iterative process able to induce templates from
anchor sets and vice versa.
The limitation of this approach is the requirement
for one input anchor set per target meaning. Prepar-
ing such input for all possible meanings in broad
domains would be a huge task. As will be explained
below, our method avoids this limitation by find-
ing all anchor sets automatically in an unsupervised
manner.
Finally, (Lin and Pantel, 2001) present a notably
different approach that relies on matching sepa-
rately single anchors. They limit the allowed struc-
ture of templates only to paths in dependency parses
connecting two anchors. The algorithm constructs
for each possible template two feature vectors, rep-
resenting its co-occurrence statistics with the two
anchors. Two templates with similar vectors are
suggested as paraphrases (termed inference rule).
Matching of single anchors relies on the gen-
eral distributional similarity principle and unlike the
other methods does not require redundancy of sets
of multiple anchors. Consequently, a much larger
number of paraphrases can be found in a regular
corpus. Lin and Pantel report experiments for 9
templates, in which their system extracted 10 cor-
rect inference rules on average per input template,
from 1GB of news data. Yet, this method also suf-
fers from certain limitations: (a) it identifies only
templates with pre-specified structures; (b) accuracy
seems more limited, due to the weaker notion of
similarity; and (c) coverage is limited to the scope
of an available corpus.
To conclude, several approaches exhaustively
process different types of corpora, obtaining vary-
ing scales of output. On the other hand, the Web is
a huge promising resource, but current Web-based
methods suffer serious scalability constraints.
2.2 Identifying Template Structure
Paraphrasing approaches learn different kinds of
template structures. Interesting algorithms are pre-
sented in (Pang et al, 2003; Barzilay and Lee,
2003). They learn linear patterns within similar con-
texts represented as finite state automata. Three
classes of syntactic template learning approaches
are presented in the literature: learning of predicate
argument templates (Yangarber et al, 2000), learn-
ing of syntactic chains (Lin and Pantel, 2001) and
learning of sub-trees (Sudo et al, 2003). The last
approach is the most general with respect to the tem-
plate form. However, its processing time increases
exponentially with the size of the templates.
As a conclusion, state of the art approaches still
learn templates of limited form and size, thus re-
stricting generality of the learning process.
3 The TE/ASE Acquisition Method
Motivated by prior experience, we identify two ma-
jor goals for scaling Web-based acquisition of en-
tailment relations: (a) Covering the broadest pos-
sible range of meanings, while requiring minimal
input and (b) Keeping template structures as gen-
eral as possible. To address the first goal we re-
quire as input only a phrasal lexicon of the rel-
evant domain (including single words and multi-
word expressions). Broad coverage lexicons are
widely available or may be constructed using known
term acquisition techniques, making it a feasible
and scalable input requirement. We then aim to
acquire entailment relations that include any of the
lexicon?s entries. The second goal is addressed by a
novel algorithm for extracting the most general tem-
plates being justified by the data.
For each lexicon entry, denoted a pivot, our
extraction method performs two phases: (a) ex-
tract promising anchor sets for that pivot (ASE,
Section 3.1), and (b) from sentences contain-
ing the anchor sets, extract templates for which
an entailment relation holds with the pivot (TE,
Section 3.2). Examples for verb pivots are:
?acquire?, ?fall to?, ?prevent? . We will use the pivot
?prevent? for examples through this section.
Before presenting the acquisition method we first
define its output. A template is a dependency parse-
tree fragment, with variable slots at some tree nodes
(e.g. ?X subj? prevent obj? Y? ). An entailment rela-
tion between two templates T1 and T2 holds if
the meaning of T2 can be inferred from the mean-
ing of T1 (or vice versa) in some contexts, but
not necessarily all, under the same variable instan-
tiation. For example, ?X subj? prevent obj? Y? entails
?X
subj
? reduce
obj
? Y risk? because the sentence ?as-
pirin reduces heart attack risk? can be inferred from
?aspirin prevents a first heart attack?. Our output
consists of pairs of templates for which an entail-
ment relation holds.
3.1 Anchor Set Extraction (ASE)
The goal of this phase is to find a substantial num-
ber of promising anchor sets for each pivot. A good
anchor-set should satisfy a proper balance between
specificity and generality. On one hand, an anchor
set should correspond to a sufficiently specific set-
ting, so that entailment would hold between its dif-
ferent occurrences. On the other hand, it should be
sufficiently frequent to appear with different entail-
ing templates.
Finding good anchor sets based on just the input
pivot is a hard task. Most methods identify good re-
peated anchors ?in retrospect?, that is after process-
ing a full corpus, while previous Web-based meth-
ods require at least one good anchor set as input.
Given our minimal input, we needed refined crite-
ria that identify a priori the relatively few promising
anchor sets within a sample of pivot occurrences.
ASE ALGORITHM STEPS:
For each pivot (a lexicon entry)
1. Create a pivot template, Tp
2. Construct a parsed sample corpus S for Tp:
(a) Retrieve an initial sample from the Web
(b) Identify associated phrases for the pivot
(c) Extend S using the associated phrases
3. Extract candidate anchor sets from S:
(a) Extract slot anchors
(b) Extract context anchors
4. Filter the candidate anchor sets:
(a) by absolute frequency
(b) by conditional pivot probability
Figure 1: Outline of the ASE algorithm.
The ASE algorithm (presented in Figure 1) per-
forms 4 main steps.
STEP (1) creates a complete template, called the
pivot template and denoted Tp, for the input pivot,
denoted P . Variable slots are added for the ma-
jor types of syntactic relations that interact with P ,
based on its syntactic type. These slots enable us to
later match Tp with other templates. For verbs, we
add slots for a subject and for an object or a modifier
(e.g. ?X subj? prevent obj? Y? ).
STEP (2) constructs a sample corpus, denoted S,
for the pivot template. STEP (2.A) utilizes a Web
search engine to initialize S by retrieving sentences
containing P . The sentences are parsed by the
MINIPAR dependency parser (Lin, 1998), keeping
only sentences that contain the complete syntactic
template Tp (with all the variables instantiated).
STEP (2.B) identifies phrases that are statistically
associated with Tp in S. We test all noun-phrases
in S , discarding phrases that are too common on
the Web (absolute frequency higher than a thresh-
old MAXPHRASEF), such as ?desire?. Then we se-
lect the N phrases with highest tf ?idf score1. These
phrases have a strong collocation relationship with
the pivot P and are likely to indicate topical (rather
than anecdotal) occurrences of P . For example, the
phrases ?patient? and ?American Dental Associa-
tion?, which indicate contexts of preventing health
problems, were selected for the pivot ?prevent?. Fi-
1Here, tf ?idf = freqS(X) ? log
(
N
freqW (X)
)
where freqS(X) is the number of occurrences in S containing
X , N is the total number of Web documents, and freqW (X)
is the number of Web documents containing X .
nally, STEP (2.C) expands S by querying the Web
with the both P and each of the associated phrases,
adding the retrieved sentences to S as in step (2.a).
STEP (3) extracts candidate anchor sets for Tp.
From each sentence in S we try to generate one can-
didate set, containing noun phrases whose Web fre-
quency is lower than MAXPHRASEF. STEP (3.A)
extracts slot anchors ? phrases that instantiate the
slot variables of Tp. Each anchor is marked
with the corresponding slot. For example, the
anchors {antibioticssubj? , miscarriage obj?} were ex-
tracted from the sentence ?antibiotics in pregnancy
prevent miscarriage?.
STEP (3.B) tries to extend each candidate set with
one additional context anchor, in order to improve
its specificity. This anchor is chosen as the highest
tf ?idf scoring phrase in the sentence, if it exists. In
the previous example, ?pregnancy? is selected.
STEP (4) filters out bad candidate anchor sets by
two different criteria. STEP (4.A) maintains only
candidates with absolute Web frequency within a
threshold range [MINSETF, MAXSETF], to guaran-
tee an appropriate specificity-generality level. STEP
(4.B) guarantees sufficient (directional) association
between the candidate anchor set c and Tp, by esti-
mating
Prob(Tp|c) ?
freqW (P ? c)
freqW (c)
where freqW is Web frequency and P is the pivot.
We maintain only candidates for which this prob-
ability falls within a threshold range [SETMINP,
SETMAXP]. Higher probability often corresponds
to a strong linguistic collocation between the
candidate and Tp, without any semantic entail-
ment. Lower probability indicates coincidental co-
occurrence, without a consistent semantic relation.
The remaining candidates in S become the in-
put anchor-sets for the template extraction phase,
for example, {Aspirinsubj? , heart attackobj?} for ?pre-
vent?.
3.2 Template Extraction (TE)
The Template Extraction algorithm accepts as its in-
put a list of anchor sets extracted from ASE for each
pivot template. Then, TE generates a set of syntactic
templates which are supposed to maintain an entail-
ment relationship with the initial pivot template. TE
performs three main steps, described in the follow-
ing subsections:
1. Acquisition of a sample corpus from the Web.
2. Extraction of maximal most general templates
from that corpus.
3. Post-processing and final ranking of extracted
templates.
3.2.1 Acquisition of a sample corpus from the
Web
For each input anchor set, TE acquires from the
Web a sample corpus of sentences containing it.
For example, a sentence from the sample corpus
for {aspirin, heart attack} is: ?Aspirin stops heart
attack??. All of the sample sentences are then
parsed with MINIPAR (Lin, 1998), which gener-
ates from each sentence a syntactic directed acyclic
graph (DAG) representing the dependency structure
of the sentence. Each vertex in this graph is labeled
with a word and some morphological information;
each graph edge is labeled with the syntactic rela-
tion between the words it connects.
TE then substitutes each slot anchor (see section
3.1) in the parse graphs with its corresponding slot
variable. Therefore, ?Aspirin stops heart attack??
will be transformed into ?X stop Y?. This way all
the anchors for a certain slot are unified under the
same variable name in all sentences. The parsed
sentences related to all of the anchor sets are sub-
sequently merged into a single set of parse graphs
S = {P1, P2, . . . , Pn} (see P1 and P2 in Figure 2).
3.2.2 Extraction of maximal most general
templates
The core of TE is a General Structure Learning al-
gorithm (GSL ) that is applied to the set of parse
graphs S resulting from the previous step. GSL
extracts single-rooted syntactic DAGs, which are
named spanning templates since they must span at
least over Na slot variables, and should also ap-
pear in at least Nr sentences from S (In our exper-
iments we set Na=2 and Nr=2). GSL learns maxi-
mal most general templates: they are spanning tem-
plates which, at the same time, (a) cannot be gener-
alized by further reduction and (b) cannot be further
extended keeping the same generality level.
In order to properly define the notion of maximal
most general templates, we introduce some formal
definitions and notations.
DEFINITION: For a spanning template t we define
a sentence set, denoted with ?(t), as the set of all
parsed sentences in S containing t.
For each pair of templates t1 and t2, we use the no-
tation t1  t2 to denote that t1 is included as a sub-
graph or is equal to t2. We use the notation t1 ? t2
when such inclusion holds strictly. We define T (S)
as the set of all spanning templates in the sample S.
DEFINITION: A spanning template t ? T (S) is
maximal most general if and only if both of the fol-
lowing conditions hold:
CONDITION A: For ?t? ? T (S), t?  t, it holds that
?(t) = ?(t?).
CONDITION B: For ?t? ? T (S), t ? t?, it holds that
?(t) ? ?(t?).
Condition A ensures that the extracted templates do
not contain spanning sub-structures that are more
?general? (i.e. having a larger sentence set); con-
dition B ensures that the template cannot be further
enlarged without reducing its sentence set.
GSL performs template extraction in two main
steps: (1) build a compact graph representation of
all the parse graphs from S; (2) extract templates
from the compact representation.
A compact graph representation is an aggregate
graph which joins all the sentence graphs from S
ensuring that all identical spanning sub-structures
from different sentences are merged into a single
one. Therefore, each vertex v (respectively, edge
e) in the aggregate graph is either a copy of a cor-
responding vertex (edge) from a sentence graph Pi
or it represents the merging of several identically
labeled vertices (edges) from different sentences in
S. The set of such sentences is defined as the sen-
tence set of v (e), and is represented through the set
of index numbers of related sentences (e.g. ?(1,2)?
in the third tree of Figure 2). We will denote with
Gi the compact graph representation of the first i
sentences in S. The parse trees P1 and P2 of two
sentences and their related compact representation
G2 are shown in Figure 2.
Building the compact graph representation
The compact graph representation is built incremen-
tally. The algorithm starts with an empty aggregate
graph G0 and then merges the sentence graphs from
S one at a time into the aggregate structure.
Let?s denote the current aggregate graph with
Gi?1(Vg, Eg) and let Pi(Vp, Ep) be the parse graph
which will be merged next. Note that the sentence
set of Pi is a single element set {i}.
During each iteration a new graph is created as
the union of both input graphs: Gi = Gi?1 ? Pi.
Then, the following merging procedure is per-
formed on the elements of Gi
1. ADDING GENERALIZED VERTICES TO Gi.
For every two vertices vg ? Vg, vp ? Vp having
equal labels, a new generalized vertex vnewg is cre-
ated and added to Gi. The new vertex takes the same
label and holds a sentence set which is formed from
the sentence set of vg by adding i to it. Still with
reference to Figure 2, the generalized vertices in G2
are ?X?, ?Y? and ?stop?. The algorithm connects the
generalized vertex vnewg with all the vertices which
are connected with vg and vp.
2. MERGING EDGES. If two edges eg ? Eg and
ep ? Ep have equal labels and their corresponding
adjacent vertices have been merged, then ea and ep
are also merged into a new edge. In Figure 2 the
edges (?stop?, ?X? ) and (?stop?, ?Y? ) from P1 and
P2 are eventually merged into G2.
3. DELETING MERGED VERTICES. Every vertex
v from Vp or Vg for which at least one generalized
vertex vnewg exists is deleted from Gi.
As an optimization step, we merge only vertices
and edges that are included in equal spanning tem-
plates.
Extracting the templates
GSL extracts all maximal most general templates
from the final compact representation Gn using the
following sub-algorithm:
1. BUILDING MINIMAL SPANNING TREES. For
every Na different slot variables in Gn having a
common ancestor, a minimal spanning tree st is
built. Its sentence set is computed as the intersec-
tion of the sentence sets of its edges and vertices.
2. EXPANDING THE SPANNING TREES. Every
minimal spanning tree st is expanded to the maxi-
mal sub-graph maxst whose sentence set is equal to
?(st). All maximal single-rooted DAGs in maxst
are extracted as candidate templates. Maximality
ensures that the extracted templates cannot be ex-
panded further while keeping the same sentence set,
satisfying condition B.
3. FILTERING. Candidates which contain an-
other candidate with a larger sentence set are filtered
out. This step guarantees condition A.
In Figure 2 the maximal most general template in
G2 is ?X
subj
? stop
obj
? Y? .
3.2.3 Post-processing and ranking of extracted
templates
As a last step, names and numbers are filtered out
from the templates. Moreover, TE removes those
templates which are very long or which appear with
just one anchor set and in less than four sentences.
Finally, the templates are sorted first by the number
of anchor sets with which each template appeared,
and then by the number of sentences in which they
appeared.
4 Evaluation
We evaluated the results of the TE/ASE algorithm
on a random lexicon of verbal forms and then as-
sessed its performance on the extracted data through
human-based judgments.
P1 : stop
subj
z
z
z
||zz
z
z
obj
A
A
A
  A
AA
A
P2 : stop
subj
z
z
z
||zz
z
z
obj

by
J
J
J
J
%%J
J
J
J
G2 : stop(1, 2)
subj(1,2)
rr
rr
xxrr
rr
obj(1,2)

by(2)
OO
OO
''O
OO
O
X Y X Y absorbing X(1, 2) Y (1, 2) absorbing(2)
Figure 2: Two parse trees and their compact representation (sentence sets are shown in parentheses).
4.1 Experimental Setting
The test set for human evaluation was generated by
picking out 53 random verbs from the 1000 most
frequent ones found in a subset of the Reuters cor-
pus2. For each verb entry in the lexicon, we pro-
vided the judges with the corresponding pivot tem-
plate and the list of related candidate entailment
templates found by the system. The judges were
asked to evaluate entailment for a total of 752 tem-
plates, extracted for 53 pivot lexicon entries; Table
1 shows a sample of the evaluated templates; all of
them are clearly good and were judged as correct
ones.
Pivot Template Entailment Templates
X prevent Y X provides protection against Y
X reduces Y
X decreases the risk of Y
X be cure for Y
X a day keeps Y away
X to combat Y
X accuse Y X call Y indictable
X testifies against Y
Y defense before X
X acquire Y X snap up Y
Y shareholders approve X
buyout
Y shareholders receive shares
of X stock
X go back to Y Y allowed X to return
Table 1: Sample of templates found by TE/ASE and
included in the evaluation test set.
Concerning the ASE algorithm, threshold pa-
rameters3 were set as PHRASEMAXF=107, SET-
MINF=102, SETMAXF=105, SETMINP=0.066,
and SETMAXP=0.666. An upper limit of 30 was
imposed on the number of possible anchor sets used
for each pivot. Since this last value turned out to
be very conservative with respect to system cover-
2Known as Reuters Corpus, Volume 1, English Language,
1996-08-20 to 1997-08-19.
3All parameters were tuned on a disjoint development lexi-
con before the actual experiment.
age, we subsequently attempted to relax it to 50 (see
Discussion in Section 4.3).
Further post-processing was necessary over ex-
tracted data in order to remove syntactic variations
referring to the same candidate template (typically
passive/active variations).
Three possible judgment categories have been
considered: Correct if an entailment relationship
in at least one direction holds between the judged
template and the pivot template in some non-bizarre
context; Incorrect if there is no reasonable context
and variable instantiation in which entailment holds;
No Evaluation if the judge cannot come to a definite
conclusion.
4.2 Results
Each of the three assessors (referred to as J#1, J#2,
and J#3) issued judgments for the 752 different
templates. Correct templates resulted to be 283,
313, and 295 with respect to the three judges. No
evaluation?s were 2, 0, and 16, while the remaining
templates were judged Incorrect.
For each verb, we calculate Yield as the absolute
number of Correct templates found and Precision as
the percentage of good templates out of all extracted
templates. Obtained Precision is 44.15%, averaged
over the 53 verbs and the 3 judges. Considering Low
Majority on judges, the precision value is 42.39%.
Average Yield was 5.5 templates per verb.
These figures may be compared (informally, as
data is incomparable) with average yield of 10.1
and average precision of 50.3% for the 9 ?pivot?
templates of (Lin and Pantel, 2001). The compar-
ison suggests that it is possible to obtain from the
(very noisy) web a similar range of precision as was
obtained from a clean news corpus. It also indi-
cates that there is potential for acquiring additional
templates per pivot, which would require further re-
search on broadening efficiently the search for addi-
tional web data per pivot.
Agreement among judges is measured by the
Kappa value, which is 0.55 between J#1 and J#2,
0.57 between J#2 and J#3, and 0.63 between J#1
and J#3. Such Kappa values correspond to moder-
ate agreement for the first two pairs and substantial
agreement for the third one. In general, unanimous
agreement among all of the three judges has been
reported on 519 out of 752 templates, which corre-
sponds to 69%.
4.3 Discussion
Our algorithm obtained encouraging results, ex-
tracting a considerable amount of interesting tem-
plates and showing inherent capability of discover-
ing complex semantic relations.
Concerning overall coverage, we managed to find
correct templates for 86% of the verbs (46 out of
53). Nonetheless, presented results show a substan-
tial margin of possible improvement. In fact yield
values (5.5 Low Majority, up to 24 in best cases),
which are our first concern, are inherently depen-
dent on the breadth of Web search performed by
the ASE algorithm. Due to computational time, the
maximal number of anchor sets processed for each
verb was held back to 30, significantly reducing the
amount of retrieved data.
In order to further investigate ASE potential, we
subsequently performed some extended experiment
trials raising the number of anchor sets per pivot
to 50. This time we randomly chose a subset of
10 verbs out of the less frequent ones in the origi-
nal main experiment. Results for these verbs in the
main experiment were an average Yield of 3 and an
average Precision of 45.19%. In contrast, the ex-
tended experiments on these verbs achieved a 6.5
Yield and 59.95% Precision (average values). These
results are indeed promising, and the substantial
growth in Yield clearly indicates that the TE/ASE
algorithms can be further improved. We thus sug-
gest that the feasibility of our approach displays the
inherent scalability of the TE/ASE process, and its
potential to acquire a large entailment relation KB
using a full scale lexicon.
A further improvement direction relates to tem-
plate ranking and filtering. While in this paper
we considered anchor sets to have equal weights,
we are also carrying out experiments with weights
based on cross-correlation between anchor sets.
5 Conclusions
We have described a scalable Web-based approach
for entailment relation acquisition which requires
only a standard phrasal lexicon as input. This min-
imal level of input is much simpler than required
by earlier web-based approaches, while succeeding
to maintain good performance. This result shows
that it is possible to identify useful anchor sets in
a fully unsupervised manner. The acquired tem-
plates demonstrate a broad range of semantic rela-
tions varying from synonymy to more complicated
entailment. These templates go beyond trivial para-
phrases, demonstrating the generality and viability
of the presented approach.
From our current experiments we can expect to
learn about 5 relations per lexicon entry, at least for
the more frequent entries. Moreover, looking at the
extended test, we can extrapolate a notably larger
yield by broadening the search space. Together with
the fact that we expect to find entailment relations
for about 85% of a lexicon, it is a significant step
towards scalability, indicating that we will be able
to extract a large scale KB for a large scale lexicon.
In future work we aim to improve the yield by in-
creasing the size of the sample-corpus in a qualita-
tive way, as well as precision, using statistical meth-
ods such as supervised learning for better anchor set
identification and cross-correlation between differ-
ent pivots. We also plan to support noun phrases
as input, in addition to verb phrases. Finally, we
would like to extend the learning task to discover the
correct entailment direction between acquired tem-
plates, completing the knowledge required by prac-
tical applications.
Like (Lin and Pantel, 2001), learning the context
for which entailment relations are valid is beyond
the scope of this paper. As stated, we learn entail-
ment relations holding for some, but not necessarily
all, contexts. In future work we also plan to find the
valid contexts for entailment relations.
Acknowledgements
The authors would like to thank Oren Glickman
(Bar Ilan University) for helpful discussions and as-
sistance in the evaluation, Bernardo Magnini for his
scientific supervision at ITC-irst, Alessandro Vallin
and Danilo Giampiccolo (ITC-irst) for their help in
developing the human based evaluation, and Prof.
Yossi Matias (Tel-Aviv University) for supervising
the first author. This work was partially supported
by the MOREWEB project, financed by Provincia
Autonoma di Trento. It was also partly carried out
within the framework of the ITC-IRST (TRENTO,
ITALY) ? UNIVERSITY OF HAIFA (ISRAEL) col-
laboration project. For data visualization and analy-
sis the authors intensively used the CLARK system
(www.bultreebank.org) developed at the Bulgarian
Academy of Sciences .
References
Regina Barzilay and Lillian Lee. 2003. Learning
to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceedings
of HLT-NAACL 2003, pages 16?23, Edmonton,
Canada.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proceedings of ACL 2001, pages 50?57, Toulose,
France.
Ido Dagan and Oren Glickman. 2004. Probabilis-
tic textual entailment: Generic applied modeling
of language variability. In PASCAL Workshop on
Learning Methods for Text Understanding and
Mining, Grenoble.
Florence Duclaye, Franc?ois Yvon, and Olivier
Collin. 2002. Using the Web as a linguistic re-
source for learning reformulations automatically.
In Proceedings of LREC 2002, pages 390?396,
Las Palmas, Spain.
Oren Glickman and Ido Dagan. 2003. Identifying
lexical paraphrases from a single corpus: a case
study for verbs. In Proceedings of RANLP 2003.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2003. Natural language based reformula-
tion resource and Web Exploitation. In Ellen M.
Voorhees and Lori P. Buckland, editors, Proceed-
ings of the 11th Text Retrieval Conference (TREC
2002), Gaithersburg, MD. NIST.
Christian Jacquemin. 1999. Syntagmatic and
paradigmatic representations of term variation.
In Proceedings of ACL 1999, pages 341?348.
Dekang Lin and Patrick Pantel. 2001. Discovery of
inference rules for Question Answering. Natural
Language Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation
of MINIPAR. In Proceedings of the Workshop
on Evaluation of Parsing Systems at LREC 1998,
Granada, Spain.
Dan Moldovan and Vasile Rus. 2001. Logic form
transformation of WordNet and its applicability
to Question Answering. In Proceedings of ACL
2001, pages 394?401, Toulose, France.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations:
Extracting paraphrases and generating new sen-
tences. In Proceedings of HLT-NAACL 2003, Ed-
monton, Canada.
Deepak Ravichandran and Eduard Hovy. 2002.
Learning surface text patterns for a Question An-
swering system. In Proceedings of ACL 2002,
Philadelphia, PA.
Ellen Riloff and Rosie Jones. 1999. Learning dic-
tionaries for Information Extraction by multi-
level bootstrapping. In Proceedings of the Six-
teenth National Conference on Artificial Intelli-
gence (AAAI-99), pages 474?479.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo,
and Ralph Grishman. 2002. Automatic para-
phrase acquisition from news articles. In Pro-
ceedings of Human Language Technology Con-
ference (HLT 2002), San Diego, USA.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern represen-
tation model for automatic IE pattern acquisition.
In Proceedings of ACL 2003.
Roman Yangarber, Ralph Grishman, Pasi
Tapanainen, and Silja Huttunen. 2000. Un-
supervised discovery of scenario-level patterns
for Information Extraction. In Proceedings of
COLING 2000.
Coling 2008: Companion volume ? Posters and Demonstrations, pages 145?148
Manchester, August 2008
Online-Monitoring of Security-Related Events
Martin Atkinson, Jakub Piskorski, Bruno Pouliquen
Ralf Steinberger, Hristo Tanev, Vanni Zavarella
Joint Research Centre of the European Commission
Institute for the Protection and Security of the Citizen
Via Fermi 2749, 21027 Ispra (VA), Italy
firstname.lastname@jrc.it
Abstract
This paper presents a fully operational
real-time event extraction system which is
capable of accurately and efficiently ex-
tracting violent and natural disaster events
from vast amount of online news articles
per day in different languages. Due to the
requirement that the system must be mul-
tilingual and easily extendable, it is based
on a shallow linguistic analysis. The event
extraction results can be viewed on a pub-
licly accessible website.
1 Introduction
Gathering information about violent and natural
disaster events from online news is of paramount
importance to better understand conflicts and to
develop global monitoring systems for the auto-
matic detection of precursors for threats in the
fields of conflict and health. This paper reports
on a fully operational live event extraction system
to detect information on violent events and natural
disasters in large multilingual collections of online
news articles collected by the news aggregation
system Europe Media Monitor (Best et al, 2005),
http://press.jrc.it/overview.html.
Although a considerable amount of work on the
automatic extraction of events has been reported,
it still appears to be a lesser studied area in com-
parison to the somewhat easier tasks of named-
entity and relation extraction. Two comprehensive
examples of the current functionality and capabil-
ities of event extraction technology dealing with
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
the identification of disease outbreaks and con-
flict incidents are given in (Grishman et al, 2002)
and (King and Lowe, 2003) respectively. The most
recent trends and developments in this area are re-
ported in (Ashish et al, 2006)
In order to be capable of processing vast
amounts of textual data in real time (as in the case
of EMM)we follow a linguistically lightweight ap-
proach and exploit clustered news at various pro-
cessing stages (pattern learning, information fu-
sion, geo-tagging, etc.). Consequently, only a tiny
fraction of each text is analysed. In a nutshell, our
system deploys simple 1 and 2-slot extraction pat-
terns to identify event-relevant entities. These pat-
terns are semi-automatically acquired in a boot-
strapping manner by using clustered news data.
Next, information about events scattered over dif-
ferent documents is integrated by applying voting
heuristics. The results of the core event extraction
system are integrated into a real-world global mon-
itoring system. Although we mainly cover the se-
curity domain, the techniques deployed in our sys-
tem can be applied to other domains, such as for
instance tracking business-related events for risk
assessment.
In the remaining part of this paper we give a
brief overview of the real-time event extraction
processing chain and describe the particularities of
selected subcomponents. Finally, the online appli-
cation is presented.
2 Real-time Event Extraction Process
The real-time event extraction processing chain is
depicted in Figure 1. First, news articles are gath-
ered by dedicated software for electronic media
monitoring, namely the EMM system (Best et al,
2005). EMM receives an average of 50,000 news
articles per day from about 1,500 news sources in
145
over 40 languages, and regularly checks for up-
dates of news. Secondly, the input data is grouped
into news clusters ideally including documents
on one topic or event. Then, clusters describing
security-related events are selected using keyword-
based heuristics. For each such cluster, the system
tries to detect and extract only the main event by
analysing all documents in the cluster.
EMM
News
Clustering / 
Geo Tag
Text Pre-
Processing 
Pattern 
Matching
Information 
Aggregation 
Events
NEXUS
Figure 1: Real-time processing chain.
Next, each cluster is processed by our core event
extraction engine. For each detected violent event,
it produces a frame, whose main slots are: date and
location, number of killed, injured or kidnapped
people, actors, type of event, weapons used, etc.
In an initial step, each document in the cluster
is linguistically pre-processed in order to produce
a more abstract representation of the texts. This
encompasses: fine-grained tokenisation, sentence
splitting, matching of known named entities, la-
belling of key terms and phrases like action words
(e.g. kill, shoot) and person groups.
Once texts are grouped into clusters and lin-
guistically pre-processed, the pattern engine ap-
plies a cascade of extraction grammars (consisting
of 1 and 2-slot extraction patterns) on each docu-
ment within a cluster. For creating extraction pat-
terns, we apply a blend of machine learning and
knowledge-based techniques. The extraction pat-
terns are matched against the first sentence and the
title of each article from the cluster. By processing
only the top sentence and the title, the system is
more likely to capture facts about the most impor-
tant event in the cluster. Even if we fail to detect
a single piece of information in one document in a
cluster, the same information is likely to be found
in another document of the cluster, where it may
be expressed in a different way.
Finally, since information about events is scat-
tered over different articles, the last step con-
sists of cross-document cluster-level information
fusion, i.e., we aggregate and validate information
extracted locally from each single article in the
same cluster. For this purpose, simple voting-like
heuristics are deployed.
Every ten minutes, EMM clusters the articles
found during the last four hours. The event extrac-
tion engine analyses each of these clusters. The
event information is thus always up-to-date. The
output of the event extraction engine constitutes
the input for a global monitoring system.
3 Geo-tagging Clusters
Challenges for geo-tagging clusters are that place
names can be homographic with person names and
with other place names. We solve the former am-
biguity by first identifying person names found
in our automatically populated database of known
people and organisations. For the latter ambiguity,
we adopted a cluster-centric approach by weight-
ing all place names found in a cluster and by select-
ing the one with the highest score. For each cluster,
we thus first establish all possible candidate loca-
tions by looking up in the texts all place, province,
region and country names found in a multilingual
gazetteer (including name variants). The weights
of the locations are then based on the place name
significance (e.g., a capital city scores higher than
a village) and on the place name hierarchy (i.e. if
the province or region to which the place belongs
are also mentioned in the text, it scores higher).
4 Pattern Acquisition
For pattern acquisition, we deploy a weakly super-
vised bootstrapping algorithm (Tanev and Oezden-
Wennerberg, 2008) similar in spirit to the one de-
scribed in (Yangarber, 2003), which involves some
manual validation. Contrary to other approaches,
the learning phase exploits the knowledge to which
cluster the news items belong. Intuitively, this
guarantees better precision of the learned patterns.
In particular, for each event-specific semantic role
(e.g. killed), a separate cycle of learning iterations
is executed (usually up to three) in order to learn
1-slot extraction patterns. Each cluster includes ar-
ticles from different sources about the same news
story. Therefore, we assume that each entity ap-
pears in the same semantic role (actor, victim, in-
jured) in the context of one cluster. An auto-
matic procedure for syntactic expansion comple-
ments the learning. This procedure accepts a man-
ually provided list of words which have identical
(or similar) syntactic usage patterns (e.g. killed,
assassinated, murdered, etc.). It then generates
new patterns from the old ones by substituting for
each other the words in the list. After 1-slot pat-
terns are acquired, some of them are used to man-
ually create 2-slot patterns like X shot Y.
146
5 Pattern matching engine
In order to guarantee that massive amounts of tex-
tual data can be processed in real time, we have
developed ExPRESS (Piskorski, 2007), an effi-
cient extraction pattern engine, which is capable of
matching thousands of patterns against MB-sized
texts within seconds. The pattern specification lan-
guage is a blend of two previously introduced IE-
oriented grammar formalisms, namely JAPE used
in GATE (Cunningham et al, 2000) and XTDL,
used in SPROUT (Dro?zd?zy?nski et al, 2004).
A single pattern is a regular expression over flat
feature structures (FS), i.e., non-recursive typed
feature structures without structure sharing, where
features are string-valued and ? unlike in XTDL
types ? are not organised in a hierarchy. Each such
regular expression is associated with a list of FSs
which constitute the output specification. Like in
XTDL, we deploy variables and functional oper-
ators for forming slot values and for establishing
contact with the ?outer world?. Further, we adapted
JAPEs feature of associating patterns with mul-
tiple actions, i.e., producing multiple annotations
(possibly nested). An empirical comparison of the
run-time behaviour of the new formalism against
the other 2 revealed that significant speed-ups can
be achieved (at least 30 times faster). ExPRESS
comes with a pool of highly efficient core linguis-
tic processing resources (Piskorski, 2008).
6 Information Aggregation
Once single pieces of information are extracted by
the pattern engine, they are merged into event de-
scriptions by applying an information aggregation
algorithm. This algorithm assumes that each clus-
ter reports at most one main event of interest. It
takes as input the text entities extracted from one
news cluster with their semantic roles and consid-
ers the sentences from which these entities are ex-
tracted. If one and the same entity has two roles as-
signed, a preference is given to the role assigned by
the most reliable group of patterns (e.g., 2-slot pat-
terns are more reliable). Another ambiguity which
has to be resolved arises from the contradictory in-
formation which news sources give about the num-
ber of victims. We use an ad-hoc heuristic for
computing the most probable estimation for these
numbers, i.e., firstly the largest group of numbers
which are close to each other is selected and sec-
ondly the number closest to the average in that
group is chosen. After this estimation is com-
puted, the system discards from each news clus-
ter all the articles whose reported victim numbers
significantly differ from the estimated numbers for
the whole cluster. Additionally, some victim arith-
metic is applied, i.e., a small taxonomy of person
classes is used to sum victim numbers (e.g., gun-
men and terrorists belong to the same class ofNon-
GovernmentalArmedGroup).
7 Event Classification
After the single pieces of information are assem-
bled into the event description, an event classifica-
tion is performed. Some of the most used event
classes are Terrorist Attack, Bombing, Shooting,
Air Attack, etc. The classification algorithm uses
a blend of keyword matching and domain spe-
cific rules. As an example, consider the following
domain-specific rule: if the event description in-
cludes named entities, which are assigned the se-
mantic role kidnapped, as well as entities which
are assigned the semantic role released, then the
type of the event is Hostage Release, rather than
Kidnapping. If the event refers to kidnapped peo-
ple and at the same time the news articles contain
words like video or videotape, then the event type
is Hostage Video Release. The second rule has a
higher priority, therefore it impedes the Hostage
Release rule to fire erroneously, when the release
of a hostage video is reported.
8 Monitoring Events
The core event extraction engine for English is
fully operational since December 2007. There are
two online applications running on top of it which
allow monitoring events. The first one is a dedi-
cated webpage using the Google Maps JavaScript
API (see Figure 2). It is publicly accessible at:
http://press.jrc.it/geo?type=event
&format=html&language=en and provides
an instant overview of what is occurring where in
the world. A small problem with this application
is that it overlays and hides events that are close to
each other.
The second application shows the same events
using the Google Earth client application. The
geo-located data is transmitted via the Keyhole
Markup Language (KML) format
1
supported di-
rectly by Google Earth.
2
The application is re-
1
http://code.google.com/apis/kml/documentation/
2
In order to run it, start Google Earth with KML:
http://press.jrc.it/geo?type=event&format=kml&language=en
147
Figure 2: Event visualisation with Google Maps
stricted to displaying at most half the globe, but
it allows expanding overlaid events.
Since it is important for stakeholders to be
quickly and efficiently informed about the type and
gravity of the event, various icons are used to rep-
resent the type or group of events visually (see Fig-
ure 3). We use general forms of icons for violent
events and specific forms of icons for natural and
man-made disasters. For violent events, the gen-
eral form represents the major consequence of the
event, except for kidnappings, where specific icons
are used. Independently of the type of event, all
icons are sized according to the damage caused,
i.e. it is dependent on the number of victims in-
volved in the event. Also, to highlight the events
with a more significant damage, a border is drawn
around the icon to indicate that a threshold of peo-
ple involved has been passed.
The online demo is available for English, Italian
and French. We are currently working on adapt-
ing the event extraction engine to other languages,
including Russian, Spanish, Polish, German and
Arabic. A more thorough description of the sys-
tem can be found in (Tanev et al, 2008; Piskorski
et al, 2008).
References
Ashish, N., D. Appelt, D. Freitag, and D. Zelenko. 2006.
Proceedings of the workshop on Event Extraction and Syn-
thesis, held in conjunction with the AAAI 2006 conference.
Menlo Park, California, USA.
Best, C., E. van der Goot, K. Blackler, T. Garcia, and
D. Horby. 2005. Europe Media Monitor. Technical Re-
port EUR 22173 EN, European Commission.
Cunningham, H., D. Maynard, and V. Tablan. 2000. JAPE: a
Java Annotation Patterns Engine (Second Edition). Tech-
nical Report, CS?00?10, University of Sheffield, Depart-
ment of Computer Science.
?
Kidnap
K
A
Arrest
R
Release
V
Video
V
Man
Made
?Violent EventUndefined Violent EventKilled Violent EventInjured Violent EventKindnapped Violent EventArrest Hostage Release VideoRelease Violent EventNo Consequneces
Man Made
Disaster
Man Made 
Fire
Man Made
Explosion
ND
!
Natural
Dister
Volcanic 
Eruption
Tsunami Earthquake Landslide
?
Avalanche Tropical
Storm
Lightning
Strike
Storm
Snow
Storm
Flood Wild Fire
Heatwave
Key to Symbols
Consequence Significance (number of people involved)
No Circle  = up to 10 Red Circle = More than 100Yellow Circle= between 10 and 100
Humanitarian
Crisis
Trial
Unclassified
Figure 3: Key to event type icons and magnitude
indicators
Dro?zd?zy?nski, W., H.-U. Krieger, J. Piskorski, U. Sch?afer,
and F. Xu. 2004. Shallow Processing with Unification
and Typed Feature Structures ? Foundations and Appli-
cations. K?unstliche Intelligenz, 2004(1):17?23.
Grishman, R., S. Huttunen, and R. Yangarber. 2002. Real-
time Event Extraction for Infectious Disease Outbreaks.
Proceedings of the Human Language Technology Confer-
ence (HLT) 2002.
King, G. and W. Lowe. 2003. An Automated Information
Extraction Tool for International Conflict Data with Per-
formance as Good as Human Coders: A Rare Events Eval-
uation Design. International Organization, 57:617?642.
Piskorski, J., H. Tanev, M. Atkinson, and E. Van der Goot.
2008. Cluster-centric Approach to News Event Extraction.
In Proceedings of MISSI 2008, Wroclaw, Poland.
Piskorski, J. 2007. ExPRESS Extraction Pattern Recogni-
tion Engine and Specification Suite. In Proceedings of the
International Workshop Finite-State Methods and Natu-
ral language Processing 2007 (FSMNLP?2007), Potsdam,
Germany.
Piskorski, J. 2008. CORLEONE ? Core Linguistic Entity
Online Extraction. Technical report 23393 EN, Joint Re-
search Centre of the European Commission, Ispra, Italy.
Tanev, H. and P. Oezden-Wennerberg. 2008. Learning to
Populate an Ontology of Violent Events (in print). In
Fogelman-Soulie, F. and Perrotta, D. and Piskorski, J. and
Steinberger, R., editor, NATO Security through Science Se-
ries: Information and Communication Security. IOS Press.
Tanev, H., J. Piskorski, and M. Atkinson. 2008. Real-
Time News Event Extraction for Global Crisis Monitor-
ing. In Proceedings of the 13
th
International Conference
on Applications of Natural Language to Information Sys-
tems (NLDB 2008, Lecture Notes in Computer Science Vol.
5039), pages 207?218. Springer-Verlag Berlin Heidelberg.
Yangarber, R. 2003. Counter-Training in Discovery of Se-
mantic Patterns. In Proceedings of the 41
st
Annual Meet-
ing of the ACL.
148
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 65?68,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Event Extraction for Balkan Languages
Vanni Zavarella, Dilek K?uc? ?uk, Hristo Tanev
European Commission
Joint Research Centre
Via E. Fermi 2749
21027 Ispra (VA), Italy
first.last@jrc.ec.europa.eu
Ali H?urriyeto
?
glu
Center for Language Studies
Radboud University Nijmegen
P.O. Box 9103
NL-6500 HD Nijmegen
a.hurriyetoglu@let.ru.nl
Abstract
We describe a system for real-time detec-
tion of security and crisis events from on-
line news in three Balkan languages: Turk-
ish, Romanian and Bulgarian. The system
classifies the events according to a fine-
grained event type set. It extracts struc-
tured information from news reports, by
using a blend of keyword matching and
finite-state grammars for entity recogni-
tion. We apply a multilingual methodol-
ogy for the development of the system?s
language resources, based on adaptation
of language-independent grammars and on
weakly-supervised learning of lexical re-
sources. Detailed performance evaluation
proves that the approach is effective in de-
veloping real-world semantic processing
applications for relatively less-resourced
languages.
1 Introduction
We describe a real-time event extraction system
for three less-resourced languages: Bulgarian, Ro-
manian and Turkish
1
. The goal of event extraction
is to identify instances of a specified set of event
types in natural language texts, and to retrieve
database-like, structured information about event
participants and attributes: these are the entities
that are involved in the event and fill type-specific
event roles (Ashish et al., 2006). For example,
in the fragment ?Three workers were injured in
a building collapse?, the phrase ?three workers?
will be assigned a semantic role Injured of the
event type ManMadeDisaster template.
Gathering and tracking such information over
time from electronic news media plays a crucial
1
While belonging to three distant language families,
namely Slavic, Romance and Turkic, respectively, they are
spoken in the same geopolitical area, the Balkans.
role for the development of open-source intelli-
gence systems, particularly in the context of global
news monitoring of security threats, mass emer-
gencies and disease outbreaks (Yangarber et al.,
2005). In this view, it has been proved that be-
ing able to rely on highly multilingual text mining
tools and language resources is of paramount im-
portance, in order to achieve an unbiased coverage
of global news content (Steinberger, 2012).
The system language components include fi-
nite state-based entity extraction grammars and
domain-specific semantic lexica. These are
adapted to the target language from existing
language-independent resources or built by using
semi-supervised machine learning algorithms, re-
spectively. Most importantly, the lexical acquisi-
tion methods we put into place neither make use
of any language knowledge nor require to have an-
notated corpora available.
Section 2 outlines the main processing stages of
the application. In Section 3 we describe the meth-
ods applied to acquire and adapt the system?s lan-
guage knowledge bases. Finally, in Section 4 we
report on an evaluation on event type classification
and on the extraction of slot fillers for event tem-
plates, and we briefly discuss system performance
and prospective improvements.
2 System Architecture
As depicted in Figure 1 (Tanev et al., 2009), first
news feeds are clustered, upstream of the event
extraction engine, by applying similarity metrics
over meta data (named entities, locations, cate-
gories) extracted from single articles by dedicated,
multilingual software.
Event extraction begins by preprocessing the ti-
tle and first three sentences of each article within
a cluster. This encompasses: fine-grained tok-
enization, sentence splitting, domain-specific dic-
tionary look-up (i.e. matching of key terms in-
dicating numbers, quantifiers, person titles, per-
65
Meta-Data Creation
Real Time Clusterer
Geo-Locating
Core Linguistic Analysis
Cascaded Event 
Extraction  Grammar 
Application
Entity Role 
Disambiguator
Victim 
Arithmetic
Event Type 
Classifier
Event Description
Assembly
News
News
News
News
News
Figure 1: Event extraction processing chain
son groups descriptors like civilians, policemen
and Shiite), and finally morphological analysis,
simply consisting of lexicon look-up on large
domain-independent morphological dictionaries
from the MULTEXT project (Erjavec, 2004). Sub-
sequently, a multi-layer cascade of finite-state ex-
traction grammars in the ExPRESS formalism
(Piskorski, 2007) is applied on such more ab-
stract representation of the article text, in order
to: a)identify entity referring phrases, such as
persons, person groups, organizations, weapons,
etc. b) assign them to event specific roles by lin-
ear combination with event triggering surface pat-
terns. For example, in the text ?Iraqi policemen
shot dead an alleged suicide bomber? the gram-
mar should extract the phrase ?Iraqi policemen?
and assign to it the semantic role Perpetrator,
while the phrase ?alleged suicide bomber? should
be extracted as Dead. We use a ?lexicon? of 1/2-
slot patterns of the form:
<DEAD[Per]> was shot by <PERP>
<KIDNAP[Per]> has been taken hostage
where each slot position is assigned an event-
specific semantic role and includes a type restric-
tion (e.g. Person) on the entity which may fill
the slot.
Finally, we aggregate and validate information
extracted locally from each single article in the
same cluster, such as entity role assignment, vic-
tim counts and event type.
We categorize the main event from each cluster
with respect to a fine-grained event type set, shown
in Table 1.
The event classification module consists of a
blend of keyword matching, event role detection
and a set of rules controlling their interaction.
First, for each event type, we deploy: a) a list
of weighted regular expression keyword patterns:
each pattern match is awarded the corresponding
weight, and an event type is triggered when the
weight sum exceeds a defined threshold; b) a set of
boolean pattern combinations: OR pattern lists are
combined by the AND operator, each pattern is a
restricted regular expression and conjunctions are
restricted by proximity constraints. For example
in order to detect TerroristAttack we use the fol-
lowing combination (translated here in English):
(?bomb? OR ?explosion? OR....) AND (?terrorist?
OR ?Al Qaida? OR..).
Besides the event TYPE, the other main
slots of an output event frame include:
TYPE, DEAD, DEAD-COUNT, ARRESTED,
ARRESTED-COUNT, PERPETRATOR, WEAPON,
etc.
The system will be demonstrated using a KML-
aware earth browser
2
. Figure 2 shows a sample
output event template.
3 Development of language resources
The system?s language components are:
Event grammar rules They consist of regular
expressions over flat feature structures whose el-
ements include, among the others, semantic types
from the domain lexica. We use them to locally
parse semantic entities such as person names, per-
son group descriptions, and their clausal combi-
nation with verbal event patterns (see Section 2).
Grammars in target languages are compiled by
adapting the existing rules from source languages,
such as English, while the bulk of grammar devel-
opment mostly consists of providing suitable lexi-
cal resources.
Semantic dictionaries Domain-specific lexica,
listing a number of (possibly multi-word) expres-
sions sub-categorized into semantic classes rel-
evant for the event domain, with limited or no
linguistic annotation, are used by entity recogni-
tion grammar rules. Such lexica were created us-
ing the weakly supervised terminology extraction
algorithm LexiClass (Ontopopulis), described in
(Tanev et al., 2009). In order to enforce syntactic
2
E.g. Google Earth. Notice that Geocoding is cur-
rently performed at the level of article text by a language-
independent algorithm which is not yet integrated within the
event detection process, while Document Creation Date is
currently used as the event Date slot filler.
66
Figure 2: A sample output template of the system
constraints (e.g. Case) into event clause rules for
Romanian language, we have enriched learnt lex-
ical entries for the semantic classes with morpho-
logical annotations, using MULTEXT resources.
For Turkish, as we do not currently perform mor-
phological analysis, we have rather included com-
mon inflected forms of the applicable lexical en-
tries, resulting in larger lexica.
Event triggering patterns They are also ac-
quired semi-automatically, starting with a set of
seed examples and an article clustering, by deploy-
ing the paraphrase learning algorithm described in
(Tanev et al., 2008). For Bulgarian, the grammar,
semantic dictionaries and event patterns were cre-
ated simultaneously, following a semi-automatic
approach, described in (Tanev and Steinberger,
2013). In particular, we learned a list of terms
referring to people, institutions and organizations
and the corresponding pre- and post-modifiers
(about 5000 terms). In the same manner, we
learned about 550 surface patterns for killing, in-
juring, kidnapping and arresting actions, together
with a 4 level grammar cascade.
Keyword terms The keyword sets used in the
event type definitions, namely the OR lists in
the boolean pattern combinations (see Section 2
above), can be viewed as instances of some more
abstract semantic classes, that a domain expert
uses to model a target event scenario. These
classes are semi-automatically acquired using the
LexiClass algorithm, and then manually com-
Table 1: Event type set
AirMissileAttack Landslide
ArmedConflict LightningStrike
Arrest ManMadeDisaster
Assassination MaritimeAccident
Avalanche PhysicalAttack
BioChemicalAttack Robbery
Bombing Shooting
Disorder/Protest/Mutiny Stabbing
Earthquake Storm
Execution TerroristAttack
Explosion TropicalStorm
Floods Tsunami
HeatWave Vandalism
HeavyWeaponsFire VolcanicEruption
HostageVideoRelease Wildfire
HumanitarianCrisis WinterStorm
Kidnapping NONE
bined. As Turkish is an agglutinative language,
we have frequently added wildcards at the ends of
keywords to cover possible inflected forms.
4 Experiments and Evaluation
System performance is evaluated on three differ-
ent extractive tasks, carried out on the titles and
first three sentences of single news articles: event
type classification, event role name/description ex-
traction and victim counting.
We collected test corpora of 52, 126 and 115
news articles for Bulgarian, Romanian and Turk-
ish, respectively, spanning over a time range of 2
months
3
. For each article in the gold standard, we
3
Articles were manually selected using news aggregators
such as Google News. Type distribution resulted in zeroes for
67
Table 2: System performance in single article extraction mode.
Lang
Type Dead Injured Arrested Kidnapped Perpetrator Weapon
MRR mF MF MSE mF MF MSE mF MF MSE mF MF MSE mF MF mF MF
BG 0.34 0.27 0.68 17.08 0.44 0.6 108.82 0.22 1.0 7.69 0.4 0.5 0.71 0.0 0.0 0.39 1.0
RO 0.22 0.48 0.73 36.53 0.46 0.97 18.57 0.39 0.82 80.5 0.2 1.0 2.14 0.07 0.67 0.1 0.2
TR 0.66 0.73 0.79 16.41 0.85 0.91 0.24 0.31 0.36 52.17 0.4 0.33 0.82 0.25 0.67 0.77 1.0
annotated: a list of applicable types, ordered by
relevance, for the main event reported in the arti-
cle; the set of all the names/descriptions occurring
in the text for each applicable event role, merging
morphological variants; the cumulative count for
the roles Dead, Injured, Kidnapped and Arrested.
Event type classification is evaluated by apply-
ing an adapted version of the mean reciprocal rank
(MRR) score, used in Information Retrieval to
evaluate processes producing a list of relevance or-
dered query responses. In our case, the MRR for a
set of N articles is:
MRR =
1
|N |
N
?
i=1
1
rank
i
where rank is the rank of the system type re-
sponse within the gold standard type list for each
article.
For each role name/description extraction sepa-
rately, we compute standard Precision, Recall and
F1-measure on system responses, based on partial,
n-gram match with gold standard responses, ignor-
ing morphological suffixes.
Finally, we record the root Mean Squared Er-
ror (MSE) of system output victim count values
against gold standard, over all applicable roles.
Table 2 summarizes the evaluation results. mF
and MF columns for each role description task
represent respectively the micro and macro aver-
age F1-measure over the test set.
Overall, the performance figures are in line with
previous evaluations on other languages (Tanev et
al., 2009). This proves the methodology is ef-
fective on adapting the system to new languages
even with little lexical and syntactical proximity.
Turkish system consistently outperforms the oth-
ers, and it also underwent the most resource devel-
opment cycles: this suggests that applying learn-
ing iterations, alternated with human filtering, to
the language resources, can increase system ac-
curacy, eventually making it usable for real-world
applications. System accuracy is still unreliable
for victim counting. One of the main reasons for
large errors in victim counting is that the system
some less frequent event types.
interprets historical victim statistics reported in ar-
ticles as event instances. We are currently imple-
menting temporal and discourse heuristics to mit-
igate this problem.
Acknowledgments
This study is supported in part by a postdoctoral
research grant from T
?
UB
?
ITAK and by the Dutch
national program COMMIT.
References
Naveen Ashish, Doug Appelt, Dayne Freitag, and
Dmitry Zelenko. 2006. Proceedings of the work-
shop on event extraction and synthesis. Technical
report, AAAI.
Tomaz Erjavec. 2004. MULTEXT-East morphosyn-
tactic specifications.
Jakub Piskorski. 2007. ExPRESS?extraction pattern
recognition engine and specification suite. In Pro-
ceedings of the International Workshop Finite-State
Methods and Natural language Processing.
Ralf Steinberger. 2012. A survey of methods to ease
the development of highly multilingual text mining
applications. Language Resources and Evaluation,
46(2):155?176.
Hristo Tanev and Josef Steinberger. 2013. Semi-
automatic acquisition of lexical resources and gram-
mars for event extraction in Bulgarian and Czech. In
Proceedings of the 4th Biennial International Work-
shop on Balto-Slavic Natural Language Processing,
pages 110?118.
Hristo Tanev, Jakub Piskorski, and Martin Atkinson.
2008. Real-time news event extraction for global
crisis monitoring. In E. Kapetanios, V. Sugumaran,
and M. Spiliopoulou, editors, Natural Language and
Information Systems, volume 5039 of Lecture Notes
in Computer Science, pages 207?218.
Hristo Tanev, Vanni Zavarella, Jens Linge, Mijail
Kabadjov, Jakub Piskorski, Martin Atkinson, and
Ralf Steinberger. 2009. Exploiting Machine Learn-
ing Techniques to Build an Event Extraction Sys-
tem for Portuguese and Spanish. Linguam?atica,
1(2):55?66.
Roman Yangarber, Lauri Jokipii, Antti Rauramo, and
Silja Huttunen. 2005. Extracting information about
outbreaks of infectious epidemics. In Proceedings
of the HLT/EMNLP.
68
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 25?30,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Detecting Event-Related Links and Sentiments from Social Media Texts
Alexandra Balahur and Hristo Tanev
European Commission Joint Research Centre
Via E. Fermi 2749, T.P. 267
21027 Ispra (VA), Italy
{alexandra.balahur, hristo.tanev}@jrc.ec.europa.eu
Abstract
Nowadays, the importance of Social Me-
dia is constantly growing, as people often
use such platforms to share mainstream
media news and comment on the events
that they relate to. As such, people no
loger remain mere spectators to the events
that happen in the world, but become part
of them, commenting on their develop-
ments and the entities involved, sharing
their opinions and distributing related con-
tent. This paper describes a system that
links the main events detected from clus-
ters of newspaper articles to tweets related
to them, detects complementary informa-
tion sources from the links they contain
and subsequently applies sentiment analy-
sis to classify them into positive, negative
and neutral. In this manner, readers can
follow the main events happening in the
world, both from the perspective of main-
stream as well as social media and the pub-
lic?s perception on them.
This system will be part of the EMM me-
dia monitoring framework working live
and it will be demonstrated using Google
Earth.
1 Introduction
In the context of the Web 2.0, the importance
of Social Media has been constantly growing in
the past years. People use Twitter, Facebook,
LinkedIn, Pinterest, blogs and Web forums to give
and get advice, share information on products,
opinions and real-time information about ongoing
and future events. In particular Twitter, with its
half a billion active members, was used during dis-
asters, protests, elections, and other events to share
updates, opinions, comments and post links to on-
line resources (e.g. news, videos, pictures, blog
posts, etc.). As such, Twitter can be used as a com-
plementary source of information, from which we
can retrieve additional facts, but also learn about
the attitude of the people towards certain events.
On the one hand, news from the traditional me-
dia focus on the factual side of events, important
for the society or at least large groups of people.
On the other hand, social media reflects subjec-
tive interpretations of facts, with different levels of
relevance (societal or only individual). Therefore,
the events reported in online news can be consid-
ered a point of intersection for both types of me-
dia, which are able to offer complementary views
on these events.
In this context, we describe a system that we
developed as an additonal component to the EMM
(Europe Media Monitor)1 news monitoring frame-
work, linking mainstream news to related texts
from social media and detecting the opinion (sen-
timent) users express on these topics.
In the EMM news monitoring system, the dif-
ferent news sites are monitored and new articles
are scraped from them, with a refresh rate of 10
minutes. Subsequently, news items are clustered
and the most important ones are displayed (top
10). These are called ?stories?. Our system subse-
quently links these stories to messages from Twit-
ter (tweets) and extracts the related URLs they
contain. Finally, it analyzes the sentiments ex-
pressed in the tweets by using a hybrid knowledge-
based and statistical sentiment detection module.
The overview of the system is depicted in Figure
1http://emm.jrc.it/NewsBrief/clusteredition/en/latest.html
25
1.
Figure 1: Overview of the news clusters-Twitter
linking and sentiment analysis system.
The system will be demonstrated using the
Google Earth interface (Figure 2), presenting the
characteristics of the event described in the story
(type, date, location, the first words in the arti-
cle that is the centroid of the news cluster for that
story). In addition, we present new information
that we extract from Twitter - links (URLs) that
we find from the tweets we retrieved linked to the
story and positive, negative and neutral sentiment,
respectively, as a proportion of the total number of
tweets retrieved.
Figure 2: Demo interface for the event-Twitter
linking and sentiment analysis.
2 Related Work and Contribution
The work presented herein is mostly related to the
linking of events with social media texts and sen-
timent analysis from Twitter.
Although Twitter was used as an information
source in the context of different crisis events, rel-
atively little work focused on linking and extract-
ing content about events which are known a priori,
e.g., Becker et al [2011].
In this context, the main challenge is to deter-
mine relevant keywords to search for event-related
tweets and rank them according to their relevance.
Related approaches (e.g., Verma et al [2011]) re-
port on the use of semantic features (e.g., objec-
tivity, impersonality, formality, etc.) for detecting
tweets with content relevant to situational aware-
ness during mass emergencies. Other approaches
elaborate on machine learning-based techniques
for Named Entity Recognition (NER) from tweets,
which are subsequently employed as search query
terms ( Ritter et al [2011], Liu et al [2011]).
Related research on sentiment analysis from
Twitter was done by Alec Go and Huang [2009],
Pak and Paroubek [2010] and Agarwal et al
[2011]. Alec Go and Huang [2009] and Pak and
Paroubek [2010] exploit the presence of emoticons
that represent positive or negative feelings to build
a training set of tweets with sentiment labels, using
which they build models based on n-gram features
and part-of-speech tags. Agarwal et al [2011] em-
ploy emoticons dictionaries and replace certain el-
ements such as URLs and topics with predefinded
labels. They employ syntactic features and spe-
cialized tree kernels and obtain around 75% to
80% accuracy for the sentiment classification.
The main contributions of our system reside in
the linking of mainstream news to the complemen-
tary content found in social media (tweets and,
through them, to the links to additional informa-
tion sources like blogs, flickr, youtube, etc.) and
the analysis of sentiment on these important news.
For events such as ?The Arab Spring?, protests, fi-
nancial news (e.g. the fluctuations of the Euro, the
bailout of different European countries, the rise in
unemployment rate, etc.), it was seen that the sen-
timent expressed in social media has a high impact
on the subsequent development of the story2 (Saif
et al [2012], Bollen et al [2011]). The impact of
sentiment expressed in social media is also visi-
ble for topics which apparently have an apriori va-
lence (e.g. disasters, crisis, etc.). Nevertheless, in
these cases, people communicate using the social
media platforms not only to express their negative
feelings, but also their will to help, their situation,
their messages of encouragement, their grateful-
ness for the help and so on.
2http://cs229.stanford.edu/proj2011/ChenLazer-
SentimentAnalysisOfTwitterFeedsForThePrediction
OfStockMarketMovement.pdf
26
Secondly, the methods employed in our system
are simple, work fast and efficient and can be eas-
ily adapted to other languages.
Finally, the methods presented take into account
the specificity of social media languages, applying
methods to normalize the language and adapting
the features considered for the supervised learning
process.
3 Linking News Clusters to Twitter
The first step in our system involves linking the
news stories detected by EMM to related tweets.
The linking system employs the Twitter Search
API3. For each news story, our application detects
relevant URLs by finding tweets that are lexically
similar to the news story, represented by a cluster
of news, and are mentioned frequently in Twitter.
In Figure 3, we provide an example of the top six
stories on the afternoon of April 2nd, 2013.
Figure 3: Top six clusters of news in the afternoon
of April 2nd, 2013.
In order to detect lexically similar tweets, we
use vector similarity: We build a term vector for
both the news story and the tweet and then we
consider as a similarity measure the projection
of the tweet vector on the story vector. We do
not calculate cosine similarity, since this would
give an advantage to short tweets. We experi-
mentally set a similarity threshold above which
the tweets with URL are accepted. To define
the similarity threshold and the coefficients in the
URL ranking formula, we used a development set
of about 100 randomly selected English-language
news clusters, downloaded during a week. The
3https://dev.twitter.com/docs/api/1/get/search
threshold and the coefficients were derived empir-
ically. We consider experimenting with SVM and
other machine-learning approaches to define these
parameters in a more consistent way.
Once the tweets that relate to the news story are
retrieved, we evaluate each URL taking into ac-
count the following parameters:
? Number of mentions, which we will desig-
nate as Mentions.
? Number of retweets, designated Retweet.
? Number of mentions in conversations, desig-
nated InConv.
? Number of times the URL was favortited,
designated Favorited.
? Number of tweets which replied to tweets,
mentioning the URL, designated ReplyTo.
The score of the URL is calculated using the
following empirically derived formula. The coef-
ficients were defined based on the empirical anal-
ysis described above.
score(URL) = ((Mentions?1)+Retweets.1, 3
+Favorited ? 4).(InConv + 2 ?ReplyTo + 1)
In this formula we give slight preference to the
retweets with respect to the mentions. We made
this choice, since retweets happen inside Twitter
and reflect the dynamics of the information spread
inside this social media. On the other hand, multi-
ple mentions of news-related tweets (which are not
retweeted) are due to clicking the ?Share in Twit-
ter? button, which nowadays is present on most
of the news sites. In this way, news from visited
web sites appear more often in Twitter. This phe-
nomena is to be further explored. It should also be
noted that our formula boosts significantly URLs,
which are mentioned inside a conversation thread
and even more the ones, to which there were ?re-
ply to? tweets. Conversations tend to be cen-
tered around topics which are of interest to Twit-
ter users and in this way they are a good indica-
tor of how interesting an URL is. Replying to a
tweet requires more time and attention than just
pressing the ?Retweet? button, therefore conversa-
tions show more interest to an URL, with respect
to retweeting. Examples of tweets extracted that
complement information from mainstream media
are presented in Figure 4.
27
Figure 4: Examples of tweets extracted on the
North Korea crisis (anonimized).
4 Sentiment Analysis on Tweets Related
to Events Reported in News
After extracting the tweets related to the main
news clusters detected by the media monitoring
system, we pass them onto the sentiment analy-
sis system, where they are classified according to
their polarity (into positive, negative and neutral).
In order to classify the tweet?s sentiment, we
employ a hybrid approach based on supervised
learning with a Support Vector Machines Sequen-
tial Minimal Optimization (SVM SMO - Platt
[1998]) linear kernel, on unigram and bigram fea-
tures, but exploiting as features sentiment dictio-
naries, emoticon lists, slang lists and other social
media-specific features. We do not employ any
specific language analysis software. The aim is to
be able to apply, in a straightforward manner, the
same approach to as many languages as possible.
The approach can be extended to other languages
by using similar dictionaries that have been cre-
ated in our team.
The sentiment analysis process contains two
stages: preprocessing and sentiment classification.
4.1 Tweet Preprocessing
The language employed in Social Media sites is
different from the one found in mainstream me-
dia and the form of the words employed is some-
times not the one we may find in a dictionary. Fur-
ther on, users of Social Media platforms employ a
special ?slang? (i.e. informal language, with spe-
cial expressions, such as ?lol?, ?omg?), emoticons,
and often emphasize words by repeating some of
their letters. Additionally, the language employed
in Twitter has specific characteristics, such as the
markup of tweets that were reposted by other users
with ?RT?, the markup of topics using the ?#?
(hash sign) and of the users using the ?@? sign.
All these aspects must be considered at the time
of processing tweets. As such, before applying su-
pervised learning to classify the sentiment of the
tweets, we preprocess them, to normalize the lan-
guage they contain. The preprocessing stage con-
tains the following steps:
? Repeated punctuation sign normalization.
In the first step of the preprocessing, we de-
tect repetitions of punctuation signs (?.?, ?!?
and ???). Multiple consecutive punctuation
signs are replaced with the labels ?multi-
stop?, for the fullstops, ?multiexclamation?
in the case of exclamation sign and ?multi-
question? for the question mark and spaces
before and after.
? Emoticon replacement. In the second step
of the preprocessing, we employ the anno-
tated list of emoticons from SentiStrength4
and match the content of the tweets against
this list. The emoticons found are replaced
with their polarity (?positive? or ?negative?)
and the ?neutral? ones are deleted.
? Lower casing and tokenization. Subse-
quently, the tweets are lower cased and split
into tokens, based on spaces and punctuation
signs.
? Slang replacement. The next step involves
the normalization of the language employed.
In order to be able to include the semantics
of the expressions frequently used in Social
Media, we employed the list of slang from a
specialized site 5.
? Word normalization. At this stage, the to-
kens are compared to entries in Roget?s The-
saurus. If no match is found, repeated
letters are sequentially reduced to two or
one until a match is found in the dictio-
nary (e.g. ?perrrrrrrrrrrrrrrrrrfeeect? becomes
?perrfeect?, ?perfeect?, ?perrfect? and subse-
quently ?perfect?). The words used in this
form are maked as ?stressed?.
? Affect word matching. Further on, the tokens
in the tweet are matched against three dif-
ferent sentiment lexicons: General Inquirer,
LIWC and MicroWNOp, which were pre-
viously split into four different categories
4http://sentistrength.wlv.ac.uk/
5http://www.chatslang.com/terms/social media
28
(?positive?, ?high positive?, ?negative? and
?high negative?). Matched words are re-
placed with their sentiment label - i.e. ?pos-
itive?, ?negative?, ?hpositive? and ?hnega-
tive?.
? Modifier word matching. Similar to the
previous step, we employ a list of expres-
sions that negate, intensify or diminish the
intensity of the sentiment expressed to detect
such words in the tweets. If such a word is
matched, it is replaced with ?negator?, ?in-
tensifier? or ?diminisher?, respectively.
? User and topic labeling. Finally, the users
mentioned in the tweet, which are marked
with ?@?, are replaced with ?PERSON? and
the topics which the tweet refers to (marked
with ?#?) are replaced with ?TOPIC?.
4.2 Sentiment Classification of Tweets
Once the tweets are preprocessed, they are passed
on to the sentiment classification module. We em-
ployed supervised learning using SVM SMO with
a linear kernel, employing boolean features - the
presence or absence of unigrams and bigrams de-
termined from the training data (tweets that were
previousely preprocessed as described above) that
appeared at least twice. Bigrams are used espe-
cially to spot the influence of modifiers (nega-
tions, intensifiers, diminishers) on the polarity of
the sentiment-bearing words. We tested the ap-
proach on different datasets and dataset splits, us-
ing the Weka data mining software 6. The training
models are built on a cluster of computers (4 cores,
5000MB of memory each).
5 Evaluation and Discussion
5.1 Evaluation of the News-Twitter Linking
Component
The algorithm employed to retrieve tweets simi-
lar to news clusters was evaluated by Tanev et al
[2012]. The precision attained was 75%. Recall
cannot be computed, as the use of the Twitter API
allows only the retrieval of a subset of tweets.
In order to evaluate the link extraction compo-
nent, we randomly chose 68 URLs, extracted from
10 different news stories. For each URL, we eval-
uated its relevance to the news story in the follow-
ing way: A URL is considered relevant only if it
6http://www.cs.waikato.ac.nz/ml/weka/
reports about the same news story or talks about
facts, like effects, post developments and motiva-
tions, directly related to this news story. It turned
out that 66 out of the 68 were relevant, which gives
accuracy of 97%.
5.2 Evaluation of the Sentiment Analysis
System
In order to evaluate the sentiment analysis sys-
tem on external resources, we employed the data
provided for training in the SemEval 2013 Task
2 ?Sentiment Analysis from Twitter? 7. The ini-
tial training data has been provided in two stages:
1) sample datasets for the first task and the sec-
ond task and 2) additional training data for the two
tasks. We employ the joint sample datasets as test
data (denoted as t?) and the data released subse-
quently as training data (denoted as T?). We em-
ploy the union of these two datasets to perform
cross-validation experiments (the joint dataset is
denoted as T ? +t?. The characteristics of the
dataset are described in Table 1. On the last col-
umn, we also include the baseline in terms of ac-
curacy, which is computed as the number of ex-
amples of the majoritary class over the total num-
ber of examples. The results of the experiments
Data #Tweet #Pos #Neg #Neu B%
T* 19241 4779 2343 12119 62
t* 2597 700 393 1504 57
T*+t* 21838 5479 2736 13623 62
Table 1: Characteristics of the training (T*), test-
ing (t*) and joint training and testing datasets.
are presented in Table 2. Given the difficulty of
Measure Train(T*) & test(t*) 10-fold CV
Acc. 0.74 0.93
Ppos 0.66 0.91
Rpos 0.88 0.69
Pneg 0.94 0.62
Rneg 0.81 0.49
Pneu 0.93 0.80
Rneg 0.97 0.82
Table 2: Results in terms of accuracy and preci-
sion and recall per polarity class on training and
test sets evaluation and 10-fold cross-validation.
language in social media, the results are good and
7http://www.cs.york.ac.uk/semeval-2013/task2/
29
useful in the context of our application (Figure 2).
6 Conclusions and Future Work
In this demo paper, we presented a system that
links mainstream media stories to tweets that com-
ment on the events covered. The system retrieves
relevant tweets, extracts the links they contain and
subsequently performs sentiment analysis. The
system works at a good level, giving an accurate
picture of the social media reaction to the main-
stream media stories.
As future work, we would like to extend the sys-
tem to more languages and analyze and include
new features that are particular to social media to
improve the performance of both the retrieval and
sentiment analysis components.
Acknowledgements
We would like to thank the EMM team of the OP-
TIMA action at the European Commission Joint
Research Centre for the technical support.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen
Rambow, and Rebecca Passonneau. Sentiment
analysis of twitter data. In Proceedings of LSM
2011, LSM ?11, pages 30?38, 2011.
Richa Bhayani Alec Go and Lei Huang. Twit-
ter sentiment classication using distant supervi-
sion. Technical report, Technical report, Stan-
ford University, 2009.
Hila Becker, Feiyang Chen, Dan Iter, Mor Naa-
man, and Luis Gravano. Automatic identifi-
cation and presentation of twitter content for
planned events. In Proceedings of ICWSM
2011, 2011.
J. Bollen, H. Mao, and X. Zeng. Twitter mood
predicts the stock market. Journal of Computa-
tional Science, 2011.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and
Ming Zhou. Recognizing Named Entities in
Tweets. In Proceedings of ACL 2011, pages
359?367, Stroudsburg, PA, USA, 2011.
Alexander Pak and Patrick Paroubek. Twitter
based system: Using twitter for disambiguat-
ing sentiment ambiguous adjectives. In Pro-
ceedings of SemEval 2010, SemEval ?10, pages
436?439, 2010.
John C. Platt. Sequential minimal optimization:
A fast algorithm for training support vector ma-
chines. Technical report, Advances in Kernel
Methods - Support Vector Learning, 1998.
Alan Ritter, Sam Clark, Mausam, and Oren Et-
zioni. Named Entity Recognition in Tweets: An
Experimental Study. In Proceedings of EMNLP
2011, pages 1524?1534, Edinburgh, Scotland,
UK., 2011.
Hassan Saif, Yulan He, and Harith Alani. Alleviat-
ing data sparsity for twitter sentiment analysis.
In Making Sense of Microposts (#MSM2012),
pages 2?9, 2012.
Hristo Tanev, Maud Ehrmann, Jakub Piskorski,
and Vanni Zavarella. Enhancing event descrip-
tions through twitter mining. In John G. Bres-
lin, Nicole B. Ellison, James G. Shanahan, and
Zeynep Tufekci, editors, ICWSM. The AAAI
Press, 2012.
Sudha Verma, Sarah Vieweg, William Corvey,
Leysia Palen, James Martin, Martha Palmer,
Aaron Schram, and Kenneth Anderson. Natural
Language Processing to the Rescue? Extracting
?Situational Awareness?? Tweets During Mass
Emergency. In Proceedings of ICWSM 2011,
pages 385?392. AAAI, 2011.
30
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 58?63, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
FSS-TimEx for TempEval-3: Extracting Temporal Information from Text
Vanni Zavarella
Joint Research Centre
European Commission
21027 Ispra, Italy
vanni.zavarella
@jrc.ec.europa.eu
Hristo Tanev
Joint Research Centre
European Commission
21027 Ispra, Italy
hristo.tanev
@jrc.ec.europa.eu
Abstract
We describe FSS-TimEx, a module for the
recognition and normalization of temporal ex-
pressions we submitted to Task A and B of
the TempEval-3 challenge. FSS-TimEx was
developed as part of a multilingual event ex-
traction system, Nexus, which runs on top of
the EMM news processing engine. It consists
of finite-state rule cascades, using minimalis-
tic text processing stages and simple heuris-
tics to model the relations between events and
temporal expressions. Although FSS-TimEx
is already deployed within an IE application
in the medical domain, we found it useful to
customize its output to the TimeML standard
in order to have an independent performance
measure and guide further developments.
1 Introduction
The FSS-TimEx (Finite State-based Shallow Time
Extractor) system participating in TempEval-3 is in-
tegrated in the event extraction engine Nexus (Tanev
et al, 2008), developed at the EC?s Joint Research
Center for extracting event information from on-line
news articles gathered by the Europe Media Mon-
itor (EMM) news aggregation and analysis family
of applications(Steinberger et al, 2009). Nexus is
highly multilingual1 and easily portable across do-
mains through semi-automatic learning of lexical re-
sources. In the domain of epidemiological surveil-
lance, the event extraction task required a particu-
larly deep temporal information analysis, in order to
1Currently, it covers English, French, Italian, Spanish, Por-
tuguese, Turkish, Russian, Arabic.
detect temporal relations among event reports and
mitigate the classical event duplication problem. As
an example, from a report like:
The overall death toll has risen to 160
since the beginning of the year, after 2
patients in Gulu and 2 in Masindi died on
Tue 5 Dec 2000.
a system might be prevented to wrongly sum up the
two victim counts (160+4) only if it is made aware of
the inclusion relation between the first time interval
and the date, which in turn implies normalizing the
two temporal expressions.
Currently, FSS-TimEx is deployed for French,
English and Italian and extensions are foreseen for
further languages. Given such requirements for mul-
tilinguality, we developed FSS-TimEx using a lin-
guistically light-weight approach, applying shallow
processing modules only. On the other hand, as we
need to extract highly structured information out of
the detected temporal expressions, to be used in the
subsequent normalization phase, we mostly opted
for a rule-based approach, using finite-state gram-
mar cascades, rather than machine learning meth-
ods. Nonetheless, some of the required lexicons
were semi-automatically learned.
In our participation in Tasks A and B of the
TempEval-3, we experimented with adapting an ex-
isting timex recognition module for the English lan-
guage, to Spanish.
We first describe our system in 2,3 and 4, then in
5 we show and shortly discuss the results for Task
A and Task B, and conclude with some thoughts on
prospective developments.
58
2 System Modules
The system makes use of cascades of finite-state
grammar rules applied to the output of a set of shal-
low text processing modules.
Text Processing Modules. These include tok-
enization, sentence splitting, domain-specific dictio-
nary look-up and morphological analysis, which are
all part of the CORLEONE (Core Linguistic Entity
Online Extraction) engine (Piskorski, 2008). Mor-
phological analysis purely consists of matching text
tokens over full-form entries of a dictionary from the
MULTEXT project (Erjavec, 2004), which encodes
rich morphological features in a cross-lingual stan-
dard. Consequently, no PoS-tagging or parsing is
performed upstream of the extraction grammars.
Finite-State Grammar Engine. We use the Ex-
PRESS finite-state grammar engine (Piskorski,
2007). Grammars in the ExPRESS formalism con-
sist of cascades of pattern-action rules, whose left-
hand side (LHS) are regular expressions over flat
feature structures (FFS) and the right-hand side
(RHS) consists of a list of FFS (see Figure 1 be-
low for an example). Variable binding from LHS to
RHS, as well as string processing and Boolean op-
erators on the RHS, allow to impose relatively com-
plex constraints in the form of Boolean-valued pred-
icates.
Weakly-supervised Learning of Lexical Re-
sources. In order to determine the Class feature
for the event extraction task, we experimented with
using a language-independent method for weakly-
supervised lexical acquisition. The algorithm takes
as input a small set of seed terms, an unannotated
text corpus and a parameter for the number of boot-
strapping iterations: it then learns a ranked list of
further terms, which are likely to belong to the same
class, based on distributional n-gram features and
term clustering (Tanev et al, in press). Although
manual post-filtering is required, output term accu-
racy is reasonably high, and very high for top ranked
terms.
3 Event and Event Feature Detection
(Task B)
Although Nexus is a high precision event extraction
system, we have not deployed it to model the event
detection task. The reason is that Nexus is cus-
tomized to recognize a number of highly domain-
specific event types (e.g. Armed Conflict,
Earthquake,Terrorist Attack) and will
necessarily perform low in recall given the general,
domain-independent definition of events in Task
B. Instead, we tentatively used a small set of
language-dependent finite-state rules to model verb
phrase structure. Rules take as input MULTEXT
morphological tokens and detect verb phrases along
with a number of VP features, including Tense,
which is used by the temporal normalizer to ground
event modifying temporal expressions (see 4.2).
Class attribute was encoded in the morphologi-
cal dictionary by using the output of the machine
learning method sketched above: for each TimeML
Event Class (Pustejovsky et al, 2003), we provided
seed verb forms for all of its sub-classes, performed
multi-class learning, and used the main Class label
to annotate the union of output forms in the lexicon,
after some manual cleaning.
The OCCURRENCE class was used as the default
Class value for event verb forms, and it was overrid-
den whenever a more specific event Class value was
present2.
We do not cover event nominal forms, as after
some tests event referring and non-event referring
noun classes appeared too difficult to tell apart by
machine learning methods. Consequently, we ex-
pect system recall in Task B to be heavily limited.
4 Temporal Expressions (Task A)
FSS-TimEx?s temporal expression processing con-
sists of two stages.
In the Recognition phase, temporal expressions
are detected and segmented in text and a more ab-
stract representation of them is filled for further
processing. Local parsing of timexes is performed
by a cascade of hand-coded, partially language-
dependent finite-state grammar rules using the Ex-
PRESS engine, resulting in an intermediate fea-
2Otherwise, we chose randomly among alternative values of
Class-ambiguous event expressions.
59
rule :> ( (lex & [TYPE:"temp_signal", SURFACE:#signal, NORMALIZED:"INCLUDED"]
| lex & [TYPE:"temp_signal", NORMALIZED:"DURING"])
lex & [TYPE:"quantifier", NORMALIZED:#mod]? determiner?
lex & [TYPE:"temp_mod", OP:#op, REF_TYPE:#ref_type]
( (lex & [TYPE: "numeral", NORMALIZED:#amount1]
lex & [TYPE: "numeral", NORMALIZED:#amount2]?)
| token & [TYPE: "any_natural_number", SURFACE:#amount1]
lex & [TYPE:"time_unit", NUM:"p", GRAN:#gran]):x
-> x: period & [DIR:#op,REF_TYPE:#ref_type,MOD:#mod,GRAN:#gran,QUANT:#amount,SIGNAL:#signal]
& #amount := ConcForSum(#amount1,#amount2).
Figure 1: Sample recognition rule
ture structure-like representation, which is subse-
quently used by a language-independent Normaliza-
tion stage to compute exact values of the time ex-
pressions, according to the TimeML standard.
We judge that such a strict coupling of recognition
and normalization is better achieved through feature
extraction rules than by deploying two separate pro-
cesses3.
4.1 Recognizing Temporal Expressions
A cascade of around 90 rules is deployed for the En-
glish language. These comprise lower-level rules, in
charge of modelling language constructions in the
target language, and typization rules that check the
attribute configuration of lower-level rule output and
return a corresponding structure, typed according to
an intermediate annotation type set, exporting all at-
tribute values relevant for normalization.
As an example, the rule shown in Figure1 detects
single-boundary period expressions (e.g. in the pre-
vious four weeks or during the next five days).
Notice that the rule output type is the non
TimeML-compliant period (i.e. an anchored time
duration). This is an intermediate annotation type
which is subsequently converted into a TimeML
type (Duration) during the Normalization phase.
The temporal lexicon referenced by the gram-
mar contains around 300 entries for the English lan-
guage, classified into as many as 24 types, each de-
scribed by a small attribute list. Sample entries from
the English lexicon are listed in Figure 2.
This lexicon structure (types and attributes) was
applied as such to the Spanish language; lexicon
population was manually done in one day of work,
by first translating lexical triggers (e.g. day, month
3This architecture is very close to the one proposed by the
ITA-Chronos system (Negri, 2007).
monday | TYPE:day_name | NORMALIZED:Monday
weeks | TYPE:time_unit | GRAN:week | NUM:p
night | TYPE:day_period_name | NORMALIZED:NI
ago | TYPE:temp_adv | OP:- | REF_TYPE:speaker
last | TYPE:temp_mod | OP:- | REF_TYPE:speaker
since | TYPE:temp_signal | NORMALIZED:BEGIN
early | TYPE:mod | NORMALIZED:START
Figure 2: Sample lexicon entries
names, numerals) and then gathering more func-
tional entries (temporal adverbs, modifiers, etc.) by
running test rules on large corpora. It turned out that,
by using a parallel lexicon structure, we could re-
duce the cross-lingual re-arrangement of extraction
rules for the Spanish grammar, minimizing the work
cost to only 2 days, excluding fine tuning.
4.2 Normalization
Normalization is a fully language-independent pro-
cess, working with calendar representations of tem-
poral expressions4 built out of the output feature
structures from the Recognition phase. It comprises
two sub-processes:
Anchor selection. First, anchor selection deter-
mines and maintains a reference time for relative
timex resolution, starting by using the Article Cre-
ation Date and updating it along the resolution pro-
cess according to a simple search heuristic: select
the closest preceding resolved timex with a compat-
ible level of granularity. We experimented with two
alternative settings for this, one restricting the search
to timexes within the same sentence, the other span-
ning over the whole article text: we noticed a sys-
tematic gain in normalization accuracy with the for-
mer setting and we used it for Task A.
4The normalization is entirely implemented in Java code.
60
Timex-Event mapping. For certain timex
classes5 we need to resort to Tense information
from event-referring verb phrases in order to dis-
ambiguate between future and past interpretation.
For this purpose, a simple, syntax-free heuristic is
implemented to compute a mapping from each time
expression onto the event it modifies, which just
uses a weighted token distance metric, promoting
events preceding the timex over those following it.
Finally, calendar arithmetic is used to resolve and
normalize the value of relative timexes.
5 Results6
5.1 Temporal Expression Extraction
For English, our system scored in the middle range
over all participant systems on relaxed match F1
measure. Strict match figures are not indicative: in-
deed, temporal signals (like on in on Friday) were
systematically included in the extracted extent, con-
trary to the TIMEX3 tag specification, because this
is required by finite-state parsing of the IE system
with which FSS-TimEx was integrated.
Compared to the best performing system (BestEN
in Table1), our approach mainly suffered from rela-
tively low recall. Although such a rate of false neg-
atives can be expected from a rule-based approach,
in our case it was mostly due to two main ?bugs? in
the normalization code: first, in the process of tun-
ing system output types to TimeML, we erroneously
discarded date expressions introduced by temporal
signals, like in from now; secondly, we do not nor-
malize single adverbial expressions (currently), al-
though they are detected by grammar rules.
We outperformed in Precision the best F1 system.
Many false positives were all coming from a single
article, where the word season in flu season was sys-
tematically annotated as an event in the gold stan-
dard. This kind of context-based inference seems to
be out of reach for our rule-based, local parsing ap-
proach.
The major flaw in porting the system to Span-
ish language was a 28% Recall drop. Main types
5E.g. what we refer to as relativeTime or
relativeOffset, like on Thursday and this weekend, re-
spectively.
6Results were obtained in 1.89 and 1.97 seconds of com-
putation time respectively for English and Spanish data, on an
Intel Core i3 M380 2.53GHz processor.
of false negatives included fuzzy expressions (e.g.
hace tiempo), and compositional expressions.
Performance in timex classification and normal-
ization still falls behind top scoring systems. Finite-
state techniques can only parse local constructions,
greedily consuming as long text spans as possible:
therefore we systematically miss clausal relations
like in: The day before Raymond Roth was pulled
where we wrongly parsed a fully specified, relative
timex The day before. Similar cases resulted at the
same time in incorrect Type assignment, like in
Two years after his brain-cancer diagnosis where
we wrongly detect a Date type expression (Two
years after).
Inaccurate event Tense attribute extraction
sometimes caused wrong timex Value normaliza-
tion. One noticeable source of such an error is re-
ported speech, which temporarily changes the dis-
course utterance time and that we do not attempt
to model in our anchor selection procedure. Inter-
estingly, we noticed that even in cases when both
timex-event mapping, and event Tense were cor-
rect,Value normalization was not. For example, in:
Northern Ireland?s World Cup qualifier with Russia
has been postponed until 15:00 GMT Saturday, one
can see that a shallow approach like ours, with no ac-
cess to lexico-semantic knowledge, cannot pick up
the implicit future tense interpretation of the event
verb.
5.2 Event and Event Attribute Extraction
Results for Spanish (Table 2) show that a small set
of rules were sufficient to detect event verbal expres-
sions with high precision. The task was much harder
for English, where morphological derivation is less
often marked and given that we were not performing
any PoS disambiguation.
Our main aim for Task B exercise was evaluating
the performance of semi-automatic methods for verb
classification, and to see how much verb tense in-
formation could help normalizing time expressions.
Class attribute performance is rather poor, even
considering that 7% of false hits in English were due
to a bug in the MULTEXT lexicon causing the fre-
quent form said not to be annotated as REPORTING
event. A high rate of overlapping occurs among
verb classes, causing our attempt to ?lexicalize? the
Class attribute, rather than trying to compute it
61
Recognition Normalization
Relaxed Strict Value Type
System F1 P R F1 P R F1 A F1 A
EN 0.85 0.90 0.80 0.49 0.52 0.46 0.58 0.68 0.69 0.81
BestEN 0.90 0.89 0.91 0.79 0.78 0.80 0.78 0.86 0.80 0.88
ES 0.65 0.86 0.52 0.49 0.65 0.39 0.50 0.77 0.62 0.95
BestES 0.90 0.96 0.84 0.85 0.90 0.80 0.85 0.94 0.87 0.97
Table 1: Performance of Temporal Expression Extraction and Normalization.
Recognition Class Tense
System F1 P R F1 A F1 A
EN 0.65 0.63 0.67 0.43 0.66 0.39 0.60
BestEN 0.81 0.81 0.81 0.72 0.89 0.60 0.73
ES 0.58 0.90 0.42 0.26 0.45 0.49 0.84
BestES 0.89 0.92 0.86 0.85 0.96 0.87 0.98
Table 2: Performance of Event and Event Attribute Extraction.
from context features of verb instances, to be unfea-
sible. Tense attribute performance7 was too low to
draw any conclusion on its impact on the Normal-
ization task. However, for Spanish its accuracy (A
in Figure 2) was higher and yet this did not result in
increased timex Value scores8.
6 Conclusion
The main positive outcome of our participation in
TempEval-3 was that we were able to build a system
with acceptable performance on Task A for Span-
ish, after a relatively quick adaptation from an ex-
isting English system. Recall was the bottleneck
of such an experiment, while precision figures did
not drop significantly, and Normalization accuracy
even increased for Spanish9, suggesting that a devel-
oper may be able to iteratively add language-specific
rules so as to reduce false negatives, without endan-
gering overall system precision.
A major flaw of our finite-state, local parsing ap-
proach is in recognizing event-anchored time ex-
pressions. In order to address this, our timex recog-
nition rules must be further tuned to the TimeML
7Tense figures are unofficial, as we did not manage to ex-
port this attribute value because of a bug in the submitted sys-
tem. However, we were able to reproduce the evaluation on a
fixed system.
8We do not have independent performance figures of the
timex-event mapping, although this mechanism was invariable
across the two languages.
9Due to low F1 for timex entity extraction.
standard in order to fully isolate temporal signals,
and event detection recall must be significantly in-
creased so as to cover event nominalizations. The
detection of event referring expressions according
to the general, context-independent definition in
TimeML is not our main research target, however
we plan to use statistical classification methods to
increase the performance on this task as this is a
prerequisite to achieve a reliable evaluation of our
event-timex mapping heuristic. Event Tense extrac-
tion should be increased with the same purpose.
Acknowledgments
Many thanks to Maud Ehrmann for several useful
discussions on the ontology of temporal entities and
the TimeML standard.
References
Tomaz Erjavec. 2004. MULTEXT -
East Morphosyntactic Specifications.
URL:http://nl.ijs.si/ME/V3/msd/html/.
Silja Huttunen, Roman Yangarber, and Ralph Grishman.
2002. Diversity of Scenarios in Information Extrac-
tion. Proceedings of the Third International Confer-
ence On Language Resources And Evaluation, Las
Palmas.
Matteo Negri. 2007. Dealing with Italian Temporal Ex-
pressions: The ITA-Chronos System. Proceedings of
EVALITA 2007, Workshop held in conjunction with
AI*IA 2007.
62
Piskorski, Jakub. 2007. ExPRESS Extraction Pat-
tern Recognition Engine and Specification Suite. In
In Proceedings of the International Workshop Finite-
State Methods and Natural language Processing 2007
(FSMNLP2007), Postdam, Germany.
Piskorski, Jakub. 2008. CORLEONE Core Linguis-
tic Entity Online Extraction. Technical Report, EN
23393, Joint Research Center of the European Com-
mission, Ispra, Italy.
James Pustejovsky, Jos M. Castao, Robert Ingria, Roser
Sauri, Robert J. Gaizauskas, Andrea Setzer, Graham
Katz, and Dragomir R. Radev. 2003. TimeML: Ro-
bust Specification of Event and Temporal Expressions
in Text. In Mark T. Maybury, editor New Directions in
Question Answering, pages 2834. AAAI Press, 2003.
Pustejovsky, J., P. Hanks, R. Sauri, A. See, R.
Gaizauskas, A. Setzer, D. Radev, B. Sundheim, D.
Day, L. Ferro, et al 2003. The TimeBank corpus.
In Corpus Linguisticsvolume 2003, 40.
Steinberger Ralf, Bruno Pouliquen & Erik van der Goot.
2009. An introduction to the Europe Media Monitor
Family of Applications. In Fredric Gey, Noriko Kando
& Jussi Karlgren (eds.): Information Access in a Mul-
tilingual World - Proceedings of the SIGIR 2009 Work-
shop (SIGIR-CLIR?2009), pp.1-8.
Tanev Hristo, Piskorski Jakub, Atkinson Martin.
2008. Real-Time News Event Extraction for
Global Crisis Monitoring. In Proceedings of NLDB
2008,2008:207218.
Hristo Tanev and Vanni Zavarella. in press. Multilin-
gual Learning and Population of Event Ontologies. A
Case Study for Social Media. In Paul Buitelaar and
Philipp Cimiano editors Towards the Multilingual Se-
mantic Web, Springer.
Naushad UzZaman, Hector Llorens, James F. Allen,
Leon Derczynski, Marc Verhagen, James Pustejovsky.
2012. TempEval-3: Evaluating Events, Time Expres-
sions, and Temporal Relations. arXiv:1206.5333v1.
Verhagen, M., R. Sauri, T. Caselli, and J. Pustejovsky
2010. SemEval-2010 task 13: TempEval-2. In Pro-
ceedings of the 5th International Workshop on Seman-
tic Evaluation, 5762, Association for Computational
Linguistics.
63
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 28?36,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Creating Sentiment Dictionaries via Triangulation
Josef Steinberger,
Polina Lenkova, Mohamed Ebrahim,
Maud Ehrmann, Ali Hurriyetoglu,
Mijail Kabadjov, Ralf Steinberger,
Hristo Tanev and Vanni Zavarella
EC Joint Research Centre
21027, Ispra (VA), Italy
Name.Surname@jrc.ec.europa.eu
Silvia Va?zquez
Universitat Pompeu Fabra
Roc Boronat, 138
08018 Barcelona
silvia.vazquez@upf.edu
Abstract
The paper presents a semi-automatic approach
to creating sentiment dictionaries in many lan-
guages. We first produced high-level gold-
standard sentiment dictionaries for two lan-
guages and then translated them automatically
into third languages. Those words that can
be found in both target language word lists
are likely to be useful because their word
senses are likely to be similar to that of the
two source languages. These dictionaries can
be further corrected, extended and improved.
In this paper, we present results that verify
our triangulation hypothesis, by evaluating tri-
angulated lists and comparing them to non-
triangulated machine-translated word lists.
1 Introduction
When developing software applications for senti-
ment analysis or opinion mining, there are basi-
cally two main options: (1) writing rules that assign
sentiment values to text or text parts (e.g. names,
products, product features), typically making use of
dictionaries consisting of sentiment words and their
positive or negative values, and (2) inferring rules
(and sentiment dictionaries), e.g. using machine
learning techniques, from previously annotated doc-
uments such as product reviews annotated with an
overall judgment of the product. While movie or
product reviews for many languages can frequently
be found online, sentiment-annotated data for other
fields are not usually available, or they are almost
exclusively available for English. Sentiment dictio-
naries are also mostly available for English only or,
if they exist for other languages, they are not com-
parable, in the sense that they have been developed
for different purposes, have different sizes, are based
on different definitions of what sentiment or opinion
means.
In this paper, we are addressing the resource bot-
tleneck for sentiment dictionaries, by developing
highly multilingual and comparable sentiment dic-
tionaries having similar sizes and based on a com-
mon specification. The aim is to develop such dic-
tionaries, consisting of typically one or two thou-
sand words, for tens of languages, although in this
paper we only present results for eight languages
(English, Spanish, Arabic, Czech, French, German,
Italian and Russian). The task raises the obvious
question how the human effort of producing this re-
source can be minimized. Simple translation, be it
using standard dictionaries or using machine trans-
lation, is not very efficient as most words have two,
five or ten different possible translations, depending
on context, part-of-speech, etc.
The approach we therefore chose is that of trian-
gulation. We first produced high-level gold-standard
sentiment dictionaries for two languages (English
and Spanish) and then translated them automatically
into third languages, e.g. French. Those words that
can be found in both target language word lists (En
Fr and Es Fr) are likely to be useful because their
word senses are likely to be similar to that of the
two source languages. These word lists can then be
used as they are or better they can be corrected, ex-
tended and improved. In this paper, we present eval-
uation results verifying our triangulation hypothesis,
by evaluating triangulated lists and comparing them
28
to non-triangulated machine-translated word lists.
Two further issues need to be addressed. The
first one concerns morphological inflection. Auto-
matic translation will yield one word form (often,
but not always the base form), which is not suffi-
cient when working with highly inflected languages:
A single English adjective typically has four Spanish
or Italian word forms (two each for gender and for
number) and many Russian word forms (due to gen-
der, number and case distinctions). The target lan-
guage word lists thus need to be expanded to cover
all these morphological variants with minimal effort
and considering the number of different languages
involved without using software, such as morpho-
logical analysers or generators. The second issue
has to do with the subjectivity involved in the human
annotation and evaluation effort. First of all, it is im-
portant that the task is well-defined (this is a chal-
lenge by itself) and, secondly, the inter-annotator
agreement for pairs of human evaluators working on
different languages has to be checked in order to get
an idea of the natural variation involved in such a
highly subjective task.
Our main field of interest is news opinion min-
ing. We would like to answer the question how cer-
tain entities (persons, organisations, event names,
programmes) are discussed in different media over
time, comparing different media sources, media in
different countries, and media written in different
languages. One possible end product would be a
graph showing how the popularity of a certain en-
tity has changed over time across different languages
and countries. News differs significantly from those
text types that are typically analysed in opinion min-
ing work, i.e. product or movie reviews: While a
product review is about a product (e.g. a printer)
and its features (e.g. speed, price or printing qual-
ity), the news is about any possible subject (news
content), which can by itself be perceived to be pos-
itive or negative. Entities mentioned in the news can
have many different roles in the events described.
If the method does not specifically separate positive
or negative news content from positive or negative
opinion about that entity, the sentiment analysis re-
sults will be strongly influenced by the news context.
For instance, the automatically identified sentiment
towards a politician would most likely to be low if
the politician is mentioned in the context of nega-
tive news content such as bombings or disasters. In
our approach, we therefore aim to distinguish news
content from sentiment values, and this distinction
has an impact on the sentiment dictionaries: unlike
in other approaches, words like death, killing, award
or winner are purposefully not included in the sen-
timent dictionaries as they typically represent news
content.
The rest of the paper is structured as follows: the
next section (2) describes related work, especially
in the context of creating sentiment resources. Sec-
tion 3 gives an overview of our approach to dic-
tionary creation, ranging from the automatic learn-
ing of the sentiment vocabulary, the triangulation
process, the expansion of the dictionaries in size
and regarding morphological inflections. Section 4
presents a number of results regarding dictionary
creation using simple translation versus triangula-
tion, morphological expansion and inter-annotator
agreement. Section 5 summarises, concludes and
points to future work.
2 Related Work
Most of the work in obtaining subjectivity lexicons
was done for English. However, there were some
authors who developed methods for the mapping of
subjectivity lexicons to other languages. Kim and
Hovy (2006) use a machine translation system and
subsequently use a subjectivity analysis system that
was developed for English. Mihalcea et al (2007)
propose a method to learn multilingual subjective
language via cross-language projections. They use
the Opinion Finder lexicon (Wilson et al, 2005)
and two bilingual English-Romanian dictionaries to
translate the words in the lexicon. Since word am-
biguity can appear (Opinion Finder does not mark
word senses), they filter as correct translations only
the most frequent words. The problem of translat-
ing multi-word expressions is solved by translating
word-by-word and filtering those translations that
occur at least three times on the Web. Another ap-
proach in obtaining subjectivity lexicons for other
languages than English was explored in Banea et al
(2008b). To this aim, the authors perform three dif-
ferent experiments, with good results. In the first
one, they automatically translate the annotations of
the MPQA corpus and thus obtain subjectivity an-
29
notated sentences in Romanian. In the second ap-
proach, they use the automatically translated entries
in the Opinion Finder lexicon to annotate a set of
sentences in Romanian. In the last experiment, they
reverse the direction of translation and verify the as-
sumption that subjective language can be translated
and thus new subjectivity lexicons can be obtained
for languages with no such resources. Finally, an-
other approach to building lexicons for languages
with scarce resources is presented in Banea et al
(2008a). In this research, the authors apply boot-
strapping to build a subjectivity lexicon for Roma-
nian, starting with a set of seed subjective entries,
using electronic bilingual dictionaries and a training
set of words. They start with a set of 60 words per-
taining to the categories of noun, verb, adjective and
adverb obtained by translating words in the Opin-
ion Finder lexicon. Translations are filtered using a
measure of similarity to the original words, based on
Latent Semantic Analysis (Landauer and Dumais,
1997) scores. Wan (2008) uses co-training to clas-
sify un-annotated Chinese reviews using a corpus
of annotated English reviews. He first translates
the English reviews into Chinese and subsequently
back to English. He then performs co-training using
all generated corpora. Banea et al (2010) translate
the MPQA corpus into five other languages (some
with a similar ethimology, others with a very differ-
ent structure). Subsequently, they expand the fea-
ture space used in a Naive Bayes classifier using the
same data translated to 2 or 3 other languages. Their
conclusion is that expanding the feature space with
data from other languages performs almost as well
as training a classifier for just one language on a
large set of training data.
3 Approach Overview
Our approach to dictionary creation starts with semi-
automatic way of colleting subjective terms in En-
glish and Spanish. These pivot language dictionaries
are then projected to other languages. The 3rd lan-
guage dictionaries are formed by the overlap of the
translations (triangulation). The lists are then man-
ually filtered and expanded, either by other relevant
terms or by their morphological variants, to gain a
wider coverage.
3.1 Gathering Subjective Terms
We started with analysing the available English
dictionaries of subjective terms: General Inquirer
(Stone et al, 1966), WordNet Affect (Strapparava
and Valitutti, 2004), SentiWordNet (Esuli and Se-
bastiani, 2006), MicroWNOp (Cerini et al, 2007).
Additionally, we used the resource of opinion words
with associated polarity from Balahur et al (2009),
which we denote as JRC Tonality Dictionary. The
positive effect of distinguishing two levels of inten-
sity was shown in (Balahur et al, 2010). We fol-
lowed the idea and each of the emloyed resources
was mapped to four categories: positive, negative,
highly positive and highly negative. We also got
inspired by the results reported in that paper and
we selected as the base dictionaries the combination
of MicroWNOp and JRC Tonality Dictionary which
gave the best results. Terms in those two dictionar-
ies were manually filtered and the other dictionar-
ies were used as lists of candidates (their highly fre-
quent terms were judged and the relevant ones were
included in the final English dictionary). Keeping in
mind the application of the dictionaries we removed
at this step terms that are more likely to describe bad
or good news content, rather than a sentiment to-
wards an entity. In addition, we manually collected
English diminishers (e.g. less or approximately), in-
tensifiers (e.g. very or indeed) and invertors (e.g.
not or barely). The English terms were translated to
Spanish and the same filtering was performed. We
extended all English and Spanish lists with the miss-
ing morphological variants of the terms.
3.2 Automatic Learning of Subjective Terms
We decided to expand our subjective term lists by
using automatic term extraction, inspired by (Riloff
and Wiebe, 2003). We look at the problem of ac-
quisition of subjective terms as learning of seman-
tic classes. Since we wanted to do this for two dif-
ferent languages, namely English and Spanish, the
multilingual term extraction algorithm Ontopopulis
(Tanev et al, 2010) was a natural choice.
Ontopopulis performs weakly supervised learning
of semantic dictionaries using distributional similar-
ity. The algorithm takes on its input a small set of
seed terms for each semantic class, which is to be
learnt, and an unannotated text corpus. For example,
30
if we want to learn the semantic class land vehicles,
we can use the seed set - bus, truck, and car. Then
it searches for the terms in the corpus and finds lin-
ear context patterns, which tend to co-occur imme-
diately before or after these terms. Some of the
highest-scored patterns, which Ontopopulis learned
about land vehicles were driver of the X, X was
parked, collided with another X, etc. Finally, the
algorithm searches for these context patterns in the
corpus and finds other terms which tend to fill the
slot of the patterns (designated by X). Considering
the land vehicles example, new terms which the sys-
tem learned were van, lorry, taxi, etc. Ontopop-
ulis is similar to the NOMEN algorithm (Lin et al,
2003). However, Ontopopulis has the advantage to
be language-independent, since it does not use any
form of language-specific processing, nor does it use
any language-specific resources, apart from a stop
word list.
In order to learn new subjective terms for each
of the languages, we passed the collected subjective
terms as an input to Ontopopulis. For English, we
divided the seed set in two classes: class A ? verbs
and class B ? nouns and adjectives. It was necessary
because each of these classes has a different syn-
tactic behaviour. It made sense to do the same for
Spanish, but we did not have enough Spanish speak-
ers available to undertake this task, therefore we put
together all the subjective Spanish words - verbs, ad-
jectives and nouns in one class. We ran Ontopopulis
for each of the three classes - the class of subjective
Spanish words and the English classes A and B. The
top scored 200 new learnt terms were taken for each
class and manually reviewed.
3.3 Triangulation and Expansion
After polishing the pivot language dictionaries we
projected them to other languages. The dictionaries
were translated by Google translator because of its
broad coverage of languages. The overlapping terms
between English and Spanish translations formed
the basis for further manual efforts. In some cases
there were overlapping terms in English and Span-
ish translations but they differed in intensity. There
was the same term translated from an English posi-
tive term and from a Spanish very positive term. In
these cases the term was assigned to the positive cat-
egory. However, more problematic cases arose when
the same 3rd language term was assigned to more
than one category. There were also cases with dif-
ferent polarity. We had to review them manually.
However, there were still lots of relevant terms in the
translated lists which were not translated from the
other language. These complement terms are a good
basis for extending the coverage of the dictionaries,
however, they need to be reviewed manually. Even if
we tried to include in the pivot lists all morpholog-
ical variants, in the triangulation output there were
only a few variants, mainly in the case of highly in-
flected languages. To deal with morphology we in-
troduced wild cards at the end of the term stem (*
stands for whatever ending and for whatever char-
acter). This step had to be performed carefully be-
cause some noise could be introduced. See the Re-
sults section for examples. Although this step was
performed by a human, we checked the most fre-
quent terms afterwards to avoid irrelavant frequent
terms.
4 Results
4.1 Pivot dictionaries
We gathered and filtered English sentiment terms
from the available corpora (see Section 3.1). The
dictionaries were then translated to Spanish (by
Google translator) and filtered afterwards. By ap-
plying automatic term extraction, we enriched the
sets of terms by 54 for English and 85 for Spanish,
after evaluating the top 200 candidates suggested by
the Ontopolulis tool for each language. The results
are encouraging, despite the relevance of the terms
(27% for English and 42.5% for Spanish where
some missing morphological variants were discov-
ered) does not seem to be very high, considering the
fact that we excluded the terms already contained
in the pivot lists. If we took them into account, the
precision would be much better. The initial step re-
sulted in obtaining high quality pivot sentiment dic-
tionaries for English and Spanish. Their statistics
are in table 1. We gathered more English terms than
Spanish (2.4k compared to 1.7k). The reason for
that is that some translations from English to Span-
ish have been filtered. Another observation is that
there is approximately the same number of negative
terms as positive ones, however, much more highly
negative than highly positive terms. Although the
31
Language English Spanish
HN 554 466
N 782 550
P 772 503
HP 171 119
INT 78 62
DIM 31 27
INV 15 10
TOTAL 2.403 1.737
Table 1: The size of the pilot dictionaries. HN=highly
negative terms, N=negative, P=positive, HP=highly posi-
tive, INV=invertors, DIM=diminishers, INV=invertors.
frequency analysis we carried out later showed that
even if there are fewer highly positive terms, they are
more frequent than the highly negative ones, which
results in almost uniform distribution.
4.2 Triangulation and Expansion
After running triangulation to other languages the
resulted terms were judged for relevance. Native
speakers could suggest to change term?s category
(e.g. negative to highly negative) or to remove it.
There were several reasons why the terms could
have been marked as ?non-sentiment?. For instance,
the term could tend to describe rather negative news
content than negative sentiment towards an entity
(e.g. dead, quake). In other cases the terms were
too ambiguous in a particular language. Examples
from English are: like or right.
Table 2 shows the quality of the triangulated dic-
tionaries. In all cases except for Italian we had only
one annotator assessing the quality. We can see that
the terms were correct in around 90% cases, how-
ever, it was a little bit worse in the case of Russian
in which the annotator suggested to change category
very often.
Terms translated from English but not from Span-
ish are less reliable but, if reviewed manually, the
dictionaries can be expanded significantly. Table 3
gives the statistics concerning these judgments. We
can see that their correctness is much lower than in
the case of the triangulated terms - the best in Italian
(54.4%) and the worst in Czech (30.7%). Of course,
the translation performance affects the results here.
However, this step extended the dictionaries by ap-
proximately 50%.
When considering terms out of context, the most
common translation error occurs when the original
word has several meanings. For instance, the En-
glish word nobility refers to the social class of no-
bles, as well as to the quality of being morally good.
In the news context we find this word mostly in the
second meaning. However, in the Russian triangu-
lated list we have found dvoryanstvo , which refers
to a social class in Russian. Likewise, we need to
keep in mind that a translation of a monosemantic
word might result polysemantic in the target lan-
guage, thereby leading to confusion. For example,
the Italian translation of the English word champion
campione is more frequently used in Italian news
context in a different meaning - sample, therefore
we must delete it from our sentiment words list for
Italian. Another difficulty we might encounter es-
pecially when dealing with inflectional languages is
the fact that a translation of a certain word might be
homographic with another word form in the target
language. Consider the English negative word ban-
dit and its Italian translation bandito, which is more
frequently used as a form of the verb bandire (to an-
nounce) in the news context. Also each annotator
had different point of view on classifying the bor-
derline cases (e.g. support, agreement or difficult).
Two main reasons are offered to explain the low
performance in Arabic. On the one hand, it seems
that some Google translation errors will be repeated
in different languages if the translated words have
the same etymological root. For example both words
? the English fresh and the Spanish fresca ? are
translated to the Arabic as YK
Yg. meaning new. The
Other reason is a more subtle one and is related to
the fact that Arabic words are not vocalized and to
the way an annotator perceive the meaning of a given
word in isolation. To illustrate this point, consider
the Arabic word ? J. ?A
	
J ?? @ , which could be used
as an adjective, meaning appropriate, or as a noun,
meaning The occasion. It appears that the annotator
would intuitively perceive the word in isolation as a
noun and not as an adjective, which leads to disre-
garding the evaluative aspects of a given word.
We tried to include in the pivot dictionaries all
morphological variants of the terms. However, in
highly inflected languages there are much more vari-
ants than those translated from English or Spanish.
32
We manually introduced wild cards to capture the
variants. We had to be attentive when compiling
wild cards for languages with a rich inflectional sys-
tem, as we might easily get undesirable words in the
output. To illustrate this, consider the third person
plural of the Italian negative word perdere (to lose)
perdono, which is also homographic with the word
meaning forgiveness in English. Naturally, it could
happen that the wildcard captures a non-sentiment
term or even a term with a different polarity. For in-
stance, the pattern care% would capture either care,
careful, carefully, but also career or careless. That
is way we perform the last manual checking after
matching the lists expanded by wildcards against a
large number of texts. The annotators were unable
to check all the variants, but only the most frequent
terms, which resulted in reviewing 70-80% of the
term mentions. This step has been performed for
only English, Czech and Russian so far. Table 5
gives the statistics. By introducing the wildcards,
the number of distinct terms grew up significantly
- 12x for Czech, 15x for Russian and 4x for En-
glish. One reason why it went up also for English
is that we captured compounds like: well-arranged,
well-balanced, well-behaved, well-chosen by a sin-
gle pattern. Another reason is that a single pat-
tern can capture different POSs: beaut% can cap-
ture beauty, beautiful, beautifully or beautify. Not
all of those words were present in the pivot dictio-
naries. For dangerous cases like care% above we
had to rather list all possible variants than using a
wildcard. This is also the reason why the number
of patterns is not much lower than the number of
initial terms. Even if this task was done manually,
some noise was added into the dictionaries (92-94%
of checked terms were correct). For example, highly
positive pattern hero% was introduced by an anno-
tator for capturing hero, heroes, heroic, heroical or
heroism. If not checked afterwards heroin would
score highly positively in the sentiment system. An-
other example is taken from Russian: word meaning
to steal ukra% - might generate Ukraine as one most
frequent negative word in Russian.
4.3 How subjective is the annotation?
Sentiment annotation is a very subjective task. In ad-
dition, annotators had to judge single terms without
any context: they had to think about all the senses of
Metric Percent Agreement Kappa
HN 0.909 0.465
N 0.796 0.368
P 0.714 0.281
HP 0.846 0
N+HN 0.829 0.396
P+HP 0.728 0.280
ALL 0.766 0.318
Table 6: Inter-annotator agreement on checking the trian-
gulated list. In the case of HP all terms were annotated as
correct by one of the annotators resulting in Kappa=0.
Metric Percent Agreement Kappa
HN 0.804 0.523
N 0.765 0.545
P 0.686 0.405
HP 0.855 0.669
N+HN 0.784 0.553
P+HP 0.783 0.559
ALL 0.826 0.614
Table 7: Inter-annotator agreement on checking the can-
didates. In ALL diminishers, intensifiers and invertors
are included as well.
the term. Only if the main sense was subjective they
agreed to leave it in the dictionary. Another sub-
jectivity level was given by concentrating on distin-
guishing news content and news sentiment. Defining
the line between negative and highly negative terms,
and similarly with positive, is also subjective. In the
case of Italian we compared judgments of two anno-
tators. The figures of inter-annotator agreement of
annotating the triangulated terms are in table 6 and
the complement terms in table 7. Based on the per-
cent agreement the annotators agree a little bit less
on the triangulated terms (76.6%) compared to the
complement terms (82.6%). However, if we look at
Kappa figures, the difference is clear. Many terms
translated only from English were clearly wrong
which led to a higher agreement between the annota-
tors (0.318 compared to 0.614). When looking at the
difference between positive and negative terms, we
can see that there was higher agreement on the neg-
ative triangulated terms then on the positive ones.
33
Language Triangulated Correct Removed Changed category
Arabic 926 606 (65.5%) 316 (34.1%) 4 (0.4%)
Czech 908 809 (89.1%) 68 (7.5%) 31 (3.4%)
French 1.085 956 (88.1%) 120 (11.1%) 9 (0.8%)
German 1.053 982 (93.3%) 50 (4.7%) 21 (2.0%)
Italian 1.032 918 (89.0%) 36 (3.5%) 78 (7.5%)
Russian 966 816 (84.5%) 49 (5.1%) 101 (10.4%)
Table 2: The size and quality of the triangulated dictionaries. Triangulated=No. of terms coming directly from triangu-
lation, Correct=terms annotated as correct, Removed=terms not relevant to sentiment analysis, Change category=terms
in wrong category (e.g., positive from triangulation, but annotator changed the category to highly positive).
Language Terms Correct Removed Changed category
Czech 1.092 335 (30.7%) 675 (61.8%) 82 (7.5%)
French 1.226 617 (50.3%) 568 (46.3%) 41 (3.4%)
German 1.182 548 (46.4%) 610 (51.6%) 24 (2.0%)
Italian 1.069 582 (54.4%) 388 (36.3%) 99 (9.3%)
Russian 1.126 572 (50.8%) 457 (40.6%) 97 (8.6%)
Table 3: The size and quality of the candidate terms (translated from English but not from Spanish). Terms=No. of
terms translated from English but not from Spanish, Correct=terms annotated as correct, Removed=terms not relevant
to sentiment analysis, Change category=terms in wrong category (e.g., positive in the original list, but annotator
changed the category to highly positive).
Language Terms Correct Removed Changed category
Czech 2.000 1.144 (57.2%) 743 (37.2%) 113 (5.6%)
French 2.311 1.573 (68.1%) 688 (29.8%) 50 (2.1%)
German 2.235 1.530 (68.5%) 660 (29.5%) 45 (2.0%)
Italian 2.101 1.500 (71.4%) 424 (20.2%) 177 (8.4%)
Russian 2.092 1.388 (66.3%) 506 (24.2%) 198 (9.5%)
Table 4: The size and quality of the translated terms from English. Terms=No. of (distinct) terms translated from En-
glish, Correct=terms annotated as correct, Removed=terms not relevant to sentiment analysis, Change category=terms
in wrong category (e.g., positive in the original list, but annotator changed the category to highly positive).
Language Initial terms Patterns Matched terms
Count Correct Checked
Czech 1.257 1.063 15.604 93.0% 74.4%
English 2.403 2.081 10.558 93.8% 81.1%
Russian 1.586 1.347 33.183 92.2% 71.0%
Table 5: Statistics of introducing wild cards and its evaluation. Initial terms=checked triangulated terms extended by
relevant translated terms from English, Patterns=number of patterns after introducing wildcards, Matched terms=terms
matched in the large corpus - their count and correctness + checked=how many mentions were checked (based on the
fact that the most frequent terms were annotated).
34
4.4 Triangulation vs. Translation
Table 4 present the results of simple translation from
English (summed up numbers from tables 2 and 3).
We can directly compare it to table 2 where only
results of triangulated terms are reported. The per-
formance of triangulation is significantly better than
the performance of translation in all languages. The
highest difference was in Czech (89.1% and 57.2%)
and the lowest was in Italian (89.0% and 71.4%).
As a task-based evaluation we used the triangu-
lated/translated dictionaries in the system analysing
news sentiment expressed towards entities. The sys-
tem analyses a fixed word window around entity
mentions. Subjective terms are summed up and the
resulting polarity is attached to the entity. Highly
negative terms score twice more than negative, di-
minishers lower and intensifiers lift up the score. In-
vertors invert the polarity but for instance inverted
highly positive terms score as only negative pre-
venting, for instance, not great to score as worst.
The system searches for the invertor only two words
around the subjective term.
We ran the system on 300 German sentences
taken from news gathered by the Europe Media
Monitor (EMM)1. In all these cases the system at-
tached a polarity to an entity mention. We ran it with
three different dictionaries - translated terms from
English, raw triangulated terms (without the man-
ual checking) and the checked triangulated terms.
This pilot experiment revealed the difference in per-
formance on this task. When translated terms were
used there were only 41.6% contexts with correct
polarity assigned by the system, with raw triangu-
lated terms 56.5%, and with checked triangulated
terms 63.4%. However, the number does not contain
neutral cases that would increase the overall perfor-
mance. There are lots of reasons why it goes wrong
here: the entity may not be the target of the sub-
jective term (we do not use parser because of deal-
ing with many languages and large amounts of news
texts), the system can miss or apply wrongly an in-
vertor, the subjective term is used in different sense,
and irony is hard to detect.
1http://emm.newsbrief.eu/overview.html
4.5 State of progress
We finished all the steps for English, Czech and Rus-
sian. French, German, Italian and Spanish dictio-
naries miss only the introduction of wild cards. In
Arabic we have checked only the triangulated terms.
For other 7 languages (Bulgarian, Dutch, Hungarian,
Polish, Portuguese, Slovak and Turkish) we have
only projected the terms by triangulation. However,
we have capabilities to finish all the steps also for
Bulgarian, Dutch, Slovak and Turkish. We haven?t
investigated using more than two pivot languages for
triangulation. It would probably results in more ac-
curate but shortened dictionaires.
5 Conclusions
We presented our semi-automatic approach and cur-
rent state of work of producing multilingual senti-
ment dictionaries suitable of assessing the sentiment
in news expressed towards an entity. The triangula-
tion approach works significantly better than simple
translation but additional manual effort can improve
it a lot in both recall and precision. We believe that
we can predict the sentiment expressed towards an
entity in a given time period based on large amounts
of data we gather in many languages even if the per-
case performance of the sentiment system as on a
moderate level. Now we are working on improving
the dictionaries in all the discussed languages. We
also run experiments to evaluate the system on vari-
ous languages.
Acknowledgments
We thank Alexandra Balahur for her collaboration
and useful comments. This research was partly sup-
ported by a IULA-Universitat Pompeu Fabra grant.
35
References
Alexandra Balahur, Ralf Steinberger, Erik van der Goot,
and Bruno Pouliquen. 2009. Opinion mining from
newspaper quotations. In Proceedings of the Work-
shop on Intelligent Analysis and Processing of Web
News Content at the IEEE / WIC / ACM International
Conferences on Web Intelligence and Intelligent Agent
Technology (WI-IAT).
A. Balahur, R. Steinberger, M. Kabadjov, V. Zavarella,
E. van der Goot, M. Halkia, B. Pouliquen, and
J. Belyaeva. 2010. Sentiment analysis in the news.
In Proceedings of LREC?10.
C. Banea, R. Mihalcea, and J. Wiebe. 2008a. A boot-
strapping method for building subjectivity lexicons for
languages with scarce resources. In Proceedings of
LREC.
C. Banea, R. Mihalcea, J. Wiebe, and S. Hassan.
2008b. Multilingual subjectivity analysis using ma-
chine translation. In Proceedings of EMNLP.
C. Banea, R. Mihalcea, and J. Wiebe. 2010. Multilingual
subjectivity: Are more languages better? In Proceed-
ings of COLING.
S. Cerini, V. Compagnoni, A. Demontis, M. Formentelli,
and G. Gandini. 2007. Micro-WNOp: A gold stan-
dard for the evaluation of automatically compiled lex-
ical resources for opinion mining. In Andrea Sanso`,
editor, Language resources and linguistic theory: Ty-
pology, second language acquisition, English linguis-
tics. Franco Angeli, Milano, IT.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A pub-
licly available resource for opinion mining. In Pro-
ceeding of the 6th International Conference on Lan-
guage Resources and Evaluation, Italy, May.
S.-M. Kim and E. Hovy. 2006. Extracting opinions,
opinion holders, and topics expressed in online news
media text. In Proceedings of the ACL Workshop on
Sentiment and Subjectivity in Text.
T. Landauer and S. Dumais. 1997. A solution to plato?s
problem: The latent semantic analysis theory of the ac-
quisition, induction, and representation of knowledge.
Psychological Review, 104:211?240.
W. Lin, R. Yangarber, and R. Grishman. 2003. Boot-
strapped learning of semantic classes from positive
and negative examples. In Proceedings of the ICML-
2003 Workshop on The Continuum from Labeled to
Unlabeled Data, Washington DC.
R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual pro-
jections. In Proceedings of ACL.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceeding of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
P.J. Stone, D.C. Dumphy, M.S. Smith, and D.M. Ogilvie.
1966. The general inquirer: a computer approach to
content analysis. M.I.T. studies in comparative poli-
tics, M.I.T. Press, Cambridge, MA.
C. Strapparava and A. Valitutti. 2004. WordNet-Affect:
an affective extension of wordnet. In Proceeding of the
4th International Conference on Language Resources
and Evaluation, pages 1083?1086, Lisbon, Portugal,
May.
H. Tanev, V. Zavarella, J. Linge, M. Kabadjov, J. Pisko-
rski, M. Atkinson, and R.Steinberger. 2010. Exploit-
ing machine learning techniques to build an event ex-
traction system for portuguese and spanish. Lingua-
matica: Revista para o Processamento Automatico das
Linguas Ibericas.
X. Wan. 2008. Co-training for cross-lingual sentiment
classification. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the Association
for Computational Linguistics and 4th International
Joint Conference on Natural Language Processing of
the Asian Federation of Natural Language Processing.
T. Wilson, J. Wiebe, and P. Hoffman. 2005. Recognizing
contextual polarity in phrase-level sentiment analysis.
In Proceedings of HLT-EMNLP.
36
Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 110?118,
Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational Linguistics
Semi-automatic Acquisition of Lexical Resources and Grammars for
Event Extraction in Bulgarian and Czech
Hristo Tanev
Joint Research Centre
European Commission
via Fermi 2749, Ispra
Italy
hristo.tanev@jrc.ec.europa.eu
Josef Steinberger
University of West Bohemia
Faculty of Applied Sciences
Department of Computer Science and Engineering
NTIS Centre Univerzini 8, 30614 Plzen
Czech Republic
jstein@kiv.zcu.cz
Abstract
In this paper we present a semi-automatic
approach for acqusition of lexico-syntactic
knowledge for event extraction in two
Slavic languages, namely Bulgarian and
Czech. The method uses several weakly-
supervised and unsupervised algorithms,
based on distributional semantics. More-
over, an intervention from a language ex-
pert is envisaged on different steps in the
learning procedure, which increases its ac-
curacy, with respect to unsupervised meth-
ods for lexical and grammar learning.
1 Introduction
Automatic detection and extraction of events from
online news provide means for tracking the devel-
opments in the World politics, economy and other
important areas of life.
Event extraction is a branch of information ex-
traction, whose goal is the automatic retrieval of
structured information about events described in
natural language texts. Events include interac-
tions among different entities, to each of which
an event-specific semantic role can be assigned.
This role reflects the way in which the entity par-
ticipates in the event and interacts with the other
entities. For example, in the fragment ?Three peo-
ple were injured in a building collapse?, the phrase
?three people? may be assigned a semantic role
injured ? victim. The list of semantic roles de-
pends on the adopted event model.
The event extraction technology may decrease
the information overload, it allows automatic con-
version of unstructured text data into structured
one, it can be used to pinpoint interesting news ar-
ticles, also extracted entities and their correspond-
ing semantic roles can provide brief summaries of
the articles.
Using lexico-syntactic knowledge is one of
the promising directions in modeling the event-
specific semantic roles (Hogenboom et al, 2011).
While for English linear patterns seem to work
quite well (Tanev et al, 2008), for other lan-
guages,where word ordering is more free, cas-
caded grammars proved to improve the results
(Zavarella et al, 2008). In particular, Slavic lan-
guages are more free-order than English; conse-
quently, using cascaded grammars may be consid-
ered a relevant approach.
In this paper we present an ongoing effort
to build event extraction cascaded grammars for
Bulgarian and Czech in the domain of violent
news. To achieve this goal we put forward a
semi-automatic approach for building of event ex-
traction grammars, which uses several weakly-
supervised algorithms for acquisition of lexical
knowledge, based on distributional semantics and
clustering. Moreover, the lexical knowledge is
learned in the form of semantic classes, which then
can be used as a basis for building of a domain-
specific ontology.
To the best of our knowledge, there are no
previous attempts to perform event extraction for
Slavic languages, apart from the work presented in
(Turchi et al, 2011).
The importance of Czech and Bulgarian lan-
guages comes from the geopolitical positions of
the countries where they are spoken: Czech Re-
public is in a central geographical position be-
tween Eastern and Western Europe; Bulgaria is on
the borders of the European Union, on a crossroad
between Europe and Asia, surrounded by different
cultures, languages and religions. These geopo-
litical factors contribute to the importance of the
news from Czech Republic and Bulgaria and con-
sequently make automatic event extraction from
these news an useful technology for political an-
alysts.
The paper has the following structure: In sec-
tion 2 we make a short overview of the related ap-
110
proaches; in section 3 we describe our method for
lexical and grammar learning; section 4 presents
our experiments and evaluation for Bulgarian and
Czech languages and section 5 discusses the out-
come of the experiments and some future direc-
tions.
2 Related Work
There are different approaches for event extrac-
tion. Most of the work up to now has aimed
at English (see among the others (Naughton et
al., 2006) and (Yangarber et al, 2000)), however
(Turchi et al, 2011) presented automatic learning
of event extraction patterns for Russian, English
and Italian.
Our work is based on weakly supervised algo-
rithms for learning of semantic classes and pat-
terns, presented in (Tanev et al, 2009) and (Tanev
and Zavarella, 2013); these approaches are based
on distributional semantics. There are different
other methods which use this paradigm: A con-
cept and pattern learning Web agent, called NELL
(Never Ending Language Learning) is presented in
(Carlson et al, 2010). Parallel learning of seman-
tic classes and patterns was presented in (Riloff
and Jones, 1999). However these approaches do
not try to derive grammars from the acquired re-
sources, but stop at purely lexical level.
Relevant to our approach are the grammar learn-
ing approaches. A survey of supervised and unsu-
pervised approaches is presented in (D?Ulizia et
al., 2011). The supervised ones require annotation
of big amounts of data which makes the develop-
ment process long and laborious. On the other
hand, unsupervised methods try to generalize all
the training data by using different heuristics like
the minimal description length. Since for event
extraction only specific parts of the text are ana-
lyzed, in order to use unsupervised grammar ac-
quisition methods for learning of event extraction
grammars, one should collect the exact phrases
which describe the events. In practice, this would
transform the unsupervised methods into super-
vised ones. With respect to the state-of-the art
grammar inference approaches, our method allows
for more interaction between the grammar expert
and the learning system. Moreover, our learning
starts from lexical items and not from annotated
texts, which decreases the development efforts.
3 Semi-automatic Learning of Lexica
and Grammars
The event extraction grammar, exploited in our ap-
proach is a cascaded grammar which on the first
levels detects references to entities, like people,
groups of people, vehicles, etc. On the upper lev-
els our cascaded grammar detects certain events
in which these entities participate: In the domain
of violent news, people may get killed, wounded,
kidnapped, arrested, etc. If we consider as an ex-
ample the following Bulgarian text: ????? ???-
????????? ???? ?????????? ????? ?? ?????
?? ???????????? ? ??????? ?? ?????????
(?A group of protesters were arrested yesterday
during demonstrations in the centre of the capi-
tal?), our grammar will detect first that ?????
???????????? (?A group of protesters?) refers
to a group of people and then, it will find that
????? ???????????? ???? ??????????'? (?A
group of protesters were arrested?) refers to an ar-
rest event where the aforementioned group of peo-
ple is assigned the semantic role arrested.
In order to build such a grammar, we acquire
semi-automatically the following resources:
1. a dictionary of words which refer to peo-
ple and other entities in the required domain-
specific context, e.g. ?????? , ?voja?k? (
?soldier? in Bulgarian and Czech), ???? ,
zena ( ?woman? in Bulgarian and Czech),
etc.
2. a list of modifiers and other words which
appear in phrases referring to those entities,
e.g. ??????? , ?civiln??? (?civil? in Bulgar-
ian and Czech), ???? (?NATO?), etc.
3. grammar rules for parsing entity-referring
phrases. For example, a simple rule can be:
PERSON PHRASE ? PER
connector ORG
where PER and ORG are words and multi-
words, referring to people and organizations,
connector ? ?? for Bulgarian or
connector ? ?? (empty string) for Czech.
This rule can parse phrases like ?????? ??
???? or ?voja?k NATO? (?NATO soldier?)
4. a list of words which participate in event
patterns like ????????? , ?zadrz?en? (?ar-
rested? in Bulgarian and Czech) or ???? ,
?zabit? ( ?killed? in Bulgarian and Czech).
111
5. a set of grammar rules which parse event-
description phrases. For example, a simple
rule can be:
KILLING ? PER connector
KILLED PARTICIPLE
where connector ? ???? for Bulgarian
or connector ? byl for Czech.
This rule will recognize phrases like ???-
??? ?? ???? ???? ???? or ?Voja?k
NATO byl zabit? (?A NATO soldier was
killed? in Bulgarian and Czech?)
In order to acquire this type of domain lexica
and a grammar, we make use of a semi-automatic
method which acquires in parallel grammar rules
and dictionaries. Our method exploits several
state-of-the-art algorithms for expanding of se-
mantic classes, distributional clustering, learning
of patterns and learning of modifiers, described in
(Tanev and Zavarella, 2013). The semantic class
expansion algorithm was presented also in (Tanev
et al, 2009). These algorithms are multilingial and
all of them are based on distributional semantics.
They use a non-annotated text corpus for training.
We integrated these algorithms in a semi-
automatic schema for grammar learning, which is
still in phase of development. Here is the basic
schema of the approach:
1. The user provides a small list of seed words,
which designate people or other domain-
specific entities, e.g.? soldiers?,?civilians?,
?fighters? (We will use only English-
language examples for short, however the
method is multilingual and consequently ap-
plicable for Czech and Bulgarian).
2. Using the multilingual semantic class ex-
pansion algorithm (Tanev et al, 2009)
other words are learned (e.g. ?policemen?,
?women?, etc.), which are likely to belong
to the same semantic class. First, the algo-
rithm finds typical contextual patterns for the
seed words from not annotated text. For ex-
ample, all the words, referring to people tend
to appear in linear patterns like [PEOPLE]
were killed, thousands of [PEOPLE] , [PEO-
PLE] are responsible, etc. Then, other words
which tend to participatre in the same con-
textual patterns are extracted from the unan-
notated text corpus. In such a way the al-
gorithm learns additional words like ?police-
men?, ?killers?, ?terrorists?, ?women?, ?chil-
dren?, etc.
3. Since automatic approaches for learning of
semantic classes always return some noise
in the output, a manual cleaning by a do-
main expert takes place as a next step of our
method.
4. Learning modifiers: At this step, for each se-
mantic class learned at the previous step (e.g.
PEOPLE, we run the modifier learning algo-
rithm, put forward by (Tanev and Zavarella,
2013) , which learns domain-specific syn-
tactic modifiers. Regarding the class PEO-
PLE), the modifiers will be words like ?
Russian?, ?American?, ?armed?, ?unarmed?,
?masked?, etc. The modifier learning algo-
rithm exploits the principle that the context
distribution of words from a semantic class
is most likely similar to the context distribu-
tion of these words with syntactic modifiers
attached. The algorithm uses this heuristic
and does not use any morphological infor-
mation to ensure applications in multilingual
settings.
5. Manual cleaning of the modifier list
6. Adding the following grammar rule at the
first level of the cascaded grammar, which
uses the semantic classes and modifiers,
learned at the previous steps:
Entity(class : C) ? (LModif(class :
C))? Word(class : C) (RModif(class :
C))?
This rule parses phrases, like ?masked gun-
men from IRA?, referring to an entity from
a semantic class C, e.g. PERSON. It should
consist of a sequence of 0 or more left mod-
ifiers for this class, e.g. ?masked?, a word
from this class (?gunmen? in this example)
and a sequence of 0 or more right modifiers
(?from IRA? in the example?).
7. Modifiers learned by the modifier learning
algorithm do not cover all the variations in
the structure of the entity-referring phrases,
since sometimes the structure is more com-
plex and cannot be encoded through a list of
lexical patterns. Consider, for example, the
following phrase ?soldiers from the special
forces of the Russian Navy?. There is a little
112
chance that our modifier learning algorithm
acquires the string ?from the special forces
of the Russian Navy?, on the other hand
the following two grammar rules can do the
parsing:
RIGHT PEOPLE MODIFIER ?
?from??MILITARY FORMATION
MILITARY FORMATION ?
LeftModMF ? MFW RightModMF?
where MILITARY FORMATION is a
phrase which refers to some organization (in
the example, shown above, the phrase is ?the
special forces of the Russian Navy?), MFW
is a term which refers to a military formation
(?the special forces?) and LeftModMF and
RightModMF are left and right modifiers
of the military formation entity (for example,
a right modifier is?of the Russian Navy?).
In order to learn such more complex struc-
ture, we propose the following procedure:
(a) The linguistic expert chooses seman-
tic classes, for which more elaborated
grammar rules should be developed.
Let?s take for example the class PEO-
PLE.
(b) Using the context learning sub-
algorithm of the semantic class expan-
sion, used in step 2, we find contextual
patterns which tend to co-occur with
this class. Apart from the patterns
shown in step 2, we also learn patterns
like [PEOPLE] from the special forces,
[PEOPLE] from the Marines, [PEO-
PLE] from the Russian Federation,
[PEOPLE] from the Czech Republic,
[PEOPLE] with guns, [PEOPLE] with
knives, [PEOPLE] with masks, etc.
(c) We generalize contextual patterns, in or-
der to create grammar rules. In the first
step we create automatically syntactic
clusters separately for left and right
contextual patterns. Syntactic clustering
puts in one cluster patterns where the
slot and the content-bearing words are
connected by the same sequence of stop
words. In the example, shown above,
we will have two syntactic clusters of
patterns: The first consists of patterns
which begin with [PEOPLE] from the
and the second contains the patterns,
which start with [PEOPLE] with. These
clusters can be represented via grammar
rules in the following way:
RIGHT PEOPLE MODIFIER ??from
the? X
X? (special forces | Marines | Russian
Federation | Czech Republic)
RIGHT PEOPLE MODIFIER ?
?with? Y
Y? (knives | guns | masks)
(d) Now, several operations can be done
with the clusters of words inside the
grammar rules:
? Words inside a cluster can be clus-
tered further on the basis of their
semantics. In our system we use
bottom up agglomerative cluster-
ing, where each word is represented
as a vector of its context features.
Manual cleaning and merging of
the clusters may be necessary af-
ter this automatic process. If words
are not many, only manual clus-
tering can also be an option. In
the example above ?special forces?
and ?Marines? may form one clus-
ter, since both words designate the
class MILITARY FORMATION and
the other two words designate coun-
tries and also form a separate seman-
tic class.
? In the grammar introduce new non-
terminal symbols, corresponding to
the newly learnt semantic classes.
Then, in the grammar rules substi-
tute lists of words with references
to these symbols. (Still we do
modification of the grammar rules
manually, however we envisage to
automate this process in the future).
For example, the rule
X ? (special forces | Marines
| Russian Federation | Czech Re-
public)
will be transformed into
X ? (MILITARY FORMATION |
COUNTRY)
MILITARY FORMATION ? (spe-
cial forces | Marines)
COUNTRY ? (Russian Federation
113
PEOPLE? (NUMBER ?? (from) )? PEOPLEa
Example: ????? ?? ??????????? ??????? (?two of the Bulgarian soldiers?)
PEOPLEa? PEOPLEb ((?? (from) | ?? (of) | ? (in)) (ORG | PLACE ))*
Example: ????????? ?? ??? (?staff from the MVR (Ministry of the Internal Affairs)?)
PEOPLEb? LeftPM* PEOPLE W RightPM*
Example: ?????????? ?????????? ? ??????? (?unknown attackers with hoods?)
Table 1: Rules for entity recognition for the Bulgarian language
| Czech Republic)
? Clusters can be expanded by using
the semantic class expansion algo-
rithm, introduced before, followed
by manual cleaning. In our example,
this will add other words for MIL-
ITARY FORMATION and COUN-
TRY. Consequently, the range of the
phrases, parsable by the grammar
rules will be augmented.
(e) The linguistic expert may choose a sub-
set of the semantic classes, obtained
on the previous step, (e.g. the the se-
mantic class MILITARY FORMATION)
to be modeled further via extending the
grammar with rules about their left and
right modifiers. Then, the semantic class
is recursively passed to the input of this
grammar learning procedure.
8. Learning event patterns: In this step we learn
patterns like [PEOPLE] ???? ??????????
or [PEOPLE] ?byl zadrz?en? ([PEOPLE]
were/was arrested in Bulgarian and Czech).
The pattern learning algorithm collects con-
text patterns for one of the considered en-
tity categories (e.g. [PEOPLE]. This is done
through the context learning sub-algorithm
described in step 2. Then, it searches for
such context patterns, which contain words,
having distributional similarity to words, de-
scribing the target event (e.g. ?????????? ,
?zadrz?en? (?arrested?)).
For example, if we want to learn patterns for
arrest events in Bulgarian, the algorithm first
learns contexts of [PEOPLE]. These con-
texts are [PEOPLE] ???? ????? ([PEO-
PLE] were killed), ?????? [PEOPLE]
(thousands of [PEOPLE]), [PEOPLE] ????
???????? ([PEOPLE] were captured), etc.
Then, we pass to the semantic expansion al-
gorithm (see step 2) seed words which ex-
press the event arrest, namely ?????????,
?????????? (?apprehended?, ?arrested?),
etc. Then, it will discover other similar words
like ???????? (?captured?). Finally, the
algorithm searches such contextual patterns,
which contain any of the seed and learnt
words. For example, the pattern [PEOPLE]
???? ???????? ([PEOPLE] were captured)
is one of the newly learnt patterns for arrest
events.
9. Generalizing the patterns: In this step we ap-
ply a generalization algorithm, described in
step 7 to learn grammar rules which parse
events. For example, two of the learned rules
for parsing of arrest events in Bulgarian are:
ARREST ? PEOPLE ???? (?were?)
ARREST PARTICIPLE
ARREST PARTICIPLE ? ( ??????????
(arrested) | ????????(captured) |
????????? (handcuffed) )
The outcome of this learning schema is a gram-
mar and dictionaries which recognize descriptions
of different types of domain-specific entities and
events, which happened with these entities. More-
over, the dictionaries describe semantic classes
from the target domain and can be used further for
creation of a domain ontology.
4 Experiments and Evaluation
In our experiments, we applied the procedure
shown above to learn grammars and dictionaries
for parsing of phrases, referring to people, groups
of people and violent events in Bulgarian and
Czech news. We used for training 1 million news
titles for Bulgarian and Czech, downloaded from
114
KILLING? KILL VERB (a (and) | i (and) | jeden (one) | jeden z (one of) )? [PEOPLE]
KILL VERB? (zabit (killed) | zabila | zahynul (died) | zabiti | ubodal (stabbed) | ubodala | ...)
KILLING? KILL ADJ [PEOPLE]
KILL ADJ? (mrtvou (dead) | mrtve?ho (dead) | ...)
KILLING? [PEOPLE] KILL VERBa
KILL VERBa? (zahynul (died) | zamr?el (died) | ...)
KILLING? [PEOPLE] byl (was) KILL VERBb
KILL VERBb? (zabit (killed) | ...)
Table 2: Rules for parsing of killing events and their victims in Czech
the Web and a small number of seed terms, refer-
ring to people and actions. We had more available
time to work for the Bulgarian language, that is
why we learned more complex grammar for Bul-
garian. Both for Czech and Bulgarian, we learned
grammar rules parsing event description phrases
with one participating entity, which is a person or
a group of people. This is simplification, since of-
ten an event contains more than one participant,
in such cases our grammar can detect the separate
phrases with their corresponding participants, but
currently it is out of the scope of the grammar to
connect these entities. The event detection rules
in our grammar are divided into semantic classes,
where each class of rules detects specific type of
events like arrest, killing, wounding, etc. and also
assigns an event specific semantic role to the par-
ticipating entity, e.g. victim, perpetrator, arrested,
kidnapped.
In order to implement our grammars, we used
the EXPRESS grammar engine (Piskorski, 2007).
It is a tool for building of cascaded grammars
where specific parts of the parsed phrase are as-
signed semantic roles. We used this last feature of
EXPRESS to assign semantic roles of the partici-
pating person entities.
For Czech we learned a grammar which de-
tects killings and their victims. For Bulgarian, we
learned a grammar, which parses phrases referring
to killings, woundings and their victims, arrests
and who is arrested, kidnappings and other violent
events with their perpetrators and targeted people.
4.1 Learning people-recognition rules
For Czech our entity extraction grammar was rel-
atively simple, since we learned just a dictionary
of left modifiers. Therefore, we skipped step 7 in
the learning schema, via which more elaborated
entity recognition grammars are learned. Thus,
the Czech grammar for recognizing phrases,
referring to people contains the following rules:
PEOPLE? LeftMod* PEOPLE TERM
LeftMod ? (?mladou? (?young?) |
?nezna?me?mu?(?unknown?) | ?stars???? (?old?) |
...)
PEOPLE TERM ? (?voja?ci? (?soldiers?) |
?civiliste??(?civilians?) | ?z?enu? (?woman?) |
...)
This grammar recognizes phrases like ?mladou
z?enu? (?young woman? in Czech). Two dictionar-
ies were acquired in the learning process: A dic-
tionary of nouns, referring to people and left mod-
ifiers of people. The dictionary of people-referring
nouns contains 268 entries, obtained as a result
of the semantic class expansion algorithm. We
used as a seed set 17 words like ?muz?i? (?men?),
?voia?ci? (?soldiers?), etc. The algorithm learned
1009 new words and bigrams, 251 of which were
correct (25%), that is refer to people. One problem
here was that not all morphological forms were
learned by our class expansion algorithm. In a
language with rich noun morphology, as Czech is,
this influenced on the coverage of our dictionaries.
After manual cleaning of the output from the
modifier learning algorithm, we obtained 603
terms; the learning accuracy of the algorithm was
found to be 55% .
For Bulgarian we learned a more elaborated
people recognition grammar, which is able to
parse more complex phrases like ???? ?? ?????-
?????? ?????????? (?one of the masked attack-
ers?) and ????? ?? ?????????? ?????????? ?
???? (?soldiers from the Bulgarian contingent
in Iraq?). The most important rules which we
learned are shown in Table 1. In these rules PEO-
PLE W encodes a noun or a bigram which refers
to people, ORG is an organization; we learned
mostly organizations, related to the domain of se-
curity, such as different types of military and other
armed formations like ?????? ?? ???? (?secu-
115
rity forces?), also governmental organizations, etc.
PLACE stands for names of places and common
nouns, referring to places such as ?????????
(?the capital?). We also learned modifiers for these
categories and added them to the grammar. (For
simplicity, we do not show the grammar rules for
parsing ORG abd PLACE; we will just mention
that both types of phrases are allowed to have a se-
quence of left modifiers, one or more nouns from
the corresponding class and a sequence of 0 or
more right modifiers.) Both categories PLACE
and ORG were obtained in step 7 of the learn-
ing schema, when exploring the clusters of words
which appear as modifiers after the nouns, refer-
ring to people, like in the following example ???-
?? ?? ?????????? ?????????? (?soldiers from
the Bulgarian contingent? ); then, we applied man-
ual unification of the clusters and their subsequent
expansion, using the semantic class expansion al-
gorithm.
Regarding the semantic class expansion, with
20 seed terms we acquired around 2100 terms,
from which we manually filtered the wrong ones
and we left 1200 correct terms, referring to peo-
ple; the accuracy of the algorithm was found to be
57% in this case.
We learned 1723 nouns for organizations and
523 place names and common nouns. We did not
track the accuracy of the learning for these two
classes. We also learned 319 relevant modifiers
for people-referring phrases; the accuracy of the
modifier learning algorithm was found to be 67%
for this task.
4.2 Learning of event detection rules
This learning takes place in step 8 and 9 of
our learning schema. As it was explained, first
linear patterns like [PEOPLE] ?byl zadrz?en?
([PEOPLE] was arrested ) are learned, then
through a semi-automatic generalization process
these patterns are transformed into rules like:
ARREST? PEOPLE ?byl? ARREST VERB
In our experiments for Czech we learned gram-
mar rules and a dictionary which recognize dif-
ferent syntactic constructions, expressing killing
events and the victims. These rules encode 156
event patterns. The most important of these rules
are shown in Table 2. Part of the event rule learn-
ing process is expansion of a seed set of verbs, and
other words, referring to the considered event (in
this case killing).For this task the semantic class
expansion algorithm showed significantly lower
accuracy with respect to expanding sets of nouns -
only 5%. Nevertheless, the algorithm learned 54
Czech words, expressing killing and death.
For Bulgarian we learned rules for detection of
killing and its victims, but also rules for parsing of
wounding events, arrests, targeting of people in vi-
olent events, kidnapping, and perpetrators of vio-
lent events. These rules encode 605 event patterns.
Some of the rules are shown in Table 3.
4.3 Evaluation of event extraction
In order to evaluate the performance of our gram-
mars, we created two types of corpora: For the
precision evaluation we created bigger corpus of
randomly picked excerpts of news from Bulgar-
ian and Czech online news sources. More pre-
cisely, we used 7?550 news titles for Czech and
12?850 news titles in Bulgarian. We also car-
ried out a preliminary recall evaluation on a very
small text collection: We manually chose sen-
tences which report about violent events of the
types which our grammars are able to capture. We
selected 17 sentences for Czech and 28 for Bul-
garian. We parsed the corpora with our EXPRESS
grammars and evaluated the correctness of the ex-
tracted events. Since each event rule has assigned
an event type and a semantic role for the partic-
ipating people reference, we considered a correct
match only when both a correct event type and a
correct semantic role are assigned to the matched
text fragment. Table 4 shows the results from our
evaluation. The low recall in Czech was mostly
due to the insufficient lexicon for people and the
too simplistic grammar.
Language Precision Recall
Bulgarian 93% 39%
Czech 88% 6%
Table 4: Event extraction accuracy
5 Discussion
In this paper we presented a semi-automatic ap-
proach for learning of grammar and lexical knowl-
edge from unannotated text corpora. The method
is multilingual and relies on distributional ap-
proaches for semantic clustering and class expan-
sion.
116
KILLING? KILL VERB (???? (were) | ?? (are)) [PEOPLE]
KILL VERB? (???????? (killed) | ????? (killed) | ???????????? (shot to death) | ...)
KILLING? KILL PHRASE ?? (of) [PEOPLE]
KILL PHRASE? (???? ?????? (took the life) | ??????? ??????? (caused the death) | ...)
WOUNDING? WOUND VERB (???? (were) | ?? (are)) [PEOPLE]
WOUND VERB? (?????? (wounded) | ???????????? (injured) | ...)
ARREST? [PEOPLE] ARREST VERB
ARREST VERB? (?????????? (arrested) | ????????? (detained) | ...)
Table 3: Some event parsing rules for Bulgarian
We are currently developing event extraction
grammars for Czech and Bulgarian. Preliminary
evaluation shows promising results for the preci-
sion, while the recall is still quite low. One of
the factors which influences the law recall was
the insufficient number of different morphological
word variations in the learned dictionaries. The
morphological richness of Slavic languages can be
considered by adding morphological dictionaries
to the system or creating an automatic procedure
which detects the most common endings of the
nouns and other words and expands the dictionar-
ies with morphological forms.
Another problem in the processing of the
Slavic languages is their relatively free order.
To cope with that, often the grammar engineer
should introduce additional variants of already
learned grammar rules. This can be done semi-
automatically, where the system may suggest ad-
ditional rules to the grammar developer. This can
be done through development of grammar meta-
rules.
With respect to other approaches, grammars
provide transparent, easy to expand model of the
domain. The automatically learned grammars can
be corrected and extended manually with hand-
crafted rules and linguistic resources, such as mor-
phological dictionaries. Moreover, one can try
to introduce grammar rules from already existing
grammars. This, of course, is not trivial because of
the different formalisms exploited by each gram-
mar. It is noteworthy that the extracted semantic
classes can be used to create an ontology of the
domain. In this clue, parallel learning of a domain-
specific grammars and ontologies could be an in-
teresting direction for future research.
The manual efforts in the development of the
grammars and the lexical resources were mainly
cleaning of already generated lists of words and
manual selection and unification of word clus-
ters. Although we did not evaluate precisely the
invested manual efforts, one can estimate them
by the size of the automatically acquired word
lists and their accuracy, given in section Semi-
automatic Learning of Lexica and Grammars.
We plan to expand the Czech grammar with
rules for more event types. Also, we think to ex-
tend both the Bulgarian and the Czech event ex-
traction grammars and the lexical resources, so
that it will be possible to detect also disasters, hu-
manitarian crises and their consequences. This
will increase the applicability and usefulness of
our event extraction grammars.
Acknowledgments
This work was partially supported by project
?NTIS - New Technologies for Information
Society?, European Center of Excellence,
CZ.1.05/1.1.00/02.0090.
References
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, R. Este-
vam, J. Hruschka, and T. Mitchell. 2010. Toward an
architecture for never-ending language learning. In
Proceedings of the Twenty-Fourth AAAI Conference
on Artificial Intelligence (AAAI-10).
A. D?Ulizia, F. Ferri, and P. Grifoni. 2011. A survey of
grammatical inference methods for natural language
learning. Artificial Intelligence Review vol. 36 issue
1.
F. Hogenboom, F. Frasincar, U. Kaymak, and F. Jong.
2011. An overview of event extraction from text.
In Workshop on Detection, Representation, and Ex-
ploitation of Events in the Semantic Web (DeRiVE
2011) at ISWC 2011.
M. Naughton, N. Kushmerick, and J. Carthy.
2006. Event Extraction from Heterogeneous News
Sources. In Proceedings of the AAAI 2006 workshop
on Event Extraction and Synthesis, Menlo Park, Cal-
ifornia, USA.
117
J. Piskorski. 2007. ExPRESS ? Extraction Pattern
Recognition Engine and Specification Suite. In Pro-
ceedings of FSMNLP 2007.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence (AAAI 99).
H. Tanev and V. Zavarella. 2013. Multilingual learn-
ing and population of event ontologies. a case study
for social media. In P. Buitelaar and P. Cimiano, ed-
itors, Towards Multilingual Semantic Web (in press).
Springer, Berlin & New York.
H. Tanev, J. Piskorski, and M. Atkinson. 2008. Real-
Time News Event Extraction for Global Crisis Mon-
itoring. In Proceedings of NLDB 2008., pages 207?
218.
H. Tanev, V. Zavarella, J. Linge, M. Kabadjov, J. Pisko-
rski, M. Atkinson, and R. Steinberger. 2009. Ex-
ploiting Machine Learning Techniques to Build an
Event Extraction System for Portuguese and Span-
ish. Linguama?tica: Revista para o Processamento
Automa?tico das L??nguas Ibe?ricas, 2:550?566.
M. Turchi, V. Zavarella, and H. Tanev. 2011. Pat-
tern learning for event extraction using monolingual
statistical machine translation. In Proceedings of
Recent Advances in Natural Language Processing
(RANLP 2011), Hissar, Bulgaria.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Unsupervised Discovery of Scenario-
Level Patterns for Information Extraction. In
Proceedings of ANLP-NAACL 2000, Seattle, USA,
2000.
V. Zavarella, H. Tanev, and J. Piskorski. 2008. Event
Extraction for Italian using a Cascade of Finite-State
Grammars. In Proceedings of FSMNLP 2008.
118
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, page 66,
Baltimore, Maryland, USA. June 27, 2014. c?2014 Association for Computational Linguistics
Challenges in Creating a Multilingual Sentiment Analysis Application 
for Social Media Mining 
 
 
 Alexandra Balahur, Hristo Tanev, Erik van der Goot 
European Commission Joint Research Centre 
Institute for the Protection and Security of the Citizen 
Via E. Fermi 2749, 21027 Ispra (VA), Italy 
Firstname.Lastname@jrc.ec.europa.eu 
 
 
  
 
Abstract of the talk 
In the past years, there has been an increasing 
amount of research done in the field of Sentiment 
Analysis. This was motivated by the growth in the 
volume of user-generated online data, the infor-
mation flood in Social Media and the applications 
Sentiment Analysis has to different fields ? Mar-
keting, Business Intelligence, e-Law Making, De-
cision Support Systems, etc. Although many 
methods have been proposed to deal with senti-
ment detection and classification in diverse types 
of texts and languages, many challenges still arise 
when passing these methods from the research 
settings to real-life applications.  
 
In this talk, we will describe the manner in which 
we employed machine translation together with 
human-annotated data to extend a sentiment anal-
ysis system to various languages. Additionally, 
we will describe how a joint multilingual model 
that detects and classifies sentiments expressed in 
texts from Social Media has been developed (at 
this point for Twitter and Facebook) and demo its 
use in a real-life application: a project aimed at 
detecting the citizens? attitude on Science and 
Technology. 
66
