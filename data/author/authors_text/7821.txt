Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Multilingual Resources for Entity Extraction 
Stephanie Strassel 
Linguistic Data Consortium 
3600 Market St., Ste 810 
Philadelphia, PA 19104 
strassel@ldc.upenn.edu 
Alexis Mitchell 
Linguistic Data Consortium 
3600 Market St., Ste 810 
Philadelphia, PA 19104 
amitche0@ldc.upenn.edu 
Shudong Huang 
Linguistic Data Consortium 
3600 Market St., Ste 810 
Philadelphia, PA 19104 
shudong@ldc.upenn.edu 
 
 
Abstract 
Progress in human language technology 
requires increasing amounts of data and 
annotation in a growing variety of lan-
guages.  Research in Named Entity ex-
traction is no exception.  Linguistic Data 
Consortium is creating annotated corpora 
to support information extraction in Eng-
lish, Chinese, Arabic, and other languages 
for a variety of US Government-
sponsored programs.  This paper covers 
the scope of annotation and research tasks 
within these programs, describes some of 
the challenges of multilingual corpus de-
velopment for entity extraction, and con-
cludes with a description of the corpora 
developed to support this research. 
1 
research, technology development and education 
Introduction 
Ongoing research in human language technol-
ogy (HLT) requires vast amounts of data for sys-
tem training and development, plus stable 
benchmark data to measure ongoing progress.  Re-
searchers require greater and greater volumes of 
data, representing a broadening inventory of hu-
man languages and ever more sophisticated 
annotation.  This presents a substantial challenge to 
the HLT community because human annotation 
and corpus creation is quite costly.  New 
approaches to research require not tens but 
hundreds and thousands of hours of speech data, 
and millions of words of text.  The availability of 
high quality language resources remains a central 
issue for the many communities involved in basic 
technology development and education related to 
language.  The role of international data centers 
continues to evolve to accommodate emerging 
needs in the speech and language technology 
community (Liberman and Cieri 2002). 
The Linguistic Data Consortium (LDC) was 
founded in 1992 at the University of Pennsylvania, 
with seed money from DARPA, specifically to 
address the need for shared language resources.  
Since then, LDC has created and published more 
than 241 linguistic databases and has accumulated 
considerable experience and skill in managing 
large-scale, multilingual data collection and anno-
tation projects.  LDC has established itself as a 
center for research into standards and best prac-
tices in linguistic resource development, while par-
ticipating actively in ongoing HLT research.    
LDC has had a major role in creating annotated 
corpora and other resources to support named en-
tity extraction, as well as larger information extrac-
tion activities, for a number of years.  Current 
work in this area falls under a handful of research 
programs.  The DARPA Program in Translingual 
Information Detection, Extraction, and Summari-
zation (TIDES 2002) combines technologies in 
detection, extraction, summarization and transla-
tion to create systems capable of searching a wide 
range of streaming multilingual text and speech 
sources, in real time, to provide effective access for 
English-speaking users.  TIDES core languages are 
English, Mandarin and Arabic; second tier lan-
guages are Korean, Spanish, and Japanese.  The 
primary medium is text though this includes 
speech recognition output.  The TIDES research 
tasks require broadcast transcripts and news texts 
to be annotated for entities, relations, and events; 
categorized by topic; translated; summarized; and 
processed in a variety of other ways.   
Another of the TIDES Program goals is to pro-
duce technology that can be easily ported to handle 
new natural languages.  To this end, the TIDES 
Surprise Language Exercise (LDC 2003b) chal-
lenges researchers to produce working systems for 
a previously untargeted language within a con-
strained time span (for instance, a single calendar 
month).   
Currently operating under the TIDES umbrella, 
the Automatic Content Extraction program (NIST 
2002) builds on the successes of previous extrac-
tion research programs.  The objective of the ACE 
Program is to develop extraction technology to 
support automatic processing of source language 
data (in the form of natural text, and as text derived 
from Optical Character Recognition and Automatic 
Speech Recognition output).  This includes classi-
fication, filtering, and selection based on the lan-
guage content of the source data, i.e., the meaning 
conveyed by the data.  Thus the ACE program re-
quires the development of technologies that auto-
matically detect and characterize this meaning.  
The ACE research objectives are viewed as the 
detection and characterization of Entities, Rela-
tions, and Events.  LDC provides data and annota-
tions to support these program goals. 
Another DARPA program, Evidence Extraction 
and Link Detection (EELD 2002), draws on lin-
guistic resources created by LDC to promote its 
research goals.  The EELD program aims for de-
velopment of technologies and tools for automated 
discovery, extraction and linking of sparse evi-
dence contained in large amounts of classified and 
unclassified data sources.  EELD is developing 
detection capabilities to extract relevant data and 
relationships about people, organizations, and ac-
tivities from message traffic and open source data.  
LDC has provided domain-specific entity-tagged 
corpora in support of the EELD technology evalua-
tion. 
2 
 
nd Arabic.   
3 Annotation 
From MUC to ACE 
While LDC's current resource development ef-
forts support ACE and related programs in particu-
lar, ACE is hardly the first program to tackle 
named entities and the larger information extrac-
tion problem.  The Message Understanding Con-
ference Program (MUC) (NIST 1999a) focused on 
named entity extraction, coreference relations 
among noun phrases, the identification of selected 
relations, and events.  High system performance 
within the English newswire domain motivated an 
expansion of the named entity task after MUC-7.  
In 1999, the DARPA Hub-4 NE Project (Chinchor 
et. al. 1999) expanded the domain of source data to 
include broadcast news transcripts.   
LDC joined the group of those developing cor-
pora to support named entity research in that same 
year, providing annotations for the TIDES Infor-
mation Extraction-Entity Recognition (IE-ER) task 
(NIST 1999b).  In the following year, LDC began 
to develop corpora and other resources to support 
the ACE program. 
ACE is substantially similar in scope to these 
earlier extraction programs, though slightly differ-
ent in focus.  ACE adds new varieties of annotated 
data to the information extraction domain. 
Annotators tag newswire, broadcast news 
transcripts and newspaper data.  Additionally, re-
search sites are evaluated on their performance on 
degraded ASR and OCR output.  Under the TIDES 
umbrella, the ACE program supports the multilin-
gual resource and system development, focusing 
currently on Chinese a
ACE also modifies the inventory of entity types 
targeted by the MUC tasks (Chinchor et. al. 1997).  
While MUC considered three entity types (person, 
organization, location), ACE further divides loca-
tions into geo-political entities and facilities, while 
the newest phase also adds weapons, substances, 
and vehicles (similar to the MUC artifact cate-
gory).  Coreference is preserved in ACE, while 
generic entities and metonymy are tackled explic-
itly. 
ACE brings together many of the separate tasks 
evaluated under different components of the MUC 
program.  All ACE tasks -- entities, relations and 
events -- evaluate not only recognition but also 
characterization of these phenomena.  While the 
MUC Template Relation and Scenario Template 
tasks targeted relations and events plus their attrib-
utes, the focus of these tasks was domain specific.  
ACE tasks, on the other hand, are defined to be 
more general and domain-independent.  
Named entity annotation is a core component of 
ACE, but the scope of the annotation required by 
the program builds substantially on this.  
3.1 The Task 
There are three main ACE tasks: Entity Detec-
tion and Tracking, Relation Detection and Charac-
terization, and Event Detection and 
Characterization.  
Entity Detection and Tracking is the most fun-
damental of the ACE tasks and was the sole focus 
of the ACE Pilot effort as well as ACE Phase One.  
The entity task provides a foundation for the re-
maining annotation and research tasks. ACE anno-
tators identify five types of entities (Mitchell et. al. 
2002).  The first two types, Person and Organiza-
tion, remain substantially similar to their defini-
tions under MUC.  Locations within ACE are 
limited to geographical entities such as land-
masses, bodies of water, and geological forma-
tions.  Two new entity types are tagged under 
ACE: Facilities, which include buildings and other 
permanent man-made structures and real estate 
improvements; and GPEs, which are geographical 
regions defined by political and/or social groups.  
GPEs are composite in nature, typically having a 
government, a populace, and a geographic loca-
tion, as well as some more abstract notion of state-
hood.   
A GPE subsumes and does not distinguish be-
tween a nation, its region, its government and its 
people.  However, annotators also assign a role to 
each textual reference (mention) of a GPE, indicat-
ing which of these aspects is most prominent for 
that mention.  In the example below, the two entity 
mentions refer to the governmental (rather than 
people or location) aspect of the entities, so in both 
cases the mentions would be tagged as GPEs with 
Organization roles: 
 
{Russia} recently held discussions with {the US} 
regarding the ongoing crisis. 
 
ACE annotators tag all mentions of each entity 
within a document, whether named, nominal or 
pronominal.  For every mention, the annotator 
identifies the maximal extent of the string that 
represents the entity.  Nested mentions are also 
captured.  Each entity is classified according to its 
type, and co-reference among mentions is re-
corded.  While the ACE Pilot Annotation effort did 
not explicitly deal with metonymic entities and 
generics, Phase One of ACE added these elements 
to the entity annotation and research tasks. 
Metonymy occurs when a single string of text 
makes reference to multiple entities.  Generally, 
these distinct entities are related to each other in 
some way.  For example, in this sentence,  
 
{Beijing} will not continue sales of anti-ship mis-
siles to {Iran}. 
 
Beijing, though literally referring to the name of 
the capital of China, is being used as a reference to 
the government of China.  The relationship be-
tween "Beijing the city" and "Beijing the seat of 
China's government" triggers this metonymic ref-
erence.  When metonymic references occur, ACE 
annotators create two separate entities, one for 
each reference. 
An entity is generic when it does not refer to a 
particular object or set of objects in the world.  
Generic entities include references to general types 
of objects, hypothetical objects and generalizations 
across sets of objects.  Annotators apply the rules 
of mention coreference to generic entities, and spe-
cifically classify each entity as specific or generic. 
In future ACE efforts the set of targeted entities 
will expand to include vehicles, weapons, and sub-
stances.  Entity subtypes will also be added. 
The second phase of the ACE program added 
relation detection and characterization to the suite 
of annotation and research tasks.  This task targets 
five relation types (Mitchell et. al. 2002b): Role, 
Part, Located, Near, and Social.  For example, 
Role relations link people to organizations and 
GPEs in employment, affiliate, and citizenship 
relationships.  The Social relation links people in 
personal, familial or professional relationships.  
Each relation type is further classified according to 
its subtype.  For instance, the Role relation in-
cludes Management, General Staff, Member, 
Owner, Founder, and Citizen-Of subtypes. 
For every relation, annotators identify two pri-
mary arguments (namely, the two ACE entities that 
are linked) as well as the relation's temporal attrib-
utes (Sundheim 2001).  Temporal information is 
drawn from pre-existing TIMEX2 1  annotation 
(Ferro et. al. 2001) wherever those values are ex-
plicitly linked to a relation.  LDC annotators also 
create more general, relative time attributes de-
                                                          
1 TIMEX2 annotation, supported by the ACE program, pro-
vides a framework for the normalized representation of tempo-
ral expressions.   
 
rived from the tense of the verb that heads the 
predication of the relation. Relations that are sup-
ported by explicit textual evidence are distin-
guished from those that depend on contextual 
inference on the part of the reader. 
The following is an example of an explicit rela-
tion of type Located: 
   
{President Bush} was in {New York} Thurs-
day. 
 
The textual evidence supports the relation between 
the entities President Bush and New York, with the 
temporal attributes was and Thursday.   
Future phases of ACE will refine the relation 
task to highlight new relations that are of particular 
interest to the program, and to allow finer categori-
zation of some existing types. 
ACE Phase Three adds a new challenge: recog-
nition and characterization of events.  Definition of 
a set of general event types and subtypes is cur-
rently underway.  Targeted types include Interac-
tion, Movement, Transfer, Creation and 
Destruction events.  Annotators label event argu-
ments (agent, patient and the like) and attributes 
(temporal, locative) according to a type-specific 
template.  They further tag the textual mention or 
anchor for each event and categorize it by type and 
subtype.  For example, the sentence below contains 
reference to an Interaction event.  
 
{Colin Powell} and {Jiang Zemin} held high-level 
talks in {Beijing} last week. 
 
Annotators extract the specific text reference to the 
event (held high-level talks); identify the meeting 
participants (Colin Powell, Jiang Zemin) as argu-
ments of the event; tag the locative (Beijing) and 
temporal (held, last week) attributes.  
The event task will expand in future phases of 
ACE to include additional event types and sub-
types, as well characterization of relations between 
events. 
3.2 The Process 
The large amounts of data, multilingual focus 
and the number and range of annotation tasks re-
quired by the ACE program lends itself to a team-
based approach to annotation.  A single project 
manager provides oversight for all LDC ACE ac-
tivities.  Language-specific lead annotators work 
directly with teams of part-time (typically student) 
annotators, providing training, monitoring progress 
and generally supervising the annotation staff.  The 
project manager works with lead annotators to de-
velop and maintain the formal ACE annotation 
task definitions and guidelines (LDC 2003a). 
The complexity of ACE annotation requires an-
notators with a solid background in linguistics, 
particularly syntax and semantics.  New annotators 
first become familiar with the basic concepts and 
terminology and study the annotation guidelines 
before annotating several sets of training files.  
Throughout the training process, supervisors pro-
vide periodic feedback, comparing the trainee?s 
annotation to a gold standard, identifying discrep-
ancies and refining the annotator?s approach to the 
data and understanding of guidelines and rules.  
Not until an annotator has achieved a certain level 
of accuracy and speed is he permitted to tag actual 
data.   
The annotation work environment is designed 
to encourage regular discussion and "groupthink" 
among the annotation team.  Problems and ques-
tions are logged for future reference, and teams 
meet regularly to discuss outstanding issues.  A 
web-based annotation manual contributes to the 
team approach.  This reference complements the 
formal task definition, documenting decisions 
about how to handle problematic constructions and 
outlier examples.  Because its content is developed 
solely by ACE annotators, the web guidelines also 
function as a training tool.  New annotators regu-
larly add to the guidelines, focusing on the aspects 
of the ACE tasks that are most difficult for them. 
During production annotation, separate annota-
tors conduct at minimum two complete passes over 
the data.  First pass annotation creates the initial 
markup, and a second pass reviews the existing 
annotation for consistency and accuracy.  Second 
passing is typically conducted by more experi-
enced senior annotators.  A targeted third pass is 
performed to further enhance annotation quality.  
During the third pass, lead annotators review the 
annotated data to catch common errors and ensure 
consistent treatment of difficult constructions.   
In addition to multiple passes over all ACE 
data, an additional 5% to 10% of the data is com-
pletely re-annotated from scratch by separate anno-
tators. Results of this dual annotation are compared 
and discrepancies adjudicated in order to establish 
inter-annotator agreement scores and identify areas 
of lingering confusion or inconsistency. While 
rates of inter-annotator agreement for ACE named 
entities are comparable to MUC consistency levels, 
the results for the more complex annotation tasks 
are considerably lower.  Particular challenges in-
clude the coreference of generic entities and the 
use of metonymy, GPE roles, and implicit vs. ex-
plicit relations. 
The first two phases of ACE annotation utilized 
MITRE's Alembic Workbench (Day 1997), which 
was customized for the ACE tasks.  With the ex-
pansion into new languages and the addition of 
events, LDC began development of a locally de-
signed, locally supported ACE toolkit.  Utilizing 
the Annotation Graphs model (Bird and Liberman 
2001), the toolkit provides for customized, plat-
form-independent, multilingual ACE annotation.  
At present the toolkit supports entity tagging only; 
focused relation and event tagging modules are 
under development.  The toolkit will also support 
customized functions for second passing, compari-
son and adjudication of dually-annotated files, and 
additional quality control features including que-
ries of the annotation database.   
3.3 
                                                          
Multilingual ACE 
In its first two phases the ACE program has fo-
cused primarily on English language data.  Under 
TIDES, the program has grown to include new lan-
guages.  LDC is supporting this expansion with 
production annotation in Arabic and Chinese, as 
well as exploratory work in Farsi. 
LDC has completed development of entity an-
notation guidelines in Chinese and Arabic.  Full-
scale Chinese annotation is well underway, while 
Arabic annotation is just beginning.  To move from 
the basic English tasks into Chinese, Arabic and 
Farsi, LDC draws on the expertise of fluent bilin-
gual linguists and language scholars.  These ex-
perts first fully learn the English annotation tasks 
and complete some training annotation in English.  
They then apply the English guidelines to texts in 
the target language, keeping careful note of any 
constructions that motivate changes or additions to 
the guidelines.  After several rounds of test annota-
tion in the target language, new guidelines are 
crafted in English, but with examples drawn exclu-
sively from the target language2.  The new guide-                                                                                           
2 This means that annotators for non-English ACE tasks must 
be fluent bilinguals.  Customarily, new annotators start by 
lines are then extensively tested with pilot annota-
tion by multiple annotators in the target language. 
Further modifications to the guidelines are made as 
new patterns in the data are observed. 
Each time a new language is targeted, lan-
guage-specific challenges emerge.  For Chinese, 
one of the most difficult problems is the lack of 
agreed-upon rules for word segmentation.  While 
English is written with white space around each 
new word, "word" is not a fundamental concept in 
Chinese, and characters are written without white 
space.  Because entity annotation requires annota-
tors to select both the maximal extent of a mention 
as well as the mention's head, it becomes difficult 
for annotators to agree on the exact series of char-
acters that constitute the head of a mention.  Anno-
tation guidelines for Chinese must include rules for 
dealing with this issue. 
Chinese also presents difficulties for tagging 
generic entities.  The rules for identifying generics 
in English rely in part on tests surrounding the ex-
istence of determiners.  However, determiners do 
not exist in Chinese, and this required the creation 
of  new annotation guidelines for generics that rely 
solely on context.  Similarly, Arabic often uses 
determiners in a way that is different from English.  
For instance, in Arabic it is common to use a con-
struction with a determiner when referring to a 
class of entities: 
 
The horse is a wonderful animal. 
 
rather than a bare plural, more common in English: 
 
 Horses are wonderful animals. 
 
The complexity of Arabic morphology presents a 
very different set of problems.  Unlike Chinese and 
English, Arabic commonly uses pronoun affixes.  
For ACE, this means that any annotation tool must 
allow partial words to be tagged as mentions of 
entities in Arabic, while disallowing this for other 
languages. 
In addition to these linguistic differences, some 
distinctive stylistic qualities of Chinese and Arabic 
news reporting present challenges for annotators 
and are worthy of note. 
 
learning the English ACE tasks then move into their language-
specific annotation.  This supports a consistent approach to 
annotation across the multiple languages despite the necessary 
language-specific modifications.   
Many of these challenges are based in cultural 
differences.  For example, many industries in 
China are government owned and operated.  Con-
sequently, names of organizations are often quite 
different than their English counterparts, and 
guidelines written with English naming conven-
tions in mind are inadequate for handling common 
Chinese name constructions like "Beijing School 
Number 4".  
Further, organizations located outside of China 
are often referred to with their country?s name pre-
ceding the company name.  This presents a chal-
lenge for annotator consistency, since it is often 
unclear whether to include the country as part of 
the extent of the company name.    
Arabic news sources regularly use very long 
sentences with multiple clauses.  This presents the 
annotator with different kinds of mention extent 
and coreference decisions than found in English 
news data. Mention extents are typically longer 
and contain more nested mentions, and pronominal 
references to entities are more easily confused. 
Another set of problems extends beyond any 
language-specific considerations; these have to do 
with the infrastructure needed to support a large-
scale multilingual data creation effort.  Finding 
qualified native speaker annotators with adequate 
training in linguistics and eligibility to work in the 
United States is a serious challenge.  Further, ex-
panding ACE into new languages is not simply a 
matter of addressing the linguistic questions, but 
also tackling the technical ones.  Maintaining data 
formats and annotation tools that can accommodate 
not only multiple annotation tasks, but also multi-
ple languages and multiple character sets and en-
codings presents a significant problem. 
Despite the range of issues described above, 
porting the ACE annotation task into new lan-
guages is relatively straightforward.  The funda-
mental work of moving into a new language for 
ACE involves identifying the syntactic and mor-
phological (i.e., surface) constructions that are 
used to refer to the entities, relations and events of 
interest.  This is not an insubstantial task, and re-
quires both the insights of trained linguists and 
many rounds of pilot annotation and exploration of 
the data.  However, the fundamental concepts tar-
geted by ACE, and the underlying semantic con-
tent discussed in the annotated texts, remain 
substantially similar from one language to the next.  
4 Corpora 
As part of the ACE program, and to further 
support both the DARPA TIDES and DARPA 
EELD Programs, LDC has developed a number of 
annotated corpora.  These corpora all draw on 
broadcast news, newspaper and newswire data.  
Sources include data from the Topic Detection and 
Tracking corpora, Chinese Treebank, Arabic Tree-
bank and other news materials.   
Corpus development for the ACE program be-
gan in 1999.  Initially, the Pilot Phase was de-
signed to develop a basic task definition for entity 
detection and tracking.  Multiple research sites in-
cluding MITRE, BBN, NYU, and LDC annotated 
the same set of 15,000 words of English data to 
establish a shared understanding of the annotation 
guidelines and resolve any inter-annotator discrep-
ancies.  This data supported technology evalua-
tions in May and November 2000.   
In ACE Phase 1, the research and annotation 
tasks were expanded to address metonymy and 
generic entities.  Multiple research sites joined 
LDC in annotating 180,000 words of training data 
to support a February 2002 evaluation.  LDC was 
solely responsible for annotating an additional 
45,000 words of evaluation data. 
ACE Phase 2 required research sites to addi-
tionally detect and characterize relations between 
entities.  During this phase of ACE, LDC acted as 
sole annotation site and also took on responsibility 
for developing and maintaining annotation guide-
lines.  Phase 2 used the entire ACE Phase 1 corpus 
as training data, and added an additional 45,000 
words of new evaluation data.  Both training and 
evaluation data were annotated for entities plus 
relations.  In support of the EELD Program, LDC 
annotators tagged another 30,000 words of do-
main-specific training data plus 20,000 words of 
test data for entities and relations.  A September 
2002 evaluation tested system performance for 
both Entities and Relations. 
LDC is currently producing English test data to 
augment the existing corpora in support of a Fall 
2003 TIDES extraction evaluation; in addition, 
LDC is creating data and annotations for multilin-
gual extraction research in Chinese and Arabic. 
100,000 words of Chinese Treebank and 10,000 
words of Arabic Treebank have already been anno-
tated for entities.    
Alongside corpus development, LDC is work-
ing in parallel to expand and refine the existing set 
of ACE tasks.  These modifications are being made 
with input from both the TIDES Extraction and 
ACE communities.  For ACE Phase 3, LDC will 
annotate 300,000 words of data in each of three 
languages: English, Chinese and Arabic; pilot an-
notation in Farsi is also targeted.  Ultimately, all 
three annotation tasks -- entities, relations and 
events -- will be represented in the data.  The cor-
pora developed by LDC to support ACE, EELD, 
and TIDES Extraction are currently available to 
program participants only (LDC 2003c).  General 
publication of the ACE Pilot and ACE Phase 1 
Corpora is slated for Summer 2003; upon publica-
tion, the data will be available to LDC members as 
well as non-members.  The remaining ACE and 
related corpora will be published after the conclu-
sion of these programs' evaluation cycles. 
Outside of the ACE program, LDC has devel-
oped a handful of additional resources for multi-
lingual extraction research.  As part of the TIDES 
Surprise Language Exercise, LDC collects and 
creates linguistic resources in a previously untar-
geted language in an extremely compressed time 
span.  During a two-week dry run in March 2003, 
the target was Cebuano, a language of the Philip-
pines.  Within the span of a few days, LDC created 
250,000 words of monolingual text, built a 20,000 
word lexicon, created 25,000 words of parallel 
text, built a morphological parser, and completed 
named entity tagging of 32,000 words of text.   
Given the severe time constraints of the exer-
cise, named entity annotators used a trimmed-
down version of the MUC Named Entity Guide-
lines rather than the more complex full MUC or 
ACE guidelines.  Despite the time constraints, in-
ter-annotator consistency remained high when 
LDC-tagged data was compared with data tagged 
by annotators at BBN.  A similar set of resources 
for a new surprise language will be developed dur-
ing the Surprise Language evaluation in June 2003.  
All of the data developed for Surprise Language is 
currently available to TIDES participants, and will 
be released as a general publication at the conclu-
sion of the Exercise.  
A final resource created to support named enti-
ties within information extraction more broadly is 
the Xinhua Chinese-English Named Entity list, 
created from Xinhua Newswire's proper name and 
who's who databases.  This corpus contains nearly 
one million proper names of various kinds, includ-
ing approximately 500,000 person names, 300,000 
place names, 30,000 organization names, and tens 
of thousands of other name types.  The data pro-
vides both Chinese to English and English to Chi-
nese name pairs.  This corpus, slated for 
publication in Summer 2003, is currently available 
to TIDES participants. 
Much of the material described above is based 
upon large volumes of text and speech best col-
lected from commercial providers.  Commercial 
sources may require the negotiation of agreements 
that permit the distribution of data to researchers 
while constraining the use of the material to lin-
guistic education, research, and technology devel-
opment. LDC coordinates all necessary intellectual 
property arrangements for data developed under 
multiple research programs including TIDES, 
ACE, and EELD to make resources gathered in 
this way available to the broader research commu-
nities.   
Sponsored common task research programs like 
TIDES and ACE rely heavily upon such shared 
resources.  LDC was in fact created specifically to 
facilitate research sharing.  In order to allow for 
expedited delivery of data to a group of researchers 
participating in a common task evaluation, LDC 
has developed a new data distribution method 
known as ECorpora.  ECorpora target expedited 
delivery of training and devtest data to support of 
formal evaluations.  Upon the conclusion of the 
formal task evaluation, pending negotiations with 
research sponsors and program coordinators, LDC 
publishes data more broadly to permit access to 
these valuable resources to all communities work-
ing in linguistic education, research, and technol-
ogy development. 
References 
Bird, Stephen and Mark Liberman, 2001, A Formal 
Framework for Linguistic Annotation.  
[http://agtk.sourceforge.net/] 
Chinchor, Nancy, et al, 1999, Named Entity Recogni-
tion Task Definition v1.4. 
[ftp://jaguar.ncsl.nist.gov/ace/phase1/ne99_taskdef_v
1_4.pdf] 
Chinchor, Nancy, 1997, MUC-7 Named Entity Task 
Definition Version 3.5 
[http://www.itl.nist.gov/iad/894.02/related_projects/
muc/proceedings/ne_task.html] 
Day, David. 1997, Alembic Workbench User's Guide. 
[http://www.mitre.org/tech/alembic-
workbench/manual/] 
EELD, 2002, DARPA Program in Evidence Extraction 
and Link Detection 
[http://www.darpa.mil/iao/EELD.htm] 
Ferro, Lisa, et al, 2001, TIDES Temporal Annotation 
Guidelines Version 1.0.2.  
LDC, 2003a, Automatic Content Extraction 
[http://www.ldc.upenn.edu/Projects/ACE/] 
LDC, 2003b, Surprise Language Project 
[http://www.ldc.upenn.edu/Projects/SurpriseLanguag
e] 
LDC, 2003c, TIDES Project 
[http://www.ldc.upenn.edu/Projects/TIDES/] 
Liberman, Mark and Christopher Cieri, 2002, TIDES 
Language Resources: A Resource Map for Translin-
gual Information Access, Proceedings of the Third 
International Language Resources and Evaluation 
Conference, Las Palmas, Spain, May-June 2002. 
Mitchell, A., et al 2002a.  Annotation Guidelines for 
Entity Detection and Tracking (EDT) Version 2.5. 
[http://www.ldc.upenn.edu/Projects/ACE] 
Mitchell, A., et al 2002b.  Annotation Guidelines for 
Relation Detection and Characterization (RDC) Ver-
sion 3.6.                      
[http://www.ldc.upenn.edu/Projects/ACE] 
NIST, 1999a, Message Understanding Conference 
[http://www.itl.nist.gov/iaui/894.02/related_projects/
muc/] 
NIST, 1999b, TIDES Information Extraction-Entity 
[http://www.nist.gov/speech/tests/ie-
er/er_99/er_99.htm] 
NIST, 2002, Automatic Content Extraction 
[http://www.nist.gov/speech/tests/ace] 
Sundheim, Beth, 2001, Preliminary RDC Guidelines for 
Time Attributes Version 1.0. 
TIDES, 2002, DARPA Program in Translingual Infor-
mation Detection Extraction and Summarization 
[http://www.darpa.mil/iao/TIDES.htm] 
 
 
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 45?52,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Document Image Collection Using Amazon?s Mechanical Turk  Audrey Le, Jerome Ajot, Mark Przybocki  Stephanie Strassel National Institute of Standards and Technology Linguistic Data Consortium 100 Bureau Drive, Stop 8940 3600 Market Street, Suite 810 Gaithersburg, MD 20876, USA Philadelphia, PA 19104, USA {audrey.le|jerome.ajot|mark.przybocki} @nist.gov strassel@ldc.upenn.edu   
Abstract 
We present findings from a collaborative effort aimed at testing the feasibility of us-ing Amazon?s Mechanical Turk as a data collection platform to build a corpus of document images. Experimental design and implementation workflow are described. Preliminary findings and directions for fu-ture work are also discussed. 1 Introduction The National Institute of Standards and Tech-nology (NIST) and Linguistic Data Consortium (LDC) at the University of Pennsylvania have a strong collaborative history of providing evalua-tion and linguistic resources for the Human Lan-guage Technology (HLT) community1. The NAACL 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk2 presents an interesting opportunity to ex-tend this collaboration in a novel data collection task. This collaborative experiment will occur in the context of the NIST Open Handwriting Rec-ognition and Translation Evaluation (Open-HaRT) (NIST, 2010), which requires a collection of Arabic handwritten document im-ages. While some Arabic handwritten document collections do exist (Combating Terrorism Cen-ter, 2006, 2007; University of Colorado at Boul-der, 1998) these resources are inadequate to support an open technology evaluation. Some existing corpora are not publicly accessible, while others are very small or limited in scope/content, or contain features (e.g. Personal                                                        1 Since 1987 NIST has conducted public evaluations of human language technologies and has collaborated with LDC to collects much of the data used in support for these evaluations. 2 http://sites.google.com/site/amtworkshop2010 
Identifying Information) that prevent their use in a NIST evaluation. New data collection for OpenHaRT using traditional methods of recruit-ing human subjects is cost-prohibitive and time consuming.  Recent studies (Callison-Burch, 2009) have demonstrated the viability of Amazon?s Me-chanical Turk as a data collection platform for tasks including English translations for foreign text sources. We propose to build on the success of previous studies, and expand data collection to target highly variable samples of foreign handwritten texts along with their English trans-lations. While data collected from this effort will be donated to the workshop and larger research community, our hope is that this collaboration will also provide a means to explore the feasibil-ity of this approach more generally, and that it will result in protocols that can be used to collect substantial volumes of handwritten text to sup-port the NIST OpenHaRT evaluation.  The re-mainder of this paper documents this pilot study, describing both the experimental design and im-plementation workflow followed by our findings and hypotheses. 2 Data Collection Experimental Design Our data collection targets images of handwrit-ten foreign language text, prepared according to a pre-defined set of characteristics. Text from the images is transcribed verbatim. English translations of the text are provided. Collected data is verified for accuracy3 and image quality.   2.1 Collection Approach 
                                                       3 Due to time constraints, transcript and translation verifica-tion was conducted offline by LDC staff. 
45
For this pilot study we collected data in two primary languages, Arabic and Spanish4. These languages are of interest for a number of rea-sons. Linguistically, they show large typological and orthographic differences. Strategically, Ara-bic is of high interest to a number of ongoing HLT evaluations, while Spanish is important to U.S. commercial interests. Practically, we also hoped to take advantage of a likely pool of Spanish-speaking Turkers5 whose facility with the written language may vary. We defined two categories or genres for collection ? shopping list and description of the current weather. The rationale for selecting the general shopping list was to elicit text with a potentially large set of vocabularies while the description of the current weather was included to elicit text with a narrow set of vocabularies. To simplify the collection process and to make the tasks as natural as possible, we placed no artificial constraints on the writers. That is, we do not regulate writing implements (e.g., pen, pencil, crayon, marker, etc.), paper types (e.g., lined, unlined, graphed, colored, etc.), ori-entation of the handwriting (e.g., straight, curved, etc.), handwriting speed, etc. To sample naturally-occurring variation in digital images, we placed no constraints on image quality (reso-lution, lighting, orientation, etc.)  Many of these features could be labeled as subsequent Me-chanical Turk HITs6. 2.2 Collection Tasks The collection has three types of tasks:  ? Image Collection ? This task requires the Turker to perform a writing assignment given a specified topic and source language. The writing assignment is electronically scanned or photographed and uploaded to our repository.  
                                                       4 A very small English collection was undertaken to pro-vide a baseline control set for comparison. 5 ?Turkers? is the term used to refer to people who perform tasks for money at the online marketplace Amazon?s Me-chanical Turk. 6 Coined by Amazon, HIT stands for Human Intelligence Task and refers to a task that a person can work on and be compensated for completing the work. 
? Image Transcription ? For each handwritten image, the corresponding text is transcribed verbatim.  ? Image Translation ? For each transcribed foreign language text, an accurate and fluent English translation is provided.  2.3 Task Implementation and Quality Con-trol Each task listed above corresponds to a single HIT. Initial payment rates for each HIT type were established after reviewing comparable HITs available between 2/19/10 ? 2/23/10. Pay-ment rates were finalized after additional review of comparable HITs in mid-March; HIT pay-ments were also adjusted to encourage rapid completion for some tasks. Arabic HITs were priced higher than Spanish HITs because we wanted to investigate the price dimension when we compared Arabic to a language that is more widely spoken by the population at large7.   Image Collection We targeted collection of 18 images per lan-guage (Spanish, Arabic) per genre (weather re-port, shopping list) for a total of 36 per language. Three English shopping list images were also collected as a control set for compari-son of Turker performance. HIT instructions were brief:  1. Take a piece of paper, and write down {a brief description of today's weather | a shopping list} in {Spanish | Arabic}. You can use any type of paper and writing im-plement (pen, pencil, etc.) you have handy, but only write on the front of the page. Use your normal handwriting. 2. Using a digital camera or scanner, take a picture or scan a copy of the {weather report | list} you just created. Make sure you don't cut off any of the handwriting. 3. Upload the image file8. 
                                                       7http://www.nationsonline.org/oneworld/most_spoken_languages.htm 8 Turkers were not given instructions about how to name the uploaded file; such instructions could have facilitated task/workflow management and should be implemented in future efforts. 
46
HIT instructions were written in English for the Spanish-language task; a note at the top of the HIT specified that the task should be per-formed by Spanish speakers. For the Arabic-language task, initial instructions were also writ-ten in English; this was later revised to use in-structions written in Arabic to better target Arabic-speaking Turkers. We did not require Turkers to take a language qualification test. The HITs remained open for one week, and time al-located per HIT was 30 minutes. Payment for the image collection task was set at $0.11 per image for Spanish and $0.15 for Arabic. Quality control for the image collection task involved annotators at LDC reviewing each submitted image and determining whether it was in fact in the targeted language and genre (weather report or shopping list).  Image Transcription Each image was then transcribed. We targeted two unique transcripts per collected, approved image9. HIT instructions were as follows:  ?The image below contains {Spanish | Ara-bic} handwriting. Your job is to transcribe exactly what you see. Type out all the words and punctuation you see, exactly as they are written. Do not correct any spelling mistakes or other errors in the handwriting. If the image contains any punctuation, copy that exactly using the punctuation character on your keyboard that is closest to what was written.  ?If the handwritten image is a list on multi-ple lines, transcribe one line at a time, in-serting a line break (by hitting the "Enter" key) after each new line.  ?For any words that you cannot read, or if you're just not sure what the writing says, just do the best you can and transcribe as much of the word as you can make out. Add ?? to the beginning of any word you are not sure of.  ?Before submitting your transcript, please double check to make sure you have tran-scribed every line in the image, without                                                        9 The total number of images assigned for transcription was lower than the number collected in some cases, due to time-line and task staging constraints. 
leaving anything out or adding anything that isn't in the image.   Instructions for the Spanish transcription task were provided in English, whereas the Arabic instructions were written in Arabic. For this task we required Turkers to have an approval rating of 95% or higher. The HITs remained open for four days for Spanish and one week for Arabic; time allocated per HIT was 30 minutes. Payment for the transcription task was set at $0.20 per image for Spanish and $0.25 for Arabic. Quality control on the transcription task involved fluent Spanish or Arabic annotators at LDC reviewing the transcripts against the image and making a three-level accuracy judgment: perfect transcript (no errors); acceptable tran-script (minor errors in transcription or punctua-tion); unacceptable transcript (major errors). Transcripts judged as "perfect" or "acceptable" were passed on to the final translation task.  Image Translation We targeted one unique translation per collected, approved transcript. HIT instructions were as follows:  Below is a brief shopping list or weather re-port in {Spanish | Arabic}. Your job is to provide an English translation of this docu-ment. Your translation should be accurate, and should use fluent English.  ?Translate every sentence, phrase or word, without leaving anything out or adding any information. ?If there are spelling mistakes or other er-rors in the {Spanish | Arabic} text, just translate what you think the intended word is. ?If there is any punctuation in the {Spanish | Arabic} text, copy that over exactly into the English translation. ?Try to follow the same document layout and line breaks as in the original {Spanish | Arabic} text. ?Some  {Spanish | Arabic} words may have ?? at the beginning. You should copy the ?? over onto the beginning of the correspond-ing English translated word. ?Put !! at the beginning of any English word whose translation you're not sure of. 
47
?NOTE: Do not use automatic translations from the web for this task. Such submis-sions will be rejected.  Because this task targeted fluent English translations, instructions were written in English for both the Spanish and Arabic translation HITs. For this task we required Turkers to have an approval rating of 95% or higher. The HITs remained open for two days for Spanish and four days for Arabic; time allocated per HIT was 1 hour. Payment for the translation task was set at $1.25 per image for Spanish and $1.50 for Ara-bic10. Quality control on this task involved LDC bi-lingual annotators checking the translation against the transcript, and making a judgment of "acceptable" or "unacceptable". Perfect transla-tion was not required but the translation had to be a generally adequate and fluent rendering of the foreign language text. Translation QC anno-tators were permitted to consult the image file for context, but were not permitted to penalize a Turker based on information only available in the image file, since Turkers working on transla-tion HITs did not have access to the image file. 3 Collection Yield and Results  Table 1 summarizes the total number of image, transcription and translation HITs made avail-able, submitted and approved for each language and genre. As originally planned, our study would have produced a total of 36 images per language, with two transcripts per image (for a total of 72 per language) and one translation per transcript (72 per language). Actual yields for the image collection task were considerably lower, and targets for the subsequent tasks were adjusted. In the case of Arabic, all approved images were made available for transcription. For Span-ish some images were submitted and approved after the transcription HITs had been assigned; time constraints did not permit creating addi-tional transcription HITs for these later images. For both languages, all approved transcription 
                                                       10 These rates were set in part based on need for rapid com-pletion of these HITs. 
HITs were made available for subsequent trans-lation. Note too that the number of submitted HITs actually exceeds the number of available HITs in some cases; this is because rejected HITs were made available for completion by new Turkers.      Avail. HITs Submtd Aprvd Images 18 13 10 Transcripts 14 16 14 Spanish Shopping List Translations 14 21 12           Images 18 7 5 Transcripts 6 6 6 Spanish Weather Report Translations 6 13 5           Images 18 11 3 Transcripts 6 9 6 Arabic Shopping List Translations 6 7 0           Images 18 6 2 Transcripts 4 5 4 Arabic Weather Report Translations 4 5 1           Images 3 3 3 Transcripts 6 6 6 English Shopping List Translations n/a  Table 1: Collection Summary   Proof of Concept: English Control Set Collection of the small English-language control set was entirely successful: Turkers quickly completed the image collection task and pro-vided accurate transcripts of each English image. Though small in number, the submitted images show considerable variation in image quality (lighting, rotation, resolution, scan versus photo) and handwriting quality (paper type and writing implement). All images were approved during the quality control pass. Transcript collection was ex-tremely fast: all six transcripts (two copies per image) were collected within minutes of posting the HITs. Transcript quality was uniformly ac-ceptable. As a baseline, the English control task demonstrates the feasibility of using MTurk for at least some kinds of image collection and tran-scription.  
48
 Figure 1: Handwritten English shopping list  Spanish Results The Spanish language collection was largely successful. The first challenge was finding fluent Spanish speaking Turkers. No special effort was made to advertise the task to Spanish speakers beyond posting the hits on MTurk. Each HIT's title, description and associated keywords in-cluded the term "Spanish" but did not contain any Spanish language content.  While we targeted a total of 36 Spanish im-ages, only 20 were submitted, of which 15 were approved. Images were rejected largely because of fraud, principally stemming from duplicate copies of the same handwritten image being submitted under different Worker IDs. Image and handwriting quality showed a great deal of variation. Turkers used plain unlined paper, lined paper and graph paper with a variety of writing implements. Some submitted printed handwriting while others used cursive. We ob-served some interesting document formatting issues; for instance some Turkers provided multi-column shopping lists. Image quality ranged from a clean, high resolution scan with the image perfectly centered, to low-quality bitmap files with edges of the paper bent or wrinkled and the page skewed off-center. Other image artifacts included lighting variation within a single image due to the use of a flash while photographing the image.  The transcription task was completed largely as planned, with two transcripts acquired for all images. Two transcripts HITs were rejected, in both cases because the Turker provided a trans-lation instead of a transcript; these images were made available for re-assignment to other Turk-
ers and accurate transcripts were eventually ob-tained. The transcription task presented several difficulties that, while anticipated, were not fully addressed in this limited pilot study. While Spanish handwriting contains numerous diacrit-ics (e.g., the tilde in pi?ata) these were variably rendered in the transcription task. Some tran-scribers tried to incorporate the diacritics di-rectly, whereas others used plain ascii for transcription resulting in either missing diacrit-ics, or non-standard symbols standing in for dia-critics. For instance, "pi?ata" might be alternately transcribed as "pi?ata", "pinata", "pin~ata" or something else. The issue of input and rendering for non-English characters is a well-known problem in corpus creation, but in this pilot study no special effort was made to control for it. Similarly, special formatting char-acters (e.g. for bullet-pointed shopping lists) were variably rendered by Turkers and did not always display as intended in the resulting out-put file. During transcription QC, LDC annota-tors made an effort to standardize rendering of such characters to facilitate the translation task. Future MTurk data collection efforts will need to devote more attention to character encoding, input and rendering issues.  
 Figure 2: Handwritten Spanish shopping list  Translation proved to be the most difficult of the three tasks. We targeted collection of one 
49
translation per approved transcript, for a planned total of two translations per image. We fell somewhat short of this collection goal, in part because timeline constraints meant the batch of translation HITs was only available for a few days. The rejection rate on translation HITs was also much higher than the rate for images or transcripts. Rejected translation HITs fell into two categories: an obvious machine translation from the web (typically Google Translate)15; or an apparent human translation that did not con-stitute fluent English.   Because the collected images were short and simple with little or no formatting or document structure, and because transcripts were QC'd prior to translation, it was believed that Turkers could create an accurate translation without making reference to the original image file. Therefore, the image was not displayed during translation; instead Turkers were given only a plain text version of the transcript. This ap-proach did contribute to some translation diffi-culties especially for special characters (like the Celsius symbol, ?C, frequently used in the weather reports).   Based on the rejection rate for individual HITs, some images proved harder to translate than others. This appears to be largely an issue of translation difficulty due to specialized termi-nology (e.g. brand names and abbreviations in a shopping list) rather than influence from errors in transcription.  Arabic Results Not surprisingly, data collection for Arabic proved quite challenging. Locating fluent Arabic speakers among Turkers was extremely difficult. As noted elsewhere our pilot study was limited to using Amazon's default MTurk infrastructure and so we did not undertake any special efforts to direct Turkers to our HITs beyond posting them on MTurk. Instructions for the image col-lection and transcription HITs were written in Arabic, and keywords for all tasks contained the words "Arabic", written in both Arabic and Eng-lish. The HIT titles also contained the word "Arabic" written in both languages.  
                                                       15 Each suspect translation was submitted to Google Trans-late during the QC/review process. 
As with Spanish we targeted a total of 36 Arabic images (18 per genre). While nearly as many images were submitted as in the corre-sponding Spanish task (17 compared to 20 for Spanish), only 5 Arabic images were approved. Reasons for rejection included the image being in English rather than Arabic; the image being typed instead of handwritten; and several cases of identical images being submitted under mul-tiple WorkerIDs. Among the approved images we again observed an exciting range of image and handwriting variation, including several cases of out-of-focus photos; an example is pro-vided in Figure 3. The Arabic transcription task proved to be fairly straightforward, and we successfully col-lected two independent transcripts for each ap-proved image. A handful of transcripts were rejected because they were grossly inaccurate (the Turker simply copied the instructions or image URL into the transcript rather than pro-viding an actual transcript).   
 Image 3: Handwritten Arabic shopping list  There was an initial concern about whether Turkers would encounter difficulties inputting Arabic text into the transcription HIT interface but this did not seem to affect performance. One unanticipated difficulty was creation of the HITs themselves; the Amazon HIT management inter-face had some difficulties rendering bi-directional text. This is a common problem in 
50
annotation tool design, and is especially prob-lematic when left-to-right and right-to-left read-ing order is required in a single line, for instance when characters like parenthesis or ascii num-bers are interspersed with Arabic text. A similar difficulty emerged when viewing batch-wide results of the transcription task. The default file output format (.csv) is intended for viewing in a tool like Excel. However, the output does not appear to be natively Unicode-compliant and therefore Arabic characters are not rendered cor-rectly. No straightforward solution presented itself within the Amazon HIT management inter-face, and the scope of this pilot project did not permit exploration of solutions using third-party APIs. Instead, results were extracted individu-ally for each HIT using the GUI, which proved to be very time consuming and resulted in a loss of some formatting information (like line wrap-ping).  Unsurprisingly, the Arabic translation task proved to be the most difficult. Of twelve sub-mitted translation HITs, only one resulted in an acceptable translation. The low success rate is likely due to a number of factors. As discussed, there appear to be few Arabic speakers (and even fewer fluent Arabic-English bilinguals) among the Turker population at large. Second, the translation task was available for only a few days. To offset this, the payment per HIT for Arabic (and Spanish) was quite high, though this may have contributed to the final challenge: fraudulent submissions. Rejected HITs followed the normal pattern. Most were machine transla-tions from the web (again, primarily from Goo-gle Translate) while others appeared to be highly disfluent human translations, of the type that might be expected from a first year Arabic stu-dent working without a dictionary.  4 Discussion and Future Plans As a feasibility study, our experiment can be called a qualified success. With respect to image collection MTurk seems to be a viable option at least for some languages and genres. While there was some "fraud", most submitted images were usable and the image properties were highly variable, suggesting that this task is well-suited to MTurk and that the HIT instructions were adequate. A wide range of writing surfaces 
emerged including lined, unlined and graph pa-per, as well as colored paper. Less variation was observed in writing implements (for instance it appears that no one used pencil, crayon or fat marker). There were some surprising features of the handwriting itself beyond the expected qual-ity variation; for instance someone writing per-pendicular to the lines on ruled paper. Image quality ranged along the expected dimensions of resolution, skew and slant, and scanning artifacts like bent corners or wrinkled pages. There were also unexpected artifacts of image photography including uneven lighting due to a flash going off and out-of-focus images. The content sub-mitted for each genre showed considerable variation as well. For instance, Turkers submit-ted shopping lists for not just groceries, but also electronics and a combined shopping/to-do list for an upcoming vacation or trip. The results are promising for future image collection efforts at least for English or other languages for which Turkers are readily accessible. The transcription task was moderately suc-cessful. Apart from finding Turkers with appro-priate language skills, the primary challenges are technical, in terms of character encoding for in-put and display/rendering. The basic Amazon MTurk interface does not provide adequate sup-port to fully resolve this and future efforts will need to explore other options. Quality control is a bigger issue for the transcription task, and ade-quate resources must be devoted to addressing this in any future collection. Multiple transcripts were generated for each image to facilitate using MTurk to collect comparative judgments on transcription quality, although time constraints prevented this from being implemented. In fu-ture we envision incorporating three kinds of MTurk QC for transcription: simple judgment of transcript accuracy; comparison of multiple tran-scripts for a single image; and correction of tran-scripts judged inadequate. It is expected that project staff (as opposed to Turkers) will still need to be involved in some degree of QC.  We anticipate that the transcription task would be substantially harder for images collected in other genres, particularly in cases where reading order is not obvious or explicit in the image. For in-stance in a collection of images of complex multi-column forms that have been completed by hand, one transcriber might work from top to 
51
bottom in column 1 then proceed to column 2, whereas another transcriber might proceed left to right (or right-to-left for Arabic) without re-spect to columns. It is unclear whether MTurk could be productively used for these more com-plex transcription tasks that typically require a customized user interface and significant annota-tor training. Unsurprisingly, translation was the most dif-ficult and least successful task, largely because of the shortage of Arabic and Spanish Turkers and the compressed timeline for translation. Still, translation of general content is a feasible task for MTurk given appropriate quality control measures. Future efforts will need to explore other options for locating appropriate Turkers. As with the transcription task, we also anticipate adding more quality control steps to the MTurk pipeline including acquisition of multiple trans-lations with staged quality judgments, compari-sion and correction. We will also revisit the question of whether translation HITs should in-clude both the transcript and the image file. While this adds complexity to the translation task, it may also help to improve the overall translation quality, and for more complex types of handwritten images translation may be im-possible without reference to the image.   In future efforts we also anticipate needing to have dedicated project staff to facilitate HIT construction and approval, data processing, and interactions with either the Amazon or third party APIs. We encountered some practical chal-lenges in this pilot study with respect manage-ment of the results across tasks. As noted earlier, naming conventions were not specified in the HIT instructions for image collection, so images had to be manually renamed to make them unique and readily identifiable by genre and lan-guage. Extracting transcription output from the results table and presenting it for the translation HIT with document formatting and character encoding intact was another challenge that re-quires additional exploration.  Future efforts should also revisit the cost model, using information about actual time re-quired to complete each type of HIT. In all cases, HITs were completed in just a few min-utes. We also need to further explore cost/quality tradeoffs, since high-paying tasks 
(like translation) are also the most prone to fraud and therefore require additional QC measures.  In conclusion, we have used MTurk to pro-duce a small pilot corpus of handwritten, tran-scribed and translated images in three languages and two genres. This study has provided evi-dence that MTurk is a viable option for image corpus creation at least for some languages, and has suggested avenues for task refinement and future work in this area. The data collected in this study will be distributed to workshop par-ticipants, and portions will be selected for use in the NIST Open HaRT evaluation. Disclaimer Certain commercial products and software are identified in this paper in order to explain our research.  Such identification does not imply recommendation or endorsement by NIST, nor does it imply that the products and software identified are necessarily the best available for the purpose. References  Chris Callison-Burch. 2009. Fast, Cheap and Creative: Evaluating Translation Quality with Amazon?s Mechanical Turk, in Proceed-ings of Empirical Methods in Natural Lan-guage Processing 2009. Combating Terrorism Center at West Point, United States Military Academy. 2007. CTC?s Harmony Reports, http://ctc.usma.edu/harmony/harmony_menu.asp (accessed March 2, 2010). Combating Terrorism Center at West Point, United States Military Academy. 2006. The Islamic Imagery Project: Visual Motifs in Ji-hadi Internet Propaganda, Combating Ter-rorism Center at West Point, West Point, NY.   NIST. 2010. NIST 2010 Open Handwriting Rec-ognition and Translation Evaluation Plan, http://www.nist.gov/itl/iad/mig/upload/OpenHaRT2010_EvalPlan_v2-7.pdf (accessed February 25, 2010). University of Colorado at Boulder Office of News Services. 1998. CU-Boulder Archives Acquires Iraqi Secret Police Files, http://www.colorado.edu/news/releases/1998/33.html (accessed March 2, 2010). 
52
Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 45?53,
Baltimore, Maryland, USA, June 22-27, 2014.
c?2014 Association for Computational Linguistics
A Comparison of the Events and Relations Across ACE, ERE, TAC-KBP,
and FrameNet Annotation Standards
Jacqueline Aguilar and Charley Beller and Paul McNamee and Benjamin Van Durme
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD, USA
Stephanie Strassel and Zhiyi Song and Joe Ellis
University of Pennsylvania
Linguistic Data Consortium (LDC)
Philadelphia, PA, USA
Abstract
The resurgence of effort within computa-
tional semantics has led to increased in-
terest in various types of relation extrac-
tion and semantic parsing. While var-
ious manually annotated resources exist
for enabling this work, these materials
have been developed with different stan-
dards and goals in mind. In an effort
to develop better general understanding
across these resources, we provide a sum-
mary overview of the standards underly-
ing ACE, ERE, TAC-KBP Slot-filling, and
FrameNet.
1 Overview
ACE and ERE are comprehensive annotation stan-
dards that aim to consistently annotate Entities,
Events, and Relations within a variety of doc-
uments. The ACE (Automatic Content Extrac-
tion) standard was developed by NIST in 1999 and
has evolved over time to support different evalua-
tion cycles, the last evaluation having occurred in
2008. The ERE (Entities, Relations, Events) stan-
dard was created under the DARPA DEFT pro-
gram as a lighter-weight version of ACE with the
goal of making annotation easier, and more con-
sistent across annotators. ERE attempts to achieve
this goal by consolidating some of the annotation
type distinctions that were found to be the most
problematic in ACE, as well as removing some
more complex annotation features.
This paper provides an overview of the relation-
ship between these two standards and compares
them to the more restricted standard of the TAC-
KBP slot-filling task and the more expansive stan-
dard of FrameNet. Sections 3 and 4 examine Rela-
tions and Events in the ACE/ERE standards, sec-
tion 5 looks at TAC-KBP slot-filling, and section
6 compares FrameNet to the other standards.
2 ACE and ERE Entity Tagging
Many of the differences in Relations and Events
annotation across the ACE and ERE standards
stem from differences in entity mention tagging.
This is simply because Relation and Event tagging
relies on the distinctions established in the entity
tagging portion of the annotation process. For ex-
ample, since ERE collapses the ACE Facility and
Location Types, any ACE Relation or Event that
relied on that distinction is revised in ERE. These
top-level differences are worth keeping in mind
when considering how Events and Relations tag-
ging is approached in ACE and ERE:
? Type Inventory: ACE and ERE share the Per-
son, Organization, Geo-Political Entity, and
Location Types. ACE has two additional
Types: Vehicle and Weapon. ERE does not
account for these Types and collapses the Fa-
cility and Location Types into Location. ERE
also includes a Title Type to address titles,
honorifics, roles, and professions (Linguis-
tic Data Consortium, 2006; Linguistic Data
Consortium, 2013a).
? Subtype Annotation: ACE further classifies
entity mentions by including Subtypes for
each determined Type; if the entity does not
fit into any Subtype, it is not annotated. ERE
annotation does not include any Subtypes.
? Entity Classes: In addition to Subtype, ACE
also classifies each entity mention according
45
1996 1998 2000 2002 2004 2006 2008 2010 2012
F
r
a
m
e
N
e
t
p
r
o
j
e
c
t
c
r
e
a
t
e
d
A
C
E
d
e
v
e
l
o
p
e
d
m
o
s
t
c
o
m
p
r
e
h
e
n
s
i
v
e
A
C
E
c
o
r
p
u
s
l
a
s
t
A
C
E
e
v
a
l
fi
r
s
t
T
A
C
-
K
B
P
E
R
E
c
r
e
a
t
e
d
Figure 1: Important Dates for the ACE, ERE, TAC-KBP, and FrameNet Standards
to entity class (Specific, Generic, Attributive,
and Underspecified).
? Taggability: ACE tags Attributive, Generic,
Specific, and Underspecified entity mentions.
ERE only tags Specific entity mentions.
? Extents and Heads: ACE marks the full noun
phrase of an entity mention and tags a head
word. ERE handles tagging based on the
mention level of an entity; in Name mentions
(NAM) the name is the extent, in Nominal
mentions (NOM) the full noun phrase is the
extent, in Pronoun mentions (PRO) the pro-
noun is the extent.
? Tags: ERE only specifies Type and Men-
tion level (NAM, NOM, PRO). ACE speci-
fies Type, Subtype, Entity Class (Attributive,
Generic, Specific, Underspecified), and Men-
tion Level (NAM, NOM, PRO, Headless).
3 Relations in ACE and ERE
In the ACE and ERE annotation models, the goal
of the Relations task is to detect and character-
ize relations of the targeted types between enti-
ties (Linguistic Data Consortium, 2008; Linguistic
Data Consortium, 2013c). The purpose of this task
is to extract a representation of the meaning of the
text, not necessarily tied to underlying syntactic
or lexical semantic representations. Both models
share similar overarching guidelines for determin-
ing what is taggable. For relations the differences
lie in the absence or presence of additional fea-
tures, syntactic classes, as well as differences in
assertion, trigger words, and minor subtype varia-
tions.
3.1 Similarities in Relations Annotation
In addition to comprising similar Types (both
models include Physical and Part.Whole Types as
well as slightly different Types to address Affilia-
tion and Social relations) used to characterize each
relation, ACE and ERE share important similar-
ities concerning their relation-tagging guidelines.
These include:
? Limiting relations to only those expressed in
a single sentence
? Tagging only for explicit mention
? No ?promoting? or ?nesting? of taggable en-
tities. In the sentence, Smith went to a hotel
in Brazil, (Smith, hotel) is a taggable Phys-
ical.Located relation, but (Smith, Brazil) is
not. This is because in order to tag this as
such, one would have to promote ?Brazil?.
? Tagging for past and former relations
? Two different Argument slots (Arg1 and
Arg2) are provided for each relation to cap-
ture the importance of Argument ordering.
? Arguments can be more than one token (al-
though ACE marks the head as well)
? Using ?templates? for each relation
Type/Subtype (e.g., in a Physical.Located
relation, the Person that is located some-
where will always be assigned to Arg1 and
the place in which the person is located will
always be assigned to Arg2).
? Neither model tags for negative relations
? Both methods contain argument span bound-
aries. That is, the relations should only in-
clude tagged entities within the extent of a
sentence.
3.2 Differences in Assertion, Modality, and
Tense
A primary difference between these two annota-
tion models is a result of ERE only annotating as-
serted events while ACE also includes hypothet-
icals. ACE accounts for these cases by including
two Modality attributes: ASSERTED and OTHER
46
(Linguistic Data Consortium, 2008). For exam-
ple, in the sentence, We are afraid that Al-Qaeda
terrorists will be in Baghdad, ACE would tag this
as an OTHER attribute, where OTHER pertains to
situations in ?some other world defined by coun-
terfactual constraints elsewhere in the context?,
whereas ERE would simply not tag a relation in
this sentence. Additionally, while both ACE and
ERE tag past and former relations, ACE goes fur-
ther to mark the Tense of each relation by means
of four attributes: Past, Future, Present and Un-
specified.
3.3 Syntactic Classes
ACE further justifies the tagging of each Relation
through Syntactic Classes. The primary function
of these classes is to serve as a sanity check on
taggability and as an additional constraint for tag-
ging. These classes include: Possessive, Prepo-
sition, PreMod, Coordination, Formulaic, Partic-
ipal, Verbal, Relations Expressed by Verbs, and
Other. Syntactic classes are not present in ERE
relations annotation.
3.4 Triggers
Explicit trigger words do not exist in ACE relation
annotation; instead, the model annotates the full
syntactic clause that serves as the ?trigger? for the
relation. ERE attempts to minimize the annotated
span by allowing for the tagging of an optional
trigger word, defined as ?the smallest extent of text
that indicates a relation Type and Subtype? (Lin-
guistic Data Consortium, 2013c). These triggers
are not limited to a single word, but can also be
composed of a phrase or any extent of the text that
indicates a Type/Subtype relation, left to the dis-
cretion of the annotator. It is common for preposi-
tions to be triggers, as in John is in Chicago. How-
ever, sometimes no trigger is needed because the
syntax of the sentence is such that it indicates a
particular relation Type/Subtype without a word to
explicitly signal the relation.
3.5 Types and Subtypes of Relations
There are three types of relations that contain var-
ied Subtypes between ERE and ACE. These are
the Physical, Part-Whole, Social and Affiliation
Types. The differences are a result of ERE collaps-
ing ACE Types and Subtypes into more concise, if
less specific, Type groups.
Physical Relation Type Differences The main
differences in the handling of the physical rela-
tions between ACE and ERE are shown in Table
1. ACE only marks Location for PERSON enti-
ties (for Arg1). ERE uses Location for PERSON
entities being located somewhere as well as for
a geographical location being part of another ge-
ographical location. Additionally, ACE includes
?Near? as a Subtype. This is used for when an en-
tity is explicitly near another entity, but neither en-
tity is a part of the other or located in/at the other.
ERE does not have an equivalent Subtype to ac-
count for this physical relation. Instead, ERE in-
cludes ?Origin? as a Subtype. This is used to de-
scribe the relation between a PER and an ORG.
ACE does not have a Physical Type equivalent,
but it does account for this type of relation within
a separate General Affiliation Type and ?Citizen-
Resident-Religion-Ethnicity? Subtype.
Part-Whole Relation Differences In Table 2,
note that ACE has a ?Geographical? Subtype
which captures the location of a FAC, LOC, or
GPE in or at, or as part of another FAC, LOC,
or GPE. Examples of this would be India con-
trolled the region or a phrase such as the Atlanta
area. ERE does not include this type of annota-
tion option. Instead, ERE tags these regional re-
lations as Physical.Located. ACE and ERE do
share a ?Subsidiary? Subtype which is defined in
both models as a ?category to capture the own-
ership, administrative and other hierarchical rela-
tionships between ORGs and/or GPEs? (Linguis-
tic Data Consortium, 2008; Linguistic Data Con-
sortium, 2013c).
Social and Affiliation Relation Differences
The most evident discrepancy in relation anno-
tation between the two models lies in the So-
cial and Affiliation Relation Types and Subtypes.
For social relations, ACE and ERE have three
Subtypes with similar goals (Business, Family,
Unspecified/Lasting-Personal) but ERE has an ad-
ditional ?Membership? Subtype, as shown in Ta-
ble 3. ACE addresses all ?Membership? relations
in its Affiliation Type. ERE also includes the ?So-
cial.Role? Subtype in order to address the TITLE
entity type, which only applies to ERE. How-
ever, both models agree that the arguments for
each relation must be PERSON entities and that
they should not include relationships implied from
interaction between two entities (e.g., President
47
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Physical Located PER, GPE, LOC GPE, LOC
Physical Origin PER, ORG GPE, LOC
ACE
Physical Located PER FAC, LOC, GPE
Physical Near PER, FAC, GPE, LOC FAC, GPE, LOC
Table 1: Comparison of Permitted Relation Arguments for the Physical Type Distinction in the ERE and
ACE Guidelines
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Part-Whole Subsidiary ORG ORG, GPE
ACE
Part-Whole Geographical FAC, LOC, GPE FAC, LOC, GPE
Part-Whole Subsidiary ORG ORG, GPE
Table 2: Comparison of Permitted Relation Arguments for the Part-Whole Type and Subtype Distinctions
in the ERE and ACE Guidelines
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Social Business PER PER
Social Family PER PER
Social Membership PER PER
Social Role TTL PER
Social Unspecified PER PER
ACE
Personal-Social Business PER PER
Personal-Social Family PER PER
Personal-Social Lasting-Personal PER PER
Table 3: Comparison of Permitted Relation Arguments for the Social Type and Subtype Distinctions in
the ERE and ACE Guidelines
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Affiliation Employment/Membership PER, ORG,
GPE
ORG, GPE
Affiliation Leadership PER ORG, GPE
ACE
ORG-Affiliation Employment PER ORG, GPE
ORG-Affiliation Ownership PER ORG
ORG-Affiliation Founder PER, ORG ORG, GPE
ORG-Affiliation Student-Alum PER ORG.Educational
ORG-Affiliation Sports-Affiliation PER ORG
ORG-Affiliation Investor-Shareholder PER, ORG,
GPE
ORG, GPE
ORG-Affiliation Membership PER, ORG,
GPE
ORG
Agent-Artifact User-Owner-Inventor-
Manufacturer
PER, ORG,
GPE
FAC
Gen-Affiliation Citizen-Resident-Religion-
Ethnicity
PER PER.Group,
LOC, GPE,
ORG
Gen-Affiliation Org-Location-Origin ORG LOC, GPE
Table 4: Comparison of Permitted Relation Arguments for the Affiliation Type and Subtype Distinctions
in the ERE and ACE Guidelines
48
Clinton met with Yasser Arafat last week would
not be considered a social relation).
As for the differences in affiliation relations,
ACE includes many Subtype possibilities which
can more accurately represent affiliation, whereas
ERE only observes two Affiliation Subtype op-
tions (Table 4).
4 Events in ACE and ERE
Events in both annotation methods are defined as
?specific occurrences?, involving ?specific partic-
ipants? (Linguistic Data Consortium, 2005; Lin-
guistic Data Consortium, 2013b). The primary
goal of Event tagging is to detect and character-
ize events that include tagged entities. The central
Event tagging difference between ACE and ERE
is the level of specificity present in ACE, whereas
ERE tends to collapse tags for a more simplified
approach.
4.1 Event Tagging Similarities
Both annotation schemas annotate the same ex-
act Event Types: LIFE, MOVEMENT, TRANS-
ACTION, BUSINESS, CONFLICT, CONTACT,
PERSONNEL, and JUSTICE events. Both anno-
tation ontologies also include 33 Subtypes for each
Type. Furthermore, both rely on the expression
of an occurrence through the use of a ?Trigger?.
ACE, however, restricts the trigger to be a single
word that most clearly expresses the event occur-
rence (usually a main verb), while ERE allows for
the trigger to be a word or a phrase that instanti-
ates the event (Linguistic Data Consortium, 2005;
Linguistic Data Consortium, 2013b). Both meth-
ods annotate modifiers when they trigger events
as well as anaphors, when they refer to previously
mentioned events. Furthermore, when there is
any ambiguity about which trigger to select, both
methods have similar rules established, such as
the Stand-Alone Noun Rule (In cases where more
than one trigger is possible, the noun that can be
used by itself to refer to the event will be selected)
and the Stand-Alone Adjective Rule (Whenever a
verb and an adjective are used together to express
the occurrence of an Event, the adjective will be
chosen as the trigger whenever it can stand-alone
to express the resulting state brought about by the
Event). Additionally, both annotation guidelines
agree on the following:
? Tagging of Resultative Events (states that re-
sult from taggable Events)
? Nominalized Events are tagged as regular
events
? Reported Events are not tagged
? Implicit events are not tagged
? Light verbs are not tagged
? Coreferential Events are tagged
? Tagging of multi-part triggers (both parts are
tagged only if they are contiguous)
4.2 Event Tagging Differences
One of the more general differences between ERE
and ACE Event tagging is the way in which each
model addresses Event Extent. ACE defines the
extent as always being the ?entire sentence within
which the Event is described? (Linguistic Data
Consortium, 2005). In ERE, the extent is the
entire document unless an event is coreferenced
(in which case, the extent is defined as the ?span
of a document from the first trigger for a par-
ticular event to the next trigger for a particular
event.? This signifies that the span can cross
sentence boundaries). Unlike ACE, ERE does
not delve into indicating Polarity, Tense, Gener-
icity, and Modality. ERE simplifies any anno-
tator confusion engendered by these features by
simply not tagging negative, future, hypotheti-
cal, conditional, uncertain or generic events (al-
though it does tag for past events). While ERE
only tags attested Events, ACE allows for irrealis
events, and includes attributes for marking them
as such: Believed Events; Hypothetical Events;
Commanded and Requested Events; Threatened,
Proposed and Discussed Events; Desired Events;
Promised Events; and Otherwise Unclear Con-
structions. Additionally both ERE and ACE tag
Event arguments as long as the arguments occur
within the event mention extent (another way of
saying that a taggable Event argument will occur
in the same sentence as the trigger word for its
Event). However, ERE and ACE have a diverging
approach to argument tagging:
? ERE is limited to pre-specified arguments for
each event and relation subtype. The pos-
sible arguments for ACE are: Event partici-
pants (limited to pre-specified roles for each
event type); Event-specific attributes that are
associated with a particular event type (e.g.,
the victim of an attack); and General event
attributes that can apply to most or all event
types (e.g., time, place).
49
? ACE tags arguments regardless of modal cer-
tainty of their involvement in the event. ERE
only tags asserted participants in the event.
? The full noun phrase is marked in both ERE
and ACE arguments, but the head is only
specified in ACE. This is because ACE han-
dles entity annotation slightly differently than
ERE does; ACE marks the full noun phrase
with a head word for entity mention, and ERE
treats mentions differently based on their syn-
tactic features (for named or pronominal en-
tity mentions the name or pronominal itself
is marked, whereas for nominal mentions the
full noun phrase is marked).
Event Type and Subtype Differences Both an-
notation methods have almost identical Event
Type and Subtype categories. The only differences
between both are present in the Contact and Move-
ment Event Types.
A minor distinction in Subtype exists as a re-
sult of the types of entities that can be trans-
ported within the Movement Type category. In
ACE, ARTIFACT entities (WEAPON or VEHI-
CLE) as well as PERSON entities can be trans-
ported, whereas in ERE, only PERSON entities
can be transported. The difference between the
Phone-Write and Communicate Subtypes merely
lies in the definition. Both Subtypes are the de-
fault Subtype to cover all Contact events where
a ?face-to-face? meeting between sender and re-
ceiver is not explicitly stated. In ACE, this contact
is limited to written or telephone communication
where at least two parties are specified to make
this event subtype less open-ended. In ERE, this
requirement is simply widened to comprise elec-
tronic communication as well, explicitly including
those via internet channels (e.g., Skype).
5 TAC-KBP
After the final ACE evaluation in 2008 there was
interest in the community to form an evaluation
explicitly focused on knowledge bases (KBs) cre-
ated from the output of extraction systems. NIST
had recently started the Text Analysis Conference
series for related NLP tasks such as Recognizing
Textual Entailment, Summarization, and Question
Answering. In 2009 the first Knowledge Base
Population track (TAC-KBP) was held featuring
two initial tasks: (a) Entity Linking ? linking en-
tities to KB entities, and (b) Slot Filling ? adding
information to entity profiles that is missing from
the KB (McNamee et al., 2010). Due to its gener-
ous license and large scale, a snapshot of English
Wikipedia from late 2008 has been used as the ref-
erence KB in the TAC-KBP evaluations.
5.1 Slot Filling Overview
Unlike ACE and ERE, Slot Filling does not have
as its primary goal the annotation of text. Rather,
the aim is to identify knowledge nuggets about a
focal named entity using a fixed inventory of re-
lations and attributes. For example, given a fo-
cal entity such as former Ukrainian prime minister
Yulia Tymoshenko, the task is to identify attributes
such as schools she attended, occupations, and im-
mediate family members. This is the same sort
of information commonly listed about prominent
people in Wikipedia Infoboxes and in derivative
databases such as FreeBase and DBpedia.
Consequently, Slot Filling is somewhat of a hy-
brid between relation extraction and question an-
swering ? slot fills can be considered as the cor-
rect responses to a fixed set of questions. The rela-
tions and attributes used in the 2013 task are pre-
sented in Table 5.
5.2 Differences with ACE-style relation
extraction
Slot Filling in TAC-KBP differs from extraction in
ACE and ERE in several significant ways:
? information is sought for named entities,
chiefly PERs and ORGs;
? the focus is on values not mentions;
? assessment is more like QA; and,
? events are handled as uncorrelated slots
In traditional IE evaluation, there was an
implicit skew towards highly attested in-
formation such as leader(Bush, US), or
capital(Paris, France). In contrast, TAC-KBP
gives full credit for finding a single instance of a
correct fill instead of every attestation of that fact.
Slot Filling assessment is somewhat simpler
than IE annotation. The assessor must decide
if provenance text is supportive of a posited fact
about the focal entity instead of annotating a doc-
ument with all evidenced relations and events for
any entity. For clarity and to increase assessor
agreement, guidelines have been developed to jus-
tify when a posited relation is deemed adequately
supported from text. Additionally, the problem of
50
Relations Attributes
per:children org:shareholders per:alternate names org:alternate names
per:other family org:founded by per:date of birth org:political religious affiliation
per:parents org:top members employees per:age org:number of employees members
per:siblings org:member of per:origin org:date founded
per:spouse org:members per:date of death org:date dissolved
per:employee or member of org:parents per:cause of death org:website
per:schools attended org:subsidiaries per:title
per:city of birth org:city of headquarters per:religion
per:stateorprovince of birth org:stateorprovince of headquarters per:charges
per:country of birth org:country of headquarters
per:cities of residence
per:statesorprovinces of residence
per:countries of residence
per:city of death
per:stateorprovince of death
per:country of death
Table 5: Relation and attributes for PERs and ORGs.
slot value equivalence becomes an issue - a sys-
tem should be penalized for redundantly asserting
that a person has four children named Tim, Beth,
Timothy, and Elizabeth, or that a person is both a
cardiologist and a doctor.
Rather than explicitly modeling events, TAC-
KBP created relations that capture events, more
in line with the notion of Infobox filling or ques-
tion answering (McNamee et al., 2010). For exam-
ple, instead of a criminal event, there is a slot fill
for charges brought against an entity. Instead of a
founding event, there are slots like org:founded by
(who) and org:date founded (when). Thus a state-
ment that ?Jobs is the founder and CEO of Apple?
is every bit as useful for the org:founded by rela-
tion as ?Jobs founded Apple in 1976.? even though
the date is not included in the former sentence.
5.3 Additional tasks
Starting in 2012 TAC-KBP introduced the ?Cold
Start? task, which is to literally produce a KB
based on the Slot Filling schema. To date, Cold
Start KBs have been built from collections of
O(50,000) documents, and due to their large size,
they are assessed by sampling. There is also
an event argument detection evaluation in KBP
planned for 2014.
Other TAC-KBP tasks have been introduced in-
cluding determining the timeline when dynamic
slot fills are valid (e.g., CEO of Microsoft), and
targeted sentiment.
6 FrameNet
The FrameNet project has rather different moti-
vations than either ACE/ERE or TAC-KBP, but
shares with them a goal of capturing informa-
tion about events and relations in text. FrameNet
stems from Charles Fillmore?s linguistic and lex-
icographic theory of Frame Semantics (Fillmore,
1976; Fillmore, 1982). Frames are descriptions
of event (or state) types and contain information
about event participants (frame elements), infor-
mation as to how event types relate to each other
(frame relations), and information about which
words or multi-word expressions can trigger a
given frame (lexical units).
FrameNet is designed with text annotation in
mind, but unlike ACE/ERE it prioritizes lexico-
graphic and linguistic completeness over ease of
annotation. As a result Frames tend to be much
finer grained than ACE/ERE events, and are more
numerous by an order of magnitude. The Berkeley
FrameNet Project (Baker et al., 1998) was devel-
oped as a machine readable database of distinct
frames and lexical units (words and multi-word
constructions) that were known to trigger specific
frames.
1
FrameNet 1.5 includes 1020 identified
frames and 11830 lexical units.
One of the most widespread uses of FrameNet
has been as a resource for Semantic Role Label-
ing (SRL) (Gildea and Jurafsky, 2002). FrameNet
related SRL was promoted as a task by the
SENSEVAL-3 workshop (Litkowski, 2004), and
the SemEval-2007 workshop (Baker et al., 2007).
(Das et al., 2010) is a current system for automatic
FrameNet annotation.
The relation and attribute types of TAC-KBP
and the relation and event types in the ACE/ERE
standards can be mapped to FrameNet frames.
The mapping is complicated by two factors.
The first is that FrameNet frames are gener-
ally more fine-grained than the ACE/ERE cate-
gories. As a result the mapping is sometimes
one-to-many. For example, the ERE relation Af-
1
This database is accessible via webpage (https:
//framenet.icsi.berkeley.edu/fndrupal/)
and as a collection of XML files by request.
51
Relations
FrameNet ACE ERE TAC-KBP
Kinship Personal-Social.Family Social.Family per:children
per:other family
per:parents
per:siblings
per:spouse
Being Employed ORG-Affiliation.Employment Affiliation.Employment/Membership per:employee or member of
Membership org:member of
Being Located Physical.Located Physical.Located org:city of headquarters
org:stateorprovince of headquarters
org:country of headquarters
Events
FrameNet ACE ERE
Contacting Phone-Write Communicate
Extradition Justice-Extradition Justice-Extradition
Attack Conflict-Attack Conflict-Attack
Being Born Life-Be Born Life-Be Born
Attributes
FrameNet TAC-KBP
Being Named per:alternate names
Age per:age
Table 6: Rough mappings between subsets of FrameNet, ACE, ERE, and TAC-KBP
filiation.Employment/Membership covers both
the Being Employed frame and the Member-
ship frame. At the same time, while TAC-
KBP has only a handful of relations relative to
FrameNet, some of these relations are more fine-
grained than the analogous frames or ACE/ERE
relations. For example, the frame Kinship, which
maps to the single ERE relation Social.Family,
maps to five TAC-KBP relations, and the Be-
ing Located, which maps to the ACE/ERE rela-
tion Being.Located, maps to three TAC-KBP re-
lations. Rough mappings from a selection of rela-
tions, events, and attributes are given in Table 6.
The second complication arises from the fact
that FrameNet frames are more complex objects
than ERE/ACE events, and considerably more
complex than TAC-KBP relations. Rather than the
two entities related via a TAC-KBP or ACE/ERE
relation, some frames have upwards of 20 frame
elements. Table 7 shows in detail the mapping be-
tween frame elements in the Extradition frame and
ACE?s and ERE?s Justice-Extradition events. The
?core? frame elements map exactly to the ERE
event, the remaining two arguments in the ACE
event map to two non-core frame elements, and
the frame includes several more non-core elements
with no analogue in either ACE or ERE standards.
7 Conclusion
The ACE and ERE annotation schemas have
closely related goals of identifying similar in-
formation across various possible types of docu-
ments, though their approaches differ due to sepa-
rate goals regarding scope and replicability. ERE
differs from ACE in collapsing different Type dis-
tinctions and in removing annotation features in
order to eliminate annotator confusion and to im-
FrameNet ACE ERE
Authorities Agent-Arg Agent-Arg
Crime jursidiction Destination-Arg Destination-Arg
Current jursidiction Origin-Arg Origin-Arg
Suspect Person-Arg Person-Arg
Reason Crime-Arg
Time Time-Arg
Legal Basis
Manner
Means
Place
Purpose
Depictive
Table 7: Mapping between frame elements of Ex-
tradition (FrameNet), and arguments of Justice-
Extradition (ACE/ERE): A line divides core frame
elements (above) from non-core (below).
prove consistency, efficiency, and higher inter-
annotator agreement. TAC-KPB slot-filling shares
some goals with ACE/ERE, but is wholly fo-
cused on a set collection of questions (slots to
be filled) concerning entities to the extent that
there is no explicit modeling of events. At the
other extreme, FrameNet seeks to capture the
full range of linguistic and lexicographic varia-
tion in event representations in text. In general, all
events, relations, and attributes that can be repre-
sented by ACE/ERE and TAC-KBP standards can
be mapped to FrameNet representations, though
adjustments need to be made for granularity of
event/relation types and granularity of arguments.
Acknowledgements
This material is partially based on research spon-
sored by the NSF under grant IIS-1249516 and
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program).
52
References
Collin F Baker, Charles J Fillmore, and John B Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 17th international conference on Compu-
tational linguistics-Volume 1, pages 86?90. Associ-
ation for Computational Linguistics.
Collin Baker, Michael Ellsworth, and Katrin Erk.
2007. Semeval-2007 task 19: Frame semantic
structure extraction. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 99?104, Prague, Czech Re-
public, June. Association for Computational Lin-
guistics.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A Smith. 2010. Probabilistic frame-semantic
parsing. In Proceedings of NAACL-HLT, pages 948?
956. Association for Computational Linguistics.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lancec Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extrac-
tion (ace) program- tasks, data, and evaluation. In
Proceedings of LREC 2004: Fourth International
Conference on Language Resources and Evaluation,
Lisbon, May 24-30.
Charles J Fillmore. 1976. Frame semantics and the na-
ture of language. Annals of the New York Academy
of Sciences, 280(1):20?32.
Charles Fillmore. 1982. Frame semantics. In Linguis-
tics in the morning calm, pages 111?137. Hanshin
Publishing Co.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245?288.
Linguistic Data Consortium. 2005. ACE (automatic
content extraction) English annotation guidelines
for events. https://www.ldc.upenn.edu/
collaborations/past-projects/ace.
Version 5.4.3 2005.07.01.
Linguistic Data Consortium. 2006. ACE (automatic
content extraction) English annotation guidelines
for entities. https://www.ldc.upenn.edu/
collaborations/past-projects/ace,
Version 5.6.6 2006.08.01.
Linguistic Data Consortium. 2008. ACE (automatic
content extraction) English annotation guidelines for
relations. https://www.ldc.upenn.edu/
collaborations/past-projects/ace.
Version 6.0 2008.01.07.
Linguistic Data Consortium. 2013a. DEFT ERE anno-
tation guidelines: Entities v1.1, 05.17.2013.
Linguistic Data Consortium. 2013b. DEFT ERE anno-
tation guidelines: Events v1.1. 05.17.2013.
Linguistic Data Consortium. 2013c. DEFT ERE anno-
tation guidelines: Relations v1.1. 05.17.2013.
Ken Litkowski. 2004. Senseval-3 task: Automatic
labeling of semantic roles. In Rada Mihalcea and
Phil Edmonds, editors, Senseval-3: Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, pages 9?12, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
Paul McNamee, Hoa Trang Dang, Heather Simpson,
Patrick Schone, and Stephanie Strassel. 2010. An
evaluation of technologies for knowledge base pop-
ulation. In Proceedings of LREC.
53
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 93?103,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Transliteration of Arabizi into Arabic Orthography: Developing a 
Parallel Annotated Arabizi-Arabic Script SMS/Chat Corpus 
 
Ann Bies, Zhiyi Song, Mohamed Maamouri, Stephen Grimes, Haejoong Lee,  
Jonathan Wright, Stephanie Strassel, Nizar Habash?, Ramy Eskander?, Owen Rambow? 
Linguistic Data Consortium, University of Pennsylvania 
{bies,zhiyi,maamouri,sgrimes,haejoong, 
jdwright,strassel}@ldc.upenn.edu 
?Computer Science Department, New York University Abu Dhabi 
?
nizar.habash@nyu.edu 
?Center for Computational Learning Systems, Columbia University 
?
{reskander,rambow}@ccls.columbia.edu 
 
  
 
Abstract 
This paper describes the process of creating a 
novel resource, a parallel Arabizi-Arabic 
script corpus of SMS/Chat data.  The lan-
guage used in social media expresses many 
differences from other written genres: its vo-
cabulary is informal with intentional devia-
tions from standard orthography such as re-
peated letters for emphasis; typos and non-
standard abbreviations are common; and non-
linguistic content is written out, such as 
laughter, sound representations, and emoti-
cons.  This situation is exacerbated in the 
case of Arabic social media for two reasons.  
First, Arabic dialects, commonly used in so-
cial media, are quite different from Modern 
Standard Arabic phonologically, morphologi-
cally and lexically, and most importantly, 
they lack standard orthographies. Second, 
Arabic speakers in social media as well as 
discussion forums, SMS messaging and 
online chat often use a non-standard romani-
zation called Arabizi.  In the context of natu-
ral language processing of social media Ara-
bic, transliterating from Arabizi of various 
dialects to Arabic script is a necessary step, 
since many of the existing state-of-the-art re-
sources for Arabic dialect processing expect 
Arabic script input.  The corpus described in 
this paper is expected to support Arabic NLP 
by providing this resource. 
1 Introduction 
The language used in social media expresses 
many differences from other written genres: its 
vocabulary is informal with intentional devia-
tions from standard orthography such as repeated 
letters for emphasis; typos and non-standard ab-
breviations are common; and non-linguistic con-
tent is written out, such as laughter, sound repre-
sentations, and emoticons. 
This situation is exacerbated in the case of Ar-
abic social media for two reasons.  First, Arabic 
dialects, commonly used in social media, are 
quite different from Modern Standard Arabic 
(MSA) phonologically, morphologically and lex-
ically, and most importantly, they lack standard 
orthographies (Maamouri et.al. 2014). Second, 
Arabic speakers in social media as well as dis-
cussion forums, Short Messaging System (SMS) 
text messaging and online chat often use a non-
standard romanization called ?Arabizi? (Dar-
wish, 2013).  Social media communication in 
Arabic takes place using a variety of orthogra-
phies and writing systems, including Arabic 
script, Arabizi, and a mixture of the two.  Alt-
hough not all social media communication uses 
Arabizi, the use of Arabizi is prevalent enough to 
pose a challenge for Arabic NLP research. 
In the context of natural language processing 
of social media Arabic, transliterating from 
Arabizi of various dialects to Arabic script is a 
necessary step, since many of the existing state-
of-the-art resources for Arabic dialect processing 
and annotation expect Arabic script input (e.g., 
Salloum and Habash, 2011; Habash et al. 2012c; 
Pasha et al., 2014). 
To our knowledge, there are no naturally oc-
curring parallel texts of Arabizi and Arabic 
script.  In this paper, we describe the process of 
creating such a novel resource at the Linguistic 
Data Consortium (LDC).  We believe this corpus 
will be essential for developing robust tools for 
converting Arabizi into Arabic script. 
93
The rest of this paper describes the collection 
of Egyptian SMS and Chat data and the creation 
of a parallel text corpus of Arabizi and Arabic 
script for the DARPA BOLT program.1  After 
reviewing the history and features in Arabizi 
(Section 2) and related work on Arabizi (Section 
3), in Section 4, we describe our approach to col-
lecting the Egyptian SMS and Chat data and the 
annotation and transliteration methodology of the 
Arabizi SMS and Chat into Arabic script, while 
in Section 5, we discuss the annotation results, 
along with issues and challenges we encountered 
in annotation. 
2 Arabizi and Egyptian Arabic Dialect 
2.1 What is Arabizi? 
Arabizi is a non-standard romanization of Arabic 
script that is widely adopted for communication 
over the Internet (World Wide Web, email) or 
for sending messages (instant messaging and 
mobile phone text messaging) when the actual 
Arabic script alphabet is either unavailable for 
technical reasons or otherwise more difficult to 
use.  The use of Arabizi is attributed to different 
reasons, from lack of good input methods on 
some mobile devices to writers? unfamiliarity 
with Arabic keyboard.  In some cases, writing in 
Arabizi makes it easier to code switch to English 
or French, which is something educated Arabic 
speakers often do.  Arabizi is used by speakers of 
a variety of Arabic dialects. 
Because of the informal nature of this system, 
there is no single ?correct? encoding, so some 
character usage overlaps.  Most of the encoding 
in the system makes use of the Latin character 
(as used in English and French) that best approx-
imates phonetically the Arabic letter that one 
wants to express (for example, either b or p cor-
responds to ?).  This may sometimes vary due to 
regional variations in the pronunciation of the 
Arabic letter (e.g., j is used to represent ? in the 
Levantine dialect, while in Egyptian dialect g is 
used) or due to differences in the most common 
non-Arabic second language (e.g., sh corre-
sponds to ? in the previously English dominated 
Middle East Arab countries, while ch shows a 
predominantly French influence as found in 
North Africa and Lebanon).  Those letters that do 
not have a close phonetic approximate in the Lat-
in script are often expressed using numerals or 
other characters, so that the numeral graphically 
                                                 
1 http://www.darpa.mil/Our_Work/I2O/Programs/Broad_Op 
erational_Language_Translation_%28BOLT%29.aspx 
approximates the Arabic letter that one wants to 
express (e.g., the numeral 3 represents ? because 
it looks like a mirror reflection of the letter). 
Due to the use of Latin characters and also 
frequent code switching in social media Arabizi, 
it can be difficult to distinguish between Arabic 
words written in Arabizi and entirely unrelated 
foreign language words (Darwish 2013).  For 
example, mesh can be the English word, or 
Arabizi for ?? ?not?.  However, in context these 
cases can be clearly labeled as either Arabic or a 
foreign word.  An additional complication is that 
many words of foreign origin have become Ara-
bic words (?borrowings?).  Examples include 
banadoora ?????? ?tomato? and mobile ?????? 
?mobile phone?.  It is a well-known practical and 
theoretical problem to distinguish borrowings 
(foreign words that have become part of a lan-
guage and are incorporated fully into the mor-
phological and syntactic system of the host lan-
guage) from actual code switching (a bilingual 
writer switches entirely to a different language, 
even if for only a single word).  Code switching 
is easy to identify if we find an extended passage 
in the foreign language which respects that lan-
guage?s syntax and morphology, such as Bas eh 
ra2yak I have the mask.  The problem arises 
when single foreign words appear without Arabic 
morphological marking: it is unclear if the writer 
switched to the foreign language for one word or 
whether he or she simply is using an Arabic 
word of foreign origin.  In the case of banadoora 
?????? ?tomato?, there is little doubt that this has 
become a fully Arabic word and the writer is not 
code switching into Italian; this is also signaled 
by the fact that a likely Arabizi spelling (such as 
banadoora) is not in fact the Italian orthography 
(pomodoro).  However, the case is less clear cut 
with mobile ?????? ?mobile phone?: even if it is a 
borrowing (clearly much more recent than bana-
doora ?????? ?tomato?), a writer will likely spell 
the word with the English orthography as mobile 
rather than write, say, mubail.  More research is 
needed on this issue.  However, because of the 
difficulty of establishing the difference between 
code switching and borrowing, we do not attempt 
to make this distinction in this annotation 
scheme. 
2.2 Egyptian Arabic Dialect 
Arabizi is used to write in multiple dialects of 
Arabic, and differences between the dialects 
themselves have an effect on the spellings cho-
sen by individual writers using Arabizi.  Because 
Egyptian Arabic is the dialect of the corpus cre-
94
ated for this project, we will briefly discuss some 
of the most relevant features of Egyptian Arabic 
with respect to Arabizi transliteration.  For a 
more extended discussion of the differences be-
tween MSA and Egyptian Arabic, see Habash et 
al. (2012a) and Maamouri et al. (2014). 
Phonologically, Egyptian Arabic is character-
ized by the following features, compared with 
MSA: 
(a) The loss of the interdentals /?/ and /?/ 
which are replaced by /d/ or /z/ and /t/ or /s/ 
respectively, thus giving those two original 
consonants a heavier load. Examples in-
clude  ??? /zakar/ ?to mention?, ???  /daba?/ 
?to slaughter?,  ???  /talg/ ?ice?,  ???  /taman/ 
?price?, and  ???  /sibit/ ?to stay in place, 
become immobile?. 
(b) The exclusion of /q/ and /?/ from the conso-
nantal system, being replaced by the /?/ and 
/g/, e.g., ???  /?u?n/ ?cotton?, and  ???  
/gamal/ ?camel?. 
At the level of morphology and syntax, the 
structures of Egyptian Arabic closely resemble 
the overall structures of MSA with relatively mi-
nor differences to speak of.  Finally, the Egyptian 
Arabic lexicon shows some significant elements 
of semantic differentiation. 
The most important morphological difference 
between Egyptian Arabic and MSA is in the use 
of some Egyptian clitics and affixes that do not 
exist in MSA.  For instance, Egyptian Arabic has 
the future proclitics h+ and ?+ as opposed to the 
standard equivalent s+. 
Lexically, there are lexical differences be-
tween Egyptian Arabic and MSA where no ety-
mological connection or no cognate spelling is 
available.  For example, the Egyptian Arabic ??  
/bu??/ ?look? is ???? /?unZur/ in MSA. 
3 Related Work 
Arabizi-Arabic Script Transliteration  Previ-
ous efforts on automatic transliterations from 
Arabizi to Arabic script include work by Chalabi 
and Gerges (2012), Darwish (2013) and Al-
Badrashiny et al. (2014).  All of these approaches 
rely on a model for character-to-character map-
ping that is used to generate a lattice of multiple 
alternative words which are then selected among 
using a language model.  The training data used 
by Darwish (2013) is publicly available but it is 
quite limited (2,200 word pairs).  The work we 
are describing here can help substantially im-
prove the quality of such system.  We use the 
system of Al-Badrashiny et al. (2014) in this pa-
per as part of the automatic transliteration step 
because they target the same conventional or-
thography of dialectal Arabic (CODA) (Habash 
et al., 2012a, 2012b), which we also target.  
There are several commercial products that con-
vert Arabizi to Arabic script, namely: Microsoft 
Maren, 2  Google Ta3reeb, 3  Basis Arabic chat 
translator4 and Yamli.5  Since these products are 
for commercial purposes, there is little infor-
mation available about their approaches, and 
whatever resources they use are not publicly 
available for research purposes.  Furthermore, as 
Al-Badrashiny et al. (2014) point out, Maren, 
Ta3reeb and Yamli are primarily intended as in-
put method support, not full text transliteration.  
As a result, their users? goal is to produce Arabic 
script text not Arabizi text, which affects the 
form of the romanization they utilize as an in-
termediate step.  The differences between such 
?functional romanization? and real Arabizi in-
clude that the users of these systems will use less 
or no code switching to English, and may em-
ploy character sequences that help them arrive at 
the target Arabic script form faster, which other-
wise they would not write if they were targeting 
Arabizi (Al-Badrashiny et al., 2014). 
Name Transliteration  There has been some 
work on machine transliteration by Knight and 
Graehl (1997).  Al-Onaizan and Knight (2002) 
introduced an approach for machine translitera-
tion of Arabic names. Freeman et al. (2006) also 
introduced a system for name matching between 
English and Arabic.  Although the general goal 
of transliterating from one script to another is 
shared between these efforts and ours, we are 
considering a more general form of the problem 
in that we do not restrict ourselves to names. 
Code Switching  There is some work on code 
switching between Modern Standard Arabic 
(MSA) and dialectal Arabic (DA).  Zaidan and 
Callison-Burch (2011) were interested in this 
problem at the inter-sentence level.  They 
crawled a large dataset of MSA-DA news com-
mentaries, and used Amazon Mechanical Turk to 
annotate the dataset at the sentence level.  
Elfardy et al. (2013) presented a system, AIDA, 
that tags each word in a sentence as either DA or 
MSA based on the context.  Lui et al. (2014) 
proposed a system for language identification in 
                                                 
2 http://www.getmaren.com 
3 http://www.google.com/ta3reeb 
4 http://www.basistech.com/arabic-chat-translator-
transforms-social-media-analysis/ 
5 http://www.yamli.com/ 
95
multilingual documents using a generative mix-
ture model that is based on supervised topic 
modeling algorithms.  Darwish (2013) and Voss 
et al. (2014) deal with exactly the problem of 
classifying tokens in Arabizi as Arabic or not.  
More specifically, Voss et al. (2014) deal with 
Moroccan Arabic, and with both French and 
English, meaning they do a three-way classifica-
tion.  Darwish (2013)'s data is more focused on 
Egyptian and Levantine Arabic and code switch-
ing with English. 
Processing Social Media Text  Finally, while 
English NLP for social media has attracted con-
siderable attention recently (Clark and Araki, 
2011; Gimpel et al., 2011; Gouws et al., 2011; 
Ritter et al., 2011; Derczynski et al., 2013), there 
has not been much work on Arabic yet.  Darwish 
et al. (2012) discuss NLP problems in retrieving 
Arabic microblogs (tweets).  They discuss many 
of the same issues we do, notably the problems 
arising from the use of dialectal Arabic such as 
the lack of a standard orthography.  Eskander et 
al. (2013) described a method for normalizing 
spontaneous orthography into CODA. 
4 Corpus Creation 
This work was prepared as part of the DARPA 
Broad Operational Language Translation 
(BOLT) program which aims at developing tech-
nology that enables English speakers to retrieve 
and understand information from informal for-
eign language sources including chat, text mes-
saging and spoken conversations. LDC collects 
and annotates informal linguistic data of English, 
Chinese and Arabic, with Egyptian Arabic being 
the representative of the Arabic language family.  
 
 
Egyptian Arabic has the advantage over all other 
dialects of Arabic of being the language of the 
largest linguistic community in the Arab region, 
and also of having a rich level of internet com-
munication.  
4.1 SMS and Chat Collection 
In BOLT Phase 2, LDC collected large volumes 
of naturally occurring informal text (SMS) and 
chat messages from individual users in English, 
Chinese and Egyptian Arabic (Song et al., 2014).  
Altogether we recruited 46 Egyptian Arabic par-
ticipants, and of those 26 contributed data.  To 
protect privacy, participation was completely 
anonymous, and demographic information was 
not collected.  Participants completed a brief lan-
guage test to verify that they were native Egyp-
tian Arabic speakers.  On average, each partici-
pant contributed 48K words.  The Egyptian Ara-
bic SMS and Chat collection consisted of 2,140 
conversations in a total of 475K words after 
manual auditing by native speakers of Egyptian 
Arabic to exclude inappropriate messages and 
messages that were not Egyptian Arabic.  96% of 
the collection came from the personal SMS or 
Chat archives of participants, while 4% was col-
lected through LDC?s platform, which paired 
participants and captured their live text messag-
ing (Song et al., 2014).  A subset of the collec-
tion was then partitioned into training and eval 
datasets.   
Table 1 shows the distribution of Arabic script 
vs. Arabizi in the training dataset.  The conversa-
tions that contain Arabizi were then further anno-
tated and transliterated to create the Arabizi-
Arabic script parallel corpus, which consists of 
 
 
 Total Arabic 
script only 
Arabizi 
only 
Mix of Arabizi and Arabic script 
Arabizi Arabic script 
Conversations 1,503 233 987 283 
Messages 101,292 18,757 74,820 3,237 4,478 
Sentence units 94,010 17,448 69,639 3,017 3,906 
Words 408,485 80,785 293,900 10,244 23,556 
Table 1. Arabic SMS and Chat Training Dataset 
 
1270 conversations. 6   All conversations in the 
training dataset were also translated into English 
to provide Arabic-English parallel training data. 
                                                 
6 In order to form single, coherent units (Sentence units) of 
an appropriate size for downstream annotation tasks using 
this data, messages that were split mid-sentence (often mid-
Not surprisingly, most Egyptian conversations 
in our collection contain at least some Arabizi; 
                                                                          
word) due to SMS messaging character limits were rejoined, 
and very long messages (especially common in chat) were 
split into two or more units, usually no longer than 3-4 sen-
tences. 
96
only 15% of conversations are entirely written in 
Arabic script, while 66% are entirely Arabizi.  
The remaining 19% contain a mixture of the two 
at the conversation level.  Most of the mixed 
conversations were mixed in the sense that one 
side of the conversation was in Arabizi and the 
other side was in Arabic script, or in the sense 
that at least one of the sides switched between 
the two forms in mid-conversation.  Only rarely 
are individual messages in mixed scripts.  The 
annotation for this project was performed on the 
Arabizi tokens only.  Arabic script tokens were 
not touched and were kept in their original 
forms.  
The use of Arabizi is predominant in the SMS 
and Chat Egyptian collection, in addition to the 
presence of other typical cross-linguistic text ef-
fects in social media data.  For example, the use 
of emoticons and emoji is frequent.  We also ob-
served the frequent use of written out representa-
tions of speech effects, including representations 
of laughter (e.g., hahaha), filled pauses (e.g., 
um), and other sounds (e.g., hmmm).  When these 
representations are written in Arabizi, many of 
them are indistinguishable from the same repre-
sentations in English SMS data.  Neologisms are 
also frequently part of SMS/Chat in Egyptian  
 
Arabic, as they are in other languages.  English 
words use Arabic morphology or determiners, as 
in el anniversary ?the anniversary?.  Sometimes 
English words are spelled in a way that is closer 
phonetically to the way an Egyptian speaker 
would pronounce them, for example lozar for 
?loser?, or beace for ?peace?. 
The adoption of Arabizi for SMS and online 
chat may also go some way to explaining the 
high frequency of code mixing in the Egyptian 
Arabic collection.  While the auditing process 
eliminated messages that were entirely in a non-
target language, many of the acceptable messag-
es contain a mixture of Egyptian Arabic and 
English. 
4.2 Annotation Methodology 
All of the Arabizi conversations, including the 
conversations containing mixtures of Arabizi and 
Arabic script were then annotated and translit-
erated: 
1. Annotation on the Arabizi source text to 
flag certain features 
2. Correction and normalization of the trans-
literation according to CODA conventions 
 
 
 
Figure 1. Arabizi Annotation and Transliteration Tool 
 
The annotators were presented with the source 
conversations in their original Arabizi form as 
well as the transliteration output from an auto-
matic Arabization system, and used a web-based 
tool developed by LDC (see Figure 1) to perform 
the two annotation tasks, which allowed annota-
tors perform both annotation and transliteration 
token by token, sentence by sentence and review 
the corrected transliteration in full context.  The 
GUI shows the full conversation in both the orig-
inal Arabizi and the resulting Arabic script trans-
literation for each sentence.  Annotators must 
97
annotate each sentence in order, and the annota-
tion is displayed in three columns.  The first col-
umn shows the annotation of flag features on the 
source tokens, the second column is the working 
panel where annotators correct the automatic 
transliteration and retokenize, and the third col-
umn displays the final corrected and retokenized 
result. 
Annotation was performed according to anno-
tation guidelines developed at the Linguistic Da-
ta Consortium specifically for this task (LDC, 
2014). 
4.3 Automatic Transliteration 
To speed up the annotation process, we utilized 
an automatic Arabizi-to-Arabic script translitera-
tion system (Al-Badrashiny et al., 2014) which 
was developed using a small vocabulary of 2,200 
words from Darwish (2013) and an additional 
6,300 Arabic-English proper name pairs (Buck-
walter, 2004).  The system has an accuracy of 
69.4%.  We estimate that using this still allowed 
us to cut down the amount of time needed to type 
in the Arabic script version of the Arabizi by 
two-thirds.  This system did not identify Foreign 
words or Names and transliterated all of the 
words.  In one quarter of the errors, the provided 
answer was plausible but not CODA-compliant 
(Al-Badrashiny et al., 2014). 
4.4 Annotation on Arabizi Source Text to 
Flag Features 
This annotation was performed only on sentences 
containing Arabizi words, with the goal of tag-
ging any words in the source Arabizi sentences 
that would be kept the same in the output of an 
English translation with the following flags: 
 
? Punctuation (not including emoticons) 
o Eh ?!//Punct  
o Ma32ula ?!//Punct 
o Ebsty ?//Punct  
 
? Sound effects, such as laughs (?haha? or 
variations), filled pauses, and other sounds 
(?mmmm? or ?shh? or ?um? etc.) 
o hahhhahhah//Sound akeed 3arfa :p da 
enty t3rafy ablia :pp 
o Hahahahaahha//Sound Tb ana ta7t fel 
ahwaa 
o Wala Ana haha//Sound 
o Mmmm//Sound okay 
 
? Foreign language words and numbers.  All 
cases of code switching and all cases of bor-
rowings which are rendered in Arabizi us-
ing standard English orthography are 
marked as ?Foreign?. 
o ana kont mt25er fe t2demm l pro-
jects//Foreign 
o oltilik okay//Foreign ya Babyy//Foreign 
balashhabal!!!! 
o zakrty ll sat//Foreign 
o Bat3at el whatsapp//Foreign 
o La la la merci//Foreign gedan bs la2 
o We 9//Foreign galaeeb dandash lel ban-
at 
 
? Names, mainly person names 
o Youmna//Name 7atigi?? 
 
4.5 Correction and Normalization of the 
Transliteration According to CODA 
Conventions 
The goal of this task was to correct all spelling in 
the Arabic script transliteration to CODA stand-
ards (Habash et al., 2012a, 2012b).  This meant 
that annotators were required to confirm both (1) 
that the word was transliterated into Arabic script 
correctly and also (2) that the transliterated word 
conformed to CODA standards.  The automatic 
transliteration was provided to the annotators, 
and manually corrected by annotators as needed. 
Correcting spelling to a single standard (CO-
DA), however, necessarily included some degree 
of normalization of the orthography, as the anno-
tators had to correct from a variety of dialect 
spellings to a single CODA-compliant spelling 
for each word.  Because the goal was to reach a 
consistent representation of each word, ortho-
graphic normalization was almost the inevitable 
effect of correcting the automatic transliteration.  
This consistent representation will allow down-
stream annotation tasks to take better advantage 
of the SMS/Chat data.  For example, more con-
sistent spelling of Egyptian Arabic words will 
lead to better coverage from the CALIMA mor-
phological analyzer and therefore improve the 
manual annotation task for morphological anno-
tation, as in Maamouri et al. (2014). 
 
Modern Standard Arabic (MSA) cognates and 
Egyptian Arabic sound changes 
Annotators were instructed to use MSA or-
thography if the word was a cognate of an MSA 
98
root, including for those consonants that have 
undergone sound changes in Egyptian Arabic.7 
? use mqfwl ?????  and not ma>fwl ?????  for 
?locked? 
? use HAfZ ???? and not HAfz ???? for the 
name (a proper noun)  
 
Long vowels 
Annotators were instructed to reinstate miss-
ing long vowels, even when they were written as 
short vowels in the Arabizi source, and to correct 
long vowels if they were included incorrectly. 
? use sAEap ???? and not saEap  ???  for 
?hour? 
? use qAlt   ????  and not qlt ??? for ?(she) 
said? 
 
Consonantal ambiguities 
Many consonants are ambiguous when written 
in Arabizi, and many of the same consonants are 
also difficult for the automatic transliteration 
script.  Annotators were instructed to correct any 
errors of this type.   
? S vs. s/ ?  vs. ? 
o use SAyg ????  and not  sAyg  ????  for 
?jeweler? 
? D vs. Z/ ?  vs. ? 
o use DAbT ????  and not  ZAbT ???? for 
?officer? 
o use Zlmp  ????  and not Dlmp  ????  for 
?darkness? 
? Dotted ya vs. Alif Maqsura/ ? vs. ?.  Alt-
hough the dotted ya/ ? and Alif Maqsura/ ? 
are often used interchangeably in Egyptian 
Arabic writing conventions, it was neces-
sary to make the distinction between the 
two for this task. 
o use Ely ???  and not ElY  ???  for ?Ali? 
(the proper name)  
? Taa marbouta.  In Arabizi and so also in the 
Arabic script transliteration, the taa mar-
bouta/ ? may be written for both nominal fi-
nal -h/ ? and verbal final -t/ ?, but for dif-
ferent reasons. 
o mdrsp Ely  ???  ?????  ?Ali?s school? 
o mdrsth  ??????  ?his school? 
 
Morphological ambiguities 
Spelling variation and informal usage can 
combine to create morphological ambiguities as 
well.  For example, the third person masculine 
                                                 
7 Both Arabic script and the Buckwalter transliteration 
(http://www.qamus.org/transliteration.htm) are shown for 
the transliterated examples in this paper. 
singular pronoun and the third person plural ver-
bal suffix can be ambiguous in informal texts.  
For example: 
? use byHbwA bED  ???  ?????? and not byHbh 
bED  ???  ?????  for ?(They) loved each oth-
er? 
? use byEmlwA  ???????  and not byEmlh  ??????  
for ?(They) did? or ?(They) worked? 
In addition, because final -h is sometimes re-
placed in speech by final /-uw/, it was occasion-
ally necessary to correct cases of overuse of the 
third person plural verbal suffix (-wA) to the 
pronoun -h as well. 
 
Merging and splitting tokens written with in-
correct word boundaries 
Annotators were instructed to correct any 
word that was incorrectly segmented.  The anno-
tation tool allowed both the merging and splitting 
of tokens. 
Clitics were corrected to be attached when 
necessary according to (MSA) standard writing 
conventions.  These include single letter proclit-
ics (both verbal and nominal) and the negation 
suffix -$, as well as pronominal clitics such as 
possessive pronouns and direct object pronouns.  
For example, 
? use fAlbyt  ??????    and not  
fAl  byt  ???  ??? or  flbyt  ?????   for ?in the 
house? 
? use EAlsTH ?????? and not  
EAl sTH ??? ??? or ElsTH ????? for ?on the 
roof? 
The conjunction w- / -? is always attached to 
its following word. 
? use wkAn  ????  and not w kAn  ??? ?    for 
?and was? 
? use wrAHt  ????? and not w  rAHt ????  ?  
for ?and (she) left? 
Words that were incorrectly segmented in the 
Arabizi source were also merged.  For example, 
? use msHwrp ?????? and not  
ms Hwrp ???? ??  for ?bewitched 
(fem.sing.)? 
? use $ErhA ????? and not $Er hA  ?? ???   for 
?her hair? 
Particles that are not attached in standard 
MSA written forms were corrected as necessary 
by the splitting function of the tool.  For exam-
ple,  
? use yA Emry  ???? ?? and not yAEmry  
??????  for ?Hey, dear!? 
? use lA trwH  ????  ? and not lAtrwH  ?????  
for ?Do not go? 
99
 
Abbreviations in Arabizi 
Three abbreviations in Arabizi received spe-
cial treatment: msa, isa, 7ma.  These three abbre-
viations only were expanded out to their full 
form using Arabic words in the corrected Arabic 
script transliteration. 
? msa: use mA $A' All~h  ? ???  ?? for ?As 
God wills? 
? isa: use <n $A' All~h  ? ???  ?? for ?God 
willing? 
? 7ma: use AlHmd ll~h for     ??? ??  ?Thank 
God, Praised be the Lord? 
All other Arabic abbreviations were not ex-
panded, and were transliterated simply letter for 
letter.  When the abbreviation was in English or 
another foreign language, it was kept as is in the 
transliteration, using both consonants and semi-
vowels to represent it. 
? use Awkyh  ????  for ?OK? (note that this is 
an abbreviation in English, but not in Egyp-
tian Arabic) 
 
Correcting Arabic typos 
Annotators were instructed to correct typos in 
the transliterated Arabic words, including typos 
in proper names.  However, typos and non-
standard spellings in the transliteration of a for-
eign words were kept as is and not corrected. 
? Ramafan  ?????  should be corrected to 
rmDAn  ?????  for ?Ramadan? 
? babyy  ????  since it is the English word ?ba-
by? it should not be corrected 
 
Flagged tokens in the correction task 
Tokens flagged during task 1 as Sound and 
Foreign were transliterated into Arabic script but 
were not corrected during task 2.  Note that even 
when a whole phrase or sentence appeared in 
English, the transliteration was not corrected. 
? ks  ??  for ?kiss? 
? Dd yA hAf fAn  ??? ???  ??  ??  for ?did you 
have fun? 
The transliteration of proper names was cor-
rected in the same way as all other words. 
Emoticons and emoji were replaced in the 
transliteration with #.  Emoticons refer to a set of 
numbers or letters or punctuation marks used to 
express feelings or mood.  Emoji refers to a spe-
cial set of images used in messages.  Both Emot-
icons and Emoji are frequent in SMS/Chat data. 
5 Discussion 
Annotation and transliteration were performed 
on all sentence units that contain Arabizi.  Sen-
tence units that contain only Arabic script were 
ignored and untouched during annotation.  In 
total, we reviewed 1270 conversations, among 
which over 42.6K sentence units (more than 
300K words) were deemed to be containing 
Arabizi and hence annotated and transliterated. 
The corpus files are in xml format.  All con-
versations have six layers: source, annotation on 
the source Arabizi tokens, automatic translitera-
tion via 3ARRIB, manual correction of the au-
tomatic transliteration, re-tokenized corrected 
transliteration, and human translation.  See Ap-
pendix A for examples of the file format. 
Each conversation was annotated by one anno-
tator, with 10 percent of the data being reviewed 
by a second annotator as a QC procedure.  Twen-
ty six conversations (roughly 3400 words) were 
also annotated dually by blind assignment to 
gauge inter-annotator agreement. 
As we noted earlier, code switching is fre-
quent in the SMS and Chat Arabizi data.  There 
were about 23K words flagged as foreign words.  
Written out speech effects in this type of data are 
also prevalent, and 6610 tokens were flagged as 
Sounds (laughter, filled pause, etc.).  Annotators 
most often agreed with each other in the detec-
tion and flagging of tokens as Foreign, Name, 
Sound or Punctuation, with over 98% agreement 
for all flags. 
The transliteration annotation was more diffi-
cult than the flagging annotation, because apply-
ing CODA requires linguistic knowledge of Ara-
bic.  Annotators went through several rounds of 
training and practice and only those who passed 
a test were allowed to work on the task.  In an 
analysis of inter-annotator agreement in the dual-
ly annotated files, the overall agreement between 
the two annotators was 86.4%.  We analyzed all 
the disagreements and classified them in four 
high level categories: 
? CODA  60% of the disagreements were related 
to CODA decisions that did not carefully follow 
the guidelines.  Two-fifths of these cases were 
related to Alif/Ya spelling (mostly Alif Hamza-
tion, rules of hamza support) and about one-fifth 
involved the spelling of common dialectal words.  
An additional one-third were due to non-CODA 
root, pattern or affix spelling.  Only one-tenth of 
the cases were because of split or merge deci-
sions.  These issues suggest that additional train-
ing may be needed.  Additionally, since some of 
100
the CODA errors may be easy to detect and cor-
rect using available tools for morphological 
analysis of Egyptian Arabic (such as the CALI-
MA-ARZ analyzer), we will consider integrating 
such support in the annotation interface in the 
future.  
? Task  In 23% of the overall disagreements, the 
annotators did not follow the task guidelines for 
handling punctuation, sounds, emoticons, names 
or foreign words.  Examples include disagree-
ment on whether a question mark should be split 
or kept attached, or whether a non-Arabic word 
should be corrected or not.  Many of these cases 
can also be caught as part of the interface; we 
will consider the necessary extensions in the fu-
ture. 
? Ambiguity  In 12% of the cases, the annota-
tors? disagreement reflected a different reading 
of the Arabizi resulting in a different lemma or 
inflectional feature.  These differences are una-
voidable and reflect the natural ambiguity in the 
task. 
? Typos  Finally, in less than 5% of the cases, 
the disagreement was a result of a typographical 
error unrelated to any of the above issues.  
Among the cases that were easy to adjudicate, 
one of the two annotators was correct 60% more 
than the other.  This is consistent with the obser-
vation that more training may be needed to fill in 
some of the knowledge gaps or increase the an-
notator?s attention to detail. 
6 Conclusion 
This is the first Arabizi-Arabic script parallel 
corpus that supports research on transliteration 
from Arabizi to Arabic script.  We expect to 
make this corpus available through the Linguistic 
Data Consortium in the near future. 
This work focuses on the novel challenges of 
developing a corpus like this, and points out the 
close interaction between the orthographic form 
of written informal genres of Arabic and the spe-
cific features of individual Arabic dialects.  The 
use of Arabizi and the use of Egyptian Arabic in 
this corpus come together to present a host of 
spelling ambiguities and multiplied forms that 
were resolved in this corpus by the use of CODA 
for Egyptian Arabic.  Developing a similar cor-
pus and transliteration for other Arabic dialects 
would be a rich area for future work. 
We believe this corpus will be essential for 
NLP work on Arabic dialects and informal gen-
res.  In fact, this corpus has recently been used in 
development by Eskander et al. (2014). 
Acknowledgements 
This material is based upon work supported by 
the Defense Advanced Research Projects Agency 
(DARPA) under Contract No. HR0011-11-C-
0145. The content does not necessarily reflect the 
position or the policy of the Government, and no 
official endorsement should be inferred. 
Nizar Habash performed most of his contribu-
tion to this paper while he was at the Center for 
Computational Learning Systems at Columbia 
University. 
References 
Mohamed Al-Badrashiny, Ramy Eskander, Nizar Ha-
bash, and Owen Rambow. 2014. Automatic Trans-
literation of Romanized Dialectal Arabic. In Pro-
ceedings of the Conference on Computational Nat-
ural Language Learning (CONLL), Baltimore, 
Maryland, 2014. 
Tim Buckwalter. 2004. Buckwalter Arabic Morpho-
logical Analyzer Version 2.0. LDC catalog number 
LDC2004L02, ISBN 1-58563-324-0.  
Achraf Chalabi and Hany Gerges. 2012. Romanized 
Arabic Transliteration. In Proceedings of the Sec- 
ond Workshop on Advances in Text Input Methods 
(WTIM 2012).  
Eleanor Clark and Kenji Araki. 2011. Text normaliza-
tion in social media: Progress, problems and ap- 
plications for a pre-processing system of casual 
English. Procedia - Social and Behavioral Scienc-
es, 27(0):2 ? 11. 
Kareem Darwish, Walid Magdy, and Ahmed Mourad. 
2012. Language processing for arabic microblog 
re- trieval. In Proceedings of the 21st ACM Inter-
national Conference on Information and 
Knowledge Management, CIKM ?12, pages 2427?
2430, New York, NY, USA. ACM.  
Kareem Darwish. 2013. Arabizi Detection and Con- 
version to Arabic. CoRR, arXiv:1306.6755 [cs.CL]. 
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina 
Bontcheva. 2013. Twitter part-of-speech tagging 
for all: Overcoming sparse and noisy data. In Pro-
ceedings of the International Conference Recent 
Advances in Natural Language Processing RANLP 
2013, pages 198?206, Hissar, Bulgaria, September. 
INCOMA Ltd. Shoumen, Bulgaria.  
Heba Elfardy, Mohamed Al-Badrashiny, and Mona 
Diab. 2013. Code Switch Point Detection in Ara-
bic. In Proceedings of the 18th International Con-
ference on Application of Natural Language to In-
formation Systems (NLDB2013), MediaCity, UK, 
June.  
Ramy Eskander, Mohamed Al-Badrashiny, Nizar Ha-
bash and Owen Rambow. 2014. Foreign Words 
101
and the Automatic Processing of Arabic Social 
Media Text Written in Roman Script. In Arabic 
Natural Language Processing Workshop, EMNLP, 
Doha, Qatar. 
Ramy Eskander, Nizar Habash, Owen Rambow, and 
Nadi Tomeh. 2013. Processing Spontaneous Or- 
thography. In Proceedings of the 2013 Conference 
of the North American Chapter of the Association 
for Computational Linguistics: Human Language 
Technologies (NAACL-HLT), Atlanta, GA.  
Andrew T. Freeman, Sherri L. Condon and Christo-
pher M. Ackerman. 2006. Cross Linguistic Name 
Matching in English and Arabic: A ?One to Many 
Mapping? Extension of the Levenshtein Edit Dis-
tance Algorithm. In Proceedings of HLT-NAACL, 
New York, NY. 
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, 
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Mi-
chael Heilman, Dani Yogatama, Jeffrey Flanigan, 
and Noah A. Smith. 2011. Part-of-speech tagging 
for twitter: Annotation, features, and experiments. 
In Proceedings of ACL-HLT ?11.  
Stephan Gouws, Donald Metzler, Congxing Cai, and 
Eduard Hovy. 2011. Contextual bearing on linguis-
tic variation in social media. In Proceedings of the 
Workshop on Languages in Social Media, LSM 
?11, pages 20?29, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics. 
Nizar Habash, Mona Diab, and Owen Rambow 
(2012a).Conventional Orthography for Dialectal 
Arabic: Principles and Guidelines ? Egyptian Ara-
bic. Technical Report CCLS-12-02, Columbia 
University Center for Computational Learning Sys-
tems.  
Nizar Habash, Mona Diab, and Owen Rabmow. 
2012b. Conventional Orthography for Dialectal 
Arabic. In Proceedings of the Language Resources 
and Evaluation Conference (LREC), Istanbul.  
Nizar Habash, Ramy Eskander, and Abdelati Haw-
wari. 2012c. A Morphological Analyzer for Egyp-
tian Arabic. In Proceedings of the Twelfth Meeting 
of the Special Interest Group on Computational 
Morphology and Phonology, pages 1?9, Montr?al, 
Canada.  
Kevin Knight and Jonathan Graehl. 1997. Machine 
Transliteration. In Proceedings of the Conference 
of the Association for Computational Linguistics 
(ACL). 
Linguistic Data Consortium. 2014. BOLT Program: 
Romanized Arabic (Arabizi) to Arabic Translitera-
tion and Normalization Guidelines, Version 3.1. 
Linguistic Data Consortium, April 21, 2014.  
Marco Lui, Jey Han Lau, and Timothy Baldwin. 
2014. Automatic detection and language identifica-
tion of multilingual documents. In Proceedings of 
the Language Resources and Evaluation Confer-
ence (LREC), Reykjavik, Iceland.  
Mohamed Maamouri, Ann Bies, Seth Kulick, Michael 
Ciul, Nizar Habash and Ramy Eskander. 2014. De-
veloping a dialectal Egyptian Arabic Treebank: 
Impact of Morphology and Syntax on Annotation 
and Tool Development. In Proceedings of the Lan-
guage Resources and Evaluation Conference 
(LREC), Reykjavik, Iceland. 
Yaser Al-Onaizan and Kevin Knight. 2002. Machine 
Transliteration of Names in Arabic Text. In Pro-
ceedings of ACL Workshop on Computational Ap-
proaches to Semitic Languages. 
Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab, 
Ahmed El Kholy, Ramy Eskander, Nizar Habash, 
Manoj Pooleery, Owen Rambow, and Ryan M. 
Roth. 2014. MADAMIRA: A Fast, Comprehensive 
Tool for Morphological Analysis and Disambigua-
tion of Arabic. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC), Rey-
kjavik, Iceland.  
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the Conference 
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?11. 
Wael Salloum and Nizar Habash. 2011. Dialectal to 
Standard Arabic Paraphrasing to Improve Arabic- 
English Statistical Machine Translation. In Pro- 
ceedings of the First Workshop on Algorithms and 
Resources for Modelling of Dialects and Language 
Varieties, pages 10?21, Edinburgh, Scotland.  
Zhiyi Song, Stephanie Strassel, Haejoong Lee, Kevin 
Walker, Jonathan Wright, Jennifer Garland, Dana 
Fore, Brian Gainor, Preston Cabe, Thomas Thom-
as, Brendan Callahan, Ann Sawyer. Collecting 
Natural SMS and Chat Conversations in Multiple 
Languages: The BOLT Phase 2 Corpus. In Pro-
ceedings of the Language Resources and Evalua-
tion Conference (LREC) 2014, Reykjavik, Iceland. 
Clare Voss, Stephen Tratz, Jamal Laoudi, and Dou- 
glas Briesch. 2014. Finding romanized Arabic dia-
lect in code-mixed tweets. In Proceedings of the 
Ninth International Conference on Language Re-
sources and Evaluation (LREC?14), Reykjavik, 
Iceland. 
Omar F Zaidan and Chris Callison-Burch. 2011. The 
arabic online commentary dataset: an annotated da-
taset of informal arabic with high dialectal content. 
In Proceedings of ACL, pages 37?41. 
102
Appendix A: File Format Examples 
 
 
 
Example 1: 
 
<su id="s1582"> 
  <source>marwan ? ana walahi knt gaya today :/</source> 
   <annotated_arabizi> 
        <token id="t0" tag="name">marwan</token> 
       <token id="t1" tag="punctuation">?</token> 
       <token id="t2">ana</token> 
      <token id="t3">walahi</token> 
   <token id="t4">knt</token> 
        <token id="t5">gaya</token> 
       <token id="t6" tag="foreign">today</token> 
        <token id="t7">:/</token> 
     </annotated_arabizi> 
    <auto_transliteration> :/ ???? ???? ??? ?? ???  ?????? </auto_transliteration> 
<corrected_transliteration> # ????  ???? ??? ?? ???  ?????? </corrected_transliteration> 
<retokenized_transliteration> # ???? ???? ??? ?? ???  ?????? </retokenized_transliteration> 
     <translation lang="eng">Marwan? I swear I was coming today :/</translation> 
     <messages> 
<message id="m2377" time="2013-10-01 22:03:34 UTC" participant="139360">marwan ? ana 
walahi knt gaya today :/</message> 
     </messages> 
  </su> 
 
Example 2: 
 
<su id="s3"> 
<source>W sha3rak ma2sersh:D haha</source> 
<annotated_arabizi> 
<token id="t0">W</token> 
<token id="t1">sha3rak</token> 
<token id="t2">ma2sersh:D</token> 
<token id="t3" tag="sound">haha</token> 
</annotated_arabizi> 
<auto_transliteration> ?? # [-]????? ???? [+]? </auto_transliteration> 
<corrected_transliteration> ?? #[-]????[-]?? ???? [+]? </corrected_transliteration> 
<retokenized_transliteration> ?? # ???? ?? ????? </retokenized_transliteration> 
<translation lang="eng">And your hair did not become short? :D Haha</translation> 
<messages> 
<message id="m0004" medium="IM" time="2012-12-22 15:36:31 UTC" participant="138112">W 
sha3rak ma2sersh:D haha</message> 
</messages> 
</su> 
 
 
 
103
