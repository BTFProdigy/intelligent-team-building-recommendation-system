Extending MT evaluation tools with translation complexity metrics 
Bogdan BABYCH 
Centre for Translation 
Studies, University of Leeds  
Leeds, UK, LS2 9JT 
bogdan@comp.leeds.ac.uk 
Debbie ELLIOTT 
School of Computing 
University of Leeds  
Leeds, UK, LS2 9JT 
debe@comp.leeds.ac.uk 
Anthony HARTLEY 
Centre for Translation 
Studies, University of Leeds 
Leeds, UK, LS2 9JT 
a.hartley@leeds.ac.uk 
 
Abstract 
In this paper we report on the results of an 
experiment in designing resource-light metrics that 
predict the potential translation complexity of a 
text or a corpus of homogenous texts for state-of-
the-art MT systems. We show that the best 
prediction of translation complexity is given by the 
average number of syllables per word (ASW). The 
translation complexity metrics based on this 
parameter are used to normalise automated MT 
evaluation scores such as BLEU, which otherwise 
are variable across texts of different types. The 
suggested approach makes a fairer comparison 
between the MT systems evaluated on different 
corpora. The translation complexity metric was 
integrated into two automated MT evaluation 
packages ? BLEU and the Weighted N-gram 
model. The extended MT evaluation tools are 
available from the first author?s web site: 
http://www.comp.leeds.ac.uk/bogdan/evalMT.html 
1 Introduction 
Automated evaluation tools for MT systems aim 
at producing scores that are consistent with the 
results of human assessment of translation quality 
parameters, such as adequacy and fluency. 
Automated metrics such as BLEU (Papineni et al, 
2002), RED (Akiba et al 2001), Weighted N-gram 
model (WNM) (Babych, 2004), syntactic relation / 
semantic vector model (Rajman and Hartley, 2001) 
have been shown to correlate closely with scoring 
or ranking by different human evaluation 
parameters. Automated evaluation is much quicker 
and cheaper than human evaluation. 
Another advantage of the scores produced by 
automated MT evaluation tools is that intuitive 
human scores depend on the exact formulation of 
an evaluation task, on the granularity of the 
measuring scale and on the relative quality of the 
presented translation variants: human judges may 
adjust their evaluation scale in order to 
discriminate between slightly better and slightly 
worse variants ? but only those variants which are 
present in the evaluation set. For example, absolute 
figures for a human evaluation of a set which 
includes MT output only are not directly 
comparable with the figures for another evaluation 
which might include MT plus a non-native human 
translation, or several human translations of 
different quality. Because of the instability of this 
intuitive scale, human evaluation figures should be 
treated as relative rather than absolute. They 
capture only a local picture within an evaluated set, 
but not the quality of the presented texts in a larger 
context. Although automated evaluation scores are 
always calibrated with respect to human evaluation 
results, only the relative performance of MT 
systems within one particular evaluation exercise 
provide meaningful information for such 
calibration. 
In this respect, automated MT evaluation scores 
have some added value: they rely on objective 
parameters in the evaluated texts, so their results 
are comparable across different evaluations. 
Furthermore, they are also comparable for 
different types of texts translated by the same MT 
system, which is not the case for human scores. 
For example, automated scores are capable of 
distinguishing improved MT performance on 
easier texts or degraded performance on harder 
texts, so the automated scores also give 
information on whether one collection of texts is 
easier or harder than the other for an MT system: 
the complexity of the evaluation task is directly 
reflected in the evaluation scores. 
However, there may be a need to avoid such 
sensitivity. MT developers and users are often 
more interested in scores that would be stable 
across different types of texts for the same MT 
system, i.e., would reliably characterise a system?s 
performance irrespective of the material used for 
evaluation. Such characterisation is especially 
important for state-of-the-art commercial MT 
systems, which typically target a wide range of 
general-purpose text types and are not specifically 
tuned to any particular genre, like weather reports 
or aircraft maintenance manuals. 
The typical problem of having ?task-dependent? 
evaluation scores (which change according to the 
complexity of the evaluated texts) is that the 
reported scores for different MT systems are not 
directly comparable. Since there is no standard 
collection of texts used for benchmarking all MT 
systems, it is not clear how a system that achieves, 
e.g., BLEUr4n4 1  score 0.556 tested on ?490 
utterances selected from the WSJ? (Cmejrek et al 
2003:89) may be compared to another system 
which achieves, e.g., the BLEUr1n4 score 0.240 
tested on 10,150 sentences from the ?Basic Travel 
Expression Corpus? (Imamura et al, 2003:161). 
Moreover, even if there is no comparison 
involved, there is a great degree of uncertainty in 
how to interpret the reported automated scores. For 
example, BLEUr2n4 0.3668 is the highest score 
for a top MT system if MT performance is 
measured on news reports, but it is a relatively 
poor score for a corpus of e-mails, and a score that 
is still beyond the state-of-the-art for a corpus of 
legal documents. These levels of perfection have to 
be established experimentally for each type of text, 
and there is no way of knowing whether some 
reported automated score is better or worse if a 
new type of text is involved in the evaluation. 
The need to use stable evaluation scores, 
normalised by the complexity of the evaluated 
task, has been recognised in other NLP areas, such 
as anaphora resolution, where the results may be 
relative with regard to a specific evaluation set. So 
?more absolute? figures are obtained if we use 
some measure which quantifies the complexity of 
anaphors to be resolved (Mitkov, 2002). 
MT evaluation is harder than evaluation of other 
NLP tasks, which makes it partially dependent on 
intuitive human judgements about text quality. 
However, automated tools are capable of capturing 
and representing the ?absolute? level of 
performance for MT systems, and this level could 
then be projected into task-dependent figures for 
harder or easier texts. In this respect, there is 
another ?added value? in using automated scores 
for MT evaluation. 
Stable evaluation scores could be achieved if a 
formal measure of a text?s complexity for 
translation could be cheaply computed for a source 
text. Firstly, the score for translation complexity 
allows the user to predict ?absolute? performance 
figures of an MT system on harder or easier texts, 
by computing the ?absolute? evaluation figures and 
the complexity scores for just one type of text. 
Secondly, it lets the user compute ?standardised? 
performance figures for an MT system that do not 
depend on the complexity of a text (they are 
reliably within some relatively small range for any 
type of evaluated texts). 
Designing such standardised evaluation scores 
requires choosing a point of reference for the 
complexity measure: e.g., one may choose an 
                                                   
1
 BLEUrXnY means the BLEU score with produced 
with X reference translations and the maximum size of 
compared N-grams = Y. 
average complexity of texts usually translated by 
MT as the reference point. Then the absolute 
scores for harder or easier texts will be corrected to 
fit the region of absolute scores for texts of average 
complexity. 
In this paper we report on the results of an 
experiment in measuring the complexity of 
translation tasks using resource-light parameters 
such as the average number of syllables per word 
(ASW), which is also used for computing the 
readability of a text. On the basis of these 
parameters we compute normalised BLEU and 
WNM scores which are relatively stable across 
translations produced by the same general-purpose 
MT systems for texts of varying difficulty. We 
suggest that further testing and fine-tuning of the 
proposed approach on larger corpora of different 
text types and using additional source text 
parameters and normalisation techniques can give 
better prediction of translation complexity and 
increase the stability of the normalised MT 
evaluation scores. 
2 Set-up of the experiment 
We compared the results of the human and 
automated evaluation of translations from French 
into English of three different types of texts which 
vary in size and style: an EU whitepaper on child 
and youth policy (120 sentences), a collection of 
36 business and private e-mails and 100 news texts 
from the DARPA 94 MT evaluation corpus (White 
et al, 1994). The translations were produced by 
two leading commercial MT systems. Human 
evaluation results are available for all of the texts, 
with the exception of the news reports translated 
by System-2, which was not part of the DARPA 94 
evaluation. However, the human evaluation scores 
were collected at different times under different 
experimental conditions using different 
formulations of the evaluation tasks, which leads to 
substantial differences between human scores 
across different evaluations, even if the evaluations 
were done at the same time.  
Further, we produced two sets of automated 
scores: BLEUr1n4, which have a high correlation 
with human scores for fluency, and WNM Recall, 
which strongly correlate with human scores for 
adequacy. These scores were produced under the 
same experimental conditions, but they uniformly 
differ for both evaluated systems: BLEU and 
WNM scores were relatively higher for e-mails 
and relatively low for the whitepaper, with the 
news texts coming in between. We interpreted 
these differences as reflecting the relative 
complexity of texts for translation. 
For the French originals of all three sets of texts 
we computed resource-light parameters used in 
standard readability measures (Flesch Reading 
Ease score or Flesch-Kincaid Grade Level score), 
i.e. average sentence length (ASL ? the number of 
words divided by the number of sentences) and 
average number of syllables per word (ASW ? the 
number of syllables divided by the number of 
words). 
We computed Pearson?s correlation coefficient r 
between the automated MT evaluation scores and 
each of the two readability parameters. Differences 
in the ASL parameter were not strongly linked to 
the differences in automated scores, but for the 
ASW parameter a strong negative correlation was 
found. 
Finally, we computed normalised (?absolute?) 
BLEU and WNM scores using the automated 
evaluation results for the DARPA news texts (the 
medium complexity texts) as a reference point. We 
compared the stability of these scores with the 
stability of the standard automated scores by 
computing standard deviations for the different 
types of text. The absolute automated scores can be 
computed on any type of text and they will indicate 
what score is achievable if the same MT system 
runs on DARPA news reports. The normalised 
scores allow the user to make comparisons 
between different MT systems evaluated on 
different texts at different times. In most cases the 
accuracy of the comparison is currently limited to 
the first rounded decimal point of the automated 
score. 
3 Results of human evaluations  
The human evaluation results were produced 
under different experimental conditions. The 
output of the compared systems was evaluated 
each time within a different evaluation set, in some 
cases together with different MT systems, or native 
or non-native human translations. As a result 
human evaluation scores are not comparable across 
different evaluations. 
Human scores available from the DARPA 94 
MT corpus of news reports were the result of a 
comparison of five MT systems (one of which was 
a statistical MT system) and a professional 
(?expert?) human translation. For our experiment 
we used DARPA scores for adequacy and fluency 
for one of the participating systems. 
We obtained human scores for translations of the 
whitepaper and the e-mails from one of our MT 
evaluation projects at the University of Leeds. This 
had involved the evaluation of French-to-English 
versions of two leading commercial MT systems ? 
System 1 and System 2 ? in order to assess the 
quality of their output and to determine whether 
updating the system dictionaries brought about an 
improvement in performance. (An earlier version 
of System 1 also participated in the DARPA 
evaluation.) Although the human evaluations of 
both texts were carried out at the same time, the 
experimental set-up was different in each case. 
The evaluation of the whitepaper for adequacy 
was performed by 20 postgraduate students who 
knew very little or no French. A professional 
human translation of each segment was available 
to the judges as a gold standard reference. Using a 
five-point scale in each case, judgments were 
solicited on adequacy by means of the following 
question: 
?For each segment, read carefully the reference 
text on the left. Then judge how much of the 
same content you can find in the candidate text.? 
Five independent judgments were collected for 
each segment. 
The whitepaper fluency evaluation was 
performed by 8 postgraduate students and 16 
business users under similar experimental 
conditions with the exception that the gold 
standard reference text was not available to the 
judges. The following question was asked: 
?Look carefully at each segment of text and give 
each one a score according to how much you 
think the text reads like fluent English written by 
a native speaker.? 
For e-mails a different quality evaluation 
parameter was used: 26 human judges (business 
users) evaluated the usability (or utility) of the 
translations. We also included translations 
produced by a non-professional, French-speaking 
translator in the evaluation set for e-mails. (This 
was intended to simulate a situation where, in the 
absence of MT, the author of the e-mail would 
have to write in a foreign language (here English); 
we anticipated that the quality would be judged 
lower than the professional, native speaker 
translations.) The non-native translations were 
dispersed anonymously in the data set and so were 
also judged. The following question was asked: 
?Using each reference e-mail on the left, rate the 
three alternative versions on the right according 
to how usable you consider them to be for 
getting business done.? 
Figure 1 and Table 1 summarise the human 
evaluation scores for the two compared MT 
systems. The judges had scored versions of the e-
mails (?em?) and whitepaper (?wp?) produced both 
before and after dictionary update (?DA?), 
although no judge saw the before and after variants 
of the same text. (The scores for the DARPA news 
texts are converted from [0, 1] to [0, 5] scale). 
00.5
1
1.5
2
2.5
3
3.5
4
4.5
5
em-USL wp-FLU wp-ADE news-FLU news-ADE
System-1 Before DA
System-1 After DA
System-2 Before DA
System-2 After DA
Non-native transl.
 
Figure 1. Human evaluation results 
 
 S1 S1da S2 S2da NN 
em [usl] 2.511 3.139 2.35 2.733 4.314 
wp [flu] 3.15 3.47 2.838 3.157  
wp [ade] 3.94 4.077 3.858 3.977  
news [flu] 2.54     
news [ade] 3.945     
Table 1. Human evaluation scores 
It can be inferred from the data that human 
evaluation scores do not allow us to make any 
meaningful comparison of the scores outside a 
particular evaluation experiment, which 
necessarily must be interpreted as relative rather 
than absolute. 
We can see that dictionary update consistently 
improves the performance of both systems, that 
System 1 is slightly better than System 2 in all 
cases, although after dictionary update System 2 is 
capable of reaching the baseline quality of System 
1. However, the usability scores for supposedly 
easier texts (e-mails) are considerably lower than 
the adequacy scores for harder texts (the 
whitepaper), although the experimental set-up for 
adequacy and usability is very similar: both used a 
gold-standard human reference translation. We 
suggest that the presence of a higher quality 
translation done by a human non-native speaker of 
the target language ?over-shadowed? lower quality 
MT output, which dragged down evaluation scores 
for e-mail usability. No such higher quality 
translation was present in the evaluation set for the 
whitepaper adequacy, so the scores went up. 
Therefore, no meaning can be given to any 
absolute value of the evaluation scores across 
different experiments involving intuitive human 
judgements. Only a relative comparison of these 
evaluation scores produced within the same 
experiment is possible. 
4 Results of automated evaluations 
Automated evaluation scores use objective 
parameters, such the number of N-gram matches in 
the evaluated text and in a gold standard reference 
translation. Therefore, these scores are more 
consistent and comparable across different 
evaluation experiments. The comparison of the 
scores indicates the relative complexity of the texts 
for translation. For the output of both MT systems 
under consideration we generated two sets of 
automated evaluation scores: BLEUr1n4 and 
WNM Recall. 
BLEU computes the modified precision of N-
gram matches between the evaluated text and a 
professional human reference translation. It was 
found to produce automated scores, which strongly 
correlate with human judgements about translation 
fluency (Papineni et al, 2002). 
WNM is an extension of BLEU with weights of 
a term?s salience within a given text. As compared 
to BLEU, the WNM recall-based evaluation score 
was found to produce a higher correlation with 
human judgements about adequacy (Babych, 
2004). The salience weights are similar to standard 
tf.idf scores and are computed as follows: ( )
)(
)()(),( /)(log),(
icorp
iidoccorpjidoc
P
NdfNPPjiS ???= ? ,  
where: 
? Pdoc(i,j) is the relative frequency of the word wi in 
the text j; (?Relative frequency? is the number 
of tokens of this word-type divided by the total 
number of tokens). 
? Pcorp-doc(i) is the relative frequency of the same 
word wi in the rest of the corpus, without this 
text; 
? dfi is the number of documents in the corpus 
where the word wi occurs; 
? N is the total number of documents in the corpus. 
? Pcorp(i) is the relative frequency of the word wi in 
the whole corpus, including this particular 
text.  
Figures 2 and 3 and Table 2 summarise the 
automated evaluation scores for the two MT 
systems. 
00.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
bleu-wp bleu-news bleu-em
S1
S1da
S2
S2da
 
Figure 2. Automated BLEUr1n4 scores 
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
wnmR-wp wnmR-news wnmR-em
S1
S1da
S2
S2da
 
Figure 3. Automated WMN Recall scores 
scores S1 S1da S2 S2da 
bleu-wp 0.1874 0.2351 0.1315 0.1701 
bleu-news 0.2831  0.1896  
bleu-em 0.3257 0.3573 0.2006 0.326 
wnmR-wp 0.3247 0.3851 0.2758 0.3172 
wnmR-news 0.3644  0.3439  
wnmR-em 0.3915 0.4256 0.3792 0.4129 
r correlation [flu] [ade/usl]   
bleu-wp 0.9827 0.9453   
bleu-em  0.7872   
wnmR-wp 0.9896 0.9705   
wnmR-em  0.9673   
Table 2. Automated evaluation scores 
It can be seen from the charts that automated 
scores consistently change according to the type of 
the evaluated text: for both evaluated systems 
BLEU and WNM are the lowest for the whitepaper 
texts, which emerge as most complex to translate, 
the news reports are in the middle and the highest 
scores are given to the e-mails, which appear to be 
relatively easy. A similar tendency also holds for 
the system after dictionary update. However, 
technically speaking the compared systems are no 
longer the same, because the dictionary update was 
done individually for each system, so the quality of 
the update is an additional factor in the system?s 
performance ? in addition to the complexity of the 
translated texts. 
The complexity of the translation task is 
integrated into the automated MT evaluation 
scores, but for the same type of texts the scores are 
perfectly comparable. For example, for the 
DARPA news texts, newly generated BLEU and 
WNM scores confirm the observation made, on the 
basis of comparison of the whitepaper and the e-
mail texts, that S1 produces higher translation 
quality than S2, although there is no human 
evaluation experiment where such translations are 
directly compared. 
Thus the automated MT evaluation scores derive 
from both the ?absolute? output quality of an 
evaluated general-purpose MT system and the 
complexity of the translated text. 
5 Readability parameters 
In order to isolate the ?absolute? MT quality and 
to filter out the contribution of the complexity of 
the evaluated text from automated scores, we need 
to find a formal parameter of translation 
complexity which should preferably be resource-
light, so as to be easily computed for any source 
text in any language submitted to an MT system. 
Since automated scores already integrate the 
translation complexity of the evaluated text, we 
can validate such a parameter by its correlation 
with automated MT evaluation scores computed on 
the same set that includes different text types. 
In our experiment, we examined the following 
resource-light parameters for their correlation with 
both automated scores: 
? Flesch Reading Ease score, which rates text on 
a 100-point scale according to how easy it is to 
understand; the score is computed as follows: 
FR = 206.835 ? (1.015 * ASL) ? (84.6 * 
ASW), where: 
ASL is the average sentence length (the 
number of words divided by the number of 
sentences); 
ASW is the average number of syllables per 
word (the number of syllables divided by the 
number of words) 
? Flesch-Kincaid Grade Level score which rates 
texts on US grade-school level and is 
computed as: 
FKGL = (0.39 * ASL) + (11.8 * ASW) ? 
15.59 
? each of the ASL and ASW parameters 
individually. 
Table 3 presents the averaged readability 
parameters for all French original texts used in our 
evaluation experiment and the r correlation 
between these parameters and the corresponding 
automated MT evaluation scores. 
 FR FKGL ASL ASW 
wp 17.3 15.7 19.65 2 
news 27.8 14.7 21.4 1.86 
em 61.44   6.98   9.22 1.608 
r/bleu-S1 0.872 -0.804 -0.641 -0.928 
r/bleu-S2 0.785 -0.701 -0.513 -0.859 
r/wnm-S1 0.92 -0.864 -0.721 -0.963 
r/wnm-S2 0.889 -0.825 -0.669 -0.941 
r Average 0.866 -0.799 -0.636 -0.923 
Table 3. Readability of French originals 
Table 3 shows that the strongest negative 
correlation exists between ASW (average number 
of syllables per word) and the automated 
evaluation scores. Therefore the ASW parameter 
can be used to normalise MT evaluation scores. 
Therefore translation complexity is highly 
dependent on the complexity of the lexicon, which 
is approximated by the ASW parameter. 
The other parameter used to compute readability 
? ASL (average sentence length in words) ? has a 
much weaker influence on the quality of MT, 
which may be due to the fact that local context is 
in many cases sufficient to produce accurate 
translation and the use of the global sentence 
structure in MT analysis is limited. 
6 Normalised evaluation scores 
We used the ASW parameter to normalise the 
automated evaluation scores in order to obtain 
absolute figures for MT performance, where the 
influence of translation complexity is neutralised. 
Normalisation requires choosing some reference 
point ? some average level of translation 
complexity ? to which all other scores for the same 
MT system will be scaled. We suggest using the 
difficulty of the news texts in the DARPA 94 MT 
evaluation corpus as one such ?absolute? reference 
point. Normalised figures obtained on other types 
of texts will mean that if the same general-purpose 
MT system is run on the DARPA news texts, it 
will produce raw BLEU or WNM scores 
approximately equal to the normalised scores. This 
allows users to make a fairer comparison between 
MT systems evaluated on different types of texts. 
We found that for the WNM scores the best 
normalisation can be achieved by multiplying the 
score by the complexity normalisation coefficient 
C, which is the ratio: 
C = ASWevalText/ ASWDARPAnews. 
For BLEU the best normalisation is achieved by 
multiplying the score by C2 (the squared value of 
ASWevalText/ ASWDARPAnews). 
Normalisation makes the evaluation relatively 
stable ? in general, the scores for the same system 
are the same up to the first rounded decimal point. 
Table 4 summarises the normalised automated 
scores for the evaluated systems. 
 
 
C S1 S1da S2 S2da 
bleu-wp 1.156 0.217 0.272 0.152 0.197 
bleu-news 1 0.283  0.19  
bleu-em 0.747 0.243 0.267 0.15 0.244 
wnmR-wp 1.075 0.349 0.414 0.297 0.341 
wnmR-news 1 0.364  0.344  
wnmR-em 0.865 0.338 0.368 0.328 0.357 
Table 4. Normalised BLEU and WNM scores 
The accuracy of the normalisation can be 
measured by standard deviations of the normalised 
scores across texts of different types. We also 
measured the improvement in stability of the 
normalised scores as compared to the stability of 
the raw scores generated on different text types. 
Standard deviation was computed using the 
formula: 
)1(
)( 22
?
?
=
   
nn
xxn
STDEV  
Table 5 summarises standard deviations of the 
raw and normalised automated scores for the e-
mails, whitepaper and news texts. 
 
 
S1 S1da S2 S2da Ave-
rage 
bleu-stdev 0.071 0.086 0.037 0.11 0.076 
N-bleu-stdev 0.033 0.003 0.022 0.033 0.023 
improved *X     3.299 
wnm-stdev 0.034 0.029 0.053 0.068 0.046 
N-wnm-stdev 0.013 0.033 0.024 0.011 0.02 
improved *X     2.253 
Table 5. Standard deviation of BLEU and WNM 
It can be seen from the table that the standard 
deviation of the normalised BLEU scores across 
different text types is 3.3 times smaller; and the 
deviation of the normalised WNM scores is 2.25 
times smaller than for the corresponding raw 
scores. So the normalised scores are much more 
stable than the raw scores across different 
evaluated text types. 
7 Conclusion and future work 
In this paper, we presented empirical evidence 
for the observation that the complexity of an MT 
task influences automated evaluation scores. We 
proposed a method for normalising the automated 
scores by using a resource-light parameter of the 
average number of syllables per word (ASW), 
which relatively accurately approximates the 
complexity of the particular text type for 
translation. 
The fact that the potential complexity of a 
particular text type for translation can be 
accurately approximated by the ASW parameter 
can have an interesting linguistic interpretation. 
The relation between the length of the word and 
the number of its meanings in a dictionary is 
governed by the Menzerath?s law (Koehler, 1993: 
49), which in its most general formulation states 
that there is a negative correlation between the 
length of a language construct and the size of its 
?components? (Menzerath, 1954; Hubey, 1999: 
239). In this particular case the size of a word?s 
components can be interpreted as the number of its 
possible word senses. We suggest that the link 
between ASW and translation difficulty can be 
explained by the fact that the presence of longer 
words with a smaller number of senses requires a 
more precise word sense disambiguation for 
shorter polysemantic words, so the task of word 
sense disambiguation becomes more demanding: 
the choice of very specific senses and the use of 
more precise (often terminological translation 
equivalents) is required. 
Future work will involve empirical testing of this 
suggestion as well as further experiments on 
improving the stability of the normalised scores by 
developing better normalisation methods. We will 
evaluate the proposed approach on larger corpora 
containing different genres, and will investigate 
other possible resource-light parameters, such as 
type/token ratio of the source text or unigram 
entropy, which can predict the complexity of the 
translated text more accurately. Another direction 
of future research is comparison of stability of 
evaluation scores on subsets of the evaluated data 
within one particular text type and across different 
text types. 
Acknowledgments 
We are very grateful for the insightful comments 
of the three anonymous reviewers. 
References  
Y. Akiba, K. Imamura and E. Sumita. 2001. Using 
multiple edit distances to automatically rank 
machine translation output. In "Proc. MT 
Summit VIII". pages 15?20. 
B. Babych. 2004. Weighted N-gram model for 
evaluating Machine Translation output. In 
"Proceedings of the 7th Annual Colloquium for 
the UK Special Interest Group for Computational 
Linguistics". M. Lee, ed., University of 
Birmingham, 6-7 January, 2004. pages 15-22. 
M. Cmejrek, J. Curin and J. Havelka. 2003. Czech-
English Dependency-based Machine 
Translation. In ?Proceedings of the 10th 
Conference of the European Chapter of 
Association for Computational Linguistics 
(EACL 2003)?. April 12th-17th 2003, Budapest, 
Hungary. 
K. Imamura, E. Sumita and Y. Matsumoto. 2003. 
Automatic Construction of Machine Translation 
Knowledge Using Translation Literalness. In 
?Proceedings of the 10th Conference of the 
European Chapter of Association for 
Computational Linguistics (EACL 2003)?. April 
12th-17th 2003, Budapest, Hungary. 
M. Hubey. 1999. Mathematical Foundations of 
Linguistics. Lincom Europa, Muenchen. 
R. Koehler. 1993. Synergetic Linguistics. In 
"Contributions to Quantitative Linguistics", 
R. Koehler and B.B. Rieger (eds.), pages 41-51. 
P. Menzerath. 1954. Die Architektonik des 
deutchen Wortschatzes. Dummler, Bonn. 
R. Mitkov. 2002. Anaphora Resolution. Longman, 
Harlow, UK. 
K. Papineni, S. Roukos, T. Ward, W-J Zhu. 2002 
BLEU: a method for automatic evaluation of ma-
chine translation. In "Proceedings of the 40th 
Annual Meeting of the Association for the 
Computational Linguistics (ACL)", Philadelphia, 
July 2002, pages 311-318. 
M. Rajman and T. Hartley. 2001. Automatically 
predicting MT systems ranking compatible with 
Fluency, Adequacy and Informativeness scores. 
In "Proceedings of the 4th ISLE Workshop on 
MT Evaluation, MT Summit VIII". Santiago de 
Compostela, September 2001. pages. 29-34. 
J. White, T. O?Connell and F. O?Mara. 1994. The 
ARPA MT evaluation methodologies: evolution, 
lessons and future approaches. Procs. 1st 
Conference of the Association for Machine 
Translation in the Americas. Columbia, MD, 
October 1994. 193-205. 
 
Extending the BLEU MT Evaluation Method with Frequency Weightings  
Bogdan Babych 
Centre for Translation Studies 
University of Leeds 
Leeds, LS2 9JT, UK 
bogdan@comp.leeds.ac.uk 
Anthony Hartley 
Centre for Translation Studies 
University of Leeds 
Leeds, LS2 9JT, UK 
a.hartley@leeds.ac.uk 
 
Abstract 
We present the results of an experiment 
on extending the automatic method of 
Machine Translation evaluation BLUE 
with statistical weights for lexical items, 
such as tf.idf scores. We show that this 
extension gives additional information 
about evaluated texts; in particular it al-
lows us to measure translation Adequacy, 
which, for statistical MT systems, is often 
overestimated by the baseline BLEU 
method. The proposed model uses a sin-
gle human reference translation, which 
increases the usability of the proposed 
method for practical purposes. The model 
suggests a linguistic interpretation which 
relates frequency weights and human in-
tuition about translation Adequacy and 
Fluency. 
1. Introduction 
Automatic methods for evaluating different as-
pects of MT quality ? such as Adequacy, Fluency 
and Informativeness ? provide an alternative to 
an expensive and time-consuming process of 
human MT evaluation. They are intended to yield 
scores that correlate with human judgments of 
translation quality and enable systems (machine 
or human) to be ranked on this basis. Several 
such automatic methods have been proposed in 
recent years. Some of them use human reference 
translations, e.g., the BLEU method (Papineni et 
al., 2002), which is based on comparison of 
N-gram models in MT output and in a set of hu-
man reference translations. 
However, a serious problem for the BLEU 
method is the lack of a model for relative impor-
tance of matched and mismatched items. Words 
in text usually carry an unequal informational 
load, and as a result are of differing importance 
for translation. It is reasonable to expect that the 
choices of right translation equivalents for certain 
key items, such as expressions denoting principal 
events, event participants and relations in a text 
are more important in the eyes of human evalua-
tors then choices of function words and a syntac-
tic perspective for sentences. Accurate rendering 
of these key items by an MT system boosts the 
quality of translation. Therefore, at least for 
evaluation of translation Adequacy (Fidelity), the 
proper choice of translation equivalents for im-
portant pieces of information should count more 
than the choice of words which are used for 
structural purposes and without a clear translation 
equivalent in the source text. (The latter may be 
more important for Fluency evaluation). 
The problem of different significance of N-
gram matches is related to the issue of legitimate 
variation in human translations, when certain 
words are less stable than others across inde-
pendently produced human translations. BLEU 
accounts for legitimate translation variation by 
using a set of several human reference transla-
tions, which are believed to be representative of 
several equally acceptable ways of translating 
any source segment. This is motivated by the 
need not to penalise deviations from the set of N-
grams in a single reference, although the re-
quirement of multiple human references makes 
automatic evaluation more expensive. 
However, the ?significance? problem is not di-
rectly addressed by the BLEU method. On the 
one hand, the matched items that are present in 
several human references receive the same 
weights as items found in just one of the refer-
ences. On the other hand the model of legitimate 
translation variation cannot fully accommodate 
the issue of varying degrees of ?salience? for 
matched lexical items, since alternative syn-
onymic translation equivalents may also be 
highly significant for an adequate translation 
from the human perspective (Babych and Hart-
ley, 2004). Therefore it is reasonable to suggest 
that introduction of a model which approximates 
intuitions about the significance of the matched 
N-grams will improve the correlation between 
automatically computed MT evaluation scores 
and human evaluation scores for translation Ade-
quacy. 
In this paper we present the result of an ex-
periment on augmenting BLEU N-gram compari-
son with statistical weight coefficients which 
capture a word?s salience within a given docu-
ment: the standard tf.idf measure used in the vec-
tor-space model for Information Retrieval (Salton 
and Leck, 1968) and the S-score proposed for 
evaluating MT output corpora for the purposes of 
Information Extraction (Babych et al, 2003). 
Both scores are computed for each term in each 
of the 100 human reference translations from 
French into English available in DARPA-94 MT 
evaluation corpus (White et al, 1994). 
The proposed weighted N-gram model for MT 
evaluation is tested on a set of translations by 
four different MT systems available in the 
DARPA corpus, and is compared with the results 
of the baseline BLEU method with respect to 
their correlation with human evaluation scores.  
The scores produced by the N-gram model 
with tf.idf and S-Score weights are shown to be 
consistent with baseline BLEU evaluation results 
for Fluency and outperform the BLEU scores for 
Adequacy (where the correlation for the S-score 
weighting is higher). We also show that the 
weighted model may still be reliably used if there 
is only one human reference translation for an 
evaluated text. 
Besides saving cost, the ability to dependably 
work with a single human translation has an addi-
tional advantage: it is now possible to create Re-
call-based evaluation measures for MT, which 
has been problematic for evaluation with multiple 
reference translations, since only one of the 
choices from the reference set is used in transla-
tion (Papineni et al 2002:314). Notably, Recall 
of weighted N-grams is found to be a good esti-
mation of human judgements about translation 
Adequacy. Using weighted N-grams is essential 
for predicting Adequacy, since correlation of Re-
call for non-weighted N-grams is much lower. 
It is possible that other automatic methods 
which use human translations as a reference may 
also benefit from an introduction of an explicit 
model for term significance, since so far these 
methods also implicitly assume that all words are 
equally important in human translation, and use 
all of them, e.g., for measuring edit distances 
(Akiba et al 2001; 2003).  
The weighted N-gram model has been imple-
mented as an MT evaluation toolkit (which in-
cludes a Perl script, example files and 
documentation). It computes evaluation scores 
with tf.idf and S-score weights for translation 
Adequacy and Fluency. The toolkit is available at 
http://www.comp.leeds.ac.uk/bogdan/evalMT.html
2. Set-up of the experiment 
The experiment used French?English transla-
tions available in the DARPA-94 MT evaluation 
corpus. The corpus contains 100 French news 
texts (each text is about 350 words long) trans-
lated into English by 5 different MT systems: 
?Systran?, ?Reverso?, ?Globalink?, ?Metal?, 
?Candide? and scored by human evaluators; there 
are no human scores for ?Reverso?, which was 
added to the corpus on a later stage. The corpus 
also contains 2 independent human translations 
of each text. Human evaluation scores are avail-
able for each of the 400 texts translated by the 4 
MT systems for 3 parameters of translation qual-
ity: ?Adequacy?, ?Fluency? and ?Informative-
ness?. The Adequacy (Fidelity) scores are given 
on a 5-point scale by comparing MT with a hu-
man reference translation. The Adequacy pa-
rameter captures how much of the original 
content of a text is conveyed, regardless of how 
grammatically imperfect the output might be. 
The Fluency scores (also given on a 5-point 
scale) determine intelligibility of MT without 
reference to the source text, i.e., how grammati-
cal and stylistically natural the translation ap-
pears to be. The Informativeness scores (which 
we didn?t use for our experiment) determine 
whether there is enough information in MT out-
put to enable evaluators to answer multiple-
choice questions on its content (White, 2003:237) 
In the first stage of the experiment, each of the 
two sets of human translations was used to com-
pute tf.idf and S-scores for each word in each of 
the 100 texts. The tf.idf score was calculated as: 
tf.idf(i,j) = (1 + log (tfi,j)) log (N / dfi), 
if tfi,j ? 1; where:  
? tfi,j is the number of occurrences of the 
word wi in the document dj; 
? dfi is the number of documents in the cor-
pus where the word wi occurs; 
?  N is the total number of documents in the 
corpus. 
The S-score was calculated as: ( )
)(
)()(),( /)(log),(
icorp
iidoccorpjidoc
P
NdfNPP
jiS
???= ?  
where: 
? Pdoc(i,j) is the relative frequency of the 
word in the text; (?Relative frequency? is 
the number of tokens of this word-type 
divided by the total number of tokens). 
? Pcorp-doc(i) is the relative frequency of the 
same word in the rest of the corpus, with-
out this text; 
? (N ? df(i)) / N is the proportion of texts in 
the corpus, where this word does not oc-
cur (number of texts, where it is not 
found,  divided by number of texts in the 
corpus); 
? Pcorp(i) is the relative frequency of the 
word in the whole corpus, including this 
particular text.  
In the second stage we carried out N-gram based 
MT evaluation, measuring Precision and Recall 
of N-grams in MT output using a single human 
reference translation. N-gram counts were ad-
justed with the tf.idf weights and S-scores for 
every matched word. The following procedure 
was used to integrate the S-scores / tf.idf scores 
for a lexical item into N-gram counts. For every 
word in a given text which received an S-score 
and tf.idf score on the basis of the human refer-
ence corpus, all counts for the N-grams contain-
ing this word are increased by the value of the 
respective score (not just by 1, as in the baseline 
BLEU approach). 
The original matches used for BLEU and the 
weighted matches are both calculated. The fol-
lowing changes have been made to the Perl script 
of the BLEU tool: apart from the operator which 
increases counts for every matched N-gram $ngr 
by 1, i.e.: 
$ngr .= $words[$i+$j] . " "; 
$$hashNgr{$ngr}++;  
the following code was introduced: 
[?] 
$WORD = $words[$i+$j]; 
$WEIGHT = 0; 
if(exists 
  $WordWeight{$TxtN}{$WORD}){ 
    $WEIGHT= 
     $WordWeight{$TxtN}{$WORD}; 
} 
 
$ngr .= $words[$i+$j] . " "; 
$$hashNgr{$ngr}++; 
 
$$hashNgrWEIGHTED{$ngr}+= $WEIGHT; 
[?] 
? where the hash data structure:  
   $WordWeight{$TxtN}{$WORD}=$WEIGHT 
represents the table of tf.idf scores or S-scores for 
words in every text in the corpus. 
The weighted N-gram evaluation scores of 
Precision, Recall and F-measure may be pro-
duced for a segment, for a text or for a corpus of 
translations generated by an MT system. 
In the third stage of the experiment the 
weighted Precision and Recall scores were tested 
for correlation with human scores for the same 
texts and compared to the results of similar tests 
for standard BLEU evaluation. 
Finally we addressed the question whether the 
proposed MT evaluation method allows us to use 
a single human reference translation reliably. In 
order to assess the stability of the weighted 
evaluation scores with a single reference, two 
runs of the experiment were carried out. The first 
run used the ?Reference? human translation, 
while the second run used the ?Expert? human 
translation (each time a single reference transla-
tion was used). The scores for both runs were 
compared using a standard deviation measure.   
3. The results of the MT evaluation with 
frequency weights 
With respect to evaluating MT systems, the cor-
relation for the weighted N-gram model was 
found to be stronger, for both Adequacy and Flu-
ency, the improvement being highest for Ade-
quacy. These results are due to the fact that the 
weighted N-gram model gives much more accu-
rate predictions about the statistical MT system 
?Candide?, whereas the standard BLEU approach 
tends to over-estimate its performance for trans-
lation Adequacy. 
Table 1 present the baseline results for non-
weighted Precision, Recall and F-score. It shows 
the following figures: 
? Human evaluation scores for Adequacy and 
Fluency (the mean scores for all texts produced 
by each MT system);  
? BLEU scores produced using 2 human refer-
ence translations and the default script settings 
(N-gram size = 4); 
? Precision, Recall and F-score for the weighted 
N-gram model produced with 1 human refer-
ence translation and N-gram size = 4. 
? Pearson?s correlation coefficient r for Preci-
sion, Recall and F-score correlated with human 
scores for Adequacy and Fluency r(2) (with 2 
degrees of freedom) for the sets which include 
scores for the 4 MT systems. 
The scores at the top of each cell show the results 
for the first run of the experiment, which used the 
?Reference? human translation; the scores at the 
bottom of the cells represent the results for the 
second run with the ?Expert? human translation. 
 
System 
[ade] / [flu] 
BLEU 
[1&2]  
Prec. 
1/2 
Recall 
1/2 
Fscore 
1/2 
CANDIDE 
0.677 / 0.455 
0.3561 0.4068 
0.4012 
0.3806 
0.3790 
0.3933 
0.3898 
GLOBALINK 
0.710 / 0.381 
0.3199 0.3429 
0.3414 
0.3465 
0.3484 
0.3447 
0.3449 
MS 
0.718 / 0.382 
0.3003 0.3289 
0.3286 
0.3650 
0.3682 
0.3460 
0.3473 
REVERSO 
NA / NA 
0.3823 0.3948 
0.3923 
0.4012 
0.4025 
0.3980 
0.3973 
SYSTRAN 
0.789 / 0.508 
0.4002 0.4029 
0.3981 
0.4129 
0.4118 
0.4078 
0.4049 
Corr r(2) with 
[ade] ? MT 
0.5918 
 
0.1809 
0.1871 
0.6691 
0.6988 
0.4063 
0.4270 
Corr r(2) with 
[flu] ? MT 
0.9807 
 
0.9096 
0.9124 
0.9540 
0.9353 
0.9836 
0.9869
Table 1. Baseline non-weighted scores. 
 
Table 2 summarises the evaluation scores for 
BLEU as compared to tf.idf weighted scores, and 
Table 3 summarises the same scores as compared 
to S-score weighed evaluation. 
 
 
System 
[ade] / [flu] 
BLEU 
[1&2]  
Prec. 
(w) 1/2 
Recall 
(w) 1/2 
Fscore 
(w) 1/2 
CANDIDE 
0.677 / 0.455 
0.3561 0.5242 
0.5176 
0.3094 
0.3051 
0.3892 
0.3839 
GLOBALINK 
0.710 / 0.381 
0.3199 0.4905 
0.4890 
0.2919 
0.2911 
0.3660 
0.3650 
MS 
0.718 / 0.382 
0.3003 0.4919 
0.4902 
0.3083 
0.3100 
0.3791 
0.3798 
REVERSO 
NA / NA 
0.3823 0.5336 
0.5342 
0.3400 
0.3413 
0.4154 
0.4165 
SYSTRAN 
0.789 / 0.508 
0.4002 0.5442 
0.5375 
0.3521 
0.3491 
0.4276 
0.4233 
Corr r(2) with 
[ade] ? MT 
0.5918 
 
0.5248 
0.5561 
0.8354 
0.8667 
0.7691 
0.8119 
Corr r(2) with 
[flu] ? MT 
0.9807 
 
0.9987 
0.9998
0.8849 
0.8350 
0.9408 
0.9070 
Table 2. BLEU vs tf.idf weighted scores. 
 
System 
[ade] / [flu] 
BLEU 
[1&2]  
Prec. 
(w) 1/2 
Recall 
(w) 1/2 
Fscore 
(w) 1/2 
CANDIDE 
0.677 / 0.455 
0.3561 0.5034 
0.4982 
0.2553 
0.2554 
0.3388 
0.3377 
GLOBALINK 
0.710 / 0.381 
0.3199 0.4677 
0.4672 
0.2464 
0.2493 
0.3228 
0.3252 
MS 
0.718 / 0.382 
0.3003 0.4766 
0.4793 
0.2635 
0.2679 
0.3394 
0.3437 
REVERSO 
NA / NA 
0.3823 0.5204 
0.5214 
0.2930 
0.2967 
0.3749 
0.3782 
SYSTRAN 
0.789 / 0.508 
0.4002 0.5314 
0.5218 
0.3034 
0.3022 
0.3863 
0.3828 
Corr r(2) with 
[ade] ? MT 
0.5918 
 
0.6055 
0.6137 
0.9069 
0.9215
0.8574 
0.8792 
Corr r(2) with 
[flu] ? MT 
0.9807 
 
0.9912 
0.9769 
0.8022 
0.7499 
0.8715 
0.8247 
Table 3. BLEU vs S-score weights. 
 
It can be seen from the table that there is a 
strong positive correlation between the baseline 
BLEU scores and human scores for Fluency: 
r(2)=0.9807, p <0.05. However, the correlation 
with Adequacy is much weaker and is not statis-
tically significant: r(2)= 0.5918, p >0.05. The 
most serious problem for BLEU is predicting 
scores for the statistical MT system Candide, 
which was judged to produce relatively fluent, 
but largely inadequate translation. For other MT 
systems (developed with the knowledge-based 
MT architecture) the scores for Adequacy and 
Fluency are consistent with each other: more flu-
ent translations are also more adequate. BLEU 
scores go in line with Candide?s Fluency scores, 
but do not account for its Adequacy scores. 
When Candide is excluded from the evaluation 
set, r correlation goes up, but it is still lower than 
the correlation for Fluency and remains statisti-
cally insignificant: r(1)=0.9608, p > 0.05. There-
fore, the baseline BLEU approach fails to 
consistently predict scores for Adequacy. 
Correlation figures between non-weighted N-
gram counts and human scores are similar to the 
results for BLEU: the highest and statistically 
significant correlation is between the F-score and 
Fluency: r(2)=0.9836, p<0.05, r(2)=0.9869, 
p<0.01, and there is somewhat smaller and statis-
tically significant correlation with Precision. This 
confirms the need to use modified Precision in 
the BLEU method that also in certain respect in-
tegrates Recall. 
The proposed weighted N-gram model outper-
forms BLEU and non-weighted N-gram evalua-
tion in its ability to predict Adequacy scores: 
weighted Recall scores have much stronger cor-
relation with Adequacy (which for MT-only 
evaluation is still statistically insignificant at the 
level p<0.05, but come very close to that point: 
t=3.729 and t=4.108; the required value for 
p<0.05 is t=4.303). 
Correlation figures for S-score-based weights 
are higher than for tf.idf weights (S-score: r(2)= 
0.9069, p > 0.05; r(2)= 0.9215, p > 0.05, tf.idf 
score: r(2)= 0.8354, p >0.05; r(2)= 0.8667, p 
>0.05). 
The improvement in the accuracy of evalua-
tion for the weighted N-gram model can be illus-
trated by the following example of translating the 
French sentence: 
ORI-French: Les trente-huit chefs d'entre-
prise mis en examen dans le dossier ont d?j? 
fait l'objet d'auditions, mais trois d'entre eux 
ont ?t? confront?s, mercredi, dans la foul?e de 
la confrontation "politique". 
English translations of this sentence by the 
knowledge-based system Systran and statistical 
MT system Candide have an equal number of 
matched unigrams (highlighted in italic), there-
fore conventional unigram Precision and Recall 
scores are the same for both systems. However, 
for each translation two of the matched unigrams 
are different (underlined) and receive different 
frequency weights (shown in brackets): 
MT ?Systran?:  
The thirty-eight heads (tf.idf=4.605; S=4.614) of 
undertaking put in examination in the file already 
were the subject of hearings, but three of them 
were confronted, Wednesday, in the tread of "po-
litical" confrontation (tf.idf=5.937; S=3.890). 
Human translation ?Expert?:  
The thirty-eight heads of companies ques-
tioned in the case had already been heard, but 
three of them were brought together Wednes-
day following the "political" confrontation. 
MT ?Candide?:  
The thirty-eight counts of company put into con-
sideration in the case (tf.idf=3.719; S=2.199) al-
ready had (tf.idf=0.562; S=0.000) the object of 
hearings, but three of them were checked, 
Wednesday, in the path of confrontal "political." 
(In the human translation the unigrams matched 
by the Systran output sentence are in italic, those 
matched by the Candide sentence are in bold). 
It can be seen from this example that the uni-
grams matched by Systran have higher term fre-
quency weights (both tf.idf and S-scores):  
heads (tf.idf=4.605;S=4.614)  
confrontation (tf.idf=5.937;S=3.890)
The output sentence of Candide instead 
matched less salient unigrams: 
case (tf.idf=3.719;S=2.199)
had (tf.idf=0.562;S=0.000)  
Therefore for the given sentence weighted uni-
gram Recall (i.e., the ability to avoid under-
generation of salient unigrams) is higher for 
Systran than for Candide (Table 4): 
 Systran Candide 
R 0.6538 0.6538 
R * tf.idf 0.5332 0.4211 
R * S-score 0.5517 0.3697 
   
P 0.5484 0.5484 
P * tf.idf 0.7402 0.9277 
P * S-score 0.7166 0.9573 
Table 4. Recall, Precision, and weighted scores  
 
Weighted Recall scores capture the intuition that 
the translation generated by Systran is more ade-
quate than the one generated by Candide, since it 
preserves more important pieces of information. 
On the other hand, weighted Precision scores 
are higher for Candide. This is due to the fact that 
Systran over-generates (doesn?t match in the hu-
man translation) much more ?exotic?, unordinary 
words, which on average have higher cumulative 
salience scores, e.g., undertaking, exami-
nation, confronted, tread ? vs. the 
corresponding words ?over-generated? by Can-
dide: company, consideration, 
checked, path. In some respect higher 
weighted precision can be interpreted as higher 
Fluency of the Candide?s output sentence, which 
intuitively is perceived as sounding more natu-
rally (although not making much sense). 
On the level of corpus statistics the weighted 
Recall scores go in line with Adequacy, and 
weighted Precision scores (as well as the Preci-
sion-based BLEU scores) ? with Fluency, which 
confirms such interpretation of weighted Preci-
sion and Recall scores in the example above. On 
the other hand, Precision-based scores and non-
weighted Recall scores fail to capture Adequacy. 
The improvement in correlation for weighted 
Recall scores with Adequacy is achieved by re-
ducing overestimation for the Candide system, 
moving its scores closer to human judgements 
about its quality in this respect. However, this is 
not completely achieved: although in terms of 
Recall weighted by the S-scores Candide is cor-
rectly ranked below MS (and not ahead of it, as 
with the BLEU scores), it is still slightly ahead of 
Globalink, contrary to human evaluation results. 
For both methods ? BLEU and the Weighted 
N-gram evaluation ? Adequacy is found to be 
harder to predict than Fluency. This is due to the 
fact that there is no good linguistic model of 
translation adequacy which can be easily formal-
ised. The introduction of S-score weights may be 
a useful step towards developing such a model, 
since correlation scores with Adequacy are much 
better for the Weighted N-gram approach than 
for BLEU. 
Also from the linguistic point of view, S-score 
weights and N-grams may only be reasonably 
good approximations of Adequacy, which in-
volves a wide range of factors, like syntactic and 
semantic issues that cannot be captured by N-
gram matches and require a thesaurus and other 
knowledge-based extensions. Accurate formal 
models of translation variation may also be use-
ful for improving automatic evaluation of Ade-
quacy. 
The proposed evaluation method also pre-
serves the ability of BLEU to consistently predict 
scores for Fluency: Precision weighted by tf.idf 
scores has the strongest positive correlation with 
this aspect of MT quality, which is slightly better 
than the values for BLEU; (S-score: r(2)= 
0.9912, p<0.01; r(2)= 0.9769, p<0.05; tf.idf 
score: r(2)= 0.9987, p<0.001; r(2)= 0.9998, 
p<0.001). 
The results suggest that weighted Precision 
gives a good approximation of Fluency. Similar 
results with non-weighted approach are only 
achieved if some aspect of Recall is integrated 
into the evaluation metric (either as modified pre-
cision, as in BLEU, or as an aspect of the F-
score). Weighted Recall (especially with S-
scores) gives a reasonably good approximation of 
Adequacy. 
On the one hand using 1 human reference with 
uniform results is essential for our methodology, 
since it means that there is no more ?trouble with 
Recall? (Papineni et al, 2002:314) ? a system?s 
ability to avoid under-generation of N-grams can 
now be reliably measured. On the other hand, 
using a single human reference translation in-
stead of multiple translations will certainly in-
crease the usability of N-gram based MT 
evaluation tools. 
The fact that non-weighted F-scores also have 
high correlation with Fluency suggests a new 
linguistic interpretation of the nature of these two 
quality criteria: it is intuitively plausible that Flu-
ency subsumes, i.e. presupposes Adequacy (simi-
larly to the way the F-score subsumes Recall, 
which among all other scores gives the best cor-
relation with Adequacy). The non-weighted F-
score correlates more strongly with Fluency than 
either of its components: Precision and Recall; 
similarly Adequacy might make a contribution to 
Fluency together with some other factors. It is 
conceivable that people need adequate transla-
tions (or at least translations that make sense) in 
order to be able to make judgments about natu-
ralness, or Fluency.  
Being able to make some sense out of a text 
could be the major ground for judging Adequacy: 
sensible mistranslations in MT are relatively rare 
events. This may be the consequence of a princi-
ple similar to the ?second law of thermodynam-
ics? applied to text structure, ? in practice it is 
much rarer to some alternative sense to be cre-
ated (even if the number of possible error types 
could be significant), than to destroy the existing 
sense in translation, so the majority of inadequate 
translations are just nonsense. However, in con-
trast to human translation, fluent mistranslations 
in MT are even rarer than disfluent ones, accord-
ing to the same principle. A real difference in 
scores is made by segments which make sense 
and may or may not be fluent, and things which 
do not make any sense and about which it is hard 
to tell whether they are fluent. 
This suggestion may be empirically tested: if 
Adequacy is a necessary precondition for Flu-
ency, there should be a greater inter-annotator 
disagreement in Fluency scores on texts or seg-
ments which have lower Adequacy scores. This 
will be a topic of future research. 
We note that for the DARPA corpus the corre-
lation scores presented are highest if the evalua-
tion unit is an entire corpus of translations 
produced by an MT system, and for text-level 
evaluation, correlation is much lower. A similar 
observation was made in (Papineni et al, 2002: 
313). This may be due to the fact that human 
judges are less consistent, especially for puzzling 
segments that do not fit the scoring guidelines, 
like nonsense segments for which it is hard to 
decide whether they are fluent or even adequate. 
However, this randomness is leveled out if the 
evaluation unit increases in size ? from the text 
level to the corpus level.  
Automatic evaluation methods such as BLEU 
(Papineni et al, 2002), RED (Akiba et al, 2001), 
or the weighted N-gram model proposed here 
may be more consistent in judging quality as 
compared to human evaluators, but human judg-
ments remain the only criteria for meta-
evaluating the automatic methods. 
4. Stability of weighted evaluation scores 
In this section we investigate how reliable is the 
use of a single human reference translation. The 
stability of the scores is central to the issue of 
computing Recall and reducing the cost of auto-
matic evaluation. We also would like to compare 
the stability of our results with the stability of the 
baseline non-weighted N-gram model using a 
single reference. 
In this stage of the experiment we measured 
the changes that occur for the scores of MT sys-
tems if an alternative reference translation is used 
? both for the baseline N-gram counts and for the 
weighted N-gram model. Standard deviation was 
computed for each pair of evaluation scores pro-
duced by the two runs of the system with alterna-
tive human references. An average of these 
standard deviations is the measure of stability for 
a given score. The results of these calculations 
are presented in Table 5. 
 systems StDev-
basln 
StDev-
tf.idf 
StDev-
S-score 
P candide 0.004 0.0047 0.0037 
 globalink 0.0011 0.0011 0.0004 
 ms 0.0002 0.0012 0.0019 
 reverso 0.0018 0.0004 0.0007 
 systran 0.0034 0.0047 0.0068 
 AVE SDEV 0.0021 0.0024 0.0027 
R candide 0.0011 0.003 0.0001 
 globalink 0.0013 0.0006 0.0021 
 ms 0.0023 0.0012 0.0031 
 reverso 0.0009 0.0009 0.0026 
 systran 0.0008 0.0021 0.0008 
 AVE SDEV 0.0013 0.0016 0.0017 
F candide 0.0025 0.0037 0.0008 
 globalink 0.0001 0.0007 0.0017 
 ms 0.0009 0.0005 0.003 
 reverso 0.0005 0.0008 0.0023 
 systran 0.0021 0.003 0.0025 
 AVE SDEV 0.0012 0.0018 0.0021 
Table 5. Stability of scores 
 
Standard deviation for weighted scores is gener-
ally slightly higher, but both the baseline and the 
weighted N-gram approaches give relatively sta-
ble results: the average standard deviation was 
not greater than 0.0027, which means that both 
will produce reliable figures with just a single 
human reference translation (although interpreta-
tion of the score with a single reference should be 
different than with multiple references). 
Somewhat higher standard deviation figures 
for the weighted N-gram model confirm the sug-
gestion that a word?s importance for translation 
cannot be straightforwardly derived from the 
model of the legitimate translation variation im-
plemented in BLEU and needs the salience 
weights, such as tf.idf or S-scores. 
5. Conclusion and future work  
The results for weighted N-gram models have a 
significantly higher correlation with human intui-
tive judgements about translation Adequacy and 
Fluency than the baseline N-gram evaluation 
measures which are used in the BLEU MT 
evaluation toolkit. This shows that they are a 
promising direction of research. Future work will 
apply our approach to evaluating MT into lan-
guages other than English, extending the experi-
ment to a larger number of MT systems built on 
different architectures and to larger corpora. 
However, the results of the experiment may 
also have implications for MT development: sig-
nificance weights may be used to rank the rela-
tive ?importance? of translation equivalents. At 
present all MT architectures (knowledge-based, 
example-based, and statistical) treat all transla-
tion equivalents equally, so MT systems cannot 
dynamically prioritise rule applications, and 
translations of the central concepts in texts are 
often lost among excessively literal translations 
of less important concepts and function words. 
For example, for statistical MT significance 
weights of lexical items may indicate which 
words have to be introduced into the target text 
using the translation model for source and target 
languages, and which need to be brought there by 
the language model for the target corpora. Simi-
lar ideas may be useful for the Example-based 
and Rule-based MT architectures. The general 
idea is that different pieces of information ex-
pressed in the source text are not equally impor-
tant for translation: MT systems that have no 
means for prioritising this information often in-
troduce excessive information noise into the tar-
get text by literally translating structural 
information, etymology of proper names, collo-
cations that are unacceptable in the target lan-
guage, etc. This information noise often obscures 
important translation equivalents and prevents 
the users from focusing on the relevant bits. MT 
quality may benefit from filtering out this exces-
sive information as much as from frequently rec-
ommended extension of knowledge sources for 
MT systems. The significance weights may 
schedule the priority for retrieving translation 
equivalents and motivate application of compen-
sation strategies in translation, e.g., adding or 
deleting implicitly inferable information in the 
target text, using non-literal strategies, such as 
transposition or modulation (Vinay and Darbel-
net, 1995). Such weights may allow MT systems 
to make an approximate distinction between sali-
ent words which require proper translation 
equivalents and structural material both in the 
source and in the target texts. Exploring applica-
bility of this idea to various MT architectures is 
another direction for future research. 
Acknowledgments 
We are very grateful for the insightful comments 
of the three anonymous reviewers. 
References 
Akiba, Y., K. Imamura and E. Sumita. 2001. Using mul-
tiple edit distances to automatically rank machine 
translation output. In Proc. MT Summit VIII. p. 15?
20. 
Akiba, Y., E. Sumita, H. Nakaiwa, S. Yamamoto and 
H.G. Okuno. 2003. Experimental Comparison of MT 
Evaluation Methods: RED vs. BLEU. In Proc. MT 
Summit IX, URL: http://www.amtaweb.org/summit/ 
MTSummit/ FinalPapers/55-Akiba-final.pdf. 
Babych, B., A. Hartley and E. Atwell. 2003. Statistical 
Modelling of MT output corpora for Information Ex-
traction. In: Proceedings of the Corpus Linguistics 
2003 conference, Lancaster University (UK), 28 - 31 
March 2003, pp. 62-70. 
Babych, B. and A. Hartley. 2004. Modelling legitimate 
translation variation for automatic evaluation of MT 
quality. In: Proceedings of LREC 2004 (forthcoming). 
Papineni, K., S. Roukos, T. Ward, W.-J. Zhu. 2002 
BLEU: a method for automatic evaluation of machine 
translation. Proceedings of the 40th Annual Meeting of 
the Association for the Computational Linguistics 
(ACL), Philadelphia, July 2002, pp. 311-318. 
Salton, G. and M.E. Lesk. 1968. Computer evaluation of 
indexing and text processing. Journal of the ACM, 
15(1) , 8-36. 
Vinay, J.P. and J.Darbelnet. 1995. Comparative stylistics 
of French and English : a methodology for translation 
/ translated and edited by Juan C. Sager, M.-J. Hamel. 
J. Benjamins Pub., Amsterdam, Philadelphia. 
White, J., T. O?Connell and F. O?Mara. 1994. The 
ARPA MT evaluation methodologies: evolution, les-
sons and future approaches. Proceedings of the 1st 
Conference of the Association for Machine Transla-
tion in the Americas. Columbia, MD, October 1994. 
pp. 193-205. 
White, J. 2003. How to evaluate machine translation. In: 
H. Somers. (Ed.) Computers and Translation: a trans-
lator?s guide. Ed. J. Benjamins B.V., Amsterdam, 
Philadelphia, pp. 211-244. 
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 739?746,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using comparable corpora
to solve problems difficult for human translators
Serge Sharoff, Bogdan Babych, Anthony Hartley
Centre for Translation Studies
University of Leeds, LS2 9JT UK
{s.sharoff,b.babych,a.hartley}@leeds.ac.uk
Abstract
In this paper we present a tool that uses
comparable corpora to find appropriate
translation equivalents for expressions that
are considered by translators as difficult.
For a phrase in the source language the
tool identifies a range of possible expres-
sions used in similar contexts in target lan-
guage corpora and presents them to the
translator as a list of suggestions. In the
paper we discuss the method and present
results of human evaluation of the perfor-
mance of the tool, which highlight its use-
fulness when dictionary solutions are lack-
ing.
1 Introduction
There is no doubt that both professional and
trainee translators need access to authentic data
provided by corpora. With respect to polyse-
mous lexical items, bilingual dictionaries list sev-
eral translation equivalents for a headword, but
words taken in their contexts can be translated
in many more ways than indicated in dictionar-
ies. For instance, the Oxford Russian Dictionary
(ORD) lacks a translation for the Russian expres-
sion ????????????? ????? (?comprehensive an-
swer?), while the Multitran Russian-English dic-
tionary suggests that it can be translated as ir-
refragable answer. Yet this expression is ex-
tremely rare in English; on the Internet it occurs
mostly in pages produced by Russian speakers.
On the other hand, translations for polysemous
words are too numerous to be listed for all pos-
sible contexts. For example, the entry for strong
in ORD already has 57 subentries and yet it fails
to mention many word combinations frequent in
the British National Corpus (BNC), such as strong
{feeling, field, opposition, sense, voice}. Strong
voice is also not listed in the Oxford French, Ger-
man or Spanish Dictionaries.
There has been surprisingly little research on
computational methods for finding translation
equivalents of words from the general lexicon.
Practically all previous studies have concerned
detection of terminological equivalence. For in-
stance, project Termight at AT&T aimed to de-
velop a tool for semi-automatic acquisition of
termbanks in the computer science domain (Da-
gan and Church, 1997). There was also a study
concerning the use of multilingual webpages to
develop bilingual lexicons and termbanks (Grefen-
stette, 2002). However, neither of them concerned
translations of words from the general lexicon. At
the same time, translators often experience more
difficulty in dealing with such general expressions
because of their polysemy, which is reflected dif-
ferently in the target language, thus causing the
dependency of their translation on the correspond-
ing context. Such variation is often not captured
by dictionaries.
Because of their importance, words from the
general lexicon are studied by translation re-
searchers, and comparable corpora are increas-
ingly used in translation practice and training
(Varantola, 2003). However, such studies are
mostly confined to lexicographic exercises, which
compare the contexts and functions of potential
translation equivalents once they are known, for
instance, absolutely vs. assolutamente in Italian
(Partington, 1998). Such studies do not pro-
vide a computational model for finding appropri-
ate translation equivalents for expressions that are
not listed or are inadequate in dictionaries.
Parallel corpora, conisting of original texts and
739
their exact translations, provide a useful supple-
ment to decontextualised translation equivalents
listed in dictionaries. However, parallel corpora
are not representative. Many of them are in the
range of a few million words, which is simply too
small to account for variations in translation of
moderately frequent words. Those that are a bit
larger, such as the Europarl corpus, are restricted
in their domain. For instance, all of the 14 in-
stances of strong voice in the English section of
Europarl are used in the sense of ?the opinion of
a political institution?. At the same time the BNC
contains 46 instances of strong voice covering sev-
eral different meanings.
In this paper we propose a computational
method for using comparable corpora to find trans-
lation equivalents for source language expressions
that are considered as difficult by trainee or pro-
fessional translators. The model is based on de-
tecting frequent multi-word expressions (MWEs)
in the source and target languages and finding a
mapping between them in comparable monolin-
gual corpora, which are designed in a similar way
in the two languages.
The described methodology is implemented in
ASSIST, a tool that helps translators to find solu-
tions for difficult translation problems. The tool
presents the results as lists of translation sugges-
tions (usually 50 to 100 items) ordered alphabeti-
cally or by their frequency in target language cor-
pora. Translators can skim through these lists and
identify an example which is most appropriate in
a given context.
In the following sections we outline our ap-
proach, evaluate the output of the prototype of AS-
SIST and discuss future work.
2 Finding translations in comparable
corpora
The proposed model finds potential translation
equivalents in four steps, which include
1. expansion of words in the original expression
using related words;
2. translation of the resultant set using existing
bilingual dictionaries;
3. further expansion of the set using related
words in the target language;
4. filtering of the set according to expressions
frequent in the target language corpus.
In this study we use several comparable cor-
pora for English and Russian, including large ref-
erence corpora (the BNC and the Russian Refer-
ence Corpus) and corpora of major British and
Russian newspapers. All corpora used in the study
are quite large, i.e. the size of each corpus is in
the range of 100-200 million words (MW), so that
they provide enough evidence to detect such col-
locations as strong voice and clear defiance.
Although the current study is restricted to the
English-Russian pair, the methodology does not
rely on any particular language. It can be ex-
tended to other languages for which large com-
parable corpora, POS-tagging and lemmatisation
tools, and bilingual dictionaries are available. For
example, we conducted a small study for transla-
tion between English and German using the Ox-
ford German Dictionary and a 200 MW German
corpus derived from the Internet (Sharoff, 2006).
2.1 Query expansion
The problem with using comparable corpora to
find translation equivalents is that there is no ob-
vious bridge between the two languages. Unlike
aligned parallel corpora, comparable corpora pro-
vide a model for each individual language, while
dictionaries, which can serve as a bridge, are inad-
equate for the task in question, because the prob-
lem we want to address involves precisely transla-
tion equivalents that are not listed there.
Therefore, a specific query needs first to be
generalised in order to then retrieve a suitable
candidate from a set of candidates. One way
to generalise the query is by using similarity
classes, i.e. groups of words with lexically simi-
lar behaviour. In his work on distributional sim-
ilarity (Lin, 1998) designed a parser to identify
grammatical relationships between words. How-
ever, broad-coverage parsers suitable for process-
ing BNC-like corpora are not available for many
languages. Another, resource-light approach treats
the context as a bag of words (BoW) and detects
the similarity of contexts on the basis of colloca-
tions in a window of a certain size, typically 3-4
words, e.g. (Rapp, 2004). Even if using a parser
can increase precision in identification of contexts
in the case of long-distance dependencies (e.g. to
cook Alice a whole meal), we can find a reason-
able set of relevant terms returned using the BoW
approach, cf. the results of human evaluation for
English and German by (Rapp, 2004).
740
For each source word s0 we produce a list of
similar words: ?(s0) = s1, . . . , sN (in our tool
we use N = 20 as the cutoff). Since lists of dis-
tributionally words can contain words irrelevant to
the source word, we filter them to produce a more
reliable similarity class S(s0) using the assump-
tion that the similarity classes of similar words
have common members:
?w ? S(s0), w ? ?(s0)&w ?
?
?(si)
This yields for experience the following similar-
ity class: knowledge, opportunity, life, encounter,
skill, feeling, reality, sensation, dream, vision,
learning, perception, learn.1 Even if there is no
requirement in the BoW approach that words in
the similarity class are of the same part of speech,
it happens quite frequently that most words have
the same part of speech because of the similarity
of contexts.
2.2 Query translation and further expansion
In the next step we produce a translation class by
translating all words from the similarity class into
the target language using a bilingual dictionary
(T (w) for the translation of w). Then for Step 3
we have two options: a full translation class (TF )
and a reduced one (TR).
TF consists of similarity classes produced for
all translations: S(T (S(s0))). However, this
causes a combinatorial explosion. If a similarity
class contains N words (the average figure is 16)
and a dictionary lists on average M equivalents
for a source word (the average figure is 11), this
procedure outputs on average M ? N2 words in
the full translation class. For instance, the com-
plete translation class for experience contains 998
words. What is worse, some words from the full
translation class do not refer to the domain im-
plied in the original expression because of the am-
biguity of the translation operation. For instance,
the word dream belongs to the similarity class of
experience. Since it can be translated into Rus-
sian as ?????? (?fairy-tale?), the latter Russian word
will be expanded in the full translation class with
words referring to legends and stories. In the later
stages of the project, word sense disambiguation
in corpora could improve precision of translation
classes. However at the present stage we attempt
to trade the recall of the tool for greater precision
by translating words in the source similarity class,
1Ordered according to the score produced by the Singular
Value Decomposition method as implemented by Rapp.
and generating the similarity classes of transla-
tions only for the source word:
TR(s0) = S(T (s0)) ? T (S(s0)).
This reduces the class of experience to 128 words.
This step crucially relies on a wide-coverage
machine readable dictionary. The bilingual dictio-
nary resources we use are derived from the source
file for the Oxford Russian Dictionary, provided
by OUP.
2.3 Filtering equivalence classes
In the final step we check all possible combina-
tions of words from the translation classes for their
frequency in target language corpora.
The number of elements in the set of theoreti-
cally possible combinations is usually very large:
?
Ti, where Ti is the number of words in the trans-
lation class of each word of the original MWE.
This number is much larger than the set of word
combinations which is found in the target lan-
guage corpora. For instance, daunting experience
has 202,594 combinations for the full translation
class of daunting experience and 6,144 for the re-
duced one. However, in the target language cor-
pora we can find only 2,256 collocations with fre-
quency > 2 for the full translation class and 92 for
the reduced one.
Each theoretically possible combination is gen-
erated and looked up in a database of MWEs
(which is much faster than querying corpora for
frequencies of potential collocations). The MWE
database was pre-compiled from corpora using a
method of filtering, similar to part-of-speech fil-
tering suggested in (Justeson and Katz, 1995): in
corpora each N-gram of length 2, 3 and 4 tokens
was checked against a set of filters.
However, instead of pre-defined patterns for en-
tire expressions our filtering method uses sets of
negative constraints, which are usually applied to
the edges of expressions. This change boosts re-
call of retrieved MWEs and allows us to use the
same set of patterns for MWEs of different length.
The filter uses constraints for both lexical and
part-of-speech features, which makes configura-
tion specifications more flexible.
The idea of applying a negative feature filter
rather than a set of positive patterns is based on
the observation that it is easier to describe unde-
sirable features than to enumerate complete lists of
patterns. For example, MWEs of any length end-
ing with a preposition are undesirable (particles in
741
British news Russian news
no of words 217,394,039 77,625,002
REs in filter 25 18
2-grams 6,361,596 5,457,848
3-grams 14,306,653 11,092,908
4-grams 19,668,956 11,514,626
Table 1: MWEs in News Corpora
phrasal verbs, which are desirable, are tagged dif-
ferently by the Tree Tagger, so there is no problem
with ambiguity here). Our filter captures this fact
by having a negative condition for the right edge of
the pattern (regular expression /_IN$/), rather than
enumerating all possible configurations which do
not contain a preposition at the end. In this sense
the filter is permissive: everything that is not ex-
plicitly forbidden is allowed, which makes the de-
scription more economical.
The same MWE database is used for check-
ing frequencies of multiword collocates for cor-
pus queries. For this task, candidate N-grams in
the vicinity of searched patterns are filtered us-
ing the same regular expression grammar of MWE
constraints, and then their corpus frequency is
checked in the database. Thus scores for mul-
tiword collocates can be computed from contin-
gency tables similarly to single-word collocates.
In addition, only MWEs with a frequency
higher than 1 are stored in the database. This fil-
ters out most expressions that co-occur by chance.
Table 1 gives an overview of the number of MWEs
from the news corpus which pass the filter. Other
corpora used in ASSIST (BNC and RRC) yield
similar results. MWE frequencies for each corpus
can be checked individually or joined together.
3 Evaluation
There are several attributes of our system which
can be evaluated, and many of them are crucial
for its efficient use in the workflow of professional
translators, including: usability, quality of final so-
lutions, trade-off between adequacy and fluency
across usable examples, precision and recall of po-
tentially relevant suggestions, as well as real-text
evaluation, i.e. ?What is the coverage of difficult
translation problems typically found in a text that
can be successfully tackled??
In this paper we focus on evaluating the quality
of potentially relevant translation solutions, which
is the central point for developing and calibrat-
ing our methodology. The evaluation experiment
discussed below was specifically designed to as-
sess the usefulness of translation suggestions gen-
erated by our tool ? in cases where translators
have doubts about the usefulness of dictionary so-
lutions. In this paper we do not evaluate other
equally important aspects of the system?s func-
tionality, which will be the matter of future re-
search.
3.1 Set-up of the experiment
For each translation direction we collected ten ex-
amples of possibly recalcitrant translation prob-
lems ? words or phrases whose translation is not
straightforward in a given context. Some of these
examples were sent to us by translators in response
to our request for difficult cases. For each exam-
ple, which we included in the evaluation kit, the
word or phrase either does not have a translation in
ORD (which is a kind of a baseline standard ref-
erence for Russian translators), or its translation
has significantly lower frequency in a target lan-
guage corpus in comparison to the frequency of
the source expression. If an MWE is not listed in
available dictionaries, we produced compositional
(word-for-word) translations using ORD. In order
to remove a possible anti-dictionary bias from our
experiment, we also checked translations in Mul-
titran, an on-line translation dictionary, which was
often quoted as one of the best resources for trans-
lation from and into Russian.
For each translation problem five solutions were
presented to translators for evaluation. One or two
of these solutions were taken from a dictionary
(usually from Multitran, and if available and dif-
ferent, from ORD). The other suggestions were
manually selected from lists of possible solutions
returned by ASSIST. Again, the criteria for se-
lection were intuitive: we included those sugges-
tions which made best sense in the given context.
Dictionary suggestions and the output of ASSIST
were indistinguishable in the questionnaires to the
evaluators. The segments were presented in sen-
tence context and translators had an option of pro-
viding their own solutions and comments. Ta-
ble 2 shows one of the questions sent to evalua-
tors. The problem example is ?????? ?????????
(?precise programme?), which is presented in the
context of a Russian sentence with the following
(non-literal) translation This team should be put
together by responsible politicians, who have a
742
Problem example
?????? ?????????, as in
??????? ??? ??????? ?????? ?????????????
????, ??????? ?????? ????????? ?????? ??
???????.
Translation suggestions Score
clear plan
clear policy
clear programme
clear strategy
concrete plan
Your suggestion ? (optional)
Table 2: Example of an entry in questionnaire
clear strategy for resolving the current crisis. The
third translation equivalent (clear programme) in
the table is found in the Multitran dictionary (ORD
offers no translation for ?????? ?????????). The
example was included because clear programme
is much less frequent in English (2 examples in the
BNC) in comparison to ?????? ????????? in Rus-
sian (70). Other translation equivalents in Table 2
are generated by ASSIST.
We then asked professional translators affiliated
to a translator?s association (identity witheld at this
stage) to rate these five potential equivalents using
a five-point scale:
5 = The suggestion is an appropriate translation
as it is.
4 = The suggestion can be used with some minor
amendment (e.g. by turning a verb into a par-
ticiple).
3 = The suggestion is useful as a hint for an-
other, appropriate translation (e.g. suggestion
elated cannot be used, but its close synonym
exhilarated can).
2 = The suggestion is not useful, even though it is
still in the same domain (e.g. fear is proposed
for a problem referring to hatred).
1 = The suggestion is totally irrelevant.
We received responses from eight translators.
Some translators did not score all solutions, but
there were at least four independent judgements
for each of the 100 translation variants. An exam-
ple of the combined answer sheet for all responses
to the question from Table 2 is given in Table 3 (t1,
Translation t1 t2 t3 t4 t5 ?
clear plan 5 5 3 4 4 0.84
clear policy 5 5 3 4 4 0.84
clear programme 5 5 3 4 4 0.84
clear strategy 5 5 5 5 5 0.00
concrete plan 1 5 3 3 5 1.67
Best Dict 5 5 3 4 4 0.84
Best Syst 5 5 5 5 5 0.00
Table 3: Scores to translation equivalents
t2,. . . denote translators; the dictionary translation
is clear programme).
3.2 Interpretation of the results
The results were surprising in so far as for the ma-
jority of problems translators preferred very differ-
ent translation solutions and did not agree in their
scores for the same solutions. For instance, con-
crete plan in Table 3 received the score 1 from
translator t1 and 5 from t2.
In general, the translators very often picked up
on different opportunities presented by the sug-
gestions from the lists, and most suggestions were
equally legitimate ways of conveying the intended
content, cf. the study of legitimate translation vari-
ation with respect to the BLEU score in (Babych
and Hartley, 2004). In this respect it may be unfair
to compute average scores for each potential solu-
tion, since for most interesting cases the scores do
not fit into the normal distribution model. So aver-
aging scores would mask the potential usability of
really inventive solutions.
In this case it is more reasonable to evaluate
two sets of solutions ? the one generated by AS-
SIST and the other found in dictionaries ? but not
each solution individually. In order to do that for
each translation problem the best scores given by
each translator in each of these two sets were se-
lected. This way of generalising data characterises
the general quality of suggestion sets, and exactly
meets the needs of translators, who collectively get
ideas from the presented sets rather than from in-
dividual examples. This also allows us to mea-
sure inter-evaluator agreement on the dictionary
set and the ASSIST set, for instance, via computing
the standard deviation ? of absolute scores across
evaluators (Table 3). This appeared to be a very
informative measure for dictionary solutions.
In particular, standard deviation scores for the
dictionary set (threshold ? = 0.5) clearly split
743
Agreement: ? for dictionary ? 0.5
Example Dict ASSIST
Ave ? Ave ?
political upheaval 4.83 0.41 4.67 0.82
Disagreement: ? for dictionary >0.5
Example Dict ASSIST
Ave ? Ave ?
clear defiance 4.14 0.90 4.60 0.55
Table 4: Examples for the two groups
Agreement: ? for dictionary ? 0.5
Sub-group Dict ASSIST
Ave ? Ave ?
Agreement E?R 4.73 0.46 4.47 0.80
Agreement R?E 4.90 0.23 4.52 0.60
Agreement?All 4.81 0.34 4.49 0.70
Disagreement: ? for dictionary >0.5
Sub-group Dict ASSIST
Ave ? Ave ?
Disagreement E?R 3.63 1.08 3.98 0.85
Disagreement R?E 3.90 1.02 3.96 0.73
Disagreement?All 3.77 1.05 3.97 0.79
Table 5: Averages for the two groups
our 20 problems into two distinct groups: the first
group below the threshold contains 8 examples,
for which translators typically agree on the qual-
ity of dictionary solutions; and the second group
above the threshold contains 12 examples, for
which there is less agreement. Table 4 shows some
examples from both groups and Table 5 presents
average evaluation scores and standard deviation
figures for both groups.
Overall performance on all 20 examples is the
same for the dictionary responses and for the sys-
tem?s responses: average of the mean top scores
is about 4.2 and average standard deviation of the
scores is 0.8 in both cases (for set-best responses).
This shows that ASSIST can reach the level of
performance of a combination of two authoritative
dictionaries for MWEs, while for its own transla-
tion step it uses just a subset of one-word transla-
tion equivalents from ORD. However, there is an-
other side to the evaluation experiment. In fact, we
are less interested in the system?s performance on
all of these examples than on those examples for
which there is greater disagreement among trans-
lators, i.e. where there is some degree of dissatis-
faction with dictionary suggestions.
012345impin
ge
politic
al uph
eaval
contro
versia
l plan
defus
e tens
ions
?????
?????
??? ?
????
?????
?????
?????
 ????
?????
?????
??
?????
?????????
?????
??
?????
?????
Figure 1: Agreement scores: dictionary
Interestingly, dictionary scores for the agree-
ment group are always higher than 4, which means
that whenever translators agreed on the dictionary
scores they were usually satisfied with the dictio-
nary solution. But they never agreed on the inap-
propriateness of the dictionary: inappropriateness
revealed itself in the form of low scores from some
translators.
This agreement/disagreement threshold can be
said to characterise two types of translation prob-
lems: those for which there exist generally ac-
cepted dictionary solutions, and those for which
translators doubt whether the solution is appropri-
ate. Best-set scores for these two groups of dic-
tionary solutions ? the agreement and disagree-
ment group ? are plotted on the radar charts in
Figures 1 and 2 respectively. The identifiers on
the charts are problematic source language expres-
sions as used in the questionnaire (not translation
solutions to these problems, because a problem
may have several solutions preferred by different
judges). Scores for both translation directions are
presented on the same chart, since both follow the
same pattern and receive the same interpretation.
Figure 1 shows that whenever there is little
doubt about the quality of dictionary solutions, the
radar chart approaches a circle shape near the edge
of the chart. In Figure 2 the picture is different:
the circle is disturbed, and some scores frequently
approach the centre. Therefore the disagreement
group contains those translation problems where
dictionaries provide little help.
The central problem in our evaluation experi-
ment is whether ASSIST is helpful for problems
in the second group, where translators doubt the
quality of dictionary solutions.
Firstly, it can be seen from the charts that judge-
744
012345
?????
?????
?????
?????
?????
???
?????
??? ????
?? ??
?????
??
?????
?????
?
?????
?????
????
?????
?????
??? ??
?????
?
due p
roces
s
negot
iated 
settle
ment
clear 
defian
ce
daunt
ing ex
perien
ce
passio
nately
 seekrecrea
tional
 fear
Figure 2: Disagreement scores: dictionary
012345
?????
?????
?????
?????
?????
???
?????
??? ????
?? ??
?????
??
?????
?????
?
?????
?????
????
?????
?????
??? ??
?????
?
due p
roces
s
negot
iated 
settle
ment
clear 
defian
ce
daunt
ing ex
perien
ce
passio
nately
 seekrecrea
tional
 fear
Figure 3: Disagreement scores: ASSIST
ments on the quality of the system output are more
consistent: score lines for system output are closer
to the circle shape in Figure 1 than those for dic-
tionary solutions in Figure 2 (formally: the stan-
dard deviation of evaluation scores, presented in
Table 4, is lower).
Secondly, as shown in Table 4, in this group av-
erage evaluation scores are slightly higher for AS-
SIST output than for dictionary solutions (3.97 vs
3.77) ? in the eyes of human evaluators ASSIST
outperforms good dictionaries. For good dictio-
nary solutions ASSIST performance is slightly
lower: (4.49 vs 4.81), but the standard deviation
is about the same.
Having said this, solutions from our system are
really not in competition with dictionary solutions:
they provide less literal translations, which often
emerge in later stages of the translation task, when
translators correct and improve an initial draft,
where they have usually put more literal equiva-
lents (Shveitser, 1988). It is a known fact in trans-
lation studies that non-literal solutions are harder
to see and translators often find them only upon
longer reflection. Yet another fact is that non-
literal translations often require re-writing other
segments of the sentence, which may not be ob-
vious at first glance.
4 Conclusions and future work
The results of evaluation show that the tool is
successful in finding translation equivalents for a
range of examples. What is more, in cases where
the problem is genuinely difficult, ASSIST consis-
tently provides scores around 4 ? ?minor adapta-
tions needed?. The precision of the tool is low, it
suggests 50-100 examples with only 2-4 useful for
the current context. However, recall of the output
is more relevant than precision, because transla-
tors typically need just one solution for their prob-
lem, and often have to look through reasonably
large lists of dictionary translations and examples
to find something suitable for a problematic ex-
pression. Even if no immediately suitable trans-
lation can be found in the list of suggestions, it
frequently contains a hint for solving the problem
in the absence of adequate dictionary information.
The current implementation of the model is re-
stricted in several respects. First, the majority of
target language constructions mirror the syntactic
structure of the source language example. Even
if the procedure for producing similarity classes
does not impose restrictions on POS properties,
nevertheless words in the similarity class tend to
follow the POS of the original word, because of
the similarity of their contexts of use. Further-
more, dictionaries also tend to translate words
using the same POS. This means that the ex-
isting method finds mostly NPs for NPs, verb-
object pairs for verb-object pairs, etc, even if the
most natural translation uses a different syntactic
structure, e.g. I like doing X instead of I do X
gladly (when translating from German ich mache
X gerne).
Second, suggestions are generated for the query
expression independently from the context it is
used in. For instance, the words judicial, military
and religious are in the similarity class of politi-
cal, just as reform is in the simclass of upheaval.
So the following example
The plan will protect EC-based investors in Russia
from political upheavals damaging their business.
creates a list of ?possible translations? evoking
various reforms and transformations.
745
These issues can be addressed by introduc-
ing a model of the semantic context of situation,
e.g. ?changes in business practice? as in the ex-
ample above, or ?unpleasant situation? as in the
case of daunting experience. This will allow
less restrictive identification of possible transla-
tion equivalents, as well as reduction of sugges-
tions irrelevant for the context of the current ex-
ample.
Currently we are working on an option to iden-
tify semantic contexts by means of ?semantic sig-
natures? obtained from a broad-coverage seman-
tic parser, such as USAS (Rayson et al, 2004).
The semantic tagset used by USAS is a language-
independent multi-tier structure with 21 major dis-
course fields, subdivided into 232 sub-categories
(such as I1.1- = Money: lack; A5.1- = Eval-
uation: bad), which can be used to detect the
semantic context. Identification of semantically
similar situations can be also improved by the
use of segment-matching algorithms as employed
in Example-Based MT (EBMT) and translation
memories (Planas and Furuse, 2000; Carl and
Way, 2003).
The proposed model looks similar to some im-
plementations of statistical machine translation
(SMT), which typically uses a parallel corpus for
its translation model, and then finds the best possi-
ble recombination that fits into the target language
model (Och and Ney, 2003). Just like an MT sys-
tem, our tool can find translation equivalents for
queries which are not explicitly coded as entries
in system dictionaries. However, from the user
perspective it resembles a dynamic dictionary or
thesaurus: it translates difficult words and phrases,
not entire sentences. The main thrust of our sys-
tem is its ability to find translation equivalents for
difficult contexts where dictionary solutions do not
exist, are questionable or inappropriate.
Acknowledgements
This research is supported by EPSRC grant
EP/C005902.
References
Bogdan Babych and Anthony Hartley. 2004. Ex-
tending the BLEU MT evaluation method with fre-
quency weightings. In Proceedings of the 42d An-
nual Meeting of the Association for Computational
Linguistics, Barcelona.
Michael Carl and Andy Way, editors. 2003. Re-
cent advances in example-based machine transla-
tion. Kluwer, Dordrecht.
Ido Dagan and Kenneth Church. 1997. Ter-
might: Coordinating humans and machines in bilin-
gual terminology acquisition. Machine Translation,
12(1/2):89?107.
Gregory Grefenstette. 2002. Multilingual corpus-
based extraction and the very large lexicon. In Lars
Borin, editor, Language and Computers, Parallel
corpora, parallel worlds, pages 137?149. Rodopi.
John S. Justeson and Slava M. Katz. 1995. Techninal
terminology: some linguistic properties and an al-
gorithm for identification in text. Natural Language
Engineering, 1(1):9?27.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Joint COLING-ACL-98, pages
768?774, Montreal.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Alan Partington. 1998. Patterns and meanings: using
corpora for English language research and teach-
ing. John Benjamins, Amsterdam.
Emmanuel Planas and Osamu Furuse. 2000. Multi-
level similar segment matching algorithm for trans-
lation memories and example-based machine trans-
lation. In COLING, 18th International Conference
on Computational Linguistics, pages 621?627.
Reinhard Rapp. 2004. A freely available automatically
generated thesaurus of related words. In Proceed-
ings of the Forth Language Resources and Evalua-
tion Conference, LREC 2004, pages 395?398, Lis-
bon.
Paul Rayson, Dawn Archer, Scott Piao, and Tony
McEnery. 2004. The UCREL semantic analysis
system. In Proc. Beyond Named Entity Recognition
Workshop in association with LREC 2004, pages 7?
12, Lisbon.
Serge Sharoff. 2006. Creating general-purpose
corpora using automated search engine queries.
In Marco Baroni and Silvia Bernardini, editors,
WaCky! Working papers on the Web as Corpus.
Gedit, Bologna.
A.D. Shveitser. 1988. ?????? ????????: ??????, ???-
?????, ???????. Nauka, Moskow. (In Russian:
Theory of Translation: Status, Problems, Aspects).
Krista Varantola. 2003. Translators and disposable
corpora. In Federico Zanettin, Silvia Bernardini,
and Dominic Stewart, editors, Corpora in Transla-
tor Education, pages 55?70. St Jerome, Manchester.
746
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 136?143,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Assisting Translators in Indirect Lexical Transfer 
Bogdan Babych, Anthony Hartley, Serge Sharoff
  Centre for Translation Studies 
  University of Leeds, UK 
{b.babych,a.hartley,s.sharoff}@leeds.ac.uk
Olga Mudraya 
  Department of Linguistics 
  Lancaster University, UK 
 o.mudraya@lancs.ac.uk 
Abstract 
We present the design and evaluation of a 
translator?s amenuensis that uses compa-
rable corpora to propose and rank non-
literal solutions to the translation of expres-
sions from the general lexicon. Using dis-
tributional similarity and bilingual diction-
aries, the method outperforms established 
techniques for extracting translation 
equivalents from parallel corpora. The in-
terface to the system is available at: 
http://corpus.leeds.ac.uk/assist/v05/  
1 Introduction 
This paper describes a system designed to assist 
humans in translating expressions that do not nec-
essarily have a literal or compositional equivalent 
in the target language (TL). In the spirit of (Kay, 
1997), it is intended as a translator's amenuensis 
"under the tight control of a human translator ? to 
help increase his productivity and not to supplant him". 
One area where human translators particularly 
appreciate assistance is in the translation of expres-
sions from the general lexicon. Unlike equivalent 
technical terms, which generally share the same 
part-of-speech (POS) across languages and are in 
the ideal case univocal, the contextually appropri-
ate equivalents of general language expressions are 
often indirect and open to variation. While the 
transfer module in RBMT may acceptably under-
generate through a many-to-one mapping between 
source and target expressions, human translators, 
even in non-literary fields, value legitimate varia-
tion. Thus the French expression il faillit ?chouer 
(lit.: he faltered to fail) may be variously rendered 
as he almost/nearly/all but failed; he was on the 
verge/brink of failing/failure; failure loomed. All 
of these translations are indirect in that they in-
volve lexical shifts or POS transformations. 
Finding such translations is a hard task that can 
benefit from automated assistance. 'Mining' such 
indirect equivalents is difficult, precisely because 
of the structural mismatch, but also because of the 
paucity of suitable aligned corpora. The approach 
adopted here includes the use of comparable cor-
pora in source and target languages, which are 
relatively easy to create. The challenge is to gener-
ate a list of usable solutions and to rank them such 
that the best are at the top. 
Thus the present system is unlike SMT (Och and 
Ney, 2003), where lexical selection is effected by a 
translation model based on aligned, parallel cor-
pora, but the novel techniques it has developed are 
exploitable in the SMT paradigm. It also differs 
from now traditional uses of comparable corpora 
for detecting translation equivalents (Rapp, 1999) 
or extracting terminology (Grefenstette, 2002), 
which allows a one-to-one correspondence irre-
spective of the context. Our system addresses diffi-
culties in expressions in the general lexicon, whose 
translation is context-dependent. 
The structure of the paper is as follows. In Sec-
tion 2 we present the method we use for mining 
translation equivalents. In Section 3 we present the 
results of an objective evaluation of the quality of 
suggestions produced by the system by comparing 
our output against a parallel corpus. Finally, in 
Section 4 we present a subjective evaluation focus-
ing on the integration of the system into the work-
flow of human translators. 
2 Methodology 
The software acts as a decision support system for 
translators. It integrates different technologies for 
136
extracting indirect translation equivalents from 
large comparable corpora. In the following subsec-
tions we give the user perspective on the system 
and describe the methodology underlying each of 
its sub-tasks. 
2.1 User perspective 
Unlike traditional dictionaries, the system is a 
dynamic translation resource in that it can success-
fully find translation equivalents for units which 
have not been stored in advance, even for idiosyn-
cratic multiword expressions which almost cer-
tainly will not figure in a dictionary. While our 
system can rectify gaps and omissions in static 
lexicographical resources, its major advantage is 
that it is able to cope with an open set of transla-
tion problems, searching for translation equivalents 
in comparable corpora in runtime. This makes it 
more than just an extended dictionary. 
Contextual descriptors 
From the user perspective the system extracts indi-
rect translation equivalents as sets of contextual 
descriptors ? content words that are lexically cen-
tral in a given sentence, phrase or construction. 
The choice of these descriptors may determine the 
general syntactic perspective of the sentence and 
the use of supporting lexical items. Many transla-
tion problems arise from the fact that the mapping 
between such descriptors is not straightforward. 
The system is designed to find possible indirect 
mappings between sets of descriptors and to verify 
the acceptability of the mapping into the TL. For 
example, in the following Russian sentence, the 
bolded contextual descriptors require indirect 
translation into English. 
???? ???????? ????? ???????????-
?????? ?????, ? ??????? ????????? 
?????? ???????????? 
(Children attend badly repaired schools, in 
which [it] is missing the most necessary) 
Combining direct translation equivalents of 
these words (e.g., translations found in the Oxford 
Russian Dictionary ? ORD) may produce a non-
natural English sentence, like the literal translation 
given above. In such cases human translators usu-
ally apply structural and lexical transformations, 
for instance changing the descriptors? POS and/or 
replacing them with near-synonyms which fit to-
gether in the context of a TL sentence (Munday, 
2001: 57-58). Thus, a structural transformation of 
????? ????????????????? (badly repaired) may 
give in poor repair while a lexical transformation 
of ????????? ?????? ???????????? ([it] is missing 
the most necessary) gives lacking basic essentials. 
Our system models such transformations of the 
descriptors and checks the consistency of the re-
sulting sets in the TL. 
Using the system 
Human translators submit queries in the form of 
one or more SL descriptors which in their opinion 
may require indirect translation. When the transla-
tors use the system for translating into their native 
language, the returned descriptors are usually suf-
ficient for them to produce a correct TL construc-
tion or phrase around them (even though the de-
scriptors do not always form a naturally sounding 
expression). When the translators work into a non-
native language, they often find it useful to gener-
ate concordances for the returned descriptors to 
verify their usage within TL constructions. 
For example, for the sentence above translators 
may submit two queries: ????? ????????-
????????? (badly repaired) and ????????? 
???????????? (missing necessary). For the first 
query the system returns a list of descriptor pairs 
(with information on their frequency in the English 
corpus) ranked by distributional proximity to the 
original query, which we explain in Section 2.2. At 
the top of the list come: 
bad repair = 30  (11.005) 
bad maintenance = 16  (5.301) 
bad restoration = 2  (5.079) 
poor repair = 60  (5.026)? 
Underlined hyperlinks lead translators to actual 
contexts in the English corpus, e.g., poor repair 
generates a concordance containing a desirable TL 
construction which is a structural transformation of 
the SL query: 
in such a poor state of repair 
bridge in as poor a state of repair as the highways 
building in poor repair. 
dwellings are in poor repair; 
Similarly, the result of the second query may 
give the translators an idea about possible lexical 
transformation: 
missing need = 14  (5.035) 
important missing = 8 (2.930) 
missing vital = 8  (2.322) 
lack necessary = 204  (1.982)? 
essential lack = 86  (0.908)? 
137
The concordance for the last pair of descriptors 
contains the phrase they lack the three essentials, 
which illustrates the transformation. The resulting 
translation may be the following: 
Children attend schools that are in poor re-
pair and lacking basic essentials 
Thus our system supports translators in making 
decisions about indirect translation equivalents in a 
number of ways: it suggests possible structural and 
lexical transformations for contextual descriptors; 
it verifies which translation variants co-occur in 
the TL corpus; and it illustrates the use of the 
transformed TL lexical descriptors in actual con-
texts. 
2.2 Generating translation equivalents 
We have generalised the method used in our previ-
ous study (Sharoff et al, 2006) for extracting 
equivalents for continuous multiword expressions 
(MWEs). Essentially, the method expands the 
search space for each word and its dictionary trans-
lations with entries from automatically computed 
thesauri, and then checks which combinations are 
possible in target corpora. These potential transla-
tion equivalents are then ranked by their similarity 
to the original query and presented to the user. The 
range of retrievable equivalents is now extended 
from a relatively limited range of two-word con-
structions which mirror POS categories in SL and 
TL to a much wider set of co-occurring lexical 
content items, which may appear in a different or-
der, at some distance from each other, and belong 
to different POS categories.  
The method works best for expressions from the 
general lexicon, which do not have established 
equivalents, but not yet for terminology. It relies 
on a high-quality bilingual dictionary (en-ru ~30k, 
ru-en ~50K words, combining ORD and the core 
part of Multitran) and large comparable corpora 
(~200M En, ~70M Ru) of news texts. 
For each of the SL query terms q the system 
generates its dictionary translation Tr(q) and its 
similarity class S(q) ? a set of words with a similar 
distribution in a monolingual corpus. Similarity is 
measured as the cosine between collocation vec-
tors, whose dimensionality is reduced by SVD us-
ing the implementation by Rapp (2004). The de-
scriptor and each word in the similarity class are 
then translated into the TL using ORD or the Mul-
titran dictionary, resulting in {Tr(q)? Tr(S(q))}. 
On the TL side we also generate similarity classes, 
but only for dictionary translations of query terms 
Tr(q) (not for Tr(S(q)), which can make output too 
noisy). We refer to the resulting set of TL words as 
a translation class T.  
T = {Tr(q) ? Tr(S(q)) ? S(Tr(q))} 
Translation classes approximate lexical and 
structural transformations which can potentially be 
applied to each of the query terms. Automatically 
computed similarity classes do not require re-
sources like WordNet, and they are much more 
suitable for modelling translation transformations, 
since they often contain a wider range of words of 
different POS which share the same context, e.g., 
the similarity class of the word lack contains words 
such as absence, insufficient, inadequate, lost, 
shortage, failure, paucity, poor, weakness, inabil-
ity, need. This clearly goes beyond the range of 
traditional thesauri. 
For multiword queries, the system performs a 
consistency check on possible combinations of 
words from different translation classes. In particu-
lar, it computes the Cartesian product for pairs of 
translation classes T1 and T2 to generate the set P 
of word pairs, where each word (w1 and w2) comes 
from a different translation class: 
P = T1 ? T2 = {(w1, w2) | w1 ? T1 and w2 ? T2}  
Then the system checks whether each word pair 
from the set P exists in the database D of discon-
tinuous content word bi-grams which actually co-
occur in the TL corpus: 
P? = P ? D 
The database contains the set of all bi-grams that 
occur in the corpus with a frequency ? 4 within a 
window of 5 words (over 9M bigrams for each 
language). The bi-grams in D and in P are sorted 
alphabetically, so their order in the query is not 
important. 
Larger N-grams (N > 2) in queries are split into 
combinations of bi-grams, which we found to be 
an optimal solution to the problem of the scarcity 
of higher order N-grams in the corpus. Thus, for 
the query gain significant importance the system 
generates P?1(significant importance), P?2(gain impor-
tance), P?3(gain significant) and computes P? as:  
P? = {(w1,w2,w3)| (w1,w2) ? P?1 & (w1, w3) ? P?2 
& (w2,w3) ? P?3 }, 
which allows the system to find an indirect equiva-
lent ???????? ??????? ???????? (lit.: receive 
weighty meaning). 
138
Even though P? on average contains about 2% - 
4% of the theoretically possible number of bi-
grams present in P, the returned number of poten-
tial translation equivalents may still be large and 
contain much noise. Typically there are several 
hundred elements in P?, of which only a few are 
really useful for translation. To make the system 
usable in practice, i.e., to get useful solutions to 
appear close to the top (preferably on the first 
screen of the output), we developed methods of 
ranking and filtering the returned TL contextual 
descriptor pairs, which we present in the following 
sections. 
2.3 Hypothesis ranking 
The system ranks the returned list of contextual 
descriptors by their distributional proximity to the 
original query, i.e. it uses scores cos(vq, vw) gener-
ated for words in similarity classes ? the cosine of 
the angle between the collocation vector for a word 
and the collocation vector for the query or diction-
ary translation of the query. Thus, words whose 
equivalents show similar usage in a comparable 
corpus receive the highest scores. These scores are 
computed for each individual word in the output, 
so there are several ways to combine them to 
weight words in translation classes and word com-
binations in the returned list of descriptors.  
We established experimentally that the best way 
to combine similarity scores is to multiply weights 
W(T) computed for each word within its translation 
class T. The weight W(P?(w1,w2)) for each pair of 
contextual descriptors (w1, w2)?P? is computed as: 
W(P?(w1,w2)) = W(T(w1)) ? W(T(w2)); 
Computing W(T(w)), however, is not straightfor-
ward either, since some words in similarity classes 
of different translation equivalents for the query 
term may be the same, or different words from the 
similarity class of the original query may have the 
same translation. Therefore, a word w within a 
translation class may have come by several routes 
simultaneously, and may have done that several 
times. For each word w in T there is a possibility 
that it arrived in T either because it is in Tr(q) or 
occurs   n times in Tr(S(q)) or k times in S(Tr(q)). 
We found that the number of occurrences n and 
k of each word w in each subset gives valuable in-
formation for ranking translation candidates. In our 
experiments we computed the weight W(T) as the 
sum of similarity scores which w receives in each 
of the subsets. We also discovered that ranking 
improves if for each query term we compute in 
addition a larger (and potentially noisy) space of 
candidates that includes TL similarity classes of 
translations of the SL similarity class S(Tr(S(q))). 
These candidates do not appear in the system out-
put, but they play an important role in ranking the 
displayed candidates. The improvement may be 
due to the fact that this space is much larger, and 
may better support relevant candidates since there 
is a greater chance that appropriate indirect equiva-
lents are found several times within SL and TL 
similarity classes. The best ranking results were 
achieved when the original W(T) scores were mul-
tiplied by 2 and added to the scores for the newly 
introduced similarity space S(Tr(S(q))): 
W(T(w))= 2?(1 if w?Tr(q) )+  
2??( cos(vq, vTr(w)) | {w | w? Tr(S(q)) } ) +  
2??( cos(vTr(q), vw) | {w | w? S(Tr(q)) } ) + 
?(cos(vq, vTr(w))?cos (vTr(q), vw) |  
{w | w? S(Tr(S(q))) } ) 
For example, the system gives the following 
ranking for the indirect translation equivalents of 
the Russian phrase ??????? ???????? (lit.: weighty 
meaning) ? figures in brackets represent W(P?) 
scores for each pair of TL descriptors: 
1. significant importance = 7 (3.610)  
2. significant value = 128    (3.211)  
3. measurable value = 6       (2.657)?  
8. dramatic importance = 2    (2.028)  
9. important significant = 70 (2.014)  
10. convincing importance = 6 (1.843) 
The Russian similarity class for ??????? 
(weighty, ponderous) contains: ???????????? 
(convincing) (0.469), ???????? (significant) 
(0.461), ???????? (notable) (0.452) ?????-
?????? (dramatic) (0.371). The equivalent of 
significant is not at the top of the similarity class of 
the Russian query, but it appears at the top of the 
final ranking of pairs in P?, because this hypothesis 
is supported by elements of the set formed by 
S(Tr(S(q))); it appears in similarity classes for no-
table (0.353) and dramatic (0.315), which contrib-
uted these values to the W(T) score of significant: 
W(T(significant)) = 
    2 ? (Tr(????????)=significant (0.461))  
+ (Tr(????????)=notable (0.452)  
  ? S(notable)=significant (0.353)) 
+ (Tr(???????????)=dramatic (0.371)  
  ? S(dramatic)= significant (0.315)) 
The word dramatic itself is not usable as a 
translation equivalent in this case, but its similarity 
139
class contains the support for relevant candidates, 
so it can be viewed as useful noise. On the other 
hand, the word convincing does not receive such 
support from the hypothesis space, even though its 
Russian equivalent is ranked higher in the SL simi-
larity class. 
2.4 Semantic filtering 
Ranking of translation candidates can be further 
improved when translators use an option to filter 
the returned list by certain lexical criteria, e.g., to 
display only those examples that contain a certain 
lexical item, or to require one of the items to be a 
dictionary translation of the query term. However, 
lexical filtering is often too restrictive: in many 
cases translators need to see a number of related 
words from the same semantic field or subject do-
main, without knowing the lexical items in ad-
vance. In this section we present the semantic fil-
ter, which is based on Russian and English seman-
tic taggers which use the same semantic field tax-
onomy for both languages. 
The semantic filter displays only those items 
which have specified semantic field tags or tag 
combinations; it can be applied to one or both 
words in each translation hypothesis in P?. The 
default setting for the semantic filter is the re-
quirement for both words in the resulting TL can-
didates to contain any of the semantic field tags 
from a SL query term. 
In the next section we present evaluation results 
for this default setting (which is applied when the 
user clicks the Semantic Filter button), but human 
translators have further options ? to filter by tags 
of individual words, to use semantic classes from 
SL or TL terms, etc. 
For example, applying the default semantic filter 
for the output of the query ????? ???????-
?????????? (badly repaired) removes the high-
lighted items from the list: 
 1. bad repair = 30       (11.005)  
[2. good repair = 154     (8.884) ] 
 3. bad rebuild = 6       (5.920)  
[4. bad maintenance = 16  (5.301) ] 
 5. bad restoration = 2   (5.079)  
 6. poor repair = 60      (5.026)  
[7. good rebuild = 38     (4.779) ] 
 8. bad construction = 14 (4.779)  
Items 2 and 7 are generated by the system be-
cause good, well and bad are in the same similar-
ity cluster for many words (they often share the 
same collocations). The semantic filter removes 
examples with good and well on the grounds that 
they do not have any of the tags which come from 
the word ????? (badly): in particular, instead of 
tag A5? (Evaluation: Negative) they have tag A5+ 
(Evaluation: Positive). Item 4 is removed on the 
grounds that the words ????????????????? 
(repaired) and maintenance do not have any tags 
in common ? they appear ontologically too far 
apart from the point of view of the semantic tagger. 
The core of the system?s multilingual semantic 
tagging is a knowledge base in which single words 
and MWEs are mapped to their potential semantic 
field categories. Often a lexical item is mapped to 
multiple semantic categories, reflecting its poten-
tial multiple senses. In such cases, the tags are ar-
ranged by the order of likelihood of meanings, 
with the most prominent first. 
3 Objective evaluation 
In the objective evaluation we tested the perform-
ance of our system on a selection of indirect trans-
lation problems, extracted from a parallel corpus 
consisting mostly of articles from English and 
Russian newspapers (118,497 words in the R-E 
direction, 589,055 words in the E-R direction). It 
has been aligned on the sentence level by JAPA 
(Langlais et al, 1998), and further on the word 
level by GIZA++ (Och and Ney, 2003). 
3.1 Comparative performance 
The intuition behind the objective evaluation 
experiment is that the capacity of our tool to find 
indirect translation equivalents in comparable cor-
pora can be compared with the results of automatic 
alignment of parallel texts used in translation mod-
els in SMT: one of the major advantages of the 
SMT paradigm is its ability to reuse indirect 
equivalents found in parallel corpora (equivalents 
that may never come up in hand-crafted dictionar-
ies). Thus, automatically generated GIZA++ dic-
tionaries with word alignment contain many exam-
ples of indirect translation equivalents. 
We use these dictionaries to simulate the genera-
tor of translation classes T, which we recombine to 
construct their Cartesian product P, similarly to the 
procedure we use to generate the output of our sys-
tem. However, the two approaches generate indi-
rect translation equivalence hypotheses on the ba-
sis of radically different material: the GIZA dic-
tionary uses evidence from parallel corpora of ex-
140
isting human translations, while our system re-
combines translation candidates on the basis of 
their distributional similarity in monolingual com-
parable corpora. Therefore we took GIZA as a 
baseline. 
Translation problems for the objective evalua-
tion experiment were manually extracted from two 
parallel corpora: a section of about 10,000 words 
of a corpus of English and Russian newspapers, 
which we also used to train GIZA, and a section of 
the same length from a corpus of interviews pub-
lished on the Euronews.net website. 
We selected expressions which represented 
cases of lexical transformations (as illustrated in 
Section 0), containing at least two content words 
both in the SL and TL. These expressions were 
converted into pairs of contextual descriptors ? 
e.g., recent success, reflect success ? and submit-
ted to the system and to the GIZA dictionary. We 
compared the ability of our system and of GIZA to 
find indirect translation equivalents which matched 
the equivalents used by human translators. The 
output from both systems was checked to see 
whether it contained the contextual descriptors 
used by human translators. We submitted 388 pairs 
of descriptors extracted from the newspaper trans-
lation corpus and 174 pairs extracted from the Eu-
ronews interview corpus. Half of these pairs were 
Russian, and the other half English. 
We computed recall figures for 2-word combi-
nations of contextual descriptors and single de-
scriptors within those combinations. We also show 
the recall of translation variants provided by the 
ORD on this data set. For example, for the query 
????????? ???????????? ([it] is missing neces-
sary [things]) human translators give the solution 
lacking essentials; the lemmatised descriptors are 
lack and essential. ORD returns direct translation 
equivalents missing and necessary. The GIZA dic-
tionary in addition contains several translation 
equivalents for the second term (with alignment 
probabilities) including: necessary ~0.332, need 
~0.226, essential ~0.023. Our system returns both 
descriptors used in human translation as a pair ? 
lack essential (ranked 41 without filtering and 22 
with the default semantic filter). Thus, for a 2-word 
combination of the descriptors only the output of 
our system matched the human solution, which we 
counted as one hit for the system and no hits for 
ORD or GIZA. For 1-word descriptors we counted 
2 hits for our system (both words in the human 
solution are matched), and 1 hit for GIZA ? it 
matches the word essential ~0.023 (which also il-
lustrates its ability to find indirect translation 
equivalents). 
 2w descriptors 1w descriptors 
 news interv news interv 
ORD 6.7% 4.6% 32.9% 29.3% 
GIZA++ 13.9% 3.4% 35.6% 29.0%
Our system 21.9% 19.5% 55.8% 49.4%
Table 1 Conservative estimate of recall 
It can be seen from Table 1 that for the newspa-
per corpus on which it was trained, GIZA covers a 
wider set of indirect translation variants than ORD. 
But our recall is even better both for 2-word and 1-
word descriptors. 
However, note that GIZA?s ability to retrieve 
from the newspaper corpus certain indirect transla-
tion equivalents may be due to the fact that it has 
previously seen them frequently enough to gener-
ate a correct alignment and the corresponding dic-
tionary entry. 
The Euronews interview corpus was not used for 
training GIZA. It represents spoken language and 
is expected to contain more ?radical? transforma-
tions. The small decline in ORD figures here can 
be attributed to the fact that there is a difference in 
genre between written and spoken texts and conse-
quently between transformation types in them. 
However, the performance of GIZA drops radi-
cally on unseen text and becomes approximately 
the same as the ORD. 
This shows that indirect translation equivalents 
in the parallel corpus used for training GIZA are 
too sparse to be learnt one by one and successfully 
applied to unseen data, since solutions which fit 
one context do not necessarily suit others. 
The performance of our system stays at about 
the same level for this new type of text; the decline 
in its performance is comparable to the decline in 
ORD figures, and can again be explained by the 
differences in genre. 
3.2 Evaluation of hypothesis ranking 
As we mentioned, correct ranking of translation 
candidates improves the usability of the system. 
Again, the objective evaluation experiment gives 
only a conservative estimate of ranking, because 
there may be many more useful indirect solutions 
further up the list in the output of the system which 
are legitimate variants of the solutions found in the 
141
parallel corpus. Therefore, evaluation figures 
should be interpreted in a comparative rather then 
an absolute sense. 
We use ranking by frequency as a baseline for 
comparing the ranking described in Section 2.3 ? 
by distributional similarity between a candidate 
and the original query. 
Table 2 shows the average rank of human solu-
tions found in parallel corpora and the recall of 
these solutions for the top 300 examples. Since 
there are no substantial differences between the 
figures for the newspaper texts and for the inter-
views, we report the results jointly for 556 transla-
tion problems in both selections (lower rank fig-
ures are better). 
 Recall Average rank 
2-word descriptors 
frequency (baseline) 16.7% rank=93.7
distributional similarity 19.5% rank=44.4
sim. + semantic filter 14.4% rank=26.7
1-word descriptors 
frequency (baseline) 48.2% rank=42.7
distributional similarity 52.8% rank=21.6
sim. + semantic filter 44.1% rank=11.3
Table 2 Ranking: frequency, similarity and filter 
It can be seen from the table that ranking by 
similarity yields almost a twofold improvement for 
the average rank figures compared to the baseline. 
There is also a small improvement in recall, since 
there are more relevant examples that appear 
within the top 300 entries. 
The semantic filter once again gives an almost 
twofold improvement in ranking, since it removes 
many noisy items. The average is now within the 
top 30 items, which means that there is a high 
chance that a translation solution will be displayed 
on the first screen. The price for improved ranking 
is decline in recall, since it may remove some rele-
vant lexical transformations if they appear to be 
ontologically too far apart. But the decline is 
smaller: about 26.2% for 2-word descriptors and 
16.5% for 1-word descriptors. The semantic filter 
is an optional tool, which can be used to great ef-
fect on noisy output: its improvement of ranking 
outweighs the decline in recall. 
Note that the distribution of ranks is not normal, 
so in Figure 1 we present frequency polygons for 
rank groups of 30 (which is the number of items 
that fit on a single screen, i.e., the number of items 
in the first group (r030) shows solutions that will 
be displayed on the first screen). The majority of 
solutions ranked by similarity appear high in the 
list (in fact, on the first two or three screens). 
0
10
20
30
40
50
60
70
r0
30
r0
60
r0
90
r1
20
r1
50
r1
80
r2
10
r2
40
r2
70
r3
00
similarity
frequency
 
Figure 1 Frequency polygons for ranks 
4 Subjective evaluation 
The objective evaluation reported above uses a 
single reference translation and is correspondingly 
conservative in estimating the coverage of the sys-
tem. However, many expressions studied have 
more than one fluent translation. For instance, in 
poor repair is not the only equivalent for the Rus-
sian expression ????? ?????????????????. It is 
also possible to translate it as unsatisfactory condi-
tion, bad state of repair, badly in need of repair, 
and so on. The objective evaluation shows that the 
system has been able to find the suggestion used 
by a particular translator for the problem studied. It 
does not tell us whether the system has found some 
other translations suitable for the context. Such 
legitimate translation variation implies that the per-
formance of a system should be studied on the ba-
sis of multiple reference translations, though typi-
cally just two reference translations are used (Pap-
ineni, et al 2001). This might be enough for the 
purposes of a fully automatic MT tool, but in the 
context of a translator's amanuensis which deals 
with expressions difficult for human translators, it 
is reasonable to work with a larger range of ac-
ceptable target expressions. 
With this in mind we evaluated the performance 
of the tool with a panel of 12 professional transla-
tors. Problematic expressions were highlighted and 
the translators were asked to find suitable sugges-
tions produced by the tool for these expressions 
and rank their usability on a scale from 1 to 5 (not 
acceptable to fully idiomatic, so 1 means that no 
usable translation was found at all). 
Sentences themselves were selected from prob-
lems discussed on professional translation forums 
proz.com and forum.lingvo.ru. Given the range of 
corpora used in the system (reference and newspa-
142
per corpora), the examples were filtered to address 
expressions used in newspapers. 
The goal of the subjective evaluation experiment 
was to establish the usefulness of the system for 
translators beyond the conservative estimate given 
by the objective evaluation. The intuition behind 
the experiment is that if there are several admissi-
ble translations for the SL contextual descriptors, 
and system output matches any of these solutions, 
then the system has generated something useful. 
Therefore, we computed recall on sets of human 
solutions rather than on individual solutions. We 
matched 210 different human solutions to 36 trans-
lation problems. To compute more realistic recall 
figures, we counted cases when the system output 
matches any of the human solutions in the set. 
Table 3 compares the conservative estimate of the 
objective evaluation and the more realistic estimate 
on a single data set. 
 2w default 2w with sem filt 
Conservative  32.4%; r=53.68 21.9%; r=34.67 
Realistic 75.0%;   r=7.48 61.1%;   r=3.95 
Table 3 Recall and rank for 2-word descriptors 
Since the data set is different, the figures for the 
conservative estimate are higher than those for the 
objective evaluation data set. However, the table 
shows the there is a gap between the conservative 
estimate and the realistic coverage of the transla-
tion problems by the system, and that real coverage 
of indirect translation equivalents is potentially 
much higher. 
Table 4 shows averages (and standard deviation 
?) of the usability scores divided in four groups: (1) 
solutions that are found both by our system and the 
ORD; (2) solutions found only by our system; (3) 
solutions found only by ORD (4) solutions found 
by neither: 
 system (+) system (?) 
ORD (+) 4.03 (0.42) 3.62 (0.89)
ORD (?) 4.25 (0.79) 3.15 (1.15)
Table 4 Human scores and ? for system output 
It can be seen from the table that human users find 
the system most useful for those problems where 
the solution does not match any of the direct dic-
tionary equivalents, but is generated by the system. 
5 Conclusions 
We have presented a method of finding indirect 
translation equivalents in comparable corpora, and 
integrated it into a system which assists translators 
in indirect lexical transfer. The method outper-
forms established methods of extracting indirect 
translation equivalents from parallel corpora. 
We can interpret these results as an indication 
that our method, rather than learning individual 
indirect transformations, models the entire family 
of transformations entailed by indirect lexical 
transfer. In other words it learns a translation strat-
egy which is based on the distributional similarity 
of words in a monolingual corpus, and applies this 
strategy to novel, previously unseen examples. 
The coverage of the tool and additional filtering 
techniques make it useful for professional transla-
tors in automating the search for non-trivial, indi-
rect translation equivalents, especially equivalents 
for multiword expressions. 
References 
Gregory Grefenstette. 2002. Multilingual corpus-based 
extraction and the very large lexicon. In: Lars Borin, 
editor, Language and Computers, Parallel corpora, 
parallel worlds, pages 137-149. Rodopi. 
Martin Kay. 1997. The proper place of men and ma-
chines in language translation. Machine Translation, 
12(1-2):3-23. 
Philippe Langlais, Michel Simard, and Jean V?ronis. 
1998. Methods and practical issues in evaluating 
alignment techniques. In Proc. Joint COLING-ACL-
98, pages 711-717. 
Jeremy Munday. 2001. Introducing translation studies. 
Theories and Applications. Routledge, New York. 
Franz Josef Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1):19-51. 
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. 
(2001). Bleu: a method for automatic evaluation of 
machine translation, RC22176 W0109-022: IBM. 
Reinhard Rapp. 1999. Automatic identification of word 
translations from unrelated English and German cor-
pora. In Procs. the 37th ACL, pages 395-398. 
Reinhard Rapp. 2004. A freely available automatically 
generated thesaurus of related words. In Procs. LREC 
2004, pages 395-398, Lisbon. 
Serge Sharoff, Bogdan Babych and Anthony Hartley 
2006. Using Comparable Corpora to Solve Problems 
Difficult for Human Translators. In: Proceedings of 
the COLING/ACL 2006 Main Conference Poster 
Sessions, pp. 739-746. 
143
Work-in-Progress  project report : CESTA - Machine Translation Evaluation 
Campaign 
 
Widad Mustafa El Hadi 
IDIST / CERSATES 
Universit? de Lille 3 
Domaine universitaire 
du "Pont de Bois" 
rue du Barreau 
BP 149 
59653 Villeneuve d'Ascq 
Cedex - France 
mustafa@univ-lille3.fr  
Marianne Dabbadie 
IDIST / CERSATES 
Universit? de Lille 3 
Domaine universitaire 
du "Pont de Bois" 
rue du Barreau 
BP 149 
59653 Villeneuve d'Ascq 
Cedex - France 
dabbadie@univ-lille3.fr 
Isma?l Timimi 
IDIST / CERSATES 
Universit? de Lille 3 
Domaine universitaire 
du "Pont de Bois" 
rue du Barreau 
BP 149 
59653 Villeneuve d'Ascq 
Cedex - France 
timimi@univ-lille3.fr  
Martin Rajman 
LIA 
Ecole 
Polytechnique 
F?d?rale de Lausanne 
B?t. INR 
CH-1015 Lausanne 
Switzerland 
martin.rajman@epfl.ch 
 
 
Philippe Langlais 
RALI / DIRO - 
Universit? de Montr?al 
C.P. 6128, 
succursale Centre-ville 
Montr?al (Qu?bec) - 
Canada, H3C 3J7 
felipe@IRO.UMontreal.
CA
Antony Hartley 
University of Leeds 
Centre for Translation 
Studies 
Woodhouse Lane 
LEEDS LS2 9JT 
UK 
a.hartley@leeds.ac.uk
Andrei Popescu Belis 
University of Geneva 40 
bvd du Pont d'Arve CH-
1211 Geneva 4 
Switzerland 
Andrei.Popescu-
Belis@issco.unige.ch  
 
Abstract 
CESTA, the first European Campaign 
dedicated to MT Evaluation, is a project 
labelled by the French Technolangue action. 
CESTA provides an evaluation of six 
commercial and academic MT systems using a 
protocol set by an international panel of 
experts. CESTA aims at producing reusable 
resources and information about reliability of 
the metrics. Two runs will be carried out: one 
using the system?s basic dictionary, another 
after terminological adaptation. Evaluation 
task, test material, resources, evaluation 
measures, metrics, will be detailed in the full 
paper. The protocol is the combination of a 
contrastive reference to: IBM ?BLEU? 
protocol (Papineni, K., S. Roukos, T. Ward 
and Z. Wei-Jing, 2001); ?BLANC? protocol 
derived from (Hartley, Rajman, 2002).; 
?ROUGE? protocol (Babych, Hartley, Atwell, 
2003). The results of the campaign will be 
published in a final report and be the object of 
two intermediary and final workshops. 
1 Introduction 
1.1 CESTA and the Technolangue Action in 
France 
 
This article is a collective paper written by the 
CESTA scientific committee that aims at 
presenting the CESTA evaluation campaign, a 
project labelled in 2002 by the French Ministry of 
Research and Education within the framework of 
the Technolangue call for projects and integrated 
to the EVALDA evaluation platform. It reports 
work in progress and therefore is the description of 
an on-going campaign for which system results are 
not yet available.  
 
In France, EVALDA is the new Evaluation 
platform, a joint venture between the French 
Ministry of Research and Technology and ELRA 
(European Language Resources and Evaluation 
Association, Paris, France). Within the framework 
of this initiative eight evaluation projets are being 
conducted:  ARCADE II: campagne d??valuation 
de l?alignement de corpus multilingues; CESART:
 campagne d'Evaluation de Syst?mes 
d?Acquisition de Ressources Terminologiques; 
CESTA : campagne d'Evaluation de Syst?mes de 
Traduction automatique; Easy: Evaluation des 
Analyseurs Syntaxiques du fran?ais; Campagne 
EQueR, Evaluation en question-r?ponse; 
Campagne ESTER, Evaluation de transcriptions 
d??missions radio; Campagne EvaSY, Evaluation 
en synth?se vocale; and Campagne MEDIA, 
Evaluation du dialogue hors et en contexte. 
 
Regarding evaluation, the objectives of the 
Action as Joseph Mariani pointed out in his 
presentation at the LREC 2002 conference are to: 
? Improve the present evaluation 
methodologies  
? Identify new (quantitative and qualitative) 
approaches for already evaluated 
technologies:  socio-technical and psycho-
cognitive aspects  
? Identify protocols for new technologies 
and applications  
? Identification of language resources 
relevant for evaluation (to promote the 
development of new linguistic resources 
for those languages and domains where 
they do not exist yet, or only exist in a 
prototype stage, or exist but cannot be 
made available to the interested users); 
 
The object of the CESTA campaign is twofold. 
It is on the one hand to provide an evaluation of 
commercial Machine Translation Systems and on 
the other hand, to work collectively on the setting 
of a new reusable Machine Translation Evaluation 
protocol that is both user oriented and accounts for 
the necessity to use semantic metrics in order to 
make available a high quality reusable machine 
translation protocol to system providers.  
 
1.2 Object of the campaign 
The object of the CESTA campaign is to 
evaluate technologies together with metrics, i.e. to 
contribute to the setting of a state of the art within 
the field of Machine Translation systems 
evaluation.  
1.3 CESTA user oriented protocol 
The campaign will last three years, starting 
from January 2003. A board of European 
experts are members of CESTA Scientific 
committee and have been working together in 
order to determine the protocol to use for the 
campaign. Six systems are being evaluated. 
Five of these systems are commercial MT 
systems and one is a prototype developed at 
the university of Montreal by the RALI 
research centre. Evaluation is carried out on 
text rather than sentences. Text approximate 
width will be 400 words. Two runs will be 
carried out. For industrial reasons, systems 
will be made anonymous. 
 
2 State-of-the-art in the field of Machine 
Translation evaluation 
 
In 1966, the ALPAC report draws light on the 
limits of Machine Translation systems. In 1979, 
the Van Slype report presented a study dedicated to 
Machine Translation metrics.  
 
In 1992, the JEIDA campaign puts the user at the 
center of evaluator?s preoccupation. JEIDA 
proposed to draw human measures on the basis of 
three questionnaires: 
? One destined to users (containing a 
hundred questions) 
? Other questionnaires are destined to 
system Machine translation systems 
editors (three different questionnaires),  
? And a set of other questionnaires reserved 
to Machine Translation systems 
developers.  
 
Scores are worked out on the background of 
fourteen categories of questions. From these 
scores, graphs are produced according to the 
answers obtained. A comparison of different 
graphs for each systems is used as a basis for 
systems classification. 
 
The first DARPA Machine Translation 
evaluation campaign (1992-1994) makes use of 
human judgments. It is a very expensive method 
but interesting however, as regards the reliability 
of the evaluation thus produced. This campaign is 
based on tests carried out from French, Spanish 
and Japanese as source languages and English as a 
target language. The measures used for each of the 
following criteria are:  
? Fidelity ? a proximity distance is worked 
out between a source sentence and a target 
sentence on a 1 to 5 scale. 
? Intelligibility, that corresponds to 
linguistic acceptability of a translation is 
measured on a 1 to 5 evaluation scale. 
? Informativeness: the test is carried out on 
reading of the target text alone. A 
questionnaire on text informative content 
is displayed allowing to work out a 
measure calculated on the basis of the 
percentage of good answers provided in 
system translation.  
 
In 1995, the OVUM report proposes to compare 
commercial Machine Translation systems on the 
basis of ten criteria. 
 
In 1996, the EAGLES report (EAGLES, 1999) 
sets new standards for Natural Language 
Processing software evaluation on the background 
of ISO 9126.  
 
Initiated in 1999, and coordinated by Pr Antonio 
Zampolli, the ISLE project is divided into three 
working groups, one being a Machine Translation 
group.  
 
Starting from ISO 9126 standard (King, 1999b), 
the aim of the project is to produce two taxonomies 
(c.f. section 3 of this article) and : 
? One defining quality subcriteria with the 
aim of refining the six criteria defined by 
ISO 9126 (i.e. functionality, reliability, 
user-friendliness, efficiency, maintenance 
portability) 
? The second one specifying use contexts 
that define the type of task induced the use 
of a by Machine Translation system, the 
types of users and input data. This 
taxonomy uses contextual parameters to 
select and order the quality criteria subject 
to evaluation. This taxonomy can be 
viewed and downloaded on the ISSCO 
website at the following address : 
http://www.issco.unige.ch/projects/isle/fe
mti/  
 
The second DARPA campaign (Papineni, K., S. 
Roukos, T. Ward and Z. Wei-Jing, 2001), making 
use of the IBM BLEU metric is mentioned in the 
CESTA protocol (c.f. section 8.1 of this article). 
 
3 User-oriented evaluations 
An emerging evaluation methodology in NLP 
technology focuses on quality requirements 
analysis. The needs and consequently the 
satisfaction of end-users, and this will depend on 
the tasks and expected results requirement 
domains, which we have identified as diagnostic 
quality dimensions. One of the most suitable 
methods in this type of evaluation is the adequacy 
evaluation that aims at finding out whether a 
system or product is adequate to someone?s needs 
(see Sparck-Jones & Gallier, 1996 and King, 1996 
among many others for a more detailed discussion 
of these issues). This approach encourages 
communication between users and developers. 
 
The definition of the CESTA evaluation 
protocol took into account the Framework for 
MT Evaluation in ISLE (FEMTI), available 
online. FEMTI offers the possibility to define 
evaluation requirements, then to select relevant 
'qualities', and the metrics commonly used to 
score them (cf. ISO/IEC 9126, 14598). The 
CESTA evaluation methodology is founded on 
a black box approach.  
 
CESTA evaluators considered a generic user, 
which is interested in general-purpose, ready-
to-use translations, preferably using an off-the-
shelf system. In addition, CESTA aims at 
producing reusable resources, and providing 
information about the reliability of the metrics 
(validation), while being cost-effective and 
fast.  
 
With these evaluation requirements in mind 
(FEMTI-1), it appears that the relevant 
qualities (FEMTI-2) are 'suitability', 'accuracy' 
and 'well-formedness'. Automated metrics best 
meet the CESTA needs for reusability, among 
which BLEU, X-score and D-score (chosen for 
internal reasons). Their validation requires the 
comparison of their scores with recognised 
human scores for the same qualities (e.g., 
human assessment of fidelity or fluency). 
'Efficiency', measured through post-editing 
time, was also discussed. For the evaluation, 
first a general-purpose dictionary could be 
used, then a domain-specific one. 
 
 
3.1 An approach based on use cases 
 
ISO 14598 directives for evaluators put forth as 
a prequisite for systems development the detailed 
identification of user needs that ought to be 
specified through the use case document. 
Moreover, conducting a full evaluation process 
involves going through the establishment of an 
evaluation requirements document. ISO 14598 
document specifies that quality requirements 
should be identified ?according to user needs, 
application area and experience, software integrity 
and experience, regulations, law, required 
standards, etc.?. 
 
The evaluation specification document is created 
using the Software Requirement Specifications 
(SRS) and the Use-Case document. The CESTA 
protocol relies on a use case that refers to a 
translation need grounded on basic syntactic 
correctness and simple understanding of a text, as 
required by information watch tasks for example, 
and excludes making a direct use of the text for 
post editing purposes.  
4 Two campaigns 
4.1 Specificities of the CESTA campaign 
Two campaigns are being organised : 
The first campaign is organised using a system?s 
default dictionary. After systems terminological 
adaptation a second campaign will be organised. 
Two studies previously carried out and presented 
respectively at the 2001 MT Summit (Mustafa El 
Hadi, Dabbadie, Timimi, 2001) and at the 2002 
LREC conference (Mustafa Mustafa El Hadi, 
Dabbadie, Timimi, 2002) allowed us to realise the 
gap in terms in terms of quality between results 
obtained on target text after terminological 
enrichment.  
 
4.2 First campaign 
The organisation of the campaign implies going 
through several steps : 
? Identification of potential participants 
? Original protocol readjustement, 
? The setting of a specific test tool that is 
currently being be implemented in 
conformity with protocol specifications 
validated by CESTA scientific 
committee. CESTA protocol 
specifications have been 
communicated to participants in 
particular as regards data formatting, 
test schedule, metrics and adaptation 
phase. For cost requirements, CESTA 
will not include a training phase. The 
first run will start during autumn 2004 
 
4.3 Second campaign 
The systems having already been tuned, an 
adaptation phase will not be carried out for the 
second campaign. However terminological 
adaptation will be necessary at this stage. The 
second series of tests being carried out on a 
thematically homogeneous corpus, the thematic 
domain only will be communicated to participants 
for terminological adaptation. For thematic  
adaptation, and in order to avoid system 
optimisation after the first series of tests, a new 
domain specific 200.000 word hiding corpus will 
be used.  
 
The terminological domain on which evaluation 
will be carried out will then have to be defined. 
This terminological domain will be communicated 
to participants but not the corpus used itself. On 
the other hand, participants will be asked to send 
organisers a written agreement by which they will 
commit themselves to provide organisers with any 
relevant information regarding system tuning and 
specific adaptations that have made on each of the 
participating MT systems, in order to allow the 
scientific committee to understand and analyse the 
origin of the potential system ranking changes. The 
second run will start during year 2005. 
 
Organisers have committed themselves not to 
publish the results between the two campaigns. 
 
After the training phase, the second campaign 
will take place. Participants will be given a fifteen 
days delay to send the results. An additional three 
months period will be necessary to carry out result 
analysis and prepare data publication and 
workshop organisation.  
 
CESTA scientific committee also decided in 
parallel with the two campaigns, to evaluate 
systems capacity to process formatted texts 
including images and HTML tags. Participants 
who do not wish to participate to this additional 
test have informed the scientific committee. Most 
of the time the reason is that their system is only 
capable of processing raw text. This is the case 
mainly for academic systems involved in the 
campaign, most of the commercial systems being 
nowadays able to process formatted text. 
 
5 Contrastive evaluation 
One of the particularities of the CESTA protocol 
is to provide a Meta evaluation of the automated 
metrics used for the campaign ? a kind of state of 
the art of evaluation metrics. The robustness of the 
metrics will be tested on minor language pairs 
through a contrastive evaluation against human 
judgement.  
 
The scientific committee has decided to use 
Arabic?French as a minor language pair. 
Evaluation on the minor language pair will be 
carried directly on two of the participating systems 
and using English as a pivotal language on the 
other systems. Translation through a pivotal 
language will then be the following : 
Arabic?English?French.  
 
Organiser are, of course, perfectly aware of the 
potential loss of quality provoked by the use of a 
pivotal language but recall however that, contrarily 
to the major language pair, evaluation carried out 
on the minor language pair through a pivotal 
system will not be used to evaluate these systems 
themselves, but metric robustness. Results of 
metric evaluation and systems evaluation will, of 
course, be obtained and disseminated separately. 
 
During the tests of the first campaign, the 
French?English system obtaining the best ranking 
will be selected to be used as a pivotal system for 
metrics robustness Meta evaluation.  
 
6 Test material 
The required material is a set of corpora as 
detailed in the following section and a test tool that 
will be implemented according to metrics 
requirements and under the responsibility of 
CESTA organisers. 
6.1 Corpus 
 
The evaluation corpus is composed of 50 texts, 
each text length is 400 words to be translated 
twice, considering that a translation already exists 
in the original corpus. The different corpora are 
provided by ELRA. The masking corpus has 
250.000 words and must be thematically 
homogeneous.  
 
 
For each language pair the following corpora 
will be used: 
 
Adaptation 
? This 200.000 ? 250.000 word corpus is a 
bilingual corpus. It is used to validate 
exchanges between organisers and 
participants and for system tuning.  
First Campaign 
? One 20.000 word evaluation corpus will be 
used (50 texts of 400 words each) 
? One 200.000 to 250.000 word masking 
corpus that hides the evaluation corpus. 
Second campaign 
? One new 20.000 word corpus will be used 
but it will have to be thematically 
homogeneous (on a specific domain that 
will be communicated to participants a few 
months before the run takes place) 
? One masking corpus similar to the 
previous one. 
 
Additional requirement 
The BLANC metric requires the use of a 
bilingual aligned corpus at document scale. 
 
Three human translations will be used for each 
of the evaluation source texts. Considering that the 
corpora used, already provide one official 
translations, only two additional human 
translations will be necessary. These translations 
will be carried out under the organisers 
responsibility. Within the framework of CESTA 
use cases, evaluation is not made in order to obtain 
a ready to publish target language translation, but 
rather to provide a foreign user a simple access to 
information within the limits of basic grammatical 
correctness, as already mentioned in this article.  
 
7 The BLEU, BLANC and ROUGE metrics 
 
Three types of metrics will be tested on the 
corpus, the CESTA protocol being the combination 
of a contrastive reference to three different 
protocols:  
 
7.1 The IBM ?BLEU? protocol (Papineni, K., 
S. Roukos, T. Ward and Z. Wei-Jing, 
2001). 
 
The IBM BLEU metric used by the DARPA for 
its 2001 evaluation campaign, uses co-occurrence 
measures based on N-Grams. The translation in 
English of 80 Chinese source documents by six 
different commercial Machine Translation 
systems, was submitted to evaluation. From a 
reference corpus of translations made by experts, 
this metric works out quality measures according 
to a distance calculated between an automatically 
produced translation and the reference translation 
corpus based on shared N-grams (n=1,2,3?). The 
results of this evaluation are then compared to 
human judgments. 
? NIST now offers an online evaluation of 
MT systems performance, i.e.:  
o A program that can be 
downloaded for research aims. 
The user then provides source 
texts and reference translations for 
a determined pair of languages. 
o An e-mail evaluation service, for 
more formal evaluations. Results 
can be obtained in a few minutes. 
 
7.2 The ?BLANC? protocol 
 
It is a metric derived from a study presented at 
the LREC 2002 conference (Hartley A., Rajman 
M., 2002). We only take into account a part of the 
protocol described in the referred paper, i.e. the X 
score, that corresponds to grammatical correctness.  
 
We will not give an exhaustive description of 
this experience and shall only detail the elements 
that are relevant to the CESTA evaluation protocol.  
 
The protocol has been tested on the following 
languages.  
? Source language: French 
? Target language: English 
? Source corpus : 100 texts ? domain : 
newspaper articles 
 
Human judgements for comparison referential: 
? 12 English monolingual students.  
? No human translation reference corpus. 
? Three criteria were tested: Fluency, 
Adequacy, Informativeness  
 
Six systems were submitted to evaluation : 
Candide (CD), Globalink (GL), MetalSystem 
(MS), Reverso (RV), Systran (SY), XS (XS) 
? Each of the systems is due to translate a 
hundred source texts ranging from 250 to 
300 words each. A corpus of 600 
translations is thus produced. 
? For each of the source texts, a corpus of 6 
translations is produced automatically. 
These translations are then regrouped by 
series of six texts.  
? According to the protocol initiated by 
(White & Forner, 2001) these series are 
then ranked by medium adequacy score. 
? Every 5 series, a series is extracted from 
the whole. Packs of twenty series of target 
translations are thus obtained and 
submitted to human evaluators.  
 
7.2.1 Evaluators? tasks 
? Each evaluator reads 10 series of 6 
translations i.e. 60 texts.  
? Each of these series is then read by six 
different evaluators 
? The evaluators must observe a ten minute 
compulsory break every two series.  
? The evaluators do not know that the texts 
have been translated automatically. 
 
The directive given to them is the following: 
? rank these six texts from best to worst. If 
you cannot manage to give a different ranking 
to two texts, regroup them under the same 
parenthesis and give them the same score, as in 
the following example : 4 [1 2] 6 [3 5].? 
The aim of this instruction is to produce 
rankings that are similar to the rankings attributed 
automatically.  
Human judgement that ranks from best to worse 
corresponds in reality to a set of the fluency, 
adequacy and Informativeness criteria that can be 
attributed to the texts translated automatically.  
 
7.2.2 Automatically generated scores 
? X-score : syntactic score 
? D-score : semantic score 
 
Within the framework of the CESTA 
evaluation campaign the scientific committee 
decided to make use of the X-score only, the 
semantic D-score having proved to be unstable 
and that it could be advantageously replaced 
by the a metric based on (Bogdan, B.; Hartley, 
A.; Atwell, 2003), a reformulation of the D-
score developed by (Rajman, M. and T. 
Hartley, 2001), and which we refer to as the 
ROUGE metric in this article. 
 
7.2.3 X-score: definition 
? This score corresponds to a grammaticality 
metric 
? Each of the texts is previously parsed with 
XELDA Xerox parser. 
? 22 types of syntactic dependencies 
identified through the corpus of automatic 
translations. 
? The syntactic profile of each source 
document is computed. This profile is then 
used to derive the X-score for each 
document, making use of the following 
formula: 
? X-score = (#RELSUBJ+#RELSUBJPASS-
#PADJ-#ADVADJ)  
 
 
7.3 The ?ROUGE? protocol  
 
This protocol, developed by Anthony Hartley in 
(Bogdan, B.; Hartley, A.; Atwell, 2003), is a 
semantic score. It is the result of a reformulation of 
the D-Score, the semantic score initiated through 
previous collaboration with Martin Rajman 
(Rajman, M. and T. Hartley, 2001), as explained in 
the previous section.  
 
The original idea on which this protocol is based 
relies on the fact that MT evaluation metrics that 
?are based on comparing the distribution of 
statistically significant words in corpora of MT 
output and in human reference translation 
corpora?.  
 
The method used to measure MT quality is the 
following:  a statistical model for MT output 
corpora and for a parallel corpus of human 
translations, each statistically significant word 
being highlighted in the corpus. On the other hand, 
a statistical significance score is given for each 
highlighted word. Then statistical models for MT 
target texts and human translations are compared, 
special attention being paid to words that are 
automatically marked as significant in MT outputs, 
whereas they do not appear to be marked as 
significant in human translations. These words are 
considered to be ?over generated?. The same 
operation is then carried out on ?under generated 
words?. At this stage, a third operation consists in 
the marking of the words equally marked as 
significant by the MT systems and the human 
translations. The overall difference is then 
calculated for each pair of texts in the corpora. 
Three measures specifying differences in statistical 
models for MT and human translations are then 
implemented : the first one aiming at avoiding 
?over generation?, the second one aiming at 
avoiding ?under generation? and the last one being 
a combination of these two measures. The average 
scores for each of the MT systems are then 
computed.  
 
As detailed in (Bogdan, B.; Hartley, A.; Atwell, 
2003): 
 
?1. The score of statistical significance is 
computed for each word (with absolute frequency 
? 2 in the particular text) for each text in the 
corpus, as follows: ( )
][
][][][
][ ln
corpallword
foundnottxtswordcorprestwordtextword
textword P
NPP
S
?
??? ??=
 
where: 
Sword[text] is the score of statistical significance for 
a particular word in a particular text 
Pword[text] is the relative frequency of the word in 
the text; 
Pword[rest-corp] is the relative frequency of the same 
word in the rest of the corpus, without this text; 
Nword[txt-not-found] is the proportion of texts in the 
corpus, where this word is not found (number of 
texts, where it is not found divided by number of 
texts in the corpus) 
Pword[all-corp] is the relative frequency of the word 
in the whole corpus, including this particular text 
 
2. In the second stage, the lists of statistically 
significant words for corresponding texts together 
with their Sword[text] scores are compared across 
different MT systems. Comparison is done in the 
following way: 
For all words which are present in lists of 
statistically significant words both in the human 
reference translation and in the MT output, we 
compute the sum of changes of their Sword[text] 
scores: ( )? ?= ].[].[. MTtextwordreferencetextworddifftext SSS  
The score Stext.diff is added to the scores of all 
"over-generated" words (words that do not appear 
in the list of statistically significant words for 
human reference translation, but are present in 
such list for MT output). The resulting score 
becomes the general "over-generation" score for 
this particular text: 
? ?? +=
textwords
textgeneratedoverworddifftexttextgenerationover SSS
.
][...
 
The opposite "under-generation" score for 
each text in the corpus is computed by adding 
Stext.dif and all Sword[text]  scores of "under-generated" 
words ? words present in the human reference 
translation, but absent from the MT output. 
?+=?
textwords
textatedundergenerworddifftexttextgenerationunder SSS
.
][...
 
It is more convenient to use inverted scores, 
which increases as the MT system improves. These 
scores, So.text and Su.text, could be interpreted as 
scores for ability to avoid "over-generation" and 
"under-generation" of statistically significant 
words. The combined (o&u) score is computed 
similarly to the F-measure, where Precision and 
Recall are equally important: 
textgenerationover
texto S
S
.
.
1
?
= ; 
 
textgenerationunder
textu S
S
.
.
1
?
= ;
 
textutexto
textutexto
textuo SS
SSS
..
..
.&
2
+=  
The number of statistically significant words 
could be different in each text, so in order to make 
the scores compatible across texts we compute the 
average over-generation and under-generation 
scores per each statistically significant word in a 
given text. For the otext score we divide So.text by the 
number of statistically significant words in the MT 
text, for the utext score we divide Su.text by the 
number of statistically significant words in the 
human (reference) translation: 
rdsInMTstatSignWo
texto
text n
So .= ;
 
rdsInHTstatSignWo
textu
text n
Su .= ; 
 
texttext
texttext
text uo
uoou +=
2&  
The general performance of an MT system for IE 
tasks could be characterised by the average o-
score, u-score and u&o-score for all texts in the 
corpus?. 
8 Time Schedule and result dissemination 
 
The CESTA evaluation campaign started in 
January 2003 after having been labeled by the 
French Ministry of Research. During year 2003 
CESTA scientific committee went through 
protocol detailed redefinition and specification and 
a time schedule was agreed upon.  
 
2004 first semester is being dedicated to corpus 
untagging and the programming of CESTA 
evaluation tool. Reference human translations will 
also have to be produced and the implemented 
evaluation tool submitted to trial and validation.  
 
After this preliminary work, the first run will 
start during autumn 2004. At the end of the first 
campaign, result analysis will be carried out. A 
workshop will then be organized for CESTA 
participants. Then the second campaign will take 
place at the end of Spring 2005, the terminological 
adaptation phase being scheduled on a five month 
scale. 
 
After carrying out result analysis and final report 
redaction, a public workshop will be organized and 
the results disseminated and subject to publication 
at the end of 2005.  
 
9 Conclusion 
 
CESTA is the first European Campaign 
dedicated to MT Evaluation. The results of the 
campaign will be published in a final report and be 
the object of an intermediary workshop between 
the two campaigns and a final workshop at the end 
of the campaign.  
 
It is a noticeable point that the CESTA campaign 
aims at providing a state of the art of automated 
metrics in order to ensure protocol reusability. The 
originality of the CESTA protocol lies in the 
combination and contrastive use of three different 
types of measures carried out in parallel with a 
Meta evaluation of the metrics. 
 
It is also important to note that CESTA aims at 
providing a black box evaluation of available 
Machine Translation technologies, rather than a 
comparison of systems and interfaces, that can be 
tuned to match a particular need. If systems had to 
be compared, the fact that these applications 
should be compared including all software lawyers 
and ergonomic properties, ought to be taken into 
consideration.  
 
Moreover apart from providing a state of the art 
through a Meta evaluation of the metrics used in its 
protocol, thanks to the setting of this original 
protocol that relies on the contrastive use of 
complementary metrics, CESTA aims at protocol 
reusability. One of the outputs of the campaign 
will be the creation of a Machine Translation 
evaluation toolkit that will be put at users and 
system developers? disposal.Acknowledgements 
References  
Besan?on, R. and Rajman, M., (2002). Evaluation 
of aVector Space similarity measure in a 
multilingual framework. Procs. 3rd 
International Conference on Language 
Resources and Evaluation, Las 
Palmas,Spain,.1252 
Bogdan, B.; Hartley, A.; Atwell E.; Statistical 
modelling of MT output corpora for 
Information Extraction Proceedings Corpus 
Linguistics 2003, Lancaster, UK, 28-31 
March 2003, pp. 62-70 
Chaudiron, S. Technolangue. In: 
http://www.apil.asso.fr/metil.htm, mars 2001 
Chaudiron, S. L??valuation des syst?mes de 
traitement de l?information textuelle : vers un 
changement de paradigmes, M?moire pour 
l?habilitation ? diriger des recherches en sciences 
de l?information, pr?sent? devant l?Universit? de 
Paris 10, Paris, novembre 2001 
Dabbadie, M, Mustafa El Hadi, W., Timimi, I. 
(2001). Setting a Methodology for Machine 
Translation Evaluation. In: Machine 
Translation Summit VIII, ISLE/EMTA, 
Santiago de Compestela, Spain, 18-23 
October 2001, pp. 49-54. 
Dabbadie, M., Mustafa El Hadi, W., Timimi, I., 
(2002). Terminological Enrichment for non-
Interactive MT Evaluation. In: LREC 2002 
Proceedings ? Las Palmas de Gran Canaria, 
Spain ? 29th ? 31st May 2002 ? vol 6 ? 1878-
1884 
EAGLES-Evaluation-Workgroup. (1996). 
EAGLES evaluation of natural language 
processing systems. Final report, Center for 
Sprogteknologi, Denmark, October 1996. 
EAGLES (1999). EAGLES Reports (Expert 
Advisory Group on Language Engineering 
Standards)http://www.issco.unige.ch/project
s/eagles/ewg99. 
ISLE (2001). MT Evaluation Classification, 
Expanded Classification. 
http://www.isi.edu/natural-
language/mteval/2b-MT-classification.htm. 
ISO/IEC-9126. 1991. ISO/IEC 9126:1991 (E) ? 
Information Technology ? Software 
Product Evaluation ? Quality 
Characteristics and Guidelines for Their Use. 
ISO/IEC, Geneva. 
ISO (1999). Standard ISO/IEC 9126-1 Information 
Technology ? Software Engineering ? 
Quality characteristics and sub-
characteristics. Software Quality 
Characteristics and Metrics - Part 1 
ISO (1999). Standard ISO/IEC 9126-2 Information 
Technology ? Software Engineering ? 
Software products Quality : External Metrics 
- Part 2 
ISO/IEC-14598. 1998-2001. ISO/IEC 14598 ? 
Information technology ? Software product 
evaluation ? Part 1: General overview 
(1999), Part 2: Planning and management 
(2000), Part 3: Process for developers 
(2000), Part 4: Process for acquirers (1999), 
Part 5: Process for evaluators (1998), Part 6: 
Documentation of evaluation modules 
(2001). ISO/IEC, Geneva. 
ISSCO (2001) Machine Translation Evaluation : 
An Invitation to Get Your Hands Dirty!, 
ISSCO, University of Geneva, Workshop 
organised by M. King (ISSCO) & F. Reed, 
(Mitre Corporation), April 19-24 2001. 
King (1999a) EAGLES Evaluation Working 
Group, report,http://www.issco.unige.ch/ 
projects/eagles. 
King, M. (1999b). ?ISO Standards as a Point of 
Departure for EAGLES Work in EELS 
Conference (European Evaluation of 
Language Systems), 12-13 April 1999. 
Mariani, Joseph. ?Language Technologies : 
Technolangue Action ?. Presentation. In: 
LREC'2002 International Strategy Panel17, Las 
Palmas, May 2002. 
Nomura, H. and J. Isahara. (1992). The JEIDA 
report on MT. In Workshop on MT 
Evaluation: Basis for Future Directions, San 
Diego, CA. Association for Machine 
Translation in the Americas (AMTA). 
Popescu-Belis, A. S. Manzi, and M. King. (2001). 
Towards a two-stage taxonomy for MT 
evaluation. In Workshop on MT Evaluation 
?Who did what to whom?? at Mt Summit 
VIII, pages 1?8, Santiago de Compostela, 
Spain. 
Rajman, M. and T. Hartley, (2001). Automatically 
predicting MT systems rankings compatible 
with Fluency, Adequacy or Informativeness 
scores. Procs. 4th ISLE Workshop on MT 
Evaluation, MT Summit VIII, 29-34. 
Rajman, M. and T. Hartley, (2002). Automatic 
ranking of MT systems. In: LREC 2002 
Proceedings ? Las Palmas de Gran Canaria, 
Spain ? 29th ? 31st May 2002 ? vol 4 ? 1247-
1253 
Reeder, F., K. Miller, J. Doyon, and J. White, J. 
(2001). The naming of things and the 
confusion of tongues: an MT metric. Procs. 
4th ISLE Workshop on MT Evaluation, MT 
Summit VIII, 55-59. 
Sparck-Jones K., Gallier, J.R. (1996). Evaluating 
Natural Language Processing Systems: An 
Analysis and Review, Springer, Berlin. 
TREC, NIST Website, last updated, August 1st, 
2000, visited by the authors, 23-03-2003  
Vanni, M. and K. Miller (2001). Scaling the ISLE 
framework: validating tests of machine 
translation quality for multi-dimensional 
measurement. Procs. 4th ISLE Workshop on 
MT Evaluation, MT Summit VIII, 21-27. 
VanSlype., G. (1979). Critical study of methods 
for evaluating the quality of MT. Technical 
Report BR 19142, European Commission / 
Directorate for General Scientific and 
Technical Information Management (DG 
XIII). 
V?ronis, J., Langlais, Ph. (2000). ARCADE: 
?valuation de syst?mes d'alignement de textes 
multilingues. In Chibout, K., Mariani, J., 
Masson, N., Neel, F. ?ds., (2000). Ressources et 
?valuation en ing?nierie de la langue, Duculot, 
Coll. Champs linguistiques, et Collection 
Universit?s Francophones (AUF). 
Mul t i l i ngua l i ty  in a Text  Generat ion  System 
For Three  Slavic Languages  
Geert-Jan Kruijff a, Elke Teich t', John Bateman ~, Ivana Kruijit;Korbayovfi", 
Hana Skoumalovg ~,Serge Sharoff 'l, Lena Sokolova d, Tony Hartley ~, 
Kamenka Staykova/, Ji~'~ Hana" 
?Charles University, Prague; ~University of the Saarland; ~University of Bremen; 
aRRIAI, Moscow; ?University of Brighton; /IIT, BAS, Sofia 
http://www.itri.brighton.ac.uk/projects/agile/ 
Abstract 
This paper describes a lnultilingual text generation 
system in the domain of CAD/CAM software in-- 
structions tbr Bulgarian, Czech and l:\[ussian. Start- 
ing from a language-independent semantic represen- 
tation, the system drafts natural, continuous text 
as typically found in software inammls. The core 
modules for strategic and tactical gene,'ation are im- 
plemented using the KPML platform for linguistic 
resource development and generation. Prominent 
characteristics of the approach implemented a.re a 
treatment of multilinguality that makes maximal use 
of the cominonalities between languages while also 
accounting for their differences and a common repre- 
sentational strategy for both text planning and sen- 
tence generation. 
1 In t roduct ion  
This paper describes the Agile system I tbr the 
multilingual generation of instructional texts as 
found in soft;ware user-manuals in Bulgarian, 
Czech and Russian. The current prototype fo- 
cuses on the automatic drafting of CAD/CAM 
software documentation; routine passages as 
found in the AutoCAD user-manual have been 
taken as target texts. The application sce- 
nario of the Agile system is as follows. First, 
a user constructs, with the help of a GUI, 
language-independent task models that spec- 
ify the contents of the documentation to be 
generated. The user additionally specifies the 
language (currently Bulgarian, Czech or Rus- 
sian) and the register of the text to be gen- 
erated. The Agile system then produces con- 
tinuous instructional texts realizing the speci- 
fied content and conforming to the style of soft- 
ware user-manuals. The texts produced are 
1EU Inco-Copernicus project PL961004: 'Automatic 
Generation of Instructional Texts in the Languages of 
Eastern Europe' 
intended to serve as drafts for final revision; 
this ~drafting' scenario is therefbre analogous to 
that first explored within the Drafter project. 
Within the Agile project, however, we have ex- 
plored a more thoroughly nmltilingual architec- 
ture, making substantial use of existing linguis- 
tic resources and components. 
The design of the Agile system overall re, sts 
on the following three assumI)tions. 
First, the input of the system should be spec- 
ified irrespective of any particular output lan- 
guage. This means that the user must be able to 
express the content that she wants the texts to 
convey, irrespective of what natural language(s) 
she masters and in what language(s) the out- 
put text shouM be realized. Such language- 
independent content specification can take the 
form of some knowledge representation pertain- 
ing to the application domain. 
Second, the texts generated as the outtmt of 
the system should be well-formulated with re- 
spect to the expectations of natiw. ? speakers of 
each particular language covered by the system. 
Since differences among languages may appear 
at any level, language-sensitive d cisions about 
the realization of the specified content must be 
possible throughout he generation process. 
And third, the notion of multilinguality em- 
ployed in the system should be recursive, in 
the sense that the modules responsible tbr the 
generation should themselves be multilingual. 
The text generation tasks which are common 
to the languages under consideration should be 
pertbrmed only once. Ideally, there should be 
one process of generation yielding output in 
multiple languages rather than a sequence of 
monolingual processes. This view of 'intrin- 
sic multilinguality' builds on the approach set 
out in Bateman et al (1999). Each module of 
the system is fnlly multilingual in that it simul- 
474 
taneously enables both integration of linguistic 
resources, defining commonalities bel;ween lan- 
guages, and resource integrity, in |;bat the in- 
dividuality of each of the language-speeitic re- 
sources of a multilingnM ensemble is always pre- 
served. 
We consider these assuml)l;ions an(l the view 
of multilinguality entailed by |;hem to be cru- 
cial for the design of efli;ctive multilingual text 
generation systems. The results so far a(:hicved 
by the Agile system SUl)port this and also ofl'er 
a ~soli(l experiential basis tbr the develot)mcnt of 
fllrther multilingnal generation systems. 
The overall operation of 1;t1(; Agile sysl;em is 
as tbllows. Al/tcr the us(u' has Sl)ecilied some 
inl;en(led text (;OlltenI; (described in Section 2) 
via the Agile GUI, the system i)ro(:eeds to gen- 
eral;e the texts required. To do this, a text 
t)lammr (Section 3) first assigns parts of the, 
task model to text elements and arranges l;h(;m 
in a hierarchical fashion a text t)lan. Then, a 
sentence plammr organizes I;he content of the 
text elements into sentence-sized elml~ks and 
ere~,tes the corresponding input fin' l;he tacti- 
ca,1 generator, expressed in standard sentence 
l)lamfing language (SPI,) lbrmulae. Finally, 1;11(; 
tactical g(meral;or generates t;he linguistic real- 
izations corresponding 1;o these Sl)l~s the text 
(Sect;ion 4). In the stage of the l)rojccI; rt}l)orte(l 
here, we, conceal;rated i)arl;icularly on \])roccdu- 
ral texts. These otlhr sl;el)-by-st;e t) des(:rit)t;ions 
of how to perlbrm domain tasks using the given 
software tools. A simplified version of one such 
procedural text is given (tbr English) in Fig- 
ure 1. This architectm:e mirrors the reference 
architecture for generation diseusse(t in I/,eiter 
8z Dale (1.997). The modules of the system are 
1)ipelined so that a continuous text is generated 
realizing the intended meaning of the inlmt se- 
mantic representation without backtracking or 
revision. 
Several important properties have ('haracter- 
ized the method of development leading to the 
Agile system. These are to a large extent re- 
sponsible for the eflhetiveness of the system. 
These include: 
Re-use  and  adaptat ion  o f  ava i lab le  re-  
sources .  We have re-used snt)stantial bodics 
of e, xisting linguistic resources at all levels rel- 
evant for the system; this t)laye(l a (:rueial role 
in achieving the Sol)histieatcd generation capa- 
7b d~nw a polylinc 
First start the PLINE command using one of these meth- 
ods: 
Windows From the Polylinc tlyout on the, l)raw tool~ 
lmr, choose Polylinc. 
DOS and UNIX  lqom the Draw menu, choose Poly- 
line. 
1.. Spccit~y the start point of the polyline. 
2. S1)ecil~y tim next point of the 1)olylinc. 
3. Press ll,cturn t;o end the polyline. 
Figure l: Example "To draw a polyline" 
bilities now displayed by the system in each of 
its languages of expertise prior to the project 
t\]m'l.'e were 11o substantial ~mtomatic generation 
systenls fi)r any of the languages covered. The 
core modules for strategic and ta(:ti('al gener- 
ation were all imt)lemcnted using the Kernel- 
Penman Multilingual system (KPML: ef Bate- 
man et al, \]999) a Common l,isp base(t gram- 
mar development environment, in addition, 
we adopted the Pemnan Upt)er Model as used 
within Pemnan/KPMl~ as the basis tbr our 
linguistic semantics; a more rcstri(:ted domain 
model (DM) rclewmt o the CAD/CAM-domain 
was &',lined as a st)e('ialization of l;he UM con- 
(:epts. The I)M was iuspired by the domain 
me(tel of the Drafter l)rojet:t, but l)res(ml;s a 
g(m(',ralizati()n ()f the latter in that it allows for 
eml)(;d(ling t:asks and illsLrut'|;ions t:o any arlfi- 
l;rm:y re(:ursive depth (i.e., more complex l;cxt; 
plans). Ah'eady existing lexical resom:ces and 
morphological modules availabh; to the 1)ro.j(',ct 
were re-used tbr Bulgarian, Czech and l~.ussian: 
the Czech and Bulgarian components were mo(t- 
ules written in C (e.g., IIaji(: L; Hla(lk~, 1997, 
tbr Czech) that were interfimed with KPMI, us- 
ing a standard set of API-methods (of. Bate- 
man & Sharoff, 1998). Finally, because no 
grammars uitable for generation in Bulgarian, 
Czech and l/.ussia,n existed, a grammar tbr En- 
glish (NIGEL: Mann & Matthiessen, 1985) was 
re-used to lmild them; tbr the theoretical basis 
of this technique see Teich (1995). 
Combinat ion  o f  two  methods  o f  resources  
deve lopment .  Two methods  were com- 
bined to enable us to develop basic general- 
language grammars and sublanguage grammars 
fin: CAD/CAM instructional texts at; |;11(; same 
time. One nmthod is the system-oriented one 
aimed at lmildiug a computational resource 
475 
with a view of the whole language system: this 
is a method strongly supported by the KPML 
development environment. The other method 
is instance-oriented, and is guided by a detailed 
register analysis. The latter method was partic- 
ularly important given the Agile goal of being 
able to generate texts belonging to rather di- 
verse text types- -  e.g., impersonal vs. personal; 
procedural, flmetional descriptions, overviews 
etc. 
Cross-linguistic resource-sharing. A cross- 
linguistic approach to linguistic specifications 
and implementation was taken by maximizing 
resource sharing, i.e. taking into account sim- 
ilarities and differences among the treated lan- 
guages o that development tasks have been dis- 
tributed across different languages and re-used 
wherever possible. 
2 Language- independent  Content 
Specif icat ions 
The content constructed by a user via the Ag- 
ile GUI is specified in terms of Assertion-bozes 
or A-boxes. These A-boxes are considered to 
be entirely neutral with respect o the language 
that will be used to express the A-box's con- 
tent. Thus individual A-boxes can be used for 
generating multiple languages. A-boxes spec- 
i(y content by instantiating concepts from ~,he 
DM or UM, and placing these concepts in rela- 
tion to one another by means of configurational 
concepts. The configurational concepts define 
adnfissible ways in which content can be struc- 
tured. Figure 2 gives the configurational con- 
cepts distinguished within Agile. 
Procedure A procedure has three slots: 
(i) GOAL (obligatory,filled by a USER-AcTION), 
(ii) METIIODS (optional, filled by a METHOD-LIsT), 
(iii) SIDE-EPFECT (optional, filled by a USER- 
EVENT). 
Method A method has three slots: 
(i) CONSTRAINT (optionM, filled by an OPERATING- 
SYSTEM), 
(ii) PaEeONDITION (optional, filled by a PROCE- 
DURE), 
(iii) SUUSTEPS (obligatory, filled by a PI~OCEDUI/E- 
LIST). 
Method-List A METIIOD-LIST is a list of h/IETIIOD'S. 
Procedure-List A PROCEI)URE-LIST is a list of 
PROCEDURE:S. 
Figure 2: Configurational concepts 
Configurational concepts are devoid of actual 
content. Tile content is provided by inst, antia- 
tions of concepts that represent various user ac- 
tions, interface events, and interface modalities 
and functions. Taken together, these instanti- 
ations provide the basic propositional content 
tbr instructional texts and are taken as input 
tbr the text planning process. 
3 Strategic Generat ion: From 
Content  Specif icat ions to Sentence 
Plans 
To realize an A-box as a text, we go through suc- 
cessive stages of text planning, sentence plan- 
ning, and lexico-grammatical generation (cf 
also Reiter & Dale, 1997). At each stage there 
is an increase in sensitivity to, or dependency 
on, the target language in which output will 
be generated. Although the text planner itself 
is language-independent, the text; plamfing re- 
sources may (lifter fl'om language to language 
as much as is required. This is exactly analo- 
gous to the situation we find within the individ- 
ual language grammars as represented within 
KPML: we therefore represent the text planning 
resources in the same fashion. For the text type 
and languages of concern here, however, w~ria- 
lion across languages at the text planning stage 
turned out to be minimal. 
The organization of an A-box is used to guide 
the text planning process. Itere, we draw a dis- 
tinction between text structure elements (TSEs) 
as the elements from which a (task-oriented) 
text, is built ut), and text templates', which con- 
dition the way TSEs are to be realized linguis- 
tically. We locus on the relation between con- 
cepts on the one hand, and TSEs on the other. 
We are specifically interested in the configura- 
tional concepts that are used to configure the 
content specified in an A-box because we want 
to maintain a close connection between how the 
content can be defined in an A-box and how 
that content is to be spelled out in text. 
3.1 Structuring and Styling 
A text structure element is a predefined com- 
ponent that needs to be filled by one or more 
specific parts of the user's content definition. 
Using the reader-oriented terminology common 
in technical authoring guides, we distinguish 
a small (recursively defined) set of text TSEs; 
these are listed in Figure 3. 
476 
Task-Docmnent  A TASK-\])OCUMFNT has tWO slots: 
(i) TASK-TFI'I,E (ol)ligatory), 
(ii) TASK-INSTI{U(ITIONS (obligatory), being a list 
of" at least one ~\[NSTRUCTION. 
Instruction An INSTRUCTION has three slots: 
(i) TASKS (obligatory), being a list of at least one 
TASK~ 
(ii) CONSTRAINT (optional), 
(iii) Pm,ZCONDITION (optional). 
Task  A TASK has two slots: 
(i) INSTRUCTIONS (ol)tional), 
(ii) SII)I';-EI,'I"I,:C'I' (ol)tional). 
Figure 3: Text Structure Elements (TSEs) 
The TSEs are placed in correspondence with 
the configurational concet)ts of the DM (cf. Fig- 
ure 2); this enat)les us to lmild a text stru('ture 
l;hat folh)ws the structuring of the content in an 
A-1)ox (cf. Figure 4). 
Orthogonal to the notion of text structure l- 
ement is the notion of text temt)late. Whereas 
TSEs capture what needs to be realized, the 
text template (:al)tures how that content is to 
1)e realized. Thus, a feint)late defines a style 
for expressing the content. Am we discuss be- 
low, we define text templates in terms of con- 
straints on the realization of si)e(:iti(" (in(tivid- 
ual) TSEs. D)r examt)le, whereas in Bulgarian 
and Czech headings (to which the '\]'ASK-TITLE 
element corresponds: of. Figure 4) are usually 
realized as nominal groups, in the Russian Au- 
toCAD ulallnal headings are realized as nonii- 
nile purpose clauses as they are ill English. 
3.2 Tex~ P lann ing  g~ Sentence  P lann ing  
The major component of the text pbmner is 
fi)rnmd by a systemic network fi)r text struc- 
turing; this network, called the text structur- 
ing region, defines an additional level of linguis- 
tic resources for the level of genre. This region 
constructs text structures in a way that is very 
similar to the way in which the systemic net- 
works of the grammars of the tactical genera- 
|or build up grammatical structures. In fact, 
by using KPML to implement his means for 
text structuring, the interaction between global 
level text generation (strategic generation) and 
lexico-grammatical expression (tactical genera- 
tion) is greatly facilitated. Moreover, this al)- 
t)roach has the advantage |;tint constraints on 
output realization can 1)e easily accmnulated 
and propagated: for example, the text plan- 
ner can iml)ose constraints on the output lexico- 
grammatical realization of particular text t)lan 
elements, such am the realization of text head- 
ings by a nominalization ill Czech and Bulgar- 
|an or by an infinite purpose clause in Rus- 
sian. This is one contribution to overcoming the 
notorious generation gap prol)leln caused when 
a text planning module lacks control over the 
line-grained istinctions that m'e available in a 
grmmnar. Ill our case, both text plamfing and 
sentence planning are integrated into one and 
the same system and are distinguished by strat- 
ification. 
TASK-TITLE ~-} GOAl, of topmost  PROCEDURE 
TASK-INSTRUCTIONS ~-} METIIODS of PROCEDUI/E 
Sll)E-EIq,'ECT ~ SIDhl-EFFI~CT of PROCEDUII.I\] 
TASK /-~ GOAL of PROCEI)IHtI,; 
(-JONSTRA1NT <-} CONSTRAINT of ~41,VI'IIO1) 
PRECONI)ITION ~ PIH?COND1TION of ~,4ETI1OI) 
1NSTIIUCTI(IN-TAsKS 1--} SUBSTH)S of a METIIOD 
INST1HJCTION +5 MI,TI'IIOD 
Figure 4: Mapping TSEs and configurational 
concepts defined in the DM 
Following on from the orthogomflity of text 
t/;mplates and text structure elements, the text 
structuring region consists of two parts. One 
1)arl; deals wil;h interpreting the A-box in terms 
of TSEs: traversing l;he network of this part of 
the region produces a text structure for the A- 
b/lx contbrufing to the definitions above. The 
second part of the region imposes constraints 
on the realization of the TSEs introduced by 
the first part. Divers(; constraints can be ira- 
posed depending on the user's choice of style, 
e.g., personal (featuring ppredominantly imper- 
atives) vs. impersonal (tbaturing indicatives). 
Tile result of text plmming is a text plan. 
This can be thought of as a hierarchical struc- 
ture (built by TSEs) with lilts of A-box content 
at; its leaves together with additional constraints 
imposed by the text planning process: e.g., that 
the Title segment of the document should not be 
realized as a full (:lause but; rather as a nominal 
phrase or a lmrt)osive det)endent clause. The 
text plan may also include constraints on pre- 
ferred layout of the docmnent elements: this 
ilflbrmation is passed on via HTML annotations. 
The sentence plmmer then takes this text plan 
as intmt, and creates SPL tbrmulae to express 
477 
the content identified by the text plan's leaves. 
The resulting SPLs can also group one or more 
leaves together (aggregation) det)ending on de- 
cisions taken by the text planner concerning dis- 
course relations. Furthennore, constraints on 
realization that were introduced by the text- 
planner are also included into the SPLs at this 
stage. 
Of particular interest multilingually is the 
way concepts may require different kinds of re- 
alizations ill different languages. For example, 
languages need not of course realize concepts 
as single words: in Czech the concept Mcn,t 
gets realized as "menu" but the interface modal- 
ity Dialogboz is realized as a multiword expres- 
sion "dialogovd okno" (whose compofients i.e., 
an adjective and a nominal head may undergo 
various grammatical operations independently). 
The Agile system sentence plammr handles uch 
cases by inserting SPL fbrms corresponding to 
the literal semantics of the complex expressions 
required; these are then expressed via the tac- 
tical generator in the usual way. The result- 
ing SPL formulas thus represent the language- 
specitic semantics of the sentences to be gener- 
ated. Otherwise, if a concept maps to a single 
word, the sentence planner leaves the fnrther 
specification of how the concept should be re- 
alized to the lexico-grammar nd its concept- 
to-word mapI)ings. More extensive diflb.rences 
between languages are handled by conditional- 
izing the text and sentence planner resources 
fltrther according to language. 
4 Tactical Generat ion:  From 
Sentence P lans  to Sentences 
The tactical generation component hat colt- 
structs sentences (and other grammatical units) 
fl'om the SPL tbrmulae specified in the text 
plan relies on linguistic resources tbr Bulgarian, 
Czech and Russian. The necessary grammars 
and lexicons have been constrncted employing 
the methods described in Section 1. As ,toted 
there, the crucial characteristic of this model 
of nmltilingual representation is that it allows 
tbr the representation f both, commonalities and 
differences between languages, as required to 
cover the observable ontrastive-linguistic phe- 
nomena. This can be applied even among typo- 
logically rather distant languages. 
We first illustrate this with respect o some 
of the contrastive-linguistic t)henomena that are 
covered by this model employing exami)les ti'om 
English, Bulgarian, Czech and Russian. We 
then show the organization of the lexicons and 
briefly describe lexical dloice. 
4.1 Semantic and grammatical 
cross-linguistic variation 
One. of the tenets of our model of cross-linguistic 
variation is that languages have a rather high 
degree of similarity semantically attd tend to 
differ syntactically. We can thus expect o have 
identical SPL expressions for Bulgarian, Czech 
and Russian in many cases, although these may 
be realized by diverging syntactic structures. 
However, we also allow for the case in which 
there is no commonality at; this level and even 
the SPL expressions diverge. 2 Example 1 illus- 
trates the latter case (high semantic divergence, 
plus grammatical divergence), and example 2 
the former (semantic ommonality, plus gram- 
matical divergence). 
Example 1: English and Russian spa- 
tial PPs .  The major lexico-grammatical d i f  
ference l)etween English and Russian preposi- 
tional phrases is that the relation expressed by 
the PP is realized by the choice of the prepo- 
sition in English, whereas in Russian, it; is in 
addition realized by case-government. In the 
are.a of spatial PPs, the choice of a particular 
preI)osition in English corresl)onds to a distinc- 
tion in the dimensionality of the object that re- 
alizes the range of the relation expressed by the 
PP. For both PPs expressing a location and PPs 
expressing movement, English distinguishes be- 
tween three-dimensional objects (in, into), one- 
or-two-dimensional objects (on, onto) and zero- 
dimensional objects (at, to). 
In Russian, in contrast, zero-or-three dimen- 
sional objects (preposition: v) are opposed 
to one-or-two-dimensional objects (preposition: 
ha). A fnrther difference between the expres- 
sion of static location vs. movement is expressed 
by case selection: na/v+locative case expresses 
static location, v/na+accusative case expresses 
inovement (entering or reaching an object) and 
the preposition k+dative case expresses move- 
inent towards an object (,lot quite reaching or 
2This distinguishes our approach fl'om interlingua- 
based systems, which typically require a common seman- 
tic (or conceptual) input. 
478 
entering it). In the {-onverse relation, motion 
away from an object, s is sele, eted tbr move- 
ment from within an oh.joel;, and ot fbr move- 
men| away from the vicinity of an ot).jeet. Her(;, 
both prel)ositions govern genitive case. The di- 
mensionality of the object is only relevant for 
the distinction between v/na and s/ot, 1)ut not 
for h. Since the concel)tualizations of spatial re- 
lations are ditf'erent across \]'3nglish and Russian, 
the input SPL expressions diverge, as shown in 
Figure 5); rather than using domain model con- 
cepts, these SPL ext)ressions restrict hemselves 
to Ut)pe, r Model concepts in order to highlight 
the cross-linguistic contrast. This examl)le illus- 
trates well how it is (}ften ne{:e, ssary t{} 'semanti- 
{:ize,' eve, nts differently in (tilt'ere|d; languages in 
order 1;o achieve the most natural results. Not;{; 
that Cze, ch is here very similar to l/nssian. 
a. SPL Russian 
(example 
:name DO-Textl-Ku 
:targetform "Pomestite fragment v bufer." 
:logicalform 
(s / dispositive-material-action 
:lex pomestitj 
:speech-act-id command 
:actee (a / object :lex fragment) 
:destination (d / THREE-D-0BJECT 
:lex bufer))) 
1}. SPL Rn: English 
(example  
:name D0-Textl-En 
:targetform "Put the selection on the clipboard." 
:iogicalform 
(s / dispositive-material-action 
:lex put 
:speech-act-id command 
:actee (a / object :lex selection) 
:destination (d / ONE-0R-TW0-D-0BJECT 
:lex clipboard))) 
Figure 5: SPI, ext}ressions 
Example 2: English, Bulgar ian and Czech 
headers in CAD/CAM texts. Grammatical 
ullits (1) (4) below show all ex~?tIllt, e of ,:r,,ss- 
linguistic commonality at the level of sen|anti{: 
int}ut and divergence at the le, vel of grammar. 
These units all time|ion as selfsutficient Task- 
titles tbr the deseril}tions of particular actions 
that can be t)erformed with the given s{}t'tware. 
(1) En: T{} draw a polyline 
(2) BU: qepTaene na IlOJII4MI4IIFIH 
Drawing- of polylineqNDEF 
NOMINAL  
(3) Cz: Kreslenl kf'ivky 
drawing-NOMINAL \]ine-GEN 
(d) \]/,ll: LIwo6I,I Hal)I4COBaTI, IIO,KHJIIIIIHIO 
in-order draw-INF l)olyline-AcC 
There are two major dit  re,,,ces (:,) (4) 
that need to 1)e accounte, d for: (i) they exhibit 
divergent grammatieal  ranks in that (1) and 
(4) are clauses (uontinite), while (2) and (3) are 
nomil,al groul,s (nominalizations); and ( i i )they 
show divergent syntact ic realizations: (2) 
and (3) ditl'er in that in Bulgarian, wlfich does 
not have (:as(',, the relation 1)etween the syntactic 
head Met)q_'aelte (ch, crtacnc) and the modifier lie- 
:mamma (polilinia) is (;xt)ressed by a t)re, position 
na (ha), whereas in Cze, ch, which has cast, this 
relation is expressed by genitive case, (k?ivky). 
\])espite these (litferen(:es, only the first diver- 
gen(:e has any (;onsequen(:{;s for the S\])L ext)res- 
sions rcquir(;d; I;hc l)asie semantic ommona\]- 
ity among (1)(4)  is 1)reserve, d. This is shown 
in Figm:e 6 t)y me, ans of the standard linguis- 
tic conditionalization 1)rovided 1)y KPML l'or all 
levels of linguistic des(:ription. The COll(tition- 
alization shows that both the English (1.) and 
the Russian (4) ar(' nontinite clauses while, the 
\]hdgarian (2) and the Czech (3) are nominM- 
izations. These S\])l, ext)ressions also show the 
use of (lom~dn ('onc(;1)ts as i)rodu('e(l by the text 
tfl~mner rathe, r than Ut)lmr model concepts as in 
the SPLs in Figure, 5. 
(example 
:name DO-Textl 
:logicalform 
(s / DM::draw 
:en :ru :PROPOSAL-Q & PROPOSAL 
:bu :cz :EXIST-SPEECHACT-Q & NOSPEECHACT 
:actee (d / DM::polyline))) 
Figure 6: Multilingual SPL e, xpression tbr the 
header examlfles 
The second differen('e is handled by the gener- 
ation grmmnars internally. Here, Bulgarian and 
Czech share the basic tractional-grammatical 
description of t)ostmotlifie, rs tbr nomilmlizati(ms 
(Figm:e 7). The ditl'erence in structure only 
479 
shows in syntagmatic realization and is separate 
from the functional description: For Bulgarian, 
the postmodifier marker Ha (ha: %f') is inserted, 
and tbr Czech, the nominal group realizing the 
Postmodifier is attr ibuted genitive ease. a
(gate 
:name MEDIUM-QUALIFIER 
:inputs processual-mediated 
:outputs 
((i.0 medium-qualifier 
(:bu :cz preselect Medium nominal-group) 
(:cz preselect Medium noun-gen-case) 
(:bu insert Mediumqualifiermarker) 
(:bu lexify Mediumqualifiermarker na))) 
:region QUALIFICATION) 
Figure 7: Shared system tbr Bulgarian and 
Czech 
4.2 Lexical choice and lexicons 
The lexical items tbr each language are selected 
from the lexicon via the domain model. A DM 
concept is annotated with one or more lexical 
items from each language. If there is more than 
one item per language, the choice is constrained 
by features imposed by the gralnmar. 
For example the concept DN::draw is anno- 
tated with two lexical items which are the im- 
perfective and perfective forms of the verb draw 
in Czech, Bulgarian and Russian. If the gram- 
mar selects imperfective aspect, tim first is cho- 
sen; if the grammar selects perfective aspect, 
the second is chosen. This mechanism is used 
also fbr the choice between a verb and its nom- 
inalization, among others. With the help of the 
lexicon, the inflectional properties collected tbr 
a particular lexical item during generation are 
translated into a format suitable tbr external 
morphological modules, which are then called. 
The result of the external module, the inflected 
tbrm, is passed back to the KPML system and 
inserted into the grammatical structure. 
5 Eva luat ion  and Conc lus ions  
A first round of evaluation has been carried 
out on the Agile prototype. This directly as- 
sessed the ability of users to control multilin- 
3This description is also valid for Russian, which has a 
nominal group structure similar to Czech. The 13ulgarian 
one is more like English. 
gual generation in tile three languages, as well 
as the design and robustness of the system eom- 
1}onents. Groups of users were given a brief 
training period and then asked to construct 
A-boxes expressing iven content. Texts were 
cross-generated: i.e., the languages were w~ried 
across the A-boxes independently of the native 
languages of the subjects who created them. Er- 
rors were then classified and recommendations 
for the next and final Agile prototype collected. 
The generated texts were then evaluated by ex- 
pert technical authors. They were generally 
judged to be of a broadly similar quality to 
the texts originating from manuals, and both 
kinds of texts received similar criticism. The 
main source of criticism and errors was the de- 
sign of the GUI which is now being improved 
for the final prototype. The overall design of 
the system has theretbre shown itself to offer an 
etfective approach tbr multilingual generation. 
We are now extending the system to cover a 
broader ange of text types as well as the further 
grammatical and semantic variation required by 
the evaluators as well as by the additional text 
types. 
Re ferences  
Bateman, J. A., Matthiessen, C. M. I. M., & Zeng, L. 
(1999). Multilingual natural anguage generation 
for multilingual software: a flmctional inguistic 
approach. Applied Artificial hdelligencc, 13(6), 
607-639. 
Bateman, J. A. & Sharoff, S. (1998). Mult, ilingual 
grammars and multilingual lexicons for nmltilin- 
gual text; generation. In Mnltilinguality in the Icz- 
icon II, ECAI'98 Workshop 13, (pp. 1-8). 
Hajie, J. 8; Hladk?, B. (1997). Probabilistic and 
rule-based tagger of an inflective language a 
comparison. In Proceedings of ANLP'97, (pp. 
111-118). 
Mmm, W. C. & Matthiessen, C. M. I. M. (1985). 
Demonstration of the Nigel text generation com- 
puter progrmn. In J. D. Benson 8: W. S. Greaves 
(Eds.), Systemic Perspectives on Discourse, Vol- 
ume 1 (pp. 50-83). Ablex. 
Reiter, E. & Dale, R. (1997). Building applied natu- 
ral language generation systems. Journal of Nat- 
ural Language Engineering, 3, 57-87. 
Tcich, E. (1995). Towards a methodology for the 
construction of multilingual resources tbr multi- 
lingual text generation. In Proceedings of the I J- 
CAI'95 workshop on multilingual generation, (pp. 
136-148). 
480 
Evaluating text quality: judging output texts without a clear source 
 
 
 
Abstract 
We consider how far two attributes of 
text quality commonly used in MT 
evaluation ? intelligibility and fidelity ? 
apply within NLG. While the former 
appears to transfer directly, the latter 
needs to be completely re-interpreted. 
We make a crucial distinction between 
the needs of symbolic authors and 
those of end-readers. We describe a 
form of textual feedback, based on a 
controlled language used for specifying 
software requirements that appears well 
suited to authors? needs, and an 
approach for incrementally improving 
the fidelity of this feedback text to the 
content model. 
1 Introduction 
Probably the most critical questions that need to 
be addressed when evaluating automatically 
generated texts are: does the text actually say 
what it?s supposed to say and is it fluent, 
coherent, clear and grammatical? The answers to 
these questions say something important about 
how good the target texts are and ? perhaps 
more to the point ? how good the system that 
generated them is. There is no a priori reason 
why the target texts should be any better or 
worse when they result from natural language 
generation (NLG) or from machine translation 
(MT): indeed, they could result from the same 
language generator. Given this, it may be natural 
to assume that NLG could appropriately adopt 
evaluation methods developed for its more 
mature sister, MT. However, while this holds 
true for issues related to intelligibility (the 
second critical question), it does not apply as 
readily to issues of fidelity (the first question). 
We go beyond our recent experience of 
evaluating the AGILE system for producing 
multilingual versions of software user manuals 
(Hartley, Scott et al, 2000; Kruijff et al, 2000) 
and raise some open questions about how best to 
evaluate the faithfulness of an output text with 
respect to its input specification. 
2 Evaluating intelligibility 
The use of rating scales to assess the 
intelligibility of MT output has been widespread 
since the early days in the field. Typically, 
monolingual raters assign a score to each 
sentence in the output text. However, this does 
not amount to an agreed methodology, since the 
number of points on the scale and their 
definition have varied considerably. For 
example, Carroll (1966) used a nine-point scale 
where point 1 was defined as ?hopelessly 
unintelligible? and point 9 as ?perfectly clear 
and intelligible?; Nagao and colleagues (Nagao 
et al, 1985), in contrast, used a five-point scale, 
while Arnold and his colleagues (Arnold et al, 
1994) suggest a four-point discrimination. In 
evaluating the intelligibility of the AGILE output, 
we asked professional translators and authors 
who were native speakers of the languages 
concerned?Bulgarian, Czech and Russian?to 
score individual text fragments on a four-point 
scale. The evaluators were also asked to give a 
summative assessment of the output?s suitability 
as the first draft of a manual. 
In a single pass, AGILE is capable of 
generating several types of text, each 
Anthony Hartley and Donia Scott 
Information Technology Research Institute, 
University of Brighton 
UK 
{firstname.lastname}@itri.bton.ac.uk
constituting a section of a typical software user 
manual?i.e., overview, short instructions, full 
instructions, and functional descriptions?and 
appearing in one of two styles (personal/direct 
or impersonal/indirect). We evaluated all of 
these text types using the same method. The 
intelligibility evaluation was complemented by 
an assessment of the grammaticality of the 
output, conducted by independent native 
speakers trained in linguistics. Following an 
approach widely used in MT (e.g., Lehrberger 
and Bourbeau, 1987), the judges referred to a 
list of error categories for their annotations. 
 
3 Evaluating fidelity 
In MT, evaluating fidelity (or ?accuracy?) 
entails a judgment about the extent to which two 
texts ?say the same thing?. Usually, the two 
texts in question are the source (i.e., original) 
text and the (machine-)translated text and the 
judges are expert translators who are again 
invited to rate the relative information content of 
pairs of sentences on an anchored scale (e.g., 
Nagao et al, 1985). But others (e.g., Caroll, 
1966) have also compared the informativeness 
of the machine translation and a human 
translation deemed to serve as a benchmark. 
Interestingly, both of these researchers found a 
high correlation between the intelligibility 
evaluations and the fidelity evaluations, which 
suggests that it may be possible to infer fidelity 
from the (less costly) evaluation of 
intelligibility. However, at the current state-of-
the-art this approach does not guarantee to 
detect cases where the translation is perfectly 
fluent but also quite wrong. 
For NLG, the story is rather different. 
Lacking a source text, we are denied the 
relatively straightforward approach of detecting 
discrepancies between artifacts of the same type: 
texts. The question is, instead, whether the 
generated text ?says the same thing? as the 
message ? i.e., the model of the intended 
semantic content together with the pragmatic 
force of the utterance. 
The message is clearly only available 
through an external representation. In translation 
generally, this external representation is the 
source text and the task is commonly 
characterized as identifying the message ? 
which originates in the writer?s mental model ? 
in order to re-express it in the target language. In 
an NLG system, the one external representation 
that is commonly available is the particular 
domain model that serves as input to the 
generation system. This model may have been 
provided directly by an artificial agent, such as 
an expert system. Alternatively, it may have 
been constructed by a human agent as the 
intended instantiation of their mental model. 
Yet, whatever its origins, directly comparing 
this intermediate representation to the output 
text is problematic. 
A recent survey of complete NLG systems 
(Cahill et al, 1999) found that half of the 18 
systems examined accepted input directly from 
another system1. A typical example is the 
Caption Generation System (Mittal et al, 1998), 
which produces paragraph-sized captions to 
accompany the complex graphics generated by 
SAGE (Roth et al, 1994). The input to generation 
includes definitions of the graphical constituents 
that are used to by SAGE to convey information: 
?spaces (e.g., charts, maps, tables), graphemes 
(e.g., labels, marks, bars), their properties (e.g., 
color, shape) and encoders?the frames of 
reference that enable their properties to be 
interpreted/translated back to data values (e.g., 
axes, graphical keys).?2 For obvious reasons, 
this does not readily lend itself to direct 
comparison with the generated text caption. 
In the remaining half of the systems 
covered, the domain model is constructed by the 
user (usually a domain expert) through a 
technique that has come to be known as 
symbolic authoring: the ?author? uses a 
specially-built knowledge editor to construct the 
symbolic source of the target text. These editors 
are interfaces that allow authors to build the 
domain model using a representation that is 
more ?natural? to them than the artificial 
language of the knowledge base.3 The purpose 
of these representations is to provide feedback 
intended to make the content of the domain 
model more available to casual inspection than 
the knowledge representation language of the 
                                                           
1 By complete systems, we refer to systems that determine 
both ?what to say? and ?how to say it?, taking as input a 
specification that is not a hand-crafted simulation of some 
intermediate representation. 
2 Mittal et al, 1998, pg. 438. 
3 See Scott, Power and Evans, 1998. 
domain model. As such, they are obvious 
candidates as the standard against which to 
measure the content of the texts that are 
generated from them. 
We first consider the case of feedback 
presented in graphical mode, and then the option 
of textual feedback, using the WYSIWYM 
technology (Power and Scott, 1998; Scott, 
Power and Evans, 1998). We go on to make 
recommendations concerning the desirable 
properties of the feedback text. 
4 Graphical representations of content 
Symbolic authoring systems typically make use 
of graphical representations of the content of the 
domain model?for example, conceptual graphs 
(Caldwell and Korelsky, 1994). Once trained in 
the language of the interface, the domain 
specialist uses standard text-editing devices such 
as menu selection and navigation with a cursor, 
together with standard text-editing actions (e.g., 
select, copy, paste, delete) to create and edit the 
content specification of the text to be generated 
in one or several selected languages. 
The user of AGILE, conceived to be a 
specialist in the domain of the particular 
software for which the manual is required (i.e., 
CAD/CAM), models the procedures for how to 
use the software. AGILE?s graphical user 
interface (Hartley, Power et al, 2000) closely 
resembles the interface that was developed for 
an earlier system, DRAFTER, which generates 
software manuals in English and French (Paris 
et al, 1995). The design of the interface 
represents the components of the procedures 
(e.g., goals, methods, preconditions, sub-steps, 
side-effects) as differently coloured boxes. The 
user builds a model of the procedures for using 
the software by constructing a series of nested 
boxes and assigning labels to them via menus 
that enable the selection of concepts from the 
underlying domain ontology. 
4.1 The input specification for the user 
As part of our evaluation of AGILE, we asked 18 
IT professionals4 to construct a number of 
predetermined content models of various 
degrees of complexity and to have the system 
                                                           
4 There were six for each of the three Eastern European 
languages; all had some (albeit limited) experience of 
CAD/CAM systems and were fluent speakers of English. 
generate text from them in specified styles in 
their native language. Since the evaluation was 
not conducted in situ with real CAD/CAM 
system designers creating real draft manuals, we 
needed to find a way to describe to the 
evaluators what domain models we wanted them 
to build. Among the possible options were to 
give them a copy of either: 
? the desired model as it would appear to 
them in the interface (e.g., Figure 1); 
? the target text that would be produced 
from the model (e.g., Figure 2); 
? a ?pseudo-text? that described the model 
in a form of English that was closer to 
the language of the AGILE interface than 
to fluent English (e.g., Figure 3). 
 
Figure 1: Graphical display of content model 
 
Figure 2: Target text 
 
 
 
 
Figure 3: Pseudo-text input specification 
 
We rejected the first option because it 
amounted to a task of replication which could be 
accomplished successfully even without users 
having any real understanding of the meaning of 
Draw a line by specifying its start and end points. 
To draw a line 
Specify the start point of the line. 
Specify the end point of the line. 
the model they were building. Therefore, it 
would shed no light on how users might be able 
to build a graphical model externalising their 
own mental model. 
We discarded the second because a text may 
not necessarily make any explicit linguistic 
distinction between different components of the 
model?for example, between a precondition on 
a method and the first step in a method 
consisting of several steps5.  Thus, in general, 
target texts may not reflect every distinction 
available in the underlying domain model 
(without this necessarily causing any confusion 
in the mind of the reader). As a result of such 
underspecification, they are ill-suited to serving 
as a staring point from which a symbolic author 
could build a formal model. 
We opted, then, for providing our evaluators 
with a pseudo-text in which there was an 
explicit and regular relationship between the 
compo seudo-
textua of the 
pseudo
 
 
 
 
 
 
 
 
 
 
 
 
F
4.2 
This p
of ju
betwe
           
5 For ex
make su
medium
pluck th
hours.? 
We focused on (a), which was of course 
mediated by (c); that is, we focused on the issue 
of creating an accurate model. This is an easier 
issue than that of the fidelity of the output text to 
the model (b), while the representations in (d) 
are too remote from one another to permit useful 
comparison. 
To measure the correspondence between the 
actual models and the desired/target models, we 
adopted the Generation String Accuracy (GSA) 
metric (Bangalore, Rambow and Whittaker, 
2000; Bangalore and Rambow, 2000) used in 
evaluating the output of a NLG system. It 
extends the simple Word Accuracy metric 
suggested in the MT literature (Alshawi et al, 
1998), based on the string edit distance between 
some reference text and the output of the 
system. As it stands, this metric fails to account 
for some of the special properties of the text 
generation task, which involves ordering word 
tokens. Thus, corrections may involve re-
ordering tokens. In order not to penalise a 
misplaced constituent twice?as both a deletion 
and an insertion?the generation accuracy 
metric treats the deletion (D) of a token from 
one location and its insertion (I) at another 
location as a single movement (M). The 
remaining deletions, insertions, and substitutions 
(S) are counted separately. Generation accuracy 
is given by the following equation, where R is 
the number of (word) tokens in the reference nents of the procedures and their p
l expression.   Figure 4 is one 
-texts used in the evaluation. 
Draw an arc 
 First, start-tool the ARC command. 
M1. Using the Windows 
operating system: choose the 3 
Points option from the Arc 
flyout on the Draw toolbar. 
M2. Using the DOS or UNIX 
operating system: igure 4: fragment of a typical pseudo-text 
Evaluating the fidelity of the output 
articular set-up afforded us the possibility 
dging the fidelity of the ?translation? 
en the following representations: 
a) desired model and model produced 
b) model produced and output text 
c) pseudo-text and model produced 
d) pseudo-text and the output text 
                                                
ample, between: ?To cook a goose: Before starting, 
re the goose has been plucked. Put the goose in a 
 oven for 1.5 hours.? and ?To cook a goose: First 
e goose. Then put it in a medium oven for 1.5 
text.







+++
?= R
SDIMAccuracyGeneration 1  
For Bangalore and his colleagues, the 
reference text is the desired text; it is a gold 
standard given a priori by a corpus representing 
the target output of the system. The generation 
accuracy of a string from the actual output of the 
system is computed on the basis of the number 
of movements, substitutions, deletions and 
insertions required to edit the string into the 
desired form. 
In our case, the correspondence was 
measured between models rather than texts, but 
we found the metric ?portable?. The tokens are 
no longer textual strings but semantic entities. 
Although this method provided a useful 
quantitative measure of the closeness of the fit 
of the actual generated text to what was 
intended, it is not without problems, some of 
choose the Arc option from 
the  Draw menu. 
choose 3 Points option. 
 Specify the start point of the arc.
which apply irrespective of whether the metric is 
applied to texts or to semantic models. For 
example, it does not capture qualitative 
differences between the generated object and the 
reference object, that is, it does not distinguish 
trivial from serious mistakes. Thus, representing 
an action as the first step in a procedure rather 
than as a precondition would have less impact 
on the end-reader?s ability to follow the 
instructions than would representing a goal as a 
side-effect.6 
5 Textual representations of content 
Once the model they represent becomes 
moderately complex, graphical representations 
prove to be difficult to interpret and unwieldy to 
visualise and manipulate (Kim, 1990; Petre, 
1995). WYSIWYM offers an alternative, textual 
modality of feedback, which is more intuitive 
and natural. As we will discuss below, there is a 
sense in which, in its current form, the feedback 
text may be too natural. 
5.1 Current status of  WYSIWYM feedback 
text 
The main purpose of the text generated in 
feedback mode, as currently conceived, is to 
show the symbolic author the possibilities for 
further expanding the model under development. 
As with AGILE?s box representation, 
clicking on a coloured ?anchor? brings up a 
menu of legitimate fillers for that particular slot 
in the content representation. Instantiating green 
anchors is optional, but all red anchors must be 
instantiated for a model to be potentially 
complete (Figure 5). Once this is the case, 
authors tend to switch to output mode, which 
produces a natural text reflecting the specified 
model and nothing else. 
 
 
 
 
 
 
 
Figure 5: fragment of a typical feedback text 
 
                                                           
6 See Hartley et al(2000) for further discussion of this 
issue and the results of the AGILE evaluation. 
In WYSIWYM systems the same generator 
is used to produce both the feedback and output 
texts; this means that the feedback text can be as 
fluent as the output text. In its current 
instantiations, this is precisely what is produced, 
even when the generator is capable of producing 
texts of rather different styles for the different 
purposes.7 
5.2 Feedback in a controlled language 
The motivation for generating a new type of 
feedback text comes from two sources. 
The first is the pseudo-texts that we 
constructed by hand for the AGILE evaluation. 
As far as the form of the models actually 
constructed is concerned, they proved 
consistently reliable guides for the symbolic 
authors. Where they proved inadequate was in 
their identification of multiple references to the 
same domain model entity; several authors 
tended to create multiple instances of an entity 
rather than multiple pointers to a single instance. 
Let us now turn from the testing scenario, where 
authors have a defined target to hit, and consider 
instead a production setting where the author is 
seeking to record a mental model. It is a simple 
matter to have the system generate a second 
feedback text, complementing the present one, 
this time in the style of the pseudo-texts8 for the 
purpose of describing unambiguously, if 
rebarbatively, the state of a potentially complete 
model. 
The second is Attempto Controlled English 
(ACE: Fuchs and Schwitter, 1996; Fuchs, 
Schwertel and Schwitter, 1999), which allows 
domain specialists to interactively formulate 
software requirements specifications. The 
specialists are required to learn a number of 
compositional rules which they must then apply 
when writing their specifications. These are 
parsed by the system. 
For all sentences that it accepts, the system 
creates a paraphrase (Figure 6) that indicates its 
interpretations by means of brackets. These 
interpretations concern phenomena like 
anaphoric reference, conjunction and 
disjunction, attachment of prepositional phrases, 
relative clauses and quantifier scope. The user 
                                                           
7 As, for example, in the ICONOCLAST system (see 
http://www.itri.bton.ac.uk/projects/iconoclast). 
8 Modulo the reference problems, for which a solution is 
indicated below 
1. Do <red>this action</red> by using 
<green>this method</green>. 
2. Schedule <red>this event</red> by 
using <green>this method</green>. 
3. Schedule the appointment by using 
<green>this method</green>. 
either accepts the interpretation or rephrases the 
input to change it. 
 
 
 
 
 
 
 
 
 
 
Figure 6: ACE paraphrases 
 
The principle of making interpretations 
explicit appears to be good one in the NLG 
context too, especially for the person 
constructing the domain model. Moreover, in 
the context where the output text is required to 
be in a controlled language, the use of 
WYSIWYM relieves the symbolic author of the 
burden of learning the specialized writing rules 
of the given control language. 
Optimising the formulation of the controlled 
language feedback is matter of iteratively 
revising it via the testing scenario, using GSA as 
the metric, until authors consistently achieve 
total fidelity of the models they construct with 
the reference models. 
6 Conclusions 
So how can go about judging whether the 
products of NLG systems express the intended 
message? A first step towards this goal is to 
enable symbolic authors to satisfy themselves 
that they have built the domain model they had 
in mind. Graphical feedback is too difficult to 
interpret, while natural language output that is 
optimised for the end-reader may not show the 
unequivocal fidelity to the domain model that 
the symbolic author requires. 
We have suggested that textual feedback in 
a form close to a controlled language used for 
specifying software requirements is a good 
candidate for this task. We have further outlined 
a method for incrementally refining this 
controlled language by monitoring symbolic 
authors? ability to construct reference domain 
models on the basis of controlled language 
feedback. The trade-off between transparency 
and naturalness in the output text intended for 
the end-reader will involve design decisions 
based on, among other things, reader profiling. 
Assessing the fidelity of the end-reader text to 
the model is also a necessary step, but not one 
that can be conflated with or precede that of 
validating the accuracy of the model with 
respect to the author?s intentions. 
 
Acknowledgements 
The work described in the paper has been 
supported by EC INCO-COPERNICUS project 
PL961104 AGILE ?Automatic generation of 
Instructions in Languages of Eastern Europe?. 
The authors express their gratitude to all the 
partners of, and participants in, the AGILE 
project, upon whose work this paper reports. 
 
References 
Alshawi, H., Bangalore, S. and Douglas, S. (1998). 
Automatic acquisition of hierarchical transduction 
models for machine translation. Proceedings of the 
36th Annual Meeting of the Association for 
Computational Linguistics and the 17th 
International Conference on Computational 
Linguistics (COLING-ACL?98), Montreal, 
Canada, pp. 41 ? 47 
Arnold, D., Balkan, L., Lee Humphreys, R., Meijer, 
S. and Sadler, L. (1994). Machine translation: an 
introductory guide. Blackwell. 
Bangalore, S. and Rambow, O. (2000). Exploiting a 
Hierarchical Model for Generation. Proceedings of 
the 18th International Conference on 
Computational Linguistics (COLING?2000), 
Saarbruecken, Germany, pp. 42 ? 48. 
Bangalore, S., Rambow, O. and Whittaker, S. (2000). 
Evaluation Metrics for Generation. Proceedings of 
the 1st International Conference on Natural 
Language Generation, Mitzpe Ramon, Israel, pp. 1 
? 8. 
Cahill, L., Doran, C., Evans, R., Mellish, C., Paiva, 
D., Reape, M., Scott, D. and Tipper, N. (1999). In 
search of a reference architecture for NLG 
systems. Proceedings of the 7th European 
Workshop on Natural Language Generation 
(EWNLG'99), Toulouse, France, pp 77 ? 85. 
Caldwell, T. and Korelsky, T. (1994). Bilingual 
generation of job descriptions from quasi-
conceptual forms. Proceedings of the Fourth 
Conference on Applied Natural Language 
Processing (ANLP?94), pp. 1 ? 6. 
Input: 
The customer enters a card and a numeric personal 
code. If it is not valid then SM rejects the card. 
 
Paraphrase: 
The customer enters a card and [the customer 
enters] a numeric personal code. If [the personal 
code] is not valid then [SimpleMat] rejects the card.
Carroll, J.B. (1966). An experiment in evaluating the 
quality of translations. In J. Pierce. Language and 
machines: computers in translation and 
linguistics. Report by the Automatic Language 
Processing Advisory Committee (ALPAC). 
Publication 1416. National Academy of Sciences 
National Research Council, pp. 67 ? 75. 
Fuchs, N.E. and Schwitter, R. (1996). Attempto 
Controlled English (ACE). Proceedings of the 1st 
International Workshop on Controlled Language 
Applications (CLAW?96), Leuven, Belgium. 
Fuchs, N.E., Schwertel, U. and Schwitter, R. (1999). 
Attempto Controlled English (ACE) Language 
Manual Version 3.0, Technical Report 99.03, 
Department of Computer Science, University of 
Zurich, August 1999. 
Hartley, A., Power, R., Scott, D. and Varbanov, S. 
(2000). Design specification of the user interface 
for the AGILE final prototype. Deliverable INTF2 
of INCO-COPERNICUS project PL961104 
AGILE: ?Automatic. Generation of Instructions in 
Languages of Eastern Europe?. Available at 
http://www.itri.bton.ac.uk. 
Hartley, A., Scott, D., Kruijff-Korbayova, I., Sharoff, 
S. et al (2000). Evaluation of the final prototype. 
Deliverable EVAL2 of INCO-COPERNICUS 
project PL961104 AGILE: ?Automatic. Generation 
of Instructions in Languages of Eastern Europe?. 
Available at http://www.itri.bton.ac.uk. 
Kim, Y. (1990). Effects of conceptual data modelling 
formalisms on user validation and analyst 
modelling of information requirements.  PhD 
thesis, University of Minnesota. 
Kruijff, G-J., Teich, E., Bateman, J., Kruijff-
Korbayova, I. et al (2000). Multilinguality in a 
text generation system for three Slavic languages. 
Proceedings of the 18th International Conference 
on Computational Linguistics (COLING?2000), 
Saarbruecken, Germany, pp. 474 ? 480. 
Lehrberger, J. & Bourbeau, L. (1987) Machine 
translation: linguistic characterisitics of MT 
systems and general methodology of evaluation. 
John Benjamins. 
Mittal, V.O, Moore, J., Carenini, G. and Roth, S. 
(1998). Describing Complex Charts in Natural 
Language: A Caption Generation System. 
Computational Linguistics, 24(3), pp. 431 ? 468. 
Nagao, M. Tsujii, J. and Nakamura, J. (1985). The 
Japanese government project for machine 
translation. Computational Linguistics, 11(2-3), 
pp. 91 ? 109. 
Paris, C., Vander Linden, K., Fischer, M., Hartley, 
T., Pemberton, L., Power, R. and Scott, D. (1995). 
A Support Tool for Writing Multilingual 
Instructions. Proceedings of the Fourteenth 
International Joint Conference in Artificial 
Intelligence (IJCAI?95), pp. 1395 ? 1404.  
Petre, M. (1995). Why looking isn?t always seeing: 
readership skills and graphical programming, 
Communications of the ACM, 38(6), pp. 33 ? 42. 
Power, R. and Scott, D. (1998) Multilingual 
Authoring Using Feedback Texts. Proceedings of 
the 36th Annual Meeting of the Association for 
Computational Linguistics and the 17th 
International Conference on Computational 
Linguistics, Montreal, Canada, pp. 1053 ? 1059. 
Roth, S.F., Kolojejchick, J., Mattis, J. and Goldstein, 
J. (1994) Interactive graphics design using 
automatic presentation knowledge.  Proceedings 
of CHI?94: Human Factors in Computing 
Systems, Boston, M.A. 
Scott, D.R., Power, R., and Evans, R. (1998) 
Generation as a Solution to Its Own Problem. 
Proceedings of the 9th International Workshop on 
Natural Language Generation (INLG'98), Niagara-
on-the-Lake, Canada, pp. 256 ?265. 
 
 
 
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 101?112,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Design of a hybrid high quality machine translation system 
Kurt Eberle 
Johanna Gei? 
Mireia Ginest?-Rosell 
Bogdan Babych  
Anthony Hartley 
Reinhard Rapp  
Lingenio GmbH Serge Sharoff 
Karlsruher Stra?e 10 
69 126 Heidelberg, Germany 
Martin Thomas 
Centre for Translation Studies 
University of Leeds 
 Leeds, LS2 9JT, UK 
[k.eberle,j.geiss,m.ginesti-rosell] 
@lingenio.de 
[B.Babych,A.Hartley,R.Rapp, 
S.Sharoff,M.Thomas]@leeds.ac.uk  
  
 
 
Abstract 
This paper gives an overview of the 
ongoing FP7 project HyghTra (2010 ? 
2014). The HyghTra project is conducted 
in a partnership between academia and 
industry involving the University of Leeds 
and Lingenio GmbH (company). It adopts a 
hybrid and bootstrapping approach to the 
enhancement of MT quality by applying 
rule-based analysis and statistical 
evaluation techniques to both parallel and 
comparable corpora in order to extract 
linguistic information and enrich the lexical 
and syntactic resources of the underlying 
(rule-based) MT system that is used for 
analysing the corpora. The project places 
special emphasis on the extension of 
systems to new language pairs and 
corresponding rapid, automated creation of 
high quality resources. The techniques are 
fielded and evaluated within an existing 
commercial MT environment. 
1 Motivation 
Statistical Machine Translation (SMT) has been 
around for about 20 years, and for roughly half of 
this time SMT and the 'traditional' Rule-based 
Machine Translation (RBMT) have been seen as 
competing paradigms. During the last decade 
however, there is a trend and growing interest in 
combining the two methodologies. In our approach 
these two approaches are viewed as 
complementary. 
Advantages of SMT are low cost and robustness, 
but definite disadvantages of (pure) SMT are that it 
needs huge amounts of data, which for many 
language pairs are not available and are unlikely to 
become available in the future. Also, SMT tends to 
disregard important classificatory knowledge (such 
as morphosyntactic, categorical and lexical class 
features), which can be provided and used 
relatively easily within non-statistical 
representations.  
On the other hand, advantages of RBMT are that 
its (grammar and lexical) rules and information are 
understandable by humans and can be exploited for 
a lot of applications outside of translation 
(dictionaries, text understanding, dialogue systems, 
etc.).  
The slot grammar approach used in Lingenio 
systems (cf.  McCord 1989, Eberle 2001) is a 
prime example of such linguistically rich 
representations that can be used for a number of 
different applications. Fig.1 shows this by a 
visualization of (an excerpt of) the entry for the 
ambiguous German verb einstellen in the database 
that underlies (a)  the Lingenio translation 
products, where it links up with corresponding set 
of the transfer rules, and (b) Lingenio?s dictionary 
product TranslateDict, which is primarily intended 
for human translators.   
 
101
 
 
Fig 1 a) data base entry einstellen 
('translation' represents links between SL and T entries) 
 
 
 
 
Fig 1 b) product entry einstellen 
 
The obvious disadvantages of RBMT are high cost, 
weaknesses in dealing with incorrect input and in 
making correct choices with respect to ambiguous 
words, structures, and transfer equivalents. 
SMT output is often surprisingly good with respect 
to short distance collocations, but often misses 
correct choices are missed in cases where 
selectional restrictions take effect on distant words. 
RBMT output is generally good if the parser 
assigns the correct analysis to a sentence and  if the 
target words can be correctly chosen from the set 
of alternatives. However, in the presence of 
ambiguous words and structures, and where 
linguistic information is lacking, the decisions may 
be wrong. 
Given the complementarity of SMT and RBMT 
and their very different strengths and weaknesses, 
we take a view that an optimized MT architecture 
must comprise elements of both paradigms. The 
key issue therefore lies in the identification of such 
elements and how to connect them to each other. 
We propose a specific type of hybrid translation ? 
hybrid high quality translation (HyghTra), where 
core RBMT systems are created and enhanced by a 
range of reliable statistical techniques. 
 
2 Development Methodology 
Many hybrid systems described in the literature 
have attempted to put some analytical abstraction 
on top of an SMT kernel.1 In our view this is not 
the best option because, according to the 
underlying philosophy, SMT is linguistically 
ignorant at the beginning and learns all linguistic 
rules automatically from corpora. However, the 
extracted information is typically represented in 
huge data sets which are not readable by humans in 
a natural way. This means that this type of 
architecture does not easily provide interfaces for 
incorporating linguistic knowledge in a canonical 
and simple way. 
Thus we approach the problem from the other end, 
, integrating information derived from corpora 
using statistical methods into RBMT systems. 
Provided the underlying RBMT systems are 
linguistically sound and sufficiently modular in 
structure, we believe this to have greater potential 
for generating high quality output. 
We currently use and carry out the following work 
plan: 
 
(I) Creation of MT systems  
(with rule-based core MT information and 
statistical extension and training): 
(a) We start out with declarative analysis and 
generation components of the considered 
languages, and with basic bilingual dictionaries 
connecting to one another the entries of relatively 
small vocabularies comprising the most frequent 
words of each language in a given translation pair 
(cf. Fig 1 a). 
(b) Having completed this phase, we extend the 
dictionaries and train the analysis-, transfer- and 
generation-components of the rule-based core 
systems using monolingual and bilingual corpora.  
 
                                                           
1 A prominent early example is Frederking and 
colleagues (Frederking & Nirenburg, 1994). For an 
overview of  hybrid MT till the late nineties see Streiter 
et al (1999). More recent  approaches include Groves & 
Way (2006a, 2006b). Commercial implementations 
include AppTek (http://www.apptek.com) and Language 
Weaver (http://www.languageweaver.com). An ongoing 
MT important project investigating hybrid methods is 
EuroMatrixPlus (http://www.euromatrixplus.net/) 
102
(II) Error detection and improvement cycle:  
(a) We automatically discover the most frequent 
problematic grammatical constructions and 
multiword expressions for commercial RBMT and 
SMT systems using automatic construction-based 
evaluation as proposed in (Babych and Hartley, 
2009) and develop a framework for fixing 
corresponding grammar rules and extending 
grammatical coverage of the systems in a semi-
automatic way. This shortens development time for 
commercial MT and contributes to yielding 
significantly higher translation quality. 
 
(III) Extension to other languages: 
Structural similarity and translation by pivot 
languages is used to obtain extension to further 
languages: 
High-quality translation between closely related 
languages (e.g., Russian and Ukrainian or 
Portuguese and Spanish) can be achieved with 
relatively simple resources (using linguistic 
similarity, but also homomorphism assumptions 
with respect to parallel text, if available), while 
greater efforts are put into ensuring better-quality 
translation between more distant languages (e.g. 
German and Russian). According to our prior 
research (Babych et al, 2007b) the pipeline 
between languages of different similarity results in 
improved translation quality for a larger number of 
language pairs (e.g., MT from Portuguese or 
Ukrainian into German is easier if there are high-
quality analysis and transfer modules for Spanish 
and Russian into German (respectively). Of course, 
(III) draws heavily on the detailed analysis and MT 
systems that the industrial partner in HyghTra 
provides for a number of languages. 
 
In the following sections we give more details of 
the work currently done with regard to (I) and with 
regard to parts of (II): the creation of a new MT 
system following the strategy sketched. We cannot 
go further into detail with (II) and (III) here, which 
will become a priority for future research. 
3 Creation of a new system 
Early pilot studies covering some aspects of the 
strategy described here (using information from 
pivot languages and similarity) showed promising 
results (Rapp, 1999; Rapp & Mart?n Vide, 2007; 
see also Koehn & Knight, 2002). 
We expect that the proposed semi-automatic 
creation of a new MT system as sketched above 
will work best if one of the two languages involved 
is already 'known' by modules to which the system 
has access. Against the background of the pipeline 
approach mentioned above in (III), this means that 
we assume an analysis and translation system that 
continuously grows by 'learning' new languages 
where 'learning' is facilitated by information about 
the languages already 'known' and by exploiting 
similarity assumptions ? and, of course, by being 
fed with information prepared and provided by the 
human 'companion' of the system. 
From this perspective, we assume the following 
steps of extending the system (with work done by 
the 'companion' and work done by the system) 
 
1. Acquire parallel and comparable corpora. 
2. Define a core of the morphology of the new 
language and compile a basic dictionary for the 
most frequent words and translations. 
Morphological representations and features for 
new languages are derived both manually and 
automatically, as proposed in (Babych et al, 
2012 (in preparation)). 
3. Using established alignment technology (e.g. 
Giza++) and parallel corpora, generate a first 
extension of this dictionary. 
4. Expand the dictionary of step 3 using 
comparable corpora as proposed in a study by 
Rapp (1999). This is applicable mainly to single 
word units. 
5. Expand coverage of multiword-units using 
novel technology. 
6. Cross-validate the new dictionary with respect 
to available ones by transitivity. 
7. Integrate the new dictionary into the new MT 
system as developing from reusing components 
and adding new components as in 8. 
8. Complete morphology and spell out declarative 
analysis and generation grammar for the new 
language. 
9. Automatically evaluate the translations of the 
most frequent grammatical constructions and 
multiword expressions in a machine-translated 
corpus, prioritising support for these 
constructions with a type of risk-assessment 
framework proposed in Babych and Hartley 
(2008). 
10. Extend support for high-priority constructions 
semi-automatically by mining correct 
103
translations from parallel corpora. 
11. Train and evaluate the new grammar and 
transfer of the new MT system using the new 
dictionary on the basis of available parallel 
corpora. 
 
The following sections give an overview of the 
different steps. 
Step 1: Acquire parallel and comparable 
corpora 
As our parallel corpus, we use the Europarl. The 
size of the current version is up to 40 million 
words per language, and several of the languages 
we are currently considering are covered. Also, we 
make use of other parallel corpora such as the 
Canadian Hansards (Proceedings of the Canadian 
Parliament) for the English?French language pair. 
For non-EU Languages (mainly Russian), we 
intend to conduct a pilot study to establish the 
feasibility of retrieving parallel corpora from the 
web, a problem for which various approaches have 
been proposed (Resnik, 1999; Munteanu & Marcu, 
2005; Wu & Fung, 2005).  
In addition to the parallel corpora, we will need 
large monolingual corpora in the future (at least 
200 million words) for each of the six languages. 
Here, we intend to use newspaper corpora 
supplemented with text collections downloadable 
from the web.  
The corpora are stored in a database that allows 
for assigning analyses of different depth and nature 
to the sentences and for alignment between the 
sentences and their analyses. The architecture of 
this database and the corresponding analysis and 
evaluation frontend is described in (Eberle et al
2010, 2012). Section Results contains examples of 
such representations. 
Step 2: Compile a basic dictionary for the most 
frequent words 
A prerequisite of the suggested hybrid approach 
with rule-based kernel is to define morphological 
classifications for the new language(s). This is 
done exploiting similarities to the classifications as 
available for the existing languages. Currently, this 
has been carried out for Dutch (on the basis of 
German) and for Spanish (on the basis of 
French/other Romance languages). The most 
frequent words (the basic vocabulary of a 
language) are typically also the most ambiguous 
ones. Since the Lingenio systems are lexically 
driven transfer systems (cf. Eberle 2001), we 
define (a) structural conditions,  which inform the 
choice of the possible target words (single words 
or multiword expressions) and (b)restructuring 
conditions, as necessary (cf. Fig 1 a:  attributes 
'transfer conditions' and 'structural change'). In 
order to ensure quality this must be done by human 
lexicographers and therefore costly for a large 
dictionary. However, we manually create only very 
small basic dictionaries and extend these (semi-
automatically) step 3 and those which follow. 
Some important morphosyntactic features of the 
language are derived from a monolingual corpus 
annotated with publicly available part-of-speech 
taggers and lemmatisers. However, these tools 
often do not explicitly represent linguistic features 
needed for the generation stage in RBMT. In 
(Babych et al, 2012) we propose a systematic 
approach to recovering such missing generation-
oriented representations from grammar models and 
statistical combinatorial properties of annotated 
features. 
Step 3: Generating dictionary extensions from 
parallel corpora 
Based on parallel corpora, dictionaries can be 
derived using established techniques of automatic 
sentence alignment and word alignment. For 
sentence alignment, the length-based Gale & 
Church aligner (1993) can be used, or ? 
alternatively ? Dan Melamed?s GSA-algorithm 
(Geometric Sentence Alignment; Melamed, 1999).  
For segmentation of text we use corresponding 
Lingenio-tools (unpublished).2 
For word alignment Giza++ (Och & Ney, 2003) is 
the standard tool. Given a word alignment, the 
extraction of a (SMT) dictionary is relatively 
straightforward. With the exception of sentence 
segmentation, these algorithms are largely 
language independent and can be used for all of the 
languages that we consider. We did this for a 
number of language pairs on the basis of the 
                                                           
2  If these cannot be applied because of  lack of 
information about a language, we intend to use the 
algorithm by Kiss & Strunk (2006). An open-source 
implementation of parts of the Kiss & Strunk algorithm 
is available from Patrick Tschorn at 
http://www.denkselbst.de/sentrick/index.html. 
104
Europarl-texts considered (as stored in our 
database). In order to optimize the results we use 
the dictionaries of step 1 as set of cognates (cf. 
Simard at al 1992, Gough & Way 2004), as well as 
other words easily obtainable from the internet that 
can be used for this purpose (like company names 
and other named entities with cross-language 
identity and terminology translations). Using the 
morphology component of the new language and 
the categorial information from the transfer 
relation, we compute the basic forms of the 
inflected words found. Later, we intend to further 
improve the accuracy of word alignment by 
exploiting chunk type syntactic information of the 
narrow context of the words (cf. Eberle & Rapp 
2008). An early stage variant of this is already used 
in Lingenio products. The corresponding function 
AutoLearn<word> extracts new word relations on 
the basis of existing dictionaries and (partial) 
syntactic analyses. (Fig 2 gives an example). 
 
 
 
 
 
 
 
 
 
 
Fig 2 AutoLearn<word>: new entries using 
transfer links and syntactic analysis 
 
Given the relatively small size of the available 
parallel corpora, we expect that the automatically 
generated dictionaries will comprise about 20,000 
entries each (This corresponds to first results on 
the basis of German?English). This is far too 
small for a serious general purpose MT system. 
Note that, in comparison, the English?German 
dictionary used in the current Lingenio MT 
product comprises more than 480,000 keywords 
and phrases. 
Step 4: Expanding dictionaries using 
comparable corpora (word equations) 
In order to expand the dictionaries using a set of 
monolingual comparable corpora, the basic 
approach pioneered by Fung & McKeown (1997) 
and Rapp (1995, 1999) is to be further developed 
and refined in the second phase of the project as to 
obtain a practical tool that can be used in an 
industrial context. 
The basic assumption underlying the approach 
is that across languages there is a correlation 
between the co-occurrences of words that are 
translations of each other. If ? for example ? in a 
text of one language two words A and B co-occur 
more often than expected by chance, then in a text 
of another language those words that are 
translations of A and B should also co-occur more 
frequently than expected. It is further assumed that 
a small dictionary (as generated in step 2) is 
available at the beginning, and that the aim is to 
expand this basic lexicon. Using a corpus of the 
target language, first a co-occurrence matrix is 
computed whose rows are all word types occurring 
in the corpus and whose columns are all target 
words appearing in the basic lexicon. Next a word 
of the source language is considered whose 
translation is to be determined. Using the source-
language corpus, a co-occurrence vector for this 
word is computed. Then all known words in this 
vector are translated to the target language. As the 
basic lexicon is small, only some of the 
translations are known. All unknown words are 
discarded from the vector and the vector positions 
are sorted in order to match the vectors of the 
target-language matrix. Using standard measures 
for vector similarity, the resulting vector is 
compared to all vectors in the co-occurrence 
matrix of the target language. The vector with the 
highest similarity is considered to be the 
translation of our source-language word. 
From a previous pilot study (Rapp, 1999) it can 
be expected that this methodology achieves an 
accuracy in the order of 70%, which means that 
only a relatively modest amount of manual post-
editing is required.  
The automatically generated results are 
improved and the amount of post-editing is 
reduced by exploiting sense (disambiguation) 
information as available from the analysis 
component for the 'known' language of the new 
language pair.. Also we try to exploit categorial 
and underspecified syntactic information of the 
contexts of the words similar to what has been 
suggested for improving word alignment in the 
previous step (see also Fig.2). Also, as the frequent 
words are already covered by the basic lexicon 
(whose production from parallel corpora on the 
basis of a manually compiled kernel does not show 
 
105
an ambiguity problem of similar significance), and 
as experience shows that most low frequency 
words in a full-size lexicon tend to be 
unambiguous, the ambiguity problem is reduced 
further for the words investigated and extracted by 
this comparison method. 
Step 5: Expanding dictionaries using 
comparable corpora (multiword units) 
In order to account for technical terms, idioms, 
collocations, and typical short phrases, an 
important feature of an MT lexicon is a high 
coverage of multiword units. Very recent work 
conducted at the University of Leeds (Sharoff et 
al., 2006) shows that dictionary entries for such 
multiword units can be derived from comparable 
corpora if a dictionary of single words is available. 
It could even be shown that this methodology can 
be superior to deriving multiword-units from 
parallel corpora (Babych et al, 2007). This is a 
major breakthrough as comparable corpora are far 
easier to acquire than parallel corpora. It even 
opens up the possibility of building domain-
specific dictionaries by using texts from different 
domains. 
The outline of the algorithm is as follows: 
? Extract collocations from a corpus of the 
source language (Smadja, 1993) 
? To translate a collocation, look up all its 
words using any dictionary 
? Generate all possible permutations 
(sequences) of the word translations 
? Count the occurrence frequencies of these 
sequences in a corpus of the target 
language and test for significance 
? Consider the most significant sequence to 
be the translation of the source language 
collocation 
Of course, in later steps of the project, we will 
experiment on filtering these sequences by 
exploiting structural knowledge similarly to what 
was described in the two previous steps. This can 
be obtained on the basis of the declarative analysis 
component of the new language which is 
developed in parallel. 
Step 6: Cross-validate dictionaries 
The combination of the corpus-based methods for 
automatic dictionary generation as described in 
steps 3 to 5 will lead to high coverage dictionaries 
as the availability of very large monolingual 
corpora is no major problem for our languages. 
However, as all steps are error prone, it can be 
expected that a considerable number of dictionary 
entries (e.g. 50%) are not correct. To facilitate (but 
not eliminate) the manual verification of the 
dictionary, we will  perform an automatic cross-
check which utilizes the dictionaries? property of 
transitivity. What we mean by this is that if we 
have two dictionaries, one translating from 
language A to language B, the other from language 
B to language C, then we can also translate from 
language A to C by use of the intermediate 
language (or interlingua) B. That is, the property of 
transitivity, although having some limitations due 
to ambiguity problems, can be exploited to 
automatically generate a raw dictionary for A to C. 
Lingenio  has some experience with this method 
having exploited it for extending and improving its 
English ? French dictionaries using French ? 
German and German ? English. 
As the corpus-based approach (steps 3 to 5) 
allows us to also generate this type of dictionary  
via comparable corpora, we have two different 
ways to generate a dictionary for a particular 
language pair. This means that we can validate one 
with the other. Furthermore, with increasing 
number of language pairs created, there are more 
and more languages that can serve as interlingua or 
'pivot': This, step by step, gives an increasing 
potential for mutual cross-validation.  
Specific attention will be paid to automating as 
far as possible the creation of selectional 
restrictions to be assigned to the transfer relations 
of the new dictionaries in all steps of dictionary 
creation (2?6). We will try to do this on the basis 
of the analysis components as available for the 
languages considered: These are: a completely 
worked out analysis component for the 'old' 
language, a declarative (chunk parsing) component 
for the new one (compare the two following steps 
for this).  
Step 7: Integrate dictionaries in existing 
machine translation systems 
Lingenio has a relatively rich infrastructure for 
automatic importation of various kinds of lexical 
information into the database used by the analyses 
and translation systems. If necessary the 
information on hand (for instance from 
conventional dictionaries of publishing houses) is 
106
completed and normalized during or before 
importation. This may be executed completely 
automatically ? by using the existing analyses 
components and resources respectively as 
databases ? or interactively ? by asking the 
lexicographer for additional information, if needed. 
For example, there may be a list of multiword 
expressions to be imported into the database. In 
order to have available correct syntactic and 
semantic information for these expressions, they 
are analysed by the parser of the corresponding 
language. From the analysis found, the information 
necessary to describe the new lemma in the lexicon 
with respect to semantic type and syntactic 
structure is obtained. The same information is used 
to automatically create correct restructuring 
constraints for translation relations which use the 
new lemma as target. If the parser does not find a 
sound syntactic description, for example because 
some basic information or the expression is 
missing in the lexical database, the lexicographer is 
asked for the missing information or is handed 
over the expression to code it manually.  
Using these tools importation of new lexical 
information, as provided in the previous steps, is 
considerably accelerated.  
Step 8: Compile rule bases for new language 
pairs 
Although experience clearly shows that 
construction and maintenance of the dictionaries is 
by far the most expensive task in (rule-based) 
Machine Translation, the grammars (analysis and 
generation) must of course be developed and 
maintained also. Lingenio has longstanding 
experience with the development of grammars, 
dictionaries and all other components of RBMT.  
The used grammar formalism (slot grammar, 
cf. McCord 1991) is unification based and its 
structuring focuses on dependency, where phrases 
are analysed into heads and grammatical roles ? so 
called (complement and adjunct) slots.  
The grammar formalism and basic rule types 
are designed in a very general way in order to 
allow good portability from one language to 
another such that spelling out the declarative part 
of a grammar does not take very much time (2-4 
person months approx. for relatively similar 
languages like Romance languages according to 
our experience). The portation of linguistic rules to 
new languages is also facilitated by the modular 
design with clearly defined interfaces that make it 
relatively straightforward to integrate information 
from corpora. 
Given a parallel corpus as acquired in step 1, 
the following procedure defines grammar develop-
ment:  
 
1. Define a declarative grammar for the new 
language and train this grammar on the parallel 
-corpus according to the following steps: 
2. Use a chunk parser for the grammar on the 
basis of an efficient part-of-speech tagger for 
the new language.  
3. Combine the chunk analyses of the sentence, 
according to suggestions for packed syntactic 
structures (cf. Schiehlen 2001 and others) and 
underspecified representation structures 
respectively (cf. Eberle, 2004, and others), 
such that the result represents a disjunction of 
the possible analyses of the sentence. 
4. Filter the alternatives of the representation by 
using mapping constraints between source and 
target sentence as can be computed from the 
lexical transfer relations and the structural 
analysis of the sentence. For instance, if we 
know, as in the example of the last section, that 
in the source sentence there is a relative clause 
with lexical elements A, B, . . . modifying a 
head H and that there are translations TH, TA, 
TB, . . . of H, A, B,. . . , in the target sentence 
which, among other possibilities, can be 
supposed to stand in a similar structural 
relation there, then we prefer this relation to 
the competing structural possibilities. (Fig. 3 in 
section results shows the corresponding 
selection for a German-Spanish example in the 
project database). 
5. For each of the remaining structural 
possibilities of the thus revised underspecified 
representation, take its lexical material and 
underspecified structuring as a context for its 
successful firing. For instance, if the 
possibility is left that O is the direct object of 
VP, where VP is an underspecified verbal 
phrase and O an underspecified nominal 
phrase (i.e. where details of the substructuring 
are not spelled out), take the sentence as a 
reference for direct object complementation 
and O and VP as contexts which accept this 
complementation. 
107
6. Develop more abstract conditions from the 
conditions learned according to (5) and 
integrate the different cases. 
7. Tune the results using standard methods of 
corpus-based linguistics. Among other things 
this means: Distinguish between training and 
test corpora, adjust weights according to the 
results of test runs, etc. 
 
The basic idea of the proposed learning procedure 
is similar to that used with respect to learning 
lexical transfer relations: Do not define the 
statistical model for the ?ignorant? state, where the 
surface items of the bilingual corpora are 
considered. Instead, define it for appropriate 
maximally abstract analyses of the sentences 
(which, of course, must be available 
automatically), because, then, much smaller sets of 
data will do. Here, the important question is: What 
is the most abstract level of representation that can 
be reached automatically and which shows reliable 
results? We think that it is the level of 
underspecified syntactic description as used in the 
procedure above. 
The result of training the grammar is a set of 
rules which assign weights and contexts to each 
filler rule of the declarative grammar and thus 
allow to estimate how likely it is that a particular 
rule is applied in a particular context in comparison 
with other rules (Fig. 4 and 5 in section results 
give an overview of the relevance of  grammar 
rules and their triggering conditions w.r.t. 
German).  
We mentioned that the task of translating texts 
into each other does not presuppose that each 
ambiguity in a source sentence is resolved. On the 
contrary, translation should be ambiguity 
preserving (cf. Kay, Gawron & Norvig 1994, 
compare the example above). It is obvious that 
underspecified syntactic representations as 
suggested here are also especially suited for 
preserving ambiguities appropriately.  
Step 9: Automatically evaluate translations of 
the most frequent grammatical constructions 
and multiword expressions in a machine-
translated corpus 
In a later work package of the project, we will run 
a large parallel corpus through available 
(competitive) MT engines, which will be enhanced 
by automatic dictionaries developed during the 
previous stages. On the source-language side of the 
corpus we will automatically generate lists of 
frequent multiword expressions (MWEs) and 
grammatical constructions using the methodology 
proposed in (Sharoff et al, 2006). For each of the 
identified MWEs and constructions we will 
generate a parallel concordance using open-source 
CSAR architecture developed by the Leeds team 
(Sharoff, 2006). The concordance will be 
generated by running queries to the sentence-
aligned parallel corpora and will return lists of 
corresponding sentences from gold-standard 
human translations and corresponding sentences 
generated by MT. Each of these concordances will 
be automatically evaluated using standard MT 
evaluation metrics, such as BLEU. Under these 
settings parallel concordances will be used as 
standard MT evaluation corpora in an automated 
MT evaluation scenario. 
Normally BLEU gives reliable results for MT 
corpora over 7000 words. However, in (Babych 
and Hartley, 2009; Babych and Hartley, 2008) we 
demonstrated that if the corpus is constructed in 
this controlled way, where evaluated fragments of 
sentences are selected as local contexts for specific 
multiword expressions or grammatical 
constructions, then BLEU scores have another 
?island of stability? for much smaller corpora, 
which now may consist of only five or more 
aligned concordance lines. This concordance-based 
evaluation scenario gives correct predictions of 
translation quality for the local context of each of 
the evaluated expressions. 
The scores for the evaluated MWEs and 
constructions will be put in a risk-assessment 
framework, where we will balance the frequency 
of constructions and their translation quality. The 
top priority receive the most frequent expressions 
that are the most problematic ones for a particular 
MT engine, i.e., with queries with lowest BLEU 
scores for their concordances. This framework will 
allow MT developers to work down the priority list 
and correct or extend coverage for those 
constructions which will have the biggest impact 
on MT quality. 
Step 10: Extend support for high-priority 
constructions semi-automatically by mining 
correct translations from parallel corpora 
At this stage we will automate the procedure of 
correcting errors and extending coverage for 
108
problematic MWEs and grammatical 
constructions, identified in Step 9. For this we will 
exploit alignment between source-language 
sentences and gold-standard human translations. In 
the target human translations we will identify 
linguistically-motivated multiword expressions, 
e.g., using part-of-speech patterns or tf-idf 
distribution templates (Babych et al, 2007) and 
run standard alignment tools (e.g., GIZA++) for 
finding the most probable candidate MWEs that 
correspond to the problematic source-language 
expressions. Source and target MWEs paired in 
this way will form the basis for automatically-
generated grammar rules. The rules will normally 
generalise several pairs of MWEs, and may be 
underspecified for certain lexical or morphological 
features. Later such rules will be manually checked 
and corrected by language specialists in MT 
development teams that work on specific 
translation directions. 
This procedure will allow to speed up the grammar 
development procedure for large-scale MT projects 
and will focus on grammatical constructions with 
the highest impact on MT quality, establishing 
them as a top priority for MT developers. In 
HyghTra and with respect to the languages 
considered there, this procedure will be integrated 
into the grammar development and optimization of 
step 8, in particular it will be related to step 4 of 
the procedure sketched there. With regard to 
integration, we aim at an interleaved architecture in 
the long run.  
Step 11: Bootstrap the system 
In Step 11, the new grammar and the transfer of 
the new MT system and the new dictionary may be 
mutually trained further using the steps before and 
applying the system to additional corpora. 
 
4 Results 
Declarative slot grammars for Dutch and Spanish 
have been developed using the patterns of German 
and French ? where declarative  means that there 
has been used no relevant semantic or other 
information in order to spell out weighting or 
filters for rule application -- the only constraint 
being morphosyntactic accessibility. The necessary 
morphological information has been adapted 
similarly from the corresponding model languages. 
The basic dictionaries have been compiled 
manually (Dutch) or extracted from a conventional 
electronic dictionary (translateDict Spanish).  
For a subset of the Spanish corpus (reference 
sentences of the grammar, parts of the open source 
Leeds corpus (Sharoff, 2006), and Europarl), 
syntactic analyses have been computed and stored 
in the database. As the number of analyses grows 
extremely with the length of sentences, only 
relatively short sentences (up to 15 words)  have 
been considered. These analyses are currently 
compared to the analyses of the German 
translations of the corresponding sentences (one 
translation per sentence), which are taken as a kind 
of 'gold' standard as the German analysis 
component (as part of the translation products) has 
proven to be sufficiently reliable. On the basis of 
the comparison a preference on the competitive 
analyses of the Spanish sentence is entailed and 
used for defining a statistical evaluation 
component for the Spanish grammar. Fig.3 shows 
the corresponding representations in the database 
for the sentence Aumenta la demana de energ?a 
el?ctrica por la ola de calor3  and its translation die 
Nachfrage nach Strom steigt wegen der 
Hitzewelle/the demand for electricity increases 
because of the heat-wave. 
 
 
 
 
 
 
 
 
 
 
Fig.3 Selection of analyses via correspondences 
(prefer first Spanish analysis because of subj-congruity) 
 
The analyses are associated with the corresponding 
creation protocols, which are structured lists whose 
items describe, via the identifiers, which rule has 
been applied when and to what structures in the 
process of creating the analysis. From the selection 
of a best analysis for a sentence, we can entail the 
circumstances under which the application of 
particular rules are preferred. This has been carried 
                                                           
3 Sentence taken from the online newspaper El D?a de 
Concepci?n del Uruguay 
 
 
109
out - not yet for the 'new' language Spanish, but for 
the 'known' language German, in order to obtain a 
measure about how correctly the existing grammar 
evaluation component can be replaced by the 
results of the corresponding statistical study.  
 
Fig.4  Frequency of applications of rules 
 
 
 cluster 
applications 
similarity feas  mod feas head 
383, 384,.. 0,86 sent, ... emosentaffv,.. 
557,558,566,.. 0,68 denselb,.. gebv, ... 
 
Fig.5  Preliminary constraints related to grammar 
rule clusters 
 
Fig.4 shows the distribution of rule usages within 
the training set of analyses (of approx.30.000 
sentences). 390 different rules were used with a 
total of 133708 rule applications. The subject rule 
(383) and the noun determiner rule (46) the most 
used rules (35% of all applications). Fig 5. 
illustrates the preliminary results of a clustering 
algorithm where different rule applications are 
grouped into clusters and the key features of the 
head and modifier phrases for each cluster are 
extracted. 
Currently, we try to determine further and tare 
the linguistic features and the weighting which 
models best the evaluation for German. (The gold 
standard that is used in this test is the set of 
analyses mentioned above). The investigations are 
not yet completed, but preliminary results on the 
basis of the morphosyntactic and semantic 
properties of the neighboring elements are 
promising. After consolidation, the findings will be 
transferred to Spanish on the basis of the selection 
procedure illustrated in Fig. 3. The next step of 
grammar training in the immediate future will 
consist of  changing the focus to underspecified 
analyses as described in step 8 
5 Conclusions 
The project tries to make state-of-the-art statistical 
methods available for dictionary development and 
grammar development for a rule-based dominated 
industrial setting and to exploit such methods 
there.  
With regard to SMT dictionary creation, it goes 
beyond the current state of the art as it also aims at 
developing and applying algorithms for the semi-
automatic generation of bilingual dictionaries from 
unrelated monolingual (i.e., comparable) corpora 
of the source and the target language, instead of 
using relatively literally translated (i.e., parallel) 
texts only. Comparable corpora are far easier to 
obtain than parallel corpora. Therefore the 
approach offers a solution to the serious data 
acquisition bottleneck in SMT. This approach is 
also more cognitively plausible than previous 
suggestions on this topic, since human bilinguality 
is normally not based on memorizing parallel texts. 
Our suggestion models human capacity to translate 
texts using linguistic knowledge acquired from 
monolingual data, so it also exemplifies many 
more features of a truly self-learning MT system 
(shared also by a human translator).  
In addition, the proposal suggests a new 
method for spelling out grammars and parsers for 
languages by splitting grammars into declarative 
kernels and trainable decision algorithms and by 
exploiting cross-linguistic knowledge for 
optimizing the results of the corresponding parsers.   
For developing different components and 
dictionaries for the system a bootstrapping 
architecture is suggested that uses the acquired 
lexical information for training the grammar of the 
new language, which in turn uses the 
(underspecified) parser results for optimizing the 
lexical information in the corresponding translation 
dictionaries. We expect that the suggested methods 
significantly improve translation quality and 
reduce the costs of creating new language pairs for 
Machine Translation. The preliminary results 
obtained so far in the project appear promising. 
6 Acknowledgments 
This research is supported by a Marie Curie IAPP 
project taking place within the 7th European 
Community Framework Programme (Grant 
agreement no.: 251534) 
110
7 References 
Armstrong, S.; Kempen, M.; McKelvie, D.; Petitpierre, D.; 
Rapp, R.; Thompson, H. (1998). Multilingual Corpora 
for Cooperation. Proceedings of the 1st International 
Conference on Linguistic Resources and Evaluation 
(LREC), Granada, Vol. 2, 975?980. 
Babych, B., Hartley, A., Sharoff S.; Mudraya, O. (2007). 
Assisting Translators in Indirect Lexical Transfer. 
Proceedings of the 45th Annual Meeting of the ACL.  
Babych, B., Anthony Hartley, & Serge Sharoff (2007b) 
Translating from under-resourced languages: 
comparing direct transfer against pivot translation. 
Proceedings of MT Summit XI, 10-14 September 
2007, Copenhagen, Denmark, 29-35 
Babych, B. & Hartley, A. (2008). Automated MT Evaluation 
for Error Analysis: Automatic Discovery of Potential 
Translation Errors for Multiword Expressions. ELRA 
Workshop on Evaluation ?Looking into the Future of 
Evaluation: When automatic metrics meet task-based  
and performance-based approaches?. Marrakech, 
Morocco 27 May 2008. Proceedings of LREC?08. 
Babych, B. and Hartley, A. (2009). Automated error analysis 
for multiword expressions: using BLEU-type scores 
for automatic discovery of potential translation errors. 
Linguistica Antverpiensia, New Series (8/2009): 
Journal of translation and interpreting studies. Special 
Issue on Evaluation of Translation Technology. 
Babych, B., Babych, S. and Eberle, K. (2012). Deriving 
generation-oriented MT resources from corpora: case 
study and evaluation of de/het classification for Dutch 
Noun (in preparation) 
Baroni, M.; Bernardini, S. (2004). BootCaT: Bootstrapping 
corpora and terms from the web. Proceedings of 
LREC 2004.  
Callison-Burch, C., Miles Osborne, & Philipp Koehn: Re-
evaluating the role of BLEU in machine translation 
research. EACL-2006: 11th Conference of the 
European Chapter of the Association for 
Computational Linguistics, Trento, Italy, April 3-7, 
2006; pp.249-256  
Charniak, E.; Knight, K.; Yamada, K. (2003). Syntax-based 
language models for statistical machine translation". 
Proceedings of MT Summit IX. 
Eberle, Kurt (2001). FUDR-based MT, head switching and the 
lexicon. Proceedings of the the eighth Machine 
Translation Summit, Santiage de Compostela.  
Eberle, Kurt (2004). Flat underspecified representation and its 
meaning for a fragment of German. 
Habilitationsschrift, Universit?t Stuttgart. 
Eberle, K.; Rapp, R. (2008). Rapid Construction of 
Explicative Dictionaries Using Hybrid Machine 
Translation. In: Storrer, A.;  Geyken, A.; Siebert, A.; 
W?rzner, K._M (eds.) Text Resources and Lexical 
Knowledge: Selected Papers from the 9th Conference 
on Natural Language Processing KONVENS 2008. 
Berlin: Mouton de Gruyter..  
Eckart,K., Eberle, K.; Heid, U. (2010) An infrastructure for 
more reliable corpus analysis. Proceedings of the 
Workshop on Web Services and Processing Pipelines 
in HLT of LREC-2010 , Valetta. 
Eberle, K.; Eckart,K., Heid, U.,Haselbach, B. (2012) A 
tool/database interface for multi-level analyses. 
Proceedings of LREC-2012 , Istanbul. 
Frederking, R.; Nirenburg, S.; Farwell, D.;  Helmreich, S.; 
Hovy, E.; Knight, K.; Beale, S.; Domashnev, C.; 
Attardo, D.; Grannes, D.; Brown, R. (1994). Integrated 
Translation from Multiple Sources within the Pangloss 
MARK II Machine Translation System. Proceedings 
of Machine Translation of the Americas, 73?80. 
Frederking, Robert and Sergei Nirenburg (1994). Three heads 
are better than one. In: Proceedings of ANLP-94, 
Stuttgart, Germany.  
Fung, P.; McKeown, K. (1997). Finding terminology 
translations from non-parallel corpora. Proceedings of 
the 5th Annual Workshop on Very Large Corpora, 
Hong Kong: August 1997, 192-202.  
Gale, W.A.; Church, K.W. (1993). A progrm for aligning 
sentences in bilingual corpora. Computational 
Linguistics, 19(1), 75?102. 
Gonz?lez, J.; Antonio L. Lagarda, Jos? R. Navarro, Laura 
Eliodoro, Adri? Gim?nez, Francisco Casacuberta, Joan 
M. de Val and Ferran Fabregat (2004). SisHiTra: A 
Spanish-to-Catalan hybrid machine translation system. 
Berlin: Springer LNCS. 
Gough, N., Way, A. (2004). Example-Based Controlled 
Translation. Proceedings of the Ninth Workshop of the 
European Association for Machine Translation, 
Valetta, Malta.  
Groves, D. & Way, A. (2006b). Hybridity in MT: Experiments 
on the Europarl Corpus. In Proceedings of the 11th 
Conference of the European Association for Machine 
Translation, Oslo, Norway, 115?124. 
Groves, D.; Way, A. (2006a). Hybrid data-driven models of 
machine translation. Machine Translation, 19(3?4). 
Special Issue on Example-Based Machine Translation. 
301?323. 
Habash, N.; Dorr, B. (2002). Handling translation 
divergences: Combining statistical and symbolic 
techniques in generation-heavy machine translation. 
Proceedings of AMTA-2002, Tiburon, California, 
USA. 
Kiss, T.; Strunk, J. (2006): Unsupervised multilingual 
sentence boundary detection. Computational 
Linguistics 32(4), 485?525. 
Koehn, P. (2005). Europarl: A Parallel Corpus for Statistical 
Machine Translation. Proceedings of MT Summit X, 
Phuket, Thailand 
Koehn, P.; Knight, K. (2002). Learning a translation lexicon 
from monolingual corpora. In: Proceedings of ACL-02 
Workshop on Unsupervised Lexical Acquisition, 
Philadelphia PA. 
Language Industry Monitor (1992). Statistical methods 
gaining ground. In: Language Industry Monitor, 
September/October 1992 issue. 
111
McCord, M. (1989). A new version of the machine translation 
system LMT.  Journal of Literary and Linguistic 
Computing, 4, 218?299. 
McCord, M. (1991). The slot grammar system.  In: Wedekind, 
J., Rohrer, C.(eds): Unification in Grammar, MIT-
Press. 
Melamed, I. Dan (1999). Bitext maps and aligment via pattern 
recognition. Computational Linguistics, 25(1), 107?
130. 
Munteanu, D.S.; Marcu, D. (2005). Improving machine 
translation performance by exploiting non-parallel 
corpora. Computational Linguistics, 31(4), 477?504. 
Och, F.J.; Ney, H. (2002). Discriminative trainig and 
maximum entropy models for statistical machine 
translation. Proceedings of the  Annual Meeting of the 
Association for Computational Linguistics, 
Philadelphia, PA, 295?302.  
Och, F.J.; Ney, H. (2003). A systematic comparison of various 
statistical alignment models. Computational 
Linguistics, 29(1), 19?51. 
Papineni, K.; Roukos, S.; Ward, T.; Zhu, W. (2002). BLEU: A 
method for automatic evaluation of machine 
translation. In: Proceedings of the 40th Annual 
Meeting of the ACL, Philadelphia, PA, 311?318. 
Rapp, R. (1995). Identifying word translations in non-parallel 
texts. In: Proceedings of the 33rd Meeting of the 
Association for Computational Linguistics. 
Cambridge, MA, 1995, 320?322 
Rapp, R. (1999). Automatic identification of word translations 
from unrelated English and German corpora. In: 
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics 1999, College 
Park, Maryland. 519?526. 
Rapp, R. (2004). A freely available automatically generated 
thesaurus of related words. In: Proceedings of the 
Fourth International Conference on Language 
Resources and Evaluation (LREC), Lisbon, Vol. II, 
395?398. 
Rapp, R.; Martin Vide, C. (2007). Statistical machine 
translation without parallel corpora. In: Georg Rehm, 
Andreas Witt, Lothar Lemnitzer (eds.): Data 
Structures for Linguistic Resources and Applications. 
Proceedings of the Biennial GLDV Conference 2007. 
T?bingen: Gunter Narr. 231?240 
Resnik, R. (1999). Mining the web for bilingual text. 
Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics. 
Sato, S.; Nagao, M. (1990). Toward memory-based 
translation. Proceedings of COLING 1990, 247?252. 
Schiehlen, M. (2001) Syntactic Underspecification. In: Special 
Research Area 340 ? Final report, University of 
Stuttgart.  
Sharoff, S. (2006) Open-source corpora: using the net to fish 
for linguistic data. In International Journal of Corpus 
Linguistics 11(4), 435?462.  
Sharoff, S.; Babych, B.; Hartley, A. (2006). Using comparable 
corpora to solve problems difficult for human 
translators. In: Proceedings of COLING/ACL 2006, 
739?746.  
Sharoff, S. (2006). A uniform interface to large-scale 
linguistic resources. In Proceedings of the Fifth 
Language Resources and Evaluation Conference, 
LREC-2006, Genoa. 
Simard, M., Foster, G., Isabelle, P. (1992). Using Cognates to 
Align Sentences in Bilingual Corpora. Proceeedings of 
the International Conference on Theoretical and 
Methodological Issues, Montr?al. 
Smadja, F. (1993). Retrieving collocations from text: Xtract. 
Computational Linguistics, 19(1), 143?177. 
Streiter, O., Carl, M., Haller, J. (eds)(1999). Hybrid 
Approaches to Machine Translation. IAI working 
papers 36. 
Streiter, O.; Carl, M.; Iomdin, L.L.: 2000, A Virtual 
Translation Machine for Hybrid Machine Translation'. 
In: Proceedings of the Dialogue'2000 International 
Seminar in Computational Linguistics and 
Applications. Tarusa, Russia.  
Streiter, O.; Iomdin, L.L. (2000). Learning Lessons from 
Bilingual Corpora: Benefits for Machine Translation. 
International Journal of Corpus Linguistics, 5(2), 199?
230. 
Thurmair, G. (2005). Hybrid architectures for machine 
translation systems. Language Resources and 
Evaluation, 39 (1), 91?108. 
Thurmair, G. (2006). Using corpus information to improve 
MT quality. Proceedings of the LR4Trans-III 
Workshop, LREC, Genova. 
Thurmair, G. (2007) Automatic evaluation in MT system 
production. MT Summit XI Workshop: Automatic 
procedures in MT evaluation, 11 September 2007, 
Copenhagen, Denmark, 
Veronis, Jean (2006). Technologies du Langue. Actualit?s ? 
Comentaires ? R?flexions. Translation. Systran or 
Reverso? 
http://aixtal.blogspot.com/2006/01/translation-systran-
or-reverso.html  
Wu, D., Fung, P. (2005). Inversion transduction grammar 
constraints for mining parallel sentences from quasi-
comparable corpora. Second International Joint 
Conference on Natural Language Processing 
(IJCNLP-2005). Jeju, Korea. 
 
112
