Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 737?745,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Answering Opinion Questions with Random Walks on Graphs
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan Zhu
State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing 100084, China
{fangtao06,tangyang9}@gmail.com,{aihuang,zxy-dcs}@tsinghua.edu.cn
Abstract
Opinion Question Answering (Opinion
QA), which aims to find the authors? sen-
timental opinions on a specific target, is
more challenging than traditional fact-
based question answering problems. To
extract the opinion oriented answers, we
need to consider both topic relevance and
opinion sentiment issues. Current solu-
tions to this problem are mostly ad-hoc
combinations of question topic informa-
tion and opinion information. In this pa-
per, we propose an Opinion PageRank
model and an Opinion HITS model to fully
explore the information from different re-
lations among questions and answers, an-
swers and answers, and topics and opin-
ions. By fully exploiting these relations,
the experiment results show that our pro-
posed algorithms outperform several state
of the art baselines on benchmark data set.
A gain of over 10% in F scores is achieved
as compared to many other systems.
1 Introduction
Question Answering (QA), which aims to pro-
vide answers to human-generated questions auto-
matically, is an important research area in natu-
ral language processing (NLP) and much progress
has been made on this topic in previous years.
However, the objective of most state-of-the-art QA
systems is to find answers to factual questions,
such as ?What is the longest river in the United
States?? and ?Who is Andrew Carnegie?? In fact,
rather than factual information, people would also
like to know about others? opinions, thoughts and
feelings toward some specific objects, people and
events. Some examples of these questions are:
?How is Bush?s decision not to ratify the Kyoto
Protocol looked upon by Japan and other US al-
lies??(Stoyanov et al, 2005) and ?Why do peo-
ple like Subway Sandwiches?? from TAC 2008
(Dang, 2008). Systems designed to deal with such
questions are called opinion QA systems. Re-
searchers (Stoyanov et al, 2005) have found that
opinion questions have very different character-
istics when compared with fact-based questions:
opinion questions are often much longer, more
likely to represent partial answers rather than com-
plete answers and vary much more widely. These
features make opinion QA a harder problem to
tackle than fact-based QA. Also as shown in (Stoy-
anov et al, 2005), directly applying previous sys-
tems designed for fact-based QA onto opinion QA
tasks would not achieve good performances.
Similar to other complex QA tasks (Chen et al,
2006; Cui et al, 2007), the problem of opinion QA
can be viewed as a sentence ranking problem. The
Opinion QA task needs to consider not only the
topic relevance of a sentence (to identify whether
this sentence matches the topic of the question)
but also the sentiment of a sentence (to identify
the opinion polarity of a sentence). Current solu-
tions to opinion QA tasks are generally in ad hoc
styles: the topic score and the opinion score are
usually separately calculated and then combined
via a linear combination (Varma et al, 2008) or
just filter out the candidate without matching the
question sentiment (Stoyanov et al, 2005). How-
ever, topic and opinion are not independent in re-
ality. The opinion words are closely associated
with their contexts. Another problem is that exist-
ing algorithms compute the score for each answer
candidate individually, in other words, they do not
consider the relations between answer candidates.
The quality of a answer candidate is not only de-
termined by the relevance to the question, but also
by other candidates. For example, the good an-
swer may be mentioned by many candidates.
In this paper, we propose two models to ad-
dress the above limitations of previous sentence
737
ranking models. We incorporate both the topic
relevance information and the opinion sentiment
information into our sentence ranking procedure.
Meanwhile, our sentence ranking models could
naturally consider the relationships between dif-
ferent answer candidates. More specifically, our
first model, called Opinion PageRank, incorpo-
rates opinion sentiment information into the graph
model as a condition. The second model, called
Opinion HITS model, considers the sentences as
authorities and both question topic information
and opinion sentiment information as hubs. The
experiment results on the TAC QA data set demon-
strate the effectiveness of the proposed Random
Walk based methods. Our proposed method per-
forms better than the best method in the TAC 2008
competition.
The rest of this paper is organized as follows:
Section 2 introduces some related works. We will
discuss our proposed models in Section 3. In Sec-
tion 4, we present an overview of our opinion QA
system. The experiment results are shown in Sec-
tion 5. Finally, Section 6 concludes this paper and
provides possible directions for future work.
2 Related Work
Few previous studies have been done on opin-
ion QA. To our best knowledge, (Stoyanov et
al., 2005) first created an opinion QA corpus
OpQA. They find that opinion QA is a more chal-
lenging task than factual question answering, and
they point out that traditional fact-based QA ap-
proaches may have difficulty on opinion QA tasks
if unchanged. (Somasundaran et al, 2007) argues
that making finer grained distinction of subjective
types (sentiment and arguing) further improves the
QA system. For non-English opinion QA, (Ku et
al., 2007) creates a Chinese opinion QA corpus.
They classify opinion questions into six types and
construct three components to retrieve opinion an-
swers. Relevant answers are further processed by
focus detection, opinion scope identification and
polarity detection. Some works on opinion min-
ing are motivated by opinion question answering.
(Yu and Hatzivassiloglou, 2003) discusses a nec-
essary component for an opinion question answer-
ing system: separating opinions from fact at both
the document and sentence level. (Soo-Min and
Hovy, 2005) addresses another important compo-
nent of opinion question answering: finding opin-
ion holders.
More recently, TAC 2008 QA track (evolved
from TREC) focuses on finding answers to opin-
ion questions (Dang, 2008). Opinion questions
retrieve sentences or passages as answers which
are relevant for both question topic and question
sentiment. Most TAC participants employ a strat-
egy of calculating two types of scores for answer
candidates, which are the topic score measure and
the opinion score measure (the opinion informa-
tion expressed in the answer candidate). How-
ever, most approaches simply combined these two
scores by a weighted sum, or removed candidates
that didn?t match the polarity of questions, in order
to extract the opinion answers.
Algorithms based on Markov Random Walk
have been proposed to solve different kinds of
ranking problems, most of which are inspired by
the PageRank algorithm (Page et al, 1998) and the
HITS algorithm (Kleinberg, 1999). These two al-
gorithms were initially applied to the task of Web
search and some of their variants have been proved
successful in a number of applications, including
fact-based QA and text summarization (Erkan and
Radev, 2004; Mihalcea and Tarau, 2004; Otter-
bacher et al, 2005; Wan and Yang, 2008). Gener-
ally, such models would first construct a directed
or undirected graph to represent the relationship
between sentences and then certain graph-based
ranking methods are applied on the graph to com-
pute the ranking score for each sentence. Sen-
tences with high scores are then added into the
answer set or the summary. However, to the best
of our knowledge, all previous Markov Random
Walk-based sentence ranking models only make
use of topic relevance information, i.e. whether
this sentence is relevant to the fact we are looking
for, thus they are limited to fact-based QA tasks.
To solve the opinion QA problems, we need to
consider both topic and sentiment in a non-trivial
manner.
3 Our Models for Opinion Sentence
Ranking
In this section, we formulate the opinion question
answering problem as a topic and sentiment based
sentence ranking task. In order to naturally inte-
grate the topic and opinion information into the
graph based sentence ranking framework, we pro-
pose two random walk based models for solving
the problem, i.e. an Opinion PageRank model and
an Opinion HITS model.
738
3.1 Opinion PageRank Model
In order to rank sentence for opinion question an-
swering, two aspects should be taken into account.
First, the answer candidate is relevant to the ques-
tion topic; second, the answer candidate is suitable
for question sentiment.
Considering Question Topic: We first intro-
duce how to incorporate the question topic into
the Markov Random Walk model, which is simi-
lar as the Topic-sensitive LexRank (Otterbacher et
al., 2005). Given the set Vs = {vi} containing all
the sentences to be ranked, we construct a graph
where each node represents a sentence and each
edge weight between sentence vi and sentence vj
is induced from sentence similarity measure as fol-
lows: p(i ? j) = f(i?j)
P|Vs|
k=1 f(i?k)
, where f(i ? j)
represents the similarity between sentence vi and
sentence vj , here is cosine similarity (Baeza-Yates
and Ribeiro-Neto, 1999). We define f(i ? i) = 0
to avoid self transition. Note that p(i ? j) is usu-
ally not equal to p(j ? i). We also compute the
similarity rel(vi|q) of a sentence vi to the question
topic q using the cosine measure. This relevance
score is then normalized as follows to make the
sum of all relevance values of the sentences equal
to 1: rel?(vi|q) = rel(vi|q)P|Vs|
k=1 rel(vk|q)
.
The saliency score Score(vi) for sentence vi
can be calculated by mixing topic relevance score
and scores of all other sentences linked with it as
follows: Score(vi) = ?
?
j 6=i Score(vj) ? p(j ?
i)+(1??)rel?(vi|q), where ? is the damping fac-
tor as in the PageRank algorithm.
The matrix form is: p? = ?M?T p? + (1 ?
?)~?, where p? = [Score(vi)]|Vs|?1 is the vec-
tor of saliency scores for the sentences; M? =
[p(i ? j)]|Vs|?|Vs| is the graph with each entry
corresponding to the transition probability; ~? =
[rel?(vi|q)]|Vs|?1 is the vector containing the rel-
evance scores of all the sentences to the ques-
tion. The above process can be considered as a
Markov chain by taking the sentences as the states
and the corresponding transition matrix is given by
A? = ?M?T + (1 ? ?)~e~?T .
Considering Topics and Sentiments To-
gether: In order to incorporate the opinion infor-
mation and topic information for opinion sentence
ranking in an unified framework, we propose an
Opinion PageRank model (Figure 1) based on a
two-layer link graph (Liu and Ma, 2005; Wan and
Yang, 2008). In our opinion PageRank model, the
Figure 1: Opinion PageRank
first layer contains all the sentiment words from a
lexicon to represent the opinion information, and
the second layer denotes the sentence relationship
in the topic sensitive Markov Random Walk model
discussed above. The dashed lines between these
two layers indicate the conditional influence be-
tween the opinion information and the sentences
to be ranked.
Formally, the new representation for the two-
layer graph is denoted as G? = ?Vs, Vo, Ess, Eso?,
where Vs = {vi} is the set of sentences and Vo =
{oj} is the set of sentiment words representing the
opinion information; Ess = {eij |vi, vj ? Vs}
corresponds to all links between sentences and
Eso = {eij |vi ? Vs, oj ? Vo} corresponds to
the opinion correlation between a sentence and
the sentiment words. For further discussions, we
let ?(oj) ? [0, 1] denote the sentiment strength
of word oj , and let ?(vi, oj) ? [0, 1] denote the
strength of the correlation between sentence vi and
word oj . We incorporate the two factors into the
transition probability from vi to vj and the new
transition probability p(i ? j|Op(vi),Op(vj)) is
defined as f(i?j|Op(vi),Op(vj ))
P|Vs|
k=1 f(i?k|Op(vi),Op(vk))
when
? f 6=
0, and defined as 0 otherwise, where Op(vi) is de-
noted as the opinion information of sentence vi,
and f(i ? j|Op(vi),Op(vj)) is the new similar-
ity score between two sentences vi and vj , condi-
tioned on the opinion information expressed by the
sentiment words they contain. We propose to com-
pute the conditional similarity score by linearly
combining the scores conditioned on the source
opinion (i.e. f(i ? j|Op(vi))) and the destina-
tion opinion (i.e. f(i ? j|Op(vj))) as follows:
f(i ? j|Op(vi),Op(vj))
= ? ? f(i ? j|Op(vi)) + (1? ?) ? f(i ? j|Op(vj))
= ? ?
X
ok?Op(vi)
f(i ? j) ? pi(ok) ? ?(ok, vi)
+ (1? ?) ?
X
o
k?
?Op(vj))
(i ? j) ? pi(ok? ) ? ?(ok? , vj) (1)
where ? ? [0, 1] is the combination weight con-
trolling the relative contributions from the source
739
opinion and the destination opinion. In this study,
for simplicity, we define ?(oj) as 1, if oj ex-
ists in the sentiment lexicon, otherwise 0. And
?(vi, oj) is described as an indicative function. In
other words, if word oj appears in the sentence vi,
?(vi, oj) is equal to 1. Otherwise, its value is 0.
Then the new row-normalized matrix M?? is de-
fined as follows: M??ij = p(i ? j|Op(i),Opj).
The final sentence score for Opinion PageR-
ank model is then denoted by: Score(vi) = ? ?
?
j 6=i Score(vj) ? M??ji + (1 ? ?) ? rel?(si|q).
The matrix form is: p? = ?M??T p? + (1 ? ?) ? ~?.
The final transition matrix is then denoted as:
A? = ?M??T +(1??)~e~?T and the sentence scores
are obtained by the principle eigenvector of the
new transition matrix A?.
3.2 Opinion HITS Model
The word?s sentiment score is fixed in Opinion
PageRank. This may encounter problem when
the sentiment score definition is not suitable for
the specific question. We propose another opin-
ion sentence ranking model based on the popular
graph ranking algorithm HITS (Kleinberg, 1999).
This model can dynamically learn the word senti-
ment score towards a specific question. HITS al-
gorithm distinguishes the hubs and authorities in
the objects. A hub object has links to many au-
thorities, and an authority object has high-quality
content and there are many hubs linking to it. The
hub scores and authority scores are computed in a
recursive way. Our proposed opinion HITS algo-
rithm contains three layers. The upper level con-
tains all the sentiment words from a lexicon, which
represent their opinion information. The lower
level contains all the words, which represent their
topic information. The middle level contains all
the opinion sentences to be ranked. We consider
both the opinion layer and topic layer as hubs and
the sentences as authorities. Figure 2 gives the bi-
partite graph representation, where the upper opin-
ion layer is merged with lower topic layer together
as the hubs, and the middle sentence layer is con-
sidered as the authority.
Formally, the representation for the bipartite
graph is denoted as G# = ?Vs, Vo, Vt, Eso, Est?,
where Vs = {vi} is the set of sentences. Vo =
{oj} is the set of all the sentiment words repre-
senting opinion information, Vt = {tj} is the set
of all the words representing topic information.
Eso = {eij |vi ? Vs, oj ? Vo} corresponds to the
Figure 2: Opinion HITS model
correlations between sentence and opinion words.
Each edge eij is associated with a weight owij de-
noting the strength of the relationship between the
sentence vi and the opinion word oj . The weight
owij is 1 if the sentence vi contains word oj , other-
wise 0. Est denotes the relationship between sen-
tence and topic word. Its weight twij is calculated
by tf ? idf (Otterbacher et al, 2005).
We define two matrixes O = (Oij)|Vs|?|Vo| and
T = (Tij)|Vs|?|Vt| as follows, for Oij = owij ,
and if sentence i contains word j, therefore owij
is assigned 1, otherwise owij is 0. Tij = twij =
tfj ? idfj (Otterbacher et al, 2005).
Our new opinion HITS model is different from
the basic HITS algorithm in two aspects. First,
we consider the topic relevance when computing
the sentence authority score based on the topic hub
level as follows: Authsen(vi) ?
?
twij>0 twij ?
topic score(j)?hubtopic(j), where topic score(j)
is empirically defined as 1, if the word j is in the
topic set (we will discuss in next section), and 0.1
otherwise.
Second, in our opinion HITS model, there are
two aspects to boost the sentence authority score:
we simultaneously consider both topic informa-
tion and opinion information as hubs.
The final scores for authority sentence, hub
topic and hub opinion in our opinion HITS model
are defined as:
Auth(n+1)sen (vi) = (2)
? ?
X
twij>0
twij ? topic score(j) ?Hub(n)topic(tj)
+ (1? ?) ?
X
owij>0
owij ?Hub(n)opinion(oj)
Hub(n+1)topic (ti) =
X
twki>0
twki ?Auth(n)sen(vi) (3)
Hub(n+1)opinion(oi) =
X
owki>0
owki ?Auth(n)sen(vi) (4)
740
Figure 3: Opinion Question Answering System
The matrix form is:
a(n+1) = ? ? T ? e ? tTs ? I ? h(n)t + (1 ? ?) ? O ? h(n)o (5)
h(n+1)t = T
T ? a(n) (6)
h(n+1)o = O
T ? a(n) (7)
where e is a |Vt|?1 vector with all elements equal
to 1 and I is a |Vt| ? |Vt| identity matrix, ts =
[topic score(j)]|Vt|?1 is the score vector for topic
words, a(n) = [Auth(n)sen(vi)]|Vs|?1 is the vector
authority scores for the sentence in the nth itera-
tion, and the same as h(n)t = [Hub
(n)
topic(tj)]|Vt|?1,
h(n)o = [Hub(n)opinion(tj)]|Vo|?1. In order to guaran-
tee the convergence of the iterative form, authority
score and hub score are normalized after each iter-
ation.
For computation of the final scores, the ini-
tial scores of all nodes, including sentences, topic
words and opinion words, are set to 1 and the
above iterative steps are used to compute the new
scores until convergence. Usually the convergence
of the iteration algorithm is achieved when the dif-
ference between the scores computed at two suc-
cessive iterations for any nodes falls below a given
threshold (10e-6 in this study). We use the au-
thority scores as the saliency scores in the Opin-
ion HITS model. The sentences are then ranked
by their saliency scores.
4 System Description
In this section, we introduce the opinion question
answering system based on the proposed graph
methods. Figure 3 shows five main modules:
Question Analysis: It mainly includes two
components. 1).Sentiment Classification: We
classify all opinion questions into two categories:
positive type or negative type. We extract several
types of features, including a set of pattern fea-
tures, and then design a classifier to identify sen-
timent polarity for each question (similar as (Yu
and Hatzivassiloglou, 2003)). 2).Topic Set Expan-
sion: The opinion question asks opinions about
a particular target. Semantic role labeling based
(Carreras and Marquez, 2005) and rule based tech-
niques can be employed to extract this target as
topic word. We also expand the topic word with
several external knowledge bases: Since all the en-
tity synonyms are redirected into the same page in
Wikipedia (Rodrigo et al, 2007), we collect these
redirection synonym words to expand topic set.
We also collect some related lists as topic words.
For example, given question ?What reasons did
people give for liking Ed Norton?s movies??, we
collect all the Norton?s movies from IMDB as this
question?s topic words.
Document Retrieval: The PRISE search en-
gine, supported by NIST (Dang, 2008), is em-
ployed to retrieve the documents with topic word.
Answer Candidate Extraction: We split re-
trieved documents into sentences, and extract sen-
tences containing topic words. In order to im-
prove recall, we carry out the following process to
handle the problem of coreference resolution: We
classify the topic word into four categories: male,
female, group and other. Several pronouns are de-
fined for each category, such as ?he?, ?him?, ?his?
for male category. If a sentence is determined to
contain the topic word, and its next sentence con-
tains the corresponding pronouns, then the next
sentence is also extracted as an answer candidate,
similar as (Chen et al, 2006).
Answer Ranking: The answer candidates
are ranked by our proposed Opinion PageRank
method or Opinion HITS method.
Answer Selection by Removing Redundancy:
We incrementally add the top ranked sentence into
the answer set, if its cosine similarity with ev-
ery extracted answer doesn?t exceed a predefined
threshold, until the number of selected sentence
(here is 40) is reached.
5 Experiments
5.1 Experiment Step
5.1.1 Dataset
We employ the dataset from the TAC 2008 QA
track. The task contains a total of 87 squishy
741
opinion questions.1 These questions have simple
forms, and can be easily divided into positive type
or negative type, for example ?Why do people like
Mythbusters?? and ?What were the specific ac-
tions or reasons given for a negative attitude to-
wards Mahmoud Ahmadinejad??. The initial topic
word for each question (called target in TAC) is
also provided. Since our work in this paper fo-
cuses on sentence ranking for opinion QA, these
characteristics of TAC data make it easy to pro-
cess question analysis. Answers for all questions
must be retrieved from the TREC Blog06 collec-
tion (Craig Macdonald and Iadh Ounis, 2006).
The collection is a large sample of the blog sphere,
crawled over an eleven-week period from Decem-
ber 6, 2005 until February 21, 2006. We retrieve
the top 50 documents for each question.
5.1.2 Evaluation Metrics
We adopt the evaluation metrics used in the TAC
squishy opinion QA task (Dang, 2008). The TAC
assessors create a list of acceptable information
nuggets for each question. Each nugget will be
assigned a normalized weight based on the num-
ber of assessors who judged it to be vital. We use
these nuggets and corresponding weights to assess
our approach. Three human assessors complete
the evaluation process. Every question is scored
using nugget recall (NR) and an approximation to
nugget precision (NP) based on length. The final
score will be calculated using F measure with TAC
official value ? = 3 (Dang, 2008). This means re-
call is 3 times as important as precision:
F (? = 3) =
(32 + 1) ?NP ?NR
32 ?NP + NR
where NP is the sum of weights of nuggets re-
turned in response over the total sum of weights
of all nuggets in nugget list, and NP = 1 ?
(length ? allowance)/(length) if length is no
less than allowance and 0 otherwise. Here
allowance = 100 ? (?nuggets returned) and
length equals to the number of non-white char-
acters in strings. We will use average F Score to
evaluate the performance for each system.
5.1.3 Baseline
The baseline combines the topic score and opinion
score with a linear weight for each answer candi-
date, similar to the previous ad-hoc algorithms:
final score = (1 ? ?) ? opinion score + ?? topic score
(8)
13 questions were dropped from the evaluation due to no
correct answers found in the corpus
The topic score is computed by the cosine sim-
ilarity between question topic words and answer
candidate. The opinion score is calculated using
the number of opinion words normalized by the
total number of words in candidate sentence.
5.2 Performance Evaluation
5.2.1 Performance on Sentimental Lexicons
Lexicon Neg Pos Description
Name Size Size
1 HowNet 2700 2009 English translation
of positive/negative
Chinese words
2 Senti- 4800 2290 Words with a positive
WordNet or negative score
above 0.6
3 Intersec- 640 518 Words appeared in
tion both 1 and 2
4 Union 6860 3781 Words appeared in
1 or 2
5 All 10228 10228 All words appeared
in 1 or 2 without
distinguishing pos
or neg
Table 1: Sentiment lexicon description
For lexicon-based opinion analysis, the selec-
tion of opinion thesaurus plays an important role
in the final performance. HowNet2 is a knowledge
database of the Chinese language, and provides an
online word list with tags of positive and negative
polarity. We use the English translation of those
sentiment words as the sentimental lexicon. Sen-
tiWordNet (Esuli and Sebastiani, 2006) is another
popular lexical resource for opinion mining. Ta-
ble 1 shows the detail information of our used sen-
timent lexicons. In our models, the positive opin-
ion words are used only for positive questions, and
negative opinion words just for negative questions.
We initially set parameter ? in Opinion PageRank
as 0 as (Liu and Ma, 2005), and other parameters
simply as 0.5, including ? in Opinion PageRank,
? in Opinion HITS, and ? in baseline. The exper-
iment results are shown in Figure 4.
We can make three conclusions from Figure 4:
1. Opinion PageRank and Opinion HITS are both
effective. The best results of Opinion PageRank
and Opinion HITS respectively achieve around
35.4% (0.199 vs 0.145), and 34.7% (0.195 vs
0.145) improvements in terms of F score over the
best baseline result. We believe this is because our
proposed models not only incorporate the topic in-
formation and opinion information, but also con-
2http://www.keenage.com/zhiwang/e zhiwang.html
742
0 15
0.2
0.25
HowNet SentiWordNet Intersection Union All
0
0.05
0.1
.
Baseline Opinion PageRank Opinion HITS
Figure 4: Sentiment Lexicon Performance
sider the relationship between different answers.
The experiment results demonstrate the effective-
ness of these relations. 2. Opinion PageRank and
Opinion HITS are comparable. Among five sen-
timental lexicons, Opinion PageRank achieves the
best results when using HowNet and Union lexi-
cons, and Opinion HITS achieves the best results
using the other three lexicons. This may be be-
cause when the sentiment lexicon is defined appro-
priately for the specific question set, the opinion
PageRank model performs better. While when the
sentiment lexicon is not suitable for these ques-
tions, the opinion HITS model may dynamically
learn a temporal sentiment lexicon and can yield
a satisfied performance. 3. Hownet achieves the
best overall performance among five sentiment
lexicons. In HowNet, English translations of the
Chinese sentiment words are annotated by non-
native speakers; hence most of them are common
and popular terms, which maybe more suitable for
the Blog environment (Zhang and Ye, 2008). We
will use HowNet as the sentiment thesaurus in the
following experiments.
In baseline, the parameter ? shows the relative
contributions for topic score and opinion score.
We vary ? from 0 to 1 with an interval of 0.1, and
find that the best baseline result 0.170 is achieved
when ?=0.1. This is because the topic informa-
tion has been considered during candidate extrac-
tion, the system considering more opinion infor-
mation (lower ?) achieves better. We will use this
best result as baseline score in following experi-
ments. Since F(3) score is more related with re-
call, F score and recall will be demonstrated. In
the next two sections, we will present the perfor-
mances of the parameters in each model. For sim-
plicity, we denote Opinion PageRank as PR, Opin-
ion HITS as HITS, baseline as Base, Recall as r, F
score as F.
0.22
0.24
0.26
PR_r PR_F Base_r Base_F
F(3)
0.12
0.14
0.16
0.18
0.2
0 0.2 0.4 0.6 0.8 1 
Figure 5: Opinion PageRank Performance with
varying parameter ? (? = 0.5)
0.22
0.24
0.26
PR_r PR_F Base_r Base_F
F(3)
0.12
0.14
0.16
0.18
0.2
0 0.2 0.4 0.6 0.8 1
 
Figure 6: Opinion PageRank Performance with
varying parameter ? (? = 0.2)
5.2.2 Opinion PageRank Performance
In Opinion PageRank model, the value ? com-
bines the source opinion and the destination opin-
ion. Figure 5 shows the experiment results on pa-
rameter ?. When we consider lower ?, the system
performs better. This demonstrates that the desti-
nation opinion score contributes more than source
opinion score in this task.
The value of ? is a trade-off between answer
reinforcement relation and topic relation to calcu-
late the scores of each node. For lower value of ?,
we give more importance to the relevance to the
question than the similarity with other sentences.
The experiment results are shown in Figure 6. The
best result is achieved when ? = 0.8. This fig-
ure also shows the importance of reinforcement
between answer candidates. If we don?t consider
the sentence similarity(? = 0), the performance
drops significantly.
5.2.3 Opinion HITS Performance
The parameter ? combines the opinion hub score
and topic hub score in the Opinion HITS model.
The higher ? is, the more contribution is given
743
0.22
0.24
0.26
HITS_r HITS_F Base_r Base_FF(3)
0.12
0.14
0.16
0.18
0.2
0 0.2 0.4 0.6 0.8 1 
Figure 7: Opinion HITS Performance with vary-
ing parameter ?
to topic hub level, while the less contribution is
given to opinion hub level. The experiment results
are shown in Figure 7. Similar to baseline param-
eter ?, since the answer candidates are extracted
based on topic information, the systems consider-
ing opinion information heavily (?=0.1 in base-
line, ?=0.2) perform best.
Opinion HITS model ranks the sentences by au-
thority scores. It can also rank the popular opin-
ion words and popular topic words from the topic
hub layer and opinion hub layer, towards a specific
question. Take the question 1024.3 ?What reasons
do people give for liking Zillow?? as an example,
its topic word is ?Zillow?, and its sentiment polar-
ity is positive. Based on the final hub scores, the
top 10 topic words and opinion words are shown
as Table 2.
Opinion real, like, accurate, rich, right, interesting,
Words better, easily, free, good
Topic zillow, estate, home, house, data, value,
Words site, information, market, worth
Table 2: Question-specific popular topic words
and opinion words generated by Opinion HITS
Zillow is a real estate site for users to see the
value of houses or homes. People like it because it
is easily used, accurate and sometimes free. From
the Table 2, we can see that the top topic words
are the most related with question topic, and the
top opinion words are question-specific sentiment
words, such as ?accurate?, ?easily?, ?free?, not
just general opinion words, like ?great?, ?excel-
lent? and ?good?.
5.2.4 Comparisons with TAC Systems
We are also interested in the performance compar-
ison with the systems in TAC QA 2008. From Ta-
ble 3, we can see Opinion PageRank and Opinion
System Precision Recall F(3)
OpPageRank 0.109 0.242 0.200
OpHITS 0.102 0.256 0.205
System 1 0.079 0.235 0.186
System 2 0.053 0.262 0.173
System 3 0.109 0.216 0.172
Table 3: Comparison results with TAC 2008 Three
Top Ranked Systems (system 1-3 demonstrate top
3 systems in TAC)
HITS respectively achieve around 10% improve-
ment compared with the best result in TAC 2008,
which demonstrates that our algorithm is indeed
performing much better than the state-of-the-art
opinion QA methods.
6 Conclusion and Future Works
In this paper, we proposed two graph based sen-
tence ranking methods for opinion question an-
swering. Our models, called Opinion PageRank
and Opinion HITS, could naturally incorporate
topic relevance information and the opinion senti-
ment information. Furthermore, the relationships
between different answer candidates can be con-
sidered. We demonstrate the usefulness of these
relations through our experiments. The experi-
ment results also show that our proposed methods
outperform TAC 2008 QA Task top ranked sys-
tems by about 10% in terms of F score.
Our random walk based graph methods inte-
grate topic information and sentiment information
in a unified framework. They are not limited to
the sentence ranking for opinion question answer-
ing. They can be used in general opinion docu-
ment search. Moreover, these models can be more
generalized to the ranking task with two types of
influencing factors.
Acknowledgments: Special thanks to Derek
Hao Hu and Qiang Yang for their valuable
comments and great help on paper prepara-
tion. We also thank Hongning Wang, Min
Zhang, Xiaojun Wan and the anonymous re-
viewers for their useful comments, and thank
Hoa Trang Dang for providing the TAC eval-
uation results. The work was supported by
973 project in China(2007CB311003), NSFC
project(60803075), Microsoft joint project ?Opin-
ion Summarization toward Opinion Search?, and
a grant from the International Development Re-
search Center, Canada.
744
References
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999.
Modern Information Retrieval. Addison Wesley,
May.
Xavier Carreras and Lluis Marquez. 2005. Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling.
Yi Chen, Ming Zhou, and Shilong Wang. 2006.
Reranking answers for definitional qa using lan-
guage modeling. In ACL-CoLing, pages 1081?1088.
Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007.
Soft pattern matching models for definitional ques-
tion answering. ACM Trans. Inf. Syst., 25(2):8.
Hoa Trang Dang. 2008. Overview of the tac
2008 opinion question answering and summariza-
tion tasks (draft). In TAC.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lex-
pagerank: Prestige in multi-document text summa-
rization. In EMNLP.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In LREC.
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604?632.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
2007. Question analysis and answer passage re-
trieval for opinion question answering systems. In
ROCLING.
Tie-Yan Liu and Wei-Ying Ma. 2005. Webpage im-
portance analysis using conditional markov random
walk. In Web Intelligence, pages 515?521.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In EMNLP.
Jahna Otterbacher, Gu?nes Erkan, and Dragomir R.
Radev. 2005. Using random walks for question-
focused sentence retrieval. In HLT/EMNLP.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1998. The pagerank citation rank-
ing: Bringing order to the web. Technical report,
Stanford University.
Swapna Somasundaran, Theresa Wilson, Janyce
Wiebe, and Veselin Stoyanov. 2007. Qa with at-
titude: Exploiting opinion type analysis for improv-
ing question answering in online discussions and the
news. In ICWSM.
Kim Soo-Min and Eduard Hovy. 2005. Identifying
opinion holders for question answering in opinion
texts. In AAAI 2005 Workshop.
Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-perspective question answering using
the opqa corpus. In HLT/EMNLP.
Vasudeva Varma, Prasad Pingali, Rahul Katragadda,
and et al 2008. Iiit hyderabad at tac 2008. In Text
Analysis Conference.
X. Wan and J Yang. 2008. Multi-document summa-
rization using cluster-based link analysis. In SIGIR,
pages 299?306.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opinion
sentences. In EMNLP.
Min Zhang and Xingyao Ye. 2008. A generation
model to unify topic relevance and lexicon-based
sentiment for opinion retrieval. In SIGIR, pages
411?418.
745
Discovering patterns to extract protein-protein interactions from full 
biomedical texts 
*Minlie Huang1, +Xiaoyan Zhu1, Donald G. Payan2, Kunbin Qu2 and ++Ming Li3,1 
1State Key Laboratory of Intelligent Technology and Systems (LITS) 
Department of Computer Science and Technology, University of Tsinghua, Beijing, 100084, China 
2Rigel Pharmaceuticals Inc, 1180 Veterans. Blvd, South San Francisco, CA 94080, USA 
3Bioinformatics Laboratory, School of Computer Science, University of Waterloo, N2L 3G1, Canada 
*huangml00@mails.tsinghua.edu.cn 
+zxy-dcs@tsinghua.edu.cn 
++mli@uwaterloo.ca 
 
Abstract 
Although there have been many research pro-
jects to extract protein pathways, most such infor-
mation still exists only in the scientific literature, 
usually written in natural languages and defying 
data mining efforts. We present a novel and robust 
approach for extracting protein-protein interactions 
from the literature. Our method uses a dynamic 
programming algorithm to compute distinguishing 
patterns by aligning relevant sentences and key 
verbs that describe protein interactions. A match-
ing algorithm is designed to extract the interactions 
between proteins. Equipped only with a protein 
name dictionary, our system achieves a recall rate 
of about 80.0% and a precision rate of about 80.5%.  
1 Introduction 
Recently there are many accomplishments in 
literature data mining for biology, most of which 
focus on extracting protein-protein interactions. 
Most of such information is scattered in the vast 
scientific literature. Many research projects have 
been designed to collect protein-protein interaction 
data. Several databases are constructed to store 
such information, for example, Database of Inter-
acting Proteins (Xenarios et al, 2000; Salwinski et 
al., 2004). Most of the data in these databases were 
accumulated manually and inadequately, at high 
costs. Yet, scientists continue to publish their 
discoveries on protein-protein interactions in scien-
tific journals, without submitting their data to the 
databases. The fact is that most protein-protein 
interaction data still exist only in the scientific 
literature, written in natural languages and hard to 
be processed with computers.  
How to extract such information has been an 
active research subject. Among all methods, 
natural language processing (NLP) techniques are 
preferred and have been widely applied. These 
methods can be regarded as parsing-based methods. 
Both full and partial (or shallow) parsing strategies 
have been used. For example, a general full parser 
with grammars applied to the biomedical domain 
was used to extract interaction events by filling 
sentences into argument structures in (Yakushiji et 
al., 2001). No recall or precision rate was given. 
Another full parsing method, using bidirectional 
incremental parsing with combinatory categorial 
grammar (CCG), was proposed (Park et al, 2001). 
This method first localizes the target verbs, and 
then it scans the left and right neighborhood of the 
verb respectively. The lexical and grammatical 
rules of CCG are even more complicated than 
those of a general CFG. The recall and precision 
rates of the system were reported to be 48% and 
80%, respectively. Another full parser utilizing a 
lexical analyzer and context free grammar (CFG), 
extracts protein, gene and small molecule inter-
actions with a recall rate of 63.9% and a precision 
rate of 70.2% (Temkin et al, 2003). Similar 
methods such as preposition-based parsing to gene-
rate templates were proposed (Leroy and Chen, 
2002), processing only abstracts with a template 
precision of 70%. A partial parsing example is the 
relational parsing for the inhibition relation (Pus-
tejovsky et al, 2002), with a comparatively low 
recall rate of 57%. In conclusion, all the methods 
are inherently complicated, requiring many re-
sources, and the performances are not satisfactory.  
Some methods only focus on several special verbs. 
Another popular approach uses pattern matching. 
As an example, a set of simple word patterns and 
part-of-speech rules were manually coded, for each 
verb, to extract special kind of interactions from 
abstracts (Ono et al, 2001). The method obtains a 
recall rate of about 85% and a precision rate of 
about 94% for yeast and Escherichia coli, which is 
the best among all reported results. However, 
manually writing patterns for every verb is not 
practical for general purpose applications. In 
GENIES, more complicated patterns with syntactic 
and semantic constraints are used (Friedman et al, 
2001). GENIES even uses semantic information. 
However, GENIES' recall rate is low. In the above 
methods, patterns are hand-coded without 
exception. Because there are many verbs and their 
22
variants describing protein interactions, manually 
coding patterns for every verb and its variants is 
not feasible in practical applications.  
Most of the above methods process MEDLINE 
abstracts (Ng and Wong 1999; Thomas et al, 2000; 
Park et al, 2001; Yakushiji et al, 2001; Wong, 
2001; Marcotte et al, 2001; Leroy and Chen, 
2002). Because there is neither an accurate task 
definition on this problem nor a standard 
benchmark, it is hard to compare the results fairly 
among various methods (Hirschman et al, 2002). 
Furthermore as MEDLINE has become a standard 
resource for researchers, the results on the more 
difficult task of mining full text have been largely 
ignored. 
In this paper, we propose a novel and surprising-
ly robust method to discover patterns to extract 
interactions between proteins. It is based on 
dynamic programming (DP). In the realm of 
homology search between protein or DNA se-
quences, global and local alignment algorithm has 
been thoroughly researched (Needleman and 
Wunsch, 1970; Smith and Waterman, 1981). In our 
method, by aligning sentences using dynamic 
programming, the similar parts in sentences could 
be extracted as patterns. Compared with the pre-
vious methods, our proposal is different in the fol-
lowing ways: Firstly, it processes full biomedical 
texts, rather than only abstracts. Secondly, it auto-
matically mines verbs for describing protein inter-
actions. Thirdly, this method automatically dis-
covers patterns from a set of sentences whose 
protein names are identified, rather than manually 
creating patterns as most previous methods. Lastly, 
our method has low time complexity. It is able to 
process very long sentences. In contrast, for any 
full or partial parsing method, it is time- and 
memory-consuming to process long sentences. 
2 Method 
2.1 Alignment algorithm 
Suppose we have two sequences ),...,,( 21 nxxxX =  
and ),...,,( 21 myyyY =  which are defined over the 
alphabet }'',...,,{ 21 ?==? laaa . Each ai is called as a 
character, and '-'  denotes a white-space or a gap. 
We want to assign a score to measure how similar 
X and Y are. Define F(i,j) as the score of the 
optimal alignment between the initial segment 
from x1 to xi of X and the initial segment from y1 to 
yj of Y. F(i,j) is recursively calculated as follows: 
???
???
?
?+?
?+?
+??=
),'(')1,(
)'',(),1(
),()1,1(
,0
max),(
j
i
ji
ysjiF
xsjiF
yxsjiF
jiF
            (1a) 
??== ji yxjFiF ,,0),0(,0)0,(                    (1b) 
where s(a,b) is defined as follows: 
]))(*)((),(log[),( bpapbapbas =                (2) 
Here, p(a) denotes the appearance probability of 
character a, and p(a,b) denotes the probability that 
a and b appear at the same position in two aligned 
sequences. Probabilities p(a) and p(a,b) can be 
easily estimated by calculating appearance fre-
quencies for each pair with pre-aligned training 
data.  
Note that the calculation of scores for a gap will 
be different. In formula (2), when a or b is a gap, 
the scores can not be directly estimated by the 
formula because of two reasons: 1) the case that a 
gap aligns to another gap will never happen in the 
alignment algorithm since it is not optimal, 
therefore, what s('-', '-') exactly means is unclear;  2) 
Gap penalty should be negative, but it is unclear 
what p('-') should be. In DNA sequence alignment, 
these gap penalties are simply assigned with 
negative constants. Similarly, we tune each gap 
penalty for every character with some fixed 
negatives. Then a linear gap model is used. 
Given a sequence of gaps with length n which 
aligns to a sequence of ),...,,( 21 nxxxX =  with no gaps, 
the linear penalty is as follows: 
? = ?= ni ixsn 1 ),'(')(?                              (3) 
For sequence X of length n and sequence Y of 
length m, totally (n+1)*(m+1) scores will be 
calculated by applying equation (1a-b) recursively. 
Store the scores in a matrix F=F(xi, yi). Through 
back-tracing in F, the optimal local alignment can 
be found. 
In our method, the alphabet consists of three 
kinds of tags: 1) part-of-speech tags, as those used 
by Brill?s tagger (Brill et al, 1995); 2) tag PTN for 
protein names; 3) tag GAP for a gap or white-space. 
Gap penalties for main tags are shown in Table 1. 
 
Tag Penalty Tag Penalty Tag Penalty
PTN -10 IN -6 VBP -7 
NN -8 CC -6 VBD -7 
NNS -7 TO -1 VBG -7 
VBN -7 VB -7 VBZ -7 
RB -1 JJ -1 
Table 1. Gap penalties for main tags 
2.2 Pattern generating algorithm 
For our problem, a data structure called se-
quence structure, instead of a flat sequence, is used. 
Sequence structure consists of a sequence of tags 
(including PTN and GAP) and word indices in the 
original sentence for each tag (for tag PTN and 
GAP, word indices are set to -1). Through the 
structure, we are able to trace which words align 
together. 
23
Similarly, we also use another data structure 
called pattern structure which is made up of three 
parts: a sequence of tags; an array of word index 
lists for each tag, where each list defines a set of 
words for a tag that can appear at the correspond-
ing position of a pattern; a count of how many 
times the pattern has been extracted out in the 
training corpus. With the structure, the pattern 
generating algorithm is shown in Figure 1. The 
filtering rules are listed in Table 2.  
Note that a threshold d is used in the algorithm. 
If a pattern appears less than d times in the corpus, 
it will be discarded; otherwise those infrequent 
patterns will cause many matching errors. Through 
adjusting this parameter, generalization and usabi-
lity of patterns can be controlled. The larger the 
threshold is, the more general and accurate 
patterns are.  
Tags like JJ (adjective) and RB (adverb) are too 
common and can appear at every position in a 
sentence; hence if patterns include such kind of 
tags, they lose the generalization power. Some 
tags such as DT (determiner) only play a func-
tional role in a sentence and they are useless to 
pattern generation. Therefore, just as the first step 
in our algorithm shown in Figure 1, we remove 
directly the useless tags such as JJ, JJS (super-
lative adjective), JJR (comparative adjective), RB, 
RBS (superlative adverb), RBR (comparative 
adverb) and DT from the sequences. Furthermore, 
to control the form of a pattern, filtering rules 
shown in Table 2 are adapted. Verb or noun tags 
define interactions between proteins, thus they are 
indispensable for a pattern, as the first rule shows. 
The second rule guarantees the integrality of a 
pattern because tags like IN and TO must be 
followed by an object. The last one requires 
symmetry between the left and right neighborhood 
of CC tag. Actually more rigid or looser filtering 
rules than those shown in Table 2 can be applied 
to meet special demands, which will affect the 
forms of patterns. 
 
Table 2. Filtering rules. 
2.3 Pattern matching algorithm 
Because one pattern possibly matches a sen-
tence at different positions, we have to explore an 
algorithm that is able to find out multiple matches. 
 
Figure 1. Pattern generating algorithm. Time com-
plexity is O(n2) in the corpus size n. 
 
Here if we think a pattern as a motif, and sentence 
as a protein sequence, then our task is similar to 
finding out all motifs in the sequence. 
Suppose that ),...,,( 21 nxxxX =  is the sequence of 
tags for a sentence in which we look for multiple 
matches, and ),...,,( 21 myyyY =  is a pattern. We still 
use a score matrix F, while the recurrence, defined 
by formulas (4a-b), is different from that of pattern 
generating algorithm. Formula (4a) only allows 
matches to end when they have score at least T.  
??
?
=??
?=
=
mjTjiF
iF
iF
F
L,2,1,),1(
)0,1(
max)0,(
0)0,0(
   (4a) 
 
???
???
?
?+?
?+?
+??=
),'(')1,(
)'',(),1(
),()1,1(
),0,(
max),(
j
i
ji
ysjiF
xsjiF
yxsjiF
iF
jiF        (4b) 
The total score of all matches is obtained by 
adding an extra cell to the matrix, F(n+1,0), using 
(4a). By tracing back from cell (n+1,0) to (0,0), 
the individual match alignments will be obtained. 
Threshold T should not be identical for different 
patterns. Threshold T is calculated as follows: 
?== mi ii yysT 1 ),(?                                (5) 
where ?  is a factor, in our method we take ?=0.5. 
The right hand of formula (5) is the maximum 
score when a pattern matches a sentence perfectly. 
A match is accepted only when three conditions 
are satisfied: 1) a pattern has a local optimal match 
with the sentence; 2) words in matching part of the 
sentence can be found in the word set of the 
pattern; 3) decision rules are satisfied.  
1. If a pattern has neither verb tag nor noun 
tag, reject it. 
2. If the last tag of a pattern is IN or TO, 
reject it. 
3. If the left neighborhood of a CC tag is not 
equal the right neighborhood of the tag in 
a pattern, reject the pattern. 
Input:  an integer d,  
a sequence set ),...,,( 21 nsssS =  
Output: pattern set P 
1. Remove useless tags from each si in S 
2. For any )(),( jiSss ji ??  do 
a) Do local alignment for si and sj. Aligned 
output is Xa and Yb; 
b) Extract the identical characters at the same 
positions in Xa and Yb as pattern p. Add the 
corresponding word indices to pattern
structure; 
c) Judge whether p is legal, using the filtering 
rules. If it is illegal, go to step 2; 
d) If p exists in P, increase the count of p 
with 1. If not, add p to P with a count of 1;
3. For every p in P , do 
If the count of p is less than d, discard p;
4. Output P.
24
 
Figure 2. Pattern matching algorithm. Time com-
plexity of |))|*|*(||(| pXPO in pattern set size |P|, 
sequence length |X| and average length of pattern 
|| p  
 
To show details how well a pattern matches a 
sentence, a measurement data structure is defined, 
which is formalized as a vector. It will be referred 
to as mVector: 
 ),,,( cVbcPtncMatchcLenmVector =                  (6) 
where cLen is the length of a pattern; cMatch is 
the number of matched tags; cPtn is the number of 
protein name tag (PTN) skipped by the alignment 
in the sentence;  cVb is the number of skipped 
verbs. Based on the structure, decision rules 
shown in Table 3 are used in the pattern matching. 
There are two parameters P and V used in the 
decision rules, which can be adjusted according to 
the performance of the experiments. Here we take 
P=0 and V=2.  
 
 
Table 3. Decision rules.  
3 System overview 
Our system uses the framework of Pathway-
Finder (Yao et al, 2004). It consists of several 
modular components, as shown in Figure 3.  
The external resource required in our method is 
a dictionary of protein names, where about 60,000 
items are collected from both databases of 
PathwayFinder and several web databases, such as 
TrEMBL, SWISSPROT (O'Donovan et al, 2002), 
and SGD (Cherry et al, 1997), including many 
synonyms. The training corpus contains about 
1200 sentences which will be explained with 
details in the next section. Patterns generated at the 
training phase are stored in the pattern database.  
For an input sentence, firstly some filtering rules 
are adapted to remove useless expressions at the 
pre-processing phase. For example, remove 
citations, such as '[1]', and listing figures, such as 
'(1)'. Then protein names in the sentence are 
identified according to the protein name dictionary 
and the names are replaced with a unique label. 
Subsequently, the sentence is part-of-speech 
tagged by Brill?s tagger (Brill et al, 1995), where 
the tag of protein names is changed to tag PTN. 
Last, since a sequence of tags is obtained, it can be 
added into the corpus at the training phase or it can 
be used by the matching algorithm at the testing 
phase.  
Because the pattern acquisition algorithm is 
aligning sequences of tags, the accuracy of part-of-
speech tagging is crucial. However, Brill?s tagger 
only obtained overall 83% accuracy for biomedical 
texts. This is because biomedical texts contain 
many unknown words. Here we propose a simple 
and effective approach called pre-tagging strategy 
to improve the accuracy, just as the method used 
by (Huang et al, 2004). 
Figure 3. Architecture of our system. 
4 Results 
Our evaluation experiments are made up of three 
parts: mining verbs for patterns, extracting patterns 
and evaluating precision and recall rates. 
4.1 Mining verbs 
The algorithm shown in Figure 1 is performed 
on the whole corpus and one more filtering rule as 
follows, is used, besides those in Table 2: 
     If the pattern has no verb tag, reject it. 
With this rule, only patterns that have verbs are 
extracted. Here the threshold d is set to 10 to 
obtain high accurate verbs for the subsequent 
Input:  a pattern set ),...,,( 21 npppP = ,  
a sequence X 
Output: aligned result set R  
1. For every pattern pi in P, do 
a) Set threshold T for pattern pi, using 
formula (5); 
b) For X and the sequence of pattern pi; build 
score matrix F using formula (4a-b); 
c) Trace-back to find multiple matches. The 
results are },,,{ 21 araar XXXA L= ;  
d) For every result Xai  in Ar 
i. Check whether every word in Xai aligned 
to pi appears in the corresponding position 
of pi,   if not, go to step d); 
ii. Fill all data in mVector ; 
iii. Determine to accept or reject the match 
according to decision rules. If reject, go to 
step d); 
iv. Add Xai to the result set R; 
2. Output R. 
Input: two parameters P and V 
1. If cMatch ? cLen, reject the match; 
2. if cPtn > P, reject the match; 
3. if  cVb > V, reject the match;  
Protein Name 
Identification 
Generating 
Algorithm 
Sentence Protein interactions 
Corpus 
Matching algorithm Preprocessing
Protein Name
Database 
Pattern 
Database
POS Tagger 
Train
Test
25
experiments. Totally 94 verbs are extracted from 
367 verbs for describing interactions. Note that 
different tense verbs that have the same base form 
are counted as different ones. There are false 
positives which do not define interactions seman-
tically at all, such as 'affect', 'infect', 'localize', 
amounting to 16. Hence the accuracy is 83.0%.  
These verbs and their variants, particularly the 
gerund and noun form, (obtained from an English 
lexicon) are added into a list of filtering words, 
which is named as FWL (Filtering Word List). For 
example, for verb 'inhibit', its variants including 
'inhibition', 'inhibiting', 'inhibited' and 'inhibitor' 
are added into FWL. At the current phase, we add 
all verbs into FWL, including false positives 
because we think these verbs are also helpful to 
understand pathway networks between proteins.  
4.2 Extracting patterns 
Pattern generating algorithm is performed on the 
whole corpus with FWL. The threshold d is 5 here. 
The filtering rules in Table 2, plus the following 
rule, are applied. 
If a pattern has any verb or noun that is not in 
FWL, reject it. 
This ensures that the patterns have a good form 
and all their words are valid. In other word, this 
rule guarantees that the main verbs or nouns in 
every pattern exactly describe protein interactions. 
The experiment runs on about 1200 sentences, with 
threshold d=5, and 134 patterns are obtained.  
Some of them are listed in Figure 4.  
4.3 Evaluating precision and recall rates 
In this part, three tests are performed. The first 
test uses 383 sentences that only contain keyword 
interact and its variants. 293 of them are used to 
extract patterns and the rest are tested. The second 
one uses 329 sentences that only contain key word 
bind and its variants. 250 of them are used to 
generate patterns and the rest are tested. The third 
one uses 1205 sentences with all keywords, where 
1020 are used to generate patterns, the rest for test. 
As described before, we do not exclude those verbs 
such as 'affect', 'infect' and so on, therefore 
relations between proteins defined by these verbs 
or nouns are thought to be interactions. Note that 
the testing and training sentences are randomly 
partitioned, and they do not overlap in all these 
tests. The results are shown in Table 4. Some 
matching examples are shown in Figure 5. Simple 
sentences as sen1-2 are matched by only one 
pattern. But it is more common that several 
patterns may match one sentence at different 
positions, as in sen3-4. In examples sen5, the same 
pattern matches repeatedly at different positions 
since we used a 'multiple matches' algorithm.  
Keywords Recall Precision F-score 
Interact 80.5% 84.6% 82.5%
Bind 81.7% 82.8% 82.2%
All verbs 79.9% 80.3% 80.2%
Table 4. The recall and precision experiments.  
5 Discussion  
We have proposed a new method for automa-
tically generating patterns and extract protein 
interactions. In contrast, our method outperforms 
the previous methods in two main aspects: first, it 
automatically mines patterns from a set of sen-
tences whose protein names are identified; second, 
it is competent to process long and complicated 
sentences from full texts. 
In our method, a threshold d is used to control 
both the number of patterns and the generalization 
power of patterns. Although infrequent patterns are 
filtered by a small threshold, a glance to these 
patterns is meaningful. For example, on 293 
sentences containing keyword 'interact' and its 
variants, patterns whose count equals one are 
shown in Figure 6. Among the results, some are 
reasonable, such as 'PTN VBZ IN PTN IN PTN ' 
(protein1 interacts with protein2 through protein3). 
These kinds of patterns are rejected because of 
both insufficient training data and infrequently 
used expressions in natural language texts. Some 
patterns are not accurate, such as 'NNS IN PTN 
PTN PTN ', because there must be a coordinating 
conjunction between the three continuous protein 
names, otherwise it will cause many errors. Some 
patterns are even wrong, such as 'PTN NN PTN ' 
because there are never such segment 'protein1 
interaction protein2' defining a real interaction 
between protein1 and protein2. Some patterns, such 
as 'PTN VBZ IN CC IN PTN ' which should be 
'PTN VBZ IN PTN CC IN PTN ' (protein1 interacts 
with protein2 and with protein3), are not precise 
because the last filtering rule in Table 2 is used.  
Nevertheless, these patterns can be filtered out 
by the threshold. However, how to evaluate and 
maintain patterns becomes a real problem. For 
example, when the pattern generating algorithm is 
applied on about 1200 sentences, with a threshold 
d=0, approximate 800 patterns are generated, most 
of which appeared only once in the corpus. It is 
necessary to reduce such large amount of patterns. 
A MDL-based algorithm that measures the con-
fidence of each pattern and maintains them without 
human intervention is under development. 
Because our matching algorithm utilizes part-of-
speech tags, and our patterns do not contain any 
adjective (JJ), interactions defined by adjectives, 
such as 'inducible' and 'inhibitable', cannot be 
extracted correctly by our method currently.  
26
Pattern 
Count 
Pattern 
Form 
Word lists of 
pattern 
1914 PTN VBZ PTN * ;modifies promotes inhibits activates mediates blocks enhances forms ;* ; 
758 PTN VBZ IN PTN * ; interacts associates; with in within ;* ; 
402 NN IN PTN CC PTN interaction association activation modification degradation ;between with of 
from by ;* ;and or but ;* ; 
270 PTN NN IN PTN * ;interaction complex conjugation modification association ;with of on by in 
within between ;* ; 
199 PTN VBZ TO PTN * ;binds; to ;* ; 
99 PTN VBZ IN PTN CC PTN * ;assembles interacts associates; of in with from ;* ;and but ;* ; 
16 PTN CC PTN NN IN PTN * ;and or ;* ;interaction conjugation complex ubiquitination degradation 
modification activation recognition ;between of with by ;* ; 
5 PTN VBP IN PTN * ;interact ;with ;* ; 
Figure 4. Pattern examples. The star symbol denotes a protein name. Words for each component of a pattern 
are separated by a semicolon. For simplicity, words in a pattern are partially listed. 
 
 
Figure 5. Examples of protein interactions extracted from sentences. Words in bold are protein names. For 
every sentence, the patterns used in the matching algorithm are listed, followed by the corresponding results. 
 
Pattern 
Count 
Pattern 
Form 
Word lists of 
pattern 
1 PTN VBZ IN CC IN PTN  * ;interacts ;with ;and ;with ;* ; 
1 PTN VBZ IN PTN IN PTN * ;interacts ;with ;* ;through;* ; 
1 PTN NN PTN * ;interaction ;* ; 
1 NNS IN PTN PTN PTN  interactions interaction ;with between ;* ;* ;* ; 
Figure 6. Some patterns whose count equals one are generated by our algorithm. 293 sentences containing 
keyword 'interact' and its variants are used in the training. 
 
This can be demonstrated by the following sen-
tence, where words in bold are protein names.  
 ?The class II proteins are expressed 
constitutively on B-cells and EBV-transformed 
B-cells, and are inducible by IFN-gamma on a 
wide variety of cell types.?   
In this sentence, interaction between class II 
proteins and IFN-gamma is defined by an 
adjective inducible (tagged as JJ) does not match 
any pattern. To solve this problem, we are 
considering using word stemming and morpheme 
recognition to convert adjectives into their 
corresponding verbs with context. 
By analyzing our experimental results, We find 
that the current matching algorithm is not optimal 
and causes approximately one-third of total errors. 
This partially derives from the simple decision 
rules used in the matching algorithm. These rules 
may work well for some texts but partially fail for 
others because the natural language texts are 
multifarious. With these considerations, a more 
accurate and complicated matching algorithm is 
under development. 
6 Conclusion 
In this paper, a method for automatically 
generating patterns to extract protein-protein inter-
actions is proposed and implemented. The method 
is capable of discovering verbs and patterns in 
biomedical texts. The algorithm is fast and able to 
process long sentences. Experiments show that a 
recall rate of about 80% and a precision rate of 
about 80% are obtained. The approach is powerful, 
robust, and applicable to real and large-scale full 
texts. 
7 Acknowledgements 
The work was supported by Chinese Natural 
Sen1: Here, we show that HIPK2 is regulated by a ubiquitin-like protein, SUMO-1.  
Pattern: PTN VBN IN PTN           result: HIPK2 regulated by SUMO-1 
Sen2: SPB association of Plo1 is the earliest fission yeast mitotic event recorded to date. 
Pattern: PTN NN IN PTN    result: SPB association of Plo1 
Sen3: In the absence of Mad2, BubR1 inhibits the activity of APC by blocking the binding of Cdc20 to APC. 
Pattern: PTN VBZ PTN    result: BubR1 inhibits APC 
Pattern: NN IN PTN TO PTN   result: binding of Cdc20 to APC 
Sen4: All proteins of this family have Cdk-binding and anion-binding sites, but only mammalian Cks1 binds to Skp2 and promotes 
the association of Skp2 with p27 phosphorylated on Thr-187. 
Pattern: PTN VBZ TO PTN    result: Cks1 binds to Skp2 
Pattern: NN IN PTN IN PTN   result: association of Skp2 with p27 
Sen5: Evidence is also provided that, in vivo, E6 can interact with p53 in the absence of E6-AP and that E6-AP can interact with 
p53 in the absence of E6. 
Pattern: PTN VB IN PTN   result: E6 interact with p53 
Pattern: PTN VB IN PTN   result: E6-AP interact with p53 
27
Science Foundation under grant No.60272019 and 
60321002, the Canadian NSERC grant 
OGP0046506, CRC Chair fund, and the Killam 
Fellowship. We would like that thank Jinbo Wang 
and Daming Yao for their collaboration on the 
PathwayFinder system. 
References  
Brill,E. (1995) Transformation-based error-driven 
learn-ing and natural language processing: a case 
study in part-of-speech tagging. Computational 
Linguistics, 21(4), 543?565. 
Cherry,JM, Ball,C, Weng,S, Juvik,G, Schmidt,R, 
Adler,C, Dunn,B, Dwight,S, Riles,L, 
Mortimer,RK, Botstein,D (1997) Genetic and 
physical maps of Saccharomyces cerevisiae. 
Nature 387(6632  Suppl), 67-73. 
Friedman,C., Kra,P., Yu,H., Krauthammer,M., and 
Rzhetsky,A. (2001) Genies: a natural-language 
processing system for the extraction of molecular 
pathways from journal articles. Bioinformatics, 
17 suppl. 1:S74?82. 
Hirschman,L., Park,JC, Tsujii,J, Wong,L., Wu,C.H. 
(2002) Accomplishments and challenges in 
literature data mining for biology. Bioinformatics, 
18:1553--1561, December 2002. 
Huang,M., Zhu,X. and Li,M. (2004) A new 
method for automatic pattern acquisition to 
extract information from biomedical texts. In the 
Seventh International Conference on Signal 
Processing, August, Beijing, China. Accepted. 
Leroy,G.  and Chen,H. (2002) Filling preposition-
based templates to capture information from 
medical abstracts. In Pacific Symposium on 
Biocomputing 7, Hawaii, USA, pp. 350-361. 
Marcotte,EM, Xenarios,I., and Eisenberg,D. (2001) 
Mining literature for protein-protein interactions. 
Bioinformatics, 17(4), 359?363. 
Needleman,S.B. and Wunsch,C.D. (1970) A 
general method applicable to the search for 
similarities in the amino acid sequence of two 
proteins. J. Mol. Biol., 48, 443-453. 
Ng,S.K. and Wong,M. (1999) Toward routine 
automatic pathway discovery from on-line 
scientific text abstracts, Proceedings of 10th 
International Workshop on Genome 
Informatics,  Tokyo, December 1999, pp. 104-
112. 
O'Donovan,C., Martin,MJ, Gattiker,A., 
Gasteiger,E., Bairoch,A. and Apweiler,R. (2002) 
High-quality pro-tein knowledge resource: 
Swiss-Prot and TrEMBL. Briefings in 
Bioinformatics 2002 Sep; 3(3), 275-284. 
Ohta,T.,  Tateishi,Y., Collier,N., Nobata,C., and 
Tsujii,J. (2000)  Building an annotated corpus 
from biology research papers. Proc. COLING-
2000 Workshop on Semantic Annotation and 
Intelligent Content, Luxembourg, pp. 28-34. 
Ono,T., Hishigaki,H., Tanigami,A., and Takagi,T. 
(2001) Automated extraction of information on 
protein-protein interactions from the biological 
literature. Bioinformatics, 17(2), 155?161. 
Park,JC, Kim,HS, and Kim,JJ (2001) Bidirectional 
incremental parsing for automatic pathway 
identify-cation with combinatory categorical 
grammar. In Proceedings of the Pacific 
Symposium Biocom-putting, Hawaii, USA, pp 
396-407. 
Pustejovsky,J, Castano,J, Zhang,J, Kotecki,M, and 
Cochran,B (2002) Robust relational parsing over 
biomedical literature: extracting inhibit relations. 
In Proceedings of the seventh Pacific Symposium 
on Biocomputing (PSB 2002), pp. 362-373. 
Salwinski,L, Miller,CS, Smith,AJ, Pettit,FK, 
Bowie,JU, Eisenberg,D (2004) The database of 
interacting proteins: 2004 update. NAR 32 
Database issue: D449-51. 
Smith,T.F. and Waterman,M.S. (1981) 
Identification of common molecular 
subsequences. J. Mol. Biol., 147, 195-197. 
Thomas,J, Milward,D, Ouzounis,C, Pulman,S and 
Carroll,M (2000) Automatic extraction of protein 
interactions from scientific abstracts. In 
Proceedings of the Pacific Symposium on 
Biocomputing, Hawaii, USA, Jan 2000, pp. 541?
551. 
Wong,L. (2001) A protein interaction extraction 
system, Proceedings of Pacific Symposium on 
Biocomputing 2001, Hawaii, January 2001, pp. 
520-530. 
Xenarios,I, Rice,D.W., Salwinski,L., Baron,M.K., 
Marcotte,E.M., Eisenberg.D. (2000) DIP: The 
data-base of interacting proteins. NAR 28, 289-
91. 
Yakushiji,A., Tateisi,Y., Miyao,Y., Tsujii,J. (2001) 
Event extraction from biomedical papers using a 
full parser. In Proceedings of the sixth Pacific 
Symposium on Biocomputing (PSB 2001), 
Hawaii, USA, pp. 408-419. 
Yao,D., Wang,J., Lu,Y., Noble,N., Sun,H., Zhu,X., 
Lin,N., Payan,D.G., Li,M., Qu,K. (2004) 
Pathway-Finder: paving the way towards 
automatic pathway extraction. In Yi-Ping Phoebe 
Chen, ed., Bioinformatics 2004: Proceedings of 
the 2nd Asia-Pacific Bioinformatics Conference 
(APBC), 29 volume of CRPIT, pp. 53-62, 
Dunedin, New Zealand, January 2004. 
Australian Computer Society. 
28
Proceedings of the Workshop on BioNLP, pages 97?105,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
 
Towards Automatic Generation of Gene Summary 
 
 
Feng Jin Minlie Huang 
Dept. Computer Science and Technology Dept. Computer Science and Technology 
Tsinghua University Tsinghua University 
Beijing 100084, China Beijing 100084, China 
jinfengfeng@gmail.com aihuang@tsinghua.edu.cn 
Zhiyong Lu Xiaoyan Zhu 
National Center for Biotechnology Information Dept. Computer Science and Technology 
National Library of Medicine Tsinghua University 
Bethesda, 20894, USA Beijing 100084, China 
luzh@ncbi.nlm.nih.gov zxy-dcs@tsinghua.edu.cn 
 
  
  
 
 
Abstract 
In this paper we present an extractive system that au-
tomatically generates gene summaries from the biomed-
ical literature. The proposed text summarization system 
selects and ranks sentences from multiple MEDLINE 
abstracts by exploiting gene-specific information and 
similarity relationships between sentences. We evaluate 
our system on a large dataset of 7,294 human genes and 
187,628 MEDLINE abstracts using Recall-Oriented 
Understudy for Gisting Evaluation (ROUGE), a widely 
used automatic evaluation metric in the text summariza-
tion community. Two baseline methods are used for 
comparison. Experimental results show that our system 
significantly outperforms the other two methods with 
regard to all ROUGE metrics. A demo website of our 
system is freely accessible at 
http://60.195.250.72/onbires/summary.jsp.  
1 Introduction 
Entrez Gene is a database for gene-centric infor-
mation maintained at the National Center for Bio-
technology Information (NCBI). It includes genes 
from completely sequenced genomes (e.g. Homo 
sapiens). An important part of a gene record is the 
summary field (shown in Table 1), which is a small 
piece of text that provides a quick synopsis of what 
is known about the gene, the function of its en-
coded protein or RNA products, disease associa-
tions, genetic interactions, etc. The summary field, 
when available, can help biologists to understand 
the target gene quickly by compressing a huge 
amount of knowledge from many papers to a small 
piece of text. At present, gene summaries are gen-
erated manually by the National Library of Medi-
cine (NLM) curators, a time- and labor-intensive 
process. A previous study has concluded that ma-
nual curation is not sufficient for annotation of ge-
nomic databases (Baumgartner et al, 2007). 
Indeed, of the 5 million genes currently in Entrez 
Gene, only about 20,000 genes have a correspond-
ing summary. Even in humans, arguably the most 
important species, the coverage is modest: only 26% 
of human genes are curated in this regard. The goal 
of this work is to develop and evaluate computa-
tional techniques towards automatic generation of 
gene summaries. 
To this end, we developed a text summarization 
system that takes as input MEDLINE documents 
related to a given target gene and outputs a small 
set of genic information rich sentences. Specifical-
ly, it first preprocesses and filters sentences that do 
 
97
 Gene Number  of 
Abstracts 
GO terms Human-writtenSummary 
EFEMP1 26 calcium ion binding 
protein binding 
extracellular region 
proteinaceous extracellu-
lar matrix 
This gene spans approximately 18 kb of genomic DNA and consists of 12 ex-
ons. Alternative splice patterns in the 5\' UTR result in three transcript variants 
encoding the same extracellular matrix protein. Mutations in this gene are asso-
ciated with Doyne honeycomb retinal dystrophy. 
IL20RA 15 blood coagulation 
receptor activity 
integral to membrane 
membrane 
The protein encoded by this gene is a receptor for interleukin 20 (IL20), a cyto-
kine that may be involved in epidermal function. The receptor of IL20 is a hete-
rodimeric receptor complex consisting of this protein and interleukin 20 
receptor beta (IL20B). This gene and IL20B are highly expressed in skin. The 
expression of both genes is found to be upregulated in Psoriasis. 
Table1. Two examples of human-written gene summaries 
not include enough informative words for gene 
summaries. Next, the remaining sentences are 
ranked by the sum of two individual scores: a) an 
authority score from a lexical PageRank algorithm 
(Erkan and Radev, 2004) and b) a similarity score 
between the sentence and the Gene Ontology (GO) 
terms with which the gene is annotated (To date, 
over 190,000 genes have two or more associated 
GO terms). Finally, redundant sentences are re-
moved and top ranked sentences are nominated for 
the target gene.  
In order to evaluate our system, we assembled a 
gold standard dataset consisting of handwritten 
summaries for 7,294 human genes and conducted 
an intrinsic evaluation by measuring the amount of 
overlap between the machine-selected sentences 
and human-written summaries. Our metric for the 
evaluation was ROUGE1, a widely used intrinsic 
summarization evaluation metric. 
2 Related Work 
Summarization systems aim to extract salient text 
fragments, especially sentences, from the original 
documents to form a summary. A number of me-
thods for sentence scoring and ranking have been 
developed. Approaches based on sentence position 
(Edmundson, 1969), cue phrase (McKeown and 
Radev, 1995), word frequency (Teufel and Moens, 
1997), and discourse segmentation (Boguraev and 
Kennedy, 1997) have been reported. Radev et al 
(Radev et al, 2004) developed an extractive multi-
document summarizer, MEAD, which extracts a 
summary from multiple documents based on the 
document cluster centroid, position and first-
sentence overlap. Recently, graph-based ranking 
methods, such as LexPageRank (Erkan and Radev, 
2004) and TextRank (Mihalcea and Tarau, 2004), 
                                                          
1 http://haydn.isi.edu/ROUGE/ 
have been proposed for multi-document summari-
zation. Similar to the original PageRank algorithm, 
these methods make use of similarity relationships 
between sentences and then rank sentences accord-
ing to the ?votes? or ?recommendations? from 
their neighboring sentences. 
Lin and Hovy (2000) first introduced topic sig-
natures which are topic relevant terms for summa-
rization. Afterwards, this technique was 
successfully used in a number of summarization 
systems (Hickl et al, 2007, Gupta and Nenkova et 
al., 2007). In order to improve sentence selection, 
we adopted the idea in a similar way to identify 
terms that tend to appear frequently in gene sum-
maries and subsequently filter sentences that in-
clude none or few such terms. 
Compared with newswire document summariza-
tion, much less attention has been paid to summa-
rizing MEDLINE documents for genic information. 
Ling et al (Ling et al, 2006 and 2007) presented 
an automatic gene summary generation system that 
constructs a summary based on six aspects of a 
gene, such as gene products, mutant phenotype, etc. 
In their system, sentences were ranked according 
to a) the relevance to each category (namely the 
aspect), b) the relevance to the document where 
they are from; and c) the position where sentences 
are located. Although the system performed well 
on a small group of genes (10~20 genes) from Fly-
base, their method relied heavily on high-quality 
training data that is often hard to obtain in practice.  
Yang et al reported a system (Yang et al, 2007 
and 2009) that produces gene summaries by focus-
ing on gene sets from microarray experiments. 
Their system first clustered gene set into functional 
related groups based on free text, Medical Subject 
Headings (MeSH?) and Gene Ontology (GO) fea-
tures. Then, an extractive summary was generated 
for each gene following the Edmundson paradigm 
98
 (Edmundson, 1969). Yang et al also presented 
evaluation results based on human ratings of eight 
gene summaries.  
Another related work is the second task of Text 
REtrieval Conference 2  (TREC) 2003 Genomics 
Track. Participants in the track were required to 
extract GeneRIFs from MEDLINE abstracts 
(Hersh and Bhupatiraju, 2003). Many teams ap-
proached the task as a sentence classification prob-
lem using GeneRIFs in the Entrez database as 
training data (Bhalotia et al, 2003; Jelier et al, 
2003). This task has also been approached as a sin-
gle document summarization problem (Lu et al, 
2006).  
The gene summarization work presented here 
differs from the TREC task in that it deals with 
multiple documents. In contrast to the previously 
described systems for gene summarization, our 
approach has three novel features. First, we are 
able to summarize all aspects of gene-specific in-
formation as opposed to a limited number of prede-
termined aspects. Second, we exploit a lexical 
PageRank algorithm to establish similarity rela-
tionships between sentences. The importance of a 
sentence is based not only on the sentence itself, 
but also on its neighbors in a graph representation. 
Finally, we conducted an intrinsic evaluation on a 
large publicly available dataset. The gold standard 
assembled in this work makes it possible for com-
parisons between different gene summarization 
systems without human judgments.  
3 Method 
To determine if a sentence is extract worthy, we 
consider three different aspects: (1) the number of 
salient or informative words that are frequently 
used by human curators for writing gene summa-
ries; (2) the relative importance of a sentence to be 
included in a gene summary; (3) the gene-specific 
information that is unique between different genes.  
Specifically, we look for signature terms in 
handwritten summaries for the first aspect. Ideally, 
computer generated summaries should resemble 
handwritten summaries. Thus the terms used by 
human curators should also occur frequently in 
automatically generated summaries. In this regard, 
we use a method similar to Lin and Hovy (2000) to 
identify signature terms and subsequently use them 
                                                          
2 http://ir.ohsu.edu/genomics/ 
to discard sentences that contain none or few such 
terms. For the second aspect, we adopt a lexical 
PageRank method to compute the sentence impor-
tance with a graph representation. For the last as-
pect, we treat each gene as having its own 
properties that distinguish it from others. To reflect 
such individual differences in the machine-
generated summaries, we exploit a gene?s GO an-
notations as a surrogate for its unique properties 
and look for their occurrence in abstract sentences.  
Our gene summarization system consists of 
three components: a preprocessing module, a sen-
tence ranking module, and a redundancy removal 
and summary generation module. Given a target 
gene, the preprocessing module retrieves corres-
ponding MEDLINE abstracts and GO terms ac-
cording to the gene2pubmed and gene2go data 
provided by Entrez Gene. Then the abstracts are 
split into sentences by the MEDLINE sentence 
splitter in the LingPipe3 toolkit. The sentence rank-
ing module takes these as input and first filters out 
some non-informative sentences. The remaining 
sentences are then scored according to a linear 
combination of the PageRank score and GO relev-
ance score.  Finally, a gene summary is generated 
after redundant sentences are removed. The system 
is illustrated in Figure 1 and is described in more 
detail in the following sections.  
 
 
Figure 1. System overview  
3.1 Signature Terms Extraction 
There are signature terms for different topic texts 
(Lin and Hovy, 2000). For example, terms such as 
eat, menu and fork that occur frequently in a cor-
pus may signify that the corpus is likely to be 
                                                          
3 http://alias-i.com/lingpipe/ 
Abstracts 
Sentence Segmentation 
Tokenization 
Stemming 
Signature Filtering 
PageRank Scoring 
GO Scoring 
Redundancy Removal 
GO Terms Summary 
99
 about cooking or restaurants. Similarly, there are 
signature terms for gene summaries. 
We use the Pearson?s chi-square test (Manning 
and Sch?tze, 1999) to extract topic signature terms 
from a set of handwritten summaries by comparing 
the occurrence of terms in the handwritten summa-
ries with that of randomly selected MEDLINE ab-
stracts. Let R denote the set of handwritten 
summaries and R denote the set of randomly se-
lected abstracts from MEDLINE. The null hypo-
thesis and alternative hypothesis are as follows:  
0H : ( | ) ( | )i iP t R p P t R= =   
1 1 2H : ( | ) ( | )i iP t R p p P t R= ? =   
The null hypothesis says that the term it appears 
in R and in R with an equal probability and it is 
independent from R . In contrast, the alternative 
hypothesis says that the term it is correlated with
R . We construct the following 2-by-2 contingency 
table:  
 
 R  R  
it  11O  12O  
it  21O  22O  
Table 2. Contingency table for the chi-square test. 
 
where 
11O : the frequency of term it occurring in R ; 
12O : the frequency of it occurring in R ; 
21O  : the frequency of term i it t? occurring in R ; 
22O :  the frequency of it in R .  
Then the Pearson?s chi-square statistic is computed 
by  
22
2
, 1
( )ij ij
i j ij
O E
X
E
=
?
=?  
where ijO is the observed frequency and ijE is the 
expected frequency.  
In our experiments, the significance level is set 
to 0.001, thus the corresponding chi-square value 
is 10.83. Terms with 2X value above 10.83 would 
be selected as signature terms. In total, we obtained 
1,169 unigram terms. The top ranked (by 2X value) 
signature terms are listed in Table 3. Given the set 
of signature terms, sentences containing less than 3 
signature terms are discarded. This parameter was 
determined empirically during the system devel-
opment.  
 
protein 
gene 
encode 
family 
transcription 
member 
variant 
domain 
splice 
subunit 
receptor 
isoform 
alternative 
bind 
involve 
Table 3. A sample of unigram topic signature terms. 
3.2 Lexical PageRank Scoring 
The lexical PageRank algorithm makes use of the 
similarity between sentences and ranks them by 
how similar a sentence is to all other sentences. It 
originates from the original PageRank algorithm 
(Page et al, 1998) that is based on the following 
two hypotheses:  
(1) A web page is important if it is linked by many 
other pages.  
(2) A web page is important if it is linked by im-
portant pages.  
The algorithm views the entire internet as a large 
graph in which a web page is a vertex and a di-
rected edge is connected according to the linkage. 
The salience of a vertex can be computed by a ran-
dom walk on the graph. Such graph-based methods 
have been widely adapted to such Natural Lan-
guage Processing (NLP) problems as text summa-
rization and word sense disambiguation. The 
advantage of such graph-based methods is obvious: 
the importance of a vertex is not only decided by 
itself, but also by its neighbors in a graph represen-
tation.  The random walk on a graph can imply 
more global dependence than other methods. Our 
PageRank scoring method consists of two steps: 
constructing the sentence graph and computing the 
salience score for each vertex of the graph.  
Let { |1 }iS s i N= ? ? be the sentence collec-
tion containing all the sentences to be summarized. 
According to the vector space model (Salton et al, 
1975), each sentence is  can be represented by a 
vector is
G
with each component being the weight of 
a term in is . The weight associated with a term w  
is calculated by ( )* ( )tf w isf w , where ( )tf w is the 
frequency of the term w in sentence is and ( )isf w
100
 is the inverse sentence frequency 4  of term w :
( ) 1 log( / )wisf w N n= + , where N is the total 
number of sentences in S  and wn is the number of 
sentences containing w .The similarity score be-
tween two sentences is computed using the inner 
product of the corresponding sentence vectors, as 
follows:  
( , )
|| || || ||
i j
i j
i j
s s
sim s s
s s
?
=
?
G G
G G  
Taking each sentence as a vertex, and the simi-
larity score as the weight of the edge between two 
sentences, a sentence graph is constructed. The 
graph is fully connected and undirected because 
the similarity score is symmetric.  
The sentence graph can be modeled by an adja-
cency matrix M , in which each element corres-
ponds to the weight of an edge in the graph. Thus
[ ]ij N NM ?=M is defined as:  
,
|| || || ||
0,
i j
i jij
s s
if i j
s sM
otherwise
??
??
?= ???
G G
G G
 
We normalize the row sum of matrix M  in or-
der to assure it is a stochastic matrix such that the 
PageRank iteration algorithm is applicable. The 
normalized matrix is: 
1 1
, 0
0,
N N
ij ij ij
j jij
M M if M
M
otherwise
= =
?
??
= ???
? ? . 
Using the normalized adjacency matrix, the sa-
lience score of a sentence is is computed in an 
iterative manner:  
1
(1 )
( ) ( )
N
i j ji
j
d
score s d score s M
N
=
?
= ? ? +?   
where d is a damping factor that is typically be-
tween 0.8 and 0.9 (Page et al, 1998).  
If we use a column vector p to denote the sa-
lience scores of all the sentences in S , the above 
equation can be written in a matrix form as follows:  
[ (1 ) ]Tp d d p= ? + ? ? ?M U  
                                                          
4 Isf is equivalent to idf if we view each sentence as a docu-
ment. 
where U is a square matrix with all elements being 
equal to 1/ N . The component (1 )d? ?U can be 
considered as a smoothing term which adds a small 
probability for a random walker to jump from the 
current vertex to any vertex in the graph. This 
guarantees that the stochastic transition matrix for 
iteration is irreducible and aperiodic. Therefore the 
iteration can converge to a stable state.  
In our implementation, the damping factor d is 
set to 0.85 as in the PageRank algorithm (Page et 
al., 1998). The column vector p is initialized with 
random values between 0 and 1. After the algo-
rithm converges, each component in the column 
vector p corresponds to the salience score of the 
corresponding sentence. This score is combined 
with the GO relevance score to rank sentences. 
3.3 GO Relevance Scoring 
Up to this point, our system considers only gene-
independent features, in both sentence filtering and 
PageRank-based sentence scoring. These features 
are universal across different genes. However, each 
gene is unique because of its own functional and 
structural properties. Thus we seek to include 
gene-specific features in this next step.  
The GO annotations provide one kind of gene-
specific information and have been shown to be 
useful for selecting GeneRIF candidates (Lu et al, 
2006). A gene?s GO annotations include descrip-
tions in three aspects: molecular function; biologi-
cal process; and cellular component. For example, 
the human gene AANAT (gene ID 15 in Entrez 
Gene) is annotated with the GO terms in Table 4. 
 
GO ID GO term 
GO:0004059 aralkylamine N-acetyltransferase activi-
ty 
GO:0007623 circadian rhythm 
GO:0008152 metabolic process 
GO:0008415 acyltransferase activity 
GO:0016740 transferase activity 
Table 4. GO terms for gene AANAT 
 
The GO relevance score is computed as follows: 
first, the GO terms and the sentences are both 
stemmed and stopwords are removed. For example, 
the GO terms in Table 4 are processed into a set of 
stemmed words: aralkylamin, N, acetyltransferas, 
activ, circadian, rhythm, metabol, process, acyl-
transferas and transferas.  
101
 Second, the total number of occurrence of the 
GO terms appearing in a sentence is counted. Fi-
nally, the GO relevance score is computed as the 
ratio of the total occurrence to the sentence length. 
The entire process can be illustrated by the follow-
ing pseudo codes: 
 
1 tokenize and stem the GO terms; 
2 tokenize and stem all the sentences, remove stop 
words; 
3 for each sentence is , 1,...,i N=  
( ) 0
i
GOScore s =  
for each word w  in is  
if w in the GO term set 
( )
i
GOScore s ++ 
end if 
end for 
( ) ( ) / ( )
i i i
GOScore s GOScore s length s=  
end for  
 
where ( )ilength s is the number of distinct non-stop 
words in is . For each sentence is , the GO relev-
ance score is combined with the PageRank score to 
get the overall score (? is a weight parameter be-
tween 0 and 1; see Section 4.2 for discussion): 
( ) ( ) (1 ) ( )i i iscore s PRScore s GOScore s? ?= ? + ? ? . 
3.4 Redundancy Removal  
A good summary contains as much diverse infor-
mation as possible for a gene, while with as little 
redundancy as possible. For many well-studied 
genes, there are thousands of relevant papers and 
much information is redundant. Hence it is neces-
sary to remove redundant sentences before produc-
ing a final summary.  
We adopt the diversity penalty method (Zhang 
et al, 2005; Wan and Xiao, 2007) for redundancy 
removal. The idea is to penalize the candidate sen-
tences according to their similarity to the ones al-
ready selected. The process is as follows:  
(1) Initialize two sets, A ?= ,
{ | 1, 2,..., }iB s i K= =  containing all the extracted 
sentences;  
(2)  Sort the sentences in B by their scores in des-
cending order;  
(3) Suppose is is the top ranked sentence in B , 
move it from B to A . Then we penalize the re-
maining sentences in B as follows: 
For each sentence js  in B , j i?  
( ) ( ) ( , ) ( )j j j i iScore s Score s sim s s Score s?= ? ? ?  
where 0? > is the penalty degree factor, 
( , )j isim s s  is the similarity between is and js .  
(4) Repeat steps 2 and 3 until enough sentences 
have been selected. 
4 Results and Discussion 
4.1 Evaluation Metrics 
Unlike the newswire summarization, there are no 
gold-standard test collections available for evaluat-
ing gene summarization systems. The two previous 
studies mentioned in Section 2 both conducted ex-
trinsic evaluations by asking human experts to rate 
system outputs. Although it is important to collect 
direct feedback from the users, involving human 
experts makes it difficult to compare different 
summarization systems and to conduct large-scale 
evaluations (both studies evaluated nothing but a 
small number of genes). In contrast, we evaluated 
our system intrinsically on a much larger dataset 
consisting of 7,294 human genes, each with a pre-
existing handwritten summary downloaded from 
the NCBI?s FTP site5.  
The handwritten summaries were used as refer-
ence summaries (i.e. a gold standard) to compare 
with the automatically generated summaries. Al-
though the length of reference summaries varies, 
the majority of these summaries contain 80 to 120 
words. To produce a summary of similar length, 
we decided to select five sentences consisting of 
about 100 words. 
For the intrinsic evaluation of a large number of 
summaries, we made use of the ROUGE metrics 
that has been widely used in automatic evaluation 
of summarization systems (Lin and Hovy, 2003; 
Hickl et al, 2007). It provides a set of evaluation 
metrics to measure the quality of a summary by 
counting overlapping units such as n-grams or 
word sequences between the generated summary 
and its reference summary.  
                                                          
5 ftp://ftp.ncbi.nih.gov/gene/DATA/ASN_BINARY/ 
102
 We computed three ROUGE measures for each 
summary, namely ROUGE-1 (unigram based), 
ROUGE-2 (bigram based) and ROUGE-SU4 
(skip-bigram and unigram) (Lin and Hovy, 2003). 
Among them, ROUGE-1 has been shown to agree 
most with human judgments (Lin and Hovy, 2003). 
However, as biomedical concepts usually contain 
more than one word (e.g. transcription factor), 
ROUGE-2 and ROUGE-SU4 scores are also im-
portant for assessing gene summaries.  
4.2 Determining parameters for best perfor-
mance 
The two important parameters in our system ? the 
linear coefficient ? for the combination of Page-
Rank and GO scores and the diversity penalty de-
gree factor ? in redundancy removal ? are 
investigated in detail on a collection of 100 ran-
domly selected genes. First, by setting ? to values 
from 0 to 1 with an increment of 0.1 while holding 
?  steady at 0.7, we observed the highest ROUGE-
1score when ? was 0.8 (Figure 2). This suggests 
that the two scores (i.e. PageRank and GO score) 
complement to each other and that the PageRank 
score plays a more dominating role in the summed 
score. Next, we varied? gradually from 0 to 5 with 
an increment of 0.25 while holding ? steady at 
0.75.The highest ROUGE-1 score was achieved 
when? was 1.3 (Figure 3). For ROURE-2, the best 
performance was obtained when ? was 0.7 and ?
was 0.5. In order to balance ROUGE-1 and 
ROUGE-2 scores, we set ? to 0.75 and ? to 0.7 
for the remaining experiments.  
 
Figure 2. The blue line represents the changes in 
ROUGE-1 scores with different values of ? while ? is 
held at 0.7. 
 
Figure 3. The blue line represents the changes in 
ROUGE-1 scores with different values of ? while ? is 
held at 0.75. 
4.3 Comparison with other methods 
Because there are no publicly available gene sum-
marization systems, we compared our system with 
two baseline methods. The first is a well known 
publicly available summarizer - MEAD (Radev et 
al., 2004). We adopted the latest version of MEAD 
3.11 and used the default setting in MEAD that 
extracts sentences according to three features: cen-
troid, position and length. The second baseline ex-
tracts different sentences randomly from abstracts. 
Comparison results are shown in the following ta-
ble:  
 
System ROUGE-1 ROUGE-2 ROUGE-SU4
Our System 0.4725 0.1247 0.1828 
MEAD 0.3890 0.0961 0.1449 
Random 0.3434 0.0577 0.1091 
Table 5. Systems comparison on 7,294 genes. 
 
As shown in Table 5, our system significantly 
outperformed the two baseline systems in all three 
ROUGE measures. Furthermore, larger perfor-
mance gains are observed in ROUGE-2 and 
ROUGE-SU4 than in ROUGE-1. This is because 
many background words (e.g. gene, protein and 
enzyme) also appeared frequently as unigrams in 
randomly selected summaries. 
 
103
  
Figure 4. ROUGE-1 score distribution 
 
In Figure 4, we show that the majority of the 
summaries have a ROUGE-1 score greater than 0.4. 
Our further analysis revealed that almost half 
summaries with a low score (smaller than 0.3) ei-
ther lacked sufficient relevant abstracts, or the ref-
erence summary was too short or too long. In 
either case, only few overlapping words can be 
found when comparing the generated gene sum-
mary with the reference. The statistics for low 
ROUGE-1 score are listed in Table 6. We also note 
that almost half of the summaries that have low 
ROUGE-1 scores were due to other causes: mostly, 
machine generated summaries differ from human 
summaries in that they describe different function-
al aspects of the same gene product. Take the gene 
TOP2A (ID: 7153) for example. While both sum-
maries (handwritten and machine generated) focus 
on its encoded protein DNA topoisomerase, the 
handwritten summary describes the chromosome 
location of the gene whereas our algorithm selects 
statements about its gene expression when treated 
with a chemotherapy agent. We plan to investigate 
such differences further in our future work. 
 
Causes for Low Score Number of 
genes 
Few (?10) related abstracts 106 
Short reference summary (< 40 words) 27 
Long reference summary (> 150 words) 76 
Other 198 
Total 407 
Table 6. Statistics for low ROUGE-1 scores (<0.3) 
4.4 Results on various summary length 
Figure 5 shows the variations of ROUGE scores as 
the summary length increases. At all lengths and 
for both ROUGE-1 and ROUGE-2 measures, our 
proposed method performed better than the two 
baseline methods. By investigating the scores of 
different summary lengths, it can be seen that the 
advantage of our method is greater when the sum-
mary is short. This is of great importance for a 
summarization system as ordinary users typically 
prefer short content for summaries.  
 
 
Figure 5. Score variation for different summary length 
 
5 Conclusions and Future Work 
In this paper we have presented a system for gene-
rating gene summaries by automatically finding 
extract-worthy sentences from the biomedical lite-
rature. By using the state-of-the-art summarization 
techniques and incorporating gene specific annota-
tions, our system is able to generate gene summa-
ries more accurately than the baseline methods. 
Note that we only evaluated our system for human 
genes in this work. More summaries are available 
for human genes than other organisms, but our me-
thod is organism-independent and can be applied 
to any other species. 
This research has implications for real-world 
applications such as assisting manual database cu-
ration or updating existing gene records. The 
ROUGE scores in our evaluation show comparable 
performance to those in the newswire summariza-
tion (Hickl et al, 2007). Nonetheless, there are 
further steps necessary before making our system 
output readily usable by human curators. For in-
stance, human curators are generally in favor of 
sentences presented in a coherent order. Thus, in-
formation-ordering algorithms in multi-document 
summarization need to be investigated. We also 
plan to study the guidelines and scope of the cura-
tion process, which may provide additional impor-
tant heuristics to further refine our system output.  
Acknowledgments 
104
 The work is supported by NSFC project No. 
60803075, Chinese 973 Project No. 
2007CB311003. ZL is supported by the Intramural 
Program of the National Institutes of Health. The 
authors are grateful to W. John Wilbur and G. 
Craig Murray for their help on the early version of 
this manuscript.  
References 
W. A. Baumgartner, B. K. Cohen, L. M. Fox, G. Ac-
quaah-Mensah, L. Hunter. 2007. Manual Curation Is 
Not Sufficient for Annotation of Genomic Databases. 
Bioinformatics, Vol. 23, No. 13. (July 2007), pp. i41-
48. 
G. Bhalotia, P. I. Nakov, A. S. Schwartz and M. A. 
Hearst, BioText Team Report for the TREC 2003 
Genomics Track. In Proceedings of TREC 2003.  
B. Boguraev and C. Kennedy. 1997. Salience-based 
Content Characterization of Text Documents. In Pro-
ceedings of Workshop on Intelligent Scalable Text 
Summarization (ACL97/EACL97), pp. 2-9. 
J. Carbonell and J. Goldstein. 1998. The Use of MMR, 
Diversity-based Reranking for Reordering Docu-
ments and Producing Summaries. In ACM SIGIR, 
pages 335?336, August. 
H. P. Edmundson. 1969. New Methods in Automatic 
Extracting. Journal of the ACM (JACM) archive Vo-
lume 16,  Issue 2  (April 1969) Pages: 264 ? 285. 
G. Erkan and D. R. Radev. 2004. LexPageRank: Pres-
tige in Multi-Document Text Summarization. In Pro-
ceedings of 2004 Conference on Empirical Methods 
in Natural Language Processing (EMNLP 2004), 
Barcelona, Spain. 
S. Gupta, A.Nenkova and D.Jurafsky. 2007. Measuring 
Importance and Query Relevance in Topic-focused 
Multi-document Summarization. Proceedings of 
ACL 2007 short papers, Prague, Czech Republic. 
W. Hersh and R. T. Bhupatiraju. 2003. TREC Genomics 
track Overview. In Proceedings of TheTwelfth Text 
REtrieval Conference, 2003. 
A. Hickl, K. Roberts and F. Lacatusu. 2007. LCC's 
GISTexter at DUC 2007: Machine Reading for Up-
date Summarization. 
R. Jelier, M. Schuemie, C. Eijk, M. Weeber, E. Mulli-
gen, B. Schijvenaars, B. Mons, J. Kors. Searching for 
geneRIFs: Concept-based Query Expansion and 
Bayes Classification. In Proceedings of TREC 2003.  
C. Lin and E. Hovy. 2000. The Automated Acquisition 
of Topic Signatures for Text Summarization. In Pro-
ceedings of the COLING Conference. 
C. Lin and E. Hovy. 2003. Automatic Evaluation of 
Summaries Using N-gram Co-Occurrence Statistics. 
In HLT-NAACL, pages 71?78. 
X. Ling, J. Jiang, X. He, Q. Mei, C. Zhai and B. Schatz. 
2006. Automatically Generating Gene Summaries 
from Biomedical Literature. Proceedings of the Pa-
cific Symposium on Biocomputing 2006. 
X. Ling, J. Jiang, X. He, Q. Mei, C. Zhai and B. Schatz. 
2007. Generating Gene Summaries from Biomedical 
Literature: A Study of Semi-Structured Summariza-
tion. Information Processing and Management 43, 
2007, 1777-1791. 
Z. Lu, K. B. Cohen and L. Hunter. 2006. Finding Ge-
neRIFs via Gene Ontology Annotations. Pac Symp-
Biocomput. 2006:52-63. 
C. Manning and H. Sch?tze. 1999. Foundations of Sta-
tistical Natural Language Processing. Chapter 5, MIT 
Press. Cambridge, MA: May 1999. 
K. R. McKeown and D. R. Radev. 1995. Generating 
Summaries of Multiple News Articles. In Proceed-
ings, ACM Conference on Research and Develop-
ment in Information Retrieval SIGIR'95, pages 74?
82. 
R. Mihalcea and P. Tarau. TextRank: Bringing Order 
into Texts, in Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP 2004), Barcelona, Spain, July 2004. 
M. Newman. 2003. The Structure and Function of 
Complex Networks. SIAM Review 45.167?256 
(2003). 
L. Page, S. Brin, R. Motwani and T. Winograd. The 
PageRank Citation Ranking: Bringing Order to the 
Web. Technical report, Stanford University, Stanford, 
CA, 1998. 
D. R. Radev, H. Jing, M. Stys and D. Tam. 2004. Cen-
troid-based Summarization of Multiple Documents. 
Information Processing and Management, 40:919?
938. 
G. Salton, A. Wong, and C. S. Yang. 1975. A Vector 
Space Model for Automatic Indexing. Communica-
tions of the ACM, vol. 18, nr.11, pages 613?620. 
S. Teufel and M. Moens. 1997. Sentence Extraction as a 
Classification Task. Workshop ?Intelligent and scala-
ble Text summarization?, ACL/EACL 1997. 
X. Wan and J. Xiao. 2007. Towards a Unified Approach 
Based on Affinity Graph to Various Multi-document 
Summarizations. ECDL 2007: 297-308.  
J. Yang, A. M. Cohen, W. Hersh. Automatic Summari-
zation of Mouse Gene Information by Clustering and 
Sentence Extraction from MEDLINE Abstracts. 
AMIA 2007 Annual Meeting. Nov. 2007 Chicago, IL. 
J. Yang, A. M. Cohen, W. Hersh. 2008. Evaluation of a 
Gene Information Summarization System by Users 
During the Analysis Process of Microarray Datasets. 
In BMC Bioinformatics 2009 10(Suppl 2):S5. 
B. Zhang, H. Li, Y. Liu, L. Ji, W. Xi, W. Fan, Z. Chen, 
W. Ma. 2005. Improving Web Search Results Using 
Affinity Graph. The 28th Annual International ACM 
SIGIR Conference (SIGIR'2005), August 2005.  
105
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 653?661,
Beijing, August 2010
Structure-Aware Review Mining and Summarization
Fangtao Li1, Chao Han1, Minlie Huang1, Xiaoyan Zhu1,
Ying-Ju Xia2, Shu Zhang2 and Hao Yu2
1State Key Laboratory of Intelligent Technology and Systems?
1Tsinghua National Laboratory for Information Science and Technology?
1Department of Computer Science and Technology, Tsinghua University
2Fujitsu Research and Development Center
fangtao06@gmail.com; zxy_dcs@tsinghua.edu.cn
Abstract
In this paper, we focus on object feature 1
1 Introduction
based review summarization. Different from 
most of previous work with linguistic rules or 
statistical methods, we formulate the review
mining task as a joint structure tagging prob-
lem. We propose a new machine learning 
framework based on Conditional Random 
Fields (CRFs). It can employ rich features to 
jointly extract positive opinions, negative opi-
nions and object features for review sentences.
The linguistic structure can be naturally inte-
grated into model representation. Besides li-
near-chain structure, we also investigate con-
junction structure and syntactic tree structure
in this framework. Through extensive experi-
ments on movie review and product review 
data sets, we show that structure-aware mod-
els outperform many state-of-the-art ap-
proaches to review mining.
With the rapid expansion of e-commerce, people 
are more likely to express their opinions and 
hands-on experiences on products or services
they have purchased. These reviews are impor-
tant for both business organizations and personal 
costumers. Companies can decide on their strat-
egies for marketing and products improvement. 
Customers can make a better decision when pur-
1 Note that there are two meanings for word ?feature?. 
We use ?object feature? to represent the target entity,
which the opinion expressed on, and use ?feature? as
the input for machine learning methods.
chasing products or services. Unfortunately, 
reading through all customer reviews is difficult, 
especially for popular items, the number of re-
views can be up to hundreds or even thousands. 
Therefore, it is necessary to provide coherent 
and concise summaries for these reviews.
Figure 1. Feature based Review Summarization
Inspired by previous work (Hu and Liu, 2004; 
Jin and Ho, 2009), we aim to provide object fea-
ture based review summarization. Figure 1 
shows a summary example for movie ?Gone 
with the wind?. The object (movie) features, 
such as ?movie?, ?actor?, with their correspond-
ing positive opinions and negative opinions, are 
listed in a structured way. The opinions are 
ranked by their frequencies. This provides a con-
cise view for reviews. To accomplish this goal, 
we need to do three tasks:  1), extract all the ob-
ject features and opinions; 2), determine the sen-
timent polarities for opinions; 3), for each object 
feature, determine the relevant opinions, i.e. ob-
ject feature-opinion pairs.
For the first two tasks, most previous studies
employ linguistic rules or statistical methods (Hu 
and Liu, 2004; Popescu and Etzioni 2005). They 
mainly use unsupervised learning methods,
which lack an effective way to address infre-
quent object features and opinions. They are also
hard to incorporate rich overlapping features.
Gone With The Wind:
Movie:
     Positive: great, good, amazing, ? , breathtaking
     Negative: bad, boring, waste time, ? , mistake
Actor: 
     Positive: charming , brilliant , great, ? , smart 
     Negative: poor, fail, dirty, ? , lame
Music:
     Positive: great, beautiful, very good, ? , top
     Negative: annoying, noise, too long, ? , unnecessary 
    ? ?
653
Actually, there are many useful features, which 
have not been fully exploited for review mining.
Meanwhile, most of previous methods extract 
object features, opinions, and determine the po-
larities for opinions separately. In fact, the object 
features, positive opinions and negative opinions
correlate with each other. 
In this paper, we formulate the first two tasks,
i.e. object feature, opinion extraction and opi-
nion polarity detection, as a joint structure tag-
ging problem, and propose a new machine learn-
ing framework based on Conditional Random 
Fields (CRFs). For each sentence in reviews, we 
employ CRFs to jointly extract object features,
positive opinions and negative opinions, which 
appear in the review sentence. This framework
can naturally encode the linguistic structure. Be-
sides the neighbor context with linear-chain 
CRFs, we propose to use Skip-chain CRFs and 
Tree CRFs to utilize the conjunction structure
and syntactic tree structure. We also propose a
new unified model, Skip-Tree CRFs to integrate 
these structures. Here, ?structure-aware? refers 
to the output structure, which model the relation-
ship among output labels. This is significantly 
different from the previous input structure me-
thods, which consider the linguistic structure as 
heuristic rules (Ding and Liu, 2007) or input fea-
tures for classification (Wilson et al 2009). Our 
proposed framework has the following advan-
tages: First, it can employ rich features for re-
view mining. We will analyze the effect of fea-
tures for review mining in this framework.
Second, the framework can utilize the relation-
ship among object features, positive opinions 
and negative opinions. It jointly extracts these 
three types of expressions in a unified way.
Third, the linguistic structure information can be 
naturally integrated into model representation,
which provides more semantic dependency for 
output labels. Through extensive experiments on 
movie review and product review, we show our 
proposed framework is effective for review min-
ing.
The rest of this paper is organized as follows: 
In Section 2, we review related work. We de-
scribe our structure aware review mining me-
thods in Section 3. Section 4 demonstrates the 
process of summary generation. In Section 5, we 
present and discuss the experiment results. Sec-
tion 6 is the conclusion and future work.
2 Related Work
Object feature based review summary has been 
studied in several papers. Zhuang et al (2006) 
summarized movie reviews by extracting object 
feature keywords and opinion keywords. Object 
feature-opinion pairs were identified by using a 
dependency grammar graph. However, it used a
manually annotated list of keywords to recognize 
movie features and opinions, and thus the system 
capability is limited. Hu and Liu (2004) pro-
posed a statistical approach to capture object 
features using association rules. They only con-
sidered adjective as opinions, and the polarities 
of opinions are recognized with WordNet expan-
sion to manually selected opinion seeds. Popescu 
and Etzioni (2005) proposed a relaxation labe-
ling approach to utilize linguistic rules for opi-
nion polarity detection. However, most of these 
studies focus on unsupervised methods, which
are hard to integrate various features. Some stu-
dies (Breck et al 2007; Wilson et al 2009; Ko-
bayashi et al 2007) have used classification 
based methods to integrate various features. But 
these methods separately extract object features
and opinions, which ignore the correlation 
among output labels, i.e. object features and opi-
nions. Qiu et al (2009) exploit the relations of 
opinions and object features by adding some lin-
guistic rules. However, they didn?t care the opi-
nion polarity. Our framework can not only em-
ploy various features, but also exploit the corre-
lations among the three types of expressions, i.e.
object features, positive opinions, and negative 
opinions, in a unified framework. Recently, Jin 
and Ho (2009) propose to use Lexicalized HMM
for review mining. Lexicalized HMM is a va-
riant of HMM. It is a generative model, which is 
hard to integrate rich, overlapping features. It 
may encounter sparse data problem, especially 
when simultaneously integrating multiple fea-
tures. Our framework is based on Conditional 
Random Fields (CRFs). CRFs is a discriminative 
model, which can easily integrate various fea-
tures.
These are some studies on opinion mining with 
Conditional Random Fields. For example, with 
CRFs, Zhao et al(2008) and McDonald et al 
(2007) performed sentiment classification in sen-
tence and document level; Breck et al(2007) 
identified opinion expressions from newswire 
documents; Choi et al (2005) determined opi-
654
nion holders to opinions also from newswire da-
ta. None of previous work focuses on jointly ex-
tracting object features, positive opinions and 
negative opinions simultaneously from review 
data. More importantly, we also show how to 
encode the linguistic structure, such as conjunc-
tion structure and syntactic tree structure, into 
model representation in our framework. This is 
significantly different from most of previous 
studies, which consider the structure information 
as heuristic rules (Hu and Liu, 2004) or input 
features (Wilson et al 2009).
Recently, there are some studies on joint sen-
timent/topic extraction (Mei et al 2007; Titov 
and McDonald, 2008; Snyder and Barzilay, 
2007). These methods represent reviews as sev-
eral coarse-grained topics, which can be consi-
dered as clusters of object features. They are
hard to indentify the low-frequency object fea-
tures and opinions. While in this paper, we will 
extract all the present object features and corres-
ponding opinions with their polarities. Besides, 
the joint sentiment/topic methods are mainly
based on review document for topic extraction.
In our framework, we focus on sentence-level
review extraction.
3 Structure Aware Review Mining
3.1 Problem Definition
To produce review summaries, we need to first 
finish two tasks: identifying object features, opi-
nions, and determining the polarities for opi-
nions. In this paper, we formulate these two 
tasks as a joint structure tagging problem. We
first describe some related definitions:
Definition (Object Feature): is defined as whole 
target expression that the subjective expressions 
have been commented on. Object features can be 
products, services or their elements and proper-
ties, such as ?character?, ?movie?, ?director? for 
movie review, and ?battery?, ?battery life?,
?memory card? for product review.
Definition (Review Opinion): is defined as the 
whole subjective expression on object features.
For example, in sentence ?The camera is easy to 
use?, ?easy to use? is a review opinion. ?opinion? 
is used for short.
Definition (Opinion Polarity): is defined as the 
sentiment category for review opinion. In this 
paper, we consider two types of polarities: posi-
tive opinion and negative opinion. For example,
?easy to use? belongs to positive opinion.
For our review mining task, we need to 
represent three types of expressions: object fea-
tures, positive opinions, and negative opinions. 
These expressions may be words, or whole
phrases. We use BIO encoding for tag represen-
tation, where the non-opinion and neutral opi-
nion words are represented as ?O?. With Nega-
tion (N), which is only one word, such as ?not?,
?don?t?, as an independent tag, there are totally 8 
tags, as shown in Table 1. The following is an 
example to denote the tags:
The/O camera/FB comes/O with/O a/O piti-
ful/CB 32mb/FB compact/FI flash/FI card/FI ./O
FB Feature Beginning CB Negative Beginning
FI Feature Inside CI Negative Inside
PB Positive Beginning N Negation Word 
PI Positive Inside O Other 
Table 1. Basic Tag Set for Review Mining
3.2 Structure Aware Model
In this section, we describe how to encode dif-
ferent linguistic structure into model representa-
tion based on our CRFs framework.
3.2.1 Using Linear CRFs.
For each sentence in a review, our task is to ex-
tract all the object features, positive opinions and 
negative opinions. This task can be modeled as a 
classification problem. Traditional classification 
tools, e.g. Maximum Entropy model (Berger et 
al, 1996), can be employed, where each word or 
phrase will be treated as an instance. However, 
they independently consider each word or 
phrase, and ignore the dependency relationship 
among them.
Actually, the context information plays an im-
portant role for review mining. For example, 
given two continuous words with same part of 
speech, if the previous word is a positive opi-
nion, the next word is more likely a positive opi-
nion. Another example is that if the previous 
word is an adjective, and it is an opinion, the 
next noun word is more likely an object feature.
To this end, we formulate the review mining 
task as a joint structure tagging problem, and 
propose a general framework based on Condi-
tional Random Fields (CRFs) (Lafferty et al, 
2001) which are able to model the dependencies 
655
y1 yn-1y3y2 yn
x1 xn-1x3x2 xn
(a) Linear-chain  CRFs
y4
x1 xn-1x3x2 xnxn-2
?
x4
y1 yn-2y3 yn
y2 yn-1
(c) Tree-CRFs
y4
x1 xn-1x3x2 xnxn-2
?
x4
y1 yn-2y3 yn
y2 yn-1
(d) Skip-Tree CRFs
(b) Skip-chain  CRFs
Figure 2 CRFs models
between nodes. (See Section 3.2.5 for more 
about CRFs)
In this section, we propose to use linear-chain
CRFs to model the sequential dependencies be-
tween continuous words, as discussed above. It 
views each word in the sentence as a node, and 
adjacent nodes are connected by an edge. The 
graphical representation is shown in Figure 2(a).
Linear CRFs can make use of dependency rela-
tionship among adjacent words.
3.2.2 Leveraging Conjunction Structure
We observe that the conjunctions play important 
roles on review mining: If the words or phrases 
are connected by conjunction ?and?, they mostly 
belong to the same opinion polarity. If the words 
or phrases are connected by conjunction ?but?, 
they mostly belong to different opinion polarity,
as reported in (Hatzivassiloglou and McKeown,
1997; Ding and Liu, 2007). For example, ?This
phone has a very cool and useful feature ? the
speakerphone?, if we only detect ?cool?, it is 
hard to determine its opinion polarity. But if we 
see ?cool? is connected with ?useful? by con-
junction ?and?, we can easily acquire the polari-
ty of ?cool? as positive. This conjunction struc-
ture not only helps to determine the opinions, but 
also helps to recognize object features. For ex-
ample, ?I like the special effects and music in 
this movie?, with word ?music? and conjunction
?and?, we can easily detect that ?special effects? 
as an object feature.
To model the long distance dependency with 
conjunctions, we use Skip-chain CRFs model to 
detect object features and opinions. The graphi-
cal representation of a Skip-chain CRFs, given in 
Figure 2(b), consists of two types of edges: li-
near-edge (

to 

) and skip-edge (

to 

). 
The linear-edge is described as linear CRFs. The 
skip-edge is imported as follows:
We first identify the conjunctions in the re-
view sentence, with a collected conjunction set,
including ?and?, ?but?, ?or?, ?however?, ?al-
though? etc. For each conjunction, we extract its 
connected two text sequences. The nearest two 
words with same part of speech from the two 
text sequences are connected with the skip-edge. 
Here, we just consider the noun, adjective, and 
adverb. For example, in ?good pictures and 
beautiful music?, there are two skip-edges: one 
connects two adjective words ?good? and ?beau-
tiful?; the other connects two nouns ?pictures? 
and ?music?. We also employ the general senti-
ment lexicons, SentiWordNet (Esuli and Sebas-
tiani, 2006), to connect opinions. Two nearest 
opinion words, detected by sentiment lexicon,
from two sequences, will also be connected by 
skip-edge. If the nearest distance exceeds the 
threshold, this skip edge will be discarded. Here,
we consider the threshold as nine.
Skip-chain CRFs improve the performance of 
review mining, because it naturally encodes the 
conjunction structure into model representation 
with skip-edges.
3.2.3 Leveraging Syntactic Tree Structure
Besides the conjunction structure, the syntactic 
tree structure also helps for review mining. The
tree denotes the syntactic relationship among 
words. In a syntactic dependency representation, 
each node is a surface word. For example, the 
corresponding dependency tree (Klein and Man-
ning, 2003) for the sentence, ?I really like this 
long movie?, is shown in Figure 3.
y1 yn-1y3y2 yn
x1 xn-1x3x2 xn
656
like
longthis
really movieI
nsubj dobjadvmod
det amod
Figure 3. Syntactic Dependency Tree Representation
In linear-chain structure and skip-chain structure, 
?like? and ?movie? have no direct edge, but in 
syntactic tree, ?movie? is directly connected 
with ?like?, and their relationship ?dobj? is also 
included, which shows ?movie? is an objective 
of ?like?. It can provide deeper syntactic depen-
dencies for object features, positive opinions and 
negative opinions. Therefore, it is important to 
consider the syntactic structure in the review 
mining task. 
In this section, we propose to use Tree CRFs to
model the syntactic tree structure for review 
mining. The representation of a Tree CRFs is 
shown in Figure 2(c). The syntactic tree structure 
is encoded into our model representation. Each 
node is corresponding to a word in the depen-
dency tree. The edge is corresponding to depen-
dency tree edge. Tree CRFs can make use of de-
pendency relationship in syntactic tree structure
to boost the performance.
3.2.4 Integrating Conjunction Structure and 
Syntactic Tree Structure
Conjunction structure provides the semantic re-
lations correlated with conjunctions. Syntactic 
tree structure provides dependency relation in 
the syntactic tree. They represent different se-
mantic dependencies. It is interesting to consider 
these two dependencies in a unified model. We 
propose Skip-Tree CRFs, to combine these two 
structure information. The graphical representa-
tion of a Skip-Tree CRFs, given in Figure 2(d),
consists of two types of edges: tree edges and 
conjunction skip-edges. We hope to simulta-
neously model the dependency in conjunction 
structure and syntactic tree structure.
We also notice that there is a relationship 
?conj? in syntactic dependency tree. However, 
we find that it only connects two head words for 
a few coordinating conjunction, such as ?and", 
?or", ?but?. Our designed conjunction skip-edge
provides more information for joint structure 
tagging. We analyze more conjunctions to con-
nect not only two head words, but also the words 
with same part of speech. We also connect the 
words with sentiment lexicon. We will show that 
the skip-tree CRFs, which combine the two 
structures, is effective in the experiment section.
3.2.5 Conditional Random Fields
A CRFs is an undirected graphical model G of 
the conditional distribution (	|
). Y are the 
random variables over the labels of the nodes 
that are globally conditioned on X, which are the 
random variables of the observations. The condi-
tional probability is defined as: 
P
(
	 
|


)
=  
1
(
)
    



(, 	|, 
)
,
+   



(, 	|, 
)
Coling 2010: Poster Volume, pages 463?471,
Beijing, August 2010
Learning to Annotate Scientific Publications 
Minlie Huang 
State Key Laboratory of Intelligent 
Technology and Systems, 
Dept. Computer Science and Tech-
nology, Tsinghua University 
aihuang@tsinghua.edu.cn  
Zhiyong Lu 
National Center for Bio-
technology Information (NCBI), 
U.S. National Library of Medi-
cine, National Institutes of Health 
luzh@ncbi.nlm.nih.gov 
 
 
Abstract 
Annotating scientific publications with 
keywords and phrases is of great 
importance to searching, indexing, and 
cataloging such documents. Unlike 
previous studies that focused on user-
centric annotation, this paper presents our 
investigation of various annotation 
characteristics on service-centric anno-
tation. Using a large number of publicly 
available annotated scientific publica-
tions, we characterized and compared the 
two different types of annotation 
processes. Furthermore, we developed an 
automatic approach of annotating 
scientific publications based on a 
machine learning algorithm and a set of 
novel features. When compared to other 
methods, our approach shows significant-
ly improved performance. Experimental 
data sets and evaluation results are pub-
licly available at the supplementary web-
site1. 
1 Introduction 
With the rapid development of the Internet, the 
online document archive is increasing quickly 
with a growing speed. Such a large volume and 
the rapid growth pose great challenges for docu-
ment searching, indexing, and cataloging. To 
facilitate these processes, many concepts have 
been proposed, such as Semantic Web (Berners-
Lee et al, 2001), Ontologies (Gruber, 1993), 
Open Directory Projects like Dmoz2, folksono-
                                                 
1 http://www.ncbi.nlm.nih.gov/CBBresearch/Lu/indexing 
2 http://www.dmoz.org/  
mies (Hotho et al, 2006), and social tagging sys-
tems like Flickr and CiteULike. Annotating doc-
uments or web-pages using Ontologies and Open 
Directories are often limited to a manually con-
trolled vocabulary (developed by service provid-
ers) and a small number of expert annotators, 
which we call service-centric annotation. By 
contrast, social tagging systems in which regis-
tered users can freely use arbitrary words to tag 
images, documents or web-pages, belong to us-
er-centric annotation. Although many advantag-
es have been reported in user-centric annotation, 
low-quality and undesired annotations are always 
observed due to uncontrolled user behaviors (Xu 
et al, 2006; Sigurbj?rnsson and Zwol, 2008). 
Moreover, the vocabulary involved in user-
centric annotation is arbitrary, unlimited, and 
rapid-growing in nature, causing more difficul-
ties in tag-based searching and browsing (Bao et 
al., 2007; Li et al, 2007). 
Service-centric annotation is of importance for 
managing online documents, particularly in serv-
ing high-quality repositories of scientific litera-
ture. For example, in biomedicine, Gene Ontolo-
gy (Ashburner et al, 2000) annotation has been 
for a decade an influential research topic of un-
ifying reliable biological knowledge from the 
vast amount of biomedical literature. Document 
annotation can also greatly help service providers 
such as ACM/IEEE portals to provide better user 
experience of search. Much work has been de-
voted to digital document annotation, such as 
ontology-based (Corcho, 2006) and semantic-
oriented (Eriksson, 2007). 
This paper focuses on service-centric annota-
tion. Our task is to assign an input document a 
list of entries. The entries are pre-defined by a 
controlled vocabulary. Due to the data availabili-
ty, we study the documents and vocabulary in the 
463
biomedical domain. We first analyze human an-
notation behaviors in two millions previously 
annotated documents. When compared to user-
centric annotation, we found that the two annota-
tion processes have major differences and that 
they also share some common grounds. Next, we 
propose to annotate new articles with a learning 
method based on the assumption that documents 
similar in content share similar annotations. To 
this end, we utilize a logistic regression algo-
rithm with a set of novel features. We evaluate 
our approach with extensive experiments and 
compare it to the state of the art. The contribu-
tions of this work are two-fold: First, we present 
an in-depth analysis on annotation behaviors be-
tween service-centric and user-centric annotation. 
Second, we develop an automatic method for 
annotating scientific publications with significant 
improvements over other systems. 
The remainder of the paper is organized as fol-
lows: We present several definitions in Section 2 
and the analysis of annotation behaviors in Sec-
tion 3. In Section 4, we presented the logistic 
regression algorithm for annotation. Benchmark-
ing results are shown in Section 5. We surveyed 
related work in Section 6 and summarized our 
work in Section 7. 
2 Definitions 
A controlled vocabulary: V, a set of pre-
specified entries for describing certain topics. 
Entries in the vocabulary are organized in a hie-
rarchical structure. This vocabulary can be mod-
ified under human supervision.  
Vocabulary Entry: an entry in a controlled 
vocabulary is defined as a triplet: VE = (MT, 
synonyms, NodeLabels). MT is a major term de-
scribing the entry, and NodeLabels are a list of 
node labels in the hierarchical tree. An entry is 
identified by its MT, and a MT may have mul-
tiple node labels as a MT may be mapped to sev-
eral nodes of a hierarchical tree.  
Entry Binary Relation: ISA(VEi, VEj) means 
entry VEj is a child of entry VEi, and SIB(VEi, 
VEj) meaning that VEj is a sibling of entry VEi. A 
set of relations determine the structure of a hie-
rarchy.  
Entry Depth: the depth of an entry relative to 
the root node in the hierarchy. The root node has 
a depth of 1 and the immediate children of a root 
node has a depth of 2, and so on. A major term 
may be mapped to several locations in the hie-
rarchy, thus we have minimal, maximal, and av-
erage depths for each MT.  
Given the above definitions, a controlled vo-
cabulary is defined as {<VEi, ISA(VEi,VEj), 
SIB(VEi,VEj)>|any i, j }. The annotation task is 
stated as follows: given a document D, predicting 
a list of entries VEs that are appropriate for anno-
tating the document. In our framework, we ap-
proach the task as a ranking problem, as detailed 
in Section 4. 
3 Analyzing Service-centric Annotation 
Behavior 
Analyzing annotation behaviors can greatly faci-
litate assessing annotation quality, reliability, and 
consistency. There has been some work on ana-
lyzing social tagging behaviors in user-centric 
annotation systems (Sigurbj?rnsson and Zwol, 
2008; Suchanek et al, 2008). However, to the 
best of our knowledge, there is no such analysis 
on service-centric annotation. In social tagging 
systems, no specific skills are required for partic-
ipating; thus users can tag the resources with ar-
bitrary words (the words may even be totally ir-
relevant to the content, such as ?todo?). By con-
trast, in service-centric annotation, the annotators 
must be trained, and they must comply with a set 
of strict guidelines to assure the consistent anno-
tation quality. Therefore, it is valuable to study 
the differences between the two annotation 
processes. 
3.1 PubMed Document Collection 
To investigate annotation behaviors, we down-
loaded 2 million documents from PubMed3, one 
of the largest search portals for biomedical ar-
ticles. These articles were published from Jan. 1, 
2000 to Dec. 31, 2008. All these documents have 
been manually annotated by National Library 
Medicine (NLM) human curators. The controlled 
vocabulary used in this system is the Medical 
Subject Headings (MeSH?)4, a thesaurus describ-
ing various biomedical topics such as diseases, 
chemicals and drugs, and organisms. There are 
25,588 entries in the vocabulary in 2010, and 
there are updates annually. By comparison, the 
vocabulary used in user-centric annotation is re-
                                                 
3 http://www.ncbi.nlm.nih.gov/pubmed/ 
4 http://www.nlm.nih.gov/mesh/ 
464
markably larger (usually more than 1 million tags) 
and more dynamic (may be updated every day). 
3.2 Annotation Characteristics 
First, we examine the distribution of the number 
of annotated entries in the document collection. 
For each number of annotated entries, we 
counted the number of documents with respect to 
different numbers of annotations. The number of 
annotations per document among these 2 million 
documents varies from 1 (with 176,383 docu-
ments) to 97 (with one document only). The av-
erage number of annotations per document is 
10.10, and the standard deviation is 5.95.  
 
Figure 1. The original distribution and simulated 
normal distribution. Each data point denotes the 
number of documents (y-axis) that has the cor-
responding number of entries (x-axis). 
 
As illustrated in Figure 1, when there are more 
than 4 annotations, the distribution fits a normal 
distribution. Comparing with user-centric annota-
tion, there are three notable observations: a), the 
maximal number of annotations per document 
(97) is much smaller (in social tagging systems 
the number amounts to over 104) due to much 
less annotators involved in service-centric anno-
tation than users in user-centric annotation; b), 
the number of annotations assigned to documents 
conforms to a normal distribution, which has not 
yet been reported in user-centric annotation; c), 
similar to user-centric annotation, the number of 
documents that have only one annotation ac-
counts for a large proportion.  
Second, we investigate whether the Zipf law 
(Zipf, 1949) holds in service-centric annotation. 
To this end, we ranked all the entries according 
to the frequency of being annotated to docu-
ments. We plotted the curve in logarithm scale, 
as illustrated in Figure 2. The curve can be simu-
lated by a linear function in logarithm scale if 
ignoring the tail which corresponds to very infre-
quently used entries. To further justify this find-
ing, we ranked all the documents according to 
the number of assigned annotations and plotted 
the curve in logarithm scale, as shown in Figure 
3. Similar phenomenon is observed. In conclu-
sion, the Zipf law also holds in service-centric 
annotation, just as reported in user-centric anno-
tation (Sigurbj?rnsson and Zwol, 2008).  
 
Figure 2. The distribution of annotated entry 
frequency. X-axis is the rank of entries (ranking 
by the annotation frequency), and y-axis is the 
frequency of an entry being used in annotation.  
 
Figure 3. The distribution of the number of an-
notated entries. X-axis is the rank of a document 
(in log10 scale), and y-axis is the number of anno-
tations assigned to documents (in log2 scale). 
 
Furthermore, as mentioned in Section 2, the 
vocabulary corresponds to a hierarchy tree once a 
set of binary relations were defined.  Thus we 
can easily obtain the minimal, maximal, and av-
erage depth of an entry. The larger depth an entry 
has, the more specific meaning it has. 
Therefore, we investigate whether service-
centric annotation is performed at very specific 
level (with larger depth) or general level (with 
smaller depth). We define prior depth and anno-
tation depth for this study, as follows: 
( )
PriorDepth                    (1)
| |VE V
Dep VE
V?
=?  
0
20000
40000
60000
80000
100000
120000
140000
160000
180000
200000
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52
original
normal
Number of annotated entries 
Num
ber of docum
ents
1
100
10000
1000000
1 10 100 1000 10000 100000
1
2
4
8
16
32
64
128
1 10 100 1000 10000 100000
Rank of the document
Num
ber of annotations
465
AnnoDepth Pr( )* ( )     (2)
VE V
VE Dep VE?=?  
( )
Pr( )                               (3)
( )
VE V
f VE
VE
f VE?
=?  
where Dep(VE) is the minimal, maximal, or av-
erage depth of an entry, f(VE) is the usage fre-
quency of VE in annotation, and |V| is the num-
ber of entries in the vocabulary. The two formu-
las are actually the mathematical expectations of 
the hierarchy?s depth under two distributions re-
spectively: a uniform distribution (1/|V|) and the 
annotation distribution (formula (3)). As shown 
in Table 1, the two expectations are close. This 
means the annotation has not been biased to ei-
ther general or specific level, which suggests that 
the annotation quality is sound.  
Dep(VE) PriorDepth AnnoDepth 
MAX 4.88 4.56 
MIN 4.25 4.02 
AVG 4.56 4.29 
Table 1. Annotation depth comparison. 
 
Figure 4. The imbalance frequency (y-axis) of 
annotated categories (x-axis). 
3.3 Annotation Categorization Imbalance 
We investigate here whether service-centric an-
notation is biased to particular categories in the 
hierarchy. We define a category as the label of 
root nodes in the hierarchy. In our vocabulary, 
there are 11 categories that have at least one an-
notation. The complete list of these categories is 
available at the website5 . Three newly created 
categories have no annotations in the document 
collection. The total number of annotations with-
in a category was divided by the number of en-
                                                 
5 http://www.nlm.nih.gov/mesh/2010/mesh_browser/MeSHtree.Z.html 
tries in that category, as different categories may 
have quite different numbers of entries. If an en-
try is mapped to multiple locations, its annota-
tions will be counted to corresponding categories 
repeatedly.  
From Figure 4, we can see that there is imbal-
ance with respect to the annotations in different 
categories. Category ?diseases? has 473.5 anno-
tations per entry (totally 4408 entries in this cat-
egory). Category ?chemicals and drugs? has 
423.0 annotations per entry (with 8815 entries in 
total). Due to the fact that diseases and chemicals 
and drugs are hot scientific topics, these catego-
ries are largely under-annotated. The most fre-
quently annotated category is: ?named groups? 
(7144.4 annotations per entry), with 199 entries 
in total. The issue of imbalanced categorization 
may be due to that the topics of the document 
collection are of imbalance; and that the vocabu-
lary was updated annually, so that the latest en-
tries were used less frequently. As shown in (Si-
gurbj?rnsson and Zwol, 2008), this imbalance 
issue was also observed in user-centric annota-
tion, such as in Flickr Tagging. 
4 Learning to Annotate 
As shown in Section 3, there are much fewer an-
notations per document in service-centric annota-
tion than in user-centric annotations. Service-
centric annotation is of high quality, and is li-
mited to a controlled vocabulary. However, ma-
nual annotation is time-consuming and labor in-
tensive, particularly when seeking high quality. 
Indeed, our analysis shows that on average it 
takes over 90 days for a PubMed citation to be 
manually annotated with MeSH terms. Thus we 
propose to annotate articles automatically. Spe-
cifically, we approach this task as a ranking 
problem: First, we retrieve k-nearest neighboring 
(KNN) documents for an input document using a 
retrieval model (Lin and Wilbur, 2007). Second, 
we obtain an initial list of annotated entries from 
those retrieved neighboring documents. Third, 
we rank those entries using a logistic regression 
model. Finally, the top N ranked entries are sug-
gested as the annotations for the target document.  
4.1 Logistic Regression 
We propose a probabilistic framework of directly 
estimating the probability that an entry can be 
used to annotate a document. Given a document 
0
1000
2000
3000
4000
5000
6000
7000
8000
An
at
om
y 
[A
]
Or
ga
ni
sm
s [
B]
Di
se
as
es
 [C
]
Ch
em
ica
ls 
an
d 
Dr
ug
s [
D]
An
al
yt
ica
l, 
Di
ag
no
st
ic 
an
d ?
Ps
yc
hi
at
ry
 a
nd
 P
sy
ch
ol
og
y 
[F
]
Ph
en
om
en
a 
an
d 
Pr
oc
es
se
s ?
Di
sc
ip
lin
es
 a
nd
 ?
An
th
ro
po
lo
gy
, E
du
ca
tio
n,
 ?
Te
ch
no
lo
gy
, I
nd
us
try
, ?
In
fo
rm
at
io
n 
Sc
ie
nc
e 
[L
]
Na
m
ed
 G
ro
up
s [
M
]
He
al
th
 C
ar
e 
[N
]
466
D and an entry VE, we compute the probability 
Pr(R(VE)|D) directly using a logistic regression 
algorithm. R(VE) is a binary random variable 
indicating whether VE should be assigned as an 
annotation of the document. According to this 
probability, we can rank the entries obtained 
from neighboring documents. Much work used 
Logistic Regression as classification: Pr(R=1|D) 
>? where ? is a threshold, but it is difficult to 
specify an appropriate value for the threshold in 
this work, as detailed in Section 5.5. 
We applied the logistic regression model to 
this task. Logistic regression has been successful-
ly employed in many applications including mul-
tiple ranking list merging (Si and Callan, 2005) 
and answer validation for question answering 
(Ko et al, 2007). The model gives the following 
probability: 
1 1
Pr( ( ) | ) exp( * ) 1 exp( * )       (4)
m m
i i i i
i i
R VE D b w x b w x
= =
? ?= + + +? ?? ?? ?
where x= (x1, x2, ?, xm) is the feature vector for 
VE and m is the number of features.  
For an input document D, we can obtain an in-
itial list of entries {VE1,VE2,?,VEn} from its 
neighboring documents. Each entry is then 
represented as a feature vector as x= (x1, x2, ?, 
xm). Given a collection of N documents that have 
been annotated manually, each document will 
have a corresponding entry list, {VE1, 
VE2,?,VEn}, and each VEi has gold-standard la-
bel yi=1 if VEi was used to annotate D, or yi=0 
otherwise. Note that the number of entries of la-
bel 0 is much larger than that of label 1 for each 
document. This may bias the learning algorithm. 
We will discuss this in Section 5.5. Given such 
data, the parameters can be estimated using the 
following formula:  
( )* *
, 1 1
, argmax log Pr( ( ) | )          (5)
jLN
i j
w b j i
w b R VE D
= =
= ??


 
where Lj is the number of entries to be ranked for 
Dj, and N is the total number of training docu-
ments. We can use the Quasi-Newton algorithm 
for parameter estimation (Minka, 2003). In this 
paper, we used the WEKA6 package to imple-
ment this model. 
4.2 Features 
We developed various novel features to build 
connections between an entry and the document 
                                                 
6http://www.cs.waikato.ac.nz/ml/weka/ .  
text. When computing these features, both the 
entry?s text (major terms, synonyms) and the 
document text (title and abstract) are tokenized 
and stemmed. To compute these features, we 
collected a set of 13,999 documents (each has 
title, abstract, and annotations) from PubMed. 
Prior probability feature. We compute the 
appearance probability of a major term (MT), 
estimated on the 2 million documents. This prior 
probability reflects the prior quality of an entry. 
Unigram overlap with the title. We count the 
number of unigrams overlapping between the MT 
of an entry and the title, dividing by the total 
number of unigrams in the MT. 
Bigram overlap with the document. We first 
concatenate the title and abstract, then count the 
number of bigram overlaps between the MT and 
the concatenated string, dividing by the total 
number of bigrams in the MT. 
Multinomial distribution feature. This fea-
ture assumes that the words in a major term ap-
pear in the document text with a multinomial 
distribution, as follows:  
#( , )Pr( | )
Pr( | ) | | !*     (6)
#( , )!
w MT
w MT
w Text
MT Text MT
w MT?
= ?
 
#( , )
Pr( | ) (1 ) Pr ( )   (7)
#( , )
i
c
iw
w Text
w Text w
w Text
? ?= ? +?  
where: 
#(w,MT) - The number of times that w appears in 
MT; Similarly for #(w,Text); 
|MT| - The number of single words in MT; 
Text - Either the title or abstract, thus we have 
two features of this type: Pr(MT|Title) and 
Pr(MT|Abstract); 
Prc(w) - The probability of word w occurring in a 
background corpus. This is obtained from a uni-
gram language model that was estimated on the 
13,999 articles; 
? ? A smoothing parameter that was empirically 
set to be 0.2. 
Query-likelihood features. The major term of 
an entry is viewed as a query, and this class of 
features computes likelihood scores between the 
query (as Q) and the article D (either the title or 
the abstract). We used the very classic okapi 
model (Robertson et al 1994), as follows: 
( ) 0.5
( , )*log
( ) 0.5
( , )   (8)
| |
0.5 1.5* ( , )
(| |)
q Q
N df q
tf q D
df q
Okapi Q D
D
tf q D
avg D
?
? ?? +? ?+? ?= ? ?+ +? ?? ?
?  
467
where:  
tf(q,D) - The count of q occurring in document D;  
|D| - The total word counts in document D;  
df(q) - The number of documents containing 
word q;  
avg(|D|) - The average length of documents in 
the collection;  
N - The total number of documents (13,999).  
We have two features: okapi(MT, Title) and 
okapi(MT, Abstract). In other words, the title and 
abstract are processed separately. The advantage 
of using such query-likelihood scores is that they 
give a probability other than a binary judgment 
of whether a major term should be annotated to 
the article, as only indirect evidence exists for 
annotating a vocabulary entry to an article in 
most cases. 
Neighborhood features. The first feature 
represents the number of neighboring documents 
that include the entry to be annotated for a doc-
ument. The second feature, instead of counting 
documents, sums document similarity scores. 
The two features are formulated as follows, re-
spectively: 
{ }( | ) | ,     (9)i i i kfreq MT D D MT D D= ? ??  
; 
( | ) ( , )          (10)
i i k
i
MT D D
sim MT D sim D D
? ??
= ?  
where ?k is the k-nearest neighbors for an input 
document D and sim(Di,Dj) is the similarity score 
between a target document and its neighboring 
document, given by the retrieval model.  
Synonym Features. Each vocabulary entry 
has synonyms. We designed two binary features: 
one judges whether there exists a synonym that 
can be exactly matched to the article text (title 
and abstract); and the other measures whether 
there exists a synonym whose unigram words 
have all been observed in the article text. 
5 Experiment 
5.1 Datasets 
To justify the effectiveness of our method, we 
collected two datasets. We randomly selected a 
set of 200 documents from PubMed to train the 
logistic regression model (named Small200). For 
testing, we used a benchmark dataset, NLM2007, 
which has been previously used in benchmarking 
biomedical document annotation7 (Aronson et al, 
                                                 
7http://ii.nlm.nih.gov/.  
2004; Vasuki and Cohen, 2009; Trieschnigg et 
al., 2009). The two datasets have no overlap with 
the aforementioned 13,999 documents. Each 
document in these two sets has only title and ab-
stract (i.e., no full text). The statistics listed in 
Table 2 show that the two datasets are alike in 
terms of annotations. Note that we also evaluate 
our method on a larger dataset of 1000 docu-
ments, but due to the length limit, the results are 
not presented in this paper. 
Dataset Documents 
Total   
annotations 
Average 
annotations 
Small200 200 2,736 13.7 
NLM2007 200 2,737 13.7 
Table 2. Statistics of the two datasets.  
5.2 Evaluation Metrics 
We use precision, recall, F-score, and mean av-
erage precision (MAP) to evaluate the ranking 
results. As can be seen from Section 3.2, the 
number of annotations per document is about 10. 
Thus we evaluated the performance with top 10 
and top 15 items. 
5.3 Comparison to Other Approaches 
We compare our approach to three methods on 
the benchmark dataset - NLM2007. The first sys-
tem is NLM?s MTI system (Aronson et al, 2004). 
This is a knowledge-rich method that employs 
NLP techniques, biomedical thesauruses, and a 
KNN module. It also utilizes handcrafted filtering 
rules for refinement. The second and third me-
thods rank entries according to Formula (9) and 
(10), respectively (Trieschnigg et al, 2009).  
We trained our model on Small200. All fea-
ture values were normalized to [0,1] using the 
maximum values of each feature. The number of 
neighbors was set to be 20. Neighboring docu-
ments were retrieved from PubMed using the 
retrieval model described in (Lin and Wilbur, 
2007). Existing document annotations were not 
used in retrieving similar documents as they 
should be treated as unavailable for new docu-
ments. As the average number of annotations per 
document is around 13 (see Table 2), we com-
puted precision, recall, F-score, and MAP with 
top 10 and 15 entries, respectively.  
Results in Table 3 demonstrate that our me-
thod outperforms all other methods. It has sub-
stantial improvements over MTI. To justify 
whether the improvement over using neighbor-
468
hood similarity is significant, we conducted the 
Paired t-test (Goulden, 1956). When comparing 
results of using learning vs. neighborhood simi-
larity in Table 3, the p-value is 0.028 for top 10 
and 0.001 for top 15 items. This shows that the 
improvement achieved by our approach is statis-
tically significant (at significance level of 0.05).  
 Methods Pre. Rec. F. MAP 
Top 
10 
MTI .468 .355 .404 .400 
Frequency .635 .464 .536 .598 
Similarity .643 .469 .542 .604 
Learning .657 .480 .555 .622 
Top 
15 
MTI .404 .442 .422 .400 
Frequency .512 .562 .536 .598 
Similarity .524 .574 .548 .604 
Learning .539 .591 .563 .622 
Table 3. Comparative results on NLM2007. 
5.4 Choosing Parameter k 
We demonstrate here our search for the optimal 
number of neighboring documents in this task. 
As shown in Table 4, the more neighbors, the 
larger number of gold-standard annotations 
would be present in neighboring documents. 
With 20 neighbors a fairly high upper-bound re-
call (UBR) is observed (about 85% of gold-
standard annotations of a target document were 
present in its 20 neighbors? annotations), and the 
average number of entries (Avg_VE) to be ranked 
is about 100.  
 
Figure 5. The performance (y-axis) varies with 
the number of neighbors (x-axis). 
Measure 
The number of neighboring documents 
5  10   15  20  25 30 
UBR .704 .793 .832 .856 .871 .882 
Avg_VE 38.8 64.1 83.6 102.2 119.7 136.4 
Table 4. The upper-bound recall (UBR) and av-
erage number of entries (Avg_VE) with different 
number of neighboring documents.  
 
To investigate whether the number of neigh-
boring documents affects performance, we expe-
rimented with different numbers of neighboring 
documents. We trained a model on Small200, 
and tested it on NLM2007. The curves in Figure 
5 show that the performance becomes very close 
when choosing no less than 10 neighbors. This 
infers that reliable performance can be obtained. 
The best performance (F-score of 0.563) is ob-
tained with 20 neighbors. Thus, the parameter k  
is set to be 20. 
5.5 Data Imbalance Issue 
As mentioned in Section 4.1, there is a data im-
balance issue in our task. For each document, we 
obtained an initial list of entries from 20 neigh-
boring documents. The average number of gold-
standard annotations is about 13, while the aver-
age number of entries to be ranked is around 100 
(see Table 4). Thus the number of entries of label 
0 (negative examples) is much larger than that of 
label 1 (positive examples). We did not apply 
any filtering strategy because the gold-standard 
annotations are not proportional to their occur-
ring frequency in the neighboring documents. In 
fact, as shown in Figure 6, the majority of gold-
standard annotations appear in only few docu-
ments among 20 neighbors. For example, there 
are about 250 gold-standard annotations appear-
ing in only one of 20 neighboring documents and 
964 appearing in less than 6 neighboring docu-
ments. Therefore, applying any filtering strategy 
based on their occurrence in neighboring docu-
ments may be harmful to the performance. 
 
Figure 6. The distribution of annotations. X-axis 
is the number of neighboring documents in 
which gold-standard annotations are found. 
5.6 Feature Analysis 
To investigate the impact of different groups of 
features, we performed a feature ablation study. 
The features were divided into four groups. For 
each round of this study, we remove one group 
of features from the entire feature set, re-train the 
model on Small200, and then test the perfor-
mance on NLM2007 with top 15 entries. We di-
vided the features into four independent groups: 
0.45
0.5
0.55
0.6
0.65
5 10 15 20 25 30
MAP
Recall
F-score
Precision
0
50
100
150
200
250
300
1 2 3 4 5 6 7 8 9 1011121314151617181920
G
oldstandard annotations
469
prior probability features, neighborhood features, 
synonym features, and other features (including 
unigram/bigram feature, query likelihood feature, 
etc., see Section 4.2). Results in Table 5 show 
that neighborhood features are dominant: remov-
ing such features leads to a remarkable decrease 
in performance. On the other hand, using only 
neighborhood features (the last row) yields sig-
nificant worse results than using all features. 
This means that combining all features together 
indeed contributes to the optimal performance. 
Feature Set Pre. Rec. F. MAP 
All features .539 .591 .563 .622 
- Prior probability .538 .590 .563  .622 
- Neighborhood features .419* .459* .438*  .467* 
- Synonym features .532 .583 .556  .611 
- Other features .529 .580 .553  .621 
Only neighborhood features .523* .573* .547* .603* 
Table 5. Feature analysis. Those marked by stars 
are significantly worse than the best results. 
5.7 Discussions 
All methods that rely on neighboring documents 
have performance ceilings. Specifically, for the 
NLM2007 dataset, the upper bound recall is 
around 85.6% with 20 neighboring documents, 
as shown in Table 5. Due to the same reason, this 
genre of methods is also limited to recommend 
entries that are recently added to the controlled 
vocabulary as such entries may have not been 
annotated to any document yet. This phenome-
non has been demonstrated in the annotation be-
havior analysis: those latest entries have substan-
tially fewer annotations than older ones.  
6 Related Work 
Our work is closely related to ontology-based or 
semantic-oriented document annotation (Corcho, 
2006; Eriksson, 2007). This work is also related 
to KNN-based tag suggestion or recommendation 
systems (Mishne, 2006). 
The task here is similar to keyword extraction 
(Nguyen and Kan, 2007; Jiang et al, 2009), but 
there is a major difference: keywords are always 
occurring in the document, while when an entry 
of a controlled vocabulary was annotated to a 
document, it may not appear in text at all.  
As for the task tackled in this paper, i.e., anno-
tating biomedical publications, three genres of 
approaches have been proposed: (1) k-Nearest 
Neighbor model: selecting annotations from 
neighboring documents, ranking and filtering 
those annotations (Vasuki and Cohen, 2009; Tri-
eschnigg et al, 2009). (2) Classification model: 
learning the association between the document 
text and an entry (Ruch, 2006). (3) Based on 
knowledge resources: using domain thesauruses 
and NLP techniques to match an entry with con-
cepts in the document text (Aronson, 2001; 
Aronson et al, 2004). (4) LDA-based topic mod-
el: (M?rchen et al, 2008). 
7 Conclusion 
This paper presents a novel study on service-
centric annotation. Based on the analysis results 
of 2 million annotated scientific publications, we 
conclude that service-centric annotation exhibits 
the following unique characteristics: a) the num-
ber of annotation per document is significant 
smaller, but it conforms to a normal distribution; 
and b) entries of different granularity (general vs. 
specific) are used appropriately by the trained 
annotators. Service-centric and user-centric an-
notations have in common that the Zipf law 
holds and categorization imbalance exists. 
Based on these observations, we introduced a 
logistic regression approach to annotate publica-
tions, with novel features. Significant improve-
ments over other systems were obtained on a 
benchmark dataset. Although our features are 
tailored for this task in biomedicine, this ap-
proach may be generalized for similar tasks in 
other domains. 
Acknowledgements 
This work was supported by the Intramural Re-
search Program of the NIH, National Library of 
Medicine. The first author was also supported by 
the Chinese Natural Science Foundation under 
grant No. 60803075 and the grant from the Inter-
national Development Research Center, Ottawa, 
Canada IRCI.  
References 
Alan R. Aronson. Effective mapping of biomedical 
text to the UMLS Metathesaurus: the metamap 
program. In Proc AMIA Symp 2001. p. 17-21. 
Alan Aronson, Alan R. Aronson, James Mork, James 
G. Mork, Clifford Gay, Clifford W. Gay, Susanne 
Humphrey, Susanne M. Humphrey, Willie Rogers, 
Willie J. Rogers. The NLM Indexing Initiative's 
470
Medical Text Indexer. Stud Health Technol In-
form. 2004;107(Pt 1):268-72. 
Michael Ashburner, Catherine A. Ball, Judith A. 
Blake, David Botstein, Heather Butler, et al Gene 
Ontology: tool for the unification of biology. Nat 
Genet. 2000 May; 25(1):25-9. 
Shenghua Bao, Xiaoyuan Wu, Ben Fei, Guirong Xue, 
Zhong Su, and Yong Yu. Optimizing Web Search 
Using Social Annotations. WWW 2007, May 8?12, 
2007, Banff, Alberta, Canada. Pp 501-510. 
Tim Berners-Lee, James Hendler and Ora Lassila. 
The Semantic Web. Scientific American Magazine. 
(May 17,  2001). 
Oscar Corcho. Ontology based document annotation: 
trends and open research problems. International 
Journal of Metadata, Semantics and Ontologies, 
Volume 1,  Issue 1, Pages: 47-57, 2006. 
Henrik Eriksson. An Annotation Tool for Semantic 
Documents. In Proceedings of the 4th European 
conference on The Semantic Web: Research and 
Applications, pages 759-768, 2007. Innsbruck, 
Austria. 
Cyril Harold Goulden. Methods of Statistical Analy-
sis, 2nd ed. New York: Wiley, pp. 50-55, 1956. 
Thomas R. Gruber (1993). A Translation Approach to 
Portable Ontology Specifications. Knowledge Ac-
quisition, 5(2), 1993, pp. 199-220. 
Andreas Hotho, Robert Jaschke, Christoph Schmitz, 
Gerd Stumme. Information Retrieval in Folksono-
mies: Search and Ranking. In ?The Semantic Web: 
Research and Applications?, Vol. 4011 (2006), pp. 
411-426. 
Xin Jiang, Yunhua Hu, Hang Li. A Ranking Ap-
proach to Keyphrase Extraction. SIGIR?09, July 
19?23, 2009, Boston, Massachusetts, USA. 
Jeongwoo Ko, Luo Si, Eric Nyberg. A Probabilistic 
Framework for Answer Selection in Question 
Answering. Proceedings of NAACL HLT 2007, 
pages 524?531, Rochester, NY, April 2007. 
Rui Li, Shenghua Bao, Ben Fei, Zhong Su, and Yong 
Yu. Towards Effective Browsing of Large Scale 
Social Annotations. In WWW ?07: Proceedings of 
the 16th international conference on World Wide 
Web, 2007.  
Jimmy Lin and W. John Wilbur. PubMed related ar-
ticles: a probabilistic topic-based model for content 
similarity. BMC Bioinformatics 8: (2007). 
Thomas P. Minka. A Comparison of Numerical Op-
timizers for Logistic Regression. 2003. Unpub-
lished draft. 
Gilad Mishne. AutoTag: A Collaborative Approach to 
Automated Tag Assignment for Weblog Posts. 
WWW 2006, May 22?26, 2006, Edinburgh, Scot-
land. pages 953?954. 
Fabian M?rchen, Math?us Dejori, Dmitriy Fradkin, 
Julien Etienne, Bernd Wachmann, Markus Bund-
schus. Anticipating annotations and emerging 
trends in biomedical literature. In KDD '08: pp. 
954-962. 
Thuy Dung Nguyen and Min-Yen Kan. Keyphrase 
Extraction in Scientific Publications. In Proc. of In-
ternational Conference on Asian Digital Libraries 
(ICADL ?07), pages 317-326. 
Stephen E. Robertson, Steve Walker, Susan Jones, 
Micheline Hancock-Beaulieu, and Mike Gatford. 
Okapi at TREC-3. In Proceedings of the Third Text 
REtrieval Conference (TREC 1994). Gaithersburg, 
USA, November 1994. 
Patrick Ruch. Automatic assignment of biomedical 
categories: toward a generic approach. Bioinfor-
matics. 2006 Mar 15;22(6):658-64. 
Luo Si and Jamie Callan. 2005 CLEF2005: Multilin-
gual retrieval by combining multiple multilingual 
ranked lists. In Proceedings of Cross-Language 
Evaluation Forum. 
B?rkur Sigurbj?rnsson and Roelof van Zwol. Flickr 
Tag Recommendation based on Collective Know-
ledge. WWW 2008, April 21?25, 2008, Beijing, 
China. Pp. 327-336. 
Fabian M. Suchanek, Milan Vojnovi?c, Dinan Guna-
wardena. Social Tags: Meaning and Suggestions. 
CIKM?08, October 26?30, 2008, Napa Valley, Cal-
ifornia, USA.   
Dolf Trieschnigg, Piotr Pezik, Vivian Lee, Franciska 
de Jong, Wessel Kraaij, Dietrich Rebholz-
Schuhmann. MeSH Up: effective MeSH text clas-
sification for improved document retrieval. Bioin-
formatics, Vol. 25 no. 11 2009, pages 1412?1418. 
 Vidya Vasuki and Trevor Cohen. Reflective Random 
Indexing for Semiautomatic Indexing of the Bio-
medical Literature. AMIA 2009, San Francisco, 
Nov. 14-18, 2009. 
Zhichen Xu, Yun Fu, Jianchang Mao, and Difu Su. 
Towards the Semantic Web: Collaborative Tag 
Suggestions. In WWW2006: Proceedings of the 
Collaborative Web Tagging Workshop (2006). 
George K. Zipf. (1949) Human Behavior and the 
Principle of Least-Effort. Addison-Wesley. 
471
Coling 2010: Poster Volume, pages 525?533,
Beijing, August 2010
A Comparative Study on Ranking and Selection Strategies for 
Multi-Document Summarization 
Feng Jin, Minlie Huang, Xiaoyan Zhu 
State Key Laboratory of Intelligent Technology and Systems 
Tsinghua National Laboratory for Information Science and Technology 
Dept. of Computer Science and Technology, Tsinghua University 
jinfengfeng@gmail.com,{aihuang,zxy-dcs}@tsinghua.edu.cn
Abstract 
This paper presents a comparative study 
on two key problems existing in extrac-
tive summarization: the ranking problem 
and the selection problem. To this end, 
we presented a systematic study of 
comparing different learning-to-rank al-
gorithms and comparing different selec-
tion strategies. This is the first work of 
providing systematic analysis on these 
problems. Experimental results on two 
benchmark datasets demonstrate three 
findings: (1) pairwise and listwise learn-
ing-to-rank algorithms outperform the 
baselines significantly; (2) there is no 
significant difference among the learn-
ing-to-rank algorithms; and (3) the in-
teger linear programming selection 
strategy generally outperformed Maxi-
mum Marginal Relevance and Diversity 
Penalty strategies. 
1 Introduction 
As the rapid development of the Internet, docu-
ment summarization has become an important 
task since document collections are growing 
larger and larger. Document summarization, 
which aims at producing a condensed version of 
the original document(s), helps users to acquire 
information that is both important and relevant 
to their information need.  So far, researchers 
have mainly focused on extractive methods 
which choose a set of salient textual units to 
form a summary.  Such textual units are typical-
ly sentences, sub-sentences (Gillick and Favre, 
2009), or excerpts (Sauper and Barzilay, 2009).  
Almost all extractive summarization methods 
face two key problems: the first problem is how 
to rank textual units, and the second one is how 
to select a subset of those ranked units. The 
ranking problem requires systems model the 
relevance of a textual unit to a topic or a query. 
In this paper, the ranking problem refers to ei-
ther sentence ranking or concept ranking. Con-
cepts can be unigrams, bigrams, semantic con-
tent units, etc., although in our experiment, only 
bigrams are used as concepts. The selection 
problem requires systems improve diversity or 
remove redundancy so that more relevant in-
formation can be covered by the summary as its 
length is limited. As our paper focuses on ex-
tractive summarization, the selection problem 
refers to selecting sentences. However, the se-
lection framework presented here is universal 
for selecting arbitrary textual units, as discussed 
in Section 4. 
There have been a variety of studies to ap-
proach the ranking problem. These include both 
unsupervised sentence ranking (Luhn, 1958; 
Radev and Jing, 2004, Erkan and Radev, 2004), 
and supervised methods (Ouyang et al, 2007; 
Shen et al, 2007; Li et al, 2009). Even given a 
list of ranked sentences, it is not trivial to select 
a subset of sentences to form a good summary 
which includes diverse information within a 
length limit. Three common selection strategies 
have been studied to address this problem: Max-
imum Marginal Relevance (MMR) (Carbonell 
and Goldstein, 1998), Diversity Penalty (DiP) 
(Wan, 2007), and integer linear programming 
(ILP) (McDonald, 2007; Gillick and Favre, 
2009). As different methods were often eva-
luated on different datasets, it is of great value 
to systematically compare ranking and selection 
strategies on the same dataset. However, to the 
best of our knowledge, there is still no work to 
compare different ranking strategies or compare 
different selection strategies.  
In this paper, we presented a comparative 
study on the ranking problem and the selection 
525
problem for extractive summarization. We 
compared three genres of learning-to-rank me-
thods for ranking sentences or concepts: SVR, a 
pointwise ranking algorithm; RankNet, a pair-
wise learning-to-rank algorithm; and ListNet, a 
listwise learning-to-rank algorithm. We adopted 
an ILP framework that is able to select sen-
tences based on sentence ranking or concept 
ranking. We compared it with other selection 
strategies such as MMR and Diversity Penalty. 
We conducted our comparative experiments on 
the TAC 2008 and TAC 2009 datasets, respec-
tively. Our contributions are two-fold: First, to 
the best of our knowledge, this is the first work 
of presenting systematic and in-depth analysis 
on comparing ranking strategies and comparing 
selection strategies. Second, this is the first 
work using pairwise and liswise learning-to-
rank algorithms to perform concept (word bi-
gram) ranking for extractive summarization.  
The rest of this paper is organized as follows. 
We introduce the related work in Section 2. In 
Section 3, we present three ranking algorithms, 
SVR, RankNet, and ListNet. We describe the 
sentence selection problem with an ILP frame-
work described in Section 4. We introduce fea-
tures in Section 5. Evaluation and experiments 
are presented in Section 6. Finally, we conclude 
this paper in Section 7. 
2 Related Work 
A number of extractive summarization studies 
used unsupervised methods with surface fea-
tures, linguistic features, and statistical features 
to guide sentence ranking (Edmundson, 1969; 
McKeown and Radev, 1995; Radev et al, 2004; 
Nekova et al, 2006). Recently, graph-based 
ranking methods have been proposed for sen-
tence ranking and scoring, such as LexRank 
(Erkan and Radev, 2004) and TextRank (Mihal-
cea and Tarau, 2004).  
There are also a variety of studies on su-
pervised learning methods for sentence ranking 
and selection. Kupiec et al (1995) developed a 
naive Bayes classifier to decide whether a sen-
tence is worthy to extract. Recently, Conditional 
Random Field (CRF) and Structural SVM have 
been employed for single document summariza-
tion (Shen et al, 2007; Li et al, 2009).  
Besides ranking sentences directly, there are 
some approaches that select sentences based on 
concept ranking. Radev et al (2004) used cen-
troid words whose tf*idf scores are above a 
threshold. Filatova and Hatzivassiloglou (2004) 
used atomic event as concept. Moreover, sum-
marization evaluation metrics such as Basic 
Element (Hovy et al, 2006), ROUGE (Lin and 
Hovy, 2003) and Pyramid (Passonneau et al, 
2005) are all counting the concept overlap be-
tween generated summaries and human-written 
summaries.  
Another important issue existing in extractive 
summarization is to find an optimal sentence 
subset which can cover diverse information. 
Maximal Marginal Relevance (MMR) (Carbo-
nell and Goldstein, 1998) and Diversity Penalty 
(Wan, 2007) are most widely used approaches 
to reduce redundancy. The two methods are es-
sentially based on greedy search. By contrast, 
ILP based approaches view summary generation 
as a global optimization problem. McDonald 
(2007) proposed a sentence-level ILP solution. 
Sauper and Barzilay (2009) presented an ex-
cerpt-level ILP method to generate Wikipedia 
articles. Gillick and Favre (2009) proposed a 
concept-level ILP, but they used document fre-
quency to score concepts (bigrams), without any 
learning process. Some recent studies (Gillick 
and Favre, 2009; Martins and Smith, 2009) also 
modeled sentence selection and compression 
jointly using ILP. Our ILP framework proposed 
here is based on these studies. Although various 
selection strategies have been proposed, there is 
no work to systematically compare these strate-
gies yet. 
Learning to rank attracts much attention in 
the information retrieval community recently. 
Pointwise, pairwise and listwise learning-to-
rank approaches have been extensively studied 
(Liu, 2009). Some of those have been applied to 
document summarization, such as SVR 
(Ouyang et al, 2007), classification SVM 
(Wang et al, 2007), and RankNet (Svore et al, 
2007). Again, there is no work to systematically 
compare these ranking algorithms. To the best 
of our knowledge, this is the first time that a 
listwise learning-to-rank algorithm, ListNet 
(Cao et al, 2007), is adapted to document sum-
marization in this paper. Moreover, pairwise 
and listwise learning-to-rank algorithms have 
never been used to perform concept ranking for 
extractive summarization.  
526
3 Ranking Sentences or Concepts 
Given a query and a collection of relevant doc-
uments, an extractive summarization system is 
required to generate a summary consisting of a 
set of text units (usually sentences). The first 
problem we need to consider is to determine the 
importance of these sentences according to the 
input query. We approach this ranking problem 
in two ways: the first way is to score sentences 
directly using learning-to-rank algorithms, and 
thus the goal of summarization is to select a 
subset of sentences, considering both relevance 
and redundancy. The second way is to score 
concepts within the document collection, and 
then the summarization task is to select a sen-
tence subset that can cover those important con-
cepts maximally. The problem of sentence se-
lection will be described in Section 4.  
Suppose the relevant document collection for 
a query q is Dq. From this collection, we obtain 
a set of sentences or concepts (e.g., word bi-
grams), S={s1,s2,?,sn} or C={c1,c2,?, cn}. Be-
fore training, each si or ci is associated with a 
gold standard score, yi. A feature vector, xj= 
?(sj/cj,q,Dq), is constructed for each sentence or 
concept. The learning algorithm will learn a 
ranking function f(xj) from a collection of 
query-document pairs {(qi,Dqi)|i= 1, 2,?,m}.  
We investigated three learning-to-rank me-
thods to learn f(xj). The first one is a pointwise 
ranking algorithm, support vector regression 
(SVR). This algorithm treats sentences (or con-
cepts) independently. The second method is a 
pairwise ranking algorithm, RankNet, which 
learns a ranking function from a list of sentence 
(or concept) pairs. Each pair is labeled as 1 if 
the first sentence si (or concept ci) ranks ahead 
of the second sj (or cj), and 0 otherwise. 
The listwise ranking algorithm, ListNet, 
learns the ranking function f(xj) in a different 
way. A list of sentences (or concepts) is treated 
as a whole. Both RankNet and ListNet take into 
account the dependency between sentences (or 
concepts). 
3.1  Support Vector Regression  
Support Vector Regression (SVR), a generaliza-
tion of the classical SVM formulation, attempts 
to learn a regression model. SVR has been ap-
plied to summarization in (Ouyang et al, 2007; 
Metzler and Kanungo, 2008). In our work, we 
train the SVR model to fit the gold standard 
score of each sentence or concept.  
Formally, the objective of SVR is to minim-
ize the following objective: 
2
, ,
1 1
|| || ( ( ))
2
i
i i
x
w b
w C v L y f x
N
?
?
?( ) =
? ?? ?? ?
+ ? + ?? ?? ?? ?? ?? ??
(1) 
where L(x)=|x|-? if x > ? and otherwise L(x)=0; 
yi is the gold standard score of xi; f(x) =wTx+b, 
the predicted score of x; C and v are two para-
meters; and N is the total number of training 
examples.  
3.2 RankNet  
RankNet is a pairwise learning-to-rank method 
(Burges et al, 2005). In this algorithm, training 
examples are handled pair by pair. Given a pair 
of feature vectors (xi, xj), the gold standard 
probability ijP is set to be 1 if the label of the 
pair is 1, which means xi ranks ahead of xj. The 
gold standard probability is 0 if the label of the 
pair is 0. Then the predicted probability Pij, 
which defines the probability of xi ranking 
ahead of xj by the model, is represented as a lo-
gistic function:  
exp( ( ) ( ))
1 exp( ( ) ( ))
i j
ij
i j
f x f x
P
f x f x
?
=
+ ?
            (2) 
where f(x) is the ranking function. The objective 
of the algorithm is to minimize the cross entro-
py between the gold standard probability and 
the predicted probability, which is defined as 
follows: 
( ) log (1 ) log(1 )ij ij ij ij ijC f P P P P= ? ? ? ?     (3) 
A three-layer neural network is used as the 
ranking function, as follows:  
3 32 2 21 2 3( ) ( ( ) )n ij jk nk j i
j k
f x g w g w x b b= + +? ?
 
 (4) 
where for weights w and bias b, the superscripts 
indicate the node layer while the subscripts in-
dicate the node indexes within each layer. And 
xnk is the k-th component of input feature vector 
xn. Then a gradient descent method is used to 
learn the parameters. For details, refer to 
(Burges et al, 2005). 
3.3 ListNet 
ListNet takes a list of items as input in the learn-
ing process. More specifically, suppose we have 
527
a list of feature vectors (x1, x2,?, xn) and each 
feature vector xi has an gold standard score yi, 
which has been assigned before training. Ac-
cordingly, we have a list of gold standard scores 
(y1, y2,?,yn). We also have a list of scores as-
signed by the algorithm during training, say, 
(f(x1), f(x2),?, f(xn)). Given a score list 
S={s1,s2,?,sn}, the probability that xj will rank 
the first place among the n items is defined as 
follows: 
1 1
( ) exp( )
( )
( ) exp( )
j j
s n n
k kk k
s s
P j
s s
= =
?
= =
?? ?
        (5) 
It is easy to prove that (Ps(1), Ps(2), ?, Ps(n)) is 
a probability distribution, as the sum of them 
equals to 1. Therefore, the cross entropy can be 
used to define the loss between the gold stan-
dard distribution Py(j) and the distribution Pf(j), 
as follows:  
1
( , ) ( ) log ( )
n
y f
j
L y f P j P j
=
= ??               (6) 
where y represents the gold standard score list  
(y1, y2,?,yn) and f=(f(x1), f(x2),?, f(xn)) is the 
score list output by the ranking algorithm.  
The function f is defined as a linear function, 
as follows: 
( ) Tw i if x w x=                          (7) 
Then the gradient of loss function L(y,f) with 
respect to the parameter vector w can be calcu-
lated as follows:  
1
1
1
( )( , )
( )
( )1
exp( ( ))
exp( ( ))
n
w jw
y j
j
n
w j
w jn
jw jj
f xL y f
w P x
w w
f x
f x
wf x
=
=
=
??? = = ?
? ?
?
+
?
?
??
 (8) 
 
During training, w is updated in a gradient des-
cent manner: w=w -??w and ? is the learning 
rate. For details, refer to (Cao et al, 2007). 
4 ILP-based Selection Framework 
After we have a way of ranking sentences or 
concepts, we face a sentence selection problem: 
selecting an optimal subset of sentences as the 
final summary. To integrate sentence/concept 
ranking, we adopted an integer linear program-
ming (ILP) framework to find the optimal sen-
tence subset (Filatova and Hatzivassiloglou, 
2004; McDonald, 2007; Gillick and Favre, 2009; 
Takamura and Okumura, 2009). ILP is a global 
optimization problem whose objective and con-
straints are linear in a set of integer variables.  
Formally, we define the problem of sentence 
selection as follows: 
maximize:  ( )*  xi i
i
f x z
? ?? ?? ??          (9) 
. .     * | |       uj j
j
z u Lims t ??  
       * ( , ) ,          u xj i
j
z I i j z i? ??  
      ( )* ( , )   ,    x xi j i jz z sim x x i j?+ < ?  
        , {0,1},      ,x ui jz z i j? ?  
where: 
xi ? the representation unit, such as a sentence 
or a concept. We term it representation unit be-
cause the summary quality is represented by the 
set of included xi; 
f(xi) - the ranking function given by the learn-
ing-to-rank algorithms; 
uj - the selection unit, for instance, a sentence in 
this paper. |uj| is the number of words in uj; 
x
iz - the indicator variable which denotes the 
presence or absence of xi in the summary; 
u
jz - the indicator variable which denotes inclu-
sion or exclusion of uj; 
I(i, j) - a  binary constant indicating that wheth-
er xi appears in uj. It is either 1 or 0; 
Lim - the length limit; 
sim(xi, xj) - a similarity measure for considering 
the redundancy; 
? - the redundancy threshold.  
The first constraint indicates the length limit. 
The second constraint asserts that if a represen-
tation unit xi is included in a summary, at least 
one selection unit that contains xi must be se-
lected. The third constraint considers redundan-
cy. If the representation unit is sentence, the 
similarity measure is defined as tf*idf similarity, 
and ?/2 is the similarity threshold, which was 
set to be 1 here. For concepts, the similarity 
measure can be defined as  
1,    
( , )
0,    otherwise
i j
i j
x x
sim x x
=?
= ?? .
 
However, other definition is also feasible, de-
pending on what has been selected as represen-
tation unit. 
528
Note that this framework is very general. If 
the representation unit xi is a sentence, the rank-
ing function is defined on sentence. Thus the 
ILP framework will find a set of sentences that 
can optimize the total scores of selected sen-
tences, subject to several constraints. If the re-
presentation unit is a concept, the ranking func-
tion measures the importance of a concept to be 
included in a summary. Thus the goal of ILP is 
to find a set of sentences by maximizing the 
scores of concepts covered by those selected 
sentences. 
 
Dq relevant document collection in response 
to query q 
d one single document 
wi unigram 
wiwi+1 bigram 
S sentence 
tfd(wi) the frequency of wi occurring in d 
dfD(wi) the number of documents containing wi 
in collection D 
Table 1. Notations for features. 
5 Features 
To facilitate the following description, some 
notations are defined in Table 1. In our dataset, 
each query has a title and narrative to precisely 
define an information need. The following is a 
query example from the TAC 2008 test dataset:  
<topic id = "D0801A">  
 <title> Airbus A380 </title> 
 <narrative> 
Describe developments in the production and 
launch of the Airbus A380. 
 </narrative> 
</topic> 
Features for sentence ranking and concept 
ranking are listed in the following. We use word 
bigrams as concept here. 
Sentence Features 
(1) Cluster frequency: ( )
qi
D iw S
tf w
??  
(2) Title frequency: ( )
i
d iw S
tf w
??  where d is a 
new document that consists of all the titles of 
documents in Dq.  
(3) Query frequency: ( )
i
d iw S
tf w
??  where d is 
a document consisting of the title and narrative 
fields of the current topic.  
(4) Theme frequency: ( )
qi i
D iw S w T
tf w
? ? ??  
where T is the top 10% frequent unigram words 
in Dq. 
(5) Document frequency of bigrams in the sen-
tence: 
1
1( )
i i
D i iw w S
df w w
+
+?? .  
(6) PageRank score: as described in (Mihalcea 
and Tarau, 2004), each sentence in Dq is a node 
in the graph and the cosine similarity between a 
pair of sentences is used as edge weight. 
Concept Features 
(1) Cluster frequency: 1( )qD i itf w w + , the fre-
quency of 1i iw w + occurring in Dq.  
(2) Title frequency: 1( )d i itf w w + , where d is a 
document consisting of all the titles of docu-
ments in Dq. 
(3) Query Frequency: the frequency of the bi-
gram occurring in the topic title and narrative. 
(4) Average term frequency: 
 1( )/ | |
q
d i i qd D
tf w w D+?? . |Dq| is the number of 
documents in the set. 
(5) Document frequency: the document fre-
quency of this bigram. 
(6) Minimal position: the minimal position of 
this bigram relative to the document length.  
(7) Average position: the average position of 
this bigram in collection Dq . 
6 Experimental Results 
6.1 Data Preprocessing 
We conducted experiments on the TAC 2008 
and TAC 2009 datasets. The task requires pro-
ducing a 100-word summary for each query (al-
so termed topic sometimes). There are 48 que-
ries in TAC 2008 and 44 queries in TAC 2009. 
A query example has been given in Section 5. 
Relevant documents for these queries have been 
specified. And four human-written summaries 
were supplied as reference summaries for each 
query. 
We segmented the relevant documents into 
sentences using the LingPipe toolkit 1  and 
stemmed words using the Porter Stemmer. 
Word bigrams are used as concepts in this paper. 
If the two words in a bigram are both stop-
words, the bigram will be discarded. The sen-
                                                 
1 http://alias-i.com/lingpipe/index.html 
529
tence features and bigram features are then cal-
culated. As our focus is on comparing different 
ranking strategies and selection strategies, we 
did not apply any sophisticated linguistic or se-
mantic processing techniques (as pre- or post-
processing). Thus we did not compare our re-
sults to those submitted to the TAC conferences.  
We train the learning algorithms on one data-
set and then evaluate the algorithms on the other. 
The generated summaries are evaluated using 
the ROUGE toolkit (Lin and Hovy, 2003).  
6.2 Preparing Training Samples 
As our work includes both sentence ranking and 
concept ranking, we need to establish two types 
of training data. Fortunately, we are able to do 
this based on the reference summaries and an-
notation results provided by the TAC confe-
rences.  
For the sentence ranking problem, we com-
pute the average ROUGE-1 score for each sen-
tence by comparing it to the four reference 
summaries for each query. This score is treated 
as the gold-standard score. In ListNet, these 
scores are directly used (see formula (5)). While 
in RankNet, the sentences for a query are 
grouped into 10 bins according to their 
ROUGE-1 scores, and then we extract sentences 
from different bins respectively to form a pair. 
We assume that a sentence in a higher scored 
bin should rank ahead of those sentences in 
lower scored bins.  
As for the concept ranking problem, gold-
standard scores are obtained from the human 
annotated Pyramid data. The weight of each 
semantic content unit (SCU) is the number of 
reference summaries in which the SCU appears. 
So straightforwardly, the gold-standard score of 
a bigram is the largest weight of all SCUs that 
contain the bigram. And if a bigram does not 
occur in any SCU, its score will be 0. Thus the 
bigram scores belong to the set {0,1,2,3,4} as 
there are four human-written summaries for 
each query. These scores are directly used in 
ListNet (see formula (5)). And in RankNet, bi-
gram pairs are constructed according to the 
gold-standard scores.  
6.3 Learning Parameters 
For SVR, the radial basis kernel function is em-
ployed and the optimal values for parameters C, 
v and g (for the kernel) are found using the gri-
dregression.py tool provided by LibSVM 
(Chang and Lin, 2001) with a 5-fold cross vali-
dation on the training set.  
RankNet applies a three-layer (one hidden 
layer) neural network with only one node in the 
output layer, as described in (Burges et al, 
2005). The number of hidden neurons was em-
pirically set to be 10. The learning rate was set 
to 0.001 for sentence ranking and 0.01 for bi-
gram ranking.  
As for ListNet, the learning rate for sentence 
ranking and concept ranking are both set to be 
0.1 empirically.  
6.4 Comparing Ranking Strategies 
In this section, we compared different ranking 
strategies for both sentence ranking and concept 
ranking. The sentence selection strategies were 
fixed to the ILP selection framework as shown 
in Section 4. We chose ILP as the selection 
strategy because we want to compare our sys-
tem with the following two methods (as base-
lines): 
(1) SENT_ILP: A sentence-level method pro-
posed by McDonald (2007) with ILP formula-
tion. We implemented the query-focused ver-
sion of the formulae as TAC 2008 and 2009 
required query-focused summarization. 
(2) DF_ILP: A concept-level ILP method using 
document frequency to score word bigrams 
(Gillick and Favre, 2009), without any learning 
process.  
The differences between our framework and 
SENT_ILP are: a) SENT_ILP used a redundan-
cy factor in the objective function whereas we 
modeled redundancy as constraints; b) 
SENT_ILP used tf*idf similarity to compute 
relevance scores whereas we used learning algo-
rithms.  
The ROUGE-1 and ROUGE-2 measures for 
each method are presented in Table 2 and Table 
3. Note that the performance on the TAC 2008 
dataset was obtained from the models that were 
trained on the TAC 2009 dataset. Then, the da-
tasets were interchanged for training and testing, 
respectively. Different learning-to-rank strate-
gies (SVR, RankNet, ListNet) do not show sig-
nificant differences between one and another, 
but they all outperform SENT_ILP substantially 
(p-value < 0.0001). And for concept ranking, 
RankNet and ListNet both achieve significantly 
better ROUGE-2 results (p-value < 0.005) than 
530
DF_ILP. This infers that considering more fea-
tures will have better results than using docu-
ment frequency to score concepts. The Wilcox-
on signed-rank test (Wilcoxon, 1945) is used for 
significance tests in our experiment. A good 
ranking strategy for modeling relevance is im-
portant for extractive summarization. RankNet 
which used a three-layer network (non-linear 
function) as the ranking function performs 
slightly better than ListNet which is based on a 
linear ranking function.  
 
Dataset Method ROUGE-1 ROUGE-2
TAC 
2008 
SVR 0.35086 0.08447 
RankNet 0.36025 0.09291 
ListNet 0.35365 0.09129 
SENT_ILP 0.31546 0.06500 
TAC 
2009 
SVR 0.36125 0.09659 
RankNet 0.36216 0.09778 
ListNet 0.35480 0.09126 
SENT_ILP 0.31962 0.07034 
Table 2. Results of sentence ranking strategies. 
 
Dataset Method ROUGE-1 ROUGE-2
TAC 2008 
SVR 0.36555 0.10291 
RankNet 0.37564 0.11213 
ListNet 0.36863 0.10660 
DF_ILP 0.36922 0.10373 
TAC 2009 
SVR 0.37126 0.10698 
RankNet 0.37513 0.11364 
ListNet 0.37499 0.11313 
DF_ILP 0.36347 0.10156 
Table 3. Results of concept ranking strategies. 
 
It is worth noting that Pyramid annotations 
may not cover all important bigrams, partly be-
cause SCUs in reference summaries have been 
rephrased by human annotators. Note that we 
simply extract original sentences to form a 
summary, thus it is possible that a bigram which 
is important in the original sentences does not 
appear in any rephrased SCUs at all. Such bi-
grams will have a gold-standard score of 0, 
which is erroneous supervision. For example, 
the bigrams hurricane katrina in topic D0804A 
about Katrina pet rescue and life support in 
D0806A about Terri Schiavo case are not anno-
tated in any SCUs, but these bigrams are both 
key terms for the topics.  
6.5 Comparing Selection Strategies 
In order to study the influence of different selec-
tion strategies, we compare the ILP selection 
strategy (as introduced in Section 4) with other 
popular selection strategies, based on the same 
sentence ranking algorithm (we chose sentence-
level RankNet). The baselines to be compared 
are as follows:  
(1) MMR: As shown in (Carbonell and 
Goldstein, 1998), the formula of MMR is: 
{ }1 2arg max ( , ) (1 ) max ( , )
i j
i i js R S s S
MMR D q s D s s? ?
? ? ?
= ? ?
 
where q is the given query; R is the set of all 
sentences; S is the set of already included sen-
tences; D1 is the normalized ranking score f(xi) 
of si, and D2 is the cosine similarity of the fea-
ture vectors for si  and sj. Our implementation 
was similar to the MMR strategy in the 
MEAD2summarizer. 
(2) DiP: Diversity penalty which penalizes the 
score of candidate sentences according to the 
already selected ones (Wan, 2007). 
 
Dataset Method ROUGE-1 ROUGE-2
TAC 2008
ILP 0.36025 0.09291 
MMR 0.35459 0.09086 
DiP 0.35263 0.08689 
TAC 2009
ILP 0.36216 0.09778 
MMR 0.35148 0.08881 
DiP 0.34714 0.08672 
Table 4. Comparing selection strategies. 
 
The corresponding ROUGE scores are pre-
sented in Table 4. ILP outperforms other selec-
tion strategies significantly on the TAC 2009 
dataset (both ILP vs. MMR and ILP vs. DiP). 
Although improvements are observed with ILP 
on the TAC 2008 dataset, the difference is not 
significant (using ILP vs. using MMR). MMR is 
comparable to DiP as they are both based on 
greedy search in nature.  
To investigate the difference between these 
strategies, we present in-depth analysis here. 
First, the average length of summaries generat-
ed by ILP is 97.1, while that by MMR and DiP 
are 95.5 and 92.7, respectively. Note that the 
required summary length is 100 and that more 
words can potentially cover more information. 
Thus, ILP can generate summaries with more 
information. This is because ILP is a global op-
timization algorithm, subject to the length con-
straint. Second, the average rank of sentences 
selected by ILP is 12.6, while that by MMR and 
                                                 
2 http://www.summarization.com/mead/ 
531
DiP is about 5, which is substantially different. 
ILP can search down the ranked list while the 
other two methods tend to only select the very 
top sentences. Third, there are 4.1 sentences on 
average in each ILP-generated summary, while 
the number for MMR and DiP generated sum-
maries are 2.7 and 2.5, respectively. Thus ILP 
tend to select shorter sentences than MMR and 
DiP. This may help reduce redundancy as long-
er sentences may contain more topic irrelevant 
clauses or phrases.  
6.6 Discussions 
Interestingly, although the learning-to-rank al-
gorithms combined with the ILP selection strat-
egy perform well in summarization, the perfor-
mance is still far from that of manual summari-
zation. In this study, we investigate the upper 
bound performance. We used the presented ILP 
framework to generate summaries based on the 
gold-standard scores, rather than the scores giv-
en by the learning algorithms. In other words, 
f(xi) in formula (9) is replaced by the gold-
standard scores. The ROUGE results are shown 
in Table 5. We also listed the best/worst/average 
ROUGE scores of human summaries in TAC by 
comparing one human summary (as generated 
summary) to the other three human summaries 
(as reference summaries). These results are sub-
stantially better than those by the learning algo-
rithms. Sentence- and concept- level ranking 
produces very close results to best human sum-
maries. Some ROUGE-2 scores are even higher 
than those of human summaries. This is reason-
able as human annotators may have difficulty in 
organizing content when there are many docu-
ments and sentences. The results reflect that 
there is a remarkable gap between the gold-
standard scores and the learned scores.  
 
Dataset Method ROUGE-1 ROUGE-2
TAC 
2008 
Sentence-level 0.44216 0.14842 
Concept-level 0.42222 0.16018 
Human Best 0.44220 0.13079 
Human Average 0.41417 0.11606 
Human Worst 0.38005 0.10736 
TAC 
2009 
Sentence-level 0.45500 0.15565 
Concept-level 0.43526 0.17118 
Human Best 0.45663 0.14864 
Human Average 0.44443 0.12680 
Human Worst 0.39652 0.11109 
Table 5. Upper bound performance. 
7 Conclusion and Future Work 
We presented systematic and extensive analysis 
on studying two key problems in extractive 
summarization: the ranking problem and the 
selection problem. We compared three genres of 
learning-to-rank algorithms for the ranking 
problem, and investigated ILP, MMR, and Di-
versity Penalty strategies for the selection prob-
lem. To the best of our knowledge, this is the 
first work of presenting systematic comparison 
and analysis on studying these problems. We 
also at the first time proposed to use learning-to-
rank algorithms to perform concept ranking for 
extractive summarization.  
Our future work will focus on: (1) exploiting 
more features that can reflect summary quality; 
(2) optimizing summarization evaluation me-
trics directly with new learning algorithms. 
Acknowledgments 
This work was partly supported by the Chinese 
Natural Science Foundation under grant No. 
60973104 and No. 60803075, and with the aid 
of a grant from the International Development 
Research Center, Ottawa, Canada IRCI project 
from the International Development.  
References 
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, 
Matt Deeds, Nicole Hamilton and Greg Hullender. 
2005. Learning to Rank Using Gradient Descent. 
In Proceedings of the 22nd International Confe-
rence on Machine Learning.  
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai and 
Hang Li. 2007. Learning to Rank: from Pairwise 
Approach to Listwise Approach. In Proceedings 
of ICML 2007.  
Jaime Carbonell and Jade Goldstein. 1998. The Use 
of MMR, Diversity-Based Reranking for Reorder-
ing Documents and Producing Summaries. In 
Proceedings of SIGIR, August 1998, pp. 335 - 336.  
Chih-Chung Chang and Chih-Jen Lin. 2001. 
LIBSVM: a Library for Support Vector Machines. 
Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm 
H. P. Edmundson. 1969. New Methods in Automatic 
Extracting. Journal of the ACM (JACM) Archive,  
Volume 16, Issue 2 (April 1969) Pages: 264 - 285.  
G. Erkan and Dragomir R. Radev. 2004. LexPage-
Rank: Prestige in Multi-Document Text Summa-
532
rization. In Proceedings of EMNLP 2004, Barce-
lona, Spain.  
Elena Filatova and Vasileios Hatzivassiloglou. 2004. 
Event-based Extractive Summarization. In Pro-
ceedings of ACL Workshop on Summarization, 
volume 111. 
Dan Gillick and Benoit Favre. 2009. A Scalable 
Global Model for Summarization. In Proceedings 
of the Workshop on Integer Linear Programming 
for Natural Language Processing.  
Eduard Hovy, Chin-yew Lin, Liang Zhou and Juni-
chi Fukumoto. 2006. Automated Summarization 
Evaluation with Basic Elements. In Proceedings 
of the Fifth Conference on Language Resources 
and Evaluation.  
Julian Kupiec, Jan Pedersen and Francine Chen. 
1995. A Trainable Document Summarizer. In 
Proceedings of SIGIR'95, pages 68 - 73, New 
York, USA.  
Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha 
and Yong Yu. 2009. Enhancing Diversity, Cover-
age and Balance for Summarization through 
Structure Learning. In Proceedings of the 18th In-
ternational Conference on World Wide Web.  
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
Occurrence Statistics. In Proceedings of HLT-
NAACL, pages 71-78. 
Tie-Yan Liu. 2009. Learning to Rank for Informa-
tion Retrieval, Foundation and Trends on Infor-
mation Retrieval.  Now Publishers.  
H.P. Luhn. 1958. The Automatic Creation of Litera-
ture Abstracts. In IBM Journal of Research and 
Development, Vol. 2, No. 2, pp. 159-165, April 
1958. 
Andr? F. T. Martins and Noah A. Smith. 2009. 
Summarization with a Joint Model for Sentence 
Extraction and Compression. In Proceedings of 
the Workshop on Integer Linear Programming for 
Natural Langauge Processing\. 
Ryan McDonald. 2007. A Study of Global Inference 
Algorithms in Multi-Document Summarization. In 
Proceedings of the 29th ECIR.  
Kathleen McKeown and Dragomir R. Radev. 1995. 
Generating Summaries of Multiple News Articles. 
In Proceedings of SIGIR'95, pages 74?82.  
Donald Metzler and Tapas Kanungo. 2008. Machine 
Learned Sentence Selection Strategies for Query-
Biased Summarization. SIGIR Learning to Rank 
Workshop.  
Rada Mihalcea and Paul Tarau. 2004. TextRank: 
Bringing Order into Texts. In Proceedings of 
EMNLP 2004, Barcelona, Spain, July 2004.  
Ani Nenkova, Lucy Vanderwende and Kathleen 
McKeown. 2006. A Compositional Context Sensi-
tive Multi-document Summarizer: Exploring the 
Factors that Influence Summarization. In Pro-
ceedings of SIGIR 2006.  
You Ouyang, Sujian Li, Wenjie Li. 2007. Develop-
ing Learning Strategies for Topic-based Summa-
rization. In Proceedings of the sixteenth ACM 
Conference on Information and Knowledge Man-
agement, 2007. 
Rebecca J. Passonneau, Ani Nenkova, Kathleen 
McKeown and Sergey Sigelman. 2005. Applying 
the Pyramid Method in DUC 2005. DUC 2005 
Workshop.  
Dragomir R. Radev, Hongyan Jing, Malgorzata Stys, 
and Daniel Tam. 2004. Centroid-based Summari-
zation of Multiple Documents. Information 
Processing and Management, 40:919?938.  
Christina Sauper and Regina Barzilay. 2009. Auto-
matically Generating Wikipedia Articles: A Struc-
ture-Aware Approach. In Proceedings of ACL 
2009.  
Dou Shen, Jian-Tao Sun, Hua Li, QiangYang and 
Zheng Chen. 2007. Document Summarization Us-
ing Conditional Random Fields. In IJCAI, pages 
2862 - 2867, 2007.  
Krysta Svore, Lucy Vanderwende, and Chris Burges. 
2007. Enhancing Single-Document Summariza-
tion by Combining RankNet and Third-Party 
Sources. In Proceedings of EMNLP-CoNLL 
(2007), pp. 448-457.. 
Hiroya Takamura and Manabu Okumura. Text 
Summarization Model Based on Maximum Cov-
erage Problem and its Variant. In Proceedings 
EACL, 2009.  
Xiaojun Wan and Jianguo Xiao. 2007. Towards a 
Unified Approach Based on Affinity Graph to 
Various Multi-document Summarizations. ECDL 
2007, 297-308.  
Changhu Wang, Feng Jing, Lei Zhang and Hong-
Jiang Zhang. 2007. Learning Query-Biased Web 
Page Summarization. In Proceedings of the six-
teenth ACM Conference on Information and 
Knowledge Management.  
Frank Wilcoxon. 1945. Individual comparisons by 
ranking methods. Biometrics, 1, 80-83. 
533
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1614?1623,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Clustering Aspect-related Phrases by Leveraging Sentiment Distribution
Consistency
Li Zhao, Minlie Huang, Haiqiang Chen*, Junjun Cheng*, Xiaoyan Zhu
State Key Laboratory of Intelligent Technology and Systems
National Laboratory for Information Science and Technology
Dept. of Computer Science and Technology, Tsinghua University, Beijing, PR China
*China Information Technology Security Evaluation Center
zhaoli19881113@126.com aihuang@tsinghua.edu.cn
Abstract
Clustering aspect-related phrases in terms
of product?s property is a precursor pro-
cess to aspect-level sentiment analysis
which is a central task in sentiment analy-
sis. Most of existing methods for address-
ing this problem are context-based models
which assume that domain synonymous
phrases share similar co-occurrence con-
texts. In this paper, we explore a novel
idea, sentiment distribution consistency,
which states that different phrases (e.g.
?price?, ?money?, ?worth?, and ?cost?) of
the same aspect tend to have consistent
sentiment distribution. Through formal-
izing sentiment distribution consistency as
soft constraint, we propose a novel unsu-
pervised model in the framework of Poste-
rior Regularization (PR) to cluster aspect-
related phrases. Experiments demonstrate
that our approach outperforms baselines
remarkably.
1 Introduction
Aspect-level sentiment analysis has become a cen-
tral task in sentiment analysis because it can ag-
gregate various opinions according to a product?s
properties, and provide much detailed, complete,
and in-depth summaries of a large number of re-
views. Aspect finding and clustering, a precursor
process of aspect-level sentiment analysis, has at-
tracted more and more attentions (Mukherjee and
Liu, 2012; Chen et al., 2013; Zhai et al., 2011a;
Zhai et al., 2010).
Aspect finding and clustering has never been a
trivial task. People often use different words or
phrases to refer to the same product property (also
called product aspect or feature in the literature).
Some terms are lexically dissimilar while seman-
tically close, which makes the task more challeng-
ing. For example, ?price?, ?money? , ?worth? and
?cost? all refer to the aspect ?price? in reviews.
In order to present aspect-specific summaries of
opinions, we first of all, have to cluster different
aspect-related phrases. It is expensive and time-
consuming to manually group hundreds of aspect-
related phrases. In this paper, we assume that the
aspect phrases have been extracted in advance and
we keep focused on clustering domain synony-
mous aspect-related phrases.
Existing studies addressing this problem are
mainly based on the assumption that different
phrases of the same aspect should have similar co-
occurrence contexts. In addition to the traditional
assumption, we develop a new angle to address the
problem, which is based on sentiment distribution
consistency assumption that different phrases of
the same aspect should have consistent sentiment
distribution, which will be detailed soon later.
Figure 1: A semi-structured Review.
This new angle is inspired by this simple obser-
vation (as illustrated in Fig. 1): two phrases within
the same cluster are not likely to be simultaneously
placed in Pros and Cons of the same review. A
straightforward way to use this information is to
formulate cannot-link knowledge in clustering al-
gorithms (Chen et al., 2013; Zhai et al., 2011b).
However, we have a particularly different manner
to leverage the knowledge.
Due to the availability of large-scale semi-
structured customer reviews (as exemplified in
Fig. 1) that are supported by many web sites,
we can easily get the estimation of sentiment dis-
tribution for each aspect phrase by simply count-
ing how many times a phrase appears in Pros and
1614
Cons respectively. As illustrated in Fig. 2, we
can see that the estimated sentiment distribution
of a phrase is close to that of its aspect. The
above observation suggests the sentiment distri-
bution consistency assumption: different phrases
of the same aspect tend to have the same senti-
ment distribution, or to have statistically close
distributions. This assumption is also verified by
our data: for most (above 91.3%) phrase with rela-
tively reliable estimation (whose occurrence?50),
the KL-divergence between the sentiment distri-
bution of a phrase and that of its corresponding
aspect is less than 0.05.
Figure 2: The sentiment distribution of aspect
?battery? and its related-phrases on nokia 5130
with a large amount of reviews.
It is worth noting that, the sentiment distribution
of a phrase can be estimated accurately only when
we obtain a sufficient number of reviews. When
the number of reviews is limited, however, the es-
timated sentiment distribution for each phrase is
unreliable (as shown in Fig. 3). A key issue,
arisen here, is how to formulate this assumption in
a statistically robust manner. The proposed model
should be robust when only a limited number of
reviews are available.
Figure 3: The sentiment distribution of aspect
?battery? and its related-phrases on nokia 3110c
with a small mumber of reviews.
To deal with this issue, we model sentiment dis-
tribution consistency as soft constraint, integrated
into a probabilistic model that maximizes the data
likelihood. We design the constraint to work in
the following way: when we have sufficient ob-
servations, the constraint becomes tighter, which
plays a more important role in the learning pro-
cess; when we have limited observations, the con-
straint becomes very loose so that it will have less
effect on the model.
In this paper, we propose a novel unsupervised
model, Sentiment Distribution Consistency Reg-
ularized Multinomial Naive Bayes (SDC-MNB).
The context part is modeled by Multinomial Naive
Bayes in which aspect is treated as latent variable,
and Sentiment distribution consistency is encoded
as soft constraint within the framework of Poste-
rior Regularization (PR) (Graca et al., 2008). The
main contributions of this paper are summarized
as follows:
? We study the problem of clustering phrases
by integrating both context information
and sentiment distribution of aspect-related
phrases.
? We explore a novel concept, sentiment distri-
bution consistency(SDC), and model it as soft
constraint to guide the clustering process.
? Experiments show that our model outper-
forms the state-of-art approaches for aspect
clustering.
The rest of this paper is organized as follows.
We introduce the SDC-MNB model in Section 2.
We present experiment results in Section 3. In
Section 4, we survey related work. We summarize
the work in Section 5.
2 Sentiment Distribution Consistency
Regularized Multinomial Naive Bayes
In this section, we firstly introduce our assumption
sentiment distribution consistency formally and
show how to model the above assumption as soft
constraint , which we term SDC-constraint. Sec-
ondly, we show how to combine SDC-constraint
with the probabilistic context model. Finally, we
present the details for context and sentiment ex-
traction.
2.1 Sentiment Distribution Consistency
We define aspect as a set of phrases that refer to
the same property of a product and each phrase is
termed aspect-related phrase (or aspect phrase in
short). For example, the aspect ?battery? contains
aspect phrases such as ?battery?, ?battery life?,
?power?, and so on.
1615
F the aspect phrase set
f
j
the j
th
aspect phrase
y
j
the aspect for aspect phrase f
j
A the aspect set
a
i
the i
th
aspect
D the set of context documents
d
j
the context document of f
j
V the word vocabulary
w
t
the t
th
word in vocabulary V
w
d
j
,k
the k
th
word in d
j
N
tj
the number of times word w
t
occurs in d
j
P the product set
p
k
the k
th
product
u
ik
the sentiment distribution parameter
of aspect a
i
on p
k
s?
jk
the estimated sentiment distribution parameter
of phrase f
j
on p
k
n
jk
the occurrence times of aspect phrase f
j
on p
k
??
jk
the sample standard deviation
? the model parameters
p
?
(a
i
|d
j
) the posterior distribution of a
i
given d
j
q(y
j
= a
i
)
the projected posterior distribution
of a
i
given d
j
Table 1: Notations
Let us consider the sentiment distribution on a
certain aspect a
i
. In a large review dataset, as-
pect a
i
could receive many comments from differ-
ent reviewers. For each comment, we assume that
people either praise or complain about the aspect.
So each comment on the aspect can be seen as a
Bernoulli trial, where the aspect receives positive
comments with probability p
a
i
1
. We introduce a
random variable X
a
i
to denote the sentiment on
aspect a
i
, where X
a
i
= 1 means that aspect a
i
receives positive comments, X
a
i
= 0 means that
aspect a
i
receives negative comments. Obviously,
the sentiment on aspect a
i
follows the Bernoulli
distribution,
Pr(X
a
i
) = p
X
a
i
a
i
? (1 ? p
a
i
)
1?X
a
i
, X
a
i
? {0, 1}. (1)
Or in short,
X
a
i
? Bernoulli(p
a
i
)
Let us see the case for aspect phrase f
j
, where
f
j
? aspect a
i
. Similarly, each comment on an as-
pect phrase f
j
can also be seen as a Bernoulli trial.
We introduce a random variable X
f
j
to denote the
sentiment on aspect phrase f
j
, where X
f
j
= 1
means that aspect f
j
receives positive comments,
X
f
j
= 0 means that aspect f
j
receives negative
comments. As just discussed, we assume that each
aspect phrase follows the same distribution with
1
positive comment means that an aspect term is observed
in Pros of a review.
the corresponding aspect. This leads to the fol-
lowing formal description:
? Sentiment Distribution Consistency : The
sentiment distribution of aspect phrase is the
same as that of the corresponding aspect.
Formally, for all aspect phrase f
j
? aspect
a
i
, X
f
j
? Bernoulli(p
a
i
).
2.2 Sentiment Distribution Consistency
Constraint
Assuming the sentiment distribution of aspect a
i
is
given in advance, we need to judge whether an as-
pect phrase f
j
belongs to the aspect a
i
with limited
observations for f
j
. Let?s consider the example in
Fig. 4. For aspect phrase 3, we have no definite
answer due to the limited number of observations.
For aspect phrase 1, it seems that the sentiment
distribution is consistent with that of the left as-
pect. However, we can not say that the phrase be-
longs to the aspect because the distribution may
be the same for two different aspects. For aspect
phrase 2, we are confident that its sentiment dis-
tribution is different from that of the left aspect,
given sufficient observations.
Figure 4: Sentiment distribution of an aspect, and
observations on aspect phrases.
To be concise, we judge an aspect phrase
doesn?t belong to certain aspect only when we are
confident that they follow different sentiment dis-
tributions.
Inspired by the intuition, we conduct interval
parameter estimation for parameter p
f
j
(sentiment
distribution for phrase f
j
) with limited observa-
tions, and thus get a confidence interval for p
f
j
.
If p
a
i
(sentiment distribution for aspect a
i
) is not
in the confidence interval of p
f
j
, we then are con-
fident that they follow different distributions. In
other words, if aspect phrase f
j
? aspect a
i
, we
are confident that p
a
i
is in the confidence interval
of p
f
j
.
More formally, we use u
ik
to denote the senti-
ment distribution parameter of aspect a
i
on prod-
uct p
k
, and assume that u
ik
is given in advance.
1616
We want to know whether the sentiment distribu-
tion on aspect phrase f
j
is the same as that of as-
pect a
i
on product p
k
given a limited number of
observations (samples). It?s straightforward to cal-
culate the confidence interval for parameter s
jk
in
the Bernoulli distribution function. Let the sam-
ple mean of n
jk
samples be s?
jk
, and the sample
standard deviation be ??
jk
. Since the sample size
is small here, we use the Student-t distribution to
calculate the confidence interval. According to our
assumption, we are confident that u
ik
is in the con-
fidence interval if f
j
? a
i
.
s?
jk
?C
??
jk
?
n
jk
? u
ik
? s?
jk
+C
??
jk
?
n
jk
, ?f
j
? a
i
,?k. (2)
where we look for t-table to find C corresponding
to a certain confidence level(such as 95%) with the
freedom of n
jk
? 1. For simplicity, we represent
the above confidence interval by [s?
jk
? d
jk
, s?
jk
+
d
jk
], where d
jk
= C
??
jk
?
n
jk
.
We introduce an indicator variable z
ij
to repre-
sent whether the aspect phrase f
j
belongs to aspect
a
i
, as follows:
z
ji
=
{
1 ; if f
j
? a
i
0 ; otherwise
(3)
This leads to our SDC-constraint function.
? = z
ji
|u
ik
? s?
jk
| ? d
jk
,?i, j, k (4)
SDC-constraint are flexible for modeling Senti-
ment Distribution Consistency. The more obser-
vations we have, the smaller d
jk
is. For frequent
aspect phrase, the constraint can be very informa-
tive because it can filter unrelated aspects for as-
pect phrase f
j
. The less observations we have,
the larger d
jk
is. For rare aspect phrases, the con-
straint can be very loose, and will not have much
effect on the clustering process for aspect phrase
f
j
. In this way, the model can work very robustly.
SDC-constraints are data-driven constraints.
Usually we have many reviews about hundreds of
products in our dataset. For each aspect phrase,
there are |A| ? |P | constraints (the number of as-
pects times the number of product). With thou-
sands of constraints about which aspect it is not
likely to belong to, the model learns to which as-
pect a phrase f
j
should be assigned. Although
most constraints may be loose because of the lim-
ited observations, SDC-constraint can still play an
important role in the learning process.
2.3 Sentiment Distribution Consistency
Regularized Multinomial Naive Bayes
(SDC-MNB)
In this section, we present our probabilistic model
which employs both context information and sen-
timent distribution.
First of all, we extract a context document d
for each aspect phrase, which will be described in
Section 2.5. In other word, a phrase is represented
by its context document. Assuming that the doc-
uments in D are independent and identically dis-
tributed, the probability of generating D is then
given by:
p
?
(D) =
|D|
?
j=1
p
?
(d
j
) =
|D|
?
j=1
?
y
j
?A
p
?
(d
j
, y
j
) (5)
where y
j
is a latent variable indicating the aspect
label for aspect phrase f
j
, and ? is the model pa-
rameter.
In our problem, we are actually more inter-
ested in the posterior distribution over aspect,
i.e., p
?
(y
j
|d
j
). Once the learned parameter ? is
obtained, we can get our clustering result from
p
?
(y
j
|d
j
), by assigning aspect a
i
with the largest
posterior to phrase f
j
. We can also enforce SDC-
constraint in expectation(on posterior p
?
). We use
q(Y ) to denote the valid posterior distribution that
satisfy our SDC-constraint, and Q to denote the
valid posterior distribution space, as follows:
Q = {q(Y ) : E
q
[z
ji
|u
ik
? s?
jk
|] ? d
jk
, ?i, j, k}. (6)
Since posterior plays such an important role in
joining the context model and SDC-constraint, we
formulate our problem in the framework of Poste-
rior Regularization (PR). PR is an efficient frame-
work to inject constraints on the posteriors of la-
tent variables. Instead of restricting p
?
directly,
which might not be feasible, PR penalizes the dis-
tance of p
?
to the constraint set Q. The posterior-
regularized objective is termed as follows:
max
?
{log p
?
(D) ? min
q?Q
KL(q(Y )||p
?
(Y |D))} (7)
By trading off the data likelihood of the ob-
served context documents (as defined in the first
term), and the KL divergence of the posteriors
to the valid posterior subspace defined by SDC-
constraint (as defined in the second term), the ob-
jective encourages models with both desired pos-
terior distribution and data likelihood. In essence,
the model attempts to maximize data likelihood of
context subject (softly) to SDC-constraint.
1617
2.3.1 Multinomial Naive Bayes
In spirit to (Zhai et al., 2011a), we use Multino-
mial Naive Bayes (MNB) to model the context
document. Let w
d
j
,k
denotes the k
th
word in doc-
ument d
j
, where each word is from the vocabulary
V = {w
1
, w
2
, ..., w
|V |
}. For each aspect phrase
f
j
, the probability of its latent aspect being a
i
and
generating context document d
i
is
p
?
(d
j
, y
j
= a
i
) = p(a
i
)
|d
j
|
?
k=1
p(w
d
j
,k
|a
i
) (8)
where p(a
i
) and p(w
d
j
,k
|a
i
) are parameters of this
model. Each word w
d
j
,k
is conditionally indepen-
dent of all other words given the aspect a
i
.
Although MNB has been used in existing work
for aspect clustering, all of the studies used it in
a semi-supervised manner, with labeled data or
pseudo-labeled data. In contrast, MNB proposed
here is used in an unsupervised manner for aspect-
related phrases clustering.
2.3.2 SDC-constraint
As mentioned above, the constraint posterior setQ
is defined by
Q = {q(Y ) : q(y
j
= a
i
)|u
ik
? s?
jk
| ? d
jk
,?i, j, k}. (9)
We can see that Q denotes a set of linear con-
straints on the projected posterior distribution q.
Note that we do not directly observe u
ik
, the sen-
timent distribution of aspect a
i
on product p
k
. For
aspect phrase f
j
that belongs to aspect a
i
, we es-
timate u
ik
by counting all sentiment samples. We
use the posterior p
?
(a
i
|d
j
) to approximately rep-
resent how likely phrase f
j
belongs to aspect a
i
.
u
ik
=
1
?
|D|
j=1
n
jk
p
?
(a
i
|d
j
)
|D|
?
j=1
n
jk
p
?
(a
i
|d
j
)s?
jk
(10)
where p
?
(a
i
|d
j
) is short for p
?
(y
j
= a
i
|d
j
), the
probability that aspect phrase f
j
belongs to a
i
given the context document d
j
. We estimate u
ik
in
this way because observations for aspect are rela-
tively sufficient for a reliable estimation since ob-
servations for an aspect are aggregated from those
for all phrases belonging to that aspect.
2.4 The Optimization Algorithm
The optimization algorithm for the objective (see
Eq. 7) is an EM-like two-stage iterative algorithm.
In E-step, we first calculate the posterior distri-
bution p
?
(a
i
|d
j
), then project it onto the valid pos-
terior distribution space Q. Given the parameters
?, the posterior distribution can be calculated by
Eq. 11.
p
?
(a
i
|d
j
) =
p(a
i
)
?
|d
j
|
k=1
p(w
d
j
,k
|a
i
)
?
|A|
r=1
p(a
r
)
?
|d
j
|
k=1
p(w
d
j
,k
|a
r
)
(11)
We use the above posterior distribution to update
the sentiment parameter for each aspect by Eq. 10.
The projected posterior distribution q is calculated
by
q = argmin
q?Q
KL(q(Y )||p
?
(Y |D)) (12)
For each instance, there are |A| ? |P | constraints.
However, we can prune a large number of useless
constraints derived from limited observations. All
constraints with d
jk
> 1 can be pruned, due to
the fact that the parameter u
ik
, s?
jk
is within [0,1],
and the difference can not be larger than 1. This
optimization problem in Eq. 12 is easily solved via
the dual form by the projected gradient algorithm
(Boyd and Vandenberghe, 2004):
max
??0
(
?
|A|
?
i=1
|P |
?
k=1
?
ik
d
jk
?
log
|A|
?
i=1
p
?
(a
i
|d
j
)exp{?
|P |
?
k=1
?
ik
|u
ik
? s?
jk
|} ? ????
)
(13)
where ? controls the slack size for constraint. After
solving the above optimization problem and ob-
taining the optimal ?, we can calculate the pro-
jected posterior distribution q by
q(y
j
= a
i
) =
1
Z
p
?
(a
i
|d
j
)exp{?
|P |
?
k=1
?
ik
|u
ik
?s?
jk
|} (14)
where Z is the normalization factor. Note that sen-
timent distribution consistency is actually modeled
as instance-level constraint here, which makes it
very efficient to solve.
In M-step, the projected posteriors q(Y ) are
then used to compute sufficient statistics and up-
date the models parameters ?. Given the projected
posteriors q(Y ), the parameters can be updated by
Eq. 15,16.
p(a
i
) =
1 +
?
|D|
j=1
q(y
j
= a
i
)
|A| + |D|
(15)
p(w
t
|a
i
) =
1 +
?
|D|
j=1
N
ti
q(y
j
= a
i
)
|V | +
?
|V |
m=1
?
|D|
j=1
N
mj
q(y
j
= a
i
)
(16)
where N
tj
is the number of times that the word w
t
occurs in document d
j
.
The parameters are initialized randomly, and we
repeat E-step and M-step until convergence.
1618
2.5 Data Extraction
2.5.1 Context Extraction
In order to extract the context document d for each
aspect phrase, we follow the approach in Zhai et
al. (2011a). For each aspect phrase, we generate
its context document by aggregating the surround-
ing texts of the phrase in all reviews. The preced-
ing and following t words of a phrase are taken as
the context where we set t = 3 in this paper. Stop-
words and other aspect phrases are removed. For
example, the following review contains two aspect
phrases, ?screen? and ?picture?,
The LCD screen gives clear picture.
For ?screen?, the surrounding texts are {the,
LCD, gives, clear, picture}. We remove stop-
words ?the?, and the aspect term ?picture?, and
the resultant context of ?screen? in this review is
context(screen) ={LCD, screen, gives, clear}.
Similarly, the context of ?picture? in this review is
context(picture) ={gives, clear}.
By aggregating the contexts of all the reviews
that contain aspect phrase f
j
, we obtain the cor-
responding context document d
j
.
2.5.2 Sentiment Extraction
Since we use semi-structured reviews, we ob-
tain the estimated sentiment distribution by sim-
ply counting how many times each aspect phrase
appears in Pros and Cons reviews for each prod-
uct respectively. So for each aspect phrase f
j
, let
n
+
jk
denotes the times that f
j
appears in Pros of
all reviews for product p
k
, and let n
?
jk
denotes the
times that f
j
appears in Cons of all reviews for
product p
k
. So the total number of occurrence of a
phrase is n
jk
= n
+
jk
+ n
?
jk
. We have samples like
(1,1,1,0,0) where 1 means a phrase occurs in Pros
of a review, and 0 in Cons. Given a sequence of
such observations, the sample mean is easily com-
puted as s?
jk
=
n
+
jk
n
+
jk
+n
?
jk
. And the sample standard
deviation is ??
jk
=
?
(1?s?
jk
)
2
?n
+
jk
+(s?
jk
)
2
?n
?
jk
n
jk
?1
.
3 Experiments
3.1 Data Preparation
The details of our review corpus are given
in Table 2. This corpus contains semi-
structured customer reviews from four do-
mains: Camera, Cellphone, Laptop, and MP3.
These reviews were crawled from the following
web sites: www.amazon.cn, www.360buy.com,
www.newegg.com.cn, and www.zol.com. The as-
pect label of each aspect phrases is annotated by
human curators.
Camera Cellphone Laptop MP3
#Products 449 694 702 329
#Reviews 101,235 579,402 102,439 129,471
#Aspect Phrases 236 230 238 166
#Aspect 12 10 14 8
Table 2: Statistics of the review corpus. # denotes
the size.
3.2 Evaluation Measures
We adapt three measures Purity, Entropy, and
Rand Index for performance evaluation. These
measures have been commonly used to evaluate
clustering algorithms.
Given a data set DS, suppose its gold-standard
partition is G = {g
1
, ..., g
j
, ..., g
k
}, where k
is the number of clusters. A clustering algo-
rithm partitions DS into k disjoint subsets, say
DS
1
, DS
2
, ..., DS
k
.
Entropy: For each resulting cluster, we can mea-
sure its entropy using Eq. 17, where P
i
(g
j
) is the
proportion of data points of class g
j
in DS
i
. The
entropy of the entire clustering result is calculated
by Eq. 18.
entropy(DS
i
) = ?
k
?
j=1
P
i
(g
j
)log
2
P
i
(g
j
) (17)
entropy(DS) =
k
?
i=1
|DS
i
|
|DS|
entropy(DS
i
) (18)
Purity: Purity measures the extent that a cluster
contains only data from one gold-standard parti-
tion. The cluster purity is computed with Eq. 19.
The total purity of the whole clustering result (all
clusters) is computed with Eq. 20.
purity(DS
i
) = max
j
P
i
(g
j
) (19)
purity(DS) =
k
?
i=1
|DS
i
|
|DS|
purity(DS
i
) (20)
RI: The Rand Index(RI) penalizes both false posi-
tive and false negative decisions during clustering.
Let TP (True Positive) denotes the number of pairs
of elements that are in the same set in DS and in
the same set in G. TN (True Negative) denotes
number of pairs of elements that are in different
sets in DS and in different sets in G. FP (False
1619
Camera Cellphone Laptop MP3
P RI E P RI E P RI E P RI E
Kmeans 43.48% 83.52% 2.098 48.91% 84.80% 1.792 43.46% 87.11% 2.211 40.00% 70.98% 2.047
L-EM 54.89% 87.07% 1.690 51.96% 86.64% 1.456 48.94% 84.53% 2.039 44.24% 75.37% 1.990
LDA 36.84% 83.28% 2.426 48.65% 85.33% 1.833 35.02% 83.53% 2.660 36.12% 76.08% 2.296
Constraint-LDA 43.30% 86.01% 2.216 47.89% 86.04% 1.974 32.35% 84.86% 2.676 50.70% 81.42% 1.924
SDC-MNB 56.42% 88.16% 1.725 67.95% 90.62% 1.266 55.52% 90.72% 1.780 58.06% 83.57% 1.578
Table 3: Comparison to unsupervised baselines. (P is short for purity, E for entropy, and RI for random
index.)
Positive) denotes number of pairs of elements in
S that are in the same set in DS and in different
sets in G. FN (False Negative) denotes number of
pairs of elements that are in different sets in DS
and in the same set in G. The Rand Index(RI) is
computed with Eq. 21.
RI(DS) =
TP + TN
TP + TN + FP + FN
(21)
3.3 Evaluation Results
3.3.1 Comparison to unsupervised baselines
We compared our approach with several existing
unsupervised methods. Some of the methods aug-
mented unsupervised models by incorporating lex-
ical similarity and other domain knowledge. All
of them are context-based models.
2
We list these
models as follows.
? Kmeans: Kmeans is the most popular cluster-
ing algorithm. Here we use the context distri-
butional similarity (cosine similarity) as the
similarity measure.
? L-EM: This is a state-of-the-art unsupervised
method for clustering aspect phrases (Zhai et
al., 2011a). L-EM employed lexical knowl-
edge to provide a better initialization for EM.
? LDA: LDA is a popular topic model(Blei et
al., 2003). Given a set of documents, it out-
puts groups of terms of different topics. In
our case, each aspect phrase is processed as a
term.
3
Each sentence in a review is consid-
ered as a document. Each aspect is consid-
ered as a topic. In LDA, a term may belong
to more than one topic/group, but we take the
topic/group with the maximum probability.
2
In our method, we collect context document for each
aspect phrase. This process is conducted for L-EM and K-
means. But for LDA and Constraint-LDA, we take each sen-
tence of reviews as a document. This setting for the LDA
baselines is adapted from previous work.
3
Each aspect phrase is pre-processed as a single word
(e.g., ?battery life? is treated as battery-life). Other words
are normally used in LDA.
? Constraint-LDA: Constraint-LDA (Zhai et
al., 2011b) is a state-of-the-art LDA-based
method that incorporates must-link and
cannot-link constraints for this task. We set
the damping factor ? = 0.3 and relaxation
factor ? = 0.9, as suggested in the original
reference.
For all methods that depend on the random ini-
tiation, we use the average results of 10 runs as the
final result. For all LDA-based models, we choose
? = 50/T , ? = 0.1, and run 1000 iterations.
Experiment results are shown in Table 3. We
can see that our approach almost outperforms all
unsupervised baseline methods by a large margin
on all domains. In addition, we have the following
observations:
? LDA and Kmeans perform poorly due to the
fact that the two methods do not use any prior
knowledge. It is also shown that only using
the context distributional information is not
sufficient for clustering aspect phrases.
? Constraint-LDA and L-EM that utilize prior
knowledge perform better. We can see that
Constraint-LDA outperforms LDA in terms
of RI (Rand Index) on all domains. L-EM
achieves the best results against the baselines.
This demonstrates the effectiveness to incor-
porate prior knowledge.
? SDC-MNB produces the optimal results
among all models for clustering. Methods
that use must-links and cannot-links may suf-
fer from noisy links. For L-EM, we find
that it is sensitive to noisy must-links. As
L-EM assumes that must-link is transitive,
several noisy must-links may totally misla-
bel the softly annotated data. For Constraint-
LDA, it is more robust than L-EM, because
it doesn?t assume the transitivity of must-
link. However, it only promotes the RI (Rand
Index) consistently by leveraging pair-wise
prior knowledge, but sometimes it hurts the
1620
performance with respect to purity or en-
tropy. Our method is consistently better on
almost all domains, which shows the advan-
tages of the proposed model.
? SDC-MNB is remarkably better than base-
lines, particularly for the cellphone domain.
We argue that this is because we have the
largest number of reviews for each product
in the cellphone domain. The larger dataset
gives us more observations on each phrase,
so that we obtain more reliable estimation of
model parameters.
3.3.2 Comparison to supervised baselines
We further compare our methods with two super-
vised models. For each supervised model, we
provide a proportion of manually labeled data for
training, which is randomly selected from gold-
standard annotations. However, we didn?t use any
labeled data for our approach.
? MNB: The labeled seeds are used to train a
MNB classifier to classify all unlabeled as-
pect phrases into different classes.
? L-Kmeans: In L-Kmeans, the clusters of the
labeled seeds are fixed at the initiation and
remain unchanged during iteration.
Purity RI Entropy
MNB-5% 53.21% 85.77% 1.854
MNB-10% 59.55% 86.70% 1.656
MNB-15% 66.06% 88.39% 1.449
L-Kmeans-10% 53.54% 86.15% 1.745
L-Kmeans-15% 57.00% 86.89% 1.643
L-Kmeans-20% 60.97% 87.63% 1.528
SDC-MNB 59.49% 88.26% 1.580
Table 4: Comparison to supervised baselines.
MNB-5% means MNB with 5% labeled data.
We experiment with several settings: taking
5%, 10% and 15% of the manually labeled aspect
phrases for training, and the remainder as unla-
beled data. Experiment results is shown in Table
4 (the results are averaged over 4 domains). We
can see that our unsupervised approach is roughly
as good as the supervised MNB with 10% labeled
data. Our unsupervised approach is also slightly
better than L-Kmeans with 15% labeled data. This
result further demonstrates the effectiveness of our
model.
3.3.3 Influence of parameters
We vary the confidence level from 90% to 99.9%
to see how it impacts on the performance of SDC-
MNB. The results are presented in Fig. 5 (the re-
sults are averaged over 4 domains). We can see
that the performance of clustering is fairly stable
when changing the confidence level, which im-
plies the robustness of our model.
Figure 5: Influence of the confidence level on
SDC-MNB.
3.3.4 Analysis of SDC-constraint
As mentioned in Section 2.2, SDC-constraint is
dependent on the number of observations. More
observations we get, more informative the con-
straint is, which means the constraint is tighter and
d
jk
(see Eq.4) is smaller. For all k, we count how
many d
jk
is less than 0.2 (and 1) on average for
each aspect phrase f
j
. d
jk
is calculated with a
confidence level of 99%. The statistics of con-
straints is given in Table 5. We can see that the
cellphone domain has the most informative and
largest constraint set, that may explain why SDC-
MNB achieves the largest purity gain(over L-EM)
in cellphone domain.
#(d
jk
< 0.2) #(0.2 < d
jk
< 1) purity gain
Camera 3.02 8.78 1.53%
Cellphone 17.29 30.5 15.99%
Laptop 4.6 13.22 6.58%
MP3MP4 6.1 10.7 13.82%
Table 5: Constraint statistics on different domains.
4 Related Work
Our work is related to two important research
topics: aspect-level sentiment analysis, and
constraint-driven learning. For aspect-level senti-
ment analysis, aspect extraction and clustering are
key tasks. For constraint-driven learning, a variety
of frameworks and models for sentiment analysis
have been studied extensively.
There have been many studies on clustering
aspect-related phrases. Most existing studies are
1621
based on context information. Some works also
encoded lexical similarity and synonyms as prior
knowledge. Carenini et al. (2005) proposed a
method that was based on several similarity met-
rics involving string similarity, synonyms, and lex-
ical distances defined with WordNet. Guo et al.
(2009) proposed a multi-level latent semantic as-
sociation model to capture expression-level and
context-level topic structure. Zhai et al. (2010)
proposed an EM-based semi-supervised learning
method to group aspect expressions into user-
specified aspects. They employed lexical knowl-
edge to provide a better initialization for EM. In
Zhai et al. (2011a), an EM-based unsupervised
version was proposed. The so-called L-EM model
first generated softly labeled data by grouping fea-
ture expressions that share words in common, and
then merged the groups by lexical similarity. Zhai
et al. (2011b) proposed a LDA-based method
that incorporates must-link and cannot-link con-
straints.
Another line of work aimed to extract and clus-
ter aspect words simultaneously using topic mod-
eling. Titov and McDonald (2008) proposed the
multi-grain topic models to discover global and
local aspects. Branavan et al. (2008) proposed
a method which first clustered the key-phrases
in Pros and Cons into some aspect categories
based on distributional similarity, then built a topic
model modeling the topics or aspects. Zhao et al.
(2010) proposed the MaxEnt-LDA (a Maximum
Entropy and LDA combination) hybrid model to
jointly discover both aspect words and aspect-
specific opinion words, which can leverage syn-
tactic features to separate aspects and sentiment
words. Mukherjee and Liu (2012) proposed a
semi-supervised topic model which used user-
provided seeds to discover aspects. Chen et al.
(2013) proposed a knowledge-based topic model
to incorporate must-link and cannot-link informa-
tion. Their model can adjust topic numbers auto-
matically by leveraging cannot-link.
Our work is also related to general constraint-
driven(or knowledge-driven) learning models.
Several general frameworks have been proposed to
fully utilize various prior knowledge in learning.
Constraint-driven learning (Chang et al., 2008)
(CODL) is an EM-like algorithm that incorpo-
rates per-instance constraints into semi-supervised
learning. Posterior regularization (Graca et al.,
2007) (PR) is a modified EM algorithm in which
the E-step is replaced by the projection of the
model posterior distribution onto the set of dis-
tributions that satisfy auxiliary expectation con-
straints. Generalized expectation criteria (Druck
et al., 2008) (GE) is a framework for incorporating
preferences about model expectations into param-
eter estimation objective functions. Liang et al.
(2009) developed a Bayesian decision-theoretic
framework to learn an exponential family model
using general measurements on the unlabeled data.
In this paper, we model our problem in the frame-
work of posterior regularization.
Many works promoted the performance of sen-
timent analysis by incorporating prior knowledge
as weak supervision. Li and Zhang (2009) in-
jected lexical prior knowledge to non-negative ma-
trix tri-factorization. Shen and Li (2011) further
extended the matrix factorization framework to
model dual supervision from both document and
word labels. Vikas Sindhwani (2008) proposed a
general framework for incorporating lexical infor-
mation as well as unlabeled data within standard
regularized least squares for sentiment prediction
tasks. Fang (2013)proposed a structural learning
model with a handful set of aspect signature terms
that are encoded as weak supervision to extract la-
tent sentiment explanations.
5 Conclusions
Aspect finding and clustering is an important task
for aspect-level sentiment analysis. In order to
cluster aspect-related phrases, this paper has ex-
plored a novel concept, sentiment distribution con-
sistency. We formalize the concept as soft con-
straint, integrate the constraint with a context-
based probabilistic model, and solve the problem
in the posterior regularization framework. The
proposed model is also designed to be robust with
both sufficient and insufficient observations. Ex-
periments show that our approach outperforms
state-of-the-art baselines consistently.
Acknowledgments
This work was partly supported by the following
grants from: the National Basic Research Program
(973 Program) under grant No.2012CB316301
and 2013CB329403, the National Science Foun-
dation of China project under grant No.61332007
and No. 61272227, and the Beijing Higher Educa-
tion Young Elite Teacher Project.
1622
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993?1022, March.
Stephen Boyd and Lieven Vandenberghe. 2004. Con-
vex Optimization. Cambridge University Press, New
York, NY, USA.
S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and
Regina Barzilay. 2008. Learning document-level
semantic properties from free-text annotations. In
Proceedings of the Association for Computational
Linguistics (ACL).
Giuseppe Carenini, Raymond T. Ng, and Ed Zwart.
2005. Extracting knowledge from evaluative text.
In Proceedings of the 3rd International Conference
on Knowledge Capture, K-CAP ?05, pages 11?18,
New York, NY, USA. ACM.
Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and
Dan Roth. 2008. Learning and inference with
constraints. In Proceedings of the 23rd National
Conference on Artificial Intelligence - Volume 3,
AAAI?08, pages 1513?1518. AAAI Press.
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Mal Castellanos, and Riddhiman Ghosh. 2013.
Exploiting domain knowledge in aspect extraction.
In EMNLP, pages 1655?1667. ACL.
Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2008. Learning from labeled features using
generalized expectation criteria. In Proceedings of
the 31st Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ?08, pages 595?602, New York,
NY, USA. ACM.
Lei Fang, Minlie Huang, and Xiaoyan Zhu. 2013. Ex-
ploring weakly supervised latent sentiment expla-
nations for aspect-level review analysis. In Qi He,
Arun Iyengar, Wolfgang Nejdl, Jian Pei, and Rajeev
Rastogi, editors, CIKM, pages 1057?1066. ACM.
Joao V. Graca, Lf Inesc-id, Kuzman Ganchev, Ben
Taskar, Joo V. Graa, L F Inesc-id, Kuzman Ganchev,
and Ben Taskar. 2007. Expectation maximization
and posterior constraints. In In Advances in NIPS,
pages 569?576.
Honglei Guo, Huijia Zhu, Zhili Guo, XiaoXun Zhang,
and Zhong Su. 2009. Product feature categorization
with multilevel latent semantic association. In Pro-
ceedings of the 18th ACM Conference on Informa-
tion and Knowledge Management, CIKM ?09, pages
1087?1096, New York, NY, USA. ACM.
Tao Li, Yi Zhang, and Vikas Sindhwani. 2009. A non-
negative matrix tri-factorization approach to senti-
ment classification with lexical prior knowledge. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1 - Volume 1, ACL ?09, pages
244?252, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning from measurements in exponential fami-
lies. In Proceedings of the 26th Annual Interna-
tional Conference on Machine Learning, ICML ?09,
pages 641?648, New York, NY, USA. ACM.
Arjun Mukherjee and Bing Liu. 2012. Aspect extrac-
tion through semi-supervised modeling. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Vol-
ume 1, ACL ?12, pages 339?348, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chao Shen and Tao Li. 2011. A non-negative matrix
factorization based approach for active dual super-
vision from document and word labels. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?11, pages 949?
958, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Vikas Sindhwani and Prem Melville. 2008.
Document-word co-regularization for semi-
supervised sentiment analysis. In ICDM, pages
1025?1030. IEEE Computer Society.
Ivan Titov and Ryan McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of the 17th International Conference on
World Wide Web, WWW ?08, pages 111?120, New
York, NY, USA. ACM.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2010.
Grouping product features using semi-supervised
learning with soft-constraints. In Proceedings of
the 23rd International Conference on Computa-
tional Linguistics, COLING ?10, pages 1272?1280,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia.
2011a. Clustering product features for opinion min-
ing. In Proceedings of the Fourth ACM Interna-
tional Conference on Web Search and Data Mining,
WSDM ?11, pages 347?354, New York, NY, USA.
ACM.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia.
2011b. Constrained lda for grouping product fea-
tures in opinion mining. In Proceedings of the 15th
Pacific-Asia Conference on Advances in Knowl-
edge Discovery and Data Mining - Volume Part
I, PAKDD?11, pages 448?459, Berlin, Heidelberg.
Springer-Verlag.
Wayne X. Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a MaxEnt-LDA hybrid. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?10, pages 56?
65, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
1623
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 483?491,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Learning to Link Entities with Knowledge Base
Zhicheng Zheng, Fangtao Li, Minlie Huang, Xiaoyan Zhu
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China
{zhengzc04,fangtao06}@gmail.com, {aihuang,zxy-dcs}@tsinghua.edu.cn
Abstract
This paper address the problem of entity link-
ing. Specifically, given an entity mentioned in
unstructured texts, the task is to link this entity
with an entry stored in the existing knowledge
base. This is an important task for informa-
tion extraction. It can serve as a convenient
gateway to encyclopedic information, and can
greatly improve the web users? experience.
Previous learning based solutions mainly fo-
cus on classification framework. However, it?s
more suitable to consider it as a ranking prob-
lem. In this paper, we propose a learning to
rank algorithm for entity linking. It effectively
utilizes the relationship information among
the candidates when ranking. The experi-
ment results on the TAC 20091 dataset demon-
strate the effectiveness of our proposed frame-
work. The proposed method achieves 18.5%
improvement in terms of accuracy over the
classification models for those entities which
have corresponding entries in the Knowledge
Base. The overall performance of the system
is also better than that of the state-of-the-art
methods.
1 Introduction
The entity linking task is to map a named-entity
mentioned in a text to a corresponding entry stored
in the existing Knowledge Base. The Knowledge
Base can be considered as an encyclopedia for en-
tities. It contains definitional, descriptive or rele-
vant information for each entity. We can acquire the
knowledge of entities by looking up the Knowledge
1http://www.nist.gov/tac/
Base. Wikipedia is an online encyclopedia, and now
it becomes one of the largest repositories of encyclo-
pedic knowledge. In this paper, we use Wikipedia as
our Knowledge Base.
Entity linking can be used to automatically aug-
ment text with links, which serve as a conve-
nient gateway to encyclopedic information, and can
greatly improve user experience. For example, Fig-
ure 1 shows news from BBC.com. When a user is
interested in ?Thierry Henry?, he can acquire more
detailed information by linking ?Thierry Henry? to
the corresponding entry in the Knowledge Base.
Figure 1: Entity linking example
Entity linking is also useful for some information
extraction (IE) applications. We can make use of
information stored in the Knowledge Base to assist
the IE problems. For example, to answer the ques-
tion ?When was the famous basketball player Jor-
dan born??, if the Knowledge Base contains the en-
483
tity of basketball player Michael Jordan and his in-
formation (such as infobox2 in Wikipedia), the cor-
rect answer ?February 17, 1963? can be easily re-
trieved.
Entity linking encounters the problem of entity
ambiguity. One entity may refer to several entries
in the Knowledge Base. For example, the entity
?Michael Jordan? can be linked to the basketball
player or the professor in UC Berkeley.
Previous solutions find that classification based
methods are effective for this task (Milne and Wit-
ten, 2008). These methods consider each candidate
entity independently, and estimate a probability that
the candidate entry corresponds to the target entity.
The candidate with the highest probability was cho-
sen as the target entity. In this way, it?s more like
a ranking problem rather than a classification prob-
lem. Learning to rank methods take into account the
relations between candidates, which is better than
considering them independently. Learning to rank
methods are popular in document information re-
trieval, but there are few studies on information ex-
traction. In this paper, we investigate the application
of learning to rank methods to the entity linking task.
And we compare several machine learning methods
for this task. We investigate the pairwise learning to
rank method, Ranking Perceptron (Shen and Joshi,
2005), and the listwise method, ListNet (Cao et al,
2007). Two classification methods, SVM and Per-
ceptron, are developed as our baselines. In com-
parison, learning to rank methods show significant
improvements over classification methods, and List-
Net achieves the best result. The best overall per-
formance is also achieved with our proposed frame-
work.
This paper is organized as follows. In the next
section we will briefly review the related work. We
present our framework for entity linking in section
3. We then describe in section 4 learning to rank
methods and features for entity linking. A top1 can-
didate validation module will be explained in section
5. Experiment results will be discussed in section 6.
Finally, we conclude the paper and discusses the fu-
ture work in section 7.
2Infoboxes are tables with semi-structured information in
some pages of Wikipedia
2 Related Work
There are a number of studies on named entity dis-
ambiguation, which is quite relevant to entity link-
ing. Bagga and Baldwin (1998) used a Bag of Words
(BOW) model to resolve ambiguities among people.
Mann and Yarowsky (2003) improved the perfor-
mance of personal names disambiguation by adding
biographic features. Fleischman (2004) trained a
Maximum Entropy model with Web Features, Over-
lap Features, and some other features to judge
whether two names refer to the same individual.
Pedersen (2005) developed features to represent the
context of an ambiguous name with the statistically
significant bigrams.
These methods determined to which entity a spe-
cific name refer by measuring the similarity between
the context of the specific name and the context of
the entities. They measured similarity with a BOW
model. Since the BOW model describes the con-
text as a term vector, the similarity is based on co-
occurrences. Although a term can be one word or
one phrase, it can?t capture various semantic rela-
tions. For example, ?Michael Jordan now is the boss
of Charlotte Bobcats? and ?Michael Jordan retired
from NBA?. The BOW model can?t describe the re-
lationship between Charlotte Bobcats and NBA. Ma-
lin and Airoldi (2005) proposed an alternative sim-
ilarity metric based on the probability of walking
from one ambiguous name to another in a random
walk within the social network constructed from all
documents. Minkov (2006) considered extended
similarity metrics for documents and other objects
embedded in graphs, facilitated via a lazy graph
walk, and used it to disambiguate names in email
documents. Bekkerman and McCallum (2005) dis-
ambiguated web appearances of people based on the
link structure of Web pages. These methods tried to
add background knowledge via social networks. So-
cial networks can capture the relatedness between
terms, so the problem of a BOW model can be
solved to some extent. Xianpei and Jun (2009) pro-
posed to use Wikipedia as the background knowl-
edge for disambiguation. By leveraging Wikipedia?s
semantic knowledge like social relatedness between
named entities and associative relatedness between
concepts, they can measure the similarity between
entities more accurately. Cucerzan (2007) and
484
Bunescu (2006) used Wikipedia?s category informa-
tion in the disambiguation process. Using different
background knowledge, researcher may find differ-
ent efficient features for disambiguation.
Hence researchers have proposed so many effi-
cient features for disambiguation. It is important to
integrate these features to improve the system per-
formance. Some researchers combine features by
manual rules or weights. However, it is not conve-
nient to directly use these rules or weights in another
data set. Some researchers also try to use machine
learning methods to combine the features. Milne and
Witten (2008) used typical classifiers such as Naive
Bayes, C4.5 and SVM to combine features. They
trained a two-class classifier to judge whether a can-
didate is a correct target. And then when they try
to do disambiguation for one query, each candidate
will be classified into the two classes: correct tar-
get or incorrect target. Finally the candidate answer
with the highest probability will be selected as the
target if there are more than one candidates classi-
fied as answers. They achieve great performance in
this way with three efficient features. The classifier
based methods can be easily used even the feature
set changed. However, as we proposed in Introduc-
tion, it?s not the best way for such work. We?ll detail
the learning to rank methods in the next section.
3 Framework for Entity Linking
Input%a%query
Output%the%
final answer%
Figure 2: The framework for entity linking
Entity linking is to align a named-entity men-
tioned in a text to a corresponding entry stored in
the existing Knowledge Base. We proposed a frame-
work to solve the ?entity linking? task. As illustrated
in Figure 2, when inputting a query which is an en-
tity mentioned in a text, the system will return the
target entry in Knowledge Base with four modules:
1. Query Processing. First, we try to correct the
spelling errors in the queries by using query
spelling correction supplied by Google. Sec-
ond, we expand the query in three ways: ex-
panding acronym queries from the text where
the entity is located, expanding queries with the
corresponding redirect pages of Wikipedia and
expanding queries by using the anchor text in
the pages from Wikipedia.
2. Candidates Generation. With the queries gen-
erated in the first step, the candidate genera-
tion module retrieves the candidates from the
Knowledge Base. The candidate generation
module also makes use of the disambiguation
pages in Wikipedia. If there is a disambigua-
tion page corresponding to the query, the linked
entities listed in the disambiguation page are
added to the candidate set.
3. Candidates Ranking. In the module, we rank all
the candidates with learning to rank methods.
4. Top1 Candidate Validation. To deal with those
queries without appropriate matching, we fi-
nally add a validation module to judge whether
the top one candidate is the target entry.
The detail information of ranking module and val-
idation module will be introduced in next two sec-
tions.
4 Learning to Rank Candidates
In this section we first introduce the learning to rank
methods, and then describe the features for ranking
methods.
4.1 Learning to rank methods
Learning to rank methods are popular in the area of
document retrieval. There are mainly two types of
learning to rank methods: pairwise and listwise. The
pairwise approach takes as instances object pairs in
a ranking list for a query in learning. In this way,
it transforms the ranking problem to the classifica-
tion problem. Each pair from ranking list is labeled
based on the relative position or with the score of
485
ranking objects. Then a classification model can be
trained on the labeled data and then be used for rank-
ing. The pairwise approach has advantages in that
the existing methodologies on classification can be
applied directly. The listwise approach takes can-
didate lists for a query as instances to train ranking
models. Then it trains a ranking function by min-
imizing a listwise loss function defined on the pre-
dicted list and the ground truth list.
To describe the learning to rank methods, we first
introduce some notations:
? Query set Q = {q(i)?i = 1 : m}.
? Each query q(i) is associated with a list of ob-
jects(in document retrieval, the objects should
be documents), d(i) = {d(i)j ?j = 1 : n(i)}.
? Each object list has a labeled score list y(i) =
{y(i)j ?j = 1 : n(i)} represents the relevance de-
gree between the objects and the query.
? Features vectors x(i)j from each query-object
pair, j = 1 : n(i).
? Ranking function f, for each x(i)j it outputs a
score f(x(i)j ). After the training phase, to rank
the objects, just use the ranking function f to
output the score list of the objects, and rank
them with the score list.
In the paper we will compare two different learn-
ing to rank approaches: Ranking Perceptron for pair-
wise and ListNet for listwise. A detailed introduc-
tion on Ranking Perceptron (Shen and Joshi, 2005)
and ListNet (Cao et al, 2007) is given.
4.1.1 Ranking Perceptron
Ranking Perceptron is a pairwise learning to rank
method. The score function f!(x(i)j ) is defined as
< !, x(i)j >.
For each pair (x(i)j1 , x
(i)
j2 ), f!(x
(i)
j1 ? x
(i)
j2 ) is com-
puted. With a given margin function g(x(i)j1 , x
(i)
j2 ) and
a positive rate  , if f!(x(i)j1 ? x
(i)
j2 ) ? g(x
(i)
j1 , x
(i)
j2 ) ,
an update is performed:
!t+1 = !t + (x(i)j1 ? x
(i)
j2 )g(x
(i)
j1 , x
(i)
j2 )
After iterating enough times, we can use the func-
tion f! to rank candidates.
4.1.2 ListNet
ListNet takes lists of objects as instances in learn-
ing. It uses a probabilistic method to calculate the
listwise loss function.
ListNet transforms into probability distributions
both the scores of the objects assigned by the ora-
cle ranking function and the real score of the objects
given by human.
Let  denote a permutation on the objects. In List-
Net alorithm, the probability of  with given scores
is defined as:
Ps() =
n
?
j=1
exp(s(j))
?n
k=j exp(s(k))
Then the top k probability of Gk(j1, j2, ..., jk) can
be calculated as:
Ps(Gk(j1, j2, ..., jk)) =
k
?
t=1
exp(sjt)
?l
l=t exp(sjl)
The ListNet uses a listwise loss function with
Cross Entropy as metric:
L(y(i), z(i)) = ?
?
?g?Gk
Py(i)(g)log(Pz(i) (g))
Denote as f! the ranking function based on
Neural Network model !. The gradient of
L(y(i), z(i)(f!)) with respect to parameter ! can be
calculated as:
?! = ?L(y
(i), z(i)(f!))
?!
= ?
?
?g?Gk
?Pz(i)(f!)(g)
?!
Py(i)(g)
Pz(i)(f!)(g)
In each iteration, the ! is updated with ? ??!
in a gradient descent way. Here  is the learning
rate.
To train a learning to rank model, the manually
evaluated score list for each query?s candidate list is
required. We assign 1 to the real target entity and 0
to the others.
486
4.2 Features for Ranking
In the section, we will introduce the features used
in the ranking module. For convenience, we define
some symbols first:
? Q represents a query, which contains a named
entity mentioned in a text. CSet represents the
candidate entries in Knowledge Base for the
query Q. C represents a candidate in CSet.
? Q?s nameString represents the name string of
Q. Q?s sourceText represents the source text of
Q. Q?s querySet represents the queries which
are expansions of Q?s nameString.
? C?s title represents the title of corresponding
Wikipedia article of C. C?s titleExpand repre-
sents the union set of the redirect set of C and
the anchor text set of C. C?s article represents
the Wikipedia article of C.
? C?s nameEntitySet represents the set of all
named entities in C?s article labeled by Stan-
ford NER (Finkel et al, 2005). Q?s nameEnti-
tySet represents the set of all named entities in
Q?s sourceText.
? C?s countrySet represents the set of all coun-
tries in C?s article, and we detect the countries
from text via a manual edited country list. Q?s
countrySet represents the set of all countries
in Q?s sourceText. C?s countrySetInTitle rep-
resents the set of countries exist in one of the
string s from C?s titleExpand.
? C?s citySetInTitle represents the set of all cities
exist in one of the string s from C?s titleExpand,
and we detect the cities from text via a manual
edited list of famous cities. Q?s citySet repre-
sents the set of all cities in Q?s sourceText.
? Q?s type represents the type of query Q. It?s la-
beled by Stanford NER. C?s type is manually
labeled already in the Knowledge Base.
The features that used in the ranking module can
be divided into 3 groups: Surface, Context and Spe-
cial. Each of these feature groups will be detailed
next.
4.2.1 Surface Features
The features in Surface group are used to measure
the similarity between the query string and candidate
entity?s name string.
? StrSimSurface. The feature value is the max-
imum similarity between the Q?s nameString
and each string s in the set C?s titleExpand. The
string similarity is measured with edit distance.
? ExactEqualSurface. The feature value is 1 if
there is a string s in set C?s titleExpand same as
the Q?s nameString, or the Candidate C is ex-
tracted from the disambiguation page. In other
case, the feature value is set to 0.
? StartWithQuery. The feature value is 1 if there
is a string s in set C?s titleExpand starting with
the Q?s nameString, and C?s ExactEqualSur-
face is not 1. In other case, the feature value
is set to 0.
? EndWithQuery. The feature value is 1 if there
is a string s in set C?s titleExpand ending with
the Q?s nameString, and C?s ExactEqualSur-
face is not 1. In other case, the feature value
is set to 0.
? StartInQuery. The feature value is 1 if there is a
string s in set C?s titleExpand that s is the prefix
of the Q?s nameString, and C?s ExactEqualSur-
face is not 1. In other case, the feature value is
set to 0.
? EndInQuery. The feature value is 1 if there is a
string s in set C?s titleExpand that s is the post-
fix of the Q?s nameString, and C?s ExactEqual-
Surface is not 1. In other case, the feature value
is set to 0.
? EqualWordNumSurface. The feature value is
the maximum number of same words between
the Q?s nameString and each string s in the set
C?s titleExpand.
? MissWordNumSurface. The feature value is
the minimum number of different words be-
tween the Q?s nameString and each string s in
the set C?s titleExpand.
487
4.2.2 Context Features
The features in Context group are used to measure
the context relevance between query and the candi-
date entity. We mainly consider the TF-IDF similar-
ity and named entity co-occurrence.
? TFSimContext. The feature value is the TF-
IDF similarity between the C?s article and Q?s
sourceText.
? TFSimRankContext. The feature value is the
inverted rank of C?s TFSimContext in the CSet.
? AllWordsInSource. The feature value is 1 if all
words in C?s title exist in Q?s sourceText, and
in other case, the feature value is set to 0.
? NENumMatch. The feature value is the num-
ber of of same named entities between C?s
nameEntitySet and Q?s nameEntitySet. Two
named entities are judged to be the same if and
only if the two named entities? strings are iden-
tical.
4.2.3 Special Features
Besides the features above, we also find that the
following features are quite significant in the entity
linking task: country names, city names and types
of queries and candidates.
? CountryInTextMatch. The feature value is the
number of same countries between C?s coun-
trySet and Q?s countrySet.
? CountryInTextMiss. The feature value is the
number of countries that exist in Q?s country-
Set but do not exist in C?s countrySet.
? CountryInTitleMatch. The feature value is the
number of same countries between C?s coun-
trySetInTitle and Q?s countrySet.
? CountryInTitleMiss. The feature value is the
number of countries that exist in C?s country-
SetInTitle but do not exist in Q?s countrySet.
? CityInTitleMatch. The feature value is the
number of same cities between C?s citySetInTi-
tle and Q?s citySet.
? TypeMatch. The feature value is 0 if C?s type is
not consistent with Q?s type, in other case, the
feature value is set to 1.
When ranking the candidates in CSet, the fea-
tures? value was normalized into [0, 1] to avoid noise
caused by large Integer value or small double value.
5 Top 1 Candidate Validation
To deal with those queries without target entities in
the Knowledge Base, we supply a Top 1 candidate
validation module. In the module, a two-class classi-
fier is used to judge whether the top one candidate is
the true target entity. The top one candidate selected
from the ranking module can be divided into two
classes: target and non-target, depending on whether
it?s the correct target link of the query. According
to the performance of classification, SVM is chosen
as the classifier (In practice, the libsvm package is
used) and the SVM classifier is trained on the entire
training set.
Most of the features used in the validation mod-
ule are the same as those in ranking module, such as
StrSimSurface, EqualWordNumSurface, MissWord-
NumSurface, TFSimContext, AllWordsInSource,
NENumMatch and TypeMatch. We also design
some other features, as follows:
? AllQueryWordsInWikiText. The feature value
is one if Q?s textRetrievalSet contains C, and
in other case the feature value is set to zero.
The case that Q?s textRetrievalSet contains C
means the candidate C?s article contains the Q?s
nameString.
? CountryInTextPer. The feature is the percent-
age of countries from Q?s countrySet exist in
C?s countrySet too. The feature can be seen as
a normalization of CountryInTextMatch/Miss
features in ranking module.
? ScoreOfRank. The feature value is the score
of the candidate given by the ranking module.
The ScoreOfRank takes many features in rank-
ing module into consideration, so only fewer
features of ranking module are used in the clas-
sifier.
488
6 Experiment and Analysis
6.1 Experiment Setting
Algorithm Accuracy Improvement
over SVM
ListNet 0.9045 +18.5%
Ranking Perceptron 0.8842 +15.8%
SVM 0.7636 -
Perceptron 0.7546 -1.2%
Table 1: Evaluation of different ranking algorithm
Entity linking is initiated as a task in this year?s
TAC-KBP3 track, so we use the data from this track.
The entity linking task in the KBP track is to map
an entity mentioned in a news text to the Knowl-
edge Base, which consist of articles from Wikipedia.
The KBP track gives a sample query set which con-
sists of 416 queries for developing. The test set con-
sists of 3904 queries. 2229 of these queries can?t
be mapped to Knowledge Base, for which the sys-
tems should return NIL links. The remaining 1675
queries all can be aligned to Knowledge Base. We
will firstly analyze the ranking methods with those
non-NIL queries, and then with an additional vali-
dation module, we train and test with all queries in-
cluding NIL queries.
As in the entity linking task of KBP track, the ac-
curacy is taken as
accuracy = #(correct answered queries)#(total queries)
6.2 Evaluation of Machine Learning Methods
in ranking
As mentioned in the section of related work, learn-
ing to rank methods in entity linking performs bet-
ter than the classification methods. To justify this,
some experiments are designed to evaluate the per-
formance of our ranking module when adopting dif-
ferent algorithms.
To evaluate the performance of the ranking mod-
ule, we use all the queries which can be aligned to a
target entry in the Knowledge Base. The training set
contains 285 valid queries and the test set contains
1675.
3http://apl.jhu.edu/ paulmac/kbp.html
Set Features in Set
Set1 Surface Features
Set2 Set1+TF-IDF Features
Set3 Set2+AllWordsInSource
Set4 Set3+NENumMatch
Set5 Set4+CountryInTitle Features
Set6 Set5+CountryInText Features
Set7 Set6+CityInTitleMatch
Set8 Set7+MatchType
Table 2: Feature Sets
Three algorithms are taken into comparison: List-
Net, Ranking Perceptron, and classifier based meth-
ods. The classifier based methods are trained by di-
viding the candidates into two classes: target and
non-target. Then, the candidates are ranked accord-
ing to their probability of being classified as target.
two different classifiers are tested here, SVM and
Perceptron.
!
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
Set1 Set2 Set3 Set4 Set5 Set6 Set7 Set8
Ac
cu
ra
cy
Feature)Set
ListNet
Ranking!Perceptron
Figure 3: Comparison of ListNet and Ranking Perceptron
As shown in Table 1, the two learning to rank
methods perform much better than the classification
based methods. The experiment results prove our
point that the learning to rank algorithms are more
suitable in this work. And the ListNet shows slight
improvement over Ranking Perceptron, but since the
improvement is not so significant, maybe it depends
on the feature set. To confirm this, we compare the
two algorithms with different features, as showed
in Table 2. In Figure 3, The ListNet outperforms
Ranking Perceptron with all feature sets except Set1,
which indicates that the listwise approach is more
suitable than the pairwise approach. The pairwise
approach suffers from two problems: first, the ob-
jective of learning is to minimize classification er-
489
Systems Accuracy of all queries Accuracy of non-NIL queries Accuracy of NIL queries
System1 0.8217 0.7654 0.8641
System2 0.8033 0.7725 0.8241
System3 0.7984 0.7063 0.8677
ListNet+SVM 0.8494 0.79 0.8941
Table 3: Evaluation of the overall performance, compared with KBP results (System 1-3 demonstrate the top three
ranked systems)
rors but not to minimize the errors in ranking; sec-
ond, the number of pairs generated from list varies
largely from list to list, which will result in a model
biased toward lists with more objects. The issues are
also discussed in (Y.B. Cao et al, 2006; Cao et al,
2007). And the listwise approach can fix the prob-
lems well.
As the feature sets are added incrementally, it can
be used for analyzing the importance of the features
to the ranking task. Although Surface Group only
takes into consideration the candidate?s title and the
query?s name string, its accuracy is still higher than
60%. This is because many queries have quite small
number of candidates, the target entry can be picked
out with the surface features only. The result shows
that after adding the TF-IDF similarity related fea-
tures, the accuracy increases significantly to 84.5%.
Although TF-IDF similarity is a simple way to mea-
sure the contextual similarity, it performs well in
practice. Another improvement is achieved when
adding the CountryInTitleMatch and CountryInTi-
tleMiss features. Since a number of queries in test
set need to disambiguate candidates with different
countries in their titles, the features about coun-
try in the candidates? title are quite useful to deal
with these queries. But it doesn?t mean that the
features mentioned above are the most important.
Because many features correlated with each other
quite closely, adding these features doesn?t lead to
remarkable improvement. The conclusion from the
results is that the Context Features significantly im-
prove the ranking performance and the Special Fea-
tures are also useful in the entity linking task.
6.3 Overall Performance Evaluation
We are also interested in overall performance with
the additional validation module. We use all the
3904 queries as the test set, including both NIL
and non-NIL queries. The top three results from
the KBP track (McNamee and Dang, 2009) are se-
lected as comparison. The evaluation result in Table
3 shows that our proposed framework outperforms
the best result in the KBP track, which demonstrates
the effectiveness of our methods.
7 Conclusions and Future Work
This paper demonstrates a framework of learning to
rank for linking entities with the Knowledge Base.
Experimenting with different ranking algorithms, it
shows that the learning to rank methods perform
much better than the classification methods in this
problem. ListNet achieves 18.5% improvement over
SVM, and Ranking Perceptron gets 15.8% improve-
ment over SVM. We also observe that the listwise
learning to rank methods are more suitable for this
problem than pairwise methods. We also add a vali-
dation module to deal with those entities which have
no corresponding entry in the Knowledge Base. We
also evaluate the proposed method on the whole data
set given by the KBP track, for which we add a bi-
nary SVM classification module to validate the top
one candidate. The result of experiment shows the
proposed strategy performs better than all the sys-
tems participated in the entity linking task.
In the future, we will try to develop more sophis-
ticated features in entity linking and design a typical
learning to rank method for the entity linking task.
Acknowledgments
This work was partly supported by the Chinese Nat-
ural Science Foundation under grant No.60973104
and No. 60803075, partly carried out with the aid
of a grant from the International Development Re-
search Center, Ottawa, Canada IRCI project from
the International Development.
490
References
Bagga and Baldwin. 1998. Entity-Based Cross-
Document Coreferencing Using the Vector Spcae
Model. in Proceedings of HLT/ACL.
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised Personal Name Disambiguation. in Proceedings
of CONIL.
Michael Ben Fleishman. 2004. Multi-Document Person
Name Resolution. in Proceedings of ACL.
Ted Pedersen, Amruta Purandare and Anagha Kulkarni.
2005. Name Discrimination by Clustering Similar
Contexts. in Proceedings of CICLing.
B.Malin and E. Airoldi. 2005. A Network Analysis
Model for Disambiguation of Names in Lists. in Pro-
ceedings of CMOT.
Einat Minkov, William W. Cohen and Andrew Y. Ng.
2006. Contextual Search and Name Disambiguation
in Email Using Graph. in Proceedings of SIGIR.
Ron Bekkerman and Andrew McCallum. 2005. Disam-
biguating Web Appearances of People in a Social Net-
work. in Proceedings of WWW.
Xianpei Han and Jun Zhao. 2009. Named Entity Disam-
biguation by Leveraging Wikipedia Semantic Knowl-
edge. in Proceedings of CIKM.
David Milne and Ian H. Witten. 2008. Learning to Link
with Wikipedia. in Proceedings of CIKM.
Herbrich, R., Graepel, T. and Obermayer K. 1999. Sup-
port vector learning for ordinal regression. in Pro-
ceedings of ICANN.
Freund, Y., Iyer, R., Schapire, R. E. and Singer, Y. 1998.
An efficient boosting algorithm for combining prefer-
ences. in Proceedings of ICML.
Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds,
M., Hamilton, N. and Hullender, G. 2005. Learning to
rank using gradient descent. in Proceedings of ICML.
Cao, Y. B., Xu, J., Liu, T. Y., Li, H., Huang, Y. L. and
Hon, H. W. 2006. Adapting ranking SVM to document
retrieval. in Proceedings of SIGIR.
Cao, Z., Qin, T., Liu, T. Y., Tsai, M. F. and Li, H. 2007.
Learning to rank: From pairwise approach to listwise
approach. in Proceedings of ICML.
Qin, T., Zhang, X.-D., Tsai, M.-F., Wang, D.-S., Liu,
T.Y., and Li, H. 2007. Query-level loss functions for
information retrieval. in Proceedings of Information
processing and management.
L. Shen and A. Joshi. 2005. Ranking and Reranking with
Perceptron. Machine Learning,60(1-3),pp. 73-96.
Silviu Cucerzan. 2007. Large-Scale Named Entity Dis-
ambiguation Based on Wikipedia Data. in Proceed-
ings of EMNLP-CoNLL.
Razvan Bunescu and Marius Pasca. 2006. Using En-
cyclopedic Knowledge for Named Entity Disambigua-
tion. in Proceedings of EACL.
Paul McNamee and Hoa Dang. 2009. Overview
of the TAC 2009 Knowledge Base Population Track
(DRAFT). in Proceedings of TAC.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sam-
pling. Proceedings of the 43nd Annual Meeting of
the Association for Computational Linguistics (ACL
2005), pp. 363-370.
491
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 760?769,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Metadata-Aware Measures for Answer Summarization
in Community Question Answering
Mattia Tomasoni ?
Dept. of Information Technology
Uppsala University, Uppsala, Sweden
mattia.tomasoni.8371@student.uu.se
Minlie Huang
Dept. Computer Science and Technology
Tsinghua University, Beijing 100084, China
aihuang@tsinghua.edu.cn
Abstract
This paper presents a framework for au-
tomatically processing information com-
ing from community Question Answering
(cQA) portals with the purpose of gen-
erating a trustful, complete, relevant and
succinct summary in response to a ques-
tion. We exploit the metadata intrinsically
present in User Generated Content (UGC)
to bias automatic multi-document summa-
rization techniques toward high quality in-
formation. We adopt a representation of
concepts alternative to n-grams and pro-
pose two concept-scoring functions based
on semantic overlap. Experimental re-
sults on data drawn from Yahoo! An-
swers demonstrate the effectiveness of our
method in terms of ROUGE scores. We
show that the information contained in the
best answers voted by users of cQA por-
tals can be successfully complemented by
our method.
1 Introduction
Community Question Answering (cQA) portals
are an example of Social Media where the infor-
mation need of a user is expressed in the form of a
question for which a best answer is picked among
the ones generated by other users. cQA websites
are becoming an increasingly popular complement
to search engines: overnight, a user can expect a
human-crafted, natural language answer tailored
to her specific needs. We have to be aware, though,
that User Generated Content (UGC) is often re-
dundant, noisy and untrustworthy (Jeon et al,
?The research was conducted while the first author was
visiting Tsinghua University.
2006; Wang et al, 2009b; Suryanto et al, 2009).
Interestingly, a great amount of information is em-
bedded in the metadata generated as a byprod-
uct of users? action and interaction on Social Me-
dia. Much valuable information is contained in an-
swers other than the chosen best one (Liu et al,
2008). Our work aims to show that such informa-
tion can be successfully extracted and made avail-
able by exploiting metadata to distill cQA content.
To this end, we casted the problem to an instance
of the query-biased multi-document summariza-
tion task, where the question was seen as a query
and the available answers as documents to be sum-
marized. We mapped each characteristic that an
ideal answer should present to a measurable prop-
erty that we wished the final summary could ex-
hibit:
? Quality to assess trustfulness in the source,
? Coverage to ensure completeness of the in-
formation presented,
? Relevance to keep focused on the user?s in-
formation need and
? Novelty to avoid redundancy.
Quality of the information was assessed via Ma-
chine Learning (ML) techniques under best an-
swer supervision in a vector space consisting of
linguistic and statistical features about the answers
and their authors. Coverage was estimated by se-
mantic comparison with the knowledge space of a
corpus of answers to similar questions which had
been retrieved through the Yahoo! Answers API 1.
Relevance was computed as information overlap
between an answer and its question, while Novelty
was calculated as inverse overlap with all other
answers to the same question. A score was as-
signed to each concept in an answer according to
1http://developer.yahoo.com/answers
760
the above properties. A score-maximizing sum-
mary under a maximum coverage model was then
computed by solving an associated Integer Linear
Programming problem (Gillick and Favre, 2009;
McDonald, 2007). We chose to express concepts
in the form of Basic Elements (BE), a semantic
unit developed at ISI2 and modeled semantic over-
lap as intersection in the equivalence classes of
two concepts (formal definitions will be given in
section 2.3).
The objective of our work was to present what
we believe is a valuable conceptual framework;
more advance machine learning and summariza-
tion techniques would most likely improve the per-
formances.
The remaining of this paper is organized as fol-
lows. In the next section Quality, Coverage, Rel-
evance and Novelty measures are presented; we
explain how they were calculated and combined
to generate a final summary of all answers to a
question. Experiments are illustrated in Section
3, where we give evidence of the effectiveness of
our method. We list related work in Section 5, dis-
cuss possible alternative approaches in Section 4
and provide our conclusions in Section 6.
2 The summarization framework
2.1 Quality as a ranking problem
Quality assessing of information available on So-
cial Media had been studied before mainly as a
binary classification problem with the objective of
detecting low quality content. We, on the other
hand, treated it as a ranking problem and made
use of quality estimates with the novel intent of
successfully combining information from sources
with different levels of trustfulness and writing
ability. This is crucial when manipulating UGC,
which is known to be subject to particularly great
variance in credibility (Jeon et al, 2006; Wang
et al, 2009b; Suryanto et al, 2009) and may be
poorly written.
An answer a was given along with information
about the user u that authored it, the set TAq (To-
tal Answers) of all answers to the same question q
and the set TAu of all answers by the same user.
Making use of results available in the literature
(Agichtein et al, 2008) 3, we designed a Quality
2Information Sciences Institute, University of Southern
California, http://www.isi.edu
3A long list of features is proposed; training a classifier
on all of them would no doubt increase the performances.
feature space to capture the following syntactic,
behavioral and statistical properties:
? ?, length of answer a
? ? , number of non-stopwords in a with a cor-
pus frequency larger than n (set to 5 in our
experiments)
? $, points awarded to user u according to the
Yahoo! Answers? points system
? %, ratio of best answers posted by user u
The features mentioned above determined a space
?; An answer a, in such feature space, assumed
the vectorial form:
?a = ( ?, ?, $, % )
Following the intuition that chosen best answers
(a?) carry high quality information, we used su-
pervised ML techniques to predict the probability
of a to have been selected as a best answer a?. We
trained a Linear Regression classifier to learn the
weight vector W = (w1, w2, w3, w4) that would
combine the above feature. Supervision was given
in the form of a training set TrQ of labeled pairs
defined as:
TrQ = {??a, isbesta ?}
isbesta was a boolean label indicating whether a
was an a? answer; the training set size was de-
termined experimentally and will be discussed in
Section 3.2. Although the value of isbesta was
known for all answers, the output of the classifier
offered us a real-valued prediction that could be
interpreted as a quality score Q(?a):
Q(?a) ? P ( isbesta = 1 | a, u, TAu, )
? P ( isbesta = 1 | ?a )
= W T ??a (1)
The Quality measure for an answer a was approx-
imated by the probability of such answer to be a
best answer (isbesta = 1) with respect to its au-
thor u and the sets TAu and TAq. It was calcu-
lated as dot product between the learned weight
vector W and the feature vector for answer ?a.
Our decision to proceed in an unsupervised di-
rection came from the consideration that any use
of external human annotation would have made it
impracticable to build an actual system on larger
scale. An alternative, completely unsupervised ap-
proach to quality detection that has not undergone
experimental analysis is discussed in Section 4.
761
2.2 Bag-of-BEs and semantic overlap
The properties that remain to be discussed, namely
Coverage, Relevance and Novelty, are measures
of semantic overlap between concepts; a concept
is the smallest unit of meaning in a portion of
written text. To represent sentences and answers
we adopted an alternative approach to classical n-
grams that could be defined bag-of-BEs. a BE
is ?a head|modifier|relation triple representation
of a document developed at ISI? (Zhou et al,
2006). BEs are a strong theoretical instrument to
tackle the ambiguity inherent in natural language
that find successful practical applications in real-
world query-based summarization systems. Dif-
ferent from n-grams, they are variant in length and
depend on parsing techniques, named entity de-
tection, part-of-speech tagging and resolution of
syntactic forms such as hyponyms, pronouns, per-
tainyms, abbreviation and synonyms. To each BE
is associated a class of semantically equivalent
BEs as result of what is called a transformation
of the original BE; the mentioned class uniquely
defines the concept. What seemed to us most re-
markable is that this makes the concept context-
dependent. A sentence is defined as a set of con-
cepts and an answer is defined as the union be-
tween the sets that represent its sentences.
The rest of this section gives formal definition
of our model of concept representation and seman-
tic overlap. From a set-theoretical point of view,
each concepts c was uniquely associated with a set
Ec = {c1, c2 . . . cm} such that:
?i, j (ci ?
L c) ? (ci 6? c) ? (ci 6? cj)
In our model, the ??? relation indicated syntac-
tic equivalence (exact pattern matching), while the
??L? relation represented semantic equivalence
under the convention of some language L (two
concepts having the same meaning). Ec was de-
fined as the set of semantically equivalent concepts
to c, called its equivalence class; each concept ci
in Ec carried the same meaning (?L) of concept c
without being syntactically identical (?); further-
more, no two concepts i and j in the same equiva-
lence class were identical.
?Climbing a tree to escape a black bear is pointless be-
cause they can climb very well.?
BE = they|climb
Ec = {climb|bears, bear|go up, climbing|animals,
climber|instincts, trees|go up, claws|climb...}
Given two concepts c and k:
c ./ k
{
c ? k or
Ec ? Ek 6= ?
We defined semantic overlap as occurring between
c and k if they were syntactically identical or if
their equivalence classes Ec and Ek had at least
one element in common. In fact, given the above
definition of equivalence class and the transitivity
of ??? relation, we have that if the equivalence
classes of two concepts are not disjoint, then they
must bare the same meaning under the convention
of some language L; in that case we said that c
semantically overlapped k. It is worth noting that
relation ?./? is symmetric, transitive and reflexive;
as a consequence all concepts with the same mean-
ing are part of a same equivalence class. BE and
equivalence class extraction were performed by
modifying the behavior of the BEwT-E-0.3 frame-
work 4. The framework itself is responsible for
the operative definition of the ??L? relation and
the creation of the equivalence classes.
2.3 Coverage via concept importance
In the scenario we proposed, the user?s informa-
tion need is addressed in the form of a unique,
summarized answer; information that is left out of
the final summary will simply be unavailable. This
raises the concern of completeness: besides ensur-
ing that the information provided could be trusted,
we wanted to guarantee that the posed question
was being answered thoroughly. We adopted the
general definition of Coverage as the portion of
relevant information about a certain subject that
is contained in a document (Swaminathan et al,
2009). We proceeded by treating each answer
to a question q as a separate document and we
retrieved through the Yahoo! Answers API a set
TKq (Total Knowledge) of 50 answers 5 to ques-
tions similar to q: the knowledge space of TKq
was chosen to approximate the entire knowledge
space related to the queried question q. We cal-
culated Coverage as a function of the portion of
answers in TKq that presented semantic overlap
with a.
4The authors can be contacted regarding the possibil-
ity of sharing the code of the modified version. Orig-
inal version available from http://www.isi.edu/
publications/licensed-sw/BE/index.html.
5such limit was imposed by the current version of the API.
Experiments with a greater corpus should be carried out in the
future.
762
C(a, q) =
?
ci?a
?(ci) ? tf(ci, a) (2)
The Coverage measure for an answer a was cal-
culated as the sum of term frequency tf(ci, a) for
concepts in the answer itself, weighted by a con-
cept importance function, ?(ci), for concepts in
the total knowledge space TKq. ?(c) was defined
as follows:
?(c) =
|TKq,c|
|TKq|
? log2
|TKq|
|TKq,c|
(3)
where TKq,c = {d ? TKq : ?k ? d, k ./ c}
The function ?(c) of concept c was calculated as
a function of the cardinality of set TKq and set
TKq,c, which was the subset of all those answers
d that contained at least one concept k which pre-
sented semantical overlap with c itself. A similar
idea of knowledge space coverage is addressed by
Swaminathan et al (2009), from which formulas
(2) and (3) were derived.
A sensible alternative would be to estimate Cov-
erage at the sentence level.
2.4 Relevance and Novelty via ./ relation
To this point, we have addressed matters of trust-
fulness and completeness. Another widely shared
concern for Information Retrieval systems is Rel-
evance to the query. We calculated relevance by
computing the semantic overlap between concepts
in the answers and the question. Intuitively, we re-
ward concepts that express meaning that could be
found in the question to be answered.
R(c, q) =
|qc|
|q|
(4)
where qc = {k ? q : k ./ c}
The Relevance measure R(c, q) of a concept c
with respect to a question q was calculated as the
ratio of the cardinality of set qc (containing all
concepts in q that semantically overlapped with c)
normalized by the total number of concepts in q.
Another property we found desirable, was to
minimize redundancy of information in the final
summary. Since all elements in TAq (the set of
all answers to q) would be used for the final sum-
mary, we positively rewarded concepts that were
expressing novel meanings.
N(c, q) = 1?
|TAq,c|
|TAq|
(5)
where TAq,c = {d ? TAq : ?k ? d, k ./ c}
The Novelty measure N(c, q) of a concept c with
respect to a question q was calculated as the ratio
of the cardinality of set TAq,c over the cardinality
of set TAq; TAq,c was the subset of all those an-
swers d in TAq that contained at least one concept
k which presented semantical overlap with c.
2.5 The concept scoring functions
We have now determined how to calculate the
scores for each property in formulas (1), (2), (4)
and (5); under the assumption that the Quality and
Coverage of a concept are the same of its answer,
every concept c part of an answer a to some ques-
tion q, could be assigned a score vector as follows:
?c = (Q(?a), C(a, q), R(c, q), N(c, q) )
What we needed at this point was a function S
of the above vector which would assign a higher
score to concepts most worthy of being included
in the final summary. Our intuition was that since
Quality, Coverage, Novelty and Relevance were
all virtues properties, S needed to be monoton-
ically increasing with respect to all its dimen-
sions. We designed two such functions. Func-
tion (6), which multiplied the scores, was based
on the probabilistic interpretation of each score as
an independent event. Further empirical consid-
erations, brought us to later introduce a logarith-
mic component that would discourage inclusion of
sentences shorter then a threshold t (a reasonable
choice for this parameter is a value around 20).
The score for concept c appearing in sentence sc
was calculated as:
S?(c) =
4?
i=1
(?ci ) ? logt(length(s
c)) (6)
A second approach that made use of human
annotation to learn a vector of weights V =
(v1, v2, v3, v4) that linearly combined the scores
was investigated. Analogously to what had been
done with scoring function (6), the ? space was
augmented with a dimension representing the
length of the answer.
S?(c) =
4?
i=1
(?ci ? vi) + length(s
c) ? v5 (7)
In order to learn the weight vector V that would
combine the above scores, we asked three human
annotators to generate question-biased extractive
summaries based on all answers available for a
certain question. We trained a Linear Regression
763
classifier with a set TrS of labeled pairs defined
as:
TrS = {? (?c, length(sc)), includec ?}
includec was a boolean label that indicated
whether sc, the sentence containing c, had
been included in the human-generated summary;
length(sc) indicated the length of sentence sc.
Questions and relative answers for the generation
of human summaries were taken from the ?filtered
dataset? described in Section 3.1.
The concept score for the same BE in two sep-
arate answers is very likely to be different be-
cause it belongs to answers with their own Quality
and Coverage values: this only makes the scoring
function context-dependent and does not interfere
with the calculation the Coverage, Relevance and
Novelty measures, which are based on information
overlap and will regard two BEs with overlapping
equivalence classes as being the same, regardless
of their score being different.
2.6 Quality constrained summarization
The previous sections showed how we quantita-
tively determined which concepts were more wor-
thy of becoming part of the final machine sum-
mary M . The final step was to generate the sum-
mary itself by automatically selecting sentences
under a length constraint. Choosing this constraint
carefully demonstrated to be of crucial importance
during the experimental phase. We again opted
for a metadata-driven approach and designed the
length constraint as a function of the lengths of
all answers to q (TAq) weighted by the respective
Quality measures:
lengthM =
?
a?TAq
length(a) ?Q(?a) (8)
The intuition was that the longer and the more
trustworthy answers to a question were, the more
space was reasonable to allocate for information
in the final, machine summarized answer M .
M was generated so as to maximize the scores
of the concepts it included. This was done under a
maximum coverage model by solving the follow-
ing Integer Linear Programming problem:
maximize:
?
i
S(ci) ? xi (9)
subject to:
?
j
length(j) ? sj ? length
M
?
j
yj ? occij ? xi ?i (10)
occij , xi, yj ? {0, 1} ?i, j
occij = 1 if ci ? sj , ?i, j
xi = 1 if ci ? M, ?i
yj = 1 if sj ? M, ?j
In the above program,M is the set of selected sen-
tences: M = {sj : yj = 1, ?j}. The integer
variables xi and yj were equals to one if the corre-
sponding concept ci and sentence sj were included
in M . Similarly occij was equal to one if concept
ci was contained in sentence sj . We maximized
the sum of scores S(ci) (for S equals to S? or S?)
for each concept ci in the final summary M . We
did so under the constraint that the total length of
all sentences sj included in M must be less than
the total expected length of the summary itself. In
addition, we imposed a consistency constraint: if
a concept ci was included in M , then at least one
sentence sj that contained the concept must also
be selected (constraint (10)). The described opti-
mization problem was solved using lp solve 6.
We conclude with an empirical side note: since
solving the above can be computationally very de-
manding for large number of concepts, we found
performance-wise very fruitful to skim about one
fourth of the concepts with lowest scores.
3 Experiments
3.1 Datasets and filters
The initial dataset was composed of 216,563 ques-
tions and 1,982,006 answers written by 171,676
user in 100 categories from the Yahoo! Answers
portal7. We will refer to this dataset as the ?un-
filtered version?. The metadata described in sec-
tion 2.1 was extracted and normalized; quality
experiments (Section 3.2) were then conducted.
The unfiltered version was later reduced to 89,814
question-answer pairs that showed statistical and
linguistic properties which made them particularly
adequate for our purpose. In particular, trivial, fac-
toid and encyclopedia-answerable questions were
6the version used was lp solve 5.5, available at http:
//lpsolve.sourceforge.net/5.5
7The reader is encouraged to contact the authors regarding
the availability of data and filters described in this Section.
764
removed by applying a series of patterns for the
identification of complex questions. The work by
Liu et al (2008) indicates some categories of ques-
tions that are particularly suitable for summariza-
tion, but due to the lack of high-performing ques-
tion classifiers we resorted to human-crafted ques-
tion patterns. Some pattern examples are the fol-
lowing:
? {Why,What is the reason} [...]
? How {to,do,does,did} [...]
? How {is,are,were,was,will} [...]
? How {could,can,would,should} [...]
We also removed questions that showed statistical
values outside of convenient ranges: the number of
answers, length of the longest answer and length
of the sum of all answers (both absolute and nor-
malized) were taken in consideration. In particular
we discarded questions with the following charac-
teristics:
? there were less than three answers 8
? the longest answer was over 400 words
(likely a copy-and-paste)
? the sum of the length of all answers outside
of the (100, 1000) words interval
? the average length of answers was outside of
the (50, 300) words interval
At this point a second version of the dataset
was created to evaluate the summarization perfor-
mance under scoring function (6) and (7); it was
generated by manually selecting questions that
arouse subjective, human interest from the pre-
vious 89,814 question-answer pairs. The dataset
size was thus reduced to 358 answers to 100 ques-
tions that were manually summarized (refer to
Section 3.3). From now on we will refer to this
second version of the dataset as the ?filtered ver-
sion?.
3.2 Quality assessing
In Section 2.1 we claimed to be able to identify
high quality content. To demonstrate it, we con-
ducted a set of experiments on the original unfil-
tered dataset to establish whether the feature space
? was powerful enough to capture the quality of
answers; our specific objective was to estimate the
8Being too easy to summarize or not requiring any sum-
marization at all, those questions wouldn?t constitute an valu-
able test of the system?s ability to extract information.
Figure 1: Precision values (Y-axis) in detecting best an-
swers a? with increasing training set size (X-axis) for a Lin-
ear Regression classifier on the unfiltered dataset.
amount of training examples needed to success-
fully train a classifier for the quality assessing task.
The Linear Regression9 method was chosen to de-
termine the probabilityQ(?a) of a to be a best an-
swer to q; as explained in Section 2.1, those prob-
abilities were interpreted as quality estimates. The
evaluation of the classifier?s output was based on
the observation that given the set of all answers
TAq relative to q and the best answer a?, a suc-
cessfully trained classifier should be able to rank
a? ahead of all other answers to the same question.
More precisely, we defined Precision as follows:
|{q ? TrQ : ?a ? TAq, Q(?a
?
) > Q(?a)}|
|TrQ|
where the numerator was the number of questions
for which the classifier was able to correctly rank
a? by giving it the highest quality estimate in TAq
and the denominator was the total number of ex-
amples in the training set TrQ. Figure 1 shows the
precision values (Y-axis) in identifying best an-
swers as the size of TrQ increases (X-axis). The
experiment started from a training set of size 100
and was repeated adding 300 examples at a time
until precision started decreasing. With each in-
crease in training set size, the experiment was re-
peated ten times and average precision values were
calculated. In all runs, training examples were
picked randomly from the unfiltered dataset de-
scribed in Section 3.1; for details on TrQ see Sec-
tion 2.1. A training set of 12,000 examples was
chosen for the summarization experiments.
9Performed with Weka 3.7.0 available at http://www.
cs.waikato.ac.nz/?ml/weka
765
System a? (baseline) S? S?
ROUGE-1 R 51.7% 67.3% 67.4%
ROUGE-1 P 62.2% 54.0% 71.2%
ROUGE-1 F 52.9% 59.3% 66.1%
ROUGE-2 R 40.5% 52.2% 58.8%
ROUGE-2 P 49.0% 41.4% 63.1%
ROUGE-2 F 41.6% 45.9% 57.9%
ROUGE-L R 50.3% 65.1% 66.3%
ROUGE-L P 60.5% 52.3% 70.7%
ROUGE-L F 51.5% 57.3% 65.1%
Table 1: Summarization Evaluation on filtered dataset (re-
fer to Section 3.1 for details). ROUGE-L, ROUGE-1 and
ROUGE-2 are presented; for each, Recall (R), Precision (P)
and F-1 score (F) are given.
3.3 Evaluating answer summaries
The objective of our work was to summarize an-
swers from cQA portals. Two systems were de-
signed: Table 1 shows the performances using
function S? (see equation (7)), and function S?
(see equation (6)). The chosen best answer a?
was used as a baseline. We calculated ROUGE-1
and ROUGE-2 scores10 against human annotation
on the filtered version of the dataset presented in
Section 3.1. The filtered dataset consisted of 358
answers to 100 questions. For each questions q,
three annotators were asked to produce an extrac-
tive summary of the information contained in TAq
by selecting sentences subject to a fixed length
limit of 250 words. The annotation resulted in 300
summaries (larger-scale annotation is still ongo-
ing). For the S? system, 200 of the 300 generated
summaries were used for training and the remain-
ing were used for testing (see the definition of TrS
Section 2.5). Cross-validation was conducted. For
the S? system, which required no training, all of
the 300 summaries were used as the test set.
S? outperformed the baseline in Recall (R) but
not in Precision (P); nevertheless, the combined F-
1 score (F) was sensibly higher (around 5 points
percentile). On the other hand, our S? system
showed very consistent improvements of an order
of 10 to 15 points percentile over the baseline on
all measures; we would like to draw attention on
the fact that even if Precision scores are higher,
it is on Recall scores that greater improvements
were achieved. This, together with the results ob-
tained by S?, suggest performances could benefit
10Available at http://berouge.com/default.
aspx
Figure 2: Increase in ROUGE-L, ROUGE-1 and ROUGE-
2 performances of the S? system as more measures are taken
in consideration in the scoring function, starting from Rele-
vance alone (R) to the complete system (RQNC). F-1 scores
are given.
from the enforcement of a more stringent length
constraint than the one proposed in (8). Further
potential improvements on S? could be obtained
by choosing a classifier able to learn a more ex-
pressive underlying function.
In order to determine what influence the single
measures had on the overall performance, we con-
ducted a final experiment on the filtered dataset to
evaluate (the S? scoring function was used). The
evaluation was conducted in terms of F-1 scores of
ROUGE-L, ROUGE-1 and ROUGE-2. First only
Relevance was tested (R) and subsequently Qual-
ity was added (RQ); then, in turn, Coverage (RQC)
and Novelty (RQN); Finally the complete system
taking all measures in consideration (RQNC). Re-
sults are shown in Figure 2. In general perfor-
mances increase smoothly with the exception of
ROUGE-2 score, which seems to be particularly
sensitive to Novelty: no matter what combination
of measures is used (R alone, RQ, RQC), changes
in ROUGE-2 score remain under one point per-
centile. Once Novelty is added, performances rise
abruptly to the system?s highest. A summary ex-
ample, along with the question and the best an-
swer, is presented in Table 2.
4 Discussion and Future Directions
We conclude by discussing a few alternatives to
the approaches we presented. The lengthM con-
straint for the final summary (Section 2.6), could
have been determined by making use of external
knowledge such as TKq: since TKq represents
766
HOW TO PROTECT YOURSELF FROM A BEAR?
http://answers.yahoo.com/question/index?qid=
20060818062414AA7VldB
***BEST ANSWER***
Great question. I have done alot of trekking through California, Montana
and Wyoming and have met Black bears (which are quite dinky and placid
but can go nuts if they have babies), and have been half an hour away from
(allegedly) the mother of all grizzley s whilst on a trail through Glacier
National park - so some other trekkerers told me... What the park wardens
say is SING, SHOUT, MAKE NOISE...do it loudly, let them know you
are there..they will get out of the way, it is a surprised bear wot will go
mental and rip your little legs off..No fun permission: anything that will
confuse them and stop them in their tracks...I have been told be an native
american buddy that to keep a bottle of perfume in your pocket...throw it at
the ground near your feet and make the place stink: they have good noses,
them bears, and a mega concentrated dose of Britney Spears Obsessive
Compulsive is gonna give em something to think about...Have you got a
rape alarm? Def take that...you only need to distract them for a second
then they will lose interest..Stick to the trails is the most important thing,
and talk to everyone you see when trekking: make sure others know where
you are.
***SUMMARIZED ANSWER***
[...] In addition if the bear actually approaches you or charges you.. still
stand your ground. Many times they will not actually come in contact
with you, they will charge, almost touch you than run away. [...] The
actions you should take are different based on the type of bear. for ex-
ample adult Grizzlies can t climb trees, but Black bears can even when
adults. They can not climb in general as thier claws are longer and not
semi-retractable like a Black bears claws. [...] I truly disagree with the
whole play dead approach because both Grizzlies and Black bears are
oppurtunistic animals and will feed on carrion as well as kill and eat an-
imals. Although Black bears are much more scavenger like and tend not
to kill to eat as much as they just look around for scraps. Grizzlies on the
other hand are very accomplished hunters and will take down large prey
animals when they want. [...] I have lived in the wilderness of Northern
Canada for many years and I can honestly say that Black bears are not at
all likely to attack you in most cases they run away as soon as they see or
smell a human, the only places where Black bears are agressive is in parks
with visitors that feed them, everywhere else the bears know that usually
humans shoot them and so fear us. [...]
Table 2: A summarized answer composed of five different
portions of text generated with the S? scoring function; the
chosen best answer is presented for comparison. The rich-
ness of the content and the good level of readability make
it a successful instance of metadata-aware summarization of
information in cQA systems. Less satisfying examples in-
clude summaries to questions that require a specific order of
sentences or a compromise between strongly discordant opin-
ions; in those cases, the summarized answer might lack logi-
cal consistency.
the total knowledge available about q, a coverage
estimate of the final answers against it would have
been ideal. Unfortunately the lack of metadata
about those answers prevented us from proceeding
in that direction. This consideration suggests the
idea of building TKq using similar answers in the
dataset itself, for which metadata is indeed avail-
able. Furthermore, similar questions in the dataset
could have been used to augment the set of an-
swers used to generate the final summary with an-
swers coming from similar questions. Wang et al
(2009a) presents a method to retrieve similar ques-
tions that could be worth taking in consideration
for the task. We suggest that the retrieval method
could be made Quality-aware. A Quality feature
space for questions is presented by Agichtein et
al. (2008) and could be used to rank the quality of
questions in a way similar to how we ranked the
quality of answers.
The Quality assessing component itself could
be built as a module that can be adjusted to the
kind of Social Media in use; the creation of cus-
tomized Quality feature spaces would make it
possible to handle different sources of UGC (fo-
rums, collaborative authoring websites such as
Wikipedia, blogs etc.). A great obstacle is the lack
of systematically available high quality training
examples: a tentative solution could be to make
use of clustering algorithms in the feature space;
high and low quality clusters could then be labeled
by comparison with examples of virtuous behav-
ior (such as Wikipedia?s Featured Articles). The
quality of a document could then be estimated as a
function of distance from the centroid of the clus-
ter it belongs to. More careful estimates could take
the position of other clusters and the concentration
of nearby documents in consideration.
Finally, in addition to the chosen best answer, a
DUC-styled query-focused multi-document sum-
mary could be used as a baseline against which
the performances of the system can be checked.
5 Related Work
A work with a similar objective to our own is
that of Liu et al (2008), where standard multi-
document summarization techniques are em-
ployed along with taxonomic information about
questions. Our approach differs in two fundamen-
tal aspects: it took in consideration the peculiari-
ties of the data in input by exploiting the nature of
UGC and available metadata; additionally, along
with relevance, we addressed challenges that are
specific to Question Answering, such as Cover-
age and Novelty. For an investigation of Coverage
in the context of Search Engines, refer to Swami-
nathan et al (2009).
At the core of our work laid information trust-
fulness, summarization techniques and alternative
concept representation. A general approach to
the broad problem of evaluating information cred-
ibility on the Internet is presented by Akamine
et al (2009) with a system that makes use of
semantic-aware Natural Language Preprocessing
techniques. With analogous goals, but a focus
on UGC, are the papers of Stvilia et al (2005),
Mcguinness et al (2006), Hu et al (2007) and
767
Zeng et al (2006), which present a thorough inves-
tigation of Quality and trust in Wikipedia. In the
cQA domain, Jeon et al (2006) presents a frame-
work to use Maximum Entropy for answer quality
estimation through non-textual features; with the
same purpose, more recent methods based on the
expertise of answerers are proposed by Suryanto
et al (2009), while Wang et al (2009b) introduce
the idea of ranking answers taking their relation to
questions in consideration. The paper that we re-
gard as most authoritative on the matter is the work
by Agichtein et al (2008) which inspired us in the
design of the Quality feature space presented in
Section 2.1.
Our approach merged trustfulness estimation
and summarization techniques: we adapted the au-
tomatic concept-level model presented by Gillick
and Favre (2009) to our needs; related work in
multi-document summarization has been carried
out by Wang et al (2008) and McDonald (2007).
A relevant selection of approaches that instead
make use of ML techniques for query-biased sum-
marization is the following: Wang et al (2007),
Metzler and Kanungo (2008) and Li et al (2009).
An aspect worth investigating is the use of par-
tially labeled or totally unlabeled data for sum-
marization in the work of Wong et al (2008) and
Amini and Gallinari (2002).
Our final contribution was to explore the use of
Basic Elements document representation instead
of the widely used n-gram paradigm: in this re-
gard, we suggest the paper by Zhou et al (2006).
6 Conclusions
We presented a framework to generate trust-
ful, complete, relevant and succinct answers to
questions posted by users in cQA portals. We
made use of intrinsically available metadata along
with concept-level multi-document summariza-
tion techniques. Furthermore, we proposed an
original use for the BE representation of concepts
and tested two concept-scoring functions to com-
bine Quality, Coverage, Relevance and Novelty
measures. Evaluation results on human annotated
data showed that our summarized answers consti-
tute a solid complement to best answers voted by
the cQA users.
We are in the process of building a system that
performs on-line summarization of large sets of
questions and answers from Yahoo! Answers.
Larger-scale evaluation of results against other
state-of-the-art summarization systems is ongoing.
Acknowledgments
This work was partly supported by the Chi-
nese Natural Science Foundation under grant No.
60803075, and was carried out with the aid of
a grant from the International Development Re-
search Center, Ottawa, Canada. We would like to
thank Prof. Xiaoyan Zhu, Mr. Yang Tang and Mr.
Guillermo Rodriguez for the valuable discussions
and comments and for their support. We would
also like to thank Dr. Chin-yew Lin and Dr. Eu-
gene Agichtein from Emory University for sharing
their data.
References
Eugene Agichtein, Carlos Castillo, Debora Donato,
Aristides Gionis, and Gilad Mishne. 2008. Find-
ing high-quality content in social media. In Marc
Najork, Andrei Z. Broder, and Soumen Chakrabarti,
editors, Proceedings of the International Conference
on Web Search and Web Data Mining, WSDM 2008,
Palo Alto, California, USA, February 11-12, 2008,
pages 183?194. ACM.
Susumu Akamine, Daisuke Kawahara, Yoshikiyo
Kato, Tetsuji Nakagawa, Kentaro Inui, Sadao Kuro-
hashi, and Yutaka Kidawara. 2009. Wisdom: a web
information credibility analysis system. In ACL-
IJCNLP ?09: Proceedings of the ACL-IJCNLP 2009
Software Demonstrations, pages 1?4, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Massih-Reza Amini and Patrick Gallinari. 2002. The
use of unlabeled data to improve supervised learning
for text summarization. In SIGIR ?02: Proceedings
of the 25th annual international ACM SIGIR con-
ference on Research and development in informa-
tion retrieval, pages 105?112, New York, NY, USA.
ACM.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In ILP ?09: Proceedings
of the Workshop on Integer Linear Programming for
Natural Langauge Processing, pages 10?18, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Meiqun Hu, Ee-Peng Lim, Aixin Sun, Hady Wirawan
Lauw, and Ba-Quy Vuong. 2007. Measuring arti-
cle quality in wikipedia: models and evaluation. In
CIKM ?07: Proceedings of the sixteenth ACM con-
ference on Conference on information and knowl-
edge management, pages 243?252, New York, NY,
USA. ACM.
Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon
Park. 2006. A framework to predict the quality of
768
answers with non-textual features. In SIGIR ?06:
Proceedings of the 29th annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 228?235, New York,
NY, USA. ACM.
Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha,
and Yong Yu. 2009. Enhancing diversity, cover-
age and balance for summarization through struc-
ture learning. In WWW ?09: Proceedings of the 18th
international conference on World wide web, pages
71?80, New York, NY, USA. ACM.
Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin,
Dingyi Han, and Yong Yu. 2008. Understand-
ing and summarizing answers in community-based
question answering services. In Proceedings of the
22nd International Conference on Computational
Linguistics (Coling 2008), pages 497?504, Manch-
ester, UK, August. Coling 2008 Organizing Com-
mittee.
Ryan T. McDonald. 2007. A study of global infer-
ence algorithms in multi-document summarization.
In Giambattista Amati, Claudio Carpineto, and Gio-
vanni Romano, editors, ECIR, volume 4425 of Lec-
ture Notes in Computer Science, pages 557?564.
Springer.
Deborah L. Mcguinness, Honglei Zeng, Paulo Pin-
heiro Da Silva, Li Ding, Dhyanesh Narayanan, and
Mayukh Bhaowal. 2006. Investigation into trust for
collaborative information repositories: A wikipedia
case study. In In Proceedings of the Workshop on
Models of Trust for the Web, pages 3?131.
Donald Metzler and Tapas Kanungo. 2008. Ma-
chine learned sentence selection strategies for query-
biased summarization. In Proceedings of SIGIR
Learning to Rank Workshop.
Besiki Stvilia, Michael B. Twidale, Linda C. Smith,
and Les Gasser. 2005. Assessing information qual-
ity of a community-based encyclopedia. In Proceed-
ings of the International Conference on Information
Quality.
Maggy Anastasia Suryanto, Ee Peng Lim, Aixin Sun,
and Roger H. L. Chiang. 2009. Quality-aware col-
laborative question answering: methods and evalu-
ation. In WSDM ?09: Proceedings of the Second
ACM International Conference on Web Search and
Data Mining, pages 142?151, New York, NY, USA.
ACM.
Ashwin Swaminathan, Cherian V. Mathew, and Darko
Kirovski. 2009. Essential pages. In WI-IAT ?09:
Proceedings of the 2009 IEEE/WIC/ACM Interna-
tional Joint Conference on Web Intelligence and In-
telligent Agent Technology, pages 173?182, Wash-
ington, DC, USA. IEEE Computer Society.
Changhu Wang, Feng Jing, Lei Zhang, and Hong-
Jiang Zhang. 2007. Learning query-biased web
page summarization. In CIKM ?07: Proceedings of
the sixteenth ACM conference on Conference on in-
formation and knowledge management, pages 555?
562, New York, NY, USA. ACM.
Dingding Wang, Tao Li, Shenghuo Zhu, and Chris
Ding. 2008. Multi-document summarization via
sentence-level semantic analysis and symmetric ma-
trix factorization. In SIGIR ?08: Proceedings of the
31st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 307?314, New York, NY, USA. ACM.
Kai Wang, Zhaoyan Ming, and Tat-Seng Chua. 2009a.
A syntactic tree matching approach to finding sim-
ilar questions in community-based qa services. In
SIGIR ?09: Proceedings of the 32nd international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 187?194, New
York, NY, USA. ACM.
Xin-Jing Wang, Xudong Tu, Dan Feng, and Lei Zhang.
2009b. Ranking community answers by modeling
question-answer relationships via analogical reason-
ing. In SIGIR ?09: Proceedings of the 32nd interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 179?186,
New York, NY, USA. ACM.
Kam-Fai Wong, Mingli Wu, and Wenjie Li. 2008. Ex-
tractive summarization using supervised and semi-
supervised learning. In COLING ?08: Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 985?992, Morristown, NJ,
USA. Association for Computational Linguistics.
Honglei Zeng, Maher A. Alhossaini, Li Ding, Richard
Fikes, and Deborah L. McGuinness. 2006. Com-
puting trust from revision history. In PST ?06: Pro-
ceedings of the 2006 International Conference on
Privacy, Security and Trust, pages 1?1, New York,
NY, USA. ACM.
Liang Zhou, Chin Y. Lin, and Eduard Hovy. 2006.
Summarizing answers for complicated questions. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC),
Genoa, Italy.
769
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 333?337,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Fine Granular Aspect Analysis using Latent Structural Models
Lei Fang1 and Minlie Huang2
State Key Laboratory of Intelligent Technology and Systems,
Tsinghua National Laboratory for Information Science and Technology,
Department of Computer Science and Technology,
Tsinghua University, Beijing 100084, PR China.
1fang-l10@mails.tsinghua.edu.cn
2aihuang@tsinghua.edu.cn
Abstract
In this paper, we present a structural learning
model for joint sentiment classification and as-
pect analysis of text at various levels of gran-
ularity. Our model aims to identify highly in-
formative sentences that are aspect-specific in
online custom reviews. The primary advan-
tages of our model are two-fold: first, it per-
forms document-level and sentence-level sen-
timent polarity classification jointly; second,
it is able to find informative sentences that are
closely related to some respects in a review,
which may be helpful for aspect-level senti-
ment analysis such as aspect-oriented sum-
marization. The proposed method was eval-
uated with 9,000 Chinese restaurant reviews.
Preliminary experiments demonstrate that our
model obtains promising performance.
1 Introduction
Online reviews have been a major resource from
which users may find opinions or comments on the
products or services they want to consume. How-
ever, users sometimes might be overwhelmed, and
not be able to read reviews one by one when facing
a considerably large number of reviews, and they
may be not be satisfied by only being served with
document-level reviews statistics (that is, the num-
ber of reviews with 1-star, 2-star, . . . , respectively).
Aspect-level review analysis may be alternative for
addressing this issue as aspect-specific opinions may
more clearly, explicitly, and completely describe the
quality of a product from different properties.
Our goal is to discover informative sentences that
are consistent with the overall rating of a review, and
simultaneously, to perform sentiment analysis at as-
pect level. Notice, that a review with a high rating
(say, 4/5 stars) may contain both negative and posi-
tive opinions, and the same to a review with a very
low rating (say, 1/2 star). From our point of view,
each review has a set of sentences that are informa-
tive and coherent to its overall rating. To perform
fine granular sentiment analysis, the first step is to
discover such coherent content.
Many information needs require the systems to
perform fine granular sentiment analysis. Aspect-
level sentiment analysis may be more useful for
users to have a global picture of opinions on the
product?s properties. Furthermore, different users
may have different preferences on different aspects
of a product. Taking the reviews on mobile phones
as an example, female users may focus more on the
appearance while male users may lay more emphasis
on the hardware configuration; younger users prefer
to the app or game resources while older users may
just pay attention to the basic function of calling or
messaging.
In recent years, there has been much work focused
multilevel sentiment classification using structural
learning models. Yi (2007) extends the standard
conditional random fields to model the local senti-
ment flow. Ryan (2007) proposed structured models
for fine-to-coarse sentiment analysis. Oscar (2011)
proposed to discover fine-grained sentiment with
hidden-state CRF(Quattoni et al, 2007). Yessenali-
na (2010) deployed the framework of latent struc-
tural SVMs(Yu and Joachims., 2009) for multilevel
sentiment classification. As for aspect level rating,
ranking, or summarization, Benjamin(2007) em-
333
ployed the good grief algorithm for multiple aspect
ranking and the extensions of the generative topic
models were also widely studied, such as (Titov and
McDonald., 2008; Brody and Elhadad., 2010; Wang
et al, 2010; Li et al, 2011; Lu et al, 2011; Jo and
Oh., 2011; Lin and He, 2009).
In this paper, we build a general structural learn-
ing model for joint sentiment classification and as-
pect analysis using a latent discriminate method.
Our model is able to predict the sentiment polari-
ty of document as well as to identify aspect-specific
sentences and predict the polarity of such sentences.
The proposed method was evaluated with 9,000 Chi-
nese restaurant reviews. Preliminary experiments
demonstrate that our model obtains promising per-
formance.
2 Model
2.1 Document Structure
We assume that the polarity of document is closely
related to some aspects for the reason that people are
writing reviews to praise or criticize certain aspect-
s. Therefore, each informative sentence of the doc-
ument characterizes one aspect, expressing aspec-
t specific polarity or subjective features. Similar to
previous work on aspect analysis (Wang et al, 2010)
and multi-level sentiment classification (Yessenali-
na et al, 2010), we define the aspect as a collection
of synonyms. For instance, the word set {?value?,
?price?, ?cost?, ?worth?, ?quality?} is a synonym
set corresponding to the aspect ?price?. For each
document, an aspect is described by one or several
sentences expressing aspect specific polarity or sub-
jective information.
Let document be denoted by x, and y ? {+1,?1}
represents the positive or negative polarity of the
document, s is the set of informative sentences, in
which each sentence is attached with certain aspect
ai ? A = {a1, ..., ak}. Yessenalina (2010) chooses
a sentence set that best explains the sentiment of the
whole document while the s here retain this proper-
ty. Let ?(x, y, s) denote the joint feature map that
outputs the features describing the quality of predict-
ing sentiment y using the sentence set s.
Let xj denote the j-th sentence of documen-
t x, and aj is the attached aspect of xj . In spirit
to (Yessenalina et al, 2010), we propose the follow-
ing formulation of the discriminate function
w?T?(x, y, s) =
1
N(x)
?
j?s
(
y ? w?Tpolaj?pol(x
j) + w?Tsubjaj?subj(x
j)
)
where N(x) is the normalizing factor, ?pol(xj) and
?subj(xj) represents the polarity and subjectivity
features of sentence xj respectively. w?pol and w?subj
denote the weight for polarity and subjectivity fea-
tures. To be specific for each aspect, we have w?pola
and w?subja representing the vector of feature weight
for aspect a to calculate the polarity and subjectivity
score.
w?Tpol =
?
?
?
w?Tpola0...
w?Tpolak
?
?
?
, w?Tsubj =
?
?
?
w?Tsubja0...
w?Tsubjak
?
?
?
To make prediction, we have the document-level
sentiment classifier as
h(x; w?) = argmax
y=?1
max
s?S(x)
w?T?(x, y, s)
where S(x) = {s ? 1, . . . , |x| : |s| ? f(|x|)},
f(|x|) is a function that depends only on the number
of sentences in x, as illustrated in (Yessenalina et al,
2010). Therefore, for each sentence xj , we compute
the joint subjectivity and polarity score with respect
to aspect a and label y as
score(xj , a, y) = y?w?Tpola?pol(x
j)+w?Tsubja?subj(x
j)
we then assign aspect aj to sentence xj if
aj = argmax
a?A
score(xj , a, y)
After sorting score(xj , aj , y) in decreasing order
and taking summation by selecting the top f(|x|) (or
fewer, if there are fewer than f(|x|) that have posi-
tive joint score) sentences as the total score for each
y?{+1,?1} , we then predict y with the higher joint
score as the sentiment of the whole document. This
formulation of w?T?(x, y, s) and classifier explains
that for each sentence, the assigned aspect has the
highest score over other aspects.
334
2.2 Feature Space
In our model, we use bag-of-words features. In or-
der to obtain a model that is jointly trained, and sat-
isfy the condition that the overall polarity of docu-
ment should influence the sentiment of extracted in-
formative sentences. We denote the weight vector
modeling the polarity of entire document as w?doc, as
follows:
w?T?(x, y, s) =
y
N(x)
?
?
?
j?s
(w?Tpolaj?pol(x
j) + w?Tdoc?pol(xj))
?
?
+ 1N(x)
?
?
?
j?s
w?Tsubjaj?subj(x
j)
?
?+y ?w?Tdoc?pol(x)
2.3 Training
We trained our model using the latent structural
SVMs (Yu and Joachims., 2009).
OP1:
min
w?,??0
1
2 ||w||
2 + CN
N
?
i=1
?i
s.t.?i :
max
s?S(xi)
w?T?(xi, yi, s) ?
max
s??S(xi)
w?T?(xi,?yi, s?) +?(yi,?yi, s?)? ?i
We define ?(yi,?yi, s?) = 1, that is, we view
document level sentiment classification loss as the
loss function. It should be noticed that OP1 is non-
convex. To circumvent the optimization difficul-
ty, we employ the framework of structural SVM-
s (Tsochantaridis et al, 2004) with latent variables
proposed by Yu (2009) using the CCCP algorith-
m (Yuille and Rangarajan., 2003). In terms of the
formulation here, since the true informative sentence
set is never observed, it is a hidden or latent variable.
Thus, we keep si fixed to compute the upper bound
for the concave part of each constraint, and rewrite
the constraints as
?i ? max
s??S(xi)
w?T?(xi,?yi, s?)? w?T?(xi, yi, si)+1
After that, we have yi completed with the laten-
t variable si as if it is observed. For each training
example, starting with an initialization sentence set
in which each sentence is with an aspect label, the
training procedure alternates between solving an in-
stance of the structural SVM using the si and pre-
dicting a new sentence until the learned weight vec-
tor w? converges. In our work, we use the perfor-
mance on a validation set to choose the halting iter-
ation, as is similar to Yessenalina (2010).
2.4 Model Initialization
To initialize the informative sentence set, following
the experiment result of Yessenalina (2010), we set
f(|x|) = 0.3 ? |x|, that is, we only select the top
30% of the total sentences as the set of informative
part of the document. The normalizing factor is set
as N(x) =
?
f |x|, as Yessenalina (2010) demon-
strates that square root normalization can be useful.
To analyze the aspect of each sentence, we need to
give an initial guess of the aspect and sentiment for
each sentence.
Sentence level sentiment initialization : To ini-
tialize the sentence level sentiment, we employ a
rule based method incorporating positive and neg-
ative sentiment terms, with adversative relation con-
sidered.
Sentence aspect assignment initialization : Obvi-
ously, if a synonym of aspect a occurs in sentence
xl, we assign aspect a to xl, and add xl to an aspect
specific sentence set Pa.For sentence xl without any
aspect term, we set a as the aspect label if
a = argmax similarity(xl, Pa?)
a??A
We select the sentences whose sentiment is consis-
tent with the overall rating of a review as the initial
guess of the latent variable.
3 Experiments
In this section, we evaluate our model in terms of
document and sentence level sentiment classifica-
tion, we also analyze the performance of aspect as-
signment for each sentence. The model is evaluated
on the Chinese restaurant reviews crawled from Di-
anping1. Each of the reviews has an overall rating
ranging from one to five stars. To be specific, we
consider a review as positive if its rating is greater
1http://www.dianping.com/
335
than or equal to 4 stars, or negative if less than or
equal to 2 stars. The corpus has 4500 positive and
4500 negative reviews. Data and an implementation
of our model are publicly available2.
We train 5 different models by splitting these re-
views into 9 folds. Two folds are left out as the test-
ing set, and each model takes 5 folds as training set,
2 folds as validation set, and the performance is aver-
aged. Besides, we also manually label 100 reviews,
in which each sentence is labeled as positive or neg-
ative corresponding to certain aspect or with no as-
pect description. On average, each review has 9.66
sentences. However, only 21.5% of the total sen-
tences can be assigned to aspect by directly match-
ing with aspect terms, which explains that keywords
based aspect sentiment analysis may fail. For restau-
rant reviews, we pre-defined 11 aspects, and for each
aspect, we select about 5 frequently used terms to
describe that aspect. Table 1 shows some examples
of the aspect synonym set used in this paper:
Aspect Synonym Set
Taste ???taste?,???flavor?
Price ???price? ,???cost?
Dishes ???dishes?,???cuisine?
Ingredients ???food? ,???ingredients?
Facility ???facility?,???seat?
Location ???location?,
Environment ???environment?,
???decoration?
Service ???service? ,????waiter?
???attitude?
Table 1: Samples of Aspect Synonym.
Document level sentiment classification We com-
pare our method with previous work on sentimen-
t classification using standard SVM(Pang et al,
2002). Our model yields an accuracy of 94.15%
while the standard SVM classifier yields an accu-
racy of 90.35%. Clearly, our model outperforms the
baseline on document level sentiment classification.
Sentence level sentiment classification Our
method can extract a set of informative sentence
that are coherent to the overall rating of a re-
view. The evaluation of sentence-level sentiment
classification is based on manual annotation. We
2http://www.qanswers.net/faculty/hml/
sample 100 reviews, and present the extracted 300
sentences to annotators who have been asked to
assign positive/negative/non-related labels. Among
the sentences, 251 correctly classified as positive
or negative while 49 are misclassified. And, 38
sentences of the 49 sentences have mix opinions or
are non-subjective sentences.
Aspect Assignment To evaluate the accuracy of as-
pect assignment, we compare the predicted aspec-
t labels with the ground truth (manual annotation).
As some of sentences have explicit aspect terms and
can be easily identified, we only consider those sen-
tences without aspect words. In the extracted 300
sentences, 78 sentences have aspect terms, and for
the rest, our model assigns correct aspect labels to
44 sentences while random guess only maps 21 sen-
tences with right labels.
4 Conclusion and Future Work
In this paper, we address the task of multilevel sen-
timent classification of online custom reviews for
fine granular aspect analysis. We present a struc-
tural learning model based on struct-SVM with la-
tent variables. The informative sentence set is re-
garded as latent variable, in which each sentence is
attached with certain aspect label. The training pro-
cedure alternates between solving an instance of the
standard structural SVM optimization and predict-
ing a new sentence set until the halting condition is
satisfied. In addition, our model is a enough gen-
eral model which can be easily extended to other
domains. Preliminary experiments demonstrate that
our model obtains promising performance.
There are several possibilities to improve our
model. For future work, we propose to incorpo-
rate prior knowledge of latent variables to the mod-
el. One possible way is to reformulate the loss func-
tion by taking the predicted aspect of the extract-
ed sentences into consideration. Another is to in-
troduce confidence score to the extracted sentences,
such that the learned support vectors that are labeled
with higher confidence shall assert more force on the
decision plane.
Acknowledgments
This paper was supported by Chinese 973 project
under No.2012CB316301 and National Chinese Sci-
336
ence Foundation projects with No.60803075 and
No.60973104.
References
Samuel Brody and Noemie Elhadad. 2010. An unsuper-
vised aspect-sentiment model for online reviews. In
Proceedings of Annual Conference of the North Amer-
ican Chapter of the ACL, (NAACL).
Yohan Jo and Alice Oh. 2011. Aspect and sentiment uni-
fication model for online review analysis. In Proceed-
ings of Conference on Web Search and Data Mining
(WSDM).
Peng Li, Yinglin Wang, Wei Gao, and Jing Jiang. 2011.
Generating aspect-oriented multi-document summa-
rization with event-aspect model. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing, (EMNLP).
Chenghua Lin and Yulan He. 2009. Joint sentimen-
t/topic model for sentiment analysis. In Proceedings
of the conference on Information and knowledge man-
agement(CIKM).
Bin Lu, Myle Ott, Claire Cardie, and Benjamin Tsou.
2011. Multi-aspect sentiment analysis with topic mod-
els. In The ICDM?2011 Workshop on Sentiment Elic-
itation from Natural Text for Information Retrieval and
Extraction.
Yi Mao and Guy Lebanon. 2007. Isotonic conditional
random fields and local sentiment flow. In Proceed-
ings of Advances in Neural Information Processing
Systems (NIPS).
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In Proceedings of
Annual Meeting of the Association for Computational
Linguistics, (ACL).
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In Proceedings of Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
A. Quattoni, S. Wang, L.-P Morency, M. Collins, and
T. Darrell. 2007. Hidden-state conditional random
fields. IEEE Transactions on Pattern Analysis and
Machine Intelligence.
Benjamin Snyder and Regina Barzilay. 2007. Multiple
aspect ranking using the good grief algorithm. In Pro-
ceedings of Annual Conference of the North American
Chapter of the ACL, (NAACL).
Oscar Ta?ckstro?m and Ryan McDonald. 2011. Discov-
ering fine-grained sentiment with latent variable struc-
tured prediction models. In Proceedings of Annual Eu-
ropean Conference on Information Retrieval , (ECIR).
Ivan Titov and Ryan McDonald. 2008. A joint model of
text and aspect ratings for sentiment summarization.
In Proceedings of Annual Meeting of the Association
for Computational Linguistics, (ACL).
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the International
Conference on Machine Learning, (ICML).
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010.
Latent aspect rating analysis on review text data: A
rating regression approach. In Proceedings of the In-
ternational Conference on Knowledge Discovery and
Data Mining (KDD).
Ainur Yessenalina, Yisong Yue, and Claire Cardie. 2010.
Multi-level structured models for document-level sen-
timent classification. In Proceedings of Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural svms with latent variables. In Pro-
ceedings of the International Conference on Machine
Learning, (ICML).
A. L. Yuille and Anand Rangarajan. 2003. The
concave-convex procedure (cccp). Neural Computa-
tion, 15:915?936.
337
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 531?541,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
New Word Detection for Sentiment Analysis
Minlie Huang, Borui Ye*, Yichen Wang, Haiqiang Chen**, Junjun Cheng**, Xiaoyan Zhu
State Key Lab. of Intelligent Technology and Systems, National Lab. for Information Science
and Technology, Dept. of Computer Science and Technology, Tsinghua University, Beijing 100084, PR China
*Dept. of Communication Engineering, Beijing University of Posts and Telecommunications
**China Information Technology Security Evaluation Center
aihuang@tsinghua.edu.cn
Abstract
Automatic extraction of new words is
an indispensable precursor to many NLP
tasks such as Chinese word segmentation,
named entity extraction, and sentimen-
t analysis. This paper aims at extract-
ing new sentiment words from large-scale
user-generated content. We propose a ful-
ly unsupervised, purely data-driven frame-
work for this purpose. We design statisti-
cal measures respectively to quantify the
utility of a lexical pattern and to measure
the possibility of a word being a newword.
The method is almost free of linguistic re-
sources (except POS tags), and requires
no elaborated linguistic rules. We also
demonstrate how new sentiment word will
benefit sentiment analysis. Experiment re-
sults demonstrate the effectiveness of the
proposed method.
1 Introduction
New words on the Internet have been emerg-
ing all the time, particularly in user-generated con-
tent. Users like to update and share their infor-
mation on social websites with their own language
styles, among which new political/social/cultural
words are constantly used.
However, such new words have made many
natural language processing tasks more challeng-
ing. Automatic extraction of new words is indis-
pensable to many tasks such as Chinese word seg-
mentation, machine translation, named entity ex-
traction, question answering, and sentiment analy-
sis. New word detection is one of the most critical
issues in Chinese word segmentation. Recent stud-
ies (Sproat and Emerson, 2003) (Chen, 2003) have
shown that more than 60% of word segmentation
errors result from new words. Statistics show that
more than 1000 new Chinese words appear every
year (Thesaurus Research Center, 2003). These
words are mostly domain-specific technical terms
and time-sensitive political/social /cultural terms.
Most of them are not yet correctly recognized by
the segmentation algorithm, and remain as out of
vocabulary (OOV) words.
New word detection is also important for sen-
timent analysis such as opinionated phrase ex-
traction and polarity classification. A sentiment
phrase with complete meaning should have a cor-
rect boundary, however, characters in a new word
may be broken up. For example, in a sentence
" ??/ n ??/ adv ?/ v ?/ n?artists' perfor-
mance is very impressive?" the two Chinese char-
acters??/v?/n(cool; powerful)?should always
be extracted together. In polarity classification,
new words can be informative features for clas-
sification models. In the previous example, "?
?(cool; powerful)" is a strong feature for clas-
sification models while each single character is
not. Adding new words as feature in classification
models will improve the performance of polarity
classification, as demonstrated later in this paper.
This paper aims to detect new word for senti-
ment analysis. We are particulary interested in ex-
tracting new sentiment word that can express opin-
ions or sentiment, which is of high value toward-
s sentiment analysis. New sentiment word, as ex-
emplified in Table 1, is a sub-class of multi-word
expressions which is a sequence of neighboring
words "whose exact and unambiguous meaning
or connotation cannot be derived from the mean-
ing or connotation of its components" (Choueka,
1988). Such new words cannot be directly iden-
tified using grammatical rules, which poses a ma-
jor challenge to automatic analysis. Moreover, ex-
isting lexical resources never have adequate and
timely coverage since new words appear constant-
ly. People thus resort to statistical methods such as
Pointwise Mutual Information (Church and Han-
ks, 1990), Symmetrical Conditional Probability
531
(da Silva and Lopes, 1999), Mutual Expectation
(Dias et al, 2000), Enhanced Mutual Information
(Zhang et al, 2009), and Multi-word Expression
Distance (Bu et al, 2010).
New word English Translation Polarity
?? lovely positive
?? tragic/tragedy negative
?? very cool; powerful positive
?? reverse one's expectation negative
Table 1: Examples of new sentiment word.
Our central idea for new sentiment word de-
tection is as follows: Starting from very few seed
words (for example, just one seed word), we can
extract lexical patterns that have strong statistical
association with the seed words; the extracted lex-
ical patterns can be further used in finding more
new words, and the most probable new words can
be added into the seed word set for the next iter-
ation; and the process can be run iteratively un-
til a stop condition is met. The key issues are to
measure the utility of a pattern and to quantify the
possibility of a word being a new word. The main
contributions of this paper are summarized as fol-
lows:
? We propose a novel framework for new word
detection from large-scale user-generated da-
ta. This framework is fully unsupervised
and purely data-driven, and requires very
lightweight linguistic resources (i.e., only
POS tags).
? We design statistical measures to quantify the
utility of a pattern and to quantify the possi-
bility of a word being a newword, respective-
ly. No elaborated linguistic rules are needed
to filter undesirable results. This feature may
enable our approach to be portable to other
languages.
? We investigate the problem of polarity predic-
tion of new sentiment word and demonstrate
that inclusion of new sentiment word benefits
sentiment classification tasks.
The rest of the paper is structured as follows:
we will introduce related work in the next section.
Wewill describe the proposedmethod in Section 3,
including definitions, the overview of the algorith-
m, and the statistical measures for addressing the
two key issues. We then present the experiments
in Section 4. Finally, the work is summarized in
Section 5.
2 Related Work
New word detection has been usually inter-
weaved with word segmentation, particularly in
Chinese NLP. In these works, new word detection
is considered as an integral part of segmentation,
where new words are identified as the most proba-
ble segments inferred by the probabilistic models;
and the detected new word can be further used to
improve word segmentation. Typical models in-
clude conditional random fields proposed by (Peng
et al, 2004), and a joint model trained with adap-
tive online gradient descent based on feature fre-
quency information (Sun et al, 2012).
Another line is to treat new word detection as
a separate task, usually preceded by part-of-speech
tagging. The first genre of such studies is to lever-
age complex linguistic rules or knowledge. For
example, Justeson and Katz (1995) extracted tech-
nical terminologies from documents using a regu-
lar expression. Argamon et al (1998) segmented
the POS sequence of a multi-word into small POS
tiles, counted tile frequency in the new word and
non-new-word on the training set respectively, and
detected new words using these counts. Chen and
Ma (2002) employed morphological and statisti-
cal rules to extract Chinese new word. The sec-
ond genre of the studies is to treat new word de-
tection as a classification problem. Zhou (2005)
proposed a discriminative Markov Model to de-
tect new words by chunking one or more separat-
ed words. In (Li et al, 2005), new word detec-
tion was viewed as a binary classification problem.
However, these supervisedmodels requires not on-
ly heavy engineering of linguistic features, but also
expensive annotation of training data.
User behavior data has recently been explored
for finding new words. Zheng et al (2009) ex-
plored user typing behaviors in Sogou Chinese
Pinyin input method to detect new words. Zhang
et al (2010) proposed to use dynamic time warp-
ing to detect new words from query logs. Howev-
er, both of the work are limited due to the public
unavailability of expensive commercial resources.
Statistical methods for new word detection
have been extensively studied, and in some sense
exhibit advantages over linguistics-based method-
s. In this setting, new word detection is mostly
532
known as multi-word expression extraction. To
measure multi-word association, the first model
is Pointwise Mutual Information (PMI) (Church
and Hanks, 1990). Since then, a variety of sta-
tistical methods have been proposed to measure
bi-gram association, such as Log-likelihood (Dun-
ning, 1993) and Symmetrical Conditional Proba-
bility (SCP) (da Silva and Lopes, 1999). Among
all the 84 bi-gram association measures, PMI has
been reported to be the best one in Czech data
(Pecina, 2005). In order to measure arbitrary n-
grams, most common strategies are to separate n-
gram into two parts X and Y so that existing bi-
gram methods can be used (da Silva and Lopes,
1999; Dias et al, 2000; Schone and Jurafsky,
2001). Zhang et al (2009) proposed Enhanced
Mutual Information (EMI) which measures the co-
hesion of n-gram by the frequency of itself and the
frequency of each single word. Based on the in-
formation distance theory, Bu et al (2010) pro-
posed multi-word expression distance (MED) and
the normalized version, and reported superior per-
formance to EMI, SCP, and other measures.
3 Methodology
3.1 Definitions
Definition 3.1 (Adverbial word). Words that are
used mainly to modify a verb or an adjective, such
as "?(too)", "??(very)", "??(very)", and "?
?(specially)".
Definition 3.2 (Auxiliary word). Words that are
auxiliaries, model particles, or punctuation marks.
In Chinese, such words are like "?,?,?,?,?",
and punctuation marks include "??????" and
so on.
Definition 3.3 (Lexical Pattern). A lexical pat-
tern is a triplet < AD, ?, AU >, where AD is an
adverbial word, the wildcard ? means an arbitrary
number of words 1, and AU denotes an auxiliary
word.
Table 2 gives some examples of lexical pat-
terns. In order to obtain lexical patterns, we can
define regular expressions with POS tags 2 and ap-
ply the regular expressions on POS tagged texts.
Since the tags of adverbial and auxiliary words are
1We set the number to 3 words in this work considering
computation costs.
2Such expressions are very simple and easy to write be-
cause we only need to consider POS tags of adverbial and
auxiliary word.
relatively static and can be easily identified, such
a method can safely obtain lexical patterns.
Pattern Frequency
<"?",*,"?"> 562,057
<"?",*,"?"> 387,649
<"?",*,"?"> 380,470
<"?",*,"?"> 369,702
Table 2: Examples of lexical pattern. The frequen-
cy is counted on 237,108,977 Weibo posts.
3.2 The Algorithm Overview
The algorithm works as follows: starting
from very few seed words (for example, a word
in Table 1), the algorithm can find lexical pattern-
s that have strong statistical association with the
seed words in which the likelihood ratio test (L-
RT) is used to quantify the degree of association.
Subsequently, the extracted lexical patterns can be
further used in finding more new words. We de-
sign several measures to quantify the possibility of
a candidate word being a new word, and the top-
ranked words will be added into the seed word set
for the next iteration. The process can be run iter-
atively until a stop condition is met. Note that we
do not augment the pattern set (P) at each iteration,
instead, we keep a fixed small number of patterns
during iteration because this strategy produces op-
timal results.
From linguistic perspectives, new sentiment
words are commonly modified by adverbial words
and thus can be extracted by lexical patterns. This
is the reason why the algorithm will work. Our al-
gorithm is in spirit to double propagation (Qiu et
al., 2011), however, the differences are apparen-
t in that: firstly, we use very lightweight linguis-
tic information (except POS tags); secondly, our
major contributions are to propose statistical mea-
sures to address the following key issues: first, to
measure the utility of lexical patterns; second, to
measure the possibility of a candidate word being
a new word.
3.3 Measuring the Utility of a Pattern
The first key issue is to quantify the utility of
a pattern at each iteration. This can be measured
by the association of a pattern to the current word
set used in the algorithm. The likelihood ratio test-
s (Dunning, 1993) is used for this purpose. This
association model has also been used to model as-
sociation between opinion target words by (Hai et
533
Algorithm 1: New word detection algorithm
Input:
D: a large set of POS tagged posts
W
s
: a set of seed words
k
p
: the number of patterns chosen at each
iteration
k
c
: the number of patterns in the candidate
pattern set
k
w
: the number of words added at each
iteration
K: the number of words returned
Output: A list of ranked new wordsW
1 Obtain all lexical patterns using regular
expressions on D;
2 Count the frequency of each lexical pattern
and extract words matched by each pattern ;
3 Obtain top k
c
frequent patterns as candidate
pattern set P
c
and top 5,000 frequent words as
candidate word setW
c
;
4 P = ?;W=W
s
; t = 0 ;
5 for |W| < K do
6 UseW to score each pattern in P
c
with
U(p) ;
7 P = {top k
p
patterns} ;
8 Use P to extract new words and if the
words are inW
c
, score them with F (w) ;
9 W = W
?
{top k
w
words} ;
10 W
c
=W
c
-W ;
11 Sort words inW with F (w) ;
12 Output the ranked list of words inW ;
al., 2012).
The LRT is well known for not relying crit-
ically on the assumption of normality, instead, it
uses the asymptotic assumption of the generalized
likelihood ratio. In practice, the use of likelihood
ratios tends to result in significant improvements
in text-analysis performance.
In our problem, LRT computes a contingency
table of a pattern p and a word w, derived from
the corpus statistics, as given in Table 3, where
k
1
(w, p) is the number of documents thatwmatch-
es pattern p, k
2
(w, p?) is the number of documents
that w occurs while p does not, k
3
(w?, p) is the
number of documents that p occurs while w does
not, and k
4
(w?, p?) is the number of documents con-
taining neither p nor w.
Statistics p p?
w k
1
(w, p) k
2
(w, p?)
w? k
3
(w?, p) k
4
(w?, p?)
Table 3: Contingency table for likelihood ratio test
(LRT).
Based on the statistics shown in Table 3, the
likelihood ratio tests (LRT) model captures the sta-
tistical association between a pattern p and a word
w by employing the following formula:
LRT (p, w) = log
L(?
1
, k
1
, n
1
) ? L(?
2
, k
2
, n
2
)
L(?, k
1
, n
1
) ? L(?, k
2
, n
2
)
(1)
where:
L(?, k, n) = ?
k
? (1 ? ?)
n?k; n
1
= k
1
+ k
3
;
n
2
= k
2
+ k
4
; ?
1
= k
1
/n
1
; ?
2
= k
2
/n
2
; ? =
(k
1
+ k
2
)/(n
1
+ n
2
).
Thus, the utility of a pattern can be measured
as follows:
U(p) =
?
w
i
?W
LRT (p, w
i
) (2)
where W is the current word set used in the algo-
rithm (see Algorithm 1).
3.4 Measuring the Possibility of Being New
Words
Another key issue in the proposed algorithm
is to quantify the possibility of a candidate word
being a new word. We consider several factors for
this purpose.
3.4.1 Likelihood Ratio Test
Very similar to the pattern utility measure, L-
RT can also be used to measure the association of
a candidate word to a given pattern set, as follows:
LRT (w) =
?
p
i
?P
LRT (w, p
i
) (3)
where P is the current pattern set used in the algo-
rithm (see Algorithm 1), and p
i
is a lexical pattern.
This measure only quantifies the association
of a candidate word to the given pattern set. It
tells nothing about the possibility of a word be-
ing a new word, however, a new sentiment word,
should have close association with the lexical pat-
terns. This has linguistic interpretations because
new sentiment words are commonly modified by
adverbial words and thus should have close associ-
ation with lexical patterns. This measure is proved
to be an influential factor by our experiments in
Section 4.3.
534
3.4.2 Left Pattern Entropy
If a candidate word is a new word, it will be
more commonly used with diversified lexical pat-
terns since the non-compositionality of new word
means that the word can be used in many differ-
ent linguistic scenarios. This can be measured by
information entropy, as follows:
LPE(w) = ?
?
l
i
?L(P
c
,w)
c(l
i
, w)
N(w)
? log
c(l
i
, w)
N(w)
(4)
where L(P
c
, w) is the set of left word of all pat-
terns by which word w can be matched in P
c
,
c(l
i
, w) is the count that word w can be matched
by patterns whose left word is l
i
, and N(w) is the
count that word w can be matched by the patterns
in P
c
. Note that we use P
c
, instead of P , because
the latter set is very small while computing entropy
needs a large number of patterns. Tuning the size
of P
c
will be further discussed in Section 4.4.
3.4.3 New Word Probability
Some words occur very frequently and can be
widely matched by lexical patterns, but they are
not new words. For example, "??(love to eat)"
and "??(love to talk)" can be matched by many
lexical patterns, however, they are not new words
due to the lack of non-compositionality. In such
words, each single character has high probability
to be a word. Thus, we design the following mea-
sure to favor this observation.
NWP (w) =
n
?
i=1
p(w
i
)
1? p(w
i
)
(5)
where w = w
1
w
2
. . . w
n
, each w
i
is a single char-
acter, and p(w
i
) is the probability of the character
w
i
being a word, as computed as follows:
p(w
i
) =
all(w
i
)? s(w
i
)
all(w
i
)
where all(w
i
) is the total frequency of w
i
, and
s(w
i
) is the frequency of w
i
being a single char-
acter word. Obviously, in order to obtain the value
of s(w
i
), some particular Chinese word segmen-
tation tool is required. In this work, we resort to
ICTCLAS (Zhang et al, 2003), a widely used tool
in the literature.
3.4.4 Non-compositionality Measures
New words are usually multi-word expres-
sions, where a variety of statistical measures have
been proposed to detect multi-word expressions.
Thus, such measures can be naturally incorporated
into our algorithm.
The first measure is enhanced mutual infor-
mation (EMI) (Zhang et al, 2009):
EMI(w) = log
2
F/N
?
n
i=1
F
i
?F
N
(6)
where F is the number of posts in which a multi-
word expression w = w
1
w
2
. . . w
n
occurs, F
i
is
the number of posts where w
i
occurs, andN is the
total number of posts. The key idea of EMI is to
measure word pair?s dependency as the ratio of its
probability of being a multi-word to its probability
of not being amulti-word. The larger the value, the
more possible the expression will be a multi-word
expression.
The second measure we take into account is
normalized multi-word expression distance (Bu et
al., 2010), which has been proposed to measure the
non-compositionality of multi-word expressions.
NMED(w) =
log|?(w)| ? log|?(w)|
logN ? log|?(w)|
(7)
where ?(w) is the set of documents in which all
single words in w = w
1
w
2
. . . w
n
co-occur, ?(w)
is the set of documents in which word w occurs
as a whole, and N is the total number of docu-
ments. Different from EMI, this measure is a strict
distance metric, meaning that a smaller value in-
dicates a larger possibility of being a multi-word
expression. As can be seen from the formula, the
key idea of this metric is to compute the ratio of the
co-occurrence of all words in a multi-word expres-
sions to the occurrence of the whole expression.
3.4.5 Configurations to Combine Various
Factors
Taking into account the aforementioned fac-
tors, we have different settings to score a new
word, as follows:
F
LRT
(w) = LRT (w) (8)
F
LPE
(w) = LRT (w) ? LPE(w) (9)
F
NWP
(w) = LRT (w) ? LPE(w) ?NWP (w) (10)
F
EMI
(w) = LRT (w) ? LPE(w) ? EMI(w) (11)
F
NMED
(w) =
LRT (w) ? LPE(w)
NMED(w)
(12)
535
4 Experiment
In this section, we will conduct the following
experiments: first, we will compare our method
to several baselines, and perform parameter tun-
ing with extensive experiments; second, we will
classify polarity of new sentiment words using t-
wo methods; third, we will demonstrate how new
sentiment words will benefit sentiment classifica-
tion.
4.1 Data Preparation
We crawled 237,108,977 Weibo posts from
http://www.weibo.com, the largest social website
in China. These posts range from January of 2011
to December of 2012. The posts were then part-of-
speech tagged using a Chinese word segmentation
tool named ICTCLAS (Zhang et al, 2003).
Then, we asked two annotators to label the top
5,000 frequent words that were extracted by lexi-
cal patterns as described in Algorithm 1. The an-
notators were requested to judge whether a candi-
date word is a new word, and also to judge the po-
larity of a new word (positive, negative, and neu-
tral). If there is a disagreement on either of the
two tasks, discussions are required to make the fi-
nal decision. The annotation led to 323 new word-
s, among which there are 116 positive words, 112
negative words, and 95 neutral words3.
4.2 Evaluation Metric
As our algorithm outputs a ranked list of
words, we adapt average precision to evaluate
the performance of new sentiment word detection.
The metric is computed as follows:
AP (K) =
?
K
k=1
P (k) ? rel(k)
?
K
k=1
rel(k)
where P (k) is the precision at cut-off k, rel(k) is
1 if the word at position k is a new word and 0 oth-
erwise, andK is the number of words in the ranked
list. A perfect list (all topK items are correct) has
an AP value of 1.0.
4.3 Evaluation of Different Measures and
Comparison to Baselines
First, we assess the influence of likelihood ra-
tio test, which measures the association of a word
to the pattern set. As can be seen from Table 4,
the associationmodel (LRT) remarkably boosts the
3All the resources are available upon request.
performance of new word detection, indicating L-
RT is a key factor for new sentiment word extrac-
tion. From linguistic perspectives, new sentiment
words are commonly modified by adverbial words
and thus should have close association with lexical
patterns.
Second, we compare different settings of our
method to two baselines. The first one is en-
hanced mutual information (EMI) where we set
F (w) = EMI(w) (Zhang et al, 2009) and the
second baseline is normalized multi-word expres-
sion distance (NMED) (Bu et al, 2010) where we
set F (w) = NMED(w). The results are shown
in Figure 1. As can be seen, all the proposed
measures outperform the two baselines (EMI and
NMED) remarkably and consistently. The set-
ting of F
NMED
produces the best performance.
AddingNMED orEMI leads to remarkable im-
provements because of their capability of measur-
ing non-compositionality of new words. Only us-
ingLRT can obtain a fairly good results whenK is
small, however, the performance drops sharply be-
cause it's unable to measure non-compositionality.
Comparison between LRT + LPE (or LRT +
LPE + NWP ) and LRT shows that inclusion
of left pattern entropy also boosts the performance
apparently. However, the new word probabili-
ty (NWP ) has only marginal contribution to im-
provement.
In the above experiments, we set k
p
= 5 (the
number of patterns chosen at each iteration) and
k
w
= 10 (the number of words added at each iter-
ation), which is the optimal setting and will be dis-
cussed in the next subsection. And only one seed
word "??(reverse one's expectation)" is used.
Figure 1: Comparative results of different measure
settings. X-axis is the number of words returned
(K), and Y-axis is average precision (AP (K)).
536
top K words ? 100 200 300 400 500
LPE 0.366 0.324 0.286 0.270 0.259
LRT+LPE 0.743 0.652 0.613 0.582 0.548
LPE+NWP 0.467 0.400 0.350 0.330 0.320
LRT+LPE+NWP 0.755 0.680 0.612 0.571 0.543
LPE+EMI 0.608 0.551 0.519 0.486 0.467
LRT+LPE+EMI 0.859 0.759 0.717 0.662 0.632
LPE+NMED 0.749 0.690 0.641 0.612 0.576
LRT+LPE+NMED 0.907 0.808 0.741 0.723 0.699
Table 4: Results with vs. without likelihood ratio test (LRT).
4.4 Parameter Tuning
Firstly, we will show how to obtain the op-
timal settings of k
p
and k
w
. The measure setting
we take here is F
NMED
(w), as shown in Formula
(12). Again, we choose only one seed word "?
?(reverse one's expectation)", and the number of
words returned is set to K = 300. Results in Ta-
ble 5 show that the performance drops consistent-
ly across different k
w
settings when the number of
patterns increases. Note that at the early stage of
Algorithm 1, larger k
p
(perhaps with noisy pattern-
s) may lead to lower quality of new words; while
larger k
w
(perhaps with noisy seed words) may
lead to lower quality of lexical patterns. Therefore,
we choose the optimal setting to small numbers, as
k
p
= 5, k
w
= 10.
Secondly, we justify whether the proposed al-
gorithm is sensitive to the number of seed words.
We set k
p
= 5 and k
w
= 10, and take F
NMED
as the weighting measure of new word. We exper-
imented with only one seed word, two, three, and
four seed words, respectively. The results in Ta-
ble 6 show very stable performance when different
numbers of seed words are chosen. It's interesting
that the performance is totally the same with dif-
ferent numbers of seed words. By looking into the
pattern set and the selected words at each iteration,
we found that the pattern set (P) converges soon
to the same set after a few iterations; and at the be-
ginning several iterations, the selected words are
almost the same although the order of adding the
words is different. Since the algorithm will finally
sort the words at step (11) and P is the same, the
ranking of the words becomes all the same.
Lastly, we need to decide the optimal number
of patterns in P
c
(that is, k
c
in Algorithm 1) be-
cause the set has been used in computing left pat-
tern entropy, see Formula (4). Too small size of
P
c
may lead to insufficient estimation of left pat-
tern entropy. Results in Table 7 shows that larg-
er P
c
decrease the performance, particularly when
the number of words returned (K) becomes larger.
Therefore, we set |P
c
| = 100.
4.5 Polarity Prediction of New Sentiment
Words
In this section, we attempt to classifying the
polarity of the annotated 323 new words. Two
methods are adapted with different settings for this
purpose. The first one is majority vote (MV), and
the second one is pointwise mutual information,
similar to (Turney and Littman, 2003). The ma-
jority vote method is formulated as below:
MV (w) =
?
w
p
?PW
#(w,w
p
)
|PW |
?
?
w
n
?NW
#(w,w
n
)
|NW |
where PW and NW are a positive and negative
set of emoticons (or seed words) respectively, and
#(w,w
p
) is the co-occurrence count of the input
wordw and the itemw
p
. The polarity is judged ac-
cording to this rule: ifMV (w) > th
1
, the word w
is positive; ifMV (w) < ?th
1
the word negative;
otherwise neutral. The threshold th
1
is manually
tuned.
And PMI is computed as follows:
PMI(w) =
?
w
p
?PW
PMI(w,w
p
)
|PW |
?
?
w
n
?NW
PMI(w,w
n
)
|NW |
where PMI(x, y) = log
2
(
Pr(x,y)
Pr(x)?Pr(y)
), and
Pr(?) denotes probability. The polarity is judged
according to the rule: if PMI(w) > th
2
, w is
positive; if PMI(w) < ?th
2
negative; otherwise
neutral. The threshold th
2
is manually tuned.
As for the resources PW and NW , we
have three settings. The first setting (denoted by
537
HH
H
H
H
H
k
w
k
p 2 3 4 5 10 20 50
5 0.753 0.738 0.746 0.741 0.741 0.734 0.715
10 0.753 0.738 0.746 0.741 0.741 0.728 0.712
15 0.753 0.738 0.746 0.741 0.754 0.734 0.718
20 0.763 0.738 0.744 0.749 0.749 0.735 0.717
Table 5: Parameter tuning results for k
p
and k
w
. The measure setting is F
NMED
(w), the seed word set
is {"??(reverse one's expectation)"}, and the number of words returned isK = 300.
# seeds ? 1 2 3 4
K=100 0.907 0.907 0.907 0.907
K=200 0.808 0.808 0.808 0.808
K=300 0.741 0.741 0.741 0.741
K=400 0.709 0.709 0.709 0.709
K=500 0.685 0.685 0.685 0.685
Table 6: Performance with different numbers of
seed words. The measure setting is F
NMED
(w),
and k
p
= 5, k
w
= 10. The seed words are chosen
from Table 1.
Large_Emo) is a set of most frequent 36 emoticons
in which there are 21 positive and 15 negative e-
moticons respectively. The second one (denoted
by Small_Emo) is a set of 10 emoticons, which
are chosen from the 36 emoticons, as shown in
Table 8. The third one (denoted by Opin_Words)
is two sets of seed opinion words, where PW={
??(happy),??(generous),??(beautiful), ?
?(kind),??(smart)} and NW ={??(sad),?
?(mean),??(ugly),??(wicked),?(stupid)}.
The performance of polarity prediction is
shown in Table 9. In two-class polarity classifi-
cation, we remove neutral words and only make
prediction with positive/negative classes. The first
observation is that the performance of using emoti-
cons is much better than that of using seed opin-
ion words. We conjecture that this may be be-
cause new sentiment words are more frequently
co-occurring with emoticons than with these opin-
ion words. The second observation is that three-
class polarity classification is much more diffi-
cult than two-class polarity classification because
many extracted new words are nouns such as "?
?(gay)","??(girl)", and "??(friend)". Such
nouns are more difficult to classify sentiment ori-
entation.
4.6 Application of New Sentiment Words to
Sentiment Classification
In this section, we justifywhether inclusion of
new sentiment word would benefit sentiment clas-
sification. For this purpose, we randomly sampled
and annotated 4,500 Weibo posts that contain at
least one opinion word in the union of the Hownet
4 opinion lexicons and our annotated new word-
s. We apply two models for polarity classification.
The first model is a lexicon-based model (denot-
ed by Lexicon) that counts the number of positive
and negative opinion words in a post respective-
ly, and classifies a post to be positive if there are
more positive words than negative ones, and to be
negative otherwise. The second model is a SVM
model in which opinion words are used as feature,
and 5-fold cross validation is conducted.
We experiment with different settings of
Hownet lexicon resources:
? Hownet opinion words (denoted by Hownet):
After removing some obviously inappropri-
ate words, the left lexicons have 627 posi-
tive opinion words and 1,038 negative opin-
ion words, respectively.
? Compact Hownet opinion words (denoted by
cptHownet): we count the frequency of the
above opinion words on the training data and
remove words whose document frequency is
less than 2. This results in 138 positive words
and 125 negative words.
Then, we add into the above resources the la-
beled new polar words(denoted byNW , including
116 positive and 112 negative words) and the top
100 words produced by the algorithm (denoted by
T100), respectively. Note that the lexicon-based
model requires the sentiment orientation of each
dictionary entry 5, we thus manually label the po-
4http://www.keenage.com/html/c_index.html.
5This is not necessary for the SVM model. All words in
the top 100 words can be used as feature.
538
|P
c
| ? 50 100 200 300 400 500
K=100 0.907 0.905 0.916 0.916 0.888 0.887
K=200 0.808 0.810 0.778 0.776 0.766 0.764
K=300 0.741 0.731 0.722 0.726 0.712 0.713
K=400 0.709 0.708 0.677 0.675 0.656 0.655
K=500 0.685 0.683 0.653 0.646 0.626 0.627
Table 7: Tuning the number of patterns in P
c
. The measure setting is F
NMED
(w), k
p
= 5, k
w
= 10,
and the seed word set is {"??(reverse one's expectation)"}.
Emoticon Polarity Emoticon Polarity
positive negative
positive negative
positive negative
positive negative
positive negative
Table 8: The ten emoticons used for polarity pre-
diction.
Methods? Majority vote PMI
Two-class polarity classification
Large_Emo 0.861 0.865
Small_Emo 0.846 0.851
Opin_Words 0.697 0.654
Three-class polarity classification
Large_Emo 0.598 0.632
Small_Emo 0.551 0.635
Opin_Words 0.449 0.486
Table 9: The accuracy of two/three-class polarity
classification.
larity of all top 100 words (we did NOT remove
incorrect new word). This results in 52 positive
and 34 negative words.
Results in Table 10 show that inclusion of
new words in both models improves the perfor-
mance remarkably. In the setting of the original
lexicon (Hownet), both models obtain 2-3% gains
from the inclusion of newwords. Similar improve-
ment is observed in the setting of the compact lex-
icon. Note, that T100 is automatically obtained
from Algorithm 1 so that it may contain words that
are not new sentiment words, but the resource also
improves performance remarkably.
5 Conclusion
In order to extract new sentiment words from
large-scale user-generated content, this paper pro-
poses a fully unsupervised, purely data-driven, and
# Pos/Neg Lexicon SVM
Hownet 627/1,038 0.737 0.756
Hownet+NW 743/1,150 0.770 0.779
Hownet+T100 679/1,172 0.761 0.774
cptHownet 138/125 0.738 0.758
cptHownet+NW 254/237 0.774 0.782
cptHownet+T100 190/159 0.764 0.775
Table 10: The accuracy of polarity classfication of
Weibo post with/without new sentiment words. N-
W includes 116/112 positive/negative words, and
T100 contains 52/34 positive/negative words.
almost knowledge-free (except POS tags) frame-
work. We design statistical measures to quantify
the utility of a lexical pattern and to measure the
possibility of a word being a new word, respec-
tively. The method is almost free of linguistic re-
sources (except POS tags), and does not rely on
elaborated linguistic rules. We conduct extensive
experiments to reveal the influence of different sta-
tistical measures in new word finding. Compara-
tive experiments show that our proposed method
outperforms baselines remarkably. Experiments
also demonstrate that inclusion of new sentiment
words benefits sentiment classification definitely.
From linguistic perspectives, our framework
is capable to extract adjective new words because
the lexical patterns usually modify adjective word-
s. As future work, we are considering how to ex-
tract other types of new sentiment words, such as
nounal new words that can express sentiment.
Acknowledgments
This work was partly supported by the fol-
lowing grants from: the National Basic Re-
search Program (973 Program) under grant No.
2012CB316301 and 2013CB329403, the National
Science Foundation of China project under grant
No. 61332007 and No. 60803075, and the Beijing
Higher Education Young Elite Teacher Project.
539
References
Shlomo Argamon, Ido Dagan, and Yuval Krymolows-
ki. 1998. A memory-based approach to
learning shallow natural language patterns. In
Proceedings of the 17th International Conference
on Computational Linguistics - Volume 1, COL-
ING '98, pages 67--73, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Fan Bu, Xiaoyan Zhu, and Ming Li. 2010. Measuring
the non-compositionality of multiword expres-
sions. In Proceedings of the 23rd International
Conference on Computational Linguistics, COL-
ING '10, pages 116--124, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Keh-Jiann Chen and Wei-Yun Ma. 2002. Un-
known word extraction for chinese documents. In
Proceedings of the 19th International Conference
on Computational Linguistics - Volume 1, COL-
ING '02, pages 1--7, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Aitao Chen. 2003. Chinese word segmentation us-
ingminimal linguistic knowledge. In Proceedings
of the Second SIGHAN Workshop on Chinese
Language Processing - Volume 17, SIGHAN '03,
pages 148--151, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Yaacov Choueka. 1988. Looking for nee-
dles in a haystack or locating interesting col-
location expressions in large textual databas-
es. In Proceeding of the RIAO'88 Conference
on User-Oriented Content-Based Text and Image
Handling, pages 21--24.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lex-
icography. Comput. Linguist., 16(1): 22--29,
March.
J Ferreira da Silva and G Pereira Lopes. 1999. A local
maxima method and a fair dispersion normaliza-
tion for extracting multi-word units from corpora.
In Sixth Meeting on Mathematics of Language,
pages 369--381.
Ga?l Dias, Sylvie Guillor?, and Jos? Gabriel Pereira
Lopes. 2000. Mining textual associations in text
corpora. 6th ACM SIGKDD Work. Text Mining.
TedDunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Comput. Linguist.,
19(1):61--74, March.
Zhen Hai, Kuiyu Chang, and Gao Cong. 2012.
One seed to find them all: Mining opinion fea-
tures via association. In Proceedings of the 21st
ACM International Conference on Information
and Knowledge Management, CIKM '12, pages
255--264, New York, NY, USA. ACM.
John S Justeson and SlavaMKatz. 1995. Technical ter-
minology: some linguistic properties and an algo-
rithm for identification in text. Natural language
engineering, 1(1):9--27.
Hongqiao Li, Chang-Ning Huang, Jianfeng Gao, and
Xiaozhong Fan. 2005. The use of svm for
chinese new word identification. In Natural
Language Processing--IJCNLP 2004, pages 723-
-732. Springer.
Pavel Pecina. 2005. An extensive empirical study of
collocation extraction methods. In Proceedings
of the ACL Student ResearchWorkshop, ACLstu-
dent '05, pages 13--18, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Fuchun Peng, Fangfang Feng, and Andrew McCal-
lum. 2004. Chinese segmentation and new
word detection using conditional random field-
s. In Proceedings of the 20th International
Conference on Computational Linguistics, COL-
ING '04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extrac-
tion through double propagation. Computational
linguistics, 37(1):9--27.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dic-
tionary headwords a solved problem. In Proc.
of the 6th Conference on Empirical Methods in
Natural Language Processing (EMNLP 2001),
pages 100--108.
Richard Sproat and Thomas Emerson. 2003. The first
international chinese word segmentation bakeoff.
In Proceedings of the Second SIGHANWorkshop
on Chinese Language Processing - Volume 17,
SIGHAN '03, pages 133--143, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Xu Sun, Houfeng Wang, and Wenjie Li. 2012.
Fast online training with frequency-adaptive
learning rates for chinese word segmentation
and new word detection. In Proceedings of
the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers -
Volume 1, ACL '12, pages 253--262, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Beijing Thesaurus Research Center. 2003. Xinhua Xin
Ciyu Cidian. Commercial Press, Beijing.
Peter D. Turney and Michael L. Littman. 2003. Mea-
suring praise and criticism: Inference of seman-
tic orientation from association. ACM Trans. Inf.
Syst., 21(4):315--346, October.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer
ictclas. In Proceedings of the Second SIGHAN
Workshop on Chinese Language Processing -
540
Volume 17, SIGHAN '03, pages 184--187,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Wen Zhang, Taketoshi Yoshida, Xijin Tang, and Tu-
Bao Ho. 2009. Improving effectiveness of
mutual information for substantival multiword
expression extraction. Expert Systems with
Applications, 36(8):10919--10930.
Yan Zhang, Maosong Sun, and Yang Zhang. 2010.
Chinese new word detection from query logs. In
Advanced Data Mining and Applications, pages
233--243. Springer.
Yabin Zheng, Zhiyuan Liu, Maosong Sun, Liyun Ru,
and Yang Zhang. 2009. Incorporating user be-
haviors in new word detection. In Proceedings of
the 21st International Jont Conference onArtifical
Intelligence, IJCAI'09, pages 2101--2106, San
Francisco, CA, USA.Morgan Kaufmann Publish-
ers Inc.
GuoDong Zhou. 2005. A chunking strategy towards
unknownword detection in chinese word segmen-
tation. In Natural Language Processing--IJCNLP
2005, pages 530--541. Springer.
541
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 10?18,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Recognizing Biomedical Named Entities using Skip-chain Conditional
Random Fields
Jingchen Liu Minlie Huang? Xiaoyan Zhu
Department of Computer Science and Technology
Tsinghua University, Beijing 100084, China
liu-jc04@mails.tsinghua.edu.cn
{aihuang, zxy-dcs}@tsinghua.edu.cn
Abstract
Linear-chain Conditional Random Fields
(CRF) has been applied to perform the
Named Entity Recognition (NER) task in
many biomedical text mining and infor-
mation extraction systems. However, the
linear-chain CRF cannot capture long dis-
tance dependency, which is very common
in the biomedical literature. In this pa-
per, we propose a novel study of capturing
such long distance dependency by defin-
ing two principles of constructing skip-
edges for a skip-chain CRF: linking sim-
ilar words and linking words having typed
dependencies. The approach is applied to
recognize gene/protein mentions in the lit-
erature. When tested on the BioCreAtIvE
II Gene Mention dataset and GENIA cor-
pus, the approach contributes significant
improvements over the linear-chain CRF.
We also present in-depth error analysis on
inconsistent labeling and study the influ-
ence of the quality of skip edges on the la-
beling performance.
1 Introduction
Named Entity Recognition (NER) is a key task in
most text mining and information extraction sys-
tems. The improvement in NER can benefit the
final system performance. NER is a challenging
task, particularly in the biomedical literature due
to the variety of biomedical terminologies and the
complicated syntactic structures.
Many studies have been devoted to biomedical
NER. To evaluate biomedical NER systems, sev-
eral challenge competitions had been held, such
as BioNLP/NLPBA in 20041, BioCreAtIvE I in
? Corresponding author
1http://research.nii.ac.jp/?collier/
workshops/JNLPBA04st.htm
2004 and BioCreAtIvE II in 20062. The overview
reports from these competitions, presenting state-
of-the-art of biomedical NER studies, show that
linear-chain Conditional Random Fields (CRF) is
one of the most commonly used models and has
the most competitive results (Yeh et al, 2005;
Smith et al, 2008). Linear-chain CRF has also
been successfully applied to other NLP tasks such
as POS-tagging (Lafferty et al, 2001) and sen-
tence chunking (Sha and Pereira, 2003). However,
in most of these applications, only linear-chain
CRF was fully exploited, assuming that only adja-
cent words are inter-dependent. The dependency
between distant words, which occurs frequently in
the biomedical literature, is yet to be captured.
In the biomedical literature, the repeated ap-
pearance of same or similar words in one sentence
is a common type of long distance dependencies.
This phenomenon is due to the complicated syn-
tactic structures and the various biomedical termi-
nologies in nature. See the following example:
?Both GH deficiency and impaired
spinal growth may result in short
stature, whereas the occurrence of early
puberty in association with GH defi-
ciency reduces the time available for
GH therapy.?
the mentions of GH are repeated three times. If
the entity are referred by a pronoun, the meaning
of the sentence will be confusing and unclear be-
cause of the complex sentence structure. In this
sentence:
?These 8-oxoguanine DNA glycosy-
lases, hOgg1 (human) and mOgg1
(murine) , are homologous to each other
and to yeast Ogg1.?
the words hOgg1, mOgg1 and Ogg1 are homolo-
gous genes belonging to different species, having
2http://www.biocreative.org/
10
very similar entity names. Some other types of
long distance dependencies also occur frequently
in the biomedical literature. For example, in this
sentence
?Western immunoblot analysis detected
p55gag and its cleavage products p39
and p27 in purified particles derived by
expression of gag and gag-pol, respec-
tively.?
the words p55gag, p39 and p27 conjuncted by
and, have similar semantic meanings but they are
separated by several tokens. A human curator
can easily recognize such long distance dependen-
cies and annotate these words consistently. How-
ever, when applying the linear-chain CRF, incon-
sistency errors in annotating these entities could
happen due to the inability of representing long
distance dependency.
In this paper, we present an approach of cap-
turing long distance dependencies between words.
We adopte the skip-chain CRF to improve the per-
formance of gene mention recognition. We de-
fine two principles of connecting skip-edges for
skip-chain CRF to capture long distance depen-
dencies. The efficacy of the principles is inves-
tigated with extensive experiments. We test our
method on two data sets and significant improve-
ments are observed over the linear-chain CRF. We
present in-depth error analysis on inconsistent la-
beling. We also investigat whether the quality of
connected edges affect the labeling performance.
The remainder of this paper is organized as fol-
lows: We survey related studies in Section 2. We
introduce linear-chain CRF and skip-chain CRF in
Section 3. The method of connecting skip-chain
edges is described in Section 4 . In Section 5 we
present our experiments and in-depth analysis. We
summarize our work in Section 6.
2 Related work
NER is a widely studied topic in text mining
research, and many new challenges are seen in
domain-specific applications, such as biomedical
NER (Zhou et al, 2004). The dictionary based
method is a common technique as biomedical the-
sauruses play a key role in understanding such
text. Most dictionary based NER systems fo-
cused on: (1) integrating and normalizing differ-
ent biomedical databases to improve the quality of
the dictionary to be used; (2) improving matching
strategies that are more suitable for biomedical ter-
minologies; and (3) making filtering rules for post-
processing to refine the matching results or to ad-
just the boundary of entities, see (Fukuda et al,
1998; Narayanaswamy et al, 2003; Yang et al,
2008). Many information extraction systems had
a dictionary matching module to perform prelim-
inary detection of named entities (Schuhmann et
al., 2007; Kolarik et al, 2007; Wang et al, 2010).
Applying machine learning techniques gener-
ally obtains superior performance for the biomedi-
cal NER task. The automated learning process can
induce patterns for recognizing biomedical names
and rules for pre- and post-processing. Gener-
ally speaking, there are two categories of ma-
chine learning based methods: one treats NER as
a classification task, while the other treats NER
as a sequence labeling task. For the first cate-
gory, Support Vector Machine (SVM) was a com-
monly adopted model (Kazama et al, 2002; Zhou
et al, 2004). Lee et al (2004) proposed a two-
step framework to perform biomedical NER using
SVM: firstly detecting the boundaries of named
entities using classifiers; secondly classifying each
named entity into predefined target types. For the
second category, a sentence was treated as a se-
quence of tokens and the objective was to find the
optimal label sequence for these tokens. The label
space was often defined as {B,I,O}, where B in-
dicates the beginning token of an entity, I denotes
the continuing token and O represents the token
outside an entity. The sequence labeling task can
be approached by Hidden Markov Model (HMM),
Conditional Random Field (CRF) , or a combina-
tion of different models (Zhou et al, 2005; Tatar
and Cicekli, 2009).
Since proposed in (Lafferty et al, 2001), CRF
has been applied to many sequence labeling
tasks, including recognizing gene mentions from
biomedical text (McDonald and Pereira, 2005).
The Gene Mention Recognition task was included
in both BioCreAtIvE I and BioCreAtIvE II chal-
lenges. CRF had been used in most of top per-
forming systems in the Gene Mention Recognition
task of BioCreAtIvE II (Smith et al, 2008). Some
novel use of linear-chain CRF was proposed. For
example, in (Kuo et al, 2007) labeling was per-
formed in forward and backward directions on the
same sentence and results were combined from
the two directions. Huang et al (2007) com-
bines a linear-chain CRF and two SVM models
11
to enhance the recall. Finkel et al (2005) used
Gibbs Sampling to add non-local dependencies
into linear-chain CRF model for information ex-
traction. However, the CRF models used in these
systems were all linear-chain CRFs. To the best of
our knowledge, no previous work has been done
on using non-linear-chain CRF in the biomedical
NER task.
Beyond the biomedical domain, skip-chain
CRF has been used in several studies to model
long distance dependency. In (Galley, 2006), skip
edges were linked between sentences with non-
local pragmatic dependencies to rank meetings.
In (Ding et al, 2008), skip-chain CRF was used
to detect the context and answers from online fo-
rums. The most close work to ours was in (Sut-
ton and McCallum, 2004), which used skip-chain
CRF to extract information from email messages
announcing seminars. By linking the same words
whose initial letter is capital, the method obtained
improvements on extracting speakers? name. Our
work is in the spirit of this idea, but we approach
it in a different way. We found that the problem is
much more difficult in the biomedical NER task:
that is why we systematically studied the princi-
ples of linking skip edges and the quality of con-
nected edges.
3 linear-chain and skip-chain CRF
Conditional Random Field is a probabilistic
graphic model. The model predicts the output
variables y for each input variables in x by calcu-
lating the conditional probability p(y|x) accord-
ing to the graph structure that represents the de-
pendencies between the y variables. Formally,
given a graph structure over y, the CRF model can
be written as:
p(y|x) =
1
Z(x)
?
Cp??
?
?c?Cp
?c(xc,yc; ?p) (1)
Z(x) is a normalization factor.
In this definition, the graph is partitioned into a
set of cliques ? = {C1, C2, . . . Cp}, where each
Cp is a clique template. Each ?c, called a factor,
is corresponding to one edge in the clique c, and
can be parameterized as:
?c(xc,yc; ?p) = exp
?
k=1
?pkfpk(xc,yc) (2)
Each feature function fpk(xc,yc) represents one
feature of x and the ?pk is the feature weight.
In the training phrase, the parameters is esti-
mated using an optimization algorithm such as
limited memory BFGS etc. In the testing phrase,
CRF finds the most likely label sequence for an
unseen instance by maximizing the probability de-
fined in (1).
In the NER task, one sentence is firstly tok-
enized into a sequences of tokens and each token
can be seen as one word. Each node in the graph is
usually corresponding to one word in a sentence.
Each x variable represents a set of features for one
word, and each y is the variable for the label of
one word. Note that when one edge is linked be-
tween two words, the edge is actually linked be-
tween their corresponding y variables. The y label
is one of {B,I,O}, in which B means the beginning
word of an entity, I means the inside word of an
entity, and O means outside an entity.
If we link each word with its immediate preced-
ing words to form a linear structure for one sen-
tence, we get a linear-chain CRF, defined as:
p?(y|x) =
1
Z(x)
T?
t=1
?t(yt, yt?1,x) (3)
This structure contains only one clique template.
If we add an extra clique template that contains
some skip edges between nonadjacent words, the
CRF become a skip-chain CRF, formulated as fol-
lows:
p?(y|x) =
1
Z(x)
T?
t=1
?t(yt, yt?1,x)?
?
(u,v)??
?uv(yu, yv,x) (4)
? is the edge set of the extra clique template con-
taining skip edges. An illustration of linear-chain
and skip-chain CRF is given in Figure 1. It is
straightforward to change a linear-chain CRF to
a skip-chain CRF by simply linking some addi-
tional skip edges. However, it must be careful to
add such edges because different graph structures
require different inference algorithms. Those in-
ference algorithms may have quite different time
complexity. For example, for the linear-chain
CRF, inference can be performed efficiently and
exactly by a dynamic-programming algorithm.
However, for the non-linear structure, approxi-
mate inference algorithms must be used. Solv-
ing arbitrary CRF graph structures is NP-hard. In
other word, we must be careful to link too many
12
Figure 1: The illustration of linear-chain CRF and skip-chain CRF. The blue edges represent the linear-
chain edges belonging to one clique template, while the red edges represent the skip edges belonging to
another clique template.
skip edges to avoid making the model impracti-
cal. Therefore, it is absolutely necessary to study
which kinds of edges will contribute to the perfor-
mance while avoiding over-connected edges.
3.1 Features
As our interest is in modifying the CRF graph
structure rather than evaluating the effectiveness
of features, we simply adopted features from the
state-of-the-art such as (McDonald and Pereira,
2005) and (Kuo et al, 2007).
? Common Features: the original word, the
stemmed word, the POS-tag of a word, the
word length, is or not the beginning or end-
ing word of the sentence etc.
? Regular Expression Features: a set of reg-
ular expressions to extract orthographic fea-
tures for the word.
? Dictionary Features: We use several lexi-
cons. For example, a protein name dictionary
compiled from SWISS-PROT, a species dic-
tionary from NCBI Taxonomy, a drug name
dictionary from DrugBank database, and a
disease name dictionary from several Internet
web site.
? N-gram Features: For each token, we ex-
tract the corresponding 2-4 grams into the
feature set.
Each word will include the adjacent words? fea-
tures within {?2,?1, 0, 1, 2} offsets. The features
used in the linear-chain CRF and skip-chain CRF
are all the same in our experiment.
4 Method
As the limitations discussed above, detecting
the necessary nodes to link should be the first
step in constructing a skip-chain CRF. In the
speaker name extraction task (Sutton and Mc-
Callum, 2004), only identical capitalized words
are linked, because there is few variations in the
speaker?s name. However, gene mentions often
involve words without obvious orthographic fea-
tures and such phenomena are common in the
biomedical literature such as RGC DNA sequence
and multisubunit TFIID protein. If we link all
the words like DNA, sequence and protein, the ef-
ficiency and performance will drop due to over-
connected edges. Therefore, the most important
step of detecting gene mentions is to determine
which edges should be connected.
4.1 Detect keywords in gene mention
We found that many gene mentions have at least
one important word for the identification of gene
mentions. For example, the word, Gal4, is such a
13
keyword in Gal4 protein and NS1A in NS1A pro-
tein. These words can distinguish gene mentions
from other common English words and phrases,
and can distinguish different gene mentions as
well. We define such words as the keyword of
a gene mention. The skip edges are limited to
only connect these keywords. We use a rule-based
method to detect keywords. By examining the an-
notated data, we defined keywords as those con-
taining at least one capital letter or digit. And at
the same time, keywords must conform to the fol-
lowing rules:
? Keywords are not stop words, single letters,
numbers, Greek letters, Roman numbers or
nucleotide sequence such as ATTCCCTGG.
? Keywords are not in the form of an upper-
case initial letter followed by lowercase let-
ters, such as Comparison and Watson. These
words have capital letters only because they
are the first word in the sentences, or they are
the names of people or other objects. This
rule will miss some correct candidates, but
reduces noise.
? Keywords do not include some common
words with capital letters such as DNA,
cDNA, RNA, mRNA, tRNA etc. and some fre-
quently appearing non-gene names such as
HIV and mmHg. We defined a lexicon for
such words on the training data.
4.2 Link similar keywords
After keyword candidates are detected, we judge
each pair of keywords in the same sentence to find
similar word pairs. Each word pair is examined by
these rules:
? They are exactly the same words.
? Words only differ in digit letters, such as
CYP1 and CYP2.
? Words with the same prefix, such as IgA and
IgG, or with the same suffix, such as ANF and
pANF.
The token pair will be linked by a skip edge if they
match at least one rule.
4.3 Link typed dependencies
Some long distance dependency cannot be de-
tected simply by string similarity. To capture such
dependency, we used stanford parser3 to parse sen-
tences and extract typed dependencies from parsed
results. The typed dependencies are a set of bi-
nary relations belonging to 55 pre-defined types to
provide a description of the grammatical relation-
ships in a sentence (Marneffe and Manning, 2008).
Some examples of typed dependencies are listed in
Table 1.
Type Description
conj conjuncted by the conjunc-
tion such as and
prep prepositional modifier
nn noun compound modifier
amod adjectival modifier
dep uncertain types
Table 1: Examples for typed dependencies.
The output of the parser is pairs of dependent
words, along with typed dependencies between
two words in a pair. For example, in the sentence:
?. . . and activate transcription of a set
of genes that includes G1 cyclins CLN1,
CLN2, and many DN, synthesis genes.?
a typed dependency nn(G1,CLN1) is extracted by
the parser, meaning the words G1 and CLN1 has a
typed dependency of nn because they form a noun
phrase under a dependency grammar: modifica-
tion. Similarly, in the sentence
?Using the same approach we have
shown that hFIRE binds the stimula-
tory proteins Sp1 and Sp3 in addition to
CBF.?
the words Sp1 and Sp3 can be detected to have a
typed dependency of conj and, and the two words
have a typed denpendency of prep in addition to
with CBF, respectively. The most common type
dependencies are conj and, nn and dep. The key-
words having typed dependencies will be linked
by a skip edge.
5 Experiment
We tested our method on two datasets: the Gene
Mention (GM) data in BioCreAtIvE II (BCIIGM)
3http://nlp.stanford.edu/software/
lex-parser.shtml
14
4and GENIA corpus5. The BCIIGM dataset was
used in the BioCreAtIvE II Gene Mention Recog-
nition task in 2006. It was built from the GENE-
TAG corpus (Tanabe et al, 2005) with some mod-
ification of the annotation. The dataset contains
15000 sentences for training and 5000 sentences
for testing. Two gold-standard sets, GENE and
ALTGENE, were provided for evaluation and an
official evaluation procedure in Perl script was
provided. The ALTGENE set provides alternate
forms for genes in the GENE set. In the official
evaluation, each identified string will be looked up
in both GENE and ALTGENE. If the correspond-
ing gene was found in either GENE or ALTGENE,
the identified string will be counted as a correct
answer.
The GENIA corpus is a widely used dataset in
many NER and information extraction tasks due
to its high quality annotation. The GENIA corpus
contains 2000 abstracts from MEDLINE, with ap-
proximately 18500 sentences. The corpus was an-
notated by biomedical experts according to a pre-
defined GENIA ontology. In this work, we only
used the annotated entities that have a category of
protein, DNA, or RNA. These categories are re-
lated to the definition of gene mention in BioCre-
AtIvE II. We only used strict matching evaluation
(no alternate forms check) for the GENIA corpus
as no ALTGENE-like annotation is available.
The performance is measured by precision, re-
call and F score. Each identified string is counted
as a true positive (TP) if it is matched by a gold-
standard gene mention, otherwise the identified
string is a false positive (FP). Each gold standard
gene mention is counted as a false negative (FN) if
it is not identified by the approach. Then the pre-
cision, recall and their harmonic average F score
is calculated as follows:
precision =
TP
TP + FP
recall =
TP
TP + FN
F =
2 ? precision ? recall
precision+ recall
To implement both linear-chain CRF and skip-
4http://sourceforge.net/projects/
biocreative/files/
5http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/home/wiki.cgi?page=Technical+Term+
Annotation
chain CRF, we used the GRMM Java package6
which is an extended version of MALLET. The
package provides an implement of arbitrary struc-
ture CRF.
5.1 Result Comparison
We evaluated our approach on the BCIIGM
dataset and GENIA corpus. For the BCIIGM
dataset, two evaluation criteria were used: official
- exactly the same as that used in the BioCreAtIvE
II competition, with the official evaluation proce-
dure; and strict - strict matching for each identi-
fied string without checking its alternate forms in
ALTGENE. The GENIA dataset were randomly
divided into 10 parts to perform a 10-fold cross
validation. However, we didn?t do cross validation
on the BCIIGM dataset because the BioCreAtIvE
II competition annotations and evaluation proce-
dure were tailored to evaluating participating sys-
tems.
The comparative results are listed in Table 2.
We compared the two edge linking principles,
linking similar words and linking words having
typed dependencies. The F score from the skip-
chain CRF is better than that from the linear-chain
CRF. Significance tests were performed to check
whether these results have significant differences.
Paired two-tail t-tests were conducted with respect
to the F scores of linear-chain CRF vs. those of the
two skip-chain CRFs, respectively. The p-value
was 1.989?10?7 for the skip-chain CRF linked by
similar words vs. linear-chain CRF. The p-value
was 3.971 ? 10?5 for the skip-chain CRF linked
by typed dependencies vs. linear-chain CRF. This
shows that the improvement is significant.
Note that we did not compare our results on the
BCIIGM dataset to those submitted to the compe-
tition. There are two reasons for this: First, our
focus is on comparing the skip-chain CRF with
the linear-chain CRF. Second, in the competition,
most participating systems that used CRF also
applied other algorithms, or sophisticated rules
for adjusting detected boundaries or refining the
recognized results, to achieve competitive perfor-
mance. By contrast, we did not employ any post-
processing rule or algorithm to further improve the
performance. In this sense, comparing our results
to those has become unfair.
6http://mallet.cs.umass.edu/grmm/
index.php
15
Data Model Precision(%) Recall(%) F score(%)
BCIIGM official
linear-chain CRF 85.16 81.50 83.29
skip-chain CRF linked by sim-words 86.68 82.75 84.67
skip-chain CRF linked by typed-dep 86.73 82.36 84.49
BCIIGM strict
linear-chain CRF 74.09 69.49 71.73
skip-chain CRF linked by sim-words 76.26 71.53 73.82
skip-chain CRF linked by typed-dep 75.99 70.49 73.14
GENIA
linear-chain CRF 76.77 74.92 75.83
skip-chain CRF linked by sim-words 78.57 77.12 77.82
skip-chain CRF linked by typed-dep 78.18 76.87 77.52
Table 2: The result comparison between the linear-chain CRF and skip-chain CRF. BCIIGM is the
BioCreAtIvE II Gene Mention Recognition dataset. official means using the official provided evalua-
tion procedure and strict means using strict matching to evaluate the results. sim-words means similar
words and typed-dep means typed dependencies. The results for GENIA are averaged over 10-fold cross
validation.
5.2 Discussion
We provided in-depth analysis of our results on the
BCIIGM dataset. As one of our motivations for
connecting words with skip edges is to enhance
the consistency of labeling, we firstly examined
whether the proposed approach can provide con-
sistent labeling. Let us start from two typical ex-
amples. In the first sentence
?The response sequences were localized
between -67 and +30 in the simian cy-
tomegalovirus IE94 promoter and up-
stream of position +9 in the HCMV IE68
promoter.?
the word IE94 is missed (not labeled) while its
similar word IE68 is labeled correctly by the
linear-chain CRF. In the second sentence
?It is suggested that biliary secretion of
both TBZ and FBZ and their metabolites
may contribute to this recycling.?
the word TBZ is labeled as a gene mention in-
correctly (false positive) while its similar word
FBZ is not labeled at all (true negative) by the
linear-chain CRF. Both sentences are correctly la-
beled by the skip-chain CRF. Similar improve-
ments are also made by the skip-chain CRF model
linked by typed dependencies. To study label-
ing consistency, we counted the statistics of in-
consistency errors, as shown in Table 3. Two
kinds of inconsistency errors were counted: false
negatives correctable by consistency (FNCC) and
false positives correctable by consistency (FPCC).
An FNCC means that a gold-standard mention is
missed by the system while its skip edge linked
gene mention is correctly labeled, which is simi-
lar to the inconsistent miss in (Sutton and McCal-
lum, 2004), as the IE94 in the first example. An
FPCC means a non-gene mention is labeled as a
gene while its skip edge linked mention (also non-
gene mention) is not recognized, as TBZ in the sec-
ond example. These two kinds of inconsistency er-
rors lead to inconsistent false negatives (FN) and
false positives (FP). A good model should reduce
as much inconsistency errors as possible. The in-
consistency errors are reduced substantially as we
expected, showing that the reduction of inconsis-
tency errors is one reason for the performance im-
provements.
The skip-chain CRF linked by similar words
had better performance than the skip-chain CRF
linked by typed dependencies. This may infer that
the quality of skip edges has impact on the per-
formance. In order to study this issue, the qual-
ity of skip edges was examined. The statistics of
skip edges in the BCIIGM dataset for the two skip-
chain CRF models (linked by similar words and by
typed dependencies respectively) is shown in the
first two rows of Table 4. A skip edge is counted as
a correct edge if the edge links two words that are
both gene mentions in the gold-standard annota-
tion. The statistics shows that the skip-chain CRF
linked by similar words has a higher precision than
the model by typed dependencies. To make the
comparison more evident, we built another skip-
chain CRF whose skip edges were randomly con-
nected. The number of skip edges in this model
16
Skip edge
Model FPCC FNCC
type
sim-words
linear-chain 112 70
skip-chain 48 20
Percentage of reduction 57.14% 71.43%
typed-dep
linear-chain 32 29
skip-chain 9 5
Percentage of reduction 71.88% 82.76%
Table 3: Statistics of inconsistency errors for
the linear-chain CRF and skip-chain CRF. FPCC
is false positives correctable by consistency and
FNCC is false negatives correctable by consis-
tency in the table. The percentage is calculated
by dividing the reduction of errors by the error
number of linear-chain CRF, for example (112 ?
48)/48 = 57.14%.
approximately equals to that in the skip-chain CRF
linked by similar words. The percentage of cor-
rect skip-edges in this model is small, as shown
in the last row of Table 4. We tested this skip-
chain CRF model on the BCIIGM dataset under
the strict matching criterion. The performance of
the randomly linked skip-chain CRF is shown in
Table 5. As can be seen from the table, the perfor-
mance of the randomly connected skip-chain CRF
droped remarkably, even worse than that of the
linear-chain CRF. This confirms that the quality
of skip edges is a key factor for the performance
improvement.
Model Edges
Correct
Percentage
edges
sim-words 1912 1344 70.29%
typed-dep 728 425 53.38%
random 1906 41 2.15%
Table 4: Statistics of skip edges and correct
skip edges for the skip-chain CRF models. sim-
words means the skip-chain CRF linked by sim-
ilar words, typed-dep means the CRF linked by
typed dependencies and random means the skip-
chain CRF has randomly connected skip edges.
The edges are counted in the BCIIGM testing data.
From the above discussion, we summarize this
section as follows: (1) the skip-chain CRF with
high quality skip edges can reduce inconsistent la-
beling errors, and (2) the quality of skip edges is
crucial to the performance improvement.
Model P (%) R(%) F(%)
linear 74.09 69.49 71.73
sim-words 76.26 71.53 73.82
typed-dep 75.99 70.49 73.14
random 73.66 69.13 71.32
Table 5: Performance comparison between the
randomly linked skip-chain CRF and other mod-
els. The result was tested on the BCIIGM dataset
under the strict matching criterion. P, R and F
denote the precision, recall and F score respec-
tively. linear denotes the linear-chain CRF. sim-
words denotes the skip-chain CRF linked by sim-
ilar words. typed-dep denotes the skip-chain CRF
linked by typed dependencies. random denotes
the skip-chain CRF having randomly linked skip
edges.
6 Conclusion
This paper proposed a method to construct a skip-
chain CRF to perform named entity recognition in
the biomedical literature. We presented two prin-
ciples to connect skip edges to address the issue
of capturing long distance dependency: linking
similar keywords and linking words having typed
dependencies. We evaluated our method on the
BioCreAtIvE II GM dataset and GENIA corpus.
Significant improvements were observed. More-
over, we presented in-depth analysis on inconsis-
tent labeling errors and the quality of skip edges.
The study shows that the quality of linked edges is
a key factor of the system performance.
The quality of linked edges plays an important
role in not only performance but also time effi-
ciency. Thus, we are planning to apply machine
learning techniques to automatically induce pat-
terns for linking high-quality skip-edges. Further-
more, to refine the recognition results, we are plan-
ning to employ post-processing algorithms or con-
struct refinement rules.
Acknowledgments
This work was partly supported by the Chi-
nese Natural Science Foundation under grant No.
60803075 and No.60973104, and partly carried
out with the aid of a grant from the International
Development Research Center, Ottawa, Canada
IRCI project from the International Development.
17
References
Shilin Ding, Gao Cong, Chin-Yew Lin and Xiaoyan
Zhu. 2008. Using Conditional Random Fields to
Extract Contexts and Answers of Questions from On-
line Forums. In Proceedings of 46th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?08), pp 710-718.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. Proceedings of the 43rd Annual Meeting
of the ACL, pages 363C370.
K. Fukuda, A. Tamura, T. Tsunoda and T. Takagi.
1998. Toward information extraction: identifying
protein names from biological papers. Pacific Sym-
posium on Biocomputing. 1998.
Michel Galley. 2006. A Skip-Chain Conditional Ran-
dom Field for Ranking Meeting Utterances by Im-
portance. Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2006), pages 364-372.
Han-Shen Huang, Yu-Shi Lin, Kuan-Ting Lin, Cheng-
Ju Kuo, Yu-Ming Chang, Bo-Hou Yang, I-Fang
Chung and Chun-Nan Hsu. 2007. High-recall gene
mention recognition by unification of multiple back-
ward parsing models. Proceedings of the Second
BioCreative Challenge Evaluation Workshop, pages
109-111.
Jun?ichi Kazama, Takaki Makino, Yoshihiro Ohta and
Jun?ichi Tsujii. 2002. Tuning support vector
machines for biomedical named entity recognition.
Proceedings of the ACL-02 workshop on Natural
language processing in the biomedical domain - Vol-
ume 3.
Corinna Kolarik, Martin Hofmann-Apitius, Marc Zim-
mermann and Juliane Fluck. 2007. Identification of
new drug classification terms in textual resources.
Bioinformatics 2007 23(13):i264-i272
Cheng-Ju Kuo, Yu-Ming Chang, Han-Shen Huang,
Kuan-Ting Lin, Bo-Hou Yang, Yu-Shi Lin, Chun-
Nan Hsu and I-Fang Chung. 2007. Rich Feature
Set, Unification of Bidirectional Parsing and Dictio-
nary Filtering for High F-Score Gene Mention Tag-
ging. Proceedings of the Second BioCreative Chal-
lenge Evaluation Workshop, pages 105-107.
Ki-Joong Lee, Young-Sook Hwang, Seonho Kim and
Hae-Chang Rim. 2004. Biomedical named entity
recognition using two-phase model based on SVMs.
Journal of Biomedical Informatics, Volume 37, Issue
6, December 2004, Pages 436-447.
John Lafferty, Andrew McCallum and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. ICML-01, pages 282-289,
2001.
Ryan McDonald and Fernando Pereira. 2005. Identi-
fying gene and protein mentions in text using con-
ditional random fields. BMC Bioinformatics 2005,
6(Suppl 1):S6.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
M. Narayanaswamy, K.E. Ravikumar and K. Vijay-
Shanker. 2003. A biological named entity recog-
nizer. Pacific Symposium on Biocomputing. 2003.
Dietrich Rebholz-Schuhmann, Harald Kirsch, Miguel
Arregui, Sylvain Gaudan, Mark Riethoven and Pe-
ter Stoehr. 2007. EBIMed?text crunching to gather
facts for proteins from Medline. Bioinformatics
2007 23(2):e237-e244
Fei Sha and Fernando Pereira. 2003. Shallow Pars-
ing with Conditional Random Fields. Proceedings
of HLT-NAACL 2003, Main Papers, pp.134-141
Larry Smith, Lorraine K Tanabe, et al 2008.
Overview of BioCreative II gene mention recogni-
tion. Genome Biology 2008, 9(Suppl 2):S2.
Charles Sutton and Andrew McCallum. 2004. Collec-
tive Segmentation and Labeling of Distant Entities
in Information Extraction. ICML workshop on Sta-
tistical Relational Learning, 2004.
Lorraine Tanabe, Natalie Xie, Lynne H Thom, Wayne
Matten and W John Wilbur. 2005. GENETAG: a
tagged corpus for gene/protein named entity recog-
nition . BMC Bioinformatics 2005, 6(Suppl 1):S3
Serhan Tatar and Ilyas Cicekli. 2009. Two learning
approaches for protein name extraction. Journal of
Biomedical Informatics 42(2009) 1046-1055
Xinglong Wang, Jun?ichi Tsujii and Sophia Anani-
adou. 2010. Disambiguating the species of biomed-
ical named entities using natural language parsers.
Bioinformatics 2010 26(5):661-667
Zhihao Yang, Hongfei Lin and Yanpeng Li. 2008. Ex-
ploiting the performance of dictionary-based bio-
entity name recognition in biomedical literature.
Computational Biology and Chemistry 32(2008)
287-291.
Alexander Yeh, Alexander Morgan, Marc Colosimo
and Lynette Hirschman. 2005. BioCreAtIvE Task
1A: gene mention finding evaluation. BMC Bioin-
formatics 2005, 6(Suppl 1):S2.
GuoDong Zhou, Jie Zhang, Jian Su, Dan Shen,
ChewLim Tan. 2004. Recognizing names in
biomedical texts: a machine learning approach.
Bioinformatics 2004, Vol.20(7),pp.1178C1190.
GuoDong Zhou, Dan Shen, Jie Zhang, Jian Su1 and
SoonHeng Tan. 2005. Recognition of protein/gene
names from text using an ensemble of classifiers.
BMC Bioinformatics 2005, 6(Suppl 1):S7.
18
