Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts,
pages 3?4, Dublin, Ireland, August 23-29 2014.
Using Neural Networks for Modeling and Representing
Natural Languages
Tomas Mikolov
Facebook A.I. Research
One Hacker Way, Menlo Park
California, US
tmikolov@fb.com
Artificial neural networks are powerful statistical models that have been shown to provide excellent re-
sults in a number of domains. In the last few years, the computer vision and automatic speech recognition
communities have been heavily influenced by these techniques. Applications to problems that involve
natural language, such as machine translation or computational semantics, are becoming mainstream in
the NLP research.
This tutorial aims to introduce the basic concepts and provide intuitive understanding of neural net-
works, including the very popular field of deep learning. This should help the researchers who are
entering this field to quickly understand the major tricks of the trade.
The structure of the tutorial is as follows:
Basic machine learning applied to natural language
? n-grams and bag-of-words representations
? logistic regression, support vector machines
Introduction to neural networks
? architecture of neural networks: neurons, layers, synapses
? activation function
? objective function
? training: stochastic gradient descent, backpropagation, learning rate, regularization
? multiple hidden layers and intuitive explanation of deep learning
Distributed representations of words
? basic application of neural networks for obtaining vector representation of words
? linguistic regularities in the word vector space
? word analogy tasks with vector representations
? representations of phrases and sentences
? simple application to machine translation of words and phrases
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
3
Neural network based language models
? feedforward and recurrent neural net architectures for language modeling
? class based softmax, hierarchical softmax
? joint training with maximum entropy model
? recurrent model with slow features
? application to language modeling, speech recognition, machine translation
Tips for future research
? understanding the current research culture
? hints how to recognize good papers and ideas
? promising future directions
Resources
? introduction to open-source software: RNNLM toolkit, word2vec and other tools
? links to large text corpora, pre-trained models
? benchmark datasets for advancing the state of the art
4
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1116?1127,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Fast Re-scoring Strategy to Capture Long-Distance Dependencies
Anoop Deoras
HLT-COE and CLSP
Johns Hopkins University
Baltimore MD 21218, USA
adeoras@jhu.edu,
Toma?s? Mikolov
Brno University of Technology
Speech@FIT
Czech Republic
imikolov@fit.vutbr.cz,
Kenneth Church
HLT-COE and CLSP
Johns Hopkins University
Baltimore MD 21218, USA
kenneth.church@jhu.edu
Abstract
A re-scoring strategy is proposed that makes
it feasible to capture more long-distance de-
pendencies in the natural language. Two pass
strategies have become popular in a num-
ber of recognition tasks such as ASR (au-
tomatic speech recognition), MT (machine
translation) and OCR (optical character recog-
nition). The first pass typically applies a
weak language model (n-grams) to a lattice
and the second pass applies a stronger lan-
guage model to N best lists. The stronger lan-
guage model is intended to capture more long-
distance dependencies. The proposed method
uses RNN-LM (recurrent neural network lan-
guage model), which is a long span LM, to re-
score word lattices in the second pass. A hill
climbing method (iterative decoding) is pro-
posed to search over islands of confusability
in the word lattice. An evaluation based on
Broadcast News shows speedups of 20 over
basic N best re-scoring, and word error rate
reduction of 8% (relative) on a highly compet-
itive setup.
1 Introduction
Statistical Language Models (LMs) have received
considerable attention in the past few decades. They
have proved to be an essential component in many
statistical recognition systems such as ASR (au-
tomatic speech recognition), MT (machine trans-
lation) and OCR (optical character recognition).
The task of a language model is to assign prob-
ability to any word sequence possible in the lan-
guage. The probability of the word sequence W ?
w1, . . . , wm ? wm1 is typically factored using the
chain rule:
P (wm1 ) =
m?
i=1
P (wi|wi?11 ) (1)
In modern statistical recognition systems, an LM
tends to be restricted to simple n-gram models,
where the distribution of the predicted word depends
on the previous (n ? 1) words i.e. P (wi|wi?11 ) ?
P (wi|wi?1i?n+1).
Noam Chomsky argued that n-grams cannot learn
long-distance dependencies that span over more than
n words (Chomsky, 1957, pp.13). While that might
seem obvious in retrospect, there was a lot of ex-
citement at the time over the Shannon-McMillan-
Breiman Theorem (Shannon, 1948) which was inter-
preted to say that, in the limit, under just a couple of
minor caveats and a little bit of not-very-important
fine print, n-gram statistics are sufficient to capture
all the information in a string (such as an English
sentence). Chomsky realized that while that may be
true in the limit, n-grams are far from the most parsi-
monious representation of many linguistic facts. In
a practical system, we will have to truncate n-grams
at some (small) fixed n (such as trigrams or perhaps
5-grams). Truncated n-gram systems can capture
many agreement facts, but not all.1
By long-distance dependencies, we mean facts
like agreement and collocations that can span over
many words. With increasing order of n-gram mod-
els we can, in theory, capture more regularities in the
1The discussion in this paragraph is taken as-is from an arti-
cle (to appear) by Church (2012).
1116
language. In addition, if we can move to more gen-
eral models then we could hope to capture more, as
well. However, due to data sparsity, it is hard to es-
timate a robust n-gram distribution for large values
of n ( say, n > 10) using the conventional Max-
imum Likelihood techniques, unless a more robust
technique is employed for modeling which gener-
alizes well on unseen events. Some of these well
known long span / complex language models which
have shown to perform very well on many speech
tasks include: structured language model (Chelba
and Jelinek, 2000; Roark, 2001; Wang and Harper,
2002; Filimonov and Harper, 2009), latent seman-
tic analysis language model (Bellegarda, 2000),
topic mixture language models (Iyer and Ostendorf,
1999), whole sentence exponential language mod-
els (Rosenfeld, 1997; Rosenfeld et al, 2001), feed-
forward neural networks (Bengio et al, 2001), re-
current neural network language models (Mikolov
et al, 2010), among many others.
Although better modeling techniques can now
capture longer dependencies in a language, their
incorporation in decoders of speech recognition or
machine translation systems becomes computation-
ally challenging. Due to the prohibitive increase in
the search space of sentence hypotheses (or longer
length word sub sequences), it becomes challenging
to use a long span language model in the first pass
decoding. A word graph (word lattices for speech
recognition systems and hypergraphs for machine
translation systems), encoding exponential number
of hypotheses is hence outputted at the first pass out-
put on which a sophisticated and complex language
model is deployed for re-scoring. However, some-
times even re-scoring of this refined search space
can be computationally expensive due to explosion
of state space.
Previously, we showed in (Deoras et al, 2011)
how to tackle the problem of incorporating long span
information during decoding in speech recogni-
tion systems by variationaly approximating (Bishop,
2006, pp. 462) the long span language model by a
tractable substitute such that this substitute model
comes closest to the long span model (closest in
terms of Kullback Leibler Divergence (Cover and
J.A.Thomas, 1991, pp. 20)). The tractable substi-
tute was then used directly in the first pass speech
recognition systems. In this paper we propose an
approach that keeps the model intact but approxi-
mates the search space instead (which can become
intractable to handle especially under a long span
model), thus enabling the use of full blown model
for re-scoring.With this approach, we can achieve
full lattice re-scoring with a complex model, at a
cost more than 20 times less than of a naive brute
force approach that is commonly used today.
The rest of the paper is organized as follows:
We discuss a particular form of long span language
model in Sec. 2. In Sec. 3 we discuss two standard
re-scoring techniques and then describe and demon-
strate our proposed technique in Sec. 4. We present
experimental results in Sec. 5 followed by conclu-
sions and some remarks in Sec. 6.
2 Recurrent Neural Networks (RNN)
There is a long history of using neural networks to
model sequences. Elman (1990) used recurrent neu-
ral network for modeling sentences of words gen-
erated by an artificial grammar. Work on statistical
language modeling of real natural language data, to-
gether with an empirical comparison of performance
to standard techniques was done by Bengio et al
(2001). His work has been followed by Schwenk
(2007), who has shown that neural network language
models actually work very well in the state-of-the-
art speech recognition systems. Recurrent Neu-
ral Network based Language Models (RNN-LMs)
(Mikolov et al, 2010) improved the ability of the
original model to capture patterns in the language
without using any additional features (such as part
of speech, morphology etc) i.e. other than lexical
ones. The RNN-LM was shown to have superior
performance than the original feedforward neural
network (Mikolov et al, 2011b). Recently, we also
showed that this model outperforms many other ad-
vanced language modeling techniques (Mikolov et
al., 2011a). We hence decided to work with this
model. This model uses whole history to make pre-
dictions, thus it lies outside the family of n-gram
models. Power of the model comes at a considerable
computational cost. Due to the requirement of un-
limited history, many optimization tricks for rescor-
ing with feedforward-based NNLMs as presented by
Schwenk (2007) cannot be applied during rescoring
with RNN LM. Thus, this model is a good candidate
1117
w( t )
s( t )
y( t )
(delayed)
U V
Figure 1: Schematic Representation of Recurrent Neu-
ral Network Language Model. The network has an input
layer w, a hidden layer s and an output layer y. Matrices
U and V represent synapses.
to show effectiveness and importance of our work.
The basic RNNLM is shown in Fig. 1. The model
has an input layer w(t) that encodes previous word
using 1 of N coding (thus, the size of the input layer
is equal to the size of the vocabulary, and only the
neuron that corresponds to the previous word in a
sequence is set to 1). The hidden layer s(t) has addi-
tional recurrent connections that are delayed by one
time step. After the network is trained, the output
layer y(t) represents probability distribution for the
current word, given the previous word and the state
of the hidden layer from the previous time step.
The training is performed by ?backpropagation-
through-time? algorithm that is commonly used for
training recurrent neural networks (Rumelhart et al,
1986). More details about training, setting initial pa-
rameters, choosing size of the hidden layer etc. are
presented in (Mikolov et al, 2010). Additional ex-
tensions that allow this model to be trained on large
corpora are presented in (Mikolov et al, 2011b).
3 Standard Approaches for Rescoring
3.1 Word Lattice Rescoring
A word lattice, L, obtained at the output of the first
pass decoding, encodes exponential number (expo-
nential in the number of states (nodes) present in
the lattice) of hypotheses in a very compact data
structure. It is a directed acyclic graph G =
(V, E , ns, Ne), where V and E denote set of vertices
(nodes / states) and edges (arcs / links), respectively.
ns and Ne denote the unique start state and set of
end states.
A path, pi, in a lattice is an element of E? with
consecutive transitions. We will denote the origin /
previous state of this path by p[pi] and destination /
next state of this path by n[pi]. A path, pi is called
a complete path if p[pi] = ns and n[pi] ? Ne. A
path, pi, is called a partial path if p[pi] = ns but n[pi]
may or may not belong to Ne. A path, pi, is called
a trailing path if p[pi] may or may not be equal to
ns and n[pi] ? Ne. We will also denote the time
stamp at the start of the path by Ts[pi] and the time
stamp at the end of the path by Te[pi]. Since there
are nodes attached to the start and end of any path,
we will denote the time stamp at any node u ? V by
T [u]. Associated with every path, pi, is also a word
sequence W [pi] ? W?, where W is the vocabulary
used during speech recognition. For the sake of sim-
plicity, we will distinguish word sequence of length
1 from the word sequences of length greater than 1
by using lower and upper casing i.e. w[?] and W [?]
respectively.
The acoustic likelihood of the path pi ? E? is then
given as:
A[pi] =
|pi|?
j=1
P (aj |w[pij ])
where ?j ? {1, 2, . . . , |pi|} pij ? E , pi = |pi|j=1pij
and P (aj |w[pij ]) is the acoustic likelihood of the
acoustic substring aj , spanning between Ts[pij ] and
Te[pij ], conditioned on the word w[pij ] associated
with the edge pij .2 Similarly, the language model
score of the path pi is given as:
L[pi] =
|pi|?
j=1
P (w[pij ]|w[pij?1], . . . , w[pij?m+1])
where P (w[pij ]|w[pij?1], . . . , w[pij?m+1]) is the
m-th order Markov approximation for estimating the
probability of a word given the context upto that
point. The speech recognizer, which uses m-th or-
der Markov LM for first pass recognition, imposes a
constraint on the word lattice such that at each state
there exists an unambiguous context of consecutive
m? 1 words.
A first pass output is then a path pi? having Max-
imum a Posterior (MAP) probability.3 Thus pi? is
2We will use  symbol to denote concatenation of paths or
word strings.
3Note that asterisk symbol here connotes that the path is op-
1118
obtained as:
pi? = argmax
pi:p[pi]=ns
n[pi]?Ne
A[pi]?L[pi],
where ? is the scaling parameter needed to balance
the dynamic variability between the distributions of
acoustic and language model (Ogawa et al, 1998).
Efficient algorithms such as single source shortest
path (Mohri et al, 2000) can be used for finding out
the MAP path.
Under a new n-gram Language Model, rescor-
ing involves replacing the existing language model
scores of all paths pi. If we denote the new language
model by Lnew and correspondingly the score of the
path pi by Lnew[pi], then it is simply obtained as:
Lnew[pi] =
|pi|?
j=1
P (w[pij ]|w[pij?1], . . . , w[pij?n+1])
where P (w[pij ]|w[pij?1], . . . , w[pij?n+1]) is the n-
th order Markov approximation for estimating the
probability of a word given the unambiguous con-
text of n ? 1 words under the new rescoring LM.
If the Markov rescoring n-gram LM needs a bigger
context for the task of prediction (i.e. n > m, where
m? 1 is the size of the unambiguous context main-
tained at every state of the word lattice), then each
state of the lattice has to be split until an unambigu-
ous context of length as large as that required by the
new re-scoring language model is not maintained.
The best path, pi? is then obtained as:
pi? = argmax
pi:p[pi]=ns
n[pi]?Ne
A[pi]?Lnew[pi],
where ? acts as the new scaling parameter which
may or may not be equal to the old scaling parameter
?.
It should be noted that if the rescoring LM needs a
context of the entire past in order to predict the next
word, then the lattice has to be expanded by splitting
the states many more times. This usually blows up
the search space even for a reasonably small number
timal under some model. This should not be confused with the
Kleene stars appearing as superscripts for E andW , which serve
the purpose of regular expressions implying 0 or many occu-
rances of the element of E and V respectively.
of state splitting iterations. When the task is to do
rescoring under a long span LM, such as RNN-LM,
then exact lattice re-scoring option is not feasible. In
order to tackle this problem, a suboptimal approach
via N best list rescoring is utilized. The details of
this method are presented next.
3.2 N best List Rescoring
N best list re-scoring is a popular way to cap-
ture some long-distance dependencies, though the
method can be slow and it can be biased toward the
weaker language model that was used in the first
pass.
Given a word lattice, L, top N paths
{pi1, . . . , piN} are extracted such that their joint
likelihood under the baseline acoustic and language
models are in descending order i.e. that:
A[pi1]?L[pi1] ? A[pi2]?L[pi2] ? . . . ? A[piN ]?L[piN ]
Efficient algorithms exist for extractingN best paths
from word lattices (Chow and Schwartz, 1989;
Mohri and Riley, 2002). If a new language model,
Lnew, is provided, which now need not be restricted
to finite state machine family, then that can be de-
ployed to get the score of the entire path pi. If we
denote the new LM scores by Lnew[?], then under N
best list paradigm, optimal path p?i is found out such
that:
p?i = argmax
pi?{pi1,...,piN}
A[pi]?Lnew[pi], (2)
where ? acts as the new scaling parameter which
may or may not be equal to ?. If N  |L| (where
|L| is the total number of complete paths in word lat-
tice, which are exponentially many), then the path
obtained using (2) is not guaranteed to be optimal
(under the rescoring model). The short list of hy-
potheses so used for re-scoring would yield subop-
timal output if the best path pi? (according to the
new model) is not present among the top N candi-
dates extracted from the lattice. This search space
is thus said to be biased towards a weaker model
mainly because the N best lists are representative of
the model generating them. To illustrate the idea,
we demonstrate below a simple analysis on a rel-
atively easy task of speech transcription on WSJ
data.4 In this setup, the recognizer made use of a bi-
4Full details about the setup can be found in (Deoras et al,
2010)
1119
gram LM to produce lattices and hence N best lists.
Each hypothesis in this set got a rank with the top
most and highest scoring hypothesis getting a rank
of 1, while the bottom most hypothesis getting a
rank of N . We then re-scored these hypotheses with
a better language model (either with a higher order
Markov LM i.e. a trigram LM (tg) or the log linear
combination of n-gram models and syntactic mod-
els (n-gram+syntactic) and re-ranked the hypothe-
ses to obtain their new ranks. We then used Spear-
man?s rank correlation factor, ?, which takes values
in [?1,+1], with ?1 meaning that the two ranked
lists are negatively correlated (one list is in a reverse
order with respect to the other list) and +1 mean-
ing that the two ranked lists are positively correlated
(the two lists are exactly the same). Spearman?s rank
correlation factor is given as:
? = 1? 6
?N
n=1 d2n
N(N2 ? 1) , (3)
where dn is the difference between the old and new
rank of the nth entry (in our case, difference between
n(? {1, 2, . . . , N}) and the new rank which the nth
hypothesis got under the rescoring model).
Table 1 shows how the correlation factor drops
dramatically when a better and a complementary
LM is used for re-scoring, suggesting that theN best
lists are heavily biased towards the starting models.
Huge re-rankings suggests there is an opportunity to
improve and also a need to explore more hypotheses,
i.e. beyond N best lists.
Model (?) WER (%)
bg 1.00 18.2%
tg 0.41 17.4%
n-gram+syntactic 0.33 15.8%
Table 1: Spearman Rank Correlation on the N best list
extracted from a bi-gram language model (bg) and re-
scored with relatively better language models including,
trigram LM (tg), and the log linear combination of n-
gram models, and syntactic models (n-gram+syntactic).
With a bigger and a better LM, the WER decreases at
the expense of huge re-rankings of N best lists, only
suggesting the fact that N best lists generated under a
weaker model, are not reflective enough of a relatively
better model.
In the next section, we propose an algorithm
which keeps the representation of search space as
simple as that of N best list, but does not restrict it-
self to topN best paths alone and hence does not get
biased towards the starting weaker model.
4 Proposed Approach for Rescoring
A high level idea of our proposed approach is to
identify islands of confusability in the word lattice
and replace the problem of global search over word
lattice by series of local search problems over these
islands in an iterative manner. The motivation be-
hind this strategy is the observation that the recog-
nizer produces bursts of errors such that they have
a temporal scope. The recognizer output (sentence
hypotheses) when aligned together typically shows
a pattern of confusions both at the word level and
at the phrase level. Regions where there are sin-
gleton words competing with one another (reminis-
cent of a confusion bin of a Confusion Network
(CN) (Mangu, 2000)), choice of 1 word edit dis-
tance works well for the formation of local neigh-
borhood. Regions where there are phrases com-
peting with other phrases, choice of variable length
neighborhood works well. Previously, Richardson
et al (1995) demonstrated a hill climbing frame-
work by exploring 1 word edit distance neighbor-
hood, while in our own previous work (Deoras and
Jelinek, 2009), we demonstrated working of iterative
decoding algorithm, a hill climbing framework, for
CNs, in which the neighborhood was formed by all
words competing with each other in any given time
slot, as defined by a confusion bin.
In this work, we propose a technique which gen-
eralizes very well on word lattices and overcomes
the limitations posed by a CN or by the limited na-
ture of local neighborhood. The size of the neigh-
borhood in our approach is a variable factor which
depends upon the confusability in any particular re-
gion of the word lattice. Thus the local neighbor-
hood are in some sense a function of the confusabil-
ity present in the lattice rather than some predeter-
mined factor. Below we describe the process, virtue
of which, we can cut the lattice to form many self
contained smaller sized sub lattices. Once these sub
lattices are formed, we follow a similar hill climbing
procedure as proposed in our previous work (Deoras
and Jelinek, 2009).
1120
4.1 Islands of Confusability
We will continue to follow the notation introduced
in section 3.1. Before we define the procedure for
cutting the lattice into many small self contained
lattices, we will define some more terms necessary
for the ease of understandability of the algorithm.5
For any node v ? V , we define forward probability,
?(v), as the probability of any partial path pi ? E?,
s.t. p[pi] = ns, n[pi] = v and it is given as:
?(v) =
?
pi?E?
s.t.p[pi]=ns,n[pi]=v
A[pi]?L[pi] (4)
Similarly, for any node v ? V , we define the
backward probability, ?(v), as the probability of any
trailing path pi ? E?, s.t. p[pi] = v, n[pi] ? Ne and it
is given as:
?(v) =
?
pi?E?
s.t.p[pi]=v,n[pi]?Ne
A[pi]?L[pi] (5)
If we define the sum of joint likelihood under the
baseline acoustic and language models of all paths
in the lattice by Z, then it can simply be obtained as:
Z =
?
u?Ne ?(u) = ?(ns)In order to cut the lattice, we want to identify sets
of nodes, S1, S2, . . . , S|S| such that for any set Si ?
S following conditions are satisfied:
1. For any two nodes u, v ? Si we have that:
T [u] = T [v]. We will define this common time
stamp of the nodes in the set by T [Si].
2. 6 ? pi ? E such that Ts[pi] < T [Si] < Te[pi].
The first property can be easily checked by first
pushing states into a linked list associated with each
time marker (this can be done by iterating over all
the states of the graph) then iterating over the unique
time markers and retrieving back the nodes asso-
ciated with it.The second property can be checked
by first iterating over the unique time markers and
for each of the marker, iterating over the arcs and
terminating the loop as soon as some arc is found
5Goel and Byrne (2000) previously demonstrated the lat-
tice segmentation procedure to solve the intractable problem of
MBR decoding. The cutting procedure in our work is different
from theirs in the sense that we rely on time information for
collating competing phrases, while they do not.
out violating property 2 for the specific time marker.
Thus the time complexity for checking property 1 is
O(|V|) and that for property 2 isO(|T |?|E|), where
|T | is the total number of unique time markers. Usu-
ally |T |  |E| and hence the time complexity for
checking property 2 is almost linear in the number
of edges. Thus effectively, the time complexity for
cutting the lattice is O(|V|+ |E|).
Having formed such sets, we can now cut the
lattice at time stamps associated with these sets
i.e. that: T [S1], . . . , T [S|S|]. It can be easily seen
that the number of sub lattices, C, will be equal
to |S| ? 1.We will identify these sub lattices as
L1,L1, . . . ,LC . At this point, we have not formed
self contained lattices yet by simply cutting the par-
ent lattice at the cut points.
Once we cut the lattice at these cut points, we im-
plicitly introduce many new starting nodes and end-
ing nodes for any sub lattice. We will refer to these
nodes as exposed starting nodes and exposed end-
ing nodes. Thus for some jth sub lattice, Lj , there
will be as many new exposed starting nodes as there
are nodes in the set Sj and as many exposed ending
nodes as there are nodes in the set Sj+1. In order
to make these sub lattices consistent with the defini-
tion of a word lattice (see Sec. 3.1), we unify all the
exposed starting nodes and exposed ending nodes.
To unify the exposed starting nodes, we introduce
as many new edges as there are nodes in the set Sj
such that they have a common starting node, ns[Lj ],
(newly created) and distinct ending nodes present
in Sj . To unify the exposed ending nodes of Lj ,
we introduce as many new edges as there are nodes
in the set Sj+1 such that they have distinct starting
nodes present in Sj+1 and a common ending node
ne[Lj ] (newly created). From the totality of these
new edges and nodes along with the ones already
present in Lj forms an induced directed acyclic sub-
graph G[Lj ] = (V[Lj ], E [Lj ], ns[Lj ], ne[Lj ]).
For any path pi ? E [Lj ] such that p[pi] = ns[Lj ]
and n[pi] ? Sj , we assign the value of ?(n[pi])
to denote the joint likelihood A[pi]?L[pi] and as-
sign epsilon for word associated with these edges
i.e. w[pi]. We assign T [Sj ] ? ?T to denote Ts[pi]
and T [Sj ] to denote Te[pi]. Similarly, for any path
pi ? E [Lj ] such that p[pi] ? Sj+1 and n[pi] = ne[Lj ],
1121
we assign the value of ?(p[pi])6 to denote the joint
likelihood A[pi]?L[pi] and assign epsilon for word
associated with these edges i.e. w[pi]. We assign
T [Sj+1] to denote Ts[pi] and T [Sj+1] + ?T to de-
note Te[pi]. This completes the process and we ob-
tain self contained lattices, which if need be, can be
independently decoded and/or analyzed.
4.2 Iterative Decoding on Word Lattices
Once we have formed the self contained lattices,
L1,L1, . . . ,LC , where C is the total number of sub
lattices formed, then the idea is to divide the re-
scoring problem into many small re-scoring prob-
lems carried over the sub lattices one at a time by
fixing single best paths from all the remaining sub
lattices.
The inputs to the algorithm are the sub lattices
(produced by cutting the parent lattice generated un-
der some Markov n-gram LM) and a new rescor-
ing LM, which now need not be restricted to fi-
nite state machine family. The output of the al-
gorithm is a word string, W?, such that it is the
concatenation of final decoded word strings from
each sub lattice. Thus if we denote the final de-
coded path (under some decoding scheme, which
will become apparent next) in the jth sub lattice
by pi?j and the concatenation symbol by ???, then
W? = W [pi?1] ?W [pi?2] ? . . . ?W [pi?C ] = Cj=1W [pi?j ].
Algorithm 1 Iterative Decoding on word lattices.
Require: {L1,L1, . . . ,LC}, Lnew
PrevHyp? null
CurrentHyp?Cj=1W [p?ij ]
while PrevHyp 6= CurrentHyp do
for i? 1 . . . C do
p?ii ? argmax
pii?E?i :
p[pii]=ns[Li]
n[pii]=ne[Li]
(
Lnew[ 1? ? . . . ?pii ? . . . ? k?]
?A[pii]?
?k
j=1
j 6=i
A[ j? ]?
)
end for
PrevHyp? CurrentHyp
CurrentHyp?Cj=1W [p?ij ]
end while
?j ? {1, 2, . . . , C} pi?j ? p?ij
6The values of ?(?) and ?(?) are computed under parent lat-
tice structure.
The algorithm is initialized by setting PrevHypo
to null and CurrHypo to the concatenation of 1-best
output from each sub lattice. During the initializa-
tion step, each sub lattice is analyzed independent of
any other sub lattice and under the baseline acoustic
scores and baseline n-gram LM scores, 1-best path
is found out. Thus if we define the best path under
baseline model in some jth sub-lattice by p?ij , Cur-
rHypo is then initialized to: W [p?i1] ? W [p?i2] ? . . . ?
W [p?iC ]. The algorithm then runs as long as Cur-
rHypo is not equal to PrevHypo. In each iteration,
the algorithm sequentially re-scores each sub-lattice
by keeping the surrounding context fixed. Once all
the sub lattices are re-scored, that constitutes one it-
eration. At the end of each iteration, CurrHypo is
set to the concatenation of 1 best paths from each
sub lattice while PrevHypo is set to the old value
of CurrHypo. Thus if we are analyzing some ith
sub-lattice in some iteration, then 1-best paths from
all but this sub-lattice is kept fixed and a new 1-best
path under the re-scoring LM is found out. It is not
hard to see that the likelihood of the output under
the new re-scoring model is guaranteed to increase
monotonically after every decoding step.
Since the cutting of parent lattices produce many
small lattices with considerably lesser number of
nodes, in practice, an exhaustive search for the 1-
best hypothesis can be carried out via N best list.
Algorithm 1 outlines the steps for iterative decoding
on word lattices.
4.3 Entropy Pruning
In this section, we will discuss a speed up technique
based on entropy of the lattice. Entropy of a lattice
reflects the confidence of the recognizer in recogniz-
ing the acoustics. Based on the observation that if
the N best list / lattice generated under some model
has a very low entropy, then the Spearman?s rank
correlation factor, ? (Eqn. 3), tends to be higher
even when the N best lists / lattice is re-ranked with
a bigger and a better model. A low entropy under
the baseline model only reflects the confidence of
the recognizer in recognizing the acoustic. Table 2
shows the rank correlation values between two sets
of N best lists. Both sets are produced by a bi-
gram LM (bg). The entropy of N best lists in the
first set is 0.05 nats or less. The N best lists in the
second set have an entropy greater than 0.05 nats.
1122
Both these sets are re-ranked with bigger and bet-
ter models (see Table 1 for model definitions). We
can see from Table 2 that the rank correlation values
tend to be higher (indicating little re-rankings) when
the entropy of the N best list, under the baseline
model, is lower. Similarly, the rank-correlation val-
ues tend to be lower (indicating more re-rankings)
whenever the entropy of the N best list is higher.
Note that these entropy values are computed with re-
spect to the starting model (in this case, bigram LM).
Of course, if the starting LM is much weaker than
the rescoring model, then the entropy values need
not be reflective of the difficulty of the overall task.
This observation then suggests that it is safe to re-
score only those N best lists whose entropy under
the starting model is higher than some threshold.
Rescoring Model ?(H?0.05) ?(H>0.05)
bg 1.00 1.00
tg 0.58 0.38
n-gram+syntactic 0.54 0.31
Table 2: Spearman Rank Correlation on the N best list
extracted from a bi-gram language model (bg) and re-
scored with relatively better language models (see Table 1
for model definitions). Entropy under the baseline model
correlates well with the rank correlation factor, suggest-
ing that exhaustive search need not be necessary for ut-
terances yielding lower entropy.
While computation of entropy for N best list is
tractable, for a word lattice, the computation of en-
tropy is intractable if one were to enumerate all the
hypotheses. Even if we were able to enumerate all
hypotheses, this method tends to be slower. Using
efficient semiring techniques introduced by Li and
Eisner (2009) or using posterior probabilities on the
edges leading to end states, we can compute the en-
tropy of a lattice in one single forward pass using
dynamic programming. It should, however, be noted
that, for dynamic programming technique to work,
only n-gram LMs can be used. One has to resort to
approximate entropy computation via N best list, if
entropy under long span LM is desired.
4.3.1 Speed Up for Iterative Decoding
Our speed up technique is simple. Once we have
formed self contained sub lattices, we want to prune
all but the top few best complete paths (obtained un-
der baseline / starting model) of those sub lattices
whose entropy is below some threshold. Thus, be-
lieving in the original model?s confidence, we want
to focus only on those sub lattices which the recog-
nizer found difficult to decode in the first pass. All
other part of the parent lattice will be not be ana-
lyzed. The thresholds for pruning is very application
and corpus specific and needs to be tuned on some
held out data.
5 Experiments and Results
We performed recognition on the Broadcast News
(BN) dev04f, rt03 and rt04 task using the state-
of-the-art acoustic models trained on the English
Broadcast News (BN) corpus (430 hours of audio)
provided to us by IBM (Chen et al, 2009). IBM also
provided us its state-of-the-art speech recognizer,
Attila (Soltau et al, 2010) and two Kneser-Ney
smoothed backoff n-gram LMs containing 4.7M n-
grams (n ? 4) and 54M n-grams (n ? 4), both
trained on 400M word tokens. We will refer to them
as KN:BN-Small and KN:BN-Big respectively. We
refer readers to (Chen et al, 2009) for more details
about the recognizer and corpora used for training
the models.
We trained two RNN based language models -
the first one, denoted further as RNN-limited, was
trained on a subset of the training data (58M tokens).
It used 400 neurons in the hidden layer. The second
model, denoted as RNN-all, was trained on all of
the training data (400M tokens), but due to the com-
putational complexity issues, we had to restrict its
hidden layer size to 320 neurons.
We followed IBM?s multi-pass decoding recipe
using KN:BN-Small in the first pass followed by ei-
ther N best list re-scoring or word lattice re-scoring
using bigger and better models.7 For the purpose
of re-scoring, we combined all the relevant statisti-
cal models in one unified log linear framework rem-
iniscent of work by Beyerlein (1998). We, however,
trained the model weights by optimizing expected
WER rather than 1-best loss as described in (De-
oras et al, 2010). Training was done on N best
lists of size 2K. We will refer to the log linear com-
7The choice of the order and size of LM to be used in the
first pass decoding was determined by taking into consideration
the capabilities of the decoder.
1123
100 101 102 103 104 105 10610.8
11
11.2
11.4
11.6
11.8
12
Size of Search Space (Number of Hypotheses for evaluation)
1 be
st W
ER(%
)
Plot of 1 best WER v/s Search Space Size
 
 
N BestIter. Dec. (ID)ID with Ent. PruningViterbi BaselineViterbi Rescoring
Figure 2: Plot of WER (y axis) on rt03+dev04f set versus
the size of the search space (x axis). The baseline WER
obtained using KN:BN-Small is 12% which then drops
to 11% when KN:BN-Big is used for re-scoring. N best
list search method obtains the same reduction in WER
by evaluating as many as 228K sentence hypotheses on
an average. The proposed method obtains the same re-
duction by evaluating 14 times smaller search space. The
search effort reduces further to 40 times if entropy based
pruning is employed during re-scoring.
bination of KN:BN-Big and RNN-limited by KN-
RNN-lim; KN:BN-Big and RNN-all by KN-RNN-
all and KN:BN-Big, RNN-limited and RNN-all by
KN-RNN-lim-all.
We used two sets for decoding: rt03+dev04f set
was used as a development set while rt04 was used
as a blind set for the purpose of evaluating the per-
formance of long span RNN models using the pro-
posed approach. We made use of OpenFst C++ li-
braries (Allauzen et al, 2007) for manipulating lat-
tice graphs and generating N best lists. Due to the
presence of hesitation tokens in reference transcripts
and the need to access the silence/pause tokens for
penalizing short sentences, we treated these tokens
as regular words before extracting sentence hypothe-
ses. This, and poorly segmented nature of the test
corpora, led to huge enumeration of sentence hy-
potheses.
5.1 n-gram LM for re-scoring
In this setup, we used KN:BN-Small as the base-
line starting LM which yielded the WER of 12%
on rt03+dev04f set. Using KN:BN-Big as the re-
scoring LM, the WER dropped to 11%. Since the
re-scoring LM belonged to the n-gram family, it was
possible to compute the optimal word string by re-
scoring the whole lattice (see Sec. 3.1). We now
compare the performance of N best list approach
(Sec. 3.2) with our proposed approach (Sec. 4).
N best list achieved the best possible reduction by
evaluating as many as 228K sentence hypotheses
on an average. As against that, our proposed ap-
proach achieved the same performance by evaluat-
ing 16.6K sentence hypotheses, thus reducing the
search efforts by 13.75 times. By carrying out en-
tropy pruning (see Sec. 4.3 ) on sub lattices, our pro-
posed approach required as little as 5.6K sentence
hypotheses evaluations to obtain the same optimal
performance, reducing the search effort by as much
as 40.46 times. For the purpose of this experiment,
entropy based pruning was carried out when the en-
tropy of the sub lattice was below 5 nats. Table 3
compares the two search methods for this setup and
Fig. 2 shows a plot of WER versus the size of the
search space (in terms of number of sentence hy-
potheses evaluated by an n-gram language model).
On rt04, the KN:BN-Small LM gave a WER of
14.1% which then dropped to 13.1% after re-scoring
with KN:BN-Big. Since the re-scoring model was
an n-gram LM, it was possible to obtain the opti-
mal performance via lattice update technique (see
Sec. 3.1). We then carried out the re-scoring of the
word lattices under KN:BN-Big using our proposed
technique and found it to give the same performance
yielding the WER of 13.1%.
5.2 Long Span LM for re-scoring
In this setup, we used the strongest n-gram LM
as our baseline. We thus used KN:BN-Big as the
baseline LM which yielded the WER of 11% on
rt03+dev04f. We then used KN-RNN-lim-all for re-
scoring. Due to long span nature of the re-scoring
LM, it was not possible to obtain the optimal WER
performance. Hence we have compared the perfor-
mance of our proposed method with N best list ap-
proach. N best list achieved the lowest possible
WER after evaluating as many as 33.8K sentence
hypotheses on an average. As against that, our pro-
posed approach in conjunction with entropy pruning
obtained the same performance by evaluating just
1.6K sentence hypotheses, thus reducing the search
by a factor of 21. Fig 3 shows a plot of WER versus
1124
100 101 102 103 10410.3
10.4
10.5
10.6
10.7
10.8
10.9
11
11.1
Size of Search Space (Number of Hypotheses for evaluation)
1 be
st W
ER(%
)
Plot of 1 best WER v/s Search Space Size
 
 
N BestID with Ent. PruningViterbi Baseline
Figure 3: Plot of WER (y axis) on rt03+dev04f set versus
the size of the search space (x axis). The baseline WER
obtained using KN:BN-Big is 11% which then drops to
10.4% when KN-RNN-lim-all is used for re-scoring. N
best list search method obtains this reduction in WER by
evaluating as many as 33.8K sentence hypotheses on an
average, while the proposed method (with entropy prun-
ing) obtains the same reduction by evaluating 21 times
smaller search space.
the size of the search space (in terms of number of
sentence hypotheses evaluated by a long span lan-
guage model).
In-spite of starting off with a very strong n-gram
LM, theN best lists so extracted were still not repre-
sentative enough of the long span rescoring models.
Had we started off with KN:BN-Small, the N best
list re-scoring method would have had no chance of
finding the optimal hypothesis in reasonable size of
hypotheses search space. Table 4 compares the two
search methods for this setup when many other long
span LMs were also used for re-scoring.
On rt04, the KN:BN-Big LM gave a WER of
13.1% which then dropped to 12.15% after re-
scoring with KN-RNN-lim-all using our proposed
technique.8 Since the re-scoring model was not an
n-gram LM, it was not possible to obtain the optimal
performance but we could enumerate huge N best
list to approximate this value. Our proposed method
is much faster than huge N best lists and no worse
in terms of WER. As far as we know, the result ob-
tained on these sets is the best performance ever
reported on the Broadcast News corpus for speech
8The WER obtained using KN-RNN-lim and KN-RNN-all
were 12.5% and 12.3% respectively.
recognition.
Models WER NBest ID Saving
KN:BN-Small 12.0 - - -
KN:BN-Big 11.0 228K 5.6K 40
Table 3: The starting LM is a weak n-gram LM (KN:BN-
Small) and the re-scoring LM is a much stronger but n-
gram LM (KN:BN-Big). The baseline WER in this case
is 12% and the optimal performance by the re-scoring LM
is 11.0%. The proposed method outperforms N best list
approach, in terms of search efforts, obtaining optimal
WER.
Models WER NBest ID Saving
KN:BN-Big 11.0 - - -
KN-RNN-lim 10.5 42K 1.1K 38
KN-RNN-all 10.5 26K 1.3K 20
KN-RNN-lim-all 10.4 34K 1.6K 21
Table 4: The starting LM is a strong n-gram LM
(KN:BN-Big) and the re-scoring model is a long span
LM (KN-RNN-*). The baseline WER is 11.0%. Due
to long span nature of the LM, optimal WER could not
be estimated. The proposed method outperfoms N best
list approach on every re-scoring task.
6 Conclusion
We proposed and demonstrated a new re-scoring
technique for general word graph structures such as
word lattices. We showed its efficacy by demonstrat-
ing huge reductions in the search effort to obtain a
new state-of-the-art performance on a very compet-
itive speech task of Broadcast news. As part of the
future work, we plan to extend this technique for hy-
pergraphs and lattices in re-scoring MT outputs with
complex and long span language models.
Acknowledgement
This work was partly funded by Human Language
Technology, Center of Excellence and by Tech-
nology Agency of the Czech Republic grant No.
TA01011328, and Grant Agency of Czech Repub-
lic project No. 102/08/0707. We would also like to
acknowledge the contribution of Frederick Jelinek
towards this work. He would be a co-author if he
were available and willing to give his consent.
1125
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A General and Efficient Weighted Finite-State Trans-
ducer Library. In Proceedings of the Ninth Interna-
tional Conference on Implementation and Application
of Automata, (CIAA 2007), volume 4783 of Lecture
Notes in Computer Science, pages 11?23. Springer.
J. R. Bellegarda. 2000. Exploiting latent semantic infor-
mation in statistical language modeling. Proceedings
of IEEE, 88(8):1279?1296.
Yoshua Bengio, Re?jean Ducharme, and Pascal Vincent.
2001. A Neural Probabilistic Language Model. In
Proceedings of Advances in Neural Information Pro-
cessing Systems.
Peter Beyerlein. 1998. Discriminative Model Combina-
tion. In Proc. of IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP).
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Ciprian Chelba and Frederick Jelinek. 2000. Struc-
tured Language Modeling. Computer Speech and Lan-
guage, 14(4):283?332.
S. F. Chen, L. Mangu, B. Ramabhadran, R. Sarikaya,
and A. Sethy. 2009. Scaling shrinkage-based lan-
guage models. In Proc. of IEEE Workshop on Auto-
matic Speech Recognition and Understanding (ASRU),
pages 299?304.
Noam Chomsky. 1957. Syntactic Structures. The
Hague: Mouton.
Yen-Lu Chow and Richard Schwartz. 1989. The N-Best
algorithm: an efficient procedure for finding top N sen-
tence hypotheses. In Proceedings of the workshop on
Speech and Natural Language, HLT ?89, pages 199?
202, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Kenneth Church. 2012. A Pendulum Swung Too Far.
Linguistic Issues in Language Technology - LiLT. to
appear.
T.M. Cover and J.A.Thomas. 1991. Elements of Infor-
mation Theory. John Wiley and Sons, Inc. N.Y.
Anoop Deoras and Frederick Jelinek. 2009. Iterative De-
coding: A Novel Re-Scoring Framework for Confu-
sion Networks. In Proc. of IEEE Workshop on Auto-
matic Speech Recognition and Understanding (ASRU),
pages 282 ?286.
Anoop Deoras, Denis Filimonov, Mary Harper, and Fred
Jelinek. 2010. Model Combination for Speech Recog-
nition using Empirical Bayes Risk Minimization. In
Proc. of IEEE Workshop on Spoken Language Tech-
nology (SLT).
Anoop Deoras, Toma?s? Mikolov, Stefan Kombrink, Mar-
tin Karafia?t, and Sanjeev Khudanpur. 2011. Varia-
tional Approximation of Long-Span Language Mod-
els for LVCSR. In Proc. of IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP).
Jeffery Elman. 1990. Finding Structure in Time. In Cog-
nitive Science, volume 14, pages 179?211.
Denis Filimonov and Mary Harper. 2009. A Joint Lan-
guage Model with Fine-grain Syntactic Tags. In Proc.
of 2009 Conference on Empirical Methods in Natural
Language Processing.
V. Goel and W. Byrne. 2000. Minimum Bayes Risk Au-
tomatic Speech Recognition. Computer, Speech and
Language.
Rukmini Iyer and Mari Ostendorf. 1999. Modeling Long
Distance Dependence in Language: Topic Mixtures
Versus Dynamic Cache Models. IEEE Transactions
on Speech and Audio Processing, 7(1):30?39.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 40?51, Singapore,
August.
Lidia Luminita Mangu. 2000. Finding consensus in
speech recognition. Ph.D. thesis, The Johns Hopkins
University. Adviser-Brill, Eric.
Toma?s? Mikolov, Martin Karafia?t, Luka?s? Burget,
Jan ?Honza? C?ernocky?, and Sanjeev Khudanpur.
2010. Recurrent Neural Network Based Language
Model. In Proc. of the ICSLP-Interspeech.
Toma?s? Mikolov, Anoop Deoras, Stefan Kombrink, Luka?s?
Burget, and Jan ?Honza? C?ernocky?. 2011a. Empirical
Evaluation and Combination of Advanced Language
Modeling Techniques. In Proc. of Interspeech.
Toma?s? Mikolov, Stefan Kombrink, Luka?s? Burget,
Jan ?Honza? C?ernocky?, and Sanjeev Khudanpur.
2011b. Extensions of Recurrent Neural Network Lan-
guage Model. In Proc. of IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP).
Mehryar Mohri and Michael Riley. 2002. An Efficient
Algorithm for the N-Best-Strings Problem. In Pro-
ceedings of the International Conference on Spoken
Language Processing (ICSLP).
M. Mohri, F.C.N. Pereira, and M. Riley. 2000. The de-
sign principles of a weighted finite-state transducer li-
brary. Theoretical Computer Science, 231:17-32.
A. Ogawa, K. Takeda, and F. Itakura. 1998. Balanc-
ing Acoustic and Linguistic Probabilities. In Proc. of
IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP).
1126
F. Richardson, M. Ostendorf, and J.R. Rohlicek. 1995.
Lattice-based search strategies for large vocabulary
speech recognition. In Proc. of IEEE International
Conference on Acoustics, Speech, and Signal Process-
ing (ICASSP).
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu. 2001.
Whole-Sentence Exponential Language Models: a Ve-
hicle for Linguistic-Statistical Integration. Computer
Speech and Language, 15(1).
Roni Rosenfeld. 1997. A Whole Sentence Maximum
Entropy Language Model. In Proc. of IEEE workshop
on Automatic Speech Recognition and Understanding
(ASRU), Santa Barbara, California, December.
D.E. Rumelhart, G. E. Hinton, and R.J. Williams. 1986.
Learning representations by back-propagating errors.
Nature, 323:533?536.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21(3):492?
518.
C. E. Shannon. 1948. A Mathematical Theory of
Communication. The Bell System Technical Journal,
27:379?423, 623?656.
H. Soltau, G. Saon, and B. Kingsbury. 2010. The IBM
Attila speech recognition toolkit. In Proc. of IEEE
Workshop on Spoken Language Technology (SLT).
Wen Wang and Mary Harper. 2002. The SuperARV lan-
guage model: investigating the effectiveness of tightly
integrating multiple knowledge sources. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP).
1127
Proceedings of NAACL-HLT 2013, pages 746?751,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Linguistic Regularities in Continuous Space Word Representations
Tomas Mikolov? , Wen-tau Yih, Geoffrey Zweig
Microsoft Research
Redmond, WA 98052
Abstract
Continuous space language models have re-
cently demonstrated outstanding results across
a variety of tasks. In this paper, we ex-
amine the vector-space word representations
that are implicitly learned by the input-layer
weights. We find that these representations
are surprisingly good at capturing syntactic
and semantic regularities in language, and
that each relationship is characterized by a
relation-specific vector offset. This allows
vector-oriented reasoning based on the offsets
between words. For example, the male/female
relationship is automatically learned, and with
the induced vector representations, ?King -
Man + Woman? results in a vector very close
to ?Queen.? We demonstrate that the word
vectors capture syntactic regularities by means
of syntactic analogy questions (provided with
this paper), and are able to correctly answer
almost 40% of the questions. We demonstrate
that the word vectors capture semantic regu-
larities by using the vector offset method to
answer SemEval-2012 Task 2 questions. Re-
markably, this method outperforms the best
previous systems.
1 Introduction
A defining feature of neural network language mod-
els is their representation of words as high dimen-
sional real valued vectors. In these models (Ben-
gio et al, 2003; Schwenk, 2007; Mikolov et al,
2010), words are converted via a learned lookup-
table into real valued vectors which are used as the
?Currently at Google, Inc.
inputs to a neural network. As pointed out by the
original proposers, one of the main advantages of
these models is that the distributed representation
achieves a level of generalization that is not possi-
ble with classical n-gram language models; whereas
a n-gram model works in terms of discrete units that
have no inherent relationship to one another, a con-
tinuous space model works in terms of word vectors
where similar words are likely to have similar vec-
tors. Thus, when the model parameters are adjusted
in response to a particular word or word-sequence,
the improvements will carry over to occurrences of
similar words and sequences.
By training a neural network language model, one
obtains not just the model itself, but also the learned
word representations, which may be used for other,
potentially unrelated, tasks. This has been used to
good effect, for example in (Collobert and Weston,
2008; Turian et al, 2010) where induced word rep-
resentations are used with sophisticated classifiers to
improve performance in many NLP tasks.
In this work, we find that the learned word repre-
sentations in fact capture meaningful syntactic and
semantic regularities in a very simple way. Specif-
ically, the regularities are observed as constant vec-
tor offsets between pairs of words sharing a par-
ticular relationship. For example, if we denote the
vector for word i as xi, and focus on the singu-
lar/plural relation, we observe that xapple?xapples ?
xcar?xcars, xfamily?xfamilies ? xcar?xcars, and
so on. Perhaps more surprisingly, we find that this
is also the case for a variety of semantic relations, as
measured by the SemEval 2012 task of measuring
relation similarity.
746
The remainder of this paper is organized as fol-
lows. In Section 2, we discuss related work; Section
3 describes the recurrent neural network language
model we used to obtain word vectors; Section 4 dis-
cusses the test sets; Section 5 describes our proposed
vector offset method; Section 6 summarizes our ex-
periments, and we conclude in Section 7.
2 Related Work
Distributed word representations have a long his-
tory, with early proposals including (Hinton, 1986;
Pollack, 1990; Elman, 1991; Deerwester et al,
1990). More recently, neural network language
models have been proposed for the classical lan-
guage modeling task of predicting a probability dis-
tribution over the ?next? word, given some preced-
ing words. These models were first studied in the
context of feed-forward networks (Bengio et al,
2003; Bengio et al, 2006), and later in the con-
text of recurrent neural network models (Mikolov et
al., 2010; Mikolov et al, 2011b). This early work
demonstrated outstanding performance in terms of
word-prediction, but also the need for more compu-
tationally efficient models. This has been addressed
by subsequent work using hierarchical prediction
(Morin and Bengio, 2005; Mnih and Hinton, 2009;
Le et al, 2011; Mikolov et al, 2011b; Mikolov et
al., 2011a). Also of note, the use of distributed
topic representations has been studied in (Hinton
and Salakhutdinov, 2006; Hinton and Salakhutdi-
nov, 2010), and (Bordes et al, 2012) presents a se-
mantically driven method for obtaining word repre-
sentations.
3 Recurrent Neural Network Model
The word representations we study are learned by a
recurrent neural network language model (Mikolov
et al, 2010), as illustrated in Figure 1. This architec-
ture consists of an input layer, a hidden layer with re-
current connections, plus the corresponding weight
matrices. The input vector w(t) represents input
word at time t encoded using 1-of-N coding, and the
output layer y(t) produces a probability distribution
over words. The hidden layer s(t) maintains a rep-
resentation of the sentence history. The input vector
w(t) and the output vector y(t) have dimensional-
ity of the vocabulary. The values in the hidden and
Figure 1: Recurrent Neural Network Language Model.
output layers are computed as follows:
s(t) = f (Uw(t) +Ws(t?1)) (1)
y(t) = g (Vs(t)) , (2)
where
f(z) =
1
1 + e?z
, g(zm) =
ezm
?
k e
zk
. (3)
In this framework, the word representations are
found in the columns of U, with each column rep-
resenting a word. The RNN is trained with back-
propagation to maximize the data log-likelihood un-
der the model. The model itself has no knowledge
of syntax or morphology or semantics. Remark-
ably, training such a purely lexical model to max-
imize likelihood will induce word representations
with striking syntactic and semantic properties.
4 Measuring Linguistic Regularity
4.1 A Syntactic Test Set
To understand better the syntactic regularities which
are inherent in the learned representation, we created
a test set of analogy questions of the form ?a is to b
as c is to ? testing base/comparative/superlative
forms of adjectives; singular/plural forms of com-
mon nouns; possessive/non-possessive forms of
common nouns; and base, past and 3rd person
present tense forms of verbs. More precisely, we
tagged 267M words of newspaper text with Penn
747
Category Relation Patterns Tested # Questions Example
Adjectives Base/Comparative JJ/JJR, JJR/JJ 1000 good:better rough:
Adjectives Base/Superlative JJ/JJS, JJS/JJ 1000 good:best rough:
Adjectives Comparative/
Superlative
JJS/JJR, JJR/JJS 1000 better:best rougher:
Nouns Singular/Plural NN/NNS,
NNS/NN
1000 year:years law:
Nouns Non-possessive/
Possessive
NN/NN POS,
NN POS/NN
1000 city:city?s bank:
Verbs Base/Past VB/VBD,
VBD/VB
1000 see:saw return:
Verbs Base/3rd Person
Singular Present
VB/VBZ, VBZ/VB 1000 see:sees return:
Verbs Past/3rd Person
Singular Present
VBD/VBZ,
VBZ/VBD
1000 saw:sees returned:
Table 1: Test set patterns. For a given pattern and word-pair, both orderings occur in the test set. For example, if
?see:saw return: ? occurs, so will ?saw:see returned: ?.
Treebank POS tags (Marcus et al, 1993). We then
selected 100 of the most frequent comparative adjec-
tives (words labeled JJR); 100 of the most frequent
plural nouns (NNS); 100 of the most frequent pos-
sessive nouns (NN POS); and 100 of the most fre-
quent base form verbs (VB). We then systematically
generated analogy questions by randomly matching
each of the 100 words with 5 other words from the
same category, and creating variants as indicated in
Table 1. The total test set size is 8000. The test set
is available online. 1
4.2 A Semantic Test Set
In addition to syntactic analogy questions, we used
the SemEval-2012 Task 2, Measuring Relation Sim-
ilarity (Jurgens et al, 2012), to estimate the extent
to which RNNLM word vectors contain semantic
information. The dataset contains 79 fine-grained
word relations, where 10 are used for training and
69 testing. Each relation is exemplified by 3 or
4 gold word pairs. Given a group of word pairs
that supposedly have the same relation, the task is
to order the target pairs according to the degree to
which this relation holds. This can be viewed as an-
other analogy problem. For example, take the Class-
Inclusion:Singular Collective relation with the pro-
1http://research.microsoft.com/en-
us/projects/rnn/default.aspx
totypical word pair clothing:shirt. To measure the
degree that a target word pair dish:bowl has the same
relation, we form the analogy ?clothing is to shirt as
dish is to bowl,? and ask how valid it is.
5 The Vector Offset Method
As we have seen, both the syntactic and semantic
tasks have been formulated as analogy questions.
We have found that a simple vector offset method
based on cosine distance is remarkably effective in
solving these questions. In this method, we assume
relationships are present as vector offsets, so that in
the embedding space, all pairs of words sharing a
particular relation are related by the same constant
offset. This is illustrated in Figure 2.
In this model, to answer the analogy question a:b
c:d where d is unknown, we find the embedding
vectors xa, xb, xc (all normalized to unit norm), and
compute y = xb ? xa + xc. y is the continuous
space representation of the word we expect to be the
best answer. Of course, no word might exist at that
exact position, so we then search for the word whose
embedding vector has the greatest cosine similarity
to y and output it:
w? = argmaxw
xwy
?xw??y?
When d is given, as in our semantic test set, we
simply use cos(xb ? xa + xc, xd) for the words
748
Figure 2: Left panel shows vector offsets for three word
pairs illustrating the gender relation. Right panel shows
a different projection, and the singular/plural relation for
two words. In high-dimensional space, multiple relations
can be embedded for a single word.
provided. We have explored several related meth-
ods and found that the proposed method performs
well for both syntactic and semantic relations. We
note that this measure is qualitatively similar to rela-
tional similarity model of (Turney, 2012), which pre-
dicts similarity between members of the word pairs
(xb, xd), (xc, xd) and dis-similarity for (xa, xd).
6 Experimental Results
To evaluate the vector offset method, we used
vectors generated by the RNN toolkit of Mikolov
(2012). Vectors of dimensionality 80, 320, and 640
were generated, along with a composite of several
systems, with total dimensionality 1600. The sys-
tems were trained with 320M words of Broadcast
News data as described in (Mikolov et al, 2011a),
and had an 82k vocabulary. Table 2 shows results
for both RNNLM and LSA vectors on the syntactic
task. LSA was trained on the same data as the RNN.
We see that the RNN vectors capture significantly
more syntactic regularity than the LSA vectors, and
do remarkably well in an absolute sense, answering
more than one in three questions correctly. 2
In Table 3 we compare the RNN vectors with
those based on the methods of Collobert and We-
ston (2008) and Mnih and Hinton (2009), as imple-
mented by (Turian et al, 2010) and available online
3 Since different words are present in these datasets,
we computed the intersection of the vocabularies of
the RNN vectors and the new vectors, and restricted
the test set and word vectors to those. This resulted
in a 36k word vocabulary, and a test set with 6632
2Guessing gets a small fraction of a percent.
3http://metaoptimize.com/projects/wordreprs/
Method Adjectives Nouns Verbs All
LSA-80 9.2 11.1 17.4 12.8
LSA-320 11.3 18.1 20.7 16.5
LSA-640 9.6 10.1 13.8 11.3
RNN-80 9.3 5.2 30.4 16.2
RNN-320 18.2 19.0 45.0 28.5
RNN-640 21.0 25.2 54.8 34.7
RNN-1600 23.9 29.2 62.2 39.6
Table 2: Results for identifying syntactic regularities for
different word representations. Percent correct.
Method Adjectives Nouns Verbs All
RNN-80 10.1 8.1 30.4 19.0
CW-50 1.1 2.4 8.1 4.5
CW-100 1.3 4.1 8.6 5.0
HLBL-50 4.4 5.4 23.1 13.0
HLBL-100 7.6 13.2 30.2 18.7
Table 3: Comparison of RNN vectors with Turian?s Col-
lobert and Weston based vectors and the Hierarchical
Log-Bilinear model of Mnih and Hinton. Percent correct.
questions. Turian?s Collobert andWeston based vec-
tors do poorly on this task, whereas the Hierarchical
Log-Bilinear Model vectors of (Mnih and Hinton,
2009) do essentially as well as the RNN vectors.
These representations were trained on 37M words
of data and this may indicate a greater robustness of
the HLBL method.
We conducted similar experiments with the se-
mantic test set. For each target word pair in a rela-
tion category, the model measures its relational sim-
ilarity to each of the prototypical word pairs, and
then uses the average as the final score. The results
are evaluated using the two standard metrics defined
in the task, Spearman?s rank correlation coefficient
? and MaxDiff accuracy. In both cases, larger val-
ues are better. To compare to previous systems, we
report the average over all 69 relations in the test set.
From Table 4, we see that as with the syntac-
tic regularity study, the RNN-based representations
perform best. In this case, however, Turian?s CW
vectors are comparable in performance to the HLBL
vectors. With the RNN vectors, the performance im-
proves as the number of dimensions increases. Sur-
prisingly, we found that even though the RNN vec-
749
Method Spearman?s ? MaxDiff Acc.
LSA-640 0.149 0.364
RNN-80 0.211 0.389
RNN-320 0.259 0.408
RNN-640 0.270 0.416
RNN-1600 0.275 0.418
CW-50 0.159 0.363
CW-100 0.154 0.363
HLBL-50 0.149 0.363
HLBL-100 0.146 0.362
UTD-NB 0.230 0.395
Table 4: Results in measuring relation similarity
tors are not trained or tuned specifically for this task,
the model achieves better results (RNN-320, RNN-
640 & RNN-1600) than the previously best perform-
ing system, UTD-NB (Rink and Harabagiu, 2012).
7 Conclusion
We have presented a generally applicable vector off-
set method for identifying linguistic regularities in
continuous space word representations. We have
shown that the word representations learned by a
RNNLM do an especially good job in capturing
these regularities. We present a new dataset for mea-
suring syntactic performance, and achieve almost
40% correct. We also evaluate semantic general-
ization on the SemEval 2012 task, and outperform
the previous state-of-the-art. Surprisingly, both re-
sults are the byproducts of an unsupervised maxi-
mum likelihood training criterion that simply oper-
ates on a large amount of text data.
References
Y. Bengio, R. Ducharme, Vincent, P., and C. Jauvin.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Reseach, 3(6).
Y. Bengio, H. Schwenk, J.S. Sene?cal, F. Morin, and J.L.
Gauvain. 2006. Neural probabilistic language models.
Innovations in Machine Learning, pages 137?186.
A. Bordes, X. Glorot, J. Weston, and Y. Bengio. 2012.
Joint learning of words and meaning representations
for open-text semantic parsing. In Proceedings of 15th
International Conference on Artificial Intelligence and
Statistics.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. In Proceedings of the 25th
international conference on Machine learning, pages
160?167. ACM.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41(96).
J.L. Elman. 1991. Distributed representations, simple re-
current networks, and grammatical structure. Machine
learning, 7(2):195?225.
G.E. Hinton and R.R. Salakhutdinov. 2006. Reducing
the dimensionality of data with neural networks. Sci-
ence, 313(5786):504?507.
G. Hinton and R. Salakhutdinov. 2010. Discovering bi-
nary codes for documents by learning deep generative
models. Topics in Cognitive Science, 3(1):74?91.
G.E. Hinton. 1986. Learning distributed representations
of concepts. In Proceedings of the eighth annual con-
ference of the cognitive science society, pages 1?12.
Amherst, MA.
David Jurgens, Saif Mohammad, Peter Turney, and Keith
Holyoak. 2012. Semeval-2012 task 2: Measuring de-
grees of relational similarity. In *SEM 2012: The First
Joint Conference on Lexical and Computational Se-
mantics (SemEval 2012), pages 356?364. Association
for Computational Linguistics.
Hai-Son Le, I. Oparin, A. Allauzen, J.-L. Gauvain, and
F. Yvon. 2011. Structured output layer neural network
language model. In Proceedings of ICASSP 2011.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the penn treebank. Computational Lin-
guistics, 19(2):313?330.
Tomas Mikolov, Martin Karafiat, Jan Cernocky, and San-
jeev Khudanpur. 2010. Recurrent neural network
based language model. In Proceedings of Interspeech
2010.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernocky. 2011a. Strategies for
Training Large Scale Neural Network Language Mod-
els. In Proceedings of ASRU 2011.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky, and Sanjeev Khudanpur. 2011b. Ex-
tensions of recurrent neural network based language
model. In Proceedings of ICASSP 2011.
Tomas Mikolov. 2012. RNN toolkit.
A. Mnih and G.E. Hinton. 2009. A scalable hierarchical
distributed language model. Advances in neural infor-
mation processing systems, 21:1081?1088.
F. Morin and Y. Bengio. 2005. Hierarchical probabilistic
neural network language model. In Proceedings of the
750
international workshop on artificial intelligence and
statistics, pages 246?252.
J.B. Pollack. 1990. Recursive distributed representa-
tions. Artificial Intelligence, 46(1):77?105.
Bryan Rink and Sanda Harabagiu. 2012. UTD: Deter-
mining relational similarity using lexical patterns. In
*SEM 2012: The First Joint Conference on Lexical
and Computational Semantics (SemEval 2012), pages
413?418. Association for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21(3):492
? 518.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of Association for
Computational Linguistics (ACL 2010).
P.D. Turney. 2012. Domain and function: A dual-space
model of semantic relations and compositions. Jour-
nal of Artificial Intelligence Research, 44:533?585.
751
Proceedings of NAACL-HLT 2013, pages 1000?1009,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Combining Heterogeneous Models for Measuring Relational Similarity
Alisa Zhila?
Instituto Politecnico Nacional
Mexico City, Mexico
alisa.zhila@gmail.com
Wen-tau Yih Christopher Meek
Microsoft Research
Redmond, WA 98052, USA
{scottyih,meek}@microsoft.com
Geoffrey Zweig
Microsoft Research
Redmond, WA 98052, USA
gzweig@microsoft.com
Tomas Mikolov?
BRNO University of Technology
BRNO, Czech Republic
tmikolov@gmail.com
Abstract
In this work, we study the problem of mea-
suring relational similarity between two word
pairs (e.g., silverware:fork and clothing:shirt).
Due to the large number of possible relations,
we argue that it is important to combine mul-
tiple models based on heterogeneous informa-
tion sources. Our overall system consists of
two novel general-purpose relational similar-
ity models and three specific word relation
models. When evaluated in the setting of a
recently proposed SemEval-2012 task, our ap-
proach outperforms the previous best system
substantially, achieving a 54.1% relative in-
crease in Spearman?s rank correlation.
1 Introduction
The problem of measuring relational similarity is
to determine the degree of correspondence between
two word pairs. For instance, the analogous word
pairs silverware:fork and clothing:shirt both exem-
plify well a Class-Inclusion:Singular Collective re-
lation and thus have high relational similarity. Un-
like the problem of attributional similarity, which
measures whether two words share similar attributes
and is addressed in extensive research work (Bu-
danitsky and Hirst, 2006; Reisinger and Mooney,
2010; Radinsky et al, 2011; Agirre et al, 2009; Yih
and Qazvinian, 2012), measuring relational similar-
ity is a relatively new research direction pioneered
by Turney (2006), but with many potential appli-
cations. For instance, problems of identifying spe-
cific relations between words, such as synonyms,
?Work conducted while interning at Microsoft Research.
antonyms or associations, can be reduced to mea-
suring relational similarity compared to prototypical
word pairs with the desired relation (Turney, 2008).
In scenarios like information extraction or question
answering, where identifying the existence of cer-
tain relations is often the core problem, measuring
relational similarity provides a more flexible solu-
tion rather than creating relational classifiers for pre-
defined or task-specific categories of relations (Tur-
ney, 2006; Jurgens et al, 2012).
In order to promote this research direction, Ju-
rgens et al (2012) proposed a new shared task of
measuring relational similarity in SemEval-2012 re-
cently. In this task, each submitted system is re-
quired to judge the degree of a target word pair
having a particular relation, measured by its re-
lational similarity compared to a few prototypical
example word pairs. The system performance is
evaluated by its correlation with the human judg-
ments using two evaluation metrics, Spearman?s
rank correlation and MaxDiff accuracy (more de-
tails of the task and evaluation metrics will be given
in Sec. 3). Although participating systems incorpo-
rated substantial amounts of information from lex-
ical resources (e.g., WordNet) and contextual pat-
terns from large corpora, only one system (Rink and
Harabagiu, 2012) is able to outperform a simple
baseline that uses PMI (pointwise mutual informa-
tion) scoring, which demonstrates the difficulty of
this task.
In this paper, we explore the problem of mea-
suring relational similarity in the same task setting.
We argue that due to the large number of possible
relations, building an ensemble of relational simi-
1000
larity models based on heterogeneous information
sources is the key to advance the state-of-the-art on
this problem. By combining two general-purpose re-
lational similarity models with three specific word-
relation models covering relations like IsA and syn-
onymy/antonymy, we improve the previous state-
of-the-art substantially ? having a relative gain of
54.1% in Spearman?s rank correlation and 14.7% in
the MaxDiff accuracy!
Our main contributions are threefold. First, we
propose a novel directional similarity method based
on the vector representation of words learned from
a recurrent neural network language model. The re-
lation of two words is captured by their vector off-
set in the latent semantic space. Similarity of rela-
tions can then be naturally measured by a distance
function in the vector space. This method alone
already performs better than all existing systems.
Second, unlike the previous finding, where SVMs
learn a much poorer model than naive Bayes (Rink
and Harabagiu, 2012), we show that using a highly-
regularized log-linear model on simple contextual
pattern features collected from a document collec-
tion of 20GB, a discriminative approach can learn a
strong model as well. Third, we demonstrate that by
augmenting existing word-relation models, which
cover only a small number of relations, the overall
system can be further improved.
The rest of this paper is organized as follows. We
first survey the related work in Sec. 2 and formally
define the problem in Sec. 3. We describe the indi-
vidual models in detail in Sec. 4. The combination
approach is depicted in Sec. 5, along with experi-
mental comparisons to individual models and exist-
ing systems. Finally, Sec. 6 concludes the paper.
2 Related Work
Building a classifier to determine whether a relation-
ship holds between a pair of words is a natural ap-
proach to the task of measuring relational similarity.
While early work was mostly based on hand-crafted
rules (Finin, 1980; Vanderwende, 1994), Rosario
and Hearst (2001) introduced a machine learning ap-
proach to classify word pairs. They targeted clas-
sifying noun modifier pairs from the medical do-
main into 13 classes of semantic relations. Fea-
tures for each noun modifier pair were constructed
using large medical lexical resources and a multi-
class classifier was trained using a feed-forward neu-
ral network with one hidden layer. This work was
later extended by Nastase and Szpakowicz (2003)
to classify general domain noun-modifier pairs into
30 semantic relations. In addition to extracting fea-
tures using WordNet and Roget?s Thesaurus, they
also experimented with several different learners in-
cluding decision trees, memory-based learning and
inductive logic programming methods like RIPPER
and FOIL. Using the same dataset as in (Nastase
and Szpakowicz, 2003), Turney and Littman (2005)
created a 128-dimentional feature vector for each
word pair based on statistics of their co-occurrence
patterns in Web documents and applied the k-NN
method (k = 1 in their work).
Measuring relational similarity, which determines
whether two word pairs share the same relation, can
be viewed as an extension of classifying relations
between two words. Treating a relational similar-
ity measure as a distance metric, a testing pair of
words can be judged by whether they have a rela-
tion that is similar to some prototypical word pairs
having a particular relation. A multi-relation clas-
sifier can thus be built easily in this framework as
demonstrated in (Turney, 2008), where the prob-
lems of identifying synonyms, antonyms and asso-
ciated words are all reduced to finding good anal-
ogous word pairs. Measuring relational similarity
has been advocated and pioneered by Turney (2006),
who proposed a latent vector space model for an-
swering SAT analogy questions (e.g., mason:stone
vs. carpenter:wood). In contrast, we take a slightly
different view when building a relational similarity
measure. Existing classifiers for specific word re-
lations (e.g., synonyms or Is-A) are combined with
general relational similarity measures. Empirically,
mixing heterogeneous models tends to make the fi-
nal relational similarity measure more robust.
Although datasets for semantic relation classifica-
tion or SAT analogous questions can be used to eval-
uate a relational similarity model, their labels are ei-
ther binary or categorical, which makes the datasets
suboptimal for determining the quality of a model
when evaluated on instances of the same relation
class. As a result, Jurgens et al (2012) proposed a
new task of ?Measuring Degrees of Relational Simi-
larity? at SemEval-2012, which includes 79 relation
1001
categories exemplified by three or four prototypical
word pairs and a schematic description. For exam-
ple, for the Class-Inclusion:Taxonomic relation, the
schematic description is ?Y is a kind/type/instance
of X?. Using Amazon Mechanical Turk1, they col-
lected word pairs for each relation, as well as their
degrees of being a good representative of a partic-
ular relation when compared with defining exam-
ples. Participants of this shared task proposed var-
ious kinds of approaches that leverage both lexical
resources and general corpora. For instance, the
Duluth systems (Pedersen, 2012) created word vec-
tors based on WordNet and estimated the degree of
a relation using cosine similarity. The BUAP sys-
tem (Tovar et al, 2012) represented each word pair
as a whole by a vector of 4 different types of fea-
tures: context, WordNet, POS tags and the aver-
age number of words separating the two words in
text. The degree of relation was then determined
by the cosine distance of the target pair from the
prototypical examples of each relation. Although
their models incorporated a significant amount of
information of words or word pairs, unfortunately,
the performance were not much better than a ran-
dom baseline, which indicates the difficulty of this
task. In comparison, a supervised learning approach
seems more promising. The UTD system (Rink and
Harabagiu, 2012), which mined lexical patterns be-
tween co-occurring words in the corpus and then
used them as features to train a Naive Bayes classi-
fier, achieved the best results. However, potentially
due to the large feature space, this strategy did not
work as well when switching the learning algorithm
to SVMs.
3 Problem Definition & Task Description
Following the setting of SemEval-2012 Task 2 (Ju-
rgens et al, 2012), the problem of measur-
ing the degree of relational similarity is to rate
word pairs by the degree to which they are
prototypical members of a given relation class.
For instance, comparing to the prototypical word
pairs, {cutlery:spoon, clothing:shirt, vermin:rat} of
the Class-Inclusion:Singular Collective relation, we
would like to know among the input word pairs
{dish:bowl, book:novel, furniture:desk}, which one
1http://www.mturk.com
best demonstrates the relation.
Because our approaches are evaluated using the
data provided in this SemEval-2012 task, we de-
scribe briefly below how the data was collected, as
well as the metrics used to evaluate system perfor-
mance. The dataset consists of 79 relation classes
that are chosen according to (Bejar et al, 1991)
and broadly fall into 10 main categories, includ-
ing Class-Inclusion, Part-Whole, Similar and more.
With the help of Amazon Mechanical Turk, Jurgens
et al (2012) used a two-phase approach to collect
word pairs and their degrees. In the first phase,
a lexical schema, such as ?a Y is one item in a
collection/group of X? for the aforementioned rela-
tion Class-Inclusion:Singular Collective, and a few
prototypical pairs for each class were given to the
workers, who were asked to provide approximately
a list of 40 word pairs representing the same rela-
tion class. Naturally, some of these pairs were bet-
ter examples than the others. Therefore, in the sec-
ond phase, the goal was to measure the degree of
their similarity to the corresponding relation. This
was done using the MaxDiff technique (Louviere
and Woodworth, 1991). For each relation, about one
hundred questions were first created. Each question
consists of four different word pairs randomly sam-
pled from the list. The worker was then asked to
choose the most and least representative word pairs
for the specific relation in each question.
The set of 79 word relations were randomly split
into training and testing sets. The former contains
10 relations and the latter has 69. Word pairs in all
79 relations were given to the task participants in ad-
vance, but only the human judgments of the training
set were available for system development. In this
work, we treat the training set as the validation set
? all the model exploration and refinement is done
using this set of data, as well as the hyper-parameter
tuning when learning the final model combination.
The quality of a relational similarity measure is
estimated by its correlation to human judgments.
This is evaluated using two metrics in the task: the
MaxDiff accuracy and Spearman?s rank correlation
coefficient (?). A system is first asked to pick the
most and least representative word pairs of each
question in the MaxDiff setting. The average accu-
racy of the predictions compared to the human an-
swers is then reported. In contrast, Spearman?s ?
1002
measures the correlation between the total orderings
of all word pairs of a relation, where the total order-
ing is derived from the MaxDiff answers (see (Jur-
gens et al, 2012) for the exact procedure).
4 Models for Relational Similarity
We investigate three types of models for relational
similarity. Operating in a word vector space, the di-
rectional similarity model compares the vector dif-
ferences of target and prototypical word pairs to es-
timate their relational similarity. The lexical pat-
tern method collects contextual information of pairs
of words when they co-occur in large corpora, and
learns a highly regularized log-linear model. Finally,
the word relation models incorporate existing, spe-
cific word relation measures for general relational
similarity.
4.1 Directional Similarity Model
Our first model for relational similarity extends pre-
vious work on semantic word vector representa-
tions to a directional similarity model for pairs of
words. There are many different methods for cre-
ating real-valued semantic word vectors, such as
the distributed representation derived from a word
co-occurrence matrix and a low-rank approxima-
tion (Landauer et al, 1998), word clustering (Brown
et al, 1992) and neural-network language model-
ing (Bengio et al, 2003; Mikolov et al, 2010). Each
element in the vectors conceptually represents some
latent topicality information of the word. The goal
of these methods is that words with similar mean-
ings will tend to be close to each other in the vector
space.
Although the vector representation of single
words has been successfully applied to problems
like semantic word similarity and text classifica-
tion (Turian et al, 2010), the issue of how to repre-
sent and compare pairs of words in a vector space
remains unclear (Turney, 2012). In a companion
paper (Mikolov et al, 2013), we present a vector
offset method which performs consistently well in
identifying both syntactic and semantic regularities.
This method measures the degree of the analogy
?a is to b as c is to d? using the cosine score of
(~vb?~va +~vc, ~vd), where a, b, c, d are the four given
words and ~va, ~vb, ~vc, ~vd are the corresponding vec-
q 
shirt
clothing
furniture
desk
v1
v2'
v2'
Figure 1: Directional vectors ?1 and ?2 capture the rela-
tions of clothing:shirt and furniture:desk respectively in
this semantic vector space. The relational similarity of
these two word pairs is estimated by the cosine of ?.
tors. In this paper, we propose a variant called the
directional similarity model, which performs bet-
ter for semantic relations. Let ?i = (wi1 , wi2) and
?j = (wj1 , wj2) be the two word pairs being com-
pared. Suppose (~vi1 , ~vi2) and (~vj1 , ~vj2) are the cor-
responding vectors of these words. The directional
vectors of ?i and ?j are defined as ~?i ? ~vi2 ? ~vi1
and ~?j ? ~vj2 ? ~vj1 , respectively. Relational simi-
larity of these two word pairs can be measured by
some distance function of ?i and ?j , such as the co-
sine function:
~?i ? ~?j
?~?i??~?j?
The rationale behind this variant is as follows. Be-
cause the difference of two word vectors reveals the
change from one word to the other in terms of mul-
tiple topicality dimensions in the vector space, two
word pairs having similar offsets (i.e., being rela-
tively parallel) can be interpreted as they have simi-
lar relations. Fig. 1 further illustrates this method.
Compared to the original method, this variant
places less emphasis on the similarity between
words wj1 and wj2 . That similarity is necessary
for syntactic relations where the words are often re-
lated by morphology, but not for semantic relations.
On semantic relations studied in this paper, the di-
rectional similarity model performs about 18% rela-
tively better in Spearman?s ? than the original one.
The quality of the directional similarity method
depends heavily on the underlying word vector
space model. We compared two choices with dif-
1003
Word Embedding Spearman?s ? MaxDiff Acc. (%)
LSA-80 0.055 34.6
LSA-320 0.066 34.4
LSA-640 0.102 35.7
RNNLM-80 0.168 37.5
RNNLM-320 0.214 39.1
RNNLM-640 0.221 39.2
RNNLM-1600 0.234 41.2
Table 1: Results of measuring relational similarity using
the directional similarity method, evaluated on the train-
ing set. The 1600-dimensional RNNLM vector space
achieves the highest Spearman?s ? and MaxDiff accuracy.
ferent dimensionality settings: the word embedding
learned from the recurrent neural network language
model (RNNLM)2 and the LSA vectors, both were
trained using the same Broadcast News corpus of
320M words as described in (Mikolov et al, 2011).
All the word vectors were first normalized to unit
vectors before applying the directional similarity
method. Given a target word pair, we computed
its relational similarity compared with the prototyp-
ical word pairs of the same relation. The average
of these measurements was taken as the final model
score. Table 1 summarizes the results when evalu-
ated on the training set. As shown in the table, the
RNNLM vectors consistently outperform their LSA
counterparts with the same dimensionality. In addi-
tion, more dimensions seem to preserve more infor-
mation and lead to better performance. Therefore,
we take the 1600-dimensional RNNLM vectors to
construct our final directional similarity model.
4.2 Lexical Pattern Model
Our second model for measuring relational similar-
ity is built based on lexical patterns. It is well-known
that contexts in which two words co-occur often pro-
vide useful cues for identifying the word relation.
For example, having observed frequent text frag-
ments like ?X such as Y?, it is likely that there is a
Class-Inclusion:Taxonomic relation between X and
Y; namely, Y is a type of X. Indeed, by mining lexical
patterns from a large corpus, the UTD system (Rink
and Harabagiu, 2012) managed to outperform other
participants in the SemEval-2012 task of measuring
relational similarity.
2http://www.fit.vutbr.cz/?imikolov/rnnlm
In order to find more co-occurrences of each pair
of words, we used a large document set that con-
sists of the Gigaword corpus (Parker et al, 2009),
Wikipedia and LA Times articles3, summing up to
more than 20 Gigabytes of texts. For each word
pair (w1, w2) that co-occur in a sentence, we col-
lected the words in between as its context (or so-
called ?raw pattern?). For instance, ?such as? would
be the context extracted from ?X such as Y? for
the word pair (X, Y). To reduce noise, contexts with
more than 9 words were dropped and 914,295 pat-
terns were collected in total.
Treating each raw pattern as a feature where the
value is the logarithm of the occurrence count, we
then built a probabilistic classifier to determine the
association of the context and relation. For each re-
lation, we treated all its word pairs as positive ex-
amples and all the word pairs in other relations as
negative examples4. 79 classifiers were trained in
total, where each one was trained using 3,218 ex-
amples. The degree of relational similarity of each
word pair can then be judged by the output of the
corresponding classifier5. Although this seems like a
standard supervised learning setting, the large num-
ber of features poses a challenge here. Using almost
1M features and 3,218 examples, the model could
easily overfit if not regularized properly, which may
explain why learning SVMs on pattern features per-
formed poorly (Rink and Harabagiu, 2012). In-
stead of employing explicit feature selection meth-
ods, we used an efficient L1 regularized log-linear
model learner (Andrew and Gao, 2007) and chose
the hyper-parameters based on model performance
on the training data. The final models we chose
were trained with L1 = 3, where 28,065 features
in average were selected automatically by the algo-
3We used a Nov-2010 dump of English Wikipedia, which
contains approximately 917M words after pre-processing. The
LA Times corpus consists of articles from 1985 to 2002 and has
about 1.1B words.
4Given that not all word pairs belonging to the same relation
category are equally good, removing those with low judgment
scores may help improve the quality of the labeled data. We
leave this study to future work.
5Training a separate classifier for each MaxDiff question us-
ing all words pairs except the four target pairs appears to be a
better setting, as it would avoid including the target pairs in the
training process. We did not use this setting because it is more
complicated and performed roughly the same empirically.
1004
rithm. The performance on the training data is 0.322
in Spearman?s ? and 41.8% in MaxDiff accuracy.
4.3 Word Relation Models
The directional similarity and lexical pattern mod-
els can be viewed as general purpose methods for
relational similarity as they do not differentiate the
specific relation categories. In contrast, for specific
word relations, there exist several high-quality meth-
ods. Although they are designed for detecting spe-
cific relations between words, incorporating them
could still improve the overall results. Next, we ex-
plore the use of some of these word relation mod-
els, including information encoded in the knowledge
base and a lexical semantic model for synonymy and
antonymy.
4.3.1 Knowledge Bases
Predetermined types of relations can often be
found in existing lexical and knowledge databases,
such as WordNet?s Is-A taxonomy and the exten-
sive relations stored in the NELL (Carlson et al,
2010) knowledge base. Although in theory, these
resources can be directly used to solve the problem
of relational similarity, such direct approaches often
suffer from two practical issues. First, the word cov-
erage of these databases is usually very limited and
it is common that the relation of a given word pair
is absent. Second, the degree of relation is often not
included, which makes the task of measuring the de-
gree of relational similarity difficult.
One counter example, however, is Probase (Wu
et al, 2012), which is a knowledge base that es-
tablishes connections between more than 2.5 mil-
lion concepts discovered automatically from the
Web. For the Is-A and Attribute relations it en-
codes, Probase also returns the probability that two
input words share the relation, based on the co-
occurrence frequency. We used some relations in
the training set to evaluate the quality of Probase.
For instance, its Is-A model performs exception-
ally well on the relation Class-Inclusion:Taxonomic,
reaching a high Spearman?s ? = 0.642 and MaxD-
iff accuracy 55.8%. Similarly, its Attribute model
performs better than our lexical pattern model
on Attribute:Agent Attribute-State with Spearman?s
? = 0.290 and MaxDiff accuracy 32.7%.
4.3.2 Lexical Semantics Measures
Most lexical semantics measures focus on the se-
mantic similarity or relatedness of two words. Since
our task focuses on distinguishing the difference be-
tween word pairs in the same relation category. The
crude relatedness model does not seem to help in our
preliminary experimental study. Instead, we lever-
age the recently proposed polarity-inducing latent
semantic analysis (PILSA) model (Yih et al, 2012),
which specifically estimates the degree of synonymy
and antonymy. This method first forms a signed co-
occurrence matrix using synonyms and antonyms in
a thesaurus and then generalizes it using a low-rank
approximation derived by SVD. Given two words,
the cosine score of their PILSA vectors tend to be
negative if they are antonymous and positive if syn-
onymous. When tested on the Similar:Synonymity
relation, it has a Spearman?s ? = 0.242 and MaxD-
iff accuracy 42.1%, both are better than those of our
directional similarity and lexical pattern models.
5 Model Combination
In order to fully leverage the diverse models pro-
posed in Sec. 4, we experiment with a model combi-
nation approach and conduct a model ablation study.
Performance of the combined and individual models
is evaluated using the test set and compared with ex-
isting systems.
We seek an optimal linear combination of all the
individual models by treating their output as fea-
tures and use a logistic regression learner to learn
the weights6. The training setting is essentially the
same as the one used to learn the lexical pattern
model (Sec. 4.2). For each relation, we treat all the
word pairs in this relation group as positive exam-
ples and all other word pairs as negative ones. Con-
sequently, 79 sets of weights for model combination
are learned in total. The average Spearman?s ? of the
10 training relations is used for selecting the values
of the L1 and L2 regularizers7. Evaluated on the re-
maining 69 relations (i.e., the test set), the average
results of each main relation group and the overall
6Nonlinear methods, such as MART (Friedman, 2001), do
not perform better in our experiments (not reported here).
7We tested 15 combinations, where L1 ? {0, 0.01, 0.1} and
L2 ? {0, 0.001, 0.01, 1, 10}. The parameter setting that gave
the highest Spearman rank correlation coefficient score on the
training set was selected.
1005
Relation Group Rand. BUAP DuluthV 0 UTDNB DS Pat. IsA Attr. PILSA Com.
Class-Inclusion 0.057 0.064 0.045 0.233 0.350 0.422 0.619 -0.137 0.029 0.519
Part-Whole 0.012 0.066 -0.061 0.252 0.317 0.244 -0.014 0.026 -0.010 0.329
Similar 0.026 -0.036 0.183 0.214 0.254 0.245 -0.020 0.133 0.058 0.303
Contrast -0.049 0.000 0.142 0.206 0.063 0.298 -0.012 -0.032 -0.079 0.268
Attribute 0.037 -0.095 0.044 0.158 0.431 0.198 -0.008 0.016 -0.052 0.406
Non-Attribute -0.070 0.009 0.079 0.098 0.195 0.117 0.036 0.078 -0.093 0.296
Case Relations 0.090 -0.037 -0.011 0.241 0.503 0.288 0.076 -0.075 0.059 0.473
Cause-Purpose -0.011 0.114 0.021 0.183 0.362 0.234 0.044 -0.059 0.038 0.296
Space-Time 0.013 0.035 0.055 0.375 0.439 0.248 0.064 -0.002 -0.018 0.443
Reference 0.142 -0.001 0.028 0.346 0.301 0.119 0.033 -0.123 0.021 0.208
Average 0.018 0.014 0.050 0.229 0.324? 0.235 0.058? -0.010? -0.009? 0.353?
Relation Group Rand. BUAP DuluthV 0 UTDNB DS Pat. IsA Attr. PILSA Com.
Class-Inclusion 30.1 29.0 26.7 39.1 46.7 43.4 59.6 24.7 32.3 51.2
Part-Whole 31.9 35.1 29.4 40.9 43.9 38.1 31.3 29.5 31.0 42.9
Similar 31.5 29.1 37.1 39.8 38.5 38.4 30.8 36.3 34.2 43.3
Contrast 30.4 32.4 38.3 40.9 33.6 42.2 32.3 31.8 30.1 42.8
Attribute 30.2 29.2 31.9 36.5 47.9 38.3 30.7 31.0 28.8 48.3
Non-Attribute 28.9 30.4 36.0 36.8 38.7 36.7 32.3 32.8 27.7 42.6
Case Relations 32.8 29.5 28.2 40.6 54.3 42.2 32.8 25.7 31.0 50.6
Cause-Purpose 30.8 35.4 29.5 36.3 45.3 38.0 30.3 28.1 32.0 41.7
Space-Time 30.6 32.5 31.9 43.2 50.0 39.2 33.2 29.3 30.6 47.7
Reference 35.1 30.0 31.9 41.2 45.7 36.9 30.4 27.2 30.2 42.5
Average 31.2 31.7 32.4 39.4 44.5? 39.2 33.3? 29.8? 30.7? 45.2?
Table 2: Average Spearman?s ? (Top) and MaxDiff accuracy (%) (Bottom) of each major relation group and all 69
testing relations. The best result in each row is highlighted in boldface font. Statistical significance tests are conducted
by comparing each of our systems with the previous best performing system, UTDNB . ? and ? indicate the difference
in the average results is statistically significant with 95% or 99% confidence level, respectively.
results are presented in Table 2. For comparison, we
also show the performance of a random baseline and
the best performing system of each participant in the
SemEval-2012 task.
We draw two conclusions from this table. First,
both of our general relational similarity models, the
directional similarity (DS) and lexical pattern (Pat)
models are fairly strong. The former outperforms
the previous best system UTDNB in both Spear-
man?s ? and MaxDiff accuracy, where the differ-
ences are statistically significant8; the latter has
comparable performance, where the differences are
not statistically significant. In contrast, while the
IsA relation from Probase is exceptionally good
in identifying Class-Inclusion relations, with high
Spearman?s ? = 0.619 and MaxDiff accuracy
8We conducted a paired-t test on the results of each of the
69 relation. The difference is considered statistically significant
if the p-value is less than 0.05.
59.6%, it does not have high correlations with hu-
man judgments in other relations. Like in the case of
Probase Attribute and PILSA, specific word-relation
models individually are not good measures for gen-
eral relational similarity. Second, as expected, com-
bining multiple diverse models (Com) is a robust
strategy, which provides the best overall perfor-
mance. It achieves superior results in both evalua-
tion metrics compared to UTDNB and only a lower
Spearman?s ? value in one of the ten relation groups
(namely, Reference). The differences are statisti-
cally significant with p-value less than 10?3.
In order to understand the interaction among dif-
ferent component models, we conducted an ablation
study by iteratively removing one model from the fi-
nal combination. The weights are re-trained using
the same procedure that finds the best regularization
parameters with the help of training data. Table 3
summarizes the results and compares them with the
1006
Spearman?s ? MaxDiff Accuracy (%)
Relation Group Com. -Attr -IsA -PILSA -DS -Pat Com. -Attr -IsA -PILSA -DS -Pat
Class-Inclusion 0.519 0.557 0.467 0.593 0.490 0.570 51.2 53.7 49.2 54.6 49.3 56.2
Part-Whole 0.329 0.326 0.335 0.331 0.277 0.285 42.9 42.1 42.6 41.8 38.5 42.9
Similar 0.303 0.269 0.302 0.281 0.256 0.144 43.3 41.2 42.7 40.5 40.2 38.9
Contrast 0.268 0.234 0.267 0.289 0.260 0.156 42.8 42.0 42.4 41.5 42.7 38.1
Attribute 0.406 0.409 0.405 0.433 0.164 0.447 48.3 47.8 48.2 49.1 36.9 49.0
Non-Attribute 0.296 0.287 0.296 0.276 0.123 0.283 42.6 42.9 42.6 41.8 36.0 43.0
Case Relations 0.473 0.497 0.470 0.484 0.309 0.498 50.6 52.5 50.2 50.9 42.9 53.2
Cause-Purpose 0.296 0.282 0.299 0.301 0.205 0.296 41.7 41.6 41.6 41.2 36.6 44.1
Space-Time 0.443 0.425 0.443 0.420 0.269 0.431 47.7 47.2 47.7 46.9 40.5 49.5
Reference 0.208 0.238 0.205 0.168 0.102 0.210 42.5 42.3 42.6 41.8 36.1 41.4
Average 0.353 0.348 0.350 0.354 0.238? 0.329 45.2 45.0 44.9? 44.7 39.6? 45.4
Table 3: Average Spearman?s ? and MaxDiff accuracy results of different model combinations. Com indicates combin-
ing all models, where other columns show the results when the specified model is removed. The best result in each row
is highlighted in boldface font. Statistical significance tests are conducted by comparing each ablation configuration
with Com. ? indicates the difference in the average results is statistically significant with 99% confidence level.
original combination model.
Overall, it is clear that the directional similarity
method based on RNNLM vectors is the most crit-
ical component model. Removing it from the fi-
nal combination decreases both the Spearman?s ?
and MaxDiff accuracy by a large margin; both dif-
ferences (Com vs. -DS) are statistically significant.
The Probase IsA model also has an important im-
pact on the performance on the Class-Inclusion re-
lation group. Eliminating the IsA model makes
the overall MaxDiff accuracy statistically signifi-
cantly lower (Com vs. -IsA). Again, the benefits
of incorporating Probase Attribute and PILSA mod-
els are not clear. Removing them from the final
combination lowers the MaxDiff accuracy, but nei-
ther the difference in Spearman?s ? nor MaxDiff
accuracy is statistically significant. Compared to
the RNNLM directional similarity model, the lex-
ical pattern model seems less critical. Removing
it lowers the Similar and Contrast relation groups,
but improves some other relation groups like Class-
Inclusion and Case Relations. The final MaxDiff ac-
curacy becomes slightly higher but the Spearman?s
? drops a little (Com vs. -Pat); neither is statistically
significant.
Notice that the main purpose of the ablation study
is to verify the importance of an individual compo-
nent model when a significant performance drop is
observed after removing it. However, occasionally
the overall performance may go up slightly. Typi-
cally this is due to the fact that some models do not
provide useful signals to a particular relation, but in-
stead introduce more noise. Such effects can often
be alleviated when there are enough quality training
data, which is unfortunately not the case here.
6 Conclusions
In this paper, we presented a system that combines
heterogeneous models based on different informa-
tion sources for measuring relational similarity. Our
two individual general-purpose relational similarity
models, directional similarity and lexical pattern
methods, perform strongly when compared to ex-
isting systems. After incorporating specific word-
relation models, the final system sets a new state-of-
the-art on the SemEval-2012 task 2 test set, achiev-
ing Spearman?s ? = 0.353 and MaxDiff accuracy
45.4% ? resulting in 54.1% and 14.7% relative im-
provement in these two metrics, respectively.
Despite its simplicity, our directional similarity
approach provides a robust model for relational sim-
ilarity and is a critical component in the final sys-
tem. When the lexical pattern model is included, our
overall model combination method can be viewed
as a two-stage learning system. As demonstrated in
our work, with an appropriate regularization strat-
egy, high-quality models can be learned in both
stages. Finally, as we observe from the positive ef-
fect of adding the Probase IsA model, specific word-
relation models can further help improve the system
1007
although they tend to cover only a small number of
relations. Incorporating more such models could be
a steady path to enhance the final system.
In the future, we plan to pursue several research
directions. First, as shown in our experimental re-
sults, the model combination approach does not al-
ways outperform individual models. Investigating
how to select models to combine for each specific re-
lation or relation group individually will be our next
step for improving this work. Second, because the
labeling process of relational similarity comparisons
is inherently noisy, it is unrealistic to request a sys-
tem to correlate human judgments perfectly. Con-
ducting some user study to estimate the performance
ceiling in each relation category may help us focus
on the weaknesses of the final system to enhance
it. Third, it is intriguing to see that the directional
similarity model based on the RNNLM vectors per-
forms strongly, even though the RNNLM training
process is not related to the task of relational sim-
ilarity. Investigating the effects of different vector
space models and proposing some theoretical jus-
tifications are certainly interesting research topics.
Finally, we would like to evaluate the utility our ap-
proach in other applications, such as the SAT anal-
ogy problems proposed by Turney (2006) and ques-
tion answering.
Acknowledgments
We thank Richard Socher for valuable discussions,
Misha Bilenko for his technical advice and anony-
mous reviewers for their comments.
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca
and A. Soroa. 2009. A study on similarity and re-
latedness using distributional and WordNet-based ap-
proaches. In NAACL ?09, pages 19?27.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In ICML ?07.
I.I. Bejar, R. Chaffin, and S.E. Embretson. 1991. Cog-
nitive and psychometric analysis of analogical prob-
lem solving. Recent research in psychology. Springer-
Verlag.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003.
A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3:1137?1155.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467?479.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of lexical semantic relatedness. Com-
putational Linguistics, 32:13?47, March.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010).
Timothy W. Finin. 1980. The Semantic Interpretation
of Compound Nominals. Ph.D. thesis, University of
Illinois at Urbana-Champaign.
J.H. Friedman. 2001. Greedy function approximation: a
gradient boosting machine. Ann. Statist, 29(5):1189?
1232.
David Jurgens, Saif Mohammad, Peter Turney, and Keith
Holyoak. 2012. SemEval-2012 Task 2: Measuring
degrees of relational similarity. In Proceedings of the
Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 356?364, Montre?al, Canada,
7-8 June. Association for Computational Linguistics.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse Processes, 25, pages 259?284.
Jordan J. Louviere and G. G. Woodworth. 1991. Best-
worst scaling: A model for the largest difference judg-
ments. Technical report, University of Alberta.
Tomas Mikolov, Martin Karafia?t, Lukas Burget, Jan Cer-
nocky?, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In INTER-
SPEECH, pages 1045?1048.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernocky. 2011. Strategies for train-
ing large scale neural network language models. In
ASRU.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013.
Linguistic regularities in continuous space word repre-
sentations. In Proceedings of NAACL-HLT.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings of
the 5th International Workshop on Computational Se-
mantics.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword fourth edi-
tion. Technical report, Linguistic Data Consortium,
Philadelphia.
Ted Pedersen. 2012. Duluth: Measuring degrees of re-
lational similarity with the gloss vector measure of se-
mantic relatedness. In Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
1008
2012), pages 497?501, Montre?al, Canada, 7-8 June.
Association for Computational Linguistics.
K. Radinsky, E. Agichtein, E. Gabrilovich, and
S. Markovitch. 2011. A word at a time: computing
word relatedness using temporal semantic analysis. In
WWW ?11, pages 337?346.
J. Reisinger and R. Mooney. 2010. Multi-prototype
vector-space models of word meaning. In NAACL ?10.
Bryan Rink and Sanda Harabagiu. 2012. UTD: Deter-
mining relational similarity using lexical patterns. In
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 413?418,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Barbara Rosario and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
the 2001 Conference on Empirical Methods in Natural
Language Processing (EMNLP-01, pages 82?90.
Mireya Tovar, J. Alejandro Reyes, Azucena Montes,
Darnes Vilarin?o, David Pinto, and Saul Leo?n. 2012.
BUAP: A first approximation to relational similarity
measuring. In Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 502?505, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of Association for
Computational Linguistics (ACL 2010).
Peter Turney and Michael Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60 (1-3), pages 251?278.
P. D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
Peter D. Turney. 2012. Domain and function: A
dual-space model of semantic relations and compo-
sitions. Journal of Artificial Intelligence Research
(JAIR), 44:533?585.
Lucy Vanderwende. 1994. Algorithm for automatic
interpretation of noun sequences. In Proceedings of
COLING-94, pages 782?788.
Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Q.
Zhu. 2012. Probase: a probabilistic taxonomy for
text understanding. In Proceedings of the 2012 ACM
SIGMOD International Conference on Management of
Data, pages 481?492, May.
Wen-tau Yih and Vahed Qazvinian. 2012. Measur-
ing word relatedness using heterogeneous vector space
models. In Proceedings of NAACL-HLT, pages 616?
620, Montre?al, Canada, June.
Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Po-
larity inducing latent semantic analysis. In Proceed-
ings of NAACL-HLT, pages 1212?1222, Jeju Island,
Korea, July.
1009
