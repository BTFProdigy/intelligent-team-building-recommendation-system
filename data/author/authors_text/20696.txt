Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1385?1390,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Language Transfer Hypotheses with Linear SVM Weights
Shervin Malmasi
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
shervin.malmasi@mq.edu.au
Mark Dras
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
mark.dras@mq.edu.au
Abstract
Language transfer, the characteristic sec-
ond language usage patterns caused by na-
tive language interference, is investigated
by Second Language Acquisition (SLA)
researchers seeking to find overused and
underused linguistic features. In this pa-
per we develop and present a methodology
for deriving ranked lists of such features.
Using very large learner data, we show
our method?s ability to find relevant can-
didates using sophisticated linguistic fea-
tures. To illustrate its applicability to SLA
research, we formulate plausible language
transfer hypotheses supported by current
evidence. This is the first work to ex-
tend Native Language Identification to a
broader linguistic interpretation of learner
data and address the automatic extraction
of underused features on a per-native lan-
guage basis.
1 Introduction
It has been noted in the linguistics literature since
the 1950s that speakers of particular languages
have characteristic production patterns when writ-
ing in a second language. This language transfer
phenomenon has been investigated independently
in a number of fields from different perspectives,
including qualitative research in Second Language
Acquisition (SLA) and more recently though pre-
dictive computational models in NLP.
Motivated by the aim of improving foreign lan-
guage teaching and learning, such analyses are of-
ten done manually in SLA, and are difficult to
perform for large corpora. Smaller studies yield
poor results due to the sample size, leading to
extreme variability (Ellis, 2008). Recently, re-
searchers have noted that NLP has the tools to use
large amounts of data to automate this analysis,
using complex feature types. This has motivated
studies in Native Language Identification (NLI), a
subtype of text classification where the goal is to
determine the native language (L1) of an author
using texts they have written in a second language
(L2) (Tetreault et al., 2013).
Despite the good results in predicting L1s, few
attempts have been made to interpret the features
that distinguish L1s. This is partly because no
methods for an SLA-oriented feature analysis have
been proposed; most work focuses on testing fea-
ture types using standard machine learning tools.
The overarching contribution of this work is to
develop a methodology that enables the transfor-
mation of the NLI paradigm into SLA applications
that can be used to link these features to their un-
derlying linguistic causes and explanations. These
candidates can then be applied in other areas such
as remedial SLA strategies or error detection.
2 Related Work
SLA research aims to find distributional differ-
ences in language use between L1s, often referred
to as overuse, the extensive use of some linguis-
tic structures, and underuse, the underutilization
of particular structures, also known as avoidance
(Gass and Selinker, 2008). While there have been
some attempts in SLA to use computational ap-
proaches on small-scale data,
1
these still use fairly
elementary techniques and have several shortcom-
ings, including in the manual approaches to an-
notation and the computational artefacts derived
from these.
Conversely, NLI work has focused on automatic
learner L1 classification using Machine Learning
with large-scale data and sophisticated linguistic
features (Tetreault et al., 2012). Here, feature
ranking could be performed with relevancy meth-
ods such as the F-score:
1
E.g. Chen (2013), Lozan?o and Mendikoetxea (2010) and
Di?ez-Bedmar and Papp (2008).
1385
F (j) ?
(
?x(+)
j
? ?x
j
)
2
+
(
?x(?)
j
? ?x
j
)
2
1
n
+
?1
n
+
?
i=1
(
x
(+)
i,j
? ?x(+)
j
)
2
+
1
n??1
n?
?
i=1
(
x
(?)
i,j
? ?x(?)
j
)
2
(1)
The F-score (Fisher score) measures the ratio
between the intraclass and interclass variance in
the values of feature j, where x represents the fea-
ture values in the negative and positive examples.
2
More discriminative features have higher scores.
Another alternative method is Information Gain
(Yang and Pedersen, 1997). As defined in equation
(2), it measures the entropy gain associated with
feature t in assigning the class label c.
G(t) = ?
?
m
i=1
Pr (c
i
) log Pr (c
i
)
+ Pr (t)
?
m
i=1
Pr (c
i
|t) log Pr (c
i
|t)
+ Pr (
?
t)
?
m
i=1
Pr (c
i
|
?
t) log Pr (c
i
|
?
t)
(2)
However, these methods are limited: they do not
provide ranked lists per-L1 class, and more impor-
tantly, they do not explicitly capture underuse.
Among the efflorescence of NLI work, a new
trend explored by Swanson and Charniak (2014)
aims to extract lists of candidate language transfer
features by comparing L2 data against the writer?s
L1 to find features where the L1 use is mirrored in
L2 use. This allows the detection of obvious ef-
fects, but Jarvis and Crossley (2012) note (p. 183)
that many transfer effects are ?too complex? to ob-
serve in this manner. Moreover, this method is un-
able to detect underuse, is only suitable for syn-
tactic features, and has only been applied to very
small data (4,000 sentences) over three L1s. Ad-
dressing these issues is the focus of the present
work.
3 Experimental Setup
3.1 Corpus
We use TOEFL11, the largest publicly available
corpus of English L2 texts (Blanchard et al.,
2013), containing 11 L1s with 1,100 texts each.
3
3.2 Features
Adaptor grammar collocations Per Wong et al.
(2012), we utilize an adaptor grammar to discover
arbitrary length n-gram collocations. We explore
both the pure part-of-speech (POS) n-grams as
2
See Chang and Lin (2008) for more details.
3
Over 4 million tokens in 12,100 texts.
well as the more promising mixtures of POS and
function words. We derive two adaptor grammars
where each is associated with a different set of vo-
cabulary: either pure POS or the mixture of POS
and function words. We use the grammar pro-
posed by Johnson (2010) for capturing topical col-
locations:
Sentence? Doc
j
j ? 1, . . . ,m
Doc
j
? j j ? 1, . . . ,m
Doc
j
? Doc
j
Topic
i
i ? 1, . . . , t;
j ? 1, . . . ,m
Topic
i
?Words i ? 1, . . . , t
Words?Word
Words?Words Word
Word? w w ? V
pos
;
w ? V
pos+fw
V
pos
contains 119 distinct POS tags based on the
Brown tagset and V
pos+fw
is extended with 398
function words. The number of topics t is set to
50. The inference algorithm for the adaptor gram-
mars are based on the Markov Chain Monte Carlo
technique made available by Johnson (2010).
4
Stanford dependencies We use Stanford de-
pendencies as a syntactic feature: for each
text we extract all the basic dependencies re-
turned by the Stanford Parser (de Marneffe et
al., 2006). We then generate all the variations
for each of the dependencies (grammatical rela-
tions) by substituting each lemma with its cor-
responding POS tag. For instance, a gram-
matical relation of det(knowledge, the)
yields the following variations: det(NN, the),
det(knowledge, DT), and det(NN, DT).
Lexical features Content and function words
are also considered as two feature types related to
learner?s vocabulary and spelling.
3.3 Extracting Linear SVM Feature Weights
Using the extracted features, we train linear Sup-
port Vector Machine (SVM) models for each
L1. We use a one-vs-rest approach to find fea-
tures most relevant to each native language. L2-
regularization is applied to remove noisy features
and reduce the size of the candidate feature list.
More specifically, we employ the LIBLINEAR
SVM package (Fan et al., 2008)
5
as it is well-
suited to text classification tasks with large num-
bers of features and texts as is the case here.
4
http://web.science.mq.edu.au/%7Emjohnson/Software.htm
5
http://www.csie.ntu.edu.tw/%7Ecjlin/liblinear/
1386
In training the models for each feature, the SVM
weight vector
6
is calculated according to (3):
w =
?
i
?
i
y
i
x
i
(3)
After training, the positive and negative weights
are split into two lists and ranked by weight.
The positive weights represent overused features,
while features whose absence (i.e. underuse) is
indicative of an L1 class will have large negative
weights. This yields two candidate language trans-
fer feature lists per L1.
4 Results
We now turn to an analysis of the output from our
system to illustrate its applicability for SLA re-
search. Table 1 lists some elements from the un-
deruse and overuse lists for various L1s. The lists
are of different feature types. They have been cho-
sen to demonstrate all feature types and also a va-
riety of different languages. For reasons of space,
only several of the top features are analysed here.
Hindi L1 writers are distinguished by certain
function words including hence, thus, and etc, and
a much higher usage rate of male pronouns. It has
been observed in the literature (Sanyal, 2007, for
example) that the English spoken in India still re-
tains characteristics of the English that was spoken
during the time of the Raj and the East India Com-
pany that have disappeared from other English va-
rieties, so it sounds more formal to other speakers,
or retains traces of an archaic business correspon-
dence style; the features noted fit that pattern.
The second list includes content words overused
by Arabic L1 learners. Analysis of content words
here, and for other L1s in our data, reveals very
frequent misspellings which are believed to be due
to orthographic or phonetic influences (Tsur and
Rappoport, 2007; Odlin, 1989). Since Arabic does
not share orthography with English, we believe
most of these are due to phonetics. Looking at
items 1, 3 and 5 we can see a common pattern:
the English letter u which has various phonetic re-
alizations is being replaced by a vowel that more
often represents that sound. Items 2 and 5 are also
phonetically similar to the intended words.
For Spanish L1 authors we provide both under-
use and overuse lists of syntactic dependencies.
The top 3 overuse rules show the word that is very
often used as the subject of verbs. This is almost
6
See Burges (1998) for a detailed explanation.
certainly a consequence of the prominent syntac-
tic role played by the Spanish word que which, de-
pending on the context, is equivalent to the English
words whom, who, which, and most commonly,
that. The fourth rule shows they often use this as a
determiner for plural nouns. A survey of the cor-
pus reveals many such errors in texts of Spanish
learners, e.g. this actions or this emissions. The
fifth rule shows that the adjectival modifier of a
plural noun is often being incorrectly pluralised to
match the noun in number as would be required in
Spanish, for example, differents subjects.
Turning to the underused features in Spanish L1
texts, we see that four related features rank highly,
showing that these is not commonly used as a de-
terminer for plural nouns and which is rarely used
as a subject. The final feature shows that no is
avoided as a determiner. This may be because
while no mostly has the same role in Spanish as it
does in English, it cannot be used as a determiner;
ning?un must be used instead. We hypothesize that
this construction is being avoided as placing no be-
fore a noun in Spanish is ungrammatical. This ex-
ample demonstrates that our two list methodology
can not only help identify overused structures, but
also uncovers the related constructs that are being
underutilized at their expense.
The final list in Table 1 is of underused Adap-
tor Grammar patterns by Chinese learners. The
first three features show that these writers signif-
icantly underuse determiners, here an, other and
these before nouns. This is not unexpected since
Chinese learners? difficulties with English articles
are well known (Robertson, 2000). More inter-
estingly, we find underuse of features like even if
and might, along with others not listed here such
as could VB
7
plus many other variants related to
the subjunctive mood. One explanation is that lin-
guistic differences between Chinese and English
in expressing counterfactuals could cause them to
avoid such constructions in L2 English. Previous
research in this area has linked the absence of sub-
junctive linguistic structures in Chinese to differ-
ent cognitive representations of the world and con-
sequences for thinking counterfactually (Bloom,
2014), although this has been disputed (Au, 1983;
Garbern Liu, 1985).
Adaptor Grammars also reveal frequent use of
the ?existential there?
8
in German L1 data while
7
e.g. could be, could have, could go and other variants
8
e.g. There is/are ..., as opposed to the locative there.
1387
Overuse Underuse
Hindi Arabic Spanish Spanish Chinese
#2: thus #2: anderstand #1: nsubj(VBP,that) #2: det(NNS,these) #12: an NN
#4: hence #4: mony #2: nsubj(VBZ,that) #3: nsubj(VBZ,which) #16: other NN
#22: his #6: besy #3: nsubj(VB,that) #6: nsubj(VB,which) #18: these NNS
#30: etc #15: diffrent #4: det(NNS,this) #7: nsubj(VBP,which) #19: even if
#33:rather #38: seccessful #25: amod(NNS,differents) #10: det(NN,no) #68: might
Table 1: Example transfer candidates and rankings from the overuse/underuse lists for various L1s and
features types, in order: Hindi function words, Arabic content words, Spanish dependencies (2) and
Chinese Adaptor Grammars.
English Spanish English Spanish
diferent diferente conclution conclusi?on
consecuence consecuencia desagree Neg. affix des-
responsability responsabilidad especific espec??fico
oportunity oportunidad necesary necesario
Table 2: Highly ranked English misspellings of
Spanish learners and their Spanish cognates.
they are highly underused in French L1 data. The
literature supports our data: The German equiv-
alent es gibt is common while French use is far
more constrained (Cappelle and Loock, 2013).
Lexical analysis also revealed Spanish?English
orthographic transfer, listed in Table 2. This list
includes many cognates, in contrast with the Ara-
bic L1 data where most misspellings were pho-
netic in nature.
We also observe other patterns which remain
unexplained. For instance, Chinese, Japanese and
Korean speakers make excessive use of phrases
such as however, first and second. One possibil-
ity is that this relates to argumentation styles that
are possibly influenced by cultural norms. More
broadly, this effect could also be teaching rather
than transfer related. For example, it may be case
that a widely-used text book for learning English
in Korea happens to overuse this construction.
Some recent findings from the 2013 NLI Shared
Task found that L1 Hindi and Telugu learners of
English had similar transfer effects and their writ-
ings were difficult to distinguish. It has been
posited that this is likely due to shared culture and
teaching environments (Malmasi et al., 2013).
Despite some clearcut instances of overuse,
9
more research is required to determine the causal
factors. We hope to expand on this in future work
using more data.
9
More than half of the Korean scripts contained a
sentence-initial however.
5 Discussion and Conclusion
Using the proposed methodology, we generated
lists of linguistic features overused and underused
by English learners of various L1 backgrounds.
Through an analysis of the top items in these
ranked lists, we demonstrated the high applicabil-
ity of the output by formulating plausible language
transfer hypotheses supported by current evidence.
We also showcased the method?s generalizability
to numerous linguistic feature types.
Our method?s output consists of two ranked lists
of linguistic features: one for overuse and the
other for underuse, something which had not been
addressed by research to date. We also found
Adaptor Grammar collocations to be highly infor-
mative for this task.
This work, an intersection of NLP, Machine
Learning and SLA, illustrates how the various dis-
ciplines can complement each other by bringing
together theoretical, experimental and computa-
tional issues. NLP provides accurate and auto-
mated tagging of large corpora with sophisticated
features not available in corpus linguistics, e.g.
with state-of-the-art dependency parsing. Sophis-
ticated machine learning techniques then enable
the processing of large quantities of data (thou-
sands of times the size of manual studies) in a way
that will let SLA researchers explore a variety of
assumptions and theoretical analyses. And con-
versely, NLP can benefit from the long-term study
and language acquisition insights from SLA.
In terms of NLI, this work is the first attempt to
expand NLI to a broad linguistic interpretation of
the data, including feature underuse. NLI systems
achieve classification accuracies of over 80% on
this 11-class task, leading to theoretical questions
about the features that make them so effective.
This work also has a backwards link in this regard
by providing qualitative evidence about the under-
pinning linguistic theories that make NLI work.
1388
The work presented here has a number of ap-
plications; chief among them is the development
of tools for SLA researchers. This would enable
them to not just provide new evidence for previ-
ous findings, but to also perform semi-automated
data-driven generation of new and viable hypothe-
ses. This, in turn, can help reduce expert effort and
involvement in the process, particularly as such
studies expand to more corpora and emerging lan-
guage like Chinese (Malmasi and Dras, 2014b)
and Arabic (Malmasi and Dras, 2014a).
The brief analysis included here represents only
a tiny portion of what can be achieved with this
methodology. We included but a few of the thou-
sands of features revealed by this method; prac-
tical SLA tools based on this would have a great
impact on current research.
In addition to language transfer hypotheses,
such systems could also be applied to aid devel-
opment of pedagogical material within a needs-
based and data-driven approach. Once language
use patterns are uncovered, they can be assessed
for teachability and used to create tailored, L1-
specific exercises and teaching material.
From the examples discussed in Section 4 these
could include highly specific and targeted student
exercises to improve spelling, expand vocabulary
and enrich syntactic knowledge ? all relative to
their mother tongue. Such exercises can not only
help beginners improve their fundamental skills
and redress their errors but also assist advanced
learners in moving closer to near-nativeness.
The extracted features and their weights could
also be used to build statistical models for gram-
matical error detection (Leacock et al., 2014).
Contrary to the norm of developing error checkers
for native writers, such models could be specifi-
cally targeted towards learners or even particular
L1?L2 pairs which could be useful in Computer-
Assisted Language Learning (CALL) systems.
One limitation here is that our features may
be corpus-dependent as they are all exam essays.
This can be addressed by augmenting the data with
new learner corpora, as they become available.
While a strength here is that we compared each L1
against others, a paired comparison only against
native texts can be insightful too.
There are several directions for future work.
The first relates to clustering the data within the
lists. Our intuition is that there might be coher-
ent clusters of related features, with these clusters
characterising typical errors or idiosyncrasies, that
are predictive of a particular L1. As shown in our
results, some features are highly related and may
be caused by the same underlying transfer phe-
nomena. For example, our list of overused syntac-
tic constructs by Spanish learners includes three
high ranking features related to the same transfer
effect. The use of unsupervised learning meth-
ods such as Bayesian mixture models may be ap-
propriate here. For parse features, tree kernels
could help measure similarity between the trees
and fragments (Collins and Duffy, 2001).
Another avenue is to implement weight-based
ranking methods to further refine and re-rank the
lists, potentially by incorporating the measures
mentioned in Section 2 to assign weights to fea-
tures. As the corpus we used includes learner
proficiency metadata, it may also be possible to
create proficiency-segregated models to find the
features that characterise errors at each language
proficiency level. Finally, the use of other lin-
guistic features such as Context-free Grammar
phrase structure rules or Tree Substitution Gram-
mars could provide additional insights.
In addition to these further technical investiga-
tions, we see as a particularly useful direction the
development of an SLA research tool to conduct a
large SLA study with a wide range of experts. We
believe that this study makes a contribution to this
area and hope that it will motivate future work.
References
Terry Kit-Fong Au. 1983. Chinese and English coun-
terfactuals: the Sapir-Whorf hypothesis revisited.
Cognition, 15(1):155?187.
Daniel Blanchard, Joel Tetreault, Derrick Higgins,
Aoife Cahill, and Martin Chodorow. 2013.
TOEFL11: A Corpus of Non-Native English. Tech-
nical report, Educational Testing Service.
Alfred H Bloom. 2014. The linguistic shaping of
thought: A study in the impact of language on think-
ing in China and the West. Psychology Press.
Christopher JC Burges. 1998. A tutorial on Support
Vector Machines for Pattern Recognition. Data min-
ing and knowledge discovery, 2(2):121?167.
Bert Cappelle and Rudy Loock. 2013. Is there in-
terference of usage constraints?: A frequency study
of existential there is and its French equivalent il
ya in translated vs. non-translated texts. Target,
25(2):252?275.
1389
Yin-Wen Chang and Chih-Jen Lin. 2008. Feature
ranking using linear svm. Causation and Prediction
Challenge Challenges in Machine Learning, Volume
2, page 47.
Meilin Chen. 2013. Overuse or underuse: A cor-
pus study of English phrasal verb use by Chinese,
British and American university students. Interna-
tional Journal of Corpus Linguistics, 18(3).
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Advances in Neu-
ral Information Processing Systems, pages 625?632.
Marie-Catherine de Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC?06),
pages 449?454, Genoa, Italy.
Mar??a Bel?en Di?ez-Bedmar and Szilvia Papp. 2008.
The use of the English article system by Chinese
and Spanish learners. Language and Computers,
66(1):147?176.
Rod Ellis. 2008. The Study of Second Language Ac-
quisition, 2nd edition. Oxford University Press, Ox-
ford, UK.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Lisa Garbern Liu. 1985. Reasoning counterfactually
in Chinese: Are there any obstacles? Cognition,
21(3):239?270.
Susan M. Gass and Larry Selinker. 2008. Second Lan-
guage Acquisition: An Introductory Course. Rout-
ledge, New York.
Scott Jarvis and Scott Crossley, editors. 2012. Ap-
proaching Language Transfer Through Text Classi-
fication: Explorations in the Detection-based Ap-
proach. Multilingual Matters, Bristol, UK.
Mark Johnson. 2010. PCFGs, Topic Models, Adaptor
Grammars and Learning Topical Collocations and
the Structure of Proper Names. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1148?1157, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2014. Automated grammati-
cal error detection for language learners. Synthesis
Lectures on Human Language Technologies, 7(1):1?
170.
Cristobal Lozan?o and Amaya Mendikoetxea. 2010. In-
terface conditions on postverbal subjects: A corpus
study of L2 English. Bilingualism: Language and
Cognition, 13(4):475?497.
Shervin Malmasi and Mark Dras. 2014a. Arabic Na-
tive Language Identification. In Proceedings of the
Arabic Natural Language Processing Workshop (co-
located with EMNLP 2014), Doha, Qatar, October.
Association for Computational Linguistics.
Shervin Malmasi and Mark Dras. 2014b. Chinese
Native Language Identification. Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, April.
Shervin Malmasi, Sze-Meng Jojo Wong, and Mark
Dras. 2013. NLI Shared Task 2013: MQ Submis-
sion. In Proceedings of the Eighth Workshop on In-
novative Use of NLP for Building Educational Ap-
plications, pages 124?133, Atlanta, Georgia, June.
Association for Computational Linguistics.
Terence Odlin. 1989. Language Transfer: Cross-
linguistic Influence in Language Learning. Cam-
bridge University Press, Cambridge, UK.
Daniel Robertson. 2000. Variability in the use of the
English article system by Chinese learners of En-
glish. Second Language Research, 16(2):135?172.
Jyoti Sanyal. 2007. Indlish: The Book for Every
English-Speaking Indian. Viva Books Private Lim-
ited.
Ben Swanson and Eugene Charniak. 2014. Data
Driven Language Transfer Hypotheses. EACL 2014,
page 169.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and
Martin Chodorow. 2012. Native Tongues, Lost
and Found: Resources and Empirical Evaluations in
Native Language Identification. In Proceedings of
COLING 2012, pages 2585?2602, Mumbai, India,
December. The COLING 2012 Organizing Commit-
tee.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Build-
ing Educational Applications, pages 48?57, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Oren Tsur and Ari Rappoport. 2007. Using classifier
features for studying the effect of native language
on the choice of written second language words. In
Proc. Workshop on Cognitive Aspects of Computat.
Language Acquisition, pages 9?16.
Sze-Meng Jojo Wong, Mark Dras, and Mark John-
son. 2012. Exploring Adaptor Grammars for Na-
tive Language Identification. In Proc. Conf. Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 699?709.
Yiming Yang and Jan O Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In ICML, volume 97, pages 412?420.
1390
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 95?99,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Chinese Native Language Identification
Shervin Malmasi
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
shervin.malmasi@mq.edu.au
Mark Dras
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
mark.dras@mq.edu.au
Abstract
We present the first application of Na-
tive Language Identification (NLI) to non-
English data. Motivated by theories of lan-
guage transfer, NLI is the task of iden-
tifying a writer?s native language (L1)
based on their writings in a second lan-
guage (the L2). An NLI system was ap-
plied to Chinese learner texts using topic-
independent syntactic models to assess
their accuracy. We find that models using
part-of-speech tags, context-free grammar
production rules and function words are
highly effective, achieving a maximum ac-
curacy of 71% . Interestingly, we also find
that when applied to equivalent English
data, the model performance is almost
identical. This finding suggests a sys-
tematic pattern of cross-linguistic transfer
may exist, where the degree of transfer is
independent of the L1 and L2.
1 Introduction
Native Language Identification (NLI) is the task of
identifying an author?s native language (L1) based
on their writings in a second language (the L2).
NLI works by identifying language use patterns
that are common to groups of speakers that share
the same native language. This process is under-
pinned by the presupposition that an author?s L1
will dispose them towards particular language pro-
duction patterns in their L2, as influenced by their
mother tongue. This relates to Cross-Linguistic
Influence (CLI), a key topic in the field of Second
Language Acquisition (SLA) that analyzes trans-
fer effects from the L1 on later learned languages
(Ortega, 2009).
While NLI has applications in security, most re-
search has a strong linguistic motivation relating to
language teaching and learning. Rising numbers
of language learners have led to an increasing need
for language learning resources, which has in turn
fuelled much of the language acquisition research
of the past decade. In this context, by identify-
ing L1-specific language usage and error patterns,
NLI can be used to better understand SLA and de-
velop teaching methods, instructions and learner
feedback that is specific to their mother tongue.
However, all of the NLI research to date has fo-
cused exclusively on English L2 data. To this end
there is a need to apply NLI to other languages,
not only to gauge their applicability but also to aid
in teaching research for other emerging languages.
Interest in learning Chinese is rapidly growing,
leading to increased research in Teaching Chinese
as a Second Language (TCSL) and the develop-
ment of related resources such as learner corpora
(Chen et al., 2010). The application of these tools
and scientific methods like NLI can greatly assist
researchers in creating effective teaching practices
and is an area of active research.
The aim of this research is to evaluate the cross-
language applicability of NLI techniques by ap-
plying them to Chinese learner texts, evaluating
their efficacy and comparing the results with their
English equivalents.
To the best of our knowledge this is the first
reported application of NLI to non-English data
and we believe this is an important step in gain-
ing deeper insights about the technique.
2 Related Work
NLI is a fairly recent, but rapidly growing area of
research. While some research was conducted in
the early 2000s, the most significant work has only
appeared in the last few years (Wong and Dras,
2009; Wong and Dras, 2011; Swanson and Char-
niak, 2012; Tetreault et al., 2012; Bykh and Meur-
ers, 2012).
Most studies approach NLI as a multi-class su-
pervised classification task. In this experimental
design, the L1 metadata are used as class labels
95
and the individual writings are used as training and
testing data. Using lexical and syntactic features
of increasing sophistication, researchers have ob-
tained good results under this paradigm. While a
detailed exposition of NLI has been omitted here
due to space constraints, a concise review can be
found in Bykh and Meurers (2012).
2.1 NLI 2013 Shared Task
This increased interest brought unprecedented
level of research focus and momentum, resulting
in the first NLI shared task being held in 2013.
1
The shared task aimed to facilitate the comparison
of results by providing a large NLI-specific dataset
and evaluation procedure, to enable direct compar-
ison of results achieved through different methods.
Overall, the event was considered a success, draw-
ing 29 entrants and experts from not only Compu-
tational Linguistics, but also SLA. The best teams
achieved accuracies of greater than 80% on this
11-class classification task. A detailed summary
of the results is presented in Tetreault et al. (2013).
3 Data
Growing interest has led to the recent develop-
ment of the Chinese Learner Corpus (Wang et al.,
2012), the first large-scale corpus of learner texts
comprised of essays written by university stu-
dents. Learners from 59 countries are represented
and proficiency levels have been sampled repre-
sentatively across beginners, intermediate and ad-
vanced learners. However, texts by native speak-
ers of other Asian countries are disproportionately
represented, likely due to geographical proximity.
For this work we extracted 3.75 million tokens
of text from the CLC in the form of individual
sentences.
2
Following the methodology of Brooke
and Hirst (2011), we combine the sentences from
the same L1 to form texts of 600 tokens on aver-
age, creating a set of documents suitable for NLI
3
.
We choose the top 11 languages, shown in Ta-
ble 1, to use in our experiments. This is due to
two considerations. First, while many L1s are rep-
resented in the corpus, most have relatively few
texts. Choosing the top 11 classes allows us to
1
Organised by the Educational Testing Service and co-
located with the eighth instalment of the Building Ed-
ucational Applications Workshop at NAACL/HLT 2013.
sites.google.com/site/nlisharedtask2013/
2
Full texts are not made available, only individual sen-
tences with the relevant metadata (proficiency/nationality).
3
Pending permission from the CLC corpus authors, we
will attempt to release the Chinese NLI dataset publicly.
Language Size Language Size
Filipino FIL 415 Indonesian IND 402
Thai THA 400 Laotian LAO 366
Burmese MYA 349 Korean
?
KOR 330
Khmer KHM 294 Vietnamese VIE 267
Japanese
?
JAP 180 Spanish
?
SPA 112
Mongolian MON 101
Table 1: Our data, broken down by language and
the number of texts in each class. Languages over-
lapping with the TOEFL11 corpus marked with
?
.
balance having a large number of classes, and also
maximizes the amount of data used. Secondly, this
is the same number of classes used in the NLI 2013
shared task, enabling us to draw cross-language
comparisons with the shared task results.
4 Experimental Setup
We also follow the supervised classification ap-
proach described in ?2. We devise and run exper-
iments using several models that capture different
types of linguistic information. For each model,
features are extracted from the texts and a clas-
sifier is trained to predict the L1 labels using the
features. As our data is not topic-balanced, we
avoid using topic-dependent lexical features such
as character or word n-grams.
Each experiment is run with two feature repre-
sentations: binary (presence/absence of a feature)
and normalized frequencies, where feature values
are normalized to text length using the l2-norm.
4.1 Parser
The Stanford CoreNLP
4
suite of NLP tools and
the provided Chinese models are used to tokenize,
PoS tag and parse the unsegmented corpus texts.
4.2 Classifier
We use Support Vector Machines for classifica-
tion. Specifically, we use the LIBLINEAR SVM
package (Fan et al., 2008) as it is well-suited to
text classification tasks with large numbers of fea-
tures and texts. We use the L2-regularized L2-loss
support vector classification (dual) solver.
4.3 Evaluation
The same evaluation metrics and standards used in
the NLI2013 Shared Task are used: we report clas-
sification accuracy under 10-fold cross-validation.
We also use the same number of classes as the
shared task to facilitate comparative analyses.
4
http://nlp.stanford.edu/software/corenlp.shtml
96
Feature Accuracy (%)
Binary Frequency
Random Baseline 9.09 9.09
PoS unigrams 20.12 35.32
Part-of-Speech bigrams 32.83 54.24
Part-of-Speech trigrams 47.24 55.60
Function Words 43.93 51.91
Production Rules 36.14 49.80
All features 61.75 70.61
Table 2: Chinese Native Language Identification
accuracy (%) for all of our models.
5 Experiments and Results
5.1 Part-of-Speech tag n-grams
Our first experiment assesses the utility of the
syntactic information captured by part-of-speech
(PoS) tags for Chinese NLI. The PoS tags for each
text are predicted and n-grams of size 1?3 are ex-
tracted from the tags. These n-grams capture (very
local) syntactic patterns of language use and are
used as classification features.
The results for these three features, and our
other models are shown in Table 2. The trigram
frequencies give the best accuracy of 55.60%, sug-
gesting that there exist group-specific patterns of
Chinese word order and category choice which
provide a highly discriminative cue about the L1.
5.2 Function Words
As opposed to content words, function words are
topic-independent grammatical words that indi-
cate the relations between other words. They
include determiners, conjunctions and auxiliary
verbs. Distributions of English function words
have been found to be useful in studies of author-
ship attribution and NLI. Unlike PoS tags, this
model analyzes the author?s specific word choices.
We compiled a list of 449 Chinese function
words
5
to be used as features in this model. As
shown in Table 2, the function word frequency
features provide the best accuracy of 51.91%,
significantly higher than the random baseline.
This again suggests the presence of L1-specific
grammatical and lexical choice patterns that can
help distinguish the L1, potentially due to cross-
linguistic transfer. Such lexical transfer effects
5
The function word list was compiled from Chinese lan-
guage teaching resources. The complete list can be accessed
at http://comp.mq.edu.au/
?
madras/research/
data/chinese-fw.txt
ROOT
IP
PU
?
VP
VP
IP
VP
VV
??
QP
CLP
M
?
CD
?
VE
?
PP
NP
NP
NN
??
DP
DT
??
P
?
NP
PN
?
IP ? NP VP PU VP ? PP VP
NP ? DP NP PP ? P NP
Figure 1: A constituent parse tree for a sentence
from the corpus along with some of the context-
free grammar production rules extracted from it.
have been previously noted by researchers and
linguists (Odlin, 1989). These effects are medi-
ated not only by cognates and similarities in word
forms, but also word semantics and meanings.
5.3 Context-free Grammar Production Rules
In the next experiment we investigate the differ-
ences in the distribution of the context-free gram-
mar production rules used by the learners. To do
this, constituent parses for all sentences are ob-
tained and the production rules, excluding lexical-
izations, are extracted. Figure 1 shows a sample
tree and rules. These context-free phrase structure
rules capture the overall structure of grammatical
constructions and are used as classification fea-
tures in this experiment.
As seen in Table 2, the model achieves an accu-
racy of 49.80%. This supports the hypothesis that
the syntactic substructures contain characteristic
constructions specific to L1 groups and that these
syntactic cues strongly signal the writer?s L1.
5.4 Combining All Features
Finally, we assess the redundancy of the informa-
tion captured by our models by combining them
all into one vector space to create a single clas-
sifier. From Table 2 we see that for each feature
representation, the combined feature results are
higher than the single best feature, with a max-
97
imum accuracy of 70.61%. This demonstrates
that for at least some of the features, the informa-
tion they capture is orthogonal and complemen-
tary, and combining them can improve results.
6 Discussion
A key finding here is that NLI models can be suc-
cessfully applied to non-English data. This is an
important step for furthering NLI research as the
field is still relatively young and many fundamen-
tal questions have yet to be answered.
All of the tested models are effective, and they
appear to be complementary as combining them
improves overall accuracy. We also note the differ-
ence in the efficacy of the feature representations
and see a clear preference for frequency-based fea-
ture values. Others have found that binary features
are the most effective for English NLI (Brooke and
Hirst, 2012), but our results indicate frequency in-
formation is more informative in this task. The
combination of both feature types has also been
reported to be effective (Malmasi et al., 2013).
To see how these models perform across lan-
guages, we also compare the results against the
TOEFL11 corpus used in the NLI2013 shared
task. We perform the same experiments on that
dataset using the English CoreNLP models, Penn
Treebank PoS tagset and a set of 400 English func-
tion words. Figure 2 shows the results side by side.
Remarkably, we see that the model results
closely mirror each other across corpora. This is a
highly interesting finding from our study that mer-
its further investigation. There is a systematic pat-
tern occurring across data from learners of com-
pletely different L1-L2 pairs. This suggests that
manifestations of CLI via surface phenomena oc-
cur at the same levels and patternings regardless
of the L2. Cross-language studies can help re-
searchers in linguistics and cognitive science to
better understand the SLA process and language
transfer effects. They can enhance our understand-
ing of how language is processed in the brain in
ways that are not possible by just studying mono-
linguals or single L1-L2 pairs, thereby providing
us with important insights that increase our knowl-
edge and understanding of the human language
faculty.
One limitation of this work is the lack of sim-
ilar amounts of training data for each language.
However, many of the early and influential NLI
studies (e.g. Koppel et al. (2005), Tsur and Rap-
poport (2007)) were performed under similar cir-
PoS-1 PoS-2 PoS-3
FW PR
0
20
40
60
A
c
c
u
r
a
c
y
(
%
)
Chinese English
Figure 2: Comparing feature performance on the
Chinese Learner Corpus and English TOEFL11
corpora. PoS-1/2/3: PoS uni/bi/trigrams, FW:
Function Words, PR: Production Rules
cumstances. This issue was noted at the time, but
did not deter researchers as corpora with similar
issues were used for many years. Non-English
NLI is also at a similar state where the extant cor-
pora are not optimal for the task, but no other al-
ternatives exist for conducting this research.
Finally, there are also a number of way to fur-
ther develop this work. Firstly, the experimental
scope could be expanded to use even more lin-
guistically sophisticated features such as depen-
dency parses. Model accuracy could potentially
be improved by using the metadata to develop
proficiency-segregated models. Classifier ensem-
bles could also help in increasing the accuracy.
7 Conclusion
In this work we have presented the first application
of NLI to non-English data. Using the Chinese
Learner Corpus, we compare models based on
PoS tags, function words and context-free gram-
mar production rules and find that they all yield
high classification accuracies.
Comparing the models against an equivalent
English learner corpus we find that the accura-
cies are almost identical across both L2s, suggest-
ing a systematic pattern of cross-linguistic transfer
where the degree of transfer is independent of the
L1 and L2. Further research with other L2 learner
corpora is needed to investigate this phenomena.
Acknowledgments
We wish to thank Associate Professor Maolin
Wang for providing access to the CLC corpus, and
Zhendong Zhao for his assistance. We also thank
the reviewers for their constructive feedback.
98
References
Julian Brooke and Graeme Hirst. 2011. Na-
tive language detection with ?cheap? learner cor-
pora. In Conference of Learner Corpus Research
(LCR2011), Louvain-la-Neuve, Belgium. Presses
universitaires de Louvain.
Julian Brooke and Graeme Hirst. 2012. Robust, Lex-
icalized Native Language Identification. In Pro-
ceedings of COLING 2012, pages 391?408, Mum-
bai, India, December. The COLING 2012 Organiz-
ing Committee.
Serhiy Bykh and Detmar Meurers. 2012. Native Lan-
guage Identification using Recurring n-grams ? In-
vestigating Abstraction and Domain Dependence.
In Proceedings of COLING 2012, pages 425?440,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
Jianguo Chen, Chuang Wang, and Jinfa Cai. 2010.
Teaching and learning Chinese: Issues and perspec-
tives. IAP.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005. Automatically determining an anonymous au-
thor?s native language. In Intelligence and Security
Informatics, volume 3495 of LNCS, pages 209?217.
Springer-Verlag.
Shervin Malmasi, Sze-Meng Jojo Wong, and Mark
Dras. 2013. Nli shared task 2013: Mq submission.
In Proceedings of the Eighth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, pages 124?133, Atlanta, Georgia, June. Asso-
ciation for Computational Linguistics.
Terence Odlin. 1989. Language Transfer: Cross-
linguistic Influence in Language Learning. Cam-
bridge University Press, Cambridge, UK.
Lourdes Ortega. 2009. Understanding Second Lan-
guage Acquisition. Hodder Education, Oxford, UK.
Benjamin Swanson and Eugene Charniak. 2012.
Native Language Detection with Tree Substitution
Grammars. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 193?197, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and
Martin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in na-
tive language identification. In Proceedings of COL-
ING 2012, pages 2585?2602, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Build-
ing Educational Applications, pages 48?57, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Oren Tsur and Ari Rappoport. 2007. Using classifier
features for studying the effect of native language
on the choice of written second language words. In
Proc. Workshop on Cognitive Aspects of Computat.
Language Acquisition, pages 9?16.
Maolin Wang, Qi Gong, Jie Kuang, and Ziyu Xiong.
2012. The development of a chinese learner corpus.
In Speech Database and Assessments (Oriental CO-
COSDA), 2012 International Conference on, pages
1?6. IEEE.
Sze-Meng Jojo Wong and Mark Dras. 2009. Con-
trastive Analysis and Native Language Identifica-
tion. In Proceedings of the Australasian Language
Technology Association Workshop 2009, pages 53?
61, Sydney, Australia, December.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing Parse Structures for Native Language Identifi-
cation. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1600?1610, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
99
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 124?133,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
NLI Shared Task 2013: MQ Submission
Shervin Malmasi Sze-Meng Jojo Wong Mark Dras
Centre for Language Technology
Macquarie University
Sydney, Australia
{shervin.malmasi,sze.wong,mark.dras}@mq.edu.au
Abstract
Our submission for this NLI shared task used
for the most part standard features found in re-
cent work. Our focus was instead on two other
aspects of our system: at a high level, on pos-
sible ways of constructing ensembles of multi-
ple classifiers; and at a low level, on the gran-
ularity of part-of-speech tags used as features.
We found that the choice of ensemble com-
bination method did not lead to much differ-
ence in results, although exploiting the vary-
ing behaviours of linear versus logistic regres-
sion SVM classifiers could be promising in fu-
ture work; but part-of-speech tagsets showed
noticeable differences.
We also note that the overall architecture, with
its feature set and ensemble approach, had an
accuracy of 83.1% on the test set when trained
on both the training data and development data
supplied, close to the best result of the task.
This suggests that basically throwing together
all the features of previous work will achieve
roughly the state of the art.
1 Introduction
Among the efflorescence of work on Native Lan-
guage Identification (NLI) noted by the shared task
organisers, there are two trends in recent work in
particular that we considered in building our sub-
mission. The first is the proposal and use of new
features that might have relevance to NLI: for exam-
ple, Wong and Dras (2011), motivated by the Con-
trastive Analysis Hypothesis (Lado, 1957) from the
field of Second Language Acquisition, introduced
syntactic structure as a feature; Swanson and Char-
niak (2012) introduced more complex Tree Substi-
tution (TSG) structures, learned by Bayesian infer-
ence; and Bykh and Meurers (2012) used recurring
n-grams, inspired by the variation n-gram approach
to corpus error annotation detection (Dickinson and
Meurers, 2003). Starting from the features intro-
duced in these papers and others, then, other recent
papers have compiled a comprehensive collection of
features based on the earlier work ? Tetreault et
al. (2012) is an example, combining and analysing
most of the features used in previous work. Given
the timeframe of the shared task, there seemed to be
not much mileage in trying new features that were
likely to be more peripheral to the task.
A second trend, most apparent in 2012, was the
examination of other corpora besides the Interna-
tional Corpus of Learner English used in earlier
work, and in particular the use of cross-corpus evalu-
ation (Brooke and Hirst, 2012; Tetreault et al, 2012)
to avoid topic bias in determining native language.
Possible topic bias had been a reason for avoiding
a full range of n-grams, in particular those contain-
ing content words (Koppel et al, 2009); the devel-
opment of new corpora and the analysis of the effect
of topic bias mitigated this. The consequent use of a
full range of n-grams further reinforced the view that
novel features were unlikely to be a major source of
interesting results.
We therefore concentrated on two areas: the use
of classifier ensembles, and the choice of part-of-
speech tags. With classifier ensembles, Tetreault
et al (2012) noted that these were highly useful in
their system; but while that paper had extensive fea-
124
ture descriptions, it did not discuss in detail the ap-
proach to its ensembles. We therefore decided to
examine a range of possible ensemble architectures.
With part-of-speech tags, most work has used the
Penn Treebank tagset, including those based on syn-
tactic structure. Kochmar (2011) on the other hand
used the CLAWS tagset,1 which is much richer and
more oriented to linguistic analysis than the Penn
Treebank one. Given the much larger size of the
TOEFL11 corpus used for this shared task than the
corpora used for much earlier work, data sparsity
could be less of an issue, and the tagset a viable one
for future work.
The description of our submission is therefore in
three parts. In ?2 we present the system description,
with a focus on the ensemble architectures we inves-
tigated; in ?3 we list the features we used, which are
basically those of much of the previous work; in ?4
we present results of some of the variants we tried,
particularly with respect to ensembles and tagsets;
and in ?5 we discuss some of the interesting charac-
teristics of the data we noted during the shared task.
2 System Design
Our overall approach in terms of features and clas-
sifiers used is a fairly standard one. One difference
from most approaches, but inspired by Tetreault et
al. (2012), is that we train multiple classifiers over
subsets of the features, over different feature rep-
resentations, and over different regularisation ap-
proaches; we then combine them in ensembles (Di-
etterich, 2000).
2.1 SVM Ensemble Construction
To construct our ensemble, we train individual clas-
sifiers on a single feature type (e.g. PoS n-grams),
using a specific feature value representation and
classifier. We utilise a parallel ensemble structure
where the classifiers are run on the input texts in-
dependently and their results are then fused into the
final output using a combiner.
Additionally, we also experiment with bagging
(bootstrap aggregating), a commonly used method
for ensemble generation (Breiman, 1996) to gener-
ate multiple ensembles per feature type.
1http://ucrel.lancs.ac.uk/claws/
For our classifier, we use SVMs, specifically the
LIBLINEAR SVM software package (Fan et al,
2008),2 which is well-suited to text classification
tasks with large numbers of features and large num-
bers of documents. LIBLINEAR provides both lo-
gistic regression and linear SVMs; we experiment
with both. In general, the linear classifier performs
better, but it only provides the decision output. The
logistic regression classifier on the other hand gives
probability estimates, which are required by most
of our combination methods (?2.3). We therefore
mostly use the logistic regression classifiers.
2.2 L1- and L2-regularized SVM Classifiers
In our preliminary experiments we noted that
some feature types performed better with L1-
regularization and others with L2. In this work we
generate classifiers using both methods and evaluate
their individual and combined performance.
2.3 Classifier Combination Methods
We experiment with the following decision combi-
nation methods, which have been discussed in the
machine learning literature. Polikar (2006) provides
an exposition of these rules and methods.
Plurality vote: Each classifier votes for a single
class label, the label with the highest number of
votes wins. Ties are broken arbitrarily.
Sum: All probability estimates are added together
and the label with the highest sum is picked.
Average: The mean of all scores for each class
is calculated and the label with the highest average
probability is chosen.
Median: Each label?s estimates are sorted and the
median value is selected as the final score for that
label. The label with the highest value is picked.
Product: For each class label, all of the probabil-
ity estimates are multiplied together to create the la-
bel?s final estimate. The label with the highest esti-
mate is selected. A single low score can have a big
effect on the outcome.
Highest Confidence: In this simple method, the
class label that receives the vote with the largest de-
gree of confidence is selected as the final output.
2Available at http://www.csie.ntu.edu.tw/
?cjlin/liblinear/
125
Borda Count: The confidence estimates are con-
verted to ranks and the final label selected using the
Borda count algorithm (Ho et al, 1994). In this
combination approach, broadly speaking points are
assigned to ranks, and these tallied for the overall
weight.
With the exception of the plurality vote, all of
these can be weighted. In our ensembles we also ex-
periment with weighting the output of each classifier
using its individual accuracy on the training data as
an indication of our degree of confidence in it.
2.4 Feature Representation
Most NLI studies have used two types of feature rep-
resentations: binary (presence or absence of a fea-
ture in a text) and normalized frequencies. Although
binary feature values have been used in some stud-
ies (e.g. Wong and Dras (2011)), most have used
frequency-based values.
In the course of our experiments we have ob-
served that the effect of the feature representation
varies with the feature type, size of the feature space
and the learning algorithm itself. In our current sys-
tem, then, we generate two classifiers for each fea-
ture type, one trained with frequency-based values
(raw counts scaled using the L2-norm) and the other
with binary. Our experiments assess both their indi-
vidual and joint performance.
2.5 Proficiency-level Based Classification
To utilise the proficiency level information provided
in the TOEFL11 corpus (texts are marked as either
low, medium or high proficiency), we also investi-
gate classifiers that are trained using only texts from
specific proficiencies.
Tetreault et al (2012) established that the classi-
fication accuracy of their system varied across pro-
ficiency levels, with high proficiency texts being the
hardest to classify. This is most likely due to the fact
that writers at differing skill levels commit distinct
types of errors at different rates (Ortega, 2009, for
example). If learners of different backgrounds com-
mit these errors with different distributions, these
patterns could be used by a learner to further im-
prove classification accuracy. We will use these fea-
tures in one of our experiments to investigate the
effectiveness of such proficiency-level based classi-
fiers for NLI.
3 Features
We roughly divide out feature types into lexical,
part-of-speech and syntactic. In all of the feature
types below, we perform no feature selection.
3.1 Lexical Features
As all previous work, we use function words as fea-
tures. In addition, given the attempts to control for
topic bias in the TOEFL11 corpus, we also make
use of various lexical features which have been pre-
viously avoided by researchers due to the reported
topic bias (Brooke and Hirst, 2011) in other NLI cor-
pora such as the ICLE corpus.
Function Words In contrast to content words,
function words do not have any meaning themselves,
but rather can be seen as indicating the grammat-
ical relations between other words. Examples in-
clude articles, determiners, conjunctions and auxil-
iary verbs. They have been widely used in studies of
authorship attribution as well as NLI and established
to be informative for these tasks. We use the list
of 398 common English function words from Wong
and Dras (2011). We also tested smaller sets, but ob-
served that the larger sets achieve higher accuracy.
Function Word n-grams We devised and tested a
new feature that attempts to capture patterns of func-
tion word use at the sentence level. We define func-
tion word n-grams as a type of word n-gram where
content words are skipped: they are thus a specific
subtype of skip-gram discussed by Guthrie et al
(2006). For example, the sentence We should all
start taking the bus would be reduced to we should
all the, from which we would extract the n-grams.
Character n-grams Tsur and Rappoport (2007)
demonstrated that character n-grams are a useful
feature for NLI. These n-grams can be considered
as a sub-word feature and their effectiveness is hy-
pothesized to be a result of phoneme transfer from
the writer?s L1. They can also capture orthographic
conventions of a language. Accordingly, we limit
our n-grams to a maximum size of 3 as longer se-
quences would correspond to short words and not
phonemes or syllables.
Word n-grams There has been a shift towards the
use of word-based features in several recent studies
(Brooke and Hirst, 2012; Bykh and Meurers, 2012;
126
Tetreault et al, 2012), with new corpora come into
use for NLI and researchers exploring and address-
ing the issues relating to topic bias that previously
prevented their use. Lexical choice is considered to
be a prime feature for studying language transfer ef-
fects, and researchers have found word n-grams to
be one of the strongest features for NLI. Tetreault
et al (2012) expanded on this by integrating 5-gram
language models into their system. While we did not
replicate this, we made use of word trigrams.
3.2 POS n-grams
Most studies have found that POS tag n-grams are
a very useful feature for NLI (Koppel et al, 2005;
Bykh and Meurers, 2012, for example). The tagset
provided by the Penn TreeBank is the most widely
used in these experiments, with tagging performed
by the Stanford Tagger (Toutanova et al, 2003).
We investigate the effect of tagset granularity
on classification accuracy by comparing the clas-
sification accuracy of texts tagged with the PTB
tagset against those annotated by the RASP Tagger
(Briscoe et al, 2006). The PTB POS tagset contains
36 unique tags, while the RASP system uses a subset
of the CLAWS2 tagset, consisting of 150 tags.
This is a significant size difference and we hy-
pothesize that a larger tagset could provide richer
levels of syntactically meaningful info which is
more fine-grained in distinction between syntactic
categories and contains more morpho-syntactic in-
formation such as gender, number, person, case
and tense. For example, while the PTB tagset
has four tags for pronouns (PRP, PRP$, WP,
WP$), the CLAWS tagset provides over 20 pronoun
tags (PPHO1, PPIS1, PPX2, PPY, etc.) dis-
tinguishing between person, number and grammati-
cal role. Consequently, these tags could help better
capture error patterns to be used for classification.
3.3 Syntactic Features
Adaptor grammar collocations Drawing on
Wong et al (2012), we also utilise an adaptor gram-
mar to discover arbitrary lengths of n-gram collo-
cations for the TOEFL11 corpus. We explore both
the pure part-of-speech (POS) n-grams as well as
the more promising mixtures of POS and function
words. Following a similar experimental setup as
per Wong et al (2012), we derive two adaptor gram-
mars where each is associated with a different set of
vocabulary: either pure POS or the mixture of POS
and function words. We use the grammar proposed
by Johnson (2010) for capturing topical collocations
as presented below:
Sentence ? Docj j ? 1, . . . ,m
Docj ? j j ? 1, . . . ,m
Docj ? Docj Topici i ? 1, . . . , t;
j ? 1, . . . ,m
Topici ? Words i ? 1, . . . , t
Words ? Word
Words ? Words Word
Word ? w w ? Vpos;
w ? Vpos+fw
As per Wong et al (2012), Vpos contains 119
distinct POS tags based on the Brown tagset and
Vpos+fw is extended with 398 function words used
in Wong and Dras (2011). The number of topics t
is set to 50 (instead of 25 as per Wong et al (2012))
given that the TOEFL corpus is larger than the ICLE
corpus. The inference algorithm for the adaptor
grammars are based on the Markov Chain Monte
Carlo technique made available by Johnson (2010).3
Tree Subtitution Grammar fragments In rela-
tion to the context-free grammar (CFG) rules ex-
plored in the previous NLI work of Wong and Dras
(2011), Tree Substitution Grammar (TSG) frag-
ments have been proposed by Swanson and Char-
niak (2012) as another form of syntactic features
for NLI classification tasks. Here, as an approxi-
mation to deploying the Bayesian approach to in-
duce a TSG (Post and Gildea, 2009; Swanson and
Charniak, 2012), we first parse each of the essays in
the TOEFL training corpus with the Stanford Parser
(version 2.0.4) (Klein and Manning, 2003) to obtain
the parse trees. We then extract the TSG fragments
from the parse trees using the TSG system made
available by Post and Gildea (2009).4
Stanford dependencies In Tetreault et al (2012),
Stanford dependencies were investigated as yet an-
other form of syntactic features. We follow a
similar approach: for each essay in the train-
ing corpus, we extract all the basic (rather than
3http://web.science.mq.edu.au/?mjohnson/
Software.htm
4https://github.com/mjpost/dptsg
127
the collapsed) dependencies returned by the Stan-
ford Parser (de Marneffe et al, 2006). Simi-
larly, we generate all the variations for each of
the dependencies (grammatical relations) by sub-
stituting each lemma with its corresponding PoS
tag. For instance, a grammatical relation of
det(knowledge, the) yields the following
variations: det(NN, the), det(knowledge,
DT), and det(NN, DT).
4 Experiments and Results
We report our results using 10-fold cross-validation
on the combined training and development sets, as
well as by training a model using the training and
development data and running it on the test set.
We note that for our submission, we trained only
on the training data; the results here thus differ from
the official ones.
4.1 Individual Feature Results and Analysis
We ran the classifiers generated for each feature type
to assess their performance. The results are summa-
rized in Table 1: the Train + Dev Set results were for
the system when trained on the training and develop-
ment data with 10 fold cross-validation, and the Test
Set results for the system trained on the training and
development data combined.
Character n-grams are an informative feature and
our results are very similar to those reported by pre-
vious researchers (Tsur and Rappoport, 2007). In
particular, it should be noted that the use of punc-
tuation is a very powerful feature for distinguishing
languages. Romance language speakers were most
likely to use more punctuation symbols (colons,
semicolons, ellipsis, parenthesis, etc.) and at higher
rates. Chinese, Japanese and Korean speakers were
far less likely to use punctuation.
The performance for word n-grams, TSG frag-
ments and Stanford Dependencies is very strong and
comparable to previously reported research. For the
adaptor grammar n-grams, the mixed POS/function
word version yielded best results and was included
in the ensemble.
4.2 POS-based Classification and Tagset Size
To compare the tagsets we trained individual classi-
fiers for n-grams of size 1?4 using both tagsets and
tested them. The results are shown in Table 2 and
Feature Train +
Dev Set
Test Set
Chance Baseline 9.1 9.1
Character unigram 33.99 34.70
Character bigram 51.64 49.80
Character trigram 66.43 66.70
RASP POS unigram 43.76 45.10
RASP POS bigram 58.93 61.60
RASP POS trigram 59.39 62.70
Function word unigram 51.38 54.00
Function word bigram 59.73 63.00
Word unigram 74.61 75.50
Word bigram 74.46 76.00
Word trigram 63.60 65.00
TSG Fragments 72.16 72.70
Stanford Dependencies 73.78 75.90
Adaptor Grammar
POS/FW n-grams
69.76 70.00
Table 1: Classification results for our individual features.
N PTB RASP
1 34.03 43.76
2 48.85 58.93
3 51.06 59.39
4 49.85 52.81
Table 2: Classification accuracy results for POS n-grams
of size N using both the PTB and RASP tagset. The larger
RASP tagset performed significantly better for all N.
N Accuracy
1 51.38
2 59.73
3 52.14
Table 3: Classification results for Function Word n-grams
of size N. Our proposed Function Word bigram and tri-
gram features outperform the commonly used unigrams.
128
Ensemble Train +
Dev Set
Test Set
Complete Ensemble 81.50 81.60
Only binary values 82.46 83.10
Only freq values 65.28 67.20
L1-regularized solver only 80.33 81.10
L2-regularized solver only 81.42 81.10
Bin, L1-regularized only 81.57 82.00
Bin, L2-regularized only 82.00 82.50
Table 4: Classification results for our ensembles, best re-
sult in column in bold (binary values with L1- and L2-
regularized solvers).
show that the RASP tagged data provided better per-
formance in all cases. While it is possible that these
differences could be attributed to other factors such
as tagging accuracy, we do not believe this to be the
case as the Stanford Tagger is known for its high ac-
curacy (97%). These differences are quite clear; this
finding also has implications for other syntactic fea-
tures that make use of POS tags, such as Adaptor
Grammars, Stanford Dependencies and Tree Substi-
tution Grammars.
4.3 Function Word n-grams
The classification results using our proposed Func-
tion Word n-gram feature are shown in Table 3.
They show that function word skip-grams are more
informative than the simple function word counts
that have been previously used.
4.4 Ensemble Results
Table 4 shows the results from our ensembles. The
feature types included in the ensemble are those
whose results are listed individually in Table 1. (So,
for example, we only use the RASP-tagged PoS n-
grams, not the Penn Treebank ones.) The complete
ensemble consists of four classifiers per feature type:
L1-/L2-regularized versions with both binary and
freq. values.
Bagging Our experiments with bagging did not
find any improvements in accuracy, even with larger
numbers of bootstrap samples (50 or more). Bag-
ging is said to be more suitable for unstable clas-
sifiers which have greater variability in their perfor-
mance and are more susceptible to noise in the train-
ing data (Breiman, 1996). In our experiments with
individual feature types we have found the classi-
fiers to be quite stable in their performance, across
different folds and training set sizes. This is one po-
tential reason why bagging did not yield significant
improvements.
Combiner Methods Of the methods outlined in
?2.3 we found the sum and weighted sum combiners
to be the best performing, but the weighted results
did not improve accuracy in general over their un-
weighted counterparts. Our results are reported us-
ing the unweighted sum combiner. A detailed com-
parison of the results for the combiners has been
omitted here due to time constraints; the differences
across all combination methods was roughly 1?2%.
Any new approach to ensemble combination meth-
ods would consequently want to be radically differ-
ent to expect a notable improvement in performance.
As noted at the start of this section, results here
are for the system trained on training and develop-
ment data. The best result on the test set (83.1%)
is almost 4% higher than our submission result, and
close to the highest result achieved (83.6%).
Binary & Frequency-Based Feature Values Our
results are consistent with those of Brooke and Hirst
(2012), who conclude that there is a preference
for binary feature values instead of frequency-based
ones. Including both types in the ensemble did not
improve results.
However, in other experiments on the TOEFL11
corpus we have also observed that use of frequency
information often leads to significantly better results
when using a linear SVM classifier: in fact, the lin-
ear classifier is better on all frequency feature types,
and also on some of the binary feature types. We
present results in Table 5 comparing the two. An ap-
proach using the linear SVM that provides an asso-
ciated probability score ? perhaps through bagging
? allowing it to be combined with the methods de-
scribed in ?2.3 could then perhaps boost results. All
these results were from a system using the training
data with 10 fold cross-validation.
Combining Regularisation Approaches Results
show that combining the L1- and L2-regularized
classifiers in the ensemble provided a small in-
129
Feature L2-norm scaled counts Binary
linear log. regr. linear log. regr.
Char unigram 31.60 26.23 25.68 26.36
Char bigram 51.59 41.81 41.20 45.11
Char trigram 65.78 54.97 58.30 61.76
RASP POS bigram 60.38 54.00 50.31 54.56
RASP POS trigram 58.75 53.92 55.93 58.58
Function word unigram 51.38 45.09 46.67 47.13
Function word bigram 58.95 53.22 54.97 58.53
Word unigram 70.33 55.60 69.40 72.00
Word bigram 73.90 54.25 73.65 74.93
Word trigram 63.78 52.46 64.78 64.94
Table 5: Classification results for our individual features.
crease in accuracy. Ensembles with either the L1 or
L2-regularized solver have lower accuracy than the
combined methods (row 2).
4.5 Proficiency-level Based Classification
Table 6 shows our results for training models with
texts of a given proficiency level and the accuracy on
the test set. The numbers show that in general texts
should be classified with a learner trained with texts
of a similar proficiency. They also show that not all
texts in a proficiency level are of uniform quality as
some levels perform better with data from the clos-
est neighbouring levels (e.g. Medium texts perform
best with data from all proficiencies), suggesting
that the three levels form a larger proficiency con-
tinuum where users may fall in the higher or lower
ends of a level. A larger scale with more than three
levels could help address this.
5 Discussion
5.1 Unused Experimental Features
We also experimented with some other feature types
that were not included in the final system.
CCG SuperTag n-grams In order to introduce
additional rich syntactic information into our sys-
tem, we investigated the use CCG SuperTags as fea-
ture for NLI classification. We used the C&C CCG
Train Test Acc. Train Test Acc.
Low Low 52.2 All Med 86.8
Med Low 72.1 M + H Med 85.3
High Low 40.3 L + M Med 83.8
All Low 75.2 Low High 16.1
L + M Low 76.0 Med High 68.1
Low Med 40.7 High High 65.7
Med Med 83.6 M + H High 74.7
High Med 62.1 All High 75.2
Table 6: Results for classifying the test set documents
using classifiers trained with a specific proficiency level.
Each level?s best result in bold.
Parser and SuperTagger (Curran et al, 2007) to ex-
tract SuperTag n-grams from the corpus, which were
then used as features to construct classifiers. The
best results were achieved by using n-grams of size
2?4, which achieved classification rates of around
44%. However, adding these features to our ensem-
ble did not improve the overall system accuracy. We
believe that this is because when coupled with the
other syntactic features in the system, the informa-
tion provided by the SuperTags is redundant, and
thus they were excluded from our final ensemble.
Hapax Legomena and Dis Legomena The spe-
cial word categories Hapax Legomena and Dis
legomena refer to words that appear only once and
130
twice, respectively, in a complete text. In practice,
these features are a subset of our Word Unigram
feature, where Hapax Legomena correspond to un-
igrams with an occurrence count of 1 and Hapax dis
legomena are unigrams with a count of 2.
In our experimental results we found that Ha-
pax Legomena alone provides an accuracy of 61%.
Combining the two features together yields an accu-
racy of 67%. This is an interesting finding as both
of these features alone provide an accuracy close to
the whole set of word unigrams.
5.2 Corpus Representativeness
We conducted a brief analysis of our extracted fea-
tures, looking at the most predictive ones according
to their Information Gain. Although we did not find
any obvious indicators of topic bias, we noted some
other issues of potential concern.
Chinese, Japanese and Korean speakers make ex-
cessive use of phrases such as However, First of all
and Secondly. At first glance, the usage rate of these
phrases seems unnaturally high (more than 50% of
Korean texts had a sentence beginning with How-
ever). This could perhaps be a cohort effect relat-
ing to those individually attempting this particular
TOEFL exam, rather than an L1 effect: it would
be useful to know how much variability there is in
terms of where candidates come from.
It was also noticed that many writers mention the
name of their country in their texts, and this could
potentially create a high correlation between those
words and the language class label, leading perhaps
to an artificial boosting of results. For example, the
words India, Turkey, Japan, Korea and Germany ap-
pear with high frequency in the texts of their corre-
sponding L1 speakers ? hundreds of times, in fact,
in contrast to frequencies in the single figures for
speakers of other L1s. These might also be an arte-
fact of the type of text, rather than related to the L1
as such.
5.3 Hindi vs. Telugu
We single out here this language pair because of
the high level of confusion between the two classes.
Looking at the results obtained by other teams, we
observe that this language pair provided the worst
classification accuracy for almost all teams. No
system was able to achieve an accuracy of 80%
for Hindi (something many achieved for other lan-
guages). In analysing the actual and predicted
classes for all documents classified as Hindi and
Telugu by our system, we find that generally all
of the actual Hindi and Telugu texts (96% and
99%, respectively) are within the set. Our classifier
is clearly having difficulty discriminating between
these two specific classes.
Given this, we posit that the confounding influ-
ence may have more to do with the particular style
of English that is spoken and taught within the
country, rather than the specific L1 itself. Consult-
ing other research about SLA differences in multi-
lingual countries could shed further light on this.
Analysing highly informative features provides
some clues about the influence of a common cul-
ture or national identity: in our classifier, the words
India, Indian and Hindu were highly predictive of
both Hindi and Telugu texts, but no other lan-
guages. In addition, there were terms that were
not geographically- or culturally-specific that were
strongly associated with both Hindi and Telugu:
these included hence, thus, and etc, and a much
higher rate of use of male pronouns. It has been
observed in a number of places (Sanyal, 2007, for
example) that the English spoken across India still
retains characteristics of the English that was spo-
ken during the time of the Raj and the East India
Company that have disappeared from other varities
of English, so that it can sound more formal to other
speakers, or retain traces of an archaic business cor-
respondence style; the features just noted would fit
that pattern. The effect is likely to occur regardless
of the L1.
Looking at individual language pairs in this way
could lead to incremental improvement in the overall
classification accuracy of NLI systems.
References
Leo Breiman. 1996. Bagging predictors. In Machine
Learning, pages 123?140.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the rasp system. In Proceedings
of the COLING/ACL on Interactive presentation ses-
sions, COLING-ACL ?06, pages 77?80, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ?cheap? learner corpora. In Conference
131
of Learner Corpus Research (LCR2011), Louvain-la-
Neuve, Belgium. Presses universitaires de Louvain.
Julian Brooke and Graeme Hirst. 2012. Robust, Lexical-
ized Native Language Identification. In Proceedings
of COLING 2012, pages 391?408, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Serhiy Bykh and Detmar Meurers. 2012. Native Lan-
guage Identification using Recurring n-grams ? In-
vestigating Abstraction and Domain Dependence. In
Proceedings of COLING 2012, pages 425?440, Mum-
bai, India, December. The COLING 2012 Organizing
Committee.
James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale nlp with c&c and
boxer. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 33?36, Prague, Czech Republic, June.
Association for Computational Linguistics.
Marie-Catherine de Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation (LREC?06), pages
449?454, Genoa, Italy.
Markus Dickinson and W. Detmar Meurers. 2003. De-
tecting errors in part-of-speech annotation. In Pro-
ceedings of the 10th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-03), pages 107?114, Budapest, Hungary.
Thomas G Dietterich. 2000. Ensemble methods in ma-
chine learning. In Multiple classifier systems, pages
1?15. Springer.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
David Guthrie, Ben Allison, Wei Liu, Louise Guthrie,
and Yorick Wilks. 2006. A Close Look at Skip-gram
Modelling. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation
(LREC 2006), pages 1222?1225, Genoa, Italy.
Tin Kam Ho, Jonathan J. Hull, and Sargur N. Srihari.
1994. Decision combination in multiple classifier
systems. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 16(1):66?75.
Mark Johnson. 2010. Pcfgs, topic models, adaptor gram-
mars and learning topical collocations and the struc-
ture of proper names. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1148?1157, Uppsala, Sweden, July.
Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, ACL ?03, pages 423?430, Sap-
poro, Japan. Association for Computational Linguis-
tics.
Ekaterina Kochmar. 2011. Identification of a writer?s na-
tive language by error analysis. Master?s thesis, Uni-
versity of Cambridge.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Automatically determining an anonymous author?s na-
tive language. Intelligence and Security Informatics,
pages 41?76.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2009. Computational Methods in Authorship Attribu-
tion. Journal of the American Society for Information
Science and Technology, 60(1):9?26.
Robert Lado. 1957. Linguistics Across Cultures: Ap-
plied Linguistics for Language Teachers. University
of Michigan Press, Ann Arbor, MI, US.
Lourdes Ortega. 2009. Understanding Second Language
Acquisition. Hodder Education, Oxford, UK.
Robi Polikar. 2006. Ensemble based systems in deci-
sion making. Circuits and Systems Magazine, IEEE,
6(3):21?45.
Matt Post and Daniel Gildea. 2009. Bayesian learn-
ing of a tree substitution grammar. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
ACLShort ?09, pages 45?48, Suntec, Singapore. As-
sociation for Computational Linguistics.
Jyoti Sanyal. 2007. Indlish: The Book for Every English-
Speaking Indian. Viva Books Private Limited.
Benjamin Swanson and Eugene Charniak. 2012. Na-
tive Language Detection with Tree Substitution Gram-
mars. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 193?197, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in native
language identification. In Proceedings of COLING
2012, pages 2585?2602, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In IN PRO-
CEEDINGS OF HLT-NAACL, pages 252?259.
Oren Tsur and Ari Rappoport. 2007. Using Classifier
Features for Studying the Effect of Native Language
on the Choice of Written Second Language Words.
In Proceedings of the Workshop on Cognitive Aspects
of Computational Language Acquisition, pages 9?16,
132
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
Parse Structures for Native Language Identification.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring Adaptor Grammars for Native Lan-
guage Identification. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 699?709, Jeju Island, Korea,
July. Association for Computational Linguistics.
133
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 180?186,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Arabic Native Language Identification
Shervin Malmasi
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
shervin.malmasi@mq.edu.au
Mark Dras
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
mark.dras@mq.edu.au
Abstract
In this paper we present the first appli-
cation of Native Language Identification
(NLI) to Arabic learner data. NLI, the task
of predicting a writer?s first language from
their writing in other languages has been
mostly investigated with English data, but
is now expanding to other languages. We
use L2 texts from the newly released Ara-
bic Learner Corpus and with a combina-
tion of three syntactic features (CFG pro-
duction rules, Arabic function words and
Part-of-Speech n-grams), we demonstrate
that they are useful for this task. Our sys-
tem achieves an accuracy of 41% against
a baseline of 23%, providing the first evi-
dence for classifier-based detection of lan-
guage transfer effects in L2 Arabic. Such
methods can be useful for studying lan-
guage transfer, developing teaching mate-
rials tailored to students? native language
and forensic linguistics. Future directions
are discussed.
1 Introduction
Researchers in Second Language Acquisition
(SLA) investigate the multiplex of factors that
influence our ability to acquire new languages
and chief among these factors is the role of the
learner?s mother tongue. Recently this fundamen-
tal factor has been studied in Native Language
Identification (NLI), which aims to infer the native
language (L1) of an author based on texts writ-
ten in a second language (L2). Machine Learning
methods are usually used to identify language use
patterns common to speakers of the same L1.
The motivations for NLI are manifold. The use
of such techniques can help SLA researchers iden-
tify important L1-specific learning and teaching
issues. In turn, the identification of such issues can
enable researchers to develop pedagogical mate-
rial that takes into consideration a learner?s L1 and
addresses them. It can also be applied in a forensic
context, for example, to glean information about
the discriminant L1 cues in an anonymous text.
While almost all NLI research to date has fo-
cused on English L2 data, there is a growing need
to apply the techniques to other language in or-
der to assess the cross-language applicability. This
need is partially driven by the increasing number
of learners of various other languages.
One such case is the teaching of Arabic as a
Foreign Language, which has experienced unpar-
alleled growth in the past two decades. For a long
time the teaching of Arabic was not considered a
priority, but this view has now changed. Arabic is
now perceived as a critical and strategically use-
ful language (Ryding, 2013), with enrolments ris-
ing rapidly and already at an all time high (Wahba
et al., 2013). This trend is also reflected in the
NLP community, evidenced by the continuously
increasing research focus on Arabic tools and re-
sources (Habash, 2010).
A key objective of this study is to investigate
the efficacy of syntactic features for Arabic, a lan-
guage which is significantly different to English.
Arabic orthography is very different to English
with right-to-left text that uses connective letters.
Moreover, this is further complicated due to the
presence of word elongation, common ligatures,
zero-width diacritics and allographic variants. The
morphology of Arabic is also quite rich with many
morphemes that can appear as prefixes, suffixes or
even circumfixes. These mark grammatical infor-
mation including case, number, gender, and defi-
niteness amongst others. This leads to a sophisti-
cated morphotactic system.
Given the aforementioned differences with En-
glish, the main objective of this study is to deter-
mine if NLI techniques can be effective for detect-
ing L1 transfer effects in L2 Arabic.
180
2 Background
NLI has drawn the attention of many researchers
in recent years. With the influx of new researchers,
the most substantive work in this field has come
in the last few years, leading to the organization
of the inaugural NLI Shared Task in 2013 which
was attended by 29 teams from the NLP and SLA
areas. A detailed exposition of the shared task re-
sults and a review of prior NLI work can be found
in Tetreault et al. (2013).
While there exists a large body of literature pro-
duced in the last decade, almost all of this work
has focused exclusively on L2 English. The most
recent work in this field successfully presented
the first application of NLI to a large non-English
dataset (Malmasi and Dras, 2014a), evidencing the
usefulness of syntactic features in distinguishing
L2 Chinese texts.
3 Data
Although the majority of currently available
learner corpora are based on English L2 (Granger,
2012), data from learners of other languages such
as Chinese have also attracted attention in the past
several years.
No Arabic learner corpora were available for a
long time. This paucity of data has been noted by
researchers (Abuhakema et al., 2008; Zaghouani
et al., 2014) and is thought to be due to issues such
as difficulties with non-Latin script and a lack of
linguistic and NLP software to work with the data.
More recently, the first version of the Arabic
Learner Corpus
1
(ALC) was released by Alfaifi
and Atwell (2013). The corpus includes texts by
Arabic learners studying in Saudi Arabia, mostly
timed essays written in class. In total, 66 different
L1 backgrounds are represented. While texts by
native Arabic speakers studying to improve their
writing are also included, we do not utilize these.
We use the more recent second version of the
ALC (Alfaifi et al., 2014) as the data for our exper-
iments. While there are 66 different L1s in the cor-
pus, the majority of these have less than 10 texts
and cannot reliably be used for NLI. Instead we
use a subset of the corpus consisting of the top
seven native languages by number of texts. The
languages and document counts in each class are
shown in Table 1.
Both plain text and XML versions of the learner
1
http://www.arabiclearnercorpus.com/
Native Language Texts
Chinese 76
Urdu 64
Malay 46
French 44
Fulani 36
English 35
Yoruba 28
Total 329
Table 1: The L1 classes included in this experi-
ment and the number of texts within each class.
texts are provided with the corpus. Here we use
text versions and strip the metadata information
from the files, leaving only the author?s writings.
4 Experimental Methodology
In this study we employ a supervised multi-class
classification approach. The learner texts are or-
ganized into classes according on the author?s L1
and these documents are used for training and test-
ing in our experiments. A diagram conceptualiz-
ing our NLI system is shown in Figure 1.
4.1 Word Segmentation
The tokenization and word segmentation of Arabic
is an important preprocessing step for addressing
the orthographic issues discussed in ?1. For this
task we utilize the Stanford Word Segmenter
2
.
4.2 Parsing and Part-of-Speech Tagging
To extract the syntactic information required for
our models, the Arabic texts are POS tagged and
parsed using the Stanford Arabic Parser
3
.
4.3 Classifier
We use a linear Support Vector Machine to per-
form multi-class classification in our experiments.
In particular, we use the LIBLINEAR
4
package
(Fan et al., 2008) which has been shown to be effi-
cient for text classification problems such as this.
4.4 Evaluation Methodology
In the same manner as many previous NLI stud-
ies and also the NLI 2013 shared task, we report
our results as classification accuracy under k-fold
cross-validation, with k = 10. In recent years this
2
http://nlp.stanford.edu/software/segmenter.shtml
3
http://nlp.stanford.edu/projects/arabic.shtml
4
http://www.csie.ntu.edu.tw/%7Ecjlin/liblinear/
181
Arabic Text Chinese L1
NLI
Arabic Text
Arabic Text
Arabic Text
French L1  
English L1 
Malay L1  
Figure 1: Illustration of our NLI system that identifies the L1 of Arabic learners from their writing.
has become a de facto standard for reporting NLI
results.
5 Experiments
We experiment using three syntactic feature types
described in this section. As the ALC is not bal-
anced for topic, we do not consider the use of lex-
ical features such as word n-grams in this study.
Topic bias can occur as a result of the subject mat-
ters or topics of the texts to be classified not not
evenly distributed across the classes. For exam-
ple, if in our training data all the texts written by
English L1 speakers are on topic A, while all the
French L1 authors write about topic B, then we
have implicitly trained our classifier on the topics
as well. In this case the classifier learns to dis-
tinguish our target variable through another con-
founding variable.
5.1 Context-free Grammar Production Rules
Context-free phrase structure rules (without lexi-
calizations) are extracted from parse trees of the
sentences in each learner text. One such con-
stituent parse tree and extracted rules are shown
in Figure 2. These production rules are used as
classification features
5
. Linguistically, they cap-
ture the global syntactic structures used by writers.
5.2 Arabic Function Words
The distributions of grammatical function words
such as determiners and auxiliary verbs have
proven to be useful in NLI. This is considered to
be a useful syntactic feature as these words indi-
cate the relations between content words and are
5
All models use relative frequency feature representations
 ????? ?? ?????? ???? ?? ???? ??? ?? ??????? ?????? ?? ???? ?????
???????? ?? ????? ????. 
 DTNN IN NN DTNN PRP VBD VBP IN VBN DTNN IN NN DTNN CC NN PRP$ IN NN JJ PUNC 
Figure 3: An example of a sentence written by a
learner and its Part-of-Speech tag sequence. Un-
igrams, bigrams and trigrams are then extracted
from this tag sequence.
topic independent. The frequency distributions of
a set of 150 function words were extracted from
the learner texts and used as features in this model.
5.3 Part-of-Speech n-grams
In this model POS n-grams of size 1?3 were ex-
tracted. These n-grams capture small and very lo-
cal syntactic patterns of language production and
were used as classification features.
6 Results
The results from all experiments are shown in Ta-
ble 2. The majority baseline is calculated by us-
ing the largest class, in this case Chinese
6
, as
the default classification. The frequency distri-
butions of the production rules yield 31.7% accu-
racy, demonstrating their ability to identify struc-
tures that are characteristic of L1 groups. Simi-
larly, the distribution of function words is helpful,
with 29.2% accuracy.
While all the models provide results well above
the baseline, POS tag n-grams are the most useful
features, with bigrams providing the highest accu-
racy for a single feature type with 37.6%. This
6
76/329 = 23.1%
182
The options for nodes are all handled by TikZ and are described in detail
in the TikZ documentation. For example, if you have a font named \ar and
want to set all the leaf labels in this font:
.ROOT
.S .
.PUNC
..
.
.S .
.VP .
.NP .
.NP
.NN
.??????
.
.CD
.200
.
.VBD
.????
.
.NP .
.NP
.PRP$
.??
.
.NN
.???
.
.CC
.?
.
.S .
.VP .
.NP .
.NP .
.PP .
.NP
.DTNN
.??????
.
.IN
.?? .
.NP
.NN
.?????
.
.NN
.??
.
.VBD
.???? .
.NP
.NNP
.?????
.
.CC
.?
1
S ? S CC S PUNC VP ? VBD NP
NP ? DTNN PP ? IN NP
Figure 2: A constituent parse tree for a sentence from the corpus along with some of the context-free
grammar production rules extracted from it.
Feature Accuracy (%)
Majority Baseline 23.1
CFG Production Rules 31.7
Function Words 29.2
Part-of-Speech unigrams 36.0
Part-of-Speech bigrams 37.6
Part-of-Speech trigrams 36.5
All features combined 41.0
Table 2: Arabic Native Language Identification
accuracy for the three experiments in this study.
seems to suggest that the greatest difference be-
tween groups lies in their word category ordering.
Combining all of the models into a single fea-
ture space provides the highest accuracy of 41%.
This demonstrates that the information captured
by the various models is complementary and that
the feature types are not redundant.
7 Discussion
The most prominent finding here is that NLI tech-
niques can be successfully applied to Arabic, a
morphologically complex language differing sig-
nificantly from English, which has been the focus
of almost all previous research.
This is one of the very first applications of NLI
to a language other than English and an important
step in the growing field of NLI, particularly with
the current drive to investigate other languages.
This research, though preliminary, presents an ap-
proach to Arabic NLI and can serve as a step to-
wards further research in this area.
NLI technology has practical applications in
various fields. One potential application of NLI
is in the field of forensic linguistics (Gibbons,
2003; Coulthard and Johnson, 2007), a juncture
where the legal system and linguistic stylistics
intersect (Gibbons and Prakasam, 2004; McMe-
namin, 2002). In this context NLI can be used as a
tool for Authorship Profiling (Grant, 2007) in or-
der to provide evidence about the linguistic back-
ground of an author.
There are a number of situations where a text,
such as an anonymous letter, is the central piece of
evidence in an investigation. The ability to extract
additional information from an anonymous text
can enable the authorities and intelligence agen-
cies to learn more about threats and those respon-
sible for them. Clues about the native language
of a writer can help investigators in determining
the source of anonymous text and the importance
of this analysis is often bolstered by the fact that in
such scenarios, the only data available to users and
investigators is the text itself. One recently studied
example is the analysis of extremist related activ-
ity on the web (Abbasi and Chen, 2005).
Accordingly, we can see that from a forensic
point of view, NLI can be a useful tool for intel-
ligence and law enforcement agencies. In fact, re-
cent NLI research such as that related to the work
presented by (Perkins, 2014) has already attracted
183
interest and funding from intelligence agencies
(Perkins, 2014, p. 17).
In addition to applications in forensic linguis-
tics, Arabic NLI can aid the development of re-
search tools for SLA researchers investigating lan-
guage transfer and cross-linguistic effects. Simi-
lar data-driven methods have been recently applied
to generate potential language transfer hypothe-
ses from the writings of English learners (Malmasi
and Dras, 2014c). With the use of an error anno-
tated corpus, which was not the case in this study,
the annotations could be used in conjunction with
similar linguistic features to study the syntactic
contexts in which different error types occur (Mal-
masi and Dras, 2014b).
Results from such approaches could be used
to create teaching material that is customized for
the learner?s L1. This approach has been pre-
viously shown to yield learning improvements
(Laufer and Girsai, 2008). The need for such
SLA tools is particularly salient for a complex lan-
guage such as Arabic which has several learning
stages (Mansouri, 2005), such as phrasal and inter-
phrasal agreement morphology, which are hierar-
chical and generally acquired in a specific order
(Nielsen, 1997).
The key shortcoming of this study, albeit be-
yond our control, is the limited amount of data
available for the experiments. To the best of our
knowledge, this is the smallest dataset used for this
task in terms of document count and length. In this
regard, we are surprised by relatively high classifi-
cation accuracy of our system, given the restricted
amount of training data available.
While it is hard to make comparisons with
most other experiments due to differing number
of classes, one comparable study is that of Wong
and Dras (2009) which used some similar features
on 7-class English dataset. Despite their use of
a much larger dataset
7
, our individual models are
only around 10% lower in accuracy.
We believe that this is a good result, given
our limited data. In their study of NLI corpora,
Brooke and Hirst (2011) showed that increasing
the amount of training data makes a very signifi-
cant difference in NLI accuracy for both syntactic
and lexical features. This was verified by Tetreault
et al. (2012) who showed that there is a very steep
rise in accuracy as the corpus size is increased to-
7
Wong and Dras (2009) had 110 texts per class, with av-
erage text lengths of more than 600 words.
wards 11,000 texts
8
. Based on this, we are con-
fident that given similarly sized training data, an
Arabic NLI system can achieve similar accuracies.
On a broader level, this highlights the need for
more large-scale L2 Arabic corpora.
Future work includes the application of our
methods to large-scale Arabic learner data as it be-
comes available. With the ongoing development
of the Arabic Learner Corpus and other projects
like the Qatar Arabic Language Bank (Mohit,
2013), this may happen in the very near future.
The application of more linguistically sophisti-
cated features also merits further investigation, but
this is limited by the availability of Arabic NLP
tools and resources. From a machine learning per-
spective, classifier ensembles have been recently
used for this task and shown to improve classifi-
cation accuracy (Malmasi et al., 2013; Tetreault et
al., 2012). Their application here could also in-
crease system accuracy.
We also leave the task of interpreting the lin-
guistic features that differentiate and characterize
L1s to future work. This seems to be the next log-
ical phase in NLI research and some methods to
automate the detection of language transfer fea-
tures have been recently proposed (Swanson and
Charniak, 2014; Malmasi and Dras, 2014c). This
research, however, is still at an early stage and
could benefit from the addition of more sophisti-
cated machine learning techniques.
More broadly, additional NLI experiments with
different languages are needed. Comparative stud-
ies using equivalent syntactic features but with dis-
tinct L1-L2 pairs can help us better understand
Cross-Linguistic Influence and its manifestations.
Such a framework could also help us better un-
derstand the differences between different L1-L2
language pairs.
8 Conclusion
In this work we identified the appropriate data and
tools to perform Arabic NLI and demonstrated that
syntactic features can be successfully applied, de-
spite a scarcity of available L2 Arabic data. Such
techniques can be used to generate cross-linguistic
hypotheses and build research tools for Arabic
SLA. As the first machine learning based inves-
tigation of language transfer effects in L2 Ara-
bic, this work contributes important additional ev-
idence to the growing body of NLI work.
8
Equivalent to 1000 texts per L1 class.
184
References
Ahmed Abbasi and Hsinchun Chen. 2005. Applying
authorship analysis to extremist-group Web forum
messages. IEEE Intelligent Systems, 20(5):67?75.
Ghazi Abuhakema, Reem Faraj, Anna Feldman, and
Eileen Fitzpatrick. 2008. Annotating an Arabic
Learner Corpus for Error. In LREC.
Abdullah Alfaifi and Eric Atwell. 2013. Arabic
Learner Corpus v1: A New Resource for Arabic
Language Research.
Abdullah Alfaifi, Eric Atwell, and I Hedaya. 2014.
Arabic learner corpus (ALC) v2: a new written and
spoken corpus of Arabic learners. In Proceedings of
the Learner Corpus Studies in Asia and the World
(LCSAW), Kobe, Japan.
Julian Brooke and Graeme Hirst. 2011. Na-
tive language detection with ?cheap? learner cor-
pora. In Conference of Learner Corpus Research
(LCR2011), Louvain-la-Neuve, Belgium. Presses
universitaires de Louvain.
Malcolm Coulthard and Alison Johnson. 2007. An in-
troduction to Forensic Linguistics: Language in evi-
dence. Routledge.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
John Gibbons and Venn Prakasam. 2004. Language in
the Law. Orient Blackswan.
John Gibbons. 2003. Forensic Linguistics: An Intro-
duction To Language In The Justice System.
Sylviane Granger. 2012. Learner corpora. The Ency-
clopedia of Applied Linguistics.
Tim Grant. 2007. Quantifying evidence in forensic
authorship analysis. International Journal of Speech
Language and the Law, 14(1):1?25.
Nizar Y Habash. 2010. Introduction to Arabic natural
language processing. Synthesis Lectures on Human
Language Technologies, 3(1):1?187.
Batia Laufer and Nany Girsai. 2008. Form-focused
instruction in second language vocabulary learning:
A case for contrastive analysis and translation. Ap-
plied Linguistics, 29(4):694?716.
Shervin Malmasi and Mark Dras. 2014a. Chinese
Native Language Identification. Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics.
Shervin Malmasi and Mark Dras. 2014b. From Vi-
sualisation to Hypothesis Construction for Second
Language Acquisition. In Graph-Based Methods for
Natural Language Processing, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.
Shervin Malmasi and Mark Dras. 2014c. Language
Transfer Hypotheses with Linear SVM Weights.
Proceedings of the 2014 Conference on Empir-
ical Methods in Natural Language Processing
(EMNLP).
Shervin Malmasi, Sze-Meng Jojo Wong, and Mark
Dras. 2013. Nli shared task 2013: Mq submission.
In Proceedings of the Eighth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, pages 124?133, Atlanta, Georgia, June. Asso-
ciation for Computational Linguistics.
Fethi Mansouri. 2005. Agreement morphology in Ara-
bic as a second language. Cross-linguistic aspects of
Processability Theory, pages 117?253.
Gerald R McMenamin. 2002. Forensic linguistics:
Advances in Forensic Stylistics. CRC press.
Behrang Mohit. 2013. QALB: Qatar Arabic language
bank. In Qatar Foundation Annual Research Con-
ference, number 2013.
Helle Lykke Nielsen. 1997. On acquisition order of
agreement procedures in Arabic learner language.
Al-Arabiyya, 30:49?93.
Ria Perkins. 2014. Linguistic identifiers of L1 Persian
speakers writing in English: NLID for authorship
analysis. Ph.D. thesis, Aston University.
Karin C. Ryding. 2013. Teaching Arabic in the United
States. In Kassem M Wahba, Zeinab A Taha, and
Liz England, editors, Handbook for Arabic language
teaching professionals in the 21st century. Rout-
ledge.
Ben Swanson and Eugene Charniak. 2014. Data
Driven Language Transfer Hypotheses. EACL 2014,
page 169.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, Beata
Beigman-Klebanov, and Martin Chodorow. 2012.
Native Tongues, Lost and Found: Resources and
Empirical Evaluations in Native Language Identifi-
cation. In Proc. Internat. Conf. on Computat. Lin-
guistics (COLING).
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Build-
ing Educational Applications, pages 48?57, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Kassem M Wahba, Zeinab A Taha, and Liz England.
2013. Handbook for Arabic language teaching pro-
fessionals in the 21st century. Routledge.
Sze-Meng Jojo Wong and Mark Dras. 2009. Con-
trastive analysis and native language identification.
In Proc. Australasian Language Technology Work-
shop (ALTA), pages 53?61.
185
Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-
sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura
Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014.
Large Scale Arabic Error Annotation: Guidelines
and Framework. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Thierry Declerck, Hrafn
Loftsson, Bente Maegaard, Joseph Mariani, Asun-
cion Moreno, Jan Odijk, and Stelios Piperidis, ed-
itors, Proceedings of the Ninth International Con-
ference on Language Resources and Evaluation
(LREC?14), Reykjavik, Iceland, may. European
Language Resources Association (ELRA).
186
Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 56?64,
October 29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
From Visualisation to Hypothesis Construction
for Second Language Acquisition
Shervin Malmasi
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
shervin.malmasi@mq.edu.au
Mark Dras
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
mark.dras@mq.edu.au
Abstract
One research goal in Second Language Acqui-
sition (SLA) is to formulate and test hypothe-
ses about errors and the environments in which
they are made, a process which often involves
substantial effort; large amounts of data and
computational visualisation techniques promise
help here. In this paper we have defined a new
task for finding contexts for errors that vary
with the native language of the speaker that are
potentially useful for SLA research. We pro-
pose four models for approaching this task, and
find that one based only on error-feature co-
occurrence and another based on determining
maximum weight cliques in a feature associ-
ation graph discover strongly distinguishing
contexts, with an apparent trade-off between
false positives and very specific contexts.
1 Introduction
SLA researchers are interested in a wide variety of as-
pects of humans learning a new language (L2) different
from their native one (L1): cognitive issues and devel-
opmental sequences for learners Pienemann (2005), so-
ciocultural factors (Lantolf, 2001), and so on. One long-
standing question, dating back to at least Lado (1957),
is expressed by Ortega (2009) in the following way:
?What is the role played by first language in L2 develop-
ment, vis-`a-vis the role of other universal development
forces??
An example of SLA research that looks at this ques-
tion is the study of Di?ez-Bedmar and Papp (2008), com-
paring Chinese and Spanish learners of English with
respect to the English article system (a, an, the) using
corpora of essays by native and non-native speakers
of English (Granger, 2011). Drawing on the 175 non-
native texts, they take a particular theoretical analysis
(the so-called Bickerton semantic wheel), use the simple
Wordsmith tools designed to extract data for lexicogra-
phers to identify errors in a semi-automatic way, and
evaluate whether Chinese and Spanish L1 speakers do
behave differently via hypothesis testing (ANOVA, chi-
square and z-tests, in their case). They conclude that
Chinese and Spanish do have characteristic differences,
with patterns of zero article and definite article use dif-
fering according to semantic context. Such studies are
typically carried out on relatively small datasets, and
use fairly elementary tools. Sources such as Ellis (2008)
and Ortega (2009) give good overviews of such studies
and of SLA research in general.
A goal of this paper is to investigate a particular way
in which Natural Language Processing (NLP) can use-
fully contribute to SLA. In terms of existing work, the
subfield of Native Language Identification (NLI) has
been quite active recently, which looks at predicting
the L1 of writers writing in a common L2 within a
classification task framework; see for example the re-
cent NLI shared task with 29 entrants (Tetreault et al.,
2013).
1
From within linguistics, there has been much
interest in how data-driven approaches can contribute to
SLA. Granger (2011) discusses a body of work based
on the the methodology of carrying out corpus-based
approaches to SLA with a focus on NLP tools; Jarvis
and Crossley (2012) in an edited collection present re-
cent work by linguists who extend the corpus-based
setup by using a text classification approach, looking at
what feature selection might say for SLA. From within
NLP, Swanson and Charniak (2013) and Swanson and
Charniak (2014) take a data-driven approach to SLA
investigations much in the spirit of this work.
One particular approach to finding aspects of texts
characteristic of their L1s that has motivated the present
work is described in Yannakoudakis et al. (2012), the
goal of which is to develop visualisation tools for SLA
researchers. They present graphs of the relationships
between errors and their contexts, such that SLA re-
searchers can navigate through the graphs to find con-
texts for particular errors that can lead to hypotheses
like that of Di?ez-Bedmar and Papp (2008) above. In this
paper, we look at approaches to finding such hypothesis
candidates automatically in the context of L1?L2 inter-
action by analysing the graphs used in the visualisations
1
http://sites.google.com/site/
nlisharedtask2013/
56
of Yannakoudakis et al. (2012). Specifically, we do the
following:
? We propose a new task that is more directly ori-
ented to SLA research than NLI has been for the
most part, with the goal of identifying error-related
contexts that are characteristic of L1s.
? We evaluate a number of models for finding such
contexts, ranging from a simple baseline to treat-
ing the problem as a graph-theoretic maximum
weighted clique one.
? We examine the results of some of the models to
see how the task and the models might contribute
to SLA research.
Because we draw heavily on the work of Yan-
nakoudakis et al. (2012), we first review relevant aspects
of that work in ?2; we then present our task definition
and experimental setup in ?3; we give results along with
a discussion in ?4; we follow with some more detail on
related work in ?5; and we conclude in ?6.
2 Developing Hypotheses: A
Visualisation Tool
The context of the Yannakoudakis et al. (2012) work
is automated grading of English as a Second or Other
Language (ESOL) exam scripts, as described in Briscoe
et al. (2010). The automated grading takes a classifi-
cation approach, using a binary discriminative learner,
with useful features including lexical and part-of-speech
(PoS) n-grams.
The publicly available dataset on which the work was
carried out consists of texts from the First Certificate in
English (FCE) exam, aimed at upper-intermediate stu-
dents of English across various L1s, and was presented
in Yannakoudakis et al. (2011). This FCE corpus
2
con-
sists of a subset of 1244 texts of the Cambridge Learner
Corpus,
3
and is manually annotated with errors and their
corrections, as well as a classification according to an
error typology, as in Figure 1.
Yannakoudakis et al. (2012) present their English
Profile (EP) visualiser as a way to ?visually analyse as
well as perform a linguistic interpretation of discrimi-
native features that characterise learner English?, using
the features of this essay classification task. They de-
fine a measure of co-occurrence of features, among
themselves and with errors, as a core part of their
analysis. Given the set of all sentences in the corpus
S = {s
1
, s
2
, . . . , s
|S|
} and the set of all features F =
{f
1
, f
2
, . . . , f
|F |
}, a feature f
i
? F is associated with
a feature f
j
? F (i 6= j, 1 ? i, j ? M ) according to
the score given in Equation (1), for s
k
? S, 1 ? k ? N
2
http://ilexir.co.uk/applications/
ep-visualiser/
3
http://www.cup.cam.ac.uk/gb/elt/
catalogue/subject/custom/item364603/
and exists() a binary function returning true if the input
feature occurs in s
k
.
score
ff
(f
j
, f
i
) =
?
|S|
k=1
exists(f
j
, f
i
, s
k
)
?
|S|
k=1
exists(f
i
, s
k
)
(1)
They mention an analogous measure for feature-error
co-occurrence; we assume given the set of all errors
E = {e
1
, e
2
, . . . , e
|E|
} that this is defined as follows:
score
ef
(f
j
, e
i
) =
?
|S|
k=1
exists(f
j
, e
i
, s
k
)
?
|S|
k=1
exists(e
i
, s
k
)
(2)
A graph is defined with features and errors as vertices;
an edge between features (resp. features and errors) is
established if score
ff
() (resp. score
ef
) is within some
user-defined range. This graph of feature?feature (resp.
feature?error) relationships is then presented visually.
The paper then presents a case study of how the EP vi-
sualiser can be used to assist SLA researchers. The case
study starts by noting that RG_JJ_NN1 is the 18th most
discriminative negative feature from the essay classi-
fier; then, further inspecting the graph of discriminative
features, that it?s linked to JJ_NN1_II and VBZ_RG.
Then, looking at feature-error relations, it investigates
an association with error MD (missing determiner), and
presents some examples that match the features (e.g.
Unix is very powerful system but there is one thing
against it), along with a discussion of relationships to
various L1s. It is this process of finding interesting fea-
tures and linking them to particular errors and L1s that
we present an approach to automating in this paper.
3 Task Definition & Experimental Setup
At a general level, our goal is to find which kinds of
constructions (in a loose sense) centred around errors
are particularly characteristic of various L1s.
The specific task we define for this paper, then, is
to select a set of features (in the terminology of Yan-
nakoudakis et al. (2012))?which we refer to as the
ERROR CONTEXT?that, when combined with the er-
ror, show a strong association with L1, in a manner
we describe below. So, for example, this may involve
finding that an MD error in the context of RG_JJ_NN1,
JJ_NN1_II and VBZ_RG shows a strong association
with L1. We investigate a number of models for this
selection process: the task then is the identification of
which models produce poor error contexts (which will
not rank highly in hypothesis testing) and which pro-
duce good ones (potentially worth considering by an
SLA researcher). Below we discuss the data we use,
the measure of association for an error and its context,
the set of errors chosen, and the models for selecting
context.
3.1 Data
The corpus we use for evaluating the models for our task
is derived from the FCE corpus of Yannakoudakis et al.
57
Verb Agreement <p>Some people <ns type="AGV"><i>says</i><c>say</c></ns> ...</p>
Incorrect Verb <p>The day I <ns type="IV"><i>shaked</i><c>shook</c></ns> their
Inflection
hands,...</p>
Missing Determiner <p>I am <ns type="MD"><c>a</c></ns> really good singer.</p>
Figure 1: FCE corpus examples. Error types indicated by <ns type>...</ns>; errors indicated by <i>...</i>;
corrections indicated by <c>...</c>.
language size
Chinese CHI 66
French FRE 146
German GER 69
Italian ITA 76
Japanese JAP 81
Korean KOR 86
Spanish SPA 200
Turkish TUR 75
Table 1: FCESUB, broken down by language
(2012). The full FCE corpus consists of 1244 scripts
over 16 languages; script counts range from 2 (Dutch)
to 200 (Spanish).
The features used by Yannakoudakis et al. (2012)
were derived from their essay classification task. As we
are interested in associations with L1, we instead use
features from a system submitted to the NLI shared task
(Anonymous, 2013), which was applied to a dataset of
Test of English as a Foreign Language (TOEFL) scripts:
the task and its designated corpus are described in the
task overview paper (Tetreault et al., 2013). In this work
we use a system trained on the TOEFL11 corpus con-
sisting of texts written in English from speakers of 11
different L1s, with 1100 essays per L1 and balanced
across topic. We only use PoS n-grams (n = 1, 2, 3) as
features in this work. Note that we use the terminology
of Yannakoudakis et al. (2012) here: what had their
origin as features in the essay classification task are still
referred to as features in the visualisation tool, although
the task carried out there is not a classification one. Sim-
ilarly, we refer to our PoS n-grams as features, although
we are not classifying errors using these features and
so are not carrying out feature selection for the typical
purpose of optimising classification performance.
For this, as did Yannakoudakis et al. (2012), we use
the RASP parser (Briscoe et al., 2006) for tagging; the
tags are consequently from the CLAWS2 tagset,
4
which
are more fine-grained in terms of linguistic analysis than
the more frequently used Penn Treebank tags.
For our task, we then used the subset of the FCE cor-
pus where the languages overlapped with the TOEFL11
corpus: we refer to this as FCESUB. This gives 799
scripts over 8 languages, distributed as in Table 1; a
positive byproduct is that the L1s are more similar in
size than the full FCE corpus.
4
http://ucrel.lancs.ac.uk/claws2tags.
html
language mean
CHI 0.885790
FRE 0.460894
GER 0.366587
ITA 0.581401
JAP 1.058159
KOR 1.067211
SPA 0.472253
TUR 1.014129
F-stat 18.031
sig. <0.001
Table 2: ANOVA results giving mean score (number
of sentences with MD error per 10 sentences) for each
language, the ANOVA F-statistic, and significance value
3.2 Association Measure
We noted in ?1 that SLA studies such as Di?ez-Bedmar
and Papp (2008) use standard hypothesis testing tech-
niques. We take this as a starting point. We could, for
example, evaluate whether a particular raw error (that
is, without a feature context) is strongly associated with
L1s by using a single factor ANOVA test.
5
The indepen-
dent variable would be the L1. The dependent variable
could be one of a number of alternatives; we choose the
number of sentences with a particular error per 10 sen-
tences.
6
To illustrate, we give the ANOVA results from
FCESUB for the MD error in Table 2. The ANOVA
calculation is based on an F-statistic which compares
variance between treatments against variance within
treatments; this is compared against critical values for
the F-statistic to determine statistical significance. The
expected value of the F-statistic under the null hypoth-
esis is 1, with values above 1 increasingly inconsistent
with the null hypothesis. The data in Table 2 shows
that the MD error does vary significantly with L1; a
post-hoc Tukey HSD test lets us identify which specific
languages exhibit this difference and shows that, for
example (and as can be observed in the means), German
L1 speakers are significantly different from Korean L1
speakers in the occurrence of MD errors.
For our task we are not interested in significance per
se. Rather, we are interested in whether we can find oc-
currences of errors plus contexts that are more strongly
associated with, or that vary across, L1s, e.g. that an
5
See, e.g., Jackson (2009).
6
We note that the texts differ significantly in length by L1,
so it would not be suitable to normalise as occurrences per
document.
58
type name F-stat p-val N
DJ Wrong Derived 3.27 .002 332
Adjective
DN Wrong Derived 0.70 .671 294
Noun
MD Missing Determiner 18.03 .000 1702
MT Missing Preposition 2.81 .007 985
UD Unnecessary Determiner 1.20 .301 807
UT Unnecessary Preposition 0.26 .968 689
UV Unnecessary Verb 0.78 .606 317
Table 3: Error types chosen for evaluation, including F-
statistic, ANOVA p-value and corpus count of sentences
containing error.
MD error in the context of RG_JJ_NN1, JJ_NN1_II
and VBZ_RG is more strongly associated with L1s; and
we are also interested in which of our proposed methods
for identifying an error?s feature context does this best.
For this purpose, then, we use just the F-statistic from
the ANOVA test, this time with the dependent variable
as the ratio of occurrences of error plus error context
per 10 sentences: a higher F-statistic shows a stronger
association with L1s.
7
We also consider the ?
2
-statistic from Pearson?s chi-
squared test, noting that it is also used in SLA hypothe-
sis testing and that it was additionally found by Swanson
and Charniak (2013) to be good at distinguishing inter-
esting features in their related task (see ?5 for more
detail). The F-statistic and ?
2
-statistic are closely re-
lated: a random variate of the F-distribution is the ratio
of two chi-squared variates scaled by their degrees of
freedom. A difference is that ?
2
compares observed
versus expected counts rather than proportions: to take
account of the differing text lengths, our observed fre-
quency is the number of sentences with error and error
context per L1; our expected frequency is the total num-
ber of sentences with that error and error context scaled
according to the proportion of sentences labelled with
that L1 relative to the corpus as a whole.
3.3 Errors Chosen
From the 74 error types in the FCE corpus, we select a
subset to evaluate our models. In addition to the MD er-
ror used in the case study of Yannakoudakis et al. (2012),
we choose a subset which has a range of F-statistic val-
ues as described above: some show very similar patterns
across L1s (i.e. with low F-statistic), such as DN Wrong
Derived Noun (e.g. hot vs heat); others do vary signif-
icantly with L1, such as DJ Wrong Derived Adjective
(e.g. reasonally vs reasonable). Having errors with
a range of F-statistic values lets us evaluate whether
finding good error contexts works only for strongly L1-
associated errors, weakly L1-associated errors, or across
7
As we are only using the F-statistic to evaluate ranks, we
do not need a multiple comparison adjustment such as the
Bonferroni correction: this would only apply for comparisons
to a significance threshold, and in any case the Bonferroni is
monotonic and does not affect rankings.
the spectrum. Our subset is in Table 3, along with their
F-statistic, ANOVA p-value and counts in FCESUB.
3.4 Models
We propose four models for choosing error contexts.
These models rank error contexts; we evaluate the
ranked error contexts by F-statistic and ?
2
-statistic val-
ues (?3.2).
ERRORCOOCC In this model we rank features by
error-feature co-occurrence scores given by Equation
(2). The L1 is not taken into account, so this will just
return common features which may be equally strongly
associated with errors across all L1s. We look at results
for when k = 1..3 features are chosen. For k = 2, 3,
we add the individual error-feature scores together for
the ranking.
8
It may be the case that interesting results
could be obtained for k > 3, but we only look at the
k = 1..3 in this preliminary work to see if there are
any discernible trends suggesting that larger values of k
could help.
L1ASSOC Here we use features that are strongly as-
sociated with the L1s from the TOEFL11 corpus and
NLI shared task. Specifically, we rank features by their
Information Gain with respect to L1s as in the process of
feature selection from the shared task.
9
The relationship
between errors and features (in the form of error-feature
co-occurrence scores) is not taken into account here.
Again, we look at results for when k = 1..3 features
are chosen, and for k = 2, 3, we add the individual
error-feature scores together for the ranking.
MAXWEIGHTCLIQUE Both of the preceding mod-
els look only at one factor that might be relevant: error-
feature scores (finding features that are related to the
errors) and a measure of the association of features with
L1s; but there is no link between them, and interaction
of features is not taken into account. In Yannakoudakis
et al. (2012), the visualiser provides to the SLA re-
searcher a graph showing the relatedness of features,
based on Equation (1), and the SLA researcher com-
bines this with error-feature scores to find interesting
candidate error contexts; we create a similar graph and
aim to imitate the process by incorporating error-feature
scores as follows.
We define a weighted undirected graph G = (V,A)
such that V is the set of features used in the above
models (i.e. PoS n-grams from ERRORCOOCC); A is
defined such that (v
i
, v
j
) ? A for vertices v
i
, v
j
? V
if 0.8 ? score
ff
(v
i
, v
j
) ? 1.0 where score
ff
() is as
defined as in Equation (1).
10
Given our set of errors
E defined at Equation (2) above, the weight of a ver-
tex v
i
is defined as score
ef
(v
i
, e
j
) for some e
j
? E.
8
For k = 2 the combinations were made from the top 100
features from k = 1, and for k = 3 from the top 50.
9
We recalculated this over the subset of eight languages
used in this paper.
10
We choose this threshold value as it is the one used in the
graph definition of Yannakoudakis et al. (2012).
59
model r
ERRORCOOCC 0.95
L1ASSOC 0.97
MAXWEIGHTCLIQUE 0.95
MAXWEIGHTCLIQUE-L1 0.92
Table 4: Average correlation coefficient r between F-
statistic and ?
2
-statistic for each model
Given this graph, it is possible to characterise the find-
ing of related features with strong aggregate associations
with errors as an instance of the MAXIMUM WEIGHT
CLIQUE PROBLEM (Bomze et al., 1999). As the name
suggests, this finds a clique of maximum weight, here
the strongest aggregate feature?error association. While
this is an NP-hard problem, there are quite efficient algo-
rithms for solving it; we use one proposed by
?
Osterg?ard
(1999).
11
MAXWEIGHTCLIQUE-L1 We also look at a vari-
ant of MAXWEIGHTCLIQUE where we construct the
graphs based only on relationships among features for
a particular L1. That is, there will be eight weighted
graphs per error of interest.
4 Results and Discussion
4.1 Overall Results
We only present the F-statistic results here; the ?
2
-
statistic showed very similar patterns. The average
correlation between the two for each model shows the
strong similarity (Table 4).
For the F-statistic results, presented in Table 5, we
report the highest F-statistic in the N -best list (N =
1, 5, 20, 50) for each model. For models ERRORCOOCC
and L1ASSOC we report the highest F-statistic for each
value of k (k = 1, 2, 3). The number of occurrences
of the error context with the highest F-statistic is given
in parentheses after the F-statistic; the highest value
for each N is in bold. For MAXWEIGHTCLIQUE-L1,
we also note the language of the graph from which the
highest score was derived.
We note by comparing Table 5 with Table 3 that for
each error type except for MD, it is possible to find
an error context that is more strongly associated with
L1s than is the raw error type alone. For MD this is
not surprising, as its frequency of occurrence is very
strongly linked to the L1, as noted in Table 2 and ?3.2.
12
(For the error type MT also, no model produces an error
context more strongly associated with the L1 for the
single best choice where N = 1, but does for larger
values of N .)
11
Code for the used wclique is available at http://tcs.
legacy.ics.tkk.fi/
?
pat/wclique.html.
12
The fact that determiner errors are very widely studied in
terms of analysing cross-linguistic influence suggests a broad
consensus that they vary strongly with L1. In addition to Di?ez-
Bedmar and Papp (2008), a sample of other studies includes
Parrish (1987), Young (1996) and Ionin and Montrul (2010).
With respect to the individual models, the simple ER-
RORCOOCC scores highly, giving the best result about
half the time, and the best results can occur for any
of k = 1, 2, 3. The number of instances returned for
each error plus error context is larger than for the other
models as well, which is not surprising as the model
aims to find contexts strongly associated with the errors
rather than with L1s. However, these are then likely to
be features that are fairly common across L1s; we look
at some examples in ?4.2.
L1ASSOC performs fairly poorly on our evaluation
measure, although in many cases it does find an error
context more strongly associated with the L1 than just
the raw error type. Counts are also lower. Also, for this
model, k = 2, 3 are always worse than k = 1: bringing
in a second context feature reduces the number of oc-
currences to such an extent that the F-statistic can drop
dramatically. This is probably in part an artefact of the
size of the FCE corpus (and particularly our FCESUB
subcorpus): these features derived from the TOEFL11
corpus just do not occur sufficiently often in our evalua-
tion corpus (and in fact there are often large numbers of
zero occurrences for k = 2, 3).
MAXWEIGHTCLIQUE also performs fairly poorly.
However, in many cases it also finds an error context
more strongly associated with L1 than the raw error type
alone (DN, MT, UD, UT, UV), even if not always for
N = 1, and it has intermediate counts of occurrences.
MAXWEIGHTCLIQUE-L1 gives the best results in
the other half of the cases where ERRORCOOCC does
not. The error contexts that it finds, however, are very
specific, often to a single language (as might be expected
by its definition) with very small numbers of counts.
4.2 Some Examples
We look at some examples in Figure 2, to illustrate both
interesting error contexts found and areas where the
models do a poor job. In these sample sentences, only
errors of interest are retained and highlighted.
The DJ error with context { JJ, NN1 } illustrates the
top result found under the ERRORCOOCC model for
N = 20. In the first sentence the model seems to find a
useful pattern: the adjective that is at the centre of the
error occurs in the context of a singular noun. On the
other hand, the second sentence illustrates a problem:
because the range of the context is the whole sentence,
frequent features such as NN1 will occur a lot in other
parts of the sentence that have no apparent relation to
the actual error. The ERRORCOOCC model is thus likely
to be picking up false positives by virtue of the relatively
high frequencies of its error contexts.
The UV error with context { TO_VV0_II, NNL1,
II, NN2, VV0_II } illustrates the top result found
under the MAXWEIGHTCLIQUE-L1 model for N =
5. This is very specific, and its three instances only
appear in Turkish. But all three are similar errors from
different documents, so it appears likely to be a genuine
pattern, although the NN2 seems only to have a tenuous
60
e
r
r
o
r
N
E
R
R
O
R
C
O
O
C
C
L
1
A
S
S
O
C
M
A
X
W
E
I
G
H
T
C
L
I
Q
U
E
M
A
X
W
E
I
G
H
T
C
L
I
Q
U
E
-
L
1
D
J
1
2
.
7
8
(
2
7
4
)
/
3
.
1
9
(
2
2
7
)
/
2
.
9
5
(
1
5
8
)
1
.
5
9
(
3
1
)
/
1
.
5
9
(
3
1
)
/
0
.
8
1
(
6
)
0
.
9
9
(
1
5
)
3
.
0
8
(
2
)
[
G
E
R
]
5
3
.
6
0
(
2
6
8
)
/
3
.
1
9
(
2
2
7
)
/
3
.
0
2
(
1
4
8
)
2
.
1
9
(
1
2
)
/
1
.
5
9
(
3
1
)
/
0
.
8
1
(
6
)
1
.
7
4
(
4
1
)
3
.
2
4
(
2
)
[
C
H
I
]
2
0
3
.
7
2
(
1
9
4
)
/
3
.
3
3
(
1
6
3
)
/
4
.
0
2
(
9
3
)
2
.
5
3
(
7
0
)
/
1
.
5
9
(
3
1
)
/
1
.
3
6
(
1
)
2
.
3
4
(
2
4
)
3
.
5
0
(
5
)
[
I
T
A
]
5
0
3
.
7
2
(
1
9
4
)
/
3
.
3
9
(
1
1
4
)
/
4
.
0
2
(
9
3
)
2
.
5
8
(
1
0
7
)
/
1
.
5
9
(
3
1
)
/
1
.
5
9
(
3
1
)
2
.
4
8
(
1
8
)
3
.
8
4
(
3
)
[
I
T
A
]
D
N
1
0
.
7
7
(
2
6
8
)
/
1
.
6
3
(
1
8
5
)
/
1
.
7
3
(
1
1
9
)
1
.
0
9
(
4
0
)
/
1
.
0
9
(
4
0
)
/
0
.
7
0
(
7
)
1
.
2
6
(
6
3
)
3
.
2
4
(
2
)
[
C
H
I
]
5
1
.
8
0
(
1
9
1
)
/
2
.
2
9
(
1
5
3
)
/
2
.
5
4
(
1
4
2
)
1
.
2
5
(
5
)
/
1
.
3
6
(
1
)
/
1
.
3
6
(
1
)
1
.
2
6
(
6
3
)
3
.
2
4
(
2
)
[
C
H
I
]
2
0
2
.
3
4
(
8
6
)
/
2
.
6
9
(
1
4
4
)
/
2
.
9
5
(
1
1
3
)
2
.
0
4
(
2
6
)
/
1
.
3
6
(
1
)
/
1
.
3
6
(
1
)
1
.
7
6
(
3
0
)
3
.
2
4
(
2
)
[
C
H
I
]
5
0
2
.
8
6
(
6
1
)
/
3
.
1
6
(
1
2
0
)
/
2
.
9
5
(
1
1
3
)
3
.
8
9
(
4
)
/
2
.
7
5
(
2
)
/
2
.
7
5
(
2
)
3
.
4
1
(
1
8
)
4
.
2
7
(
1
0
)
[
S
P
A
]
M
D
1
1
4
.
2
8
(
1
3
1
9
)
/
9
.
0
9
(
9
8
5
)
/
6
.
3
8
(
7
5
3
)
5
.
8
3
(
1
9
8
)
/
5
.
8
3
(
1
9
8
)
/
0
.
5
4
(
2
)
3
.
0
7
(
2
9
7
)
4
.
0
5
(
9
1
)
[
K
O
R
]
5
1
4
.
2
8
(
1
3
1
0
)
/
1
2
.
1
8
(
7
6
9
)
/
6
.
7
5
(
5
8
2
)
8
.
2
0
(
2
6
8
)
/
5
.
8
3
(
1
9
8
)
/
1
.
9
3
(
3
)
5
.
8
3
(
1
9
8
)
5
.
8
3
(
1
9
8
)
[
K
O
R
]
2
0
1
4
.
4
1
(
8
5
0
)
/
1
2
.
1
8
(
7
6
9
)
/
6
.
8
2
(
5
9
3
)
8
.
2
0
(
2
6
8
)
/
5
.
8
3
(
1
9
8
)
/
2
.
6
0
(
3
6
)
5
.
8
3
(
1
9
8
)
5
.
8
3
(
1
9
8
)
[
K
O
R
]
5
0
1
4
.
4
1
(
8
5
0
)
/
1
2
.
1
8
(
7
6
9
)
/
7
.
9
9
(
4
8
3
)
8
.
3
6
(
8
3
1
)
/
5
.
8
3
(
1
9
8
)
/
5
.
8
3
(
1
9
8
)
5
.
8
3
(
1
9
8
)
6
.
4
7
(
1
1
0
)
[
K
O
R
]
M
T
1
3
.
3
4
(
7
9
4
)
/
3
.
0
0
(
6
6
6
)
/
3
.
0
2
(
4
8
5
)
1
.
8
5
(
7
9
)
/
1
.
8
5
(
7
9
)
/
1
.
5
5
(
1
3
)
1
.
7
0
(
6
1
)
2
.
4
8
(
2
0
)
[
C
H
I
]
5
3
.
3
4
(
7
9
4
)
/
3
.
4
6
(
4
7
8
)
/
3
.
3
7
(
3
7
8
)
2
.
5
4
(
1
0
1
)
/
1
.
8
5
(
7
9
)
/
1
.
5
5
(
1
3
)
2
.
1
4
(
6
4
)
4
.
4
7
(
3
)
[
C
H
I
]
2
0
4
.
4
4
(
2
9
5
)
/
3
.
6
4
(
3
7
5
)
/
4
.
6
0
(
2
9
4
)
4
.
4
4
(
2
9
5
)
/
3
.
1
1
(
2
5
)
/
3
.
1
1
(
2
5
)
2
.
7
9
(
4
4
)
4
.
4
7
(
3
)
[
C
H
I
]
5
0
4
.
5
0
(
2
7
7
)
/
5
.
2
1
(
2
4
7
)
/
4
.
7
2
(
2
1
5
)
4
.
4
4
(
2
9
5
)
/
3
.
8
6
(
3
3
)
/
3
.
1
1
(
2
5
)
4
.
5
4
(
7
4
)
4
.
6
1
(
3
)
[
G
E
R
]
U
D
1
0
.
6
9
(
6
7
9
)
/
1
.
0
5
(
4
7
5
)
/
2
.
0
8
(
3
3
4
)
1
.
4
5
(
6
2
)
/
1
.
4
5
(
6
2
)
/
0
.
7
3
(
1
0
)
0
.
6
4
(
4
7
)
1
.
5
4
(
2
0
)
[
G
E
R
]
5
1
.
7
0
(
4
0
5
)
/
1
.
1
7
(
4
5
2
)
/
2
.
0
8
(
3
3
4
)
1
.
5
9
(
2
6
)
/
1
.
4
5
(
6
2
)
/
1
.
3
6
(
1
)
1
.
4
5
(
6
2
)
3
.
5
4
(
9
)
[
C
H
I
]
2
0
2
.
0
8
(
2
2
3
)
/
2
.
1
1
(
3
6
0
)
/
2
.
3
2
(
2
7
6
)
3
.
4
1
(
5
1
)
/
1
.
4
5
(
6
2
)
/
1
.
3
6
(
1
)
1
.
9
0
(
2
9
)
3
.
9
3
(
3
)
[
I
T
A
]
5
0
3
.
2
7
(
1
1
2
)
/
3
.
0
1
(
1
8
8
)
/
2
.
3
3
(
1
9
8
)
3
.
4
1
(
5
1
)
/
1
.
5
4
(
4
)
/
1
.
5
4
(
4
)
2
.
8
5
(
6
6
)
4
.
0
6
(
3
)
[
I
T
A
]
U
T
1
0
.
1
4
(
5
4
8
)
/
0
.
4
5
(
4
1
4
)
/
1
.
1
2
(
2
5
9
)
1
.
0
1
(
5
1
)
/
1
.
0
1
(
5
1
)
/
0
.
4
3
(
1
)
0
.
8
1
(
3
5
)
3
.
0
6
(
2
)
[
G
E
R
]
5
0
.
8
2
(
3
6
8
)
/
1
.
1
6
(
3
2
1
)
/
1
.
5
8
(
2
4
9
)
2
.
2
8
(
2
3
)
/
1
.
3
6
(
1
)
/
1
.
3
6
(
1
)
1
.
0
1
(
5
1
)
4
.
1
0
(
3
)
[
T
U
R
]
2
0
1
.
5
1
(
3
5
1
)
/
1
.
7
7
(
2
7
5
)
/
1
.
8
9
(
2
2
5
)
2
.
9
1
(
5
1
)
/
1
.
5
3
(
6
)
/
1
.
3
6
(
1
)
2
.
5
8
(
4
5
)
4
.
1
0
(
3
)
[
T
U
R
]
5
0
2
.
2
5
(
1
1
2
)
/
2
.
6
6
(
2
0
1
)
/
3
.
1
8
(
1
7
8
)
2
.
9
1
(
5
1
)
/
1
.
5
3
(
6
)
/
1
.
3
6
(
1
)
2
.
5
8
(
4
5
)
4
.
1
0
(
3
)
[
T
U
R
]
U
V
1
0
.
8
8
(
2
6
0
)
/
0
.
9
7
(
1
8
6
)
/
1
.
1
8
(
1
1
9
)
1
.
0
6
(
1
5
)
/
1
.
0
6
(
1
5
)
/
1
.
2
9
(
2
)
1
.
4
9
(
2
8
)
2
.
5
3
(
2
)
[
J
A
P
]
5
2
.
2
2
(
1
7
5
)
/
2
.
2
1
(
1
6
2
)
/
1
.
6
8
(
1
0
9
)
2
.
2
9
(
8
)
/
1
.
2
9
(
2
)
/
1
.
2
9
(
2
)
1
.
4
9
(
2
8
)
4
.
0
9
(
3
)
[
T
U
R
]
2
0
2
.
2
5
(
1
2
5
)
/
2
.
8
2
(
1
2
7
)
/
3
.
1
3
(
9
6
)
3
.
2
2
(
8
)
/
1
.
5
2
(
1
)
/
1
.
5
2
(
1
)
2
.
3
8
(
1
5
)
4
.
0
9
(
3
)
[
T
U
R
]
5
0
2
.
5
6
(
6
1
)
/
3
.
0
1
(
1
0
1
)
/
3
.
1
3
(
9
6
)
3
.
2
2
(
8
)
/
1
.
5
2
(
1
)
/
1
.
5
2
(
1
)
2
.
3
8
(
1
5
)
4
.
6
3
(
3
)
[
C
H
I
]
T
a
b
l
e
5
:
R
e
s
u
l
t
s
f
o
r
t
h
e
c
h
o
s
e
n
e
r
r
o
r
t
y
p
e
s
u
n
d
e
r
t
h
e
f
o
u
r
p
r
o
p
o
s
e
d
m
o
d
e
l
s
.
A
l
l
e
r
r
o
r
t
y
p
e
s
a
n
d
m
o
d
e
l
s
r
e
p
o
r
t
t
h
e
b
e
s
t
F
-
s
t
a
t
i
s
t
i
c
f
o
r
t
h
e
s
e
l
e
c
t
e
d
e
r
r
o
r
c
o
n
t
e
x
t
a
n
d
f
r
e
q
u
e
n
c
y
w
i
t
h
i
n
t
h
e
t
o
p
N
(
N
=
1
,
5
,
2
0
,
5
0
)
.
E
R
R
O
R
C
O
O
C
C
a
n
d
L
1
A
S
S
O
C
g
i
v
e
t
h
e
b
e
s
t
s
c
o
r
e
f
o
r
t
h
e
s
e
t
o
f
k
f
e
a
t
u
r
e
s
(
k
=
1
,
2
,
3
)
.
M
A
X
W
E
I
G
H
T
C
L
I
Q
U
E
-
L
1
a
l
s
o
n
o
t
e
s
t
h
e
l
a
n
g
u
a
g
e
g
r
a
p
h
w
i
t
h
t
h
e
b
e
s
t
r
e
s
u
l
t
.
61
error context example sentences
DJ JJ, NN1 Basically/RR ,/, I/PPIS1 helped/VVD them/PPHO2 liaise/VV0
with/IW the/AT local/JJ police/NN and/CC get/VV0 some/DD
<ns type="DJ"><i>electronical</i><c>electronic/JJ</c></ns> equipmen-
t/NN1 that/CST they/PPHS2 needed/VVD.
The/AT show/NN1 will/VM be/VB0 at/II the/AT Central/JJ
Exhibition/NN1 Hall/NP1 and/CC it/PPH1 will/VM be/VB0
<ns type="DJ"><i>opened</i><c>open/JJ</c></ns> until/ICS 7/MC.
UV TO_VV0_II,
NNL1, II, NN2,
VV0_II
I/PPIS1 used/VMK to/TO <ns type="UV"><i>be</i></ns> play/VV0 in/II the/AT
school/NNL1 team/NN1 . . . and/CC our/APP$ team/NN1 was/VBDZ one/MC1 of/IO the/AT
best/JJT basketball/NN1 teams/NN2 . . .
DN XX, XX_VV0,
VM_XX_VV0, NN1
Never/RR the/AT less/DAR ,/, in/II summer/NNT1 we/PPIS2 can/VM n?t/XX resist/VV0
such/DA <ns type="DN"><i>hot</i><c>heat/NN1</c></ns>!
. . . I/PPIS1 think/VV0 you/PPY should/VM have/VH0 a/AT1 <ns type="DN"><i>baby-
parking</i><c>kindergarten/NP1</c></ns> ,/, in/II fact/NN1 a/AT1 certain/JJ num-
ber/NN1 of/IO women/NN2 could/VM n?t/XX see/VV0 the/AT Festival/NN1 because/CS
of/IO their/APP$ sons/NN2.
MD VBZ_RG,
RG_JJ_NN1
The/AT first/MD and/CC most/RR important/JJ thing/NN1 is/VBZ that/RG modern/JJ
technology/NN1 has/VHZ made/VVN our/APP$ life/NN1 easier/JJR ,/, for/IF instance/NN1
<ns type="MD"><c>the/AT</c></ns> rice/NN1 cooker/NN1 is/VBZ a/AT1 great/JJ
invention/NN1 . . .
Figure 2: Examples for sample error types and specific error contexts. Error contexts are bolded.
connection.
The DN error with context { XX, XX_VV0,
VM_XX_VV0, NN1 } illustrates the top result found un-
der the MAXWEIGHTCLIQUE-L1 model for N = 50.
A number of this reasonably sized set are similar to the
first sentence, where the context appears interesting. In
this example, hot is used for heat; the other examples
of this type are from Spanish and Italian (similarly, e.g.,
live for life), where the error seems to be connected
to words where the English derivational morphology
is not simply affixation. However, there are some like
the second sentence, where (as for the DJ error) the
error context appears in a different clause, and likely
irrelevant.
The MD error in the last row we examine because (a
more complex version of) it was the focus of the case
study in Yannakoudakis et al. (2012), which from the
examples of that paper looked quite convincing as an
error context of relevance to SLA research. However, it
and the related examples of Yannakoudakis et al. (2012)
were not in the publicly available corpus,
13
and in fact
there is only one example of this error and context in the
whole FCE corpus, illustrating the issue of data sparsity.
Further, this example also illustrates the issue of tagging
error: that is tagged as RG (degree adverb) where it
should be CST.
So as might be anticipated from the frequency num-
bers in Table 5, the MAXWEIGHTCLIQUE-L1 model
produces context that looks interesting from an SLA per-
spective, but is relatively limited in scope; the ERROR-
COOCC model produces a much larger set of candidates,
and can successfully find error context such that they
behave differently with respect to the L1s according
to the ANOVA F-statistic, but produces false positives.
Overall, a recurring issue illustrated for all models by
13
We assume that the multiple examples come from the
larger CLC corpus.
the examples is the proposal of error context far away
from any likely relevance to SLA.
5 Related Work
While Native Language Identification (NLI) as a sub-
field of NLP has seen much new work in the last few
years ? the papers from the shared task (Tetreault et
al., 2013) provide a recent sample ? the emphasis on
optimising classification task results, for example by
using classifier ensembles (Malmasi et al., 2013), ver-
sus analysing features for relevance to other tasks has
varied. Below we discuss works which directly look
at how features might be related to language-learning
tasks or SLA research.
The seminal work of Koppel et al. (2005) that pre-
sented NLI as a classification task included, in addition
to standard lexical and PoS n-gram features, errors made
by the writers; these errors were automatically identi-
fied using Microsoft Word grammar checker. Kochmar
(2011) used the FCE corpus for NLI, including the man-
ually annotated errors as features, and presented an anal-
ysis of usefulness of features (including errors) with
respect to L1.
Wong and Dras (2011) used syntactic features on the
basis of SLA theory that posits that L1 constructions
may be reflected in some form of characteristic errors or
patterns in L2 constructions to some extent, or through
overuse or avoidance of particular constructions in L2
(Lado, 1957; Ellis, 2008); they did note distributional
differences of features related to L1. Wong et al. (2012)
induced topic models over function words and PoS n-
grams, where some of the topics appeared to reflect L1-
specific characteristics. These works, while interested
in the nature of the features, do not evaluate them except
via classification accuracy.
Swanson and Charniak (2012) similarly explore us-
ing syntax, where they propose a richer representation
62
for L1-specific constructions through Tree Substitution
Grammar (TSG). Swanson and Charniak (2013) sub-
sequently examine both relevancy and redundancy of
features through a number of metrics (including the
?
2
-statistic used in this paper). They then extend a
Bayesian induction model for TSG inference based on
a supervised mixture of hierarchical grammars, in order
to extract a filtered set of more linguistically informed
features that could benefit both NLI and SLA research;
an aim was to find relatively rare features that are nev-
ertheless useful for L1 prediction. Swanson and Char-
niak (2014) continue on from this with a data-driven
approach to inferring possible relationships between L1
and L2 structures, again using TSGs. Malmasi and Dras
(2014c) also propose a method for identifying potential
language transfer effects by using additional linguistic
features such as adaptor grammars and grammatical de-
pendencies to analyse differences in learner language.
This body of work thus shares some similarities with the
present paper, but our focus is on errors rather than on
the distributional differences, and we look at error con-
texts that may not constitute a TSG tree or grammatical
dependency.
Coming from a linguistic perspective, the works in
Jarvis and Crossley (2012) use Linear Discriminant
Analysis for classification of texts by L1, and identify
interesting features by a stepwise feature selection pro-
cess in the course of classification, rather than via the
measurement of their variability across L1s as here.
More recently, several of these NLI techniques have
been adapted and applied to languages other than En-
glish, such as Arabic and Chinese (Malmasi and Dras,
2014a; Malmasi and Dras, 2014b).
6 Conclusion
In this paper, prompted by work on using computa-
tional visualisation techniques to help SLA researchers
form hypotheses about errors and the environments in
which they are made, we have defined a new task for
finding interesting contexts for errors that vary with
the native language of the speaker. We proposed four
models, ranging from one based on simple error-feature
co-occurrence statistics to one based on the maximum
weighted clique on an L1-specific feature association
graph; these all managed to find contexts that were more
strongly associated with L1s than the raw errors alone,
and produced (albeit with many false positives in the
case of the simple model) some error contexts that look
potentially useful for SLA.
This paper is largely intended to prompt more work
on applying NLP techniques to SLA more broadly. As
such, there are many ways in which the work could be
further developed. First, to get rid of obviously incor-
rect cases, the size of the area over which the feature-
feature and feature-error scores are calculated could be
restricted, perhaps to the relevant clause or a certain
window size. Second, it may not be the case that the
ANOVA F-statistic or ?
2
are the best evaluation mea-
sure: in medical work, for example, there is the notion
of clinical significance, which takes effect size into ac-
count and is often more relevant to the practitioner than
statistical significance. Similarly, the current features
may not be the most meaningful. As part of this, an im-
portant step would be to bring in SLA researchers, to as-
sess proposed error contexts and look at what evaluation
measures best relate to this. The role of the present work
would then be to rule out models for producing error
contexts (like L1ASSOC) that produce weaker results in
hypothesis testing: it would thus be complementary to
the visualisation work from which it stems, guiding SLA
researchers away from unproductive areas of the space
of possible hypotheses. And third, the size of the corpus
is (as always) an issue: as these error-annotated corpora
are few and far between, a semi-supervised approach
or one that in some way incorporated unannotated data
would be useful, perhaps using some of the extensive
recent work on error annotation.
References
Immanuel M. Bomze, Marco Budinich, Panos Parda-
los, and Marcello Pelillo. 1999. The Maximum
Clique Problem. In D.-Z. Du and P. M. Pardalos, edi-
tors, Handbook of Combinatorial Optimization (supp.
Vol. A), pages 1?74. Kluwer Academic, Dordrecht,
Netherlands.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proc. of
the COLING/ACL Interactive Presentation Sessions,
pages 77?80, Stroudsburg, PA, USA.
Ted Briscoe, Ben Medlock, and ?istein Andersen. 2010.
Automated Assessment of ESOL Free Text Exami-
nations. Technical Report TR-790, University of
Cambridge, Computer Laboratory.
Mar??a Bel?en Di?ez-Bedmar and Szilvia Papp. 2008. The
use of the English article system by Chinese and Span-
ish learners. Language and Computers, 66(1):147?
176.
Rod Ellis. 2008. The Study of Second Language Acqui-
sition, 2nd edition. Oxford University Press, Oxford,
UK.
Sylviane Granger. 2011. How to Use Foreign and Sec-
ond Language Learner Corpora. In Alison Mackey
and Susan M. Gass, editors, Research Methods in
Second Language Acquisition: A Practical Guide.
Wiley-Blackwell.
Tania Ionin and Silvina Montrul. 2010. The role of L1
transfer in the interpretation of articles with definite
plurals. Language Learning, 60(4):877?925.
Sherri L. Jackson. 2009. Statistics: Plain and Simple.
Wadsworth, Cengage Learning, Belmont, CA, US.
Scott Jarvis and Scott Crossley, editors. 2012. Ap-
proaching Language Transfer Through Text Classi-
fication: Explorations in the Detection-based Ap-
proach. Multilingual Matters, Bristol, UK.
63
Ekaterina Kochmar. 2011. Identification of a writer?s
native language by error analysis. MPhil thesis, Uni-
versity of Cambridge.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Automatically determining an anonymous author?s
native language. In Intelligence and Security In-
formatics, volume 3495 of LNCS, pages 209?217.
Springer-Verlag.
Robert Lado. 1957. Linguistics Across Cultures: Ap-
plied Linguistics for Language Teachers. Univ. of
Michigan Press, Ann Arbor, MI, US.
James P. Lantolf. 2001. Sociocultural Theory and
Second Language Learning. Oxford University Press,
Oxford, UK.
Shervin Malmasi and Mark Dras. 2014a. Arabic Na-
tive Language Identification. In Proceedings of the
Arabic Natural Language Processing Workshop (co-
located with EMNLP 2014), Doha, Qatar, October.
Association for Computational Linguistics.
Shervin Malmasi and Mark Dras. 2014b. Chinese Na-
tive Language Identification. Proceedings of the 14th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
Shervin Malmasi and Mark Dras. 2014c. Language
Transfer Hypotheses with Linear SVM Weights. Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Shervin Malmasi, Sze-Meng Jojo Wong, and Mark Dras.
2013. NLI Shared Task 2013: MQ Submission. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 124?133, Atlanta, Georgia, June. Association
for Computational Linguistics.
Lourdes Ortega. 2009. Understanding Second Lan-
guage Acquisition. Hodder Education, Oxford, UK.
Patric
?
Osterg?ard. 1999. A New Algorithm for the
Maximum-Weight Clique Problem. Electronic Notes
in Discrete Mathematics, 3:153?156, May.
Betsy Parrish. 1987. A New Look at Methodologies in
the Study of Article Acquisition for Learners of ESL.
Language Learning, 37(3):361?384.
Manfred Pienemann. 2005. Cross-linguistic Aspects of
Processability Theory. John Benjamins, Amsterdam,
Netherlands.
Benjamin Swanson and Eugene Charniak. 2012. Native
Language Detection with Tree Substitution Gram-
mars. In Proc. Meeting Assoc. Computat. Linguistics
(ACL), pages 193?197.
Ben Swanson and Eugene Charniak. 2013. Extracting
the native language signal for second language ac-
quisition. In Proc. Conf. North American Assoc. for
Computat. Linguistics: Human Language Technolo-
gies (NAACL-HLT), pages 85?94, Atlanta, Georgia,
June.
Ben Swanson and Eugene Charniak. 2014. Data Driven
Language Transfer Hypotheses. In Proc. Conf. Euro-
pean Assoc. for Computat. Linguistics (EACL), pages
169?173, Gothenburg, Sweden, April.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications (BEA), pages 48?57, Atlanta,
Georgia, June.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
parse structures for native language identification. In
Proc. Conf. Empirical Methods in Natural Language
Processing (EMNLP), pages 1600?1610.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring Adaptor Grammars for Native Lan-
guage Identification. In Proc. Conf. Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
699?709.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automatically
Grading ESOL Texts. In Proc. Meeting Assoc. Com-
putat. Linguistics (ACL), pages 180?189.
Helen Yannakoudakis, Ted Briscoe, and Theodora Alex-
opoulou. 2012. Automating Second Language Ac-
quisition Research: Integrating Information Visualisa-
tion and Machine Learning. In Proc. EACL Workshop
of LINGVIS & UNCLH, pages 35?43.
Richard Young. 1996. Form-Function Relations in Arti-
cles in English Interlanguage. In R. Bayley and D. R.
Preston, editors, Second Language Acquisition and
Linguistic Variation, pages 135?175. John Benjamins,
Amsterdam, The Netherlands.
64
