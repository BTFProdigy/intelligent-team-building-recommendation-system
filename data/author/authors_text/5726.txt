Information Extraction for Question Answering:
Improving Recall Through Syntactic Patterns
Valentin Jijkoun and Maarten de Rijke
Informatics Institute
University of Amsterdam
{jijkoun,mdr}@science.uva.nl
Jori Mur
Information Science
University of Groningen
mur@let.rug.nl
Abstract
We investigate the impact of the precision/recall
trade-off of information extraction on the per-
formance of an offline corpus-based question
answering (QA) system. One of our findings is
that, because of the robust final answer selection
mechanism of the QA system, recall is more im-
portant. We show that the recall of the extrac-
tion component can be improved using syntac-
tic parsing instead of more common surface text
patterns, substantially increasing the number of
factoid questions answered by the QA system.
1 Introduction
Current retrieval systems allow us to locate docu-
ments that might contain the pertinent information,
but most of them leave it to the user to extract the
useful information from a ranked list of documents.
Hence, the (often unwilling) user is left with a rel-
atively large amount of text to consume. There is
a need for tools that reduce the amount of text one
might have to read to obtain the desired informa-
tion. Corpus-based question answering is designed
to take a step closer to information retrieval rather
than document retrieval. The question answering
(QA) task is to find, in a large collection of data,
an answer to a question posed in natural language.
One particular QA strategy that has proved suc-
cessful on large collections uses surface patterns de-
rived from the question to identify answers. For ex-
ample, for questions like When was Gandhi born?,
typical phrases containing the answer are Gandhi
was born in 1869 and Gandhi (1869?1948). These
examples suggest that text patterns such as ?name
was born in birth date? and ?name (birth
year?death year)? formulated as regular ex-
pressions, can be used to select the answer phrase.
Similarly, such lexical or lexico-syntactic pat-
terns can be used to extract specific information on
semantic relations from a corpus offline, before ac-
tual questions are known, and store it in a repository
for quick and easy access. This strategy allows one
to handle some frequent question types: Who is. . . ,
Where is. . . , What is the capital of. . . etc. (Fleis-
chman et al, 2003; Jijkoun et al, 2003).
A great deal of work has addressed the problem
of extracting semantic relations from unstructured
text. Building on this, much recent work in QA
has focused on systems that extract answers from
large bodies of text using simple lexico-syntactic
patterns. These studies indicate two distinct prob-
lems associated with using patterns to extract se-
mantic information from text. First, the patterns
yield only a small subset of the information that may
be present in a text (the recall problem). Second, a
fraction of the information that the patterns yield is
unreliable (the precision problem). The precision of
the extracted information can be improved signif-
icantly by using machine learning methods to filter
out noise (Fleischman et al, 2003). The recall prob-
lem is usually addressed by increasing the amount
of text data for extraction (taking larger collections
(Fleischman et al, 2003)) or by developing more
surface patterns (Soubbotin and Soubbotin, 2002).
Some previous studies indicate that in the setting
of an end-to-end state-of-the-art QA system, with
additional answer finding strategies, sanity check-
ing, and statistical candidate answer re-ranking, re-
call is more of a problem than precision (Bernardi et
al., 2003; Jijkoun et al, 2003): it often seems use-
ful to have more data rather than better data. The
aim of this paper is to address the recall problem
by using extraction methods that are linguistically
more sophisticated than surface pattern matching.
Specifically, we use dependency parsing to extract
syntactic relations between entities in a text, which
are not necessarily adjacent on the surface level. A
small set of hand-built syntactic patterns allows us
to detect relevant semantic information. A com-
parison of the parsing-based approach to a surface-
pattern-based method on a set of TREC questions
about persons shows a substantial improvement in
the amount of the extracted information and num-
ber of correctly answered questions.
In our experiments we tried to understand
whether linguistically involved methods such as
parsing can be beneficial for information extraction,
where rather shallow techniques are traditionally
employed, and whether the abstraction from surface
to syntactic structure of the text does indeed help to
find more information, at the same time avoiding the
time-consuming manual development of increasing
numbers of surface patterns.
The remainder of the paper is organized as fol-
lows. In Section 2 we discuss related work on
extracting semantic information. We describe our
main research questions and experimental setting in
Section 3. Then, in Section 4 we provide details
on the extraction methods used (surface and syntac-
tic). Sections 5 and 6 contain a description of our
experiments and results, and an error analysis, re-
spectively. We conclude in Section 7.
2 Related Work
There is a large body of work on extracting seman-
tic information using lexical patterns. Hearst (1992)
explored the use of lexical patterns for extracting
hyponym relations, with patterns such as ?such as.?
Berland and Charniak (1999) extract ?part-of? rela-
tions. Mann (2002) describes a method for extract-
ing instances from text by means of part-of-speech
patterns involving proper nouns.
The use of lexical patterns to identify answers in
corpus-based QA received lots of attention after a
team taking part in one of the earlier QA Tracks
at TREC showed that the approach was competi-
tive at that stage (Soubbotin and Soubbotin, 2002;
Ravichandran and Hovy, 2002). Different aspects of
pattern-based methods have been investigated since.
E.g., Ravichandran et al (2003) collect surface pat-
terns automatically in an unsupervised fashion us-
ing a collection of trivia question and answer pairs
as seeds. These patterns are then used to generate
and assess answer candidates for a statistical QA
system. Fleischman et al (2003) focus on the preci-
sion of the information extracted using simple part-
of-speech patterns. They describe a machine learn-
ing method for removing noise in the collected data
and showed that the QA system based on this ap-
proach outperforms an earlier state-of-the-art sys-
tem. Similarly, Bernardi et al (2003) combine the
extraction of surface text patterns with WordNet-
based filtering of name-apposition pairs to increase
precision, but found that it hurt recall more than it
helped precision, resulting in fewer questions an-
swered correctly when the extracted information is
deployed for QA.
The application of deeper NLP methods has also
received much attention in the QA community. The
open-domain QA system by LCC (Moldovan et al,
2002) uses predicate-argument relations and lexical
chaining to actually prove that a text snippet pro-
vides an answer to a question. Katz and Lin (2003)
use syntactic dependency parsing to extract rela-
tions between words, and use these relations rather
than individual words to retrieve sentences relevant
to a question. They report a substantial improve-
ment for certain types of questions for which the
usual term-based retrieval performs quite poorly,
but argue that deeper text analysis methods should
be applied with care.
3 Experimental Setting
We set up experiments to address two related issues.
First, we wanted to understand how the usual pre-
cision/recall trade-off shows up in off-line corpus-
based QA, and specifically, whether extracting more
data of lower quality (i.e., favoring recall) gives
a QA system a better performance than extracting
smaller amounts of more accurate data (i.e., favor-
ing precision). Second, we tried to verify the hy-
pothesis that syntactic parsing for information ex-
traction does increase the extraction recall by iden-
tifying relations between entities not adjacent on the
surface layer but connected syntactically.
There are different approaches to the evaluation
of information extraction modules. The usual recall
and precision metrics (e.g., how many of the inter-
esting bits of information were detected, and how
many of the found bits were actually correct) require
either a test corpus previously annotated with the
required information, or manual evaluation (Fleis-
chman et al, 2003). Although intrinsic evaluation
of an IE module is important, we were mainly inter-
ested in measuring the performance of this module
in context, that is, working as a sub-part of a QA
system. We used the number of questions answered
correctly as our main performance indicator.
3.1 QA System
For the experiments described below we used an
open-domain corpus-based QA system QUARTZ
(Jijkoun et al, 2004). The system implements
a multi-stream approach, where several different
strategies are used in parallel to find possible an-
swers to a question. We ran the system turning on
only one stream, Table Lookup, which implements
an off-line strategy for QA.
The Table Lookup stream uses a number of
knowledge bases created by pre-processing a doc-
ument collection. Currently, QUARTZ? knowledge
bases include 14 semi-structured tables containing
various kinds of information: birth dates of persons,
dates of events, geographical locations of different
objects, capitals and currencies of countries, etc. All
this information is extracted from the corpus off-
line, before actual questions are known.
An incoming question is analyzed and assigned
to one of 37 predefined question types. Based on
the question type, the Table Lookup stream identi-
fies knowledge bases where answers to the question
can potentially be found. The stream uses keywords
from the question to identify relevant entries in the
selected knowledge bases and extracts candidate an-
swers. Finally, the QA system reranks and sanity
checks the candidates and selects the final answer.
3.2 Questions and Corpus
To get a clear picture of the impact of using dif-
ferent information extraction methods for the off-
line construction of knowledge bases, similarly to
(Fleischman et al, 2003), we focused only on
questions about persons, taken from the TREC-
8 through TREC 2003 question sets. The ques-
tions we looked at were of two different types:
person identification (e.g., 2301. What composer
wrote ?Die Go?tterda?mmerung??) and person defi-
nition (e.g., 959. Who was Abraham Lincoln?). The
knowledge base relevant for answering questions of
these types is a table with several fields containing
a person name, an information bit about the per-
son (e.g., occupation, position, activities), the con-
fidence value assigned by the extraction modules
to this information bit (based on its frequency and
the reliability of the patterns used for extraction),
and the source document identification. The Table
Lookup finds the entries whose relevant fields best
match the keywords from the question.
We performed our experiments with the 336
TREC questions about persons that are known to
have at least one answer in the collection. The
collection used at TREC 8, 9 and 10 (referred to
as TREC-8 in the rest of the paper) consists of
1,727,783 documents, with 239 of the correspond-
ing questions identified by our system as asking
about persons. The collection used at TREC 2002
and 2003 (AQUAINT) contains 1,033,461 docu-
ments and 97 of the questions for these editions of
TREC are person questions.
4 Extraction of Role Information
In this section we describe the two extraction meth-
ods we used to create knowledge bases containing
information about persons: extraction using surface
text patterns and using syntactic patterns.
Clearly, the performance of an information ex-
traction module depends on the set of language phe-
nomena or patterns covered, but this relation is not
straightforward: having more patterns allows one to
find more information, and thus increases recall, but
it might introduce additional noise that hurts preci-
sion. Since in our experiments we aimed at com-
paring extraction modules based on surface text vs.
syntactic patterns, we tried to keep these two mod-
ules parallel in terms of the phenomena covered.
First, the collections were tagged with a Named
Entity tagger based on TnT (TnT, 2003) and trained
on CoNLL data (CoNLL, 2003). The Named Entity
tagger was used mainly to identify person names as
separate entities. Although the tagging itself was
not perfect, we found it useful for restricting our
surface text patterns.
Below we describe the two extraction methods.
4.1 Extraction with Surface Text Patterns
To extract information about roles, we used the set
of surface patterns originally developed for the QA
system we used at TREC 2003 (Jijkoun et al, 2004).
The patterns are listed in Table 1.
In these patterns, person is a phrase that is
tagged as person by the Named Entity tagger, role
is a word from a list of roles extracted from the
WordNet (all hyponyms of the word ?person,? 15703
entries),1 role-verb is from a manually con-
structed list of ?important? verbs (discovered, in-
vented, etc.; 48 entries), leader is a phrase identify-
ing leadership from a manually created list of lead-
ers (president, minister, etc.; 22 entries). Finally,
superlat is the superlative form of an adjective
and location is a phrase tagged as location by
the Named Entity tagger.
4.2 Extraction with Syntactic Patterns
To use the syntactic structure of sentences for role
information extraction, the collections were parsed
with Minipar (Lin, 1998), a broad coverage depen-
dency parser for English. Minipar is reported to
achieve 88% precision and 80% recall with respect
to dependency relations when evaluated on the SU-
SANNE corpus. We found that it performed well
on the newpaper and newswire texts of our collec-
tions and was fairly robust to fragmented and not
well-formed sentences frequent in this domain. Be-
fore extraction, Minipar?s output was cleaned and
made more compact. For example, we removed
some empty nodes in the dependency parse to re-
solve non-local dependencies. While not loosing
any important information, this made parses easier
to analyse when developing patterns for extraction.
Table 2 lists the patterns that were used to ex-
tract information about persons; we show syntactic
dependencies as arrows from dependents to heads,
with Minipar?s dependency labels above the arrows.
As with the earlier surface patterns, role is one
of the nouns in the list of roles (hyponyms of person
1The list of roles is used to increase precision by filtering
out snippets that may not be about roles; in some of the experi-
ments below, we turn this filtering mechanism off.
Pattern Example
... role, person The British actress, Emma Thompson
... (superlat|first|last)..., person The first man to set foot on the moon, Armstrong
person,... role... Audrey Hepburn, goodwill ambassador for UNICEF.
person,... (superlat|first|last)... Brown, Democrats? first black chairman.
person,... role-verb... Christopher Columbus, who discovered America,
... role person District Attoney Gil Garcetti
role... person The captain of the Titanic Edward John Smith
person,... leader... location Tony Blair, the prime minister of England
location... leader, person The British foreign secretary , Jack Straw
Table 1: Surface patterns.
Pattern Example
Apposition person appo????role a major developer, Joseph Beard
Apposition person appo????role Jerry Lewis, a Republican congressman
Clause person subj????role-verb Bell invented the telephone
Person person person????role Vice President Al Gore
Nominal modifier person nn????role businessman Bill Shockley
Subject person subj????role Alvarado was chancellor from 1983 to 1984
Conjunction person conj????role Fu Wanzhong, director of the Provincial Department of Foreign Trade
(this is a frequent parsing error)
Table 2: Syntactic patterns.
in WordNet), role-verb is one of the ?important
verbs.? The only restriction for person was that it
should contain a proper noun.
When an occurence of a pattern was found in
a parsed sentence, the relation (person; info-
bit) was extracted, where info-bit is a se-
quence of all words below role or role-verb
in the dependency graph (i.e., all dependents along
with their dependents etc.), excluding the per-
son. For example, for the sentence Jane Goodall,
an expert on chimps, says that evidence for so-
phisticated mental performances by apes has be-
come ever more convincing, that matches the pat-
tern person appo????role, the extracted informa-
tion was (Jane Goodall; an expert on chimps).
5 Experiments and Results
We ran both surface pattern and syntactic pattern
extraction modules on the two collections, with a
switch for role filtering. The performance of the Ta-
ble Lookup stream of our QA system was then eval-
uated on the 336 role questions using the answer
patterns provided by the TREC organizers. An early
error analysis showed that many of the incorrect
answers were due to the table lookup process (see
Section 3) rather than the information extraction
method itself: correct answers were in the tables,
but the lookup mechanism failed to find them or
picked up other, irrelevant bits of information. Since
we were interested in evaluating the two extraction
methods rather than the lookup mechanism, we per-
formed another experiment: we reduced the sizes
of the collections to simplify the automatic lookup.
For each TREC question with an answer in the col-
lection, NIST provides a list of documents that are
known to contain an answer to this question. We put
together the document lists for all questions, which
left us with much smaller sub-collections (16.4 MB
for the questions for the TREC-8 collection and 3.2
MB for the AQUAINT collection). Then, we ran the
two extraction modules on these small collections
and evaluated the performance of the QA system on
the resulting tables. All the results reported below
were obtained with these sub-collections. Compari-
son of the extraction modules on the full TREC col-
lections gave very similar relative results.
Table 3 gives the results of the different runs for
the syntactic pattern extraction and the surface pat-
tern extraction on the TREC-8 collection: the num-
ber of correct answers (in the top one and the top
three answer candidates) for the 239 person ques-
tions. The columns labeled Roles+ show the results
for the extraction modules using the list of possible
roles from WordNet (Section 4), and the columns la-
beled Roles ? show the results when the extraction
modules consider any word as possibly denoting a
role. The results of the runs on the AQUAINT col-
lection with 97 questions are shown in Table 4.
The syntactic pattern module without role filter-
ing scored best of all, with more than a third of the
Syntactic patterns Surface patterns
Rank Roles ? Roles + Roles ? Roles +
1 80 (34%) 73 (31%) 59 (25%) 54 (23%)
1?3 90 (38%) 79 (33%) 68 (29%) 59 (25%)
Table 3: Correct answers for the TREC-8 collection
(239 questions).
Syntactic patterns Surface patterns
Rank Roles ? Roles + Roles ? Roles +
1 16 (17%) 14 (14%) 9 (9%) 6 (6%)
1?3 20 (21%) 14 (14%) 11 (11%) 6 (6%)
Table 4: Correct answers for the AQUAINT collec-
tion (97 questions).
questions answered correctly for the TREC-8 col-
lection. Another interesting observation is that in all
experiments the modules based on syntactic patterns
outperformed the surface-text-based extraction.
Furthermore, there is a striking difference be-
tween the results in Table 3 (questions from
TREC 8, 9 and 10) and the results in Table 4
(questions from TREC 2002 and 2003). The ques-
tions from the more recent editions of TREC are
known to be much harder: indeed, the Table Lookup
stream answers only 21% of the questions from
TREC 2002 and 2003, vs. 38% for earlier TRECs.
In all experiments, both for syntactic and surface
patterns, using the list of roles as a filtering mecha-
nism decreases the number of correct answers. Us-
ing lexical information from WordNet improves the
precision of the extraction modules less than it hurts
the recall. Moreover, in the context of our knowl-
edge base lookup mechanism, low precision of the
extracted information does not seem to be an ob-
stacle: the irrelevant information that gets into the
tables is either never asked for or filtered out during
the final sanity check and answer selection stage.
This confirms the conclusions of (Bernardi et al,
2003): in this specific task having more data seems
to be more useful than having better data.
To illustrate the interplay between the precision
and recall of the extraction module and the perfor-
mance of the QA system, Table 5 gives the com-
parison of the different extraction mechanisms (syn-
tactic and surface patterns, using or not using the
list of roles for filtering). The row labelled # facts
shows the size of the created knowledge base, i.e.,
the number of entries of the form (person, info), ex-
tracted by each method. The row labelled Preci-
sion shows the precision of the extracted informa-
tion (i.e., how many entries are correct, according to
a human annotator) estimated by random sampling
and manual evaluation of 1% of the data for each ta-
ble, similar to (Fleischman et al, 2003). The row la-
belled Corr. answers gives the number of questions
correctly answered using the extracted information.
Syntactic patterns Surface patterns
Roles ? Roles + Roles ? Roles +
# facts 29890 9830 28803 6028
Precision 54% 61% 23% 68%
Corr. answers 34% 31% 25% 23%
Table 5: Comparison of the tables built with differ-
ent extraction methods on the TREC-8 collection.
The results in Table 5 indicate that role filtering af-
fects the syntactic and surfaces modules quite dif-
ferently. Filtering seems almost essential for the
surface-pattern-based extraction, as it increases the
precision from 23% to 68%. This confirms the re-
sults of Fleischman et al (2003): shallow methods
may benefit significantly from the post-processing.
On the other hand, the precision improvement for
the syntactic module is modest: from 54% to 61%.
The data from the syntactic module contains
much less noise, although the sizes of the extracted
tables before role filtering are almost the same. Af-
ter filtering, the number of valid entries from the
syntactic module (i.e., the table size multiplied by
the estimated precision) is about 6000. This is sub-
stantially better than the recall of the surface module
(about 4100 valid entries).
6 Error Analysis
In theory, all relatively simple facts extracted by the
surface pattern module should also be extracted by
the syntactic pattern module. Moreover, the syn-
tactic patterns should extract more facts, especially
ones whose structure deviates from the patterns pre-
defined in the surface pattern module, e.g., where
elements adjacent in the syntactic parse tree are far
apart on the surface level. To better understand the
differences between the two extraction approaches
and to verify the conjecture that syntactic parsing
does indeed increase the recall of the extracted in-
formation, we performed a further (manual) error
analysis, identifying questions that were answered
with one extraction method but not with the other.
Tables 6 and 7 gives the breakdown of the per-
formance of the two modules, again in terms of the
questions answered correctly. We show the results
for the 239 questions on the TREC-8 collection; for
the 97 questions on the AQUAINT corpus the rela-
tive scores are similar. As Tables 6 and 7 indicate,
not all questions answered by the surface pattern
module were also answered by the syntactic pattern
module, contrary to our expectations. We took a
closer look at the questions for which the two mod-
ules performed differently.
Syntactic patterns
Su
rfa
ce
pa
tte
rn
s correct incorrect
correct 47 12
incorrect 32 148
Table 6: Performance analysis for the TREC-8 col-
lection with role filtering.
Syntactic patterns
Su
rfa
ce
pa
tte
rn
s correct incorrect
correct 51 17
incorrect 39 132
Table 7: Performance analysis for the TREC-8 col-
lection without role filtering.
6.1 Syntactic Patterns vs. Surface Patterns
There were three types of errors responsible for pro-
ducing an incorrect answer by the syntactic pattern
module for questions correctly answered with sur-
face patterns. The most frequent errors were pars-
ing errors. For 6 out of 12 questions (see Table 6)
the answer was not extracted by the syntactic pat-
tern method, because the sentences containing the
answers were not parsed correctly. The next most
frequent error was caused by the table lookup pro-
cess. For 4 questions out of the 12, the required
information was extracted but simply not selected
from the table as the answer due to a failure of the
lookup algorithm. The remaining errors (2 out of
12) were of a different type: for these 2 cases the
surface pattern extraction did perform better than
the syntactic method. In both cases this was because
of wildcards allowed in the surface patterns. E.g.,
for the sentence . . . aviator Charles Lindbergh mar-
ried Anne Spencer Morrow. . . the syntactic pattern
method extracted only the relation
(Charles Lindbergh; aviator),
whereas the surface pattern method also extracted
(Anne Spencer Morrow; aviator Charles Lindbergh
married),
because of the pattern ?role. . . person? with
role instantiated with aviator and person with
Anne Spencer Morrow. In fact, the extracted in-
formation is not even correct, because Anne is not
an aviator but Lindbergh?s wife. However, due to
the fuzzy nature of the lookup mechanism, this new
entry in the knowledge base allows the QA sys-
tem to answer correctly the question 646. Who was
Charles Lindbergh?s wife?, which is not answered
with the syntactic pattern extraction module.
To summarize, of the 12 questions where the sur-
face patterns outperformed the syntactic patterns
? 6 questions were not answered by the syntactic
method due to parsing errors,
? 4 were not answered because of the table
lookup failure and
? for 2 the surface-based method was more ap-
propriate.
6.2 Surface Patterns vs. Syntactic Patterns
We also took a closer look at the 32 questions for
which the syntactic extraction performed better than
the surface patterns (see Table 6). For the sur-
face pattern extraction module there were also three
types of errors. First, some patterns were miss-
ing, e.g., person role-verb.... The only
difference from one of the actually used patterns
(person,... role-verb...) is that there
is no comma between person and role-verb.
This type of incompleteness of the set of the surface
patterns was the cause for 16 errors out of 32.
The second class of errors was caused by the
Named Entity tagger. E.g., Abraham Lincoln was
always tagged as location, so the name never
matched any of the surface patterns. Out of 32 ques-
tions, 10 were answered incorrectly for this reason.
Finally, for 6 questions out of 32, the syntactic
extraction performed better because the information
could not be captured on the surface level. For ex-
ample, the surface pattern module did not extract
the fact that Oswald killed Kennedy from the sen-
tence . . . when Lee Harvey Oswald allegedly shot
and killed President John F. Kennedy. . . , because
none of the patterns matched. Indeed, Lee Harvey
Oswald and the potentially interesting verb killed
are quite far apart in the text, but there is an imme-
diate relation (subject) on the syntactic level.
It is worth pointing out that there were no lookup
errors for the surface pattern method, even though
it used the exact same lookup mechanism as the
approach based on syntactic patterns (that did ex-
perience various lookup errors, as we have seen).
It seems that the increased recall of the syntactic
pattern approach caused problems by making the
lookup process harder.
To summarize, out of 32 questions answered us-
ing syntactic extraction method but not by the sur-
face pattern approach
? 16 questions would have required extending
the set of surface patterns,
? 10 questions were not answered because of NE
tagging error, and
? 6 questions required syntactic analysis for ex-
traction of the relevant information.
6.3 Adding Patterns?
We briefly return to a problem noted for extrac-
tion based on surface patterns: the absence of cer-
tain surface patterns. The surface pattern person
role-verb... was not added because, we felt,
it would introduce too much noise in the knowledge
base. With dependency parsing this is not an is-
sue as we can require that person is the subject
of role-verb. So in this case the syntactic pat-
tern module has a clear advantage. More generally,
while we believe that extraction methods based on
hand-crafted patterns are necessarily incomplete (in
that they will fail to extract certain relevant facts),
these observations suggest that coping with the in-
completeness is a more serious problem for the sur-
face patterns than for the syntactic ones.
7 Conclusions
We described a set of experiments aimed at com-
paring different information extraction methods in
the context of off-line corpus-based Question An-
swering. Our main finding is that a linguistically
deeper method, based on dependency parsing and a
small number of simple syntactic patterns, allows an
off-line QA system to correctly answer substantially
more questions than a traditional method based on
surface text patterns. Although the syntactic method
showed lower precision of the extracted facts (61%
vs. 68%), in spite of parsing errors the recall was
higher than that of the surface-based method, judg-
ing by the number of correctly answered questions
(31% vs. 23%). Thus, the syntactic analysis can in
fact be considered as another, intensive way of im-
proving the recall of information extraction, in ad-
dition to successfully used extensive ways, such as
developing larger numbers of surface patterns or in-
creasing the size of the collection.
Moreover, we confirmed the claim that for a com-
plex off-line QA system, with statistical as well as
knowledge-intensive sanity checking answer selec-
tion modules, recall of the information extraction
module is more important than precision, and a sim-
ple WordNet-based method for improving precision
does not help QA. In our future work we plan to in-
vestigate the effect of more sophisticated and, prob-
ably, more accurate filtering methods (Fleischman
et al, 2003) on the QA results.
8 Acknowledgements
Valentin Jijkoun and Maarten de Rijke were sup-
ported by a grant from the Netherlands Organiza-
tion for Scientific Research (NWO) under project
number 220-80-001. De Rijke was also sup-
ported by NWO under project numbers 365-20-
005, 612.069.006, 612.000.106, 612.000.207, and
612.066.302.
References
M. Berland and E. Charniak. 1999. Finding parts in
very large corpora. In Proceedings of the 37th Annual
Meeting of the ACL.
R. Bernardi, V. Jijkoun, G. Mishne, and M. de Rijke.
2003. Selectively using linguistic resources through-
out the question answering pipeline. In Proceedings
of the 2nd CoLogNET-ElsNET Symposium.
M. Fleischman, E. Hovy, and A. Echihabi. 2003. Offline
strategies for online question answering: answering
questions before they are asked. In Proceedings of the
41st Annual Meeting of the ACL.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th
International Conference on Computational Linguis-
tics (COLING-92).
V. Jijkoun, G. Mishne, and M. de Rijke. 2003. Prepro-
cessing Documents to Answer Dutch Questions. In
Proceedings of the 15th Belgian-Dutch Conference on
Artificial Intelligence (BNAIC?03).
V. Jijkoun, G. Mishne, C. Monz, M. de Rijke,
S. Schlobach, and O. Tsur. 2004. The University of
Amsterdam at the TREC 2003 Question Answering
Track. In Proceedings of the TREC-2003 Conference.
B. Katz and J. Lin. 2003. Selectively using relations to
improve precision in question answering. In Proceed-
ings of the EACL-2003 Workshop on Natural Lan-
guage Processing for Question Answering.
D. Lin. 1998. Dependency-based evaluation of Minipar.
In Proceedings of the Workshop on the Evaluation of
Parsing Systems.
G. Mann. 2002. Fine-grained proper noun ontologies
for question answering. In SemaNet?02: Building and
Using Semantic Networks.
D. Moldovan, S. Harabagiu, R. Girju, P. Morarescu,
A. Novischi F. Lacatusu, A. Badulescu, and O. Bolo-
han. 2002. LCC tools for question answering. In Pro-
ceedings of the TREC-2002.
TnT Statistical Part of Speech Tagging. 2003.
URL: http://www.coli.uni-sb.de/
?thorsten/tnt/.
CoNLL: Conference on Natural Language Learn-
ing. 2003. URL: http://cnts.uia.ac.be/
signll/shared.html.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In Pro-
ceedings of the 40th Annual Meeting of the ACL.
D. Ravichandran, A. Ittycheriah, and S. Roukos. 2003.
Automatic derivation of surface text patterns for a
maximum entropy based question answering system.
In Proceedings of the HLT-NAACL Conference.
M.M. Soubbotin and S.M. Soubbotin. 2002. Use of pat-
terns for detection of likely answer strings: A system-
atic approach. In Proceedings of the TREC-2002 Con-
ference.
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 17?25
Manchester, UK. August 2008
Simple is Best: Experiments with Different Document Segmentation
Strategies for Passage Retrieval
Jo?rg Tiedemann
Information Science
University of Groningen
j.tiedemann@rug.nl
Jori Mur
Information Science
University of Groningen
j.mur@rug.nl
Abstract
Passage retrieval is used in QA to fil-
ter large document collections in order
to find text units relevant for answering
given questions. In our QA system we ap-
ply standard IR techniques and index-time
passaging in the retrieval component. In
this paper we investigate several ways of
dividing documents into passages. In par-
ticular we look at semantically motivated
approaches (using coreference chains and
discourse clues) compared with simple
window-based techniques. We evaluate
retrieval performance and the overall QA
performance in order to study the impact
of the different segmentation approaches.
From our experiments we can conclude
that the simple techniques using fixed-
sized windows clearly outperform the se-
mantically motivated approaches, which
indicates that uniformity in size seems to
be more important than semantic coher-
ence in our setup.
1 Introduction
Passage retrieval in question answering is differ-
ent from information retrieval in general. Extract-
ing relevant passages from large document col-
lections is only one step in answering a natural
language question. There are two main differ-
ences: i) Passage retrieval queries are generated
from complete sentences (questions) compared to
bag-of-keyword queries usually used in IR. ii) Re-
trieved passages have to be processed further in or-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
der to extract concrete answers to the given ques-
tion. Hence, the size of the passages retrieved is
important and smaller units are preferred. Here,
the division of documents into passages is crucial.
The textual units have to be big enough to en-
sure IR works properly and they have to be small
enough to enable efficient and accurate QA. In this
study we investigate whether semantically moti-
vated passages in the retrieval component lead to
better QA performance compared to the use of
document retrieval and window-based segmenta-
tion approaches.
1.1 Index-time versus Search-time Passaging
In this paper, we experiment with various possi-
bilities of dividing documents into passages before
indexing them. This is also called index-time pas-
saging and refers to a one-step process of retriev-
ing appropriate textual units for subsequent an-
swer extraction modules (Roberts and Gaizauskas,
2004; Greenwood, 2004). This is in contrast to
other strategies using a two-step procedure consist-
ing of document retrieval and search-time passag-
ing thereafter. Here, we can distinguish between
approaches that only return one passage per rel-
evant document (see, for example, (Robertson et
al., 1992)) and the ones that allow multiple pas-
sages per document (see, for example (Moldovan
et al, 2000)). In general, allowing multiple pas-
sages per document is preferable for QA as possi-
ble answers can be contained at various positions
in a document (Roberts and Gaizauskas, 2004).
For this, an index-time approach has the advan-
tage that the retrieval of multiple passages per doc-
uments is straightforward because all of them com-
pete which each other in the same index using the
same metric for ranking.
A comparison between index-time and search-
17
time passaging has been carried out in (Roberts
and Gaizauskas, 2004). In their experiments,
index-time passaging performs similarly to search-
time passaging in terms of coverage and redun-
dancy (measures which have been introduced in
the same paper; see section 4.2 for more informa-
tion). Significant differences between the various
approaches can only be observed in redundancy on
higher ranks (above 50). However, as we will see
later in our experiments (section 4.2), redundancy
is not as important as coverage for our QA system
. Furthermore, retrieving more than about 40 pas-
sages does not produce significant improvements
of the QA system anymore but slows down the pro-
cessing time substantially.
Another argument for our focus on a one-step
retrieval procedure can be taken from (Tellex et al,
2003). In this paper, the authors do not actually use
any index-time passaging approach but compare
various search-time passage retrieval algorithms.
However, they obtain a huge performance differ-
ence when applying an oracle document retriever
(only returning relevant documents in the first re-
trieval step) instead of a standard IR engine. Com-
pared to this, the differences between the various
passage retrieval approaches tested is very small.
From this we can conclude that much improve-
ment can be gained by improving the initial re-
trieval step, which seems to be the bottleneck in the
entire process. Unfortunately, the authors do not
compare their results with index-time approaches.
However, looking at the potential gain in document
retrieval and keeping in mind that the performance
of index-time and search-time approaches is rather
similar (as we have discussed earlier) we believe
that the index-time approach is preferable.
1.2 Passages in IR
Certainly, IR performance is effected by chang-
ing the size of the units to be indexed. The task
in document segmentation for our index-time pas-
saging approach is to find the proper division of
documents into text passages which optimize the
retrieval in terms of overall QA performance.
The general advantages of passage retrieval over
full-text document retrieval has been investigated
in various studies, e.g., (Kaszkiel and Zobel, 2001;
Callan, 1994; Hearst and Plaunt, 1993; Kaszkiel
and Zobel, 1997). Besides the argument of de-
creasing the search space for subsequent answer
extraction modules in QA, passage retrieval also
improves standard IR techniques by ?normaliz-
ing? textual units in terms of size which is espe-
cially important in cases where documents come
from very diverse sources. IR is based on similar-
ity measures between documents and queries and
standard approaches have shortcomings when ap-
plying them to documents of various sizes and text
types. Often there is a bias for certain types raising
problems of discrimination between documents of
different lengths and content densities. Passages
on the other hand provide convenient units to be
returned to the user avoiding such ranking difficul-
ties (Kaszkiel and Zobel, 2001). For IR, passage-
level evidence may be incorporated into document
retrieval (Callan, 1994; Hearst and Plaunt, 1993)
or passages may be used directly as retrieval unit
(Kaszkiel and Zobel, 2001; Kaszkiel and Zobel,
1997). For QA only the latter is interesting and
will be applied in our experiments.
Passages can be defined in various ways. The
most obvious way is to use existing markup (ex-
plicit discourse information) to divide documents
into smaller units. Unfortunately, such markup is
not always available or ambiguous with other types
of separators. For example, headers, list elements
or table cells might be separated in the same way
(for example using an empty line) as discourse
related paragraphs. Also, the division into para-
graphs may differ a lot depending on the source
of the document. For example, Wikipedia entries
are divided on various levels into rather small units
whereas newspaper articles often include very long
paragraphs.
There are several ways of automatically divid-
ing documents into passages without relying on
existing markup. One way is to search for linguis-
tic clues that indicate a separation of consecutive
text blocks. These clues may include lexical pat-
terns and relations. We refer to such approaches
as semantically motivated document segmentation.
Another approach is to cut documents into arbi-
trary pieces ignoring any other type of informa-
tion. For example, we can use fixed-sized win-
dows to divide documents into passages of simi-
lar size. Such windows can be defined in terms of
words and characters (Kaszkiel and Zobel, 2001;
Monz, 2003) or sentences and paragraphs (Zobel
et al, 1995; Llopis et al, 2002). It is also possi-
ble to allow varying window sizes and overlapping
sections to be indexed (Kaszkiel and Zobel, 2001;
Monz, 2003). In this case it is up to the IR engine
18
to decide which of the competing window types is
preferred and it may even return overlapping sec-
tions multiple times.
In the following sections we will discuss
two techniques of semantically motivated docu-
ment segmentation and compare them to simple
window-based techniques in terms of passage re-
trieval and QA performance.
2 Passage Retrieval in our QA system
Our QA system is an open-domain question an-
swering system for Dutch. It includes two
strategies: (1) A table-lookup strategy using fact
databases that have been created off-line, and, (2)
an ?on-line? answer extraction strategy with pas-
sage retrieval and subsequent answer identification
and ranking modules. We will only look at the
second strategy as we are interested in the passage
retrieval component and its impact on QA perfor-
mance.
The passage retrieval component is imple-
mented as an interface to several open-source IR
engines. The query is generated from the given
natural language question after question analysis.
Keywords are sent to the IR engine(s) and results
(in form of sentence IDs) are returned to the QA
system.
In the experiments described here, we apply
Zettair (Lester et al, 2006), an open-source IR en-
gine developed by the search engine group at the
RMIT University in Melbourne, Australia. It im-
plements a very efficient standard IR engine with
high retrieval performance according to our exper-
iments with various alternative systems. Zettair is
optimized for speed and is very efficient in both
indexing and retrieval. The outstanding speed in
indexing is very fortunate for our experiments in
which we had to create various indexes with dif-
ferent document segmentation strategies.
3 Document Segmentation
We now discuss the different methods for docu-
ment segmentation, starting with the semantically
motivated ones and then looking at the window-
based techniques.
3.1 Using Coreference Chains
Coreference is the relation which holds between
two NPs both of which are interpreted as refer-
ring to the same unique referent in the context
in which they occur ((Van Deemter and Kibble,
2000)). Since the coreference relation is an equiv-
alence relation and consequently a transitive rela-
tion chains of coreferring entities can be detected
in arbitrary documents. We can use these coref-
erence chains to demarcate passages in the text.
The assumption in this approach is that corefer-
ence chains mark semantically coherent passages,
which are good candidates for splitting up docu-
ments.
Figure 1 illustrates chains detected by a resolu-
tion system in five successive sentences.
1. [Jim McClements en Susan Sandvig-Shobe]
i
hebben
een onrechtmatig argument gebruikt.
2. [De Nederlandse scheidsrechter]
j
[Jacques de Koning]
j
bevestigt dit.
3. [Kuipers]
k
versloeg zondag in een rechtstreeks duel
[Shani Davis]
m
.
4. Toch werd [hij]
k
in de rangschikking achter [de
Amerikaan]
m
geklasseerd.
5. [De twee hoofdarbiters]
i
verklaarden dat [Kuipers?]
k
voorste schaats niet op de grond stond.
Cluster i (1,5): [Jim McClements en Susan Sandvig-Shobe]
[De twee hoofdarbiters]
Cluster j (2): [De Nederlandse scheidsrechter]
[Jacques de Koning]
Cluster k (3-5): [Kuipers] [hij] [Kuipers?]
Cluster m (3,4): [Shani Davis] [de Amerikaan]
Figure 1: Example of coreference chains used for
document segmentation
The coreferential units can then be used to form
passages consisting of all sentences the corefer-
ence chain spans over, i.e. the boundaries of pas-
sages are sentences containing the first occurrence
of the referent and the last occurrence of a refer-
ent. Thus, in the example in figure 1 we obtain
four passages: 1) sentence one to sentence five, 2)
sentence two, 3) sentence three to five, and, 4) sen-
tence three and four. Note that such passages can
be included in others and may overlap with yet oth-
ers. Furthermore, there might be sentences which
are not included in any chain which have to be han-
dled by some other techniques.
For our purposes we used our own coreference
resolution system which is based on information
derived from Alpino, a wide-coverage dependency
parser for Dutch (van Noord, 2006). We ap-
proached the task of coreference resolution as a
19
clustering-based ranking task. Some NP pairs are
more likely to be coreferent than others. The sys-
tem ranks possible antecedents for each anaphor
considering syntactic features, semantic features
and surface structure features from the anaphor
and the candidate itself, as well as features from
the cluster to which the candidate belongs. It picks
the most likely candidate as the coreferring an-
tecedent.
References relations are detected between pro-
nouns, common nouns and named entities. The
resolution system yields a precision of 67.9% and
a recall of 45.6% (F-score = 54.5%) using MUC
scores (Vilain et al, 1993) on the annotated test
corpus developed by (Hoste, 2005) which consist
of articles taken from KNACK, a Flemish weekly
news magazine.
3.2 TextTiling
TextTiling is a well-known algorithm for segment-
ing texts into subtopic passages (Hearst, 1997).
It is based on the assumption that a significant
portion of a set of lexical items in use during
the course of a given subtopic discussion changes
when that subtopic in the text changes.
Topic shifts are found by searching for lexi-
cal co-occurrence patterns and comparing adja-
cent blocks. First the text is subdivided into
pseudo-sentences of a predefined size rather than
using syntactically-determined sentences. These
pseudo-sentences are called token-sequences by
Hearst.
The algorithm identifies discourse boundaries
by calculating a score for each token-sequence
gap. This score is based on two methods,
block comparison and vocabulary introduction.
The block comparison method compares adjacent
blocks of text to see how similar they are accord-
ing to how many words the adjacent blocks have
in common. The vocabulary introduction method
is based on how many new words were seen in
the interval in which the token-sequence gap is the
midpoint.
The boundaries are assumed to occur at the
largest valleys in the graph that results from plot-
ting the token-sequences against their scores. In
this way the algorithm produces a flat subtopic
structure from a given document.
3.3 Window-based
The simplest way of dividing documents into pas-
sages is to use a fixed-sized window. Here we
do not take any discourse information nor seman-
tic clue into account but split documents at arbi-
trary positions. Windows can be defined in various
ways, in terms of characters, words or sentences.
In our case it is important to keep sentences to-
gether because of the answer extraction compo-
nent in our QA system that works on that level
and expects complete sentences. Window-based
segmentation techniques may be applied with var-
ious amounts of overlaps. The simplest method is
to split documents into passages in a greedy way,
starting a new passage immediately after the pre-
vious one (and starting the entire process at the be-
ginning of each document)1. Another method is to
allow some overlap between consecutive passages,
i.e. starting a new passage at some position within
the previous one. If we use the maximum possible
overlap such an approach is usually called a ?slid-
ing window? in which the difference between two
consecutive passages is only two basic units (sen-
tences) - the first and the last one.
4 Experiments
4.1 Setup
For our experiments we applied the Dutch news-
paper corpus used at the QA track at CLEF, the
cross-language evaluation forum. It contains about
190,000 documents consisting of about 4,000,000
sentences (roughly 80 million words). As men-
tioned earlier, we applied the open-source IR en-
gine, Zettair, in our experiments and used a lan-
guage modeling metric with Dirichlet smoothing,
which is implemented in the system.
The evaluation is based on 778 Dutch CLEF
questions from the QA tracks in the years 2003 ?
2005 which are annotated with their answers. We
use simple matching of possible answer strings to
determine if a passage is relevant for finding an
accepted answer or not. Similarly, answer string
matching is applied to evaluate the output of the
entire QA system; i.e. an answer by the system
is counted as correct if it is identical to one of the
accepted answer strings without looking at the sup-
porting sentence/passage. For evaluation we used
the standard measure of MRR which is defined as
follows:
1Note that in our approach we still keep the document
boundaries intact, i.e. the segmentation ends at the end of
each document and starts from scratch at the beginning of the
next document. In this way, the last passage in a document
may be smaller than the pre-defined fixed size.
20
MRR
QA
=
1
N
N
?
1
1
rank(first correct answer)
Using the string matching strategy for evalu-
ation this corresponds to the lenient MRR mea-
sures frequently used in the literature. Strict MRR
scores (requiring a match with supporting docu-
ments) is less appropriate for our data coming from
the CLEF QA tracks. In CLEF there are usually
only a few participants and, therefore, only a small
fraction of relevant documents are known for the
given questions.
4.2 Evaluation of Passage Retrieval
There are various metrics that can be employed for
evaluating passage retrieval. Commonly it is ar-
gued that passage retrieval for QA is merely a fil-
tering task and ranking (precision) is less impor-
tant than recall. Therefore, the measure of redun-
dancy has been introduced which is defined as the
average number of relevant passages retrieved per
question (independent of any ranking). Passage re-
trieval is, of course, a bottleneck in QA systems
that make use of such a component. The system
has no chance to find an answer if the retrieval en-
gine fails to return relevant passages. Therefore,
another measure, coverage is often used in combi-
nation with redundancy. It is defined as the pro-
portion of questions for which at least one relevant
passage is found. In order to validate the use of
these measures in our setup we experimented with
retrieving various amounts of paragraphs. Figure 2
illustrates the relation of coverage and redundancy
scores compared to the overall QA performance
measured in terms of MRR scores.
From the figure we can conclude that cover-
age is more important than redundancy in our sys-
tem. In other words, our QA system is quite good
in finding appropriate answers if there is at least
one relevant passage in the set of retrieved ones.
Redundancy on the other hand does not seem to
provide valuable insides for the end-to-end perfor-
mance of our QA system.
However, our system also uses the passage re-
trieval score (and, hence, the ranking) as a clue
for answer extraction. Therefore, other standard
IR measures might be interesting for our investi-
gations as well. The following three metrics are
common in the IR literature.
 0
 20
 40
 60
 80
 100
 0  20  40  60  80  100
co
ve
ra
ge
/M
RR
 (in
 %
)
number of paragraphs retrieved
IR coverage
re
du
nd
an
cy
2.5
5
7.5
10
IR redundancy       QA MRR
Figure 2: The correlation between coverage and
redundancy and MRR
QA
with varying numbers
of paragraphs retrieved. Note that redundancy and
coverage use different scales on the y-axis which
makes them not directly comparable. The inten-
tion of this plot is to illustrate the tendency of both
measures in comparison with QA performance.
Mean average precision (MAP): Average of
precision scores for top k documents; MAP
is the mean of these averages over all the N
queries.
MAP =
1
N
N
?
n=1
1
K
K
?
k=1
P
n
(1..k)
(P
n
(1..k) is the precision of the top k docu-
ments retrieved for query q
n
)
Uninterpolated average precision (UAP):
Average of precision scores at each relevant
document retrieved; UAP is the mean of
these averages over the N queries.
UAP =
1
N
N
?
n=1
1
|D
n
r
|
?
k:d
k
?D
n
r
P
n
(1..k)
(Dn
r
is the set of relevant documents among
the ones retrieved for question n)
Mean reciprocal ranks: The mean of the recip-
rocal rank of the first relevant passage re-
trieved.
MRR
IR
=
1
N
N
?
1
1
rank(first relevant passage)
In figure 3 the correlation of these measures with
the overall QA performance is illustrated.
21
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 0  20  40  60  80  100
M
R
R
/U
AP
/M
AP
 (in
 %
)
number of paragraphs retrieved
  IR MRR
  IR UAP
  IR MAP QA MRR
Figure 3: The correlation between IR evaluation
measures (MAP , UAP and MRR
IR
) and QA
evaluation scores (MRR
QA
) with varying num-
bers of paragraphs retrieved.
From the picture we can clearly see that the
MRR
IR
scores correlate the most with the QA
evaluation scores when retrieving different num-
bers of paragraphs. This, again, confirms the im-
portance of coverage as the MRR
IR
score only
takes the first relevant passage into account and ig-
nores the fact that there might be more answers
to be found in lower ranked passages. Hence,
MRR
IR
seems to be a good measure that com-
bines coverage with an evaluation of the rank-
ing and, therefore, we will use it as our main IR
evaluation metric instead of coverage, redundancy,
MAP & UAP.
4.3 Baselines
The CLEF newspaper corpus comes with para-
graph markup which can easily be used as the seg-
mentation granularity for passage retrieval. Table
1 shows the scores obtained by different baseline
retrieval approaches using either sentences, para-
graphs or documents as base units.
We can see from the results that document re-
trieval (used for QA) is clearly outperformed by
both sentence and paragraph retrieval. Surpris-
ingly, sentence retrieval works even better than
paragraph retrieval when looking at the QA per-
formance even though all IR evaluation measures
(cov, red, MRR
IR
) suggest a lower score. Note
that MRR
IR
is almost as good as MRR
QA
for
sentence retrieval whereas the difference between
them is quite large for the other settings. This indi-
cates the importance of narrowing down the search
space for the answer extraction modules. The
MRR
#sent cov red IR QA CLEF
sent 16,737 0.784 2.95 0.490 0.487 0.430
par 80,046 0.842 4.17 0.565 0.483 0.416
doc 618,865 0.877 6.13 0.666 0.457 0.387
Table 1: Baselines with sentence (sent), paragraph
(par) and document (doc) retrieval (20 units).
MRR
QA
is measured on the top 5 answers re-
trieved. CLEF is the accuracy of the QA system
measured on the top answer provided by the sys-
tem. cov refers to coverage and red refers to redun-
dancy. #sent gives the total number of sentences
included in the retrieved text units to give an im-
pression about the amount of text to be processed
by subsequent answer extraction modules.
amount of data to be processed is much smaller
for sentence retrieval than for the other two while
coverage is still reasonably high. The CLEF scores
(accuracy measured on the top answer provided by
the system) follow the same pattern. Here, the dif-
ference between sentence retrieval and document
retrieval is even more apparent.
Certainly, the success of the retrieval compo-
nent depends on the metric used for ranking doc-
uments as implemented in the IR engine. In or-
der to verify the importance of document seg-
mentation in a QA setting we also ran experi-
ments with another standard metric implemented
in Zettair, the Okapi BM-25 metric (Robertson et
al., 1992). Similar to the previous setting using
the LM metric, QA with paragraph retrieval (now
yielding MRR
QA
= 0.460) outperforms QA with
document retrieval (MRR
QA
= 0.449). How-
ever, sentence retrieval does not perform as well
(MRR
QA
= 0.420) which suggests that the Okapi
metric is not suited for very small retrieval units.
Still, the success of paragraph retrieval supports
the advantage of passage retrieval compared to
document retrieval and suggests potential QA per-
formance gains with improved document segmen-
tation strategies. In the remaining we only report
results using the LM metric for retrieval due to its
superior performance.
4.4 Semantically Motivated Passages
As described earlier, coreference chains can be
used to extract semantically coherent passages
from textual documents. In our experiments we
used several settings for the integration of such
passages in the retrieval engine. First of all, coref-
22
erence chains have been used as the only way
of forming passages. Sentences which are not
included in any passage are included as single-
sentence passages. This settings is referred to as
sent/coref.
In the second setting we restrict the passages in
length. Coreference chains can be arbitrary long
and, as we can see in the results in table 2, the
IR engine tends to prefer long passages which is
not desirable in the QA setting. Hence, we define
the constraint that passages have to be longer than
200 characters and shorter than 1000. This setup is
referred to as sent/coref (200-1000).
In the third setting we combine paragraphs (us-
ing existing markup) and coreference chain pas-
sages including the length restriction. This is
mainly to get rid of the single-sentence passages
included in the previous settings. Note that all
paragraphs are used even if all sentences within
them are included in coreferential passages. Note
also that in all settings passages may refer to
overlapping text units as coreference chains may
stretch over various overlapping passages of a doc-
ument.
We did not perform an exhaustive optimization
of the length restriction. However, we experi-
mented with various settings and 200-1000 was the
best performing one in our experiments. For illus-
tration we include one additional experiment using
a slightly different length constraint (200-400) in
table 2.
For the document segmentation strategy us-
ing TextTiling we used a freely available im-
plementation of that algorithm (the Perl Module
Lingua::EN::Segmenter::TextTiling available
at CPAN). Note that we do not include other pas-
sages in this approach (paragraphs using existing
markup nor single-sentence passages).
Table 2 summarizes the scores obtained by the
various settings when applied for passage retrieval
and when embedded into the QA system.
It is worth noting that including coreferential
chains without length restriction forced the re-
trieval engine to return a lot of very long passages
which resulted in a degraded QA performance
(also in terms of processing time which is not
shown here). The combination of paragraphs and
coreferential passages with length restrictions pro-
duced MRR
QA
scores above the baseline. How-
ever, these improvements are not statistically sig-
nificant according to the Wilcoxon matched-pair
MRR
#sent IR QA CLEF
sent/coref 490,968 0.604 0.469 0.405
sent/coref (200-1000) 76,865 0.535 0.462 0.395
par+coref (200-1000) 82,378 0.560 0.493 0.426
par+coref (200-400) 67,580 0.555 0.489 0.422
TextTiling 107,879 0.586 ? 0.503 0.434
Table 2: Passage retrieval with document segmen-
tation using coreference chains and TextTiling (re-
trieving a maximum of 20 passages; ? means sig-
nificant with p < 0.05 and Wilcoxon Matched-pair
Signed-Ranks Test compared to paragraph base-
line ? only tested for MRR
QA
)
signed-ranks test and looking at the corresponding
CLEF scores we can even see a slight drop in per-
formance. Applying TextTiling yielded improved
scores in both passage retrieval and QA perfor-
mance (MRR
QA
and CLEF). The MRR
QA
im-
provement is statistically significant according to
the same test.
4.5 Window-based Passages
In comparison to the semantically motivated pas-
sages discussed above we also looked at simple
window-based passages as described earlier. Here
we do not consider any linguistic clues for divid-
ing the documents besides the sentence and docu-
ment boundaries. Table 3 summarizes the results
obtained for various fixed-sized windows used for
document segmentation.
MRR
#sent IR QA CLEF
2 sentences 33468 0.545 ? 0.506 0.443
3 sentences 50190 0.554 0.504 0.436
4 sentences 66800 0.581 ? 0.512 0.447
5 sentences 83575 0.588 0.493 0.422
6 sentences 100110 0.583 0.489 0.423
7 sentences 116872 0.572 0.491 0.422
8 sentences 133504 0.577 0.481 0.409
9 sentences 150156 0.578 0.475 0.405
10 sentences 166810 0.596 0.470 0.396
Table 3: Passage retrieval with window-based doc-
ument segmentation (? means significant with
p < 0.05 and Wilcoxon Matched-pair Signed-
Ranks Test)
Surprisingly, we can see that window-based seg-
mentation approaches with small sizes between 2
and 7 sentences yield improved scores compared
to the baseline. Two of the improvements (using
2-sentence passages and 4-sentence passages) are
statistically significant. Three settings also out-
23
perform the best semantically motivated segmen-
tation approach. This result was unexpected espe-
cially considering the naive way of splitting docu-
ments into parts disregarding any discourse struc-
ture (besides document boundaries) and other se-
mantic clues.
We did another experiment using window-based
segmentation and a sliding window approach.
Here, fixed-sized passages are included starting at
every point in the document and, hence, various
overlapping passages are included in the index. In
this way we split documents at various points and
leave it to the IR engine to select the most ap-
propriate ones for a given query. The results are
shown in table 4.
MRR
#sent IR QA CLEF
2 sent (sliding) 29095 0.548 ? 0.516 0.456
3 sent (sliding) 36415 0.549 0.484 0.411
4 sent (sliding) 41565 0.546 0.476 0.409
5 sent (sliding) 45737 0.534 0.465 0.403
6 sent (sliding) 49091 0.528 0.454 0.390
7 sent (sliding) 51823 0.529 0.439 0.372
8 sent (sliding) 54600 0.535 0.428 0.360
9 sent (sliding) 57071 0.531 0.420 0.351
10 sent (sliding) 59352 0.542 0.420 0.354
Table 4: Passage retrieval with window-based doc-
ument segmentation and a sliding window
Again, we see a significant improvement with
2-sentence passages (the overall best score so far)
but the performance degrades when increasing the
window size. Note that the number of sentences re-
trieved is growing very slowly for larger windows.
This is because more and more of the overlapping
regions are retrieved and, hence, the total number
of unique sentences does not grow with the size
of the window as we have seen in the non-sliding
approach.
5 Discussion & Conclusions
Our experiments show that passage retrieval is in-
deed different to general document retrieval. Im-
proved retrieval scores do not necessarily lead to
better QA performance. Important for QA is to
reduce the search space for subsequent answer ex-
traction modules and, hence, passage retrieval has
to balance retrieval accuracy and retrieval size. In
our setup it seems to be preferable to return very
small units with a reasonable coverage. For this,
index-time passaging is very effective.
In this study we were especially interested in se-
mantically motivated approaches to document seg-
mentation. In particular, two techniques have been
investigated, one using the well-known TextTil-
ing algorithm and one using coreference chains for
passage boundary detection. We compared them
to simple window-based techniques using various
sizes. From our experiments we can conclude that
simple document segmentation techniques using
small fixed-sized windows work best among the
ones tested here. Semantically motivated passages
in the retrieval component helped to slightly im-
prove QA performance but do not justify the effort
spent in producing them. One of the main reasons
for the failure of using coreference chains for seg-
mentation might be the fact that this approach pro-
duces many overlapping passages which does not
seem to be favorable for passage retrieval. This can
also be seen in the sliding window approach which
did not perform as well as the one without over-
lapping units (except for two-sentence passages).
In conclusion, uniformity in terms of length and
uniqueness (in terms of non-overlapping contents)
seem to be more important than semantic coher-
ence for one-step passage retrieval in QA. A fu-
ture direction could be to test an approach that bal-
ances both a uniform document segmentation and
semantic coherence.
References
Callan, James P. 1994. Passage-level evidence in doc-
ument retrieval. In SIGIR ?94: Proceedings of the
17th annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 302?310, New York, NY, USA. Springer-
Verlag New York, Inc.
Greenwood, Mark A. 2004. Using pertainyms to im-
prove passage retrieval for questions requesting in-
formation about a location. In Proceedings of the
Workshop on Information Retrieval for Question An-
swering (SIGIR 2004), Sheffield, UK.
Hearst, Marti A. and Christian Plaunt. 1993. Subtopic
structuring for full-length document access. In Re-
search and Development in Information Retrieval,
pages 59?68.
Hearst, Marti A. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Hoste, V. 2005. Optimization Issues in Machine Learn-
ing of Coreference Resolution. Ph.D. thesis, Univer-
sity of Antwerp.
Kaszkiel, Marcin and Justin Zobel. 1997. Passage re-
trieval revisited. In SIGIR ?97: Proceedings of the
20th annual international ACM SIGIR conference on
24
Research and development in information retrieval,
pages 178?185, New York, NY, USA. ACM Press.
Kaszkiel, Marcin and Justin Zobel. 2001. Ef-
fective ranking with arbitrary passages. Journal
of the American Society of Information Science,
52(4):344?364.
Lester, Nicholas, Hugh Williams, Justin Zobel, Falk
Scholer, Dirk Bahle, John Yiannis, Bodo von
Billerbeck, Steven Garcia, and William Web-
ber. 2006. The Zettair search engine.
http://www.seg.rmit.edu.au/zettair/.
Llopis, F., J. Vicedo, and A. Ferra?ndez. 2002. Pas-
sage selection to improve question answering. In
Proceedings of the COLING 2002 Workshop on Mul-
tilingual Summarization and Question Answering.
Moldovan, D., S. Harabagiu, M. Pasca, R. Mihalcea,
R. Girju, R. Goodrum, and V. Rus. 2000. The struc-
ture and performance of an open-domain question
answering system.
Monz, Christof. 2003. From Document Retrieval to
Question Answering. Ph.D. thesis, University of
Amsterdam.
Roberts, Ian and Robert Gaizauskas. 2004. Evaluating
passage retrieval approaches for question answering.
In Proceedings of 26th European Conference on In-
formation Retrieval.
Robertson, Stephen E., Steve Walker, Micheline
Hancock-Beaulieu, Aarron Gull, and Marianna Lau.
1992. Okapi at TREC-3. In Text REtrieval Confer-
ence, pages 21?30.
Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton.
2003. Quantitative evaluation of passage retrieval al-
gorithms for question answering. In Proceedings of
the SIGIR conference on Research and development
in informaion retrieval, pages 41?47. ACM Press.
Van Deemter, K. and R. Kibble. 2000. On coreferring:
Coreference in muc and related annotation schemes.
Computational Linguistics, 26(4):629?637.
van Noord, Gertjan. 2006. At Last Parsing Is Now
Operational. In TALN 2006 Verbum Ex Machina,
Actes De La 13e Conference sur Le Traitement
Automatique des Langues naturelles, pages 20?42,
Leuven.
Vilain, M., J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1993. A model-theoretic coreference
scoring scheme. In Proceedings of the 6th confer-
ence on Message understanding (MUC 6), pages 45?
52.
Zobel, Justin, Alistair Moffat, Ross Wilkinson, and Ron
Sacks-Davis. 1995. Efficient retrieval of partial doc-
uments. Information Processing and Management,
31(3):361?377.
25
