Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 487?494,
New York, June 2006. c?2006 Association for Computational Linguistics
Unlimited vocabulary speech recognition for agglutinative languages
Mikko Kurimo1, Antti Puurula1, Ebru Arisoy2, Vesa Siivola1,
Teemu Hirsim?ki1, Janne Pylkk?nen1, Tanel Alum?e3, Murat Saraclar2
1 Adaptive Informatics Research Centre, Helsinki University of Technology
P.O.Box 5400, FIN-02015 HUT, Finland
{Mikko.Kurimo,Antti.Puurula,Vesa.Siivola}@tkk.fi
2 Bogazici University, Electrical and Electronics Eng. Dept.
34342 Bebek, Istanbul, Turkey
{arisoyeb,murat.saraclar}@boun.edu.tr
3 Laboratory of Phonetics and Speech Technology,
Institute of Cybernetics, Tallinn Technical University, Estonia
tanel.alumae@phon.ioc.ee
Abstract
It is practically impossible to build a
word-based lexicon for speech recogni-
tion in agglutinative languages that would
cover all the relevant words. The prob-
lem is that words are generally built by
concatenating several prefixes and suffixes
to the word roots. Together with com-
pounding and inflections this leads to mil-
lions of different, but still frequent word
forms. Due to inflections, ambiguity and
other phenomena, it is also not trivial to
automatically split the words into mean-
ingful parts. Rule-based morphological
analyzers can perform this splitting, but
due to the handcrafted rules, they also suf-
fer from an out-of-vocabulary problem. In
this paper we apply a recently proposed
fully automatic and rather language and
vocabulary independent way to build sub-
word lexica for three different agglutina-
tive languages. We demonstrate the lan-
guage portability as well by building a
successful large vocabulary speech recog-
nizer for each language and show superior
recognition performance compared to the
corresponding word-based reference sys-
tems.
1 Introduction
Speech recognition for dictation or prepared radio
and television broadcasts has had huge advances
during the last decades. For example, broadcast
news (BN) in English can now be recognized with
about ten percent word error rate (WER) (NIST,
2000) which results in mostly quite understandable
text. Some rare and new words may be missing but
the result has proven to be sufficient for many im-
portant applications, such as browsing and retrieval
of recorded speech and information retrieval from
the speech (Garofolo et al, 2000). However, besides
the development of powerful computers and new al-
gorithms, a crucial factor in this development is the
vast amount of transcribed speech and suitable text
data that has been collected for training the mod-
els. The problem faced in porting the BN recogni-
tion systems to conversational speech or to other lan-
guages is that almost as much new speech and text
data have to be collected again for the new task.
The reason for the need for a vast amount of train-
ing texts is that the state-of-the-art statistical lan-
guage models contain a huge amount of parameters
to be estimated in order to provide a proper probabil-
ity for any possible word sequence. The main reason
for the huge model size is that for an acceptable cov-
erage in an English BN task, the vocabulary must
be very large, at least 50,000 words, or more. For
languages with a higher degree of word inflections
than English, even larger vocabularies are required.
This paper focuses on the agglutinative languages in
which words are frequently formed by concatenat-
ing one or more stems, prefixes, and suffixes. For
these languages in which the words are often highly
inflected as well as formed from several morphemes,
even a vocabulary of 100,000 most common words
would not give sufficient coverage (Kneissler and
487
Klakow, 2001; Hirsim?ki et al, 2005). Thus, the
solution to the language modeling clearly has to in-
volve splitting of words into smaller modeling units
that could then be adequately modeled.
This paper focuses on solving the vocabulary
problem for several languages in which the speech
and text database resources are much smaller than
for the world?s main languages. A common fea-
ture for the agglutinative languages, such as Finnish,
Estonian, Hungarian and Turkish is that the large
vocabulary continuous speech recognition (LVCSR)
attempts so far have not resulted comparable perfor-
mance to the English systems. The reason for this
is not only the language modeling difficulties, but,
of course, the lack of suitable speech and text train-
ing data resources. In (Geutner et al, 1998; Sii-
vola et al, 2001) the systems aim at reducing the
active vocabulary and language models to a feasi-
ble size by clustering and focusing. In (Szarvas and
Furui, 2003; Alum?e, 2005; Hacioglu et al, 2003)
the words are split into morphemes by language-
dependent hand-crafted morphological rules. In
(Kneissler and Klakow, 2001; Arisoy and Arslan,
2005) different combinations of words, grammati-
cal morphemes and endings are utilized to decrease
the OOV rate and optimize the speech recognition
accuracy. However, constant large improvements
over the conventional word-based language models
in LVCSR have been rare.
The approach presented in this paper relies on a
data-driven algorithm called Morfessor (Creutz and
Lagus, 2002; Creutz and Lagus, 2005) which is a
language independent unsupervised machine learn-
ing method to find morpheme-like units (called sta-
tistical morphs) from a large text corpus. This
method has several advantages over the rule-based
grammatical morphemes, e.g. that no hand-crafted
rules are needed and all words can be processed,
even the foreign ones. Even if good grammatical
morphemes are available, the language modeling re-
sults by the statistical morphs seem to be at least as
good, if not better (Hirsim?ki et al, 2005). In this
paper we evaluate the statistical morphs for three
agglutinative languages and describe three different
speech recognition systems that successfully utilize
the n-gram language models trained for these units
in the corresponding LVCSR tasks.
2 Building the lexicon and language
models
2.1 Unsupervised discovery of morph units
Naturally, there are many ways to split the words
into smaller units to reduce a lexicon to a tractable
size. However, for a subword lexicon suitable
for language modeling applications such as speech
recognition, several properties are desirable:
1. The size of the lexicon should be small enough
that the n-gram modeling becomes more feasi-
ble than the conventional word based modeling.
2. The coverage of the target language by words
that can be built by concatenating the units
should be high enough to avoid the out-of-
vocabulary problem.
3. The units should be somehow meaningful, so
that the previously observed units can help in
predicting the next one.
4. In speech recognition one should be able to de-
termine the pronunciation for each unit.
A common approach to find the subword units
is to program the language-dependent grammatical
rules into a morphological analyzer and utilize that
to then split the text corpus into morphemes as in
e.g. (Hirsim?ki et al, 2005; Alum?e, 2005; Ha-
cioglu et al, 2003). There are some problems re-
lated to ambiguous splits and pronunciations of very
short inflection-type units, but also the coverage in,
e.g., news texts may be poor because of many names
and foreign words.
In this paper we have adopted a similar approach
as (Hirsim?ki et al, 2005). We use unsupervised
learning to find the best units according to some cost
function. In the Morfessor algorithm the minimized
cost is the coding length of the lexicon and the words
in the corpus represented by the units of the lexicon.
This minimum description length based cost func-
tion is especially appealing, because it tends to give
units that are both as frequent and as long as possi-
ble to suit well for both training the language models
and also decoding of the speech. Full coverage of
the language is also guaranteed by splitting the rare
words into very short units, even to single phonemes
if necessary. For language models utilized in speech
488
recognition, the lexicon of the statistical morphs can
be further reduced by omitting the rare words from
the input of the Morfessor algorithm. This operation
does not reduce the coverage of the lexicon, because
it just splits the rare words then into smaller units,
but the smaller lexicon may offer a remarkable speed
up of the recognition.
The pronunciation of, especially, the short units
may be ambiguous and may cause severe problems
in languages like English, in which the pronuncia-
tions can not be adequately determined from the or-
thography. In most agglutinative languages, such as
Finnish, Estonian and Turkish, rather simple letter-
to-phoneme rules are, however, sufficient for most
cases.
2.2 Building the lexicon for open vocabulary
The whole training text corpus is first passed through
a word splitting transformation as in Figure 1. Based
on the learned subword unit lexicon, the best split
for each word is determined by performing a Viterbi
search with the unigram probabilities of the units. At
this point the word break symbols are added between
each word in order to incorporate that information in
the statistical language models, as well. Then the n-
gram models are trained similarly as if the language
units were words including word and sentence break
symbols as additional units.
2.3 Building the n-gram model over morphs
Even though the required morph lexicon is much
smaller than the lexicon for the corresponding word
n-gram estimation, the data sparsity problem is still
important. Interpolated Kneser-Ney smoothing is
utilized to tune the language model probabilities in
the same way as found best for the word n-grams.
The n-grams that are not very useful for modeling
the language can be discarded from the model in
order to keep the model size down. For Turkish,
we used the entropy based pruning (Stolcke, 1998),
where the n-grams, that change the model entropy
less than a given treshold, are discarded from the
model. For Finnish and Estonian, we used n-gram
growing (Siivola and Pellom, 2005). The n-grams
that increase the training set likelihood enough with
respect to the corresponding increase in the model
size are accepted into the model (as in the minimum
description length principle). After the growing pro-
Morph lexicon
+ probabilities
word forms
Distinct
Text with words
segmented into
morphs
model
Language
Text corpus
segmentation
Viterbi
segmentation
MorphExtract
vocabulary
Train
n?grams
Figure 1: The steps in the process of estimating a
language model based on statistical morphs from a
text corpus (Hirsim?ki et al, 2005).
cess the model is further pruned with entropy based
pruning. The method allows us to train models with
higher order n-grams, since the memory consump-
tion is lower and also gives somewhat better mod-
els. Both methods can also be viewed as choosing
the correct model complexity for the training data to
avoid over-learning.
3 Statistical properties of Finnish,
Estonian and Turkish
Before presenting the speech recognition results,
some statistical properties are presented for the three
agglutinative languages studied. If we consider
choosing a vocabulary of the 50k-70k most common
words, as usual in English broadcast news LVCSR
systems, the out-of-vocabulary (OOV) rate in En-
glish is typically smaller than 1%. Using the lan-
guage model training data the following OOV rates
can be found for a vocabulary including only the
most common words: 15% OOV for 69k in Finnish
(Hirsim?ki et al, 2005), 10% for 60k in Estonian
and 9% for 50k in Turkish. As shown in (Hacioglu et
al., 2003) this does not only mean the same amount
of extra speech recognition errors, but even more,
because the recognizer tends to lose track when un-
known words get mapped to those that are in the vo-
cabulary. Even doubling the vocabulary is not a suf-
489
0 1 2 3
x 106
0
2
4
6
8 x 10
5
Number of sentences
N
um
be
r o
f d
ist
in
ct
 u
ni
ts
0 1 2 3
x 106
2.6
2.8
3
3.2
3.4
3.6 x 10
4
Number of sentences
N
um
be
r o
f d
ist
in
ct
 m
or
ph
s
Morphs
Words Morphs
Figure 2: Vocabulary growth of words and morphs
for Turkish language
ficient solution, because a vocabulary twice as large
(120k) would only reduce the OOV rate to 6% in
Estonian and 5% in Turkish. In Finnish even a 400k
vocabulary of the most common words still gives 5%
OOV in the language model training material.
Figure 2 illustrates the vocabulary explosion en-
countered when using words and how using morphs
avoids this problem for Turkish. The figure on the
left shows the vocabulary growth for both words and
morphs. The figure on the right shows the graph
for morphs in more detail. As seen in the figure,
the number of new words encountered continues to
increase as the corpus size gets larger whereas the
number of new morphs encountered levels off.
4 Speech recognition experiments
4.1 About selection of the recognition tasks
In this work the morph-based language models have
been applied in speech recognition for three differ-
ent agglutinative languages, Finnish, Estonian and
Turkish. The recognition tasks are speaker depen-
dent and independent fluent dictation of sentences
taken from newspapers and books, which typically
require very large vocabulary language models.
4.2 Finnish
Finnish is a highly inflected language, in which
words are formed mainly by agglutination and com-
pounding. Finnish is also the language for which the
algorithm for the unsupervised morpheme discovery
(Creutz and Lagus, 2002) was originally developed.
The units of the morph lexicon for the experiments
in this paper were learned from a joint corpus con-
taining newspapers, books and newswire stories of
totally about 150 million words (CSC, 2001). We
obtained a lexicon of 25k morphs by feeding the
learning algorithm with the word list containing the
160k most common words. For language model
training we used the same text corpus and the re-
cently developed growing n-gram training algorithm
(Siivola and Pellom, 2005). The amount of resulted
n-grams are listed in Table 4. The average length
of a morph is such that a word corresponds to 2.52
morphs including a word break symbol.
The speech recognition task consisted of a book
read aloud by one female speaker as in (Hirsim?ki et
al., 2005). Speaker dependent cross-word triphone
models were trained using the first 12 hours of data
and evaluated by the last 27 minutes. The models
included tied state hidden Markov models (HMMs)
of totally 1500 different states, 8 Gaussian mixtures
(GMMs) per state, short-time mel-cepstral features
(MFCCs), maximum likelihood linear transforma-
tion (MLLT) and explicit phone duration models
(Pylkk?nen and Kurimo, 2004). The real-time fac-
tor of recognition speed was less than 10 xRT with
a 2.2 GHz CPU. However, with the efficient LVCSR
decoder utilized (Pylkk?nen, 2005) it seems that by
making an even smaller morph lexicon, such as 10k,
the decoding speed could be optimized to only a few
times real-time without an excessive trade-off with
recognition performance.
4.3 Estonian
Estonian is closely related to Finnish and a similar
language modeling approach was directly applied
to the Estonian recognition task. The text corpus
used to learn the morph units and train the statis-
tical language model consisted of newspapers and
books, altogether about 55 million words (Segakor-
pus, 2005). At first, 45k morph units were obtained
as the best subword unit set from the list of the 470k
most common words in the corpora. For speed-
ing up the recognition, the morph lexicon was after-
wards reduced to 37k by splitting the rarest morphs
(occurring in only one or two words) further into
smaller ones. Corresponding growing n-gram lan-
guage models as in Finnish were trained from the
Estonian corpora resulting the n-grams in Table 4.
The speech recognition task in Estonian consisted
of long sentences read by 50 randomly picked held-
out test speakers, 7 sentences each (a part of (Meister
490
et al, 2002)). Unlike the Finnish and Turkish micro-
phone data, this data was recorded from telephone,
i.e. 8 kHz sampling rate and narrow band data in-
stead of 16 kHz and normal (full) bandwidth. The
phoneme models were trained for speaker indepen-
dent recognition using windowed cepstral mean sub-
traction and significantly more data (over 200 hours
and 1300 speakers) than for the Finnish task. The
speaker independence, together with the telephone
quality and occasional background noises, made this
task still a considerably more difficult one. Other-
wise the acoustic models were similar cross-word
triphone GMM-HMMs with MFCC features, MLLT
transformation and the explicit phone duration mod-
eling, except larger: 5100 different states and 16
GMMs per state. Thus, the recognition speed is
also slower than in Finnish, about 20 xRT (2.2GHz
CPU).
4.4 Turkish
Turkish is another a highly-inflected and agglutina-
tive language with relatively free word order. The
same Morfessor tool (Creutz and Lagus, 2005) as in
Finnish and Estonian was applied to Turkish texts
as well. Using the 360k most common words from
the training corpus, 34k morph units were obtained.
The training corpus consists of approximately 27M
words taken from literature, law, politics, social
sciences, popular science, information technology,
medicine, newspapers, magazines and sports news.
N-gram language models for different orders with
interpolated Kneser-Ney smoothing as well as en-
tropy based pruning were built for this morph lexi-
con using the SRILM toolkit (Stolcke, 2002). The
number of n-grams for the highest order we tried (6-
grams without entropy-based pruning) are reported
in Table 4. In average, there are 2.37 morphs per
word including the word break symbol.
The recognition task in Turkish consisted of ap-
proximately one hour of newspaper sentences read
by one female speaker. We used decision-tree state
clustered cross-word triphone models with approx-
imately 5000 HMM states. Instead of using letter
to phoneme rules, the acoustic models were based
directly on letters. Each state of the speaker inde-
pendent HMMs had a GMM with 6 mixture compo-
nents. The HTK frontend (Young et al, 2002) was
used to get the MFCC based acoustic features. The
explicit phone duration models were not applied.
The training data contained 17 hours of speech from
over 250 speakers. Instead of the LVCSR decoder
used in Finnish and Estonian (Pylkk?nen, 2005), the
Turkish evaluation was performed using another de-
coder (AT&T, 2003), Using a 3.6GHz CPU, the real-
time factor was around one.
5 Results
The recognition results for the three different tasks:
Finnish, Estonian and Turkish, are provided in Ta-
bles 1 ? 3. In each task the word error rate (WER)
and letter error rate (LER) statistics for the morph-
based system is compared to a corresponding word-
based system. The resulting morpheme strings are
glued to words according to the word break symbols
included in the language model (see Section 2.2) and
the WER is computed as the sum of substituted, in-
serted and deleted words divided by the correct num-
ber of words. LER is included here as well, because
although WER is a more common measure, it is not
comparable between languages. For example, in ag-
glutinative languages the words are long and contain
a variable amount of morphemes. Thus, any incor-
rect prefix or suffix would make the whole word in-
correct. The n-gram language model statistics are
given in Table 4.
Finnish lexicon WER LER
Words 400k 8.5 1.20
Morphs 25k 7.0 0.95
Table 1: The LVCSR performance for the speaker-
dependent Finnish task consisting of book-reading
(see Section 4.2). For a reference (word-based) lan-
guage model a 400k lexicon was chosen.
Estonian lexicon WER LER
Words 60k 56.3 22.4
Morphs 37k 47.6 18.9
Table 2: The LVCSR performance for the speaker-
independent Estonian task consisting of read sen-
tences recorded via telephone (see Section 4.3). For
a reference (word-based) language model a 60k lex-
icon was used here.
491
Turkish lexicon WER LER
Words
3-gram 50k 38.8 15.2
Morphs
3-gram 34k 39.2 14.8
4-gram 34k 35.0 13.1
5-gram 34k 33.9 12.4
Morphs, rescored by morph 6-gram
3-gram 34k 33.8 12.4
4-gram 34k 33.2 12.3
5-gram 34k 33.3 12.2
Table 3: The LVCSR performance for the speaker-
independent Turkish task consisting of read news-
paper sentences (see Section 4.4). For the refer-
ence 50k (word-based) language model the accuracy
given by 4 and 5-grams did not improve from that of
3-grams.
In the Turkish recognizer the memory constraints
during network optimization (Allauzen et al, 2004)
allowed the use of language models only up to 5-
grams. The language model pruning thresholds were
optimized over a range of values and the best re-
sults are shown in Table 3. We also tried the same
experiments with two-pass recognition. In the first
pass, instead of the best path, lattice output was gen-
erated with the same language models with prun-
ing. Then these lattices were rescored using the non-
pruned 6-gram language models (see Table 4) and
the best path was taken as the recognition output.
For the word-based reference model, the two-pass
recognition gave no improvements. It is likely that
the language model training corpus was too small to
train proper 6-gram word models. However, for the
morph-based model, we obtained a slight improve-
ment (0.7 % absolute) by two-pass recognition.
6 Discussion
The key result of this paper is that we can success-
fully apply the unsupervised statistical morphs in
large vocabulary language models in all the three ex-
perimented agglutinative languages. Furthermore,
the results show that in all the different LVCSR
tasks, the morph-based language models perform
very well and constantly dominate the reference lan-
guage model based on words. The way that the lexi-
# morph-based models
ngrams Finnish Estonian Turkish
1grams 24,833 37,061 34,332
2grams 2,188,476 1,050,127 655,621
3grams 17,064,072 7,133,902 1,936,263
4grams 25,200,308 8,201,543 3,824,362
5grams 7,167,021 3,298,429 4,857,125
6grams 624,832 691,899 5,523,922
7grams 23,851 55,363 -
8grams 0 1045 -
Sum 52,293,393 20,469,369 16,831,625
Table 4: The amount of different n-grams in each
language model based on statistical morphs. Note
that the Turkish language model was not prepared
by the growing n-gram algorithm as the others and
the model was limited to 6-grams.
con is built from the word fragments allows the con-
struction of statistical language models, in practice,
for almost an unlimited vocabulary by a lexicon that
still has a convenient size.
The recognition was here restricted to agglutina-
tive languages and tasks in which the language used
is both rather general and matches fairly well with
the available training texts. Significant performance
variation in different languages can be observed
here, because of the different tasks and the fact that
comparable recognition conditions and training re-
sources have not been possible to arrange. However,
we believe that the tasks are still both difficult and
realistic enough to illustrate the difference of per-
formance when using language models based on a
lexicon of morphs vs. words in each task. There are
no directly comparable previous LVCSR results on
the same tasks and data, but the closest ones which
can be found are slightly over 20% WER for the
Finnish task (Hirsim?ki et al, 2005), slightly over
40 % WER for the Estonian task (Alum?e, 2005)
and slightly over 30 % WER for the Turkish task
(Erdogan et al, 2005).
Naturally, it is also possible to prepare a huge lex-
icon and still succeed in recognition fairly well (Sar-
aclar et al, 2002; McTait and Adda-Decker, 2003;
Hirsim?ki et al, 2005), but this is not a very con-
venient approach because of the resulting huge lan-
guage models or the heavy pruning required to keep
492
them still tractable. The word-based language mod-
els that were constructed in this paper as reference
models were trained as much as possible in the same
way as the corresponding morph language models.
For Finnish and Estonian the growing n-grams (Sii-
vola and Pellom, 2005) were used including the op-
tion of constructing the OOV words from phonemes
as in (Hirsim?ki et al, 2005). For Turkish a con-
ventional n-gram was built by SRILM similarly as
for the morphs. The recognition approach taken for
Turkish involves a static decoding network construc-
tion and optimization resulting in near real time de-
coding. However, the memory requirements of net-
work optimization becomes prohibitive for large lex-
icon and language models as presented in this paper.
In this paper the recognition speed was not a ma-
jor concern, but from the application point of view
that is a very important factor to be taken into a ac-
count in the comparison. It seems that the major fac-
tors that make the recognition slower are short lexi-
cal units, large lexicon and language models and the
amount of Gaussian mixtures in the acoustic model.
7 Conclusions
This work presents statistical language models
trained on different agglutinative languages utilizing
a lexicon based on the recently proposed unsuper-
vised statistical morphs. To our knowledge this is
the first work in which similarly developed subword
unit lexica are developed and successfully evaluated
in three different LVCSR systems in different lan-
guages. In each case the morph-based approach con-
stantly shows a significant improvement over a con-
ventional word-based LVCSR language models. Fu-
ture work will be the further development of also
the grammatical morph-based language models and
comparison of that to the current approach, as well
as extending this evaluation work to new languages.
8 Acknowledgments
We thank the Finnish Federation of the Visually Im-
paired for providing the Finnish speech data and the
Finnish news agency (STT) and the Finnish IT cen-
ter for science (CSC) for the text data. Our work was
supported by the Academy of Finland in the projects
New information processing principles, Adaptive In-
formatics and New adaptive and learning methods in
speech recognition. This work was supported in part
by the IST Programme of the European Community,
under the PASCAL Network of Excellence, IST-
2002-506778. The authors would like to thank Sa-
banci and ODTU universities for the Turkish acous-
tic and text data and AT&T Labs ? Research for
the software. This research is partially supported
by SIMILAR Network of Excellence and TUBITAK
BDP (Unified Doctorate Program of the Scientific
and Technological Research Council of Turkey).
References
Cyril Allauzen, Mehryar Mohri, Michael Riley, and Brian
Roark. 2004. A generalized construction of integrated
speech recognition transducers. In Proceedings of the
IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), Montreal, Canada.
Tanel Alum?e. 2005. Phonological and morphologi-
cal modeling in large vocabulary continuous Estonian
speech recognition system. In Proceedings of Second
Baltic Conference on Human Language Technologies,
pages 89?94.
Mehryar Mohri and Michael D. Riley. DCD Library ?
Speech Recognition Decoder Library. AT&T Labs ?
Research. http://www.research.att.com/
sw/tools/dcd/.
Ebru Arisoy and Levent Arslan. 2005. Turkish dictation
system for broadcast news applications. In 13th Euro-
pean Signal Processing Conference - EUSIPCO 2005,
Antalya, Turkey, September.
Mathias Creutz and Krista Lagus. 2002. Unsupervised
discovery of morphemes. In Proceedings of the Work-
shop on Morphological and Phonological Learning of
ACL-02, pages 21?30.
Mathias Creutz and Krista Lagus. 2005. Unsuper-
vised morpheme segmentation and morphology in-
duction from text corpora using Morfessor. Techni-
cal Report A81, Publications in Computer and Infor-
mation Science, Helsinki University of Technology.
URL: http://www.cis.hut.fi/projects/
morpho/.
J. Garofolo, G. Auzanne, and E. Voorhees. 2000. The
TREC spoken document retrieval track: A success
story. In Proceedings of Content Based Multimedia
Information Access Conference, April 12-14.
P. Geutner, M. Finke, and P. Scheytt. 1998. Adap-
tive vocabularies for transcribing multilingual broad-
cast news. In Proceedings of the IEEE International
Conference on Acoustics, Speech and Signal Process-
ing (ICASSP), Seattle, WA, USA, May.
493
H. Erdogan, O. Buyuk, K. Oflazer. 2005. Incorporating
language constraints in sub-word based speech recog-
nition. IEEE Automatic Speech Recognition and Un-
derstanding Workshop, Cancun, Mexico.
Kadri Hacioglu, Brian Pellom, Tolga Ciloglu, Ozlem Oz-
turk, Mikko Kurimo, and Mathias Creutz. 2003. On
lexicon creation for Turkish LVCSR. In Proceedings
of 8th European Conference on Speech Communica-
tion and Technology, pages 1165?1168.
Teemu Hirsim?ki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja, and Janne Pylkk?nen. 2005.
Unlimited vocabulary speech recognition with morph
language models applied to Finnish. Computer Speech
and Language. (accepted for publication).
Jan Kneissler and Dietrich Klakow. 2001. Speech recog-
nition for huge vocabularies by using optimized sub-
word units. In Proceedings of the 7th European Con-
ference on Speech Communication and Technology
(Eurospeech), pages 69?72, Aalborg, Denmark.
CSC Tieteellinen laskenta Oy. 2001. Finnish Lan-
guage Text Bank: Corpora Books, Newspapers,
Magazines and Other. http://www.csc.fi/
kielipankki/.
Kevin McTait and Martine Adda-Decker. 2003. The
300k LIMSI German Broadcast News Transcription
System. In Proceedings of 8th European Conference
on Speech Communication and Technology.
Einar Meister, J?rgen Lasn, and Lya Meister. 2002. Esto-
nian SpeechDat: a project in progress. In Proceedings
of the Fonetiikan P?iv?t ? Phonetics Symposium 2002
in Finland, pages 21?26.
NIST. 2000. Proceedings of DARPA workshop on Auto-
matic Transcription of Broadcast News. NIST, Wash-
ington DC, May.
Janne Pylkk?nen. 2005. New pruning criteria for effi-
cient decoding. In Proceedings of 9th European Con-
ference on Speech Communication and Technology.
Janne Pylkk?nen and Mikko Kurimo. 2004. Duration
modeling techniques for continuous speech recogni-
tion. In Proceedings of the International Conference
on Spoken Language Processing.
Murat Saraclar, Michael Riley, Enrico Bocchieri, and
Vincent Goffin. 2002. Towards automatic closed cap-
tioning: Low latency real time broadcast news tran-
scription. In Proceedings of the International Confer-
ence on Spoken Language Processing (ICSLP), Den-
ver, CO, USA.
Segakorpus ? Mixed Corpus of Estonian. Tartu Uni-
versity. http://test.cl.ut.ee/korpused/
segakorpus/.
Vesa Siivola and Bryan Pellom. 2005. Growing an n-
gram language model. In Proceedings of 9th European
Conference on Speech Communication and Technol-
ogy.
Vesa Siivola, Mikko Kurimo, and Krista Lagus. 2001.
Large vocabulary statistical language modeling for
continuous speech recognition. In Proceedings of 7th
European Conference on Speech Communication and
Technology, pages 737?747, Aalborg, Copenhagen.
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In Proc. DARPA Broadcast News
Transcription and Understanding Workshop, pages
270?274.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901?904.
Mate Szarvas and Sadaoki Furui. 2003. Evaluation of the
stochastic morphosyntactic language model on a one
million word Hungarian task. In Proceedings of the
8th European Conference on Speech Communication
and Technology (Eurospeech), pages 2297?2300.
S. Young, D. Ollason, V. Valtchev, and P. Woodland.
2002. The HTK book (for HTK version 3.2.), March.
494
Proceedings of NAACL HLT 2007, pages 380?387,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Analysis of Morph-Based Speech Recognition and the Modeling of
Out-of-Vocabulary Words Across Languages
Mathias Creutz?, Teemu Hirsima?ki?, Mikko Kurimo?, Antti Puurula?, Janne Pylkko?nen?,
Vesa Siivola?, Matti Varjokallio?, Ebru Ar?soy?, Murat Sarac?lar?, and Andreas Stolcke?
? Helsinki University of Technology, <firstname>.<lastname>@tkk.fi,
? Bog?azic?i University, arisoyeb@boun.edu.tr, murat.saraclar@boun.edu.tr,
? SRI International / International Computer Science Institute, stolcke@speech.sri.com
Abstract
We analyze subword-based language
models (LMs) in large-vocabulary
continuous speech recognition across
four ?morphologically rich? languages:
Finnish, Estonian, Turkish, and Egyptian
Colloquial Arabic. By estimating n-gram
LMs over sequences of morphs instead
of words, better vocabulary coverage
and reduced data sparsity is obtained.
Standard word LMs suffer from high
out-of-vocabulary (OOV) rates, whereas
the morph LMs can recognize previously
unseen word forms by concatenating
morphs. We show that the morph LMs
generally outperform the word LMs and
that they perform fairly well on OOVs
without compromising the accuracy
obtained for in-vocabulary words.
1 Introduction
As automatic speech recognition systems are being
developed for an increasing number of languages,
there is growing interest in language modeling ap-
proaches that are suitable for so-called ?morpholog-
ically rich? languages. In these languages, the num-
ber of possible word forms is very large because
of many productive morphological processes; words
are formed through extensive use of, e.g., inflection,
derivation and compounding (such as the English
words ?rooms?, ?roomy?, ?bedroom?, which all stem
from the noun ?room?).
For some languages, language modeling based on
surface forms of words has proven successful, or at
least satisfactory. The most studied language, En-
glish, is not characterized by a multitude of word
forms. Thus, the recognition vocabulary can sim-
ply consist of a list of words observed in the training
text, and n-gram language models (LMs) are esti-
mated over word sequences. The applicability of the
word-based approach to morphologically richer lan-
guages has been questioned. In highly compounding
languages, such as the Germanic languages German,
Dutch and Swedish, decomposition of compound
words can be carried out to reduce the vocabulary
size. Highly inflecting languages are found, e.g.,
among the Slavic, Romance, Turkic, and Semitic
language families. LMs incorporating morphologi-
cal knowledge about these languages can be applied.
A further challenging category comprises languages
that are both highly inflecting and compounding,
such as the Finno-Ugric languages Finnish and Es-
tonian.
Morphology modeling aims to reduce the out-
of-vocabulary (OOV) rate as well as data sparsity,
thereby producing more effective language mod-
els. However, obtaining considerable improvements
in speech recognition accuracy seems hard, as is
demonstrated by the fairly meager improvements
(1?4 % relative) over standard word-based models
accomplished by, e.g., Berton et al (1996), Ordel-
man et al (2003), Kirchhoff et al (2006), Whit-
taker and Woodland (2000), Kwon and Park (2003),
and Shafran and Hall (2006) for Dutch, Arabic, En-
glish, Korean, and Czech, or even the worse perfor-
mance reported by Larson et al (2000) for German
and Byrne et al (2001) for Czech. Nevertheless,
clear improvements over a word baseline have been
achieved for Serbo-Croatian (Geutner et al, 1998),
Finnish, Estonian (Kurimo et al, 2006b) and Turk-
ish (Kurimo et al, 2006a).
In this paper, subword language models in the
recognition of speech of four languages are ana-
380
lyzed: Finnish, Estonian, Turkish, and the dialect
of Arabic spoken in Egypt, Egyptian Colloquial
Arabic (ECA). All these languages are considered
?morphologically rich?, but the benefits of using
subword-based LMs differ across languages. We at-
tempt to discover explanations for these differences.
In particular, the focus is on the analysis of OOVs:
A perceived strength of subword models, when con-
trasted with word models, is that subword models
can generalize to previously unseen word forms by
recognizing them as sequences of shorter familiar
word fragments.
2 Morfessor
Morfessor is an unsupervised, data-driven, method
for the segmentation of words into morpheme-like
units. The general idea is to discover as com-
pact a description of the input text corpus as possi-
ble. Substrings occurring frequently enough in sev-
eral different word forms are proposed as morphs,
and the words in the corpus are then represented
as a concatenation of morphs, e.g., ?hand, hand+s,
left+hand+ed, hand+ful?. Through maximum a pos-
teriori optimization (MAP), an optimal balance is
sought between the compactness of the inventory of
morphs, i.e., the morph lexicon, versus the compact-
ness of the representation of the corpus.
Among others, de Marcken (1996), Brent (1999),
Goldsmith (2001), Creutz and Lagus (2002), and
Creutz (2006) have shown that models based on
the above approach produce segmentations that re-
semble linguistic morpheme segmentations, when
formulated mathematically in a probabilistic frame-
work or equivalently using the Minimum Descrip-
tion Length (MDL) principle (Rissanen, 1989).
Similarly, Goldwater et al (2006) use a hierarchical
Dirichlet model in combination with morph bigram
probabilities.
The Morfessor model has been developed over
the years, and different model versions exist. The
model used in the speech recognition experiments of
the current paper is the original, so-called Morfes-
sor Baseline algorithm, which is publicly available
for download.1. The mathematics of the Morfessor
Baseline model is briefly outlined in the following;
consult Creutz (2006) for details.
1http://www.cis.hut.fi/projects/morpho/
2.1 MAP Optimization Criterion
In slightly simplified form, the optimization crite-
rion utilized in the model corresponds to the maxi-
mization of the following posterior probability:
P (lexicon | corpus) ?
P (lexicon) ? P (corpus | lexicon) =
?
letters ?
P (?) ?
?
morphs ?
P (?). (1)
The lexicon consists of all distinct morphs spelled
out; this forms a long string of letters ?, in which
each morph is separated from the next morph using
a morph boundary character. The probability of the
lexicon is the product of the probability of each let-
ter in this string. Analogously, the corpus is repre-
sented as a sequence of morphs, which corresponds
to a particular segmentation of the words in the cor-
pus. The probability of this segmentation equals the
product of the probability of each morph token ?.
Letter and morph probabilities are maximum likeli-
hood estimates (empirical Bayes).
2.2 From Morphs to n-Grams
As a result of the probabilistic (or MDL) approach,
the morph inventory discovered by the Morfessor
Baseline algorithm is larger the more training data
there is. In some speech recognition experiments,
however, it has been desirable to restrict the size of
the morph inventory. This has been achieved by set-
ting a frequency threshold on the words on which
Morfessor is trained, such that the rarest words will
not affect the learning process. Nonetheless, the
rarest words can be split into morphs in accordance
with the model learned, by using the Viterbi algo-
rithm to select the most likely segmentation. The
process is depicted in Figure 1.
2.3 Grapheme-to-Phoneme Mapping
The mapping between graphemes (letters) and
phonemes is straightforward in the languages stud-
ied in the current paper. More or less, there is
a one-to-one correspondence between letters and
phonemes. That is, the spelling of a word indicates
the pronunciation of the word, and when splitting the
word into parts, the pronunciation of the parts in iso-
lation does not differ much from the pronunciation
of the parts in context. However, a few exceptions
381
Morph
inventory
+ probs
n?grams
Train
cut?off
Frequency
Viterbi
segm.
Text with words
segmented into
LM
morphs
MorfessorExtractwords
Text corpus
Figure 1: How to train a segmentation model using
the Morfessor Baseline algorithm, and how to fur-
ther train an n-gram model based on morphs.
have been treated more rigorously in the Arabic ex-
periments: e.g., in some contexts the same (spelled)
morph can have multiple possible pronunciations.
3 Experiments and Analysis
The goal of the conducted experiments is to com-
pare n-gram language models based on morphs to
standard word n-gram models in automatic speech
recognition across languages.
3.1 Data Sets and Recognition Systems
The results from eight different tests have been an-
alyzed. Some central properties of the test config-
urations are shown in Table 1. The Finnish, Esto-
nian, and Turkish test configurations are slight vari-
ations of experiments reported earlier in Hirsima?ki
et al (2006) (Fin1: ?News task?, Fin2: ?Book task?),
Kurimo et al (2006a) (Fin3, Tur1), and Kurimo et
al. (2006b) (Fin4, Est, Tur2).
Three different recognition platforms have been
used, all of which are state-of-the-art large vocab-
ulary continuous speech recognition (LVCSR) sys-
tems. The Finnish and Estonian experiments have
been run on the HUT speech recognition system de-
veloped at Helsinki University of Technology.
The Turkish tests were performed using the
AT&T decoder (Mohri and Riley, 2002); the acous-
tic features were produced using the HTK front end
(Young et al, 2002). The experiments on Egyptian
Colloquial Arabic (ECA) were carried out using the
SRI DecipherTM speech recognition system.
3.1.1 Speech Data and Acoustic Models
The type and amount of speech data vary from
one language to another. The Finnish data con-
sists of news broadcasts read by one single female
speaker (Fin1), as well as an audio book read by an-
other female speaker (Fin2, Fin3, Fin4). The Finnish
acoustic models are speaker dependent (SD). Mono-
phones (mon) were used in the earlier experiments
(Fin1, Fin2), but these were later replaced by cross-
context triphones (tri).
The Estonian speech data has been collected from
a large number of speakers and consists of sen-
tences from newspapers as well as names and dig-
its read aloud. The acoustic models are speaker-
independent triphones (SI tri) adapted online using
Cepstral Mean Subtraction and Constrained Maxi-
mum Likelihood Linear Regression. Also the Turk-
ish acoustic training data contains speech from hun-
dreds of speakers. The test set is composed of news-
paper text read by one female speaker. Speaker-
independent triphones are used as acoustic models.
The Finnish, Estonian, and Turkish data sets con-
tain planned speech, i.e., written text read aloud.
By contrast, the Arabic data consists of transcribed
spontaneous telephone conversations,2 which are
characterized by disfluencies and by the presence
of ?non-speech?, such as laugh and cough sounds.
There are multiple speakers in the Arabic data, and
online speaker adaptation has been performed.
3.1.2 Text Data and Language Models
The n-gram language models are trained using
the SRILM toolkit (Stolcke, 2002) (Fin1, Fin2,
Tur1, Tur2, ECA) or similar software developed
at HUT (Siivola and Pellom, 2005) (Fin3, Fin4,
Est). All models utilize the Modified Interpolated
Kneser-Ney smoothing technique (Chen and Good-
man, 1999). The Arabic LM is trained on the
same corpus that is used for acoustic training. This
data set is regrettably small (160 000 words), but it
matches the test set well in style, as it consists of
transcribed spontaneous speech. The LM training
corpora used for the other languages contain fairly
large amounts of mainly news and book texts and
conceivably match the style of the test data well.
In the morph-based models, words are split into
morphs using Morfessor, and statistics are collected
for morph n-grams. As the desired output of the
2LDC CallHome corpus of Egyptian Colloquial Ara-
bic: http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC97S45
382
Table 1: Test configurations
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA
Recognizer HUT HUT HUT HUT HUT AT&T AT&T SRI
Speech data
Type of speech read read read read read read read spont.
Training set [kwords] 20 49 49 49 790 230 110 160
Speakers in training set 1 1 1 1 1300 550 250 310
Test set [kwords] 4.3 1.9 1.9 1.9 3.7 7.0 7.0 16
Speakers in test set 1 1 1 1 50 1 1 57
Text data
LM training set [Mwords] 36 36 32 150 53 17 27 0.16
Models
Acoustic models SD mon SD mon SD tri SD tri SI tri SI tri SI tri SI tri
Morph lexicon [kmorphs] 66 66 120 25 37 52 34 6.1
Word lexicon [kwords] 410 410 410 ? 60 120 50 18
Out-of-vocabulary words
OOV LM training set [%] 5.0 5.0 5.9 ? 14 5.3 9.6 0.61
OOV test set [%] 5.0 7.2 7.3 ? 19 5.5 12 9.9
New words in test set [%] 2.7 3.0 3.1 1.5 3.4 1.6 1.5 9.8
speech recognizer is a sequence of words rather than
morphs, the LM explicitly models word breaks as
special symbols occurring in the morph sequence.
For comparison, word n-gram models have been
tested. The vocabulary cannot typically include ev-
ery word form occurring in the training set (because
of the large number of different words), so the most
frequent words are given priority; the actual lexicon
sizes used in each experiment are shown in Table 1.
Any word not contained in the lexicon is replaced by
a special out-of-vocabulary symbol.
As words and morphs are units of different length,
their optimal performance may occur at different or-
ders of the n-gram. The best order of the n-gram
has been optimized on development test sets in the
following cases: Fin1, Fin2, Tur1, ECA (4-grams
for both morphs and words) and Tur2 (5-grams for
morphs, 3-grams for words). The models have ad-
ditionally been pruned using entropy-based pruning
(Tur1, Tur2, ECA) (Stolcke, 1998). In the other
experiments (Fin3, Fin4, Est), no fixed maximum
value of n was selected. n-Gram growing was per-
formed (Siivola and Pellom, 2005), such that those
n-grams that maximize the training set likelihood
are gradually added to the model. The unrestricted
growth of the model is counterbalanced by an MDL-
type complexity term. The highest order of n-grams
accepted was 7 for Finnish and 8 for Estonian.
Note that the optimization procedure is neutral
with respect to morphs vs. words. Roughly the
same number of parameters are allowed in the result-
ing LMs, but typically the morph n-gram LMs are
smaller than the corresponding word n-gram LMs.
3.1.3 Out-of-Vocabulary Words
Table 1 further shows statistics on out-of-
vocabulary rates in the data sets. This is relevant
for the assessment of the word models, as the OOV
rates define the limits of these models.
The OOV rate for the LM training set corresponds
to the proportion of words replaced by the OOV
symbol in the LM training data, i.e., words that were
not included in the recognition vocabulary. The high
OOV rates for Estonian (14 %) and Tur2 (9.6 %) in-
dicate that the word lexicons have poor coverage of
these sets. By contrast, the ECA word lexicon cov-
ers virtually the entire training set vocabulary.
Correspondingly, the test set OOV rate is the pro-
portion of words that occur in the data sets used
for running the speech recognition tests, but that are
missing from the recognition lexicons. This value
is thus the minimum error that can be obtained by
the word models, or put differently, the recognizer
is guaranteed to get at least this proportion of words
wrong. Again, the values are very high for Estonian
(19 %) and Tur2 (12 %), but also for Arabic (9.9 %)
because of the insufficient amount of training data.
Finally, the figures labeled ?new words in test set?
denote the proportion of words in the test set that do
not occur in the LM training set. Thus, these values
indicate the minimum error achievable by any word
model trained on the training sets available.
383
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA0
10
20
30
40
50
60
70
80
90
100
76.6
71.9
79.1
67.8
89.8
82.2
92.8
66.7
51.9
68.667.466.7
61.2
40.141.8
W
or
d 
ac
cu
ra
cy
 [%
]
 
 
Morphs
Words
Figure 2: Word accuracies for the different speech
recognition test configurations.
3.2 Results and Analysis
The morph-based and word-based results of the con-
ducted speech recognition experiments are shown in
Figure 2 (for Fin4, no comparable word experiment
has been carried out). The evaluation measure used
is word accuracy (WAC): the number of correctly
recognized words minus the number of incorrectly
inserted words divided by the number of words in
the reference transcript. (Another frequently used
measure is the word error rate, WER, which relates
to word accuracy as WER = 100 % ? WAC.)
Figure 2 shows that the morph models perform
better than the word models, with the exception
of the Arabic experiment (ECA), where the word
model outperforms the morph model. The statisti-
cal significance of these differences is confirmed by
one-tailed paired Wilcoxon signed-rank tests at the
significance level of 0.05.
Overall, the best performance is observed for the
Finnish data sets, which is explained by the speaker-
dependent acoustic models and clean noise condi-
tions. The Arabic setup suffers from the insufficient
amount of LM training data.
3.2.1 In-Vocabulary Words
For a further investigation of the outcome of the
experiments, the test sets have been partitioned into
regions based on the types of words they contain.
The recognition output is aligned with the refer-
ence transcript, and the regions aligned with in-
vocabulary (IV) reference words (words contained
in the vocabulary of the word model) are put in
one partition and the remaining words (OOVs) are
put in another partition. Word accuracies are then
computed separately for the two partitions. Inserted
words, i.e., words that are not aligned with any word
in the reference, are put in the IV partition, unless
they are adjacent to an OOV region, in which case
they are put in the OOV partition.
Figure 3a shows word accuracies for the in-
vocabulary words. Without exception, the accuracy
for the IVs is higher than that of the entire test set vo-
cabulary. One could imagine that the word models
would do better than the morph models on the IVs,
since the word models are totally focused on these
words, whereas the morph models reserve modeling
capacity for a much larger set of words. The word
accuracies in Fig. 3a also partly seem to support this
view. However, Wilcoxon signed-rank tests (level
0.05) show that the superiority of the word model is
statistically significant only for Arabic and for Fin3.
With few exceptions, it is thus possible to draw
the conclusion that morph models are capable of
modeling a much larger set of words than word
models without, however, compromising the perfor-
mance on the limited vocabulary covered by the
word models in a statistically significant way.
3.2.2 Out-of-Vocabulary Words
Since the word model and morph model perform
equally well on the subset of words that are included
in the lexicon of the word model, the overall supe-
riority of the morph model needs to come from its
successful coping with out-of-vocabulary words.
In Figure 3b, word accuracies have been plot-
ted for the out-of-vocabulary words contained in the
test set. It is clear that the recognition accuracy for
the OOVs is much lower than the overall accuracy.
Also, negative accuracy values are observed. This
happens when the number of insertions exceeds the
number of correctly recognized units.
In Figure 3b, if speaker-dependent and speaker-
independent setups are considered separately (and
Arabic is left out), there is a tendency for the morph
models to recognize the OOVs more accurately, the
higher the OOV rate is. One could say that a morph
model has a double advantage over a correspond-
ing word model: the larger the proportion of OOVs
384
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA0
10
20
30
40
50
60
70
80
90
100
79.979.781.977.9
92.594.6
73.374.771.872.671.771.9
45.6
48.1
W
or
d 
ac
cu
ra
cy
 fo
r i
n?
vo
ca
bu
la
ry
 w
or
ds
 [%
]
 
 
Morphs
Words
(a)
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA?100
?80
?60
?40
?20
0
20
40
60
80
100
76.671.9
79.1
67.8
89.8
82.2
92.8
66.7
51.9
68.667.466.7
61.2
40.141.8
15.1
?74.8
43.2
?62.6
55.1
?76.1
38.0
?47.7
13.2
?21.8
29.2
?19.4
?10.1
?14.6
W
or
d 
ac
cu
ra
cy
 fo
r O
O
Vs
 [%
]
 
 
Morphs
Words
(b)
Figure 3: Word accuracies computed separately for those words in the test sets that are (a) included in and
(b) excluded from the vocabularies of the word vocabulary; cf. figures listed on the row ?OOV test set? in
Table 1. Together these two partitions make up the entire test set vocabulary. For comparison, the results for
the entire sets are shown using gray-shaded bars (also displayed in Figure 2).
in the word model is, the larger the proportion of
words that the morph model can recognize but the
word model cannot, a priori. In addition, the larger
the proportion of OOVs, the more frequent and more
?easily modelable? words are left out of the word
model, and the more successfully these words are
indeed learned by the morph model.
3.2.3 New Words in the Test Set
All words present in the training data (some of
which are OOVs in the word models) ?leave some
trace? in the morph models, in the n-gram statistics
that are collected for morph sequences. How, then,
about new words that occur only in the test set, but
not in the training set? In order to recognize such
words correctly, the model must combine morphs in
ways it has not observed before.
Figure 4 demonstrates that the new unseen words
are very challenging. Now, also the morph mod-
els mostly obtain negative word accuracies, which
means that the number of insertions adjacent to new
words exceeds the number of correctly recognized
new words. The best results are obtained in clean
acoustic conditions (Fin2, Fin3, Fin4) with only few
foreign names, which are difficult to get right using
typical Finnish phoneme-to-grapheme mappings (as
the negative accuracy of Fin1 suggests).
3.3 Vocabulary Growth and Arabic
Figure 5 shows the development of the size of
the vocabulary (unique word forms) for growing
amounts of text in different corpora. The corpora
used for Finnish, Estonian, and Turkish (planned
speech/text), as well as Arabic (spontaneous speech)
are the LM training sets used in the experiments.
Additional sources have been provided for Arabic
and English: Arabic text (planned) from the FBIS
corpus of Modern Standard Arabic (a collection
of transcribed radio newscasts from various radio
stations in the Arabic speaking world), as well as
text from the New York Times magazine (English
planned) and spontaneous transcribed English tele-
phone conversations from the Fisher corpus.
The figure illustrates two points: (1) The faster
the vocabulary growth is, the larger the potential ad-
vantage of morph models is in comparison to stan-
dard word models, because of OOV and data spar-
sity problems. The obtained speech recognition re-
sults seem to support this hypothesis; the applied
morph LMs are clearly beneficial for Finnish and
Estonian, mostly beneficial for Turkish, and slightly
detrimental for ECA. (2) A more slowly growing
vocabulary is used in spontaneous speech than in
planned speech (or written text). Moreover, the
Arabic ?spontaneous? curve is located fairly close
385
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA?100
?80
?60
?40
?20
0
20
40
60
80
100
76.671.9
79.1
67.8
89.8
82.2
92.8
66.7
51.9
68.667.466.7
61.2
40.141.8
?8.7
?93.0
20.7
?75.9
31.0
?81.0
17.9
?32.3
?64.6
?6.1
?24.6
?5.9
?24.5
?10.1
?14.6
W
or
d 
ac
cu
ra
cy
 fo
r u
ns
ee
n 
wo
rd
s 
[%
]
 
 
Morphs
Words
Figure 4: Word accuracies computed for the words
in the test sets that do not occur at all in the train-
ing sets; cf. figures listed on the row ?new words
in test set? in Table 1. For comparison, the gray-
shaded bars show the corresponding results for the
entire test sets (also displayed in Figure 2).
to the English ?planned? curve and much below
the Finnish, Estonian, and Turkish curves. Thus,
even though Arabic is considered a ?morphologi-
cally rich? language, this is not manifested through
a considerable vocabulary growth (and high OOV
rate) in the Egyptian Colloquial Arabic data used in
the current speech recognition experiments. Conse-
quently, it may not be that surprising that the morph
model did not work particularly well for Arabic.
Arabic words consist of a stem surrounded by pre-
fixes and suffixes, which are fairly successfully seg-
mented out by Morfessor. However, Arabic also
has templatic morphology, i.e., the stem is formed
through the insertion of a vowel pattern into a ?con-
sonantal skeleton?.
Additional experiments have been performed us-
ing the ECA data and Factored Language Models
(FLMs) (Kirchhoff et al, 2006). The FLM is a
powerful model that makes use of several sources
of information, in particular a morphological lexi-
con of ECA. The FLM incorporates mechanisms for
handling templatic morphology, but despite its so-
phistication, it barely outperforms the standard word
model: The word accuracy of the FLM is 42.3 % and
that of the word model is 41.8 %. The speech recog-
nition implementation of both the FLM and the word
0 20 40 60 80 100 120 140 160 1800
5
10
15
20
25
30
35
40
45
50
Corpus size [1000 words]
Un
iq
ue
 w
or
ds
 [1
00
0 w
ord
s]
Fin
nish
 (pla
nned
)
Est
onia
n (pl
anne
d)
Turk
ish (pl
anned
)
Arabic 
(spontan
eous)Ara
bic (plann
ed)
English (plann
ed)
English (spontaneous)
Figure 5: Vocabulary growth curves for the differ-
ent corpora of spontaneous and planned speech (or
written text). For growing amounts of text (word
tokens) the number of unique different word forms
(word types) occurring in the corpus are plotted.
model is based on whole words (although subword
units are used for assigning probabilities to word
forms in the FLM). This contrasts these models with
the morph model, which splits words into subword
units also in the speech recognition implementation.
It seems that the splitting is a source of errors in this
experimental setup with very little data available.
4 Discussion
Alternative morph-based and word-based ap-
proaches exist. We have tried some, but none of
them has outperformed the described morph models
for Finnish, Estonian, and Turkish, or the word and
FLM models for Egyptian Arabic (in a statistically
significant way). The tested models comprise
more linguistically accurate morph segmentations
obtained using later Morfessor versions (Categories-
ML and Categories-MAP) (Creutz, 2006), as well
as analyses obtained from morphological parsers.
Hybrids, i.e., word models augmented with
phonemes or other subword units have been pro-
posed (Bazzi and Glass, 2000; Galescu, 2003;
Bisani and Ney, 2005). In our experiments, such
models have outperformed the standard word mod-
els, but not the morph models.
Simply growing the word vocabulary to cover the
386
entire vocabulary of large training corpora could be
one (fairly ?brute-force?) approach, but this is hardly
feasible for languages such as Finnish. The en-
tire Finnish LM training data of 150 million words
(used in Fin4) contains more than 4 million unique
word forms, a value ten times the size of the rather
large word lexicon currently used. And even if a 4-
million-word lexicon were to be used, the OOV rate
of the test set would still be relatively high: 1.5 %.
Judging by the Arabic experiments, there seems
to be some potential in Factored Language Models.
The FLMs might work well also for the other lan-
guages, and in fact, to do justice to the more ad-
vanced morph models from later versions of Mor-
fessor, FLMs or some other refined techniques may
be necessary as a complement to the currently used
standard n-grams.
Acknowledgments
We are most grateful to Katrin Kirchhoff and Dimitra Vergyri
for their valuable help on issues related to Arabic, and to the EU
AMI training program for funding part of this work. The work
was also partly funded by DARPA under contract No. HR0011-
06-C-0023 (approved for public release, distribution is unlim-
ited). The views herein are those of the authors and do not nec-
essarily reflect the views of the funding agencies.
References
I. Bazzi and J. R. Glass. 2000. Modeling out-of-vocabulary
words for robust speech recognition. In Proc. ICSLP, Bei-
jing, China.
A. Berton, P. Fetter, and P. Regel-Brietzmann. 1996. Com-
pound words in large-vocabulary German speech recognition
systems. In Proc. ICSLP, pp. 1165?1168, Philadelphia, PA,
USA.
M. Bisani and H. Ney. 2005. Open vocabulary speech recog-
nition with flat hybrid models. In Proc. Interspeech, Lisbon,
Portugal.
M. R. Brent. 1999. An efficient, probabilistically sound algo-
rithm for segmentation and word discovery. Machine Learn-
ing, 34:71?105.
W. Byrne, J. Hajic?, P. Ircing, F. Jelinek, S. Khudanpur, P. Kr-
bec, and J. Psutka. 2001. On large vocabulary continuous
speech recognition of highly inflectional language ? Czech.
In Proc. Eurospeech, pp. 487?489, Aalborg, Denmark.
S. F. Chen and J. Goodman. 1999. An empirical study of
smoothing techniques for language modeling. Computer
Speech and Language, 13:359?394.
M. Creutz and K. Lagus. 2002. Unsupervised discovery of
morphemes. In Proc. ACL SIGPHON, pp. 21?30, Philadel-
phia, PA, USA.
M. Creutz. 2006. Induction of the Morphology of Natural
Language: Unsupervised Morpheme Segmentation with Ap-
plication to Automatic Speech Recognition. Ph.D. thesis,
Helsinki University of Technology. http://lib.tkk.fi/
Diss/2006/isbn9512282119/.
C. G. de Marcken. 1996. Unsupervised Language Acquisition.
Ph.D. thesis, MIT.
L. Galescu. 2003. Recognition of out-of-vocabulary words
with sub-lexical language models. In Proc. Eurospeech, pp.
249?252, Geneva, Switzerland.
P. Geutner, M. Finke, and P. Scheytt. 1998. Adaptive vocabu-
laries for transcribing multilingual broadcast news. In Proc.
ICASSP, pp. 925?928, Seattle, WA, USA.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Linguistics,
27(2):153?198.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2006. Contex-
tual dependencies in unsupervised word segmentation. In
Proc. Coling/ACL, pp. 673?680, Sydney, Australia.
T. Hirsima?ki, M. Creutz, V. Siivola, M. Kurimo, S. Virpioja, and
J. Pylkko?nen. 2006. Unlimited vocabulary speech recogni-
tion with morph language models applied to Finnish. Com-
puter Speech and Language, 20(4):515?541.
K. Kirchhoff, D. Vergyri, J. Bilmes, K. Duh, and A. Stol-
cke. 2006. Morphology-based language modeling for Ara-
bic speech recognition. Computer Speech and Language,
20(4):589?608.
M. Kurimo, M. Creutz, M. Varjokallio, E. Ar?soy, and
M. Sarac?lar. 2006a. Unsupervised segmentation of words
into morphemes ? Morpho Challenge 2005, Application to
automatic speech recognition. In Proc. Interspeech, Pitts-
burgh, PA, USA.
M. Kurimo, A. Puurula, E. Ar?soy, V. Siivola, T. Hirsima?ki,
J. Pylkko?nen, T. Aluma?e, and M. Sarac?lar. 2006b. Un-
limited vocabulary speech recognition for agglutinative lan-
guages. In Proc. NAACL-HLT, New York, USA.
O.-W. Kwon and J. Park. 2003. Korean large vocabulary con-
tinuous speech recognition with morpheme-based recogni-
tion units. Speech Communication, 39(3?4):287?300.
M. Larson, D. Willett, J. Koehler, and G. Rigoll. 2000. Com-
pound splitting and lexical unit recombination for improved
performance of a speech recognition system for German par-
liamentary speeches. In Proc. ICSLP.
M. Mohri and M. D. Riley. 2002. DCD library, Speech
recognition decoder library. AT&T Labs Research. http:
//www.research.att.com/sw/tools/dcd/.
R. Ordelman, A. van Hessen, and F. de Jong. 2003. Compound
decomposition in Dutch large vocabulary speech recogni-
tion. In Proc. Eurospeech, pp. 225?228, Geneva, Switzer-
land.
J. Rissanen. 1989. Stochastic complexity in statistical inquiry.
World Scientific Series in Computer Science, 15:79?93.
I. Shafran and K. Hall. 2006. Corrective models for speech
recognition of inflected languages. In Proc. EMNLP, Syd-
ney, Australia.
V. Siivola and B. Pellom. 2005. Growing an n-gram model. In
Proc. Interspeech, pp. 1309?1312, Lisbon, Portugal.
A. Stolcke. 1998. Entropy-based pruning of backoff language
models. In Proc. DARPA BNTU Workshop, pp. 270?274,
Lansdowne, VA, USA.
A. Stolcke. 2002. SRILM ? an extensible language modeling
toolkit. In Proc. ICSLP, pp. 901?904. http://www.speech.
sri.com/projects/srilm/.
E. W. D. Whittaker and P. C. Woodland. 2000. Particle-based
language modelling. In Proc. ICSLP, pp. 170?173, Beijing,
China.
S. Young, D. Ollason, V. Valtchev, and P. Woodland. 2002.
The HTK book (for version 3.2 of HTK). University of Cam-
bridge.
387
Proceedings of NAACL HLT 2009: Short Papers, pages 193?196,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Analysing Recognition Errors in Unlimited-Vocabulary Speech Recognition
Teemu Hirsima?ki and Mikko Kurimo
Adaptive Informatics Research Centre
Helsinki University of Technology
P.O. Box 5400, 02015, TKK, Finland
teemu.hirsimaki@tkk.fi
Abstract
We analyze the recognition errors made by
a morph-based continuous speech recognition
system, which practically allows an unlim-
ited vocabulary. Examining the role of the
acoustic and language models in erroneous
regions shows how speaker adaptive training
(SAT) and discriminative training with mini-
mum phone frame error (MPFE) criterion de-
crease errors in different error classes. An-
alyzing the errors with respect to word fre-
quencies and manually classified error types
reveals the most potential areas for improving
the system.
1 Introduction
Large vocabulary speech recognizers have become
very complex. Understanding how the parts of the
system affect the results separately or together is far
from trivial. Still, analyzing the recognition errors
may suggest how to reduce the errors further.
There exist previous work on analyzing recogni-
tion errors. Chase (1997) developed error region
analysis (ERA), which reveals whether the errors
are due to acoustic or language models. Greenberg
et al (2000) analyzed errors made by eight recog-
nition systems on the Switchboard corpus. The er-
rors correlated with the phone misclassification and
speech rate, and conclusion was that the acoustic
front ends should be improved further. Duta et al
(2006) analyzed the main errors made by the 2004
BBN speech recognition system. They showed that
errors typically occur in clusters and differ between
broadcast news (BN) and conversational telephone
speech (CTS) domains. Named entities were a com-
mon cause for errors in the BN domain, and hesita-
tion, repeats and partially spoken words in the CTS
domain.
This paper analyzes the errors made by a Finnish
morph-based continuous recognition system (Hir-
sima?ki et al, 2009). In addition to partitioning the
errors using ERA, we compare the number of let-
ter errors in different regions and analyze what kind
of errors are corrected when speaker adaptive train-
ing and discriminative training are taken in use. The
most potential error sources are also studied by par-
titioning the errors according to manual error classes
and word frequencies.
2 Data and Recognition System
The language model training data used in the experi-
ments consist of 150 million words from the Finnish
Kielipankki corpus. Before training the n-gram
models, the words of the training data were split
into morphs using the Morfessor algorithm, which
has been shown to improve Finnish speech recogni-
tion (Hirsima?ki et al, 2006). The resulting morph
lexicon contains 50 000 distinct morphs. A growing
algorithm (Siivola et al, 2007) was used for training
a Kneser-Ney smoothed high-order variable-length
n-gram model containing 52 million n-grams.
The acoustic phoneme models were trained on the
Finnish SpeechDat telephone speech database: 39
hours from 3838 speakers for training, 46 minutes
from 79 speakers for development and another simi-
lar set for evaluation. Only full sentences were used
and sentences with severe noise or mispronuncia-
tions were removed.
193
AM score
LM score
LM score
AM score
Hyp.
Ref. tiedon
tiedon valta
valta tie
tien
mullista
mullista
a
a
#
#
#
#
?423
?127
?10.8
?6.62
?136
?39.7
?114
?33.0
?15.3
?0.01
?269
?181
?36.5
?18.7
?36.5
?18.7
?242
?203
?11.1
?1.55
?133
?12.9
?136
?39.7
?10.8
?6.62
?423
?127
AM: ?398.3  LM: ?214.01  TOT: ?612.31
AM: ?386.1  LM: ?217.45  TOT: ?603.55
Figure 1: An example of a HYP-AM error region. The
scores are log probabilities. Word boundaries are denoted
by ?#?. The error region only contains one letter error (an
inserted ?n?).
The acoustic front-end consist of 39-dimensional
feature vectors (Mel-frequency cepstral coefficients
with first and second time-derivatives), global max-
imum likelihood linear transform, decision-tree tied
HMM triphones with Gaussian mixture models, and
cepstral mean subtraction.
Three models are trained: The first one is a max-
imum likelihood (ML) model without any adap-
tation. The second model (ML+SAT) enhances
the ML model with three iterations of speaker
adaptive training (SAT) using constrained maxi-
mum likelihood linear regression (CMLLR) (Gales,
1998). In recognition, unsupervised adaptation
is applied in the second pass. The third model
(ML+SAT+MPFE) adds four iterations of discrim-
inative training with minimum phone frame error
(MPFE) criterion (Zheng and Stolcke, 2005) to the
ML+SAT model.
3 Analysis
3.1 Error Region Analysis
Error Region Analysis (Chase, 1997) can be used
to find out whether the language model (LM), the
acoustic model (AM) or both can be blamed for
an erroneous region in the recognition output. Fig-
ure 1 illustrates the procedure. For each utter-
ance, the final hypothesis is compared to the forced
alignment of the reference transcript and segmented
into correct and error regions. An error region is
a contiguous sequence of morphs that differ from
the corresponding reference morphs with respect to
morph identity, boundary time-stamps, AM score,
Letter errors
Region ML ML+SAT ML+SAT+MPFE
HYP-BOTH 962 909 783
HYP-AM 1059 709 727
HYP-LM 623 597 425
REF-TOT 82 60 15
Total 2726 2275 1950
LER (%) 6.8 5.6 4.8
Table 1: SpeechDat: Letter errors for different training
methods and error regions. The reference transcript con-
tains 40355 letters in total.
LM score, or n-gram history1.
By comparing the AM and LM scores in the hy-
pothesis and reference regions, the regions can be
divided in classes. We denote the recognition hy-
pothesis as HYP, and the reference transcript as REF.
The relevant classes for the analysis are the follow-
ing. REF-TOT: the reference would have better to-
tal score, but it has been erroneously pruned. HYP-
AM: the hypothesis has better score, but only AM
favors HYP over REF. HYP-LM: the hypothesis has
better score, but only LM favors HYP over REF.
HYP-BOTH: both the AM and LM favor HYP.
Since the error regions are independent, the let-
ter error rate2 (LER) can be computed separately for
each region. Table 1 shows the error rates for three
different acoustic models: ML training, ML+SAT,
andML+SAT+MPFE.We see that SAT decreases all
error types, but the biggest reduction is in the HYP-
AM class. This should be expected. In the ML case,
the Gaussian mixtures contain much variance due to
different unnormalized speakers, and since the test
set contains only unseen speakers, many errors are
expected for some speakers. Adapting the models to
the test set is expected to increase the acoustic score
of the reference transcript, and since in the HYP-AM
regions the LM already prefers REF, corrections be-
cause of SAT are most probable there.
On the other hand, adding MPFE after SAT seems
1A region may be defined as an error region even if the tran-
scription is correct (only the segmentation differs). However,
since we are going to analyze the number of letter errors in the
error regions, the ?correct? error regions do not matter.
2The words in Finnish are often long and consist of several
morphs, so the performance is measured in letter errors instead
of word errors to have finer resolution for the results.
194
Letter errors
Class label Total HYP-BOTH HYP-AM HYP-LM REF-TOT Class description
Foreign 156 89 61 6 Foreign proper name
Inflect 143 74 26 43 Small error in inflection
Poor 131 37 84 10 Poor pronunciation or repair
Noise 124 21 97 6 Error segment contains some noise
Name 81 29 29 23 Finnish proper name
Delete 65 29 9 27 Small word missing
Acronym 53 44 6 3 Acronym
Compound 42 11 8 23 Word boundary missing or inserted
Correct 37 15 19 3 Hypothesis can be considered correct
Rare 27 11 3 13 Reference contains a very rare word
Insert 9 3 6 Small word inserted incorrectly
Other 1082 421 379 277 5 Other error
Table 2: Manual error classes and the number of letter errors for the ML+SAT+MPFE system.
to reduce HYP-BOTH and HYP-LM errors, but not
HYP-AM errors. The number of search errors (REF-
TOT) also decreases.
All in all, for all models, there seems to be more
HYP-AM errors than HYP-LM errors. Chase (1997)
lists the following possible reasons for the HYP-
AM regions: noise, speaker pronounces badly, pro-
nunciation model is poor, some phoneme models
not trained to discriminate, or reference is plainly
wrong. The next section studies these issues further.
3.2 Manual Error Classification
Next, the letter errors in the error regions were
manually classified according to the most probable
cause. Table 2 shows the classes, the total number
of letter errors for each class, and the errors divided
to different error region types.
All errors that did not seem to have an obvious
cause are put under the class Other. Some of the er-
rors were a bit surprising, since the quality of the
audio and language seemed perfectly normal, but
still the recognizer got the sentences wrong. On the
other hand, the class also contains regions where the
speech is very fast or the signal level is quite low.
The largest class with a specific cause is Foreign,
which contains about 8 % of all letter errors. Cur-
rently, the morph based recognizer does not have
any foreign pronunciation modeling, so it is natural
that words like Ching, Yem Yung, Villeneuve, Schu-
macher, Direct TV, Thunderbayssa are not recog-
nized correctly, since the mapping between the writ-
ten form and pronunciation does not follow the nor-
mal Finnish convention. In Table 2 we see, that the
acoustic model prefers the incorrect hypothesis in al-
most all cases. A better pronunciation model would
be essential to improve the recognition. However,
integrating exceptions in pronunciation to morph-
based recognition is not completely straightforward.
Another difficulty with foreign names is that they
are often rare words, so they will get low language
model probability anyway.
The errors in the Acronym class are pretty much
similar to foreign names. Since the letter-by-letter
pronunciation is not modelled, the acronyms usually
cause errors.
The next largest class is Inflect, which contains
errors where the root of the word is correctly rec-
ognized, but the inflectional form is slightly wrong
(for example: autolla/autolle, kirjeeksi/kirjeiksi). In
these errors, it is usually the language model that
prefers the erroneous hypothesis.
The most difficult classes to improve are perhaps
Poor and Noise. For bad pronunciations and repairs
it is not even clear what the correct answer should
be. Should it be the word the speaker tried to say,
or the word that was actually said? As expected, the
language model would have preferred the correct hy-
pothesis in most cases, but the acoustic model have
chosen the wrong hypothesis.
The Name and Rare are also difficult classes.
Contrary to the foreign names and acronyms, the
pronunciation model is not a problem.
195
05000
10000
Le
tte
rs
 in
 re
fe
re
nc
e
0
200
400
Le
tte
r e
rro
rs
0?1 1?3 3?7 7?15  ?31  ?63  ?127  ?255  ?511 ?4116 New
0
5
10
15
Le
tte
r e
rro
r r
at
e 
(%
)
Subset of training data vocabulary (x 1000)
Figure 2: Frequency analysis of the SAT+MPFE system.
Number of letters in reference (top), number of letter er-
rors (middle), and letter error rate (bottom) partitioned
according to word frequencies. The leftmost bar corre-
sponds to the 1000 most frequent words, the next bar to
the 2000 next frequent words, and so on. The rightmost
bar corresponds to words not present in the training data.
The Compound errors are mainly in HYP-LM re-
gions, which is natural since there is usually lit-
tle acoustic evidence at the word boundary. Fur-
thermore, it is sometimes difficult even for humans
to know if two words are written together or not.
Sometimes the recognizer made a compound word
error because the compound word was often written
incorrectly in the language model training data.
3.3 Frequency Analysis
In order to study the effect of rare words in more de-
tail, the words in the test data were grouped accord-
ing their frequencies in the LM training data: The
first group contained all the words that were among
the 1000 most common words, the next group con-
tained the next 2000 words, then 4000, and so on,
until the final group contained all words not present
in the training data.
Figure 2 shows the number of letters in the ref-
erence (top), number of letter errors (middle), and
letter error rate (bottom) for each group. Quite ex-
pectedly, the error rates (bottom) rise steadily for the
infrequent words and is highest for the new words
that were not seen in the training data. But looking
at the absolute number of letter errors (middle), the
majority occur in the 1000 most frequent words.
4 Conclusions
SAT and MPFE training seem to correct different
error regions: SAT helps when the acoustic model
dominates and MPFE elsewhere. The manual error
classification suggests that improving the pronunci-
ation modeling of foreign words and acronyms is a
potential area for improvement. The frequency anal-
ysis shows that a major part of the recognition errors
occur still in the 1000 most common words. One
solution might be to develop methods for detecting
when the problem is in acoustics and to trust the lan-
guage model more in these regions.
Acknowledgments
This work was partly funded from the EC?s FP7
project EMIME (213845).
References
Lin Chase. 1997. Error-Responsive Feedback Mecha-
nisms for Speech Recognizers. Ph.D. thesis, Robotics
Institute, Carnegie Mellon University.
Nicolae Duta, Richard Schwartz, and John Makhoul.
2006. Analysis of the errors produced by the 2004
BBN speech recognition system in the DARPA EARS
evaluations. IEEE Trans. Audio, Speech Lang. Pro-
cess., 14(5):1745?1753.
M. J. F. Gales. 1998. Maximum likelihood linear trans-
formations for HMM-based speech recognition. Com-
puter Speech and Language, 12(2):75?98.
Steven Greenberg, Shuangyu Chang, and Joy Hollen-
back. 2000. An introduction to the diagnostic eval-
uation of the Switchboard-corpus automatic speech
recognition systems. In Proc. NIST Speech Transcrip-
tion Workshop.
Teemu Hirsima?ki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja, and Janne Pylkko?nen. 2006.
Unlimited vocabulary speech recognition with morph
language models applied to Finnish. Computer Speech
and Language, 20(4):515?541.
Teemu Hirsima?ki, Janne Pylkko?nen, and Mikko Kurimo.
2009. Importance of high-order n-gram models in
morph-based speech recognition. IEEE Trans. Audio,
Speech Lang. Process., 17(4):724?732.
Vesa Siivola, Teemu Hirsima?ki, and Sami Virpioja. 2007.
On growing and pruning Kneser-Ney smoothed n-
gram models. IEEE Trans. Audio, Speech Lang. Pro-
cess., 15(5):1617?1624.
Jing Zheng and Andreas Stolcke. 2005. Improved dis-
criminative training using phone lattices. In Proc. In-
terspeech, pages 2125?2128.
196
Proceedings of NAACL HLT 2009: Demonstrations, pages 13?16,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Morpho Challenge - Evaluation of algorithms for unsupervised learning of
morphology in various task s and languages
Mik k o K urimo, S ami V irpioja, V ille T urunen, T eemu H irsima?k i
Adaptive Informatics Research Centre
H elsink i U niversity of T echnolog y
F I- 0 2 0 15 , T K K , F inland
Firstname.Lastname@tkk.fi
A b strac t
After the release of the open sou rce softw are
implementation of M orfessor alg orithm, a se-
ries of several open evalu ations has b een or-
g aniz ed for u nsu pervised morpheme analy -
sis and morpheme-b ased speech recog nition
and information retrieval. T he u nsu pervised
morpheme analy sis is a particu larly attrac-
tive approach for speech and lang u ag e tech-
nolog y for the morpholog ically complex lan-
g u ag es. W hen the amou nt of distinct w ord
forms b ecomes prohib itive for the constru c-
tion of a su ffi cient lex icon, it is important
that the w ords can b e seg mented into smaller
meaning fu l lang u ag e modeling u nits. In this
presentation w e w ill demonstrate the resu lts
of the evalu ations, the b aseline sy stems b u ilt
u sing the open sou rce tools, and invite re-
search g rou ps to participate in the nex t eval-
u ation w here the task is to enhance statistical
machine translation b y morpheme analy sis.
A proposal for a T y pe I I D emo
1 Ex tended A b strac t
1 .1 T he segmentation of w ords into
morphemes
O ne of the fu ndamental task s in natu ral lang u ag e
processing applications, su ch as larg e-vocab u lary
speech recog nition (L V CS R), statistical machine
translation (S M T ) and information retrieval (IR),
is the morpholog ical analy sis of w ords. It is par-
ticu larly important for the morpholog ically com-
plex lang u ag es, w here the amou nt of different
w ord forms is su b stantially increased b y infl ection,
derivation and composition. T he decomposition of
w ords is req u ired not only for u nderstanding the sen-
tence, b u t in many lang u ag es also for ju st represent-
ing the lang u ag e b y any tractab le and trainab le sta-
tistical model and lex icon. T he manu ally composed
ru le-b ased morpholog ical analy z ers can solve these
prob lems to some ex tent, b u t only a fraction of the
ex isting lang u ag es have b een covered so far, and for
many the coverag e of the relevant content is insu ffi -
cient.
T he ob jective of the M orpho Challeng e1 is to de-
sig n and evalu ate new u nsu pervised statistical ma-
chine learning alg orithms that discover w hich mor-
phemes (smallest individu ally meaning fu l u nits of
lang u ag e) w ords consist of. T he g oal is to discover
b asic vocab u lary u nits su itab le for different task s,
su ch as L V CS R, S M T and IR. In u nsu pervised learn-
ing the list of morphemes is not pre-specifi ed for
each lang u ag e, b u t the optimal morpheme lex icon
and morpheme analy sis of all different w ord forms
is statistically optimiz ed from a larg e tex t corpu s in
a completely data-driven manner.
T he evalu ation of the morpheme analy sis alg o-
rithms is performed b oth b y a ling u istic and an ap-
plication oriented task . T he analy sis ob tained for
a long list of w ords is fi rst compared to the lin-
g u istic g old standard representing a g rammatically
correct analy sis b y verify ing that the morpheme-
sharing w ord pairs are the correct ones (K u rimo et
al., 2 0 0 7 ) . T his is repeated in different lang u ag es
and then the ob tained decomposition of w ords is
applied in state-of-the-art sy stems ru nning variou s
1S ee http://w w w .cis.hu t.fi /morphochalleng e2 0 0 9 /
13
NLP applications. The suitability of the morphemes
is v erifi ed by comparing the performance of the sys-
tems to each other and to systems using unprocessed
w ord s or conv entional w ord processing alg orithms
lik e stemming or rule-based d ecompositions.
A s a baseline method in all application, w e hav e
built systems by applying the M orfessor alg orithm,
w hich is an unsuperv ised w ord d ecomposition alg o-
rithm d ev eloped at our research g roup (C reutz and
Lag us, 20 0 2) and released as open source softw are
implementation2.
1.2 Morphemes in Information Retrieval
In information retriev al ( I R ) from tex t d ocuments a
typical task is to look for the most relev ant d ocu-
ments for a g iv en q uery. O ne of the k ey challeng es
is to red uce all the infl ected w ord forms to a common
root or stem for effectiv e ind ex ing . F rom the mor-
pheme analysis point of v iew this task is to d ecom-
pose all the w ord s in the q uery and tex t d ocuments
and fi nd out those common morphemes w hich form
the most relev ant link s.
In M orpho C halleng e the IR systems built using
the unsuperv ised morpheme analysis alg orithms are
compared in state-of-the-art C LE F task s in F innish,
G erman and E ng lish (K urimo and Turunen, 20 0 8 )
using the mean av erag e precision metric. The results
are also compared to those obtained by the g rammat-
ical morphemes as w ell as the stemming and w ord
normaliz ation method s conv entionally used in IR .
1.3 Morphemes in S peec h Rec og nition
In larg e-v ocabulary continuous speech recog nition
(LV C S R ) one k ey part of the process is the statis-
tical lang uag e mod eling w hich d etermines the prior
probabilities of all the possible w ord seq uences. A n
especially challeng ing task is to cov er all the pos-
sible w ord forms w ith suffi cient accuracy, because
any out-of-v ocabulary w ord s w ill not only be nev er
correctly recog niz ed , but also sev erely d eg rad e the
mod eling of the other nearby w ord s. B y d ecompos-
ing the w ord s into meaning ful sub-w ord units, such
as morphemes, larg e-v ocabulary lang uag e mod els
can be successfully built ev en for the most d iffi cult
ag g lutinativ e lang uag es, lik e F innish, E stonian and
Turk ish (K urimo et al, 20 0 6 b).
2S ee http://w w w .cis.hut.fi /projects/morpho/
In M orpho C halleng e the unsuperv ised mor-
pheme alg orithms hav e been compared by using
the morphemes to train statistical lang uag e mod els
and applying the mod els in state-of-the-art LV C S R
task s in F innish and Turk ish (K urimo et al, 20 0 6 a) .
B enchmark s for the same task s w ere obtained by
mod els that utiliz e the g rammatical morphemes as
w ell as trad itional w ord -based lang uag e mod els.
1.4 Morphemes in Mac hine T ranslation
The state-of-the-art statistical machine translation
(S M T) systems are affected by the morpholog ical
v ariation of w ord s at tw o d ifferent stag es (V irpi-
oja et al, 20 0 7 ) . In the fi rst stag e, the alig nment
of the source and targ et lang uag e w ord s in a par-
allel training corpus and the training of the transla-
tion mod el can benefi t from the d ecomposition of
complex w ord s into morphemes. This is particularly
important w hen either the targ et or the source lan-
g uag e, or both, are morpholog ically complex . The
fi nal stag e w here the targ et lang uag e tex t is g ener-
ated , may also req uire morpheme-based mod els, be-
cause the larg e-v ocabulary statistical lang uag e mod -
els are applied in the same w ay as in LV C S R .
In the on-g oing M orpho C halleng e 20 0 9 compe-
tition, the morpheme analysis alg orithms are com-
pared in S M T task s, w here the analysis is need ed
for the source lang uag e tex ts. The E uropean Par-
liament parallel corpus (K oehn, 20 0 5 ) is used in
the ev aluation. The source lang uag es are F innish
and G erman and the targ et in both task s is E ng lish.
To obtain a state-of-the-art performance in the task s
the morpheme-based S M T w ill be combined w ith a
w ord -based S M T using the M inimum B ayes R isk
( M B R ) interpolation of the N-best translation hy-
pothesis of both systems (d e G ispert et al, 20 0 9 ) .
1.5 Morpho C halleng e 20 0 9
A s its pred ecessors, the M orpho C halleng e 20 0 9
competition is open to all and free of charg e. The
participants? are ex pected to use their unsuperv ised
machine learning alg orithms to analyz e the w ord
lists of d ifferent lang uag es prov id ed by the org aniz -
ers and submit the results of their morpheme analy-
sis. The org aniz ers w ill then run the ling uistic ev al-
uations and build the IR and S M T systems and pro-
v id e all the results and comparisons of the d ifferent
systems. The participated alg orithms and ev aluation
14
results will be presented at the Morpho Challenge
work shop that is c urrently planned to tak e plac e
within the H L T - N A A CL 2 0 1 0 c onferenc e.
Acknowledgments
T he Morpho Challenge c om petitions and work shops
are part of the E U N etwork of E x c ellenc e P A S CA L
Challenge program and organiz ed in c ollaboration
with CL E F . W e are grateful to Mathias Creutz , E bru
A risoy , S tefan B ordag, N iz ar H abash and Majdi
S awalha for c ontributions in proc essing the training
data and c reating the gold standards. T he A c adem y
of F inland has supported the work in the projec ts
Adaptive Informatics and N ew adaptive and learn-
ing meth ods in speech recog nition.
R efer ences
M. Creutz and K . L agus. 2 0 0 2 . U nsuperv ised disc ov ery
of m orphem es. In W ork sh op on M orph olog ical and
P h onolog ical L earning of AC L - 0 2 .
A . de G ispert, S . V irpioja, M. K urim o, and W . B y rne.
2 0 0 9 . Minim um B ay es risk c om bination of translation
hy potheses from alternativ e m orphologic al dec om po-
sitions. S ubm itted to H L T - N AAC L .
P . K oehn. 2 0 0 5 . E uroparl: A parallel c orpus for statisti-
c al m ac hine translation. In M T S u mmit X .
M. K urim o and V . T urunen. 2 0 0 8 . U nsuperv ised m or-
phem e analy sis ev aluation by IR ex perim ents ? Mor-
pho Challenge 2 0 0 8 . In C L E F .
M. K urim o, M. Creutz , M. V arjok allio, E . A risoy , and M.
S arac lar. 2 0 0 6 a. U nsuperv ised segm entation of words
into m orphem es - Challenge 2 0 0 5 , an introduc tion and
ev aluation report. In P AS C AL C h alleng e W ork sh op on
U nsu pervised seg mentation of w ords into morph emes.
M. K urim o, A . P uurula, E . A risoy , V . S iiv ola, T . H ir-
sim a?k i, J . P y lk k o?nen, T . A lum a?e, and M. S arac lar.
2 0 0 6 b. U nlim ited v oc abulary speec h rec ognition for
agglutinativ e languages. In H L T - N AAC L .
M. K urim o, M. Creutz , and M. V arjok allio. 2 0 0 7 . Mor-
pho Challenge ev aluation using a linguistic G old S tan-
dard. In C L E F .
S . V irpioja, J . J . V a?y ry nen, M. Creutz , and M. S adeniem i.
2 0 0 7 . Morphology -aware statistic al m ac hine transla-
tion based on m orphs induc ed in an unsuperv ised m an-
ner. In M T S u mmit X I. D enm ark .
2 S cr ip t ou tline for th e demo p r esenta tion
In this dem o we will present the ac hiev em ents of the
Morpho Challenge 2 0 0 5 - 2 0 0 8 c om petition in graphs
and the baseline sy stem s for v arious languages de-
v eloped using the Morfessor algorithm for word de-
c om position, I R , L V CS R and S MT . T he audienc e
will also be welc om e to try their own input for these
baseline sy stem s and v iew the results.
T he sc ript is presented below for a poster-sty le
and try -it- y ourself on laptop dem o, but it will work
well as a lec ture-sty le show, too, if needed.
In the poster we illustrate the following points:
1 . B asic c harac teristic s of the unsuperv ised learn-
ing algorithm s and m orphem e analy sis results
in different languages (F innish, T urk ish, G er-
m an, E nglish, A rabic ) as in T able 1 , dem o:
h ttp://w w w .cis.h u t.fi /projects/morph o/.
2 . T he results of the ev aluations against the lin-
guistic gold standard m orphem es in different
languages, see e.g. F igure 1 .
3 . T he results of the IR ev aluations and c om par-
isons to the perform anc e of gram m atic al m or-
phem es, word-based m ethods and stem m ing in
different languages, see e.g. F igure 2 .
4 . T he results of the L V CS R ev aluations with
c om parisons to gram m atic al m orphem es and
word-based m ethods, see e.g. F igure 3 .
5 . T he c all for partic ipation in the Morpho Chal-
lenge 2 0 0 9 c om petition where the new ev alua-
tion task is using m orphem es in S MT .
F igure 1 : F - m easures for the T urk ish m orphem e analy sis.
T he laptop is used to dem onstrate the baseline
sy stem s we hav e rec ently dev eloped for different
task s that are all based on unsuperv ised m orphem es:
15
Example word M orfes s or an aly s is G old S tan dard
Finnish: lin u xiin lin u x + iin lin u x N + I L L
T u r k ish: popU lerliG in i pop + U + ler + liG in i popU ler + D ER lH g + P O S 2 S + A C C ,
popU ler + D ER lH g + P O S 3 + A C C 3
A r a b ic : A lmtH dp A l+ mtH d + p mu t aH idap P O S :P N A l+ + S G ,
mu t aH id P O S :A J A l+ + S G
G e r m a n: z u ru ec k z u b eh alten z u ru ec k + z u + b e+ h alten z u ru ec k B z u b e h alt V + I N F
E ng lish: b ab y - s itters b ab y - + s itter + s b ab y N s it V er s + P L
T ab le 1 : M orph eme an aly s is examples in differen t lan g u ag es .
F ig u re 2 : P rec is ion performan c es for th e G erman IR .
F ig u re 3 : L V C S R error rates for th e T u rk is h tas k .
1 . O n lin e L V C S R s y s tem for h ig h ly ag g lu tin ativ e
lan g u ag es , s ee e.g . s c reen s h ot in F ig u re 4 .
2 . O n lin e I R s y s tem for h ig h ly ag g lu tin ativ e lan -
g u ag es .
3 . O n lin e S M T s y s tem wh ere th e s ou rc e lan g u ag e
is a h ig h ly ag g lu tin ativ e lan g u ag e, s ee e.g .
s c reen s h ot in F ig u re 5 .
F ig u re 4 : S c reen s h ot of th e morph eme-b as ed s peec h rec -
og n iz er in ac tion for F in n is h . A n offl in e v ers ion c an b e
tried in http://www.cis.hut.fi/projects/speech/.
F ig u re 5 : S c reen s h ot of th e morph eme-b as ed mac h in e
tran s lator in ac tion for F in n is h -En g lis h . A s implifi ed web
in terfac e to th e s y s tem is als o av ailab le (pleas e email to
th e au th ors for a lin k ) .
16
Proceedings of the ACL 2010 System Demonstrations, pages 48?53,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Personalising speech-to-speech translation in the EMIME project
Mikko Kurimo1?, William Byrne6, John Dines3, Philip N. Garner3, Matthew Gibson6,
Yong Guan5, Teemu Hirsima?ki1, Reima Karhila1, Simon King2, Hui Liang3, Keiichiro
Oura4, Lakshmi Saheer3, Matt Shannon6, Sayaka Shiota4, Jilei Tian5, Keiichi Tokuda4,
Mirjam Wester2, Yi-Jian Wu4, Junichi Yamagishi2
1 Aalto University, Finland, 2 University of Edinburgh, UK, 3 Idiap Research Institute,
Switzerland, 4 Nagoya Institute of Technology, Japan, 5 Nokia Research Center Beijing, China,
6 University of Cambridge, UK
?Corresponding author: Mikko.Kurimo@tkk.fi
Abstract
In the EMIME project we have studied un-
supervised cross-lingual speaker adapta-
tion. We have employed an HMM statisti-
cal framework for both speech recognition
and synthesis which provides transfor-
mation mechanisms to adapt the synthe-
sized voice in TTS (text-to-speech) using
the recognized voice in ASR (automatic
speech recognition). An important ap-
plication for this research is personalised
speech-to-speech translation that will use
the voice of the speaker in the input lan-
guage to utter the translated sentences in
the output language. In mobile environ-
ments this enhances the users? interaction
across language barriers by making the
output speech sound more like the origi-
nal speaker?s way of speaking, even if she
or he could not speak the output language.
1 Introduction
A mobile real-time speech-to-speech translation
(S2ST) device is one of the grand challenges in
natural language processing (NLP). It involves
several important NLP research areas: auto-
matic speech recognition (ASR), statistical ma-
chine translation (SMT) and speech synthesis, also
known as text-to-speech (TTS). In recent years
significant advance have also been made in rele-
vant technological devices: the size of powerful
computers has decreased to fit in a mobile phone
and fast WiFi and 3G networks have spread widely
to connect them to even more powerful computa-
tion servers. Several hand-held S2ST applications
and devices have already become available, for ex-
ample by IBM, Google or Jibbigo1, but there are
still serious limitations in vocabulary and language
selection and performance.
When an S2ST device is used in practical hu-
man interaction across a language barrier, one fea-
ture that is often missed is the personalization of
the output voice. Whoever speaks to the device in
what ever manner, the output voice always sounds
the same. Producing high-quality synthesis voices
is expensive and even if the system had many out-
put voices, it is hard to select one that would sound
like the input voice. There are many features in the
output voice that could raise the interaction expe-
rience to a much more natural level, for example,
emotions, speaking rate, loudness and the speaker
identity.
After the recent development in hidden Markov
model (HMM) based TTS, it has become possi-
ble to adapt the output voice using model trans-
formations that can be estimated from a small
number of speech samples. These techniques, for
instance the maximum likelihood linear regres-
sion (MLLR), are adopted from HMM-based ASR
where they are very powerful in fast adaptation of
speaker and recording environment characteristics
(Gales, 1998). Using hierarchical regression trees,
the TTS and ASR models can further be coupled
in a way that enables unsupervised TTS adaptation
(King et al, 2008). In unsupervised adaptation
samples are annotated by applying ASR. By elimi-
nating the need for human intervention it becomes
possible to perform voice adaptation for TTS in
almost real-time.
The target in the EMIME project2 is to study
unsupervised cross-lingual speaker adaptation for
S2ST systems. The first results of the project have
1http://www.jibbigo.com
2http://emime.org
48
been, for example, to bridge the gap between the
ASR and TTS (Dines et al, 2009), to improve
the baseline ASR (Hirsima?ki et al, 2009) and
SMT (de Gispert et al, 2009) systems for mor-
phologically rich languages, and to develop robust
TTS (Yamagishi et al, 2010). The next step has
been preliminary experiments in intra-lingual and
cross-lingual speaker adaptation (Wu et al, 2008).
For cross-lingual adaptation several new methods
have been proposed for mapping the HMM states,
adaptation data and model transformations (Wu et
al., 2009).
In this presentation we can demonstrate the var-
ious new results in ASR, SMT and TTS. Even
though the project is still ongoing, we have an
initial version of mobile S2ST system and cross-
lingual speaker adaptation to show.
2 Baseline ASR, TTS and SMT systems
The baseline ASR systems in the project are devel-
oped using the HTK toolkit (Young et al, 2001)
for Finnish, English, Mandarin and Japanese. The
systems can also utilize various real-time decoders
such as Julius (Kawahara et al, 2000), Juicer at
IDIAP and the TKK decoder (Hirsima?ki et al,
2006). The main structure of the baseline sys-
tems for each of the four languages is similar and
fairly standard and in line with most other state-of-
the-art large vocabulary ASR systems. Some spe-
cial flavors for have been added, such as the mor-
phological analysis for Finnish (Hirsima?ki et al,
2009). For speaker adaptation, the MLLR trans-
formation based on hierarchical regression classes
is included for all languages.
The baseline TTS systems in the project utilize
the HTS toolkit (Yamagishi et al, 2009) which
is built on top of the HTK framework. The
HMM-based TTS systems have been developed
for Finnish, English, Mandarin and Japanese. The
systems include an average voice model for each
language trained over hundreds of speakers taken
from standard ASR corpora, such as Speecon
(Iskra et al, 2002). Using speaker adaptation
transforms, thousands of new voices have been
created (Yamagishi et al, 2010) and new voices
can be added using a small number of either su-
pervised or unsupervised speech samples. Cross-
lingual adaptation is possible by creating a map-
ping between the HMM states in the input and the
output language (Wu et al, 2009).
Because the resources of the EMIME project
have been focused on ASR, TTS and speaker
adaptation, we aim at relying on existing solu-
tions for SMT as far as possible. New methods
have been studied concerning the morphologically
rich languages (de Gispert et al, 2009), but for the
S2ST system we are currently using Google trans-
late3.
3 Demonstrations to show
3.1 Monolingual systems
In robust speech synthesis, a computer can learn
to speak in the desired way after processing only a
relatively small amount of training speech. The
training speech can even be a normal quality
recording outside the studio environment, where
the target speaker is speaking to a standard micro-
phone and the speech is not annotated. This differs
dramatically from conventional TTS, where build-
ing a new voice requires an hour or more careful
repetition of specially selected prompts recorded
in an anechoic chamber with high quality equip-
ment.
Robust TTS has recently become possible us-
ing the statistical HMM framework for both ASR
and TTS. This framework enables the use of ef-
ficient speaker adaptation transformations devel-
oped for ASR to be used also for the TTS mod-
els. Using large corpora collected for ASR, we can
train average voice models for both ASR and TTS.
The training data may include a small amount of
speech with poor coverage of phonetic contexts
from each single speaker, but by summing the ma-
terial over hundreds of speakers, we can obtain
sufficient models for an average speaker. Only a
small amount of adaptation data is then required to
create transformations for tuning the average voice
closer to the target voice.
In addition to the supervised adaptation us-
ing annotated speech, it is also possible to em-
ploy ASR to create annotations. This unsu-
pervised adaptation enables the system to use a
much broader selection of sources, for example,
recorded samples from the internet, to learn a new
voice.
The following systems will demonstrate the re-
sults of monolingual adaptation:
1. In EMIME Voice cloning in Finnish and En-
glish the goal is that the users can clone their
own voice. The user will dictate for about
3http://translate.google.com
49
Figure 1: Geographical representation of HTS voices trained on ASR corpora for EMIME projects.
Blue markers show male speakers and red markers show female speakers. Available online via
http://www.emime.org/learn/speech-synthesis/listen/Examples-for-D2.1
10 minutes and then after half an hour of
processing time, the TTS system has trans-
formed the average model towards the user?s
voice and can speak with this voice. The
cloned voices may become especially valu-
able, for example, if a person?s voice is later
damaged in an accident or by a disease.
2. In EMIME Thousand voices map the goal is
to browse the world?s largest collection of
synthetic voices by using a world map in-
terface (Yamagishi et al, 2010). The user
can zoom in the world map and select any
voice, which are organized according to the
place of living of the adapted speaker, to ut-
ter the given sentence. This interactive ge-
ographical representation is shown in Figure
1. Each marker corresponds to an individual
speaker. Blue markers show male speakers
and red markers show female speakers. Some
markers are in arbitrary locations (in the cor-
rect country) because precise location infor-
mation is not available for all speakers. This
geographical representation, which includes
an interactive TTS demonstration of many of
the voices, is available from the URL pro-
vided. Clicking on a marker will play syn-
thetic speech from that speaker4. As well as
4Currently the interactive mode supports English and
Spanish only. For other languages this only provides pre-
being a convenient interface to compare the
many voices, the interactive map is an attrac-
tive and easy-to-understand demonstration of
the technology being developed in EMIME.
3. The models developed in the HMM frame-
work can be demonstrated also in adapta-
tion of an ASR system for large-vocabulary
continuous speech recognition. By utilizing
morpheme-based language models instead of
word-based models the Finnish ASR system
is able to cover practically an unlimited vo-
cabulary (Hirsima?ki et al, 2006). This is
necessary for morphologically rich languages
where, due to inflection, derivation and com-
position, there exists so many different word
forms that word based language modeling be-
comes impractical.
3.2 Cross-lingual systems
In the EMIME project the goal is to learn cross-
lingual speaker adaptation. Here the output lan-
guage ASR or TTS system is adapted from speech
samples in the input language. The results so far
are encouraging, especially for TTS: Even though
the cross-lingual adaptation may somewhat de-
grade the synthesis quality, the adapted speech
now sounds more like the target speaker. Sev-
eral recent evaluations of the cross-lingual speaker
synthesised examples, but we plan to add an interactive type-
in text-to-speech feature in the near future.
50
Figure 2: All English HTS voices can be used as online TTS on the geographical map.
adaptation methods can be found in (Gibson et al,
2010; Oura et al, 2010; Liang et al, 2010; Oura
et al, 2009).
The following systems have been created to
demonstrate cross-lingual adaptation:
1. In EMIME Cross-lingual Finnish/English
and Mandarin/English TTS adaptation the
input language sentences dictated by the user
will be used to learn the characteristics of her
or his voice. The adapted cross-lingual model
will be used to speak output language (En-
glish) sentences in the user?s voice. The user
does not need to be bilingual and only reads
sentences in their native language.
2. In EMIME Real-time speech-to-speech mo-
bile translation demo two users will interact
using a pair of mobile N97 devices (see Fig-
ure 3). The system will recognize the phrase
the other user is speaking in his native lan-
guage and translate and speak it in the native
language of the other user. After a few sen-
tences the system will have the speaker adap-
tation transformations ready and can apply
them in the synthesized voices to make them
sound more like the original speaker instead
of a standard voice. The first real-time demo
version is available for the Mandarin/English
language pair.
3. The morpheme-based translation system for
Finnish/English and English/Finnish can be
compared to a word based translation for
arbitrary sentences. The morpheme-based
approach is particularly useful for language
pairs where one or both languages are mor-
phologically rich ones where the amount and
complexity of different word forms severely
limits the performance for word-based trans-
lation. The morpheme-based systems can
learn translation models for phrases where
morphemes are used instead of words (de
Gispert et al, 2009). Recent evaluations (Ku-
rimo et al, 2009) have shown that the perfor-
mance of the unsupervised data-driven mor-
pheme segmentation can rival the conven-
tional rule-based ones. This is very useful if
hand-crafted morphological analyzers are not
available or their coverage is not sufficient for
all languages.
Acknowledgments
The research leading to these results was partly
funded from the European Communitys Seventh
51
  
ASR SMT TTS
Cross-lingualSpeaker adaptation
Speakeradaptation
input outputspeech
Figure 3: EMIME Real-time speech-to-speech
mobile translation demo
Framework Programme (FP7/2007-2013) under
grant agreement 213845 (the EMIME project).
References
A. de Gispert, S. Virpioja, M. Kurimo, and W. Byrne.
2009. Minimum Bayes risk combination of transla-
tion hypotheses from alternative morphological de-
compositions. In Proc. NAACL-HLT.
J. Dines, J. Yamagishi, and S. King. 2009. Measur-
ing the gap between HMM-based ASR and TTS. In
Proc. Interspeech ?09, Brighton, UK.
M. Gales. 1998. Maximum likelihood linear transfor-
mations for HMM-based speech recognition. Com-
puter Speech and Language, 12(2):75?98.
M. Gibson, T. Hirsima?ki, R. Karhila, M. Kurimo,
and W. Byrne. 2010. Unsupervised cross-lingual
speaker adaptation for HMM-based speech synthe-
sis using two-pass decision tree construction. In
Proc. of ICASSP, page to appear, March.
T. Hirsima?ki, M. Creutz, V. Siivola, M. Kurimo, S.
Virpioja, and J. Pylkko?nen. 2006. Unlimited vo-
cabulary speech recognition with morph language
models applied to finnish. Computer Speech & Lan-
guage, 20(4):515?541, October.
T. Hirsima?ki, J. Pylkko?nen, and M Kurimo. 2009.
Importance of high-order n-gram models in morph-
based speech recognition. IEEE Trans. Audio,
Speech, and Language Process., 17:724?732.
D. Iskra, B. Grosskopf, K. Marasek, H. van den
Heuvel, F. Diehl, and A. Kiessling. 2002.
SPEECON speech databases for consumer devices:
Database specification and validation. In Proc.
LREC, pages 329?333.
T. Kawahara, A. Lee, T. Kobayashi, K. Takeda,
N. Minematsu, S. Sagayama, K. Itou, A. Ito, M. Ya-
mamoto, A. Yamada, T. Utsuro, and K. Shikano.
2000. Free software toolkit for japanese large vo-
cabulary continuous speech recognition. In Proc.
ICSLP-2000, volume 4, pages 476?479.
S. King, K. Tokuda, H. Zen, and J. Yamagishi. 2008.
Unsupervised adaptation for HMM-based speech
synthesis. In Proc. Interspeech 2008, pages 1869?
1872, September.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2009.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.
H. Liang, J. Dines, and L. Saheer. 2010. A
comparison of supervised and unsupervised cross-
lingual speaker adaptation approaches for HMM-
based speech synthesis. In Proc. of ICASSP, page
to appear, March.
Keiichiro Oura, Junichi Yamagishi, Simon King, Mir-
jam Wester, and Keiichi Tokuda. 2009. Unsuper-
vised speaker adaptation for speech-to-speech trans-
lation system. In Proc. SLP (Spoken Language Pro-
cessing), number 356 in 109, pages 13?18.
K. Oura, K. Tokuda, J. Yamagishi, S. King, and
M. Wester. 2010. Unsupervised cross-lingual
speaker adaptation for HMM-based speech synthe-
sis. In Proc. of ICASSP, page to appear, March.
Y.-J. Wu, S. King, and K. Tokuda. 2008. Cross-lingual
speaker adaptation for HMM-based speech synthe-
sis. In Proc. of ISCSLP, pages 1?4, December.
Y.-J. Wu, Y. Nankaku, and K. Tokuda. 2009. State
mapping based method for cross-lingual speaker
adaptation in HMM-based speech synthesis. In
Proc. of Interspeech, pages 528?531, September.
J. Yamagishi, T. Nose, H. Zen, Z.-H. Ling, T. Toda,
K. Tokuda, S. King, and S. Renals. 2009. Robust
speaker-adaptive HMM-based text-to-speech syn-
thesis. IEEE Trans. Audio, Speech and Language
Process., 17(6):1208?1230. (in press).
J. Yamagishi, B. Usabaev, S. King, O. Watts, J. Dines,
J. Tian, R. Hu, K. Oura, K. Tokuda, R. Karhila, and
M. Kurimo. 2010. Thousands of voices for hmm-
based speech synthesis. IEEE Trans. Speech, Audio
& Language Process. (in press).
52
S. Young, G. Everman, D. Kershaw, G. Moore, J.
Odell, D. Ollason, V. Valtchev, and P. Woodland,
2001. The HTK Book Version 3.1, December.
53
