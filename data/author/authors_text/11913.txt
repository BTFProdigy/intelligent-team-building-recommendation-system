Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 109?113,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Joint Syntactic and Semantic Dependency Parsing System based on
Maximum Entropy Models
Buzhou Tang1 Lu Li2 Xinxin Li1 Xuan Wang2 Xiaolong Wang2
Shenzhen Graduate School
Harbin Institute of Technology
Shenzhen,518055, China
1{tangbuzhou,lixxin2}@gmail.com
2{lli,wangxuan,wangxl}@insun.hit.edu.cn
Abstract
A joint syntactic and semantic dependency
parsing system submitted to the CoNLL-2009
shared task is presented in this paper. The
system is composed of three components: a
syntactic dependency parser, a predicate clas-
sifier and a semantic parser. The first-order
MSTParser is used as our syntactic depen-
dency pasrser. Projective and non-projective
MSTParsers are compared with each other on
seven languages. Predicate classification and
semantic parsing are both recognized as clas-
sification problem, and the Maximum Entropy
Models are used for them in our system. For
semantic parsing and predicate classifying, we
focus on finding optimized features on multi-
ple languages. The average Macro F1 Score
of our system is 73.97 for joint task in closed
challenge.
1 Introduction
The task for CoNLL-2009 is an extension of the
CoNLL-2008 shared task to multiple languages: En-
glish (Surdeanu et al, 2008), Catalan plus Span-
ish (Mariona Taule? et al, 2008), Chinese (Martha
Palmer et al, 2009), Czech (Jan Hajic? et al,
2006), German (Aljoscha Burchardt et al, 2006) and
Japanese (Daisuke Kawahara et al, 2002). Com-
pared to the CoNLL-2008 shared task, the predi-
cates are given for us in semantic dependencies task.
Therefore, we have only need to label the semantic
roles of nouns and verbs, and the frames of predi-
cates.
In this paper, a joint syntactic and semantic de-
pendency parsing system submitted to the CoNLL-
2009 shared task is presented. The system is com-
posed of three components: a syntactic dependency
parser, a predicate classifier and a semantic parser.
The first-order MSTParser is used as our syntactic
dependency parser. Projective and non-projective
MSTParsers are compared with each other on seven
languages. The predicate classifier labeling the
frames of predicates and the semantic parser label-
ing the semantic roles of nouns and verbs for each
predicate are both recognized as classification prob-
lem, and the Maximum Entropy Models (MEs) are
used for them in our system. Among three com-
ponents, we mainly focus on the predicate classifier
and the semantic parser.
For semantic parsing and predicate classifying,
features of different types are selected to our sys-
tem. The effect of them on multiple languages will
be described in the following sections in detail.
2 System Description
Generally Speaking, a syntactic and semantic de-
pendency parsing system is usually divided into four
separate subtasks: syntactic parsing, predicate iden-
tification, predicate classification, and semantic role
labeling. In the CoNLL-2009 shared task, the pred-
icate identification is not required, since the pred-
icates are given for us. Therefore, the system we
present is only composed of three components: a
syntactic dependency parser, a predicate classifier
and a semantic parser. The syntactic dependencies
are processed with the MSTParser 0.4.3b. The pred-
icates identification and semantic role label are pro-
cessed with MEs-based classifier respectively. Un-
like conventional systems, the predicates identifica-
109
tion and the semantic parser are independent with
each other. Figure 1 is the architecture of our sys-
tem.
Figure 1: System Architecture
In our system, we firstly select an appropriate
mode (projective or non-projective) of Graph-based
Parser (MSTParser) for each language, then con-
struct the MEs-based predicates classification and
the MEs-based semantic parser with syntactic de-
pendency relationships and predicate classification
respectively.
2.1 Syntactic Dependency Parsing
MSTParser (McDonald, 2008) is used as our syn-
tactic dependency parser. It is a state-of-the-art de-
pendency parser that searches for maximum span-
ning trees (MST) over directed graph. Both of pro-
jective and non-projective are supported by MST-
Parser. Our system employs the first-order frame-
work with projective and non-projective modes on
seven given languages.
2.2 Predicate Classification
In this phase, we label the sense of each predicate
and the MEs are adopted for classification. Features
of different types are extracted for each predicate,
and an optimized combination of them is adopted in
our final system. Table 1 lists all features. 1-20 are
the features used in Li?s system (Lu Li et al, 2008),
No Features No Features
1 w0 20 Lemma
2 p0 21 DEPREL
3 p?1 22 CHD POS
4 p1 23 CHD POS U
5 p?1p0 24 CHD REL
6 p0p1 25 CHD REL U
7 p?2p0 26 SIB REL
8 p0p2 27 SIB REL U
9 p?3p0 28 SIB POS
10 p0p3 29 SIB POS U
11 p?1p0p1 30 VERB V
12 w0p0 31 4+11
13 w0p?1p0 32 Indegree
14 w0p0p1 33 Outdegree
15 w0p?2p0 34 Degree
16 w0p0p2 35 ARG IN
17 w0p?3p0 36 ARG OUT
18 w0p0p3 37 ARG Degree
19 w0p?1p0p1 38 Span
Table 1: Features for Predicate Classification.
and 21-31 are a part of the optimized features pre-
sented in Che?s system (Wanxiang Che et al, 2008)
In Table 1, ?w? denotes the word and ?p? de-
notes POS of the words. Features in the form of
part1 part2 denote the part2 of the part1, while fea-
tures in the form of part1+part2 denote the combi-
nation of the part1 and part2. ?CHD? and ?SIB? de-
note a sequence of the child and the sibling words
respectively, ?REL? denotes the type of relations,
?U? denotes the result after reducing the adjacent
duplicate tags to one, ?V? denotes whether the part
is a voice, ?In? and ?OUT? denote the in degree and
out degree, which denotes how many dependency
relations coming into this word and going away from
this word,and ?ARG? denotes the semantic roles of
the predicate. The ?Span? denotes the maximum
length between the predicate and its arguments. The
final optimized feature combination is :1-31 and 33-
37.
2.3 Semantic Role Labeling
The semantic role labeling usually contains two sub-
tasks: argument identification and argument classi-
fication. In our system, we perform them in a single
110
stage through one classifier, which specifies a par-
ticular role label to the argument candidates directly
and assigns ?NONE? label to the argument candi-
dates with no role. MEs are also adopted for classifi-
cation. For each word in a sentence, MEs gives each
candidate label (including semantic role labels and
none label) a probability for the predicate. The fea-
tures except for the feature (lemma plus sense num-
ber of the predicate in (Lu Li et al, 2008)) and the
features 32-38 in Table 1 are selected in our system.
3 Experiments and Results
We train the first-order MSTParser 1 with projective
and non-projective modes in terms of default param-
eters respectively. Our maximum entropy classifiers
are implemented with the Maximum Entropy Mod-
eling Toolkit 2 . The default classifier parameters are
used in our system except for iterations. All mod-
els are trained using all training data, and tested on
the whole development data and test data, with 64-
bit 3.00GHz Intel(R) Pentium(R) D CPU and 4.0G
memory.
3.1 Syntactic Dependency Parsing
Table 2 is a performance comparison between pro-
jective parser and non-projective parser on the devel-
opment data of seven languages. In Table 2, ?LAS?,
?ULAS? and ?LCS? denote as Labeled attachment
score, Unlabeled attachment score and Label accu-
racy score respectively.
The experiments show that Catalan, Chinese and
Spanish have projective property and others have
non-projective property.
3.2 Predicate Classification
To get the optimized system, three group features are
used for comparison.
? group 1: features 1-20 in Table 1.
? group 2: features 1-31 in Table 1.
? group 3: all features in Table 1.
The performance of predicate classification on the
development data of the six languages, which con-
tain this subtask, are given in Table 3. The results
1http://sourceforge.net/projects/mstparser.
2http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.
LAS(%) ULAS(%) LCS(%)
Catalan 84.18 88.18 91.76
83.69 87.74 91.59
Chinese 72.58 77.06 82.07
62.85 69.47 73.00
Czech 72.79 81.40 80.93
73.18 81.86 81.30
English 86.89 90.29 91.50
86.88 90.34 91.58
German 83.43 86.89 90.24
84.00 87.40 90.61
Japanese 92.23 93.16 98.38
92.23 93.14 98.45
Spanish 83.88 87.93 91.36
83.46 87.46 91.37
Table 2: Performance of Syntactic Dependency
Parsing with different modes. The above line is the
performance of projective mode, while the below
one is the performance of non-projective mode for
each language.
group 1 group 2 group 3
Catalan 75.51 80.90 82.23
Chinese 93.79 94.99 94.75
Czech 91.83 91.77 91.86
English 92.12 92.48 93.20
German 74.49 74.14 75.85
Spanish 74.01 76.22 76.53
Table 3: Performance of predicate classification (F1
scores) for different group features on the develop-
ment data of the six languages.
show that Che?s features and the degrees of the pred-
icate and its arguments are useful for all languages,
the former improves the labeled F1 measure by 0.3%
to 5.4%, and the latter by 0.3% to 1.7%.
3.3 Semantic Role Labeling
In this phase, feature selection and performance lose
caused by P-columns are studied. Firstly, we com-
pare the following two group features:
? group 1: The features except for the lemma
plus sense number of the predicate in (Lu Li
et al, 2008).
111
LF1 ULF1 PF1
Catalan 73.25 92.69 38.41
72.71 91.93 35.22
83.23 100.00 61.88
Chinese 69.60 82.15 28.35
71.49 81.71 29.41
85.44 95.21 58.20
Czech 80.62 92.49 70.04
79.10 91.44 68.34
85.42 96.93 77.78
English 73.91 87.26 33.16
76.10 88.58 36.28
79.35 91.74 43.32
German 64.85 88.05 27.21
65.36 88.63 26.70
72.78 94.54 41.50
Japanese 69.43 82.79 29.27
69.87 83.31 29.69
72.80 87.13 34.96
Spanish 73.49 93.15 39.64
78.18 91.68 33.57
81.96 99.98 59.20
Table 4: Performance of Semantic Role Labeling
(F1 score) with different features.
? group 2: group1+the degrees of the predicate
and its arguments presented in the last section.
Secondly, features extracted from golden-columns
and P-columns are both used for testing.
The performance of them are given in Table 4,
where ?LF1?, ?ULF1? and ?PF1? denote as Labeled
F1 score, Unlabeled F1 score and Proposition F1
score respectively. The above line is the F1 scores of
Semantic Role Labeling with different features. The
uppermost line is the result of group1 features, the
middle line is the result of group2 features extracted
from P-columns, and the downmost one is the result
of group2 features extracted from golden-columns
for each language.
The results show that the features of degree also
improves the labeled F1 measure by 3.4% to 15.8%,
the different labeled F1 between golden-columns
and P-columns is about 2.9%?13.9%.
LAS LF1 M LF1
Catalan 84.18 72.71 81.46
75.68 66.95 71.32
Chinese 72.58 71.49 72.20
63.95 67.06 65.53
Czech 73.18 79.10 76.37
72.60 79.08 75.85
Czech-ood 69.81 79.80 74.81
English 86.88 76.10 82.89
86.61 77.17 81.92
English-ood 80.09 67.21 73.69
German 84.00 65.36 83.06
79.85 61.98 70.93
German-ood 71.86 61.83 66.86
Japanese 92.23 69.87 83.77
91.26 69.58 80.49
Spanish 83.88 71.18 80.74
77.21 66.23 71.72
Table 5: Overall performance of our final joint sys-
tem.
3.4 Overall Performance
In the final system, we select the optimized feature
subset discussed in the former sections. The overall
performance of the system on the development data ,
test data and Out-of-domain data are shown in Table
5 (all features are extracted from P-columns). The
average Macro F1 Scores of our system are 73.97
on test data and 71.79 on Out-of-domain data.
In Table 5, ?LAS?, ?LF1? and ?M LF1? denote
as Labeled accuracy score for Syntactic Dependency
Parsing, Labeled F1 score for Semantic Role Label-
ing, and Overall Macro Labeled F1 score respec-
tively. The topmost line is the result on the devel-
opment data, the middle one is the result on the test
data for each language and the downmost one is the
result on the Out-of-domain data if the data exist.
4 Conclusion and Discussion
We present a joint syntactic and semantic depen-
dency parsing system for CoNLL2009 Shared Task,
which composed of three components: a syntac-
tic dependency parser, a predicate classifier and a
semantic parser. All of them are built with some
state-of-the-art methods. For the predicate classifier
and the semantic parser, a new kind of features?
112
degrees, which reflect the activeness of the words
in a sentence improves their performance. In order
to improve the performance further, we will study
new machine learning methods for semantic depen-
dency parsing, especially the joint learning methods,
which can avoid the information loss problem of our
system.
Acknowledgments
We would like to thank McDonald for providing
the MSTParser program, to Zhang Le for provid-
ing the Maxent program. This research has been
partially supported by the National Natural Science
Foundation of China(No.60703015) and the Na-
tional 863 Program of China (No.2006AA01Z197,
No.2007AA01Z194).
References
Jan Hajic? and Massimiliano Ciaramita and Richard Jo-
hansson and Daisuke Kawahara and Maria Anto`nia
Mart?? and Llu??s Ma`rquez and Adam Meyers and
Joakim Nivre and Sebastian Pado? and Jan S?te?pa?nek
and Pavel Stran?a?k and Miahi Surdeanu and Nianwen
Xue and Yi Zhang. 2009. The CoNLL-2009 Shared
Task: Syntactic and Semantic Dependencies in Multi-
ple Languages. Proceedings of the 13th Conference on
Computational Natural Language Learning (CoNLL-
2009), June 4-5. Boulder, Colorado, USA.
Mariona Taule? and Maria Anto`nia Mart?? and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Cor-
pora for Catalan and Spanish. Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008). Marrakesh, Morroco.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1),pages 143?172.
Jan Hajic? and Jarmila Panevova? and Eva Hajic?ova? and
Petr Sgall and Petr Pajas and Jan S?te?pa?nek and Jir???
Havelka and Marie Mikulova? and Zdene?k Z?abokrtsky?.
2006. Prague Dependency Treebank 2.0. CD-ROM,
Cat. No. LDC2006T01, ISBN 1-58563-370-4. Lin-
guistic Data Consortium, Philadelphia, Pennsylvania,
USA. URL: http://ldc.upenn.edu.
Surdeanu, Mihai and Johansson, Richard and Meyers,
Adam and Ma`rquez, Llu??s and Nivre, Joakim. 2008.
The CoNLL-2008 Shared Task on Joint Parsing of
Syntactic and Semantic Dependencies. Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning(CoNLL-2008).
Aljoscha Burchardt and Katrin Erk and Anette Frank and
Andrea Kowalski and Sebastian Pado? and Manfred
Pinkal. 2006. The SALSA corpus: a German corpus
resource for lexical semantics. Proceedings of the 5rd
International Conference on Language Resources and
Evaluation (LREC-2006), pages 2008?2013. Genoa,
Italy.
Daisuke Kawahara and Sadao Kurohashi and Ko?iti
Hasida. 2002. Construction of a Japanese Relevance-
tagged Corpus. Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013. Las Palmas, Canary
Islands.
McDonald and Ryan. 2006. Discriminative Learning
and Spanning Tree Algorithms for Dependency Pars-
ing, Ph.D. thesis. University of Pennsylvania.
Lu Li, Shixi Fan, Xuan Wang, XiaolongWang. 2008.
Discriminative Learning of Syntactic and Semantic
Dependencies. CoNLL 2008: Proceedings of the
12th Conference on Computational Natural Language
Learning, pages 218?222. Manchester.
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li,
Bing Qin, Ting Liu, Sheng Li. 2008. A Cascaded
Syntactic and Semantic Dependency Parsing System.
CoNLL 2008: Proceedings of the 12th Conference
on Computational Natural Language Learning, pages
238?242. Manchester.
113
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 430?434,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Zhijun Wu: Chinese Semantic Dependency Parsing with Third-Order 
Features 
 
 
Zhijun Wu Xuan Wang Xinxin Li 
Computer Application Research Center 
School of Computer Science and Technology 
Harbin Institute of Technology Shenzhen Graduate School 
Shenzhen 518055, P.R.China 
mattwu305@gmail.com wangxuan@insun.hit.edu.cn  lixin2@gmail.com 
 
 
 
 
 
 
Abstract 
This paper presents our system participated on 
SemEval-2012 task: Chinese Semantic De-
pendency Parsing. Our system extends the 
second-order MST model by adding two 
third-order features. The two third-order fea-
tures are grand-sibling and tri-sibling. In the 
decoding phase, we keep the k best results for 
each span. After using the selected third-order 
features, our system presently achieves LAS 
of 61.58% ignoring punctuation tokens which 
is 0.15% higher than the result of purely 
second-order model on the test dataset. 
1 Introduction 
Recently, semantic role labeling (SRL) has been a 
hot research topic. CoNLL shared tasks for joint 
parsing for syntactic and semantic dependencies 
both in the year 2008 and 2009, cf. (Surdeanu et al, 
2008; Haji? et al, 2009; Bohnet, 2009). Same 
shared tasks in SemEval-2007 (Sameer S., 2007). 
The SRL is traditionally implemented as two sub-
tasks, argument identification and classification. 
However, there are some problems for the seman-
tic representation method used by the semantic role 
labeling. For example, the SRL only considers the 
predicate-argument relations and ignores the rela-
tions between a noun and its modifier, the meaning 
of semantic roles is related with special predicates. 
In order to overcome those problems, semantic 
dependency parsing (SDP) is introduced. Semantic 
dependencies express semantic links between pre-
dicates and arguments and represent relations be-
tween entities and events in text. The SDP is a kind 
of dependency parsing, and its task is to build a 
dependency structure for an input sentence and to 
label the semantic relation between a word and its 
head. However, semantic relations are different 
from syntactic relations, such as position indepen-
dent. Table 1 shows the position independent of 
semantic relations for the sentence XiaoMing hit 
XiaoBai with a book today.  
Today, XiaoMing hit XiaoBai with a book. 
XiaoBai was hit by XiaoMing today with a book. 
With a book, XiaoMing hit XiaoBai today. 
XiaoMing hit XiaoBai with a book today. 
Table 1: An example position dependency 
   Although semantic relations are different from 
syntactic relations, yet they are identical in the de-
pendency tree. That means the methods used in 
syntactic dependency parsing can also be applied 
in SDP. 
    Two main approaches to syntactic dependency 
paring are Maximum Spanning Tree (MST) based 
dependency parsing and Transition based depen-
dency parsing (Eisner, 1996; Nivre et al, 2004; 
McDonald and Pereira, 2006). The main idea of 
430
MSTParser is to take dependency parsing as a 
problem of searching a maximum spanning tree 
(MST) in a directed graph (Dependency Tree). We 
see MSTParser a better chance to improve the 
parsing speed and MSTParser provides the state-
of-the-art performance for both projective and non-
projective tree banks. For the reasons above, we 
choose MSTParser as our SemEval-2012 shared 
task participating system basic framework. 
2 System Architecture  
Our parser is based on the projective MSTParser 
using all the features described by (McDonald et 
al., 2006) as well as some third-order features de-
scribed in the following sections. Semantic depen-
dency paring is introduced in Section 3. We 
explain the reasons why we choose projective 
MSTParser in Section 4 which also contains the 
experiment result analysis in various conditions. 
Section 5 gives our conclusion and future work. 
3 Semantic Dependency parsers 
3.1 First-Order Model 
Dependency tree parsing as the search for the max-
imum spanning tree in a directed graph was pro-
posed by McDonald et al (2005c). This 
formulation leads to efficient parsing algorithms 
for both projective and non-projective dependency 
trees with the Eisner algorithm (Eisner, 1996) and 
the Chu-Liu-Edmonds algorithm (Chu and Liu, 
1965; Edmonds, 1967) respectively. The formula-
tion works by defining in McDonald et al(2005a). 
The score of a dependency tree y for sentence x is 
( )
( , )
, ( , ) ( , )
i j y
s x y s i j w f i j
?
= = ?? ?
 
f(i, j) is a multidimensional feature vector repre-
sentation of the edge from node i to node j. We set 
the value of f(i, j) as 1 if there an edge from node i 
to node j. w is the corresponding weight vector 
between the two nodes that will be learned during 
training. Hence, finding a dependency tree with 
highest score is equivalent to finding a maximum 
spanning tree. Obviously, the scores are restricted 
to a single edge in the dependency tree, thus we 
call this first-order dependency parsing. This is a 
standard linear classifier. The features used in the 
first-order dependency parser are based on those 
listed in (Johansson, 2008). Table 2 shows the fea-
tures we choose in the first-order parsing. We use 
some shorthand notations in order to simplify the 
feature representations: h is the abbreviation for 
head, d for dependent, s for nearby nodes (may not 
be siblings), f for form, le for the lemmas, pos for 
part-of-speech tags, dir for direction, dis for dis-
tance, ?+1? and ?-1? for right and left position re-
spectively. Additional features are built by adding 
the direction and the distance plus the direction. 
The direction is left if the dependent is left to its 
head otherwise right. The distance is the number of 
words minus one between the head and the depen-
dent in a certain sentence, if ? 5, 5 if > 5, 10 if > 
10. ? means  that previous part is built once and 
the additional part follow ? together with the pre-
vious part is built again.  
Head and Dependent 
h-f, h-l, d-pos ?dir(h, d) ?dis(h, d) 
h-l, h-pos, d-f ?dir(h, d) ?dis(h, d) 
h-pos, h-f, d-l ?dir(h, d) ?dis(h, d) 
h-f, d-l, d-pos ?dir(h, d)  ?dis(h, d) 
h-f, d-f, d-l  ?dir(h, d) ?dis(h, d) 
h-f, h-l, d-f, d-l  ?dir(h, d) ?dis(h, d) 
h-f, h-l, d-f, d-pos ?dir(h, d) ?dis(h, d) 
h-f, h-pos, d-f, d-pos ?dir(h, d) ?dis(h, d) 
h-l, h-pos, d-l, d-pos ?dir(h, d) ?dis(h, d) 
Dependent and Nearby 
d-pos-1, d-pos, s-pos ?dir(d, s) ?dis(d, s) 
d-pos-1, s-pos, s-pos+1 ?dir(d, s) ?dis(d, s) 
d-pos-1, d-pos, s-pos+1 ?dir(d, s) ?dis(d, s) 
d-pos, s-pos, s-pos+1 ?dir(d, s) ?dis(d, s) 
d-pos, d-pos+1, s-pos-1 ?dir(d, s) ?dis(d, s) 
d-pos-1, d-pos, s-pos-1 ?dir(d, s) ?dis(d, s) 
d-pos, d-pos+1, s-pos ?dir(d, s) ?dis(d, s) 
d-pos, s-pos-1, s-pos ?dir(d, s) ?dis(d, s) 
d-pos+1, s-pos-1, s-pos ?dir(d, s) ?dis(d, s) 
d-pos-1, d-pos, s-pos-1, s-pos ? dir(d, s) ?
dis(d, s) 
d-pos, d-pos+1, s-pos-1, s-pos ?dir(d, s) ?
dis(d, s) 
d-pos-1, d-pos, s-pos, s-pos+1 ?dir(d, s) ?
dis(d, s) 
Table 2: Selected features in first order parsing 
431
3.2 Second-Order Model 
A second order model proposed by McDonald 
(McDonald and Pereira, 2006) alleviates some of 
the first order factorization limitations. Because the 
first order parsing restricts scores to a single edge 
in a dependency tree, the procedure is sufficient. 
However, in the second order parsing scenario 
where more than one edge are considered by the 
parsing algorithm, combinations of two edges 
might be more accurate which will be described in 
the Section 4. The second-order parsing can be 
defined as below: 
( )
( , )
, ( , , )
i j y
s x y s i k j
?
= ?  
where k and j are adjacent,  same-side children of i 
in the tree y. The shortcoming of this definition is 
that it restricts i on the same side of its sibling. In 
our system, we extend this restriction by adding 
the feature that as long as i is another child of k or j. 
In that case, i may be the child or grandchild of k 
or j which is shown in Figure 1. 
k  i  j ? k  i j
 
Figure 1: Sibling and grand-child relations. 
Siblings 
c1-pos, c2-pos?dir(c1, c2)?dis(c1, c2) 
c1-f, c2-f?dir(c1, c2) 
c1-f, c2-pos?dir(c1, c2) 
c1-pos, c2-f?dir(c1, c2) 
Parent and Two Children 
p-pos, c1-pos, c2-pos?dir(c1, c2)?dis(c1, c2) 
p-f, c1-pos, c2-pos?dir(c1, c2)?dis(c1, c2) 
p-f, c1-f, c2-pos?dir(c1, c2) ?dis(c1, c2) 
p-f, c1-f, c2-f ?dir(c1, c2) ?dis(c1, c2) 
p-pos, c1-f, c2-f?dir(c1, c2) ?dis(c1, c2) 
p-pos, c1-f, c2-pos?dir(c1, c2) ?dis(c1, c2) 
p-pos, c1-pos, c2-f?dir(c1, c2) ?dis(c1, c2) 
Table 3: Selected features in second-order parsing 
   Shorthand notations are almost the same with the 
Section 3.1 except for that we use c1 and c2 to 
represent the two children and p for parent. In 
second-order parsing? the features selected are 
shown in Table 3. We divide the dependency dis-
tance into six parts which are 1 if > 1, 2 if > 2, ? , 
5 if  > 5, 10 if > 10. 
3.3 Third-Order Features 
The order of parsing is defined according to the 
number of dependencies it contains (Koo and Col-
lins, 2010). Collins classifies the third-order as two 
models, Model 1 is all grand-siblings, and Model 2 
is grand-siblings and tri-siblings. A grand-sibling 
is a 4-tuple of indices (g, h, m, s) where g is grand-
father. (h, m, s) is a sibling part and (g, h, m) is a 
grandchild part as well as (g, h, s). A tri-sibling 
part is also a 4-tuple of indices (h, m, s, t). Both (h, 
m, s) and (h, s, t) are siblings. Figure 2 clearly 
shows these relations. 
g h  s  m ?h t  s m
 
Figure 2: Grand-siblings and tri-siblings dependency. 
   Collins and Koo implement an efficient third-
order dependency parsing algorithm, but still time 
consuming compared with the second-order 
(McDonald, 2006). For that reason, we only add 
third-order relation features into our system instead 
of implementing the third-order dependency pars-
ing model. These features shown in Table 4 are 
grand-sibling and tri-sibling described above. 
Shorthand notations are almost the same with the 
Section 3.1 and 3.2 except that we use c3 for the 
third sibling and g represent the grandfather. We 
attempt to add features of words form and parts-of-
speech as well as directions into our system, which 
is used both in first-order and second-order as fea-
tures, but result shows that these decrease the sys-
tem performance. 
Tri-Sibling 
c1-pos, c2-pos, c3-pos?dir(c1, c2) 
Grandfather and Two Children 
g-pos, c1-pos, c2-pos?dir(c1, c2) 
g-pos, p-pos, c1-pos, c2-pos?dir(c1, c2) 
Table 4: Third-order features. 
432
4 Experiment result analysis 
As we all know that projective dependency parsing 
using edge based factorization can be processed by 
the Einster algorithm (Einster, 1996). The corpus 
given by SemEval-2012 is consists of 10000 sen-
tences converting into dependency structures from 
Chinese Penn Treebank randomly. We find that 
none of non-projective sentence existing by testing 
the 8301 sentences in training data. For this reason, 
we set the MSTParser into projective parsing mode. 
    We perform a number of experiments where we 
compare the first-order, second-order and second-
order by adding third-order features proposed in 
the previous sections. We train the model on the 
full training set which contains 8301 sentences to-
tally. We use 10 training iterations and projective 
decoding in the experiments. Experimental results 
show that 10 training iterations are better than oth-
ers. After adjusting the features of third-order, our 
best result reaches the labeled attachment score of 
62.48% on the developing dataset which ignores 
punctuation. We submitted our currently best result 
to SemEval-2012 which is 61.58% on the test data-
set. The results in Table 5 show that by adding 
third-order features to second-order model, we im-
prove the dependency parsing accuracies by 1.21% 
comparing to first-order model and 0.15% compar-
ing to second-order model. 
Models LAS UAS 
First-Order 61.26 80.18 
Second-Order 62.33 81.40 
Second-Order+ 62.48 81.43 
Table 5: Experimental results. Second-Order+ means 
second-order model by adding third-order features. 
Results are tested under the developping dataset which 
contains the heads and semantic relations given by 
organizer. 
5 Conclusion and Future Work  
In this paper, we have presented the semantic de-
pendency parsing and shown it works on the first-
order model, second-order model and second-order 
model by adding third-order features. Our experi-
mental results show more significant improve-
ments than the conventional approaches of third-
order model. 
In the future, we firstly plan to implement the 
third-order model by adding higher-order features, 
such as forth-order features. We have found that 
both in the first-order and second-order model of 
MSTParser, words form and lemmas are recog-
nized as two different features. These features are 
essential in languages that have different grid, 
however, which are the same in Chinese in the giv-
en dataset. Things are the same in POS (part-of-
speech tags) and CPOS (fine-grid POS) which are 
viewed as different features. For the applications of 
syntactic and semantic parsing, the parsing time 
and memory footprint are very important. There-
fore, secondly, we decide to remove these repeated 
features in order to reduce to training time as well 
as the space if it does not lower the system perfor-
mance.  
Acknowledgments 
The authors would like to thank the reviewers for 
their helpful comments. 
References  
Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and JoakimNivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic andse-
mantic dependencies. In Proceedings of the 12th
Jan Haji?, Massimiliano Ciaramita, Richard Johansson, 
Daisuke Kawahara, Maria Antonia Mart?, Llu?is 
M?rquez, Adam Meyers, Joakim Nivre, SebastianPa-
do, Jan ?tep?nek, Pavel Stran?k, Miahi Surdeanu, 
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 Shared Task: Syntactic and Semantic Depen-
dencies in Multiple Languages. In Proceedings of the 
13th CoNLL-2009, June 4-5, Boulder, Colorado, 
USA. 
 
CoNLL-2008. 
Bohnet, Bernd. 2009. Efficient parsing of syntactic and 
semantic dependency structures. In Proceedings of 
CoNLL-09. 
Ryan McDonald. 2006. Discriminative Learning and       
Spanning Tree Algorithms for Dependency Parsing.     
Ph.D. thesis, University of Pennsylvania. 
Ryan McDonald and Fernando Pereira. 2006. Online 
     Learning of Approximate Dependency Parsing Al-
grithms. In In Proc. of EACL, pages 81?88. 
Ryan. McDonald, K. Crammer, and F. Pereira. 2005a.         
Online large-margin training of dependency parsers. 
In Proc. of the 43rd Annual Meeting of the ACL. 
Ryan. McDonald, F. Pereira, K. Ribarov, and J. Haji?c. 
2005c. Non-projective dependency parsing using 
spanning tree algorithms. In Proc. HLT-EMNLP. 
Richard Johansson. 2008. Dependency-based Semantic 
Analysis of Natural-language Text. Ph.D. thesis, 
Lund University. 
433
Jason Eisner. 1996. Three new probabilistic models for   
dependency parsing: An exploration. In Proceedings 
of the 16th International Conference on Computa-
tional Linguistics (COLING-96), pages 340?345, 
Copenhaen. 
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. 
Memory-Based Dependency Parsing. In Proceedings 
of the 8th CoNLL, pages 49?56, Boston, Massachu-
setts. 
Terry Koo, Michael Collins. 2010. Efficient Third-order 
Dependency Parsers. Proceedings of the 48th Annual 
Meeting of the Association for Computational Lin-
guistics, pages 1?11. 
 
434
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 78?83,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Exploiting Rich Features for Detecting Hedges and Their Scope 
Xinxin Li, Jianping Shen, Xiang Gao, Xuan Wang 
Harbin Institute of Technology Shenzhen Graduate School 
Shenzhen, Guangdong, China 
{lixxin2, jpshen2008}@gmail.com,  
sky0306201@163.com, wangxuan@insun.hit.edu.cn 
 
Abstract 
This paper describes our system about 
detecting hedges and their scope in natural 
language texts for our participation in CoNLL-
2010 shared tasks. We formalize these two 
tasks as sequence labeling problems, and 
implement them using conditional random 
fields (CRFs) model. In the first task, we use a 
greedy forward procedure to select features for 
the classifier. These features include part-of-
speech tag, word form, lemma, chunk tag of 
tokens in the sentence. In the second task, our 
system exploits rich syntactic features about 
dependency structures and phrase structures, 
which achieves a better performance than only 
using the flat sequence features. Our system 
achieves the third score in biological data set 
for the first task, and achieves 0.5265 F1 score 
for the second task. 
1 Introduction 
In recent years, a fair amount of approaches have 
been developed on detecting speculative and 
negative information from biomedical and 
natural language texts, for its benefit to the 
applications like information extraction. These 
approaches evolve from hand-crafted rule-based 
approaches, which use regular expressions to 
match the sentences or its grammatical parsing, 
such as NegEx (Chapman et al, 2001), 
Negfinder (Mutalik et al, 2001), and 
NegExpander (Aronow et al, 1999), to machine 
learning approaches, including semi-supervised 
methods (Medlock and Briscoe, 2007; Szarvas, 
2008), and supervised methods (Morante and 
Daelemans, 2009).  
In this paper, we describe the machine 
learning system submitted to CoNLL-2010 
Shared task (Farkas et al, 2010). Our system 
formalizes these two tasks as consecutive 
sequence labeling problems, and learns the 
classifiers using conditional random fields 
approach. In the first task, a model is trained to 
identify the hedge cues in sentences, and in the 
second task, another model is used to find the 
corresponding scope for each hedge cue 
generated in the first task. Our system follows 
the study of Morante and Daelemans (2009), but 
applies more refined feature selection. In the first 
task, we use a greedy forward procedure to select 
features for the classifier. In the second task, we 
exploit rich syntactic information to improve the 
performance of the model, from dependency 
structures and phrase structures. A rule-based 
post processing procedure is used to eliminate 
the errors brought by the classifier for each task. 
The remainder of the paper is organized as 
follows. In section 2, we briefly describe the task 
and the details of our system, including how to 
select features for the hedge cue detection 
system, and how to find the corresponding scope 
for each hedge cue. The experimental results are 
discussed in section 3. In section 4 we put 
forward some conclusion. 
2 System Description  
We model these two tasks for identifying the 
hedge cues and finding their scope as two 
consecutive sequence labeling problems, such as 
chunking, segmentation and named entity 
recognition, and train the classifiers using 
conditional random fields approach (Lafferty et 
al., 2001). For each task, a post-processing 
procedure is used to refine the results from the 
classifier. 
In the first task, we detect the hedge cue by 
classifying the tokens of a sentence as being at 
the beginning of, inside or outside of the hedge 
signal. In the second task, we find the scope of a 
hedge cue by classifying the tokens of a sentence 
as being the first one of, the last one or neither of 
the scope.  
A sentence from biological full articles data 
set omitting the id number is shown below in 
Figure 1. In this sentence, there is only one 
hedge cue, the phrase ?raises an interesting 
question?, and its corresponding scope is the 
sequence from token ?raises? to token ?acid?. 
78
<sentence>This <xcope><cue>raises an 
interesting question</cue>: "Is there a 23rd 
amino acid</xcope>?".</sentence> 
 
Figure 1: A sentence with hedge cue and scope 
annotation in biological full articles data set 
2.1 Hedge detection 
Since hedge cues usually consist of one or more 
tokens, we predict the tokens in BIO 
representation, whether the token is the first 
token of a hedge cue (B-cue), inside a hedge cue 
(I-cue), or outside of the hedge cue (O-cue). For 
the sentence in Figure 1, token ?raises? is 
denoted as B-cue, tokens ?an interesting 
question? all as I-cue, and the other tokens in the 
sentence as O-cue. 
The classifier is trained using conditional 
random fields (Lafferty et al, 2001), which 
combines the benefits of conditional models with 
the global normalization of random field models, 
and avoid the label bias problem that exists in 
maximum entropy Markov models (MEMMs). 
The CRF model we use is implemented as 
CRF++ 0.51 1 . The parameters of the CRF 
classifier are set as defaults. 
We use a greedy forward procedure to select a 
better feature sets for the classifier according to 
the evaluation results in the development set. We 
first start from a basic feature set, and then add 
each feature outside the basic set and remove 
each feature inside the basic set one by one to 
check the effectiveness of each feature by the 
performance change in the development set. This 
procedure is repeated until no feature is added or 
removed or the performance is not improved. 
The selected features are listed below: 
? Cn (n=-2,-1, 0, 1, 2) 
? CnCn+1 (n=-1,0) 
? Cn-1CnCn+1 (n=-1,0,1) 
? Cn-2Cn-1CnCn+1  (n=0,1) 
Where C denote features of each token, 
including FORM, LEMMA, and POS (in Table 
1), C0 represents the feature of current token and 
Cn(C-n) represents the feature of the token n 
positions to the right (left) of current token. 
CnCn+1 denote the combination of Cn and Cn+1. So 
are Cn-1CnCn+1 and Cn-2Cn-1CnCn+1. 
 
 
                                                 
1
 http://crfpp.sourceforge.net/ 
Feature 
Name 
Description 
FORM Word form or punctuation symbol. 
LEMMA Lemma or stem of word form. 
POS Part-of-speech tag of the token. 
CHUNK Chunk tag of the token, e.g. B_NP, 
B_ SBAR, and I_NP. 
TCHUNK Chunk type of the token, e.g. NP. 
 
Table 1: Description of features of each token 
 
Although our system is based on token, chunk 
features are also important. Analyzing the 
training data set, it is shown that if one token in a 
chunk is in the hedge cue, the other tokens in the 
chunk are usually in the same hedge cue. The 
chunk feature can provide more information for 
the multiword hedge cues. The LEMMA, POS, 
and CHUNK of each token used in our system 
are determined using GENIA tagger (Tsuruoka et 
al., 2005).  
The selected CHUNK features in our system 
are listed as follows: 
? Cn (n=-3, -2,-1, 0, 1, 2, 3 ) 
? CnCn+1 (n=-3, -2,-1, 0, 1, 2, 3 ) 
? Cn-1CnCn+1  (n=-2,-1,0,1,-2) 
? Cn-2Cn-1CnCn+1 (n=-1,0,1,2) 
We can obtain the preliminary results using 
the CRF model-based classifier, but there are 
some missed or incorrectly classified hedge cues 
which can be recognized by rule-based patterns. 
Through statistical analysis on the training and 
development data sets, we obtain some effective 
rules for post processing, including: 
 
? If the first token of a NP chunk tag is 
annotated as I-cue, the whole NP chunk is 
in the hedge cues. 
? If the B-VP chunk tag of a token is 
followed by a B-SBAR chunk tag, the 
token is annotated as B-cue. 
? If token ?that? follows token ?indicate? 
and the POS of token ?that? is IN, the 
chunk tag of token ?that? is B-SBAR, then 
the ?indicate? will be annotated with B-
cue and ?that? will be annotated with I-
cue. 
? If token ?indicate? is followed by token 
?an? or token ?a?, then the token 
?indicate? is annotated as B-cue. 
79
2.2 Scope finding 
In this task, we train a classifier to predict 
whether each token in the sentence is in the 
scope by classifying them as the first one (F-
scope), the last one (L-scope), or neither 
(NONE) of the scope, which is the same as 
Morante and Daelemans (2009). For the sentence 
in Figure 1, token ?raises? is denoted as F-scope, 
token ?acid? as L-scope, and the other tokens in 
the sentence as NONE.  
After the classification, a post processing 
procedure is used to match the scope to each 
hedge, guaranteeing that each hedge has only one 
corresponding scope sequence, and must be 
inside its scope sequence. There is no cross 
between different scope sequences, but inclusion 
is allowed. The hedges are selected from the first 
task. 
The classifier is also implemented using 
conditional random fields model, and the 
parameters of the CRF classifier are set as 
defaults. We first build a set of baseline sequence 
features for the classifier, some borrowed from 
Morante and Daelemans (2009). The selected 
baseline sequence features are: 
? Of the token in focus: FORM, POS, 
LEMMA, CHUNK, TCHUNK, 
combination of FORM and POS; POS, 
LEMMA, CHUNK, TCHUNK of two 
tokens to the left and three tokens to the 
right; first word, last word, chain of 
FORM, POS of two chunks to the left and 
two chunks to the right; All combination 
of POS in the window of length less than 3; 
All combination of CHUNK in the 
window of length 2. 
? Of the left closest hedge: chain of the 
FORM, POS, LEMMA, CHUNK, and 
TCHUNK; All combination of POS and 
FORM in the window of length 2. 
? Of the right closest hedge: chain of the 
FORM, POS, LEMMA, CHUNK, and 
TCHUNK; All combination of POS and 
FORM in the window of length 2. 
? Of the tokens between the left closest 
hedge and the token in focus: chain of 
FORM, POS, LEMMA, CHUNK and 
TCHUNK; the number. 
? Of the tokens between the right closest 
hedge and the token in focus: chain of 
FORM, POS, LEMMA, CHUNK and 
TCHUNK; the number. 
? Others: the number of hedge cues in the 
sentence; the sequence relation between 
the token in focus and hedge cues (LEFT, 
RIGHT, MIDDLE, IN, NULL) 
Besides the sequence features listed above, 
syntactic features between the token in focus and 
hedge cues are explored in our classifier. Huang 
and Low (2007) notes that structure information 
stored in parse trees helps identifying the scope 
of negative hedge cues, and Szarvas (2008) 
points out that the scope of a keyword can be 
determined on the basic of syntax. Thus we 
believe that a highly accurate extraction of 
syntactic structure would be beneficial for this 
task.  
For sentences in the dataset, their dependency 
structures are extracted using GENIA 
Dependency parser (Sagae and Tsujii, 2007), and 
phrase structure using Brown self-trained 
biomedical parser (McClosky, 2009). Figure 2 
shows the corresponding dependency tree and 
Figure 3 shows the corresponding phrase 
structure tree for the sentence in Figure 1. In the 
following part in the section, we will illustrate 
these syntactic features and give examples for 
their value. We take the token ?acid? as the token 
in focus, to determine whether it is classified as 
F-scope, L-scope or NONE. 
 
 
 
Figure 2: Dependency tree of the sentence in 
Figure 1 
 
For the token ?acid? in the dependency trees 
in Figure 2, its father node is the token ?there?, 
and the dependency relation between these two 
token is ?NMOD?. 
Dependency features between the token in 
focus and the left closest hedge cue are: 
? Dependency relation of the token in 
focus to its father, left closest hedge to its 
80
father and the dependency relation pair: 
NOMD, ROOT, ROOT+NMOD. 
? Chain of POS: ->VBZ<-VBZ<-EX<-NN 
? Chain of POS without consecutive 
redundant POS: ->VBZ <-EX<-NN 
? POS of their nearest co-father: VBZ 
? Whether it is a linear relation (self, up , 
down, no): up 
? Kinship (grandfather, grandson, father, 
son, brother, self, no): no. 
? The number of tokens in the chain: 4 
Similar features are extracted for dependency 
relation between the token in focus and its right 
closest hedge cue. There is no right hedge cue for 
token ?acid?. Thus these features are set as 
?NULL?. 
 
This raises an interesting question :  " Is there a 23rd amino acid ? " .
DT VBZ DT JJ NN : NN VBZ RB DT NN NN NN . RB .
NP NP NP ADVP NP
VP
S
NP
ADVP
NP
VP
S
S
 
 
Figure 3: Phrase structure tree of the sentence in 
Figure 1 
 
Phrase structure features between the token in 
focus and its left closest hedge cue are: 
? Chain of syntactic categories: VBZ-
>VP<- NP <-NP <-S<-VP <-NP<-NN 
? syntactic categories without consecutive 
redundant ones: VBZ->VP<-NP<-S<-
VP<- NP<-NN 
? Syntactic category of their nearest co-
father: VP 
? The number of syntactic categories in the 
chain: 8 
The phrase structure features between the 
token in focus and the nearest right hedge cue are 
similar, setting as ?NULL?. 
Scope finding requires each hedge cue has 
only one corresponding scope. A hedge-scope 
pair is true positive only if the hedge cue and its 
corresponding scope are correctly identified. We 
perform the post processing procedure in 
sequence: 
? For each hedge cue from the beginning 
to the end of the sentence, find its left 
closest F-scope which has not been 
identified by other hedge cues, and 
identify it as its F-scope. 
? For each hedge cue from the end to the 
beginning of the sentence, find its right 
closest L-scope which has not been 
identified by other hedge cues, and 
identify it as its L-scope. 
? For each hedge:  
 If both its F-scope and L-scope is 
identified, then done;  
 If only its F-scope is identified, then 
its L-scope is set as L-scope of the 
last hedge cue in the sentence if it 
exists or according to the dictionary 
which we build with training data 
set; 
 If only its L-scope is identified, then 
its F-scope is set as its first token; 
 If none of its F-scope and L-scope is 
identified, then discard the hedge 
cue. 
3 Overall Results 
In this section we will present our experimental 
results for these two tasks. In the first task, the 
chief evaluation is carried on sentence level: 
whether a sentence contains hedge/weasel cue or 
not. Our system compares the performance of 
different machine learning algorithm, CRF and 
SVM-HMM on hedge cue detection. A post 
processing procedure is used to increase the 
recall measure for our system. 
In the second task, three experiments are 
performed. The first experiment is used to 
validate the benefit of dependency features and 
phrase structure features for scope finding. The 
second experiment is designed to evaluate the 
effect of abstract dataset on full article dataset. 
These two experiments are all performed using 
gold hedge cues. The performance of our scope 
finding system with predicted hedge cues is 
presented in the third experiment. 
81
3.1 Hedge detection 
The first experiment is used to compare two 
machine learning algorithms, SVM-HMM and 
CRF. We train the classifiers on abstract and full 
articles data sets. The results of the classifier on 
evaluation data set are shown in Table 2. 
 
Model Precision Recall F1 
SVM-HMM 88.71 81.52 84.96 
CRF 90.4 81.01 85.45 
 
Table 2: Results of hedge cues detection using 
CRF and SVM-HMM 
 
From Table 1, it is shown that CRF model 
outperforms SVM-HMM model in both 
precision and recall measure. The results are 
obtained without post processing. The 
experimental result with post processing is 
shown in Table 3. 
 
Feature Precision Recall F1 
Without Post 
processing 
90.4 81.01 85.45 
Post  
processing 
90.1 82.05 85.89 
 
Table 3: Result of biological evaluation data set 
without/with post processing 
 
By post processing, some mislabeled or 
incorrectly classified hedge cues can be 
recognized, especially the recall of the I-cue 
improved largely, from 55.26% to 68.51%. 
Though the precision is a little lower, the F1 
measure increases 0.44%. 
3.2 Scope finding 
To measure the benefit of syntactic features on 
scope finding task, we perform the experiment 
with different features on abstract data set, of 
which we split two-thirds as training data, and 
the other one third as testing data. The results are 
presented in Table 4. 
We take the classifier with sequence features 
as baseline classifier. From Table 4, it is shown 
that adding dependency features achieves a 
slightly better performance than the baseline 
classifier, and adding phrase structure features 
improve much better, about 1.2% F1-score. The 
classifier with all syntactic features achieves the 
best F1-score, 2.19% higher than baseline 
classifier. However, in later experiment on 
evaluation dataset after the shared task, we 
observed that dependency features actually 
harmed the performance for full articles dataset. 
 
Feature set Precision Recall F1 
Sequence  
(Baseline) 
82.20 81.61 81.90 
Sequence + 
Dependency 
82.28 82.09 82.19 
Sequence  
+ Phrase structure 
83.14 83.04 83.09 
All 84.19 83.99 84.09 
 
Table 4: Results of scope finding system with 
different feature sets on abstract data set 
 
Three experiments are designed to evaluate 
the benefit of abstract dataset for full articles 
dataset. The first one is performed on full articles 
data set, of which we split two-thirds for training, 
and the other one third for testing. The second 
experiment is trained on abstract data set, and 
evaluated on full articles data set. In the third 
experiment, we take abstract data set and one 
third of full articles as training data, and evaluate 
on the remaining full articles data set. The results 
are shown below in Table 5. 
 
Training 
data 
Testing 
data 
Prec. Recall F1 
Part Art. Part Art. 53.14 51.80 52.46 
Abs. Full Art. 54.32 54.64 54.48 
Mix Part Art. 59.59 59.74 59.66 
 
Table 5: Results of scope finding system with 
gold-standard hedge cues 
 
Results in Table 5 reveal that more abstract 
and full article dataset are added to the classifier 
as training data, better performance the system 
achieve. Thus we use the combination of abstract 
and full articles as training data for the final 
evaluation.  
Table 6 presents the results of our scope 
finding system with or without dependency 
features, using both gold-standard hedge cues 
and predicated hedge cues generated by our 
hedge cue finding system. 
Comparing the results in Table 4, 5, and 6, we 
observe that the performance of scope finding 
classifier on full article dataset is much lower 
than on abstract dataset, and dependency features 
are beneficial for the abstract dataset, but useless 
for full article dataset. We ascribe this 
phenomenon to the lack of enough full articles 
training data and the different properties of 
82
abstract and full articles data sets. Deep research 
is expected to continue. 
 
Hedge 
cues 
Dep. 
features 
Prec. Recall F1 
with 57.42 47.92 52.
24 
Predicted 
without 58.13 48.11 52.
65 
with 59.43 58.28 58.
85 
Gold 
standard 
without 60.20 58.86 59.
52 
 
Table 6: Results of scope finding system 
with/without dependency features using both 
gold-standard and predicated hedge cues 
4 Conclusion 
In this paper, we describe a machine learning 
system for detecting hedges and their scope in 
natural language texts. These two tasks are 
formalized as sequence labeling problems, and 
implemented using conditional random fields 
approach. We use a greedy forward procedure to 
select features for the classifier, and exploit rich 
syntactic features to achieve a better performance. 
In the in-domain evaluation, our system achieves 
the third score in biological data set for the first 
task, and achieves 0.5265 F1 score for the second 
task. 
 
Acknowledgments 
The authors would like to thank Buzhou Tang for 
useful discussions of the paper. This work is 
supported by the National High-tech R&D 
Program of China (863 Program, No. 
2007AA01Z194). 
References  
David B. Aronow, Fangfang Feng, and W. Bruce 
Croft. 1999. Ad Hoc Classification of Radiology 
Reports. Journal of the American Medical 
Informatics Association, 6(5):393?411. 
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper, and Bruce G. Buchanan. 2001. 
A Simple Algorithm for Identifying Negated 
Findings and Diseases in Discharge Summaries. 
Journal of Biomedical Informatics, 34:301?310. 
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, 
J?nos Csirik, and Gy?rgy Szarvas. 2010. The 
CoNLL-2010 Shared Task: Learning to Detect 
Hedges and their Scope in Natural Language Text. 
In Proceedings of the Fourteenth Conference 
on Computational Natural Language Learning 
(CoNLL-2010): Shared Task, pages 1?12.  
Yang Huang, and Henry J. Lowe. 2007. A novel 
hybrid approach to automated negation detection in 
clinical radiology reports. Journal of the 
American Medical Informatics Association, 
14(3):304?311. 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional random fields: 
Probabilistic models for segmenting and labeling 
sequence data. In Proceedings of the Eighteenth 
International Conference on Machine 
Learning, pages 282?289. 
David McClosky. 2009. Any Domain Parsing: 
Automatic Domain Adaptation for Natural 
Language Parsing. Ph.D. thesis, Department of 
Computer Science, Brown University. 
Ben Medlock, and Ted Briscoe. 2007. Weakly 
supervised learning for hedge classification in 
scientific literature. In Proc. of ACL 2007, pages 
992?999. 
Roser Morante, and Walter Daelemans. 2009. 
Learning the scope of hedge cues in biomedical 
texts. In Proceedings of the Workshop on 
BioNLP, pages 28?36.  
Pradeep G. Mutalik, Aniruddha Deshpande, and 
Prakash M. Nadkarni. 2001. Use of general-
purpose negation detection to augment concept 
indexing of medical documents: a quantitative 
study using the UMLS. Journal of the American 
Medical Informatics Association, 8(6):598?609. 
Kenji Sagae, and Jun?ichi Tsujii. 2007. Dependency 
parsing and domain adaptation with LR models 
and parser ensembles. In Proceedings of the 
CoNLL-2007 Shared Task, pages 82?94 
Gy?rgy Szarvas. 2008. Hedge classification in 
biomedical texts with a weakly supervised 
selection of keywords. In Proc. of ACL 2008, 
pages 281?289, Columbus, Ohio, USA. ACL. 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas, 
and J?nos Csirik. 2008. The BioScope corpus: 
annotation for negation, uncertainty and their scope 
in biomedical texts. In Proc. of BioNLP 2008, 
pages 38?45, Columbus, Ohio. ACL. 
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim, 
Tomoko Ohta, John McNaught, Sophia Ananiadou, 
and Jun'ichi Tsujii. 2005. Developing a Robust 
Part-of-Speech Tagger for Biomedical Text. 
Advances in Informatics - 10th Panhellenic 
Conference on Informatics, LNCS 3746, pages 
382?392. 
83
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 107?111,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Coreference Resolution with Loose Transitivity Constraints
Xinxin Li, Xuan Wang, Shuhan Qi
Shenzhen Graduate School
Harbin Institute of Technology, ShenZhen, China
lixxin2@gmail.com, wangxuan@insun.hit.edu.cn
shuhan qi@qq.com
Abstract
Our system treats coreference resolution as
an integer linear programming (ILP) problem.
Extending Denis and Baldridge (2007) and
Finkel andManning (2008)?s work, we exploit
loose transitivity constraints on coreference
pairs. Instead of enforcing transitivity closure
constraints, which brings O(n3) complexity,
we employ a strategy to reduce the number
of constraints without large performance de-
crease, i.e., eliminating coreference pairs with
probability below a threshold . Experimental
results show that it achieves a better perfor-
mance than pairwise classifiers.
1 Introduction
This paper describes our coreference resolution sys-
tem participating in the close track of CoNLL 2011
shared task (Pradhan et al, 2011). The task aims to
identify all mentions of entities and events and clus-
ter them into equivalence classes in OntoNotes Cor-
pus (Pradhan et al, 2007a). During the last decade,
several machine learning methods for coreference
resolution have been developed, from local pair-
wise classifiers (Soon et al, 2001) to global learn-
ing methods (Luo et al, 2004; Ng, 2005; Denis
and Baldridge, 2007), from simple morphological,
grammatical features to more liguistically rich fea-
tures on syntactic structures and semantic relations
(Pradhan et al, 2007b; Haghighi and Klein, 2009).
Our system supports both local classifiers and
global learning. Maximum entropy model is used
for anaphoricity and coreference, because it assigns
probability mass to mentions and coreference pairs
directly. In global phase, instead of determining
each coreference pair independently in a greedy
fashion, we employ an integer linear programming
(ILP) formulation for this problem. Extending (De-
nis and Baldridge, 2007) and (Finkel and Manning,
2008)?s work, we introduce a loose selection strat-
egy for transitivity constraints, attempting to over-
come huge computation complexity brought by tran-
sitivity closure constraints. Details are described in
section 2.3.
2 System Description
2.1 Mention Detection
Mention detection is a method that identifies the
anaphoricity and non-anaphoricity mentions before
coreference resolution. The non-anaphoric men-
tions usually influence the performance of corefer-
ence resolution as noises. Coreference resolution
can benefit from accurate mention detection since
it might eliminate the non-anaphoric mentions. We
take mention detection as the first step, and then
combine coreference classifier into one system.
Total 70 candidate features are used for mention
detection, including lexical, syntactic, semantic fea-
tures (Ng and Cardie, 2002). Features are selected
according to the information gain ratio (Han and
Kamber, 2006)
GainRation(A) =
Gain(A)
SplitInfo(A)
The top 10 features with highest gain ratio are:
string match, head word match, all uppercase, pro-
noun, starting with article, number, following prepo-
sition, nesting in verb phrase, nesting in preposition,
107
and starting with definite article. Many string fea-
tures that cannot be calculated by gain ratio method
are also added.
2.2 Coreference Determination
For coreference determination, we first build sev-
eral baseline systems with different training in-
stance generation methods and clustering algo-
rithms. These strategies are shown below. Detailed
description can be found in Ng (2005).
 training instance generation methods: Mc-
Carthy and Lehnerts method, Soon et al?s
method, Ng and Cardie?s method.
 clustering algorithms: closest-first clustering,
best-first clustering, and aggressive merge clus-
tering.
Overall 65 features are considered in our system.
Features are extracted from various linguistic infor-
mation, including:
 distance: sentence distance, minimum edit dis-
tance (Strube et al, 2002)
 lexical: string match, partial match, head word
match (Daume? III and Marcu, 2005)
 grammar: gender agreement, number agree-
ment(Soon et al, 2001)
 syntactic: same head, path (Yang et al, 2006)
 semantic: semantic class agreement, predicate
(Ponzetto and Strube, 2006; Ng, 2007)
Combining different training instance generation
methods and clustering algorithms, we get total 9
baseline systems. For each system, we use a greedy
forward approach to select features. Starting from
a base feature set (Soon et al, 2001), each feature
out of the base set is added one by one according to
the performance change on development data. Fi-
nally, the procedure is ended until the performance
is not improved. The baseline system with best per-
formance is selected for further improvement.
2.3 ILP with Loose Transitivity Constraints
Previous systems usually take coreference resolu-
tion as binary classification problem, and build the
coreference chain by determining each coreference
pair indepedently. The binary classifier is easily
implemented, but may cause inconsistency between
coreference pairs. Several work have been devel-
oped to overcome the problem, e.g., Bell trees (Luo
et al, 2004), conditional random fields (McCallum
and Wellner, 2004) and reranker (Ng, 2005).
Denis and Baldridge (2007) proposed an ILP for-
mulation to find the optimal solution for the prob-
lem. It utilizes the output of other local classifiers
and performs global learning. The objective func-
tion for their conference-only model takes the form:
min
X
hi;ji2M
2
c
hi;ji
 x
hi;ji
+ c
hi;ji
 (1  x
hi;ji
)
where c
hi;ji
=   log(P
C
), c
hi;ji
=   log(1   P
C
).
M is the candidate mention set for each document.
P
C
refers to the probability of coreference link be-
tween two mentions produced by our maximum en-
tropy model, and x
hi;ji
is a binary variable that is set
to 1 if two mentions are coreferent, 0 otherwise.
However, as Finkel and Manning showed, D&B?s
coreference-only model without transitivity con-
straints is not really necessary, because they only se-
lect the coreference links with probability P
C
> 0:5.
Klenner (2007) and Finkel and Manning (2008)?s
work extended the ILP framework to support tran-
sitivity constraints. The transitivity constraints are
formulated as
8i; j; k 2 M(i < j < k)
x
hi;ji
 x
hj;ki
+ x
hi;ki
  1
x
hj;ki
 x
hi;ji
+ x
hi;ki
  1
x
hi;ki
 x
hi;ji
+ x
hj;ki
  1
These constraints ensure that when any two core-
frent links (e.g., x
hi;ji
, x
hi;ki
) among three men-
tions exist, the third one x
hj;ki
must also be a link.
However, these constraints also bring huge time and
space complexity with n3 constraints (n is number of
candidate mention set M, which is larger than 700
in some documents), and cannot be solved in a re-
stricted time and memory environment. We intro-
duce a loose method to eliminate conference links
108
Ratio Recall Precision F-value
0.4 84.03 43.75 57.54
0.6 70.6 70.85 70.72
0.8 64.24 74.35 68.93
1.0 58.63 76.13 66.25
Table 1: Results of mention dection
below a probability threshold . The constraints are
transformed as
x
hi;ki
+ x
hj;ki
 1 (1)
x
hi;ji
= 0 (2)
when P
C
(i; j) < . The threshold  is tuned on de-
velopment data for faster computation without large
performance decrease.
3 Experiments and Analysis
In the paper we mainly take noun phrases (NPs) and
pronouns as candidate mentions, and ignore other
phrases since more than 91% of the mentions are
NPs and pronouns.
3.1 Mention Detection
We observe that the ratio of positive examples and
negative examples is about 1:3 in training data. To
balance the bias, we propose a ratio control method
which sets a ratio to limit the number of negative
examples. Our system will select all positive exam-
ples, and part of negative examples according to the
ratio. By tuning the ratio, we can control the propor-
tion of positive and negative examples. With differ-
ent ratios for negative feature selection, the results
on development data are shown in table 1.
From table 1, we can see that as the ratio in-
creases, recall becomes smaller and precision be-
comes larger. Small threshold means less negative
examples are generated in training procedure, and
the classifier tends to determine a mention as posi-
tive. Finally, we choose the ratio 0.6 for our model
because it gets the best F-value on the development
data.
3.2 Coreference Resolution
Our system participates in the close track with
auto mention and gold boundary annotation. The
TIGM Soon Soon Soon Ng
CA A B C B
MUC 44.29 46.18 46.18 45.33
B
3 59.76 61.39 60.03 60.93
CEAF(M) 42.77 44.43 43.01 44.41
CEAF(E) 35.77 36.37 36.08 36.54
BLANC 60.22 63.94 59.9 63.96
Official 46.6 47.98 46.76 47.6
Table 2: Results of baseline systems
the performance is evaluated on MUC, B-CUBED,
CEAF(M), CEAF(E), BLANC metrics. The official
metric is calculated as (MUC+B
3
+CEAF )
=
3
.
Table 2 summarizes the performance of top 4 of
9 baseline systems with different training instance
generation methods and clustering algorithms on de-
velopment data. In the table, TIGM means training
instance generation method, and CA denotes clus-
tering algorithm, which includes C as closest-first,
B as best-first, and A as aggressive-merge clustering
algorithm. The results in Table 2 show that the sys-
tem with Soon?s training instance generation method
and best-first clustering algorithm achieves the best
performance. We take it as baseline for further im-
provement.
In ILP model, we perform experiments on docu-
ments with less than 150 candidate mentions to find
the suitable probability threshold  for loose tran-
sitivity constraints. There are totol 181 documents
meeting the condition in development data. We take
two strategies to loose transitivity constraints: (I)
formula 1 and 2, and (II) formula 2 only. Glpk pack-
age is used to solve our ILP optimization problems.1
Table 3 shows that as threshold  increases, the
running time reduces dramatically with a small per-
formance decrease from 49.06 to 48.88. Strategy I
has no benefit for the performance. Finally strategy
II and  = 0:06 are used in our system.
We also combine mentions identified in first phase
into coreference resolution. Two strategies are used:
feature model and cascaded model. For feature
model, we add two features which indicate whether
the two candidate mentions of a coreference pair are
mentions identified in first phase or not. For cas-
caded model, we take mentions identified in first
phase as inputs for coreference resolution. For ILP
1http://www.gnu.org/software/glpk/
109
 0 0.02 0.02 0.04 0.04 0.06 0.06 0.08 0.08 0.1 0.1
Strategy I II I II I II I II I II
MUC 40.95 40.64 40.92 40.64 40.83 40.64 40.8 40.64 40.75 40.64 40.68
B
3 65.6 65.47 65.59 65.47 65.58 65.47 65.57 65.47 65.5 65.47 65.49
CEAF(M) 48.62 48.39 48.59 48.39 48.56 48.39 48.54 48.39 48.42 48.39 48.39
CEAF(E) 40.62 40.47 40.62 40.47 40.63 40.47 40.61 40.47 40.5 40.47 40.47
BLANC 61.87 61.76 61.85 61.76 61.84 61.76 61.83 61.76 61.79 61.76 61.78
Official 49.06 48.88 49.04 48.88 49.01 48.88 48.99 48.88 48.92 48.88 48.88
Time(s) 1726 1047 913 571 451 361 264 253 166 153 109
Table 3: Results on different probability thresholds and strategies
Model Feature Cascade ILP
MUC 41.08 47.41 45.89
B
3 59.74 57.67 61.85
CEAF(M) 41.9 42.04 44.52
CEAF(E) 34.72 32.33 36.85
BLANC 61.1 62.99 63.92
Official 45.18 45.81 48.19
Table 4: Results of coreference resolution systems.
model, we perform experiments on coreference-only
system with our loose transitivity constraints. The
results on development data are shown in Table 4.
In Core Quad 2.40G CPU and 2G memory ma-
chine, our ILP model can optimize one document
per minute on average. From table 4, we can see that
the ILP model achieves the best F-value, implying
the benefit of our algorithm. It also shows that tra-
ditional coreference resolution methods combining
mention detection decrease the performance. For
restricted time deadline, other constraints strategies
(Klenner, 2007) and joint anaphoricity-coreference
ILP model are not used in our system. It would be
in our future work.
3.3 Test
Table 5 shows the performance of our system for
both development and test data, with auto mention
and gold boundary annotation.
The results in table 5 show that in auto mention
annotation, the performance on test data is a little
bit better than development data. The reason might
be that the system on test data uses more data to
train, including development data. A phenomenon
surprises us is that the performance on test data with
gold annotation is less than on development data,
Data Dev Dev Test Test
Mention Auto Gold Auto Gold
MUC 45.89 46.75 46.62 44.00
B
3 61.85 61.48 61.93 57.42
CEAF(M) 44.52 45.17 44.75 42.36
CEAF(E) 36.85 37.19 36.83 34.22
BLANC 63.92 63.83 64.27 62.96
Official 48.19 48.47 48.46 45.21
Table 5: Results for development and test data
even than auto annotation. It turns out that the mis-
take is made because we confuse the the definition
of gold bourdaries as gold mentions, which are ?all?
and ?only? mentions in coreference chains.
4 Conclusion
In this paper, we present a coreference resolution
system which employs an ILP formulation for global
optimization. To reduce computation complexity,
our system employs loose transitivity constraints to
the ILP model. Experimental results show that it
achieves a better performance than pairwise classi-
fiers.
References
Hal Daume? III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 97?104, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
110
ing integer programming. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
236?243, Rochester, New York, April. Association for
Computational Linguistics.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of ACL-08: HLT, Short Papers, pages 45?
48, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1152?1161, Singapore, August. Association for Com-
putational Linguistics.
J. Han and M. Kamber. 2006. Data mining: concepts
and techniques. Morgan Kaufmann.
Manfred Klenner. 2007. Enforcing consistency on coref-
erence sets. In Recent Advances in Natural Language
Processing (RANLP), pages 323?328.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 135?142, Barcelona,
Spain, July.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In NIPS 2004.
Vincent Ng and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. In Proceedings of the 19th in-
ternational conference on Computational linguistics -
Volume 1, COLING ?02, pages 1?7, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Vincent Ng. 2005. Machine learning for coreference res-
olution: From local classification to global ranking. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
157?164, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of IJCAI, pages 1689?
1694.
Simone Paolo Ponzetto and Michael Strube. 2006. Ex-
ploiting semantic role labeling, wordnet and wikipedia
for coreference resolution. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Main Conference, pages 192?199, New York City,
USA, June. Association for Computational Linguis-
tics.
Sameer Pradhan, Eduard Hovy, Mitch Marcus, Martha
Palmer, Lance Ramshaw, and Ralph Weischedel.
2007a. Ontonotes: A unified relational semantic rep-
resentation. International Journal of Semantic Com-
puting (IJSC), 1(4):405?419.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007b. Un-
restricted coreference: Identifying entities and events
in ontonotes. In in Proceedings of the IEEE Inter-
national Conference on Semantic Computing (ICSC),
September 17-19.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to coref-
erence resolution of noun phrases. Comput. Linguist.,
27:521?544, December.
Michael Strube, Stefan Rapp, and Christoph Mu?ller.
2002. The influence of minimum edit distance on
reference resolution. In Proceedings of the ACL-02
conference on Empirical methods in natural language
processing - Volume 10, EMNLP ?02, pages 312?319,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006.
Kernel-based pronoun resolution with structured syn-
tactic knowledge. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 41?48, Sydney, Australia,
July. Association for Computational Linguistics.
111
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 83?87,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Simple Maximum Entropy Models for Multilingual Coreference Resolution
Xinxin Li, Xuan Wang, Xingwei Liao
Computer Application Research Center
Harbin Institute of Technology Shenzhen Graduate School
Shenzhen, China
lixxin2@gmail.com
Abstract
This paper describes our system participat-
ing in the CoNLL-2012 shared task: Mod-
eling Multilingual Unrestricted Coreference
in Ontonotes. Maximum entropy models are
used for our system as classifiers to deter-
mine the coreference relationship between ev-
ery two mentions (usually noun phrases and
pronouns) in each document. We exploit rich
lexical, syntactic and semantic features for the
system, and the final features are selected us-
ing a greedy forward and backward strategy
from an initial feature set. Our system partici-
pated in the closed track for both English and
Chinese languages.
1 Introduction
In this paper, we present our system for the CoNLL-
2012 shared task which aims to model coreference
resolution for multiple languages. The task of coref-
erence resolution is to group different mentions in a
document into coreference equivalent classes (Prad-
han et al, 2012). Plenty of machine learning al-
gorithms such as Decision tree (Ng and Cardie,
2002), maximum entropy model, logistic regres-
sion (Bjo?rkelund and Nugues, 2011), Support Vec-
tor Machines, have been used to solve this problem.
Meanwhile, the CoNLL-2011 shared task on En-
glish language show that a well-designed rule-based
approach can achieve a comparable performance as
a statistical one (Pradhan et al, 2011).
Our system treats coreference resolution problem
as classification problem by determining whether
every two mentions in a document has a corefer-
ence relationship or not. We use maximum entropy
(ME) models to train the classifiers. Previous work
reveal that features play an important role on coref-
erence resolution problem, and many different kinds
of features has been exploited. In this paper, we use
many different lexical, syntactic and semantic fea-
tures as candidate features, and use a greedy forward
and backward approach for feature selection for ME
models.
2 System Description
The framework of our system is shown in figure 1. It
includes four components: candidate mention selec-
tion, training example generation, model generation,
and decoding algorithm for test data. The details of
each component as described below.
2.1 Candidate Mention Selection
In both training and test sets, our system only con-
sider all noun phrases (NP) and pronouns (PRP,
PRP$) as candidate mentions for both English and
Chinese. The mentions in each sentence are ob-
tained from given syntactic tree by their syntactic
label. Other phrases in the syntactic tree are omit-
ted due to their small proportion. For example, in
the English training dataset, our candidate mentions
includes about 91% of golden mentions.
2.2 Training Example Generation
There are many different training example gen-
eration algorithms, e.g., McCarthy and Lehnert?s
method, Soon et als method, Ng and Cardies
method (Ng, 2005). For our baseline system, we
choose Soon et al?s method because it is easily un-
derstandable, implemented and popularly used. It
83
Figure 1: The framework of our coreference resolution
system
selects pairs of two coreferent mentions as positive
examples, and pairs between mentions among the
two mentions and the last mention as negative ex-
amples.
2.3 Feature Selection
Rich and meaningful features are important for
coreference resolution. Our system starts with
Soon?s 12 features as baseline features (Soon et al,
2001), and exploits many lexical, syntactic, and se-
mantic features as candidate features. Totally 71 fea-
tures are considered in our system, and summarized
below:
 Distance features: sentence distance, distance
in phrases, whether it?s a first mention (Strube
et al, 2002)
 Lexical features: string match, partial match,
apposition, proper name match, head word
match, partial head word match, minimum edit
distance (Daume? III and Marcu, 2005)
 Grammatical features: pronoun, demonstrative
noun phrase, embedded noun, gender agree-
ment, number agreement (Soon et al, 2001)
 Syntactic features: same head, maximal NP,
syntactic path (Yang et al, 2006)
 Semantic features: semantic class agreement,
governing verb and its grammatical role, predi-
cate (Ponzetto and Strube, 2006)
For English, the number agreement and gender
agreement features can be obtained through the gen-
der corpus provided. However, there is no corpus
for Chinese. Our system obtains this information
by collecting dictionaries for number and gender in-
formation from training dataset. For example, the
Algorithm 1 Greedy forward and backward feature
selection
Initialization: all candidate features in set C
Choose initial feature set 
Compute F1 with features c
while forward jj backward:
while forward:
for each feature f in C-c
Compute F1 with features c+f
if best(F1) increases:
backward = true, c=c+f, continue forward
else forward = false
while backward:
for each feature f in in c
Compute F1 with features c-f
if best(F1) increases:
forward = true, c=c-f continue backward
else backward = false
pronoun ??? (he) denotes a male mention, and the
noun phrase ?s?? (girlfriend) represents a female
mention. Similarly for number information, e.g., the
mentions containing ??? (and), ??? (group) are
plural. We use these words to build number and
gender dictionaries, and determine the number and
gender information of a new mention by checking
whether one of the words in the dictionaries is in the
mention.
For semantic class agreement feature in English,
the relation between two mentions is extracted from
WordNet 3.0 (Ng, 2007),(Miller, 1995). There is no
corresponding dictionary for Chinese, so we keep
it blank. The head word for each mention is se-
lected by its dependency head, which can be ex-
tracted throught the conversion head rules ( English
1 and Chinese 2).
Maximum Entropy modeling is used to train the
classifier for our system 3. We employ a greedy for-
ward and backward procedure for feature selection.
The procedure is shown in Algorithm 1.
The algorithm will iterate forward and backward
procedures until the performance does not improve.
We use two initial feature sets: a blank set and
Soon?s baseline feature set. Both feature sets start
1http://w3.msi.vxu.se/ nivre/research/headrules.txt
2http://w3.msi.vxu.se/ nivre/research/chn headrules.txt
3http://homepages.inf.ed.ac.uk/lzhang10/maxent.html
84
with a forward procedure.
2.4 Decoding
For every candidate mention pair, to determine their
coreference relationship is simple because the prob-
ability whether they are coreferent can be obtained
by our maximum entropy model. We can just set a
threshold  = 0:5 and select the pairs with probabil-
ity larger than . But usually it is hard for multiple
mentions. Suppose there are three mentions A, B, C
where the probability between A and B, A and C is
larger than , but B and C is small. Thus choosing
an appropriate decoding algorithm is necessary.
We use best-first clustering method for our system
which for each candidate mention in a document,
chooses the mention before it with best probability
larger than threshold . The difference between En-
glish and Chinese is that we consider the coreference
relationship of two mentions nested in Chinese, but
not in English.
3 Experiments
3.1 Setting
Our system participates in the English and Chinese
closed tracks with auto mentions. For both the En-
glish and Chinese datasets, we use gold annotated
training data for training, and a portion of auto an-
notated development data for feature selection. Only
part of development data is chosen because the eval-
uation procedure takes lot of time. To simplify, We
only select one or two file in each directory as our
development data.
The performance of the system is evaluated on
MUC, B-CUBED, CEAF(M), CEAF(E), BLANC
metrics. The official metric is calculated as
(MUC+B
3
+CEAF )
=
3
.
3.2 Development set
Figures 2 and 3 show the performance on the En-
glish and Chinese development datasets using fea-
ture selection starting from a empty feature set and
Soon?s baseline feature set. The x-axis means the
number of iterations with either forward or back-
ward selection. The performance on Soon?s baseline
feature set for both languages are shown on 1st itera-
tion. The performance from empty feature set starts
on 2nd iteration. From these figures, we can see that
Figure 2: Performance of English development data with
Feature selection
Figure 3: Performance of Chinese development data with
Feature selection
using feature selection in both initial feature sets, the
performance improves.
However the performance of our system is im-
proved only on a few iteration. The best system for
English stops at the 4th iteration with total 10 fea-
tures left, which starts from Soon?s baseline feature
set. Similarly, the system for Chinese achieves its
best performance at the 4th iteration with only 8 fea-
tures. The phenomenon reveals that most of the fea-
tures left for our system are still from Soon?s base-
line features, and our newly exploited lexical, syn-
tactic, and semantic features are not well utilized.
Then we evaluate our model on the entire devel-
opment data. The results are shown on Table 1.
Comparing Figures 2, 3 and Table 1, we can observe
that the performance on entire development data is
lower than part one, about 1% decrease.
3.3 Test
For test data, we retrain our model on both gold
training data and development data using the se-
lected features. The final results for English and
Chinese are shown in Table 2.
85
Model English Chinese
MUC 49.28 48.31
B
3 62.79 67.97
CEAF(M) 46.77 49.49
CEAF(E) 38.19 38.9
BLANC 66.31 68.91
Average 50.09 51.73
Table 1: Results on entire development data
Model English Chinese
MUC 48.27 48.09
B
3 61.37 68.31
CEAF(M) 44.83 49.92
CEAF(E) 36.68 38.89
BLANC 65.42 71.44
Official 48.77 51.76
Table 2: Results on test data
Comparing tables 2 and 1, we can observe that
the performance for the Chinese test data is similar
as the development data. The result seems reason-
able because the model for testing use additional de-
velopment data which is much smaller than training
data. However, the result on English test data seem a
little odd. The performance is about 1.4% less than
that on the development data. The result needs fur-
ther analysis.
4 Conclusion
In this paper, we presented our coreference resolu-
tion system which uses maximum entropy model to
determine the coreference relationship between two
mentions. Our system exploits many lexical, syn-
tactic and semantic features. However, using greedy
forward and backward feature selection strategy for
ME model, these rich features are not well utilized.
In future work we will analyze the reason for this
phenomenon and extend these features to other ma-
chine learning algorithms.
References
Anders Bjo?rkelund and Pierre Nugues. 2011. Explor-
ing lexicalized features for coreference resolution. In
Proceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 45?50, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
Hal Daume? III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 97?104, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
George A. Miller. 1995. Wordnet: a lexical database
for english. Communications of the ACM, 38:39?41,
November.
Vincent Ng and Claire Cardie. 2002. Improvingmachine
learning approaches to coreference resolution. In Pro-
ceedings of the ACL, pages 104?111.
Vincent Ng. 2005. Machine learning for coreference res-
olution: From local classification to global ranking. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
157?164, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Vincent Ng. 2007. Semantic class induction and coref-
erence resolution. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 536?543, Prague, Czech Republic, June.
Association for Computational Linguistics.
Simone Paolo Ponzetto and Michael Strube. 2006. Ex-
ploiting semantic role labeling, wordnet and wikipedia
for coreference resolution. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Main Conference, pages 192?199, New York City,
USA, June. Association for Computational Linguis-
tics.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?27, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unrestricted
coreference in OntoNotes. In Proceedings of the
Sixteenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2012), Jeju, Korea.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to coref-
erence resolution of noun phrases. Comput. Linguist.,
27:521?544, December.
Michael Strube, Stefan Rapp, and Christoph Mu?ller.
2002. The influence of minimum edit distance on
86
reference resolution. In Proceedings of the ACL-02
conference on Empirical methods in natural language
processing - Volume 10, EMNLP ?02, pages 312?319,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006.
Kernel-based pronoun resolution with structured syn-
tactic knowledge. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 41?48, Sydney, Australia,
July. Association for Computational Linguistics.
87
