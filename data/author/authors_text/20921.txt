Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 32?41,
Gothenburg, Sweden, April 26 2014.
c?2014 Association for Computational Linguistics
A Multilingual Evaluation of Three Spelling Normalisation
Methods for Historical Text
Eva Pettersson
1,2
, Be?ta Megyesi
1
and Joakim Nivre
1
(1) Department of Linguistics and Philology
Uppsala University
(2) Swedish National Graduate School
of Language Technology
firstname.lastname@lingfil.uu.se
Abstract
We present a multilingual evaluation of
approaches for spelling normalisation of
historical text based on data from five
languages: English, German, Hungarian,
Icelandic, and Swedish. Three different
normalisation methods are evaluated: a
simplistic filtering model, a Levenshtein-
based approach, and a character-based sta-
tistical machine translation approach. The
evaluation shows that the machine transla-
tion approach often gives the best results,
but also that all approaches improve over
the baseline and that no single method
works best for all languages.
1 Introduction
Language technology for historical text is a field
of research imposing a variety of challenges. Nev-
ertheless, there is an increasing need for natural
language processing (NLP) tools adapted to his-
torical texts, as an aid for researchers in the hu-
manities field. For example, the historians in the
Gender and Work project are studying what men
and women did for a living in the Early Mod-
ern Swedish society (?gren et al., 2011). In this
project, researchers have found that the most im-
portant words in revealing this information are
verbs such as fishing, selling etc. Instead of man-
ually going through written sources from this time
period, it is therefore assumed that an NLP tool
that automatically searches through a number of
historical documents and presents the contained
verbs (and possibly their complements), would
make the process of finding relevant text passages
more effective.
A major challenge in developing language tech-
nology for historical text is that historical language
often is under-resourced with regard to annotated
data needed for training NLP tools. This prob-
lem is further aggravated by the fact that histori-
cal texts may refer to texts from a long period of
time, during which language has changed. NLP
tools trained on 13th century texts may thus not
perform well on texts from the 18th century. Fur-
thermore, historical language usually shows a sub-
stantial variation in spelling and grammar between
different genres, different authors and even within
the same text written by the same author, due to
the lack of spelling conventions.
To deal with the limited resources and the high
degree of spelling variation, one commonly ap-
plied approach is to automatically normalise the
original spelling to a more modern spelling, be-
fore applying the NLP tools. This way, NLP tools
available for the modern language may be used
to analyse historical text. Even though there may
be structural differences as well between histor-
ical and modern language, spelling is the most
striking difference. Moreover, language technol-
ogy tools such as taggers often to some degree
rely on statistics on word form n-grams and to-
ken frequencies, implying that spelling moderni-
sation is an important step for improving the per-
formance of such tools when applied to historical
text. This paper presents an evaluation of three
approaches to spelling normalisation: 1) a filter-
ing approach based on corpus data, 2) an approach
based on Levenshtein edit distance, and 3) an
approach implementing character-based statistical
machine translation (SMT) techniques. These ap-
proaches have previously solely been evaluated in
isolation, without comparison to each other, and
for one or two languages only. We compare the
results of the different methods in a multilingual
evaluation including five languages, and we show
that all three approaches have a positive impact on
normalisation accuracy as compared to the base-
line. There is no single method that yields the
highest normalisation accuracy for all languages,
but for four out of five languages within the scope
32
of our study, the SMT-based approach gives the
best results.
2 Related Work
Spelling normalisation of historical text has pre-
viously been approached using techniques such as
dictionary lookup, edit distance calculations, and
machine translation.
Rayson et al. (2005) tried an approach based on
dictionary lookup, where a mapping scheme from
historical to modern spelling for 16th to 19th cen-
tury English texts was manually created, resulting
in the VARD tool (VARiant Detector) comprising
45,805 entries. The performance of the normal-
isation tool was evaluated on a set of 17th cen-
tury texts, and compared to the performance of
modern spell checkers on the same text. The re-
sults showed that between a third and a half of
all tokens (depending on which test text was used)
were correctly normalised by both VARD and MS
Word, whereas approximately one third of the to-
kens were correctly normalised only when using
VARD. The percentage of tokens correctly nor-
malised only by MS Word was substantially lower;
approximately 6%. VARD was later further devel-
oped into VARD2, combining the original word
list with data-driven techniques in the form of pho-
netic matching against a modern dictionary, and
letter replacement rules based on common spelling
variation patterns (Baron and Rayson, 2008).
Jurish (2008) argued that due to the lack of or-
thographic conventions, spelling generally reflects
the phonetic form of the word to a higher de-
gree in historical text. Furthermore, it is assumed
that phonetic properties are less resistant to di-
achronic change than orthography. Accordingly,
Jurish explored the idea of comparing the simi-
larity between phonetic forms rather than ortho-
graphic forms. For grapheme-to-phoneme conver-
sion, a module of the IMS German Festival text-
to-speech system (Black and Taylor, 1997) was
used, with a rule-set adapted to historical word
forms. Evaluation was performed on a corpus of
historical German verse quotations extracted from
Deutsches W?rterbuch, containing 5,491,982 to-
kens (318,383 types). Without normalisation, ap-
proximately 84% of the tokens were recognised
by a morphological analyser. After normalisa-
tion, 92% of the tokens were recognised. Adding
lemma-based heuristics, coverage increased fur-
ther to 94% of the tokens.
A Levenshtein similarity approach to normal-
isation was presented by Bollmann et al. (2011)
for Early New High German, where Levenshtein-
based normalisation rules were automatically de-
rived from a word-aligned parallel corpus consist-
ing of the Martin Luther Bible in its 1545 edi-
tion and its 1892 version, respectively. Using this
normalisation technique, the proportion of words
with a spelling identical to the modern spelling in-
creased from 65% in the original text to 91% in the
normalised text. This normalisation method was
further evaluated by Bollmann (2013), comparing
the performance of the RFTagger applied to histor-
ical text before and after normalisation. For every
evaluation text, the tagger was trained on between
100 and 1,000 manually normalised tokens, and
evaluated on the remaining tokens in the same text.
For one manuscript from the 15th century, tagging
accuracy was improved from approximately 29%
to 78% using this method.
Another Levenshtein-based approach to nor-
malisation was presented by Pettersson et al.
(2013b), using context-sensitive, weighted edit
distance calculations combined with compound
splitting. This method requires no annotated his-
torical training data, since normalisation candi-
dates are extracted by Levenshtein comparisons
between the original historical word form and
present-day dictionary entries. However, if a cor-
pus of manually normalised historical text is avail-
able, this can optionally be included for dictio-
nary lookup and weighted Levenshtein calcula-
tions, improving precision. This technique was
evaluated for Early Modern Swedish, and in the
best setting, the proportion of words in the his-
torical text with a spelling identical to the mod-
ern gold standard spelling increased from 64.6%
to 86.9%.
Pettersson et al. (2013a) treated the normalisa-
tion task as a translation problem, using character-
based SMT techniques in the spelling normalisa-
tion process. With the SMT-based approach, the
proportion of tokens in the historical text with
a spelling identical to the modern gold standard
spelling increased from 64.6% to 92.3% for Early
Modern Swedish, and from 64.8% to 83.9% for
15th century Icelandic. It was also shown that nor-
malisation had a positive effect on subsequent tag-
ging and parsing.
Language technology for historical text also has
a lot in common with adaptation of NLP tools
33
for handling present-day SMS messages and mi-
croblog text such as Twitter. In both genres there
is a high degree of spelling variation, ad hoc ab-
breviations and ungrammatical structures impos-
ing the problem of data sparseness. Similar meth-
ods for spelling normalisation may thus be used
for both tasks. Han and Baldwin (2011) pre-
sented a method for normalising SMS and Twitter
text based on morphophonemic similarity, com-
bining lexical edit distance, phonemic edit dis-
tance, prefix substring, suffix substring, and the
longest common subsequence. Context was taken
into account by means of dependency structures
generated by the Stanford Parser applied to a cor-
pus of New York Times articles. In the best set-
ting, a token-level F-score of 75.5% and 75.3%
was reported for SMS messages and Twitter texts
respectively.
3 Approaches
3.1 The Filtering Approach
The filtering approach presupposes access to a par-
allel training corpus of token pairs with historical
word forms mapped to their modernised spelling.
In the normalisation process, whenever a token is
encountered that also occurred in the training data,
the most frequent modern spelling associated with
that token in the training corpus is chosen for nor-
malisation. Other tokens are left unchanged.
3.2 The Levenshtein-based Approach
The Levenshtein-based approach was originally
presented by Pettersson et al. (2013b). In its basic
version, no historical training data is needed,
which is an important aspect considering the
common data sparseness issue, as discussed in
Section 1. Instead, a modern language dictionary
or corpus is required, from which normalisation
candidates are extracted based on edit distance
comparisons to the original historical word form.
If there is parallel data available, i.e. the same
text in its historical and its modernised spelling,
this data can be used to make more reliable Lev-
enshtein calculations by assigning weights lower
than 1 to frequently occurring edits observed in
the training data. The weights are then calculated
by comparing the frequency of each edit occurring
in the training corpus to the frequency with which
the specific source characters are left unchanged,
in accordance with the following formula:
Frequency of Unchanged
Frequency of Edit + Frequency of Unchanged
Context-sensitive weights are added to handle ed-
its affecting more than one character. The context-
sensitive weights are calculated by the same for-
mula as the single-character weights, and include
the following operations:
? double deletion: personnes? persons
? double insertion: strait? straight
? single-to-double substitution: juge? judge
? double-to-single substitution: moost? most
For all historical word forms in the training cor-
pus that are not identical in the modern spelling,
all possible single-character edits as well as multi-
character edits are counted for weighting. Hence,
the historical word form personnes, mapped to
the modern spelling persons, will yield weights
for double-to-single deletion of -ne, as illustrated
above, but also for single deletion of -n and single
deletion of -e.
Finally, a tuning corpus is used to set a
threshold for which maximum edit distance
to allow between the original word form and
its normalisation candidate(s). Based on the
average edit distance between the historical
word forms and their modern spelling in the
tuning corpus, the threshold is calculated by the
following formula (where 1.96 times the stan-
dard deviation is added to cover 95% of the cases):
avg editdistance +(1.96?standard deviation)
If several normalisation candidates have the same
edit distance as compared to the source word, the
most frequent candidate is chosen, based on mod-
ern corpus data. If none of the highest-ranked nor-
malisation candidates are present in the corpus, or
if there are several candidates with the same fre-
quency distribution, a final candidate is randomly
chosen.
3.3 The SMT-based Approach
In the SMT-based approach, originally presented
by Pettersson et al. (2013a), spelling normali-
sation is treated as a translation task. To ad-
dress changes in spelling rather than full transla-
tion of words and phrases, character-based trans-
lation (without lexical reordering) is performed,
a well-known technique for transliteration and
34
character-level translation between closely related
languages (Matthews, 2007; Vilar et al., 2007;
Nakov and Tiedemann, 2012). In character-level
SMT, phrases are modeled as character sequences
instead of word sequences, and translation models
are trained on character-aligned parallel corpora
whereas language models are trained on character
N-grams.
Since the set of possible characters in a lan-
guage is far more limited than the number of pos-
sible word forms, and the same corpus will present
a larger quantity of character instances than token
instances, only a rather small amount of parallel
data is needed for training the translation models
and the language models in character-based trans-
lation. Pettersson et al. (2013a) showed that with
a training and tuning set of only 1,000 pairs of his-
torical word forms mapped to modern spelling, a
normalisation accuracy of 76.5% was achieved for
Icelandic, as compared to 83.9% with a full-sized
training corpus of 33,888 token pairs. Their full
experiment on varying the size of the training data
is illustrated in Figure 1.
 76 77
 78 79
 80 81
 82 83
 84 85
 0  5  10  15  20  25  30  35
N
o
r
m
a
l
i
s
a
t
i
o
n
 
a
c
c
u
r
a
c
y
Size of training data (K tokens)
Normalisation accuracy for different sizes of the alignment training data
Figure 1: Normalisation accuracy when varying
the size of the alignment training data.
We use the same set of training data for the SMT
approach as for the filtering approach and for the
assignment of weights in the Levenshtein-based
approach, i.e. a set of token pairs mapping his-
torical word forms to their manually modernised
spelling. These corpora have the format of one to-
ken per line, with blank lines separating sentences.
To fully adapt this format to the format needed
for training the character-based translation mod-
els, the characters within each token are separated
by space. The SMT system will now regard each
character as a word, the full token as a sentence
and the entire sentence as a section.
The SMT engine used is Moses with all its stan-
dard components. A phrase-based model is ap-
plied, where the feature weights are trained us-
ing MERT with BLEU over character-sequences
as the objective function. The maximum size of a
phrase (sequence of characters) is set to 10.
Two different character alignment techniques
are tested: (i) the word alignment toolkit GIZA++
(Och and Ney, 2000), and (ii) a weighted finite
state transducer implemented in the m2m-aligner
(Jiampojamarn et al., 2007). GIZA is run with
standard word alignment models for character un-
igrams and bigrams, whereas the m2m aligner
implements transducer models based on context-
independent single character and multi-character
edit operations. The transducer is trained us-
ing EM on (unaligned) parallel training data, and
the final model can then be used to produce a
Viterbi alignment between given pairs of charac-
ter strings.
An example is given in Figure 2, where the Ice-
landic word forms me?r? me?ur and giallda?
galda have been aligned at a character-level using
the m2m-aligner. In this example, the  symbol
represents empty alignments, meaning insertions
or deletions. The  symbol in the source word
me?r denotes the insertion of u in the target word
me?ur. Likewise, the  symbol in the target word
galda denotes the deletion of i as compared to the
source word giallda. Furthermore, the alignment
of giallda to galda illustrates the inclusion of
multi-character edit operations, where the colon
denotes a 2:1 alignment where both letters l and d
in the source word correspond to the single letter
d in the target word.
m|e|?||r| m|e|?|u|r|
g|i|a|l|l:d|a| g||a|l|d|a|
Figure 2: m2m character-level alignment.
4 Data
In the following, we will describe the data sets
used for running the filtering approach, the Lev-
enshtein edit distance approach, and the character-
based SMT approach for historical spelling nor-
malisation applied to five languages: English, Ger-
man, Hungarian, Icelandic, and Swedish. For
convenience, we use the notions of training, tun-
35
ing and evaluation corpora, which are well-known
concepts within SMT. These data sets have been
created by extracting every 9th sentence from the
total corpus to the tuning corpus, and every 10th
sentence to the evaluation corpus, whereas the rest
of the sentences have been extracted to a training
corpus.
1
In the filtering approach, there is in fact no
distinction between training and tuning corpora,
since both data sets are combined in the dictionary
lookup process. As for the Levenshtein edit dis-
tance approach, the training corpus is used for ex-
tracting single-character and multi-character edits
by comparing the historical word forms to their
modern spelling. The edits extracted from the
training corpus are then weighted based on their
relative frequency in the tuning corpus.
The historical texts used for training and evalu-
ation are required to be available both in their orig-
inal, historical spelling and in a manually mod-
ernised and validated spelling. A modern trans-
lation of a historical text is generally not usable,
since word order and sentence structure have to re-
main the same to enable training and evaluation of
the proposed methods. The access to such data is
very limited, meaning that the data sets used in our
experiments vary in size, genres and time periods
between the languages.
4.1 English
For training, tuning and evaluation in the En-
glish experiments, we use the Innsbruck Cor-
pus of English Letters, a manually normalised
collection of letters from the period 1386?1698.
This corpus is a subset of the Innsbruck Com-
puter Archive of Machine-Readable English Texts,
ICAMET (Markus, 1999). A subset of the British
National Corpus (BNC) is used as the single mod-
ern language resource both for the Levenshtein-
based and for the SMT-based approach. Table 1
presents in more detail the data sets used in the
English experiments.
4.2 German
For training, tuning and evaluation in the German
experiments, we use a manually normalised sub-
set of the GerManC corpus of German texts from
the period 1650?1800 (Scheible et al., 2011). This
subset contains 22 texts from the period 1659?
1780, within the genres of drama, newspaper text,
1
For information on how to access the data sets used in
our experiments, please contact the authors.
Resource Data Tokens Types
Training ICAMET 148,852 18,267
Tuning ICAMET 16,461 4,391
Evaluation ICAMET 17,791 4,573
Lev. dict. BNC 2,088,680 69,153
Lev. freq. BNC 2,088,680 69,153
SMT lm BNC 2,088,680 69,153
Table 1: Language resources for English.
letters, sermons, narrative prose, humanities, sci-
ence och legal documents. The German Parole
corpus is used as the single modern language re-
source both for the Levenshtein-based and for the
SMT-based approach (Teubert (ed.), 2003). Table
2 presents in more detail the data sets used in the
German experiments.
Resource Data Tokens Types
Training GerManC 39,887 9,055
Tuning GerManC 5,418 2,056
Evaluation GerManC 5,005 1,966
Lev. dict. Parole 18,662,243 662,510
Lev. freq. Parole 18,662,243 662,510
SMT lm Parole 18,662,243 662,510
Table 2: Language resources for German.
4.3 Hungarian
For training, tuning and evaluation in the Hungar-
ian experiments, we use a collection of manually
normalised codices from the Hungarian Gener-
ative Diachronic Syntax project, HGDS (Simon,
To appear), in total 11 codices from the time pe-
riod 1440?1541. The Szeged Treebank is used
as the single modern language resource both for
the Levenshtein-based and for the SMT-based ap-
proach (Csendes et al., 2005). Table 3 presents
in more detail the data sets used in the Hungarian
experiments.
Resource Data Tokens Types
Training HGDS 137,669 45,529
Tuning HGDS 17 181 8 827
Evaluation HGDS 17,214 8,798
Lev. dict. Szeged 1,257,089 144,248
Lev. freq. Szeged 1,257,089 144,248
SMT lm Szeged 1,257,089 144,248
Table 3: Language resources for Hungarian.
36
4.4 Icelandic
For training, tuning and evaluation in the Ice-
landic experiments, we use a manually normalised
subset of the Icelandic Parsed Historical Cor-
pus (IcePaHC), a manually tagged and parsed di-
achronic corpus of texts from the time period
1150?2008 (R?gnvaldsson et al., 2012). This sub-
set contains four texts from the 15th century: three
sagas (Vilhj?lm?s saga, Jarlmann?s saga, and Ec-
tor?s saga) and one narrative-religious text (Mi?al-
da?vint?ri). As a dictionary for Levenshtein cal-
culations we use a combination of Beygingar-
l?sing ?slensks N?t?mam?ls, B?N (a database of
modern Icelandic inflectional forms (Bjarnad?t-
tir, 2012)), and all tokens occurring 100 times or
more in the Tagged Icelandic Corpus of Contem-
porary Icelandic texts, M?M (Helgad?ttir et al.,
2012).
2
The frequency-based choice of a final nor-
malisation candidate in the Levenshtein approach,
as well as the training of a language model in the
SMT approach, are done on all tokens occurring
100 times or more in the M?M corpus. Table 4
presents in more detail the data sets used in the
Icelandic experiments.
Resource Data Tokens Types
Training IcePaHC 52,440 9,748
Tuning IcePaHC 6,443 2,270
Evaluation IcePaHC 6,384 2,244
Lev. dict. B?N+M?M 27,224,798 2,820,623
Lev. freq. M?M 21,339,384 9,461
SMT lm M?M 21,339,384 9,461
Table 4: Language resources for Icelandic.
4.5 Swedish
For training, tuning and evaluation in the Swedish
experiments, we use balanced subsets of the Gen-
der and Work corpus (GaW) of court records and
church documents from the time period 1527?
1812 (?gren et al., 2011). As a dictionary for Lev-
enshtein calculations we use SALDO, a lexical re-
source developed for present-day written Swedish
(Borin et al., 2008). For frequency-based choice of
a final normalisation candidate, we use the Stock-
holm Ume? corpus (SUC) of text representative of
the Swedish language in the 1990s (Ejerhed and
K?llgren, 1997). The SUC corpus is also used
2
The B?N database alone is not sufficient for Levenshtein
calculations, since it only contains content words.
to train a language model in the SMT-based ap-
proach. Table 5 presents in more detail the data
sets used in the Swedish experiments.
Resource Data Tokens Types
Training GaW 28,237 7,925
Tuning GaW 2,590 1,260
Evaluation GaW 33,544 8,859
Lev. dict. SALDO 1,110,731 723,138
Lev. freq. SUC 1,166,593 97,670
SMT lm SUC 1,166,593 97,670
Table 5: Language resources for Swedish.
5 Results
Table 6 presents the results for different languages
and normalisation methods, given in terms of nor-
malisation accuracy, i.e. the percentage of tokens
in the normalised text with a spelling identical
to the manually modernised gold standard, and
character error rate (CER), providing a more pre-
cise estimation of the similarity between the nor-
malised token and the gold standard version at a
character level. Table 7 summarises the results in
terms of Precision (Pre), Recall (Rec) and F-score
(F) for the filtering approach, the Levenshtein-
based approach (with and without filtering), and
the best-performing SMT-based approach.
For the Levenshtein experiments, we have used
context-sensitive weights, as described in Section
3.2. In the SMT approach, we run GIZA with
standard word alignment models for character un-
igrams (un) and bigrams (bi). The m2m aligner is
implemented with single character edit operations
(1:1) and multi-character operations (2:2).
The baseline case shows the proportion of to-
kens in the original, historical text that already
have a spelling identical to the modern gold stan-
dard spelling. In the Hungarian text, only 17.1%
of the historial tokens have a modern spelling,
with a character error rate of 0.85. For German
on the other hand, accuracy is as high as 84.4%,
with a character error rate of only 0.16. At a
first glance, the historical spelling in the Hungar-
ian corpus appears to be very similar to the mod-
ern spelling. A closer look however reveals re-
current differences involving single letter substi-
tutions and/or the use of accents, as for fiayval?
fiaival, m?eghalanac?meghal?nak and hazaba?
h?z?ba.
37
English German Hungarian Icelandic Swedish
Acc CER Acc CER Acc CER Acc CER Acc CER
baseline 75.8 0.26 84.4 0.16 17.1 0.85 50.5 0.51 64.6 0.36
filter 91.7 0.20 94.6 0.26 75.0 0.30 81.7 0.25 86.2 0.27
Lev 82.9 0.19 87.3 0.13 31.7 0.71 67.3 0.35 79.4 0.22
Lev+filter 92.9 0.09 95.1 0.06 76.4 0.35 84.6 0.19 90.8 0.10
giza un 94.3 0.07 96.6 0.04 79.9 0.21 71.8 0.30 92.9 0.07
giza bi 92.4 0.09 95.5 0.05 80.1 0.21 71.5 0.30 92.5 0.08
m2m 1:1 un 90.6 0.11 96.0 0.04 79.4 0.21 71.2 0.31 92.3 0.08
m2m 1:1 bi 88.0 0.14 95.6 0.05 79.5 0.21 71.5 0.30 92.2 0.08
m2m 2:2 un 90.7 0.11 96.4 0.04 77.3 0.24 71.0 0.31 91.3 0.09
m2m 2:2 bi 87.5 0.14 95.5 0.05 79.1 0.22 71.4 0.31 92.1 0.08
Table 6: Normalisation results given in accuracy (Acc) and character error rate (CER).
English German Hungarian Icelandic Swedish
Pre Rec F Pre Rec F Pre Rec F Pre Rec F Pre Rec F
filter 93.6 97.8 95.7 95.0 99.6 97.2 77.4 96.0 85.7 89.3 90.6 89.9 87.5 98.3 92.6
Lev 92.7 88.6 90.7 91.0 95.6 93.2 68.0 37.3 48.2 85.4 76.1 80.5 90.5 86.6 88.5
Lev+filter 97.4 95.2 96.3 97.3 97.7 97.5 96.2 78.8 86.7 95.6 88.0 91.7 96.6 93.8 95.2
SMT 98.2 95.9 97.0 98.7 97.9 98.3 98.3 81.3 89.0 82.0 85.2 83.6 98.6 94.1 96.3
Table 7: Normalisation results given in precision (Pre), recall (Rec) and F-score (F).
The Icelandic corpus also has a relatively low
number of tokens with a spelling identical to the
modern spelling. Even though the Hungarian and
Icelandic texts are older than the English, German,
and Swedish texts, the rather low proportion of to-
kens with a modern spelling in the Icelandic cor-
pus is rather surprising, since the Icelandic lan-
guage is generally seen as conservative in spelling.
A closer inspection of the Icelandic corpus reveals
the same kind of subtle single letter divergences
and differences in the use of accents as for Hun-
garian, e.g. ad? a? and hun? h?n.
The simplistic filtering approach (filter), re-
lying solely on previously seen tokens in the
training data, captures frequently occurring word
forms and works surprisingly well, improving
normalisation accuracy by up to 63 percentage
units. The Levenshtein-based approach (Lev)
in its basic version, with no parallel training
data available, also improves normalisation ac-
curacy as compared to the baseline. However,
for all languages, the simplistic filtering approach
yields significantly higher normalisation accuracy
than the more sophisticated Levenshtein-based ap-
proach does. This could be partly explained by
the fact that frequently occurring word forms have
a high chance of being captured by the filter-
ing approach, whereas the Levenshtein-based ap-
proach runs the risk of consistently normalising
high-frequent word forms incorrectly. For exam-
ple, in the English Levenshtein normalisation pro-
cess, the high-frequent word form stonde has con-
sistently been normalised to stone instead of stand,
due to the larger edit distance between stonde and
stand. The even more common word form ben,
which should optimally be normalised to been, has
consistently been left unchanged as ben, since the
BNC corpus, which is used for dictionary lookup
in the English setup, contains the proper name
Ben. The issue of proper names would not be
a problem if a modern dictionary were used for
Levenshtein comparisons instead of a corpus, or if
casing was taken into account in the Levenshtein
comparisons. There would however still be cases
left like stonde being incorrectly normalised to
stone as described above, which would be disad-
vantageous to the Levenshtein-based method. The
low recall figures, especially for Hungarian, also
indicates that there may be old word forms that
are not present in modern dictionaries and thus are
out of reach for the Levenshtein-based method, as
for the previously discussed Hungarian word form
meghal?nak.
In the Lev+filter setting, the filter is used as a
first step in the normalisation process. Only to-
kens that could not be matched through dictio-
nary lookup based on the training corpus are nor-
malised by Levenshtein comparisons. The idea is
38
that combining these two techniques would per-
form better than one approach only, since high-
frequent word forms are consistently normalised
correctly by the filter, whereas previously unseen
tokens are handled through Levenshtein compar-
isons. This combination does indeed perform bet-
ter for all languages, and for Icelandic this is by far
the most successful normalisation method of all.
For the SMT-based approach, it is interesting to
note that the simple unigram models in many cases
perform better than the more advanced bigram and
multi-character models. We also tried adding the
filter to the SMT approach, so that only tokens that
could not be matched through dictionary lookup
based on the training corpus, would be considered
for normalisation by the SMT model. This did
however not have a positive effect on normalisa-
tion accuracy, probably because the training data
has already been taken care of by the SMT model,
so adding the filter only led to redundant informa-
tion and incorrect matches, deteriorating the re-
sults. For four out of five languages, the GIZA un-
igram setting yields the highest normalisation ac-
curacy of all SMT models evaluated. For Hungar-
ian, the GIZA bigram modell performs marginally
better than the unigram model.
From the presented results, it is not obvious
which normalisation approach to choose for a new
language. For Icelandic, the Levenshtein-based
approach combined with the filter leads to the
highest normalisation accuracy. For the rest of
the languages, the SMT-based approach with the
GIZA unigram or bigram setting gives the best re-
sults. Generally, the Levenshtein-based method
could be used for languages lacking access to an-
notated historical data with information on both
original and modernised spelling. If, on the other
hand, such data is available, the filtering approach,
or the combination of filtering and Levenshtein
calculations, would be likely to improve normal-
isation accuracy. Moreover, the effort of training
a character-based SMT system for normalisation
would be likely to further improve the results.
It would be interesting to also compare the re-
sults between the languages, in a language evo-
lution perspective. This is however not feasible
within the scope of this study, due to the differ-
ences in corpus size, genres and covered time pe-
riods, as discussed in Section 4.
6 Conclusion
We have performed a multilingual evaluation
of three approaches to spelling modernisation
of historical text: a simplistic filtering model,
a Levenshtein-based approach and a character-
based statistical machine translation method. The
results were evaluated on historical texts from
five languages: English, German, Hungarian, Ice-
landic and Swedish. We see that all approaches are
successful in increasing the proportion of tokens in
the historical text with a spelling identical to the
modernised gold standard spelling. We conclude
that the proposed methods have the potential of
enabling us to use modern NLP tools for analysing
historical texts. Which approach to choose is not
clear, since the results vary for the different lan-
guages in our study, even though the SMT-based
approach generally works best. If no historical
training data is available, the Levenshtein-based
approach could still be used, since only a mod-
ern dictionary is required for edit distance com-
parisons. If there is a corpus of token pairs with
historical and modern spelling available, training
an SMT model could however result in improved
normalisation accuracy. Since the SMT models
are character-based, only a rather small amount of
training data is needed for this task, as discussed
in Section 3.3.
We believe that our results would be of interest
to several research fields. From a language evolu-
tion perspective, future research would include a
thorough investigation of why certain approaches
work better for some languages but not for other
languages, and what the results would be if the
data sets for the different languages were more
similar with regard to time period, size, genre etc.
The latter could however be problematic, due to
data sparseness. For historians interested in us-
ing modern NLP tools for analysing historical text,
an extrinsic evaluation is called for, comparing
the results of tagging and parsing using modern
tools, before and after spelling normalisation. Fi-
nally, the proposed methods all treat words in iso-
lation in the normalisation process. From a lan-
guage technology perspective, it would be inter-
esting to also explore ways of handling grammat-
ical and structural differences between historical
and modern language as part of the normalisation
process. This would be particularly interesting
when evaluating subsequent tagging and parsing
performance.
39
References
Maria ?gren, Rosemarie Fiebranz, Erik Lindberg, and
Jonas Lindstr?m. 2011. Making verbs count. The
research project ?Gender and Work? and its method-
ology. Scandinavian Economic History Review,
59(3):271?291. Forthcoming.
Alistair Baron and Paul Rayson. 2008. Vard2: A tool
for dealing with spelling variation in historical cor-
pora. In Postgraduate Conference in Corpus Lin-
guistics, Aston University, Birmingham.
Krist?n Bjarnad?ttir. 2012. The Database of Modern
Icelandic Inflection. In AfLaT2012/SALTMIL joint
workshop on Language technology for normalisa-
tion of less-resourced languages, Istanbul, May.
Alan W. Black and Paul Taylor. 1997. Festival speech
synthesis system: system documentation. Technical
report, University of Edinburgh, Centre for Speech
Technology Research.
Marcel Bollmann, Florian Petran, and Stefanie Dipper.
2011. Rule-based normalization of historical texts.
In Proceedings of the Workshop on Language Tech-
nologies for Digital Humanities and Cultural Her-
itage, pages 34?42, Hissar, Bulgaria.
Marcel Bollmann. 2013. POS tagging for historical
texts with sparse training data. In Proceedings of
the 7th Linguistic Annotation Workshop & Interop-
erability with Discourse, pages 11?18, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Lars Borin, Markus Forsberg, and Lennart L?nngren.
2008. Saldo 1.0 (svenskt associationslexikon ver-
sion 2). Spr?kbanken, University of Gothenburg.
C. Csendes, J. Csirik, T. Gyim?thy, and A. Kocsor.
2005. The Szeged Treebank. In Proceedings of
the Eighth International Conference on Text, Speech
and Dialogue (TSD 2005), Karlovy Vary, Czech Re-
public.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Produced by Depart-
ment of Linguistics, Ume? University and Depart-
ment of Linguistics, Stockholm University. ISBN
91-7191-348-3.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twitter.
In Association for Computational Linguistics, edi-
tor, Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics, pages
368?378, Portland, Oregon, USA, June.
Sigr?n Helgad?ttir, ?sta Svavarsd?ttir, Eir?kur R?gn-
valdsson, Krist?n Bjarnad?ttir, and Hrafn Loftsson.
2012. The Tagged Icelandic Corpus (M?M). In
Proceedings of the Workshop on Language Tech-
nology for Normalisation of Less-Resourced Lan-
guages, pages 67?72.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme
conversion. In Proceedings of the Annual Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL-HLT
2007), pages 372?379, Rochester, NY, April.
Bryan Jurish. 2008. Finding canonical forms
for historical German text. In Angelika Storrer,
Alexander Geyken, Alexander Siebert, and Kay-
Michael W?rzner, editors, Text Resources and Lex-
ical Knowledge: Selected Papers from the 9th Con-
ference on Natural Language Processing (KON-
VENS 2008), pages 27?37. Mouton de Gruyter,
Berlin.
Manfred Markus, 1999. Manual of ICAMET (Inns-
bruck Computer Archive of Machine-Readable En-
glish Texts). Leopold-Franzens-Universit?t Inns-
bruck.
David Matthews. 2007. Machine transliteration of
proper names. Master?s thesis, School of Informat-
ics.
Preslav Nakov and J?rg Tiedemann. 2012. Combin-
ing word-level and character-level models for ma-
chine translation between closely-related languages.
In Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 301?305, Jeju Island, Korea,
July. Association for Computational Linguistics.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. pages 440?447, Hongkong, China,
October.
Eva Pettersson, Be?ta Megyesi, and Tiedemann J?rg.
2013a. An SMT approach to automatic annotation
of historical text. In Proceedings of the NoDaLiDa
2013 workshop on Computational Historical Lin-
guistics, May.
Eva Pettersson, Be?ta Megyesi, and Joakim Nivre.
2013b. Normalisation of historical text using
context-sensitive weighted Levenshtein distance and
compound splitting. In Proceedings of the 19th
Nordic Conference on Computational Linguistics
(NoDaLiDa), May.
Paul Rayson, Dawn Archer, and Nicholas Smith. 2005.
VARD versus Word ? A comparison of the UCREL
variant detector and modern spell checkers on En-
glish historical corpora. In Proceedings from the
Corpus Linguistics Conference Series on-line e-
journal, volume 1, Birmingham, UK, July.
Eir?kur R?gnvaldsson, Anton Karl Ingason, Einar Freyr
Sigurdsson, and Joel Wallenberg. 2012. The Ice-
landic Parsed Historical Corpus (IcePaHC). In Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Is-
tanbul, Turkey, May. European Language Resources
Association (ELRA).
40
Silke Scheible, Richard J. Whitt, Martin Durrell, and
Paul Bennett. 2011. A Gold Standard Corpus of
Early Modern German. In Proceedings of the 5th
Linguistic Annotation Workshop, pages 124?128,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Eszter Simon. To appear. Corpus building from Old
Hungarian codices. In Katalin ?. Kiss, editor, The
Evolution of Functional Left Peripheries in Hungar-
ian Syntax. Oxford University Press.
Wolfgang Teubert (ed.). 2003. German Parole Corpus.
Electronic resource, Oxford Text Archive.
David Vilar, Jan-Thorsten Peter, and Hermann Ney.
2007. Can we translate letters? In Proceedings of
the Second Workshop on Statistical Machine Trans-
lation, pages 33?39, Prague, Czech Republic, June.
Association for Computational Linguistics.
41
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 94?103,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
EACL - Expansion of Abbreviations in CLinical text
Lisa Tengstrand*, Be
?
ata Megyesi*, Aron Henriksson
+
, Martin Duneld
+
and Maria Kvist
+
*Department of Linguistics and Philology,
Uppsala University, Sweden
tengstrand@ling.su.se, beata.megyesi@lingfil.uu.se
+
Department of Computer and System Sciences,
Stockholm University, Sweden
aronhen@dsv.su.se, xmartin@dsv.su.se, maria.kvist@karolinska.se
Abstract
In the medical domain, especially in clin-
ical texts, non-standard abbreviations are
prevalent, which impairs readability for
patients. To ease the understanding of the
physicians? notes, abbreviations need to be
identified and expanded to their original
forms. We present a distributional seman-
tic approach to find candidates of the origi-
nal form of the abbreviation, and combine
this with Levenshtein distance to choose
the correct candidate among the semanti-
cally related words. We apply the method
to radiology reports and medical journal
texts, and compare the results to general
Swedish. The results show that the cor-
rect expansion of the abbreviation can be
found in 40% of the cases, an improve-
ment by 24 percentage points compared to
the baseline (0.16), and an increase by 22
percentage points compared to using word
space models alone (0.18).
1 Introduction
Abbreviations are prevalent in text, especially in
certain text types where the author has either lim-
ited space or time to write the written message and
therefore shortens some words or phrases. This
might, however, make it difficult for the reader
to understand the meaning of the actual abbre-
viation. Although some abbreviations are well-
known, and frequently used by most of us (e.g.,
i.e., pm, etc.), most of the abbreviations used in
specialized domains are often less known to the
public. Interpreting them is not an easy task, as ab-
breviations are often ambiguous and their correct
meaning depends on the context in which they ap-
pear. For example, military and governmental staff
would naturally read EACL as Emergency Action
Checklist, people in the food and beverage busi-
ness might think of the company name EACL, lin-
guists would probably interpret it as the European
Chapter of Chinese Linguistics, while computa-
tional linguists would generally claim that EACL
stands for the European Chapter of the Associa-
tion for Computational Linguistics. However, the
readers of this particular article know, as the title
suggests, that the intended meaning here is the Ex-
pansion of Abbreviations in CLinical text.
It has been shown that abbreviations are fre-
quently occurring in various domains and genres,
such as in historical documents, messages in so-
cial media, as well as in different registers used
by specialists within a particular field of exper-
tise. Clinical texts produced by health care per-
sonnel is an example of the latter. The clinical
texts are communication artifacts, and the clini-
cal setting requires that information is expressed
in an efficient way, resulting in short telegraphic
messages. Physicians and nurses need to docu-
ment their work to describe findings, treatments
and procedures precisely and compactly, often un-
der time pressure.
In recent years, governments and health care ac-
tors have started making electronic health records
accessible, not only to other caretakers, but also
to patients in order to enable them to participate
actively in their own health care processes. How-
ever, several studies have shown that patients have
difficulties to comprehend their own health care
reports and other medical texts due to the different
linguistic features that characterize these, aswell
as to medical jargon and technical terminology
(Elhadad, 2006; Rudd et al., 1999; Keselman et
al., 2007). It has also been shown that physicians
rarely adapt their writing style in order to produce
documents that are accessible to lay readers (Al-
lvin, 2010). Besides the use of different termi-
nologies and technical terms, an important obsta-
cle for patients to comprehend medical texts is the
frequent use of ? for the patients unknown ? ab-
94
breviations (Keselman et al., 2007; Adnan et al.,
2010).
In health records, abbreviations, which consti-
tute linguistic units that are inherently difficult to
decode, are commonly used and often non stan-
dard (Skeppstedt, 2012). An important step in
order to increase readability for lay readers is to
translate abbreviated words into their correspond-
ing full length words.
The aim of this study is to explore a distri-
butional semantic approach combined with word
normalization, measured by Levenshtein distance,
to abbreviation expansion. Using distributional
semantic models, which can be applied to large
amounts of data, has been shown to be a viable
approach to extracting candidates for the underly-
ing, original word of an abbreviation. In order to
find the correct expansion among the semantically
related candidates, we apply the Levenshtein dis-
tance measure. We report on experiments on com-
parative studies of various text types in Swedish,
including radiology reports, medical journals and
texts taken from a corpus of general Swedish.
2 Background
An abbreviation is a shorter ? abbreviated ? form
of a word or phrase, often originating from a tech-
nical term or a named entity. Abbreviations are
typically formed in one of three ways: by (i) clip-
ping the last character sequence of the word (e.g.,
pat for patient or pathology), (ii) merging the ini-
tial letter(s) of the words to form an acronym (e.g.,
UU for Uppsala University), or (iii) merging some
of the letters ? often the initial letter of the sylla-
bles ? in the word (e.g., msg for message). Abbre-
viations can also be formed as a combination of
these three categories (e.g., EACL for Expansion
of Abbreviations in CLinical text).
Automatically expanding abbreviations to their
original form has been of interest to computational
linguists as a means to improve text-to-speech, in-
formation retrieval and information extraction sys-
tems. Rule-based systems as well as statistical and
machine learning methods have been proposed to
detect and expand abbreviations. A common com-
ponent of most solutions is their reliance on the as-
sumption that an abbreviation and its correspond-
ing definition will appear in the same text.
Taghva and Gilbreth (1999) present a method
for automatic acronym-definition extraction in
technical literature, where acronym detection is
based on case and token length constraints. The
surrounding text is subsequently searched for pos-
sible definitions corresponding to the detected
acronym using an inexact pattern-matching algo-
rithm. The resulting set of candidate definitions
is then narrowed down by applying the Longest
Common Subsequence (LCS) algorithm (Nakatsu
et al., 1982) to the candidate pairs. They report
98% precision and 93% recall when excluding
acronyms of two or fewer characters.
Park and Byrd (2001), along somewhat similar
lines, propose a hybrid text mining approach for
abbreviation expansion in technical literature. Or-
thographic constraints and stop lists are first used
to detect abbreviations; candidate definitions are
then extracted from the adjacent text based on a set
of pre-specified conditions. The abbreviations and
definitions are converted into patterns, for which
transformation rules are constructed. An initial
rule-base comprising the most frequent rules is
subsequently employed for automatic abbreviation
expansion. They report 98% precision and 94%
recall as an average over three document types.
In the medical domain, most approaches to
abbreviation resolution also rely on the co-
occurrence of abbreviations and definitions in a
text, typically by exploiting the fact that abbrevi-
ations are sometimes defined on their first men-
tion. These studies extract candidate abbreviation-
definition pairs by assuming that either the defi-
nition or the abbreviation is written in parenthe-
ses (Schwartz and Hearst, 2003). The process of
determining which of the extracted abbreviation-
definition pairs are likely to be correct is then
performed either by rule-based (Ao and Takagi,
2005) or machine learning (Chang et al., 2002;
Movshovitz-Attias and Cohen, 2012) methods.
Most of these studies have been conducted on
English corpora; however, there is one study on
Swedish medical text (Dann?ells, 2006). There are
problems with this popular approach to abbrevia-
tion expansion: Yu et al. (2002) found that around
75% of all abbreviations in the biomedical litera-
ture are never defined.
The application of this method to clinical text
is even more problematic, as it seems highly un-
likely that abbreviations would be defined in this
way. The telegraphic style of clinical narrative,
with its many non-standard abbreviations, is rea-
sonably explained by time constraints in the clin-
ical setting. There has been some work on iden-
95
tifying such undefined abbreviations in clinical
text (Isenius et al., 2012), as well as on finding
the intended abbreviation expansion among candi-
dates in an abbreviation dictionary (Gaudan et al.,
2005).
Henriksson et al. (2012; 2014) present a method
for expanding abbreviations in clinical text that
does not require abbreviations to be defined, or
even co-occur, in the text. The method is based
on distributional semantic models by effectively
treating abbreviations and their corresponding def-
inition as synonymous, at least in the sense of shar-
ing distributional properties. Distributional se-
mantics (see Cohen and Widdows (2009) for an
overview) is based on the observation that words
that occur in similar contexts tend to be semanti-
cally related (Harris, 1954). These relationships
are captured in a Random Indexing (RI) word
space model (Kanerva et al., 2000), where se-
mantic similarity between words is represented as
proximity in high-dimensional vector space. The
RI word space representation of a corpus is ob-
tained by assigning to each unique word an ini-
tially empty, n-dimensional context vector, as well
as a static, n-dimensional index vector, which con-
tains a small number of randomly distributed non-
zero elements (-1s and 1s), with the rest of the
elements set to zero
1
. For each occurrence of a
word in the corpus, the index vectors of the sur-
rounding words are added to the target word?s con-
text vector. The semantic similarity between two
words can then be estimated by calculating, for in-
stance, the cosine similarity between their context
vectors. A set of word space models are induced
from unstructured clinical data and subsequently
combined in various ways with different parame-
ter settings (i.e., sliding window size for extracting
word contexts). The models and their combina-
tions are evaluated for their ability to map a given
abbreviation to its corresponding definition. The
best model achieves 42% recall. Improvement of
the post-processing of candidate definitions is sug-
gested in order to obtain enhanced performance on
this task.
The estimate of word relatedness that is ob-
tained from a word space model is purely statis-
tical and has no linguistic knowledge. When word
pairs should not only share distributional proper-
ties, but also have similar orthographic represen-
1
Generating sparse vectors of a sufficiently high dimen-
sionality in this manner ensures that the index vectors will be
nearly orthogonal.
tations ? as is the case for abbreviation-definition
pairs ? normalization procedures could be ap-
plied. Given a set of candidate definitions for a
given abbreviation, the task of identifying plausi-
ble candidates can be viewed as a normalization
problem. Petterson et al. (2013) utilize a string
distance measure, Levenshtein distance (Leven-
shtein, 1966), in order to normalize historical
spelling of words into modern spelling. Adjusting
parameters, i.e., the maximum allowed distance
between source and target, according to observed
distances between known word pairs of historical
and modern spelling, gives a normalization accu-
racy of 77%. In addition to using a Levenshtein
distance weighting factor of 1, they experiment
with context free and context-sensitive weights for
frequently occurring edits between word pairs in a
training corpus. The context-free weights are cal-
culated on the basis of one-to-one standard edits
involving two characters; in this setting the nor-
malization accuracy is increased to 78.7%. Fre-
quently occurring edits that involve more than two
characters, e.g., substituting two characters for
one, serve as the basis for calculating context-
sensitive weights and gives a normalization accu-
racy of 79.1%. Similar ideas are here applied to
abbreviation expansion by utilizing a normaliza-
tion procedure for candidate expansion selection.
3 Method
The current study aims to replicate and extend
a subset of the experiments conducted by Hen-
riksson et al. (2012), namely those that concern
the abbreviation expansion task. This includes
the various word space combinations and the pa-
rameter optimization. The evaluation procedure
is similar to the one described in (Henriksson et
al., 2012). The current study, however, focuses on
post-processing of the semantically related words
by introducing a filter and a normalization proce-
dure in an attempt to improve performance. An
overview of the approach is depicted in Figure 1.
Abbreviation expansion can be viewed as a two-
step procedure, where the first step involves de-
tection, or extraction, of abbreviations, and the
second step involves identifying plausible expan-
sions. Here, the first step is achieved by extracting
abbreviations from a clinical corpus with clinical
abbreviation detection software and using a list of
known medical abbreviations. The second step is
performed by first extracting a set of semantically
96
clinical text
abbreviation
extraction
abbreviations
baseline corpus
word space
induction
expansion
word
extraction
clinical word space
expansion
word
filtering
Levenshtein
distance
normal-
ization
abbreviation-candidate expansions
evaluation
Figure 1: The abbreviation expansion process of
the current study.
similar words for each abbreviation and treating
these as initial expansions. More plausible expan-
sions of each abbreviation are then obtained by fil-
tering the expansion words and applying a normal-
ization procedure.
3.1 Data
3.1.1 Corpora
Four corpora are used in the experiments: two
clinical corpora, a medical (non-clinical) corpus
and a general Swedish corpus (Table 1).
The clinical corpora are subsets of the Stock-
holm EPR Corpus (Dalianis et al., 2009), com-
prising health records for over one million pa-
tients from 512 clinical units in the Stockholm re-
gion over a five-year period (2006-2010)
2
. One
of the clinical corpora contains records from vari-
ous clinical units, for the first five months of 2008,
henceforth referred to as SEPR, and the other con-
tains radiology examination reports, produced in
2009 and 2010, the Stockholm EPR X-ray Corpus
(Kvist and Velupillai, 2013) henceforth referred to
as SEPR-X. The clinical corpora were lemmatized
2
This research has been approved by the Regional Ethical
Review Board in Stockholm (Etikpr?ovningsnamnden i Stock-
holm), permission number 2012/2028-31/5
using Granska (Knutsson et al., 2003).
The experiments in the current study also in-
clude a medical corpus. The electronic editions of
L?akartidningen (Journal of the Swedish Medical
Association), with issues from 1996 to 2010, have
been compiled into a corpus (Kokkinakis, 2012),
here referred to as LTK.
To compare the medical texts to general
Swedish, the third version of the Stockholm Ume?a
Corpus (SUC 3.0) (K?allgren, 1998) is used. It is
a balanced corpus and consists of written Swedish
texts from the early 1990?s from various genres.
Corpus #Tokens #Types #Lemmas
SEPR 109,663,052 853,341 431,932
SEPR-X 20,290,064 200,703 162,387
LTK 24,406,549 551,456 498,811
SUC 1,166,593 97,124 65,268
Table 1: Statistical descriptions of the corpora
3.1.2 Reference standards
A list of medical abbreviation-definition pairs is
used as test data and treated as the reference stan-
dard in the evaluation. The list is derived from
Cederblom (2005) and comprises 6384 unique ab-
breviations from patient records, referrals and sci-
entific articles. To increase the size of the test
data, the 40 most frequent abbreviations are ex-
tracted by a heuristics-based clinical abbreviation
detection tool called SCAN (Isenius et al., 2012).
A domain expert validated these abbreviations and
manually provided the correct expansion(s).
An inherent property of word space models is
that they model semantic relationships between
unigrams. There are, however, abbreviations that
expand into multiword expressions. Ongoing re-
search on modeling semantic composition with
word space models exists, but, in the current study
abbreviations that expanded to multiword defini-
tions were simply removed from the test data set.
The two sets of abbreviation-expansion pairs were
merged into a single test set, containing 1231
unique entries in total.
In order to obtain statistically reliable seman-
tic relations in the word space, the terms of inter-
est must be sufficiently frequent in the data. As a
result, only abbreviation-expansion pairs with fre-
quencies over 50 in SEPR and SEPR-X, respec-
tively, were included in each test set. The SEPR
test set contains 328 entries and the SEPR-X test
97
set contains 211 entries. Each of the two test data
sets is split into a development set (80%) for model
selection, and a test set (20%) for final perfor-
mance estimation.
3.2 Expansion word extraction
For the experiments where semantically related
words were used for extraction of expansion
words, the top 100 most correlated words for each
of the abbreviations were retrieved from each of
the word space model configurations that achieved
the best results in the parameter optimization ex-
periments.
The optimal parameter settings of a word space
vary with the task and data at hand. It has been
shown that when modeling paradigmatic (e.g.,
synonymous) relations in word spaces, a fairly
small context window size is preferable (Sahlgren,
2006). Following the best results of Henriksson et
al. (2012), we experiment with window sizes of
1+1, 2+2, and 4+4.
Two word space algorithms are explored: Ran-
dom Indexing (RI), to retrieve the words that occur
in a similar context as the query term, and Random
Permutation (RP), which also incorporates word
order information when accumulating the context
vectors (Sahlgren et al., 2008). In order to exploit
the advantages of both algorithms, and to combine
models with different parameter settings, RI and
RP model combinations are also evaluated. The
models and their combinations are:
? Random Indexing (RI): words with a contextually high
similarity are returned; word order within the context
window is ignored.
? Random Permutation (RP): words that are contextu-
ally similar and used in the same relative positions are
returned; these are more likely to share grammatical
properties.
? RP-filtered RI candidates (RI RP): returns the top ten
terms in the RI model that are among the top thirty
terms in the RP model.
? RI-filtered RP candidates (RP RI): returns the top ten
terms in the RP model that are among the top thirty
terms in the RI model.
? RI and RP combination of similarity scores (RI+RP):
sums the cosine similarity scores from the two models
for each candidate term and returns the candidates with
the highest aggregate score.
All models are induced with three different con-
text window sizes for the two clinical corpora,
SEPR and SEPR-X. For each corpus, two variants
are used for word space induction, one where stop
words are removed and one where stop words are
retained. All word spaces are induced with a di-
mensionality of 1000.
For parameter optimization and model selec-
tion, the models and model combinations are
queried for semantically similar words. For each
of the abbreviations in the development set, the ten
most similar words are retrieved. Recall is com-
puted with regard to this list of candidate words,
whether the correct expansion is among these ten
candidates. Since the size of the test data is rather
limited, 3-fold cross validation is performed on
the development set for the parameter optimiza-
tion experiments. For both SEPR and SEPR-X de-
velopment sets, a combination of a RI model with
a context window size of 4+4 and a RP model with
4+4 context window size in the summing similar-
ity scores setting were among the most successful
with recall scores of 0.25 for SEPR and 0.17 for
SEPR-X.
3.3 Filtering expansion words
Given the expansion words, extracted from clini-
cal word spaces or baseline corpora (the baselines
are more thoroughly accounted for in 3.5), a filter
was applied in order to generate candidate expan-
sions. The filter was defined as a set of require-
ments, which had to be met in order for the expan-
sion word to be extracted as a candidate expansion.
The requirements were that the intitial letter of the
abbreviation and expansion word had to be iden-
tical. All the letters of the abbreviation also had
to be present in the expansion word in the same
order.
String length difference was also a part of the
requirements: the expansion word had to be at
least one character longer than the abbreviation.
In order to define an upper bound for expansion to-
ken length, string length differences of the SEPR
and SEPR-X development sets were obtained.
The distribution of string length differences for
abbreviation-expansion pairs in the SEPR devel-
opment set ranged from 1 to 21 characters. If a
maximum string length difference of 14 was al-
lowed, 95.2% of the abbreviation-expansion pairs
were covered. As for the string length differences
in the SEPR-X development set, the distribution
ranged from 1 to 21 characters. If a string length
difference of up to and including 14 characters
was allowed, 96.3% of the abbreviation-expansion
pairs were covered. Thus, a maximum difference
98
in string length of 14 was also required for the ex-
pansion word to be extracted as a candidate expan-
sion.
3.4 Levenshtein distance normalization
Given the set of filtered candidate expansions for
the abbreviations, choosing the correct one can be
seen as a normalization problem. The goal is to
map a source word to a target word, similarly to
for instance methods for spelling correction. The
target word is chosen from a list of words, and the
choice is based on the distance between the source
and the target where a small distance implies high
plausibility. However, we cannot adopt the same
assumptions as for the problem of spelling correc-
tion, where the most common distance between a
source word and the correct target word is 1 (Ku-
kich, 1992). Intuitively, we can expect that there
are abbreviations that expand to words within a
larger distance than 1. It would seem somewhat
useless to abbreviate words by one character only,
although it is not entirely improbable.
Similarly to measuring the string length differ-
ence in order to define an upper bound for filtering
candidate expansions, the Levenshtein distances
for abbreviation-expansion pairs in the develop-
ment sets were obtained.
For the SEPR and SEPR-X development sets,
allowing a Levenshtein distance up to and in-
cluding 14 covers 97.8% and 96.6% of the
abbreviation-expansion pairs, as shown in Table 2.
Given the filtered candidate expansions, the
Levenshtein distance for the abbreviation and each
of the candidate expansions were computed. For
each one of the candidate expansions, the Leven-
shtein distance beween the entry and the abbrevi-
ation was associated with the entry. The result-
ing list was sorted in ascending order according to
Levenshtein distance.
Going through the candidate expansion list, if
the Levenshtein distance was less than or identical
to the upper bound for Levenshtein distance (14),
the candidate expansion was added to the expan-
sion list that was subsequently used in the evalu-
ation. In the Levenshtein distance normalization
experiments, a combination of semantically re-
lated words and words from LTK was used. When
compiling the expansion list, semantically related
words were prioritized. This implied that word
space candidate expansion would occupy the top
positions in the expansion list, in ascending order
SEPR SEPR SEPR-X SEPR-X
LD Avg % SDev Avg % SDev
1 1 0.3 0.4 0.2
2 4.6 0.4 5 0.6
3 13 1.2 14.7 1.3
4 12.2 1 15.1 0.6
5 12.7 1.3 14.5 2.2
6 12.7 0.8 12.9 0.9
7 8.4 0.7 7.8 0.3
8 10.4 1.5 9.8 2
9 5.7 0.7 4.9 0.5
10 4.1 0.7 2.9 0.3
11 3 0.5 2.6 0.4
12 3 0.6 2.6 0.4
13 3.8 5.5 1.3 0.5
14 3.5 1.1 2.2 0.8
15 1.3 0.5 1.3 0.5
16 1.6 0.4 0.4 0.2
17 0.2 0.1
18 0.8 0.3 1 0.1
20 0.2 0.1
21 0.2 0.1 0.5 0
Table 2: Levenshtein distance distribution for
abbreviation-expansion pairs. Average proportion
over 5 folds at each Levensthein distance with
standard deviation (SDev) in SEPR and SEPR-X
development sets.
according to Levenshtein distance. The size of the
list was restricted to ten, and the remaining posi-
tions, if there were any, were populated by LTK
candidate expansions in ascending order accord-
ing to Levenshtein distance to the abbreviation. If
there were more than one candidate expansion at
a specific Levenshtein distance, ranking of these
was randomized.
3.5 Evaluation
The evaluation procedure of the abbreviation ex-
pansion implied assessing the ability of finding the
correct expansions for abbreviations. In order to
evaluate the performance gain of using semantic
similarity to produce the list of candidate expan-
sions over using the filtering and normalization
procedure alone, a baseline was created. For the
baseline, expansion words were instead extracted
from the baseline corpora, the corpus of general
Swedish SUC 3.0 and the medical corpus LTK.
A list of all the lemma forms from each baseline
99
corpus (separately) was provided for each abbre-
viation as initial expansion words. The filter and
normalization procedure was then applied to these
expansion words.
The reference standard contained abbreviation-
expansion pairs, as described in 3.1.2. If any of the
correct expansions (some of the abbreviations had
multiple correct expansions) was present in the ex-
pansion list provided for each abbreviation in the
test set, this was regarded as a true positive. Preci-
sion was computed with regard to the position of
the correct expansion in the list and the number of
expansions in the expansion list, as suggested in
Henriksson (2013). For an abbreviation that ex-
panded to one word only, this implied that the ex-
pansion list besides holding the correct expansion,
also contained nine incorrect expansions, which
was taken into account when computing precision.
The list size was static: ten expansions were pro-
vided for each abbreviation, and this resulted in
an overall low precision. Few of the abbreviations
in the development set expanded to more than one
word, giving a precision of 0.17-0.18 for all exper-
iments.
Results of baseline abbreviation expansion in
the development sets are given in table 3. Recall
is given as an average of 5 folds, as cross valida-
tion was performed. The baseline achieves over-
all low recall, with the lowest score of 0.08 for the
SEPR-X development set using SUC for candidate
expansion extraction. The rest of the recall results
are around 0.11.
Corpus SEPR SEPR SEPR-X SEPR-X
Recall SDev Recall SDev
SUC 0.10 0.05 0.08 0.06
LTK 0.11 0.06 0.11 0.11
Table 3: Baseline average recall for SEPR and
SEPR-X development sets.
Results from abbreviation expansion using se-
mantically related words with filtering and nor-
malization to refine the selection of expansions on
SEPR and SEPR-X development sets are shown in
Table 4. Recall is given as an average of 5 folds,
as cross validation was performed. The seman-
tically related words are extracted from the word
space model configuration that had the top recall
scores in the parameter optimization experiments
described in 3.2, namely the combination of an
RI model and an RP model both with 4+4 context
window sizes. Recall is increased by 14 percent-
age points for SEPR and 20 percentage points for
SEPR-X when applying filtering and normaliza-
tion to the semantically related words.
SEPR SEPR SEPR-X SEPR-X
Recall SDev Recall SDev
0.39 0.05 0.37 0.1
Table 4: Abbreviation expansion results for SEPR
and SEPR-X development sets using the best
model from parameter optimization experiments
(RI.4+4+RP.4+4).
4 Results
4.1 Expansion word extraction
The models and model combinations that had the
best recall scores in the word space parameter op-
timization were also evaluated on the test set. The
models that had top recall scores in 3.2 achieved
0.2 and 0.18 for SEPR and SEPR-X test sets re-
spectively, compared to 0.25 and 0.17 in the word
space parameter optimization.
4.2 Filtering expansion words and
Levenshtein normalization
Abbreviation expansion with filtering and normal-
ization was evaluated on the SEPR and SEPR-X
test sets. The results are summarized in Table 5.
SEPR SEPR-X
SUC 0.09 0.16
LTK 0.08 0.14
Expansion word extraction 0.20 0.18
Filtering and normalization 0.38 0.40
Table 5: SEPR and SEPR-X test set results in ab-
breviation expansion.
Baseline recall scores were 0.09 and 0.08 for
SUC and LTK respectively, showing a lower score
for LTK compared to the results on the SEPR de-
velopment set. For abbreviation expansion (with
filtering and normalization) using semantically re-
lated words in combination with LTK, the best re-
call score was 0.38 for the SEPR test set, com-
pared to 0.39 for the same model evaluated on the
SEPR development set. Compared to the results of
using semantically related words only (expansion
word extraction), recall increased by 18 percent-
100
age points for the same model when filtering and
normalization was applied.
Evaluation on the SEPR-X test set gave higher
recall scores for both baseline corpora compared
to the baseline results for the SEPR-X develop-
ment set: the SUC result increased by 8 percentage
points for recall. For LTK, there was an increase in
recall of 3 percentage points. For the SEPR-X test
set, recall increased by 22 percentage points when
filtering and normalization was applied to seman-
tically related words extracted from the best model
configuration.
In comparison to the results of Henriksson et
al (2012), where recall of the best model is 0.31
without and 0.42 with post-processing of the ex-
pansion words for word spaces induced from the
data set (i.e., an increase in recall by 11 percentage
points), the filtering and normalization procedure
for expansion words of the current study yielded
an increase by 18 percentage points.
5 Discussion
The filter combined with the Levenshtein normali-
sation procedure to refine candidate expansion se-
lection showed a slight improvement compared to
using post-processing, although the normalization
procedure should be elaborated in order to be able
to confidently claim that Levenshtein distance nor-
malization is a better approach to expansion candi-
date selection. A suggestion for future work is to
introduce weights based on frequently occurring
edits between abbreviations and expansions and to
apply these in abbreviation normalization.
The approach presented in this study is limited
to abbreviations that translate into one full length
word. Future research should include handling
multiword expressions, not only unigrams, in or-
der to process acronyms and initialisms.
Recall of the development sets in the word
space parameter optimization experiments showed
higher scores for SEPR (0.25) compared to SEPR-
X (0.17). An explanation to this could be that the
amount of data preprocessing done prior to word
space induction might have varied, in terms of ex-
cluding sentences with little or no clinical con-
tent. This will of course affect word space co-
occurrence information, as word context is accu-
mulated without taking sentence boundaries into
account.
The lemmatization of the clinical text used for
word space induction left some words in their
original form, causing test data and semantically
related words to be morphologically discrepant.
Lemmatization adapted to clinical text might have
improved results. Spelling errors were also fre-
quent in the clinical text, and abbreviations were
sometimes normalized into a misspelled variant of
the correct expansion. In the future, spelling cor-
rection could be added and combined with abbre-
viation expansion.
The impact that this apporach to abbreviation
expansion might have on readability of clinical
texts should also be assessed by means of an ex-
trinsic evaluation, a matter to be pursued in future
research.
6 Conclusions
We presented automatic expansion of abbrevia-
tions consisting of unigram full-length words in
clinical texts. We applied a distributional semantic
approach by using word space models and com-
bined this with Levenshtein distance measures to
choose the correct candidate among the semanti-
cally related words. The results show that the cor-
rect expansion of the abbreviation can be found
in 40% of the cases, an improvement by 24 per-
centage points compared to the baseline (0.16) and
an increase by 22 percentage points compared to
using word space models alone (0.18). Applying
Levenshtein distance to refine the selection of se-
mantically related candidate expansions yields a
total recall of 0.38 and 0.40 for radiology reports
and medical health records, respectively.
Acknowledgments
The study was partly funded by the V?ardal Fun-
dation and supported by the Swedish Foundation
for Strategic Research through the project High-
Performance Data Mining for Drug Effect Detec-
tion (ref. no. IIS11-0053) at Stockholm Univer-
sity, Sweden. The authors would also like to direct
thanks to the reviewers for valuable comments.
References
M. Adnan, J. Warren, and M. Orr. 2010. Assess-
ing text characteristics of electronic discharge sum-
maries and their implications for patient readability.
In Proceedings of the Fourth Australasian Workshop
on Health Informatics and Knowledge Management-
Volume 108, pages 77?84. Australian Computer So-
ciety, Inc.
101
H. Allvin. 2010. Patientjournalen som genre: En text-
och genreanalys om patientjournalers relation till pa-
tientdatalagen. Master?s thesis, Stockholm Univer-
sity.
H. Ao and T. Takagi. 2005. ALICE: an algorithm
to extract abbreviations from MEDLINE. Journal
of the American Medical Informatics Association,
12(5):576?586.
S. Cederblom. 2005. Medicinska f?orkortningar och
akronymer (In Swedish). Studentlitteratur.
J.T. Chang, H. Sch?utze, and R.B. Altman. 2002. Creat-
ing an online dictionary of abbreviations from med-
line. Journal of the American Medical Informatics
Association, 9:612?620.
T. Cohen and D. Widdows. 2009. Empirical dis-
tributional semantics: Methods and biomedical ap-
plications. Journal of Biomedical Informatics,
42(2):390?405.
H. Dalianis, M. Hassel, and S. Velupillai. 2009. The
Stockholm EPR Corpus ? Characteristics and some
initial findings. In Proceedings of the 14th Interna-
tional Symposium on Health Information Manage-
ment Research, pages 243?249.
D. Dann?ells. 2006. Automatic acronym recognition.
In Proceedings of the 11th conference on European
chapter of the Association for Computational Lin-
guistics (EACL), pages 167?170.
N. Elhadad. 2006. User-sensitive text summarization:
Application to the medical domain. Ph.D. thesis,
Columbia University.
S. Gaudan, H. Kirsch, and D. Rebholz-Schuhmann.
2005. Resolving abbreviations to their senses in
MEDLINE. Bioinformatics, 21(18):3658?3664,
September.
Z.S. Harris. 1954. Distributional structure. Word,
10:146?162.
A. Henriksson, H. Moen, M. Skeppstedt, A. Eklund,
V. Daudaravicius, and M. Hassel. 2012. Syn-
onym Extraction of Medical Terms from Clinical
Text Using Combinations of Word Space Models.
In Proceedings of Semantic Mining in Biomedicine
(SMBM 2012), pages 10?17.
A. Henriksson, H. Moen, M. Skeppstedt, V. Daudar-
avicius, and M. Duneld. 2014. Synonym extrac-
tion and abbreviation expansion with ensembles of
semantic spaces. Journal of Biomedical Semantics,
5(6).
A. Henriksson. 2013. Semantic Spaces of Clini-
cal Text: Leveraging Distributional Semantics for
Natural Language Processing of Electronic Health
Records. Licentiate thesis, Department of Computer
and Systems Sciences, Stockholm University.
N. Isenius, S. Velupillai, and M. Kvist. 2012.
Initial Results in the Development of SCAN: a
Swedish Clinical Abbreviation Normalizer. In Pro-
ceedings of the CLEF 2012 Workshop on Cross-
Language Evaluation of Methods, Applications, and
Resources for eHealth Document Analysis (CLEFe-
Health2012).
G. K?allgren. 1998. Documentation of the Stockholm-
Ume?a corpus. Department of Linguistics, Stockholm
University.
P. Kanerva, J. Kristoferson, and A. Holst. 2000. Ran-
dom indexing of text samples for latent semantic
analysis. In Proceedings of the 22nd annual con-
ference of the cognitive science society, page 1036.
A. Keselman, L. Slaughter, C. Arnott-Smith, H. Kim,
G. Divita, A. Browne, C. Tsai, and Q. Zeng-Treitler.
2007. Towards consumer-friendly PHRs: patients
experience with reviewing their health records.
In AMIA Annual Symposium Proceedings, volume
2007, pages 399?403.
O. Knutsson, J. Bigert, and V. Kann. 2003. A ro-
bust shallow parser for Swedish. In Proceedings of
Nodalida.
D. Kokkinakis. 2012. The Journal of the
Swedish Medical Association-a Corpus Resource
for Biomedical Text Mining in Swedish. In Pro-
ceedings of Third Workshop on Building and Eval-
uating Resources for Biomedical Text Mining Work-
shop Programme, page 40.
K. Kukich. 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys
(CSUR), 24(4):377?439.
M. Kvist and S. Velupillai. 2013. Professional Lan-
guage in Swedish Radiology Reports ? Charac-
terization for Patient-Adapted Text Simplification.
In Scandinavian Conference on Health Informatics
2013, pages 55?59.
V.I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. In Soviet
physics doklady, volume 10, page 707.
D. Movshovitz-Attias and W.W. Cohen. 2012.
Alignment-HMM-based Extraction of Abbrevia-
tions from Biomedical Text. In Proceedings of the
2012 Workshop on Biomedical Natural Language
Processing (BioNLP 2012), pages 47?55.
N. Nakatsu, Y. Kambayashi, and S. Yajima. 1982. A
longest common subsequence algorithm suitable for
similar text strings. Acta Informatica, 18(2):171?
179.
Y. Park and R.J. Byrd. 2001. Hybrid text mining for
finding abbreviations and their definitions. In Pro-
ceedings of the 2001 conference on empirical meth-
ods in natural language processing, pages 126?133.
102
E. Pettersson, B. Megyesi, and J. Nivre. 2013. Nor-
malisation of historical text using context-sensitive
weighted levenshtein distance and compound split-
ting. In Proceedings of the 19th Nordic Conference
of Computational Linguistics (NODALIDA 2013),
pages 163?179.
R.E. Rudd, B.A. Moeykens, and T.C. Colton. 1999.
Health and literacy: a review of medical and pub-
lic health literature. Office of Educational Research
and Improvement.
M. Sahlgren, A. Holst, and P. Kanerva. 2008. Permu-
tations as a means to encode order in word space. In
Proceedings of the 30th Annual Meeting of the Cog-
nitive Science Society, pages 1300?1305.
M. Sahlgren. 2006. The Word-space model. Ph.D.
thesis, Stockholm University.
A.S. Schwartz and M.A. Hearst. 2003. A simple al-
gorithm for identifying abbreviation definitions in
biomedical text. In Proceedings of Pacific Sympo-
sium on Biocomputing, pages 451?462.
M. Skeppstedt. 2012. From Disorder to Order: Ex-
tracting clinical findings from unstructured text. Li-
centiate thesis, Department of Computer and Sys-
tems Sciences, Stockholm University.
K. Taghva and J. Gilbreth. 1999. Recogniz-
ing acronyms and their definitions. International
Journal on Document Analysis and Recognition,
1(4):191?198.
H. Yu, G. Hripcsak, and C. Friedman. 2002. Map-
ping abbreviations to full forms in biomedical arti-
cles. Journal of the American Medical Informatics
Association, 9(3):262?272.
103
