Proceedings of NAACL-HLT 2013, pages 507?517,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Robust Systems for Preposition Error Correction Using Wikipedia Revisions
Aoife Cahill?, Nitin Madnani?, Joel Tetreault? and Diane Napolitano?
? Educational Testing Service, 660 Rosedale Road, Princeton, NJ 08541, USA
{acahill, nmadnani, dnapolitano}@ets.org
? Nuance Communications, Inc., 1198 E. Arques Ave, Sunnyvale, CA 94085, USA
Joel.Tetreault@nuance.com
Abstract
We show that existing methods for training
preposition error correction systems, whether
using well-edited text or error-annotated cor-
pora, do not generalize across very differ-
ent test sets. We present a new, large error-
annotated corpus and use it to train systems
that generalize across three different test sets,
each from a different domain and with differ-
ent error characteristics. This new corpus is
automatically extracted from Wikipedia revi-
sions and contains over one million instances
of preposition corrections.
1 Introduction
One of the main themes that has defined the field of
automatic grammatical error correction has been the
availability of error-annotated learner data to train
and test a system. Some errors, such as determiner-
noun number agreement, are easily corrected us-
ing rules and regular expressions (Leacock et al,
2010). On the other hand, errors involving the usage
of prepositions and articles are influenced by sev-
eral factors including the local context, the prior dis-
course and semantics. These errors are better han-
dled by statistical models which potentially require
millions of training examples.
Most statistical approaches to grammatical error
correction have used one of the following training
paradigms: 1) training solely on examples of cor-
rect usage (Han et al, 2006); 2) training on exam-
ples of correct usage and artificially generated er-
rors (Rozovskaya and Roth, 2010); and 3) training
on examples of correct usage and real learner er-
rors (Dahlmeier and Ng, 2011; Dale et al, 2012).
The latter two methods require annotated corpora of
errors, and while they have shown great promise,
manually annotating grammatical errors in a large
enough corpus of learner writing is often a costly
and time-consuming endeavor.
In order to efficiently and automatically acquire a
very large corpus of annotated learner errors, we in-
vestigate the use of error corrections extracted from
Wikipedia revision history. While Wikipedia re-
vision history has shown promise for other NLP
tasks including paraphrase generation (Max and
Wisniewski, 2010; Nelken and Yamangil, 2008) and
spelling correction (Zesch, 2012), this resource has
not been used for the task of grammatical error cor-
rection.
To evaluate the usefulness of Wikipedia revision
history for grammatical error correction, we address
the task of correcting errors in preposition selection
(i.e., where the context licenses the use of a prepo-
sition, but the writer selects the wrong one). We
first train a model directly on instances of correct
and incorrect preposition usage extracted from the
Wikipedia revision data. We also generate artificial
errors using the confusion distributions derived from
this data. We compare both of these approaches to
models trained on well-edited text and evaluate each
on three test sets with a range of different character-
istics. Each training paradigm is applied to multiple
data sources for comparison. With these multiple
evaluations, we address the following research ques-
tions:
1. Across multiple test sets, which data source
507
is more useful for correcting preposition er-
rors: a large amount of well-edited text, a large
amount of potentially noisy error-annotated
data (either artificially generated or automati-
cally extracted) or a smaller amount of higher
quality error-annotated data?
2. Given error-annotated data, is it better to train
on the corrections directly or to use the con-
fusion distributions derived from these correc-
tions for generating artificial errors in well-
edited text?
3. What is the impact of having a mismatch in the
error distributions of the training and test sets?
2 Related Work
In this section, we only review work in preposi-
tion error correction in terms of the three training
paradigms and refer the reader to Leacock et al
(2010) for a more comprehensive review of the field.
2.1 Training on Well-Edited Text
Early approaches to error detection and correction
did not have access to large amounts of error-
annotated data to train statistical models and thus,
systems were trained on millions of well-edited ex-
amples from news text instead (Gamon et al, 2008;
Tetreault and Chodorow, 2008; De Felice and Pul-
man, 2009). Feature sets usually consisted of n-
grams around the preposition, POS sequences, syn-
tactic features and semantic information. Since the
model only had knowledge of correct usage, an error
was flagged if the system?s prediction for a particu-
lar preposition context differed from the preposition
the writer used.
2.2 Artificial Errors
The issue with training solely on correct usage was
that the systems had no knowledge of typical learner
errors. Ideally, a system would be trained on ex-
amples of correct and incorrect usage, however, for
many years, such error-annotated corpora were not
available. Instead, several researchers generated ar-
tificial errors based on the error distributions derived
from the error-annotated learner corpora available at
the time. Izumi et al (2003) was the first to evaluate
a model trained on incorrect usage as well as artifi-
cial errors for the task of correcting several different
error types, including prepositions. However, with
limited training data, system performance was quite
poor. Rozovskaya and Roth (2010) evaluated dif-
ferent ways of generating artificial errors and found
that a system trained on artificial errors could outper-
form the more traditional training paradigm of using
only well-edited texts. Most recently, Imamura et al
(2012) showed that performance could be improved
by training a model on artificial errors and address-
ing domain adaptation for the task of Japanese par-
ticle correction.
2.3 Error-Annotated Learner Corpora
Recently, error-annotated learner data has become
more readily and publicly available allowing models
to be trained on both examples of correct usage as
well typical learner errors. Han et al (2010) showed
that a preposition error detection and correction sys-
tem trained on 100,000 annotated preposition errors
from the Chungdahm Corpus of Korean Learner En-
glish (in addition to 1 million examples of correct
usage) outperformed a model trained only on 5 mil-
lion examples of correct usage. Gamon (2010) and
Dahlmeier and Ng (2011) showed that combining
models trained separately on examples of correct
and incorrect usage could also improve the perfor-
mance of a preposition error correction system.
3 Mining Wikipedia Revisions for
Grammatical Error Corrections
3.1 Related Work
Many NLP researchers have taken advantage of the
wealth of information available in Wikipedia revi-
sions. Dutrey et al (2011) define a typology of mod-
ifications found in the French Wikipedia (WiCo-
PaCo). They show that the kinds of edits made range
from specific lexical changes to more general rewrite
edits. Similar types of edits are found in the En-
glish Wikipedia. The data extracted from Wikipedia
revisions has been used for a wide variety of tasks
including spelling correction (Max and Wisniewski,
2010; Zesch, 2012), lexical error detection (Nelken
and Yamangil, 2008), sentence compression (Ya-
mangil and Nelken, 2008), paraphrase generation
(Max and Wisniewski, 2010; Nelken and Yamangil,
2008), lexical simplification (Yatskar et al, 2010)
and entailment (Zanzotto and Pennacchiotti, 2010;
508
(1) [Wiki clean] In addition, sometimes it is also left to stand overnight (at? in) the refrigerator.
(2) [Wiki clean] Also none of the witnesses present (of? on) those dates supports Ranneft?s claims.
(3) [Wiki dirty] . . . cirque has a permanent production (to? at) the Mirage, love.
(4) [Wiki dirty] In the late 19th century Vasilli Andreyev a salon violinist took up the balalaika in his
performances for French tourists (in? to) Petersburg.
Figure 1: Example sentences with preposition errors extracted from Wikipedia revisions. The second preposition is
assumed to be the correction.
Cabrio et al, 2012). To our knowledge, no one has
previously extracted data for training a grammatical
error detection system from Wikipedia revisions.
3.2 Extracting Preposition Correction Data
from Wikipedia Revisions
As the source of our Wikipedia revisions, we used an
XML snapshot of Wikipedia generated in July 2011
containing 8,735,890 articles and 288,583,063 revi-
sions.1 We then used the following process to ex-
tract preposition errors and their corresponding cor-
rections from this snapshot:
Step 1: Extract the plain text versions of all revi-
sions of all articles using the Java Wikipedia
Library (Ferschke et al, 2011).
Step 2: For each Wikipedia article, compare each
revision with the revision immediately preced-
ing it using an efficient diff algorithm.2
Step 3: Compute all 1-word edit chains for the arti-
cle, i.e., sequences of related edits derived from
all revisions of the same article. For example,
say revision 10 of an article inserts the preposi-
tion of into a sentence and revision 12 changes
that preposition to on. Assuming that no other
revisions change this sentence, the correspond-
ing edit chain would contain the following 3 el-
ements: ?of?on. The extracted chains con-
tain the full context on either side of the 1-word
edit, up to the automatically detected sentence
boundaries.
Step 4: (a) Ignore any circular chains, i.e., where
the first element in the edit chain is the same as
the last element. (b) Collapse all non-circular
1http://dumps.wikimedia.org/enwiki/
2http://code.google.com/p/google-diff-match-patch/
chains, i.e., only retain the first and the last ele-
ments in a chain. Both these decisions are mo-
tivated by the assumption that the intermediate
links in the chain are unreliable for training an
error correction system since a Wikipedia con-
tributor modified them.
Step 5 : From all remaining 2-element chains, find
those where a preposition is replaced with an-
other preposition. If the preposition edit is the
only edit in the sentence, we convert the chain
into a sentence pair and label it clean. If there
are other 1-word edits but not within 5 words of
the preposition edit on either side, we label the
sentence somewhat clean. Otherwise, we label
it dirty. The motivation is that the presence of
other nearby edits make the preposition correc-
tion less reliable when used in isolation, due to
the possible dependencies between corrections.
All extracted sentences were part-of-speech tagged
using the Stanford Tagger (Toutanova et al, 2003).
Using the above process, we are able to extract ap-
proximately 2 million sentences containing preposi-
tions errors and their corrections. Some examples
of the sentences we extracted are given in Figure 1.
Example (4) shows an example of a bad correction.
4 Corpora
We use several corpora for training and testing our
preposition error correction system. The proper-
ties of each are outlined in Table 1, organized by
paradigm. For each corpus we report the total num-
ber of prepositions used for training, as well as the
number and percentage of preposition corrections.
4.1 Well-edited Text
We train our system on two well-edited corpora.
The first is the same corpus used by Tetreault and
509
Corpus Total # Preps # Corrected Preps
Well-edited Text
Wikipedia Snapshot (10m sents) 26,069,860 0 (0%)
Lexile/SJM 6,719,077 0 (0%)
Artificially Generated
Errors
Wikipedia Snapshot 26,127,464 2,844,227 (10.9%)
Lexile/SJM 6,723,206 792,195 (11.8%)
Naturally Occurring
Errors
Wikipedia Revisions All 7,125,317 1,027,643 (20.6%)
Wikipedia Revisions ?Clean 3,001,900 381,644 (12.7%)
Wikipedia Revisions Clean 1,978,802 266,275 (14.4%)
Lang-8 129,987 53,493 (41.2%)
NUCLE Train 72,741 922 (1.3%)
Test Corpora
NUCLE Test 9,366 125 (1.3%)
FCE 33,243 2,900 (8.7%)
HOO 2011 Test 1,703 81 (4.8%)
Table 1: Corpora characteristics
Chodorow (2008), comprising roughly 1.8 million
sentences from the San Jose Mercury News Corpus3
and roughly 1.8 million sentences from grades 11
and 12 of the MetaMetrics Lexile Corpus. Our sec-
ond corpus is a random sample of 10 million sen-
tences containing at least one preposition from the
June 2012 snapshot of English Wikipedia Articles.4
4.2 Artificially Generated Errors
Similar to Foster and Andersen (2009) and Ro-
zovskaya and Roth (2010), we artificially introduce
preposition errors into well-edited corpora (the two
described above). We do this based on a distribu-
tion of possible confusions and train a model that
is aware of the corrections. The two sets of con-
fusion distributions we used were derived based on
the errors extracted from Wikipedia revisions and
Lang-8 respectively (discussed in Section 4.3). For
each corrected preposition pi in the revision data,
we calculated P (pi|pj), where pj is each of the pos-
sible original prepositions that were confused with
pi. Then, for each sentence in the well-edited text,
all prepositions are extracted. A preposition is ran-
domly selected (without replacement) and changed
based on the distribution of possible confusions
(note that the original preposition is also included
in the distribution, usually with a high probabil-
3The San Jose Mercury News is available from the Linguis-
tic Data Consortium (catalog number LDC93T3A).
4We used a newer version of the Wikipedia text for the well-
edited text, since we assume that more recent versions of the
text will be most grammatical, and therefore closer to well-
edited.
ity, meaning that there is a strong preference not to
change the preposition). If a preposition is changed
to something other than the original preposition, all
remaining prepositions in the sentence are left un-
changed.
4.3 Naturally Occurring Errors
We have a number of corpora that contain annotated
preposition errors. Note that we are only considering
incorrectly selected prepositions, we do not consider
missing or extraneous.
NUCLE The NUS Corpus of Learner English (NU-
CLE)5 contains one million words of learner
essay text, manually annotated with error tags
and corrections. We use the same training, dev
and test splits as Dahlmeier and Ng (2011).
FCE The CLC FCE Dataset6 is a collection of
1,244 exam scripts written by learners of En-
glish as part of the Cambridge ESOL First Cer-
tificate in English (Yannakoudakis et al, 2011).
It includes demographic metadata about the
candidate, a grade for each essay and manually-
annotated error corrections.
Wikipedia We use three versions of the preposi-
tion errors extracted from the Wikipedia revi-
sions as described in Section 3.2. The first in-
cludes corrections where the preposition was
the only word corrected in the entire sentence
5http://bit.ly/nuclecorpus
6http://ilexir.co.uk/applications/clc-fce-dataset/
510
(clean). The second contains all clean cor-
rections, as well as all corrections where there
were no other edits within a five-word span on
either side of the preposition (?clean). The
third contains all corrections regardless of any
other changes in the surrounding context (all).
Lang-8 The Lang-8 website contains journals writ-
ten by language learners, where native speakers
highlight and correct errors on a sentence-by-
sentence basis. As a result, it contains typical
grammatical mistakes made by language learn-
ers, which can be easily downloaded. We auto-
matically extract 75,622 sentences with prepo-
sition errors and corrections from the first mil-
lion journal entries.7
HOO 2011 We take the test set from the HOO 2011
shared task (Dale and Kilgarriff, 2011) and ex-
tract all examples of preposition selection er-
rors. The texts are fragments of ACL papers
that have been manually annotated for gram-
matical errors.8
It is important to note that the three test sets we use
are from entirely different domains: exam scripts
from non-native English speakers (FCE), essays by
highly proficient college students in Singapore (NU-
CLE) and ACL papers (HOO). In addition, they have
a different number of total prepositions as well as er-
roneous prepositions.
5 Preposition Error Correction
Experiments
We use the preposition error correction model de-
scribed in Tetreault and Chodorow (2008)9 to eval-
uate the many ways of using Wikipedia error cor-
rections as described in the Section 4. We use this
system since it has been recreated for other work
(Dahlmeier and Ng, 2011; Tetreault et al, 2010) and
is similar in methodology to Gamon et al (2008)
7Tajiri et al (2012) extract a corpus of English verb phrases
corrected for tense/aspect errors from Lang-8. They kindly pro-
vided us with their scripts to carry out the scraping of Lang-8.
8The results of the HOO 2011 shared task were not reported
at level of preposition selection error, therefore it is not possible
to compare the results presented in this paper with those results.
9Note that in that work, the model was evaluated in terms of
preposition error detection rather than correction, however the
model itself does not change.
and De Felice and Pulman (2009). In short, the
method models the problem of preposition error cor-
rection (for replacement errors) as a 36-way classifi-
cation problem using a multinomial logistic regres-
sion model.10 The system uses 25 lexical, syntac-
tic and n-gram features derived from the contexts of
each preposition training instance.
We modified the training paradigm of Tetreault
and Chodorow (2008) so that a model could be
trained on examples of correct usage as well as ac-
tual errors. We did this by adding a new feature
specifying the writer?s original preposition (as in
Han et al (2010) and Dahlmeier and Ng (2011)).
5.1 Results
We train a preposition correction system using each
of the three data paradigms and test on the FCE,
NUCLE and HOO 2011 test corpora. For each
preposition in the test corpus, we record whether
the system predicted that it should be changed,
and if so, what it should be changed to. We then
compare the prediction to the annotation in the test
corpus. We report results in terms of f-score, where
precision and recall are calculated as follows:11
Precision = Number of correct preposition correctionsTotal number of corrections suggested
Recall = Number of correct preposition correctionsTotal number of corrections in test set
Note that due to the high volume of unchanged
prepositions in the test corpus, we obtain very high
accuracies, which are not indicative of true perfor-
mance, and are not included in our results.
The results of our experiments are presented in
Table 2.12 The first part of the table shows the f-
scores of preposition error correction systems that
10We use liblinear (Fan et al, 2008) with the L1-regularized
logistic regression solver and default parameters.
11As Chodorow et al (2012) note, it is not clear how to han-
dle cases where the system predicts a preposition that is neither
the same as the writer preposition nor the correct preposition.
We count these cases as false positives.
12No thresholds were used in the systems that were trained
on well-edited text. Traditionally, thresholds are applied so as
to only predict a correction when the system is highly confident.
This has the effect of increasing precision at the cost of recall,
and sometimes leads to an overall improved f-score. Here we
take the prediction of the system, regardless of the confidence,
reflecting a lower-bound of this method.
511
Data Source Paradigm CLC-FCE NUCLE HOO2011
N=33,243 N=9,366 N=1,703
Without
Wikipedia
Revisions
(nonWikiRev)
Wikipedia Snapshot Well-edited Text 24.43? 5.02? 12.36?
Lexile/SJM Well-edited Text 24.73? 4.29? 9.73?
Wikipedia Snapshot Artificial Errors (Lang-8) 42.15? 19.91? 28.75
Lexile/SJM Artificial Errors (Lang-8) 45.36 18.00? 25.15
Lang-8 Error-annotated Text 38.22? 8.18? 24.00
NUCLE train Error-annotated Text 5.38? 20.14 4.82?
With
Wikipedia
Revisions
(WikiRev)
Wikipedia Snapshot Artificial Errors (Wiki) 31.17? 24.52 28.30
Lexile/SJM Artificial Errors (Wiki) 34.35? 23.38 32.76
Wikipedia Revisions All Error-annotated Text 33.59? 26.39 36.84
Wikipedia Revisions ?Clean Error-annotated Text 29.68? 22.13 36.04
Wikipedia Revisions Clean Error-annotated Text 28.09? 21.74 28.30
Table 2: Preposition selection error correction results (f-score). The systems with scores in bold are statistically
significantly better than all systems marked with an asterisk (p < 0.01). Confidence intervals were obtained using
bootstrap resampling with 50,000 replicates.
one might be able to train with publicly available
data excluding the Wikipedia revisions that we have
extracted. We refer to these systems as nonWikiRev
systems. The second part of the table shows the f-
scores of systems trained on the Wikipedia revisions
data ? either directly on the annotated errors or on
the artificial errors produced using the confusion dis-
tributions derived from these annotated errors. We
refer to this second set of systems as WikiRev sys-
tems. The nonWikiRev systems perform inconsis-
tently, heavily dependent on the characteristics of
the test set in question. On the other hand, it is
obvious that the WikiRev systems ? while not al-
ways outperforming the best nonWikiRev systems
? generalize much better across the three test sets.
In fact, for the NUCLE test set, the best WikiRev
system performs as well as the nonWikiRev system
trained on data from the same domain and with iden-
tical error characteristics as the test set. The distri-
butions of errors in the three test sets are not sim-
ilar, and therefore, the stability in performance of
the WikiRev systems cannot be attributed to the hy-
pothesis that the WikiRev training data error distri-
butions are more similar to the test data than any of
the other training corpora. Therefore, we claim that
if a preposition error correction system is to be de-
ployed on data for which the error characteristics are
not known in advance, i.e. most real-world scenar-
ios, training the system using Wikipedia revisions is
likely to be the most robust option.
6 Discussion
We examine the results of our experiments in light
of the research questions we posed in Section 1.
6.1 Which Data Source is More Useful?
We wanted to know whether it was better to have
a smaller corpus of carefully annotated corrections,
or a much larger (but automatically generated, and
therefore noisier) error-annotated corpus. We also
wanted to compare this scenario to training on large
amounts of well-edited text. From our experiments,
it is clear that the composition of the test set plays
a major role in answering this question. On a test
set with few corrections (NUCLE), training on well-
edited text (and without using thresholds) performs
particularly poorly. On the other hand, when eval-
uating on the FCE test set which contains far more
errors, training on well-edited text performs reason-
ably well (though statistically significantly worse
than training on all of the Wikipedia errors). Sim-
ilarly, training on the smaller, high-quality NU-
CLE corpus and evaluating on the NUCLE test set
achieves good results, however training on NUCLE
and testing on FCE achieves the lowest f-score of all
our systems on that test set.
Figure 2 shows the learning curves obtained by
increasing the size of the training data for two
of the test sets.13 Although one might assume
13For space reasons, the graph for HOO2011 is omitted. Also
note that the results in Table 2 may not appear in the graph,
512
Wiki (All)Wiki (Clean)Lang-8NUCLELexile (artificial via Wiki)Lexile (artificial via Lang-8)
F-s
cor
e
0
10
20
30
40
50
log(training data size in thousands of instances)1 2 3 4
Wiki (All)Wiki (Clean)Lang-8NUCLELexile (artificial via Wiki)Lexile (artificial via Lang-8)
F-s
cor
e
0
5
10
15
20
25
1 2 3 4
(a) NUCLE
(b) FCE
Figure 2: The effect of varying the size of the training corpus
that Wikipedia-clean would be more reliable than
Wikipedia-all, the cleanness of the Wikipedia data
seems to make very little difference, probably be-
cause the data extracted in the dirty contexts is not
as noisy as we expected. Interestingly, it also seems
that additional data would lead to further improve-
ments for models trained on artificial errors in Lexile
data and for those trained on all of the automatically
extracted Wikipedia errors.
Another interesting aspect of Figure 2 is that
since we were sampling at specific data points which did not
correspond exactly to the total sizes of the training corpora.
training on the Lang-8 data shows a very steep rising
trend. This suggests that automatically-scraped data
that is highly targeted towards language learners is
very useful in correcting preposition errors in texts
where they are reasonably frequent.
6.2 Natural or Artificially Generated Errors?
Table 2 shows that training on artificially generated
errors via Wikipedia revisions performs fairly con-
sistently across test corpora. While using Lang-8
for artificial error generation is also quite promis-
ing for FCE, it does not generalize across test sets.
513
Wiki (All)Wiki (Clean)Lang-8Lexile (artificial via Wiki)Lexile (artificial via Lang-8)
F-s
cor
e
0
10
20
30
40
50
Percentage of Errors in Training Data0 5 10 15 20 25 30 35 40 45 50 55
Wiki (All)Wiki (Clean)Lang-8Lexile (artificial via Wiki)Lexile (artificial via Lang-8)
F-s
cor
e
5
10
15
20
25
30
0 5 10 15 20 25 30 35 40 45 50 55
(a) NUCLE
(b) FCE
Figure 3: The effect of varying the percentage of errors in the training corpus
On FCE it achieves the highest results, on NUCLE
it performs statistically significantly worse than the
best system, and on HOO 2011 it achieves a lower
(though not statistically significant) result than the
best system. This highlights that extracting errors
from Wikipedia is useful in two ways: (1) training a
system on the errors alone works well and (2) gener-
ating artificial errors in well-edited corpora of differ-
ent domains and training a system on that also works
well. It also indicates that if the system were to be
applied to a specific domain, applying the confusion
distributions to a domain specific corpus ? if avail-
able ? would likely yield the best results.
6.3 Mismatching Distributions
The proportion of errors in the training and test data
plays an important role in the performance of any
preposition error correction system. This is clearly
evident by comparing system performances across
the three test sets which have fairly different compo-
sitions. FCE contains a much higher proportion of
errors than NUCLE, and HOO falls somewhere in
between. Interestingly, the system trained on Lang-
8 data (which contains the highest proportion of er-
514
rors among all training corpora) performs best on
the FCE data. On the other hand, the same sys-
tem performs poorly on NUCLE test which contains
far fewer errors. In this instance, the system learns
to predict an incorrect preposition too often. We
see a similar pattern with the system trained on the
NUCLE training data. It performs poorly on FCE
which contains many errors, but well on NUCLE
test which contains a similar proportion of errors.
In order to better understand the relationship be-
tween the percentage of errors in the training data
and system performance, we vary the percentage of
errors in each training corpus from 1-50% and test
on the unchanged FCE and NUCLE test corpora.
For each training corpus, we reduce the size to be
twice the size of the total number of errors.14 Keep-
ing this size constant, we then artificially change the
percentage of errors. Note that because the total size
of the corpus has changed, the results in Table 2 may
not appear in the graph. Figure 3 shows the effect on
f-score when the data composition is changed. For
both test sets, there is a peak after which increas-
ing the proportion of errors in the training corpus is
detrimental. For NUCLE test with its low number
of preposition errors, this peak is very pronounced.
For FCE, it is more of a gentle degradation in per-
formance, but the pattern is clear. Also noteworthy
is the fact that the degradation for models trained on
artificial errors is less steep suggesting that they may
be more stable across test sets.
In general, these results indicate that when
building a preposition error detection using error-
annotated data, the characteristics of the data to
which the system will be applied should play a vital
role in how the system is to be trained. Our results
show that the WikiRev systems are robust across
test sets, however if the exact distribution of errors
in the data is known in advance, other models may
perform better.
7 Conclusion
Although previous approaches to preposition er-
ror correction using either well-edited text or small
hand-annotated corrections performed well on some
specific test set, they did not generalize well across
14We omit the NUCLE train corpus from this comparison,
because it contains too few errors to obtain a meaningful result.
very different test sets. In this paper, we present
work that automatically extracts preposition error
corrections from Wikipedia Revisions and uses it
to build robust error correction systems. We show
that this data is useful for two purposes. Firstly, a
model trained directly on the corrections performs
well across test sets. Secondly, models trained on ar-
tificial errors generated from the distribution of con-
fusions in the Wikipedia data perform equally well.
The distribution of confusions can also be applied to
other well-edited corpora in different domains, pro-
viding a very powerful method of automatically gen-
erating error corpora. The results of our experiments
also highlight the importance of the distribution of
expected errors in the test set. Models that perform
well on one kind of distribution may not necessar-
ily work on a completely different one, as evident
in the performances of the systems trained on either
Lang-8 or NUCLE. In general, the WikiRev mod-
els perform well across distributions. We also con-
ducted some preliminary system combination exper-
iments and found that while they yielded promising
results, further investigation is necessary. We have
also made the Wikipedia preposition correction cor-
pus available for download.15
In future work, we will examine whether the
results we obtain for English generalize to other
Wikipedia languages. We also plan to extract multi-
word corrections for other types of errors and to ex-
amine the usefulness of including error contexts in
our confusion distributions (e.g., preposition confu-
sions following verbs versus those following nouns).
Acknowledgments
The authors would like to thank Daniel Dahlmeier,
Torsten Zesch, Mamoru Komachi, Tajiri Toshikazu,
Tomoya Mizumoto and Yuji Matsumoto for provid-
ing scripts and data that enabled us to carry out
this research. We would also like to thank Martin
Chodorow and the anonymous reviewers for their
helpful suggestions and comments.
References
Elena Cabrio, Bernardo Magnini, and Angelina Ivanova.
2012. Extracting Context-Rich Entailment Rules from
15http://bit.ly/etsprepdata
515
Wikipedia Revision History. In Proceedings of the 3rd
Workshop on the People?s Web Meets NLP: Collabora-
tively Constructed Semantic Resources and their Ap-
plications to NLP, pages 34?43, Jeju, Republic of Ko-
rea, July. Association for Computational Linguistics.
Martin Chodorow, Markus Dickinson, Ross Israel, and
Joel Tetreault. 2012. Problems in Evaluating Gram-
matical Error Detection Systems. In Proceedings of
COLING 2012, pages 611?628, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammat-
ical Error Correction with Alternating Structure Op-
timization. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 915?923, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 Pilot Shared Task. In Pro-
ceedings of the Generation Challenges Session at the
13th European Workshop on Natural Language Gener-
ation, pages 242?249, Nancy, France, September. As-
sociation for Computational Linguistics.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition
and Determiner Error Correction Shared Task. In
Proceedings of the Seventh Workshop on Building
Educational Applications Using NLP, pages 54?62,
Montre?al, Canada, June. Association for Computa-
tional Linguistics.
Rachele De Felice and Stephen G. Pulman. 2009. Auto-
matic detection of preposition errors in learner writing.
CALICO Journal, 26(3):512?528.
Camille Dutrey, Houda Bouamor, Delphine Bernhard,
and Aure?lien Max. 2011. Local modifications and
paraphrases in Wikipedias revision history. SEPLN
journal(Revista de Procesamiento del Lenguaje Nat-
ural), 46:51?58.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Oliver Ferschke, Torsten Zesch, and Iryna Gurevych.
2011. Wikipedia Revision Toolkit: Efficiently Access-
ing Wikipedia?s Edit History. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies.
System Demonstrations.
Jennifer Foster and Oistein Andersen. 2009. Gen-
ERRate: Generating Errors for Use in Grammatical
Error Detection. In Proceedings of the Fourth Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications, pages 82?90, Boulder, Colorado,
June. Association for Computational Linguistics.
Michael Gamon, Jianfeng Gao, Chris Brockett, Alex
Klementiev, William B. Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using Contextual Speller
Techniques and Language Modeling for ESL Error
Correction. In Proceedings of the International Joint
Conference on Natural Language Processing (IJC-
NLP), pages 449?456, Hyderabad, India.
Michael Gamon. 2010. Using Mostly Native Data to
Correct Errors in Learners? Writing. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 163?171, Los An-
geles, California, June. Association for Computational
Linguistics.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12(2):115?129.
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-
Young Ha. 2010. Using Error-Annotated ESL Data
to Develop an ESL Error Correction System. In Pro-
ceedings of the Seventh International Conference on
Language Resources and Evaluation (LREC), Malta.
Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, and
Hitoshi Nishikawa. 2012. Grammar Error Correc-
tion Using Pseudo-Error Sentences and Domain Adap-
tation. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 388?392, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai
Supnithi, and Hitoshi Isahara. 2003. Automatic Error
Detection in the Japanese Learners? English Spoken
Data. In The Companion Volume to the Proceedings
of 41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 145?148, Sapporo, Japan,
July. Association for Computational Linguistics.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
Claypool.
Aure?lien Max and Guillaume Wisniewski. 2010. Mining
Naturally-occurring Corrections and Paraphrases from
Wikipedia?s Revision History. In Nicoletta Calzo-
lari (Conference Chair), Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, Proceed-
ings of the Seventh conference on International Lan-
guage Resources and Evaluation (LREC?10), Valletta,
Malta, may. European Language Resources Associa-
tion (ELRA).
Rami Nelken and Elif Yamangil. 2008. Mining
Wikipedias Article Revision History for Training
516
Computational Linguistics Algorithms. In Proceed-
ings of the 1st AAAI Workshop on Wikipedia and Arti-
ficial Intelligence, pages 31?36, Chicago, IL.
Alla Rozovskaya and Dan Roth. 2010. Generating Con-
fusion Sets for Context-Sensitive Error Correction.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 961?
970, Cambridge, MA, October. Association for Com-
putational Linguistics.
Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-
sumoto. 2012. Tense and Aspect Error Correction
for ESL Learners Using Global Context. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), Short Papers, pages
198?202, Jeju Island, Korea.
Joel R. Tetreault and Martin Chodorow. 2008. The
Ups and Downs of Preposition Error Detection in
ESL Writing. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 865?872, Manchester, UK.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using Parse Features for Preposition Selection
and Error Detection. In Proceedings of the ACL 2010
Conference Short Papers, pages 353?358, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich Part-of-speech
Tagging with a Cyclic Dependency Network. In Pro-
ceedings of NAACL, pages 173?180.
Elif Yamangil and Rani Nelken. 2008. Mining
Wikipedia Revision Histories for Improving Sentence
Compression. In Proceedings of ACL-08: HLT, Short
Papers, pages 137?140, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automati-
cally Grading ESOL Texts. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 180?189, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: Unsupervised extraction of lexical simplifications
from Wikipedia. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 365?368, Los Angeles, California, June.
Association for Computational Linguistics.
Fabio Massimo Zanzotto and Marco Pennacchiotti.
2010. Expanding textual entailment corpora from
Wikipedia using co-training. In Proceedings of the
2nd Workshop on The People?s Web Meets NLP: Col-
laboratively Constructed Semantic Resources, pages
28?36, Beijing, China, August. Coling 2010 Organiz-
ing Committee.
Torsten Zesch. 2012. Measuring Contextual Fitness Us-
ing Error Contexts Extracted from the Wikipedia Revi-
sion History. In Proceedings of the 13th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 529?538, Avignon, France,
April. Association for Computational Linguistics.
517
Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual Accessibility (NLP4ITA), pages 49?58,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
A Two-Stage Approach for Generating Unbiased                                                   
Estimates of Text Complexity 
 
 
Kathleen M. Sheehan Michael Flor Diane Napolitano 
   
Educational Testing Service 
Princeton, NJ, USA 
{ksheehan, mflor, dnapolitano}@ets.org 
 
 
 
 
 
 
Abstract 
Many existing approaches for measuring text 
complexity tend to overestimate the complexi-
ty levels of informational texts while simulta-
neously underestimating the complexity levels 
of literary texts. We present a two-stage esti-
mation technique that successfully addresses 
this problem.  At Stage 1, each text is classi-
fied into one or another of three possible ge-
nres:  informational, literary or mixed.  Next, 
at Stage 2, a complexity score is generated for 
each text by applying one or another of three 
possible prediction models:  one optimized for 
application to informational texts, one opti-
mized for application to literary texts, and one 
optimized for application to mixed texts.  
Each model combines lexical, syntactic and 
discourse features, as appropriate, to best rep-
licate human complexity judgments. We dem-
onstrate that resulting text complexity 
predictions are both unbiased, and highly cor-
related with classifications provided by expe-
rienced educators. 
1 Introduction 
Automated text analysis systems, such as reada-
bility metrics, are frequently used to assess the 
probability that texts with varying combinations of 
linguistic features will be more or less accessible to 
readers with varying levels of reading comprehen-
sion skill (Stajner, Evans, Orasan and Mitkov, 
2012).  This paper introduces TextEvaluator, a ful-
ly-automated text analysis system designed to faci-
litate such work.1
 Our approach for addressing these differences 
can be summarized as follows.  First, a large set of 
lexical, syntactic and discourse features is ex-
tracted from each text.  Next, either human raters, 
or an automated genre classifier is used to classify 
each text into one or another of three possible ge-
nre categories: informational, literary, or mixed.  
Finally, a complexity score is generated for each 
text by applying one or another of three possible 
prediction models: one optimized for application to 
informational texts, one optimized for application 
to literary texts, and one optimized for application 
to mixed texts. We demonstrate that resulting 
complexity measures are both unbiased, and highly 
correlated with text grade level (GL) classifications 
provided by experienced educators.  
 TextEvaluator successfully 
addresses an important limitation of many existing 
readability metrics:  the tendency to over-predict 
the complexity levels of informational texts, while 
simultaneously under-predicting the complexity 
levels of literary texts (Sheehan, Kostin & Futagi, 
2008; Sheehan, Kostin, Futagi & Flor, 2010). We 
illustrate this phenomenon, and argue that it results 
from two fundamental differences between infor-
mational and literary texts:  (a) differences in the 
way that common every-day words are used and 
combined; and (b) differences in the rate at which 
rare words are repeated.  
                                                          
1 TextEvaluator was previously called SourceRater. 
49
Our paper is organized as follows.  Section 2 
summarizes related work on readability assess-
ment. Section 3 describes the two corpora assem-
bled for use in this study, and outlines how genre 
and GL classifications were assigned.  Section 4 
illustrates the problem of genre bias by considering 
the specific biases detected in two widely-used 
readability metrics.  Section 5 describes the Text- 
Evaluator features, methods and results. Section 6 
presents a summary and discussion. 
2    Related Work  
Despite the large numbers of text features that may 
potentially contribute to the ease or difficulty of 
comprehending complex text, many widely-used 
readability metrics are based on extremely limited 
feature sets.  For example, the Flesch-Kincaid GL 
score (Kincaid, et al, 1975), the FOG Index (Gun-
ning, 1952), and the Lexile Framework (Stenner, et 
al., 2006) each consider just two features: a single 
measure of syntactic complexity (average sentence 
length) and a single measure of lexical difficulty 
(either average word length in syllables, average 
frequency of multi-syllable words, or average word 
familiarity estimated via a word frequency, WF, 
index).  
Recently, more computationally sophisticated 
modeling techniques such as Statistical Language 
Models (Si and Callan, 2001; Collins-Thompson 
and Callan, 2004, Heilman, et al, 2007, Pitler and 
Nenkova, 2008), Support Vector Machines 
(Schwarm and Ostendorf, 2005), Principal Com-
ponents Analyses (Sheehan, et al, 2010) and Mul-
ti-Layer Perceptron classifiers (Vajjala and 
Meurers, 2012) have enabled researchers to inves-
tigate a broader range of potentially useful fea-
tures.  For example: Schwarm and Ostendorf 
(2005) demonstrated that vocabulary measures 
based on trigrams were effective at distinguishing 
articles targeted at younger and older readers; Pit-
ler and Nenkova (2008) reported improved validity 
for measures based on the likelihood of vocabulary 
and the likelihood of discourse relations; and Vaj-
jala and Meurers (2012) demonstrated that features 
inspired by Second Language Acquisition research 
also contributed to validity improvements.  Impor-
tantly, however, while this research has contributed 
to our understanding of the types of text features 
that may cause texts to be more or less compre-
hensible, evaluations focused on the presence and 
degree of genre bias have not been reported. 
3   Corpora   
Two text collections are considered in this re-
search.  Our training corpus includes 934 passages 
selected from a set of previously administered 
standardized assessments constructed to provide 
valid and reliable feedback about the types of ver-
bal reasoning skills described in U.S. state and na-
tional assessment frameworks. Human judgments 
of genre (informational, literary or mixed) and GL 
(grades 3-12) were available for all texts.  Genre 
classifications were based on established guide-
lines which place texts structured to inform or per-
suade (e.g., newspaper text, excerpts from science 
or social studies textbooks) in the informational 
category, and texts structured to provide a reward-
ing literary experience (e.g., folk tales, short sto-
ries, excerpts from novels) in the literary category 
(see American Institutes for Research, 2008). We 
added a Mixed category to accommodate texts 
classified as incorporating both informational and 
literary elements.  Nelson, Perfetti, Liben and Li-
ben (2012) describe an earlier, somewhat smaller 
version of this dataset.  We added additional pas-
sages downloaded from State Department of Edu-
cation web sites, and from the National 
Assessment of Educational Progress (NAEP).  In 
each case, GL classifications reflected the GLs at 
which passages were administered to students.  
Thus, all passages classified at Grade 3 appeared 
on high-stakes assessments constructed to provide 
evidence of student performance relative to Grade 
3 reading standards.  
Two important characteristics of this dataset 
should be noted.  First, unlike many previous cor-
pora, (e.g., Stenner, et al, 2006; Zeno, et al, 2005) 
accurate paragraph markings are included for all 
texts. Second, while many of the datasets consi-
dered in previous readability research were com-
prised entirely of informational text (e.g., Pitler 
and Nenkova, 2008; Schwarm and Ostendorf, 
2005;  Vajjala and Meurers, 2012) the current da-
taset covers the full range of text types considered 
by teachers and students in U.S. classrooms.   
Table 1 shows the numbers of informational, li-
terary and mixed training passages at each targeted 
GL.  Passage lengths ranged from 112 words at 
Grade 3, to more than 2000 words at Grade 12. 
50
Average passage lengths were 569 words and 695 
words in the informational and literary subsets, 
respectively.  
 
Grade 
Level 
Genre  
Total Inf. Lit. Mixed 
3 46 60 8 114 
4 51 74 7 132 
5 44 46 12 102 
6 41 40 6 87 
7 36 58 6 100 
8 70 63 18 151 
9 23 23 2 48 
10 26 49 2 77 
11 15 24 0 39 
12 47 15 22 84 
Total 399 452 83 934 
 
Table 1.  Numbers of passages in the model develop-
ment/training dataset, by grade level and genre.  
 
A validation dataset was also constructed.  It in-
cludes the 168 texts that were published as Appen-
dix B of the new Common Core State Standards 
(CCSSI, 2010), a new standards document that has 
now been adopted in 46 U.S. states. Individual 
texts were contributed by teachers, librarians, cur-
riculum experts, and reading researchers.  GL clas-
sifications are designed to illustrate the ?staircase 
of increasing complexity? that teachers and test 
developers are being encouraged to replicate when 
selecting texts for use in K-12 instruction and as-
sessment in the U.S.  The staircase is specified in 
terms of five grade bands:  Grades 2-3, Grades 4-5, 
Grades 6-8, Grades 9-10 or Grades 11+.  Table 2 
shows the numbers of informational, literary and 
?Other? texts (includes both Mixed and speeches) 
included at each grade band.   
 
Grade 
Band 
Genre  
Total Inf. Lit. Other 
2-3 6 10 4 20 
4-5 16 10 4 30 
6-8 12 16 13 41 
9-10 12 10 17 39 
11+ 8 10 20 38 
Total 54 56 58 168 
 
Table 2.  Numbers of passages in the validation dataset, 
by grade band and genre. 
4   Genre Bias 
   
This section examines the root causes of genre bi-
as. We focus on two fundamental differences be-
tween informational and literary texts: differences 
in the types of vocabularies employed, and differ-
ences in the rate at which rare words are repeated.  
These differences have been examined in several 
previous studies.  For example, Lee (2001) docu-
mented differences in the use of ?core? vocabulary 
within a corpus of informational and literary texts 
that included over one million words downloaded 
from the British National Corpus.  Core vocabulary 
was defined in terms of a list of 2000 common 
words classified as appropriate for use in the dic-
tionary definitions presented in the Longman Dic-
tionary of Contemporary English.  The analyses 
demonstrated that core vocabulary usage was high-
er in literary texts than in informational texts.  For 
example, when literary texts such as fiction, poetry 
and drama were considered, the percent of total 
words classified as ?core? vocabulary ranged from 
81% to 84%.  By contrast, when informational 
texts such as science and social studies texts were 
considered, the percent of total words classified as 
?core? vocabulary ranged from 66% to 71%.  In 
interpreting these results Lee suggested that the 
creativity and imaginativeness typically associated 
with literary writing may be less closely tied to the 
type or level of vocabulary employed and more 
closely tied to the way that core words are used 
and combined.  Note that this implies that an indi-
vidual word detected in a literary text may not be 
indicative of the same level of processing chal-
lenge as that same word detected in an informa-
tional text. 
Differences in the vocabularies employed within 
informational and literary texts, and subsequent 
impacts on readability metrics, are also discussed 
in Appendix A of the Common Core State Stan-
dards (CCSSI, 2010).  The tendency of many exist-
ing readability metrics to underestimate the 
complexity levels of literary texts is described as 
follows: ?The Lexile Framework, like traditional 
formulas, may underestimate the difficulty of texts 
that use simple, familiar language to convey so-
phisticated ideas, as is true of much high-quality 
fiction written for adults and appropriate for older 
students? (p. 7).  
Genre bias may also result from genre-specific 
differences in word repetition rates.  Hiebert and 
51
Mesmer (2013, p.46) describe this phenomenon as 
follows:  ?Content area texts often receive inflated 
readability scores since key concept words that are 
rare (e.g., photosynthesis, inflation) are often re-
peated which increases vocabulary load, even 
though repetition of content words can support 
student learning (Cohen & Steinberg, 1983)?.  
Table 3 provides empirical evidence of these 
trends.  The table presents mean GL classifications 
estimated conditional on mean WF scores, for the 
informational (n = 399) and literary (n = 452) pas-
sages in our training dataset.  WF scores were gen-
erated via an in-house WF index constructed from 
a corpus of more than 400 million word tokens.  
The corpus includes more than 17,000 complete 
books, including both fiction and nonfiction titles.   
 
 
Avg. WF 
Informational Literary 
N GL SD N GL SD 
51.0?52.5 2 12.0 0.0 0 -- -- 
52.5?54.0 16 10.8 1.9 0 -- -- 
54.0?55.5 68 9.6 2.0 1 10.0 -- 
55.5?57.0 89 7.8 2.7 18 9.9 1.9 
57.0?58.5 96 6.6 2.3 46 9.2 2.0 
58.5?60.0 78 5.3 1.8 92 7.6 2.4 
60.0?61.5 44 4.6 1.8 142 6.2 2.4 
61.5?63.0 6 3.7 0.8 119 5.5 2.1 
63.0?64.5 0 -- -- 31 4.5 1.9 
64.5?66.0 0 -- -- 3 4.0 1.7 
Total 399 57.4 2.1 452 60.6 1.9 
 
Table 3.  Mean GL classifications, by Average WF 
score, for informational and literary passages targeted at 
readers in grades 3 through 12.   
 
The results in Table 3 confirm that, consistent 
with expectations, texts with lower average WF 
scores are more likely to appear on assessments 
targeted at older readers, while texts with higher 
average WF scores are more likely to appear on 
assessments targeted at younger readers.  But note 
that large genre differences are also present. Figure 
1 provides a graphical representation of these 
trends.  Results for informational texts are plotted 
with a solid line; those for literary texts are plotted 
with a dashed line. Note that the literary curve ap-
pears above the informational curve throughout the 
entire observed range of the data. This suggests 
that a given value of the Average WF measure is 
indicative of a higher GL classification if the text 
in question is a literary text, and a lower GL classi-
fication if the text in question is an informational 
text. Since a readability measure that includes this 
feature (or a feature similar to this feature) without 
also accounting for genre effects will tend to yield 
predictions that fall between the two curves, result-
ing GL predictions will tend to be too high for in-
formational texts (positive bias) and too low for 
literary texts (negative bias).   
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
Figure 1.  Mean text GL plotted conditional on average 
WF score. (One literary mean score based on evidence 
from a single text is not plotted.) 
 
Figure 2 confirms that this evidence-based pre-
diction holds true for two widely-used readability 
metrics: the Flesch-Kincaid GL score and the Lex-
ile Framework2
                                                          
2 All Lexile scores were obtained via the Lexile Analyzer 
available at www.lexile.com. Scores are only available for a 
subset of texts since our training corpus included just 548 
passages at the time that these data were collected. Corres-
ponding human GL classifications were approximately evenly 
distributed across grades 3 through 12. 
. Each individual plot compares 
Flesch-Kincaid GL scores (top row), or Lexile 
scores (bottom row) to the human GL classifica-
tions stored in our training dataset, i.e., classifica-
tions that were developed and reviewed by 
experienced educators, and were subsequently used 
to make high-stakes decisions about students and 
teachers, e.g., requiring students to repeat a grade 
rather than advancing to the next GL. The plots 
confirm that, in each case, the predicted pattern of 
over- and under-estimation is present. That is, on 
average, both Flesch-Kincaid scores and Lexile 
scores tend to be slightly too high for informational 
texts, and slightly too low for literary texts, thereby 
calling into doubt any cross-genre comparisons. 
Average ETS Word Frequency
M
ea
n 
G
ra
de
 L
ev
el
52 54 56 58 60 62 64 66
4
6
8
10
12 Literary
Informational
 
52
Human Grade Level
Le
xi
le
 S
co
re
0 5 10 15
60
0
80
0
10
00
12
00
14
00
Informational (n = 243)
Human Grade Level
Le
xi
le
 S
co
re
0 5 10 15
60
0
80
0
10
00
12
00
14
00
Literary (n = 305)
Human Grade Level
Fl
es
ch
-K
in
ca
id
 G
ra
de
 L
ev
el
0 5 10 15
0
5
10
15
Informational (n = 399)
Human Grade Level
Fl
es
ch
-K
in
ca
id
 G
ra
de
 L
ev
el
0 5 10 15
0
5
10
15
Literary (n = 452) 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2.  Passage complexity scores generated via the 
Flesch-Kincaid GL score (top) and the Lexile Frame-
work (bottom) compared to GL classifications provided 
by experienced educators. 
5  Features, Components and Results 
5.1 Features 
 
The TextEvaluator feature set is designed to 
measure the ease or difficulty of implementing 
four types of processes believed to be critically 
involved in comprehending complex text: (1) 
processes involved in word recognition and decod-
ing, (2) processes associated with using relevant 
syntactic knowledge to assemble words into mea-
ningful propositions, (3) processes associated with 
inferring connections across propositions or larger 
sections of text, and (4) processes associated with 
using relevant prior knowledge and experience to 
develop a more complete, more integrated mental 
representation of a text. (See Kintsch, 1998). 
A total of 43 candidate features were developed. 
Since many of these were expected to be moderate-
ly inter-correlated, a Principal Components Analy-
sis (PCA) was used to locate clusters of features 
that exhibited high within-cluster correlation and 
low between-cluster correlation.  Linear combina-
tions defined in terms of the resulting feature clus-
ters provided the independent variables considered 
in subsequent investigations.  Biber and his col-
leagues (2004) justify this approach by noting that, 
because many important aspects of text variation 
are not well captured by individual linguistic fea-
tures, investigation of such characteristics requires 
a focus on ?constellations of co-occurring linguis-
tic features? as opposed to individual features (p. 
45). 
The PCA suggested that more than 60% of the 
variation captured by the full set of 43 features 
could be accounted for via a set of eight compo-
nent scores, where each component is estimated as 
a linear combination of multiple correlated fea-
tures, and only 3 of the 43 features had moderately 
high loadings on more than one component, and 
most loadings exceeded 0.70.  The individual fea-
tures comprising each component are described 
below.  
Component #1:  Academic Vocabulary.  Ten 
features loaded heavily on this component.  Two 
are based on the Academic Word List described in 
Coxhead (2000). These include:  the frequency per 
thousand words of all words on the Academic 
Word List, and the ratio of listed words to total 
words.  In a previous study, Vajjala and Meurers 
(2012)  demonstrated that the ratio of listed words 
to total wards was very effective at distinguishing 
texts at lower and higher levels in the Weekly 
Reader corpus. Two additional features focus on 
the frequency of nominalizations, including one 
estimated from token counts and one estimated 
from type counts. Four additional features are 
based on word lists developed by Biber and his 
colleagues.  These include the frequency per thou-
sand words of academic verbs, abstract nouns, top-
ical adjectives and cognitive process nouns (see 
Biber, 1986, 1988; and Biber, et al, 2004). Two 
measures of word length also loaded on this di-
mension:  average word length measured in syl-
lables, and the frequency per thousand words of 
words containing more than 8 characters.  
Component #2:  Syntactic Complexity. Seven 
features loaded heavily on this component. These 
include features determined from the output of the 
Stanford  Parser (Klein and Manning, 2003), as 
well as more easily computed measures such as 
average sentence length, average frequency of long 
sentences (>= 25 words), and average number of 
53
words between punctuation marks (commas, semi-
colons, etc.).  Parse-based features include average 
number of dependent clauses, and an automated 
version of the word ?depth? measure introduced by 
Yngve (1960). This last feature, called Average 
Maximum Yngve Depth, is designed to capture 
variation in the memory load imposed by sentences 
with varying syntactic structures. It is estimated by 
first assigning a depth classification to each word 
in the text, then determining the maximum depth 
represented within each sentence, and then averag-
ing over resulting sentence-level estimates to ob-
tain a passage-level estimate.  Several studies of 
this word depth measure have been reported. For 
example, Bormuth (1964) reported a correlation of 
-0.78 between mean word depth scores and cloze 
fill-in rates provided by Japanese EFL learners.  
Component #3:  Concreteness. Words that are 
more concrete are more likely to evoke meaningful 
mental images, a response that has been shown to 
facilitate comprehension (Coltheart, 1981). Alder-
son (2000) argued that the level of concreteness 
present in a text is a useful feature to consider 
when evaluating passages for use on reading as-
sessments targeted at L2 readers. A total of five 
concreteness and imageability measures loaded 
heavily on this dimension.  All five measures are 
based on concreteness and imageability ratings 
downloaded from the MRC psycholinguistic data-
base (Coltheart, 1981).  Ratings are expressed on a 
7 point scale with 1 indicating least concrete, or 
least imageable, and 7 indicating most concrete or 
most imageable.   
Component #4:  Word Unfamiliarity. This com-
ponent summarizes variation detected via six dif-
ferent features.  Two features are measures of 
average word familiarity: one estimated via our in-
house WF Index, and one estimated via the TASA 
WF Index (see Zeno, et al, 1995).  Both features 
have negative loadings, suggesting that the com-
ponent is measuring vocabulary difficulty as op-
posed to vocabulary easiness. The other features 
with high loadings on this component are all meas-
ures of rare word frequency. These all have posi-
tive loadings since texts with large numbers of rare 
words are expected to be more difficult. Two types 
of rare word indices are included: indices based on 
token counts and indices based on type counts. 
Vocabulary measures based on token counts view 
each new word as an independent comprehension 
challenge, even when the same word occurs re-
peatedly throughout the text. By contrast, vocabu-
lary measures based on type counts assume that a 
passage containing five different unfamiliar words 
may be more challenging than a passage contain-
ing the same unfamiliar word repeated five times. 
This difference is consistent with the notion that 
each repetition of an unknown word provides an 
additional opportunity to connect to prior know-
ledge (Cohen & Steinberg, 1983).  
Component #5:  Interactive/Conversational 
Style.  This component includes the frequency per 
thousand words of:  conversation verbs, fiction 
verbs, communication verbs, 1st person plural pro-
nouns, contractions, and words enclosed in quotes.  
Verb types were determined from one or more of 
the following studies: Biber (1986),  Biber (1988), 
and Biber, et al (2004).   
Component #6:  Degree of Narrativity. Three 
features had high positive loadings on this dimen-
sion:  Frequency of past perfect aspect verbs, fre-
quency of past tense verbs and frequency of 3rd 
person singular pronouns.  All three features have 
previously been classified as providing positive 
evidence of the degree of narrativity exhibited in a 
text (see Biber, 1986 and Biber, 1988). 
Component #7:  Cohesion. Cohesion is that 
property of a text that enables it to be interpreted as 
a ?coherent message? rather than a collection of 
unrelated clauses and sentences.  Halliday and Ha-
san (1976) argued that readers are more likely to 
interpret a text as a ?coherent message?  when cer-
tain observable features are present.  These include 
repeated content words and explicit connectives.  
The seventh component extracted in the PCA in-
cludes three different types of cohesion features.  
The first two features measure the frequency of 
content word repetition across adjacent sentences 
within paragraphs. These measures differ from the 
cohesion measures discussed in Graesser et al 
(2004) and in Pitler and Nenkova (2008) in that a 
psychometric linking procedure is used to ensure 
that results for different texts are reported on com-
parable scales (See Sheehan, in press).  The fre-
quency of causal conjuncts (therefore, 
consequently, etc.) also loads on this dimension. 
Component #8:  Argumentation.  Two features 
have high loadings on this dimension:  the fre-
quency of concessive and adversative conjuncts 
(although, though, alternatively, in contrast, etc.), 
and the frequency of negations (no, neither, etc.), 
Just and Carpenter, (1987).  
54
5.2  An Automated Genre Classifier 
 
A preliminary automated genre classifier was 
developed by training a logistic regression model 
to predict the probability that a text is classified as 
informational  as opposed to literary.  A signifi-
cant positive coefficient was obtained for the Aca-
demic Vocabulary component defined above, 
suggesting that a high score on this component 
may be interpreted as an indication that the text is 
more likely to be informational.  Significant nega-
tive coefficients were obtained for Narrativity, In-
teractive/Conversational Style, and Syntactic 
Complexity, indicating that a high score on any of 
these components may be interpreted as an indica-
tion that the text is more likely to be literary.  Two 
individual features that were not included in the 
PCA were also significant:  the proportion of adja-
cent sentences containing at least one overlapping 
stemmed content word, and the frequency of 1st 
person singular pronouns.  These features were not 
included in the PCA because they are not reliably 
indicative of differences in text complexity (See 
Sheehan, in press; Pitler and Nenkova, 2008.) Re-
sults confirmed, however, that these features are 
useful for predicting a text?s genre classification.  
Alternative decision rules based on this model 
were investigated. Table 4 summarizes the levels 
of precision (P), recall (R) and F1 = 2RP/(R+P) 
obtained for the selected decision rule which was 
defined as follows: Classify as informational if 
P(Inf) >= 0.52, classify as literary if P(inf) < 0.48, 
else classify as mixed. This decision rule is defined 
such that few texts are classified into the mixed 
category since, at present, the training dataset in-
cludes very few mixed texts. The table shows de-
creased precision in the Validation dataset since 
many more mixed texts are included, and the ma-
jority of these were classified as informational. 
 
Dataset Genre N R P F1 
Training Inf 399 .84 .79 .81 
Training Lit 452 .88 .79 .83 
Training Mixed 83 .01 .09 .01 
Validation Inf 67 .91 .56 .69 
Validation Lit 56 .80 .80 .80 
Validation Mixed 45 .07 1.0 .13 
 
Table 4.  Levels of Precision, Recall and F1 obtained for 
1, 089 texts in the training and validation datasets.  
Speeches are not included in this summary. 
5.3  Prediction Equations 
 
We use separate genre-specific regression mod-
els to generate GL predictions for texts classified 
as informational, literary, or mixed. The coeffi-
cients estimated for informational and literary texts 
are shown in Table 5. Note that each component is 
significant in one or both models.  The table also 
highlights key genre differences. For example, note 
that the Interactive/Conv. Style score is significant 
in the Inf. model but not in the Literary model.  
This reflects the fact that, while literary texts at all 
GLs tend to exhibit relatively high interactivity, 
similarly high interactivity among inf. texts tends 
to only be present at the lowest GLs.  Thus, a high 
Interactivity is an indication of low complexity if 
the text in question is an informational text, but 
provides no statistically significant evidence about 
complexity if the text in question is a literary text.  
 
Component Informational Literary 
Academic Voc. 1.126* .824* 
Word Unfamiliarity .802* .793* 
Word Concreteness -.610* -.483* 
Syn. Complexity .983* 1.404* 
Lexical Cohesion -.266* -.440* 
Interactive/Conv. Style -.518* ns 
Degree of Narrativity ns -.361* 
Argumentation .431* ns 
 
Table 5. Regression coefficients estimated from training 
texts.  *p < .01, ns = not significant. 
 
5.4  Validity Evidence 
 
Two aspects of system validity are of interest: 
(a) whether genre bias is present, and (b) whether 
complexity scores correlate well with judgments 
provided by professional educators, i.e., the educa-
tors involved in selecting texts for use on high-
stakes state reading assessments. The issue of ge-
nre bias is addressed in Figure 3. Each plot com-
pares GL predictions generated via TextEvaluator 
to GL predictions provided by experienced educa-
tors.  Note that no evidence of a systematic tenden-
cy to under-predict the complexity levels of 
literary texts is present. This suggests that our 
strategy of developing distinct prediction models 
for informational and literary texts has succeeded 
in overcoming the genre biases present among 
many key features.  
55
Human Grade Level
Te
xt
E
va
lu
at
or
 G
ra
de
 L
ev
el
0 5 10 15
0
5
10
15
Informational (n = 399)
Human Grade Level
Te
xt
E
va
lu
at
or
 G
ra
de
 L
ev
el
0 5 10 15
0
5
10
15
Literary (n = 452)
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3.  TextEvaluator GL predictions compared to 
human GL classifications for informational and literary 
texts. 
 
TestEvaluator performance relative to the goal of 
predicting the human grade band classifications in 
the validation dataset was also examined. Results 
are summarized in Table 6 along with correspond-
ing results for the Lexile Framework (Stenner, et 
al., 2006) and the REAP system (Heilman, et al, 
2007).  All results are reprinted, with permission, 
from Nelson, et al, (2012).  In each case, perfor-
mance is summarized in terms of the Spearman 
rank order correlation between the readability 
scores generated for each text, and corresponding 
human grade band classifications.  95% confidence 
limits estimated via the Fisher r to z transformation 
are also listed.   
 
 
 
System 
Lower 
95% 
Bound 
 
Correlation 
Coefficient 
Upper 
95% 
Bound 
TextEvaluator 0.683 0.76 0.814 
REAP 0.427 0.54 0.641 
Lexile 0.380 0.50 0.607 
 
Table 6. Correlation  between readability scores and 
human grade band classifications for the 168 Common 
Core texts in the validation dataset.   
 
The comparison suggests that, relative to the task 
of  predicting the human grade band classifications 
assigned to the informational, literary and mixed 
texts in Appendix B of the new Common Core 
State Standards, TextEvaluator is significantly 
more effective than both the Lexile Framework 
and the REAP system. 
6  Summary and Discussion 
 
In many recent studies, proposed readability me-
trics have been trained and validated on text collec-
tions composed entirely of informational text, e.g., 
Wall Street Journal articles (Pitler and Nenkova, 
2008), Encyclopedia Britannica articles (Schwarm 
and Ostendorf, 2005) and Weekly Reader articles 
(Vajjala and Meurers, 2012). This paper considers 
the more challenging task of predicting human-
assigned GL classifications in a corpus of texts 
constructed to be representative of the broad range 
of reading materials considered by teachers and 
students in U.S. classrooms.   
Two approaches for modeling the complexity 
characteristics of these passages were compared.  
In Approach #1, a single, non-genre specific pre-
diction equation is estimated, and that equation is 
then applied to texts in all genres.  Two measures 
developed via this approach were evaluated:  the 
Lexile Framework and the REAP system.    
Approach #2 differs from Approach #1 in that 
genre-specific prediction equations are used, there-
by ensuring that important genre effects are ac-
commodated.  This approach is currently only 
available via the TextEvaluator system.  
Measures developed via each approach were 
evaluated on a held-out sample.  Results confirmed 
that complexity classifications obtained via                       
TextEvaluator are significantly more highly corre-
lated with the human grade band classifications in 
the held-out sample than are classifications ob-
tained via the Lexile Framework or REAP system.  
This study also demonstrated that, when genre 
effects are ignored, readability scores for informa-
tional texts tend to be overestimated, while those 
for literary texts tend to be underestimated. Note 
that this finding significantly complicates the 
process of using readability metrics to generate 
valid cross-genre comparisons. For example, 
Stajner, et al (2012) conclude that SimpleWiki 
may not serve as a ?gold standard? of high acces-
sibility because comparisons based on readability 
metrics suggest that it is more complex than Fic-
tion. We intend to further investigate this finding 
using TextEvaluator since conclusions that are not 
impacted by genre bias can then be reported. Addi-
tional planned work involves investigating addi-
tional measures of genre, and incorporating these 
into our genre classifier.    
56
References  
 
Alderson, J. C. (2000). Assessing reading. Cam-
bridge: Cambridge University Press. 
American Institutes for Research (2008). Reading 
framework for the 2009 National Assessment of 
Educational Progress.  Washington, DC: Na-
tional Assessment Governing Board. 
Biber, D. (1986).  Spoken and written textual di-
mension in English: Resolving the contradictory 
findings.  Language, 62: 394-414. 
Biber, D. (1988).  Variation across Speech and 
Writing.  Cambridge: Cambridge University 
Press. 
Biber, D., Conrad, S., Reppen, R., Byrd, P., Helt, 
M., Clark, V., et al, (2004).  Representing lan-
guage use in the university:  Analysis of the 
TOEFL 2000 Spoken and Written Academic 
Language Corpus.  TOEFL Monograph Series, 
MS-25, January 2004.  Princeton, NJ: Educa-
tional Testing Service.  
Bormuth, J.R. (1964).  Mean word depth as a pre-
dictor of comprehension difficulty.  California 
Journal of Educational Research, 15, 226-231. 
Cohen, S. A. & Steinberg, J. E. (1983). Effects of 
three types of vocabulary on readability of in-
termediate grade science textbooks:  An applica-
tion of Finn?s transfer feature theory.  Reading 
Research Quarterly, 19(1), 86-101. 
Collins-Thompson, K. and Callan, J. (2004). A 
language modeling approach to predicting read-
ing difficulty. In Proceedings of HLT/NAACL 
2004, Boston, USA. 
Coltheart, M. (1981).  The MRC psycholinguistic 
database, Quarterly Journal of Experimental 
Psychology, 33A, 497-505. 
Common Core State Standards Initiative (2010).  
Common core state standards for English lan-
guage arts & literacy in history/social studies, 
science and technical subjects.  Washington, 
DC: CCSSO & National Governors Association. 
Coxhead, A. (2000)  A new academic word list.  
TESOL Quarterly, 34(2), 213-238.  
Gunning, R. (1952).  The technique of clear writ-
ing. McGraw-Hill: New York. 
Graesser, A.C., McNamara, D. S., Louwerse, 
M.W. and Cai, Z. (2004).  Coh-Metrix:  Analy-
sis of text on cohesion and language. Behavior 
Research Methods, Instruments & Computers, 
36(2), 193-202.  
Halliday, M. A.K. & Hasan, R. (1976) Cohesion in 
English. Longman, London. 
Hiebert, E. H. & Mesmer, H. A. E. (2013).  Upping 
the ante of text complexity in the Common Core 
State Standards: Examining its potential impact 
on young readers. Educational Researcher, 
42(1), 44-51. 
Heilman, M., Collins-Thompson, K., Callan, J. & 
Eskenazi, M. (2007). Combining lexical and 
grammatical features to improve readability 
measures for first and second language texts. In 
Human Language Technologies 2007: The Con-
ference of the North American Chapter of the 
Association for Computational Linguistics 
(HLT-NAACL?07), 460-467. 
Just, M. A. & Carpenter, P. A. (1987). The psy-
chology of reading and language comprehen-
sion. Boston: Allyn & Bacon. 
Kincaid, J.P., Fishburne, R.P, Rogers, R.L. & 
Chissom, B.S. (1975). Derivation of new reada-
bility formulas (automated readability index, 
Fog count and Flesch reading ease formula) for 
navy enlisted personnel. Research Branch Re-
port 8-75. Naval Air Station, Memphis, TN. 
Kintsch, W. (1998). Comprehension: A paradigm 
for cognition. Cambridge, UK: Cambridge Uni-
versity Press. 
Klein, D. & Manning, C. D. (2003). Accurate Un-
lexicalized Parsing. In Proceedings of the 41st 
Meeting of the Association for Computational 
Linguistics, 423-430. 
Lee, D. Y. W. (2001)  Defining core vocabulary 
and tracking its distribution across spoken and 
written genres.  Journal of English Linguistics.  
29, 250-278. 
Nelson, J., Perfetti, C., Liben, D. and Liben, M. 
(2012). Measures of text difficulty: Testing their 
predictive value for grade levels and student 
performance. Technical Report, The Council of 
Chief State School Officers. 
57
Pitler, E. & Nenkova, A (2008). Revisiting reada-
bility:  A unified framework for predicting text 
quality. In Proceedings of the 2008 Conference 
on Empirical Methods in Natural Language 
Processing, Association for Computational Lin-
guistics, 186-195. 
Schwarm, S. & Ostendorf, M. (2005). Reading 
level assessment using support vector machines 
and statistical language models.  In Proceedings 
of the 43rd Annual Meeting of the Association for 
Computational Linguistics (ACL?05), 523-530. 
Sheehan, K.M. (in press).  Measuring cohesion: An 
approach that accounts for differences in the de-
gree of integration challenge presented by dif-
ferent types of sentences.  Educational 
Measurement: Issues and Practice. 
Sheehan, K.M., Kostin, I & Futagi, Y. (2008). 
When do standard approaches for measuring 
vocabulary difficulty, syntactic complexity and 
referential cohesion yield biased estimates of 
text difficulty?  In B.C. Love, K. McRae, & 
V.M. Sloutsky (Eds.), Proceedings of the 30th 
Annual Conference of the Cognitive Science 
Society, Washington D.C. 
 
Sheehan, K.M., Kostin, I., Futagi, Y. & Flor, M.  
(2010). Generating automated text complexity 
classifications that are aligned with targeted 
text complexity standards. (ETS RR-10-28). 
Princeton, NJ: ETS. 
 
Si, L. & Callan, J. (2001). A statistical model for 
scientific readability. In Proceedings of the 10th 
International Conference on Information and 
Knowledge Management (CIKM), 574-576. 
?tajner, S., Evans, R., Orasan, C., & Mitkov, R. 
(2012). What Can Readability Measures Really 
Tell Us About Text Complexity?. In Natural 
Language Processing for Improving Textual Ac-
cessibility (NLP4ITA) Workshop Programme                   
(p. 14). 
 
Stenner, A. J., Burdick, H., Sanford, E. & Burdick, 
D. (2006). How accurate are Lexile text meas-
ures?  Journal of Applied Measurement, 7(3), 
307-322. 
 
Vajjala, S. & Meurers, D. (2012). On improving 
the accuracy of readability classification using 
insights from second language acquisition. In 
Proceedings of the 7th Workshop on the Innova-
tive Use of NLP for Building Educational Appli-
cations, 163-173. 
Yngve, V.H. (1960).  A model and an hypothesis 
for language structure.  Proceedings of the 
American Philosophical Society, 104, 444-466. 
Zeno, S. M., Ivens, S. H., Millard, R. T., Duvvuri, 
R. (1995). The educator?s word frequency 
guide. Brewster, NY: Touchstone Applied 
Science Associates.  
  
58
